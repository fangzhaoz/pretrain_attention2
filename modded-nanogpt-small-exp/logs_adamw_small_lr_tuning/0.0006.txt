import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--adamw_lr', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_lr_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_lr_tuning/{args2.adamw_lr}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=float(args2.adamw_lr),  betas=(0.85, 0.85), eps=1e-10, weight_decay=0.0, fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_lr_tuning/{args2.adamw_lr}", exist_ok=True)
            torch.save(log, f"logs_adamw_small_lr_tuning/{args2.adamw_lr}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 01:19:17 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   35C    P0             118W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   35C    P0             118W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             118W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   28C    P0             115W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   34C    P0             119W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             114W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.06ms
step:1/2330 train_time:85ms step_avg:84.85ms
step:2/2330 train_time:195ms step_avg:97.38ms
step:3/2330 train_time:214ms step_avg:71.30ms
step:4/2330 train_time:233ms step_avg:58.37ms
step:5/2330 train_time:286ms step_avg:57.15ms
step:6/2330 train_time:343ms step_avg:57.20ms
step:7/2330 train_time:398ms step_avg:56.89ms
step:8/2330 train_time:458ms step_avg:57.20ms
step:9/2330 train_time:513ms step_avg:57.02ms
step:10/2330 train_time:571ms step_avg:57.13ms
step:11/2330 train_time:627ms step_avg:56.96ms
step:12/2330 train_time:685ms step_avg:57.12ms
step:13/2330 train_time:741ms step_avg:56.97ms
step:14/2330 train_time:800ms step_avg:57.12ms
step:15/2330 train_time:855ms step_avg:57.01ms
step:16/2330 train_time:913ms step_avg:57.08ms
step:17/2330 train_time:969ms step_avg:56.98ms
step:18/2330 train_time:1028ms step_avg:57.09ms
step:19/2330 train_time:1084ms step_avg:57.03ms
step:20/2330 train_time:1147ms step_avg:57.36ms
step:21/2330 train_time:1205ms step_avg:57.39ms
step:22/2330 train_time:1265ms step_avg:57.51ms
step:23/2330 train_time:1322ms step_avg:57.50ms
step:24/2330 train_time:1381ms step_avg:57.54ms
step:25/2330 train_time:1437ms step_avg:57.47ms
step:26/2330 train_time:1495ms step_avg:57.49ms
step:27/2330 train_time:1551ms step_avg:57.43ms
step:28/2330 train_time:1609ms step_avg:57.46ms
step:29/2330 train_time:1666ms step_avg:57.43ms
step:30/2330 train_time:1724ms step_avg:57.46ms
step:31/2330 train_time:1779ms step_avg:57.39ms
step:32/2330 train_time:1838ms step_avg:57.43ms
step:33/2330 train_time:1893ms step_avg:57.36ms
step:34/2330 train_time:1952ms step_avg:57.42ms
step:35/2330 train_time:2010ms step_avg:57.42ms
step:36/2330 train_time:2069ms step_avg:57.47ms
step:37/2330 train_time:2126ms step_avg:57.45ms
step:38/2330 train_time:2186ms step_avg:57.52ms
step:39/2330 train_time:2242ms step_avg:57.49ms
step:40/2330 train_time:2301ms step_avg:57.54ms
step:41/2330 train_time:2358ms step_avg:57.52ms
step:42/2330 train_time:2417ms step_avg:57.55ms
step:43/2330 train_time:2473ms step_avg:57.51ms
step:44/2330 train_time:2531ms step_avg:57.53ms
step:45/2330 train_time:2587ms step_avg:57.49ms
step:46/2330 train_time:2645ms step_avg:57.51ms
step:47/2330 train_time:2701ms step_avg:57.47ms
step:48/2330 train_time:2760ms step_avg:57.49ms
step:49/2330 train_time:2815ms step_avg:57.45ms
step:50/2330 train_time:2873ms step_avg:57.47ms
step:51/2330 train_time:2929ms step_avg:57.44ms
step:52/2330 train_time:2989ms step_avg:57.48ms
step:53/2330 train_time:3045ms step_avg:57.45ms
step:54/2330 train_time:3104ms step_avg:57.48ms
step:55/2330 train_time:3161ms step_avg:57.47ms
step:56/2330 train_time:3220ms step_avg:57.50ms
step:57/2330 train_time:3277ms step_avg:57.49ms
step:58/2330 train_time:3336ms step_avg:57.51ms
step:59/2330 train_time:3392ms step_avg:57.49ms
step:60/2330 train_time:3451ms step_avg:57.52ms
step:61/2330 train_time:3507ms step_avg:57.49ms
step:62/2330 train_time:3566ms step_avg:57.51ms
step:63/2330 train_time:3622ms step_avg:57.49ms
step:64/2330 train_time:3680ms step_avg:57.50ms
step:65/2330 train_time:3735ms step_avg:57.47ms
step:66/2330 train_time:3794ms step_avg:57.49ms
step:67/2330 train_time:3850ms step_avg:57.46ms
step:68/2330 train_time:3909ms step_avg:57.49ms
step:69/2330 train_time:3965ms step_avg:57.47ms
step:70/2330 train_time:4024ms step_avg:57.49ms
step:71/2330 train_time:4080ms step_avg:57.47ms
step:72/2330 train_time:4139ms step_avg:57.49ms
step:73/2330 train_time:4195ms step_avg:57.47ms
step:74/2330 train_time:4254ms step_avg:57.49ms
step:75/2330 train_time:4310ms step_avg:57.47ms
step:76/2330 train_time:4370ms step_avg:57.50ms
step:77/2330 train_time:4427ms step_avg:57.50ms
step:78/2330 train_time:4486ms step_avg:57.51ms
step:79/2330 train_time:4542ms step_avg:57.50ms
step:80/2330 train_time:4601ms step_avg:57.51ms
step:81/2330 train_time:4656ms step_avg:57.49ms
step:82/2330 train_time:4716ms step_avg:57.51ms
step:83/2330 train_time:4771ms step_avg:57.48ms
step:84/2330 train_time:4830ms step_avg:57.50ms
step:85/2330 train_time:4886ms step_avg:57.48ms
step:86/2330 train_time:4945ms step_avg:57.50ms
step:87/2330 train_time:5001ms step_avg:57.48ms
step:88/2330 train_time:5060ms step_avg:57.50ms
step:89/2330 train_time:5115ms step_avg:57.48ms
step:90/2330 train_time:5176ms step_avg:57.51ms
step:91/2330 train_time:5231ms step_avg:57.49ms
step:92/2330 train_time:5292ms step_avg:57.52ms
step:93/2330 train_time:5347ms step_avg:57.49ms
step:94/2330 train_time:5406ms step_avg:57.51ms
step:95/2330 train_time:5462ms step_avg:57.50ms
step:96/2330 train_time:5521ms step_avg:57.51ms
step:97/2330 train_time:5577ms step_avg:57.49ms
step:98/2330 train_time:5636ms step_avg:57.51ms
step:99/2330 train_time:5691ms step_avg:57.49ms
step:100/2330 train_time:5750ms step_avg:57.50ms
step:101/2330 train_time:5806ms step_avg:57.48ms
step:102/2330 train_time:5865ms step_avg:57.50ms
step:103/2330 train_time:5920ms step_avg:57.48ms
step:104/2330 train_time:5980ms step_avg:57.50ms
step:105/2330 train_time:6035ms step_avg:57.48ms
step:106/2330 train_time:6095ms step_avg:57.50ms
step:107/2330 train_time:6150ms step_avg:57.48ms
step:108/2330 train_time:6211ms step_avg:57.51ms
step:109/2330 train_time:6266ms step_avg:57.49ms
step:110/2330 train_time:6325ms step_avg:57.50ms
step:111/2330 train_time:6382ms step_avg:57.49ms
step:112/2330 train_time:6441ms step_avg:57.50ms
step:113/2330 train_time:6496ms step_avg:57.49ms
step:114/2330 train_time:6555ms step_avg:57.50ms
step:115/2330 train_time:6611ms step_avg:57.49ms
step:116/2330 train_time:6670ms step_avg:57.50ms
step:117/2330 train_time:6726ms step_avg:57.49ms
step:118/2330 train_time:6785ms step_avg:57.50ms
step:119/2330 train_time:6840ms step_avg:57.48ms
step:120/2330 train_time:6900ms step_avg:57.50ms
step:121/2330 train_time:6956ms step_avg:57.49ms
step:122/2330 train_time:7015ms step_avg:57.50ms
step:123/2330 train_time:7070ms step_avg:57.48ms
step:124/2330 train_time:7130ms step_avg:57.50ms
step:125/2330 train_time:7186ms step_avg:57.49ms
step:126/2330 train_time:7246ms step_avg:57.51ms
step:127/2330 train_time:7302ms step_avg:57.50ms
step:128/2330 train_time:7361ms step_avg:57.51ms
step:129/2330 train_time:7417ms step_avg:57.50ms
step:130/2330 train_time:7477ms step_avg:57.51ms
step:131/2330 train_time:7533ms step_avg:57.50ms
step:132/2330 train_time:7592ms step_avg:57.51ms
step:133/2330 train_time:7648ms step_avg:57.50ms
step:134/2330 train_time:7706ms step_avg:57.51ms
step:135/2330 train_time:7762ms step_avg:57.49ms
step:136/2330 train_time:7821ms step_avg:57.51ms
step:137/2330 train_time:7877ms step_avg:57.49ms
step:138/2330 train_time:7935ms step_avg:57.50ms
step:139/2330 train_time:7990ms step_avg:57.48ms
step:140/2330 train_time:8051ms step_avg:57.51ms
step:141/2330 train_time:8108ms step_avg:57.50ms
step:142/2330 train_time:8167ms step_avg:57.51ms
step:143/2330 train_time:8223ms step_avg:57.50ms
step:144/2330 train_time:8282ms step_avg:57.51ms
step:145/2330 train_time:8338ms step_avg:57.50ms
step:146/2330 train_time:8396ms step_avg:57.51ms
step:147/2330 train_time:8452ms step_avg:57.50ms
step:148/2330 train_time:8511ms step_avg:57.51ms
step:149/2330 train_time:8568ms step_avg:57.50ms
step:150/2330 train_time:8627ms step_avg:57.51ms
step:151/2330 train_time:8683ms step_avg:57.51ms
step:152/2330 train_time:8742ms step_avg:57.51ms
step:153/2330 train_time:8798ms step_avg:57.50ms
step:154/2330 train_time:8857ms step_avg:57.51ms
step:155/2330 train_time:8912ms step_avg:57.50ms
step:156/2330 train_time:8972ms step_avg:57.51ms
step:157/2330 train_time:9027ms step_avg:57.50ms
step:158/2330 train_time:9086ms step_avg:57.51ms
step:159/2330 train_time:9142ms step_avg:57.50ms
step:160/2330 train_time:9202ms step_avg:57.51ms
step:161/2330 train_time:9258ms step_avg:57.50ms
step:162/2330 train_time:9317ms step_avg:57.51ms
step:163/2330 train_time:9373ms step_avg:57.50ms
step:164/2330 train_time:9432ms step_avg:57.51ms
step:165/2330 train_time:9488ms step_avg:57.50ms
step:166/2330 train_time:9548ms step_avg:57.52ms
step:167/2330 train_time:9604ms step_avg:57.51ms
step:168/2330 train_time:9664ms step_avg:57.52ms
step:169/2330 train_time:9720ms step_avg:57.52ms
step:170/2330 train_time:9779ms step_avg:57.52ms
step:171/2330 train_time:9834ms step_avg:57.51ms
step:172/2330 train_time:9895ms step_avg:57.53ms
step:173/2330 train_time:9950ms step_avg:57.52ms
step:174/2330 train_time:10009ms step_avg:57.52ms
step:175/2330 train_time:10064ms step_avg:57.51ms
step:176/2330 train_time:10123ms step_avg:57.52ms
step:177/2330 train_time:10179ms step_avg:57.51ms
step:178/2330 train_time:10238ms step_avg:57.52ms
step:179/2330 train_time:10293ms step_avg:57.51ms
step:180/2330 train_time:10353ms step_avg:57.52ms
step:181/2330 train_time:10409ms step_avg:57.51ms
step:182/2330 train_time:10468ms step_avg:57.52ms
step:183/2330 train_time:10524ms step_avg:57.51ms
step:184/2330 train_time:10583ms step_avg:57.51ms
step:185/2330 train_time:10639ms step_avg:57.51ms
step:186/2330 train_time:10698ms step_avg:57.51ms
step:187/2330 train_time:10753ms step_avg:57.51ms
step:188/2330 train_time:10812ms step_avg:57.51ms
step:189/2330 train_time:10869ms step_avg:57.51ms
step:190/2330 train_time:10927ms step_avg:57.51ms
step:191/2330 train_time:10983ms step_avg:57.50ms
step:192/2330 train_time:11042ms step_avg:57.51ms
step:193/2330 train_time:11097ms step_avg:57.50ms
step:194/2330 train_time:11156ms step_avg:57.51ms
step:195/2330 train_time:11212ms step_avg:57.50ms
step:196/2330 train_time:11272ms step_avg:57.51ms
step:197/2330 train_time:11329ms step_avg:57.51ms
step:198/2330 train_time:11387ms step_avg:57.51ms
step:199/2330 train_time:11442ms step_avg:57.50ms
step:200/2330 train_time:11502ms step_avg:57.51ms
step:201/2330 train_time:11558ms step_avg:57.50ms
step:202/2330 train_time:11617ms step_avg:57.51ms
step:203/2330 train_time:11672ms step_avg:57.50ms
step:204/2330 train_time:11731ms step_avg:57.51ms
step:205/2330 train_time:11787ms step_avg:57.50ms
step:206/2330 train_time:11846ms step_avg:57.51ms
step:207/2330 train_time:11902ms step_avg:57.50ms
step:208/2330 train_time:11961ms step_avg:57.51ms
step:209/2330 train_time:12017ms step_avg:57.50ms
step:210/2330 train_time:12076ms step_avg:57.50ms
step:211/2330 train_time:12131ms step_avg:57.49ms
step:212/2330 train_time:12191ms step_avg:57.50ms
step:213/2330 train_time:12247ms step_avg:57.50ms
step:214/2330 train_time:12305ms step_avg:57.50ms
step:215/2330 train_time:12361ms step_avg:57.49ms
step:216/2330 train_time:12420ms step_avg:57.50ms
step:217/2330 train_time:12475ms step_avg:57.49ms
step:218/2330 train_time:12534ms step_avg:57.50ms
step:219/2330 train_time:12591ms step_avg:57.49ms
step:220/2330 train_time:12650ms step_avg:57.50ms
step:221/2330 train_time:12707ms step_avg:57.50ms
step:222/2330 train_time:12765ms step_avg:57.50ms
step:223/2330 train_time:12821ms step_avg:57.49ms
step:224/2330 train_time:12879ms step_avg:57.50ms
step:225/2330 train_time:12935ms step_avg:57.49ms
step:226/2330 train_time:12994ms step_avg:57.50ms
step:227/2330 train_time:13050ms step_avg:57.49ms
step:228/2330 train_time:13109ms step_avg:57.50ms
step:229/2330 train_time:13166ms step_avg:57.49ms
step:230/2330 train_time:13224ms step_avg:57.50ms
step:231/2330 train_time:13280ms step_avg:57.49ms
step:232/2330 train_time:13339ms step_avg:57.49ms
step:233/2330 train_time:13395ms step_avg:57.49ms
step:234/2330 train_time:13455ms step_avg:57.50ms
step:235/2330 train_time:13510ms step_avg:57.49ms
step:236/2330 train_time:13570ms step_avg:57.50ms
step:237/2330 train_time:13626ms step_avg:57.49ms
step:238/2330 train_time:13685ms step_avg:57.50ms
step:239/2330 train_time:13741ms step_avg:57.49ms
step:240/2330 train_time:13800ms step_avg:57.50ms
step:241/2330 train_time:13856ms step_avg:57.49ms
step:242/2330 train_time:13915ms step_avg:57.50ms
step:243/2330 train_time:13971ms step_avg:57.50ms
step:244/2330 train_time:14030ms step_avg:57.50ms
step:245/2330 train_time:14086ms step_avg:57.49ms
step:246/2330 train_time:14145ms step_avg:57.50ms
step:247/2330 train_time:14201ms step_avg:57.49ms
step:248/2330 train_time:14260ms step_avg:57.50ms
step:249/2330 train_time:14317ms step_avg:57.50ms
step:250/2330 train_time:14375ms step_avg:57.50ms
step:250/2330 val_loss:4.8994 train_time:14454ms step_avg:57.82ms
step:251/2330 train_time:14472ms step_avg:57.66ms
step:252/2330 train_time:14492ms step_avg:57.51ms
step:253/2330 train_time:14548ms step_avg:57.50ms
step:254/2330 train_time:14611ms step_avg:57.52ms
step:255/2330 train_time:14666ms step_avg:57.51ms
step:256/2330 train_time:14730ms step_avg:57.54ms
step:257/2330 train_time:14785ms step_avg:57.53ms
step:258/2330 train_time:14845ms step_avg:57.54ms
step:259/2330 train_time:14900ms step_avg:57.53ms
step:260/2330 train_time:14961ms step_avg:57.54ms
step:261/2330 train_time:15016ms step_avg:57.53ms
step:262/2330 train_time:15075ms step_avg:57.54ms
step:263/2330 train_time:15130ms step_avg:57.53ms
step:264/2330 train_time:15188ms step_avg:57.53ms
step:265/2330 train_time:15244ms step_avg:57.52ms
step:266/2330 train_time:15302ms step_avg:57.53ms
step:267/2330 train_time:15358ms step_avg:57.52ms
step:268/2330 train_time:15417ms step_avg:57.53ms
step:269/2330 train_time:15475ms step_avg:57.53ms
step:270/2330 train_time:15534ms step_avg:57.53ms
step:271/2330 train_time:15590ms step_avg:57.53ms
step:272/2330 train_time:15652ms step_avg:57.55ms
step:273/2330 train_time:15708ms step_avg:57.54ms
step:274/2330 train_time:15768ms step_avg:57.55ms
step:275/2330 train_time:15824ms step_avg:57.54ms
step:276/2330 train_time:15884ms step_avg:57.55ms
step:277/2330 train_time:15939ms step_avg:57.54ms
step:278/2330 train_time:15998ms step_avg:57.55ms
step:279/2330 train_time:16053ms step_avg:57.54ms
step:280/2330 train_time:16112ms step_avg:57.54ms
step:281/2330 train_time:16167ms step_avg:57.54ms
step:282/2330 train_time:16226ms step_avg:57.54ms
step:283/2330 train_time:16282ms step_avg:57.53ms
step:284/2330 train_time:16340ms step_avg:57.54ms
step:285/2330 train_time:16396ms step_avg:57.53ms
step:286/2330 train_time:16456ms step_avg:57.54ms
step:287/2330 train_time:16512ms step_avg:57.53ms
step:288/2330 train_time:16572ms step_avg:57.54ms
step:289/2330 train_time:16629ms step_avg:57.54ms
step:290/2330 train_time:16688ms step_avg:57.55ms
step:291/2330 train_time:16744ms step_avg:57.54ms
step:292/2330 train_time:16804ms step_avg:57.55ms
step:293/2330 train_time:16859ms step_avg:57.54ms
step:294/2330 train_time:16919ms step_avg:57.55ms
step:295/2330 train_time:16974ms step_avg:57.54ms
step:296/2330 train_time:17035ms step_avg:57.55ms
step:297/2330 train_time:17090ms step_avg:57.54ms
step:298/2330 train_time:17150ms step_avg:57.55ms
step:299/2330 train_time:17205ms step_avg:57.54ms
step:300/2330 train_time:17264ms step_avg:57.55ms
step:301/2330 train_time:17319ms step_avg:57.54ms
step:302/2330 train_time:17380ms step_avg:57.55ms
step:303/2330 train_time:17436ms step_avg:57.54ms
step:304/2330 train_time:17495ms step_avg:57.55ms
step:305/2330 train_time:17552ms step_avg:57.55ms
step:306/2330 train_time:17611ms step_avg:57.55ms
step:307/2330 train_time:17668ms step_avg:57.55ms
step:308/2330 train_time:17727ms step_avg:57.56ms
step:309/2330 train_time:17783ms step_avg:57.55ms
step:310/2330 train_time:17843ms step_avg:57.56ms
step:311/2330 train_time:17899ms step_avg:57.55ms
step:312/2330 train_time:17959ms step_avg:57.56ms
step:313/2330 train_time:18015ms step_avg:57.56ms
step:314/2330 train_time:18076ms step_avg:57.57ms
step:315/2330 train_time:18132ms step_avg:57.56ms
step:316/2330 train_time:18190ms step_avg:57.56ms
step:317/2330 train_time:18246ms step_avg:57.56ms
step:318/2330 train_time:18304ms step_avg:57.56ms
step:319/2330 train_time:18360ms step_avg:57.55ms
step:320/2330 train_time:18419ms step_avg:57.56ms
step:321/2330 train_time:18475ms step_avg:57.55ms
step:322/2330 train_time:18534ms step_avg:57.56ms
step:323/2330 train_time:18591ms step_avg:57.56ms
step:324/2330 train_time:18650ms step_avg:57.56ms
step:325/2330 train_time:18707ms step_avg:57.56ms
step:326/2330 train_time:18765ms step_avg:57.56ms
step:327/2330 train_time:18821ms step_avg:57.56ms
step:328/2330 train_time:18881ms step_avg:57.56ms
step:329/2330 train_time:18936ms step_avg:57.56ms
step:330/2330 train_time:18997ms step_avg:57.57ms
step:331/2330 train_time:19052ms step_avg:57.56ms
step:332/2330 train_time:19112ms step_avg:57.57ms
step:333/2330 train_time:19168ms step_avg:57.56ms
step:334/2330 train_time:19227ms step_avg:57.57ms
step:335/2330 train_time:19283ms step_avg:57.56ms
step:336/2330 train_time:19341ms step_avg:57.56ms
step:337/2330 train_time:19397ms step_avg:57.56ms
step:338/2330 train_time:19458ms step_avg:57.57ms
step:339/2330 train_time:19514ms step_avg:57.56ms
step:340/2330 train_time:19572ms step_avg:57.57ms
step:341/2330 train_time:19630ms step_avg:57.56ms
step:342/2330 train_time:19689ms step_avg:57.57ms
step:343/2330 train_time:19745ms step_avg:57.57ms
step:344/2330 train_time:19804ms step_avg:57.57ms
step:345/2330 train_time:19859ms step_avg:57.56ms
step:346/2330 train_time:19918ms step_avg:57.57ms
step:347/2330 train_time:19974ms step_avg:57.56ms
step:348/2330 train_time:20034ms step_avg:57.57ms
step:349/2330 train_time:20090ms step_avg:57.57ms
step:350/2330 train_time:20150ms step_avg:57.57ms
step:351/2330 train_time:20206ms step_avg:57.57ms
step:352/2330 train_time:20265ms step_avg:57.57ms
step:353/2330 train_time:20320ms step_avg:57.56ms
step:354/2330 train_time:20380ms step_avg:57.57ms
step:355/2330 train_time:20436ms step_avg:57.57ms
step:356/2330 train_time:20496ms step_avg:57.57ms
step:357/2330 train_time:20552ms step_avg:57.57ms
step:358/2330 train_time:20611ms step_avg:57.57ms
step:359/2330 train_time:20667ms step_avg:57.57ms
step:360/2330 train_time:20726ms step_avg:57.57ms
step:361/2330 train_time:20782ms step_avg:57.57ms
step:362/2330 train_time:20841ms step_avg:57.57ms
step:363/2330 train_time:20897ms step_avg:57.57ms
step:364/2330 train_time:20956ms step_avg:57.57ms
step:365/2330 train_time:21012ms step_avg:57.57ms
step:366/2330 train_time:21072ms step_avg:57.57ms
step:367/2330 train_time:21129ms step_avg:57.57ms
step:368/2330 train_time:21187ms step_avg:57.57ms
step:369/2330 train_time:21243ms step_avg:57.57ms
step:370/2330 train_time:21301ms step_avg:57.57ms
step:371/2330 train_time:21357ms step_avg:57.57ms
step:372/2330 train_time:21417ms step_avg:57.57ms
step:373/2330 train_time:21473ms step_avg:57.57ms
step:374/2330 train_time:21532ms step_avg:57.57ms
step:375/2330 train_time:21588ms step_avg:57.57ms
step:376/2330 train_time:21647ms step_avg:57.57ms
step:377/2330 train_time:21703ms step_avg:57.57ms
step:378/2330 train_time:21763ms step_avg:57.57ms
step:379/2330 train_time:21819ms step_avg:57.57ms
step:380/2330 train_time:21879ms step_avg:57.58ms
step:381/2330 train_time:21935ms step_avg:57.57ms
step:382/2330 train_time:21994ms step_avg:57.58ms
step:383/2330 train_time:22051ms step_avg:57.57ms
step:384/2330 train_time:22110ms step_avg:57.58ms
step:385/2330 train_time:22166ms step_avg:57.57ms
step:386/2330 train_time:22225ms step_avg:57.58ms
step:387/2330 train_time:22281ms step_avg:57.57ms
step:388/2330 train_time:22341ms step_avg:57.58ms
step:389/2330 train_time:22396ms step_avg:57.57ms
step:390/2330 train_time:22457ms step_avg:57.58ms
step:391/2330 train_time:22513ms step_avg:57.58ms
step:392/2330 train_time:22572ms step_avg:57.58ms
step:393/2330 train_time:22630ms step_avg:57.58ms
step:394/2330 train_time:22688ms step_avg:57.58ms
step:395/2330 train_time:22744ms step_avg:57.58ms
step:396/2330 train_time:22802ms step_avg:57.58ms
step:397/2330 train_time:22858ms step_avg:57.58ms
step:398/2330 train_time:22918ms step_avg:57.58ms
step:399/2330 train_time:22974ms step_avg:57.58ms
step:400/2330 train_time:23033ms step_avg:57.58ms
step:401/2330 train_time:23090ms step_avg:57.58ms
step:402/2330 train_time:23150ms step_avg:57.59ms
step:403/2330 train_time:23206ms step_avg:57.58ms
step:404/2330 train_time:23264ms step_avg:57.58ms
step:405/2330 train_time:23320ms step_avg:57.58ms
step:406/2330 train_time:23380ms step_avg:57.59ms
step:407/2330 train_time:23435ms step_avg:57.58ms
step:408/2330 train_time:23496ms step_avg:57.59ms
step:409/2330 train_time:23553ms step_avg:57.59ms
step:410/2330 train_time:23613ms step_avg:57.59ms
step:411/2330 train_time:23669ms step_avg:57.59ms
step:412/2330 train_time:23728ms step_avg:57.59ms
step:413/2330 train_time:23784ms step_avg:57.59ms
step:414/2330 train_time:23843ms step_avg:57.59ms
step:415/2330 train_time:23898ms step_avg:57.59ms
step:416/2330 train_time:23958ms step_avg:57.59ms
step:417/2330 train_time:24014ms step_avg:57.59ms
step:418/2330 train_time:24074ms step_avg:57.59ms
step:419/2330 train_time:24130ms step_avg:57.59ms
step:420/2330 train_time:24190ms step_avg:57.60ms
step:421/2330 train_time:24246ms step_avg:57.59ms
step:422/2330 train_time:24304ms step_avg:57.59ms
step:423/2330 train_time:24361ms step_avg:57.59ms
step:424/2330 train_time:24419ms step_avg:57.59ms
step:425/2330 train_time:24475ms step_avg:57.59ms
step:426/2330 train_time:24536ms step_avg:57.60ms
step:427/2330 train_time:24592ms step_avg:57.59ms
step:428/2330 train_time:24651ms step_avg:57.60ms
step:429/2330 train_time:24707ms step_avg:57.59ms
step:430/2330 train_time:24766ms step_avg:57.60ms
step:431/2330 train_time:24823ms step_avg:57.59ms
step:432/2330 train_time:24881ms step_avg:57.60ms
step:433/2330 train_time:24937ms step_avg:57.59ms
step:434/2330 train_time:24997ms step_avg:57.60ms
step:435/2330 train_time:25053ms step_avg:57.59ms
step:436/2330 train_time:25113ms step_avg:57.60ms
step:437/2330 train_time:25170ms step_avg:57.60ms
step:438/2330 train_time:25229ms step_avg:57.60ms
step:439/2330 train_time:25285ms step_avg:57.60ms
step:440/2330 train_time:25344ms step_avg:57.60ms
step:441/2330 train_time:25399ms step_avg:57.60ms
step:442/2330 train_time:25461ms step_avg:57.60ms
step:443/2330 train_time:25516ms step_avg:57.60ms
step:444/2330 train_time:25578ms step_avg:57.61ms
step:445/2330 train_time:25634ms step_avg:57.60ms
step:446/2330 train_time:25693ms step_avg:57.61ms
step:447/2330 train_time:25750ms step_avg:57.61ms
step:448/2330 train_time:25809ms step_avg:57.61ms
step:449/2330 train_time:25865ms step_avg:57.61ms
step:450/2330 train_time:25924ms step_avg:57.61ms
step:451/2330 train_time:25980ms step_avg:57.61ms
step:452/2330 train_time:26040ms step_avg:57.61ms
step:453/2330 train_time:26096ms step_avg:57.61ms
step:454/2330 train_time:26156ms step_avg:57.61ms
step:455/2330 train_time:26213ms step_avg:57.61ms
step:456/2330 train_time:26272ms step_avg:57.61ms
step:457/2330 train_time:26329ms step_avg:57.61ms
step:458/2330 train_time:26388ms step_avg:57.62ms
step:459/2330 train_time:26444ms step_avg:57.61ms
step:460/2330 train_time:26503ms step_avg:57.62ms
step:461/2330 train_time:26559ms step_avg:57.61ms
step:462/2330 train_time:26618ms step_avg:57.62ms
step:463/2330 train_time:26674ms step_avg:57.61ms
step:464/2330 train_time:26734ms step_avg:57.62ms
step:465/2330 train_time:26791ms step_avg:57.61ms
step:466/2330 train_time:26849ms step_avg:57.62ms
step:467/2330 train_time:26905ms step_avg:57.61ms
step:468/2330 train_time:26964ms step_avg:57.61ms
step:469/2330 train_time:27020ms step_avg:57.61ms
step:470/2330 train_time:27081ms step_avg:57.62ms
step:471/2330 train_time:27136ms step_avg:57.61ms
step:472/2330 train_time:27197ms step_avg:57.62ms
step:473/2330 train_time:27253ms step_avg:57.62ms
step:474/2330 train_time:27311ms step_avg:57.62ms
step:475/2330 train_time:27368ms step_avg:57.62ms
step:476/2330 train_time:27427ms step_avg:57.62ms
step:477/2330 train_time:27483ms step_avg:57.62ms
step:478/2330 train_time:27542ms step_avg:57.62ms
step:479/2330 train_time:27598ms step_avg:57.61ms
step:480/2330 train_time:27659ms step_avg:57.62ms
step:481/2330 train_time:27714ms step_avg:57.62ms
step:482/2330 train_time:27775ms step_avg:57.62ms
step:483/2330 train_time:27831ms step_avg:57.62ms
step:484/2330 train_time:27890ms step_avg:57.62ms
step:485/2330 train_time:27946ms step_avg:57.62ms
step:486/2330 train_time:28005ms step_avg:57.62ms
step:487/2330 train_time:28060ms step_avg:57.62ms
step:488/2330 train_time:28120ms step_avg:57.62ms
step:489/2330 train_time:28176ms step_avg:57.62ms
step:490/2330 train_time:28236ms step_avg:57.62ms
step:491/2330 train_time:28292ms step_avg:57.62ms
step:492/2330 train_time:28351ms step_avg:57.62ms
step:493/2330 train_time:28407ms step_avg:57.62ms
step:494/2330 train_time:28466ms step_avg:57.62ms
step:495/2330 train_time:28522ms step_avg:57.62ms
step:496/2330 train_time:28581ms step_avg:57.62ms
step:497/2330 train_time:28637ms step_avg:57.62ms
step:498/2330 train_time:28697ms step_avg:57.62ms
step:499/2330 train_time:28753ms step_avg:57.62ms
step:500/2330 train_time:28811ms step_avg:57.62ms
step:500/2330 val_loss:4.4118 train_time:28891ms step_avg:57.78ms
step:501/2330 train_time:28910ms step_avg:57.70ms
step:502/2330 train_time:28930ms step_avg:57.63ms
step:503/2330 train_time:28987ms step_avg:57.63ms
step:504/2330 train_time:29049ms step_avg:57.64ms
step:505/2330 train_time:29105ms step_avg:57.63ms
step:506/2330 train_time:29169ms step_avg:57.65ms
step:507/2330 train_time:29225ms step_avg:57.64ms
step:508/2330 train_time:29283ms step_avg:57.64ms
step:509/2330 train_time:29339ms step_avg:57.64ms
step:510/2330 train_time:29400ms step_avg:57.65ms
step:511/2330 train_time:29455ms step_avg:57.64ms
step:512/2330 train_time:29515ms step_avg:57.65ms
step:513/2330 train_time:29570ms step_avg:57.64ms
step:514/2330 train_time:29628ms step_avg:57.64ms
step:515/2330 train_time:29683ms step_avg:57.64ms
step:516/2330 train_time:29742ms step_avg:57.64ms
step:517/2330 train_time:29797ms step_avg:57.63ms
step:518/2330 train_time:29856ms step_avg:57.64ms
step:519/2330 train_time:29913ms step_avg:57.64ms
step:520/2330 train_time:29973ms step_avg:57.64ms
step:521/2330 train_time:30030ms step_avg:57.64ms
step:522/2330 train_time:30091ms step_avg:57.65ms
step:523/2330 train_time:30148ms step_avg:57.64ms
step:524/2330 train_time:30208ms step_avg:57.65ms
step:525/2330 train_time:30264ms step_avg:57.65ms
step:526/2330 train_time:30325ms step_avg:57.65ms
step:527/2330 train_time:30381ms step_avg:57.65ms
step:528/2330 train_time:30440ms step_avg:57.65ms
step:529/2330 train_time:30495ms step_avg:57.65ms
step:530/2330 train_time:30554ms step_avg:57.65ms
step:531/2330 train_time:30610ms step_avg:57.65ms
step:532/2330 train_time:30668ms step_avg:57.65ms
step:533/2330 train_time:30724ms step_avg:57.64ms
step:534/2330 train_time:30782ms step_avg:57.64ms
step:535/2330 train_time:30838ms step_avg:57.64ms
step:536/2330 train_time:30898ms step_avg:57.65ms
step:537/2330 train_time:30954ms step_avg:57.64ms
step:538/2330 train_time:31015ms step_avg:57.65ms
step:539/2330 train_time:31071ms step_avg:57.65ms
step:540/2330 train_time:31132ms step_avg:57.65ms
step:541/2330 train_time:31189ms step_avg:57.65ms
step:542/2330 train_time:31249ms step_avg:57.66ms
step:543/2330 train_time:31306ms step_avg:57.65ms
step:544/2330 train_time:31365ms step_avg:57.66ms
step:545/2330 train_time:31420ms step_avg:57.65ms
step:546/2330 train_time:31481ms step_avg:57.66ms
step:547/2330 train_time:31536ms step_avg:57.65ms
step:548/2330 train_time:31595ms step_avg:57.66ms
step:549/2330 train_time:31651ms step_avg:57.65ms
step:550/2330 train_time:31710ms step_avg:57.65ms
step:551/2330 train_time:31765ms step_avg:57.65ms
step:552/2330 train_time:31824ms step_avg:57.65ms
step:553/2330 train_time:31880ms step_avg:57.65ms
step:554/2330 train_time:31939ms step_avg:57.65ms
step:555/2330 train_time:31995ms step_avg:57.65ms
step:556/2330 train_time:32056ms step_avg:57.66ms
step:557/2330 train_time:32113ms step_avg:57.65ms
step:558/2330 train_time:32173ms step_avg:57.66ms
step:559/2330 train_time:32229ms step_avg:57.65ms
step:560/2330 train_time:32288ms step_avg:57.66ms
step:561/2330 train_time:32345ms step_avg:57.66ms
step:562/2330 train_time:32403ms step_avg:57.66ms
step:563/2330 train_time:32460ms step_avg:57.66ms
step:564/2330 train_time:32519ms step_avg:57.66ms
step:565/2330 train_time:32575ms step_avg:57.66ms
step:566/2330 train_time:32635ms step_avg:57.66ms
step:567/2330 train_time:32690ms step_avg:57.65ms
step:568/2330 train_time:32750ms step_avg:57.66ms
step:569/2330 train_time:32805ms step_avg:57.65ms
step:570/2330 train_time:32865ms step_avg:57.66ms
step:571/2330 train_time:32921ms step_avg:57.65ms
step:572/2330 train_time:32980ms step_avg:57.66ms
step:573/2330 train_time:33036ms step_avg:57.65ms
step:574/2330 train_time:33096ms step_avg:57.66ms
step:575/2330 train_time:33152ms step_avg:57.66ms
step:576/2330 train_time:33212ms step_avg:57.66ms
step:577/2330 train_time:33269ms step_avg:57.66ms
step:578/2330 train_time:33329ms step_avg:57.66ms
step:579/2330 train_time:33385ms step_avg:57.66ms
step:580/2330 train_time:33444ms step_avg:57.66ms
step:581/2330 train_time:33500ms step_avg:57.66ms
step:582/2330 train_time:33559ms step_avg:57.66ms
step:583/2330 train_time:33615ms step_avg:57.66ms
step:584/2330 train_time:33674ms step_avg:57.66ms
step:585/2330 train_time:33731ms step_avg:57.66ms
step:586/2330 train_time:33790ms step_avg:57.66ms
step:587/2330 train_time:33846ms step_avg:57.66ms
step:588/2330 train_time:33905ms step_avg:57.66ms
step:589/2330 train_time:33961ms step_avg:57.66ms
step:590/2330 train_time:34020ms step_avg:57.66ms
step:591/2330 train_time:34076ms step_avg:57.66ms
step:592/2330 train_time:34136ms step_avg:57.66ms
step:593/2330 train_time:34192ms step_avg:57.66ms
step:594/2330 train_time:34252ms step_avg:57.66ms
step:595/2330 train_time:34309ms step_avg:57.66ms
step:596/2330 train_time:34368ms step_avg:57.66ms
step:597/2330 train_time:34425ms step_avg:57.66ms
step:598/2330 train_time:34484ms step_avg:57.67ms
step:599/2330 train_time:34540ms step_avg:57.66ms
step:600/2330 train_time:34598ms step_avg:57.66ms
step:601/2330 train_time:34654ms step_avg:57.66ms
step:602/2330 train_time:34714ms step_avg:57.66ms
step:603/2330 train_time:34770ms step_avg:57.66ms
step:604/2330 train_time:34829ms step_avg:57.66ms
step:605/2330 train_time:34885ms step_avg:57.66ms
step:606/2330 train_time:34943ms step_avg:57.66ms
step:607/2330 train_time:34999ms step_avg:57.66ms
step:608/2330 train_time:35060ms step_avg:57.66ms
step:609/2330 train_time:35115ms step_avg:57.66ms
step:610/2330 train_time:35176ms step_avg:57.67ms
step:611/2330 train_time:35232ms step_avg:57.66ms
step:612/2330 train_time:35292ms step_avg:57.67ms
step:613/2330 train_time:35348ms step_avg:57.66ms
step:614/2330 train_time:35407ms step_avg:57.67ms
step:615/2330 train_time:35464ms step_avg:57.67ms
step:616/2330 train_time:35523ms step_avg:57.67ms
step:617/2330 train_time:35580ms step_avg:57.67ms
step:618/2330 train_time:35638ms step_avg:57.67ms
step:619/2330 train_time:35694ms step_avg:57.66ms
step:620/2330 train_time:35753ms step_avg:57.67ms
step:621/2330 train_time:35809ms step_avg:57.66ms
step:622/2330 train_time:35868ms step_avg:57.67ms
step:623/2330 train_time:35925ms step_avg:57.66ms
step:624/2330 train_time:35983ms step_avg:57.67ms
step:625/2330 train_time:36039ms step_avg:57.66ms
step:626/2330 train_time:36098ms step_avg:57.66ms
step:627/2330 train_time:36154ms step_avg:57.66ms
step:628/2330 train_time:36214ms step_avg:57.67ms
step:629/2330 train_time:36270ms step_avg:57.66ms
step:630/2330 train_time:36330ms step_avg:57.67ms
step:631/2330 train_time:36386ms step_avg:57.66ms
step:632/2330 train_time:36445ms step_avg:57.67ms
step:633/2330 train_time:36502ms step_avg:57.67ms
step:634/2330 train_time:36561ms step_avg:57.67ms
step:635/2330 train_time:36616ms step_avg:57.66ms
step:636/2330 train_time:36676ms step_avg:57.67ms
step:637/2330 train_time:36732ms step_avg:57.66ms
step:638/2330 train_time:36791ms step_avg:57.67ms
step:639/2330 train_time:36847ms step_avg:57.66ms
step:640/2330 train_time:36906ms step_avg:57.67ms
step:641/2330 train_time:36962ms step_avg:57.66ms
step:642/2330 train_time:37021ms step_avg:57.66ms
step:643/2330 train_time:37076ms step_avg:57.66ms
step:644/2330 train_time:37136ms step_avg:57.66ms
step:645/2330 train_time:37193ms step_avg:57.66ms
step:646/2330 train_time:37252ms step_avg:57.67ms
step:647/2330 train_time:37308ms step_avg:57.66ms
step:648/2330 train_time:37367ms step_avg:57.67ms
step:649/2330 train_time:37424ms step_avg:57.66ms
step:650/2330 train_time:37482ms step_avg:57.66ms
step:651/2330 train_time:37537ms step_avg:57.66ms
step:652/2330 train_time:37599ms step_avg:57.67ms
step:653/2330 train_time:37655ms step_avg:57.67ms
step:654/2330 train_time:37714ms step_avg:57.67ms
step:655/2330 train_time:37770ms step_avg:57.66ms
step:656/2330 train_time:37830ms step_avg:57.67ms
step:657/2330 train_time:37886ms step_avg:57.67ms
step:658/2330 train_time:37945ms step_avg:57.67ms
step:659/2330 train_time:38001ms step_avg:57.66ms
step:660/2330 train_time:38060ms step_avg:57.67ms
step:661/2330 train_time:38116ms step_avg:57.66ms
step:662/2330 train_time:38176ms step_avg:57.67ms
step:663/2330 train_time:38232ms step_avg:57.67ms
step:664/2330 train_time:38292ms step_avg:57.67ms
step:665/2330 train_time:38348ms step_avg:57.67ms
step:666/2330 train_time:38407ms step_avg:57.67ms
step:667/2330 train_time:38462ms step_avg:57.66ms
step:668/2330 train_time:38522ms step_avg:57.67ms
step:669/2330 train_time:38578ms step_avg:57.67ms
step:670/2330 train_time:38639ms step_avg:57.67ms
step:671/2330 train_time:38694ms step_avg:57.67ms
step:672/2330 train_time:38755ms step_avg:57.67ms
step:673/2330 train_time:38811ms step_avg:57.67ms
step:674/2330 train_time:38872ms step_avg:57.67ms
step:675/2330 train_time:38929ms step_avg:57.67ms
step:676/2330 train_time:38988ms step_avg:57.67ms
step:677/2330 train_time:39044ms step_avg:57.67ms
step:678/2330 train_time:39104ms step_avg:57.67ms
step:679/2330 train_time:39160ms step_avg:57.67ms
step:680/2330 train_time:39220ms step_avg:57.68ms
step:681/2330 train_time:39275ms step_avg:57.67ms
step:682/2330 train_time:39334ms step_avg:57.67ms
step:683/2330 train_time:39390ms step_avg:57.67ms
step:684/2330 train_time:39450ms step_avg:57.67ms
step:685/2330 train_time:39505ms step_avg:57.67ms
step:686/2330 train_time:39564ms step_avg:57.67ms
step:687/2330 train_time:39620ms step_avg:57.67ms
step:688/2330 train_time:39680ms step_avg:57.67ms
step:689/2330 train_time:39735ms step_avg:57.67ms
step:690/2330 train_time:39795ms step_avg:57.67ms
step:691/2330 train_time:39852ms step_avg:57.67ms
step:692/2330 train_time:39912ms step_avg:57.68ms
step:693/2330 train_time:39968ms step_avg:57.67ms
step:694/2330 train_time:40028ms step_avg:57.68ms
step:695/2330 train_time:40084ms step_avg:57.68ms
step:696/2330 train_time:40143ms step_avg:57.68ms
step:697/2330 train_time:40199ms step_avg:57.67ms
step:698/2330 train_time:40259ms step_avg:57.68ms
step:699/2330 train_time:40315ms step_avg:57.67ms
step:700/2330 train_time:40375ms step_avg:57.68ms
step:701/2330 train_time:40431ms step_avg:57.68ms
step:702/2330 train_time:40491ms step_avg:57.68ms
step:703/2330 train_time:40546ms step_avg:57.68ms
step:704/2330 train_time:40605ms step_avg:57.68ms
step:705/2330 train_time:40661ms step_avg:57.68ms
step:706/2330 train_time:40720ms step_avg:57.68ms
step:707/2330 train_time:40776ms step_avg:57.67ms
step:708/2330 train_time:40836ms step_avg:57.68ms
step:709/2330 train_time:40893ms step_avg:57.68ms
step:710/2330 train_time:40952ms step_avg:57.68ms
step:711/2330 train_time:41008ms step_avg:57.68ms
step:712/2330 train_time:41067ms step_avg:57.68ms
step:713/2330 train_time:41123ms step_avg:57.68ms
step:714/2330 train_time:41182ms step_avg:57.68ms
step:715/2330 train_time:41238ms step_avg:57.68ms
step:716/2330 train_time:41297ms step_avg:57.68ms
step:717/2330 train_time:41353ms step_avg:57.67ms
step:718/2330 train_time:41413ms step_avg:57.68ms
step:719/2330 train_time:41470ms step_avg:57.68ms
step:720/2330 train_time:41529ms step_avg:57.68ms
step:721/2330 train_time:41586ms step_avg:57.68ms
step:722/2330 train_time:41644ms step_avg:57.68ms
step:723/2330 train_time:41700ms step_avg:57.68ms
step:724/2330 train_time:41760ms step_avg:57.68ms
step:725/2330 train_time:41816ms step_avg:57.68ms
step:726/2330 train_time:41877ms step_avg:57.68ms
step:727/2330 train_time:41933ms step_avg:57.68ms
step:728/2330 train_time:41992ms step_avg:57.68ms
step:729/2330 train_time:42049ms step_avg:57.68ms
step:730/2330 train_time:42108ms step_avg:57.68ms
step:731/2330 train_time:42165ms step_avg:57.68ms
step:732/2330 train_time:42224ms step_avg:57.68ms
step:733/2330 train_time:42279ms step_avg:57.68ms
step:734/2330 train_time:42339ms step_avg:57.68ms
step:735/2330 train_time:42394ms step_avg:57.68ms
step:736/2330 train_time:42454ms step_avg:57.68ms
step:737/2330 train_time:42510ms step_avg:57.68ms
step:738/2330 train_time:42570ms step_avg:57.68ms
step:739/2330 train_time:42626ms step_avg:57.68ms
step:740/2330 train_time:42685ms step_avg:57.68ms
step:741/2330 train_time:42741ms step_avg:57.68ms
step:742/2330 train_time:42800ms step_avg:57.68ms
step:743/2330 train_time:42856ms step_avg:57.68ms
step:744/2330 train_time:42916ms step_avg:57.68ms
step:745/2330 train_time:42972ms step_avg:57.68ms
step:746/2330 train_time:43032ms step_avg:57.68ms
step:747/2330 train_time:43089ms step_avg:57.68ms
step:748/2330 train_time:43148ms step_avg:57.68ms
step:749/2330 train_time:43204ms step_avg:57.68ms
step:750/2330 train_time:43262ms step_avg:57.68ms
step:750/2330 val_loss:4.2106 train_time:43342ms step_avg:57.79ms
step:751/2330 train_time:43362ms step_avg:57.74ms
step:752/2330 train_time:43382ms step_avg:57.69ms
step:753/2330 train_time:43436ms step_avg:57.68ms
step:754/2330 train_time:43502ms step_avg:57.69ms
step:755/2330 train_time:43557ms step_avg:57.69ms
step:756/2330 train_time:43621ms step_avg:57.70ms
step:757/2330 train_time:43676ms step_avg:57.70ms
step:758/2330 train_time:43736ms step_avg:57.70ms
step:759/2330 train_time:43791ms step_avg:57.70ms
step:760/2330 train_time:43851ms step_avg:57.70ms
step:761/2330 train_time:43907ms step_avg:57.70ms
step:762/2330 train_time:43965ms step_avg:57.70ms
step:763/2330 train_time:44020ms step_avg:57.69ms
step:764/2330 train_time:44079ms step_avg:57.69ms
step:765/2330 train_time:44135ms step_avg:57.69ms
step:766/2330 train_time:44193ms step_avg:57.69ms
step:767/2330 train_time:44251ms step_avg:57.69ms
step:768/2330 train_time:44311ms step_avg:57.70ms
step:769/2330 train_time:44370ms step_avg:57.70ms
step:770/2330 train_time:44432ms step_avg:57.70ms
step:771/2330 train_time:44489ms step_avg:57.70ms
step:772/2330 train_time:44552ms step_avg:57.71ms
step:773/2330 train_time:44609ms step_avg:57.71ms
step:774/2330 train_time:44670ms step_avg:57.71ms
step:775/2330 train_time:44727ms step_avg:57.71ms
step:776/2330 train_time:44787ms step_avg:57.71ms
step:777/2330 train_time:44843ms step_avg:57.71ms
step:778/2330 train_time:44903ms step_avg:57.72ms
step:779/2330 train_time:44959ms step_avg:57.71ms
step:780/2330 train_time:45018ms step_avg:57.72ms
step:781/2330 train_time:45074ms step_avg:57.71ms
step:782/2330 train_time:45134ms step_avg:57.72ms
step:783/2330 train_time:45191ms step_avg:57.72ms
step:784/2330 train_time:45250ms step_avg:57.72ms
step:785/2330 train_time:45308ms step_avg:57.72ms
step:786/2330 train_time:45368ms step_avg:57.72ms
step:787/2330 train_time:45427ms step_avg:57.72ms
step:788/2330 train_time:45487ms step_avg:57.72ms
step:789/2330 train_time:45545ms step_avg:57.73ms
step:790/2330 train_time:45605ms step_avg:57.73ms
step:791/2330 train_time:45663ms step_avg:57.73ms
step:792/2330 train_time:45723ms step_avg:57.73ms
step:793/2330 train_time:45779ms step_avg:57.73ms
step:794/2330 train_time:45839ms step_avg:57.73ms
step:795/2330 train_time:45896ms step_avg:57.73ms
step:796/2330 train_time:45955ms step_avg:57.73ms
step:797/2330 train_time:46011ms step_avg:57.73ms
step:798/2330 train_time:46072ms step_avg:57.73ms
step:799/2330 train_time:46128ms step_avg:57.73ms
step:800/2330 train_time:46188ms step_avg:57.73ms
step:801/2330 train_time:46245ms step_avg:57.73ms
step:802/2330 train_time:46305ms step_avg:57.74ms
step:803/2330 train_time:46363ms step_avg:57.74ms
step:804/2330 train_time:46423ms step_avg:57.74ms
step:805/2330 train_time:46480ms step_avg:57.74ms
step:806/2330 train_time:46541ms step_avg:57.74ms
step:807/2330 train_time:46597ms step_avg:57.74ms
step:808/2330 train_time:46659ms step_avg:57.75ms
step:809/2330 train_time:46716ms step_avg:57.75ms
step:810/2330 train_time:46776ms step_avg:57.75ms
step:811/2330 train_time:46833ms step_avg:57.75ms
step:812/2330 train_time:46893ms step_avg:57.75ms
step:813/2330 train_time:46950ms step_avg:57.75ms
step:814/2330 train_time:47010ms step_avg:57.75ms
step:815/2330 train_time:47068ms step_avg:57.75ms
step:816/2330 train_time:47128ms step_avg:57.75ms
step:817/2330 train_time:47184ms step_avg:57.75ms
step:818/2330 train_time:47244ms step_avg:57.76ms
step:819/2330 train_time:47300ms step_avg:57.75ms
step:820/2330 train_time:47362ms step_avg:57.76ms
step:821/2330 train_time:47418ms step_avg:57.76ms
step:822/2330 train_time:47478ms step_avg:57.76ms
step:823/2330 train_time:47536ms step_avg:57.76ms
step:824/2330 train_time:47596ms step_avg:57.76ms
step:825/2330 train_time:47653ms step_avg:57.76ms
step:826/2330 train_time:47715ms step_avg:57.77ms
step:827/2330 train_time:47772ms step_avg:57.76ms
step:828/2330 train_time:47833ms step_avg:57.77ms
step:829/2330 train_time:47889ms step_avg:57.77ms
step:830/2330 train_time:47950ms step_avg:57.77ms
step:831/2330 train_time:48007ms step_avg:57.77ms
step:832/2330 train_time:48067ms step_avg:57.77ms
step:833/2330 train_time:48124ms step_avg:57.77ms
step:834/2330 train_time:48184ms step_avg:57.77ms
step:835/2330 train_time:48241ms step_avg:57.77ms
step:836/2330 train_time:48300ms step_avg:57.78ms
step:837/2330 train_time:48357ms step_avg:57.77ms
step:838/2330 train_time:48417ms step_avg:57.78ms
step:839/2330 train_time:48473ms step_avg:57.78ms
step:840/2330 train_time:48535ms step_avg:57.78ms
step:841/2330 train_time:48592ms step_avg:57.78ms
step:842/2330 train_time:48653ms step_avg:57.78ms
step:843/2330 train_time:48710ms step_avg:57.78ms
step:844/2330 train_time:48770ms step_avg:57.78ms
step:845/2330 train_time:48827ms step_avg:57.78ms
step:846/2330 train_time:48887ms step_avg:57.79ms
step:847/2330 train_time:48945ms step_avg:57.79ms
step:848/2330 train_time:49004ms step_avg:57.79ms
step:849/2330 train_time:49061ms step_avg:57.79ms
step:850/2330 train_time:49120ms step_avg:57.79ms
step:851/2330 train_time:49177ms step_avg:57.79ms
step:852/2330 train_time:49237ms step_avg:57.79ms
step:853/2330 train_time:49294ms step_avg:57.79ms
step:854/2330 train_time:49355ms step_avg:57.79ms
step:855/2330 train_time:49411ms step_avg:57.79ms
step:856/2330 train_time:49472ms step_avg:57.79ms
step:857/2330 train_time:49530ms step_avg:57.79ms
step:858/2330 train_time:49590ms step_avg:57.80ms
step:859/2330 train_time:49648ms step_avg:57.80ms
step:860/2330 train_time:49708ms step_avg:57.80ms
step:861/2330 train_time:49765ms step_avg:57.80ms
step:862/2330 train_time:49825ms step_avg:57.80ms
step:863/2330 train_time:49881ms step_avg:57.80ms
step:864/2330 train_time:49941ms step_avg:57.80ms
step:865/2330 train_time:49998ms step_avg:57.80ms
step:866/2330 train_time:50058ms step_avg:57.80ms
step:867/2330 train_time:50115ms step_avg:57.80ms
step:868/2330 train_time:50175ms step_avg:57.81ms
step:869/2330 train_time:50232ms step_avg:57.80ms
step:870/2330 train_time:50292ms step_avg:57.81ms
step:871/2330 train_time:50349ms step_avg:57.81ms
step:872/2330 train_time:50409ms step_avg:57.81ms
step:873/2330 train_time:50466ms step_avg:57.81ms
step:874/2330 train_time:50527ms step_avg:57.81ms
step:875/2330 train_time:50584ms step_avg:57.81ms
step:876/2330 train_time:50643ms step_avg:57.81ms
step:877/2330 train_time:50700ms step_avg:57.81ms
step:878/2330 train_time:50760ms step_avg:57.81ms
step:879/2330 train_time:50817ms step_avg:57.81ms
step:880/2330 train_time:50878ms step_avg:57.82ms
step:881/2330 train_time:50934ms step_avg:57.81ms
step:882/2330 train_time:50995ms step_avg:57.82ms
step:883/2330 train_time:51052ms step_avg:57.82ms
step:884/2330 train_time:51113ms step_avg:57.82ms
step:885/2330 train_time:51170ms step_avg:57.82ms
step:886/2330 train_time:51229ms step_avg:57.82ms
step:887/2330 train_time:51286ms step_avg:57.82ms
step:888/2330 train_time:51347ms step_avg:57.82ms
step:889/2330 train_time:51403ms step_avg:57.82ms
step:890/2330 train_time:51463ms step_avg:57.82ms
step:891/2330 train_time:51520ms step_avg:57.82ms
step:892/2330 train_time:51579ms step_avg:57.82ms
step:893/2330 train_time:51637ms step_avg:57.82ms
step:894/2330 train_time:51697ms step_avg:57.83ms
step:895/2330 train_time:51755ms step_avg:57.83ms
step:896/2330 train_time:51815ms step_avg:57.83ms
step:897/2330 train_time:51872ms step_avg:57.83ms
step:898/2330 train_time:51933ms step_avg:57.83ms
step:899/2330 train_time:51989ms step_avg:57.83ms
step:900/2330 train_time:52050ms step_avg:57.83ms
step:901/2330 train_time:52108ms step_avg:57.83ms
step:902/2330 train_time:52167ms step_avg:57.84ms
step:903/2330 train_time:52224ms step_avg:57.83ms
step:904/2330 train_time:52283ms step_avg:57.84ms
step:905/2330 train_time:52340ms step_avg:57.83ms
step:906/2330 train_time:52400ms step_avg:57.84ms
step:907/2330 train_time:52457ms step_avg:57.84ms
step:908/2330 train_time:52517ms step_avg:57.84ms
step:909/2330 train_time:52574ms step_avg:57.84ms
step:910/2330 train_time:52635ms step_avg:57.84ms
step:911/2330 train_time:52691ms step_avg:57.84ms
step:912/2330 train_time:52752ms step_avg:57.84ms
step:913/2330 train_time:52810ms step_avg:57.84ms
step:914/2330 train_time:52870ms step_avg:57.84ms
step:915/2330 train_time:52927ms step_avg:57.84ms
step:916/2330 train_time:52987ms step_avg:57.85ms
step:917/2330 train_time:53044ms step_avg:57.85ms
step:918/2330 train_time:53103ms step_avg:57.85ms
step:919/2330 train_time:53160ms step_avg:57.85ms
step:920/2330 train_time:53220ms step_avg:57.85ms
step:921/2330 train_time:53277ms step_avg:57.85ms
step:922/2330 train_time:53337ms step_avg:57.85ms
step:923/2330 train_time:53394ms step_avg:57.85ms
step:924/2330 train_time:53454ms step_avg:57.85ms
step:925/2330 train_time:53511ms step_avg:57.85ms
step:926/2330 train_time:53571ms step_avg:57.85ms
step:927/2330 train_time:53629ms step_avg:57.85ms
step:928/2330 train_time:53688ms step_avg:57.85ms
step:929/2330 train_time:53745ms step_avg:57.85ms
step:930/2330 train_time:53805ms step_avg:57.85ms
step:931/2330 train_time:53861ms step_avg:57.85ms
step:932/2330 train_time:53922ms step_avg:57.86ms
step:933/2330 train_time:53979ms step_avg:57.85ms
step:934/2330 train_time:54040ms step_avg:57.86ms
step:935/2330 train_time:54096ms step_avg:57.86ms
step:936/2330 train_time:54157ms step_avg:57.86ms
step:937/2330 train_time:54213ms step_avg:57.86ms
step:938/2330 train_time:54273ms step_avg:57.86ms
step:939/2330 train_time:54330ms step_avg:57.86ms
step:940/2330 train_time:54390ms step_avg:57.86ms
step:941/2330 train_time:54448ms step_avg:57.86ms
step:942/2330 train_time:54507ms step_avg:57.86ms
step:943/2330 train_time:54565ms step_avg:57.86ms
step:944/2330 train_time:54625ms step_avg:57.87ms
step:945/2330 train_time:54683ms step_avg:57.87ms
step:946/2330 train_time:54742ms step_avg:57.87ms
step:947/2330 train_time:54800ms step_avg:57.87ms
step:948/2330 train_time:54859ms step_avg:57.87ms
step:949/2330 train_time:54916ms step_avg:57.87ms
step:950/2330 train_time:54975ms step_avg:57.87ms
step:951/2330 train_time:55032ms step_avg:57.87ms
step:952/2330 train_time:55093ms step_avg:57.87ms
step:953/2330 train_time:55150ms step_avg:57.87ms
step:954/2330 train_time:55211ms step_avg:57.87ms
step:955/2330 train_time:55267ms step_avg:57.87ms
step:956/2330 train_time:55328ms step_avg:57.87ms
step:957/2330 train_time:55384ms step_avg:57.87ms
step:958/2330 train_time:55445ms step_avg:57.88ms
step:959/2330 train_time:55503ms step_avg:57.88ms
step:960/2330 train_time:55563ms step_avg:57.88ms
step:961/2330 train_time:55620ms step_avg:57.88ms
step:962/2330 train_time:55680ms step_avg:57.88ms
step:963/2330 train_time:55736ms step_avg:57.88ms
step:964/2330 train_time:55797ms step_avg:57.88ms
step:965/2330 train_time:55854ms step_avg:57.88ms
step:966/2330 train_time:55916ms step_avg:57.88ms
step:967/2330 train_time:55972ms step_avg:57.88ms
step:968/2330 train_time:56033ms step_avg:57.89ms
step:969/2330 train_time:56090ms step_avg:57.88ms
step:970/2330 train_time:56150ms step_avg:57.89ms
step:971/2330 train_time:56208ms step_avg:57.89ms
step:972/2330 train_time:56267ms step_avg:57.89ms
step:973/2330 train_time:56324ms step_avg:57.89ms
step:974/2330 train_time:56384ms step_avg:57.89ms
step:975/2330 train_time:56441ms step_avg:57.89ms
step:976/2330 train_time:56501ms step_avg:57.89ms
step:977/2330 train_time:56559ms step_avg:57.89ms
step:978/2330 train_time:56617ms step_avg:57.89ms
step:979/2330 train_time:56674ms step_avg:57.89ms
step:980/2330 train_time:56735ms step_avg:57.89ms
step:981/2330 train_time:56792ms step_avg:57.89ms
step:982/2330 train_time:56852ms step_avg:57.89ms
step:983/2330 train_time:56909ms step_avg:57.89ms
step:984/2330 train_time:56970ms step_avg:57.90ms
step:985/2330 train_time:57026ms step_avg:57.89ms
step:986/2330 train_time:57087ms step_avg:57.90ms
step:987/2330 train_time:57144ms step_avg:57.90ms
step:988/2330 train_time:57204ms step_avg:57.90ms
step:989/2330 train_time:57260ms step_avg:57.90ms
step:990/2330 train_time:57321ms step_avg:57.90ms
step:991/2330 train_time:57377ms step_avg:57.90ms
step:992/2330 train_time:57437ms step_avg:57.90ms
step:993/2330 train_time:57495ms step_avg:57.90ms
step:994/2330 train_time:57555ms step_avg:57.90ms
step:995/2330 train_time:57612ms step_avg:57.90ms
step:996/2330 train_time:57673ms step_avg:57.90ms
step:997/2330 train_time:57730ms step_avg:57.90ms
step:998/2330 train_time:57790ms step_avg:57.91ms
step:999/2330 train_time:57848ms step_avg:57.91ms
step:1000/2330 train_time:57908ms step_avg:57.91ms
step:1000/2330 val_loss:4.0629 train_time:57989ms step_avg:57.99ms
step:1001/2330 train_time:58007ms step_avg:57.95ms
step:1002/2330 train_time:58026ms step_avg:57.91ms
step:1003/2330 train_time:58083ms step_avg:57.91ms
step:1004/2330 train_time:58150ms step_avg:57.92ms
step:1005/2330 train_time:58207ms step_avg:57.92ms
step:1006/2330 train_time:58272ms step_avg:57.92ms
step:1007/2330 train_time:58328ms step_avg:57.92ms
step:1008/2330 train_time:58389ms step_avg:57.93ms
step:1009/2330 train_time:58446ms step_avg:57.92ms
step:1010/2330 train_time:58505ms step_avg:57.93ms
step:1011/2330 train_time:58561ms step_avg:57.92ms
step:1012/2330 train_time:58622ms step_avg:57.93ms
step:1013/2330 train_time:58678ms step_avg:57.92ms
step:1014/2330 train_time:58736ms step_avg:57.93ms
step:1015/2330 train_time:58792ms step_avg:57.92ms
step:1016/2330 train_time:58852ms step_avg:57.92ms
step:1017/2330 train_time:58910ms step_avg:57.93ms
step:1018/2330 train_time:58971ms step_avg:57.93ms
step:1019/2330 train_time:59030ms step_avg:57.93ms
step:1020/2330 train_time:59091ms step_avg:57.93ms
step:1021/2330 train_time:59149ms step_avg:57.93ms
step:1022/2330 train_time:59209ms step_avg:57.93ms
step:1023/2330 train_time:59266ms step_avg:57.93ms
step:1024/2330 train_time:59328ms step_avg:57.94ms
step:1025/2330 train_time:59385ms step_avg:57.94ms
step:1026/2330 train_time:59445ms step_avg:57.94ms
step:1027/2330 train_time:59502ms step_avg:57.94ms
step:1028/2330 train_time:59561ms step_avg:57.94ms
step:1029/2330 train_time:59618ms step_avg:57.94ms
step:1030/2330 train_time:59677ms step_avg:57.94ms
step:1031/2330 train_time:59733ms step_avg:57.94ms
step:1032/2330 train_time:59794ms step_avg:57.94ms
step:1033/2330 train_time:59851ms step_avg:57.94ms
step:1034/2330 train_time:59912ms step_avg:57.94ms
step:1035/2330 train_time:59970ms step_avg:57.94ms
step:1036/2330 train_time:60031ms step_avg:57.94ms
step:1037/2330 train_time:60089ms step_avg:57.95ms
step:1038/2330 train_time:60149ms step_avg:57.95ms
step:1039/2330 train_time:60208ms step_avg:57.95ms
step:1040/2330 train_time:60268ms step_avg:57.95ms
step:1041/2330 train_time:60325ms step_avg:57.95ms
step:1042/2330 train_time:60385ms step_avg:57.95ms
step:1043/2330 train_time:60442ms step_avg:57.95ms
step:1044/2330 train_time:60500ms step_avg:57.95ms
step:1045/2330 train_time:60557ms step_avg:57.95ms
step:1046/2330 train_time:60617ms step_avg:57.95ms
step:1047/2330 train_time:60673ms step_avg:57.95ms
step:1048/2330 train_time:60733ms step_avg:57.95ms
step:1049/2330 train_time:60789ms step_avg:57.95ms
step:1050/2330 train_time:60849ms step_avg:57.95ms
step:1051/2330 train_time:60906ms step_avg:57.95ms
step:1052/2330 train_time:60967ms step_avg:57.95ms
step:1053/2330 train_time:61025ms step_avg:57.95ms
step:1054/2330 train_time:61085ms step_avg:57.96ms
step:1055/2330 train_time:61143ms step_avg:57.96ms
step:1056/2330 train_time:61203ms step_avg:57.96ms
step:1057/2330 train_time:61260ms step_avg:57.96ms
step:1058/2330 train_time:61321ms step_avg:57.96ms
step:1059/2330 train_time:61378ms step_avg:57.96ms
step:1060/2330 train_time:61438ms step_avg:57.96ms
step:1061/2330 train_time:61495ms step_avg:57.96ms
step:1062/2330 train_time:61556ms step_avg:57.96ms
step:1063/2330 train_time:61612ms step_avg:57.96ms
step:1064/2330 train_time:61671ms step_avg:57.96ms
step:1065/2330 train_time:61728ms step_avg:57.96ms
step:1066/2330 train_time:61788ms step_avg:57.96ms
step:1067/2330 train_time:61844ms step_avg:57.96ms
step:1068/2330 train_time:61905ms step_avg:57.96ms
step:1069/2330 train_time:61962ms step_avg:57.96ms
step:1070/2330 train_time:62023ms step_avg:57.97ms
step:1071/2330 train_time:62079ms step_avg:57.96ms
step:1072/2330 train_time:62140ms step_avg:57.97ms
step:1073/2330 train_time:62196ms step_avg:57.96ms
step:1074/2330 train_time:62258ms step_avg:57.97ms
step:1075/2330 train_time:62315ms step_avg:57.97ms
step:1076/2330 train_time:62375ms step_avg:57.97ms
step:1077/2330 train_time:62431ms step_avg:57.97ms
step:1078/2330 train_time:62493ms step_avg:57.97ms
step:1079/2330 train_time:62549ms step_avg:57.97ms
step:1080/2330 train_time:62609ms step_avg:57.97ms
step:1081/2330 train_time:62665ms step_avg:57.97ms
step:1082/2330 train_time:62727ms step_avg:57.97ms
step:1083/2330 train_time:62783ms step_avg:57.97ms
step:1084/2330 train_time:62843ms step_avg:57.97ms
step:1085/2330 train_time:62900ms step_avg:57.97ms
step:1086/2330 train_time:62960ms step_avg:57.97ms
step:1087/2330 train_time:63017ms step_avg:57.97ms
step:1088/2330 train_time:63077ms step_avg:57.98ms
step:1089/2330 train_time:63133ms step_avg:57.97ms
step:1090/2330 train_time:63195ms step_avg:57.98ms
step:1091/2330 train_time:63252ms step_avg:57.98ms
step:1092/2330 train_time:63313ms step_avg:57.98ms
step:1093/2330 train_time:63370ms step_avg:57.98ms
step:1094/2330 train_time:63431ms step_avg:57.98ms
step:1095/2330 train_time:63489ms step_avg:57.98ms
step:1096/2330 train_time:63548ms step_avg:57.98ms
step:1097/2330 train_time:63606ms step_avg:57.98ms
step:1098/2330 train_time:63666ms step_avg:57.98ms
step:1099/2330 train_time:63723ms step_avg:57.98ms
step:1100/2330 train_time:63782ms step_avg:57.98ms
step:1101/2330 train_time:63839ms step_avg:57.98ms
step:1102/2330 train_time:63899ms step_avg:57.98ms
step:1103/2330 train_time:63956ms step_avg:57.98ms
step:1104/2330 train_time:64017ms step_avg:57.99ms
step:1105/2330 train_time:64074ms step_avg:57.99ms
step:1106/2330 train_time:64134ms step_avg:57.99ms
step:1107/2330 train_time:64190ms step_avg:57.99ms
step:1108/2330 train_time:64251ms step_avg:57.99ms
step:1109/2330 train_time:64309ms step_avg:57.99ms
step:1110/2330 train_time:64369ms step_avg:57.99ms
step:1111/2330 train_time:64427ms step_avg:57.99ms
step:1112/2330 train_time:64486ms step_avg:57.99ms
step:1113/2330 train_time:64543ms step_avg:57.99ms
step:1114/2330 train_time:64602ms step_avg:57.99ms
step:1115/2330 train_time:64660ms step_avg:57.99ms
step:1116/2330 train_time:64719ms step_avg:57.99ms
step:1117/2330 train_time:64776ms step_avg:57.99ms
step:1118/2330 train_time:64836ms step_avg:57.99ms
step:1119/2330 train_time:64893ms step_avg:57.99ms
step:1120/2330 train_time:64953ms step_avg:57.99ms
step:1121/2330 train_time:65010ms step_avg:57.99ms
step:1122/2330 train_time:65071ms step_avg:58.00ms
step:1123/2330 train_time:65128ms step_avg:57.99ms
step:1124/2330 train_time:65189ms step_avg:58.00ms
step:1125/2330 train_time:65246ms step_avg:58.00ms
step:1126/2330 train_time:65307ms step_avg:58.00ms
step:1127/2330 train_time:65364ms step_avg:58.00ms
step:1128/2330 train_time:65426ms step_avg:58.00ms
step:1129/2330 train_time:65482ms step_avg:58.00ms
step:1130/2330 train_time:65542ms step_avg:58.00ms
step:1131/2330 train_time:65598ms step_avg:58.00ms
step:1132/2330 train_time:65658ms step_avg:58.00ms
step:1133/2330 train_time:65715ms step_avg:58.00ms
step:1134/2330 train_time:65775ms step_avg:58.00ms
step:1135/2330 train_time:65832ms step_avg:58.00ms
step:1136/2330 train_time:65892ms step_avg:58.00ms
step:1137/2330 train_time:65949ms step_avg:58.00ms
step:1138/2330 train_time:66009ms step_avg:58.00ms
step:1139/2330 train_time:66068ms step_avg:58.00ms
step:1140/2330 train_time:66128ms step_avg:58.01ms
step:1141/2330 train_time:66185ms step_avg:58.01ms
step:1142/2330 train_time:66245ms step_avg:58.01ms
step:1143/2330 train_time:66302ms step_avg:58.01ms
step:1144/2330 train_time:66361ms step_avg:58.01ms
step:1145/2330 train_time:66418ms step_avg:58.01ms
step:1146/2330 train_time:66477ms step_avg:58.01ms
step:1147/2330 train_time:66533ms step_avg:58.01ms
step:1148/2330 train_time:66596ms step_avg:58.01ms
step:1149/2330 train_time:66653ms step_avg:58.01ms
step:1150/2330 train_time:66713ms step_avg:58.01ms
step:1151/2330 train_time:66771ms step_avg:58.01ms
step:1152/2330 train_time:66831ms step_avg:58.01ms
step:1153/2330 train_time:66889ms step_avg:58.01ms
step:1154/2330 train_time:66948ms step_avg:58.01ms
step:1155/2330 train_time:67006ms step_avg:58.01ms
step:1156/2330 train_time:67065ms step_avg:58.01ms
step:1157/2330 train_time:67123ms step_avg:58.01ms
step:1158/2330 train_time:67182ms step_avg:58.02ms
step:1159/2330 train_time:67239ms step_avg:58.01ms
step:1160/2330 train_time:67299ms step_avg:58.02ms
step:1161/2330 train_time:67356ms step_avg:58.02ms
step:1162/2330 train_time:67416ms step_avg:58.02ms
step:1163/2330 train_time:67472ms step_avg:58.02ms
step:1164/2330 train_time:67533ms step_avg:58.02ms
step:1165/2330 train_time:67590ms step_avg:58.02ms
step:1166/2330 train_time:67650ms step_avg:58.02ms
step:1167/2330 train_time:67706ms step_avg:58.02ms
step:1168/2330 train_time:67768ms step_avg:58.02ms
step:1169/2330 train_time:67825ms step_avg:58.02ms
step:1170/2330 train_time:67885ms step_avg:58.02ms
step:1171/2330 train_time:67942ms step_avg:58.02ms
step:1172/2330 train_time:68002ms step_avg:58.02ms
step:1173/2330 train_time:68059ms step_avg:58.02ms
step:1174/2330 train_time:68119ms step_avg:58.02ms
step:1175/2330 train_time:68177ms step_avg:58.02ms
step:1176/2330 train_time:68236ms step_avg:58.02ms
step:1177/2330 train_time:68293ms step_avg:58.02ms
step:1178/2330 train_time:68354ms step_avg:58.03ms
step:1179/2330 train_time:68411ms step_avg:58.02ms
step:1180/2330 train_time:68471ms step_avg:58.03ms
step:1181/2330 train_time:68528ms step_avg:58.03ms
step:1182/2330 train_time:68588ms step_avg:58.03ms
step:1183/2330 train_time:68645ms step_avg:58.03ms
step:1184/2330 train_time:68706ms step_avg:58.03ms
step:1185/2330 train_time:68762ms step_avg:58.03ms
step:1186/2330 train_time:68823ms step_avg:58.03ms
step:1187/2330 train_time:68879ms step_avg:58.03ms
step:1188/2330 train_time:68940ms step_avg:58.03ms
step:1189/2330 train_time:68996ms step_avg:58.03ms
step:1190/2330 train_time:69057ms step_avg:58.03ms
step:1191/2330 train_time:69114ms step_avg:58.03ms
step:1192/2330 train_time:69175ms step_avg:58.03ms
step:1193/2330 train_time:69231ms step_avg:58.03ms
step:1194/2330 train_time:69292ms step_avg:58.03ms
step:1195/2330 train_time:69349ms step_avg:58.03ms
step:1196/2330 train_time:69410ms step_avg:58.03ms
step:1197/2330 train_time:69467ms step_avg:58.03ms
step:1198/2330 train_time:69526ms step_avg:58.04ms
step:1199/2330 train_time:69583ms step_avg:58.03ms
step:1200/2330 train_time:69643ms step_avg:58.04ms
step:1201/2330 train_time:69699ms step_avg:58.03ms
step:1202/2330 train_time:69759ms step_avg:58.04ms
step:1203/2330 train_time:69816ms step_avg:58.04ms
step:1204/2330 train_time:69876ms step_avg:58.04ms
step:1205/2330 train_time:69932ms step_avg:58.04ms
step:1206/2330 train_time:69994ms step_avg:58.04ms
step:1207/2330 train_time:70050ms step_avg:58.04ms
step:1208/2330 train_time:70111ms step_avg:58.04ms
step:1209/2330 train_time:70168ms step_avg:58.04ms
step:1210/2330 train_time:70228ms step_avg:58.04ms
step:1211/2330 train_time:70286ms step_avg:58.04ms
step:1212/2330 train_time:70346ms step_avg:58.04ms
step:1213/2330 train_time:70403ms step_avg:58.04ms
step:1214/2330 train_time:70462ms step_avg:58.04ms
step:1215/2330 train_time:70519ms step_avg:58.04ms
step:1216/2330 train_time:70580ms step_avg:58.04ms
step:1217/2330 train_time:70636ms step_avg:58.04ms
step:1218/2330 train_time:70697ms step_avg:58.04ms
step:1219/2330 train_time:70753ms step_avg:58.04ms
step:1220/2330 train_time:70814ms step_avg:58.04ms
step:1221/2330 train_time:70871ms step_avg:58.04ms
step:1222/2330 train_time:70932ms step_avg:58.05ms
step:1223/2330 train_time:70989ms step_avg:58.04ms
step:1224/2330 train_time:71048ms step_avg:58.05ms
step:1225/2330 train_time:71105ms step_avg:58.05ms
step:1226/2330 train_time:71166ms step_avg:58.05ms
step:1227/2330 train_time:71223ms step_avg:58.05ms
step:1228/2330 train_time:71282ms step_avg:58.05ms
step:1229/2330 train_time:71339ms step_avg:58.05ms
step:1230/2330 train_time:71400ms step_avg:58.05ms
step:1231/2330 train_time:71456ms step_avg:58.05ms
step:1232/2330 train_time:71518ms step_avg:58.05ms
step:1233/2330 train_time:71574ms step_avg:58.05ms
step:1234/2330 train_time:71635ms step_avg:58.05ms
step:1235/2330 train_time:71693ms step_avg:58.05ms
step:1236/2330 train_time:71753ms step_avg:58.05ms
step:1237/2330 train_time:71810ms step_avg:58.05ms
step:1238/2330 train_time:71870ms step_avg:58.05ms
step:1239/2330 train_time:71927ms step_avg:58.05ms
step:1240/2330 train_time:71987ms step_avg:58.05ms
step:1241/2330 train_time:72045ms step_avg:58.05ms
step:1242/2330 train_time:72105ms step_avg:58.06ms
step:1243/2330 train_time:72163ms step_avg:58.06ms
step:1244/2330 train_time:72222ms step_avg:58.06ms
step:1245/2330 train_time:72279ms step_avg:58.06ms
step:1246/2330 train_time:72339ms step_avg:58.06ms
step:1247/2330 train_time:72397ms step_avg:58.06ms
step:1248/2330 train_time:72456ms step_avg:58.06ms
step:1249/2330 train_time:72514ms step_avg:58.06ms
step:1250/2330 train_time:72573ms step_avg:58.06ms
step:1250/2330 val_loss:3.9809 train_time:72655ms step_avg:58.12ms
step:1251/2330 train_time:72675ms step_avg:58.09ms
step:1252/2330 train_time:72696ms step_avg:58.06ms
step:1253/2330 train_time:72753ms step_avg:58.06ms
step:1254/2330 train_time:72817ms step_avg:58.07ms
step:1255/2330 train_time:72874ms step_avg:58.07ms
step:1256/2330 train_time:72937ms step_avg:58.07ms
step:1257/2330 train_time:72994ms step_avg:58.07ms
step:1258/2330 train_time:73054ms step_avg:58.07ms
step:1259/2330 train_time:73110ms step_avg:58.07ms
step:1260/2330 train_time:73169ms step_avg:58.07ms
step:1261/2330 train_time:73225ms step_avg:58.07ms
step:1262/2330 train_time:73285ms step_avg:58.07ms
step:1263/2330 train_time:73342ms step_avg:58.07ms
step:1264/2330 train_time:73402ms step_avg:58.07ms
step:1265/2330 train_time:73457ms step_avg:58.07ms
step:1266/2330 train_time:73517ms step_avg:58.07ms
step:1267/2330 train_time:73575ms step_avg:58.07ms
step:1268/2330 train_time:73636ms step_avg:58.07ms
step:1269/2330 train_time:73695ms step_avg:58.07ms
step:1270/2330 train_time:73756ms step_avg:58.08ms
step:1271/2330 train_time:73813ms step_avg:58.07ms
step:1272/2330 train_time:73874ms step_avg:58.08ms
step:1273/2330 train_time:73931ms step_avg:58.08ms
step:1274/2330 train_time:73992ms step_avg:58.08ms
step:1275/2330 train_time:74048ms step_avg:58.08ms
step:1276/2330 train_time:74109ms step_avg:58.08ms
step:1277/2330 train_time:74165ms step_avg:58.08ms
step:1278/2330 train_time:74225ms step_avg:58.08ms
step:1279/2330 train_time:74281ms step_avg:58.08ms
step:1280/2330 train_time:74342ms step_avg:58.08ms
step:1281/2330 train_time:74398ms step_avg:58.08ms
step:1282/2330 train_time:74458ms step_avg:58.08ms
step:1283/2330 train_time:74515ms step_avg:58.08ms
step:1284/2330 train_time:74575ms step_avg:58.08ms
step:1285/2330 train_time:74631ms step_avg:58.08ms
step:1286/2330 train_time:74692ms step_avg:58.08ms
step:1287/2330 train_time:74748ms step_avg:58.08ms
step:1288/2330 train_time:74811ms step_avg:58.08ms
step:1289/2330 train_time:74867ms step_avg:58.08ms
step:1290/2330 train_time:74929ms step_avg:58.08ms
step:1291/2330 train_time:74986ms step_avg:58.08ms
step:1292/2330 train_time:75047ms step_avg:58.09ms
step:1293/2330 train_time:75104ms step_avg:58.08ms
step:1294/2330 train_time:75163ms step_avg:58.09ms
step:1295/2330 train_time:75220ms step_avg:58.08ms
step:1296/2330 train_time:75280ms step_avg:58.09ms
step:1297/2330 train_time:75336ms step_avg:58.09ms
step:1298/2330 train_time:75397ms step_avg:58.09ms
step:1299/2330 train_time:75453ms step_avg:58.09ms
step:1300/2330 train_time:75514ms step_avg:58.09ms
step:1301/2330 train_time:75571ms step_avg:58.09ms
step:1302/2330 train_time:75631ms step_avg:58.09ms
step:1303/2330 train_time:75688ms step_avg:58.09ms
step:1304/2330 train_time:75748ms step_avg:58.09ms
step:1305/2330 train_time:75805ms step_avg:58.09ms
step:1306/2330 train_time:75866ms step_avg:58.09ms
step:1307/2330 train_time:75923ms step_avg:58.09ms
step:1308/2330 train_time:75985ms step_avg:58.09ms
step:1309/2330 train_time:76041ms step_avg:58.09ms
step:1310/2330 train_time:76102ms step_avg:58.09ms
step:1311/2330 train_time:76158ms step_avg:58.09ms
step:1312/2330 train_time:76219ms step_avg:58.09ms
step:1313/2330 train_time:76275ms step_avg:58.09ms
step:1314/2330 train_time:76336ms step_avg:58.09ms
step:1315/2330 train_time:76392ms step_avg:58.09ms
step:1316/2330 train_time:76452ms step_avg:58.09ms
step:1317/2330 train_time:76509ms step_avg:58.09ms
step:1318/2330 train_time:76569ms step_avg:58.09ms
step:1319/2330 train_time:76625ms step_avg:58.09ms
step:1320/2330 train_time:76686ms step_avg:58.10ms
step:1321/2330 train_time:76743ms step_avg:58.09ms
step:1322/2330 train_time:76804ms step_avg:58.10ms
step:1323/2330 train_time:76860ms step_avg:58.10ms
step:1324/2330 train_time:76923ms step_avg:58.10ms
step:1325/2330 train_time:76980ms step_avg:58.10ms
step:1326/2330 train_time:77041ms step_avg:58.10ms
step:1327/2330 train_time:77098ms step_avg:58.10ms
step:1328/2330 train_time:77158ms step_avg:58.10ms
step:1329/2330 train_time:77214ms step_avg:58.10ms
step:1330/2330 train_time:77274ms step_avg:58.10ms
step:1331/2330 train_time:77332ms step_avg:58.10ms
step:1332/2330 train_time:77391ms step_avg:58.10ms
step:1333/2330 train_time:77448ms step_avg:58.10ms
step:1334/2330 train_time:77507ms step_avg:58.10ms
step:1335/2330 train_time:77564ms step_avg:58.10ms
step:1336/2330 train_time:77624ms step_avg:58.10ms
step:1337/2330 train_time:77681ms step_avg:58.10ms
step:1338/2330 train_time:77743ms step_avg:58.10ms
step:1339/2330 train_time:77800ms step_avg:58.10ms
step:1340/2330 train_time:77861ms step_avg:58.11ms
step:1341/2330 train_time:77918ms step_avg:58.10ms
step:1342/2330 train_time:77979ms step_avg:58.11ms
step:1343/2330 train_time:78036ms step_avg:58.11ms
step:1344/2330 train_time:78097ms step_avg:58.11ms
step:1345/2330 train_time:78153ms step_avg:58.11ms
step:1346/2330 train_time:78213ms step_avg:58.11ms
step:1347/2330 train_time:78270ms step_avg:58.11ms
step:1348/2330 train_time:78331ms step_avg:58.11ms
step:1349/2330 train_time:78388ms step_avg:58.11ms
step:1350/2330 train_time:78448ms step_avg:58.11ms
step:1351/2330 train_time:78504ms step_avg:58.11ms
step:1352/2330 train_time:78564ms step_avg:58.11ms
step:1353/2330 train_time:78620ms step_avg:58.11ms
step:1354/2330 train_time:78681ms step_avg:58.11ms
step:1355/2330 train_time:78739ms step_avg:58.11ms
step:1356/2330 train_time:78799ms step_avg:58.11ms
step:1357/2330 train_time:78856ms step_avg:58.11ms
step:1358/2330 train_time:78917ms step_avg:58.11ms
step:1359/2330 train_time:78974ms step_avg:58.11ms
step:1360/2330 train_time:79034ms step_avg:58.11ms
step:1361/2330 train_time:79091ms step_avg:58.11ms
step:1362/2330 train_time:79150ms step_avg:58.11ms
step:1363/2330 train_time:79206ms step_avg:58.11ms
step:1364/2330 train_time:79267ms step_avg:58.11ms
step:1365/2330 train_time:79324ms step_avg:58.11ms
step:1366/2330 train_time:79384ms step_avg:58.11ms
step:1367/2330 train_time:79440ms step_avg:58.11ms
step:1368/2330 train_time:79501ms step_avg:58.11ms
step:1369/2330 train_time:79558ms step_avg:58.11ms
step:1370/2330 train_time:79619ms step_avg:58.12ms
step:1371/2330 train_time:79676ms step_avg:58.12ms
step:1372/2330 train_time:79736ms step_avg:58.12ms
step:1373/2330 train_time:79793ms step_avg:58.12ms
step:1374/2330 train_time:79854ms step_avg:58.12ms
step:1375/2330 train_time:79912ms step_avg:58.12ms
step:1376/2330 train_time:79971ms step_avg:58.12ms
step:1377/2330 train_time:80029ms step_avg:58.12ms
step:1378/2330 train_time:80088ms step_avg:58.12ms
step:1379/2330 train_time:80145ms step_avg:58.12ms
step:1380/2330 train_time:80205ms step_avg:58.12ms
step:1381/2330 train_time:80261ms step_avg:58.12ms
step:1382/2330 train_time:80323ms step_avg:58.12ms
step:1383/2330 train_time:80379ms step_avg:58.12ms
step:1384/2330 train_time:80441ms step_avg:58.12ms
step:1385/2330 train_time:80497ms step_avg:58.12ms
step:1386/2330 train_time:80557ms step_avg:58.12ms
step:1387/2330 train_time:80614ms step_avg:58.12ms
step:1388/2330 train_time:80674ms step_avg:58.12ms
step:1389/2330 train_time:80731ms step_avg:58.12ms
step:1390/2330 train_time:80792ms step_avg:58.12ms
step:1391/2330 train_time:80849ms step_avg:58.12ms
step:1392/2330 train_time:80909ms step_avg:58.12ms
step:1393/2330 train_time:80966ms step_avg:58.12ms
step:1394/2330 train_time:81026ms step_avg:58.13ms
step:1395/2330 train_time:81083ms step_avg:58.12ms
step:1396/2330 train_time:81144ms step_avg:58.13ms
step:1397/2330 train_time:81200ms step_avg:58.12ms
step:1398/2330 train_time:81260ms step_avg:58.13ms
step:1399/2330 train_time:81317ms step_avg:58.13ms
step:1400/2330 train_time:81377ms step_avg:58.13ms
step:1401/2330 train_time:81434ms step_avg:58.13ms
step:1402/2330 train_time:81494ms step_avg:58.13ms
step:1403/2330 train_time:81552ms step_avg:58.13ms
step:1404/2330 train_time:81612ms step_avg:58.13ms
step:1405/2330 train_time:81668ms step_avg:58.13ms
step:1406/2330 train_time:81729ms step_avg:58.13ms
step:1407/2330 train_time:81785ms step_avg:58.13ms
step:1408/2330 train_time:81847ms step_avg:58.13ms
step:1409/2330 train_time:81903ms step_avg:58.13ms
step:1410/2330 train_time:81964ms step_avg:58.13ms
step:1411/2330 train_time:82020ms step_avg:58.13ms
step:1412/2330 train_time:82081ms step_avg:58.13ms
step:1413/2330 train_time:82138ms step_avg:58.13ms
step:1414/2330 train_time:82199ms step_avg:58.13ms
step:1415/2330 train_time:82255ms step_avg:58.13ms
step:1416/2330 train_time:82316ms step_avg:58.13ms
step:1417/2330 train_time:82373ms step_avg:58.13ms
step:1418/2330 train_time:82434ms step_avg:58.13ms
step:1419/2330 train_time:82491ms step_avg:58.13ms
step:1420/2330 train_time:82551ms step_avg:58.13ms
step:1421/2330 train_time:82607ms step_avg:58.13ms
step:1422/2330 train_time:82667ms step_avg:58.13ms
step:1423/2330 train_time:82724ms step_avg:58.13ms
step:1424/2330 train_time:82785ms step_avg:58.14ms
step:1425/2330 train_time:82843ms step_avg:58.14ms
step:1426/2330 train_time:82903ms step_avg:58.14ms
step:1427/2330 train_time:82959ms step_avg:58.14ms
step:1428/2330 train_time:83020ms step_avg:58.14ms
step:1429/2330 train_time:83077ms step_avg:58.14ms
step:1430/2330 train_time:83138ms step_avg:58.14ms
step:1431/2330 train_time:83195ms step_avg:58.14ms
step:1432/2330 train_time:83254ms step_avg:58.14ms
step:1433/2330 train_time:83310ms step_avg:58.14ms
step:1434/2330 train_time:83371ms step_avg:58.14ms
step:1435/2330 train_time:83428ms step_avg:58.14ms
step:1436/2330 train_time:83489ms step_avg:58.14ms
step:1437/2330 train_time:83545ms step_avg:58.14ms
step:1438/2330 train_time:83606ms step_avg:58.14ms
step:1439/2330 train_time:83661ms step_avg:58.14ms
step:1440/2330 train_time:83723ms step_avg:58.14ms
step:1441/2330 train_time:83779ms step_avg:58.14ms
step:1442/2330 train_time:83841ms step_avg:58.14ms
step:1443/2330 train_time:83898ms step_avg:58.14ms
step:1444/2330 train_time:83958ms step_avg:58.14ms
step:1445/2330 train_time:84015ms step_avg:58.14ms
step:1446/2330 train_time:84075ms step_avg:58.14ms
step:1447/2330 train_time:84132ms step_avg:58.14ms
step:1448/2330 train_time:84192ms step_avg:58.14ms
step:1449/2330 train_time:84249ms step_avg:58.14ms
step:1450/2330 train_time:84309ms step_avg:58.14ms
step:1451/2330 train_time:84366ms step_avg:58.14ms
step:1452/2330 train_time:84426ms step_avg:58.14ms
step:1453/2330 train_time:84482ms step_avg:58.14ms
step:1454/2330 train_time:84543ms step_avg:58.15ms
step:1455/2330 train_time:84599ms step_avg:58.14ms
step:1456/2330 train_time:84660ms step_avg:58.15ms
step:1457/2330 train_time:84717ms step_avg:58.15ms
step:1458/2330 train_time:84778ms step_avg:58.15ms
step:1459/2330 train_time:84836ms step_avg:58.15ms
step:1460/2330 train_time:84896ms step_avg:58.15ms
step:1461/2330 train_time:84954ms step_avg:58.15ms
step:1462/2330 train_time:85014ms step_avg:58.15ms
step:1463/2330 train_time:85070ms step_avg:58.15ms
step:1464/2330 train_time:85130ms step_avg:58.15ms
step:1465/2330 train_time:85186ms step_avg:58.15ms
step:1466/2330 train_time:85247ms step_avg:58.15ms
step:1467/2330 train_time:85304ms step_avg:58.15ms
step:1468/2330 train_time:85364ms step_avg:58.15ms
step:1469/2330 train_time:85420ms step_avg:58.15ms
step:1470/2330 train_time:85481ms step_avg:58.15ms
step:1471/2330 train_time:85538ms step_avg:58.15ms
step:1472/2330 train_time:85598ms step_avg:58.15ms
step:1473/2330 train_time:85655ms step_avg:58.15ms
step:1474/2330 train_time:85717ms step_avg:58.15ms
step:1475/2330 train_time:85773ms step_avg:58.15ms
step:1476/2330 train_time:85834ms step_avg:58.15ms
step:1477/2330 train_time:85891ms step_avg:58.15ms
step:1478/2330 train_time:85951ms step_avg:58.15ms
step:1479/2330 train_time:86007ms step_avg:58.15ms
step:1480/2330 train_time:86067ms step_avg:58.15ms
step:1481/2330 train_time:86124ms step_avg:58.15ms
step:1482/2330 train_time:86185ms step_avg:58.15ms
step:1483/2330 train_time:86242ms step_avg:58.15ms
step:1484/2330 train_time:86302ms step_avg:58.16ms
step:1485/2330 train_time:86359ms step_avg:58.15ms
step:1486/2330 train_time:86419ms step_avg:58.16ms
step:1487/2330 train_time:86475ms step_avg:58.15ms
step:1488/2330 train_time:86537ms step_avg:58.16ms
step:1489/2330 train_time:86594ms step_avg:58.16ms
step:1490/2330 train_time:86654ms step_avg:58.16ms
step:1491/2330 train_time:86710ms step_avg:58.16ms
step:1492/2330 train_time:86771ms step_avg:58.16ms
step:1493/2330 train_time:86827ms step_avg:58.16ms
step:1494/2330 train_time:86888ms step_avg:58.16ms
step:1495/2330 train_time:86945ms step_avg:58.16ms
step:1496/2330 train_time:87005ms step_avg:58.16ms
step:1497/2330 train_time:87061ms step_avg:58.16ms
step:1498/2330 train_time:87122ms step_avg:58.16ms
step:1499/2330 train_time:87178ms step_avg:58.16ms
step:1500/2330 train_time:87240ms step_avg:58.16ms
step:1500/2330 val_loss:3.9016 train_time:87320ms step_avg:58.21ms
step:1501/2330 train_time:87339ms step_avg:58.19ms
step:1502/2330 train_time:87359ms step_avg:58.16ms
step:1503/2330 train_time:87419ms step_avg:58.16ms
step:1504/2330 train_time:87486ms step_avg:58.17ms
step:1505/2330 train_time:87545ms step_avg:58.17ms
step:1506/2330 train_time:87605ms step_avg:58.17ms
step:1507/2330 train_time:87662ms step_avg:58.17ms
step:1508/2330 train_time:87722ms step_avg:58.17ms
step:1509/2330 train_time:87778ms step_avg:58.17ms
step:1510/2330 train_time:87838ms step_avg:58.17ms
step:1511/2330 train_time:87893ms step_avg:58.17ms
step:1512/2330 train_time:87953ms step_avg:58.17ms
step:1513/2330 train_time:88009ms step_avg:58.17ms
step:1514/2330 train_time:88069ms step_avg:58.17ms
step:1515/2330 train_time:88125ms step_avg:58.17ms
step:1516/2330 train_time:88184ms step_avg:58.17ms
step:1517/2330 train_time:88241ms step_avg:58.17ms
step:1518/2330 train_time:88301ms step_avg:58.17ms
step:1519/2330 train_time:88358ms step_avg:58.17ms
step:1520/2330 train_time:88422ms step_avg:58.17ms
step:1521/2330 train_time:88481ms step_avg:58.17ms
step:1522/2330 train_time:88545ms step_avg:58.18ms
step:1523/2330 train_time:88602ms step_avg:58.18ms
step:1524/2330 train_time:88665ms step_avg:58.18ms
step:1525/2330 train_time:88721ms step_avg:58.18ms
step:1526/2330 train_time:88782ms step_avg:58.18ms
step:1527/2330 train_time:88838ms step_avg:58.18ms
step:1528/2330 train_time:88899ms step_avg:58.18ms
step:1529/2330 train_time:88957ms step_avg:58.18ms
step:1530/2330 train_time:89016ms step_avg:58.18ms
step:1531/2330 train_time:89073ms step_avg:58.18ms
step:1532/2330 train_time:89132ms step_avg:58.18ms
step:1533/2330 train_time:89189ms step_avg:58.18ms
step:1534/2330 train_time:89248ms step_avg:58.18ms
step:1535/2330 train_time:89305ms step_avg:58.18ms
step:1536/2330 train_time:89367ms step_avg:58.18ms
step:1537/2330 train_time:89424ms step_avg:58.18ms
step:1538/2330 train_time:89488ms step_avg:58.18ms
step:1539/2330 train_time:89545ms step_avg:58.18ms
step:1540/2330 train_time:89609ms step_avg:58.19ms
step:1541/2330 train_time:89667ms step_avg:58.19ms
step:1542/2330 train_time:89729ms step_avg:58.19ms
step:1543/2330 train_time:89785ms step_avg:58.19ms
step:1544/2330 train_time:89847ms step_avg:58.19ms
step:1545/2330 train_time:89903ms step_avg:58.19ms
step:1546/2330 train_time:89966ms step_avg:58.19ms
step:1547/2330 train_time:90022ms step_avg:58.19ms
step:1548/2330 train_time:90084ms step_avg:58.19ms
step:1549/2330 train_time:90141ms step_avg:58.19ms
step:1550/2330 train_time:90201ms step_avg:58.19ms
step:1551/2330 train_time:90258ms step_avg:58.19ms
step:1552/2330 train_time:90319ms step_avg:58.20ms
step:1553/2330 train_time:90377ms step_avg:58.19ms
step:1554/2330 train_time:90439ms step_avg:58.20ms
step:1555/2330 train_time:90498ms step_avg:58.20ms
step:1556/2330 train_time:90560ms step_avg:58.20ms
step:1557/2330 train_time:90618ms step_avg:58.20ms
step:1558/2330 train_time:90679ms step_avg:58.20ms
step:1559/2330 train_time:90736ms step_avg:58.20ms
step:1560/2330 train_time:90797ms step_avg:58.20ms
step:1561/2330 train_time:90855ms step_avg:58.20ms
step:1562/2330 train_time:90916ms step_avg:58.20ms
step:1563/2330 train_time:90974ms step_avg:58.20ms
step:1564/2330 train_time:91033ms step_avg:58.21ms
step:1565/2330 train_time:91090ms step_avg:58.20ms
step:1566/2330 train_time:91152ms step_avg:58.21ms
step:1567/2330 train_time:91208ms step_avg:58.21ms
step:1568/2330 train_time:91270ms step_avg:58.21ms
step:1569/2330 train_time:91326ms step_avg:58.21ms
step:1570/2330 train_time:91388ms step_avg:58.21ms
step:1571/2330 train_time:91446ms step_avg:58.21ms
step:1572/2330 train_time:91508ms step_avg:58.21ms
step:1573/2330 train_time:91565ms step_avg:58.21ms
step:1574/2330 train_time:91627ms step_avg:58.21ms
step:1575/2330 train_time:91683ms step_avg:58.21ms
step:1576/2330 train_time:91746ms step_avg:58.21ms
step:1577/2330 train_time:91803ms step_avg:58.21ms
step:1578/2330 train_time:91866ms step_avg:58.22ms
step:1579/2330 train_time:91923ms step_avg:58.22ms
step:1580/2330 train_time:91984ms step_avg:58.22ms
step:1581/2330 train_time:92041ms step_avg:58.22ms
step:1582/2330 train_time:92101ms step_avg:58.22ms
step:1583/2330 train_time:92159ms step_avg:58.22ms
step:1584/2330 train_time:92221ms step_avg:58.22ms
step:1585/2330 train_time:92278ms step_avg:58.22ms
step:1586/2330 train_time:92338ms step_avg:58.22ms
step:1587/2330 train_time:92396ms step_avg:58.22ms
step:1588/2330 train_time:92456ms step_avg:58.22ms
step:1589/2330 train_time:92514ms step_avg:58.22ms
step:1590/2330 train_time:92576ms step_avg:58.22ms
step:1591/2330 train_time:92633ms step_avg:58.22ms
step:1592/2330 train_time:92693ms step_avg:58.22ms
step:1593/2330 train_time:92750ms step_avg:58.22ms
step:1594/2330 train_time:92814ms step_avg:58.23ms
step:1595/2330 train_time:92871ms step_avg:58.23ms
step:1596/2330 train_time:92932ms step_avg:58.23ms
step:1597/2330 train_time:92989ms step_avg:58.23ms
step:1598/2330 train_time:93051ms step_avg:58.23ms
step:1599/2330 train_time:93107ms step_avg:58.23ms
step:1600/2330 train_time:93169ms step_avg:58.23ms
step:1601/2330 train_time:93226ms step_avg:58.23ms
step:1602/2330 train_time:93287ms step_avg:58.23ms
step:1603/2330 train_time:93344ms step_avg:58.23ms
step:1604/2330 train_time:93405ms step_avg:58.23ms
step:1605/2330 train_time:93463ms step_avg:58.23ms
step:1606/2330 train_time:93523ms step_avg:58.23ms
step:1607/2330 train_time:93580ms step_avg:58.23ms
step:1608/2330 train_time:93642ms step_avg:58.24ms
step:1609/2330 train_time:93700ms step_avg:58.24ms
step:1610/2330 train_time:93762ms step_avg:58.24ms
step:1611/2330 train_time:93820ms step_avg:58.24ms
step:1612/2330 train_time:93881ms step_avg:58.24ms
step:1613/2330 train_time:93938ms step_avg:58.24ms
step:1614/2330 train_time:94000ms step_avg:58.24ms
step:1615/2330 train_time:94057ms step_avg:58.24ms
step:1616/2330 train_time:94120ms step_avg:58.24ms
step:1617/2330 train_time:94177ms step_avg:58.24ms
step:1618/2330 train_time:94238ms step_avg:58.24ms
step:1619/2330 train_time:94296ms step_avg:58.24ms
step:1620/2330 train_time:94356ms step_avg:58.24ms
step:1621/2330 train_time:94413ms step_avg:58.24ms
step:1622/2330 train_time:94475ms step_avg:58.25ms
step:1623/2330 train_time:94532ms step_avg:58.25ms
step:1624/2330 train_time:94593ms step_avg:58.25ms
step:1625/2330 train_time:94651ms step_avg:58.25ms
step:1626/2330 train_time:94713ms step_avg:58.25ms
step:1627/2330 train_time:94770ms step_avg:58.25ms
step:1628/2330 train_time:94831ms step_avg:58.25ms
step:1629/2330 train_time:94888ms step_avg:58.25ms
step:1630/2330 train_time:94950ms step_avg:58.25ms
step:1631/2330 train_time:95007ms step_avg:58.25ms
step:1632/2330 train_time:95068ms step_avg:58.25ms
step:1633/2330 train_time:95124ms step_avg:58.25ms
step:1634/2330 train_time:95187ms step_avg:58.25ms
step:1635/2330 train_time:95243ms step_avg:58.25ms
step:1636/2330 train_time:95305ms step_avg:58.25ms
step:1637/2330 train_time:95362ms step_avg:58.25ms
step:1638/2330 train_time:95424ms step_avg:58.26ms
step:1639/2330 train_time:95482ms step_avg:58.26ms
step:1640/2330 train_time:95543ms step_avg:58.26ms
step:1641/2330 train_time:95600ms step_avg:58.26ms
step:1642/2330 train_time:95663ms step_avg:58.26ms
step:1643/2330 train_time:95721ms step_avg:58.26ms
step:1644/2330 train_time:95782ms step_avg:58.26ms
step:1645/2330 train_time:95841ms step_avg:58.26ms
step:1646/2330 train_time:95902ms step_avg:58.26ms
step:1647/2330 train_time:95961ms step_avg:58.26ms
step:1648/2330 train_time:96021ms step_avg:58.27ms
step:1649/2330 train_time:96079ms step_avg:58.27ms
step:1650/2330 train_time:96140ms step_avg:58.27ms
step:1651/2330 train_time:96197ms step_avg:58.27ms
step:1652/2330 train_time:96258ms step_avg:58.27ms
step:1653/2330 train_time:96316ms step_avg:58.27ms
step:1654/2330 train_time:96377ms step_avg:58.27ms
step:1655/2330 train_time:96434ms step_avg:58.27ms
step:1656/2330 train_time:96494ms step_avg:58.27ms
step:1657/2330 train_time:96552ms step_avg:58.27ms
step:1658/2330 train_time:96614ms step_avg:58.27ms
step:1659/2330 train_time:96671ms step_avg:58.27ms
step:1660/2330 train_time:96732ms step_avg:58.27ms
step:1661/2330 train_time:96788ms step_avg:58.27ms
step:1662/2330 train_time:96850ms step_avg:58.27ms
step:1663/2330 train_time:96907ms step_avg:58.27ms
step:1664/2330 train_time:96969ms step_avg:58.27ms
step:1665/2330 train_time:97025ms step_avg:58.27ms
step:1666/2330 train_time:97087ms step_avg:58.28ms
step:1667/2330 train_time:97144ms step_avg:58.27ms
step:1668/2330 train_time:97205ms step_avg:58.28ms
step:1669/2330 train_time:97262ms step_avg:58.28ms
step:1670/2330 train_time:97325ms step_avg:58.28ms
step:1671/2330 train_time:97382ms step_avg:58.28ms
step:1672/2330 train_time:97442ms step_avg:58.28ms
step:1673/2330 train_time:97500ms step_avg:58.28ms
step:1674/2330 train_time:97563ms step_avg:58.28ms
step:1675/2330 train_time:97621ms step_avg:58.28ms
step:1676/2330 train_time:97682ms step_avg:58.28ms
step:1677/2330 train_time:97740ms step_avg:58.28ms
step:1678/2330 train_time:97800ms step_avg:58.28ms
step:1679/2330 train_time:97857ms step_avg:58.28ms
step:1680/2330 train_time:97919ms step_avg:58.29ms
step:1681/2330 train_time:97977ms step_avg:58.28ms
step:1682/2330 train_time:98037ms step_avg:58.29ms
step:1683/2330 train_time:98095ms step_avg:58.29ms
step:1684/2330 train_time:98155ms step_avg:58.29ms
step:1685/2330 train_time:98213ms step_avg:58.29ms
step:1686/2330 train_time:98273ms step_avg:58.29ms
step:1687/2330 train_time:98330ms step_avg:58.29ms
step:1688/2330 train_time:98391ms step_avg:58.29ms
step:1689/2330 train_time:98448ms step_avg:58.29ms
step:1690/2330 train_time:98511ms step_avg:58.29ms
step:1691/2330 train_time:98568ms step_avg:58.29ms
step:1692/2330 train_time:98629ms step_avg:58.29ms
step:1693/2330 train_time:98686ms step_avg:58.29ms
step:1694/2330 train_time:98748ms step_avg:58.29ms
step:1695/2330 train_time:98805ms step_avg:58.29ms
step:1696/2330 train_time:98868ms step_avg:58.29ms
step:1697/2330 train_time:98924ms step_avg:58.29ms
step:1698/2330 train_time:98987ms step_avg:58.30ms
step:1699/2330 train_time:99044ms step_avg:58.30ms
step:1700/2330 train_time:99105ms step_avg:58.30ms
step:1701/2330 train_time:99162ms step_avg:58.30ms
step:1702/2330 train_time:99224ms step_avg:58.30ms
step:1703/2330 train_time:99281ms step_avg:58.30ms
step:1704/2330 train_time:99343ms step_avg:58.30ms
step:1705/2330 train_time:99401ms step_avg:58.30ms
step:1706/2330 train_time:99464ms step_avg:58.30ms
step:1707/2330 train_time:99521ms step_avg:58.30ms
step:1708/2330 train_time:99581ms step_avg:58.30ms
step:1709/2330 train_time:99639ms step_avg:58.30ms
step:1710/2330 train_time:99699ms step_avg:58.30ms
step:1711/2330 train_time:99756ms step_avg:58.30ms
step:1712/2330 train_time:99817ms step_avg:58.30ms
step:1713/2330 train_time:99875ms step_avg:58.30ms
step:1714/2330 train_time:99936ms step_avg:58.31ms
step:1715/2330 train_time:99993ms step_avg:58.30ms
step:1716/2330 train_time:100053ms step_avg:58.31ms
step:1717/2330 train_time:100110ms step_avg:58.31ms
step:1718/2330 train_time:100171ms step_avg:58.31ms
step:1719/2330 train_time:100228ms step_avg:58.31ms
step:1720/2330 train_time:100289ms step_avg:58.31ms
step:1721/2330 train_time:100345ms step_avg:58.31ms
step:1722/2330 train_time:100409ms step_avg:58.31ms
step:1723/2330 train_time:100466ms step_avg:58.31ms
step:1724/2330 train_time:100529ms step_avg:58.31ms
step:1725/2330 train_time:100585ms step_avg:58.31ms
step:1726/2330 train_time:100647ms step_avg:58.31ms
step:1727/2330 train_time:100703ms step_avg:58.31ms
step:1728/2330 train_time:100765ms step_avg:58.31ms
step:1729/2330 train_time:100822ms step_avg:58.31ms
step:1730/2330 train_time:100884ms step_avg:58.31ms
step:1731/2330 train_time:100942ms step_avg:58.31ms
step:1732/2330 train_time:101002ms step_avg:58.32ms
step:1733/2330 train_time:101061ms step_avg:58.32ms
step:1734/2330 train_time:101121ms step_avg:58.32ms
step:1735/2330 train_time:101179ms step_avg:58.32ms
step:1736/2330 train_time:101240ms step_avg:58.32ms
step:1737/2330 train_time:101298ms step_avg:58.32ms
step:1738/2330 train_time:101360ms step_avg:58.32ms
step:1739/2330 train_time:101417ms step_avg:58.32ms
step:1740/2330 train_time:101478ms step_avg:58.32ms
step:1741/2330 train_time:101535ms step_avg:58.32ms
step:1742/2330 train_time:101595ms step_avg:58.32ms
step:1743/2330 train_time:101651ms step_avg:58.32ms
step:1744/2330 train_time:101713ms step_avg:58.32ms
step:1745/2330 train_time:101770ms step_avg:58.32ms
step:1746/2330 train_time:101832ms step_avg:58.32ms
step:1747/2330 train_time:101888ms step_avg:58.32ms
step:1748/2330 train_time:101950ms step_avg:58.32ms
step:1749/2330 train_time:102007ms step_avg:58.32ms
step:1750/2330 train_time:102069ms step_avg:58.33ms
step:1750/2330 val_loss:3.8174 train_time:102151ms step_avg:58.37ms
step:1751/2330 train_time:102170ms step_avg:58.35ms
step:1752/2330 train_time:102190ms step_avg:58.33ms
step:1753/2330 train_time:102251ms step_avg:58.33ms
step:1754/2330 train_time:102314ms step_avg:58.33ms
step:1755/2330 train_time:102371ms step_avg:58.33ms
step:1756/2330 train_time:102433ms step_avg:58.33ms
step:1757/2330 train_time:102489ms step_avg:58.33ms
step:1758/2330 train_time:102549ms step_avg:58.33ms
step:1759/2330 train_time:102606ms step_avg:58.33ms
step:1760/2330 train_time:102667ms step_avg:58.33ms
step:1761/2330 train_time:102724ms step_avg:58.33ms
step:1762/2330 train_time:102783ms step_avg:58.33ms
step:1763/2330 train_time:102839ms step_avg:58.33ms
step:1764/2330 train_time:102899ms step_avg:58.33ms
step:1765/2330 train_time:102956ms step_avg:58.33ms
step:1766/2330 train_time:103016ms step_avg:58.33ms
step:1767/2330 train_time:103072ms step_avg:58.33ms
step:1768/2330 train_time:103140ms step_avg:58.34ms
step:1769/2330 train_time:103196ms step_avg:58.34ms
step:1770/2330 train_time:103262ms step_avg:58.34ms
step:1771/2330 train_time:103319ms step_avg:58.34ms
step:1772/2330 train_time:103380ms step_avg:58.34ms
step:1773/2330 train_time:103437ms step_avg:58.34ms
step:1774/2330 train_time:103499ms step_avg:58.34ms
step:1775/2330 train_time:103555ms step_avg:58.34ms
step:1776/2330 train_time:103617ms step_avg:58.34ms
step:1777/2330 train_time:103673ms step_avg:58.34ms
step:1778/2330 train_time:103735ms step_avg:58.34ms
step:1779/2330 train_time:103791ms step_avg:58.34ms
step:1780/2330 train_time:103852ms step_avg:58.34ms
step:1781/2330 train_time:103908ms step_avg:58.34ms
step:1782/2330 train_time:103969ms step_avg:58.34ms
step:1783/2330 train_time:104027ms step_avg:58.34ms
step:1784/2330 train_time:104090ms step_avg:58.35ms
step:1785/2330 train_time:104148ms step_avg:58.35ms
step:1786/2330 train_time:104212ms step_avg:58.35ms
step:1787/2330 train_time:104269ms step_avg:58.35ms
step:1788/2330 train_time:104331ms step_avg:58.35ms
step:1789/2330 train_time:104388ms step_avg:58.35ms
step:1790/2330 train_time:104450ms step_avg:58.35ms
step:1791/2330 train_time:104508ms step_avg:58.35ms
step:1792/2330 train_time:104569ms step_avg:58.35ms
step:1793/2330 train_time:104626ms step_avg:58.35ms
step:1794/2330 train_time:104686ms step_avg:58.35ms
step:1795/2330 train_time:104743ms step_avg:58.35ms
step:1796/2330 train_time:104803ms step_avg:58.35ms
step:1797/2330 train_time:104859ms step_avg:58.35ms
step:1798/2330 train_time:104919ms step_avg:58.35ms
step:1799/2330 train_time:104975ms step_avg:58.35ms
step:1800/2330 train_time:105037ms step_avg:58.35ms
step:1801/2330 train_time:105094ms step_avg:58.35ms
step:1802/2330 train_time:105158ms step_avg:58.36ms
step:1803/2330 train_time:105214ms step_avg:58.35ms
step:1804/2330 train_time:105276ms step_avg:58.36ms
step:1805/2330 train_time:105332ms step_avg:58.36ms
step:1806/2330 train_time:105396ms step_avg:58.36ms
step:1807/2330 train_time:105453ms step_avg:58.36ms
step:1808/2330 train_time:105514ms step_avg:58.36ms
step:1809/2330 train_time:105570ms step_avg:58.36ms
step:1810/2330 train_time:105634ms step_avg:58.36ms
step:1811/2330 train_time:105690ms step_avg:58.36ms
step:1812/2330 train_time:105751ms step_avg:58.36ms
step:1813/2330 train_time:105807ms step_avg:58.36ms
step:1814/2330 train_time:105868ms step_avg:58.36ms
step:1815/2330 train_time:105925ms step_avg:58.36ms
step:1816/2330 train_time:105987ms step_avg:58.36ms
step:1817/2330 train_time:106045ms step_avg:58.36ms
step:1818/2330 train_time:106105ms step_avg:58.36ms
step:1819/2330 train_time:106163ms step_avg:58.36ms
step:1820/2330 train_time:106225ms step_avg:58.37ms
step:1821/2330 train_time:106283ms step_avg:58.37ms
step:1822/2330 train_time:106345ms step_avg:58.37ms
step:1823/2330 train_time:106402ms step_avg:58.37ms
step:1824/2330 train_time:106464ms step_avg:58.37ms
step:1825/2330 train_time:106521ms step_avg:58.37ms
step:1826/2330 train_time:106581ms step_avg:58.37ms
step:1827/2330 train_time:106638ms step_avg:58.37ms
step:1828/2330 train_time:106700ms step_avg:58.37ms
step:1829/2330 train_time:106757ms step_avg:58.37ms
step:1830/2330 train_time:106818ms step_avg:58.37ms
step:1831/2330 train_time:106874ms step_avg:58.37ms
step:1832/2330 train_time:106936ms step_avg:58.37ms
step:1833/2330 train_time:106993ms step_avg:58.37ms
step:1834/2330 train_time:107055ms step_avg:58.37ms
step:1835/2330 train_time:107111ms step_avg:58.37ms
step:1836/2330 train_time:107173ms step_avg:58.37ms
step:1837/2330 train_time:107230ms step_avg:58.37ms
step:1838/2330 train_time:107292ms step_avg:58.37ms
step:1839/2330 train_time:107349ms step_avg:58.37ms
step:1840/2330 train_time:107411ms step_avg:58.38ms
step:1841/2330 train_time:107468ms step_avg:58.37ms
step:1842/2330 train_time:107531ms step_avg:58.38ms
step:1843/2330 train_time:107587ms step_avg:58.38ms
step:1844/2330 train_time:107650ms step_avg:58.38ms
step:1845/2330 train_time:107706ms step_avg:58.38ms
step:1846/2330 train_time:107767ms step_avg:58.38ms
step:1847/2330 train_time:107825ms step_avg:58.38ms
step:1848/2330 train_time:107885ms step_avg:58.38ms
step:1849/2330 train_time:107943ms step_avg:58.38ms
step:1850/2330 train_time:108003ms step_avg:58.38ms
step:1851/2330 train_time:108060ms step_avg:58.38ms
step:1852/2330 train_time:108120ms step_avg:58.38ms
step:1853/2330 train_time:108177ms step_avg:58.38ms
step:1854/2330 train_time:108239ms step_avg:58.38ms
step:1855/2330 train_time:108295ms step_avg:58.38ms
step:1856/2330 train_time:108359ms step_avg:58.38ms
step:1857/2330 train_time:108415ms step_avg:58.38ms
step:1858/2330 train_time:108478ms step_avg:58.38ms
step:1859/2330 train_time:108534ms step_avg:58.38ms
step:1860/2330 train_time:108597ms step_avg:58.39ms
step:1861/2330 train_time:108654ms step_avg:58.38ms
step:1862/2330 train_time:108717ms step_avg:58.39ms
step:1863/2330 train_time:108772ms step_avg:58.39ms
step:1864/2330 train_time:108835ms step_avg:58.39ms
step:1865/2330 train_time:108890ms step_avg:58.39ms
step:1866/2330 train_time:108953ms step_avg:58.39ms
step:1867/2330 train_time:109010ms step_avg:58.39ms
step:1868/2330 train_time:109071ms step_avg:58.39ms
step:1869/2330 train_time:109128ms step_avg:58.39ms
step:1870/2330 train_time:109190ms step_avg:58.39ms
step:1871/2330 train_time:109248ms step_avg:58.39ms
step:1872/2330 train_time:109309ms step_avg:58.39ms
step:1873/2330 train_time:109366ms step_avg:58.39ms
step:1874/2330 train_time:109427ms step_avg:58.39ms
step:1875/2330 train_time:109484ms step_avg:58.39ms
step:1876/2330 train_time:109545ms step_avg:58.39ms
step:1877/2330 train_time:109604ms step_avg:58.39ms
step:1878/2330 train_time:109664ms step_avg:58.39ms
step:1879/2330 train_time:109721ms step_avg:58.39ms
step:1880/2330 train_time:109781ms step_avg:58.39ms
step:1881/2330 train_time:109837ms step_avg:58.39ms
step:1882/2330 train_time:109898ms step_avg:58.39ms
step:1883/2330 train_time:109955ms step_avg:58.39ms
step:1884/2330 train_time:110016ms step_avg:58.39ms
step:1885/2330 train_time:110072ms step_avg:58.39ms
step:1886/2330 train_time:110135ms step_avg:58.40ms
step:1887/2330 train_time:110191ms step_avg:58.40ms
step:1888/2330 train_time:110254ms step_avg:58.40ms
step:1889/2330 train_time:110311ms step_avg:58.40ms
step:1890/2330 train_time:110373ms step_avg:58.40ms
step:1891/2330 train_time:110430ms step_avg:58.40ms
step:1892/2330 train_time:110493ms step_avg:58.40ms
step:1893/2330 train_time:110550ms step_avg:58.40ms
step:1894/2330 train_time:110612ms step_avg:58.40ms
step:1895/2330 train_time:110669ms step_avg:58.40ms
step:1896/2330 train_time:110730ms step_avg:58.40ms
step:1897/2330 train_time:110786ms step_avg:58.40ms
step:1898/2330 train_time:110848ms step_avg:58.40ms
step:1899/2330 train_time:110906ms step_avg:58.40ms
step:1900/2330 train_time:110966ms step_avg:58.40ms
step:1901/2330 train_time:111023ms step_avg:58.40ms
step:1902/2330 train_time:111084ms step_avg:58.40ms
step:1903/2330 train_time:111141ms step_avg:58.40ms
step:1904/2330 train_time:111201ms step_avg:58.40ms
step:1905/2330 train_time:111258ms step_avg:58.40ms
step:1906/2330 train_time:111320ms step_avg:58.41ms
step:1907/2330 train_time:111377ms step_avg:58.40ms
step:1908/2330 train_time:111440ms step_avg:58.41ms
step:1909/2330 train_time:111497ms step_avg:58.41ms
step:1910/2330 train_time:111559ms step_avg:58.41ms
step:1911/2330 train_time:111615ms step_avg:58.41ms
step:1912/2330 train_time:111677ms step_avg:58.41ms
step:1913/2330 train_time:111733ms step_avg:58.41ms
step:1914/2330 train_time:111795ms step_avg:58.41ms
step:1915/2330 train_time:111852ms step_avg:58.41ms
step:1916/2330 train_time:111913ms step_avg:58.41ms
step:1917/2330 train_time:111969ms step_avg:58.41ms
step:1918/2330 train_time:112031ms step_avg:58.41ms
step:1919/2330 train_time:112088ms step_avg:58.41ms
step:1920/2330 train_time:112150ms step_avg:58.41ms
step:1921/2330 train_time:112208ms step_avg:58.41ms
step:1922/2330 train_time:112269ms step_avg:58.41ms
step:1923/2330 train_time:112327ms step_avg:58.41ms
step:1924/2330 train_time:112388ms step_avg:58.41ms
step:1925/2330 train_time:112446ms step_avg:58.41ms
step:1926/2330 train_time:112507ms step_avg:58.41ms
step:1927/2330 train_time:112565ms step_avg:58.41ms
step:1928/2330 train_time:112627ms step_avg:58.42ms
step:1929/2330 train_time:112684ms step_avg:58.42ms
step:1930/2330 train_time:112745ms step_avg:58.42ms
step:1931/2330 train_time:112803ms step_avg:58.42ms
step:1932/2330 train_time:112862ms step_avg:58.42ms
step:1933/2330 train_time:112919ms step_avg:58.42ms
step:1934/2330 train_time:112979ms step_avg:58.42ms
step:1935/2330 train_time:113035ms step_avg:58.42ms
step:1936/2330 train_time:113099ms step_avg:58.42ms
step:1937/2330 train_time:113155ms step_avg:58.42ms
step:1938/2330 train_time:113217ms step_avg:58.42ms
step:1939/2330 train_time:113273ms step_avg:58.42ms
step:1940/2330 train_time:113335ms step_avg:58.42ms
step:1941/2330 train_time:113392ms step_avg:58.42ms
step:1942/2330 train_time:113454ms step_avg:58.42ms
step:1943/2330 train_time:113510ms step_avg:58.42ms
step:1944/2330 train_time:113572ms step_avg:58.42ms
step:1945/2330 train_time:113628ms step_avg:58.42ms
step:1946/2330 train_time:113691ms step_avg:58.42ms
step:1947/2330 train_time:113748ms step_avg:58.42ms
step:1948/2330 train_time:113810ms step_avg:58.42ms
step:1949/2330 train_time:113867ms step_avg:58.42ms
step:1950/2330 train_time:113928ms step_avg:58.42ms
step:1951/2330 train_time:113986ms step_avg:58.42ms
step:1952/2330 train_time:114048ms step_avg:58.43ms
step:1953/2330 train_time:114105ms step_avg:58.43ms
step:1954/2330 train_time:114166ms step_avg:58.43ms
step:1955/2330 train_time:114223ms step_avg:58.43ms
step:1956/2330 train_time:114285ms step_avg:58.43ms
step:1957/2330 train_time:114343ms step_avg:58.43ms
step:1958/2330 train_time:114403ms step_avg:58.43ms
step:1959/2330 train_time:114460ms step_avg:58.43ms
step:1960/2330 train_time:114520ms step_avg:58.43ms
step:1961/2330 train_time:114577ms step_avg:58.43ms
step:1962/2330 train_time:114640ms step_avg:58.43ms
step:1963/2330 train_time:114696ms step_avg:58.43ms
step:1964/2330 train_time:114759ms step_avg:58.43ms
step:1965/2330 train_time:114815ms step_avg:58.43ms
step:1966/2330 train_time:114877ms step_avg:58.43ms
step:1967/2330 train_time:114933ms step_avg:58.43ms
step:1968/2330 train_time:114996ms step_avg:58.43ms
step:1969/2330 train_time:115052ms step_avg:58.43ms
step:1970/2330 train_time:115114ms step_avg:58.43ms
step:1971/2330 train_time:115170ms step_avg:58.43ms
step:1972/2330 train_time:115232ms step_avg:58.43ms
step:1973/2330 train_time:115289ms step_avg:58.43ms
step:1974/2330 train_time:115352ms step_avg:58.44ms
step:1975/2330 train_time:115409ms step_avg:58.43ms
step:1976/2330 train_time:115470ms step_avg:58.44ms
step:1977/2330 train_time:115527ms step_avg:58.44ms
step:1978/2330 train_time:115588ms step_avg:58.44ms
step:1979/2330 train_time:115646ms step_avg:58.44ms
step:1980/2330 train_time:115708ms step_avg:58.44ms
step:1981/2330 train_time:115767ms step_avg:58.44ms
step:1982/2330 train_time:115827ms step_avg:58.44ms
step:1983/2330 train_time:115884ms step_avg:58.44ms
step:1984/2330 train_time:115945ms step_avg:58.44ms
step:1985/2330 train_time:116002ms step_avg:58.44ms
step:1986/2330 train_time:116062ms step_avg:58.44ms
step:1987/2330 train_time:116118ms step_avg:58.44ms
step:1988/2330 train_time:116179ms step_avg:58.44ms
step:1989/2330 train_time:116236ms step_avg:58.44ms
step:1990/2330 train_time:116298ms step_avg:58.44ms
step:1991/2330 train_time:116354ms step_avg:58.44ms
step:1992/2330 train_time:116416ms step_avg:58.44ms
step:1993/2330 train_time:116473ms step_avg:58.44ms
step:1994/2330 train_time:116536ms step_avg:58.44ms
step:1995/2330 train_time:116592ms step_avg:58.44ms
step:1996/2330 train_time:116656ms step_avg:58.44ms
step:1997/2330 train_time:116712ms step_avg:58.44ms
step:1998/2330 train_time:116774ms step_avg:58.45ms
step:1999/2330 train_time:116830ms step_avg:58.44ms
step:2000/2330 train_time:116892ms step_avg:58.45ms
step:2000/2330 val_loss:3.7556 train_time:116974ms step_avg:58.49ms
step:2001/2330 train_time:116992ms step_avg:58.47ms
step:2002/2330 train_time:117013ms step_avg:58.45ms
step:2003/2330 train_time:117071ms step_avg:58.45ms
step:2004/2330 train_time:117140ms step_avg:58.45ms
step:2005/2330 train_time:117197ms step_avg:58.45ms
step:2006/2330 train_time:117259ms step_avg:58.45ms
step:2007/2330 train_time:117315ms step_avg:58.45ms
step:2008/2330 train_time:117376ms step_avg:58.45ms
step:2009/2330 train_time:117432ms step_avg:58.45ms
step:2010/2330 train_time:117493ms step_avg:58.45ms
step:2011/2330 train_time:117550ms step_avg:58.45ms
step:2012/2330 train_time:117611ms step_avg:58.45ms
step:2013/2330 train_time:117667ms step_avg:58.45ms
step:2014/2330 train_time:117728ms step_avg:58.45ms
step:2015/2330 train_time:117784ms step_avg:58.45ms
step:2016/2330 train_time:117844ms step_avg:58.45ms
step:2017/2330 train_time:117902ms step_avg:58.45ms
step:2018/2330 train_time:117963ms step_avg:58.46ms
step:2019/2330 train_time:118023ms step_avg:58.46ms
step:2020/2330 train_time:118085ms step_avg:58.46ms
step:2021/2330 train_time:118142ms step_avg:58.46ms
step:2022/2330 train_time:118205ms step_avg:58.46ms
step:2023/2330 train_time:118263ms step_avg:58.46ms
step:2024/2330 train_time:118324ms step_avg:58.46ms
step:2025/2330 train_time:118382ms step_avg:58.46ms
step:2026/2330 train_time:118443ms step_avg:58.46ms
step:2027/2330 train_time:118500ms step_avg:58.46ms
step:2028/2330 train_time:118559ms step_avg:58.46ms
step:2029/2330 train_time:118617ms step_avg:58.46ms
step:2030/2330 train_time:118676ms step_avg:58.46ms
step:2031/2330 train_time:118733ms step_avg:58.46ms
step:2032/2330 train_time:118792ms step_avg:58.46ms
step:2033/2330 train_time:118848ms step_avg:58.46ms
step:2034/2330 train_time:118910ms step_avg:58.46ms
step:2035/2330 train_time:118967ms step_avg:58.46ms
step:2036/2330 train_time:119028ms step_avg:58.46ms
step:2037/2330 train_time:119086ms step_avg:58.46ms
step:2038/2330 train_time:119148ms step_avg:58.46ms
step:2039/2330 train_time:119205ms step_avg:58.46ms
step:2040/2330 train_time:119268ms step_avg:58.46ms
step:2041/2330 train_time:119325ms step_avg:58.46ms
step:2042/2330 train_time:119387ms step_avg:58.47ms
step:2043/2330 train_time:119445ms step_avg:58.47ms
step:2044/2330 train_time:119506ms step_avg:58.47ms
step:2045/2330 train_time:119563ms step_avg:58.47ms
step:2046/2330 train_time:119624ms step_avg:58.47ms
step:2047/2330 train_time:119681ms step_avg:58.47ms
step:2048/2330 train_time:119741ms step_avg:58.47ms
step:2049/2330 train_time:119798ms step_avg:58.47ms
step:2050/2330 train_time:119858ms step_avg:58.47ms
step:2051/2330 train_time:119916ms step_avg:58.47ms
step:2052/2330 train_time:119977ms step_avg:58.47ms
step:2053/2330 train_time:120034ms step_avg:58.47ms
step:2054/2330 train_time:120096ms step_avg:58.47ms
step:2055/2330 train_time:120152ms step_avg:58.47ms
step:2056/2330 train_time:120217ms step_avg:58.47ms
step:2057/2330 train_time:120273ms step_avg:58.47ms
step:2058/2330 train_time:120336ms step_avg:58.47ms
step:2059/2330 train_time:120392ms step_avg:58.47ms
step:2060/2330 train_time:120455ms step_avg:58.47ms
step:2061/2330 train_time:120511ms step_avg:58.47ms
step:2062/2330 train_time:120573ms step_avg:58.47ms
step:2063/2330 train_time:120629ms step_avg:58.47ms
step:2064/2330 train_time:120691ms step_avg:58.47ms
step:2065/2330 train_time:120747ms step_avg:58.47ms
step:2066/2330 train_time:120809ms step_avg:58.47ms
step:2067/2330 train_time:120866ms step_avg:58.47ms
step:2068/2330 train_time:120928ms step_avg:58.48ms
step:2069/2330 train_time:120987ms step_avg:58.48ms
step:2070/2330 train_time:121047ms step_avg:58.48ms
step:2071/2330 train_time:121105ms step_avg:58.48ms
step:2072/2330 train_time:121167ms step_avg:58.48ms
step:2073/2330 train_time:121225ms step_avg:58.48ms
step:2074/2330 train_time:121286ms step_avg:58.48ms
step:2075/2330 train_time:121344ms step_avg:58.48ms
step:2076/2330 train_time:121404ms step_avg:58.48ms
step:2077/2330 train_time:121462ms step_avg:58.48ms
step:2078/2330 train_time:121522ms step_avg:58.48ms
step:2079/2330 train_time:121579ms step_avg:58.48ms
step:2080/2330 train_time:121639ms step_avg:58.48ms
step:2081/2330 train_time:121695ms step_avg:58.48ms
step:2082/2330 train_time:121757ms step_avg:58.48ms
step:2083/2330 train_time:121814ms step_avg:58.48ms
step:2084/2330 train_time:121875ms step_avg:58.48ms
step:2085/2330 train_time:121932ms step_avg:58.48ms
step:2086/2330 train_time:121993ms step_avg:58.48ms
step:2087/2330 train_time:122051ms step_avg:58.48ms
step:2088/2330 train_time:122113ms step_avg:58.48ms
step:2089/2330 train_time:122170ms step_avg:58.48ms
step:2090/2330 train_time:122234ms step_avg:58.48ms
step:2091/2330 train_time:122290ms step_avg:58.48ms
step:2092/2330 train_time:122353ms step_avg:58.49ms
step:2093/2330 train_time:122410ms step_avg:58.49ms
step:2094/2330 train_time:122472ms step_avg:58.49ms
step:2095/2330 train_time:122529ms step_avg:58.49ms
step:2096/2330 train_time:122590ms step_avg:58.49ms
step:2097/2330 train_time:122647ms step_avg:58.49ms
step:2098/2330 train_time:122708ms step_avg:58.49ms
step:2099/2330 train_time:122765ms step_avg:58.49ms
step:2100/2330 train_time:122826ms step_avg:58.49ms
step:2101/2330 train_time:122884ms step_avg:58.49ms
step:2102/2330 train_time:122945ms step_avg:58.49ms
step:2103/2330 train_time:123003ms step_avg:58.49ms
step:2104/2330 train_time:123064ms step_avg:58.49ms
step:2105/2330 train_time:123123ms step_avg:58.49ms
step:2106/2330 train_time:123183ms step_avg:58.49ms
step:2107/2330 train_time:123241ms step_avg:58.49ms
step:2108/2330 train_time:123301ms step_avg:58.49ms
step:2109/2330 train_time:123359ms step_avg:58.49ms
step:2110/2330 train_time:123419ms step_avg:58.49ms
step:2111/2330 train_time:123476ms step_avg:58.49ms
step:2112/2330 train_time:123538ms step_avg:58.49ms
step:2113/2330 train_time:123594ms step_avg:58.49ms
step:2114/2330 train_time:123656ms step_avg:58.49ms
step:2115/2330 train_time:123713ms step_avg:58.49ms
step:2116/2330 train_time:123772ms step_avg:58.49ms
step:2117/2330 train_time:123829ms step_avg:58.49ms
step:2118/2330 train_time:123890ms step_avg:58.49ms
step:2119/2330 train_time:123947ms step_avg:58.49ms
step:2120/2330 train_time:124008ms step_avg:58.49ms
step:2121/2330 train_time:124065ms step_avg:58.49ms
step:2122/2330 train_time:124127ms step_avg:58.50ms
step:2123/2330 train_time:124184ms step_avg:58.49ms
step:2124/2330 train_time:124246ms step_avg:58.50ms
step:2125/2330 train_time:124303ms step_avg:58.50ms
step:2126/2330 train_time:124364ms step_avg:58.50ms
step:2127/2330 train_time:124423ms step_avg:58.50ms
step:2128/2330 train_time:124483ms step_avg:58.50ms
step:2129/2330 train_time:124541ms step_avg:58.50ms
step:2130/2330 train_time:124601ms step_avg:58.50ms
step:2131/2330 train_time:124659ms step_avg:58.50ms
step:2132/2330 train_time:124718ms step_avg:58.50ms
step:2133/2330 train_time:124776ms step_avg:58.50ms
step:2134/2330 train_time:124836ms step_avg:58.50ms
step:2135/2330 train_time:124893ms step_avg:58.50ms
step:2136/2330 train_time:124955ms step_avg:58.50ms
step:2137/2330 train_time:125012ms step_avg:58.50ms
step:2138/2330 train_time:125073ms step_avg:58.50ms
step:2139/2330 train_time:125130ms step_avg:58.50ms
step:2140/2330 train_time:125192ms step_avg:58.50ms
step:2141/2330 train_time:125249ms step_avg:58.50ms
step:2142/2330 train_time:125312ms step_avg:58.50ms
step:2143/2330 train_time:125369ms step_avg:58.50ms
step:2144/2330 train_time:125430ms step_avg:58.50ms
step:2145/2330 train_time:125487ms step_avg:58.50ms
step:2146/2330 train_time:125550ms step_avg:58.50ms
step:2147/2330 train_time:125606ms step_avg:58.50ms
step:2148/2330 train_time:125668ms step_avg:58.50ms
step:2149/2330 train_time:125725ms step_avg:58.50ms
step:2150/2330 train_time:125786ms step_avg:58.51ms
step:2151/2330 train_time:125844ms step_avg:58.50ms
step:2152/2330 train_time:125904ms step_avg:58.51ms
step:2153/2330 train_time:125963ms step_avg:58.51ms
step:2154/2330 train_time:126023ms step_avg:58.51ms
step:2155/2330 train_time:126081ms step_avg:58.51ms
step:2156/2330 train_time:126141ms step_avg:58.51ms
step:2157/2330 train_time:126198ms step_avg:58.51ms
step:2158/2330 train_time:126259ms step_avg:58.51ms
step:2159/2330 train_time:126316ms step_avg:58.51ms
step:2160/2330 train_time:126377ms step_avg:58.51ms
step:2161/2330 train_time:126434ms step_avg:58.51ms
step:2162/2330 train_time:126495ms step_avg:58.51ms
step:2163/2330 train_time:126552ms step_avg:58.51ms
step:2164/2330 train_time:126613ms step_avg:58.51ms
step:2165/2330 train_time:126670ms step_avg:58.51ms
step:2166/2330 train_time:126732ms step_avg:58.51ms
step:2167/2330 train_time:126788ms step_avg:58.51ms
step:2168/2330 train_time:126850ms step_avg:58.51ms
step:2169/2330 train_time:126907ms step_avg:58.51ms
step:2170/2330 train_time:126970ms step_avg:58.51ms
step:2171/2330 train_time:127027ms step_avg:58.51ms
step:2172/2330 train_time:127088ms step_avg:58.51ms
step:2173/2330 train_time:127146ms step_avg:58.51ms
step:2174/2330 train_time:127207ms step_avg:58.51ms
step:2175/2330 train_time:127265ms step_avg:58.51ms
step:2176/2330 train_time:127325ms step_avg:58.51ms
step:2177/2330 train_time:127383ms step_avg:58.51ms
step:2178/2330 train_time:127444ms step_avg:58.51ms
step:2179/2330 train_time:127500ms step_avg:58.51ms
step:2180/2330 train_time:127562ms step_avg:58.51ms
step:2181/2330 train_time:127619ms step_avg:58.51ms
step:2182/2330 train_time:127680ms step_avg:58.52ms
step:2183/2330 train_time:127737ms step_avg:58.51ms
step:2184/2330 train_time:127797ms step_avg:58.52ms
step:2185/2330 train_time:127854ms step_avg:58.51ms
step:2186/2330 train_time:127916ms step_avg:58.52ms
step:2187/2330 train_time:127973ms step_avg:58.52ms
step:2188/2330 train_time:128036ms step_avg:58.52ms
step:2189/2330 train_time:128092ms step_avg:58.52ms
step:2190/2330 train_time:128154ms step_avg:58.52ms
step:2191/2330 train_time:128211ms step_avg:58.52ms
step:2192/2330 train_time:128273ms step_avg:58.52ms
step:2193/2330 train_time:128329ms step_avg:58.52ms
step:2194/2330 train_time:128391ms step_avg:58.52ms
step:2195/2330 train_time:128447ms step_avg:58.52ms
step:2196/2330 train_time:128509ms step_avg:58.52ms
step:2197/2330 train_time:128566ms step_avg:58.52ms
step:2198/2330 train_time:128628ms step_avg:58.52ms
step:2199/2330 train_time:128686ms step_avg:58.52ms
step:2200/2330 train_time:128747ms step_avg:58.52ms
step:2201/2330 train_time:128804ms step_avg:58.52ms
step:2202/2330 train_time:128866ms step_avg:58.52ms
step:2203/2330 train_time:128924ms step_avg:58.52ms
step:2204/2330 train_time:128984ms step_avg:58.52ms
step:2205/2330 train_time:129042ms step_avg:58.52ms
step:2206/2330 train_time:129102ms step_avg:58.52ms
step:2207/2330 train_time:129161ms step_avg:58.52ms
step:2208/2330 train_time:129221ms step_avg:58.52ms
step:2209/2330 train_time:129278ms step_avg:58.52ms
step:2210/2330 train_time:129337ms step_avg:58.52ms
step:2211/2330 train_time:129394ms step_avg:58.52ms
step:2212/2330 train_time:129457ms step_avg:58.52ms
step:2213/2330 train_time:129513ms step_avg:58.52ms
step:2214/2330 train_time:129576ms step_avg:58.53ms
step:2215/2330 train_time:129632ms step_avg:58.52ms
step:2216/2330 train_time:129695ms step_avg:58.53ms
step:2217/2330 train_time:129752ms step_avg:58.53ms
step:2218/2330 train_time:129814ms step_avg:58.53ms
step:2219/2330 train_time:129870ms step_avg:58.53ms
step:2220/2330 train_time:129933ms step_avg:58.53ms
step:2221/2330 train_time:129989ms step_avg:58.53ms
step:2222/2330 train_time:130052ms step_avg:58.53ms
step:2223/2330 train_time:130109ms step_avg:58.53ms
step:2224/2330 train_time:130170ms step_avg:58.53ms
step:2225/2330 train_time:130227ms step_avg:58.53ms
step:2226/2330 train_time:130288ms step_avg:58.53ms
step:2227/2330 train_time:130346ms step_avg:58.53ms
step:2228/2330 train_time:130407ms step_avg:58.53ms
step:2229/2330 train_time:130465ms step_avg:58.53ms
step:2230/2330 train_time:130525ms step_avg:58.53ms
step:2231/2330 train_time:130583ms step_avg:58.53ms
step:2232/2330 train_time:130644ms step_avg:58.53ms
step:2233/2330 train_time:130702ms step_avg:58.53ms
step:2234/2330 train_time:130762ms step_avg:58.53ms
step:2235/2330 train_time:130820ms step_avg:58.53ms
step:2236/2330 train_time:130880ms step_avg:58.53ms
step:2237/2330 train_time:130937ms step_avg:58.53ms
step:2238/2330 train_time:130997ms step_avg:58.53ms
step:2239/2330 train_time:131054ms step_avg:58.53ms
step:2240/2330 train_time:131115ms step_avg:58.53ms
step:2241/2330 train_time:131172ms step_avg:58.53ms
step:2242/2330 train_time:131233ms step_avg:58.53ms
step:2243/2330 train_time:131290ms step_avg:58.53ms
step:2244/2330 train_time:131353ms step_avg:58.54ms
step:2245/2330 train_time:131410ms step_avg:58.53ms
step:2246/2330 train_time:131473ms step_avg:58.54ms
step:2247/2330 train_time:131529ms step_avg:58.54ms
step:2248/2330 train_time:131592ms step_avg:58.54ms
step:2249/2330 train_time:131648ms step_avg:58.54ms
step:2250/2330 train_time:131709ms step_avg:58.54ms
step:2250/2330 val_loss:3.7080 train_time:131792ms step_avg:58.57ms
step:2251/2330 train_time:131810ms step_avg:58.56ms
step:2252/2330 train_time:131832ms step_avg:58.54ms
step:2253/2330 train_time:131891ms step_avg:58.54ms
step:2254/2330 train_time:131956ms step_avg:58.54ms
step:2255/2330 train_time:132012ms step_avg:58.54ms
step:2256/2330 train_time:132076ms step_avg:58.54ms
step:2257/2330 train_time:132132ms step_avg:58.54ms
step:2258/2330 train_time:132194ms step_avg:58.54ms
step:2259/2330 train_time:132250ms step_avg:58.54ms
step:2260/2330 train_time:132310ms step_avg:58.54ms
step:2261/2330 train_time:132366ms step_avg:58.54ms
step:2262/2330 train_time:132426ms step_avg:58.54ms
step:2263/2330 train_time:132482ms step_avg:58.54ms
step:2264/2330 train_time:132543ms step_avg:58.54ms
step:2265/2330 train_time:132600ms step_avg:58.54ms
step:2266/2330 train_time:132659ms step_avg:58.54ms
step:2267/2330 train_time:132716ms step_avg:58.54ms
step:2268/2330 train_time:132778ms step_avg:58.54ms
step:2269/2330 train_time:132837ms step_avg:58.54ms
step:2270/2330 train_time:132899ms step_avg:58.55ms
step:2271/2330 train_time:132957ms step_avg:58.55ms
step:2272/2330 train_time:133019ms step_avg:58.55ms
step:2273/2330 train_time:133076ms step_avg:58.55ms
step:2274/2330 train_time:133138ms step_avg:58.55ms
step:2275/2330 train_time:133195ms step_avg:58.55ms
step:2276/2330 train_time:133255ms step_avg:58.55ms
step:2277/2330 train_time:133312ms step_avg:58.55ms
step:2278/2330 train_time:133373ms step_avg:58.55ms
step:2279/2330 train_time:133429ms step_avg:58.55ms
step:2280/2330 train_time:133490ms step_avg:58.55ms
step:2281/2330 train_time:133546ms step_avg:58.55ms
step:2282/2330 train_time:133608ms step_avg:58.55ms
step:2283/2330 train_time:133664ms step_avg:58.55ms
step:2284/2330 train_time:133725ms step_avg:58.55ms
step:2285/2330 train_time:133784ms step_avg:58.55ms
step:2286/2330 train_time:133845ms step_avg:58.55ms
step:2287/2330 train_time:133904ms step_avg:58.55ms
step:2288/2330 train_time:133966ms step_avg:58.55ms
step:2289/2330 train_time:134024ms step_avg:58.55ms
step:2290/2330 train_time:134087ms step_avg:58.55ms
step:2291/2330 train_time:134145ms step_avg:58.55ms
step:2292/2330 train_time:134206ms step_avg:58.55ms
step:2293/2330 train_time:134263ms step_avg:58.55ms
step:2294/2330 train_time:134323ms step_avg:58.55ms
step:2295/2330 train_time:134381ms step_avg:58.55ms
step:2296/2330 train_time:134441ms step_avg:58.55ms
step:2297/2330 train_time:134497ms step_avg:58.55ms
step:2298/2330 train_time:134558ms step_avg:58.55ms
step:2299/2330 train_time:134614ms step_avg:58.55ms
step:2300/2330 train_time:134674ms step_avg:58.55ms
step:2301/2330 train_time:134731ms step_avg:58.55ms
step:2302/2330 train_time:134793ms step_avg:58.55ms
step:2303/2330 train_time:134850ms step_avg:58.55ms
step:2304/2330 train_time:134912ms step_avg:58.56ms
step:2305/2330 train_time:134969ms step_avg:58.55ms
step:2306/2330 train_time:135033ms step_avg:58.56ms
step:2307/2330 train_time:135090ms step_avg:58.56ms
step:2308/2330 train_time:135152ms step_avg:58.56ms
step:2309/2330 train_time:135209ms step_avg:58.56ms
step:2310/2330 train_time:135271ms step_avg:58.56ms
step:2311/2330 train_time:135328ms step_avg:58.56ms
step:2312/2330 train_time:135389ms step_avg:58.56ms
step:2313/2330 train_time:135445ms step_avg:58.56ms
step:2314/2330 train_time:135506ms step_avg:58.56ms
step:2315/2330 train_time:135563ms step_avg:58.56ms
step:2316/2330 train_time:135624ms step_avg:58.56ms
step:2317/2330 train_time:135681ms step_avg:58.56ms
step:2318/2330 train_time:135741ms step_avg:58.56ms
step:2319/2330 train_time:135799ms step_avg:58.56ms
step:2320/2330 train_time:135859ms step_avg:58.56ms
step:2321/2330 train_time:135918ms step_avg:58.56ms
step:2322/2330 train_time:135978ms step_avg:58.56ms
step:2323/2330 train_time:136036ms step_avg:58.56ms
step:2324/2330 train_time:136096ms step_avg:58.56ms
step:2325/2330 train_time:136153ms step_avg:58.56ms
step:2326/2330 train_time:136214ms step_avg:58.56ms
step:2327/2330 train_time:136270ms step_avg:58.56ms
step:2328/2330 train_time:136332ms step_avg:58.56ms
step:2329/2330 train_time:136388ms step_avg:58.56ms
step:2330/2330 train_time:136449ms step_avg:58.56ms
step:2330/2330 val_loss:3.6927 train_time:136531ms step_avg:58.60ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
