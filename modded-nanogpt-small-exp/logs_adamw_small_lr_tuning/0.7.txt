import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--adamw_lr', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_lr_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_lr_tuning/{args2.adamw_lr}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=float(args2.adamw_lr),  betas=(0.85, 0.85), eps=1e-10, weight_decay=0.0, fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_lr_tuning/{args2.adamw_lr}", exist_ok=True)
            torch.save(log, f"logs_adamw_small_lr_tuning/{args2.adamw_lr}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 03:18:45 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   35C    P0             117W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   35C    P0             117W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             119W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             115W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   34C    P0             118W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             114W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:92ms step_avg:91.93ms
step:2/2330 train_time:192ms step_avg:96.25ms
step:3/2330 train_time:210ms step_avg:70.09ms
step:4/2330 train_time:229ms step_avg:57.32ms
step:5/2330 train_time:283ms step_avg:56.60ms
step:6/2330 train_time:340ms step_avg:56.71ms
step:7/2330 train_time:394ms step_avg:56.29ms
step:8/2330 train_time:451ms step_avg:56.38ms
step:9/2330 train_time:505ms step_avg:56.11ms
step:10/2330 train_time:562ms step_avg:56.19ms
step:11/2330 train_time:616ms step_avg:55.99ms
step:12/2330 train_time:673ms step_avg:56.06ms
step:13/2330 train_time:727ms step_avg:55.89ms
step:14/2330 train_time:783ms step_avg:55.94ms
step:15/2330 train_time:837ms step_avg:55.80ms
step:16/2330 train_time:894ms step_avg:55.86ms
step:17/2330 train_time:947ms step_avg:55.73ms
step:18/2330 train_time:1004ms step_avg:55.79ms
step:19/2330 train_time:1058ms step_avg:55.70ms
step:20/2330 train_time:1116ms step_avg:55.80ms
step:21/2330 train_time:1171ms step_avg:55.74ms
step:22/2330 train_time:1229ms step_avg:55.85ms
step:23/2330 train_time:1283ms step_avg:55.80ms
step:24/2330 train_time:1341ms step_avg:55.87ms
step:25/2330 train_time:1395ms step_avg:55.81ms
step:26/2330 train_time:1453ms step_avg:55.90ms
step:27/2330 train_time:1507ms step_avg:55.83ms
step:28/2330 train_time:1565ms step_avg:55.91ms
step:29/2330 train_time:1620ms step_avg:55.85ms
step:30/2330 train_time:1677ms step_avg:55.89ms
step:31/2330 train_time:1731ms step_avg:55.84ms
step:32/2330 train_time:1788ms step_avg:55.88ms
step:33/2330 train_time:1842ms step_avg:55.82ms
step:34/2330 train_time:1899ms step_avg:55.86ms
step:35/2330 train_time:1954ms step_avg:55.82ms
step:36/2330 train_time:2011ms step_avg:55.86ms
step:37/2330 train_time:2065ms step_avg:55.82ms
step:38/2330 train_time:2123ms step_avg:55.87ms
step:39/2330 train_time:2178ms step_avg:55.84ms
step:40/2330 train_time:2235ms step_avg:55.88ms
step:41/2330 train_time:2290ms step_avg:55.85ms
step:42/2330 train_time:2349ms step_avg:55.92ms
step:43/2330 train_time:2403ms step_avg:55.89ms
step:44/2330 train_time:2461ms step_avg:55.93ms
step:45/2330 train_time:2515ms step_avg:55.89ms
step:46/2330 train_time:2573ms step_avg:55.94ms
step:47/2330 train_time:2628ms step_avg:55.91ms
step:48/2330 train_time:2686ms step_avg:55.96ms
step:49/2330 train_time:2741ms step_avg:55.93ms
step:50/2330 train_time:2799ms step_avg:55.97ms
step:51/2330 train_time:2853ms step_avg:55.94ms
step:52/2330 train_time:2911ms step_avg:55.98ms
step:53/2330 train_time:2965ms step_avg:55.94ms
step:54/2330 train_time:3023ms step_avg:55.99ms
step:55/2330 train_time:3078ms step_avg:55.96ms
step:56/2330 train_time:3136ms step_avg:55.99ms
step:57/2330 train_time:3190ms step_avg:55.96ms
step:58/2330 train_time:3248ms step_avg:56.00ms
step:59/2330 train_time:3303ms step_avg:55.98ms
step:60/2330 train_time:3361ms step_avg:56.01ms
step:61/2330 train_time:3416ms step_avg:56.00ms
step:62/2330 train_time:3473ms step_avg:56.02ms
step:63/2330 train_time:3528ms step_avg:56.01ms
step:64/2330 train_time:3586ms step_avg:56.03ms
step:65/2330 train_time:3640ms step_avg:56.01ms
step:66/2330 train_time:3699ms step_avg:56.04ms
step:67/2330 train_time:3753ms step_avg:56.01ms
step:68/2330 train_time:3811ms step_avg:56.05ms
step:69/2330 train_time:3866ms step_avg:56.03ms
step:70/2330 train_time:3924ms step_avg:56.06ms
step:71/2330 train_time:3979ms step_avg:56.04ms
step:72/2330 train_time:4037ms step_avg:56.06ms
step:73/2330 train_time:4091ms step_avg:56.05ms
step:74/2330 train_time:4149ms step_avg:56.07ms
step:75/2330 train_time:4204ms step_avg:56.05ms
step:76/2330 train_time:4261ms step_avg:56.07ms
step:77/2330 train_time:4316ms step_avg:56.05ms
step:78/2330 train_time:4374ms step_avg:56.08ms
step:79/2330 train_time:4430ms step_avg:56.07ms
step:80/2330 train_time:4487ms step_avg:56.09ms
step:81/2330 train_time:4541ms step_avg:56.07ms
step:82/2330 train_time:4599ms step_avg:56.09ms
step:83/2330 train_time:4654ms step_avg:56.07ms
step:84/2330 train_time:4712ms step_avg:56.09ms
step:85/2330 train_time:4767ms step_avg:56.08ms
step:86/2330 train_time:4825ms step_avg:56.10ms
step:87/2330 train_time:4880ms step_avg:56.09ms
step:88/2330 train_time:4937ms step_avg:56.11ms
step:89/2330 train_time:4992ms step_avg:56.09ms
step:90/2330 train_time:5050ms step_avg:56.11ms
step:91/2330 train_time:5106ms step_avg:56.11ms
step:92/2330 train_time:5163ms step_avg:56.12ms
step:93/2330 train_time:5218ms step_avg:56.11ms
step:94/2330 train_time:5276ms step_avg:56.13ms
step:95/2330 train_time:5331ms step_avg:56.11ms
step:96/2330 train_time:5391ms step_avg:56.15ms
step:97/2330 train_time:5445ms step_avg:56.14ms
step:98/2330 train_time:5504ms step_avg:56.16ms
step:99/2330 train_time:5559ms step_avg:56.15ms
step:100/2330 train_time:5616ms step_avg:56.16ms
step:101/2330 train_time:5671ms step_avg:56.15ms
step:102/2330 train_time:5729ms step_avg:56.17ms
step:103/2330 train_time:5784ms step_avg:56.16ms
step:104/2330 train_time:5842ms step_avg:56.18ms
step:105/2330 train_time:5897ms step_avg:56.17ms
step:106/2330 train_time:5955ms step_avg:56.18ms
step:107/2330 train_time:6009ms step_avg:56.16ms
step:108/2330 train_time:6068ms step_avg:56.19ms
step:109/2330 train_time:6122ms step_avg:56.17ms
step:110/2330 train_time:6182ms step_avg:56.20ms
step:111/2330 train_time:6236ms step_avg:56.18ms
step:112/2330 train_time:6295ms step_avg:56.20ms
step:113/2330 train_time:6350ms step_avg:56.19ms
step:114/2330 train_time:6408ms step_avg:56.21ms
step:115/2330 train_time:6463ms step_avg:56.20ms
step:116/2330 train_time:6521ms step_avg:56.22ms
step:117/2330 train_time:6577ms step_avg:56.21ms
step:118/2330 train_time:6634ms step_avg:56.22ms
step:119/2330 train_time:6689ms step_avg:56.21ms
step:120/2330 train_time:6748ms step_avg:56.23ms
step:121/2330 train_time:6802ms step_avg:56.22ms
step:122/2330 train_time:6860ms step_avg:56.23ms
step:123/2330 train_time:6915ms step_avg:56.22ms
step:124/2330 train_time:6974ms step_avg:56.24ms
step:125/2330 train_time:7030ms step_avg:56.24ms
step:126/2330 train_time:7087ms step_avg:56.25ms
step:127/2330 train_time:7142ms step_avg:56.24ms
step:128/2330 train_time:7201ms step_avg:56.26ms
step:129/2330 train_time:7256ms step_avg:56.24ms
step:130/2330 train_time:7314ms step_avg:56.26ms
step:131/2330 train_time:7369ms step_avg:56.25ms
step:132/2330 train_time:7427ms step_avg:56.27ms
step:133/2330 train_time:7482ms step_avg:56.25ms
step:134/2330 train_time:7541ms step_avg:56.27ms
step:135/2330 train_time:7596ms step_avg:56.27ms
step:136/2330 train_time:7654ms step_avg:56.28ms
step:137/2330 train_time:7709ms step_avg:56.27ms
step:138/2330 train_time:7768ms step_avg:56.29ms
step:139/2330 train_time:7822ms step_avg:56.27ms
step:140/2330 train_time:7882ms step_avg:56.30ms
step:141/2330 train_time:7938ms step_avg:56.30ms
step:142/2330 train_time:7996ms step_avg:56.31ms
step:143/2330 train_time:8051ms step_avg:56.30ms
step:144/2330 train_time:8109ms step_avg:56.32ms
step:145/2330 train_time:8164ms step_avg:56.31ms
step:146/2330 train_time:8224ms step_avg:56.33ms
step:147/2330 train_time:8279ms step_avg:56.32ms
step:148/2330 train_time:8337ms step_avg:56.33ms
step:149/2330 train_time:8392ms step_avg:56.32ms
step:150/2330 train_time:8451ms step_avg:56.34ms
step:151/2330 train_time:8506ms step_avg:56.33ms
step:152/2330 train_time:8565ms step_avg:56.35ms
step:153/2330 train_time:8620ms step_avg:56.34ms
step:154/2330 train_time:8678ms step_avg:56.35ms
step:155/2330 train_time:8733ms step_avg:56.35ms
step:156/2330 train_time:8792ms step_avg:56.36ms
step:157/2330 train_time:8847ms step_avg:56.35ms
step:158/2330 train_time:8907ms step_avg:56.37ms
step:159/2330 train_time:8962ms step_avg:56.37ms
step:160/2330 train_time:9020ms step_avg:56.38ms
step:161/2330 train_time:9076ms step_avg:56.37ms
step:162/2330 train_time:9134ms step_avg:56.38ms
step:163/2330 train_time:9189ms step_avg:56.37ms
step:164/2330 train_time:9247ms step_avg:56.38ms
step:165/2330 train_time:9302ms step_avg:56.38ms
step:166/2330 train_time:9360ms step_avg:56.39ms
step:167/2330 train_time:9416ms step_avg:56.38ms
step:168/2330 train_time:9475ms step_avg:56.40ms
step:169/2330 train_time:9530ms step_avg:56.39ms
step:170/2330 train_time:9589ms step_avg:56.41ms
step:171/2330 train_time:9644ms step_avg:56.40ms
step:172/2330 train_time:9702ms step_avg:56.41ms
step:173/2330 train_time:9757ms step_avg:56.40ms
step:174/2330 train_time:9817ms step_avg:56.42ms
step:175/2330 train_time:9872ms step_avg:56.41ms
step:176/2330 train_time:9930ms step_avg:56.42ms
step:177/2330 train_time:9986ms step_avg:56.42ms
step:178/2330 train_time:10045ms step_avg:56.43ms
step:179/2330 train_time:10100ms step_avg:56.42ms
step:180/2330 train_time:10158ms step_avg:56.43ms
step:181/2330 train_time:10213ms step_avg:56.43ms
step:182/2330 train_time:10272ms step_avg:56.44ms
step:183/2330 train_time:10327ms step_avg:56.43ms
step:184/2330 train_time:10385ms step_avg:56.44ms
step:185/2330 train_time:10441ms step_avg:56.44ms
step:186/2330 train_time:10499ms step_avg:56.45ms
step:187/2330 train_time:10554ms step_avg:56.44ms
step:188/2330 train_time:10613ms step_avg:56.45ms
step:189/2330 train_time:10668ms step_avg:56.44ms
step:190/2330 train_time:10727ms step_avg:56.46ms
step:191/2330 train_time:10782ms step_avg:56.45ms
step:192/2330 train_time:10841ms step_avg:56.46ms
step:193/2330 train_time:10896ms step_avg:56.45ms
step:194/2330 train_time:10955ms step_avg:56.47ms
step:195/2330 train_time:11011ms step_avg:56.46ms
step:196/2330 train_time:11070ms step_avg:56.48ms
step:197/2330 train_time:11125ms step_avg:56.47ms
step:198/2330 train_time:11184ms step_avg:56.49ms
step:199/2330 train_time:11240ms step_avg:56.48ms
step:200/2330 train_time:11297ms step_avg:56.49ms
step:201/2330 train_time:11352ms step_avg:56.48ms
step:202/2330 train_time:11411ms step_avg:56.49ms
step:203/2330 train_time:11467ms step_avg:56.49ms
step:204/2330 train_time:11526ms step_avg:56.50ms
step:205/2330 train_time:11581ms step_avg:56.49ms
step:206/2330 train_time:11639ms step_avg:56.50ms
step:207/2330 train_time:11694ms step_avg:56.49ms
step:208/2330 train_time:11753ms step_avg:56.51ms
step:209/2330 train_time:11808ms step_avg:56.50ms
step:210/2330 train_time:11868ms step_avg:56.51ms
step:211/2330 train_time:11923ms step_avg:56.51ms
step:212/2330 train_time:11983ms step_avg:56.52ms
step:213/2330 train_time:12039ms step_avg:56.52ms
step:214/2330 train_time:12097ms step_avg:56.53ms
step:215/2330 train_time:12152ms step_avg:56.52ms
step:216/2330 train_time:12211ms step_avg:56.53ms
step:217/2330 train_time:12267ms step_avg:56.53ms
step:218/2330 train_time:12326ms step_avg:56.54ms
step:219/2330 train_time:12382ms step_avg:56.54ms
step:220/2330 train_time:12440ms step_avg:56.55ms
step:221/2330 train_time:12495ms step_avg:56.54ms
step:222/2330 train_time:12554ms step_avg:56.55ms
step:223/2330 train_time:12608ms step_avg:56.54ms
step:224/2330 train_time:12669ms step_avg:56.56ms
step:225/2330 train_time:12724ms step_avg:56.55ms
step:226/2330 train_time:12783ms step_avg:56.56ms
step:227/2330 train_time:12838ms step_avg:56.55ms
step:228/2330 train_time:12898ms step_avg:56.57ms
step:229/2330 train_time:12952ms step_avg:56.56ms
step:230/2330 train_time:13012ms step_avg:56.58ms
step:231/2330 train_time:13067ms step_avg:56.57ms
step:232/2330 train_time:13128ms step_avg:56.58ms
step:233/2330 train_time:13183ms step_avg:56.58ms
step:234/2330 train_time:13242ms step_avg:56.59ms
step:235/2330 train_time:13297ms step_avg:56.58ms
step:236/2330 train_time:13357ms step_avg:56.60ms
step:237/2330 train_time:13413ms step_avg:56.59ms
step:238/2330 train_time:13472ms step_avg:56.61ms
step:239/2330 train_time:13528ms step_avg:56.60ms
step:240/2330 train_time:13587ms step_avg:56.61ms
step:241/2330 train_time:13643ms step_avg:56.61ms
step:242/2330 train_time:13702ms step_avg:56.62ms
step:243/2330 train_time:13758ms step_avg:56.62ms
step:244/2330 train_time:13817ms step_avg:56.63ms
step:245/2330 train_time:13873ms step_avg:56.62ms
step:246/2330 train_time:13932ms step_avg:56.63ms
step:247/2330 train_time:13987ms step_avg:56.63ms
step:248/2330 train_time:14047ms step_avg:56.64ms
step:249/2330 train_time:14102ms step_avg:56.64ms
step:250/2330 train_time:14162ms step_avg:56.65ms
step:250/2330 val_loss:6.6705 train_time:14241ms step_avg:56.96ms
step:251/2330 train_time:14259ms step_avg:56.81ms
step:252/2330 train_time:14278ms step_avg:56.66ms
step:253/2330 train_time:14333ms step_avg:56.65ms
step:254/2330 train_time:14397ms step_avg:56.68ms
step:255/2330 train_time:14452ms step_avg:56.67ms
step:256/2330 train_time:14517ms step_avg:56.71ms
step:257/2330 train_time:14572ms step_avg:56.70ms
step:258/2330 train_time:14632ms step_avg:56.71ms
step:259/2330 train_time:14687ms step_avg:56.71ms
step:260/2330 train_time:14747ms step_avg:56.72ms
step:261/2330 train_time:14802ms step_avg:56.71ms
step:262/2330 train_time:14860ms step_avg:56.72ms
step:263/2330 train_time:14915ms step_avg:56.71ms
step:264/2330 train_time:14974ms step_avg:56.72ms
step:265/2330 train_time:15029ms step_avg:56.71ms
step:266/2330 train_time:15087ms step_avg:56.72ms
step:267/2330 train_time:15143ms step_avg:56.71ms
step:268/2330 train_time:15202ms step_avg:56.72ms
step:269/2330 train_time:15258ms step_avg:56.72ms
step:270/2330 train_time:15318ms step_avg:56.73ms
step:271/2330 train_time:15374ms step_avg:56.73ms
step:272/2330 train_time:15436ms step_avg:56.75ms
step:273/2330 train_time:15492ms step_avg:56.75ms
step:274/2330 train_time:15553ms step_avg:56.76ms
step:275/2330 train_time:15608ms step_avg:56.76ms
step:276/2330 train_time:15668ms step_avg:56.77ms
step:277/2330 train_time:15723ms step_avg:56.76ms
step:278/2330 train_time:15783ms step_avg:56.77ms
step:279/2330 train_time:15838ms step_avg:56.77ms
step:280/2330 train_time:15897ms step_avg:56.77ms
step:281/2330 train_time:15952ms step_avg:56.77ms
step:282/2330 train_time:16012ms step_avg:56.78ms
step:283/2330 train_time:16067ms step_avg:56.77ms
step:284/2330 train_time:16126ms step_avg:56.78ms
step:285/2330 train_time:16181ms step_avg:56.78ms
step:286/2330 train_time:16242ms step_avg:56.79ms
step:287/2330 train_time:16297ms step_avg:56.79ms
step:288/2330 train_time:16358ms step_avg:56.80ms
step:289/2330 train_time:16414ms step_avg:56.80ms
step:290/2330 train_time:16476ms step_avg:56.81ms
step:291/2330 train_time:16531ms step_avg:56.81ms
step:292/2330 train_time:16593ms step_avg:56.82ms
step:293/2330 train_time:16648ms step_avg:56.82ms
step:294/2330 train_time:16708ms step_avg:56.83ms
step:295/2330 train_time:16763ms step_avg:56.82ms
step:296/2330 train_time:16822ms step_avg:56.83ms
step:297/2330 train_time:16877ms step_avg:56.83ms
step:298/2330 train_time:16937ms step_avg:56.83ms
step:299/2330 train_time:16993ms step_avg:56.83ms
step:300/2330 train_time:17052ms step_avg:56.84ms
step:301/2330 train_time:17107ms step_avg:56.83ms
step:302/2330 train_time:17167ms step_avg:56.84ms
step:303/2330 train_time:17222ms step_avg:56.84ms
step:304/2330 train_time:17282ms step_avg:56.85ms
step:305/2330 train_time:17339ms step_avg:56.85ms
step:306/2330 train_time:17399ms step_avg:56.86ms
step:307/2330 train_time:17456ms step_avg:56.86ms
step:308/2330 train_time:17514ms step_avg:56.87ms
step:309/2330 train_time:17570ms step_avg:56.86ms
step:310/2330 train_time:17631ms step_avg:56.87ms
step:311/2330 train_time:17686ms step_avg:56.87ms
step:312/2330 train_time:17746ms step_avg:56.88ms
step:313/2330 train_time:17802ms step_avg:56.87ms
step:314/2330 train_time:17861ms step_avg:56.88ms
step:315/2330 train_time:17917ms step_avg:56.88ms
step:316/2330 train_time:17976ms step_avg:56.89ms
step:317/2330 train_time:18032ms step_avg:56.88ms
step:318/2330 train_time:18092ms step_avg:56.89ms
step:319/2330 train_time:18147ms step_avg:56.89ms
step:320/2330 train_time:18208ms step_avg:56.90ms
step:321/2330 train_time:18263ms step_avg:56.89ms
step:322/2330 train_time:18323ms step_avg:56.90ms
step:323/2330 train_time:18379ms step_avg:56.90ms
step:324/2330 train_time:18438ms step_avg:56.91ms
step:325/2330 train_time:18496ms step_avg:56.91ms
step:326/2330 train_time:18555ms step_avg:56.92ms
step:327/2330 train_time:18611ms step_avg:56.91ms
step:328/2330 train_time:18671ms step_avg:56.92ms
step:329/2330 train_time:18726ms step_avg:56.92ms
step:330/2330 train_time:18786ms step_avg:56.93ms
step:331/2330 train_time:18842ms step_avg:56.92ms
step:332/2330 train_time:18901ms step_avg:56.93ms
step:333/2330 train_time:18957ms step_avg:56.93ms
step:334/2330 train_time:19016ms step_avg:56.93ms
step:335/2330 train_time:19072ms step_avg:56.93ms
step:336/2330 train_time:19133ms step_avg:56.94ms
step:337/2330 train_time:19188ms step_avg:56.94ms
step:338/2330 train_time:19249ms step_avg:56.95ms
step:339/2330 train_time:19304ms step_avg:56.94ms
step:340/2330 train_time:19364ms step_avg:56.95ms
step:341/2330 train_time:19420ms step_avg:56.95ms
step:342/2330 train_time:19480ms step_avg:56.96ms
step:343/2330 train_time:19536ms step_avg:56.96ms
step:344/2330 train_time:19595ms step_avg:56.96ms
step:345/2330 train_time:19651ms step_avg:56.96ms
step:346/2330 train_time:19711ms step_avg:56.97ms
step:347/2330 train_time:19766ms step_avg:56.96ms
step:348/2330 train_time:19827ms step_avg:56.97ms
step:349/2330 train_time:19883ms step_avg:56.97ms
step:350/2330 train_time:19942ms step_avg:56.98ms
step:351/2330 train_time:19998ms step_avg:56.97ms
step:352/2330 train_time:20057ms step_avg:56.98ms
step:353/2330 train_time:20113ms step_avg:56.98ms
step:354/2330 train_time:20173ms step_avg:56.99ms
step:355/2330 train_time:20228ms step_avg:56.98ms
step:356/2330 train_time:20289ms step_avg:56.99ms
step:357/2330 train_time:20345ms step_avg:56.99ms
step:358/2330 train_time:20405ms step_avg:57.00ms
step:359/2330 train_time:20461ms step_avg:57.00ms
step:360/2330 train_time:20521ms step_avg:57.00ms
step:361/2330 train_time:20577ms step_avg:57.00ms
step:362/2330 train_time:20636ms step_avg:57.01ms
step:363/2330 train_time:20693ms step_avg:57.00ms
step:364/2330 train_time:20752ms step_avg:57.01ms
step:365/2330 train_time:20808ms step_avg:57.01ms
step:366/2330 train_time:20869ms step_avg:57.02ms
step:367/2330 train_time:20924ms step_avg:57.01ms
step:368/2330 train_time:20984ms step_avg:57.02ms
step:369/2330 train_time:21039ms step_avg:57.02ms
step:370/2330 train_time:21099ms step_avg:57.02ms
step:371/2330 train_time:21155ms step_avg:57.02ms
step:372/2330 train_time:21215ms step_avg:57.03ms
step:373/2330 train_time:21270ms step_avg:57.03ms
step:374/2330 train_time:21330ms step_avg:57.03ms
step:375/2330 train_time:21386ms step_avg:57.03ms
step:376/2330 train_time:21446ms step_avg:57.04ms
step:377/2330 train_time:21502ms step_avg:57.04ms
step:378/2330 train_time:21561ms step_avg:57.04ms
step:379/2330 train_time:21617ms step_avg:57.04ms
step:380/2330 train_time:21678ms step_avg:57.05ms
step:381/2330 train_time:21733ms step_avg:57.04ms
step:382/2330 train_time:21794ms step_avg:57.05ms
step:383/2330 train_time:21850ms step_avg:57.05ms
step:384/2330 train_time:21910ms step_avg:57.06ms
step:385/2330 train_time:21965ms step_avg:57.05ms
step:386/2330 train_time:22025ms step_avg:57.06ms
step:387/2330 train_time:22081ms step_avg:57.06ms
step:388/2330 train_time:22140ms step_avg:57.06ms
step:389/2330 train_time:22196ms step_avg:57.06ms
step:390/2330 train_time:22256ms step_avg:57.07ms
step:391/2330 train_time:22312ms step_avg:57.06ms
step:392/2330 train_time:22373ms step_avg:57.07ms
step:393/2330 train_time:22428ms step_avg:57.07ms
step:394/2330 train_time:22489ms step_avg:57.08ms
step:395/2330 train_time:22544ms step_avg:57.07ms
step:396/2330 train_time:22603ms step_avg:57.08ms
step:397/2330 train_time:22659ms step_avg:57.08ms
step:398/2330 train_time:22718ms step_avg:57.08ms
step:399/2330 train_time:22774ms step_avg:57.08ms
step:400/2330 train_time:22834ms step_avg:57.08ms
step:401/2330 train_time:22889ms step_avg:57.08ms
step:402/2330 train_time:22950ms step_avg:57.09ms
step:403/2330 train_time:23005ms step_avg:57.09ms
step:404/2330 train_time:23065ms step_avg:57.09ms
step:405/2330 train_time:23121ms step_avg:57.09ms
step:406/2330 train_time:23181ms step_avg:57.10ms
step:407/2330 train_time:23238ms step_avg:57.10ms
step:408/2330 train_time:23297ms step_avg:57.10ms
step:409/2330 train_time:23353ms step_avg:57.10ms
step:410/2330 train_time:23412ms step_avg:57.10ms
step:411/2330 train_time:23468ms step_avg:57.10ms
step:412/2330 train_time:23528ms step_avg:57.11ms
step:413/2330 train_time:23584ms step_avg:57.10ms
step:414/2330 train_time:23643ms step_avg:57.11ms
step:415/2330 train_time:23700ms step_avg:57.11ms
step:416/2330 train_time:23759ms step_avg:57.11ms
step:417/2330 train_time:23815ms step_avg:57.11ms
step:418/2330 train_time:23876ms step_avg:57.12ms
step:419/2330 train_time:23931ms step_avg:57.11ms
step:420/2330 train_time:23991ms step_avg:57.12ms
step:421/2330 train_time:24047ms step_avg:57.12ms
step:422/2330 train_time:24106ms step_avg:57.12ms
step:423/2330 train_time:24162ms step_avg:57.12ms
step:424/2330 train_time:24221ms step_avg:57.12ms
step:425/2330 train_time:24278ms step_avg:57.12ms
step:426/2330 train_time:24337ms step_avg:57.13ms
step:427/2330 train_time:24393ms step_avg:57.13ms
step:428/2330 train_time:24453ms step_avg:57.13ms
step:429/2330 train_time:24508ms step_avg:57.13ms
step:430/2330 train_time:24569ms step_avg:57.14ms
step:431/2330 train_time:24625ms step_avg:57.13ms
step:432/2330 train_time:24684ms step_avg:57.14ms
step:433/2330 train_time:24741ms step_avg:57.14ms
step:434/2330 train_time:24800ms step_avg:57.14ms
step:435/2330 train_time:24856ms step_avg:57.14ms
step:436/2330 train_time:24917ms step_avg:57.15ms
step:437/2330 train_time:24973ms step_avg:57.15ms
step:438/2330 train_time:25033ms step_avg:57.15ms
step:439/2330 train_time:25089ms step_avg:57.15ms
step:440/2330 train_time:25149ms step_avg:57.16ms
step:441/2330 train_time:25204ms step_avg:57.15ms
step:442/2330 train_time:25264ms step_avg:57.16ms
step:443/2330 train_time:25321ms step_avg:57.16ms
step:444/2330 train_time:25379ms step_avg:57.16ms
step:445/2330 train_time:25435ms step_avg:57.16ms
step:446/2330 train_time:25495ms step_avg:57.16ms
step:447/2330 train_time:25551ms step_avg:57.16ms
step:448/2330 train_time:25611ms step_avg:57.17ms
step:449/2330 train_time:25666ms step_avg:57.16ms
step:450/2330 train_time:25727ms step_avg:57.17ms
step:451/2330 train_time:25783ms step_avg:57.17ms
step:452/2330 train_time:25842ms step_avg:57.17ms
step:453/2330 train_time:25899ms step_avg:57.17ms
step:454/2330 train_time:25958ms step_avg:57.18ms
step:455/2330 train_time:26014ms step_avg:57.17ms
step:456/2330 train_time:26074ms step_avg:57.18ms
step:457/2330 train_time:26129ms step_avg:57.17ms
step:458/2330 train_time:26190ms step_avg:57.18ms
step:459/2330 train_time:26245ms step_avg:57.18ms
step:460/2330 train_time:26305ms step_avg:57.19ms
step:461/2330 train_time:26362ms step_avg:57.18ms
step:462/2330 train_time:26421ms step_avg:57.19ms
step:463/2330 train_time:26477ms step_avg:57.19ms
step:464/2330 train_time:26536ms step_avg:57.19ms
step:465/2330 train_time:26592ms step_avg:57.19ms
step:466/2330 train_time:26652ms step_avg:57.19ms
step:467/2330 train_time:26708ms step_avg:57.19ms
step:468/2330 train_time:26768ms step_avg:57.20ms
step:469/2330 train_time:26824ms step_avg:57.19ms
step:470/2330 train_time:26883ms step_avg:57.20ms
step:471/2330 train_time:26939ms step_avg:57.20ms
step:472/2330 train_time:26999ms step_avg:57.20ms
step:473/2330 train_time:27054ms step_avg:57.20ms
step:474/2330 train_time:27115ms step_avg:57.20ms
step:475/2330 train_time:27171ms step_avg:57.20ms
step:476/2330 train_time:27230ms step_avg:57.21ms
step:477/2330 train_time:27286ms step_avg:57.20ms
step:478/2330 train_time:27346ms step_avg:57.21ms
step:479/2330 train_time:27402ms step_avg:57.21ms
step:480/2330 train_time:27461ms step_avg:57.21ms
step:481/2330 train_time:27517ms step_avg:57.21ms
step:482/2330 train_time:27577ms step_avg:57.21ms
step:483/2330 train_time:27633ms step_avg:57.21ms
step:484/2330 train_time:27693ms step_avg:57.22ms
step:485/2330 train_time:27749ms step_avg:57.21ms
step:486/2330 train_time:27808ms step_avg:57.22ms
step:487/2330 train_time:27864ms step_avg:57.22ms
step:488/2330 train_time:27923ms step_avg:57.22ms
step:489/2330 train_time:27980ms step_avg:57.22ms
step:490/2330 train_time:28039ms step_avg:57.22ms
step:491/2330 train_time:28096ms step_avg:57.22ms
step:492/2330 train_time:28156ms step_avg:57.23ms
step:493/2330 train_time:28212ms step_avg:57.23ms
step:494/2330 train_time:28273ms step_avg:57.23ms
step:495/2330 train_time:28329ms step_avg:57.23ms
step:496/2330 train_time:28389ms step_avg:57.24ms
step:497/2330 train_time:28444ms step_avg:57.23ms
step:498/2330 train_time:28503ms step_avg:57.24ms
step:499/2330 train_time:28559ms step_avg:57.23ms
step:500/2330 train_time:28619ms step_avg:57.24ms
step:500/2330 val_loss:6.0754 train_time:28699ms step_avg:57.40ms
step:501/2330 train_time:28718ms step_avg:57.32ms
step:502/2330 train_time:28738ms step_avg:57.25ms
step:503/2330 train_time:28793ms step_avg:57.24ms
step:504/2330 train_time:28859ms step_avg:57.26ms
step:505/2330 train_time:28915ms step_avg:57.26ms
step:506/2330 train_time:28979ms step_avg:57.27ms
step:507/2330 train_time:29035ms step_avg:57.27ms
step:508/2330 train_time:29095ms step_avg:57.27ms
step:509/2330 train_time:29151ms step_avg:57.27ms
step:510/2330 train_time:29211ms step_avg:57.28ms
step:511/2330 train_time:29266ms step_avg:57.27ms
step:512/2330 train_time:29325ms step_avg:57.28ms
step:513/2330 train_time:29381ms step_avg:57.27ms
step:514/2330 train_time:29439ms step_avg:57.28ms
step:515/2330 train_time:29495ms step_avg:57.27ms
step:516/2330 train_time:29553ms step_avg:57.27ms
step:517/2330 train_time:29609ms step_avg:57.27ms
step:518/2330 train_time:29668ms step_avg:57.27ms
step:519/2330 train_time:29724ms step_avg:57.27ms
step:520/2330 train_time:29784ms step_avg:57.28ms
step:521/2330 train_time:29841ms step_avg:57.28ms
step:522/2330 train_time:29902ms step_avg:57.28ms
step:523/2330 train_time:29959ms step_avg:57.28ms
step:524/2330 train_time:30020ms step_avg:57.29ms
step:525/2330 train_time:30077ms step_avg:57.29ms
step:526/2330 train_time:30136ms step_avg:57.29ms
step:527/2330 train_time:30193ms step_avg:57.29ms
step:528/2330 train_time:30252ms step_avg:57.30ms
step:529/2330 train_time:30307ms step_avg:57.29ms
step:530/2330 train_time:30367ms step_avg:57.30ms
step:531/2330 train_time:30423ms step_avg:57.29ms
step:532/2330 train_time:30482ms step_avg:57.30ms
step:533/2330 train_time:30537ms step_avg:57.29ms
step:534/2330 train_time:30596ms step_avg:57.30ms
step:535/2330 train_time:30652ms step_avg:57.29ms
step:536/2330 train_time:30712ms step_avg:57.30ms
step:537/2330 train_time:30768ms step_avg:57.30ms
step:538/2330 train_time:30829ms step_avg:57.30ms
step:539/2330 train_time:30885ms step_avg:57.30ms
step:540/2330 train_time:30945ms step_avg:57.31ms
step:541/2330 train_time:31002ms step_avg:57.30ms
step:542/2330 train_time:31061ms step_avg:57.31ms
step:543/2330 train_time:31118ms step_avg:57.31ms
step:544/2330 train_time:31178ms step_avg:57.31ms
step:545/2330 train_time:31233ms step_avg:57.31ms
step:546/2330 train_time:31292ms step_avg:57.31ms
step:547/2330 train_time:31347ms step_avg:57.31ms
step:548/2330 train_time:31407ms step_avg:57.31ms
step:549/2330 train_time:31463ms step_avg:57.31ms
step:550/2330 train_time:31521ms step_avg:57.31ms
step:551/2330 train_time:31578ms step_avg:57.31ms
step:552/2330 train_time:31637ms step_avg:57.31ms
step:553/2330 train_time:31692ms step_avg:57.31ms
step:554/2330 train_time:31753ms step_avg:57.32ms
step:555/2330 train_time:31809ms step_avg:57.31ms
step:556/2330 train_time:31870ms step_avg:57.32ms
step:557/2330 train_time:31926ms step_avg:57.32ms
step:558/2330 train_time:31987ms step_avg:57.33ms
step:559/2330 train_time:32043ms step_avg:57.32ms
step:560/2330 train_time:32103ms step_avg:57.33ms
step:561/2330 train_time:32160ms step_avg:57.33ms
step:562/2330 train_time:32219ms step_avg:57.33ms
step:563/2330 train_time:32275ms step_avg:57.33ms
step:564/2330 train_time:32334ms step_avg:57.33ms
step:565/2330 train_time:32390ms step_avg:57.33ms
step:566/2330 train_time:32449ms step_avg:57.33ms
step:567/2330 train_time:32505ms step_avg:57.33ms
step:568/2330 train_time:32564ms step_avg:57.33ms
step:569/2330 train_time:32621ms step_avg:57.33ms
step:570/2330 train_time:32680ms step_avg:57.33ms
step:571/2330 train_time:32737ms step_avg:57.33ms
step:572/2330 train_time:32797ms step_avg:57.34ms
step:573/2330 train_time:32853ms step_avg:57.33ms
step:574/2330 train_time:32914ms step_avg:57.34ms
step:575/2330 train_time:32969ms step_avg:57.34ms
step:576/2330 train_time:33032ms step_avg:57.35ms
step:577/2330 train_time:33087ms step_avg:57.34ms
step:578/2330 train_time:33148ms step_avg:57.35ms
step:579/2330 train_time:33204ms step_avg:57.35ms
step:580/2330 train_time:33264ms step_avg:57.35ms
step:581/2330 train_time:33321ms step_avg:57.35ms
step:582/2330 train_time:33379ms step_avg:57.35ms
step:583/2330 train_time:33435ms step_avg:57.35ms
step:584/2330 train_time:33494ms step_avg:57.35ms
step:585/2330 train_time:33549ms step_avg:57.35ms
step:586/2330 train_time:33609ms step_avg:57.35ms
step:587/2330 train_time:33665ms step_avg:57.35ms
step:588/2330 train_time:33725ms step_avg:57.36ms
step:589/2330 train_time:33781ms step_avg:57.35ms
step:590/2330 train_time:33840ms step_avg:57.36ms
step:591/2330 train_time:33897ms step_avg:57.35ms
step:592/2330 train_time:33956ms step_avg:57.36ms
step:593/2330 train_time:34012ms step_avg:57.36ms
step:594/2330 train_time:34073ms step_avg:57.36ms
step:595/2330 train_time:34129ms step_avg:57.36ms
step:596/2330 train_time:34189ms step_avg:57.36ms
step:597/2330 train_time:34245ms step_avg:57.36ms
step:598/2330 train_time:34305ms step_avg:57.37ms
step:599/2330 train_time:34361ms step_avg:57.36ms
step:600/2330 train_time:34420ms step_avg:57.37ms
step:601/2330 train_time:34476ms step_avg:57.36ms
step:602/2330 train_time:34536ms step_avg:57.37ms
step:603/2330 train_time:34592ms step_avg:57.37ms
step:604/2330 train_time:34652ms step_avg:57.37ms
step:605/2330 train_time:34708ms step_avg:57.37ms
step:606/2330 train_time:34768ms step_avg:57.37ms
step:607/2330 train_time:34823ms step_avg:57.37ms
step:608/2330 train_time:34883ms step_avg:57.37ms
step:609/2330 train_time:34939ms step_avg:57.37ms
step:610/2330 train_time:34999ms step_avg:57.38ms
step:611/2330 train_time:35055ms step_avg:57.37ms
step:612/2330 train_time:35115ms step_avg:57.38ms
step:613/2330 train_time:35171ms step_avg:57.37ms
step:614/2330 train_time:35231ms step_avg:57.38ms
step:615/2330 train_time:35287ms step_avg:57.38ms
step:616/2330 train_time:35347ms step_avg:57.38ms
step:617/2330 train_time:35403ms step_avg:57.38ms
step:618/2330 train_time:35462ms step_avg:57.38ms
step:619/2330 train_time:35518ms step_avg:57.38ms
step:620/2330 train_time:35578ms step_avg:57.38ms
step:621/2330 train_time:35634ms step_avg:57.38ms
step:622/2330 train_time:35693ms step_avg:57.38ms
step:623/2330 train_time:35749ms step_avg:57.38ms
step:624/2330 train_time:35809ms step_avg:57.39ms
step:625/2330 train_time:35865ms step_avg:57.38ms
step:626/2330 train_time:35926ms step_avg:57.39ms
step:627/2330 train_time:35982ms step_avg:57.39ms
step:628/2330 train_time:36040ms step_avg:57.39ms
step:629/2330 train_time:36097ms step_avg:57.39ms
step:630/2330 train_time:36156ms step_avg:57.39ms
step:631/2330 train_time:36213ms step_avg:57.39ms
step:632/2330 train_time:36272ms step_avg:57.39ms
step:633/2330 train_time:36328ms step_avg:57.39ms
step:634/2330 train_time:36388ms step_avg:57.39ms
step:635/2330 train_time:36443ms step_avg:57.39ms
step:636/2330 train_time:36504ms step_avg:57.40ms
step:637/2330 train_time:36560ms step_avg:57.39ms
step:638/2330 train_time:36619ms step_avg:57.40ms
step:639/2330 train_time:36676ms step_avg:57.40ms
step:640/2330 train_time:36735ms step_avg:57.40ms
step:641/2330 train_time:36792ms step_avg:57.40ms
step:642/2330 train_time:36851ms step_avg:57.40ms
step:643/2330 train_time:36907ms step_avg:57.40ms
step:644/2330 train_time:36967ms step_avg:57.40ms
step:645/2330 train_time:37024ms step_avg:57.40ms
step:646/2330 train_time:37083ms step_avg:57.40ms
step:647/2330 train_time:37140ms step_avg:57.40ms
step:648/2330 train_time:37199ms step_avg:57.41ms
step:649/2330 train_time:37255ms step_avg:57.40ms
step:650/2330 train_time:37315ms step_avg:57.41ms
step:651/2330 train_time:37371ms step_avg:57.40ms
step:652/2330 train_time:37431ms step_avg:57.41ms
step:653/2330 train_time:37486ms step_avg:57.41ms
step:654/2330 train_time:37546ms step_avg:57.41ms
step:655/2330 train_time:37602ms step_avg:57.41ms
step:656/2330 train_time:37662ms step_avg:57.41ms
step:657/2330 train_time:37718ms step_avg:57.41ms
step:658/2330 train_time:37778ms step_avg:57.41ms
step:659/2330 train_time:37834ms step_avg:57.41ms
step:660/2330 train_time:37893ms step_avg:57.41ms
step:661/2330 train_time:37949ms step_avg:57.41ms
step:662/2330 train_time:38010ms step_avg:57.42ms
step:663/2330 train_time:38066ms step_avg:57.41ms
step:664/2330 train_time:38126ms step_avg:57.42ms
step:665/2330 train_time:38182ms step_avg:57.42ms
step:666/2330 train_time:38241ms step_avg:57.42ms
step:667/2330 train_time:38298ms step_avg:57.42ms
step:668/2330 train_time:38358ms step_avg:57.42ms
step:669/2330 train_time:38414ms step_avg:57.42ms
step:670/2330 train_time:38473ms step_avg:57.42ms
step:671/2330 train_time:38529ms step_avg:57.42ms
step:672/2330 train_time:38589ms step_avg:57.42ms
step:673/2330 train_time:38644ms step_avg:57.42ms
step:674/2330 train_time:38704ms step_avg:57.42ms
step:675/2330 train_time:38760ms step_avg:57.42ms
step:676/2330 train_time:38820ms step_avg:57.43ms
step:677/2330 train_time:38876ms step_avg:57.42ms
step:678/2330 train_time:38936ms step_avg:57.43ms
step:679/2330 train_time:38992ms step_avg:57.43ms
step:680/2330 train_time:39052ms step_avg:57.43ms
step:681/2330 train_time:39108ms step_avg:57.43ms
step:682/2330 train_time:39168ms step_avg:57.43ms
step:683/2330 train_time:39223ms step_avg:57.43ms
step:684/2330 train_time:39283ms step_avg:57.43ms
step:685/2330 train_time:39340ms step_avg:57.43ms
step:686/2330 train_time:39398ms step_avg:57.43ms
step:687/2330 train_time:39455ms step_avg:57.43ms
step:688/2330 train_time:39515ms step_avg:57.43ms
step:689/2330 train_time:39571ms step_avg:57.43ms
step:690/2330 train_time:39631ms step_avg:57.44ms
step:691/2330 train_time:39687ms step_avg:57.43ms
step:692/2330 train_time:39746ms step_avg:57.44ms
step:693/2330 train_time:39802ms step_avg:57.43ms
step:694/2330 train_time:39862ms step_avg:57.44ms
step:695/2330 train_time:39918ms step_avg:57.44ms
step:696/2330 train_time:39978ms step_avg:57.44ms
step:697/2330 train_time:40034ms step_avg:57.44ms
step:698/2330 train_time:40093ms step_avg:57.44ms
step:699/2330 train_time:40149ms step_avg:57.44ms
step:700/2330 train_time:40209ms step_avg:57.44ms
step:701/2330 train_time:40265ms step_avg:57.44ms
step:702/2330 train_time:40325ms step_avg:57.44ms
step:703/2330 train_time:40381ms step_avg:57.44ms
step:704/2330 train_time:40440ms step_avg:57.44ms
step:705/2330 train_time:40496ms step_avg:57.44ms
step:706/2330 train_time:40556ms step_avg:57.45ms
step:707/2330 train_time:40612ms step_avg:57.44ms
step:708/2330 train_time:40672ms step_avg:57.45ms
step:709/2330 train_time:40728ms step_avg:57.44ms
step:710/2330 train_time:40788ms step_avg:57.45ms
step:711/2330 train_time:40844ms step_avg:57.45ms
step:712/2330 train_time:40904ms step_avg:57.45ms
step:713/2330 train_time:40960ms step_avg:57.45ms
step:714/2330 train_time:41019ms step_avg:57.45ms
step:715/2330 train_time:41075ms step_avg:57.45ms
step:716/2330 train_time:41135ms step_avg:57.45ms
step:717/2330 train_time:41191ms step_avg:57.45ms
step:718/2330 train_time:41252ms step_avg:57.45ms
step:719/2330 train_time:41308ms step_avg:57.45ms
step:720/2330 train_time:41367ms step_avg:57.45ms
step:721/2330 train_time:41423ms step_avg:57.45ms
step:722/2330 train_time:41482ms step_avg:57.45ms
step:723/2330 train_time:41539ms step_avg:57.45ms
step:724/2330 train_time:41597ms step_avg:57.45ms
step:725/2330 train_time:41654ms step_avg:57.45ms
step:726/2330 train_time:41713ms step_avg:57.46ms
step:727/2330 train_time:41769ms step_avg:57.45ms
step:728/2330 train_time:41830ms step_avg:57.46ms
step:729/2330 train_time:41886ms step_avg:57.46ms
step:730/2330 train_time:41946ms step_avg:57.46ms
step:731/2330 train_time:42003ms step_avg:57.46ms
step:732/2330 train_time:42061ms step_avg:57.46ms
step:733/2330 train_time:42118ms step_avg:57.46ms
step:734/2330 train_time:42177ms step_avg:57.46ms
step:735/2330 train_time:42234ms step_avg:57.46ms
step:736/2330 train_time:42293ms step_avg:57.46ms
step:737/2330 train_time:42349ms step_avg:57.46ms
step:738/2330 train_time:42409ms step_avg:57.47ms
step:739/2330 train_time:42465ms step_avg:57.46ms
step:740/2330 train_time:42525ms step_avg:57.47ms
step:741/2330 train_time:42581ms step_avg:57.46ms
step:742/2330 train_time:42640ms step_avg:57.47ms
step:743/2330 train_time:42697ms step_avg:57.47ms
step:744/2330 train_time:42756ms step_avg:57.47ms
step:745/2330 train_time:42811ms step_avg:57.46ms
step:746/2330 train_time:42872ms step_avg:57.47ms
step:747/2330 train_time:42927ms step_avg:57.47ms
step:748/2330 train_time:42988ms step_avg:57.47ms
step:749/2330 train_time:43043ms step_avg:57.47ms
step:750/2330 train_time:43103ms step_avg:57.47ms
step:750/2330 val_loss:5.8174 train_time:43183ms step_avg:57.58ms
step:751/2330 train_time:43201ms step_avg:57.52ms
step:752/2330 train_time:43220ms step_avg:57.47ms
step:753/2330 train_time:43275ms step_avg:57.47ms
step:754/2330 train_time:43339ms step_avg:57.48ms
step:755/2330 train_time:43395ms step_avg:57.48ms
step:756/2330 train_time:43457ms step_avg:57.48ms
step:757/2330 train_time:43513ms step_avg:57.48ms
step:758/2330 train_time:43573ms step_avg:57.48ms
step:759/2330 train_time:43628ms step_avg:57.48ms
step:760/2330 train_time:43687ms step_avg:57.48ms
step:761/2330 train_time:43743ms step_avg:57.48ms
step:762/2330 train_time:43802ms step_avg:57.48ms
step:763/2330 train_time:43858ms step_avg:57.48ms
step:764/2330 train_time:43917ms step_avg:57.48ms
step:765/2330 train_time:43974ms step_avg:57.48ms
step:766/2330 train_time:44032ms step_avg:57.48ms
step:767/2330 train_time:44090ms step_avg:57.48ms
step:768/2330 train_time:44152ms step_avg:57.49ms
step:769/2330 train_time:44210ms step_avg:57.49ms
step:770/2330 train_time:44272ms step_avg:57.50ms
step:771/2330 train_time:44329ms step_avg:57.50ms
step:772/2330 train_time:44391ms step_avg:57.50ms
step:773/2330 train_time:44448ms step_avg:57.50ms
step:774/2330 train_time:44508ms step_avg:57.50ms
step:775/2330 train_time:44565ms step_avg:57.50ms
step:776/2330 train_time:44626ms step_avg:57.51ms
step:777/2330 train_time:44683ms step_avg:57.51ms
step:778/2330 train_time:44743ms step_avg:57.51ms
step:779/2330 train_time:44800ms step_avg:57.51ms
step:780/2330 train_time:44860ms step_avg:57.51ms
step:781/2330 train_time:44917ms step_avg:57.51ms
step:782/2330 train_time:44977ms step_avg:57.51ms
step:783/2330 train_time:45033ms step_avg:57.51ms
step:784/2330 train_time:45093ms step_avg:57.52ms
step:785/2330 train_time:45151ms step_avg:57.52ms
step:786/2330 train_time:45210ms step_avg:57.52ms
step:787/2330 train_time:45268ms step_avg:57.52ms
step:788/2330 train_time:45329ms step_avg:57.52ms
step:789/2330 train_time:45385ms step_avg:57.52ms
step:790/2330 train_time:45447ms step_avg:57.53ms
step:791/2330 train_time:45504ms step_avg:57.53ms
step:792/2330 train_time:45566ms step_avg:57.53ms
step:793/2330 train_time:45622ms step_avg:57.53ms
step:794/2330 train_time:45683ms step_avg:57.53ms
step:795/2330 train_time:45739ms step_avg:57.53ms
step:796/2330 train_time:45800ms step_avg:57.54ms
step:797/2330 train_time:45856ms step_avg:57.54ms
step:798/2330 train_time:45916ms step_avg:57.54ms
step:799/2330 train_time:45972ms step_avg:57.54ms
step:800/2330 train_time:46033ms step_avg:57.54ms
step:801/2330 train_time:46089ms step_avg:57.54ms
step:802/2330 train_time:46150ms step_avg:57.54ms
step:803/2330 train_time:46208ms step_avg:57.54ms
step:804/2330 train_time:46268ms step_avg:57.55ms
step:805/2330 train_time:46325ms step_avg:57.55ms
step:806/2330 train_time:46386ms step_avg:57.55ms
step:807/2330 train_time:46443ms step_avg:57.55ms
step:808/2330 train_time:46505ms step_avg:57.56ms
step:809/2330 train_time:46562ms step_avg:57.55ms
step:810/2330 train_time:46622ms step_avg:57.56ms
step:811/2330 train_time:46679ms step_avg:57.56ms
step:812/2330 train_time:46740ms step_avg:57.56ms
step:813/2330 train_time:46797ms step_avg:57.56ms
step:814/2330 train_time:46858ms step_avg:57.56ms
step:815/2330 train_time:46914ms step_avg:57.56ms
step:816/2330 train_time:46974ms step_avg:57.57ms
step:817/2330 train_time:47031ms step_avg:57.57ms
step:818/2330 train_time:47091ms step_avg:57.57ms
step:819/2330 train_time:47148ms step_avg:57.57ms
step:820/2330 train_time:47208ms step_avg:57.57ms
step:821/2330 train_time:47266ms step_avg:57.57ms
step:822/2330 train_time:47327ms step_avg:57.57ms
step:823/2330 train_time:47384ms step_avg:57.57ms
step:824/2330 train_time:47444ms step_avg:57.58ms
step:825/2330 train_time:47501ms step_avg:57.58ms
step:826/2330 train_time:47562ms step_avg:57.58ms
step:827/2330 train_time:47619ms step_avg:57.58ms
step:828/2330 train_time:47680ms step_avg:57.58ms
step:829/2330 train_time:47737ms step_avg:57.58ms
step:830/2330 train_time:47797ms step_avg:57.59ms
step:831/2330 train_time:47854ms step_avg:57.59ms
step:832/2330 train_time:47914ms step_avg:57.59ms
step:833/2330 train_time:47971ms step_avg:57.59ms
step:834/2330 train_time:48031ms step_avg:57.59ms
step:835/2330 train_time:48087ms step_avg:57.59ms
step:836/2330 train_time:48148ms step_avg:57.59ms
step:837/2330 train_time:48205ms step_avg:57.59ms
step:838/2330 train_time:48265ms step_avg:57.60ms
step:839/2330 train_time:48322ms step_avg:57.60ms
step:840/2330 train_time:48383ms step_avg:57.60ms
step:841/2330 train_time:48440ms step_avg:57.60ms
step:842/2330 train_time:48501ms step_avg:57.60ms
step:843/2330 train_time:48558ms step_avg:57.60ms
step:844/2330 train_time:48619ms step_avg:57.61ms
step:845/2330 train_time:48675ms step_avg:57.60ms
step:846/2330 train_time:48736ms step_avg:57.61ms
step:847/2330 train_time:48793ms step_avg:57.61ms
step:848/2330 train_time:48854ms step_avg:57.61ms
step:849/2330 train_time:48911ms step_avg:57.61ms
step:850/2330 train_time:48970ms step_avg:57.61ms
step:851/2330 train_time:49027ms step_avg:57.61ms
step:852/2330 train_time:49088ms step_avg:57.61ms
step:853/2330 train_time:49144ms step_avg:57.61ms
step:854/2330 train_time:49205ms step_avg:57.62ms
step:855/2330 train_time:49262ms step_avg:57.62ms
step:856/2330 train_time:49322ms step_avg:57.62ms
step:857/2330 train_time:49379ms step_avg:57.62ms
step:858/2330 train_time:49440ms step_avg:57.62ms
step:859/2330 train_time:49498ms step_avg:57.62ms
step:860/2330 train_time:49558ms step_avg:57.63ms
step:861/2330 train_time:49615ms step_avg:57.62ms
step:862/2330 train_time:49675ms step_avg:57.63ms
step:863/2330 train_time:49732ms step_avg:57.63ms
step:864/2330 train_time:49792ms step_avg:57.63ms
step:865/2330 train_time:49850ms step_avg:57.63ms
step:866/2330 train_time:49909ms step_avg:57.63ms
step:867/2330 train_time:49966ms step_avg:57.63ms
step:868/2330 train_time:50026ms step_avg:57.63ms
step:869/2330 train_time:50083ms step_avg:57.63ms
step:870/2330 train_time:50143ms step_avg:57.64ms
step:871/2330 train_time:50200ms step_avg:57.63ms
step:872/2330 train_time:50260ms step_avg:57.64ms
step:873/2330 train_time:50317ms step_avg:57.64ms
step:874/2330 train_time:50377ms step_avg:57.64ms
step:875/2330 train_time:50434ms step_avg:57.64ms
step:876/2330 train_time:50494ms step_avg:57.64ms
step:877/2330 train_time:50552ms step_avg:57.64ms
step:878/2330 train_time:50612ms step_avg:57.64ms
step:879/2330 train_time:50669ms step_avg:57.64ms
step:880/2330 train_time:50730ms step_avg:57.65ms
step:881/2330 train_time:50788ms step_avg:57.65ms
step:882/2330 train_time:50849ms step_avg:57.65ms
step:883/2330 train_time:50906ms step_avg:57.65ms
step:884/2330 train_time:50965ms step_avg:57.65ms
step:885/2330 train_time:51022ms step_avg:57.65ms
step:886/2330 train_time:51083ms step_avg:57.66ms
step:887/2330 train_time:51140ms step_avg:57.66ms
step:888/2330 train_time:51200ms step_avg:57.66ms
step:889/2330 train_time:51257ms step_avg:57.66ms
step:890/2330 train_time:51317ms step_avg:57.66ms
step:891/2330 train_time:51374ms step_avg:57.66ms
step:892/2330 train_time:51434ms step_avg:57.66ms
step:893/2330 train_time:51490ms step_avg:57.66ms
step:894/2330 train_time:51551ms step_avg:57.66ms
step:895/2330 train_time:51608ms step_avg:57.66ms
step:896/2330 train_time:51669ms step_avg:57.67ms
step:897/2330 train_time:51725ms step_avg:57.66ms
step:898/2330 train_time:51786ms step_avg:57.67ms
step:899/2330 train_time:51844ms step_avg:57.67ms
step:900/2330 train_time:51905ms step_avg:57.67ms
step:901/2330 train_time:51962ms step_avg:57.67ms
step:902/2330 train_time:52022ms step_avg:57.67ms
step:903/2330 train_time:52079ms step_avg:57.67ms
step:904/2330 train_time:52141ms step_avg:57.68ms
step:905/2330 train_time:52197ms step_avg:57.68ms
step:906/2330 train_time:52259ms step_avg:57.68ms
step:907/2330 train_time:52315ms step_avg:57.68ms
step:908/2330 train_time:52375ms step_avg:57.68ms
step:909/2330 train_time:52432ms step_avg:57.68ms
step:910/2330 train_time:52493ms step_avg:57.68ms
step:911/2330 train_time:52549ms step_avg:57.68ms
step:912/2330 train_time:52610ms step_avg:57.69ms
step:913/2330 train_time:52667ms step_avg:57.69ms
step:914/2330 train_time:52728ms step_avg:57.69ms
step:915/2330 train_time:52784ms step_avg:57.69ms
step:916/2330 train_time:52846ms step_avg:57.69ms
step:917/2330 train_time:52903ms step_avg:57.69ms
step:918/2330 train_time:52964ms step_avg:57.69ms
step:919/2330 train_time:53020ms step_avg:57.69ms
step:920/2330 train_time:53082ms step_avg:57.70ms
step:921/2330 train_time:53138ms step_avg:57.70ms
step:922/2330 train_time:53200ms step_avg:57.70ms
step:923/2330 train_time:53256ms step_avg:57.70ms
step:924/2330 train_time:53317ms step_avg:57.70ms
step:925/2330 train_time:53374ms step_avg:57.70ms
step:926/2330 train_time:53434ms step_avg:57.70ms
step:927/2330 train_time:53490ms step_avg:57.70ms
step:928/2330 train_time:53551ms step_avg:57.71ms
step:929/2330 train_time:53608ms step_avg:57.71ms
step:930/2330 train_time:53668ms step_avg:57.71ms
step:931/2330 train_time:53727ms step_avg:57.71ms
step:932/2330 train_time:53786ms step_avg:57.71ms
step:933/2330 train_time:53844ms step_avg:57.71ms
step:934/2330 train_time:53904ms step_avg:57.71ms
step:935/2330 train_time:53962ms step_avg:57.71ms
step:936/2330 train_time:54021ms step_avg:57.71ms
step:937/2330 train_time:54078ms step_avg:57.71ms
step:938/2330 train_time:54138ms step_avg:57.72ms
step:939/2330 train_time:54194ms step_avg:57.71ms
step:940/2330 train_time:54255ms step_avg:57.72ms
step:941/2330 train_time:54312ms step_avg:57.72ms
step:942/2330 train_time:54372ms step_avg:57.72ms
step:943/2330 train_time:54429ms step_avg:57.72ms
step:944/2330 train_time:54490ms step_avg:57.72ms
step:945/2330 train_time:54547ms step_avg:57.72ms
step:946/2330 train_time:54607ms step_avg:57.72ms
step:947/2330 train_time:54665ms step_avg:57.72ms
step:948/2330 train_time:54725ms step_avg:57.73ms
step:949/2330 train_time:54782ms step_avg:57.73ms
step:950/2330 train_time:54842ms step_avg:57.73ms
step:951/2330 train_time:54899ms step_avg:57.73ms
step:952/2330 train_time:54960ms step_avg:57.73ms
step:953/2330 train_time:55017ms step_avg:57.73ms
step:954/2330 train_time:55077ms step_avg:57.73ms
step:955/2330 train_time:55133ms step_avg:57.73ms
step:956/2330 train_time:55194ms step_avg:57.73ms
step:957/2330 train_time:55251ms step_avg:57.73ms
step:958/2330 train_time:55312ms step_avg:57.74ms
step:959/2330 train_time:55370ms step_avg:57.74ms
step:960/2330 train_time:55430ms step_avg:57.74ms
step:961/2330 train_time:55486ms step_avg:57.74ms
step:962/2330 train_time:55547ms step_avg:57.74ms
step:963/2330 train_time:55604ms step_avg:57.74ms
step:964/2330 train_time:55664ms step_avg:57.74ms
step:965/2330 train_time:55721ms step_avg:57.74ms
step:966/2330 train_time:55781ms step_avg:57.74ms
step:967/2330 train_time:55838ms step_avg:57.74ms
step:968/2330 train_time:55898ms step_avg:57.75ms
step:969/2330 train_time:55955ms step_avg:57.74ms
step:970/2330 train_time:56015ms step_avg:57.75ms
step:971/2330 train_time:56073ms step_avg:57.75ms
step:972/2330 train_time:56133ms step_avg:57.75ms
step:973/2330 train_time:56189ms step_avg:57.75ms
step:974/2330 train_time:56250ms step_avg:57.75ms
step:975/2330 train_time:56307ms step_avg:57.75ms
step:976/2330 train_time:56368ms step_avg:57.75ms
step:977/2330 train_time:56425ms step_avg:57.75ms
step:978/2330 train_time:56485ms step_avg:57.76ms
step:979/2330 train_time:56541ms step_avg:57.75ms
step:980/2330 train_time:56603ms step_avg:57.76ms
step:981/2330 train_time:56659ms step_avg:57.76ms
step:982/2330 train_time:56721ms step_avg:57.76ms
step:983/2330 train_time:56777ms step_avg:57.76ms
step:984/2330 train_time:56839ms step_avg:57.76ms
step:985/2330 train_time:56895ms step_avg:57.76ms
step:986/2330 train_time:56956ms step_avg:57.76ms
step:987/2330 train_time:57013ms step_avg:57.76ms
step:988/2330 train_time:57073ms step_avg:57.77ms
step:989/2330 train_time:57130ms step_avg:57.77ms
step:990/2330 train_time:57190ms step_avg:57.77ms
step:991/2330 train_time:57247ms step_avg:57.77ms
step:992/2330 train_time:57307ms step_avg:57.77ms
step:993/2330 train_time:57364ms step_avg:57.77ms
step:994/2330 train_time:57424ms step_avg:57.77ms
step:995/2330 train_time:57481ms step_avg:57.77ms
step:996/2330 train_time:57542ms step_avg:57.77ms
step:997/2330 train_time:57599ms step_avg:57.77ms
step:998/2330 train_time:57660ms step_avg:57.78ms
step:999/2330 train_time:57716ms step_avg:57.77ms
step:1000/2330 train_time:57777ms step_avg:57.78ms
step:1000/2330 val_loss:5.5670 train_time:57858ms step_avg:57.86ms
step:1001/2330 train_time:57876ms step_avg:57.82ms
step:1002/2330 train_time:57897ms step_avg:57.78ms
step:1003/2330 train_time:57950ms step_avg:57.78ms
step:1004/2330 train_time:58016ms step_avg:57.78ms
step:1005/2330 train_time:58072ms step_avg:57.78ms
step:1006/2330 train_time:58135ms step_avg:57.79ms
step:1007/2330 train_time:58191ms step_avg:57.79ms
step:1008/2330 train_time:58250ms step_avg:57.79ms
step:1009/2330 train_time:58306ms step_avg:57.79ms
step:1010/2330 train_time:58366ms step_avg:57.79ms
step:1011/2330 train_time:58422ms step_avg:57.79ms
step:1012/2330 train_time:58481ms step_avg:57.79ms
step:1013/2330 train_time:58537ms step_avg:57.79ms
step:1014/2330 train_time:58597ms step_avg:57.79ms
step:1015/2330 train_time:58653ms step_avg:57.79ms
step:1016/2330 train_time:58712ms step_avg:57.79ms
step:1017/2330 train_time:58769ms step_avg:57.79ms
step:1018/2330 train_time:58835ms step_avg:57.79ms
step:1019/2330 train_time:58892ms step_avg:57.79ms
step:1020/2330 train_time:58954ms step_avg:57.80ms
step:1021/2330 train_time:59011ms step_avg:57.80ms
step:1022/2330 train_time:59073ms step_avg:57.80ms
step:1023/2330 train_time:59129ms step_avg:57.80ms
step:1024/2330 train_time:59189ms step_avg:57.80ms
step:1025/2330 train_time:59246ms step_avg:57.80ms
step:1026/2330 train_time:59306ms step_avg:57.80ms
step:1027/2330 train_time:59362ms step_avg:57.80ms
step:1028/2330 train_time:59421ms step_avg:57.80ms
step:1029/2330 train_time:59478ms step_avg:57.80ms
step:1030/2330 train_time:59537ms step_avg:57.80ms
step:1031/2330 train_time:59594ms step_avg:57.80ms
step:1032/2330 train_time:59654ms step_avg:57.80ms
step:1033/2330 train_time:59711ms step_avg:57.80ms
step:1034/2330 train_time:59773ms step_avg:57.81ms
step:1035/2330 train_time:59830ms step_avg:57.81ms
step:1036/2330 train_time:59892ms step_avg:57.81ms
step:1037/2330 train_time:59949ms step_avg:57.81ms
step:1038/2330 train_time:60011ms step_avg:57.81ms
step:1039/2330 train_time:60068ms step_avg:57.81ms
step:1040/2330 train_time:60128ms step_avg:57.82ms
step:1041/2330 train_time:60185ms step_avg:57.81ms
step:1042/2330 train_time:60245ms step_avg:57.82ms
step:1043/2330 train_time:60302ms step_avg:57.82ms
step:1044/2330 train_time:60362ms step_avg:57.82ms
step:1045/2330 train_time:60419ms step_avg:57.82ms
step:1046/2330 train_time:60478ms step_avg:57.82ms
step:1047/2330 train_time:60534ms step_avg:57.82ms
step:1048/2330 train_time:60594ms step_avg:57.82ms
step:1049/2330 train_time:60650ms step_avg:57.82ms
step:1050/2330 train_time:60711ms step_avg:57.82ms
step:1051/2330 train_time:60768ms step_avg:57.82ms
step:1052/2330 train_time:60830ms step_avg:57.82ms
step:1053/2330 train_time:60887ms step_avg:57.82ms
step:1054/2330 train_time:60947ms step_avg:57.82ms
step:1055/2330 train_time:61004ms step_avg:57.82ms
step:1056/2330 train_time:61065ms step_avg:57.83ms
step:1057/2330 train_time:61122ms step_avg:57.83ms
step:1058/2330 train_time:61183ms step_avg:57.83ms
step:1059/2330 train_time:61239ms step_avg:57.83ms
step:1060/2330 train_time:61300ms step_avg:57.83ms
step:1061/2330 train_time:61356ms step_avg:57.83ms
step:1062/2330 train_time:61416ms step_avg:57.83ms
step:1063/2330 train_time:61473ms step_avg:57.83ms
step:1064/2330 train_time:61533ms step_avg:57.83ms
step:1065/2330 train_time:61590ms step_avg:57.83ms
step:1066/2330 train_time:61650ms step_avg:57.83ms
step:1067/2330 train_time:61706ms step_avg:57.83ms
step:1068/2330 train_time:61767ms step_avg:57.83ms
step:1069/2330 train_time:61824ms step_avg:57.83ms
step:1070/2330 train_time:61885ms step_avg:57.84ms
step:1071/2330 train_time:61942ms step_avg:57.84ms
step:1072/2330 train_time:62003ms step_avg:57.84ms
step:1073/2330 train_time:62059ms step_avg:57.84ms
step:1074/2330 train_time:62121ms step_avg:57.84ms
step:1075/2330 train_time:62178ms step_avg:57.84ms
step:1076/2330 train_time:62238ms step_avg:57.84ms
step:1077/2330 train_time:62294ms step_avg:57.84ms
step:1078/2330 train_time:62355ms step_avg:57.84ms
step:1079/2330 train_time:62411ms step_avg:57.84ms
step:1080/2330 train_time:62472ms step_avg:57.84ms
step:1081/2330 train_time:62529ms step_avg:57.84ms
step:1082/2330 train_time:62589ms step_avg:57.85ms
step:1083/2330 train_time:62646ms step_avg:57.84ms
step:1084/2330 train_time:62706ms step_avg:57.85ms
step:1085/2330 train_time:62763ms step_avg:57.85ms
step:1086/2330 train_time:62824ms step_avg:57.85ms
step:1087/2330 train_time:62881ms step_avg:57.85ms
step:1088/2330 train_time:62942ms step_avg:57.85ms
step:1089/2330 train_time:62999ms step_avg:57.85ms
step:1090/2330 train_time:63059ms step_avg:57.85ms
step:1091/2330 train_time:63117ms step_avg:57.85ms
step:1092/2330 train_time:63177ms step_avg:57.85ms
step:1093/2330 train_time:63234ms step_avg:57.85ms
step:1094/2330 train_time:63294ms step_avg:57.86ms
step:1095/2330 train_time:63350ms step_avg:57.85ms
step:1096/2330 train_time:63411ms step_avg:57.86ms
step:1097/2330 train_time:63467ms step_avg:57.86ms
step:1098/2330 train_time:63528ms step_avg:57.86ms
step:1099/2330 train_time:63585ms step_avg:57.86ms
step:1100/2330 train_time:63645ms step_avg:57.86ms
step:1101/2330 train_time:63702ms step_avg:57.86ms
step:1102/2330 train_time:63762ms step_avg:57.86ms
step:1103/2330 train_time:63819ms step_avg:57.86ms
step:1104/2330 train_time:63880ms step_avg:57.86ms
step:1105/2330 train_time:63938ms step_avg:57.86ms
step:1106/2330 train_time:63999ms step_avg:57.87ms
step:1107/2330 train_time:64056ms step_avg:57.86ms
step:1108/2330 train_time:64116ms step_avg:57.87ms
step:1109/2330 train_time:64173ms step_avg:57.87ms
step:1110/2330 train_time:64234ms step_avg:57.87ms
step:1111/2330 train_time:64291ms step_avg:57.87ms
step:1112/2330 train_time:64351ms step_avg:57.87ms
step:1113/2330 train_time:64408ms step_avg:57.87ms
step:1114/2330 train_time:64467ms step_avg:57.87ms
step:1115/2330 train_time:64524ms step_avg:57.87ms
step:1116/2330 train_time:64584ms step_avg:57.87ms
step:1117/2330 train_time:64641ms step_avg:57.87ms
step:1118/2330 train_time:64701ms step_avg:57.87ms
step:1119/2330 train_time:64757ms step_avg:57.87ms
step:1120/2330 train_time:64818ms step_avg:57.87ms
step:1121/2330 train_time:64875ms step_avg:57.87ms
step:1122/2330 train_time:64936ms step_avg:57.88ms
step:1123/2330 train_time:64993ms step_avg:57.87ms
step:1124/2330 train_time:65054ms step_avg:57.88ms
step:1125/2330 train_time:65111ms step_avg:57.88ms
step:1126/2330 train_time:65172ms step_avg:57.88ms
step:1127/2330 train_time:65229ms step_avg:57.88ms
step:1128/2330 train_time:65289ms step_avg:57.88ms
step:1129/2330 train_time:65345ms step_avg:57.88ms
step:1130/2330 train_time:65406ms step_avg:57.88ms
step:1131/2330 train_time:65463ms step_avg:57.88ms
step:1132/2330 train_time:65523ms step_avg:57.88ms
step:1133/2330 train_time:65579ms step_avg:57.88ms
step:1134/2330 train_time:65639ms step_avg:57.88ms
step:1135/2330 train_time:65696ms step_avg:57.88ms
step:1136/2330 train_time:65756ms step_avg:57.88ms
step:1137/2330 train_time:65813ms step_avg:57.88ms
step:1138/2330 train_time:65873ms step_avg:57.88ms
step:1139/2330 train_time:65929ms step_avg:57.88ms
step:1140/2330 train_time:65990ms step_avg:57.89ms
step:1141/2330 train_time:66047ms step_avg:57.89ms
step:1142/2330 train_time:66108ms step_avg:57.89ms
step:1143/2330 train_time:66165ms step_avg:57.89ms
step:1144/2330 train_time:66226ms step_avg:57.89ms
step:1145/2330 train_time:66283ms step_avg:57.89ms
step:1146/2330 train_time:66343ms step_avg:57.89ms
step:1147/2330 train_time:66399ms step_avg:57.89ms
step:1148/2330 train_time:66460ms step_avg:57.89ms
step:1149/2330 train_time:66517ms step_avg:57.89ms
step:1150/2330 train_time:66579ms step_avg:57.89ms
step:1151/2330 train_time:66635ms step_avg:57.89ms
step:1152/2330 train_time:66696ms step_avg:57.90ms
step:1153/2330 train_time:66753ms step_avg:57.89ms
step:1154/2330 train_time:66813ms step_avg:57.90ms
step:1155/2330 train_time:66869ms step_avg:57.90ms
step:1156/2330 train_time:66930ms step_avg:57.90ms
step:1157/2330 train_time:66987ms step_avg:57.90ms
step:1158/2330 train_time:67047ms step_avg:57.90ms
step:1159/2330 train_time:67105ms step_avg:57.90ms
step:1160/2330 train_time:67164ms step_avg:57.90ms
step:1161/2330 train_time:67221ms step_avg:57.90ms
step:1162/2330 train_time:67282ms step_avg:57.90ms
step:1163/2330 train_time:67339ms step_avg:57.90ms
step:1164/2330 train_time:67400ms step_avg:57.90ms
step:1165/2330 train_time:67456ms step_avg:57.90ms
step:1166/2330 train_time:67517ms step_avg:57.91ms
step:1167/2330 train_time:67574ms step_avg:57.90ms
step:1168/2330 train_time:67634ms step_avg:57.91ms
step:1169/2330 train_time:67690ms step_avg:57.90ms
step:1170/2330 train_time:67751ms step_avg:57.91ms
step:1171/2330 train_time:67807ms step_avg:57.91ms
step:1172/2330 train_time:67868ms step_avg:57.91ms
step:1173/2330 train_time:67925ms step_avg:57.91ms
step:1174/2330 train_time:67986ms step_avg:57.91ms
step:1175/2330 train_time:68043ms step_avg:57.91ms
step:1176/2330 train_time:68103ms step_avg:57.91ms
step:1177/2330 train_time:68160ms step_avg:57.91ms
step:1178/2330 train_time:68221ms step_avg:57.91ms
step:1179/2330 train_time:68278ms step_avg:57.91ms
step:1180/2330 train_time:68338ms step_avg:57.91ms
step:1181/2330 train_time:68395ms step_avg:57.91ms
step:1182/2330 train_time:68456ms step_avg:57.92ms
step:1183/2330 train_time:68512ms step_avg:57.91ms
step:1184/2330 train_time:68574ms step_avg:57.92ms
step:1185/2330 train_time:68631ms step_avg:57.92ms
step:1186/2330 train_time:68691ms step_avg:57.92ms
step:1187/2330 train_time:68747ms step_avg:57.92ms
step:1188/2330 train_time:68807ms step_avg:57.92ms
step:1189/2330 train_time:68864ms step_avg:57.92ms
step:1190/2330 train_time:68923ms step_avg:57.92ms
step:1191/2330 train_time:68980ms step_avg:57.92ms
step:1192/2330 train_time:69041ms step_avg:57.92ms
step:1193/2330 train_time:69098ms step_avg:57.92ms
step:1194/2330 train_time:69158ms step_avg:57.92ms
step:1195/2330 train_time:69215ms step_avg:57.92ms
step:1196/2330 train_time:69276ms step_avg:57.92ms
step:1197/2330 train_time:69333ms step_avg:57.92ms
step:1198/2330 train_time:69393ms step_avg:57.92ms
step:1199/2330 train_time:69450ms step_avg:57.92ms
step:1200/2330 train_time:69510ms step_avg:57.92ms
step:1201/2330 train_time:69567ms step_avg:57.92ms
step:1202/2330 train_time:69627ms step_avg:57.93ms
step:1203/2330 train_time:69684ms step_avg:57.93ms
step:1204/2330 train_time:69744ms step_avg:57.93ms
step:1205/2330 train_time:69800ms step_avg:57.93ms
step:1206/2330 train_time:69861ms step_avg:57.93ms
step:1207/2330 train_time:69917ms step_avg:57.93ms
step:1208/2330 train_time:69978ms step_avg:57.93ms
step:1209/2330 train_time:70034ms step_avg:57.93ms
step:1210/2330 train_time:70096ms step_avg:57.93ms
step:1211/2330 train_time:70153ms step_avg:57.93ms
step:1212/2330 train_time:70214ms step_avg:57.93ms
step:1213/2330 train_time:70270ms step_avg:57.93ms
step:1214/2330 train_time:70332ms step_avg:57.93ms
step:1215/2330 train_time:70389ms step_avg:57.93ms
step:1216/2330 train_time:70449ms step_avg:57.93ms
step:1217/2330 train_time:70506ms step_avg:57.93ms
step:1218/2330 train_time:70566ms step_avg:57.94ms
step:1219/2330 train_time:70623ms step_avg:57.94ms
step:1220/2330 train_time:70683ms step_avg:57.94ms
step:1221/2330 train_time:70740ms step_avg:57.94ms
step:1222/2330 train_time:70801ms step_avg:57.94ms
step:1223/2330 train_time:70857ms step_avg:57.94ms
step:1224/2330 train_time:70918ms step_avg:57.94ms
step:1225/2330 train_time:70975ms step_avg:57.94ms
step:1226/2330 train_time:71035ms step_avg:57.94ms
step:1227/2330 train_time:71092ms step_avg:57.94ms
step:1228/2330 train_time:71153ms step_avg:57.94ms
step:1229/2330 train_time:71209ms step_avg:57.94ms
step:1230/2330 train_time:71269ms step_avg:57.94ms
step:1231/2330 train_time:71326ms step_avg:57.94ms
step:1232/2330 train_time:71386ms step_avg:57.94ms
step:1233/2330 train_time:71443ms step_avg:57.94ms
step:1234/2330 train_time:71503ms step_avg:57.94ms
step:1235/2330 train_time:71560ms step_avg:57.94ms
step:1236/2330 train_time:71620ms step_avg:57.95ms
step:1237/2330 train_time:71677ms step_avg:57.94ms
step:1238/2330 train_time:71738ms step_avg:57.95ms
step:1239/2330 train_time:71795ms step_avg:57.95ms
step:1240/2330 train_time:71855ms step_avg:57.95ms
step:1241/2330 train_time:71912ms step_avg:57.95ms
step:1242/2330 train_time:71972ms step_avg:57.95ms
step:1243/2330 train_time:72028ms step_avg:57.95ms
step:1244/2330 train_time:72089ms step_avg:57.95ms
step:1245/2330 train_time:72146ms step_avg:57.95ms
step:1246/2330 train_time:72206ms step_avg:57.95ms
step:1247/2330 train_time:72263ms step_avg:57.95ms
step:1248/2330 train_time:72324ms step_avg:57.95ms
step:1249/2330 train_time:72380ms step_avg:57.95ms
step:1250/2330 train_time:72441ms step_avg:57.95ms
step:1250/2330 val_loss:5.4065 train_time:72522ms step_avg:58.02ms
step:1251/2330 train_time:72540ms step_avg:57.99ms
step:1252/2330 train_time:72560ms step_avg:57.96ms
step:1253/2330 train_time:72619ms step_avg:57.96ms
step:1254/2330 train_time:72683ms step_avg:57.96ms
step:1255/2330 train_time:72740ms step_avg:57.96ms
step:1256/2330 train_time:72799ms step_avg:57.96ms
step:1257/2330 train_time:72856ms step_avg:57.96ms
step:1258/2330 train_time:72915ms step_avg:57.96ms
step:1259/2330 train_time:72972ms step_avg:57.96ms
step:1260/2330 train_time:73032ms step_avg:57.96ms
step:1261/2330 train_time:73088ms step_avg:57.96ms
step:1262/2330 train_time:73148ms step_avg:57.96ms
step:1263/2330 train_time:73204ms step_avg:57.96ms
step:1264/2330 train_time:73264ms step_avg:57.96ms
step:1265/2330 train_time:73320ms step_avg:57.96ms
step:1266/2330 train_time:73380ms step_avg:57.96ms
step:1267/2330 train_time:73437ms step_avg:57.96ms
step:1268/2330 train_time:73497ms step_avg:57.96ms
step:1269/2330 train_time:73555ms step_avg:57.96ms
step:1270/2330 train_time:73616ms step_avg:57.97ms
step:1271/2330 train_time:73675ms step_avg:57.97ms
step:1272/2330 train_time:73735ms step_avg:57.97ms
step:1273/2330 train_time:73793ms step_avg:57.97ms
step:1274/2330 train_time:73853ms step_avg:57.97ms
step:1275/2330 train_time:73910ms step_avg:57.97ms
step:1276/2330 train_time:73970ms step_avg:57.97ms
step:1277/2330 train_time:74026ms step_avg:57.97ms
step:1278/2330 train_time:74086ms step_avg:57.97ms
step:1279/2330 train_time:74142ms step_avg:57.97ms
step:1280/2330 train_time:74202ms step_avg:57.97ms
step:1281/2330 train_time:74258ms step_avg:57.97ms
step:1282/2330 train_time:74318ms step_avg:57.97ms
step:1283/2330 train_time:74375ms step_avg:57.97ms
step:1284/2330 train_time:74434ms step_avg:57.97ms
step:1285/2330 train_time:74491ms step_avg:57.97ms
step:1286/2330 train_time:74552ms step_avg:57.97ms
step:1287/2330 train_time:74609ms step_avg:57.97ms
step:1288/2330 train_time:74670ms step_avg:57.97ms
step:1289/2330 train_time:74727ms step_avg:57.97ms
step:1290/2330 train_time:74788ms step_avg:57.98ms
step:1291/2330 train_time:74845ms step_avg:57.97ms
step:1292/2330 train_time:74906ms step_avg:57.98ms
step:1293/2330 train_time:74963ms step_avg:57.98ms
step:1294/2330 train_time:75023ms step_avg:57.98ms
step:1295/2330 train_time:75079ms step_avg:57.98ms
step:1296/2330 train_time:75139ms step_avg:57.98ms
step:1297/2330 train_time:75196ms step_avg:57.98ms
step:1298/2330 train_time:75255ms step_avg:57.98ms
step:1299/2330 train_time:75312ms step_avg:57.98ms
step:1300/2330 train_time:75372ms step_avg:57.98ms
step:1301/2330 train_time:75428ms step_avg:57.98ms
step:1302/2330 train_time:75489ms step_avg:57.98ms
step:1303/2330 train_time:75545ms step_avg:57.98ms
step:1304/2330 train_time:75607ms step_avg:57.98ms
step:1305/2330 train_time:75664ms step_avg:57.98ms
step:1306/2330 train_time:75726ms step_avg:57.98ms
step:1307/2330 train_time:75783ms step_avg:57.98ms
step:1308/2330 train_time:75844ms step_avg:57.98ms
step:1309/2330 train_time:75901ms step_avg:57.98ms
step:1310/2330 train_time:75962ms step_avg:57.99ms
step:1311/2330 train_time:76018ms step_avg:57.98ms
step:1312/2330 train_time:76077ms step_avg:57.99ms
step:1313/2330 train_time:76134ms step_avg:57.98ms
step:1314/2330 train_time:76194ms step_avg:57.99ms
step:1315/2330 train_time:76250ms step_avg:57.98ms
step:1316/2330 train_time:76310ms step_avg:57.99ms
step:1317/2330 train_time:76367ms step_avg:57.99ms
step:1318/2330 train_time:76427ms step_avg:57.99ms
step:1319/2330 train_time:76483ms step_avg:57.99ms
step:1320/2330 train_time:76544ms step_avg:57.99ms
step:1321/2330 train_time:76601ms step_avg:57.99ms
step:1322/2330 train_time:76662ms step_avg:57.99ms
step:1323/2330 train_time:76719ms step_avg:57.99ms
step:1324/2330 train_time:76780ms step_avg:57.99ms
step:1325/2330 train_time:76837ms step_avg:57.99ms
step:1326/2330 train_time:76897ms step_avg:57.99ms
step:1327/2330 train_time:76955ms step_avg:57.99ms
step:1328/2330 train_time:77015ms step_avg:57.99ms
step:1329/2330 train_time:77072ms step_avg:57.99ms
step:1330/2330 train_time:77132ms step_avg:57.99ms
step:1331/2330 train_time:77189ms step_avg:57.99ms
step:1332/2330 train_time:77249ms step_avg:57.99ms
step:1333/2330 train_time:77305ms step_avg:57.99ms
step:1334/2330 train_time:77366ms step_avg:58.00ms
step:1335/2330 train_time:77423ms step_avg:57.99ms
step:1336/2330 train_time:77482ms step_avg:58.00ms
step:1337/2330 train_time:77539ms step_avg:57.99ms
step:1338/2330 train_time:77598ms step_avg:58.00ms
step:1339/2330 train_time:77655ms step_avg:58.00ms
step:1340/2330 train_time:77715ms step_avg:58.00ms
step:1341/2330 train_time:77772ms step_avg:58.00ms
step:1342/2330 train_time:77833ms step_avg:58.00ms
step:1343/2330 train_time:77891ms step_avg:58.00ms
step:1344/2330 train_time:77951ms step_avg:58.00ms
step:1345/2330 train_time:78008ms step_avg:58.00ms
step:1346/2330 train_time:78068ms step_avg:58.00ms
step:1347/2330 train_time:78124ms step_avg:58.00ms
step:1348/2330 train_time:78184ms step_avg:58.00ms
step:1349/2330 train_time:78240ms step_avg:58.00ms
step:1350/2330 train_time:78301ms step_avg:58.00ms
step:1351/2330 train_time:78358ms step_avg:58.00ms
step:1352/2330 train_time:78418ms step_avg:58.00ms
step:1353/2330 train_time:78475ms step_avg:58.00ms
step:1354/2330 train_time:78535ms step_avg:58.00ms
step:1355/2330 train_time:78593ms step_avg:58.00ms
step:1356/2330 train_time:78653ms step_avg:58.00ms
step:1357/2330 train_time:78709ms step_avg:58.00ms
step:1358/2330 train_time:78770ms step_avg:58.00ms
step:1359/2330 train_time:78827ms step_avg:58.00ms
step:1360/2330 train_time:78887ms step_avg:58.00ms
step:1361/2330 train_time:78943ms step_avg:58.00ms
step:1362/2330 train_time:79004ms step_avg:58.01ms
step:1363/2330 train_time:79061ms step_avg:58.00ms
step:1364/2330 train_time:79122ms step_avg:58.01ms
step:1365/2330 train_time:79178ms step_avg:58.01ms
step:1366/2330 train_time:79239ms step_avg:58.01ms
step:1367/2330 train_time:79296ms step_avg:58.01ms
step:1368/2330 train_time:79355ms step_avg:58.01ms
step:1369/2330 train_time:79413ms step_avg:58.01ms
step:1370/2330 train_time:79472ms step_avg:58.01ms
step:1371/2330 train_time:79530ms step_avg:58.01ms
step:1372/2330 train_time:79590ms step_avg:58.01ms
step:1373/2330 train_time:79647ms step_avg:58.01ms
step:1374/2330 train_time:79707ms step_avg:58.01ms
step:1375/2330 train_time:79763ms step_avg:58.01ms
step:1376/2330 train_time:79824ms step_avg:58.01ms
step:1377/2330 train_time:79881ms step_avg:58.01ms
step:1378/2330 train_time:79941ms step_avg:58.01ms
step:1379/2330 train_time:79997ms step_avg:58.01ms
step:1380/2330 train_time:80058ms step_avg:58.01ms
step:1381/2330 train_time:80115ms step_avg:58.01ms
step:1382/2330 train_time:80174ms step_avg:58.01ms
step:1383/2330 train_time:80231ms step_avg:58.01ms
step:1384/2330 train_time:80291ms step_avg:58.01ms
step:1385/2330 train_time:80347ms step_avg:58.01ms
step:1386/2330 train_time:80408ms step_avg:58.01ms
step:1387/2330 train_time:80464ms step_avg:58.01ms
step:1388/2330 train_time:80526ms step_avg:58.02ms
step:1389/2330 train_time:80582ms step_avg:58.01ms
step:1390/2330 train_time:80643ms step_avg:58.02ms
step:1391/2330 train_time:80700ms step_avg:58.02ms
step:1392/2330 train_time:80760ms step_avg:58.02ms
step:1393/2330 train_time:80817ms step_avg:58.02ms
step:1394/2330 train_time:80877ms step_avg:58.02ms
step:1395/2330 train_time:80934ms step_avg:58.02ms
step:1396/2330 train_time:80994ms step_avg:58.02ms
step:1397/2330 train_time:81051ms step_avg:58.02ms
step:1398/2330 train_time:81111ms step_avg:58.02ms
step:1399/2330 train_time:81168ms step_avg:58.02ms
step:1400/2330 train_time:81229ms step_avg:58.02ms
step:1401/2330 train_time:81285ms step_avg:58.02ms
step:1402/2330 train_time:81346ms step_avg:58.02ms
step:1403/2330 train_time:81402ms step_avg:58.02ms
step:1404/2330 train_time:81463ms step_avg:58.02ms
step:1405/2330 train_time:81519ms step_avg:58.02ms
step:1406/2330 train_time:81580ms step_avg:58.02ms
step:1407/2330 train_time:81637ms step_avg:58.02ms
step:1408/2330 train_time:81697ms step_avg:58.02ms
step:1409/2330 train_time:81754ms step_avg:58.02ms
step:1410/2330 train_time:81814ms step_avg:58.02ms
step:1411/2330 train_time:81871ms step_avg:58.02ms
step:1412/2330 train_time:81931ms step_avg:58.02ms
step:1413/2330 train_time:81987ms step_avg:58.02ms
step:1414/2330 train_time:82048ms step_avg:58.03ms
step:1415/2330 train_time:82104ms step_avg:58.02ms
step:1416/2330 train_time:82165ms step_avg:58.03ms
step:1417/2330 train_time:82222ms step_avg:58.03ms
step:1418/2330 train_time:82281ms step_avg:58.03ms
step:1419/2330 train_time:82338ms step_avg:58.03ms
step:1420/2330 train_time:82398ms step_avg:58.03ms
step:1421/2330 train_time:82455ms step_avg:58.03ms
step:1422/2330 train_time:82515ms step_avg:58.03ms
step:1423/2330 train_time:82573ms step_avg:58.03ms
step:1424/2330 train_time:82632ms step_avg:58.03ms
step:1425/2330 train_time:82689ms step_avg:58.03ms
step:1426/2330 train_time:82749ms step_avg:58.03ms
step:1427/2330 train_time:82806ms step_avg:58.03ms
step:1428/2330 train_time:82867ms step_avg:58.03ms
step:1429/2330 train_time:82923ms step_avg:58.03ms
step:1430/2330 train_time:82983ms step_avg:58.03ms
step:1431/2330 train_time:83040ms step_avg:58.03ms
step:1432/2330 train_time:83100ms step_avg:58.03ms
step:1433/2330 train_time:83157ms step_avg:58.03ms
step:1434/2330 train_time:83217ms step_avg:58.03ms
step:1435/2330 train_time:83274ms step_avg:58.03ms
step:1436/2330 train_time:83335ms step_avg:58.03ms
step:1437/2330 train_time:83391ms step_avg:58.03ms
step:1438/2330 train_time:83451ms step_avg:58.03ms
step:1439/2330 train_time:83508ms step_avg:58.03ms
step:1440/2330 train_time:83569ms step_avg:58.03ms
step:1441/2330 train_time:83625ms step_avg:58.03ms
step:1442/2330 train_time:83685ms step_avg:58.03ms
step:1443/2330 train_time:83742ms step_avg:58.03ms
step:1444/2330 train_time:83803ms step_avg:58.04ms
step:1445/2330 train_time:83859ms step_avg:58.03ms
step:1446/2330 train_time:83919ms step_avg:58.04ms
step:1447/2330 train_time:83976ms step_avg:58.03ms
step:1448/2330 train_time:84036ms step_avg:58.04ms
step:1449/2330 train_time:84092ms step_avg:58.03ms
step:1450/2330 train_time:84153ms step_avg:58.04ms
step:1451/2330 train_time:84209ms step_avg:58.04ms
step:1452/2330 train_time:84270ms step_avg:58.04ms
step:1453/2330 train_time:84327ms step_avg:58.04ms
step:1454/2330 train_time:84388ms step_avg:58.04ms
step:1455/2330 train_time:84445ms step_avg:58.04ms
step:1456/2330 train_time:84506ms step_avg:58.04ms
step:1457/2330 train_time:84563ms step_avg:58.04ms
step:1458/2330 train_time:84623ms step_avg:58.04ms
step:1459/2330 train_time:84679ms step_avg:58.04ms
step:1460/2330 train_time:84740ms step_avg:58.04ms
step:1461/2330 train_time:84797ms step_avg:58.04ms
step:1462/2330 train_time:84857ms step_avg:58.04ms
step:1463/2330 train_time:84914ms step_avg:58.04ms
step:1464/2330 train_time:84974ms step_avg:58.04ms
step:1465/2330 train_time:85031ms step_avg:58.04ms
step:1466/2330 train_time:85091ms step_avg:58.04ms
step:1467/2330 train_time:85148ms step_avg:58.04ms
step:1468/2330 train_time:85208ms step_avg:58.04ms
step:1469/2330 train_time:85264ms step_avg:58.04ms
step:1470/2330 train_time:85326ms step_avg:58.04ms
step:1471/2330 train_time:85382ms step_avg:58.04ms
step:1472/2330 train_time:85444ms step_avg:58.05ms
step:1473/2330 train_time:85500ms step_avg:58.04ms
step:1474/2330 train_time:85561ms step_avg:58.05ms
step:1475/2330 train_time:85617ms step_avg:58.05ms
step:1476/2330 train_time:85677ms step_avg:58.05ms
step:1477/2330 train_time:85734ms step_avg:58.05ms
step:1478/2330 train_time:85794ms step_avg:58.05ms
step:1479/2330 train_time:85850ms step_avg:58.05ms
step:1480/2330 train_time:85910ms step_avg:58.05ms
step:1481/2330 train_time:85967ms step_avg:58.05ms
step:1482/2330 train_time:86027ms step_avg:58.05ms
step:1483/2330 train_time:86083ms step_avg:58.05ms
step:1484/2330 train_time:86144ms step_avg:58.05ms
step:1485/2330 train_time:86201ms step_avg:58.05ms
step:1486/2330 train_time:86261ms step_avg:58.05ms
step:1487/2330 train_time:86318ms step_avg:58.05ms
step:1488/2330 train_time:86377ms step_avg:58.05ms
step:1489/2330 train_time:86435ms step_avg:58.05ms
step:1490/2330 train_time:86495ms step_avg:58.05ms
step:1491/2330 train_time:86552ms step_avg:58.05ms
step:1492/2330 train_time:86613ms step_avg:58.05ms
step:1493/2330 train_time:86670ms step_avg:58.05ms
step:1494/2330 train_time:86730ms step_avg:58.05ms
step:1495/2330 train_time:86786ms step_avg:58.05ms
step:1496/2330 train_time:86847ms step_avg:58.05ms
step:1497/2330 train_time:86903ms step_avg:58.05ms
step:1498/2330 train_time:86964ms step_avg:58.05ms
step:1499/2330 train_time:87021ms step_avg:58.05ms
step:1500/2330 train_time:87081ms step_avg:58.05ms
step:1500/2330 val_loss:5.2570 train_time:87161ms step_avg:58.11ms
step:1501/2330 train_time:87180ms step_avg:58.08ms
step:1502/2330 train_time:87200ms step_avg:58.06ms
step:1503/2330 train_time:87259ms step_avg:58.06ms
step:1504/2330 train_time:87324ms step_avg:58.06ms
step:1505/2330 train_time:87381ms step_avg:58.06ms
step:1506/2330 train_time:87443ms step_avg:58.06ms
step:1507/2330 train_time:87500ms step_avg:58.06ms
step:1508/2330 train_time:87560ms step_avg:58.06ms
step:1509/2330 train_time:87616ms step_avg:58.06ms
step:1510/2330 train_time:87676ms step_avg:58.06ms
step:1511/2330 train_time:87733ms step_avg:58.06ms
step:1512/2330 train_time:87792ms step_avg:58.06ms
step:1513/2330 train_time:87848ms step_avg:58.06ms
step:1514/2330 train_time:87907ms step_avg:58.06ms
step:1515/2330 train_time:87963ms step_avg:58.06ms
step:1516/2330 train_time:88023ms step_avg:58.06ms
step:1517/2330 train_time:88079ms step_avg:58.06ms
step:1518/2330 train_time:88140ms step_avg:58.06ms
step:1519/2330 train_time:88198ms step_avg:58.06ms
step:1520/2330 train_time:88259ms step_avg:58.07ms
step:1521/2330 train_time:88317ms step_avg:58.07ms
step:1522/2330 train_time:88378ms step_avg:58.07ms
step:1523/2330 train_time:88436ms step_avg:58.07ms
step:1524/2330 train_time:88496ms step_avg:58.07ms
step:1525/2330 train_time:88553ms step_avg:58.07ms
step:1526/2330 train_time:88613ms step_avg:58.07ms
step:1527/2330 train_time:88669ms step_avg:58.07ms
step:1528/2330 train_time:88729ms step_avg:58.07ms
step:1529/2330 train_time:88787ms step_avg:58.07ms
step:1530/2330 train_time:88845ms step_avg:58.07ms
step:1531/2330 train_time:88901ms step_avg:58.07ms
step:1532/2330 train_time:88962ms step_avg:58.07ms
step:1533/2330 train_time:89019ms step_avg:58.07ms
step:1534/2330 train_time:89079ms step_avg:58.07ms
step:1535/2330 train_time:89137ms step_avg:58.07ms
step:1536/2330 train_time:89198ms step_avg:58.07ms
step:1537/2330 train_time:89255ms step_avg:58.07ms
step:1538/2330 train_time:89318ms step_avg:58.07ms
step:1539/2330 train_time:89375ms step_avg:58.07ms
step:1540/2330 train_time:89436ms step_avg:58.08ms
step:1541/2330 train_time:89495ms step_avg:58.08ms
step:1542/2330 train_time:89555ms step_avg:58.08ms
step:1543/2330 train_time:89613ms step_avg:58.08ms
step:1544/2330 train_time:89674ms step_avg:58.08ms
step:1545/2330 train_time:89731ms step_avg:58.08ms
step:1546/2330 train_time:89791ms step_avg:58.08ms
step:1547/2330 train_time:89848ms step_avg:58.08ms
step:1548/2330 train_time:89908ms step_avg:58.08ms
step:1549/2330 train_time:89965ms step_avg:58.08ms
step:1550/2330 train_time:90026ms step_avg:58.08ms
step:1551/2330 train_time:90084ms step_avg:58.08ms
step:1552/2330 train_time:90144ms step_avg:58.08ms
step:1553/2330 train_time:90201ms step_avg:58.08ms
step:1554/2330 train_time:90264ms step_avg:58.08ms
step:1555/2330 train_time:90320ms step_avg:58.08ms
step:1556/2330 train_time:90381ms step_avg:58.09ms
step:1557/2330 train_time:90438ms step_avg:58.09ms
step:1558/2330 train_time:90499ms step_avg:58.09ms
step:1559/2330 train_time:90557ms step_avg:58.09ms
step:1560/2330 train_time:90617ms step_avg:58.09ms
step:1561/2330 train_time:90676ms step_avg:58.09ms
step:1562/2330 train_time:90736ms step_avg:58.09ms
step:1563/2330 train_time:90794ms step_avg:58.09ms
step:1564/2330 train_time:90854ms step_avg:58.09ms
step:1565/2330 train_time:90911ms step_avg:58.09ms
step:1566/2330 train_time:90972ms step_avg:58.09ms
step:1567/2330 train_time:91030ms step_avg:58.09ms
step:1568/2330 train_time:91091ms step_avg:58.09ms
step:1569/2330 train_time:91148ms step_avg:58.09ms
step:1570/2330 train_time:91210ms step_avg:58.10ms
step:1571/2330 train_time:91267ms step_avg:58.09ms
step:1572/2330 train_time:91330ms step_avg:58.10ms
step:1573/2330 train_time:91386ms step_avg:58.10ms
step:1574/2330 train_time:91448ms step_avg:58.10ms
step:1575/2330 train_time:91504ms step_avg:58.10ms
step:1576/2330 train_time:91568ms step_avg:58.10ms
step:1577/2330 train_time:91624ms step_avg:58.10ms
step:1578/2330 train_time:91686ms step_avg:58.10ms
step:1579/2330 train_time:91743ms step_avg:58.10ms
step:1580/2330 train_time:91804ms step_avg:58.10ms
step:1581/2330 train_time:91860ms step_avg:58.10ms
step:1582/2330 train_time:91921ms step_avg:58.10ms
step:1583/2330 train_time:91978ms step_avg:58.10ms
step:1584/2330 train_time:92039ms step_avg:58.11ms
step:1585/2330 train_time:92096ms step_avg:58.10ms
step:1586/2330 train_time:92156ms step_avg:58.11ms
step:1587/2330 train_time:92213ms step_avg:58.11ms
step:1588/2330 train_time:92275ms step_avg:58.11ms
step:1589/2330 train_time:92332ms step_avg:58.11ms
step:1590/2330 train_time:92394ms step_avg:58.11ms
step:1591/2330 train_time:92451ms step_avg:58.11ms
step:1592/2330 train_time:92511ms step_avg:58.11ms
step:1593/2330 train_time:92569ms step_avg:58.11ms
step:1594/2330 train_time:92630ms step_avg:58.11ms
step:1595/2330 train_time:92688ms step_avg:58.11ms
step:1596/2330 train_time:92749ms step_avg:58.11ms
step:1597/2330 train_time:92805ms step_avg:58.11ms
step:1598/2330 train_time:92867ms step_avg:58.11ms
step:1599/2330 train_time:92923ms step_avg:58.11ms
step:1600/2330 train_time:92985ms step_avg:58.12ms
step:1601/2330 train_time:93043ms step_avg:58.12ms
step:1602/2330 train_time:93102ms step_avg:58.12ms
step:1603/2330 train_time:93160ms step_avg:58.12ms
step:1604/2330 train_time:93219ms step_avg:58.12ms
step:1605/2330 train_time:93278ms step_avg:58.12ms
step:1606/2330 train_time:93337ms step_avg:58.12ms
step:1607/2330 train_time:93395ms step_avg:58.12ms
step:1608/2330 train_time:93455ms step_avg:58.12ms
step:1609/2330 train_time:93513ms step_avg:58.12ms
step:1610/2330 train_time:93573ms step_avg:58.12ms
step:1611/2330 train_time:93631ms step_avg:58.12ms
step:1612/2330 train_time:93692ms step_avg:58.12ms
step:1613/2330 train_time:93749ms step_avg:58.12ms
step:1614/2330 train_time:93810ms step_avg:58.12ms
step:1615/2330 train_time:93867ms step_avg:58.12ms
step:1616/2330 train_time:93929ms step_avg:58.12ms
step:1617/2330 train_time:93986ms step_avg:58.12ms
step:1618/2330 train_time:94048ms step_avg:58.13ms
step:1619/2330 train_time:94104ms step_avg:58.12ms
step:1620/2330 train_time:94165ms step_avg:58.13ms
step:1621/2330 train_time:94222ms step_avg:58.13ms
step:1622/2330 train_time:94284ms step_avg:58.13ms
step:1623/2330 train_time:94340ms step_avg:58.13ms
step:1624/2330 train_time:94402ms step_avg:58.13ms
step:1625/2330 train_time:94459ms step_avg:58.13ms
step:1626/2330 train_time:94519ms step_avg:58.13ms
step:1627/2330 train_time:94577ms step_avg:58.13ms
step:1628/2330 train_time:94638ms step_avg:58.13ms
step:1629/2330 train_time:94696ms step_avg:58.13ms
step:1630/2330 train_time:94756ms step_avg:58.13ms
step:1631/2330 train_time:94813ms step_avg:58.13ms
step:1632/2330 train_time:94874ms step_avg:58.13ms
step:1633/2330 train_time:94932ms step_avg:58.13ms
step:1634/2330 train_time:94992ms step_avg:58.13ms
step:1635/2330 train_time:95050ms step_avg:58.13ms
step:1636/2330 train_time:95111ms step_avg:58.14ms
step:1637/2330 train_time:95168ms step_avg:58.14ms
step:1638/2330 train_time:95229ms step_avg:58.14ms
step:1639/2330 train_time:95286ms step_avg:58.14ms
step:1640/2330 train_time:95348ms step_avg:58.14ms
step:1641/2330 train_time:95404ms step_avg:58.14ms
step:1642/2330 train_time:95466ms step_avg:58.14ms
step:1643/2330 train_time:95523ms step_avg:58.14ms
step:1644/2330 train_time:95586ms step_avg:58.14ms
step:1645/2330 train_time:95643ms step_avg:58.14ms
step:1646/2330 train_time:95703ms step_avg:58.14ms
step:1647/2330 train_time:95760ms step_avg:58.14ms
step:1648/2330 train_time:95821ms step_avg:58.14ms
step:1649/2330 train_time:95879ms step_avg:58.14ms
step:1650/2330 train_time:95939ms step_avg:58.14ms
step:1651/2330 train_time:95998ms step_avg:58.15ms
step:1652/2330 train_time:96058ms step_avg:58.15ms
step:1653/2330 train_time:96115ms step_avg:58.15ms
step:1654/2330 train_time:96176ms step_avg:58.15ms
step:1655/2330 train_time:96234ms step_avg:58.15ms
step:1656/2330 train_time:96294ms step_avg:58.15ms
step:1657/2330 train_time:96352ms step_avg:58.15ms
step:1658/2330 train_time:96413ms step_avg:58.15ms
step:1659/2330 train_time:96470ms step_avg:58.15ms
step:1660/2330 train_time:96531ms step_avg:58.15ms
step:1661/2330 train_time:96588ms step_avg:58.15ms
step:1662/2330 train_time:96649ms step_avg:58.15ms
step:1663/2330 train_time:96705ms step_avg:58.15ms
step:1664/2330 train_time:96768ms step_avg:58.15ms
step:1665/2330 train_time:96824ms step_avg:58.15ms
step:1666/2330 train_time:96886ms step_avg:58.15ms
step:1667/2330 train_time:96942ms step_avg:58.15ms
step:1668/2330 train_time:97004ms step_avg:58.16ms
step:1669/2330 train_time:97061ms step_avg:58.15ms
step:1670/2330 train_time:97121ms step_avg:58.16ms
step:1671/2330 train_time:97179ms step_avg:58.16ms
step:1672/2330 train_time:97239ms step_avg:58.16ms
step:1673/2330 train_time:97297ms step_avg:58.16ms
step:1674/2330 train_time:97357ms step_avg:58.16ms
step:1675/2330 train_time:97414ms step_avg:58.16ms
step:1676/2330 train_time:97476ms step_avg:58.16ms
step:1677/2330 train_time:97533ms step_avg:58.16ms
step:1678/2330 train_time:97594ms step_avg:58.16ms
step:1679/2330 train_time:97652ms step_avg:58.16ms
step:1680/2330 train_time:97712ms step_avg:58.16ms
step:1681/2330 train_time:97770ms step_avg:58.16ms
step:1682/2330 train_time:97830ms step_avg:58.16ms
step:1683/2330 train_time:97887ms step_avg:58.16ms
step:1684/2330 train_time:97949ms step_avg:58.16ms
step:1685/2330 train_time:98006ms step_avg:58.16ms
step:1686/2330 train_time:98068ms step_avg:58.17ms
step:1687/2330 train_time:98124ms step_avg:58.16ms
step:1688/2330 train_time:98187ms step_avg:58.17ms
step:1689/2330 train_time:98243ms step_avg:58.17ms
step:1690/2330 train_time:98304ms step_avg:58.17ms
step:1691/2330 train_time:98361ms step_avg:58.17ms
step:1692/2330 train_time:98422ms step_avg:58.17ms
step:1693/2330 train_time:98479ms step_avg:58.17ms
step:1694/2330 train_time:98539ms step_avg:58.17ms
step:1695/2330 train_time:98597ms step_avg:58.17ms
step:1696/2330 train_time:98657ms step_avg:58.17ms
step:1697/2330 train_time:98715ms step_avg:58.17ms
step:1698/2330 train_time:98775ms step_avg:58.17ms
step:1699/2330 train_time:98833ms step_avg:58.17ms
step:1700/2330 train_time:98893ms step_avg:58.17ms
step:1701/2330 train_time:98951ms step_avg:58.17ms
step:1702/2330 train_time:99012ms step_avg:58.17ms
step:1703/2330 train_time:99069ms step_avg:58.17ms
step:1704/2330 train_time:99131ms step_avg:58.18ms
step:1705/2330 train_time:99188ms step_avg:58.17ms
step:1706/2330 train_time:99251ms step_avg:58.18ms
step:1707/2330 train_time:99307ms step_avg:58.18ms
step:1708/2330 train_time:99370ms step_avg:58.18ms
step:1709/2330 train_time:99426ms step_avg:58.18ms
step:1710/2330 train_time:99489ms step_avg:58.18ms
step:1711/2330 train_time:99545ms step_avg:58.18ms
step:1712/2330 train_time:99607ms step_avg:58.18ms
step:1713/2330 train_time:99663ms step_avg:58.18ms
step:1714/2330 train_time:99724ms step_avg:58.18ms
step:1715/2330 train_time:99780ms step_avg:58.18ms
step:1716/2330 train_time:99842ms step_avg:58.18ms
step:1717/2330 train_time:99899ms step_avg:58.18ms
step:1718/2330 train_time:99959ms step_avg:58.18ms
step:1719/2330 train_time:100017ms step_avg:58.18ms
step:1720/2330 train_time:100077ms step_avg:58.18ms
step:1721/2330 train_time:100134ms step_avg:58.18ms
step:1722/2330 train_time:100194ms step_avg:58.18ms
step:1723/2330 train_time:100252ms step_avg:58.18ms
step:1724/2330 train_time:100314ms step_avg:58.19ms
step:1725/2330 train_time:100371ms step_avg:58.19ms
step:1726/2330 train_time:100432ms step_avg:58.19ms
step:1727/2330 train_time:100490ms step_avg:58.19ms
step:1728/2330 train_time:100550ms step_avg:58.19ms
step:1729/2330 train_time:100607ms step_avg:58.19ms
step:1730/2330 train_time:100669ms step_avg:58.19ms
step:1731/2330 train_time:100725ms step_avg:58.19ms
step:1732/2330 train_time:100788ms step_avg:58.19ms
step:1733/2330 train_time:100844ms step_avg:58.19ms
step:1734/2330 train_time:100906ms step_avg:58.19ms
step:1735/2330 train_time:100962ms step_avg:58.19ms
step:1736/2330 train_time:101024ms step_avg:58.19ms
step:1737/2330 train_time:101082ms step_avg:58.19ms
step:1738/2330 train_time:101142ms step_avg:58.19ms
step:1739/2330 train_time:101198ms step_avg:58.19ms
step:1740/2330 train_time:101259ms step_avg:58.19ms
step:1741/2330 train_time:101316ms step_avg:58.19ms
step:1742/2330 train_time:101377ms step_avg:58.20ms
step:1743/2330 train_time:101435ms step_avg:58.20ms
step:1744/2330 train_time:101495ms step_avg:58.20ms
step:1745/2330 train_time:101552ms step_avg:58.20ms
step:1746/2330 train_time:101613ms step_avg:58.20ms
step:1747/2330 train_time:101670ms step_avg:58.20ms
step:1748/2330 train_time:101732ms step_avg:58.20ms
step:1749/2330 train_time:101790ms step_avg:58.20ms
step:1750/2330 train_time:101851ms step_avg:58.20ms
step:1750/2330 val_loss:5.1446 train_time:101934ms step_avg:58.25ms
step:1751/2330 train_time:101953ms step_avg:58.23ms
step:1752/2330 train_time:101972ms step_avg:58.20ms
step:1753/2330 train_time:102029ms step_avg:58.20ms
step:1754/2330 train_time:102093ms step_avg:58.21ms
step:1755/2330 train_time:102150ms step_avg:58.20ms
step:1756/2330 train_time:102212ms step_avg:58.21ms
step:1757/2330 train_time:102268ms step_avg:58.21ms
step:1758/2330 train_time:102328ms step_avg:58.21ms
step:1759/2330 train_time:102384ms step_avg:58.21ms
step:1760/2330 train_time:102444ms step_avg:58.21ms
step:1761/2330 train_time:102500ms step_avg:58.21ms
step:1762/2330 train_time:102560ms step_avg:58.21ms
step:1763/2330 train_time:102617ms step_avg:58.21ms
step:1764/2330 train_time:102677ms step_avg:58.21ms
step:1765/2330 train_time:102733ms step_avg:58.21ms
step:1766/2330 train_time:102793ms step_avg:58.21ms
step:1767/2330 train_time:102850ms step_avg:58.21ms
step:1768/2330 train_time:102915ms step_avg:58.21ms
step:1769/2330 train_time:102972ms step_avg:58.21ms
step:1770/2330 train_time:103038ms step_avg:58.21ms
step:1771/2330 train_time:103095ms step_avg:58.21ms
step:1772/2330 train_time:103159ms step_avg:58.22ms
step:1773/2330 train_time:103215ms step_avg:58.21ms
step:1774/2330 train_time:103277ms step_avg:58.22ms
step:1775/2330 train_time:103333ms step_avg:58.22ms
step:1776/2330 train_time:103396ms step_avg:58.22ms
step:1777/2330 train_time:103452ms step_avg:58.22ms
step:1778/2330 train_time:103514ms step_avg:58.22ms
step:1779/2330 train_time:103570ms step_avg:58.22ms
step:1780/2330 train_time:103631ms step_avg:58.22ms
step:1781/2330 train_time:103687ms step_avg:58.22ms
step:1782/2330 train_time:103747ms step_avg:58.22ms
step:1783/2330 train_time:103804ms step_avg:58.22ms
step:1784/2330 train_time:103865ms step_avg:58.22ms
step:1785/2330 train_time:103923ms step_avg:58.22ms
step:1786/2330 train_time:103984ms step_avg:58.22ms
step:1787/2330 train_time:104042ms step_avg:58.22ms
step:1788/2330 train_time:104103ms step_avg:58.22ms
step:1789/2330 train_time:104160ms step_avg:58.22ms
step:1790/2330 train_time:104221ms step_avg:58.22ms
step:1791/2330 train_time:104279ms step_avg:58.22ms
step:1792/2330 train_time:104340ms step_avg:58.23ms
step:1793/2330 train_time:104397ms step_avg:58.22ms
step:1794/2330 train_time:104458ms step_avg:58.23ms
step:1795/2330 train_time:104515ms step_avg:58.23ms
step:1796/2330 train_time:104574ms step_avg:58.23ms
step:1797/2330 train_time:104631ms step_avg:58.23ms
step:1798/2330 train_time:104693ms step_avg:58.23ms
step:1799/2330 train_time:104749ms step_avg:58.23ms
step:1800/2330 train_time:104811ms step_avg:58.23ms
step:1801/2330 train_time:104868ms step_avg:58.23ms
step:1802/2330 train_time:104931ms step_avg:58.23ms
step:1803/2330 train_time:104988ms step_avg:58.23ms
step:1804/2330 train_time:105051ms step_avg:58.23ms
step:1805/2330 train_time:105107ms step_avg:58.23ms
step:1806/2330 train_time:105170ms step_avg:58.23ms
step:1807/2330 train_time:105227ms step_avg:58.23ms
step:1808/2330 train_time:105290ms step_avg:58.24ms
step:1809/2330 train_time:105346ms step_avg:58.23ms
step:1810/2330 train_time:105407ms step_avg:58.24ms
step:1811/2330 train_time:105464ms step_avg:58.24ms
step:1812/2330 train_time:105525ms step_avg:58.24ms
step:1813/2330 train_time:105582ms step_avg:58.24ms
step:1814/2330 train_time:105642ms step_avg:58.24ms
step:1815/2330 train_time:105700ms step_avg:58.24ms
step:1816/2330 train_time:105759ms step_avg:58.24ms
step:1817/2330 train_time:105817ms step_avg:58.24ms
step:1818/2330 train_time:105878ms step_avg:58.24ms
step:1819/2330 train_time:105935ms step_avg:58.24ms
step:1820/2330 train_time:105997ms step_avg:58.24ms
step:1821/2330 train_time:106054ms step_avg:58.24ms
step:1822/2330 train_time:106115ms step_avg:58.24ms
step:1823/2330 train_time:106172ms step_avg:58.24ms
step:1824/2330 train_time:106234ms step_avg:58.24ms
step:1825/2330 train_time:106291ms step_avg:58.24ms
step:1826/2330 train_time:106353ms step_avg:58.24ms
step:1827/2330 train_time:106409ms step_avg:58.24ms
step:1828/2330 train_time:106471ms step_avg:58.24ms
step:1829/2330 train_time:106527ms step_avg:58.24ms
step:1830/2330 train_time:106589ms step_avg:58.25ms
step:1831/2330 train_time:106645ms step_avg:58.24ms
step:1832/2330 train_time:106706ms step_avg:58.25ms
step:1833/2330 train_time:106763ms step_avg:58.24ms
step:1834/2330 train_time:106823ms step_avg:58.25ms
step:1835/2330 train_time:106880ms step_avg:58.25ms
step:1836/2330 train_time:106941ms step_avg:58.25ms
step:1837/2330 train_time:106999ms step_avg:58.25ms
step:1838/2330 train_time:107059ms step_avg:58.25ms
step:1839/2330 train_time:107117ms step_avg:58.25ms
step:1840/2330 train_time:107178ms step_avg:58.25ms
step:1841/2330 train_time:107235ms step_avg:58.25ms
step:1842/2330 train_time:107297ms step_avg:58.25ms
step:1843/2330 train_time:107354ms step_avg:58.25ms
step:1844/2330 train_time:107415ms step_avg:58.25ms
step:1845/2330 train_time:107472ms step_avg:58.25ms
step:1846/2330 train_time:107533ms step_avg:58.25ms
step:1847/2330 train_time:107589ms step_avg:58.25ms
step:1848/2330 train_time:107651ms step_avg:58.25ms
step:1849/2330 train_time:107708ms step_avg:58.25ms
step:1850/2330 train_time:107769ms step_avg:58.25ms
step:1851/2330 train_time:107825ms step_avg:58.25ms
step:1852/2330 train_time:107886ms step_avg:58.25ms
step:1853/2330 train_time:107943ms step_avg:58.25ms
step:1854/2330 train_time:108004ms step_avg:58.25ms
step:1855/2330 train_time:108061ms step_avg:58.25ms
step:1856/2330 train_time:108121ms step_avg:58.26ms
step:1857/2330 train_time:108179ms step_avg:58.25ms
step:1858/2330 train_time:108239ms step_avg:58.26ms
step:1859/2330 train_time:108297ms step_avg:58.26ms
step:1860/2330 train_time:108358ms step_avg:58.26ms
step:1861/2330 train_time:108416ms step_avg:58.26ms
step:1862/2330 train_time:108477ms step_avg:58.26ms
step:1863/2330 train_time:108534ms step_avg:58.26ms
step:1864/2330 train_time:108595ms step_avg:58.26ms
step:1865/2330 train_time:108652ms step_avg:58.26ms
step:1866/2330 train_time:108714ms step_avg:58.26ms
step:1867/2330 train_time:108770ms step_avg:58.26ms
step:1868/2330 train_time:108833ms step_avg:58.26ms
step:1869/2330 train_time:108889ms step_avg:58.26ms
step:1870/2330 train_time:108950ms step_avg:58.26ms
step:1871/2330 train_time:109006ms step_avg:58.26ms
step:1872/2330 train_time:109068ms step_avg:58.26ms
step:1873/2330 train_time:109125ms step_avg:58.26ms
step:1874/2330 train_time:109187ms step_avg:58.26ms
step:1875/2330 train_time:109243ms step_avg:58.26ms
step:1876/2330 train_time:109304ms step_avg:58.26ms
step:1877/2330 train_time:109361ms step_avg:58.26ms
step:1878/2330 train_time:109421ms step_avg:58.26ms
step:1879/2330 train_time:109479ms step_avg:58.26ms
step:1880/2330 train_time:109539ms step_avg:58.27ms
step:1881/2330 train_time:109597ms step_avg:58.27ms
step:1882/2330 train_time:109658ms step_avg:58.27ms
step:1883/2330 train_time:109714ms step_avg:58.27ms
step:1884/2330 train_time:109776ms step_avg:58.27ms
step:1885/2330 train_time:109833ms step_avg:58.27ms
step:1886/2330 train_time:109894ms step_avg:58.27ms
step:1887/2330 train_time:109950ms step_avg:58.27ms
step:1888/2330 train_time:110012ms step_avg:58.27ms
step:1889/2330 train_time:110068ms step_avg:58.27ms
step:1890/2330 train_time:110130ms step_avg:58.27ms
step:1891/2330 train_time:110186ms step_avg:58.27ms
step:1892/2330 train_time:110249ms step_avg:58.27ms
step:1893/2330 train_time:110305ms step_avg:58.27ms
step:1894/2330 train_time:110366ms step_avg:58.27ms
step:1895/2330 train_time:110423ms step_avg:58.27ms
step:1896/2330 train_time:110483ms step_avg:58.27ms
step:1897/2330 train_time:110541ms step_avg:58.27ms
step:1898/2330 train_time:110601ms step_avg:58.27ms
step:1899/2330 train_time:110658ms step_avg:58.27ms
step:1900/2330 train_time:110719ms step_avg:58.27ms
step:1901/2330 train_time:110777ms step_avg:58.27ms
step:1902/2330 train_time:110837ms step_avg:58.27ms
step:1903/2330 train_time:110894ms step_avg:58.27ms
step:1904/2330 train_time:110956ms step_avg:58.28ms
step:1905/2330 train_time:111013ms step_avg:58.27ms
step:1906/2330 train_time:111075ms step_avg:58.28ms
step:1907/2330 train_time:111131ms step_avg:58.28ms
step:1908/2330 train_time:111195ms step_avg:58.28ms
step:1909/2330 train_time:111250ms step_avg:58.28ms
step:1910/2330 train_time:111314ms step_avg:58.28ms
step:1911/2330 train_time:111370ms step_avg:58.28ms
step:1912/2330 train_time:111432ms step_avg:58.28ms
step:1913/2330 train_time:111488ms step_avg:58.28ms
step:1914/2330 train_time:111551ms step_avg:58.28ms
step:1915/2330 train_time:111607ms step_avg:58.28ms
step:1916/2330 train_time:111669ms step_avg:58.28ms
step:1917/2330 train_time:111726ms step_avg:58.28ms
step:1918/2330 train_time:111786ms step_avg:58.28ms
step:1919/2330 train_time:111843ms step_avg:58.28ms
step:1920/2330 train_time:111903ms step_avg:58.28ms
step:1921/2330 train_time:111961ms step_avg:58.28ms
step:1922/2330 train_time:112022ms step_avg:58.28ms
step:1923/2330 train_time:112080ms step_avg:58.28ms
step:1924/2330 train_time:112141ms step_avg:58.29ms
step:1925/2330 train_time:112198ms step_avg:58.28ms
step:1926/2330 train_time:112259ms step_avg:58.29ms
step:1927/2330 train_time:112315ms step_avg:58.28ms
step:1928/2330 train_time:112377ms step_avg:58.29ms
step:1929/2330 train_time:112435ms step_avg:58.29ms
step:1930/2330 train_time:112497ms step_avg:58.29ms
step:1931/2330 train_time:112553ms step_avg:58.29ms
step:1932/2330 train_time:112615ms step_avg:58.29ms
step:1933/2330 train_time:112671ms step_avg:58.29ms
step:1934/2330 train_time:112733ms step_avg:58.29ms
step:1935/2330 train_time:112790ms step_avg:58.29ms
step:1936/2330 train_time:112852ms step_avg:58.29ms
step:1937/2330 train_time:112908ms step_avg:58.29ms
step:1938/2330 train_time:112969ms step_avg:58.29ms
step:1939/2330 train_time:113026ms step_avg:58.29ms
step:1940/2330 train_time:113088ms step_avg:58.29ms
step:1941/2330 train_time:113144ms step_avg:58.29ms
step:1942/2330 train_time:113205ms step_avg:58.29ms
step:1943/2330 train_time:113262ms step_avg:58.29ms
step:1944/2330 train_time:113322ms step_avg:58.29ms
step:1945/2330 train_time:113380ms step_avg:58.29ms
step:1946/2330 train_time:113441ms step_avg:58.29ms
step:1947/2330 train_time:113498ms step_avg:58.29ms
step:1948/2330 train_time:113559ms step_avg:58.30ms
step:1949/2330 train_time:113616ms step_avg:58.29ms
step:1950/2330 train_time:113677ms step_avg:58.30ms
step:1951/2330 train_time:113734ms step_avg:58.30ms
step:1952/2330 train_time:113796ms step_avg:58.30ms
step:1953/2330 train_time:113852ms step_avg:58.30ms
step:1954/2330 train_time:113913ms step_avg:58.30ms
step:1955/2330 train_time:113969ms step_avg:58.30ms
step:1956/2330 train_time:114032ms step_avg:58.30ms
step:1957/2330 train_time:114089ms step_avg:58.30ms
step:1958/2330 train_time:114150ms step_avg:58.30ms
step:1959/2330 train_time:114206ms step_avg:58.30ms
step:1960/2330 train_time:114268ms step_avg:58.30ms
step:1961/2330 train_time:114324ms step_avg:58.30ms
step:1962/2330 train_time:114387ms step_avg:58.30ms
step:1963/2330 train_time:114443ms step_avg:58.30ms
step:1964/2330 train_time:114505ms step_avg:58.30ms
step:1965/2330 train_time:114561ms step_avg:58.30ms
step:1966/2330 train_time:114622ms step_avg:58.30ms
step:1967/2330 train_time:114679ms step_avg:58.30ms
step:1968/2330 train_time:114740ms step_avg:58.30ms
step:1969/2330 train_time:114796ms step_avg:58.30ms
step:1970/2330 train_time:114857ms step_avg:58.30ms
step:1971/2330 train_time:114915ms step_avg:58.30ms
step:1972/2330 train_time:114976ms step_avg:58.30ms
step:1973/2330 train_time:115033ms step_avg:58.30ms
step:1974/2330 train_time:115095ms step_avg:58.31ms
step:1975/2330 train_time:115152ms step_avg:58.30ms
step:1976/2330 train_time:115214ms step_avg:58.31ms
step:1977/2330 train_time:115270ms step_avg:58.31ms
step:1978/2330 train_time:115333ms step_avg:58.31ms
step:1979/2330 train_time:115389ms step_avg:58.31ms
step:1980/2330 train_time:115452ms step_avg:58.31ms
step:1981/2330 train_time:115508ms step_avg:58.31ms
step:1982/2330 train_time:115570ms step_avg:58.31ms
step:1983/2330 train_time:115627ms step_avg:58.31ms
step:1984/2330 train_time:115687ms step_avg:58.31ms
step:1985/2330 train_time:115743ms step_avg:58.31ms
step:1986/2330 train_time:115805ms step_avg:58.31ms
step:1987/2330 train_time:115861ms step_avg:58.31ms
step:1988/2330 train_time:115922ms step_avg:58.31ms
step:1989/2330 train_time:115979ms step_avg:58.31ms
step:1990/2330 train_time:116040ms step_avg:58.31ms
step:1991/2330 train_time:116097ms step_avg:58.31ms
step:1992/2330 train_time:116159ms step_avg:58.31ms
step:1993/2330 train_time:116216ms step_avg:58.31ms
step:1994/2330 train_time:116278ms step_avg:58.31ms
step:1995/2330 train_time:116336ms step_avg:58.31ms
step:1996/2330 train_time:116397ms step_avg:58.32ms
step:1997/2330 train_time:116454ms step_avg:58.31ms
step:1998/2330 train_time:116515ms step_avg:58.32ms
step:1999/2330 train_time:116572ms step_avg:58.31ms
step:2000/2330 train_time:116634ms step_avg:58.32ms
step:2000/2330 val_loss:5.0520 train_time:116715ms step_avg:58.36ms
step:2001/2330 train_time:116734ms step_avg:58.34ms
step:2002/2330 train_time:116754ms step_avg:58.32ms
step:2003/2330 train_time:116813ms step_avg:58.32ms
step:2004/2330 train_time:116877ms step_avg:58.32ms
step:2005/2330 train_time:116934ms step_avg:58.32ms
step:2006/2330 train_time:116996ms step_avg:58.32ms
step:2007/2330 train_time:117052ms step_avg:58.32ms
step:2008/2330 train_time:117113ms step_avg:58.32ms
step:2009/2330 train_time:117169ms step_avg:58.32ms
step:2010/2330 train_time:117230ms step_avg:58.32ms
step:2011/2330 train_time:117286ms step_avg:58.32ms
step:2012/2330 train_time:117346ms step_avg:58.32ms
step:2013/2330 train_time:117402ms step_avg:58.32ms
step:2014/2330 train_time:117463ms step_avg:58.32ms
step:2015/2330 train_time:117519ms step_avg:58.32ms
step:2016/2330 train_time:117579ms step_avg:58.32ms
step:2017/2330 train_time:117635ms step_avg:58.32ms
step:2018/2330 train_time:117698ms step_avg:58.32ms
step:2019/2330 train_time:117756ms step_avg:58.32ms
step:2020/2330 train_time:117818ms step_avg:58.33ms
step:2021/2330 train_time:117876ms step_avg:58.33ms
step:2022/2330 train_time:117938ms step_avg:58.33ms
step:2023/2330 train_time:117995ms step_avg:58.33ms
step:2024/2330 train_time:118056ms step_avg:58.33ms
step:2025/2330 train_time:118112ms step_avg:58.33ms
step:2026/2330 train_time:118174ms step_avg:58.33ms
step:2027/2330 train_time:118231ms step_avg:58.33ms
step:2028/2330 train_time:118292ms step_avg:58.33ms
step:2029/2330 train_time:118348ms step_avg:58.33ms
step:2030/2330 train_time:118409ms step_avg:58.33ms
step:2031/2330 train_time:118465ms step_avg:58.33ms
step:2032/2330 train_time:118524ms step_avg:58.33ms
step:2033/2330 train_time:118581ms step_avg:58.33ms
step:2034/2330 train_time:118642ms step_avg:58.33ms
step:2035/2330 train_time:118699ms step_avg:58.33ms
step:2036/2330 train_time:118760ms step_avg:58.33ms
step:2037/2330 train_time:118819ms step_avg:58.33ms
step:2038/2330 train_time:118880ms step_avg:58.33ms
step:2039/2330 train_time:118937ms step_avg:58.33ms
step:2040/2330 train_time:118998ms step_avg:58.33ms
step:2041/2330 train_time:119056ms step_avg:58.33ms
step:2042/2330 train_time:119117ms step_avg:58.33ms
step:2043/2330 train_time:119174ms step_avg:58.33ms
step:2044/2330 train_time:119235ms step_avg:58.33ms
step:2045/2330 train_time:119291ms step_avg:58.33ms
step:2046/2330 train_time:119353ms step_avg:58.33ms
step:2047/2330 train_time:119410ms step_avg:58.33ms
step:2048/2330 train_time:119470ms step_avg:58.33ms
step:2049/2330 train_time:119526ms step_avg:58.33ms
step:2050/2330 train_time:119587ms step_avg:58.34ms
step:2051/2330 train_time:119644ms step_avg:58.33ms
step:2052/2330 train_time:119705ms step_avg:58.34ms
step:2053/2330 train_time:119763ms step_avg:58.34ms
step:2054/2330 train_time:119824ms step_avg:58.34ms
step:2055/2330 train_time:119881ms step_avg:58.34ms
step:2056/2330 train_time:119942ms step_avg:58.34ms
step:2057/2330 train_time:120000ms step_avg:58.34ms
step:2058/2330 train_time:120061ms step_avg:58.34ms
step:2059/2330 train_time:120120ms step_avg:58.34ms
step:2060/2330 train_time:120180ms step_avg:58.34ms
step:2061/2330 train_time:120237ms step_avg:58.34ms
step:2062/2330 train_time:120298ms step_avg:58.34ms
step:2063/2330 train_time:120355ms step_avg:58.34ms
step:2064/2330 train_time:120416ms step_avg:58.34ms
step:2065/2330 train_time:120473ms step_avg:58.34ms
step:2066/2330 train_time:120534ms step_avg:58.34ms
step:2067/2330 train_time:120591ms step_avg:58.34ms
step:2068/2330 train_time:120654ms step_avg:58.34ms
step:2069/2330 train_time:120710ms step_avg:58.34ms
step:2070/2330 train_time:120772ms step_avg:58.34ms
step:2071/2330 train_time:120828ms step_avg:58.34ms
step:2072/2330 train_time:120891ms step_avg:58.35ms
step:2073/2330 train_time:120948ms step_avg:58.34ms
step:2074/2330 train_time:121010ms step_avg:58.35ms
step:2075/2330 train_time:121067ms step_avg:58.35ms
step:2076/2330 train_time:121127ms step_avg:58.35ms
step:2077/2330 train_time:121185ms step_avg:58.35ms
step:2078/2330 train_time:121244ms step_avg:58.35ms
step:2079/2330 train_time:121302ms step_avg:58.35ms
step:2080/2330 train_time:121362ms step_avg:58.35ms
step:2081/2330 train_time:121420ms step_avg:58.35ms
step:2082/2330 train_time:121480ms step_avg:58.35ms
step:2083/2330 train_time:121538ms step_avg:58.35ms
step:2084/2330 train_time:121598ms step_avg:58.35ms
step:2085/2330 train_time:121655ms step_avg:58.35ms
step:2086/2330 train_time:121717ms step_avg:58.35ms
step:2087/2330 train_time:121774ms step_avg:58.35ms
step:2088/2330 train_time:121836ms step_avg:58.35ms
step:2089/2330 train_time:121893ms step_avg:58.35ms
step:2090/2330 train_time:121955ms step_avg:58.35ms
step:2091/2330 train_time:122011ms step_avg:58.35ms
step:2092/2330 train_time:122073ms step_avg:58.35ms
step:2093/2330 train_time:122129ms step_avg:58.35ms
step:2094/2330 train_time:122191ms step_avg:58.35ms
step:2095/2330 train_time:122248ms step_avg:58.35ms
step:2096/2330 train_time:122309ms step_avg:58.35ms
step:2097/2330 train_time:122366ms step_avg:58.35ms
step:2098/2330 train_time:122426ms step_avg:58.35ms
step:2099/2330 train_time:122483ms step_avg:58.35ms
step:2100/2330 train_time:122544ms step_avg:58.35ms
step:2101/2330 train_time:122601ms step_avg:58.35ms
step:2102/2330 train_time:122662ms step_avg:58.35ms
step:2103/2330 train_time:122721ms step_avg:58.36ms
step:2104/2330 train_time:122782ms step_avg:58.36ms
step:2105/2330 train_time:122839ms step_avg:58.36ms
step:2106/2330 train_time:122900ms step_avg:58.36ms
step:2107/2330 train_time:122958ms step_avg:58.36ms
step:2108/2330 train_time:123018ms step_avg:58.36ms
step:2109/2330 train_time:123076ms step_avg:58.36ms
step:2110/2330 train_time:123136ms step_avg:58.36ms
step:2111/2330 train_time:123192ms step_avg:58.36ms
step:2112/2330 train_time:123255ms step_avg:58.36ms
step:2113/2330 train_time:123311ms step_avg:58.36ms
step:2114/2330 train_time:123373ms step_avg:58.36ms
step:2115/2330 train_time:123430ms step_avg:58.36ms
step:2116/2330 train_time:123491ms step_avg:58.36ms
step:2117/2330 train_time:123548ms step_avg:58.36ms
step:2118/2330 train_time:123610ms step_avg:58.36ms
step:2119/2330 train_time:123666ms step_avg:58.36ms
step:2120/2330 train_time:123728ms step_avg:58.36ms
step:2121/2330 train_time:123785ms step_avg:58.36ms
step:2122/2330 train_time:123845ms step_avg:58.36ms
step:2123/2330 train_time:123903ms step_avg:58.36ms
step:2124/2330 train_time:123963ms step_avg:58.36ms
step:2125/2330 train_time:124020ms step_avg:58.36ms
step:2126/2330 train_time:124081ms step_avg:58.36ms
step:2127/2330 train_time:124138ms step_avg:58.36ms
step:2128/2330 train_time:124199ms step_avg:58.36ms
step:2129/2330 train_time:124256ms step_avg:58.36ms
step:2130/2330 train_time:124318ms step_avg:58.37ms
step:2131/2330 train_time:124374ms step_avg:58.36ms
step:2132/2330 train_time:124437ms step_avg:58.37ms
step:2133/2330 train_time:124493ms step_avg:58.37ms
step:2134/2330 train_time:124555ms step_avg:58.37ms
step:2135/2330 train_time:124612ms step_avg:58.37ms
step:2136/2330 train_time:124674ms step_avg:58.37ms
step:2137/2330 train_time:124730ms step_avg:58.37ms
step:2138/2330 train_time:124792ms step_avg:58.37ms
step:2139/2330 train_time:124849ms step_avg:58.37ms
step:2140/2330 train_time:124911ms step_avg:58.37ms
step:2141/2330 train_time:124967ms step_avg:58.37ms
step:2142/2330 train_time:125029ms step_avg:58.37ms
step:2143/2330 train_time:125086ms step_avg:58.37ms
step:2144/2330 train_time:125146ms step_avg:58.37ms
step:2145/2330 train_time:125203ms step_avg:58.37ms
step:2146/2330 train_time:125263ms step_avg:58.37ms
step:2147/2330 train_time:125321ms step_avg:58.37ms
step:2148/2330 train_time:125382ms step_avg:58.37ms
step:2149/2330 train_time:125439ms step_avg:58.37ms
step:2150/2330 train_time:125501ms step_avg:58.37ms
step:2151/2330 train_time:125559ms step_avg:58.37ms
step:2152/2330 train_time:125619ms step_avg:58.37ms
step:2153/2330 train_time:125678ms step_avg:58.37ms
step:2154/2330 train_time:125738ms step_avg:58.37ms
step:2155/2330 train_time:125795ms step_avg:58.37ms
step:2156/2330 train_time:125857ms step_avg:58.38ms
step:2157/2330 train_time:125913ms step_avg:58.37ms
step:2158/2330 train_time:125975ms step_avg:58.38ms
step:2159/2330 train_time:126031ms step_avg:58.37ms
step:2160/2330 train_time:126094ms step_avg:58.38ms
step:2161/2330 train_time:126150ms step_avg:58.38ms
step:2162/2330 train_time:126211ms step_avg:58.38ms
step:2163/2330 train_time:126268ms step_avg:58.38ms
step:2164/2330 train_time:126330ms step_avg:58.38ms
step:2165/2330 train_time:126386ms step_avg:58.38ms
step:2166/2330 train_time:126447ms step_avg:58.38ms
step:2167/2330 train_time:126504ms step_avg:58.38ms
step:2168/2330 train_time:126564ms step_avg:58.38ms
step:2169/2330 train_time:126622ms step_avg:58.38ms
step:2170/2330 train_time:126683ms step_avg:58.38ms
step:2171/2330 train_time:126740ms step_avg:58.38ms
step:2172/2330 train_time:126801ms step_avg:58.38ms
step:2173/2330 train_time:126858ms step_avg:58.38ms
step:2174/2330 train_time:126919ms step_avg:58.38ms
step:2175/2330 train_time:126976ms step_avg:58.38ms
step:2176/2330 train_time:127037ms step_avg:58.38ms
step:2177/2330 train_time:127094ms step_avg:58.38ms
step:2178/2330 train_time:127155ms step_avg:58.38ms
step:2179/2330 train_time:127210ms step_avg:58.38ms
step:2180/2330 train_time:127274ms step_avg:58.38ms
step:2181/2330 train_time:127331ms step_avg:58.38ms
step:2182/2330 train_time:127393ms step_avg:58.38ms
step:2183/2330 train_time:127450ms step_avg:58.38ms
step:2184/2330 train_time:127511ms step_avg:58.38ms
step:2185/2330 train_time:127567ms step_avg:58.38ms
step:2186/2330 train_time:127629ms step_avg:58.38ms
step:2187/2330 train_time:127686ms step_avg:58.38ms
step:2188/2330 train_time:127747ms step_avg:58.39ms
step:2189/2330 train_time:127804ms step_avg:58.38ms
step:2190/2330 train_time:127864ms step_avg:58.39ms
step:2191/2330 train_time:127921ms step_avg:58.38ms
step:2192/2330 train_time:127982ms step_avg:58.39ms
step:2193/2330 train_time:128040ms step_avg:58.39ms
step:2194/2330 train_time:128100ms step_avg:58.39ms
step:2195/2330 train_time:128158ms step_avg:58.39ms
step:2196/2330 train_time:128218ms step_avg:58.39ms
step:2197/2330 train_time:128276ms step_avg:58.39ms
step:2198/2330 train_time:128337ms step_avg:58.39ms
step:2199/2330 train_time:128394ms step_avg:58.39ms
step:2200/2330 train_time:128456ms step_avg:58.39ms
step:2201/2330 train_time:128513ms step_avg:58.39ms
step:2202/2330 train_time:128576ms step_avg:58.39ms
step:2203/2330 train_time:128631ms step_avg:58.39ms
step:2204/2330 train_time:128695ms step_avg:58.39ms
step:2205/2330 train_time:128752ms step_avg:58.39ms
step:2206/2330 train_time:128813ms step_avg:58.39ms
step:2207/2330 train_time:128869ms step_avg:58.39ms
step:2208/2330 train_time:128930ms step_avg:58.39ms
step:2209/2330 train_time:128987ms step_avg:58.39ms
step:2210/2330 train_time:129048ms step_avg:58.39ms
step:2211/2330 train_time:129105ms step_avg:58.39ms
step:2212/2330 train_time:129165ms step_avg:58.39ms
step:2213/2330 train_time:129222ms step_avg:58.39ms
step:2214/2330 train_time:129283ms step_avg:58.39ms
step:2215/2330 train_time:129341ms step_avg:58.39ms
step:2216/2330 train_time:129401ms step_avg:58.39ms
step:2217/2330 train_time:129459ms step_avg:58.39ms
step:2218/2330 train_time:129519ms step_avg:58.39ms
step:2219/2330 train_time:129576ms step_avg:58.39ms
step:2220/2330 train_time:129637ms step_avg:58.40ms
step:2221/2330 train_time:129694ms step_avg:58.39ms
step:2222/2330 train_time:129755ms step_avg:58.40ms
step:2223/2330 train_time:129812ms step_avg:58.40ms
step:2224/2330 train_time:129874ms step_avg:58.40ms
step:2225/2330 train_time:129930ms step_avg:58.40ms
step:2226/2330 train_time:129991ms step_avg:58.40ms
step:2227/2330 train_time:130047ms step_avg:58.40ms
step:2228/2330 train_time:130110ms step_avg:58.40ms
step:2229/2330 train_time:130166ms step_avg:58.40ms
step:2230/2330 train_time:130227ms step_avg:58.40ms
step:2231/2330 train_time:130284ms step_avg:58.40ms
step:2232/2330 train_time:130344ms step_avg:58.40ms
step:2233/2330 train_time:130401ms step_avg:58.40ms
step:2234/2330 train_time:130462ms step_avg:58.40ms
step:2235/2330 train_time:130519ms step_avg:58.40ms
step:2236/2330 train_time:130580ms step_avg:58.40ms
step:2237/2330 train_time:130638ms step_avg:58.40ms
step:2238/2330 train_time:130699ms step_avg:58.40ms
step:2239/2330 train_time:130756ms step_avg:58.40ms
step:2240/2330 train_time:130817ms step_avg:58.40ms
step:2241/2330 train_time:130874ms step_avg:58.40ms
step:2242/2330 train_time:130935ms step_avg:58.40ms
step:2243/2330 train_time:130992ms step_avg:58.40ms
step:2244/2330 train_time:131054ms step_avg:58.40ms
step:2245/2330 train_time:131111ms step_avg:58.40ms
step:2246/2330 train_time:131173ms step_avg:58.40ms
step:2247/2330 train_time:131229ms step_avg:58.40ms
step:2248/2330 train_time:131291ms step_avg:58.40ms
step:2249/2330 train_time:131347ms step_avg:58.40ms
step:2250/2330 train_time:131409ms step_avg:58.40ms
step:2250/2330 val_loss:4.9802 train_time:131490ms step_avg:58.44ms
step:2251/2330 train_time:131509ms step_avg:58.42ms
step:2252/2330 train_time:131531ms step_avg:58.41ms
step:2253/2330 train_time:131589ms step_avg:58.41ms
step:2254/2330 train_time:131656ms step_avg:58.41ms
step:2255/2330 train_time:131712ms step_avg:58.41ms
step:2256/2330 train_time:131776ms step_avg:58.41ms
step:2257/2330 train_time:131832ms step_avg:58.41ms
step:2258/2330 train_time:131895ms step_avg:58.41ms
step:2259/2330 train_time:131951ms step_avg:58.41ms
step:2260/2330 train_time:132012ms step_avg:58.41ms
step:2261/2330 train_time:132068ms step_avg:58.41ms
step:2262/2330 train_time:132128ms step_avg:58.41ms
step:2263/2330 train_time:132185ms step_avg:58.41ms
step:2264/2330 train_time:132245ms step_avg:58.41ms
step:2265/2330 train_time:132301ms step_avg:58.41ms
step:2266/2330 train_time:132361ms step_avg:58.41ms
step:2267/2330 train_time:132419ms step_avg:58.41ms
step:2268/2330 train_time:132480ms step_avg:58.41ms
step:2269/2330 train_time:132538ms step_avg:58.41ms
step:2270/2330 train_time:132601ms step_avg:58.41ms
step:2271/2330 train_time:132658ms step_avg:58.41ms
step:2272/2330 train_time:132721ms step_avg:58.42ms
step:2273/2330 train_time:132779ms step_avg:58.42ms
step:2274/2330 train_time:132839ms step_avg:58.42ms
step:2275/2330 train_time:132896ms step_avg:58.42ms
step:2276/2330 train_time:132956ms step_avg:58.42ms
step:2277/2330 train_time:133013ms step_avg:58.42ms
step:2278/2330 train_time:133074ms step_avg:58.42ms
step:2279/2330 train_time:133131ms step_avg:58.42ms
step:2280/2330 train_time:133191ms step_avg:58.42ms
step:2281/2330 train_time:133247ms step_avg:58.42ms
step:2282/2330 train_time:133309ms step_avg:58.42ms
step:2283/2330 train_time:133365ms step_avg:58.42ms
step:2284/2330 train_time:133426ms step_avg:58.42ms
step:2285/2330 train_time:133482ms step_avg:58.42ms
step:2286/2330 train_time:133545ms step_avg:58.42ms
step:2287/2330 train_time:133602ms step_avg:58.42ms
step:2288/2330 train_time:133664ms step_avg:58.42ms
step:2289/2330 train_time:133721ms step_avg:58.42ms
step:2290/2330 train_time:133782ms step_avg:58.42ms
step:2291/2330 train_time:133840ms step_avg:58.42ms
step:2292/2330 train_time:133899ms step_avg:58.42ms
step:2293/2330 train_time:133956ms step_avg:58.42ms
step:2294/2330 train_time:134018ms step_avg:58.42ms
step:2295/2330 train_time:134076ms step_avg:58.42ms
step:2296/2330 train_time:134136ms step_avg:58.42ms
step:2297/2330 train_time:134193ms step_avg:58.42ms
step:2298/2330 train_time:134253ms step_avg:58.42ms
step:2299/2330 train_time:134311ms step_avg:58.42ms
step:2300/2330 train_time:134372ms step_avg:58.42ms
step:2301/2330 train_time:134429ms step_avg:58.42ms
step:2302/2330 train_time:134490ms step_avg:58.42ms
step:2303/2330 train_time:134547ms step_avg:58.42ms
step:2304/2330 train_time:134609ms step_avg:58.42ms
step:2305/2330 train_time:134665ms step_avg:58.42ms
step:2306/2330 train_time:134728ms step_avg:58.43ms
step:2307/2330 train_time:134785ms step_avg:58.42ms
step:2308/2330 train_time:134848ms step_avg:58.43ms
step:2309/2330 train_time:134904ms step_avg:58.43ms
step:2310/2330 train_time:134965ms step_avg:58.43ms
step:2311/2330 train_time:135022ms step_avg:58.43ms
step:2312/2330 train_time:135083ms step_avg:58.43ms
step:2313/2330 train_time:135140ms step_avg:58.43ms
step:2314/2330 train_time:135200ms step_avg:58.43ms
step:2315/2330 train_time:135257ms step_avg:58.43ms
step:2316/2330 train_time:135316ms step_avg:58.43ms
step:2317/2330 train_time:135374ms step_avg:58.43ms
step:2318/2330 train_time:135434ms step_avg:58.43ms
step:2319/2330 train_time:135492ms step_avg:58.43ms
step:2320/2330 train_time:135552ms step_avg:58.43ms
step:2321/2330 train_time:135609ms step_avg:58.43ms
step:2322/2330 train_time:135672ms step_avg:58.43ms
step:2323/2330 train_time:135728ms step_avg:58.43ms
step:2324/2330 train_time:135791ms step_avg:58.43ms
step:2325/2330 train_time:135848ms step_avg:58.43ms
step:2326/2330 train_time:135910ms step_avg:58.43ms
step:2327/2330 train_time:135966ms step_avg:58.43ms
step:2328/2330 train_time:136029ms step_avg:58.43ms
step:2329/2330 train_time:136085ms step_avg:58.43ms
step:2330/2330 train_time:136147ms step_avg:58.43ms
step:2330/2330 val_loss:4.9636 train_time:136228ms step_avg:58.47ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
