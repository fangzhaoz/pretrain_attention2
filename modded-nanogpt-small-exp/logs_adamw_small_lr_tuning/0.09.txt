import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--adamw_lr', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_lr_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_lr_tuning/{args2.adamw_lr}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=float(args2.adamw_lr),  betas=(0.85, 0.85), eps=1e-10, weight_decay=0.0, fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_lr_tuning/{args2.adamw_lr}", exist_ok=True)
            torch.save(log, f"logs_adamw_small_lr_tuning/{args2.adamw_lr}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 02:49:58 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   35C    P0             118W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   35C    P0             118W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             118W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   28C    P0             115W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   34C    P0             118W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             114W / 700W |   2074MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:94ms step_avg:93.94ms
step:2/2330 train_time:190ms step_avg:95.15ms
step:3/2330 train_time:209ms step_avg:69.79ms
step:4/2330 train_time:229ms step_avg:57.36ms
step:5/2330 train_time:281ms step_avg:56.26ms
step:6/2330 train_time:339ms step_avg:56.42ms
step:7/2330 train_time:393ms step_avg:56.09ms
step:8/2330 train_time:449ms step_avg:56.16ms
step:9/2330 train_time:503ms step_avg:55.94ms
step:10/2330 train_time:560ms step_avg:56.01ms
step:11/2330 train_time:614ms step_avg:55.85ms
step:12/2330 train_time:671ms step_avg:55.92ms
step:13/2330 train_time:725ms step_avg:55.77ms
step:14/2330 train_time:782ms step_avg:55.84ms
step:15/2330 train_time:836ms step_avg:55.73ms
step:16/2330 train_time:893ms step_avg:55.81ms
step:17/2330 train_time:947ms step_avg:55.70ms
step:18/2330 train_time:1003ms step_avg:55.75ms
step:19/2330 train_time:1058ms step_avg:55.68ms
step:20/2330 train_time:1116ms step_avg:55.78ms
step:21/2330 train_time:1171ms step_avg:55.76ms
step:22/2330 train_time:1229ms step_avg:55.86ms
step:23/2330 train_time:1285ms step_avg:55.88ms
step:24/2330 train_time:1343ms step_avg:55.95ms
step:25/2330 train_time:1398ms step_avg:55.90ms
step:26/2330 train_time:1455ms step_avg:55.95ms
step:27/2330 train_time:1509ms step_avg:55.90ms
step:28/2330 train_time:1568ms step_avg:55.98ms
step:29/2330 train_time:1622ms step_avg:55.93ms
step:30/2330 train_time:1679ms step_avg:55.96ms
step:31/2330 train_time:1733ms step_avg:55.90ms
step:32/2330 train_time:1790ms step_avg:55.93ms
step:33/2330 train_time:1844ms step_avg:55.88ms
step:34/2330 train_time:1901ms step_avg:55.92ms
step:35/2330 train_time:1956ms step_avg:55.87ms
step:36/2330 train_time:2012ms step_avg:55.90ms
step:37/2330 train_time:2067ms step_avg:55.86ms
step:38/2330 train_time:2124ms step_avg:55.90ms
step:39/2330 train_time:2179ms step_avg:55.88ms
step:40/2330 train_time:2236ms step_avg:55.91ms
step:41/2330 train_time:2291ms step_avg:55.88ms
step:42/2330 train_time:2349ms step_avg:55.93ms
step:43/2330 train_time:2404ms step_avg:55.91ms
step:44/2330 train_time:2462ms step_avg:55.95ms
step:45/2330 train_time:2517ms step_avg:55.92ms
step:46/2330 train_time:2574ms step_avg:55.96ms
step:47/2330 train_time:2629ms step_avg:55.94ms
step:48/2330 train_time:2687ms step_avg:55.98ms
step:49/2330 train_time:2741ms step_avg:55.95ms
step:50/2330 train_time:2800ms step_avg:56.00ms
step:51/2330 train_time:2855ms step_avg:55.97ms
step:52/2330 train_time:2913ms step_avg:56.01ms
step:53/2330 train_time:2967ms step_avg:55.98ms
step:54/2330 train_time:3025ms step_avg:56.02ms
step:55/2330 train_time:3079ms step_avg:55.99ms
step:56/2330 train_time:3137ms step_avg:56.02ms
step:57/2330 train_time:3192ms step_avg:55.99ms
step:58/2330 train_time:3250ms step_avg:56.03ms
step:59/2330 train_time:3305ms step_avg:56.01ms
step:60/2330 train_time:3362ms step_avg:56.04ms
step:61/2330 train_time:3417ms step_avg:56.02ms
step:62/2330 train_time:3475ms step_avg:56.05ms
step:63/2330 train_time:3529ms step_avg:56.02ms
step:64/2330 train_time:3588ms step_avg:56.06ms
step:65/2330 train_time:3643ms step_avg:56.04ms
step:66/2330 train_time:3700ms step_avg:56.06ms
step:67/2330 train_time:3755ms step_avg:56.04ms
step:68/2330 train_time:3812ms step_avg:56.06ms
step:69/2330 train_time:3867ms step_avg:56.04ms
step:70/2330 train_time:3924ms step_avg:56.05ms
step:71/2330 train_time:3978ms step_avg:56.03ms
step:72/2330 train_time:4036ms step_avg:56.05ms
step:73/2330 train_time:4092ms step_avg:56.06ms
step:74/2330 train_time:4149ms step_avg:56.07ms
step:75/2330 train_time:4205ms step_avg:56.06ms
step:76/2330 train_time:4262ms step_avg:56.07ms
step:77/2330 train_time:4317ms step_avg:56.07ms
step:78/2330 train_time:4374ms step_avg:56.08ms
step:79/2330 train_time:4429ms step_avg:56.06ms
step:80/2330 train_time:4486ms step_avg:56.08ms
step:81/2330 train_time:4542ms step_avg:56.07ms
step:82/2330 train_time:4600ms step_avg:56.09ms
step:83/2330 train_time:4654ms step_avg:56.08ms
step:84/2330 train_time:4712ms step_avg:56.10ms
step:85/2330 train_time:4768ms step_avg:56.09ms
step:86/2330 train_time:4825ms step_avg:56.10ms
step:87/2330 train_time:4880ms step_avg:56.10ms
step:88/2330 train_time:4938ms step_avg:56.11ms
step:89/2330 train_time:4993ms step_avg:56.10ms
step:90/2330 train_time:5050ms step_avg:56.11ms
step:91/2330 train_time:5106ms step_avg:56.11ms
step:92/2330 train_time:5163ms step_avg:56.12ms
step:93/2330 train_time:5218ms step_avg:56.11ms
step:94/2330 train_time:5276ms step_avg:56.13ms
step:95/2330 train_time:5332ms step_avg:56.12ms
step:96/2330 train_time:5390ms step_avg:56.14ms
step:97/2330 train_time:5445ms step_avg:56.13ms
step:98/2330 train_time:5502ms step_avg:56.14ms
step:99/2330 train_time:5557ms step_avg:56.13ms
step:100/2330 train_time:5616ms step_avg:56.16ms
step:101/2330 train_time:5670ms step_avg:56.14ms
step:102/2330 train_time:5729ms step_avg:56.16ms
step:103/2330 train_time:5784ms step_avg:56.15ms
step:104/2330 train_time:5841ms step_avg:56.17ms
step:105/2330 train_time:5896ms step_avg:56.15ms
step:106/2330 train_time:5955ms step_avg:56.18ms
step:107/2330 train_time:6009ms step_avg:56.16ms
step:108/2330 train_time:6069ms step_avg:56.19ms
step:109/2330 train_time:6124ms step_avg:56.18ms
step:110/2330 train_time:6183ms step_avg:56.21ms
step:111/2330 train_time:6238ms step_avg:56.20ms
step:112/2330 train_time:6296ms step_avg:56.21ms
step:113/2330 train_time:6350ms step_avg:56.20ms
step:114/2330 train_time:6410ms step_avg:56.23ms
step:115/2330 train_time:6465ms step_avg:56.22ms
step:116/2330 train_time:6524ms step_avg:56.24ms
step:117/2330 train_time:6579ms step_avg:56.23ms
step:118/2330 train_time:6636ms step_avg:56.24ms
step:119/2330 train_time:6691ms step_avg:56.23ms
step:120/2330 train_time:6749ms step_avg:56.24ms
step:121/2330 train_time:6804ms step_avg:56.23ms
step:122/2330 train_time:6862ms step_avg:56.25ms
step:123/2330 train_time:6917ms step_avg:56.24ms
step:124/2330 train_time:6975ms step_avg:56.25ms
step:125/2330 train_time:7030ms step_avg:56.24ms
step:126/2330 train_time:7088ms step_avg:56.25ms
step:127/2330 train_time:7144ms step_avg:56.25ms
step:128/2330 train_time:7202ms step_avg:56.26ms
step:129/2330 train_time:7257ms step_avg:56.26ms
step:130/2330 train_time:7314ms step_avg:56.26ms
step:131/2330 train_time:7369ms step_avg:56.26ms
step:132/2330 train_time:7428ms step_avg:56.27ms
step:133/2330 train_time:7483ms step_avg:56.27ms
step:134/2330 train_time:7541ms step_avg:56.28ms
step:135/2330 train_time:7597ms step_avg:56.28ms
step:136/2330 train_time:7655ms step_avg:56.29ms
step:137/2330 train_time:7710ms step_avg:56.28ms
step:138/2330 train_time:7770ms step_avg:56.30ms
step:139/2330 train_time:7825ms step_avg:56.29ms
step:140/2330 train_time:7884ms step_avg:56.31ms
step:141/2330 train_time:7939ms step_avg:56.30ms
step:142/2330 train_time:7997ms step_avg:56.32ms
step:143/2330 train_time:8053ms step_avg:56.31ms
step:144/2330 train_time:8111ms step_avg:56.33ms
step:145/2330 train_time:8167ms step_avg:56.32ms
step:146/2330 train_time:8225ms step_avg:56.33ms
step:147/2330 train_time:8280ms step_avg:56.33ms
step:148/2330 train_time:8339ms step_avg:56.34ms
step:149/2330 train_time:8394ms step_avg:56.34ms
step:150/2330 train_time:8453ms step_avg:56.35ms
step:151/2330 train_time:8508ms step_avg:56.35ms
step:152/2330 train_time:8568ms step_avg:56.37ms
step:153/2330 train_time:8623ms step_avg:56.36ms
step:154/2330 train_time:8683ms step_avg:56.38ms
step:155/2330 train_time:8737ms step_avg:56.37ms
step:156/2330 train_time:8797ms step_avg:56.39ms
step:157/2330 train_time:8852ms step_avg:56.38ms
step:158/2330 train_time:8911ms step_avg:56.40ms
step:159/2330 train_time:8967ms step_avg:56.39ms
step:160/2330 train_time:9025ms step_avg:56.41ms
step:161/2330 train_time:9081ms step_avg:56.40ms
step:162/2330 train_time:9140ms step_avg:56.42ms
step:163/2330 train_time:9195ms step_avg:56.41ms
step:164/2330 train_time:9253ms step_avg:56.42ms
step:165/2330 train_time:9308ms step_avg:56.41ms
step:166/2330 train_time:9367ms step_avg:56.43ms
step:167/2330 train_time:9422ms step_avg:56.42ms
step:168/2330 train_time:9481ms step_avg:56.43ms
step:169/2330 train_time:9536ms step_avg:56.42ms
step:170/2330 train_time:9595ms step_avg:56.44ms
step:171/2330 train_time:9649ms step_avg:56.43ms
step:172/2330 train_time:9710ms step_avg:56.45ms
step:173/2330 train_time:9765ms step_avg:56.45ms
step:174/2330 train_time:9824ms step_avg:56.46ms
step:175/2330 train_time:9880ms step_avg:56.46ms
step:176/2330 train_time:9938ms step_avg:56.46ms
step:177/2330 train_time:9993ms step_avg:56.46ms
step:178/2330 train_time:10052ms step_avg:56.47ms
step:179/2330 train_time:10108ms step_avg:56.47ms
step:180/2330 train_time:10167ms step_avg:56.48ms
step:181/2330 train_time:10223ms step_avg:56.48ms
step:182/2330 train_time:10281ms step_avg:56.49ms
step:183/2330 train_time:10337ms step_avg:56.49ms
step:184/2330 train_time:10396ms step_avg:56.50ms
step:185/2330 train_time:10451ms step_avg:56.49ms
step:186/2330 train_time:10510ms step_avg:56.51ms
step:187/2330 train_time:10566ms step_avg:56.50ms
step:188/2330 train_time:10625ms step_avg:56.51ms
step:189/2330 train_time:10680ms step_avg:56.51ms
step:190/2330 train_time:10739ms step_avg:56.52ms
step:191/2330 train_time:10794ms step_avg:56.51ms
step:192/2330 train_time:10854ms step_avg:56.53ms
step:193/2330 train_time:10909ms step_avg:56.52ms
step:194/2330 train_time:10968ms step_avg:56.54ms
step:195/2330 train_time:11024ms step_avg:56.53ms
step:196/2330 train_time:11083ms step_avg:56.55ms
step:197/2330 train_time:11138ms step_avg:56.54ms
step:198/2330 train_time:11197ms step_avg:56.55ms
step:199/2330 train_time:11252ms step_avg:56.54ms
step:200/2330 train_time:11311ms step_avg:56.56ms
step:201/2330 train_time:11368ms step_avg:56.56ms
step:202/2330 train_time:11426ms step_avg:56.57ms
step:203/2330 train_time:11482ms step_avg:56.56ms
step:204/2330 train_time:11540ms step_avg:56.57ms
step:205/2330 train_time:11596ms step_avg:56.57ms
step:206/2330 train_time:11655ms step_avg:56.58ms
step:207/2330 train_time:11710ms step_avg:56.57ms
step:208/2330 train_time:11770ms step_avg:56.58ms
step:209/2330 train_time:11826ms step_avg:56.58ms
step:210/2330 train_time:11884ms step_avg:56.59ms
step:211/2330 train_time:11940ms step_avg:56.59ms
step:212/2330 train_time:11999ms step_avg:56.60ms
step:213/2330 train_time:12054ms step_avg:56.59ms
step:214/2330 train_time:12112ms step_avg:56.60ms
step:215/2330 train_time:12168ms step_avg:56.60ms
step:216/2330 train_time:12227ms step_avg:56.61ms
step:217/2330 train_time:12282ms step_avg:56.60ms
step:218/2330 train_time:12341ms step_avg:56.61ms
step:219/2330 train_time:12397ms step_avg:56.61ms
step:220/2330 train_time:12455ms step_avg:56.62ms
step:221/2330 train_time:12512ms step_avg:56.61ms
step:222/2330 train_time:12571ms step_avg:56.62ms
step:223/2330 train_time:12626ms step_avg:56.62ms
step:224/2330 train_time:12685ms step_avg:56.63ms
step:225/2330 train_time:12742ms step_avg:56.63ms
step:226/2330 train_time:12800ms step_avg:56.64ms
step:227/2330 train_time:12856ms step_avg:56.63ms
step:228/2330 train_time:12914ms step_avg:56.64ms
step:229/2330 train_time:12970ms step_avg:56.64ms
step:230/2330 train_time:13029ms step_avg:56.65ms
step:231/2330 train_time:13085ms step_avg:56.64ms
step:232/2330 train_time:13143ms step_avg:56.65ms
step:233/2330 train_time:13199ms step_avg:56.65ms
step:234/2330 train_time:13258ms step_avg:56.66ms
step:235/2330 train_time:13314ms step_avg:56.65ms
step:236/2330 train_time:13373ms step_avg:56.66ms
step:237/2330 train_time:13428ms step_avg:56.66ms
step:238/2330 train_time:13488ms step_avg:56.67ms
step:239/2330 train_time:13544ms step_avg:56.67ms
step:240/2330 train_time:13603ms step_avg:56.68ms
step:241/2330 train_time:13659ms step_avg:56.68ms
step:242/2330 train_time:13717ms step_avg:56.68ms
step:243/2330 train_time:13773ms step_avg:56.68ms
step:244/2330 train_time:13832ms step_avg:56.69ms
step:245/2330 train_time:13888ms step_avg:56.69ms
step:246/2330 train_time:13948ms step_avg:56.70ms
step:247/2330 train_time:14004ms step_avg:56.69ms
step:248/2330 train_time:14063ms step_avg:56.71ms
step:249/2330 train_time:14119ms step_avg:56.70ms
step:250/2330 train_time:14178ms step_avg:56.71ms
step:250/2330 val_loss:6.6160 train_time:14256ms step_avg:57.02ms
step:251/2330 train_time:14275ms step_avg:56.87ms
step:252/2330 train_time:14296ms step_avg:56.73ms
step:253/2330 train_time:14348ms step_avg:56.71ms
step:254/2330 train_time:14416ms step_avg:56.76ms
step:255/2330 train_time:14471ms step_avg:56.75ms
step:256/2330 train_time:14536ms step_avg:56.78ms
step:257/2330 train_time:14591ms step_avg:56.78ms
step:258/2330 train_time:14651ms step_avg:56.79ms
step:259/2330 train_time:14707ms step_avg:56.78ms
step:260/2330 train_time:14765ms step_avg:56.79ms
step:261/2330 train_time:14820ms step_avg:56.78ms
step:262/2330 train_time:14878ms step_avg:56.79ms
step:263/2330 train_time:14934ms step_avg:56.78ms
step:264/2330 train_time:14992ms step_avg:56.79ms
step:265/2330 train_time:15047ms step_avg:56.78ms
step:266/2330 train_time:15105ms step_avg:56.79ms
step:267/2330 train_time:15164ms step_avg:56.79ms
step:268/2330 train_time:15223ms step_avg:56.80ms
step:269/2330 train_time:15280ms step_avg:56.80ms
step:270/2330 train_time:15339ms step_avg:56.81ms
step:271/2330 train_time:15395ms step_avg:56.81ms
step:272/2330 train_time:15456ms step_avg:56.83ms
step:273/2330 train_time:15512ms step_avg:56.82ms
step:274/2330 train_time:15573ms step_avg:56.84ms
step:275/2330 train_time:15628ms step_avg:56.83ms
step:276/2330 train_time:15688ms step_avg:56.84ms
step:277/2330 train_time:15744ms step_avg:56.84ms
step:278/2330 train_time:15803ms step_avg:56.84ms
step:279/2330 train_time:15858ms step_avg:56.84ms
step:280/2330 train_time:15917ms step_avg:56.84ms
step:281/2330 train_time:15972ms step_avg:56.84ms
step:282/2330 train_time:16030ms step_avg:56.84ms
step:283/2330 train_time:16086ms step_avg:56.84ms
step:284/2330 train_time:16146ms step_avg:56.85ms
step:285/2330 train_time:16202ms step_avg:56.85ms
step:286/2330 train_time:16262ms step_avg:56.86ms
step:287/2330 train_time:16318ms step_avg:56.86ms
step:288/2330 train_time:16378ms step_avg:56.87ms
step:289/2330 train_time:16433ms step_avg:56.86ms
step:290/2330 train_time:16494ms step_avg:56.88ms
step:291/2330 train_time:16550ms step_avg:56.87ms
step:292/2330 train_time:16611ms step_avg:56.89ms
step:293/2330 train_time:16667ms step_avg:56.88ms
step:294/2330 train_time:16726ms step_avg:56.89ms
step:295/2330 train_time:16782ms step_avg:56.89ms
step:296/2330 train_time:16841ms step_avg:56.89ms
step:297/2330 train_time:16896ms step_avg:56.89ms
step:298/2330 train_time:16954ms step_avg:56.89ms
step:299/2330 train_time:17011ms step_avg:56.89ms
step:300/2330 train_time:17069ms step_avg:56.90ms
step:301/2330 train_time:17125ms step_avg:56.89ms
step:302/2330 train_time:17184ms step_avg:56.90ms
step:303/2330 train_time:17241ms step_avg:56.90ms
step:304/2330 train_time:17300ms step_avg:56.91ms
step:305/2330 train_time:17357ms step_avg:56.91ms
step:306/2330 train_time:17416ms step_avg:56.91ms
step:307/2330 train_time:17471ms step_avg:56.91ms
step:308/2330 train_time:17530ms step_avg:56.92ms
step:309/2330 train_time:17586ms step_avg:56.91ms
step:310/2330 train_time:17646ms step_avg:56.92ms
step:311/2330 train_time:17702ms step_avg:56.92ms
step:312/2330 train_time:17761ms step_avg:56.93ms
step:313/2330 train_time:17817ms step_avg:56.92ms
step:314/2330 train_time:17875ms step_avg:56.93ms
step:315/2330 train_time:17931ms step_avg:56.92ms
step:316/2330 train_time:17990ms step_avg:56.93ms
step:317/2330 train_time:18047ms step_avg:56.93ms
step:318/2330 train_time:18105ms step_avg:56.93ms
step:319/2330 train_time:18161ms step_avg:56.93ms
step:320/2330 train_time:18220ms step_avg:56.94ms
step:321/2330 train_time:18276ms step_avg:56.94ms
step:322/2330 train_time:18336ms step_avg:56.94ms
step:323/2330 train_time:18392ms step_avg:56.94ms
step:324/2330 train_time:18451ms step_avg:56.95ms
step:325/2330 train_time:18507ms step_avg:56.95ms
step:326/2330 train_time:18566ms step_avg:56.95ms
step:327/2330 train_time:18622ms step_avg:56.95ms
step:328/2330 train_time:18682ms step_avg:56.96ms
step:329/2330 train_time:18738ms step_avg:56.95ms
step:330/2330 train_time:18797ms step_avg:56.96ms
step:331/2330 train_time:18853ms step_avg:56.96ms
step:332/2330 train_time:18912ms step_avg:56.96ms
step:333/2330 train_time:18969ms step_avg:56.96ms
step:334/2330 train_time:19027ms step_avg:56.97ms
step:335/2330 train_time:19083ms step_avg:56.97ms
step:336/2330 train_time:19142ms step_avg:56.97ms
step:337/2330 train_time:19198ms step_avg:56.97ms
step:338/2330 train_time:19257ms step_avg:56.97ms
step:339/2330 train_time:19312ms step_avg:56.97ms
step:340/2330 train_time:19372ms step_avg:56.98ms
step:341/2330 train_time:19428ms step_avg:56.97ms
step:342/2330 train_time:19487ms step_avg:56.98ms
step:343/2330 train_time:19544ms step_avg:56.98ms
step:344/2330 train_time:19603ms step_avg:56.98ms
step:345/2330 train_time:19659ms step_avg:56.98ms
step:346/2330 train_time:19718ms step_avg:56.99ms
step:347/2330 train_time:19773ms step_avg:56.98ms
step:348/2330 train_time:19832ms step_avg:56.99ms
step:349/2330 train_time:19888ms step_avg:56.99ms
step:350/2330 train_time:19947ms step_avg:56.99ms
step:351/2330 train_time:20003ms step_avg:56.99ms
step:352/2330 train_time:20063ms step_avg:57.00ms
step:353/2330 train_time:20119ms step_avg:56.99ms
step:354/2330 train_time:20177ms step_avg:57.00ms
step:355/2330 train_time:20233ms step_avg:56.99ms
step:356/2330 train_time:20292ms step_avg:57.00ms
step:357/2330 train_time:20348ms step_avg:57.00ms
step:358/2330 train_time:20408ms step_avg:57.00ms
step:359/2330 train_time:20464ms step_avg:57.00ms
step:360/2330 train_time:20523ms step_avg:57.01ms
step:361/2330 train_time:20579ms step_avg:57.01ms
step:362/2330 train_time:20638ms step_avg:57.01ms
step:363/2330 train_time:20695ms step_avg:57.01ms
step:364/2330 train_time:20753ms step_avg:57.01ms
step:365/2330 train_time:20810ms step_avg:57.01ms
step:366/2330 train_time:20868ms step_avg:57.02ms
step:367/2330 train_time:20925ms step_avg:57.02ms
step:368/2330 train_time:20984ms step_avg:57.02ms
step:369/2330 train_time:21039ms step_avg:57.02ms
step:370/2330 train_time:21098ms step_avg:57.02ms
step:371/2330 train_time:21154ms step_avg:57.02ms
step:372/2330 train_time:21213ms step_avg:57.03ms
step:373/2330 train_time:21270ms step_avg:57.02ms
step:374/2330 train_time:21329ms step_avg:57.03ms
step:375/2330 train_time:21385ms step_avg:57.03ms
step:376/2330 train_time:21445ms step_avg:57.04ms
step:377/2330 train_time:21502ms step_avg:57.03ms
step:378/2330 train_time:21561ms step_avg:57.04ms
step:379/2330 train_time:21617ms step_avg:57.04ms
step:380/2330 train_time:21676ms step_avg:57.04ms
step:381/2330 train_time:21732ms step_avg:57.04ms
step:382/2330 train_time:21791ms step_avg:57.04ms
step:383/2330 train_time:21848ms step_avg:57.04ms
step:384/2330 train_time:21907ms step_avg:57.05ms
step:385/2330 train_time:21964ms step_avg:57.05ms
step:386/2330 train_time:22022ms step_avg:57.05ms
step:387/2330 train_time:22079ms step_avg:57.05ms
step:388/2330 train_time:22136ms step_avg:57.05ms
step:389/2330 train_time:22193ms step_avg:57.05ms
step:390/2330 train_time:22251ms step_avg:57.05ms
step:391/2330 train_time:22307ms step_avg:57.05ms
step:392/2330 train_time:22366ms step_avg:57.06ms
step:393/2330 train_time:22423ms step_avg:57.06ms
step:394/2330 train_time:22482ms step_avg:57.06ms
step:395/2330 train_time:22538ms step_avg:57.06ms
step:396/2330 train_time:22596ms step_avg:57.06ms
step:397/2330 train_time:22652ms step_avg:57.06ms
step:398/2330 train_time:22711ms step_avg:57.06ms
step:399/2330 train_time:22769ms step_avg:57.06ms
step:400/2330 train_time:22827ms step_avg:57.07ms
step:401/2330 train_time:22884ms step_avg:57.07ms
step:402/2330 train_time:22942ms step_avg:57.07ms
step:403/2330 train_time:22998ms step_avg:57.07ms
step:404/2330 train_time:23056ms step_avg:57.07ms
step:405/2330 train_time:23113ms step_avg:57.07ms
step:406/2330 train_time:23171ms step_avg:57.07ms
step:407/2330 train_time:23228ms step_avg:57.07ms
step:408/2330 train_time:23286ms step_avg:57.07ms
step:409/2330 train_time:23342ms step_avg:57.07ms
step:410/2330 train_time:23401ms step_avg:57.08ms
step:411/2330 train_time:23458ms step_avg:57.08ms
step:412/2330 train_time:23516ms step_avg:57.08ms
step:413/2330 train_time:23572ms step_avg:57.08ms
step:414/2330 train_time:23631ms step_avg:57.08ms
step:415/2330 train_time:23687ms step_avg:57.08ms
step:416/2330 train_time:23746ms step_avg:57.08ms
step:417/2330 train_time:23803ms step_avg:57.08ms
step:418/2330 train_time:23862ms step_avg:57.09ms
step:419/2330 train_time:23917ms step_avg:57.08ms
step:420/2330 train_time:23976ms step_avg:57.09ms
step:421/2330 train_time:24032ms step_avg:57.08ms
step:422/2330 train_time:24092ms step_avg:57.09ms
step:423/2330 train_time:24148ms step_avg:57.09ms
step:424/2330 train_time:24207ms step_avg:57.09ms
step:425/2330 train_time:24263ms step_avg:57.09ms
step:426/2330 train_time:24322ms step_avg:57.09ms
step:427/2330 train_time:24379ms step_avg:57.09ms
step:428/2330 train_time:24437ms step_avg:57.10ms
step:429/2330 train_time:24493ms step_avg:57.09ms
step:430/2330 train_time:24553ms step_avg:57.10ms
step:431/2330 train_time:24609ms step_avg:57.10ms
step:432/2330 train_time:24667ms step_avg:57.10ms
step:433/2330 train_time:24723ms step_avg:57.10ms
step:434/2330 train_time:24783ms step_avg:57.10ms
step:435/2330 train_time:24838ms step_avg:57.10ms
step:436/2330 train_time:24898ms step_avg:57.11ms
step:437/2330 train_time:24954ms step_avg:57.10ms
step:438/2330 train_time:25013ms step_avg:57.11ms
step:439/2330 train_time:25069ms step_avg:57.10ms
step:440/2330 train_time:25128ms step_avg:57.11ms
step:441/2330 train_time:25184ms step_avg:57.11ms
step:442/2330 train_time:25243ms step_avg:57.11ms
step:443/2330 train_time:25299ms step_avg:57.11ms
step:444/2330 train_time:25358ms step_avg:57.11ms
step:445/2330 train_time:25414ms step_avg:57.11ms
step:446/2330 train_time:25473ms step_avg:57.12ms
step:447/2330 train_time:25529ms step_avg:57.11ms
step:448/2330 train_time:25588ms step_avg:57.12ms
step:449/2330 train_time:25644ms step_avg:57.11ms
step:450/2330 train_time:25704ms step_avg:57.12ms
step:451/2330 train_time:25760ms step_avg:57.12ms
step:452/2330 train_time:25819ms step_avg:57.12ms
step:453/2330 train_time:25875ms step_avg:57.12ms
step:454/2330 train_time:25934ms step_avg:57.12ms
step:455/2330 train_time:25990ms step_avg:57.12ms
step:456/2330 train_time:26050ms step_avg:57.13ms
step:457/2330 train_time:26106ms step_avg:57.12ms
step:458/2330 train_time:26165ms step_avg:57.13ms
step:459/2330 train_time:26222ms step_avg:57.13ms
step:460/2330 train_time:26280ms step_avg:57.13ms
step:461/2330 train_time:26336ms step_avg:57.13ms
step:462/2330 train_time:26395ms step_avg:57.13ms
step:463/2330 train_time:26451ms step_avg:57.13ms
step:464/2330 train_time:26510ms step_avg:57.13ms
step:465/2330 train_time:26566ms step_avg:57.13ms
step:466/2330 train_time:26626ms step_avg:57.14ms
step:467/2330 train_time:26682ms step_avg:57.13ms
step:468/2330 train_time:26741ms step_avg:57.14ms
step:469/2330 train_time:26797ms step_avg:57.14ms
step:470/2330 train_time:26856ms step_avg:57.14ms
step:471/2330 train_time:26912ms step_avg:57.14ms
step:472/2330 train_time:26971ms step_avg:57.14ms
step:473/2330 train_time:27027ms step_avg:57.14ms
step:474/2330 train_time:27085ms step_avg:57.14ms
step:475/2330 train_time:27142ms step_avg:57.14ms
step:476/2330 train_time:27201ms step_avg:57.14ms
step:477/2330 train_time:27256ms step_avg:57.14ms
step:478/2330 train_time:27315ms step_avg:57.14ms
step:479/2330 train_time:27372ms step_avg:57.14ms
step:480/2330 train_time:27431ms step_avg:57.15ms
step:481/2330 train_time:27487ms step_avg:57.15ms
step:482/2330 train_time:27546ms step_avg:57.15ms
step:483/2330 train_time:27603ms step_avg:57.15ms
step:484/2330 train_time:27662ms step_avg:57.15ms
step:485/2330 train_time:27718ms step_avg:57.15ms
step:486/2330 train_time:27778ms step_avg:57.16ms
step:487/2330 train_time:27833ms step_avg:57.15ms
step:488/2330 train_time:27892ms step_avg:57.16ms
step:489/2330 train_time:27949ms step_avg:57.15ms
step:490/2330 train_time:28007ms step_avg:57.16ms
step:491/2330 train_time:28063ms step_avg:57.15ms
step:492/2330 train_time:28122ms step_avg:57.16ms
step:493/2330 train_time:28178ms step_avg:57.16ms
step:494/2330 train_time:28237ms step_avg:57.16ms
step:495/2330 train_time:28292ms step_avg:57.16ms
step:496/2330 train_time:28352ms step_avg:57.16ms
step:497/2330 train_time:28408ms step_avg:57.16ms
step:498/2330 train_time:28467ms step_avg:57.16ms
step:499/2330 train_time:28523ms step_avg:57.16ms
step:500/2330 train_time:28582ms step_avg:57.16ms
step:500/2330 val_loss:6.0447 train_time:28661ms step_avg:57.32ms
step:501/2330 train_time:28681ms step_avg:57.25ms
step:502/2330 train_time:28701ms step_avg:57.17ms
step:503/2330 train_time:28756ms step_avg:57.17ms
step:504/2330 train_time:28820ms step_avg:57.18ms
step:505/2330 train_time:28876ms step_avg:57.18ms
step:506/2330 train_time:28937ms step_avg:57.19ms
step:507/2330 train_time:28993ms step_avg:57.19ms
step:508/2330 train_time:29053ms step_avg:57.19ms
step:509/2330 train_time:29109ms step_avg:57.19ms
step:510/2330 train_time:29167ms step_avg:57.19ms
step:511/2330 train_time:29223ms step_avg:57.19ms
step:512/2330 train_time:29282ms step_avg:57.19ms
step:513/2330 train_time:29338ms step_avg:57.19ms
step:514/2330 train_time:29396ms step_avg:57.19ms
step:515/2330 train_time:29452ms step_avg:57.19ms
step:516/2330 train_time:29510ms step_avg:57.19ms
step:517/2330 train_time:29566ms step_avg:57.19ms
step:518/2330 train_time:29625ms step_avg:57.19ms
step:519/2330 train_time:29682ms step_avg:57.19ms
step:520/2330 train_time:29741ms step_avg:57.19ms
step:521/2330 train_time:29799ms step_avg:57.20ms
step:522/2330 train_time:29860ms step_avg:57.20ms
step:523/2330 train_time:29916ms step_avg:57.20ms
step:524/2330 train_time:29976ms step_avg:57.21ms
step:525/2330 train_time:30031ms step_avg:57.20ms
step:526/2330 train_time:30090ms step_avg:57.21ms
step:527/2330 train_time:30146ms step_avg:57.20ms
step:528/2330 train_time:30205ms step_avg:57.21ms
step:529/2330 train_time:30261ms step_avg:57.20ms
step:530/2330 train_time:30320ms step_avg:57.21ms
step:531/2330 train_time:30375ms step_avg:57.20ms
step:532/2330 train_time:30433ms step_avg:57.21ms
step:533/2330 train_time:30489ms step_avg:57.20ms
step:534/2330 train_time:30548ms step_avg:57.21ms
step:535/2330 train_time:30604ms step_avg:57.20ms
step:536/2330 train_time:30663ms step_avg:57.21ms
step:537/2330 train_time:30720ms step_avg:57.21ms
step:538/2330 train_time:30780ms step_avg:57.21ms
step:539/2330 train_time:30836ms step_avg:57.21ms
step:540/2330 train_time:30896ms step_avg:57.22ms
step:541/2330 train_time:30952ms step_avg:57.21ms
step:542/2330 train_time:31011ms step_avg:57.22ms
step:543/2330 train_time:31067ms step_avg:57.21ms
step:544/2330 train_time:31127ms step_avg:57.22ms
step:545/2330 train_time:31184ms step_avg:57.22ms
step:546/2330 train_time:31243ms step_avg:57.22ms
step:547/2330 train_time:31299ms step_avg:57.22ms
step:548/2330 train_time:31358ms step_avg:57.22ms
step:549/2330 train_time:31413ms step_avg:57.22ms
step:550/2330 train_time:31472ms step_avg:57.22ms
step:551/2330 train_time:31528ms step_avg:57.22ms
step:552/2330 train_time:31586ms step_avg:57.22ms
step:553/2330 train_time:31642ms step_avg:57.22ms
step:554/2330 train_time:31702ms step_avg:57.22ms
step:555/2330 train_time:31759ms step_avg:57.22ms
step:556/2330 train_time:31818ms step_avg:57.23ms
step:557/2330 train_time:31874ms step_avg:57.23ms
step:558/2330 train_time:31933ms step_avg:57.23ms
step:559/2330 train_time:31989ms step_avg:57.23ms
step:560/2330 train_time:32048ms step_avg:57.23ms
step:561/2330 train_time:32104ms step_avg:57.23ms
step:562/2330 train_time:32165ms step_avg:57.23ms
step:563/2330 train_time:32221ms step_avg:57.23ms
step:564/2330 train_time:32280ms step_avg:57.23ms
step:565/2330 train_time:32335ms step_avg:57.23ms
step:566/2330 train_time:32395ms step_avg:57.24ms
step:567/2330 train_time:32451ms step_avg:57.23ms
step:568/2330 train_time:32510ms step_avg:57.24ms
step:569/2330 train_time:32567ms step_avg:57.23ms
step:570/2330 train_time:32625ms step_avg:57.24ms
step:571/2330 train_time:32681ms step_avg:57.23ms
step:572/2330 train_time:32740ms step_avg:57.24ms
step:573/2330 train_time:32797ms step_avg:57.24ms
step:574/2330 train_time:32856ms step_avg:57.24ms
step:575/2330 train_time:32912ms step_avg:57.24ms
step:576/2330 train_time:32971ms step_avg:57.24ms
step:577/2330 train_time:33027ms step_avg:57.24ms
step:578/2330 train_time:33086ms step_avg:57.24ms
step:579/2330 train_time:33142ms step_avg:57.24ms
step:580/2330 train_time:33201ms step_avg:57.24ms
step:581/2330 train_time:33258ms step_avg:57.24ms
step:582/2330 train_time:33317ms step_avg:57.25ms
step:583/2330 train_time:33373ms step_avg:57.24ms
step:584/2330 train_time:33432ms step_avg:57.25ms
step:585/2330 train_time:33489ms step_avg:57.25ms
step:586/2330 train_time:33547ms step_avg:57.25ms
step:587/2330 train_time:33604ms step_avg:57.25ms
step:588/2330 train_time:33663ms step_avg:57.25ms
step:589/2330 train_time:33720ms step_avg:57.25ms
step:590/2330 train_time:33779ms step_avg:57.25ms
step:591/2330 train_time:33835ms step_avg:57.25ms
step:592/2330 train_time:33894ms step_avg:57.25ms
step:593/2330 train_time:33951ms step_avg:57.25ms
step:594/2330 train_time:34009ms step_avg:57.25ms
step:595/2330 train_time:34065ms step_avg:57.25ms
step:596/2330 train_time:34124ms step_avg:57.26ms
step:597/2330 train_time:34181ms step_avg:57.25ms
step:598/2330 train_time:34240ms step_avg:57.26ms
step:599/2330 train_time:34296ms step_avg:57.26ms
step:600/2330 train_time:34355ms step_avg:57.26ms
step:601/2330 train_time:34411ms step_avg:57.26ms
step:602/2330 train_time:34470ms step_avg:57.26ms
step:603/2330 train_time:34526ms step_avg:57.26ms
step:604/2330 train_time:34585ms step_avg:57.26ms
step:605/2330 train_time:34641ms step_avg:57.26ms
step:606/2330 train_time:34699ms step_avg:57.26ms
step:607/2330 train_time:34755ms step_avg:57.26ms
step:608/2330 train_time:34815ms step_avg:57.26ms
step:609/2330 train_time:34871ms step_avg:57.26ms
step:610/2330 train_time:34931ms step_avg:57.26ms
step:611/2330 train_time:34987ms step_avg:57.26ms
step:612/2330 train_time:35045ms step_avg:57.26ms
step:613/2330 train_time:35102ms step_avg:57.26ms
step:614/2330 train_time:35160ms step_avg:57.26ms
step:615/2330 train_time:35217ms step_avg:57.26ms
step:616/2330 train_time:35275ms step_avg:57.26ms
step:617/2330 train_time:35331ms step_avg:57.26ms
step:618/2330 train_time:35390ms step_avg:57.27ms
step:619/2330 train_time:35448ms step_avg:57.27ms
step:620/2330 train_time:35506ms step_avg:57.27ms
step:621/2330 train_time:35563ms step_avg:57.27ms
step:622/2330 train_time:35622ms step_avg:57.27ms
step:623/2330 train_time:35678ms step_avg:57.27ms
step:624/2330 train_time:35737ms step_avg:57.27ms
step:625/2330 train_time:35793ms step_avg:57.27ms
step:626/2330 train_time:35852ms step_avg:57.27ms
step:627/2330 train_time:35908ms step_avg:57.27ms
step:628/2330 train_time:35967ms step_avg:57.27ms
step:629/2330 train_time:36023ms step_avg:57.27ms
step:630/2330 train_time:36083ms step_avg:57.27ms
step:631/2330 train_time:36139ms step_avg:57.27ms
step:632/2330 train_time:36198ms step_avg:57.28ms
step:633/2330 train_time:36254ms step_avg:57.27ms
step:634/2330 train_time:36313ms step_avg:57.28ms
step:635/2330 train_time:36368ms step_avg:57.27ms
step:636/2330 train_time:36428ms step_avg:57.28ms
step:637/2330 train_time:36485ms step_avg:57.28ms
step:638/2330 train_time:36544ms step_avg:57.28ms
step:639/2330 train_time:36601ms step_avg:57.28ms
step:640/2330 train_time:36660ms step_avg:57.28ms
step:641/2330 train_time:36716ms step_avg:57.28ms
step:642/2330 train_time:36774ms step_avg:57.28ms
step:643/2330 train_time:36830ms step_avg:57.28ms
step:644/2330 train_time:36889ms step_avg:57.28ms
step:645/2330 train_time:36945ms step_avg:57.28ms
step:646/2330 train_time:37004ms step_avg:57.28ms
step:647/2330 train_time:37060ms step_avg:57.28ms
step:648/2330 train_time:37120ms step_avg:57.28ms
step:649/2330 train_time:37176ms step_avg:57.28ms
step:650/2330 train_time:37235ms step_avg:57.29ms
step:651/2330 train_time:37291ms step_avg:57.28ms
step:652/2330 train_time:37351ms step_avg:57.29ms
step:653/2330 train_time:37406ms step_avg:57.28ms
step:654/2330 train_time:37466ms step_avg:57.29ms
step:655/2330 train_time:37522ms step_avg:57.29ms
step:656/2330 train_time:37582ms step_avg:57.29ms
step:657/2330 train_time:37638ms step_avg:57.29ms
step:658/2330 train_time:37698ms step_avg:57.29ms
step:659/2330 train_time:37754ms step_avg:57.29ms
step:660/2330 train_time:37813ms step_avg:57.29ms
step:661/2330 train_time:37869ms step_avg:57.29ms
step:662/2330 train_time:37929ms step_avg:57.29ms
step:663/2330 train_time:37985ms step_avg:57.29ms
step:664/2330 train_time:38044ms step_avg:57.30ms
step:665/2330 train_time:38101ms step_avg:57.29ms
step:666/2330 train_time:38161ms step_avg:57.30ms
step:667/2330 train_time:38217ms step_avg:57.30ms
step:668/2330 train_time:38275ms step_avg:57.30ms
step:669/2330 train_time:38331ms step_avg:57.30ms
step:670/2330 train_time:38390ms step_avg:57.30ms
step:671/2330 train_time:38446ms step_avg:57.30ms
step:672/2330 train_time:38506ms step_avg:57.30ms
step:673/2330 train_time:38563ms step_avg:57.30ms
step:674/2330 train_time:38622ms step_avg:57.30ms
step:675/2330 train_time:38678ms step_avg:57.30ms
step:676/2330 train_time:38736ms step_avg:57.30ms
step:677/2330 train_time:38792ms step_avg:57.30ms
step:678/2330 train_time:38852ms step_avg:57.30ms
step:679/2330 train_time:38909ms step_avg:57.30ms
step:680/2330 train_time:38967ms step_avg:57.30ms
step:681/2330 train_time:39024ms step_avg:57.30ms
step:682/2330 train_time:39084ms step_avg:57.31ms
step:683/2330 train_time:39140ms step_avg:57.31ms
step:684/2330 train_time:39199ms step_avg:57.31ms
step:685/2330 train_time:39255ms step_avg:57.31ms
step:686/2330 train_time:39313ms step_avg:57.31ms
step:687/2330 train_time:39369ms step_avg:57.31ms
step:688/2330 train_time:39428ms step_avg:57.31ms
step:689/2330 train_time:39484ms step_avg:57.31ms
step:690/2330 train_time:39543ms step_avg:57.31ms
step:691/2330 train_time:39600ms step_avg:57.31ms
step:692/2330 train_time:39659ms step_avg:57.31ms
step:693/2330 train_time:39715ms step_avg:57.31ms
step:694/2330 train_time:39774ms step_avg:57.31ms
step:695/2330 train_time:39830ms step_avg:57.31ms
step:696/2330 train_time:39889ms step_avg:57.31ms
step:697/2330 train_time:39945ms step_avg:57.31ms
step:698/2330 train_time:40004ms step_avg:57.31ms
step:699/2330 train_time:40061ms step_avg:57.31ms
step:700/2330 train_time:40120ms step_avg:57.31ms
step:701/2330 train_time:40176ms step_avg:57.31ms
step:702/2330 train_time:40235ms step_avg:57.32ms
step:703/2330 train_time:40291ms step_avg:57.31ms
step:704/2330 train_time:40351ms step_avg:57.32ms
step:705/2330 train_time:40407ms step_avg:57.32ms
step:706/2330 train_time:40466ms step_avg:57.32ms
step:707/2330 train_time:40522ms step_avg:57.32ms
step:708/2330 train_time:40581ms step_avg:57.32ms
step:709/2330 train_time:40637ms step_avg:57.32ms
step:710/2330 train_time:40698ms step_avg:57.32ms
step:711/2330 train_time:40754ms step_avg:57.32ms
step:712/2330 train_time:40813ms step_avg:57.32ms
step:713/2330 train_time:40869ms step_avg:57.32ms
step:714/2330 train_time:40929ms step_avg:57.32ms
step:715/2330 train_time:40985ms step_avg:57.32ms
step:716/2330 train_time:41045ms step_avg:57.33ms
step:717/2330 train_time:41101ms step_avg:57.32ms
step:718/2330 train_time:41161ms step_avg:57.33ms
step:719/2330 train_time:41218ms step_avg:57.33ms
step:720/2330 train_time:41276ms step_avg:57.33ms
step:721/2330 train_time:41333ms step_avg:57.33ms
step:722/2330 train_time:41391ms step_avg:57.33ms
step:723/2330 train_time:41447ms step_avg:57.33ms
step:724/2330 train_time:41507ms step_avg:57.33ms
step:725/2330 train_time:41564ms step_avg:57.33ms
step:726/2330 train_time:41623ms step_avg:57.33ms
step:727/2330 train_time:41680ms step_avg:57.33ms
step:728/2330 train_time:41738ms step_avg:57.33ms
step:729/2330 train_time:41794ms step_avg:57.33ms
step:730/2330 train_time:41853ms step_avg:57.33ms
step:731/2330 train_time:41909ms step_avg:57.33ms
step:732/2330 train_time:41968ms step_avg:57.33ms
step:733/2330 train_time:42025ms step_avg:57.33ms
step:734/2330 train_time:42084ms step_avg:57.34ms
step:735/2330 train_time:42140ms step_avg:57.33ms
step:736/2330 train_time:42200ms step_avg:57.34ms
step:737/2330 train_time:42257ms step_avg:57.34ms
step:738/2330 train_time:42316ms step_avg:57.34ms
step:739/2330 train_time:42372ms step_avg:57.34ms
step:740/2330 train_time:42431ms step_avg:57.34ms
step:741/2330 train_time:42487ms step_avg:57.34ms
step:742/2330 train_time:42547ms step_avg:57.34ms
step:743/2330 train_time:42604ms step_avg:57.34ms
step:744/2330 train_time:42663ms step_avg:57.34ms
step:745/2330 train_time:42719ms step_avg:57.34ms
step:746/2330 train_time:42778ms step_avg:57.34ms
step:747/2330 train_time:42834ms step_avg:57.34ms
step:748/2330 train_time:42893ms step_avg:57.34ms
step:749/2330 train_time:42949ms step_avg:57.34ms
step:750/2330 train_time:43008ms step_avg:57.34ms
step:750/2330 val_loss:5.7393 train_time:43087ms step_avg:57.45ms
step:751/2330 train_time:43106ms step_avg:57.40ms
step:752/2330 train_time:43125ms step_avg:57.35ms
step:753/2330 train_time:43183ms step_avg:57.35ms
step:754/2330 train_time:43249ms step_avg:57.36ms
step:755/2330 train_time:43305ms step_avg:57.36ms
step:756/2330 train_time:43367ms step_avg:57.36ms
step:757/2330 train_time:43423ms step_avg:57.36ms
step:758/2330 train_time:43483ms step_avg:57.37ms
step:759/2330 train_time:43540ms step_avg:57.36ms
step:760/2330 train_time:43599ms step_avg:57.37ms
step:761/2330 train_time:43654ms step_avg:57.36ms
step:762/2330 train_time:43713ms step_avg:57.37ms
step:763/2330 train_time:43768ms step_avg:57.36ms
step:764/2330 train_time:43826ms step_avg:57.36ms
step:765/2330 train_time:43884ms step_avg:57.36ms
step:766/2330 train_time:43941ms step_avg:57.36ms
step:767/2330 train_time:43998ms step_avg:57.36ms
step:768/2330 train_time:44057ms step_avg:57.37ms
step:769/2330 train_time:44114ms step_avg:57.37ms
step:770/2330 train_time:44174ms step_avg:57.37ms
step:771/2330 train_time:44232ms step_avg:57.37ms
step:772/2330 train_time:44294ms step_avg:57.38ms
step:773/2330 train_time:44351ms step_avg:57.37ms
step:774/2330 train_time:44411ms step_avg:57.38ms
step:775/2330 train_time:44468ms step_avg:57.38ms
step:776/2330 train_time:44528ms step_avg:57.38ms
step:777/2330 train_time:44586ms step_avg:57.38ms
step:778/2330 train_time:44646ms step_avg:57.39ms
step:779/2330 train_time:44703ms step_avg:57.39ms
step:780/2330 train_time:44763ms step_avg:57.39ms
step:781/2330 train_time:44820ms step_avg:57.39ms
step:782/2330 train_time:44879ms step_avg:57.39ms
step:783/2330 train_time:44936ms step_avg:57.39ms
step:784/2330 train_time:44996ms step_avg:57.39ms
step:785/2330 train_time:45053ms step_avg:57.39ms
step:786/2330 train_time:45112ms step_avg:57.39ms
step:787/2330 train_time:45170ms step_avg:57.40ms
step:788/2330 train_time:45230ms step_avg:57.40ms
step:789/2330 train_time:45288ms step_avg:57.40ms
step:790/2330 train_time:45348ms step_avg:57.40ms
step:791/2330 train_time:45406ms step_avg:57.40ms
step:792/2330 train_time:45466ms step_avg:57.41ms
step:793/2330 train_time:45523ms step_avg:57.41ms
step:794/2330 train_time:45583ms step_avg:57.41ms
step:795/2330 train_time:45641ms step_avg:57.41ms
step:796/2330 train_time:45701ms step_avg:57.41ms
step:797/2330 train_time:45758ms step_avg:57.41ms
step:798/2330 train_time:45817ms step_avg:57.41ms
step:799/2330 train_time:45873ms step_avg:57.41ms
step:800/2330 train_time:45933ms step_avg:57.42ms
step:801/2330 train_time:45990ms step_avg:57.42ms
step:802/2330 train_time:46050ms step_avg:57.42ms
step:803/2330 train_time:46107ms step_avg:57.42ms
step:804/2330 train_time:46167ms step_avg:57.42ms
step:805/2330 train_time:46224ms step_avg:57.42ms
step:806/2330 train_time:46285ms step_avg:57.43ms
step:807/2330 train_time:46343ms step_avg:57.43ms
step:808/2330 train_time:46403ms step_avg:57.43ms
step:809/2330 train_time:46460ms step_avg:57.43ms
step:810/2330 train_time:46521ms step_avg:57.43ms
step:811/2330 train_time:46578ms step_avg:57.43ms
step:812/2330 train_time:46638ms step_avg:57.44ms
step:813/2330 train_time:46695ms step_avg:57.44ms
step:814/2330 train_time:46754ms step_avg:57.44ms
step:815/2330 train_time:46811ms step_avg:57.44ms
step:816/2330 train_time:46871ms step_avg:57.44ms
step:817/2330 train_time:46928ms step_avg:57.44ms
step:818/2330 train_time:46987ms step_avg:57.44ms
step:819/2330 train_time:47044ms step_avg:57.44ms
step:820/2330 train_time:47104ms step_avg:57.44ms
step:821/2330 train_time:47161ms step_avg:57.44ms
step:822/2330 train_time:47221ms step_avg:57.45ms
step:823/2330 train_time:47279ms step_avg:57.45ms
step:824/2330 train_time:47340ms step_avg:57.45ms
step:825/2330 train_time:47397ms step_avg:57.45ms
step:826/2330 train_time:47457ms step_avg:57.45ms
step:827/2330 train_time:47514ms step_avg:57.45ms
step:828/2330 train_time:47574ms step_avg:57.46ms
step:829/2330 train_time:47631ms step_avg:57.46ms
step:830/2330 train_time:47691ms step_avg:57.46ms
step:831/2330 train_time:47748ms step_avg:57.46ms
step:832/2330 train_time:47808ms step_avg:57.46ms
step:833/2330 train_time:47864ms step_avg:57.46ms
step:834/2330 train_time:47925ms step_avg:57.46ms
step:835/2330 train_time:47981ms step_avg:57.46ms
step:836/2330 train_time:48042ms step_avg:57.47ms
step:837/2330 train_time:48099ms step_avg:57.47ms
step:838/2330 train_time:48160ms step_avg:57.47ms
step:839/2330 train_time:48216ms step_avg:57.47ms
step:840/2330 train_time:48279ms step_avg:57.47ms
step:841/2330 train_time:48336ms step_avg:57.47ms
step:842/2330 train_time:48396ms step_avg:57.48ms
step:843/2330 train_time:48454ms step_avg:57.48ms
step:844/2330 train_time:48513ms step_avg:57.48ms
step:845/2330 train_time:48570ms step_avg:57.48ms
step:846/2330 train_time:48630ms step_avg:57.48ms
step:847/2330 train_time:48687ms step_avg:57.48ms
step:848/2330 train_time:48748ms step_avg:57.49ms
step:849/2330 train_time:48804ms step_avg:57.48ms
step:850/2330 train_time:48864ms step_avg:57.49ms
step:851/2330 train_time:48921ms step_avg:57.49ms
step:852/2330 train_time:48982ms step_avg:57.49ms
step:853/2330 train_time:49038ms step_avg:57.49ms
step:854/2330 train_time:49099ms step_avg:57.49ms
step:855/2330 train_time:49156ms step_avg:57.49ms
step:856/2330 train_time:49215ms step_avg:57.49ms
step:857/2330 train_time:49273ms step_avg:57.49ms
step:858/2330 train_time:49332ms step_avg:57.50ms
step:859/2330 train_time:49389ms step_avg:57.50ms
step:860/2330 train_time:49449ms step_avg:57.50ms
step:861/2330 train_time:49507ms step_avg:57.50ms
step:862/2330 train_time:49566ms step_avg:57.50ms
step:863/2330 train_time:49624ms step_avg:57.50ms
step:864/2330 train_time:49684ms step_avg:57.51ms
step:865/2330 train_time:49741ms step_avg:57.50ms
step:866/2330 train_time:49801ms step_avg:57.51ms
step:867/2330 train_time:49858ms step_avg:57.51ms
step:868/2330 train_time:49919ms step_avg:57.51ms
step:869/2330 train_time:49976ms step_avg:57.51ms
step:870/2330 train_time:50037ms step_avg:57.51ms
step:871/2330 train_time:50094ms step_avg:57.51ms
step:872/2330 train_time:50154ms step_avg:57.52ms
step:873/2330 train_time:50211ms step_avg:57.52ms
step:874/2330 train_time:50271ms step_avg:57.52ms
step:875/2330 train_time:50328ms step_avg:57.52ms
step:876/2330 train_time:50388ms step_avg:57.52ms
step:877/2330 train_time:50445ms step_avg:57.52ms
step:878/2330 train_time:50506ms step_avg:57.52ms
step:879/2330 train_time:50563ms step_avg:57.52ms
step:880/2330 train_time:50624ms step_avg:57.53ms
step:881/2330 train_time:50681ms step_avg:57.53ms
step:882/2330 train_time:50741ms step_avg:57.53ms
step:883/2330 train_time:50799ms step_avg:57.53ms
step:884/2330 train_time:50859ms step_avg:57.53ms
step:885/2330 train_time:50916ms step_avg:57.53ms
step:886/2330 train_time:50976ms step_avg:57.54ms
step:887/2330 train_time:51034ms step_avg:57.54ms
step:888/2330 train_time:51093ms step_avg:57.54ms
step:889/2330 train_time:51150ms step_avg:57.54ms
step:890/2330 train_time:51210ms step_avg:57.54ms
step:891/2330 train_time:51267ms step_avg:57.54ms
step:892/2330 train_time:51327ms step_avg:57.54ms
step:893/2330 train_time:51385ms step_avg:57.54ms
step:894/2330 train_time:51445ms step_avg:57.54ms
step:895/2330 train_time:51502ms step_avg:57.54ms
step:896/2330 train_time:51562ms step_avg:57.55ms
step:897/2330 train_time:51619ms step_avg:57.55ms
step:898/2330 train_time:51679ms step_avg:57.55ms
step:899/2330 train_time:51736ms step_avg:57.55ms
step:900/2330 train_time:51796ms step_avg:57.55ms
step:901/2330 train_time:51853ms step_avg:57.55ms
step:902/2330 train_time:51913ms step_avg:57.55ms
step:903/2330 train_time:51970ms step_avg:57.55ms
step:904/2330 train_time:52030ms step_avg:57.56ms
step:905/2330 train_time:52088ms step_avg:57.56ms
step:906/2330 train_time:52148ms step_avg:57.56ms
step:907/2330 train_time:52205ms step_avg:57.56ms
step:908/2330 train_time:52265ms step_avg:57.56ms
step:909/2330 train_time:52322ms step_avg:57.56ms
step:910/2330 train_time:52383ms step_avg:57.56ms
step:911/2330 train_time:52441ms step_avg:57.56ms
step:912/2330 train_time:52501ms step_avg:57.57ms
step:913/2330 train_time:52558ms step_avg:57.57ms
step:914/2330 train_time:52618ms step_avg:57.57ms
step:915/2330 train_time:52675ms step_avg:57.57ms
step:916/2330 train_time:52734ms step_avg:57.57ms
step:917/2330 train_time:52791ms step_avg:57.57ms
step:918/2330 train_time:52851ms step_avg:57.57ms
step:919/2330 train_time:52908ms step_avg:57.57ms
step:920/2330 train_time:52968ms step_avg:57.57ms
step:921/2330 train_time:53025ms step_avg:57.57ms
step:922/2330 train_time:53085ms step_avg:57.58ms
step:923/2330 train_time:53143ms step_avg:57.58ms
step:924/2330 train_time:53203ms step_avg:57.58ms
step:925/2330 train_time:53260ms step_avg:57.58ms
step:926/2330 train_time:53319ms step_avg:57.58ms
step:927/2330 train_time:53376ms step_avg:57.58ms
step:928/2330 train_time:53437ms step_avg:57.58ms
step:929/2330 train_time:53495ms step_avg:57.58ms
step:930/2330 train_time:53553ms step_avg:57.58ms
step:931/2330 train_time:53610ms step_avg:57.58ms
step:932/2330 train_time:53671ms step_avg:57.59ms
step:933/2330 train_time:53727ms step_avg:57.59ms
step:934/2330 train_time:53788ms step_avg:57.59ms
step:935/2330 train_time:53846ms step_avg:57.59ms
step:936/2330 train_time:53905ms step_avg:57.59ms
step:937/2330 train_time:53963ms step_avg:57.59ms
step:938/2330 train_time:54022ms step_avg:57.59ms
step:939/2330 train_time:54079ms step_avg:57.59ms
step:940/2330 train_time:54141ms step_avg:57.60ms
step:941/2330 train_time:54199ms step_avg:57.60ms
step:942/2330 train_time:54258ms step_avg:57.60ms
step:943/2330 train_time:54315ms step_avg:57.60ms
step:944/2330 train_time:54375ms step_avg:57.60ms
step:945/2330 train_time:54432ms step_avg:57.60ms
step:946/2330 train_time:54492ms step_avg:57.60ms
step:947/2330 train_time:54549ms step_avg:57.60ms
step:948/2330 train_time:54609ms step_avg:57.60ms
step:949/2330 train_time:54666ms step_avg:57.60ms
step:950/2330 train_time:54726ms step_avg:57.61ms
step:951/2330 train_time:54783ms step_avg:57.61ms
step:952/2330 train_time:54843ms step_avg:57.61ms
step:953/2330 train_time:54900ms step_avg:57.61ms
step:954/2330 train_time:54960ms step_avg:57.61ms
step:955/2330 train_time:55017ms step_avg:57.61ms
step:956/2330 train_time:55079ms step_avg:57.61ms
step:957/2330 train_time:55135ms step_avg:57.61ms
step:958/2330 train_time:55196ms step_avg:57.62ms
step:959/2330 train_time:55253ms step_avg:57.61ms
step:960/2330 train_time:55313ms step_avg:57.62ms
step:961/2330 train_time:55370ms step_avg:57.62ms
step:962/2330 train_time:55429ms step_avg:57.62ms
step:963/2330 train_time:55487ms step_avg:57.62ms
step:964/2330 train_time:55547ms step_avg:57.62ms
step:965/2330 train_time:55604ms step_avg:57.62ms
step:966/2330 train_time:55664ms step_avg:57.62ms
step:967/2330 train_time:55721ms step_avg:57.62ms
step:968/2330 train_time:55781ms step_avg:57.63ms
step:969/2330 train_time:55838ms step_avg:57.62ms
step:970/2330 train_time:55899ms step_avg:57.63ms
step:971/2330 train_time:55956ms step_avg:57.63ms
step:972/2330 train_time:56016ms step_avg:57.63ms
step:973/2330 train_time:56073ms step_avg:57.63ms
step:974/2330 train_time:56133ms step_avg:57.63ms
step:975/2330 train_time:56190ms step_avg:57.63ms
step:976/2330 train_time:56250ms step_avg:57.63ms
step:977/2330 train_time:56307ms step_avg:57.63ms
step:978/2330 train_time:56367ms step_avg:57.64ms
step:979/2330 train_time:56424ms step_avg:57.63ms
step:980/2330 train_time:56485ms step_avg:57.64ms
step:981/2330 train_time:56542ms step_avg:57.64ms
step:982/2330 train_time:56602ms step_avg:57.64ms
step:983/2330 train_time:56660ms step_avg:57.64ms
step:984/2330 train_time:56719ms step_avg:57.64ms
step:985/2330 train_time:56777ms step_avg:57.64ms
step:986/2330 train_time:56837ms step_avg:57.64ms
step:987/2330 train_time:56894ms step_avg:57.64ms
step:988/2330 train_time:56954ms step_avg:57.65ms
step:989/2330 train_time:57011ms step_avg:57.65ms
step:990/2330 train_time:57071ms step_avg:57.65ms
step:991/2330 train_time:57128ms step_avg:57.65ms
step:992/2330 train_time:57189ms step_avg:57.65ms
step:993/2330 train_time:57246ms step_avg:57.65ms
step:994/2330 train_time:57306ms step_avg:57.65ms
step:995/2330 train_time:57363ms step_avg:57.65ms
step:996/2330 train_time:57423ms step_avg:57.65ms
step:997/2330 train_time:57481ms step_avg:57.65ms
step:998/2330 train_time:57541ms step_avg:57.66ms
step:999/2330 train_time:57600ms step_avg:57.66ms
step:1000/2330 train_time:57659ms step_avg:57.66ms
step:1000/2330 val_loss:5.6887 train_time:57741ms step_avg:57.74ms
step:1001/2330 train_time:57758ms step_avg:57.70ms
step:1002/2330 train_time:57778ms step_avg:57.66ms
step:1003/2330 train_time:57833ms step_avg:57.66ms
step:1004/2330 train_time:57902ms step_avg:57.67ms
step:1005/2330 train_time:57957ms step_avg:57.67ms
step:1006/2330 train_time:58021ms step_avg:57.67ms
step:1007/2330 train_time:58077ms step_avg:57.67ms
step:1008/2330 train_time:58138ms step_avg:57.68ms
step:1009/2330 train_time:58194ms step_avg:57.67ms
step:1010/2330 train_time:58253ms step_avg:57.68ms
step:1011/2330 train_time:58310ms step_avg:57.68ms
step:1012/2330 train_time:58369ms step_avg:57.68ms
step:1013/2330 train_time:58426ms step_avg:57.68ms
step:1014/2330 train_time:58485ms step_avg:57.68ms
step:1015/2330 train_time:58541ms step_avg:57.68ms
step:1016/2330 train_time:58600ms step_avg:57.68ms
step:1017/2330 train_time:58661ms step_avg:57.68ms
step:1018/2330 train_time:58723ms step_avg:57.68ms
step:1019/2330 train_time:58782ms step_avg:57.69ms
step:1020/2330 train_time:58842ms step_avg:57.69ms
step:1021/2330 train_time:58899ms step_avg:57.69ms
step:1022/2330 train_time:58960ms step_avg:57.69ms
step:1023/2330 train_time:59017ms step_avg:57.69ms
step:1024/2330 train_time:59078ms step_avg:57.69ms
step:1025/2330 train_time:59135ms step_avg:57.69ms
step:1026/2330 train_time:59194ms step_avg:57.69ms
step:1027/2330 train_time:59251ms step_avg:57.69ms
step:1028/2330 train_time:59310ms step_avg:57.69ms
step:1029/2330 train_time:59366ms step_avg:57.69ms
step:1030/2330 train_time:59426ms step_avg:57.70ms
step:1031/2330 train_time:59483ms step_avg:57.69ms
step:1032/2330 train_time:59543ms step_avg:57.70ms
step:1033/2330 train_time:59601ms step_avg:57.70ms
step:1034/2330 train_time:59661ms step_avg:57.70ms
step:1035/2330 train_time:59719ms step_avg:57.70ms
step:1036/2330 train_time:59779ms step_avg:57.70ms
step:1037/2330 train_time:59837ms step_avg:57.70ms
step:1038/2330 train_time:59898ms step_avg:57.71ms
step:1039/2330 train_time:59956ms step_avg:57.71ms
step:1040/2330 train_time:60016ms step_avg:57.71ms
step:1041/2330 train_time:60074ms step_avg:57.71ms
step:1042/2330 train_time:60134ms step_avg:57.71ms
step:1043/2330 train_time:60192ms step_avg:57.71ms
step:1044/2330 train_time:60252ms step_avg:57.71ms
step:1045/2330 train_time:60308ms step_avg:57.71ms
step:1046/2330 train_time:60368ms step_avg:57.71ms
step:1047/2330 train_time:60424ms step_avg:57.71ms
step:1048/2330 train_time:60485ms step_avg:57.71ms
step:1049/2330 train_time:60541ms step_avg:57.71ms
step:1050/2330 train_time:60601ms step_avg:57.71ms
step:1051/2330 train_time:60658ms step_avg:57.71ms
step:1052/2330 train_time:60718ms step_avg:57.72ms
step:1053/2330 train_time:60776ms step_avg:57.72ms
step:1054/2330 train_time:60836ms step_avg:57.72ms
step:1055/2330 train_time:60893ms step_avg:57.72ms
step:1056/2330 train_time:60953ms step_avg:57.72ms
step:1057/2330 train_time:61011ms step_avg:57.72ms
step:1058/2330 train_time:61070ms step_avg:57.72ms
step:1059/2330 train_time:61126ms step_avg:57.72ms
step:1060/2330 train_time:61187ms step_avg:57.72ms
step:1061/2330 train_time:61244ms step_avg:57.72ms
step:1062/2330 train_time:61304ms step_avg:57.73ms
step:1063/2330 train_time:61362ms step_avg:57.72ms
step:1064/2330 train_time:61421ms step_avg:57.73ms
step:1065/2330 train_time:61478ms step_avg:57.73ms
step:1066/2330 train_time:61538ms step_avg:57.73ms
step:1067/2330 train_time:61596ms step_avg:57.73ms
step:1068/2330 train_time:61655ms step_avg:57.73ms
step:1069/2330 train_time:61712ms step_avg:57.73ms
step:1070/2330 train_time:61772ms step_avg:57.73ms
step:1071/2330 train_time:61829ms step_avg:57.73ms
step:1072/2330 train_time:61890ms step_avg:57.73ms
step:1073/2330 train_time:61946ms step_avg:57.73ms
step:1074/2330 train_time:62007ms step_avg:57.73ms
step:1075/2330 train_time:62064ms step_avg:57.73ms
step:1076/2330 train_time:62124ms step_avg:57.74ms
step:1077/2330 train_time:62180ms step_avg:57.73ms
step:1078/2330 train_time:62240ms step_avg:57.74ms
step:1079/2330 train_time:62297ms step_avg:57.74ms
step:1080/2330 train_time:62357ms step_avg:57.74ms
step:1081/2330 train_time:62414ms step_avg:57.74ms
step:1082/2330 train_time:62474ms step_avg:57.74ms
step:1083/2330 train_time:62531ms step_avg:57.74ms
step:1084/2330 train_time:62591ms step_avg:57.74ms
step:1085/2330 train_time:62648ms step_avg:57.74ms
step:1086/2330 train_time:62708ms step_avg:57.74ms
step:1087/2330 train_time:62765ms step_avg:57.74ms
step:1088/2330 train_time:62825ms step_avg:57.74ms
step:1089/2330 train_time:62882ms step_avg:57.74ms
step:1090/2330 train_time:62942ms step_avg:57.75ms
step:1091/2330 train_time:62999ms step_avg:57.74ms
step:1092/2330 train_time:63059ms step_avg:57.75ms
step:1093/2330 train_time:63117ms step_avg:57.75ms
step:1094/2330 train_time:63177ms step_avg:57.75ms
step:1095/2330 train_time:63234ms step_avg:57.75ms
step:1096/2330 train_time:63294ms step_avg:57.75ms
step:1097/2330 train_time:63352ms step_avg:57.75ms
step:1098/2330 train_time:63412ms step_avg:57.75ms
step:1099/2330 train_time:63469ms step_avg:57.75ms
step:1100/2330 train_time:63529ms step_avg:57.75ms
step:1101/2330 train_time:63586ms step_avg:57.75ms
step:1102/2330 train_time:63646ms step_avg:57.75ms
step:1103/2330 train_time:63702ms step_avg:57.75ms
step:1104/2330 train_time:63763ms step_avg:57.76ms
step:1105/2330 train_time:63820ms step_avg:57.76ms
step:1106/2330 train_time:63880ms step_avg:57.76ms
step:1107/2330 train_time:63937ms step_avg:57.76ms
step:1108/2330 train_time:63997ms step_avg:57.76ms
step:1109/2330 train_time:64055ms step_avg:57.76ms
step:1110/2330 train_time:64115ms step_avg:57.76ms
step:1111/2330 train_time:64173ms step_avg:57.76ms
step:1112/2330 train_time:64232ms step_avg:57.76ms
step:1113/2330 train_time:64289ms step_avg:57.76ms
step:1114/2330 train_time:64349ms step_avg:57.76ms
step:1115/2330 train_time:64407ms step_avg:57.76ms
step:1116/2330 train_time:64466ms step_avg:57.77ms
step:1117/2330 train_time:64523ms step_avg:57.76ms
step:1118/2330 train_time:64583ms step_avg:57.77ms
step:1119/2330 train_time:64640ms step_avg:57.77ms
step:1120/2330 train_time:64700ms step_avg:57.77ms
step:1121/2330 train_time:64757ms step_avg:57.77ms
step:1122/2330 train_time:64818ms step_avg:57.77ms
step:1123/2330 train_time:64875ms step_avg:57.77ms
step:1124/2330 train_time:64935ms step_avg:57.77ms
step:1125/2330 train_time:64992ms step_avg:57.77ms
step:1126/2330 train_time:65052ms step_avg:57.77ms
step:1127/2330 train_time:65109ms step_avg:57.77ms
step:1128/2330 train_time:65169ms step_avg:57.77ms
step:1129/2330 train_time:65226ms step_avg:57.77ms
step:1130/2330 train_time:65285ms step_avg:57.77ms
step:1131/2330 train_time:65342ms step_avg:57.77ms
step:1132/2330 train_time:65402ms step_avg:57.78ms
step:1133/2330 train_time:65460ms step_avg:57.78ms
step:1134/2330 train_time:65520ms step_avg:57.78ms
step:1135/2330 train_time:65577ms step_avg:57.78ms
step:1136/2330 train_time:65637ms step_avg:57.78ms
step:1137/2330 train_time:65695ms step_avg:57.78ms
step:1138/2330 train_time:65754ms step_avg:57.78ms
step:1139/2330 train_time:65811ms step_avg:57.78ms
step:1140/2330 train_time:65872ms step_avg:57.78ms
step:1141/2330 train_time:65929ms step_avg:57.78ms
step:1142/2330 train_time:65990ms step_avg:57.78ms
step:1143/2330 train_time:66048ms step_avg:57.78ms
step:1144/2330 train_time:66109ms step_avg:57.79ms
step:1145/2330 train_time:66165ms step_avg:57.79ms
step:1146/2330 train_time:66225ms step_avg:57.79ms
step:1147/2330 train_time:66281ms step_avg:57.79ms
step:1148/2330 train_time:66342ms step_avg:57.79ms
step:1149/2330 train_time:66399ms step_avg:57.79ms
step:1150/2330 train_time:66458ms step_avg:57.79ms
step:1151/2330 train_time:66516ms step_avg:57.79ms
step:1152/2330 train_time:66576ms step_avg:57.79ms
step:1153/2330 train_time:66633ms step_avg:57.79ms
step:1154/2330 train_time:66693ms step_avg:57.79ms
step:1155/2330 train_time:66751ms step_avg:57.79ms
step:1156/2330 train_time:66810ms step_avg:57.79ms
step:1157/2330 train_time:66867ms step_avg:57.79ms
step:1158/2330 train_time:66927ms step_avg:57.80ms
step:1159/2330 train_time:66984ms step_avg:57.79ms
step:1160/2330 train_time:67045ms step_avg:57.80ms
step:1161/2330 train_time:67101ms step_avg:57.80ms
step:1162/2330 train_time:67161ms step_avg:57.80ms
step:1163/2330 train_time:67218ms step_avg:57.80ms
step:1164/2330 train_time:67278ms step_avg:57.80ms
step:1165/2330 train_time:67335ms step_avg:57.80ms
step:1166/2330 train_time:67394ms step_avg:57.80ms
step:1167/2330 train_time:67451ms step_avg:57.80ms
step:1168/2330 train_time:67511ms step_avg:57.80ms
step:1169/2330 train_time:67567ms step_avg:57.80ms
step:1170/2330 train_time:67628ms step_avg:57.80ms
step:1171/2330 train_time:67686ms step_avg:57.80ms
step:1172/2330 train_time:67745ms step_avg:57.80ms
step:1173/2330 train_time:67802ms step_avg:57.80ms
step:1174/2330 train_time:67862ms step_avg:57.80ms
step:1175/2330 train_time:67919ms step_avg:57.80ms
step:1176/2330 train_time:67980ms step_avg:57.81ms
step:1177/2330 train_time:68037ms step_avg:57.81ms
step:1178/2330 train_time:68098ms step_avg:57.81ms
step:1179/2330 train_time:68155ms step_avg:57.81ms
step:1180/2330 train_time:68215ms step_avg:57.81ms
step:1181/2330 train_time:68272ms step_avg:57.81ms
step:1182/2330 train_time:68332ms step_avg:57.81ms
step:1183/2330 train_time:68389ms step_avg:57.81ms
step:1184/2330 train_time:68449ms step_avg:57.81ms
step:1185/2330 train_time:68505ms step_avg:57.81ms
step:1186/2330 train_time:68566ms step_avg:57.81ms
step:1187/2330 train_time:68623ms step_avg:57.81ms
step:1188/2330 train_time:68683ms step_avg:57.81ms
step:1189/2330 train_time:68739ms step_avg:57.81ms
step:1190/2330 train_time:68800ms step_avg:57.81ms
step:1191/2330 train_time:68857ms step_avg:57.81ms
step:1192/2330 train_time:68918ms step_avg:57.82ms
step:1193/2330 train_time:68976ms step_avg:57.82ms
step:1194/2330 train_time:69036ms step_avg:57.82ms
step:1195/2330 train_time:69092ms step_avg:57.82ms
step:1196/2330 train_time:69153ms step_avg:57.82ms
step:1197/2330 train_time:69209ms step_avg:57.82ms
step:1198/2330 train_time:69271ms step_avg:57.82ms
step:1199/2330 train_time:69327ms step_avg:57.82ms
step:1200/2330 train_time:69388ms step_avg:57.82ms
step:1201/2330 train_time:69445ms step_avg:57.82ms
step:1202/2330 train_time:69504ms step_avg:57.82ms
step:1203/2330 train_time:69561ms step_avg:57.82ms
step:1204/2330 train_time:69621ms step_avg:57.82ms
step:1205/2330 train_time:69678ms step_avg:57.82ms
step:1206/2330 train_time:69738ms step_avg:57.83ms
step:1207/2330 train_time:69795ms step_avg:57.83ms
step:1208/2330 train_time:69856ms step_avg:57.83ms
step:1209/2330 train_time:69913ms step_avg:57.83ms
step:1210/2330 train_time:69973ms step_avg:57.83ms
step:1211/2330 train_time:70031ms step_avg:57.83ms
step:1212/2330 train_time:70091ms step_avg:57.83ms
step:1213/2330 train_time:70148ms step_avg:57.83ms
step:1214/2330 train_time:70208ms step_avg:57.83ms
step:1215/2330 train_time:70265ms step_avg:57.83ms
step:1216/2330 train_time:70325ms step_avg:57.83ms
step:1217/2330 train_time:70381ms step_avg:57.83ms
step:1218/2330 train_time:70442ms step_avg:57.83ms
step:1219/2330 train_time:70500ms step_avg:57.83ms
step:1220/2330 train_time:70559ms step_avg:57.84ms
step:1221/2330 train_time:70616ms step_avg:57.83ms
step:1222/2330 train_time:70676ms step_avg:57.84ms
step:1223/2330 train_time:70732ms step_avg:57.84ms
step:1224/2330 train_time:70793ms step_avg:57.84ms
step:1225/2330 train_time:70850ms step_avg:57.84ms
step:1226/2330 train_time:70910ms step_avg:57.84ms
step:1227/2330 train_time:70967ms step_avg:57.84ms
step:1228/2330 train_time:71026ms step_avg:57.84ms
step:1229/2330 train_time:71083ms step_avg:57.84ms
step:1230/2330 train_time:71143ms step_avg:57.84ms
step:1231/2330 train_time:71200ms step_avg:57.84ms
step:1232/2330 train_time:71261ms step_avg:57.84ms
step:1233/2330 train_time:71317ms step_avg:57.84ms
step:1234/2330 train_time:71378ms step_avg:57.84ms
step:1235/2330 train_time:71435ms step_avg:57.84ms
step:1236/2330 train_time:71497ms step_avg:57.85ms
step:1237/2330 train_time:71554ms step_avg:57.84ms
step:1238/2330 train_time:71614ms step_avg:57.85ms
step:1239/2330 train_time:71671ms step_avg:57.85ms
step:1240/2330 train_time:71731ms step_avg:57.85ms
step:1241/2330 train_time:71787ms step_avg:57.85ms
step:1242/2330 train_time:71849ms step_avg:57.85ms
step:1243/2330 train_time:71906ms step_avg:57.85ms
step:1244/2330 train_time:71965ms step_avg:57.85ms
step:1245/2330 train_time:72022ms step_avg:57.85ms
step:1246/2330 train_time:72082ms step_avg:57.85ms
step:1247/2330 train_time:72139ms step_avg:57.85ms
step:1248/2330 train_time:72199ms step_avg:57.85ms
step:1249/2330 train_time:72256ms step_avg:57.85ms
step:1250/2330 train_time:72316ms step_avg:57.85ms
step:1250/2330 val_loss:5.3230 train_time:72396ms step_avg:57.92ms
step:1251/2330 train_time:72413ms step_avg:57.88ms
step:1252/2330 train_time:72435ms step_avg:57.86ms
step:1253/2330 train_time:72493ms step_avg:57.86ms
step:1254/2330 train_time:72556ms step_avg:57.86ms
step:1255/2330 train_time:72613ms step_avg:57.86ms
step:1256/2330 train_time:72674ms step_avg:57.86ms
step:1257/2330 train_time:72731ms step_avg:57.86ms
step:1258/2330 train_time:72792ms step_avg:57.86ms
step:1259/2330 train_time:72848ms step_avg:57.86ms
step:1260/2330 train_time:72908ms step_avg:57.86ms
step:1261/2330 train_time:72965ms step_avg:57.86ms
step:1262/2330 train_time:73024ms step_avg:57.86ms
step:1263/2330 train_time:73080ms step_avg:57.86ms
step:1264/2330 train_time:73139ms step_avg:57.86ms
step:1265/2330 train_time:73196ms step_avg:57.86ms
step:1266/2330 train_time:73255ms step_avg:57.86ms
step:1267/2330 train_time:73313ms step_avg:57.86ms
step:1268/2330 train_time:73374ms step_avg:57.87ms
step:1269/2330 train_time:73433ms step_avg:57.87ms
step:1270/2330 train_time:73492ms step_avg:57.87ms
step:1271/2330 train_time:73550ms step_avg:57.87ms
step:1272/2330 train_time:73610ms step_avg:57.87ms
step:1273/2330 train_time:73668ms step_avg:57.87ms
step:1274/2330 train_time:73729ms step_avg:57.87ms
step:1275/2330 train_time:73785ms step_avg:57.87ms
step:1276/2330 train_time:73844ms step_avg:57.87ms
step:1277/2330 train_time:73900ms step_avg:57.87ms
step:1278/2330 train_time:73960ms step_avg:57.87ms
step:1279/2330 train_time:74017ms step_avg:57.87ms
step:1280/2330 train_time:74077ms step_avg:57.87ms
step:1281/2330 train_time:74133ms step_avg:57.87ms
step:1282/2330 train_time:74193ms step_avg:57.87ms
step:1283/2330 train_time:74249ms step_avg:57.87ms
step:1284/2330 train_time:74310ms step_avg:57.87ms
step:1285/2330 train_time:74366ms step_avg:57.87ms
step:1286/2330 train_time:74427ms step_avg:57.87ms
step:1287/2330 train_time:74484ms step_avg:57.87ms
step:1288/2330 train_time:74545ms step_avg:57.88ms
step:1289/2330 train_time:74602ms step_avg:57.88ms
step:1290/2330 train_time:74663ms step_avg:57.88ms
step:1291/2330 train_time:74720ms step_avg:57.88ms
step:1292/2330 train_time:74780ms step_avg:57.88ms
step:1293/2330 train_time:74836ms step_avg:57.88ms
step:1294/2330 train_time:74897ms step_avg:57.88ms
step:1295/2330 train_time:74954ms step_avg:57.88ms
step:1296/2330 train_time:75013ms step_avg:57.88ms
step:1297/2330 train_time:75070ms step_avg:57.88ms
step:1298/2330 train_time:75129ms step_avg:57.88ms
step:1299/2330 train_time:75185ms step_avg:57.88ms
step:1300/2330 train_time:75245ms step_avg:57.88ms
step:1301/2330 train_time:75301ms step_avg:57.88ms
step:1302/2330 train_time:75362ms step_avg:57.88ms
step:1303/2330 train_time:75420ms step_avg:57.88ms
step:1304/2330 train_time:75479ms step_avg:57.88ms
step:1305/2330 train_time:75537ms step_avg:57.88ms
step:1306/2330 train_time:75598ms step_avg:57.89ms
step:1307/2330 train_time:75655ms step_avg:57.88ms
step:1308/2330 train_time:75715ms step_avg:57.89ms
step:1309/2330 train_time:75772ms step_avg:57.89ms
step:1310/2330 train_time:75832ms step_avg:57.89ms
step:1311/2330 train_time:75889ms step_avg:57.89ms
step:1312/2330 train_time:75948ms step_avg:57.89ms
step:1313/2330 train_time:76005ms step_avg:57.89ms
step:1314/2330 train_time:76064ms step_avg:57.89ms
step:1315/2330 train_time:76121ms step_avg:57.89ms
step:1316/2330 train_time:76181ms step_avg:57.89ms
step:1317/2330 train_time:76237ms step_avg:57.89ms
step:1318/2330 train_time:76298ms step_avg:57.89ms
step:1319/2330 train_time:76355ms step_avg:57.89ms
step:1320/2330 train_time:76415ms step_avg:57.89ms
step:1321/2330 train_time:76473ms step_avg:57.89ms
step:1322/2330 train_time:76533ms step_avg:57.89ms
step:1323/2330 train_time:76590ms step_avg:57.89ms
step:1324/2330 train_time:76650ms step_avg:57.89ms
step:1325/2330 train_time:76707ms step_avg:57.89ms
step:1326/2330 train_time:76767ms step_avg:57.89ms
step:1327/2330 train_time:76823ms step_avg:57.89ms
step:1328/2330 train_time:76884ms step_avg:57.89ms
step:1329/2330 train_time:76941ms step_avg:57.89ms
step:1330/2330 train_time:77001ms step_avg:57.90ms
step:1331/2330 train_time:77057ms step_avg:57.89ms
step:1332/2330 train_time:77118ms step_avg:57.90ms
step:1333/2330 train_time:77174ms step_avg:57.90ms
step:1334/2330 train_time:77234ms step_avg:57.90ms
step:1335/2330 train_time:77291ms step_avg:57.90ms
step:1336/2330 train_time:77351ms step_avg:57.90ms
step:1337/2330 train_time:77408ms step_avg:57.90ms
step:1338/2330 train_time:77467ms step_avg:57.90ms
step:1339/2330 train_time:77524ms step_avg:57.90ms
step:1340/2330 train_time:77584ms step_avg:57.90ms
step:1341/2330 train_time:77641ms step_avg:57.90ms
step:1342/2330 train_time:77702ms step_avg:57.90ms
step:1343/2330 train_time:77759ms step_avg:57.90ms
step:1344/2330 train_time:77820ms step_avg:57.90ms
step:1345/2330 train_time:77876ms step_avg:57.90ms
step:1346/2330 train_time:77936ms step_avg:57.90ms
step:1347/2330 train_time:77993ms step_avg:57.90ms
step:1348/2330 train_time:78054ms step_avg:57.90ms
step:1349/2330 train_time:78110ms step_avg:57.90ms
step:1350/2330 train_time:78170ms step_avg:57.90ms
step:1351/2330 train_time:78227ms step_avg:57.90ms
step:1352/2330 train_time:78286ms step_avg:57.90ms
step:1353/2330 train_time:78343ms step_avg:57.90ms
step:1354/2330 train_time:78403ms step_avg:57.91ms
step:1355/2330 train_time:78460ms step_avg:57.90ms
step:1356/2330 train_time:78520ms step_avg:57.91ms
step:1357/2330 train_time:78577ms step_avg:57.91ms
step:1358/2330 train_time:78637ms step_avg:57.91ms
step:1359/2330 train_time:78694ms step_avg:57.91ms
step:1360/2330 train_time:78754ms step_avg:57.91ms
step:1361/2330 train_time:78811ms step_avg:57.91ms
step:1362/2330 train_time:78870ms step_avg:57.91ms
step:1363/2330 train_time:78928ms step_avg:57.91ms
step:1364/2330 train_time:78987ms step_avg:57.91ms
step:1365/2330 train_time:79044ms step_avg:57.91ms
step:1366/2330 train_time:79103ms step_avg:57.91ms
step:1367/2330 train_time:79160ms step_avg:57.91ms
step:1368/2330 train_time:79220ms step_avg:57.91ms
step:1369/2330 train_time:79277ms step_avg:57.91ms
step:1370/2330 train_time:79337ms step_avg:57.91ms
step:1371/2330 train_time:79395ms step_avg:57.91ms
step:1372/2330 train_time:79455ms step_avg:57.91ms
step:1373/2330 train_time:79512ms step_avg:57.91ms
step:1374/2330 train_time:79572ms step_avg:57.91ms
step:1375/2330 train_time:79629ms step_avg:57.91ms
step:1376/2330 train_time:79688ms step_avg:57.91ms
step:1377/2330 train_time:79745ms step_avg:57.91ms
step:1378/2330 train_time:79805ms step_avg:57.91ms
step:1379/2330 train_time:79862ms step_avg:57.91ms
step:1380/2330 train_time:79922ms step_avg:57.91ms
step:1381/2330 train_time:79979ms step_avg:57.91ms
step:1382/2330 train_time:80039ms step_avg:57.92ms
step:1383/2330 train_time:80096ms step_avg:57.91ms
step:1384/2330 train_time:80156ms step_avg:57.92ms
step:1385/2330 train_time:80213ms step_avg:57.92ms
step:1386/2330 train_time:80273ms step_avg:57.92ms
step:1387/2330 train_time:80329ms step_avg:57.92ms
step:1388/2330 train_time:80391ms step_avg:57.92ms
step:1389/2330 train_time:80447ms step_avg:57.92ms
step:1390/2330 train_time:80507ms step_avg:57.92ms
step:1391/2330 train_time:80564ms step_avg:57.92ms
step:1392/2330 train_time:80624ms step_avg:57.92ms
step:1393/2330 train_time:80681ms step_avg:57.92ms
step:1394/2330 train_time:80740ms step_avg:57.92ms
step:1395/2330 train_time:80797ms step_avg:57.92ms
step:1396/2330 train_time:80858ms step_avg:57.92ms
step:1397/2330 train_time:80916ms step_avg:57.92ms
step:1398/2330 train_time:80975ms step_avg:57.92ms
step:1399/2330 train_time:81032ms step_avg:57.92ms
step:1400/2330 train_time:81092ms step_avg:57.92ms
step:1401/2330 train_time:81148ms step_avg:57.92ms
step:1402/2330 train_time:81208ms step_avg:57.92ms
step:1403/2330 train_time:81265ms step_avg:57.92ms
step:1404/2330 train_time:81325ms step_avg:57.92ms
step:1405/2330 train_time:81382ms step_avg:57.92ms
step:1406/2330 train_time:81443ms step_avg:57.93ms
step:1407/2330 train_time:81500ms step_avg:57.92ms
step:1408/2330 train_time:81559ms step_avg:57.93ms
step:1409/2330 train_time:81617ms step_avg:57.93ms
step:1410/2330 train_time:81676ms step_avg:57.93ms
step:1411/2330 train_time:81733ms step_avg:57.93ms
step:1412/2330 train_time:81793ms step_avg:57.93ms
step:1413/2330 train_time:81851ms step_avg:57.93ms
step:1414/2330 train_time:81910ms step_avg:57.93ms
step:1415/2330 train_time:81968ms step_avg:57.93ms
step:1416/2330 train_time:82027ms step_avg:57.93ms
step:1417/2330 train_time:82083ms step_avg:57.93ms
step:1418/2330 train_time:82144ms step_avg:57.93ms
step:1419/2330 train_time:82201ms step_avg:57.93ms
step:1420/2330 train_time:82260ms step_avg:57.93ms
step:1421/2330 train_time:82318ms step_avg:57.93ms
step:1422/2330 train_time:82378ms step_avg:57.93ms
step:1423/2330 train_time:82435ms step_avg:57.93ms
step:1424/2330 train_time:82495ms step_avg:57.93ms
step:1425/2330 train_time:82553ms step_avg:57.93ms
step:1426/2330 train_time:82612ms step_avg:57.93ms
step:1427/2330 train_time:82669ms step_avg:57.93ms
step:1428/2330 train_time:82729ms step_avg:57.93ms
step:1429/2330 train_time:82786ms step_avg:57.93ms
step:1430/2330 train_time:82845ms step_avg:57.93ms
step:1431/2330 train_time:82902ms step_avg:57.93ms
step:1432/2330 train_time:82962ms step_avg:57.93ms
step:1433/2330 train_time:83020ms step_avg:57.93ms
step:1434/2330 train_time:83080ms step_avg:57.94ms
step:1435/2330 train_time:83136ms step_avg:57.93ms
step:1436/2330 train_time:83197ms step_avg:57.94ms
step:1437/2330 train_time:83254ms step_avg:57.94ms
step:1438/2330 train_time:83315ms step_avg:57.94ms
step:1439/2330 train_time:83372ms step_avg:57.94ms
step:1440/2330 train_time:83432ms step_avg:57.94ms
step:1441/2330 train_time:83489ms step_avg:57.94ms
step:1442/2330 train_time:83548ms step_avg:57.94ms
step:1443/2330 train_time:83605ms step_avg:57.94ms
step:1444/2330 train_time:83665ms step_avg:57.94ms
step:1445/2330 train_time:83722ms step_avg:57.94ms
step:1446/2330 train_time:83782ms step_avg:57.94ms
step:1447/2330 train_time:83839ms step_avg:57.94ms
step:1448/2330 train_time:83899ms step_avg:57.94ms
step:1449/2330 train_time:83956ms step_avg:57.94ms
step:1450/2330 train_time:84016ms step_avg:57.94ms
step:1451/2330 train_time:84073ms step_avg:57.94ms
step:1452/2330 train_time:84134ms step_avg:57.94ms
step:1453/2330 train_time:84191ms step_avg:57.94ms
step:1454/2330 train_time:84251ms step_avg:57.94ms
step:1455/2330 train_time:84308ms step_avg:57.94ms
step:1456/2330 train_time:84368ms step_avg:57.95ms
step:1457/2330 train_time:84426ms step_avg:57.94ms
step:1458/2330 train_time:84485ms step_avg:57.95ms
step:1459/2330 train_time:84541ms step_avg:57.94ms
step:1460/2330 train_time:84602ms step_avg:57.95ms
step:1461/2330 train_time:84659ms step_avg:57.95ms
step:1462/2330 train_time:84719ms step_avg:57.95ms
step:1463/2330 train_time:84776ms step_avg:57.95ms
step:1464/2330 train_time:84836ms step_avg:57.95ms
step:1465/2330 train_time:84893ms step_avg:57.95ms
step:1466/2330 train_time:84953ms step_avg:57.95ms
step:1467/2330 train_time:85010ms step_avg:57.95ms
step:1468/2330 train_time:85069ms step_avg:57.95ms
step:1469/2330 train_time:85126ms step_avg:57.95ms
step:1470/2330 train_time:85186ms step_avg:57.95ms
step:1471/2330 train_time:85242ms step_avg:57.95ms
step:1472/2330 train_time:85303ms step_avg:57.95ms
step:1473/2330 train_time:85359ms step_avg:57.95ms
step:1474/2330 train_time:85420ms step_avg:57.95ms
step:1475/2330 train_time:85478ms step_avg:57.95ms
step:1476/2330 train_time:85537ms step_avg:57.95ms
step:1477/2330 train_time:85595ms step_avg:57.95ms
step:1478/2330 train_time:85655ms step_avg:57.95ms
step:1479/2330 train_time:85712ms step_avg:57.95ms
step:1480/2330 train_time:85771ms step_avg:57.95ms
step:1481/2330 train_time:85828ms step_avg:57.95ms
step:1482/2330 train_time:85888ms step_avg:57.95ms
step:1483/2330 train_time:85945ms step_avg:57.95ms
step:1484/2330 train_time:86004ms step_avg:57.95ms
step:1485/2330 train_time:86061ms step_avg:57.95ms
step:1486/2330 train_time:86122ms step_avg:57.96ms
step:1487/2330 train_time:86179ms step_avg:57.96ms
step:1488/2330 train_time:86239ms step_avg:57.96ms
step:1489/2330 train_time:86296ms step_avg:57.96ms
step:1490/2330 train_time:86357ms step_avg:57.96ms
step:1491/2330 train_time:86414ms step_avg:57.96ms
step:1492/2330 train_time:86474ms step_avg:57.96ms
step:1493/2330 train_time:86532ms step_avg:57.96ms
step:1494/2330 train_time:86591ms step_avg:57.96ms
step:1495/2330 train_time:86647ms step_avg:57.96ms
step:1496/2330 train_time:86707ms step_avg:57.96ms
step:1497/2330 train_time:86764ms step_avg:57.96ms
step:1498/2330 train_time:86824ms step_avg:57.96ms
step:1499/2330 train_time:86881ms step_avg:57.96ms
step:1500/2330 train_time:86941ms step_avg:57.96ms
step:1500/2330 val_loss:5.0948 train_time:87021ms step_avg:58.01ms
step:1501/2330 train_time:87039ms step_avg:57.99ms
step:1502/2330 train_time:87060ms step_avg:57.96ms
step:1503/2330 train_time:87118ms step_avg:57.96ms
step:1504/2330 train_time:87184ms step_avg:57.97ms
step:1505/2330 train_time:87242ms step_avg:57.97ms
step:1506/2330 train_time:87303ms step_avg:57.97ms
step:1507/2330 train_time:87361ms step_avg:57.97ms
step:1508/2330 train_time:87421ms step_avg:57.97ms
step:1509/2330 train_time:87477ms step_avg:57.97ms
step:1510/2330 train_time:87537ms step_avg:57.97ms
step:1511/2330 train_time:87593ms step_avg:57.97ms
step:1512/2330 train_time:87652ms step_avg:57.97ms
step:1513/2330 train_time:87709ms step_avg:57.97ms
step:1514/2330 train_time:87767ms step_avg:57.97ms
step:1515/2330 train_time:87824ms step_avg:57.97ms
step:1516/2330 train_time:87883ms step_avg:57.97ms
step:1517/2330 train_time:87940ms step_avg:57.97ms
step:1518/2330 train_time:88000ms step_avg:57.97ms
step:1519/2330 train_time:88059ms step_avg:57.97ms
step:1520/2330 train_time:88121ms step_avg:57.97ms
step:1521/2330 train_time:88179ms step_avg:57.97ms
step:1522/2330 train_time:88240ms step_avg:57.98ms
step:1523/2330 train_time:88298ms step_avg:57.98ms
step:1524/2330 train_time:88359ms step_avg:57.98ms
step:1525/2330 train_time:88417ms step_avg:57.98ms
step:1526/2330 train_time:88477ms step_avg:57.98ms
step:1527/2330 train_time:88534ms step_avg:57.98ms
step:1528/2330 train_time:88593ms step_avg:57.98ms
step:1529/2330 train_time:88651ms step_avg:57.98ms
step:1530/2330 train_time:88709ms step_avg:57.98ms
step:1531/2330 train_time:88766ms step_avg:57.98ms
step:1532/2330 train_time:88826ms step_avg:57.98ms
step:1533/2330 train_time:88883ms step_avg:57.98ms
step:1534/2330 train_time:88942ms step_avg:57.98ms
step:1535/2330 train_time:89000ms step_avg:57.98ms
step:1536/2330 train_time:89061ms step_avg:57.98ms
step:1537/2330 train_time:89120ms step_avg:57.98ms
step:1538/2330 train_time:89181ms step_avg:57.98ms
step:1539/2330 train_time:89238ms step_avg:57.98ms
step:1540/2330 train_time:89299ms step_avg:57.99ms
step:1541/2330 train_time:89358ms step_avg:57.99ms
step:1542/2330 train_time:89419ms step_avg:57.99ms
step:1543/2330 train_time:89477ms step_avg:57.99ms
step:1544/2330 train_time:89538ms step_avg:57.99ms
step:1545/2330 train_time:89596ms step_avg:57.99ms
step:1546/2330 train_time:89658ms step_avg:57.99ms
step:1547/2330 train_time:89715ms step_avg:57.99ms
step:1548/2330 train_time:89775ms step_avg:57.99ms
step:1549/2330 train_time:89832ms step_avg:57.99ms
step:1550/2330 train_time:89892ms step_avg:58.00ms
step:1551/2330 train_time:89950ms step_avg:57.99ms
step:1552/2330 train_time:90010ms step_avg:58.00ms
step:1553/2330 train_time:90067ms step_avg:58.00ms
step:1554/2330 train_time:90127ms step_avg:58.00ms
step:1555/2330 train_time:90185ms step_avg:58.00ms
step:1556/2330 train_time:90246ms step_avg:58.00ms
step:1557/2330 train_time:90304ms step_avg:58.00ms
step:1558/2330 train_time:90364ms step_avg:58.00ms
step:1559/2330 train_time:90422ms step_avg:58.00ms
step:1560/2330 train_time:90482ms step_avg:58.00ms
step:1561/2330 train_time:90540ms step_avg:58.00ms
step:1562/2330 train_time:90601ms step_avg:58.00ms
step:1563/2330 train_time:90659ms step_avg:58.00ms
step:1564/2330 train_time:90720ms step_avg:58.01ms
step:1565/2330 train_time:90778ms step_avg:58.01ms
step:1566/2330 train_time:90839ms step_avg:58.01ms
step:1567/2330 train_time:90896ms step_avg:58.01ms
step:1568/2330 train_time:90957ms step_avg:58.01ms
step:1569/2330 train_time:91013ms step_avg:58.01ms
step:1570/2330 train_time:91075ms step_avg:58.01ms
step:1571/2330 train_time:91132ms step_avg:58.01ms
step:1572/2330 train_time:91194ms step_avg:58.01ms
step:1573/2330 train_time:91251ms step_avg:58.01ms
step:1574/2330 train_time:91312ms step_avg:58.01ms
step:1575/2330 train_time:91369ms step_avg:58.01ms
step:1576/2330 train_time:91431ms step_avg:58.01ms
step:1577/2330 train_time:91488ms step_avg:58.01ms
step:1578/2330 train_time:91549ms step_avg:58.02ms
step:1579/2330 train_time:91606ms step_avg:58.01ms
step:1580/2330 train_time:91665ms step_avg:58.02ms
step:1581/2330 train_time:91723ms step_avg:58.02ms
step:1582/2330 train_time:91785ms step_avg:58.02ms
step:1583/2330 train_time:91842ms step_avg:58.02ms
step:1584/2330 train_time:91903ms step_avg:58.02ms
step:1585/2330 train_time:91960ms step_avg:58.02ms
step:1586/2330 train_time:92020ms step_avg:58.02ms
step:1587/2330 train_time:92077ms step_avg:58.02ms
step:1588/2330 train_time:92138ms step_avg:58.02ms
step:1589/2330 train_time:92195ms step_avg:58.02ms
step:1590/2330 train_time:92258ms step_avg:58.02ms
step:1591/2330 train_time:92317ms step_avg:58.02ms
step:1592/2330 train_time:92377ms step_avg:58.03ms
step:1593/2330 train_time:92434ms step_avg:58.03ms
step:1594/2330 train_time:92496ms step_avg:58.03ms
step:1595/2330 train_time:92553ms step_avg:58.03ms
step:1596/2330 train_time:92613ms step_avg:58.03ms
step:1597/2330 train_time:92670ms step_avg:58.03ms
step:1598/2330 train_time:92731ms step_avg:58.03ms
step:1599/2330 train_time:92788ms step_avg:58.03ms
step:1600/2330 train_time:92849ms step_avg:58.03ms
step:1601/2330 train_time:92906ms step_avg:58.03ms
step:1602/2330 train_time:92967ms step_avg:58.03ms
step:1603/2330 train_time:93024ms step_avg:58.03ms
step:1604/2330 train_time:93084ms step_avg:58.03ms
step:1605/2330 train_time:93143ms step_avg:58.03ms
step:1606/2330 train_time:93203ms step_avg:58.03ms
step:1607/2330 train_time:93262ms step_avg:58.03ms
step:1608/2330 train_time:93324ms step_avg:58.04ms
step:1609/2330 train_time:93381ms step_avg:58.04ms
step:1610/2330 train_time:93442ms step_avg:58.04ms
step:1611/2330 train_time:93500ms step_avg:58.04ms
step:1612/2330 train_time:93561ms step_avg:58.04ms
step:1613/2330 train_time:93618ms step_avg:58.04ms
step:1614/2330 train_time:93680ms step_avg:58.04ms
step:1615/2330 train_time:93737ms step_avg:58.04ms
step:1616/2330 train_time:93798ms step_avg:58.04ms
step:1617/2330 train_time:93856ms step_avg:58.04ms
step:1618/2330 train_time:93916ms step_avg:58.04ms
step:1619/2330 train_time:93974ms step_avg:58.04ms
step:1620/2330 train_time:94034ms step_avg:58.05ms
step:1621/2330 train_time:94091ms step_avg:58.05ms
step:1622/2330 train_time:94152ms step_avg:58.05ms
step:1623/2330 train_time:94209ms step_avg:58.05ms
step:1624/2330 train_time:94270ms step_avg:58.05ms
step:1625/2330 train_time:94327ms step_avg:58.05ms
step:1626/2330 train_time:94388ms step_avg:58.05ms
step:1627/2330 train_time:94446ms step_avg:58.05ms
step:1628/2330 train_time:94506ms step_avg:58.05ms
step:1629/2330 train_time:94564ms step_avg:58.05ms
step:1630/2330 train_time:94625ms step_avg:58.05ms
step:1631/2330 train_time:94682ms step_avg:58.05ms
step:1632/2330 train_time:94743ms step_avg:58.05ms
step:1633/2330 train_time:94801ms step_avg:58.05ms
step:1634/2330 train_time:94861ms step_avg:58.05ms
step:1635/2330 train_time:94919ms step_avg:58.05ms
step:1636/2330 train_time:94979ms step_avg:58.06ms
step:1637/2330 train_time:95036ms step_avg:58.06ms
step:1638/2330 train_time:95098ms step_avg:58.06ms
step:1639/2330 train_time:95156ms step_avg:58.06ms
step:1640/2330 train_time:95216ms step_avg:58.06ms
step:1641/2330 train_time:95274ms step_avg:58.06ms
step:1642/2330 train_time:95335ms step_avg:58.06ms
step:1643/2330 train_time:95392ms step_avg:58.06ms
step:1644/2330 train_time:95454ms step_avg:58.06ms
step:1645/2330 train_time:95511ms step_avg:58.06ms
step:1646/2330 train_time:95572ms step_avg:58.06ms
step:1647/2330 train_time:95629ms step_avg:58.06ms
step:1648/2330 train_time:95690ms step_avg:58.06ms
step:1649/2330 train_time:95747ms step_avg:58.06ms
step:1650/2330 train_time:95808ms step_avg:58.07ms
step:1651/2330 train_time:95864ms step_avg:58.06ms
step:1652/2330 train_time:95925ms step_avg:58.07ms
step:1653/2330 train_time:95982ms step_avg:58.07ms
step:1654/2330 train_time:96045ms step_avg:58.07ms
step:1655/2330 train_time:96103ms step_avg:58.07ms
step:1656/2330 train_time:96164ms step_avg:58.07ms
step:1657/2330 train_time:96223ms step_avg:58.07ms
step:1658/2330 train_time:96284ms step_avg:58.07ms
step:1659/2330 train_time:96342ms step_avg:58.07ms
step:1660/2330 train_time:96402ms step_avg:58.07ms
step:1661/2330 train_time:96461ms step_avg:58.07ms
step:1662/2330 train_time:96522ms step_avg:58.08ms
step:1663/2330 train_time:96579ms step_avg:58.08ms
step:1664/2330 train_time:96640ms step_avg:58.08ms
step:1665/2330 train_time:96698ms step_avg:58.08ms
step:1666/2330 train_time:96758ms step_avg:58.08ms
step:1667/2330 train_time:96816ms step_avg:58.08ms
step:1668/2330 train_time:96876ms step_avg:58.08ms
step:1669/2330 train_time:96933ms step_avg:58.08ms
step:1670/2330 train_time:96995ms step_avg:58.08ms
step:1671/2330 train_time:97052ms step_avg:58.08ms
step:1672/2330 train_time:97112ms step_avg:58.08ms
step:1673/2330 train_time:97169ms step_avg:58.08ms
step:1674/2330 train_time:97230ms step_avg:58.08ms
step:1675/2330 train_time:97288ms step_avg:58.08ms
step:1676/2330 train_time:97348ms step_avg:58.08ms
step:1677/2330 train_time:97404ms step_avg:58.08ms
step:1678/2330 train_time:97465ms step_avg:58.08ms
step:1679/2330 train_time:97523ms step_avg:58.08ms
step:1680/2330 train_time:97584ms step_avg:58.09ms
step:1681/2330 train_time:97641ms step_avg:58.09ms
step:1682/2330 train_time:97701ms step_avg:58.09ms
step:1683/2330 train_time:97759ms step_avg:58.09ms
step:1684/2330 train_time:97820ms step_avg:58.09ms
step:1685/2330 train_time:97878ms step_avg:58.09ms
step:1686/2330 train_time:97939ms step_avg:58.09ms
step:1687/2330 train_time:97997ms step_avg:58.09ms
step:1688/2330 train_time:98057ms step_avg:58.09ms
step:1689/2330 train_time:98115ms step_avg:58.09ms
step:1690/2330 train_time:98175ms step_avg:58.09ms
step:1691/2330 train_time:98233ms step_avg:58.09ms
step:1692/2330 train_time:98293ms step_avg:58.09ms
step:1693/2330 train_time:98350ms step_avg:58.09ms
step:1694/2330 train_time:98411ms step_avg:58.09ms
step:1695/2330 train_time:98468ms step_avg:58.09ms
step:1696/2330 train_time:98529ms step_avg:58.09ms
step:1697/2330 train_time:98585ms step_avg:58.09ms
step:1698/2330 train_time:98646ms step_avg:58.10ms
step:1699/2330 train_time:98704ms step_avg:58.10ms
step:1700/2330 train_time:98764ms step_avg:58.10ms
step:1701/2330 train_time:98821ms step_avg:58.10ms
step:1702/2330 train_time:98882ms step_avg:58.10ms
step:1703/2330 train_time:98940ms step_avg:58.10ms
step:1704/2330 train_time:99001ms step_avg:58.10ms
step:1705/2330 train_time:99059ms step_avg:58.10ms
step:1706/2330 train_time:99120ms step_avg:58.10ms
step:1707/2330 train_time:99178ms step_avg:58.10ms
step:1708/2330 train_time:99239ms step_avg:58.10ms
step:1709/2330 train_time:99297ms step_avg:58.10ms
step:1710/2330 train_time:99358ms step_avg:58.10ms
step:1711/2330 train_time:99415ms step_avg:58.10ms
step:1712/2330 train_time:99475ms step_avg:58.10ms
step:1713/2330 train_time:99532ms step_avg:58.10ms
step:1714/2330 train_time:99594ms step_avg:58.11ms
step:1715/2330 train_time:99650ms step_avg:58.11ms
step:1716/2330 train_time:99712ms step_avg:58.11ms
step:1717/2330 train_time:99768ms step_avg:58.11ms
step:1718/2330 train_time:99829ms step_avg:58.11ms
step:1719/2330 train_time:99886ms step_avg:58.11ms
step:1720/2330 train_time:99947ms step_avg:58.11ms
step:1721/2330 train_time:100004ms step_avg:58.11ms
step:1722/2330 train_time:100065ms step_avg:58.11ms
step:1723/2330 train_time:100123ms step_avg:58.11ms
step:1724/2330 train_time:100184ms step_avg:58.11ms
step:1725/2330 train_time:100242ms step_avg:58.11ms
step:1726/2330 train_time:100303ms step_avg:58.11ms
step:1727/2330 train_time:100361ms step_avg:58.11ms
step:1728/2330 train_time:100421ms step_avg:58.11ms
step:1729/2330 train_time:100479ms step_avg:58.11ms
step:1730/2330 train_time:100540ms step_avg:58.12ms
step:1731/2330 train_time:100597ms step_avg:58.12ms
step:1732/2330 train_time:100658ms step_avg:58.12ms
step:1733/2330 train_time:100716ms step_avg:58.12ms
step:1734/2330 train_time:100778ms step_avg:58.12ms
step:1735/2330 train_time:100836ms step_avg:58.12ms
step:1736/2330 train_time:100895ms step_avg:58.12ms
step:1737/2330 train_time:100952ms step_avg:58.12ms
step:1738/2330 train_time:101013ms step_avg:58.12ms
step:1739/2330 train_time:101070ms step_avg:58.12ms
step:1740/2330 train_time:101132ms step_avg:58.12ms
step:1741/2330 train_time:101189ms step_avg:58.12ms
step:1742/2330 train_time:101250ms step_avg:58.12ms
step:1743/2330 train_time:101307ms step_avg:58.12ms
step:1744/2330 train_time:101367ms step_avg:58.12ms
step:1745/2330 train_time:101425ms step_avg:58.12ms
step:1746/2330 train_time:101485ms step_avg:58.12ms
step:1747/2330 train_time:101544ms step_avg:58.12ms
step:1748/2330 train_time:101604ms step_avg:58.13ms
step:1749/2330 train_time:101662ms step_avg:58.13ms
step:1750/2330 train_time:101723ms step_avg:58.13ms
step:1750/2330 val_loss:4.9640 train_time:101805ms step_avg:58.17ms
step:1751/2330 train_time:101824ms step_avg:58.15ms
step:1752/2330 train_time:101843ms step_avg:58.13ms
step:1753/2330 train_time:101899ms step_avg:58.13ms
step:1754/2330 train_time:101971ms step_avg:58.14ms
step:1755/2330 train_time:102028ms step_avg:58.14ms
step:1756/2330 train_time:102093ms step_avg:58.14ms
step:1757/2330 train_time:102149ms step_avg:58.14ms
step:1758/2330 train_time:102210ms step_avg:58.14ms
step:1759/2330 train_time:102267ms step_avg:58.14ms
step:1760/2330 train_time:102327ms step_avg:58.14ms
step:1761/2330 train_time:102384ms step_avg:58.14ms
step:1762/2330 train_time:102443ms step_avg:58.14ms
step:1763/2330 train_time:102500ms step_avg:58.14ms
step:1764/2330 train_time:102559ms step_avg:58.14ms
step:1765/2330 train_time:102616ms step_avg:58.14ms
step:1766/2330 train_time:102675ms step_avg:58.14ms
step:1767/2330 train_time:102733ms step_avg:58.14ms
step:1768/2330 train_time:102797ms step_avg:58.14ms
step:1769/2330 train_time:102856ms step_avg:58.14ms
step:1770/2330 train_time:102917ms step_avg:58.15ms
step:1771/2330 train_time:102975ms step_avg:58.15ms
step:1772/2330 train_time:103037ms step_avg:58.15ms
step:1773/2330 train_time:103093ms step_avg:58.15ms
step:1774/2330 train_time:103154ms step_avg:58.15ms
step:1775/2330 train_time:103212ms step_avg:58.15ms
step:1776/2330 train_time:103272ms step_avg:58.15ms
step:1777/2330 train_time:103329ms step_avg:58.15ms
step:1778/2330 train_time:103390ms step_avg:58.15ms
step:1779/2330 train_time:103448ms step_avg:58.15ms
step:1780/2330 train_time:103508ms step_avg:58.15ms
step:1781/2330 train_time:103565ms step_avg:58.15ms
step:1782/2330 train_time:103625ms step_avg:58.15ms
step:1783/2330 train_time:103683ms step_avg:58.15ms
step:1784/2330 train_time:103745ms step_avg:58.15ms
step:1785/2330 train_time:103802ms step_avg:58.15ms
step:1786/2330 train_time:103864ms step_avg:58.15ms
step:1787/2330 train_time:103920ms step_avg:58.15ms
step:1788/2330 train_time:103984ms step_avg:58.16ms
step:1789/2330 train_time:104041ms step_avg:58.16ms
step:1790/2330 train_time:104104ms step_avg:58.16ms
step:1791/2330 train_time:104161ms step_avg:58.16ms
step:1792/2330 train_time:104222ms step_avg:58.16ms
step:1793/2330 train_time:104279ms step_avg:58.16ms
step:1794/2330 train_time:104339ms step_avg:58.16ms
step:1795/2330 train_time:104395ms step_avg:58.16ms
step:1796/2330 train_time:104456ms step_avg:58.16ms
step:1797/2330 train_time:104512ms step_avg:58.16ms
step:1798/2330 train_time:104573ms step_avg:58.16ms
step:1799/2330 train_time:104630ms step_avg:58.16ms
step:1800/2330 train_time:104691ms step_avg:58.16ms
step:1801/2330 train_time:104749ms step_avg:58.16ms
step:1802/2330 train_time:104810ms step_avg:58.16ms
step:1803/2330 train_time:104870ms step_avg:58.16ms
step:1804/2330 train_time:104931ms step_avg:58.17ms
step:1805/2330 train_time:104989ms step_avg:58.17ms
step:1806/2330 train_time:105049ms step_avg:58.17ms
step:1807/2330 train_time:105108ms step_avg:58.17ms
step:1808/2330 train_time:105169ms step_avg:58.17ms
step:1809/2330 train_time:105226ms step_avg:58.17ms
step:1810/2330 train_time:105287ms step_avg:58.17ms
step:1811/2330 train_time:105344ms step_avg:58.17ms
step:1812/2330 train_time:105405ms step_avg:58.17ms
step:1813/2330 train_time:105461ms step_avg:58.17ms
step:1814/2330 train_time:105522ms step_avg:58.17ms
step:1815/2330 train_time:105579ms step_avg:58.17ms
step:1816/2330 train_time:105639ms step_avg:58.17ms
step:1817/2330 train_time:105696ms step_avg:58.17ms
step:1818/2330 train_time:105757ms step_avg:58.17ms
step:1819/2330 train_time:105815ms step_avg:58.17ms
step:1820/2330 train_time:105875ms step_avg:58.17ms
step:1821/2330 train_time:105933ms step_avg:58.17ms
step:1822/2330 train_time:105994ms step_avg:58.17ms
step:1823/2330 train_time:106052ms step_avg:58.17ms
step:1824/2330 train_time:106113ms step_avg:58.18ms
step:1825/2330 train_time:106171ms step_avg:58.18ms
step:1826/2330 train_time:106232ms step_avg:58.18ms
step:1827/2330 train_time:106289ms step_avg:58.18ms
step:1828/2330 train_time:106351ms step_avg:58.18ms
step:1829/2330 train_time:106409ms step_avg:58.18ms
step:1830/2330 train_time:106469ms step_avg:58.18ms
step:1831/2330 train_time:106526ms step_avg:58.18ms
step:1832/2330 train_time:106588ms step_avg:58.18ms
step:1833/2330 train_time:106645ms step_avg:58.18ms
step:1834/2330 train_time:106706ms step_avg:58.18ms
step:1835/2330 train_time:106763ms step_avg:58.18ms
step:1836/2330 train_time:106824ms step_avg:58.18ms
step:1837/2330 train_time:106882ms step_avg:58.18ms
step:1838/2330 train_time:106942ms step_avg:58.18ms
step:1839/2330 train_time:106998ms step_avg:58.18ms
step:1840/2330 train_time:107060ms step_avg:58.18ms
step:1841/2330 train_time:107117ms step_avg:58.18ms
step:1842/2330 train_time:107177ms step_avg:58.19ms
step:1843/2330 train_time:107235ms step_avg:58.18ms
step:1844/2330 train_time:107295ms step_avg:58.19ms
step:1845/2330 train_time:107352ms step_avg:58.19ms
step:1846/2330 train_time:107412ms step_avg:58.19ms
step:1847/2330 train_time:107470ms step_avg:58.19ms
step:1848/2330 train_time:107531ms step_avg:58.19ms
step:1849/2330 train_time:107588ms step_avg:58.19ms
step:1850/2330 train_time:107649ms step_avg:58.19ms
step:1851/2330 train_time:107707ms step_avg:58.19ms
step:1852/2330 train_time:107767ms step_avg:58.19ms
step:1853/2330 train_time:107825ms step_avg:58.19ms
step:1854/2330 train_time:107885ms step_avg:58.19ms
step:1855/2330 train_time:107942ms step_avg:58.19ms
step:1856/2330 train_time:108004ms step_avg:58.19ms
step:1857/2330 train_time:108061ms step_avg:58.19ms
step:1858/2330 train_time:108123ms step_avg:58.19ms
step:1859/2330 train_time:108179ms step_avg:58.19ms
step:1860/2330 train_time:108241ms step_avg:58.19ms
step:1861/2330 train_time:108297ms step_avg:58.19ms
step:1862/2330 train_time:108359ms step_avg:58.20ms
step:1863/2330 train_time:108416ms step_avg:58.19ms
step:1864/2330 train_time:108476ms step_avg:58.20ms
step:1865/2330 train_time:108533ms step_avg:58.19ms
step:1866/2330 train_time:108594ms step_avg:58.20ms
step:1867/2330 train_time:108651ms step_avg:58.20ms
step:1868/2330 train_time:108712ms step_avg:58.20ms
step:1869/2330 train_time:108769ms step_avg:58.20ms
step:1870/2330 train_time:108830ms step_avg:58.20ms
step:1871/2330 train_time:108888ms step_avg:58.20ms
step:1872/2330 train_time:108948ms step_avg:58.20ms
step:1873/2330 train_time:109007ms step_avg:58.20ms
step:1874/2330 train_time:109067ms step_avg:58.20ms
step:1875/2330 train_time:109124ms step_avg:58.20ms
step:1876/2330 train_time:109187ms step_avg:58.20ms
step:1877/2330 train_time:109244ms step_avg:58.20ms
step:1878/2330 train_time:109306ms step_avg:58.20ms
step:1879/2330 train_time:109363ms step_avg:58.20ms
step:1880/2330 train_time:109424ms step_avg:58.20ms
step:1881/2330 train_time:109480ms step_avg:58.20ms
step:1882/2330 train_time:109542ms step_avg:58.20ms
step:1883/2330 train_time:109598ms step_avg:58.20ms
step:1884/2330 train_time:109659ms step_avg:58.21ms
step:1885/2330 train_time:109715ms step_avg:58.20ms
step:1886/2330 train_time:109776ms step_avg:58.21ms
step:1887/2330 train_time:109834ms step_avg:58.21ms
step:1888/2330 train_time:109894ms step_avg:58.21ms
step:1889/2330 train_time:109952ms step_avg:58.21ms
step:1890/2330 train_time:110013ms step_avg:58.21ms
step:1891/2330 train_time:110071ms step_avg:58.21ms
step:1892/2330 train_time:110133ms step_avg:58.21ms
step:1893/2330 train_time:110191ms step_avg:58.21ms
step:1894/2330 train_time:110252ms step_avg:58.21ms
step:1895/2330 train_time:110311ms step_avg:58.21ms
step:1896/2330 train_time:110371ms step_avg:58.21ms
step:1897/2330 train_time:110430ms step_avg:58.21ms
step:1898/2330 train_time:110490ms step_avg:58.21ms
step:1899/2330 train_time:110548ms step_avg:58.21ms
step:1900/2330 train_time:110608ms step_avg:58.21ms
step:1901/2330 train_time:110665ms step_avg:58.21ms
step:1902/2330 train_time:110726ms step_avg:58.22ms
step:1903/2330 train_time:110783ms step_avg:58.21ms
step:1904/2330 train_time:110844ms step_avg:58.22ms
step:1905/2330 train_time:110900ms step_avg:58.22ms
step:1906/2330 train_time:110963ms step_avg:58.22ms
step:1907/2330 train_time:111020ms step_avg:58.22ms
step:1908/2330 train_time:111082ms step_avg:58.22ms
step:1909/2330 train_time:111139ms step_avg:58.22ms
step:1910/2330 train_time:111200ms step_avg:58.22ms
step:1911/2330 train_time:111257ms step_avg:58.22ms
step:1912/2330 train_time:111318ms step_avg:58.22ms
step:1913/2330 train_time:111374ms step_avg:58.22ms
step:1914/2330 train_time:111436ms step_avg:58.22ms
step:1915/2330 train_time:111493ms step_avg:58.22ms
step:1916/2330 train_time:111555ms step_avg:58.22ms
step:1917/2330 train_time:111612ms step_avg:58.22ms
step:1918/2330 train_time:111672ms step_avg:58.22ms
step:1919/2330 train_time:111730ms step_avg:58.22ms
step:1920/2330 train_time:111790ms step_avg:58.22ms
step:1921/2330 train_time:111848ms step_avg:58.22ms
step:1922/2330 train_time:111909ms step_avg:58.23ms
step:1923/2330 train_time:111967ms step_avg:58.23ms
step:1924/2330 train_time:112027ms step_avg:58.23ms
step:1925/2330 train_time:112085ms step_avg:58.23ms
step:1926/2330 train_time:112147ms step_avg:58.23ms
step:1927/2330 train_time:112204ms step_avg:58.23ms
step:1928/2330 train_time:112265ms step_avg:58.23ms
step:1929/2330 train_time:112323ms step_avg:58.23ms
step:1930/2330 train_time:112384ms step_avg:58.23ms
step:1931/2330 train_time:112441ms step_avg:58.23ms
step:1932/2330 train_time:112502ms step_avg:58.23ms
step:1933/2330 train_time:112559ms step_avg:58.23ms
step:1934/2330 train_time:112620ms step_avg:58.23ms
step:1935/2330 train_time:112676ms step_avg:58.23ms
step:1936/2330 train_time:112737ms step_avg:58.23ms
step:1937/2330 train_time:112794ms step_avg:58.23ms
step:1938/2330 train_time:112855ms step_avg:58.23ms
step:1939/2330 train_time:112912ms step_avg:58.23ms
step:1940/2330 train_time:112974ms step_avg:58.23ms
step:1941/2330 train_time:113033ms step_avg:58.23ms
step:1942/2330 train_time:113093ms step_avg:58.24ms
step:1943/2330 train_time:113151ms step_avg:58.24ms
step:1944/2330 train_time:113211ms step_avg:58.24ms
step:1945/2330 train_time:113269ms step_avg:58.24ms
step:1946/2330 train_time:113329ms step_avg:58.24ms
step:1947/2330 train_time:113387ms step_avg:58.24ms
step:1948/2330 train_time:113448ms step_avg:58.24ms
step:1949/2330 train_time:113505ms step_avg:58.24ms
step:1950/2330 train_time:113566ms step_avg:58.24ms
step:1951/2330 train_time:113623ms step_avg:58.24ms
step:1952/2330 train_time:113683ms step_avg:58.24ms
step:1953/2330 train_time:113740ms step_avg:58.24ms
step:1954/2330 train_time:113801ms step_avg:58.24ms
step:1955/2330 train_time:113858ms step_avg:58.24ms
step:1956/2330 train_time:113919ms step_avg:58.24ms
step:1957/2330 train_time:113977ms step_avg:58.24ms
step:1958/2330 train_time:114038ms step_avg:58.24ms
step:1959/2330 train_time:114095ms step_avg:58.24ms
step:1960/2330 train_time:114154ms step_avg:58.24ms
step:1961/2330 train_time:114211ms step_avg:58.24ms
step:1962/2330 train_time:114273ms step_avg:58.24ms
step:1963/2330 train_time:114330ms step_avg:58.24ms
step:1964/2330 train_time:114392ms step_avg:58.24ms
step:1965/2330 train_time:114450ms step_avg:58.24ms
step:1966/2330 train_time:114510ms step_avg:58.25ms
step:1967/2330 train_time:114569ms step_avg:58.25ms
step:1968/2330 train_time:114630ms step_avg:58.25ms
step:1969/2330 train_time:114688ms step_avg:58.25ms
step:1970/2330 train_time:114749ms step_avg:58.25ms
step:1971/2330 train_time:114807ms step_avg:58.25ms
step:1972/2330 train_time:114868ms step_avg:58.25ms
step:1973/2330 train_time:114925ms step_avg:58.25ms
step:1974/2330 train_time:114987ms step_avg:58.25ms
step:1975/2330 train_time:115044ms step_avg:58.25ms
step:1976/2330 train_time:115106ms step_avg:58.25ms
step:1977/2330 train_time:115163ms step_avg:58.25ms
step:1978/2330 train_time:115224ms step_avg:58.25ms
step:1979/2330 train_time:115281ms step_avg:58.25ms
step:1980/2330 train_time:115344ms step_avg:58.25ms
step:1981/2330 train_time:115400ms step_avg:58.25ms
step:1982/2330 train_time:115462ms step_avg:58.26ms
step:1983/2330 train_time:115519ms step_avg:58.25ms
step:1984/2330 train_time:115579ms step_avg:58.26ms
step:1985/2330 train_time:115635ms step_avg:58.25ms
step:1986/2330 train_time:115696ms step_avg:58.26ms
step:1987/2330 train_time:115753ms step_avg:58.26ms
step:1988/2330 train_time:115814ms step_avg:58.26ms
step:1989/2330 train_time:115871ms step_avg:58.26ms
step:1990/2330 train_time:115934ms step_avg:58.26ms
step:1991/2330 train_time:115991ms step_avg:58.26ms
step:1992/2330 train_time:116053ms step_avg:58.26ms
step:1993/2330 train_time:116111ms step_avg:58.26ms
step:1994/2330 train_time:116172ms step_avg:58.26ms
step:1995/2330 train_time:116230ms step_avg:58.26ms
step:1996/2330 train_time:116291ms step_avg:58.26ms
step:1997/2330 train_time:116349ms step_avg:58.26ms
step:1998/2330 train_time:116409ms step_avg:58.26ms
step:1999/2330 train_time:116466ms step_avg:58.26ms
step:2000/2330 train_time:116527ms step_avg:58.26ms
step:2000/2330 val_loss:4.8904 train_time:116610ms step_avg:58.30ms
step:2001/2330 train_time:116629ms step_avg:58.29ms
step:2002/2330 train_time:116649ms step_avg:58.27ms
step:2003/2330 train_time:116710ms step_avg:58.27ms
step:2004/2330 train_time:116773ms step_avg:58.27ms
step:2005/2330 train_time:116831ms step_avg:58.27ms
step:2006/2330 train_time:116894ms step_avg:58.27ms
step:2007/2330 train_time:116951ms step_avg:58.27ms
step:2008/2330 train_time:117013ms step_avg:58.27ms
step:2009/2330 train_time:117069ms step_avg:58.27ms
step:2010/2330 train_time:117130ms step_avg:58.27ms
step:2011/2330 train_time:117187ms step_avg:58.27ms
step:2012/2330 train_time:117246ms step_avg:58.27ms
step:2013/2330 train_time:117302ms step_avg:58.27ms
step:2014/2330 train_time:117362ms step_avg:58.27ms
step:2015/2330 train_time:117418ms step_avg:58.27ms
step:2016/2330 train_time:117478ms step_avg:58.27ms
step:2017/2330 train_time:117534ms step_avg:58.27ms
step:2018/2330 train_time:117595ms step_avg:58.27ms
step:2019/2330 train_time:117654ms step_avg:58.27ms
step:2020/2330 train_time:117716ms step_avg:58.28ms
step:2021/2330 train_time:117774ms step_avg:58.28ms
step:2022/2330 train_time:117837ms step_avg:58.28ms
step:2023/2330 train_time:117894ms step_avg:58.28ms
step:2024/2330 train_time:117957ms step_avg:58.28ms
step:2025/2330 train_time:118014ms step_avg:58.28ms
step:2026/2330 train_time:118076ms step_avg:58.28ms
step:2027/2330 train_time:118134ms step_avg:58.28ms
step:2028/2330 train_time:118194ms step_avg:58.28ms
step:2029/2330 train_time:118251ms step_avg:58.28ms
step:2030/2330 train_time:118311ms step_avg:58.28ms
step:2031/2330 train_time:118369ms step_avg:58.28ms
step:2032/2330 train_time:118428ms step_avg:58.28ms
step:2033/2330 train_time:118485ms step_avg:58.28ms
step:2034/2330 train_time:118544ms step_avg:58.28ms
step:2035/2330 train_time:118601ms step_avg:58.28ms
step:2036/2330 train_time:118662ms step_avg:58.28ms
step:2037/2330 train_time:118719ms step_avg:58.28ms
step:2038/2330 train_time:118780ms step_avg:58.28ms
step:2039/2330 train_time:118837ms step_avg:58.28ms
step:2040/2330 train_time:118900ms step_avg:58.28ms
step:2041/2330 train_time:118957ms step_avg:58.28ms
step:2042/2330 train_time:119019ms step_avg:58.29ms
step:2043/2330 train_time:119077ms step_avg:58.29ms
step:2044/2330 train_time:119136ms step_avg:58.29ms
step:2045/2330 train_time:119193ms step_avg:58.29ms
step:2046/2330 train_time:119255ms step_avg:58.29ms
step:2047/2330 train_time:119313ms step_avg:58.29ms
step:2048/2330 train_time:119373ms step_avg:58.29ms
step:2049/2330 train_time:119432ms step_avg:58.29ms
step:2050/2330 train_time:119492ms step_avg:58.29ms
step:2051/2330 train_time:119550ms step_avg:58.29ms
step:2052/2330 train_time:119610ms step_avg:58.29ms
step:2053/2330 train_time:119668ms step_avg:58.29ms
step:2054/2330 train_time:119729ms step_avg:58.29ms
step:2055/2330 train_time:119787ms step_avg:58.29ms
step:2056/2330 train_time:119849ms step_avg:58.29ms
step:2057/2330 train_time:119906ms step_avg:58.29ms
step:2058/2330 train_time:119968ms step_avg:58.29ms
step:2059/2330 train_time:120025ms step_avg:58.29ms
step:2060/2330 train_time:120087ms step_avg:58.29ms
step:2061/2330 train_time:120143ms step_avg:58.29ms
step:2062/2330 train_time:120204ms step_avg:58.29ms
step:2063/2330 train_time:120261ms step_avg:58.29ms
step:2064/2330 train_time:120321ms step_avg:58.30ms
step:2065/2330 train_time:120378ms step_avg:58.29ms
step:2066/2330 train_time:120439ms step_avg:58.30ms
step:2067/2330 train_time:120496ms step_avg:58.29ms
step:2068/2330 train_time:120557ms step_avg:58.30ms
step:2069/2330 train_time:120615ms step_avg:58.30ms
step:2070/2330 train_time:120676ms step_avg:58.30ms
step:2071/2330 train_time:120735ms step_avg:58.30ms
step:2072/2330 train_time:120795ms step_avg:58.30ms
step:2073/2330 train_time:120853ms step_avg:58.30ms
step:2074/2330 train_time:120914ms step_avg:58.30ms
step:2075/2330 train_time:120972ms step_avg:58.30ms
step:2076/2330 train_time:121033ms step_avg:58.30ms
step:2077/2330 train_time:121090ms step_avg:58.30ms
step:2078/2330 train_time:121151ms step_avg:58.30ms
step:2079/2330 train_time:121207ms step_avg:58.30ms
step:2080/2330 train_time:121268ms step_avg:58.30ms
step:2081/2330 train_time:121324ms step_avg:58.30ms
step:2082/2330 train_time:121386ms step_avg:58.30ms
step:2083/2330 train_time:121442ms step_avg:58.30ms
step:2084/2330 train_time:121504ms step_avg:58.30ms
step:2085/2330 train_time:121560ms step_avg:58.30ms
step:2086/2330 train_time:121621ms step_avg:58.30ms
step:2087/2330 train_time:121678ms step_avg:58.30ms
step:2088/2330 train_time:121738ms step_avg:58.30ms
step:2089/2330 train_time:121796ms step_avg:58.30ms
step:2090/2330 train_time:121857ms step_avg:58.30ms
step:2091/2330 train_time:121914ms step_avg:58.30ms
step:2092/2330 train_time:121976ms step_avg:58.31ms
step:2093/2330 train_time:122034ms step_avg:58.31ms
step:2094/2330 train_time:122094ms step_avg:58.31ms
step:2095/2330 train_time:122151ms step_avg:58.31ms
step:2096/2330 train_time:122212ms step_avg:58.31ms
step:2097/2330 train_time:122270ms step_avg:58.31ms
step:2098/2330 train_time:122330ms step_avg:58.31ms
step:2099/2330 train_time:122388ms step_avg:58.31ms
step:2100/2330 train_time:122448ms step_avg:58.31ms
step:2101/2330 train_time:122505ms step_avg:58.31ms
step:2102/2330 train_time:122566ms step_avg:58.31ms
step:2103/2330 train_time:122622ms step_avg:58.31ms
step:2104/2330 train_time:122683ms step_avg:58.31ms
step:2105/2330 train_time:122740ms step_avg:58.31ms
step:2106/2330 train_time:122801ms step_avg:58.31ms
step:2107/2330 train_time:122857ms step_avg:58.31ms
step:2108/2330 train_time:122919ms step_avg:58.31ms
step:2109/2330 train_time:122977ms step_avg:58.31ms
step:2110/2330 train_time:123038ms step_avg:58.31ms
step:2111/2330 train_time:123095ms step_avg:58.31ms
step:2112/2330 train_time:123157ms step_avg:58.31ms
step:2113/2330 train_time:123214ms step_avg:58.31ms
step:2114/2330 train_time:123275ms step_avg:58.31ms
step:2115/2330 train_time:123333ms step_avg:58.31ms
step:2116/2330 train_time:123393ms step_avg:58.31ms
step:2117/2330 train_time:123451ms step_avg:58.31ms
step:2118/2330 train_time:123513ms step_avg:58.32ms
step:2119/2330 train_time:123570ms step_avg:58.32ms
step:2120/2330 train_time:123631ms step_avg:58.32ms
step:2121/2330 train_time:123688ms step_avg:58.32ms
step:2122/2330 train_time:123749ms step_avg:58.32ms
step:2123/2330 train_time:123806ms step_avg:58.32ms
step:2124/2330 train_time:123868ms step_avg:58.32ms
step:2125/2330 train_time:123924ms step_avg:58.32ms
step:2126/2330 train_time:123986ms step_avg:58.32ms
step:2127/2330 train_time:124042ms step_avg:58.32ms
step:2128/2330 train_time:124104ms step_avg:58.32ms
step:2129/2330 train_time:124160ms step_avg:58.32ms
step:2130/2330 train_time:124221ms step_avg:58.32ms
step:2131/2330 train_time:124277ms step_avg:58.32ms
step:2132/2330 train_time:124340ms step_avg:58.32ms
step:2133/2330 train_time:124396ms step_avg:58.32ms
step:2134/2330 train_time:124457ms step_avg:58.32ms
step:2135/2330 train_time:124514ms step_avg:58.32ms
step:2136/2330 train_time:124576ms step_avg:58.32ms
step:2137/2330 train_time:124634ms step_avg:58.32ms
step:2138/2330 train_time:124695ms step_avg:58.32ms
step:2139/2330 train_time:124753ms step_avg:58.32ms
step:2140/2330 train_time:124813ms step_avg:58.32ms
step:2141/2330 train_time:124871ms step_avg:58.32ms
step:2142/2330 train_time:124932ms step_avg:58.33ms
step:2143/2330 train_time:124989ms step_avg:58.32ms
step:2144/2330 train_time:125051ms step_avg:58.33ms
step:2145/2330 train_time:125108ms step_avg:58.33ms
step:2146/2330 train_time:125170ms step_avg:58.33ms
step:2147/2330 train_time:125227ms step_avg:58.33ms
step:2148/2330 train_time:125288ms step_avg:58.33ms
step:2149/2330 train_time:125344ms step_avg:58.33ms
step:2150/2330 train_time:125404ms step_avg:58.33ms
step:2151/2330 train_time:125461ms step_avg:58.33ms
step:2152/2330 train_time:125522ms step_avg:58.33ms
step:2153/2330 train_time:125579ms step_avg:58.33ms
step:2154/2330 train_time:125639ms step_avg:58.33ms
step:2155/2330 train_time:125697ms step_avg:58.33ms
step:2156/2330 train_time:125758ms step_avg:58.33ms
step:2157/2330 train_time:125815ms step_avg:58.33ms
step:2158/2330 train_time:125877ms step_avg:58.33ms
step:2159/2330 train_time:125934ms step_avg:58.33ms
step:2160/2330 train_time:125995ms step_avg:58.33ms
step:2161/2330 train_time:126053ms step_avg:58.33ms
step:2162/2330 train_time:126114ms step_avg:58.33ms
step:2163/2330 train_time:126171ms step_avg:58.33ms
step:2164/2330 train_time:126232ms step_avg:58.33ms
step:2165/2330 train_time:126290ms step_avg:58.33ms
step:2166/2330 train_time:126350ms step_avg:58.33ms
step:2167/2330 train_time:126407ms step_avg:58.33ms
step:2168/2330 train_time:126468ms step_avg:58.33ms
step:2169/2330 train_time:126525ms step_avg:58.33ms
step:2170/2330 train_time:126587ms step_avg:58.33ms
step:2171/2330 train_time:126643ms step_avg:58.33ms
step:2172/2330 train_time:126704ms step_avg:58.34ms
step:2173/2330 train_time:126760ms step_avg:58.33ms
step:2174/2330 train_time:126822ms step_avg:58.34ms
step:2175/2330 train_time:126878ms step_avg:58.33ms
step:2176/2330 train_time:126939ms step_avg:58.34ms
step:2177/2330 train_time:126996ms step_avg:58.34ms
step:2178/2330 train_time:127058ms step_avg:58.34ms
step:2179/2330 train_time:127115ms step_avg:58.34ms
step:2180/2330 train_time:127177ms step_avg:58.34ms
step:2181/2330 train_time:127235ms step_avg:58.34ms
step:2182/2330 train_time:127295ms step_avg:58.34ms
step:2183/2330 train_time:127353ms step_avg:58.34ms
step:2184/2330 train_time:127414ms step_avg:58.34ms
step:2185/2330 train_time:127472ms step_avg:58.34ms
step:2186/2330 train_time:127533ms step_avg:58.34ms
step:2187/2330 train_time:127591ms step_avg:58.34ms
step:2188/2330 train_time:127652ms step_avg:58.34ms
step:2189/2330 train_time:127710ms step_avg:58.34ms
step:2190/2330 train_time:127770ms step_avg:58.34ms
step:2191/2330 train_time:127827ms step_avg:58.34ms
step:2192/2330 train_time:127889ms step_avg:58.34ms
step:2193/2330 train_time:127946ms step_avg:58.34ms
step:2194/2330 train_time:128007ms step_avg:58.34ms
step:2195/2330 train_time:128063ms step_avg:58.34ms
step:2196/2330 train_time:128124ms step_avg:58.34ms
step:2197/2330 train_time:128180ms step_avg:58.34ms
step:2198/2330 train_time:128241ms step_avg:58.34ms
step:2199/2330 train_time:128298ms step_avg:58.34ms
step:2200/2330 train_time:128360ms step_avg:58.35ms
step:2201/2330 train_time:128417ms step_avg:58.34ms
step:2202/2330 train_time:128478ms step_avg:58.35ms
step:2203/2330 train_time:128535ms step_avg:58.35ms
step:2204/2330 train_time:128595ms step_avg:58.35ms
step:2205/2330 train_time:128652ms step_avg:58.35ms
step:2206/2330 train_time:128713ms step_avg:58.35ms
step:2207/2330 train_time:128770ms step_avg:58.35ms
step:2208/2330 train_time:128831ms step_avg:58.35ms
step:2209/2330 train_time:128888ms step_avg:58.35ms
step:2210/2330 train_time:128950ms step_avg:58.35ms
step:2211/2330 train_time:129007ms step_avg:58.35ms
step:2212/2330 train_time:129068ms step_avg:58.35ms
step:2213/2330 train_time:129125ms step_avg:58.35ms
step:2214/2330 train_time:129187ms step_avg:58.35ms
step:2215/2330 train_time:129243ms step_avg:58.35ms
step:2216/2330 train_time:129305ms step_avg:58.35ms
step:2217/2330 train_time:129362ms step_avg:58.35ms
step:2218/2330 train_time:129422ms step_avg:58.35ms
step:2219/2330 train_time:129479ms step_avg:58.35ms
step:2220/2330 train_time:129539ms step_avg:58.35ms
step:2221/2330 train_time:129596ms step_avg:58.35ms
step:2222/2330 train_time:129658ms step_avg:58.35ms
step:2223/2330 train_time:129715ms step_avg:58.35ms
step:2224/2330 train_time:129777ms step_avg:58.35ms
step:2225/2330 train_time:129834ms step_avg:58.35ms
step:2226/2330 train_time:129895ms step_avg:58.35ms
step:2227/2330 train_time:129952ms step_avg:58.35ms
step:2228/2330 train_time:130014ms step_avg:58.35ms
step:2229/2330 train_time:130073ms step_avg:58.35ms
step:2230/2330 train_time:130133ms step_avg:58.36ms
step:2231/2330 train_time:130192ms step_avg:58.36ms
step:2232/2330 train_time:130252ms step_avg:58.36ms
step:2233/2330 train_time:130309ms step_avg:58.36ms
step:2234/2330 train_time:130371ms step_avg:58.36ms
step:2235/2330 train_time:130428ms step_avg:58.36ms
step:2236/2330 train_time:130489ms step_avg:58.36ms
step:2237/2330 train_time:130545ms step_avg:58.36ms
step:2238/2330 train_time:130606ms step_avg:58.36ms
step:2239/2330 train_time:130662ms step_avg:58.36ms
step:2240/2330 train_time:130724ms step_avg:58.36ms
step:2241/2330 train_time:130781ms step_avg:58.36ms
step:2242/2330 train_time:130841ms step_avg:58.36ms
step:2243/2330 train_time:130897ms step_avg:58.36ms
step:2244/2330 train_time:130960ms step_avg:58.36ms
step:2245/2330 train_time:131016ms step_avg:58.36ms
step:2246/2330 train_time:131078ms step_avg:58.36ms
step:2247/2330 train_time:131134ms step_avg:58.36ms
step:2248/2330 train_time:131196ms step_avg:58.36ms
step:2249/2330 train_time:131254ms step_avg:58.36ms
step:2250/2330 train_time:131315ms step_avg:58.36ms
step:2250/2330 val_loss:4.7123 train_time:131396ms step_avg:58.40ms
step:2251/2330 train_time:131416ms step_avg:58.38ms
step:2252/2330 train_time:131436ms step_avg:58.36ms
step:2253/2330 train_time:131494ms step_avg:58.36ms
step:2254/2330 train_time:131560ms step_avg:58.37ms
step:2255/2330 train_time:131617ms step_avg:58.37ms
step:2256/2330 train_time:131678ms step_avg:58.37ms
step:2257/2330 train_time:131735ms step_avg:58.37ms
step:2258/2330 train_time:131795ms step_avg:58.37ms
step:2259/2330 train_time:131853ms step_avg:58.37ms
step:2260/2330 train_time:131912ms step_avg:58.37ms
step:2261/2330 train_time:131968ms step_avg:58.37ms
step:2262/2330 train_time:132027ms step_avg:58.37ms
step:2263/2330 train_time:132084ms step_avg:58.37ms
step:2264/2330 train_time:132144ms step_avg:58.37ms
step:2265/2330 train_time:132201ms step_avg:58.37ms
step:2266/2330 train_time:132261ms step_avg:58.37ms
step:2267/2330 train_time:132318ms step_avg:58.37ms
step:2268/2330 train_time:132379ms step_avg:58.37ms
step:2269/2330 train_time:132438ms step_avg:58.37ms
step:2270/2330 train_time:132501ms step_avg:58.37ms
step:2271/2330 train_time:132560ms step_avg:58.37ms
step:2272/2330 train_time:132623ms step_avg:58.37ms
step:2273/2330 train_time:132681ms step_avg:58.37ms
step:2274/2330 train_time:132741ms step_avg:58.37ms
step:2275/2330 train_time:132799ms step_avg:58.37ms
step:2276/2330 train_time:132859ms step_avg:58.37ms
step:2277/2330 train_time:132916ms step_avg:58.37ms
step:2278/2330 train_time:132976ms step_avg:58.37ms
step:2279/2330 train_time:133033ms step_avg:58.37ms
step:2280/2330 train_time:133093ms step_avg:58.37ms
step:2281/2330 train_time:133150ms step_avg:58.37ms
step:2282/2330 train_time:133211ms step_avg:58.37ms
step:2283/2330 train_time:133268ms step_avg:58.37ms
step:2284/2330 train_time:133328ms step_avg:58.37ms
step:2285/2330 train_time:133385ms step_avg:58.37ms
step:2286/2330 train_time:133447ms step_avg:58.38ms
step:2287/2330 train_time:133504ms step_avg:58.38ms
step:2288/2330 train_time:133566ms step_avg:58.38ms
step:2289/2330 train_time:133625ms step_avg:58.38ms
step:2290/2330 train_time:133686ms step_avg:58.38ms
step:2291/2330 train_time:133743ms step_avg:58.38ms
step:2292/2330 train_time:133804ms step_avg:58.38ms
step:2293/2330 train_time:133861ms step_avg:58.38ms
step:2294/2330 train_time:133923ms step_avg:58.38ms
step:2295/2330 train_time:133981ms step_avg:58.38ms
step:2296/2330 train_time:134041ms step_avg:58.38ms
step:2297/2330 train_time:134099ms step_avg:58.38ms
step:2298/2330 train_time:134159ms step_avg:58.38ms
step:2299/2330 train_time:134217ms step_avg:58.38ms
step:2300/2330 train_time:134277ms step_avg:58.38ms
step:2301/2330 train_time:134334ms step_avg:58.38ms
step:2302/2330 train_time:134397ms step_avg:58.38ms
step:2303/2330 train_time:134453ms step_avg:58.38ms
step:2304/2330 train_time:134516ms step_avg:58.38ms
step:2305/2330 train_time:134573ms step_avg:58.38ms
step:2306/2330 train_time:134636ms step_avg:58.38ms
step:2307/2330 train_time:134692ms step_avg:58.38ms
step:2308/2330 train_time:134754ms step_avg:58.39ms
step:2309/2330 train_time:134810ms step_avg:58.38ms
step:2310/2330 train_time:134872ms step_avg:58.39ms
step:2311/2330 train_time:134929ms step_avg:58.39ms
step:2312/2330 train_time:134989ms step_avg:58.39ms
step:2313/2330 train_time:135045ms step_avg:58.39ms
step:2314/2330 train_time:135108ms step_avg:58.39ms
step:2315/2330 train_time:135164ms step_avg:58.39ms
step:2316/2330 train_time:135225ms step_avg:58.39ms
step:2317/2330 train_time:135282ms step_avg:58.39ms
step:2318/2330 train_time:135343ms step_avg:58.39ms
step:2319/2330 train_time:135402ms step_avg:58.39ms
step:2320/2330 train_time:135462ms step_avg:58.39ms
step:2321/2330 train_time:135521ms step_avg:58.39ms
step:2322/2330 train_time:135582ms step_avg:58.39ms
step:2323/2330 train_time:135640ms step_avg:58.39ms
step:2324/2330 train_time:135701ms step_avg:58.39ms
step:2325/2330 train_time:135758ms step_avg:58.39ms
step:2326/2330 train_time:135820ms step_avg:58.39ms
step:2327/2330 train_time:135877ms step_avg:58.39ms
step:2328/2330 train_time:135938ms step_avg:58.39ms
step:2329/2330 train_time:135994ms step_avg:58.39ms
step:2330/2330 train_time:136057ms step_avg:58.39ms
step:2330/2330 val_loss:4.6890 train_time:136139ms step_avg:58.43ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
