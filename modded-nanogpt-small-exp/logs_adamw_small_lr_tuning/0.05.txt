import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--adamw_lr', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_lr_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_lr_tuning/{args2.adamw_lr}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=float(args2.adamw_lr),  betas=(0.85, 0.85), eps=1e-10, weight_decay=0.0, fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_lr_tuning/{args2.adamw_lr}", exist_ok=True)
            torch.save(log, f"logs_adamw_small_lr_tuning/{args2.adamw_lr}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 02:34:14 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   26C    P0             116W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   25C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   27C    P0             113W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   26C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:92ms step_avg:91.64ms
step:2/2330 train_time:185ms step_avg:92.64ms
step:3/2330 train_time:203ms step_avg:67.73ms
step:4/2330 train_time:223ms step_avg:55.69ms
step:5/2330 train_time:276ms step_avg:55.20ms
step:6/2330 train_time:333ms step_avg:55.58ms
step:7/2330 train_time:388ms step_avg:55.38ms
step:8/2330 train_time:445ms step_avg:55.60ms
step:9/2330 train_time:499ms step_avg:55.41ms
step:10/2330 train_time:556ms step_avg:55.58ms
step:11/2330 train_time:610ms step_avg:55.42ms
step:12/2330 train_time:667ms step_avg:55.57ms
step:13/2330 train_time:721ms step_avg:55.43ms
step:14/2330 train_time:778ms step_avg:55.55ms
step:15/2330 train_time:832ms step_avg:55.46ms
step:16/2330 train_time:889ms step_avg:55.56ms
step:17/2330 train_time:943ms step_avg:55.47ms
step:18/2330 train_time:1000ms step_avg:55.55ms
step:19/2330 train_time:1054ms step_avg:55.48ms
step:20/2330 train_time:1111ms step_avg:55.57ms
step:21/2330 train_time:1167ms step_avg:55.56ms
step:22/2330 train_time:1224ms step_avg:55.64ms
step:23/2330 train_time:1278ms step_avg:55.58ms
step:24/2330 train_time:1336ms step_avg:55.67ms
step:25/2330 train_time:1390ms step_avg:55.59ms
step:26/2330 train_time:1448ms step_avg:55.71ms
step:27/2330 train_time:1502ms step_avg:55.64ms
step:28/2330 train_time:1561ms step_avg:55.74ms
step:29/2330 train_time:1615ms step_avg:55.69ms
step:30/2330 train_time:1672ms step_avg:55.74ms
step:31/2330 train_time:1726ms step_avg:55.69ms
step:32/2330 train_time:1784ms step_avg:55.74ms
step:33/2330 train_time:1838ms step_avg:55.69ms
step:34/2330 train_time:1896ms step_avg:55.76ms
step:35/2330 train_time:1950ms step_avg:55.72ms
step:36/2330 train_time:2008ms step_avg:55.78ms
step:37/2330 train_time:2062ms step_avg:55.73ms
step:38/2330 train_time:2120ms step_avg:55.80ms
step:39/2330 train_time:2174ms step_avg:55.76ms
step:40/2330 train_time:2233ms step_avg:55.83ms
step:41/2330 train_time:2288ms step_avg:55.80ms
step:42/2330 train_time:2346ms step_avg:55.85ms
step:43/2330 train_time:2401ms step_avg:55.83ms
step:44/2330 train_time:2459ms step_avg:55.88ms
step:45/2330 train_time:2513ms step_avg:55.85ms
step:46/2330 train_time:2571ms step_avg:55.90ms
step:47/2330 train_time:2626ms step_avg:55.88ms
step:48/2330 train_time:2684ms step_avg:55.92ms
step:49/2330 train_time:2738ms step_avg:55.88ms
step:50/2330 train_time:2797ms step_avg:55.95ms
step:51/2330 train_time:2852ms step_avg:55.92ms
step:52/2330 train_time:2910ms step_avg:55.95ms
step:53/2330 train_time:2964ms step_avg:55.93ms
step:54/2330 train_time:3022ms step_avg:55.97ms
step:55/2330 train_time:3076ms step_avg:55.93ms
step:56/2330 train_time:3134ms step_avg:55.97ms
step:57/2330 train_time:3189ms step_avg:55.94ms
step:58/2330 train_time:3247ms step_avg:55.99ms
step:59/2330 train_time:3302ms step_avg:55.96ms
step:60/2330 train_time:3361ms step_avg:56.02ms
step:61/2330 train_time:3415ms step_avg:55.98ms
step:62/2330 train_time:3474ms step_avg:56.03ms
step:63/2330 train_time:3528ms step_avg:56.00ms
step:64/2330 train_time:3587ms step_avg:56.05ms
step:65/2330 train_time:3642ms step_avg:56.03ms
step:66/2330 train_time:3700ms step_avg:56.06ms
step:67/2330 train_time:3755ms step_avg:56.04ms
step:68/2330 train_time:3813ms step_avg:56.07ms
step:69/2330 train_time:3867ms step_avg:56.04ms
step:70/2330 train_time:3926ms step_avg:56.08ms
step:71/2330 train_time:3980ms step_avg:56.06ms
step:72/2330 train_time:4038ms step_avg:56.08ms
step:73/2330 train_time:4092ms step_avg:56.05ms
step:74/2330 train_time:4151ms step_avg:56.10ms
step:75/2330 train_time:4206ms step_avg:56.08ms
step:76/2330 train_time:4264ms step_avg:56.11ms
step:77/2330 train_time:4319ms step_avg:56.09ms
step:78/2330 train_time:4377ms step_avg:56.12ms
step:79/2330 train_time:4432ms step_avg:56.10ms
step:80/2330 train_time:4490ms step_avg:56.12ms
step:81/2330 train_time:4545ms step_avg:56.11ms
step:82/2330 train_time:4603ms step_avg:56.13ms
step:83/2330 train_time:4657ms step_avg:56.11ms
step:84/2330 train_time:4716ms step_avg:56.14ms
step:85/2330 train_time:4770ms step_avg:56.12ms
step:86/2330 train_time:4829ms step_avg:56.15ms
step:87/2330 train_time:4884ms step_avg:56.13ms
step:88/2330 train_time:4942ms step_avg:56.16ms
step:89/2330 train_time:4996ms step_avg:56.14ms
step:90/2330 train_time:5054ms step_avg:56.16ms
step:91/2330 train_time:5109ms step_avg:56.14ms
step:92/2330 train_time:5167ms step_avg:56.16ms
step:93/2330 train_time:5222ms step_avg:56.15ms
step:94/2330 train_time:5281ms step_avg:56.18ms
step:95/2330 train_time:5336ms step_avg:56.17ms
step:96/2330 train_time:5394ms step_avg:56.19ms
step:97/2330 train_time:5449ms step_avg:56.17ms
step:98/2330 train_time:5508ms step_avg:56.20ms
step:99/2330 train_time:5564ms step_avg:56.20ms
step:100/2330 train_time:5622ms step_avg:56.22ms
step:101/2330 train_time:5678ms step_avg:56.21ms
step:102/2330 train_time:5735ms step_avg:56.23ms
step:103/2330 train_time:5789ms step_avg:56.21ms
step:104/2330 train_time:5848ms step_avg:56.23ms
step:105/2330 train_time:5904ms step_avg:56.23ms
step:106/2330 train_time:5962ms step_avg:56.25ms
step:107/2330 train_time:6018ms step_avg:56.24ms
step:108/2330 train_time:6075ms step_avg:56.25ms
step:109/2330 train_time:6130ms step_avg:56.24ms
step:110/2330 train_time:6189ms step_avg:56.26ms
step:111/2330 train_time:6244ms step_avg:56.25ms
step:112/2330 train_time:6302ms step_avg:56.27ms
step:113/2330 train_time:6357ms step_avg:56.26ms
step:114/2330 train_time:6416ms step_avg:56.28ms
step:115/2330 train_time:6470ms step_avg:56.26ms
step:116/2330 train_time:6530ms step_avg:56.29ms
step:117/2330 train_time:6585ms step_avg:56.28ms
step:118/2330 train_time:6643ms step_avg:56.30ms
step:119/2330 train_time:6699ms step_avg:56.29ms
step:120/2330 train_time:6757ms step_avg:56.31ms
step:121/2330 train_time:6812ms step_avg:56.29ms
step:122/2330 train_time:6870ms step_avg:56.31ms
step:123/2330 train_time:6924ms step_avg:56.30ms
step:124/2330 train_time:6983ms step_avg:56.31ms
step:125/2330 train_time:7038ms step_avg:56.30ms
step:126/2330 train_time:7096ms step_avg:56.32ms
step:127/2330 train_time:7151ms step_avg:56.31ms
step:128/2330 train_time:7209ms step_avg:56.32ms
step:129/2330 train_time:7265ms step_avg:56.31ms
step:130/2330 train_time:7323ms step_avg:56.33ms
step:131/2330 train_time:7377ms step_avg:56.32ms
step:132/2330 train_time:7436ms step_avg:56.33ms
step:133/2330 train_time:7490ms step_avg:56.32ms
step:134/2330 train_time:7548ms step_avg:56.33ms
step:135/2330 train_time:7603ms step_avg:56.32ms
step:136/2330 train_time:7662ms step_avg:56.34ms
step:137/2330 train_time:7717ms step_avg:56.33ms
step:138/2330 train_time:7776ms step_avg:56.35ms
step:139/2330 train_time:7831ms step_avg:56.34ms
step:140/2330 train_time:7889ms step_avg:56.35ms
step:141/2330 train_time:7944ms step_avg:56.34ms
step:142/2330 train_time:8002ms step_avg:56.35ms
step:143/2330 train_time:8058ms step_avg:56.35ms
step:144/2330 train_time:8116ms step_avg:56.36ms
step:145/2330 train_time:8171ms step_avg:56.35ms
step:146/2330 train_time:8229ms step_avg:56.36ms
step:147/2330 train_time:8284ms step_avg:56.35ms
step:148/2330 train_time:8342ms step_avg:56.36ms
step:149/2330 train_time:8397ms step_avg:56.36ms
step:150/2330 train_time:8455ms step_avg:56.37ms
step:151/2330 train_time:8510ms step_avg:56.36ms
step:152/2330 train_time:8569ms step_avg:56.37ms
step:153/2330 train_time:8624ms step_avg:56.37ms
step:154/2330 train_time:8682ms step_avg:56.38ms
step:155/2330 train_time:8737ms step_avg:56.37ms
step:156/2330 train_time:8795ms step_avg:56.38ms
step:157/2330 train_time:8850ms step_avg:56.37ms
step:158/2330 train_time:8908ms step_avg:56.38ms
step:159/2330 train_time:8963ms step_avg:56.37ms
step:160/2330 train_time:9022ms step_avg:56.39ms
step:161/2330 train_time:9077ms step_avg:56.38ms
step:162/2330 train_time:9135ms step_avg:56.39ms
step:163/2330 train_time:9190ms step_avg:56.38ms
step:164/2330 train_time:9248ms step_avg:56.39ms
step:165/2330 train_time:9303ms step_avg:56.38ms
step:166/2330 train_time:9361ms step_avg:56.39ms
step:167/2330 train_time:9417ms step_avg:56.39ms
step:168/2330 train_time:9475ms step_avg:56.40ms
step:169/2330 train_time:9530ms step_avg:56.39ms
step:170/2330 train_time:9589ms step_avg:56.40ms
step:171/2330 train_time:9644ms step_avg:56.40ms
step:172/2330 train_time:9704ms step_avg:56.42ms
step:173/2330 train_time:9759ms step_avg:56.41ms
step:174/2330 train_time:9818ms step_avg:56.42ms
step:175/2330 train_time:9873ms step_avg:56.42ms
step:176/2330 train_time:9932ms step_avg:56.43ms
step:177/2330 train_time:9986ms step_avg:56.42ms
step:178/2330 train_time:10047ms step_avg:56.44ms
step:179/2330 train_time:10102ms step_avg:56.43ms
step:180/2330 train_time:10161ms step_avg:56.45ms
step:181/2330 train_time:10216ms step_avg:56.44ms
step:182/2330 train_time:10275ms step_avg:56.45ms
step:183/2330 train_time:10329ms step_avg:56.44ms
step:184/2330 train_time:10388ms step_avg:56.46ms
step:185/2330 train_time:10443ms step_avg:56.45ms
step:186/2330 train_time:10501ms step_avg:56.46ms
step:187/2330 train_time:10556ms step_avg:56.45ms
step:188/2330 train_time:10615ms step_avg:56.46ms
step:189/2330 train_time:10671ms step_avg:56.46ms
step:190/2330 train_time:10730ms step_avg:56.48ms
step:191/2330 train_time:10786ms step_avg:56.47ms
step:192/2330 train_time:10844ms step_avg:56.48ms
step:193/2330 train_time:10900ms step_avg:56.48ms
step:194/2330 train_time:10959ms step_avg:56.49ms
step:195/2330 train_time:11014ms step_avg:56.48ms
step:196/2330 train_time:11073ms step_avg:56.49ms
step:197/2330 train_time:11128ms step_avg:56.49ms
step:198/2330 train_time:11187ms step_avg:56.50ms
step:199/2330 train_time:11243ms step_avg:56.50ms
step:200/2330 train_time:11301ms step_avg:56.51ms
step:201/2330 train_time:11357ms step_avg:56.50ms
step:202/2330 train_time:11416ms step_avg:56.51ms
step:203/2330 train_time:11471ms step_avg:56.51ms
step:204/2330 train_time:11530ms step_avg:56.52ms
step:205/2330 train_time:11585ms step_avg:56.51ms
step:206/2330 train_time:11645ms step_avg:56.53ms
step:207/2330 train_time:11700ms step_avg:56.52ms
step:208/2330 train_time:11759ms step_avg:56.53ms
step:209/2330 train_time:11814ms step_avg:56.53ms
step:210/2330 train_time:11873ms step_avg:56.54ms
step:211/2330 train_time:11929ms step_avg:56.53ms
step:212/2330 train_time:11987ms step_avg:56.54ms
step:213/2330 train_time:12043ms step_avg:56.54ms
step:214/2330 train_time:12103ms step_avg:56.56ms
step:215/2330 train_time:12158ms step_avg:56.55ms
step:216/2330 train_time:12217ms step_avg:56.56ms
step:217/2330 train_time:12272ms step_avg:56.55ms
step:218/2330 train_time:12331ms step_avg:56.57ms
step:219/2330 train_time:12387ms step_avg:56.56ms
step:220/2330 train_time:12446ms step_avg:56.57ms
step:221/2330 train_time:12502ms step_avg:56.57ms
step:222/2330 train_time:12560ms step_avg:56.58ms
step:223/2330 train_time:12615ms step_avg:56.57ms
step:224/2330 train_time:12674ms step_avg:56.58ms
step:225/2330 train_time:12729ms step_avg:56.57ms
step:226/2330 train_time:12789ms step_avg:56.59ms
step:227/2330 train_time:12845ms step_avg:56.59ms
step:228/2330 train_time:12903ms step_avg:56.59ms
step:229/2330 train_time:12959ms step_avg:56.59ms
step:230/2330 train_time:13018ms step_avg:56.60ms
step:231/2330 train_time:13073ms step_avg:56.59ms
step:232/2330 train_time:13131ms step_avg:56.60ms
step:233/2330 train_time:13187ms step_avg:56.59ms
step:234/2330 train_time:13246ms step_avg:56.61ms
step:235/2330 train_time:13302ms step_avg:56.60ms
step:236/2330 train_time:13361ms step_avg:56.61ms
step:237/2330 train_time:13416ms step_avg:56.61ms
step:238/2330 train_time:13474ms step_avg:56.61ms
step:239/2330 train_time:13530ms step_avg:56.61ms
step:240/2330 train_time:13589ms step_avg:56.62ms
step:241/2330 train_time:13645ms step_avg:56.62ms
step:242/2330 train_time:13704ms step_avg:56.63ms
step:243/2330 train_time:13760ms step_avg:56.63ms
step:244/2330 train_time:13819ms step_avg:56.64ms
step:245/2330 train_time:13874ms step_avg:56.63ms
step:246/2330 train_time:13933ms step_avg:56.64ms
step:247/2330 train_time:13989ms step_avg:56.63ms
step:248/2330 train_time:14047ms step_avg:56.64ms
step:249/2330 train_time:14103ms step_avg:56.64ms
step:250/2330 train_time:14163ms step_avg:56.65ms
step:250/2330 val_loss:6.5011 train_time:14241ms step_avg:56.96ms
step:251/2330 train_time:14259ms step_avg:56.81ms
step:252/2330 train_time:14279ms step_avg:56.66ms
step:253/2330 train_time:14334ms step_avg:56.66ms
step:254/2330 train_time:14396ms step_avg:56.68ms
step:255/2330 train_time:14452ms step_avg:56.67ms
step:256/2330 train_time:14518ms step_avg:56.71ms
step:257/2330 train_time:14573ms step_avg:56.70ms
step:258/2330 train_time:14633ms step_avg:56.72ms
step:259/2330 train_time:14689ms step_avg:56.71ms
step:260/2330 train_time:14748ms step_avg:56.72ms
step:261/2330 train_time:14803ms step_avg:56.72ms
step:262/2330 train_time:14862ms step_avg:56.73ms
step:263/2330 train_time:14917ms step_avg:56.72ms
step:264/2330 train_time:14975ms step_avg:56.73ms
step:265/2330 train_time:15031ms step_avg:56.72ms
step:266/2330 train_time:15089ms step_avg:56.73ms
step:267/2330 train_time:15145ms step_avg:56.72ms
step:268/2330 train_time:15204ms step_avg:56.73ms
step:269/2330 train_time:15259ms step_avg:56.73ms
step:270/2330 train_time:15318ms step_avg:56.73ms
step:271/2330 train_time:15374ms step_avg:56.73ms
step:272/2330 train_time:15436ms step_avg:56.75ms
step:273/2330 train_time:15492ms step_avg:56.75ms
step:274/2330 train_time:15552ms step_avg:56.76ms
step:275/2330 train_time:15607ms step_avg:56.75ms
step:276/2330 train_time:15666ms step_avg:56.76ms
step:277/2330 train_time:15722ms step_avg:56.76ms
step:278/2330 train_time:15781ms step_avg:56.77ms
step:279/2330 train_time:15837ms step_avg:56.76ms
step:280/2330 train_time:15895ms step_avg:56.77ms
step:281/2330 train_time:15950ms step_avg:56.76ms
step:282/2330 train_time:16009ms step_avg:56.77ms
step:283/2330 train_time:16065ms step_avg:56.77ms
step:284/2330 train_time:16123ms step_avg:56.77ms
step:285/2330 train_time:16178ms step_avg:56.77ms
step:286/2330 train_time:16237ms step_avg:56.77ms
step:287/2330 train_time:16293ms step_avg:56.77ms
step:288/2330 train_time:16353ms step_avg:56.78ms
step:289/2330 train_time:16409ms step_avg:56.78ms
step:290/2330 train_time:16469ms step_avg:56.79ms
step:291/2330 train_time:16525ms step_avg:56.79ms
step:292/2330 train_time:16584ms step_avg:56.79ms
step:293/2330 train_time:16640ms step_avg:56.79ms
step:294/2330 train_time:16700ms step_avg:56.80ms
step:295/2330 train_time:16756ms step_avg:56.80ms
step:296/2330 train_time:16814ms step_avg:56.80ms
step:297/2330 train_time:16869ms step_avg:56.80ms
step:298/2330 train_time:16928ms step_avg:56.80ms
step:299/2330 train_time:16983ms step_avg:56.80ms
step:300/2330 train_time:17042ms step_avg:56.81ms
step:301/2330 train_time:17097ms step_avg:56.80ms
step:302/2330 train_time:17156ms step_avg:56.81ms
step:303/2330 train_time:17211ms step_avg:56.80ms
step:304/2330 train_time:17270ms step_avg:56.81ms
step:305/2330 train_time:17325ms step_avg:56.80ms
step:306/2330 train_time:17386ms step_avg:56.82ms
step:307/2330 train_time:17442ms step_avg:56.81ms
step:308/2330 train_time:17501ms step_avg:56.82ms
step:309/2330 train_time:17557ms step_avg:56.82ms
step:310/2330 train_time:17615ms step_avg:56.82ms
step:311/2330 train_time:17670ms step_avg:56.82ms
step:312/2330 train_time:17729ms step_avg:56.83ms
step:313/2330 train_time:17785ms step_avg:56.82ms
step:314/2330 train_time:17844ms step_avg:56.83ms
step:315/2330 train_time:17900ms step_avg:56.83ms
step:316/2330 train_time:17959ms step_avg:56.83ms
step:317/2330 train_time:18015ms step_avg:56.83ms
step:318/2330 train_time:18073ms step_avg:56.83ms
step:319/2330 train_time:18128ms step_avg:56.83ms
step:320/2330 train_time:18187ms step_avg:56.83ms
step:321/2330 train_time:18243ms step_avg:56.83ms
step:322/2330 train_time:18302ms step_avg:56.84ms
step:323/2330 train_time:18358ms step_avg:56.84ms
step:324/2330 train_time:18417ms step_avg:56.84ms
step:325/2330 train_time:18472ms step_avg:56.84ms
step:326/2330 train_time:18531ms step_avg:56.84ms
step:327/2330 train_time:18587ms step_avg:56.84ms
step:328/2330 train_time:18647ms step_avg:56.85ms
step:329/2330 train_time:18703ms step_avg:56.85ms
step:330/2330 train_time:18762ms step_avg:56.85ms
step:331/2330 train_time:18818ms step_avg:56.85ms
step:332/2330 train_time:18876ms step_avg:56.86ms
step:333/2330 train_time:18932ms step_avg:56.85ms
step:334/2330 train_time:18991ms step_avg:56.86ms
step:335/2330 train_time:19047ms step_avg:56.86ms
step:336/2330 train_time:19106ms step_avg:56.86ms
step:337/2330 train_time:19162ms step_avg:56.86ms
step:338/2330 train_time:19220ms step_avg:56.87ms
step:339/2330 train_time:19277ms step_avg:56.86ms
step:340/2330 train_time:19335ms step_avg:56.87ms
step:341/2330 train_time:19391ms step_avg:56.87ms
step:342/2330 train_time:19450ms step_avg:56.87ms
step:343/2330 train_time:19506ms step_avg:56.87ms
step:344/2330 train_time:19565ms step_avg:56.88ms
step:345/2330 train_time:19621ms step_avg:56.87ms
step:346/2330 train_time:19681ms step_avg:56.88ms
step:347/2330 train_time:19736ms step_avg:56.88ms
step:348/2330 train_time:19795ms step_avg:56.88ms
step:349/2330 train_time:19851ms step_avg:56.88ms
step:350/2330 train_time:19909ms step_avg:56.88ms
step:351/2330 train_time:19965ms step_avg:56.88ms
step:352/2330 train_time:20024ms step_avg:56.89ms
step:353/2330 train_time:20080ms step_avg:56.88ms
step:354/2330 train_time:20139ms step_avg:56.89ms
step:355/2330 train_time:20196ms step_avg:56.89ms
step:356/2330 train_time:20254ms step_avg:56.89ms
step:357/2330 train_time:20310ms step_avg:56.89ms
step:358/2330 train_time:20369ms step_avg:56.90ms
step:359/2330 train_time:20425ms step_avg:56.89ms
step:360/2330 train_time:20484ms step_avg:56.90ms
step:361/2330 train_time:20540ms step_avg:56.90ms
step:362/2330 train_time:20598ms step_avg:56.90ms
step:363/2330 train_time:20654ms step_avg:56.90ms
step:364/2330 train_time:20713ms step_avg:56.90ms
step:365/2330 train_time:20769ms step_avg:56.90ms
step:366/2330 train_time:20829ms step_avg:56.91ms
step:367/2330 train_time:20885ms step_avg:56.91ms
step:368/2330 train_time:20943ms step_avg:56.91ms
step:369/2330 train_time:20998ms step_avg:56.91ms
step:370/2330 train_time:21058ms step_avg:56.91ms
step:371/2330 train_time:21113ms step_avg:56.91ms
step:372/2330 train_time:21173ms step_avg:56.92ms
step:373/2330 train_time:21228ms step_avg:56.91ms
step:374/2330 train_time:21287ms step_avg:56.92ms
step:375/2330 train_time:21344ms step_avg:56.92ms
step:376/2330 train_time:21403ms step_avg:56.92ms
step:377/2330 train_time:21458ms step_avg:56.92ms
step:378/2330 train_time:21517ms step_avg:56.92ms
step:379/2330 train_time:21573ms step_avg:56.92ms
step:380/2330 train_time:21631ms step_avg:56.92ms
step:381/2330 train_time:21687ms step_avg:56.92ms
step:382/2330 train_time:21746ms step_avg:56.93ms
step:383/2330 train_time:21802ms step_avg:56.93ms
step:384/2330 train_time:21861ms step_avg:56.93ms
step:385/2330 train_time:21917ms step_avg:56.93ms
step:386/2330 train_time:21975ms step_avg:56.93ms
step:387/2330 train_time:22031ms step_avg:56.93ms
step:388/2330 train_time:22090ms step_avg:56.93ms
step:389/2330 train_time:22146ms step_avg:56.93ms
step:390/2330 train_time:22204ms step_avg:56.93ms
step:391/2330 train_time:22260ms step_avg:56.93ms
step:392/2330 train_time:22319ms step_avg:56.94ms
step:393/2330 train_time:22375ms step_avg:56.93ms
step:394/2330 train_time:22434ms step_avg:56.94ms
step:395/2330 train_time:22490ms step_avg:56.94ms
step:396/2330 train_time:22549ms step_avg:56.94ms
step:397/2330 train_time:22605ms step_avg:56.94ms
step:398/2330 train_time:22663ms step_avg:56.94ms
step:399/2330 train_time:22719ms step_avg:56.94ms
step:400/2330 train_time:22778ms step_avg:56.94ms
step:401/2330 train_time:22834ms step_avg:56.94ms
step:402/2330 train_time:22893ms step_avg:56.95ms
step:403/2330 train_time:22949ms step_avg:56.94ms
step:404/2330 train_time:23007ms step_avg:56.95ms
step:405/2330 train_time:23063ms step_avg:56.95ms
step:406/2330 train_time:23122ms step_avg:56.95ms
step:407/2330 train_time:23178ms step_avg:56.95ms
step:408/2330 train_time:23237ms step_avg:56.95ms
step:409/2330 train_time:23293ms step_avg:56.95ms
step:410/2330 train_time:23353ms step_avg:56.96ms
step:411/2330 train_time:23408ms step_avg:56.95ms
step:412/2330 train_time:23467ms step_avg:56.96ms
step:413/2330 train_time:23523ms step_avg:56.96ms
step:414/2330 train_time:23582ms step_avg:56.96ms
step:415/2330 train_time:23637ms step_avg:56.96ms
step:416/2330 train_time:23697ms step_avg:56.96ms
step:417/2330 train_time:23753ms step_avg:56.96ms
step:418/2330 train_time:23812ms step_avg:56.97ms
step:419/2330 train_time:23867ms step_avg:56.96ms
step:420/2330 train_time:23927ms step_avg:56.97ms
step:421/2330 train_time:23983ms step_avg:56.97ms
step:422/2330 train_time:24042ms step_avg:56.97ms
step:423/2330 train_time:24098ms step_avg:56.97ms
step:424/2330 train_time:24157ms step_avg:56.97ms
step:425/2330 train_time:24212ms step_avg:56.97ms
step:426/2330 train_time:24271ms step_avg:56.97ms
step:427/2330 train_time:24327ms step_avg:56.97ms
step:428/2330 train_time:24387ms step_avg:56.98ms
step:429/2330 train_time:24443ms step_avg:56.98ms
step:430/2330 train_time:24502ms step_avg:56.98ms
step:431/2330 train_time:24557ms step_avg:56.98ms
step:432/2330 train_time:24617ms step_avg:56.98ms
step:433/2330 train_time:24673ms step_avg:56.98ms
step:434/2330 train_time:24731ms step_avg:56.98ms
step:435/2330 train_time:24787ms step_avg:56.98ms
step:436/2330 train_time:24846ms step_avg:56.99ms
step:437/2330 train_time:24902ms step_avg:56.98ms
step:438/2330 train_time:24961ms step_avg:56.99ms
step:439/2330 train_time:25017ms step_avg:56.99ms
step:440/2330 train_time:25076ms step_avg:56.99ms
step:441/2330 train_time:25132ms step_avg:56.99ms
step:442/2330 train_time:25191ms step_avg:56.99ms
step:443/2330 train_time:25247ms step_avg:56.99ms
step:444/2330 train_time:25305ms step_avg:56.99ms
step:445/2330 train_time:25362ms step_avg:56.99ms
step:446/2330 train_time:25420ms step_avg:57.00ms
step:447/2330 train_time:25476ms step_avg:56.99ms
step:448/2330 train_time:25535ms step_avg:57.00ms
step:449/2330 train_time:25591ms step_avg:56.99ms
step:450/2330 train_time:25649ms step_avg:57.00ms
step:451/2330 train_time:25705ms step_avg:57.00ms
step:452/2330 train_time:25765ms step_avg:57.00ms
step:453/2330 train_time:25820ms step_avg:57.00ms
step:454/2330 train_time:25880ms step_avg:57.00ms
step:455/2330 train_time:25936ms step_avg:57.00ms
step:456/2330 train_time:25995ms step_avg:57.01ms
step:457/2330 train_time:26050ms step_avg:57.00ms
step:458/2330 train_time:26109ms step_avg:57.01ms
step:459/2330 train_time:26165ms step_avg:57.00ms
step:460/2330 train_time:26225ms step_avg:57.01ms
step:461/2330 train_time:26282ms step_avg:57.01ms
step:462/2330 train_time:26341ms step_avg:57.01ms
step:463/2330 train_time:26398ms step_avg:57.01ms
step:464/2330 train_time:26456ms step_avg:57.02ms
step:465/2330 train_time:26512ms step_avg:57.02ms
step:466/2330 train_time:26570ms step_avg:57.02ms
step:467/2330 train_time:26626ms step_avg:57.01ms
step:468/2330 train_time:26685ms step_avg:57.02ms
step:469/2330 train_time:26741ms step_avg:57.02ms
step:470/2330 train_time:26800ms step_avg:57.02ms
step:471/2330 train_time:26855ms step_avg:57.02ms
step:472/2330 train_time:26914ms step_avg:57.02ms
step:473/2330 train_time:26970ms step_avg:57.02ms
step:474/2330 train_time:27029ms step_avg:57.02ms
step:475/2330 train_time:27085ms step_avg:57.02ms
step:476/2330 train_time:27144ms step_avg:57.03ms
step:477/2330 train_time:27201ms step_avg:57.02ms
step:478/2330 train_time:27259ms step_avg:57.03ms
step:479/2330 train_time:27316ms step_avg:57.03ms
step:480/2330 train_time:27374ms step_avg:57.03ms
step:481/2330 train_time:27430ms step_avg:57.03ms
step:482/2330 train_time:27489ms step_avg:57.03ms
step:483/2330 train_time:27545ms step_avg:57.03ms
step:484/2330 train_time:27604ms step_avg:57.03ms
step:485/2330 train_time:27659ms step_avg:57.03ms
step:486/2330 train_time:27718ms step_avg:57.03ms
step:487/2330 train_time:27773ms step_avg:57.03ms
step:488/2330 train_time:27832ms step_avg:57.03ms
step:489/2330 train_time:27888ms step_avg:57.03ms
step:490/2330 train_time:27946ms step_avg:57.03ms
step:491/2330 train_time:28002ms step_avg:57.03ms
step:492/2330 train_time:28061ms step_avg:57.03ms
step:493/2330 train_time:28117ms step_avg:57.03ms
step:494/2330 train_time:28176ms step_avg:57.04ms
step:495/2330 train_time:28232ms step_avg:57.03ms
step:496/2330 train_time:28291ms step_avg:57.04ms
step:497/2330 train_time:28347ms step_avg:57.04ms
step:498/2330 train_time:28406ms step_avg:57.04ms
step:499/2330 train_time:28462ms step_avg:57.04ms
step:500/2330 train_time:28521ms step_avg:57.04ms
step:500/2330 val_loss:5.7556 train_time:28600ms step_avg:57.20ms
step:501/2330 train_time:28618ms step_avg:57.12ms
step:502/2330 train_time:28638ms step_avg:57.05ms
step:503/2330 train_time:28693ms step_avg:57.04ms
step:504/2330 train_time:28759ms step_avg:57.06ms
step:505/2330 train_time:28816ms step_avg:57.06ms
step:506/2330 train_time:28879ms step_avg:57.07ms
step:507/2330 train_time:28935ms step_avg:57.07ms
step:508/2330 train_time:28994ms step_avg:57.07ms
step:509/2330 train_time:29049ms step_avg:57.07ms
step:510/2330 train_time:29109ms step_avg:57.08ms
step:511/2330 train_time:29165ms step_avg:57.07ms
step:512/2330 train_time:29223ms step_avg:57.08ms
step:513/2330 train_time:29279ms step_avg:57.07ms
step:514/2330 train_time:29337ms step_avg:57.08ms
step:515/2330 train_time:29392ms step_avg:57.07ms
step:516/2330 train_time:29451ms step_avg:57.07ms
step:517/2330 train_time:29506ms step_avg:57.07ms
step:518/2330 train_time:29565ms step_avg:57.07ms
step:519/2330 train_time:29621ms step_avg:57.07ms
step:520/2330 train_time:29680ms step_avg:57.08ms
step:521/2330 train_time:29737ms step_avg:57.08ms
step:522/2330 train_time:29798ms step_avg:57.08ms
step:523/2330 train_time:29854ms step_avg:57.08ms
step:524/2330 train_time:29916ms step_avg:57.09ms
step:525/2330 train_time:29971ms step_avg:57.09ms
step:526/2330 train_time:30031ms step_avg:57.09ms
step:527/2330 train_time:30086ms step_avg:57.09ms
step:528/2330 train_time:30146ms step_avg:57.09ms
step:529/2330 train_time:30201ms step_avg:57.09ms
step:530/2330 train_time:30260ms step_avg:57.09ms
step:531/2330 train_time:30315ms step_avg:57.09ms
step:532/2330 train_time:30374ms step_avg:57.09ms
step:533/2330 train_time:30429ms step_avg:57.09ms
step:534/2330 train_time:30488ms step_avg:57.09ms
step:535/2330 train_time:30543ms step_avg:57.09ms
step:536/2330 train_time:30603ms step_avg:57.09ms
step:537/2330 train_time:30659ms step_avg:57.09ms
step:538/2330 train_time:30718ms step_avg:57.10ms
step:539/2330 train_time:30775ms step_avg:57.10ms
step:540/2330 train_time:30835ms step_avg:57.10ms
step:541/2330 train_time:30892ms step_avg:57.10ms
step:542/2330 train_time:30951ms step_avg:57.11ms
step:543/2330 train_time:31007ms step_avg:57.10ms
step:544/2330 train_time:31066ms step_avg:57.11ms
step:545/2330 train_time:31122ms step_avg:57.11ms
step:546/2330 train_time:31181ms step_avg:57.11ms
step:547/2330 train_time:31237ms step_avg:57.11ms
step:548/2330 train_time:31296ms step_avg:57.11ms
step:549/2330 train_time:31351ms step_avg:57.11ms
step:550/2330 train_time:31410ms step_avg:57.11ms
step:551/2330 train_time:31465ms step_avg:57.11ms
step:552/2330 train_time:31524ms step_avg:57.11ms
step:553/2330 train_time:31579ms step_avg:57.11ms
step:554/2330 train_time:31639ms step_avg:57.11ms
step:555/2330 train_time:31695ms step_avg:57.11ms
step:556/2330 train_time:31755ms step_avg:57.11ms
step:557/2330 train_time:31811ms step_avg:57.11ms
step:558/2330 train_time:31870ms step_avg:57.11ms
step:559/2330 train_time:31926ms step_avg:57.11ms
step:560/2330 train_time:31987ms step_avg:57.12ms
step:561/2330 train_time:32043ms step_avg:57.12ms
step:562/2330 train_time:32103ms step_avg:57.12ms
step:563/2330 train_time:32159ms step_avg:57.12ms
step:564/2330 train_time:32217ms step_avg:57.12ms
step:565/2330 train_time:32273ms step_avg:57.12ms
step:566/2330 train_time:32332ms step_avg:57.12ms
step:567/2330 train_time:32388ms step_avg:57.12ms
step:568/2330 train_time:32447ms step_avg:57.13ms
step:569/2330 train_time:32503ms step_avg:57.12ms
step:570/2330 train_time:32563ms step_avg:57.13ms
step:571/2330 train_time:32618ms step_avg:57.12ms
step:572/2330 train_time:32677ms step_avg:57.13ms
step:573/2330 train_time:32733ms step_avg:57.13ms
step:574/2330 train_time:32792ms step_avg:57.13ms
step:575/2330 train_time:32849ms step_avg:57.13ms
step:576/2330 train_time:32909ms step_avg:57.13ms
step:577/2330 train_time:32965ms step_avg:57.13ms
step:578/2330 train_time:33025ms step_avg:57.14ms
step:579/2330 train_time:33082ms step_avg:57.14ms
step:580/2330 train_time:33141ms step_avg:57.14ms
step:581/2330 train_time:33196ms step_avg:57.14ms
step:582/2330 train_time:33256ms step_avg:57.14ms
step:583/2330 train_time:33311ms step_avg:57.14ms
step:584/2330 train_time:33371ms step_avg:57.14ms
step:585/2330 train_time:33427ms step_avg:57.14ms
step:586/2330 train_time:33485ms step_avg:57.14ms
step:587/2330 train_time:33541ms step_avg:57.14ms
step:588/2330 train_time:33600ms step_avg:57.14ms
step:589/2330 train_time:33655ms step_avg:57.14ms
step:590/2330 train_time:33714ms step_avg:57.14ms
step:591/2330 train_time:33770ms step_avg:57.14ms
step:592/2330 train_time:33830ms step_avg:57.14ms
step:593/2330 train_time:33886ms step_avg:57.14ms
step:594/2330 train_time:33945ms step_avg:57.15ms
step:595/2330 train_time:34001ms step_avg:57.14ms
step:596/2330 train_time:34060ms step_avg:57.15ms
step:597/2330 train_time:34115ms step_avg:57.14ms
step:598/2330 train_time:34175ms step_avg:57.15ms
step:599/2330 train_time:34231ms step_avg:57.15ms
step:600/2330 train_time:34291ms step_avg:57.15ms
step:601/2330 train_time:34347ms step_avg:57.15ms
step:602/2330 train_time:34406ms step_avg:57.15ms
step:603/2330 train_time:34462ms step_avg:57.15ms
step:604/2330 train_time:34521ms step_avg:57.15ms
step:605/2330 train_time:34576ms step_avg:57.15ms
step:606/2330 train_time:34635ms step_avg:57.15ms
step:607/2330 train_time:34690ms step_avg:57.15ms
step:608/2330 train_time:34750ms step_avg:57.16ms
step:609/2330 train_time:34807ms step_avg:57.15ms
step:610/2330 train_time:34866ms step_avg:57.16ms
step:611/2330 train_time:34922ms step_avg:57.16ms
step:612/2330 train_time:34982ms step_avg:57.16ms
step:613/2330 train_time:35039ms step_avg:57.16ms
step:614/2330 train_time:35097ms step_avg:57.16ms
step:615/2330 train_time:35153ms step_avg:57.16ms
step:616/2330 train_time:35212ms step_avg:57.16ms
step:617/2330 train_time:35269ms step_avg:57.16ms
step:618/2330 train_time:35329ms step_avg:57.17ms
step:619/2330 train_time:35384ms step_avg:57.16ms
step:620/2330 train_time:35444ms step_avg:57.17ms
step:621/2330 train_time:35499ms step_avg:57.16ms
step:622/2330 train_time:35558ms step_avg:57.17ms
step:623/2330 train_time:35614ms step_avg:57.17ms
step:624/2330 train_time:35673ms step_avg:57.17ms
step:625/2330 train_time:35729ms step_avg:57.17ms
step:626/2330 train_time:35788ms step_avg:57.17ms
step:627/2330 train_time:35844ms step_avg:57.17ms
step:628/2330 train_time:35905ms step_avg:57.17ms
step:629/2330 train_time:35961ms step_avg:57.17ms
step:630/2330 train_time:36020ms step_avg:57.17ms
step:631/2330 train_time:36075ms step_avg:57.17ms
step:632/2330 train_time:36136ms step_avg:57.18ms
step:633/2330 train_time:36191ms step_avg:57.17ms
step:634/2330 train_time:36252ms step_avg:57.18ms
step:635/2330 train_time:36307ms step_avg:57.18ms
step:636/2330 train_time:36367ms step_avg:57.18ms
step:637/2330 train_time:36422ms step_avg:57.18ms
step:638/2330 train_time:36481ms step_avg:57.18ms
step:639/2330 train_time:36537ms step_avg:57.18ms
step:640/2330 train_time:36596ms step_avg:57.18ms
step:641/2330 train_time:36652ms step_avg:57.18ms
step:642/2330 train_time:36710ms step_avg:57.18ms
step:643/2330 train_time:36766ms step_avg:57.18ms
step:644/2330 train_time:36826ms step_avg:57.18ms
step:645/2330 train_time:36882ms step_avg:57.18ms
step:646/2330 train_time:36941ms step_avg:57.18ms
step:647/2330 train_time:36997ms step_avg:57.18ms
step:648/2330 train_time:37056ms step_avg:57.18ms
step:649/2330 train_time:37111ms step_avg:57.18ms
step:650/2330 train_time:37170ms step_avg:57.19ms
step:651/2330 train_time:37227ms step_avg:57.18ms
step:652/2330 train_time:37287ms step_avg:57.19ms
step:653/2330 train_time:37343ms step_avg:57.19ms
step:654/2330 train_time:37401ms step_avg:57.19ms
step:655/2330 train_time:37457ms step_avg:57.19ms
step:656/2330 train_time:37516ms step_avg:57.19ms
step:657/2330 train_time:37572ms step_avg:57.19ms
step:658/2330 train_time:37631ms step_avg:57.19ms
step:659/2330 train_time:37688ms step_avg:57.19ms
step:660/2330 train_time:37747ms step_avg:57.19ms
step:661/2330 train_time:37803ms step_avg:57.19ms
step:662/2330 train_time:37862ms step_avg:57.19ms
step:663/2330 train_time:37918ms step_avg:57.19ms
step:664/2330 train_time:37977ms step_avg:57.19ms
step:665/2330 train_time:38033ms step_avg:57.19ms
step:666/2330 train_time:38092ms step_avg:57.20ms
step:667/2330 train_time:38148ms step_avg:57.19ms
step:668/2330 train_time:38208ms step_avg:57.20ms
step:669/2330 train_time:38264ms step_avg:57.20ms
step:670/2330 train_time:38324ms step_avg:57.20ms
step:671/2330 train_time:38380ms step_avg:57.20ms
step:672/2330 train_time:38439ms step_avg:57.20ms
step:673/2330 train_time:38495ms step_avg:57.20ms
step:674/2330 train_time:38554ms step_avg:57.20ms
step:675/2330 train_time:38609ms step_avg:57.20ms
step:676/2330 train_time:38669ms step_avg:57.20ms
step:677/2330 train_time:38725ms step_avg:57.20ms
step:678/2330 train_time:38785ms step_avg:57.20ms
step:679/2330 train_time:38841ms step_avg:57.20ms
step:680/2330 train_time:38899ms step_avg:57.21ms
step:681/2330 train_time:38955ms step_avg:57.20ms
step:682/2330 train_time:39015ms step_avg:57.21ms
step:683/2330 train_time:39070ms step_avg:57.20ms
step:684/2330 train_time:39130ms step_avg:57.21ms
step:685/2330 train_time:39187ms step_avg:57.21ms
step:686/2330 train_time:39246ms step_avg:57.21ms
step:687/2330 train_time:39302ms step_avg:57.21ms
step:688/2330 train_time:39361ms step_avg:57.21ms
step:689/2330 train_time:39417ms step_avg:57.21ms
step:690/2330 train_time:39476ms step_avg:57.21ms
step:691/2330 train_time:39533ms step_avg:57.21ms
step:692/2330 train_time:39591ms step_avg:57.21ms
step:693/2330 train_time:39648ms step_avg:57.21ms
step:694/2330 train_time:39707ms step_avg:57.21ms
step:695/2330 train_time:39763ms step_avg:57.21ms
step:696/2330 train_time:39822ms step_avg:57.22ms
step:697/2330 train_time:39878ms step_avg:57.21ms
step:698/2330 train_time:39936ms step_avg:57.22ms
step:699/2330 train_time:39992ms step_avg:57.21ms
step:700/2330 train_time:40050ms step_avg:57.21ms
step:701/2330 train_time:40106ms step_avg:57.21ms
step:702/2330 train_time:40165ms step_avg:57.22ms
step:703/2330 train_time:40221ms step_avg:57.21ms
step:704/2330 train_time:40281ms step_avg:57.22ms
step:705/2330 train_time:40337ms step_avg:57.22ms
step:706/2330 train_time:40396ms step_avg:57.22ms
step:707/2330 train_time:40452ms step_avg:57.22ms
step:708/2330 train_time:40511ms step_avg:57.22ms
step:709/2330 train_time:40567ms step_avg:57.22ms
step:710/2330 train_time:40626ms step_avg:57.22ms
step:711/2330 train_time:40682ms step_avg:57.22ms
step:712/2330 train_time:40742ms step_avg:57.22ms
step:713/2330 train_time:40797ms step_avg:57.22ms
step:714/2330 train_time:40856ms step_avg:57.22ms
step:715/2330 train_time:40912ms step_avg:57.22ms
step:716/2330 train_time:40972ms step_avg:57.22ms
step:717/2330 train_time:41028ms step_avg:57.22ms
step:718/2330 train_time:41088ms step_avg:57.23ms
step:719/2330 train_time:41144ms step_avg:57.22ms
step:720/2330 train_time:41204ms step_avg:57.23ms
step:721/2330 train_time:41260ms step_avg:57.23ms
step:722/2330 train_time:41318ms step_avg:57.23ms
step:723/2330 train_time:41374ms step_avg:57.23ms
step:724/2330 train_time:41434ms step_avg:57.23ms
step:725/2330 train_time:41490ms step_avg:57.23ms
step:726/2330 train_time:41549ms step_avg:57.23ms
step:727/2330 train_time:41605ms step_avg:57.23ms
step:728/2330 train_time:41664ms step_avg:57.23ms
step:729/2330 train_time:41721ms step_avg:57.23ms
step:730/2330 train_time:41779ms step_avg:57.23ms
step:731/2330 train_time:41834ms step_avg:57.23ms
step:732/2330 train_time:41894ms step_avg:57.23ms
step:733/2330 train_time:41950ms step_avg:57.23ms
step:734/2330 train_time:42010ms step_avg:57.23ms
step:735/2330 train_time:42065ms step_avg:57.23ms
step:736/2330 train_time:42126ms step_avg:57.24ms
step:737/2330 train_time:42182ms step_avg:57.23ms
step:738/2330 train_time:42241ms step_avg:57.24ms
step:739/2330 train_time:42297ms step_avg:57.24ms
step:740/2330 train_time:42357ms step_avg:57.24ms
step:741/2330 train_time:42412ms step_avg:57.24ms
step:742/2330 train_time:42472ms step_avg:57.24ms
step:743/2330 train_time:42528ms step_avg:57.24ms
step:744/2330 train_time:42587ms step_avg:57.24ms
step:745/2330 train_time:42643ms step_avg:57.24ms
step:746/2330 train_time:42703ms step_avg:57.24ms
step:747/2330 train_time:42759ms step_avg:57.24ms
step:748/2330 train_time:42819ms step_avg:57.24ms
step:749/2330 train_time:42874ms step_avg:57.24ms
step:750/2330 train_time:42935ms step_avg:57.25ms
step:750/2330 val_loss:5.2932 train_time:43014ms step_avg:57.35ms
step:751/2330 train_time:43034ms step_avg:57.30ms
step:752/2330 train_time:43054ms step_avg:57.25ms
step:753/2330 train_time:43108ms step_avg:57.25ms
step:754/2330 train_time:43172ms step_avg:57.26ms
step:755/2330 train_time:43229ms step_avg:57.26ms
step:756/2330 train_time:43291ms step_avg:57.26ms
step:757/2330 train_time:43346ms step_avg:57.26ms
step:758/2330 train_time:43405ms step_avg:57.26ms
step:759/2330 train_time:43460ms step_avg:57.26ms
step:760/2330 train_time:43519ms step_avg:57.26ms
step:761/2330 train_time:43575ms step_avg:57.26ms
step:762/2330 train_time:43634ms step_avg:57.26ms
step:763/2330 train_time:43689ms step_avg:57.26ms
step:764/2330 train_time:43748ms step_avg:57.26ms
step:765/2330 train_time:43804ms step_avg:57.26ms
step:766/2330 train_time:43863ms step_avg:57.26ms
step:767/2330 train_time:43919ms step_avg:57.26ms
step:768/2330 train_time:43979ms step_avg:57.26ms
step:769/2330 train_time:44035ms step_avg:57.26ms
step:770/2330 train_time:44097ms step_avg:57.27ms
step:771/2330 train_time:44154ms step_avg:57.27ms
step:772/2330 train_time:44217ms step_avg:57.28ms
step:773/2330 train_time:44274ms step_avg:57.28ms
step:774/2330 train_time:44335ms step_avg:57.28ms
step:775/2330 train_time:44392ms step_avg:57.28ms
step:776/2330 train_time:44452ms step_avg:57.28ms
step:777/2330 train_time:44509ms step_avg:57.28ms
step:778/2330 train_time:44568ms step_avg:57.29ms
step:779/2330 train_time:44625ms step_avg:57.28ms
step:780/2330 train_time:44684ms step_avg:57.29ms
step:781/2330 train_time:44740ms step_avg:57.29ms
step:782/2330 train_time:44800ms step_avg:57.29ms
step:783/2330 train_time:44855ms step_avg:57.29ms
step:784/2330 train_time:44916ms step_avg:57.29ms
step:785/2330 train_time:44973ms step_avg:57.29ms
step:786/2330 train_time:45033ms step_avg:57.29ms
step:787/2330 train_time:45090ms step_avg:57.29ms
step:788/2330 train_time:45151ms step_avg:57.30ms
step:789/2330 train_time:45208ms step_avg:57.30ms
step:790/2330 train_time:45270ms step_avg:57.30ms
step:791/2330 train_time:45326ms step_avg:57.30ms
step:792/2330 train_time:45387ms step_avg:57.31ms
step:793/2330 train_time:45444ms step_avg:57.31ms
step:794/2330 train_time:45503ms step_avg:57.31ms
step:795/2330 train_time:45559ms step_avg:57.31ms
step:796/2330 train_time:45619ms step_avg:57.31ms
step:797/2330 train_time:45675ms step_avg:57.31ms
step:798/2330 train_time:45735ms step_avg:57.31ms
step:799/2330 train_time:45791ms step_avg:57.31ms
step:800/2330 train_time:45852ms step_avg:57.31ms
step:801/2330 train_time:45908ms step_avg:57.31ms
step:802/2330 train_time:45969ms step_avg:57.32ms
step:803/2330 train_time:46025ms step_avg:57.32ms
step:804/2330 train_time:46086ms step_avg:57.32ms
step:805/2330 train_time:46142ms step_avg:57.32ms
step:806/2330 train_time:46202ms step_avg:57.32ms
step:807/2330 train_time:46258ms step_avg:57.32ms
step:808/2330 train_time:46320ms step_avg:57.33ms
step:809/2330 train_time:46376ms step_avg:57.33ms
step:810/2330 train_time:46437ms step_avg:57.33ms
step:811/2330 train_time:46493ms step_avg:57.33ms
step:812/2330 train_time:46554ms step_avg:57.33ms
step:813/2330 train_time:46610ms step_avg:57.33ms
step:814/2330 train_time:46671ms step_avg:57.34ms
step:815/2330 train_time:46727ms step_avg:57.33ms
step:816/2330 train_time:46787ms step_avg:57.34ms
step:817/2330 train_time:46843ms step_avg:57.34ms
step:818/2330 train_time:46903ms step_avg:57.34ms
step:819/2330 train_time:46960ms step_avg:57.34ms
step:820/2330 train_time:47019ms step_avg:57.34ms
step:821/2330 train_time:47076ms step_avg:57.34ms
step:822/2330 train_time:47137ms step_avg:57.34ms
step:823/2330 train_time:47194ms step_avg:57.34ms
step:824/2330 train_time:47254ms step_avg:57.35ms
step:825/2330 train_time:47311ms step_avg:57.35ms
step:826/2330 train_time:47373ms step_avg:57.35ms
step:827/2330 train_time:47430ms step_avg:57.35ms
step:828/2330 train_time:47491ms step_avg:57.36ms
step:829/2330 train_time:47548ms step_avg:57.36ms
step:830/2330 train_time:47607ms step_avg:57.36ms
step:831/2330 train_time:47664ms step_avg:57.36ms
step:832/2330 train_time:47723ms step_avg:57.36ms
step:833/2330 train_time:47780ms step_avg:57.36ms
step:834/2330 train_time:47839ms step_avg:57.36ms
step:835/2330 train_time:47895ms step_avg:57.36ms
step:836/2330 train_time:47956ms step_avg:57.36ms
step:837/2330 train_time:48012ms step_avg:57.36ms
step:838/2330 train_time:48073ms step_avg:57.37ms
step:839/2330 train_time:48130ms step_avg:57.37ms
step:840/2330 train_time:48189ms step_avg:57.37ms
step:841/2330 train_time:48246ms step_avg:57.37ms
step:842/2330 train_time:48307ms step_avg:57.37ms
step:843/2330 train_time:48363ms step_avg:57.37ms
step:844/2330 train_time:48425ms step_avg:57.38ms
step:845/2330 train_time:48480ms step_avg:57.37ms
step:846/2330 train_time:48541ms step_avg:57.38ms
step:847/2330 train_time:48597ms step_avg:57.37ms
step:848/2330 train_time:48658ms step_avg:57.38ms
step:849/2330 train_time:48714ms step_avg:57.38ms
step:850/2330 train_time:48775ms step_avg:57.38ms
step:851/2330 train_time:48831ms step_avg:57.38ms
step:852/2330 train_time:48891ms step_avg:57.38ms
step:853/2330 train_time:48947ms step_avg:57.38ms
step:854/2330 train_time:49007ms step_avg:57.39ms
step:855/2330 train_time:49064ms step_avg:57.38ms
step:856/2330 train_time:49124ms step_avg:57.39ms
step:857/2330 train_time:49181ms step_avg:57.39ms
step:858/2330 train_time:49240ms step_avg:57.39ms
step:859/2330 train_time:49297ms step_avg:57.39ms
step:860/2330 train_time:49357ms step_avg:57.39ms
step:861/2330 train_time:49414ms step_avg:57.39ms
step:862/2330 train_time:49475ms step_avg:57.40ms
step:863/2330 train_time:49533ms step_avg:57.40ms
step:864/2330 train_time:49593ms step_avg:57.40ms
step:865/2330 train_time:49649ms step_avg:57.40ms
step:866/2330 train_time:49709ms step_avg:57.40ms
step:867/2330 train_time:49766ms step_avg:57.40ms
step:868/2330 train_time:49826ms step_avg:57.40ms
step:869/2330 train_time:49882ms step_avg:57.40ms
step:870/2330 train_time:49942ms step_avg:57.40ms
step:871/2330 train_time:49997ms step_avg:57.40ms
step:872/2330 train_time:50058ms step_avg:57.41ms
step:873/2330 train_time:50114ms step_avg:57.40ms
step:874/2330 train_time:50175ms step_avg:57.41ms
step:875/2330 train_time:50232ms step_avg:57.41ms
step:876/2330 train_time:50292ms step_avg:57.41ms
step:877/2330 train_time:50348ms step_avg:57.41ms
step:878/2330 train_time:50410ms step_avg:57.41ms
step:879/2330 train_time:50467ms step_avg:57.41ms
step:880/2330 train_time:50528ms step_avg:57.42ms
step:881/2330 train_time:50584ms step_avg:57.42ms
step:882/2330 train_time:50644ms step_avg:57.42ms
step:883/2330 train_time:50700ms step_avg:57.42ms
step:884/2330 train_time:50761ms step_avg:57.42ms
step:885/2330 train_time:50817ms step_avg:57.42ms
step:886/2330 train_time:50878ms step_avg:57.42ms
step:887/2330 train_time:50934ms step_avg:57.42ms
step:888/2330 train_time:50994ms step_avg:57.43ms
step:889/2330 train_time:51051ms step_avg:57.43ms
step:890/2330 train_time:51112ms step_avg:57.43ms
step:891/2330 train_time:51168ms step_avg:57.43ms
step:892/2330 train_time:51228ms step_avg:57.43ms
step:893/2330 train_time:51286ms step_avg:57.43ms
step:894/2330 train_time:51345ms step_avg:57.43ms
step:895/2330 train_time:51401ms step_avg:57.43ms
step:896/2330 train_time:51462ms step_avg:57.43ms
step:897/2330 train_time:51518ms step_avg:57.43ms
step:898/2330 train_time:51578ms step_avg:57.44ms
step:899/2330 train_time:51635ms step_avg:57.44ms
step:900/2330 train_time:51695ms step_avg:57.44ms
step:901/2330 train_time:51751ms step_avg:57.44ms
step:902/2330 train_time:51812ms step_avg:57.44ms
step:903/2330 train_time:51868ms step_avg:57.44ms
step:904/2330 train_time:51929ms step_avg:57.44ms
step:905/2330 train_time:51984ms step_avg:57.44ms
step:906/2330 train_time:52046ms step_avg:57.45ms
step:907/2330 train_time:52102ms step_avg:57.44ms
step:908/2330 train_time:52162ms step_avg:57.45ms
step:909/2330 train_time:52218ms step_avg:57.45ms
step:910/2330 train_time:52279ms step_avg:57.45ms
step:911/2330 train_time:52336ms step_avg:57.45ms
step:912/2330 train_time:52395ms step_avg:57.45ms
step:913/2330 train_time:52452ms step_avg:57.45ms
step:914/2330 train_time:52512ms step_avg:57.45ms
step:915/2330 train_time:52569ms step_avg:57.45ms
step:916/2330 train_time:52630ms step_avg:57.46ms
step:917/2330 train_time:52685ms step_avg:57.45ms
step:918/2330 train_time:52747ms step_avg:57.46ms
step:919/2330 train_time:52803ms step_avg:57.46ms
step:920/2330 train_time:52863ms step_avg:57.46ms
step:921/2330 train_time:52920ms step_avg:57.46ms
step:922/2330 train_time:52980ms step_avg:57.46ms
step:923/2330 train_time:53036ms step_avg:57.46ms
step:924/2330 train_time:53097ms step_avg:57.46ms
step:925/2330 train_time:53153ms step_avg:57.46ms
step:926/2330 train_time:53214ms step_avg:57.47ms
step:927/2330 train_time:53270ms step_avg:57.46ms
step:928/2330 train_time:53330ms step_avg:57.47ms
step:929/2330 train_time:53387ms step_avg:57.47ms
step:930/2330 train_time:53448ms step_avg:57.47ms
step:931/2330 train_time:53505ms step_avg:57.47ms
step:932/2330 train_time:53565ms step_avg:57.47ms
step:933/2330 train_time:53622ms step_avg:57.47ms
step:934/2330 train_time:53681ms step_avg:57.47ms
step:935/2330 train_time:53738ms step_avg:57.47ms
step:936/2330 train_time:53798ms step_avg:57.48ms
step:937/2330 train_time:53854ms step_avg:57.48ms
step:938/2330 train_time:53915ms step_avg:57.48ms
step:939/2330 train_time:53971ms step_avg:57.48ms
step:940/2330 train_time:54031ms step_avg:57.48ms
step:941/2330 train_time:54088ms step_avg:57.48ms
step:942/2330 train_time:54149ms step_avg:57.48ms
step:943/2330 train_time:54205ms step_avg:57.48ms
step:944/2330 train_time:54265ms step_avg:57.48ms
step:945/2330 train_time:54321ms step_avg:57.48ms
step:946/2330 train_time:54381ms step_avg:57.49ms
step:947/2330 train_time:54437ms step_avg:57.48ms
step:948/2330 train_time:54498ms step_avg:57.49ms
step:949/2330 train_time:54554ms step_avg:57.49ms
step:950/2330 train_time:54614ms step_avg:57.49ms
step:951/2330 train_time:54671ms step_avg:57.49ms
step:952/2330 train_time:54732ms step_avg:57.49ms
step:953/2330 train_time:54789ms step_avg:57.49ms
step:954/2330 train_time:54849ms step_avg:57.49ms
step:955/2330 train_time:54905ms step_avg:57.49ms
step:956/2330 train_time:54966ms step_avg:57.50ms
step:957/2330 train_time:55023ms step_avg:57.49ms
step:958/2330 train_time:55083ms step_avg:57.50ms
step:959/2330 train_time:55140ms step_avg:57.50ms
step:960/2330 train_time:55199ms step_avg:57.50ms
step:961/2330 train_time:55256ms step_avg:57.50ms
step:962/2330 train_time:55316ms step_avg:57.50ms
step:963/2330 train_time:55372ms step_avg:57.50ms
step:964/2330 train_time:55433ms step_avg:57.50ms
step:965/2330 train_time:55490ms step_avg:57.50ms
step:966/2330 train_time:55550ms step_avg:57.51ms
step:967/2330 train_time:55606ms step_avg:57.50ms
step:968/2330 train_time:55668ms step_avg:57.51ms
step:969/2330 train_time:55724ms step_avg:57.51ms
step:970/2330 train_time:55785ms step_avg:57.51ms
step:971/2330 train_time:55841ms step_avg:57.51ms
step:972/2330 train_time:55901ms step_avg:57.51ms
step:973/2330 train_time:55958ms step_avg:57.51ms
step:974/2330 train_time:56018ms step_avg:57.51ms
step:975/2330 train_time:56075ms step_avg:57.51ms
step:976/2330 train_time:56135ms step_avg:57.51ms
step:977/2330 train_time:56191ms step_avg:57.51ms
step:978/2330 train_time:56252ms step_avg:57.52ms
step:979/2330 train_time:56309ms step_avg:57.52ms
step:980/2330 train_time:56369ms step_avg:57.52ms
step:981/2330 train_time:56425ms step_avg:57.52ms
step:982/2330 train_time:56486ms step_avg:57.52ms
step:983/2330 train_time:56543ms step_avg:57.52ms
step:984/2330 train_time:56602ms step_avg:57.52ms
step:985/2330 train_time:56658ms step_avg:57.52ms
step:986/2330 train_time:56718ms step_avg:57.52ms
step:987/2330 train_time:56774ms step_avg:57.52ms
step:988/2330 train_time:56835ms step_avg:57.53ms
step:989/2330 train_time:56892ms step_avg:57.52ms
step:990/2330 train_time:56953ms step_avg:57.53ms
step:991/2330 train_time:57009ms step_avg:57.53ms
step:992/2330 train_time:57069ms step_avg:57.53ms
step:993/2330 train_time:57125ms step_avg:57.53ms
step:994/2330 train_time:57186ms step_avg:57.53ms
step:995/2330 train_time:57242ms step_avg:57.53ms
step:996/2330 train_time:57303ms step_avg:57.53ms
step:997/2330 train_time:57359ms step_avg:57.53ms
step:998/2330 train_time:57419ms step_avg:57.53ms
step:999/2330 train_time:57476ms step_avg:57.53ms
step:1000/2330 train_time:57536ms step_avg:57.54ms
step:1000/2330 val_loss:5.0462 train_time:57617ms step_avg:57.62ms
step:1001/2330 train_time:57637ms step_avg:57.58ms
step:1002/2330 train_time:57657ms step_avg:57.54ms
step:1003/2330 train_time:57713ms step_avg:57.54ms
step:1004/2330 train_time:57778ms step_avg:57.55ms
step:1005/2330 train_time:57834ms step_avg:57.55ms
step:1006/2330 train_time:57897ms step_avg:57.55ms
step:1007/2330 train_time:57953ms step_avg:57.55ms
step:1008/2330 train_time:58013ms step_avg:57.55ms
step:1009/2330 train_time:58069ms step_avg:57.55ms
step:1010/2330 train_time:58129ms step_avg:57.55ms
step:1011/2330 train_time:58185ms step_avg:57.55ms
step:1012/2330 train_time:58244ms step_avg:57.55ms
step:1013/2330 train_time:58300ms step_avg:57.55ms
step:1014/2330 train_time:58359ms step_avg:57.55ms
step:1015/2330 train_time:58415ms step_avg:57.55ms
step:1016/2330 train_time:58474ms step_avg:57.55ms
step:1017/2330 train_time:58531ms step_avg:57.55ms
step:1018/2330 train_time:58591ms step_avg:57.56ms
step:1019/2330 train_time:58649ms step_avg:57.56ms
step:1020/2330 train_time:58709ms step_avg:57.56ms
step:1021/2330 train_time:58767ms step_avg:57.56ms
step:1022/2330 train_time:58829ms step_avg:57.56ms
step:1023/2330 train_time:58887ms step_avg:57.56ms
step:1024/2330 train_time:58947ms step_avg:57.57ms
step:1025/2330 train_time:59004ms step_avg:57.56ms
step:1026/2330 train_time:59064ms step_avg:57.57ms
step:1027/2330 train_time:59120ms step_avg:57.57ms
step:1028/2330 train_time:59179ms step_avg:57.57ms
step:1029/2330 train_time:59236ms step_avg:57.57ms
step:1030/2330 train_time:59295ms step_avg:57.57ms
step:1031/2330 train_time:59351ms step_avg:57.57ms
step:1032/2330 train_time:59411ms step_avg:57.57ms
step:1033/2330 train_time:59467ms step_avg:57.57ms
step:1034/2330 train_time:59526ms step_avg:57.57ms
step:1035/2330 train_time:59583ms step_avg:57.57ms
step:1036/2330 train_time:59645ms step_avg:57.57ms
step:1037/2330 train_time:59703ms step_avg:57.57ms
step:1038/2330 train_time:59763ms step_avg:57.58ms
step:1039/2330 train_time:59822ms step_avg:57.58ms
step:1040/2330 train_time:59882ms step_avg:57.58ms
step:1041/2330 train_time:59940ms step_avg:57.58ms
step:1042/2330 train_time:59999ms step_avg:57.58ms
step:1043/2330 train_time:60055ms step_avg:57.58ms
step:1044/2330 train_time:60116ms step_avg:57.58ms
step:1045/2330 train_time:60173ms step_avg:57.58ms
step:1046/2330 train_time:60232ms step_avg:57.58ms
step:1047/2330 train_time:60288ms step_avg:57.58ms
step:1048/2330 train_time:60347ms step_avg:57.58ms
step:1049/2330 train_time:60403ms step_avg:57.58ms
step:1050/2330 train_time:60463ms step_avg:57.58ms
step:1051/2330 train_time:60519ms step_avg:57.58ms
step:1052/2330 train_time:60579ms step_avg:57.58ms
step:1053/2330 train_time:60637ms step_avg:57.58ms
step:1054/2330 train_time:60696ms step_avg:57.59ms
step:1055/2330 train_time:60753ms step_avg:57.59ms
step:1056/2330 train_time:60813ms step_avg:57.59ms
step:1057/2330 train_time:60870ms step_avg:57.59ms
step:1058/2330 train_time:60930ms step_avg:57.59ms
step:1059/2330 train_time:60987ms step_avg:57.59ms
step:1060/2330 train_time:61047ms step_avg:57.59ms
step:1061/2330 train_time:61104ms step_avg:57.59ms
step:1062/2330 train_time:61164ms step_avg:57.59ms
step:1063/2330 train_time:61221ms step_avg:57.59ms
step:1064/2330 train_time:61281ms step_avg:57.60ms
step:1065/2330 train_time:61337ms step_avg:57.59ms
step:1066/2330 train_time:61397ms step_avg:57.60ms
step:1067/2330 train_time:61453ms step_avg:57.59ms
step:1068/2330 train_time:61514ms step_avg:57.60ms
step:1069/2330 train_time:61570ms step_avg:57.60ms
step:1070/2330 train_time:61630ms step_avg:57.60ms
step:1071/2330 train_time:61687ms step_avg:57.60ms
step:1072/2330 train_time:61746ms step_avg:57.60ms
step:1073/2330 train_time:61803ms step_avg:57.60ms
step:1074/2330 train_time:61865ms step_avg:57.60ms
step:1075/2330 train_time:61922ms step_avg:57.60ms
step:1076/2330 train_time:61983ms step_avg:57.60ms
step:1077/2330 train_time:62039ms step_avg:57.60ms
step:1078/2330 train_time:62099ms step_avg:57.61ms
step:1079/2330 train_time:62155ms step_avg:57.60ms
step:1080/2330 train_time:62215ms step_avg:57.61ms
step:1081/2330 train_time:62271ms step_avg:57.61ms
step:1082/2330 train_time:62331ms step_avg:57.61ms
step:1083/2330 train_time:62388ms step_avg:57.61ms
step:1084/2330 train_time:62447ms step_avg:57.61ms
step:1085/2330 train_time:62504ms step_avg:57.61ms
step:1086/2330 train_time:62564ms step_avg:57.61ms
step:1087/2330 train_time:62621ms step_avg:57.61ms
step:1088/2330 train_time:62681ms step_avg:57.61ms
step:1089/2330 train_time:62738ms step_avg:57.61ms
step:1090/2330 train_time:62798ms step_avg:57.61ms
step:1091/2330 train_time:62854ms step_avg:57.61ms
step:1092/2330 train_time:62915ms step_avg:57.61ms
step:1093/2330 train_time:62971ms step_avg:57.61ms
step:1094/2330 train_time:63032ms step_avg:57.62ms
step:1095/2330 train_time:63088ms step_avg:57.61ms
step:1096/2330 train_time:63148ms step_avg:57.62ms
step:1097/2330 train_time:63205ms step_avg:57.62ms
step:1098/2330 train_time:63265ms step_avg:57.62ms
step:1099/2330 train_time:63322ms step_avg:57.62ms
step:1100/2330 train_time:63382ms step_avg:57.62ms
step:1101/2330 train_time:63439ms step_avg:57.62ms
step:1102/2330 train_time:63498ms step_avg:57.62ms
step:1103/2330 train_time:63554ms step_avg:57.62ms
step:1104/2330 train_time:63614ms step_avg:57.62ms
step:1105/2330 train_time:63671ms step_avg:57.62ms
step:1106/2330 train_time:63730ms step_avg:57.62ms
step:1107/2330 train_time:63787ms step_avg:57.62ms
step:1108/2330 train_time:63847ms step_avg:57.62ms
step:1109/2330 train_time:63904ms step_avg:57.62ms
step:1110/2330 train_time:63965ms step_avg:57.63ms
step:1111/2330 train_time:64023ms step_avg:57.63ms
step:1112/2330 train_time:64082ms step_avg:57.63ms
step:1113/2330 train_time:64139ms step_avg:57.63ms
step:1114/2330 train_time:64198ms step_avg:57.63ms
step:1115/2330 train_time:64255ms step_avg:57.63ms
step:1116/2330 train_time:64315ms step_avg:57.63ms
step:1117/2330 train_time:64371ms step_avg:57.63ms
step:1118/2330 train_time:64431ms step_avg:57.63ms
step:1119/2330 train_time:64488ms step_avg:57.63ms
step:1120/2330 train_time:64548ms step_avg:57.63ms
step:1121/2330 train_time:64605ms step_avg:57.63ms
step:1122/2330 train_time:64666ms step_avg:57.63ms
step:1123/2330 train_time:64723ms step_avg:57.63ms
step:1124/2330 train_time:64783ms step_avg:57.64ms
step:1125/2330 train_time:64840ms step_avg:57.64ms
step:1126/2330 train_time:64900ms step_avg:57.64ms
step:1127/2330 train_time:64956ms step_avg:57.64ms
step:1128/2330 train_time:65017ms step_avg:57.64ms
step:1129/2330 train_time:65074ms step_avg:57.64ms
step:1130/2330 train_time:65133ms step_avg:57.64ms
step:1131/2330 train_time:65189ms step_avg:57.64ms
step:1132/2330 train_time:65249ms step_avg:57.64ms
step:1133/2330 train_time:65306ms step_avg:57.64ms
step:1134/2330 train_time:65367ms step_avg:57.64ms
step:1135/2330 train_time:65423ms step_avg:57.64ms
step:1136/2330 train_time:65483ms step_avg:57.64ms
step:1137/2330 train_time:65539ms step_avg:57.64ms
step:1138/2330 train_time:65599ms step_avg:57.64ms
step:1139/2330 train_time:65656ms step_avg:57.64ms
step:1140/2330 train_time:65715ms step_avg:57.64ms
step:1141/2330 train_time:65772ms step_avg:57.64ms
step:1142/2330 train_time:65831ms step_avg:57.65ms
step:1143/2330 train_time:65888ms step_avg:57.64ms
step:1144/2330 train_time:65948ms step_avg:57.65ms
step:1145/2330 train_time:66005ms step_avg:57.65ms
step:1146/2330 train_time:66064ms step_avg:57.65ms
step:1147/2330 train_time:66121ms step_avg:57.65ms
step:1148/2330 train_time:66181ms step_avg:57.65ms
step:1149/2330 train_time:66238ms step_avg:57.65ms
step:1150/2330 train_time:66298ms step_avg:57.65ms
step:1151/2330 train_time:66354ms step_avg:57.65ms
step:1152/2330 train_time:66415ms step_avg:57.65ms
step:1153/2330 train_time:66471ms step_avg:57.65ms
step:1154/2330 train_time:66532ms step_avg:57.65ms
step:1155/2330 train_time:66588ms step_avg:57.65ms
step:1156/2330 train_time:66648ms step_avg:57.65ms
step:1157/2330 train_time:66704ms step_avg:57.65ms
step:1158/2330 train_time:66764ms step_avg:57.65ms
step:1159/2330 train_time:66820ms step_avg:57.65ms
step:1160/2330 train_time:66880ms step_avg:57.66ms
step:1161/2330 train_time:66937ms step_avg:57.65ms
step:1162/2330 train_time:66996ms step_avg:57.66ms
step:1163/2330 train_time:67052ms step_avg:57.65ms
step:1164/2330 train_time:67113ms step_avg:57.66ms
step:1165/2330 train_time:67169ms step_avg:57.66ms
step:1166/2330 train_time:67230ms step_avg:57.66ms
step:1167/2330 train_time:67286ms step_avg:57.66ms
step:1168/2330 train_time:67347ms step_avg:57.66ms
step:1169/2330 train_time:67403ms step_avg:57.66ms
step:1170/2330 train_time:67464ms step_avg:57.66ms
step:1171/2330 train_time:67522ms step_avg:57.66ms
step:1172/2330 train_time:67581ms step_avg:57.66ms
step:1173/2330 train_time:67637ms step_avg:57.66ms
step:1174/2330 train_time:67697ms step_avg:57.66ms
step:1175/2330 train_time:67753ms step_avg:57.66ms
step:1176/2330 train_time:67813ms step_avg:57.66ms
step:1177/2330 train_time:67869ms step_avg:57.66ms
step:1178/2330 train_time:67929ms step_avg:57.67ms
step:1179/2330 train_time:67987ms step_avg:57.66ms
step:1180/2330 train_time:68046ms step_avg:57.67ms
step:1181/2330 train_time:68103ms step_avg:57.67ms
step:1182/2330 train_time:68164ms step_avg:57.67ms
step:1183/2330 train_time:68220ms step_avg:57.67ms
step:1184/2330 train_time:68281ms step_avg:57.67ms
step:1185/2330 train_time:68338ms step_avg:57.67ms
step:1186/2330 train_time:68398ms step_avg:57.67ms
step:1187/2330 train_time:68453ms step_avg:57.67ms
step:1188/2330 train_time:68513ms step_avg:57.67ms
step:1189/2330 train_time:68569ms step_avg:57.67ms
step:1190/2330 train_time:68629ms step_avg:57.67ms
step:1191/2330 train_time:68686ms step_avg:57.67ms
step:1192/2330 train_time:68745ms step_avg:57.67ms
step:1193/2330 train_time:68801ms step_avg:57.67ms
step:1194/2330 train_time:68862ms step_avg:57.67ms
step:1195/2330 train_time:68919ms step_avg:57.67ms
step:1196/2330 train_time:68978ms step_avg:57.67ms
step:1197/2330 train_time:69035ms step_avg:57.67ms
step:1198/2330 train_time:69094ms step_avg:57.67ms
step:1199/2330 train_time:69151ms step_avg:57.67ms
step:1200/2330 train_time:69210ms step_avg:57.68ms
step:1201/2330 train_time:69266ms step_avg:57.67ms
step:1202/2330 train_time:69328ms step_avg:57.68ms
step:1203/2330 train_time:69384ms step_avg:57.68ms
step:1204/2330 train_time:69444ms step_avg:57.68ms
step:1205/2330 train_time:69501ms step_avg:57.68ms
step:1206/2330 train_time:69561ms step_avg:57.68ms
step:1207/2330 train_time:69617ms step_avg:57.68ms
step:1208/2330 train_time:69677ms step_avg:57.68ms
step:1209/2330 train_time:69734ms step_avg:57.68ms
step:1210/2330 train_time:69793ms step_avg:57.68ms
step:1211/2330 train_time:69849ms step_avg:57.68ms
step:1212/2330 train_time:69910ms step_avg:57.68ms
step:1213/2330 train_time:69966ms step_avg:57.68ms
step:1214/2330 train_time:70026ms step_avg:57.68ms
step:1215/2330 train_time:70083ms step_avg:57.68ms
step:1216/2330 train_time:70143ms step_avg:57.68ms
step:1217/2330 train_time:70199ms step_avg:57.68ms
step:1218/2330 train_time:70260ms step_avg:57.68ms
step:1219/2330 train_time:70316ms step_avg:57.68ms
step:1220/2330 train_time:70376ms step_avg:57.68ms
step:1221/2330 train_time:70431ms step_avg:57.68ms
step:1222/2330 train_time:70492ms step_avg:57.69ms
step:1223/2330 train_time:70548ms step_avg:57.68ms
step:1224/2330 train_time:70608ms step_avg:57.69ms
step:1225/2330 train_time:70665ms step_avg:57.69ms
step:1226/2330 train_time:70725ms step_avg:57.69ms
step:1227/2330 train_time:70782ms step_avg:57.69ms
step:1228/2330 train_time:70842ms step_avg:57.69ms
step:1229/2330 train_time:70899ms step_avg:57.69ms
step:1230/2330 train_time:70957ms step_avg:57.69ms
step:1231/2330 train_time:71015ms step_avg:57.69ms
step:1232/2330 train_time:71074ms step_avg:57.69ms
step:1233/2330 train_time:71131ms step_avg:57.69ms
step:1234/2330 train_time:71190ms step_avg:57.69ms
step:1235/2330 train_time:71247ms step_avg:57.69ms
step:1236/2330 train_time:71307ms step_avg:57.69ms
step:1237/2330 train_time:71363ms step_avg:57.69ms
step:1238/2330 train_time:71423ms step_avg:57.69ms
step:1239/2330 train_time:71480ms step_avg:57.69ms
step:1240/2330 train_time:71540ms step_avg:57.69ms
step:1241/2330 train_time:71597ms step_avg:57.69ms
step:1242/2330 train_time:71656ms step_avg:57.69ms
step:1243/2330 train_time:71713ms step_avg:57.69ms
step:1244/2330 train_time:71773ms step_avg:57.70ms
step:1245/2330 train_time:71830ms step_avg:57.69ms
step:1246/2330 train_time:71890ms step_avg:57.70ms
step:1247/2330 train_time:71946ms step_avg:57.70ms
step:1248/2330 train_time:72006ms step_avg:57.70ms
step:1249/2330 train_time:72062ms step_avg:57.70ms
step:1250/2330 train_time:72123ms step_avg:57.70ms
step:1250/2330 val_loss:4.7548 train_time:72203ms step_avg:57.76ms
step:1251/2330 train_time:72222ms step_avg:57.73ms
step:1252/2330 train_time:72243ms step_avg:57.70ms
step:1253/2330 train_time:72303ms step_avg:57.70ms
step:1254/2330 train_time:72365ms step_avg:57.71ms
step:1255/2330 train_time:72422ms step_avg:57.71ms
step:1256/2330 train_time:72482ms step_avg:57.71ms
step:1257/2330 train_time:72538ms step_avg:57.71ms
step:1258/2330 train_time:72598ms step_avg:57.71ms
step:1259/2330 train_time:72654ms step_avg:57.71ms
step:1260/2330 train_time:72716ms step_avg:57.71ms
step:1261/2330 train_time:72772ms step_avg:57.71ms
step:1262/2330 train_time:72831ms step_avg:57.71ms
step:1263/2330 train_time:72887ms step_avg:57.71ms
step:1264/2330 train_time:72946ms step_avg:57.71ms
step:1265/2330 train_time:73002ms step_avg:57.71ms
step:1266/2330 train_time:73061ms step_avg:57.71ms
step:1267/2330 train_time:73117ms step_avg:57.71ms
step:1268/2330 train_time:73176ms step_avg:57.71ms
step:1269/2330 train_time:73233ms step_avg:57.71ms
step:1270/2330 train_time:73296ms step_avg:57.71ms
step:1271/2330 train_time:73353ms step_avg:57.71ms
step:1272/2330 train_time:73415ms step_avg:57.72ms
step:1273/2330 train_time:73472ms step_avg:57.72ms
step:1274/2330 train_time:73532ms step_avg:57.72ms
step:1275/2330 train_time:73588ms step_avg:57.72ms
step:1276/2330 train_time:73649ms step_avg:57.72ms
step:1277/2330 train_time:73704ms step_avg:57.72ms
step:1278/2330 train_time:73764ms step_avg:57.72ms
step:1279/2330 train_time:73820ms step_avg:57.72ms
step:1280/2330 train_time:73879ms step_avg:57.72ms
step:1281/2330 train_time:73935ms step_avg:57.72ms
step:1282/2330 train_time:73996ms step_avg:57.72ms
step:1283/2330 train_time:74053ms step_avg:57.72ms
step:1284/2330 train_time:74112ms step_avg:57.72ms
step:1285/2330 train_time:74169ms step_avg:57.72ms
step:1286/2330 train_time:74229ms step_avg:57.72ms
step:1287/2330 train_time:74286ms step_avg:57.72ms
step:1288/2330 train_time:74347ms step_avg:57.72ms
step:1289/2330 train_time:74404ms step_avg:57.72ms
step:1290/2330 train_time:74464ms step_avg:57.72ms
step:1291/2330 train_time:74521ms step_avg:57.72ms
step:1292/2330 train_time:74581ms step_avg:57.73ms
step:1293/2330 train_time:74638ms step_avg:57.72ms
step:1294/2330 train_time:74697ms step_avg:57.73ms
step:1295/2330 train_time:74755ms step_avg:57.73ms
step:1296/2330 train_time:74814ms step_avg:57.73ms
step:1297/2330 train_time:74871ms step_avg:57.73ms
step:1298/2330 train_time:74930ms step_avg:57.73ms
step:1299/2330 train_time:74986ms step_avg:57.73ms
step:1300/2330 train_time:75045ms step_avg:57.73ms
step:1301/2330 train_time:75102ms step_avg:57.73ms
step:1302/2330 train_time:75161ms step_avg:57.73ms
step:1303/2330 train_time:75218ms step_avg:57.73ms
step:1304/2330 train_time:75280ms step_avg:57.73ms
step:1305/2330 train_time:75336ms step_avg:57.73ms
step:1306/2330 train_time:75397ms step_avg:57.73ms
step:1307/2330 train_time:75454ms step_avg:57.73ms
step:1308/2330 train_time:75515ms step_avg:57.73ms
step:1309/2330 train_time:75572ms step_avg:57.73ms
step:1310/2330 train_time:75632ms step_avg:57.73ms
step:1311/2330 train_time:75689ms step_avg:57.73ms
step:1312/2330 train_time:75749ms step_avg:57.74ms
step:1313/2330 train_time:75805ms step_avg:57.73ms
step:1314/2330 train_time:75864ms step_avg:57.74ms
step:1315/2330 train_time:75921ms step_avg:57.73ms
step:1316/2330 train_time:75979ms step_avg:57.74ms
step:1317/2330 train_time:76036ms step_avg:57.73ms
step:1318/2330 train_time:76096ms step_avg:57.74ms
step:1319/2330 train_time:76152ms step_avg:57.73ms
step:1320/2330 train_time:76213ms step_avg:57.74ms
step:1321/2330 train_time:76270ms step_avg:57.74ms
step:1322/2330 train_time:76331ms step_avg:57.74ms
step:1323/2330 train_time:76387ms step_avg:57.74ms
step:1324/2330 train_time:76447ms step_avg:57.74ms
step:1325/2330 train_time:76503ms step_avg:57.74ms
step:1326/2330 train_time:76565ms step_avg:57.74ms
step:1327/2330 train_time:76621ms step_avg:57.74ms
step:1328/2330 train_time:76682ms step_avg:57.74ms
step:1329/2330 train_time:76738ms step_avg:57.74ms
step:1330/2330 train_time:76798ms step_avg:57.74ms
step:1331/2330 train_time:76854ms step_avg:57.74ms
step:1332/2330 train_time:76914ms step_avg:57.74ms
step:1333/2330 train_time:76971ms step_avg:57.74ms
step:1334/2330 train_time:77030ms step_avg:57.74ms
step:1335/2330 train_time:77087ms step_avg:57.74ms
step:1336/2330 train_time:77146ms step_avg:57.74ms
step:1337/2330 train_time:77203ms step_avg:57.74ms
step:1338/2330 train_time:77263ms step_avg:57.75ms
step:1339/2330 train_time:77320ms step_avg:57.74ms
step:1340/2330 train_time:77380ms step_avg:57.75ms
step:1341/2330 train_time:77437ms step_avg:57.75ms
step:1342/2330 train_time:77497ms step_avg:57.75ms
step:1343/2330 train_time:77554ms step_avg:57.75ms
step:1344/2330 train_time:77613ms step_avg:57.75ms
step:1345/2330 train_time:77670ms step_avg:57.75ms
step:1346/2330 train_time:77729ms step_avg:57.75ms
step:1347/2330 train_time:77785ms step_avg:57.75ms
step:1348/2330 train_time:77846ms step_avg:57.75ms
step:1349/2330 train_time:77902ms step_avg:57.75ms
step:1350/2330 train_time:77962ms step_avg:57.75ms
step:1351/2330 train_time:78018ms step_avg:57.75ms
step:1352/2330 train_time:78078ms step_avg:57.75ms
step:1353/2330 train_time:78134ms step_avg:57.75ms
step:1354/2330 train_time:78194ms step_avg:57.75ms
step:1355/2330 train_time:78251ms step_avg:57.75ms
step:1356/2330 train_time:78312ms step_avg:57.75ms
step:1357/2330 train_time:78369ms step_avg:57.75ms
step:1358/2330 train_time:78428ms step_avg:57.75ms
step:1359/2330 train_time:78484ms step_avg:57.75ms
step:1360/2330 train_time:78545ms step_avg:57.75ms
step:1361/2330 train_time:78602ms step_avg:57.75ms
step:1362/2330 train_time:78661ms step_avg:57.75ms
step:1363/2330 train_time:78717ms step_avg:57.75ms
step:1364/2330 train_time:78778ms step_avg:57.75ms
step:1365/2330 train_time:78835ms step_avg:57.75ms
step:1366/2330 train_time:78894ms step_avg:57.76ms
step:1367/2330 train_time:78951ms step_avg:57.75ms
step:1368/2330 train_time:79011ms step_avg:57.76ms
step:1369/2330 train_time:79067ms step_avg:57.76ms
step:1370/2330 train_time:79127ms step_avg:57.76ms
step:1371/2330 train_time:79183ms step_avg:57.76ms
step:1372/2330 train_time:79243ms step_avg:57.76ms
step:1373/2330 train_time:79299ms step_avg:57.76ms
step:1374/2330 train_time:79359ms step_avg:57.76ms
step:1375/2330 train_time:79416ms step_avg:57.76ms
step:1376/2330 train_time:79476ms step_avg:57.76ms
step:1377/2330 train_time:79532ms step_avg:57.76ms
step:1378/2330 train_time:79593ms step_avg:57.76ms
step:1379/2330 train_time:79649ms step_avg:57.76ms
step:1380/2330 train_time:79709ms step_avg:57.76ms
step:1381/2330 train_time:79766ms step_avg:57.76ms
step:1382/2330 train_time:79825ms step_avg:57.76ms
step:1383/2330 train_time:79881ms step_avg:57.76ms
step:1384/2330 train_time:79941ms step_avg:57.76ms
step:1385/2330 train_time:79998ms step_avg:57.76ms
step:1386/2330 train_time:80058ms step_avg:57.76ms
step:1387/2330 train_time:80115ms step_avg:57.76ms
step:1388/2330 train_time:80175ms step_avg:57.76ms
step:1389/2330 train_time:80232ms step_avg:57.76ms
step:1390/2330 train_time:80291ms step_avg:57.76ms
step:1391/2330 train_time:80348ms step_avg:57.76ms
step:1392/2330 train_time:80407ms step_avg:57.76ms
step:1393/2330 train_time:80464ms step_avg:57.76ms
step:1394/2330 train_time:80524ms step_avg:57.76ms
step:1395/2330 train_time:80580ms step_avg:57.76ms
step:1396/2330 train_time:80640ms step_avg:57.76ms
step:1397/2330 train_time:80696ms step_avg:57.76ms
step:1398/2330 train_time:80756ms step_avg:57.77ms
step:1399/2330 train_time:80813ms step_avg:57.76ms
step:1400/2330 train_time:80873ms step_avg:57.77ms
step:1401/2330 train_time:80930ms step_avg:57.77ms
step:1402/2330 train_time:80989ms step_avg:57.77ms
step:1403/2330 train_time:81045ms step_avg:57.77ms
step:1404/2330 train_time:81107ms step_avg:57.77ms
step:1405/2330 train_time:81163ms step_avg:57.77ms
step:1406/2330 train_time:81223ms step_avg:57.77ms
step:1407/2330 train_time:81279ms step_avg:57.77ms
step:1408/2330 train_time:81339ms step_avg:57.77ms
step:1409/2330 train_time:81396ms step_avg:57.77ms
step:1410/2330 train_time:81456ms step_avg:57.77ms
step:1411/2330 train_time:81512ms step_avg:57.77ms
step:1412/2330 train_time:81574ms step_avg:57.77ms
step:1413/2330 train_time:81630ms step_avg:57.77ms
step:1414/2330 train_time:81690ms step_avg:57.77ms
step:1415/2330 train_time:81745ms step_avg:57.77ms
step:1416/2330 train_time:81807ms step_avg:57.77ms
step:1417/2330 train_time:81863ms step_avg:57.77ms
step:1418/2330 train_time:81923ms step_avg:57.77ms
step:1419/2330 train_time:81979ms step_avg:57.77ms
step:1420/2330 train_time:82039ms step_avg:57.77ms
step:1421/2330 train_time:82096ms step_avg:57.77ms
step:1422/2330 train_time:82156ms step_avg:57.78ms
step:1423/2330 train_time:82213ms step_avg:57.77ms
step:1424/2330 train_time:82273ms step_avg:57.78ms
step:1425/2330 train_time:82328ms step_avg:57.77ms
step:1426/2330 train_time:82389ms step_avg:57.78ms
step:1427/2330 train_time:82446ms step_avg:57.78ms
step:1428/2330 train_time:82506ms step_avg:57.78ms
step:1429/2330 train_time:82562ms step_avg:57.78ms
step:1430/2330 train_time:82622ms step_avg:57.78ms
step:1431/2330 train_time:82678ms step_avg:57.78ms
step:1432/2330 train_time:82739ms step_avg:57.78ms
step:1433/2330 train_time:82796ms step_avg:57.78ms
step:1434/2330 train_time:82855ms step_avg:57.78ms
step:1435/2330 train_time:82912ms step_avg:57.78ms
step:1436/2330 train_time:82972ms step_avg:57.78ms
step:1437/2330 train_time:83028ms step_avg:57.78ms
step:1438/2330 train_time:83088ms step_avg:57.78ms
step:1439/2330 train_time:83145ms step_avg:57.78ms
step:1440/2330 train_time:83204ms step_avg:57.78ms
step:1441/2330 train_time:83260ms step_avg:57.78ms
step:1442/2330 train_time:83320ms step_avg:57.78ms
step:1443/2330 train_time:83376ms step_avg:57.78ms
step:1444/2330 train_time:83437ms step_avg:57.78ms
step:1445/2330 train_time:83494ms step_avg:57.78ms
step:1446/2330 train_time:83554ms step_avg:57.78ms
step:1447/2330 train_time:83611ms step_avg:57.78ms
step:1448/2330 train_time:83671ms step_avg:57.78ms
step:1449/2330 train_time:83727ms step_avg:57.78ms
step:1450/2330 train_time:83787ms step_avg:57.78ms
step:1451/2330 train_time:83843ms step_avg:57.78ms
step:1452/2330 train_time:83903ms step_avg:57.78ms
step:1453/2330 train_time:83959ms step_avg:57.78ms
step:1454/2330 train_time:84019ms step_avg:57.78ms
step:1455/2330 train_time:84076ms step_avg:57.78ms
step:1456/2330 train_time:84137ms step_avg:57.79ms
step:1457/2330 train_time:84193ms step_avg:57.79ms
step:1458/2330 train_time:84253ms step_avg:57.79ms
step:1459/2330 train_time:84309ms step_avg:57.79ms
step:1460/2330 train_time:84370ms step_avg:57.79ms
step:1461/2330 train_time:84425ms step_avg:57.79ms
step:1462/2330 train_time:84486ms step_avg:57.79ms
step:1463/2330 train_time:84542ms step_avg:57.79ms
step:1464/2330 train_time:84602ms step_avg:57.79ms
step:1465/2330 train_time:84658ms step_avg:57.79ms
step:1466/2330 train_time:84718ms step_avg:57.79ms
step:1467/2330 train_time:84775ms step_avg:57.79ms
step:1468/2330 train_time:84835ms step_avg:57.79ms
step:1469/2330 train_time:84892ms step_avg:57.79ms
step:1470/2330 train_time:84953ms step_avg:57.79ms
step:1471/2330 train_time:85010ms step_avg:57.79ms
step:1472/2330 train_time:85069ms step_avg:57.79ms
step:1473/2330 train_time:85126ms step_avg:57.79ms
step:1474/2330 train_time:85185ms step_avg:57.79ms
step:1475/2330 train_time:85242ms step_avg:57.79ms
step:1476/2330 train_time:85301ms step_avg:57.79ms
step:1477/2330 train_time:85358ms step_avg:57.79ms
step:1478/2330 train_time:85417ms step_avg:57.79ms
step:1479/2330 train_time:85474ms step_avg:57.79ms
step:1480/2330 train_time:85535ms step_avg:57.79ms
step:1481/2330 train_time:85591ms step_avg:57.79ms
step:1482/2330 train_time:85651ms step_avg:57.79ms
step:1483/2330 train_time:85707ms step_avg:57.79ms
step:1484/2330 train_time:85767ms step_avg:57.79ms
step:1485/2330 train_time:85824ms step_avg:57.79ms
step:1486/2330 train_time:85883ms step_avg:57.79ms
step:1487/2330 train_time:85939ms step_avg:57.79ms
step:1488/2330 train_time:86000ms step_avg:57.80ms
step:1489/2330 train_time:86057ms step_avg:57.79ms
step:1490/2330 train_time:86118ms step_avg:57.80ms
step:1491/2330 train_time:86174ms step_avg:57.80ms
step:1492/2330 train_time:86234ms step_avg:57.80ms
step:1493/2330 train_time:86291ms step_avg:57.80ms
step:1494/2330 train_time:86351ms step_avg:57.80ms
step:1495/2330 train_time:86408ms step_avg:57.80ms
step:1496/2330 train_time:86467ms step_avg:57.80ms
step:1497/2330 train_time:86525ms step_avg:57.80ms
step:1498/2330 train_time:86583ms step_avg:57.80ms
step:1499/2330 train_time:86640ms step_avg:57.80ms
step:1500/2330 train_time:86700ms step_avg:57.80ms
step:1500/2330 val_loss:4.5614 train_time:86780ms step_avg:57.85ms
step:1501/2330 train_time:86798ms step_avg:57.83ms
step:1502/2330 train_time:86818ms step_avg:57.80ms
step:1503/2330 train_time:86877ms step_avg:57.80ms
step:1504/2330 train_time:86941ms step_avg:57.81ms
step:1505/2330 train_time:86998ms step_avg:57.81ms
step:1506/2330 train_time:87059ms step_avg:57.81ms
step:1507/2330 train_time:87115ms step_avg:57.81ms
step:1508/2330 train_time:87176ms step_avg:57.81ms
step:1509/2330 train_time:87232ms step_avg:57.81ms
step:1510/2330 train_time:87291ms step_avg:57.81ms
step:1511/2330 train_time:87347ms step_avg:57.81ms
step:1512/2330 train_time:87407ms step_avg:57.81ms
step:1513/2330 train_time:87463ms step_avg:57.81ms
step:1514/2330 train_time:87522ms step_avg:57.81ms
step:1515/2330 train_time:87578ms step_avg:57.81ms
step:1516/2330 train_time:87637ms step_avg:57.81ms
step:1517/2330 train_time:87693ms step_avg:57.81ms
step:1518/2330 train_time:87753ms step_avg:57.81ms
step:1519/2330 train_time:87811ms step_avg:57.81ms
step:1520/2330 train_time:87872ms step_avg:57.81ms
step:1521/2330 train_time:87929ms step_avg:57.81ms
step:1522/2330 train_time:87990ms step_avg:57.81ms
step:1523/2330 train_time:88047ms step_avg:57.81ms
step:1524/2330 train_time:88107ms step_avg:57.81ms
step:1525/2330 train_time:88164ms step_avg:57.81ms
step:1526/2330 train_time:88224ms step_avg:57.81ms
step:1527/2330 train_time:88280ms step_avg:57.81ms
step:1528/2330 train_time:88340ms step_avg:57.81ms
step:1529/2330 train_time:88398ms step_avg:57.81ms
step:1530/2330 train_time:88456ms step_avg:57.81ms
step:1531/2330 train_time:88513ms step_avg:57.81ms
step:1532/2330 train_time:88572ms step_avg:57.81ms
step:1533/2330 train_time:88629ms step_avg:57.81ms
step:1534/2330 train_time:88688ms step_avg:57.82ms
step:1535/2330 train_time:88745ms step_avg:57.81ms
step:1536/2330 train_time:88805ms step_avg:57.82ms
step:1537/2330 train_time:88863ms step_avg:57.82ms
step:1538/2330 train_time:88925ms step_avg:57.82ms
step:1539/2330 train_time:88983ms step_avg:57.82ms
step:1540/2330 train_time:89044ms step_avg:57.82ms
step:1541/2330 train_time:89101ms step_avg:57.82ms
step:1542/2330 train_time:89162ms step_avg:57.82ms
step:1543/2330 train_time:89219ms step_avg:57.82ms
step:1544/2330 train_time:89280ms step_avg:57.82ms
step:1545/2330 train_time:89337ms step_avg:57.82ms
step:1546/2330 train_time:89398ms step_avg:57.83ms
step:1547/2330 train_time:89456ms step_avg:57.83ms
step:1548/2330 train_time:89515ms step_avg:57.83ms
step:1549/2330 train_time:89572ms step_avg:57.83ms
step:1550/2330 train_time:89632ms step_avg:57.83ms
step:1551/2330 train_time:89689ms step_avg:57.83ms
step:1552/2330 train_time:89748ms step_avg:57.83ms
step:1553/2330 train_time:89805ms step_avg:57.83ms
step:1554/2330 train_time:89865ms step_avg:57.83ms
step:1555/2330 train_time:89924ms step_avg:57.83ms
step:1556/2330 train_time:89985ms step_avg:57.83ms
step:1557/2330 train_time:90043ms step_avg:57.83ms
step:1558/2330 train_time:90103ms step_avg:57.83ms
step:1559/2330 train_time:90160ms step_avg:57.83ms
step:1560/2330 train_time:90220ms step_avg:57.83ms
step:1561/2330 train_time:90278ms step_avg:57.83ms
step:1562/2330 train_time:90339ms step_avg:57.84ms
step:1563/2330 train_time:90395ms step_avg:57.83ms
step:1564/2330 train_time:90456ms step_avg:57.84ms
step:1565/2330 train_time:90512ms step_avg:57.84ms
step:1566/2330 train_time:90573ms step_avg:57.84ms
step:1567/2330 train_time:90631ms step_avg:57.84ms
step:1568/2330 train_time:90692ms step_avg:57.84ms
step:1569/2330 train_time:90748ms step_avg:57.84ms
step:1570/2330 train_time:90808ms step_avg:57.84ms
step:1571/2330 train_time:90865ms step_avg:57.84ms
step:1572/2330 train_time:90925ms step_avg:57.84ms
step:1573/2330 train_time:90983ms step_avg:57.84ms
step:1574/2330 train_time:91043ms step_avg:57.84ms
step:1575/2330 train_time:91100ms step_avg:57.84ms
step:1576/2330 train_time:91161ms step_avg:57.84ms
step:1577/2330 train_time:91218ms step_avg:57.84ms
step:1578/2330 train_time:91279ms step_avg:57.84ms
step:1579/2330 train_time:91338ms step_avg:57.85ms
step:1580/2330 train_time:91397ms step_avg:57.85ms
step:1581/2330 train_time:91455ms step_avg:57.85ms
step:1582/2330 train_time:91515ms step_avg:57.85ms
step:1583/2330 train_time:91572ms step_avg:57.85ms
step:1584/2330 train_time:91633ms step_avg:57.85ms
step:1585/2330 train_time:91691ms step_avg:57.85ms
step:1586/2330 train_time:91750ms step_avg:57.85ms
step:1587/2330 train_time:91806ms step_avg:57.85ms
step:1588/2330 train_time:91867ms step_avg:57.85ms
step:1589/2330 train_time:91923ms step_avg:57.85ms
step:1590/2330 train_time:91985ms step_avg:57.85ms
step:1591/2330 train_time:92042ms step_avg:57.85ms
step:1592/2330 train_time:92102ms step_avg:57.85ms
step:1593/2330 train_time:92160ms step_avg:57.85ms
step:1594/2330 train_time:92221ms step_avg:57.85ms
step:1595/2330 train_time:92279ms step_avg:57.86ms
step:1596/2330 train_time:92339ms step_avg:57.86ms
step:1597/2330 train_time:92397ms step_avg:57.86ms
step:1598/2330 train_time:92456ms step_avg:57.86ms
step:1599/2330 train_time:92512ms step_avg:57.86ms
step:1600/2330 train_time:92573ms step_avg:57.86ms
step:1601/2330 train_time:92631ms step_avg:57.86ms
step:1602/2330 train_time:92691ms step_avg:57.86ms
step:1603/2330 train_time:92748ms step_avg:57.86ms
step:1604/2330 train_time:92807ms step_avg:57.86ms
step:1605/2330 train_time:92864ms step_avg:57.86ms
step:1606/2330 train_time:92924ms step_avg:57.86ms
step:1607/2330 train_time:92981ms step_avg:57.86ms
step:1608/2330 train_time:93041ms step_avg:57.86ms
step:1609/2330 train_time:93098ms step_avg:57.86ms
step:1610/2330 train_time:93159ms step_avg:57.86ms
step:1611/2330 train_time:93216ms step_avg:57.86ms
step:1612/2330 train_time:93277ms step_avg:57.86ms
step:1613/2330 train_time:93334ms step_avg:57.86ms
step:1614/2330 train_time:93396ms step_avg:57.87ms
step:1615/2330 train_time:93453ms step_avg:57.87ms
step:1616/2330 train_time:93513ms step_avg:57.87ms
step:1617/2330 train_time:93570ms step_avg:57.87ms
step:1618/2330 train_time:93630ms step_avg:57.87ms
step:1619/2330 train_time:93687ms step_avg:57.87ms
step:1620/2330 train_time:93747ms step_avg:57.87ms
step:1621/2330 train_time:93803ms step_avg:57.87ms
step:1622/2330 train_time:93864ms step_avg:57.87ms
step:1623/2330 train_time:93921ms step_avg:57.87ms
step:1624/2330 train_time:93980ms step_avg:57.87ms
step:1625/2330 train_time:94038ms step_avg:57.87ms
step:1626/2330 train_time:94098ms step_avg:57.87ms
step:1627/2330 train_time:94154ms step_avg:57.87ms
step:1628/2330 train_time:94215ms step_avg:57.87ms
step:1629/2330 train_time:94272ms step_avg:57.87ms
step:1630/2330 train_time:94333ms step_avg:57.87ms
step:1631/2330 train_time:94390ms step_avg:57.87ms
step:1632/2330 train_time:94450ms step_avg:57.87ms
step:1633/2330 train_time:94507ms step_avg:57.87ms
step:1634/2330 train_time:94568ms step_avg:57.88ms
step:1635/2330 train_time:94625ms step_avg:57.87ms
step:1636/2330 train_time:94685ms step_avg:57.88ms
step:1637/2330 train_time:94742ms step_avg:57.88ms
step:1638/2330 train_time:94802ms step_avg:57.88ms
step:1639/2330 train_time:94859ms step_avg:57.88ms
step:1640/2330 train_time:94920ms step_avg:57.88ms
step:1641/2330 train_time:94977ms step_avg:57.88ms
step:1642/2330 train_time:95038ms step_avg:57.88ms
step:1643/2330 train_time:95095ms step_avg:57.88ms
step:1644/2330 train_time:95156ms step_avg:57.88ms
step:1645/2330 train_time:95213ms step_avg:57.88ms
step:1646/2330 train_time:95273ms step_avg:57.88ms
step:1647/2330 train_time:95330ms step_avg:57.88ms
step:1648/2330 train_time:95390ms step_avg:57.88ms
step:1649/2330 train_time:95447ms step_avg:57.88ms
step:1650/2330 train_time:95507ms step_avg:57.88ms
step:1651/2330 train_time:95564ms step_avg:57.88ms
step:1652/2330 train_time:95625ms step_avg:57.88ms
step:1653/2330 train_time:95683ms step_avg:57.88ms
step:1654/2330 train_time:95742ms step_avg:57.89ms
step:1655/2330 train_time:95800ms step_avg:57.88ms
step:1656/2330 train_time:95859ms step_avg:57.89ms
step:1657/2330 train_time:95916ms step_avg:57.89ms
step:1658/2330 train_time:95976ms step_avg:57.89ms
step:1659/2330 train_time:96034ms step_avg:57.89ms
step:1660/2330 train_time:96093ms step_avg:57.89ms
step:1661/2330 train_time:96150ms step_avg:57.89ms
step:1662/2330 train_time:96210ms step_avg:57.89ms
step:1663/2330 train_time:96266ms step_avg:57.89ms
step:1664/2330 train_time:96328ms step_avg:57.89ms
step:1665/2330 train_time:96385ms step_avg:57.89ms
step:1666/2330 train_time:96445ms step_avg:57.89ms
step:1667/2330 train_time:96502ms step_avg:57.89ms
step:1668/2330 train_time:96563ms step_avg:57.89ms
step:1669/2330 train_time:96620ms step_avg:57.89ms
step:1670/2330 train_time:96680ms step_avg:57.89ms
step:1671/2330 train_time:96738ms step_avg:57.89ms
step:1672/2330 train_time:96799ms step_avg:57.89ms
step:1673/2330 train_time:96857ms step_avg:57.89ms
step:1674/2330 train_time:96916ms step_avg:57.90ms
step:1675/2330 train_time:96974ms step_avg:57.89ms
step:1676/2330 train_time:97034ms step_avg:57.90ms
step:1677/2330 train_time:97091ms step_avg:57.90ms
step:1678/2330 train_time:97151ms step_avg:57.90ms
step:1679/2330 train_time:97208ms step_avg:57.90ms
step:1680/2330 train_time:97269ms step_avg:57.90ms
step:1681/2330 train_time:97326ms step_avg:57.90ms
step:1682/2330 train_time:97386ms step_avg:57.90ms
step:1683/2330 train_time:97442ms step_avg:57.90ms
step:1684/2330 train_time:97503ms step_avg:57.90ms
step:1685/2330 train_time:97560ms step_avg:57.90ms
step:1686/2330 train_time:97620ms step_avg:57.90ms
step:1687/2330 train_time:97678ms step_avg:57.90ms
step:1688/2330 train_time:97738ms step_avg:57.90ms
step:1689/2330 train_time:97795ms step_avg:57.90ms
step:1690/2330 train_time:97855ms step_avg:57.90ms
step:1691/2330 train_time:97912ms step_avg:57.90ms
step:1692/2330 train_time:97973ms step_avg:57.90ms
step:1693/2330 train_time:98031ms step_avg:57.90ms
step:1694/2330 train_time:98091ms step_avg:57.90ms
step:1695/2330 train_time:98148ms step_avg:57.90ms
step:1696/2330 train_time:98207ms step_avg:57.91ms
step:1697/2330 train_time:98264ms step_avg:57.90ms
step:1698/2330 train_time:98325ms step_avg:57.91ms
step:1699/2330 train_time:98382ms step_avg:57.91ms
step:1700/2330 train_time:98442ms step_avg:57.91ms
step:1701/2330 train_time:98499ms step_avg:57.91ms
step:1702/2330 train_time:98559ms step_avg:57.91ms
step:1703/2330 train_time:98616ms step_avg:57.91ms
step:1704/2330 train_time:98676ms step_avg:57.91ms
step:1705/2330 train_time:98734ms step_avg:57.91ms
step:1706/2330 train_time:98793ms step_avg:57.91ms
step:1707/2330 train_time:98851ms step_avg:57.91ms
step:1708/2330 train_time:98911ms step_avg:57.91ms
step:1709/2330 train_time:98968ms step_avg:57.91ms
step:1710/2330 train_time:99029ms step_avg:57.91ms
step:1711/2330 train_time:99086ms step_avg:57.91ms
step:1712/2330 train_time:99146ms step_avg:57.91ms
step:1713/2330 train_time:99203ms step_avg:57.91ms
step:1714/2330 train_time:99263ms step_avg:57.91ms
step:1715/2330 train_time:99320ms step_avg:57.91ms
step:1716/2330 train_time:99382ms step_avg:57.91ms
step:1717/2330 train_time:99438ms step_avg:57.91ms
step:1718/2330 train_time:99498ms step_avg:57.92ms
step:1719/2330 train_time:99555ms step_avg:57.91ms
step:1720/2330 train_time:99615ms step_avg:57.92ms
step:1721/2330 train_time:99673ms step_avg:57.92ms
step:1722/2330 train_time:99732ms step_avg:57.92ms
step:1723/2330 train_time:99790ms step_avg:57.92ms
step:1724/2330 train_time:99850ms step_avg:57.92ms
step:1725/2330 train_time:99907ms step_avg:57.92ms
step:1726/2330 train_time:99967ms step_avg:57.92ms
step:1727/2330 train_time:100024ms step_avg:57.92ms
step:1728/2330 train_time:100085ms step_avg:57.92ms
step:1729/2330 train_time:100142ms step_avg:57.92ms
step:1730/2330 train_time:100202ms step_avg:57.92ms
step:1731/2330 train_time:100259ms step_avg:57.92ms
step:1732/2330 train_time:100320ms step_avg:57.92ms
step:1733/2330 train_time:100376ms step_avg:57.92ms
step:1734/2330 train_time:100438ms step_avg:57.92ms
step:1735/2330 train_time:100495ms step_avg:57.92ms
step:1736/2330 train_time:100556ms step_avg:57.92ms
step:1737/2330 train_time:100614ms step_avg:57.92ms
step:1738/2330 train_time:100674ms step_avg:57.93ms
step:1739/2330 train_time:100731ms step_avg:57.92ms
step:1740/2330 train_time:100792ms step_avg:57.93ms
step:1741/2330 train_time:100848ms step_avg:57.93ms
step:1742/2330 train_time:100908ms step_avg:57.93ms
step:1743/2330 train_time:100965ms step_avg:57.93ms
step:1744/2330 train_time:101026ms step_avg:57.93ms
step:1745/2330 train_time:101083ms step_avg:57.93ms
step:1746/2330 train_time:101143ms step_avg:57.93ms
step:1747/2330 train_time:101201ms step_avg:57.93ms
step:1748/2330 train_time:101260ms step_avg:57.93ms
step:1749/2330 train_time:101318ms step_avg:57.93ms
step:1750/2330 train_time:101379ms step_avg:57.93ms
step:1750/2330 val_loss:4.4317 train_time:101459ms step_avg:57.98ms
step:1751/2330 train_time:101479ms step_avg:57.95ms
step:1752/2330 train_time:101499ms step_avg:57.93ms
step:1753/2330 train_time:101560ms step_avg:57.94ms
step:1754/2330 train_time:101624ms step_avg:57.94ms
step:1755/2330 train_time:101682ms step_avg:57.94ms
step:1756/2330 train_time:101742ms step_avg:57.94ms
step:1757/2330 train_time:101799ms step_avg:57.94ms
step:1758/2330 train_time:101858ms step_avg:57.94ms
step:1759/2330 train_time:101915ms step_avg:57.94ms
step:1760/2330 train_time:101974ms step_avg:57.94ms
step:1761/2330 train_time:102031ms step_avg:57.94ms
step:1762/2330 train_time:102091ms step_avg:57.94ms
step:1763/2330 train_time:102147ms step_avg:57.94ms
step:1764/2330 train_time:102206ms step_avg:57.94ms
step:1765/2330 train_time:102263ms step_avg:57.94ms
step:1766/2330 train_time:102321ms step_avg:57.94ms
step:1767/2330 train_time:102378ms step_avg:57.94ms
step:1768/2330 train_time:102442ms step_avg:57.94ms
step:1769/2330 train_time:102500ms step_avg:57.94ms
step:1770/2330 train_time:102562ms step_avg:57.94ms
step:1771/2330 train_time:102619ms step_avg:57.94ms
step:1772/2330 train_time:102680ms step_avg:57.95ms
step:1773/2330 train_time:102738ms step_avg:57.95ms
step:1774/2330 train_time:102798ms step_avg:57.95ms
step:1775/2330 train_time:102855ms step_avg:57.95ms
step:1776/2330 train_time:102915ms step_avg:57.95ms
step:1777/2330 train_time:102972ms step_avg:57.95ms
step:1778/2330 train_time:103032ms step_avg:57.95ms
step:1779/2330 train_time:103089ms step_avg:57.95ms
step:1780/2330 train_time:103149ms step_avg:57.95ms
step:1781/2330 train_time:103205ms step_avg:57.95ms
step:1782/2330 train_time:103265ms step_avg:57.95ms
step:1783/2330 train_time:103321ms step_avg:57.95ms
step:1784/2330 train_time:103383ms step_avg:57.95ms
step:1785/2330 train_time:103440ms step_avg:57.95ms
step:1786/2330 train_time:103501ms step_avg:57.95ms
step:1787/2330 train_time:103558ms step_avg:57.95ms
step:1788/2330 train_time:103620ms step_avg:57.95ms
step:1789/2330 train_time:103678ms step_avg:57.95ms
step:1790/2330 train_time:103738ms step_avg:57.95ms
step:1791/2330 train_time:103795ms step_avg:57.95ms
step:1792/2330 train_time:103855ms step_avg:57.95ms
step:1793/2330 train_time:103912ms step_avg:57.95ms
step:1794/2330 train_time:103972ms step_avg:57.96ms
step:1795/2330 train_time:104029ms step_avg:57.95ms
step:1796/2330 train_time:104089ms step_avg:57.96ms
step:1797/2330 train_time:104145ms step_avg:57.95ms
step:1798/2330 train_time:104204ms step_avg:57.96ms
step:1799/2330 train_time:104261ms step_avg:57.95ms
step:1800/2330 train_time:104321ms step_avg:57.96ms
step:1801/2330 train_time:104379ms step_avg:57.96ms
step:1802/2330 train_time:104439ms step_avg:57.96ms
step:1803/2330 train_time:104496ms step_avg:57.96ms
step:1804/2330 train_time:104556ms step_avg:57.96ms
step:1805/2330 train_time:104613ms step_avg:57.96ms
step:1806/2330 train_time:104675ms step_avg:57.96ms
step:1807/2330 train_time:104732ms step_avg:57.96ms
step:1808/2330 train_time:104792ms step_avg:57.96ms
step:1809/2330 train_time:104849ms step_avg:57.96ms
step:1810/2330 train_time:104909ms step_avg:57.96ms
step:1811/2330 train_time:104967ms step_avg:57.96ms
step:1812/2330 train_time:105027ms step_avg:57.96ms
step:1813/2330 train_time:105084ms step_avg:57.96ms
step:1814/2330 train_time:105144ms step_avg:57.96ms
step:1815/2330 train_time:105200ms step_avg:57.96ms
step:1816/2330 train_time:105260ms step_avg:57.96ms
step:1817/2330 train_time:105317ms step_avg:57.96ms
step:1818/2330 train_time:105377ms step_avg:57.96ms
step:1819/2330 train_time:105435ms step_avg:57.96ms
step:1820/2330 train_time:105494ms step_avg:57.96ms
step:1821/2330 train_time:105551ms step_avg:57.96ms
step:1822/2330 train_time:105613ms step_avg:57.97ms
step:1823/2330 train_time:105670ms step_avg:57.96ms
step:1824/2330 train_time:105731ms step_avg:57.97ms
step:1825/2330 train_time:105788ms step_avg:57.97ms
step:1826/2330 train_time:105847ms step_avg:57.97ms
step:1827/2330 train_time:105904ms step_avg:57.97ms
step:1828/2330 train_time:105965ms step_avg:57.97ms
step:1829/2330 train_time:106022ms step_avg:57.97ms
step:1830/2330 train_time:106082ms step_avg:57.97ms
step:1831/2330 train_time:106138ms step_avg:57.97ms
step:1832/2330 train_time:106199ms step_avg:57.97ms
step:1833/2330 train_time:106256ms step_avg:57.97ms
step:1834/2330 train_time:106316ms step_avg:57.97ms
step:1835/2330 train_time:106373ms step_avg:57.97ms
step:1836/2330 train_time:106434ms step_avg:57.97ms
step:1837/2330 train_time:106492ms step_avg:57.97ms
step:1838/2330 train_time:106552ms step_avg:57.97ms
step:1839/2330 train_time:106609ms step_avg:57.97ms
step:1840/2330 train_time:106670ms step_avg:57.97ms
step:1841/2330 train_time:106728ms step_avg:57.97ms
step:1842/2330 train_time:106788ms step_avg:57.97ms
step:1843/2330 train_time:106845ms step_avg:57.97ms
step:1844/2330 train_time:106906ms step_avg:57.97ms
step:1845/2330 train_time:106963ms step_avg:57.97ms
step:1846/2330 train_time:107023ms step_avg:57.98ms
step:1847/2330 train_time:107079ms step_avg:57.97ms
step:1848/2330 train_time:107139ms step_avg:57.98ms
step:1849/2330 train_time:107195ms step_avg:57.97ms
step:1850/2330 train_time:107256ms step_avg:57.98ms
step:1851/2330 train_time:107313ms step_avg:57.98ms
step:1852/2330 train_time:107374ms step_avg:57.98ms
step:1853/2330 train_time:107431ms step_avg:57.98ms
step:1854/2330 train_time:107492ms step_avg:57.98ms
step:1855/2330 train_time:107549ms step_avg:57.98ms
step:1856/2330 train_time:107609ms step_avg:57.98ms
step:1857/2330 train_time:107666ms step_avg:57.98ms
step:1858/2330 train_time:107727ms step_avg:57.98ms
step:1859/2330 train_time:107784ms step_avg:57.98ms
step:1860/2330 train_time:107845ms step_avg:57.98ms
step:1861/2330 train_time:107902ms step_avg:57.98ms
step:1862/2330 train_time:107963ms step_avg:57.98ms
step:1863/2330 train_time:108020ms step_avg:57.98ms
step:1864/2330 train_time:108080ms step_avg:57.98ms
step:1865/2330 train_time:108136ms step_avg:57.98ms
step:1866/2330 train_time:108196ms step_avg:57.98ms
step:1867/2330 train_time:108253ms step_avg:57.98ms
step:1868/2330 train_time:108314ms step_avg:57.98ms
step:1869/2330 train_time:108371ms step_avg:57.98ms
step:1870/2330 train_time:108430ms step_avg:57.98ms
step:1871/2330 train_time:108488ms step_avg:57.98ms
step:1872/2330 train_time:108549ms step_avg:57.99ms
step:1873/2330 train_time:108606ms step_avg:57.99ms
step:1874/2330 train_time:108667ms step_avg:57.99ms
step:1875/2330 train_time:108724ms step_avg:57.99ms
step:1876/2330 train_time:108784ms step_avg:57.99ms
step:1877/2330 train_time:108841ms step_avg:57.99ms
step:1878/2330 train_time:108901ms step_avg:57.99ms
step:1879/2330 train_time:108958ms step_avg:57.99ms
step:1880/2330 train_time:109017ms step_avg:57.99ms
step:1881/2330 train_time:109075ms step_avg:57.99ms
step:1882/2330 train_time:109135ms step_avg:57.99ms
step:1883/2330 train_time:109191ms step_avg:57.99ms
step:1884/2330 train_time:109252ms step_avg:57.99ms
step:1885/2330 train_time:109308ms step_avg:57.99ms
step:1886/2330 train_time:109369ms step_avg:57.99ms
step:1887/2330 train_time:109425ms step_avg:57.99ms
step:1888/2330 train_time:109486ms step_avg:57.99ms
step:1889/2330 train_time:109542ms step_avg:57.99ms
step:1890/2330 train_time:109603ms step_avg:57.99ms
step:1891/2330 train_time:109660ms step_avg:57.99ms
step:1892/2330 train_time:109721ms step_avg:57.99ms
step:1893/2330 train_time:109778ms step_avg:57.99ms
step:1894/2330 train_time:109839ms step_avg:57.99ms
step:1895/2330 train_time:109896ms step_avg:57.99ms
step:1896/2330 train_time:109955ms step_avg:57.99ms
step:1897/2330 train_time:110012ms step_avg:57.99ms
step:1898/2330 train_time:110073ms step_avg:57.99ms
step:1899/2330 train_time:110130ms step_avg:57.99ms
step:1900/2330 train_time:110190ms step_avg:57.99ms
step:1901/2330 train_time:110247ms step_avg:57.99ms
step:1902/2330 train_time:110307ms step_avg:58.00ms
step:1903/2330 train_time:110362ms step_avg:57.99ms
step:1904/2330 train_time:110424ms step_avg:58.00ms
step:1905/2330 train_time:110480ms step_avg:57.99ms
step:1906/2330 train_time:110541ms step_avg:58.00ms
step:1907/2330 train_time:110598ms step_avg:58.00ms
step:1908/2330 train_time:110659ms step_avg:58.00ms
step:1909/2330 train_time:110715ms step_avg:58.00ms
step:1910/2330 train_time:110776ms step_avg:58.00ms
step:1911/2330 train_time:110834ms step_avg:58.00ms
step:1912/2330 train_time:110894ms step_avg:58.00ms
step:1913/2330 train_time:110951ms step_avg:58.00ms
step:1914/2330 train_time:111011ms step_avg:58.00ms
step:1915/2330 train_time:111067ms step_avg:58.00ms
step:1916/2330 train_time:111128ms step_avg:58.00ms
step:1917/2330 train_time:111186ms step_avg:58.00ms
step:1918/2330 train_time:111246ms step_avg:58.00ms
step:1919/2330 train_time:111303ms step_avg:58.00ms
step:1920/2330 train_time:111363ms step_avg:58.00ms
step:1921/2330 train_time:111420ms step_avg:58.00ms
step:1922/2330 train_time:111481ms step_avg:58.00ms
step:1923/2330 train_time:111537ms step_avg:58.00ms
step:1924/2330 train_time:111598ms step_avg:58.00ms
step:1925/2330 train_time:111655ms step_avg:58.00ms
step:1926/2330 train_time:111716ms step_avg:58.00ms
step:1927/2330 train_time:111773ms step_avg:58.00ms
step:1928/2330 train_time:111834ms step_avg:58.01ms
step:1929/2330 train_time:111891ms step_avg:58.00ms
step:1930/2330 train_time:111951ms step_avg:58.01ms
step:1931/2330 train_time:112008ms step_avg:58.01ms
step:1932/2330 train_time:112069ms step_avg:58.01ms
step:1933/2330 train_time:112126ms step_avg:58.01ms
step:1934/2330 train_time:112186ms step_avg:58.01ms
step:1935/2330 train_time:112243ms step_avg:58.01ms
step:1936/2330 train_time:112304ms step_avg:58.01ms
step:1937/2330 train_time:112361ms step_avg:58.01ms
step:1938/2330 train_time:112421ms step_avg:58.01ms
step:1939/2330 train_time:112478ms step_avg:58.01ms
step:1940/2330 train_time:112539ms step_avg:58.01ms
step:1941/2330 train_time:112596ms step_avg:58.01ms
step:1942/2330 train_time:112655ms step_avg:58.01ms
step:1943/2330 train_time:112712ms step_avg:58.01ms
step:1944/2330 train_time:112772ms step_avg:58.01ms
step:1945/2330 train_time:112829ms step_avg:58.01ms
step:1946/2330 train_time:112890ms step_avg:58.01ms
step:1947/2330 train_time:112946ms step_avg:58.01ms
step:1948/2330 train_time:113007ms step_avg:58.01ms
step:1949/2330 train_time:113064ms step_avg:58.01ms
step:1950/2330 train_time:113124ms step_avg:58.01ms
step:1951/2330 train_time:113181ms step_avg:58.01ms
step:1952/2330 train_time:113241ms step_avg:58.01ms
step:1953/2330 train_time:113298ms step_avg:58.01ms
step:1954/2330 train_time:113357ms step_avg:58.01ms
step:1955/2330 train_time:113414ms step_avg:58.01ms
step:1956/2330 train_time:113476ms step_avg:58.01ms
step:1957/2330 train_time:113533ms step_avg:58.01ms
step:1958/2330 train_time:113594ms step_avg:58.02ms
step:1959/2330 train_time:113652ms step_avg:58.02ms
step:1960/2330 train_time:113712ms step_avg:58.02ms
step:1961/2330 train_time:113769ms step_avg:58.02ms
step:1962/2330 train_time:113829ms step_avg:58.02ms
step:1963/2330 train_time:113887ms step_avg:58.02ms
step:1964/2330 train_time:113947ms step_avg:58.02ms
step:1965/2330 train_time:114004ms step_avg:58.02ms
step:1966/2330 train_time:114063ms step_avg:58.02ms
step:1967/2330 train_time:114120ms step_avg:58.02ms
step:1968/2330 train_time:114181ms step_avg:58.02ms
step:1969/2330 train_time:114238ms step_avg:58.02ms
step:1970/2330 train_time:114298ms step_avg:58.02ms
step:1971/2330 train_time:114355ms step_avg:58.02ms
step:1972/2330 train_time:114415ms step_avg:58.02ms
step:1973/2330 train_time:114472ms step_avg:58.02ms
step:1974/2330 train_time:114533ms step_avg:58.02ms
step:1975/2330 train_time:114590ms step_avg:58.02ms
step:1976/2330 train_time:114652ms step_avg:58.02ms
step:1977/2330 train_time:114708ms step_avg:58.02ms
step:1978/2330 train_time:114769ms step_avg:58.02ms
step:1979/2330 train_time:114826ms step_avg:58.02ms
step:1980/2330 train_time:114887ms step_avg:58.02ms
step:1981/2330 train_time:114944ms step_avg:58.02ms
step:1982/2330 train_time:115004ms step_avg:58.02ms
step:1983/2330 train_time:115061ms step_avg:58.02ms
step:1984/2330 train_time:115121ms step_avg:58.02ms
step:1985/2330 train_time:115177ms step_avg:58.02ms
step:1986/2330 train_time:115238ms step_avg:58.03ms
step:1987/2330 train_time:115295ms step_avg:58.02ms
step:1988/2330 train_time:115355ms step_avg:58.03ms
step:1989/2330 train_time:115412ms step_avg:58.02ms
step:1990/2330 train_time:115472ms step_avg:58.03ms
step:1991/2330 train_time:115530ms step_avg:58.03ms
step:1992/2330 train_time:115590ms step_avg:58.03ms
step:1993/2330 train_time:115647ms step_avg:58.03ms
step:1994/2330 train_time:115708ms step_avg:58.03ms
step:1995/2330 train_time:115765ms step_avg:58.03ms
step:1996/2330 train_time:115825ms step_avg:58.03ms
step:1997/2330 train_time:115883ms step_avg:58.03ms
step:1998/2330 train_time:115942ms step_avg:58.03ms
step:1999/2330 train_time:115999ms step_avg:58.03ms
step:2000/2330 train_time:116060ms step_avg:58.03ms
step:2000/2330 val_loss:4.3360 train_time:116140ms step_avg:58.07ms
step:2001/2330 train_time:116159ms step_avg:58.05ms
step:2002/2330 train_time:116179ms step_avg:58.03ms
step:2003/2330 train_time:116240ms step_avg:58.03ms
step:2004/2330 train_time:116306ms step_avg:58.04ms
step:2005/2330 train_time:116363ms step_avg:58.04ms
step:2006/2330 train_time:116423ms step_avg:58.04ms
step:2007/2330 train_time:116479ms step_avg:58.04ms
step:2008/2330 train_time:116540ms step_avg:58.04ms
step:2009/2330 train_time:116597ms step_avg:58.04ms
step:2010/2330 train_time:116657ms step_avg:58.04ms
step:2011/2330 train_time:116714ms step_avg:58.04ms
step:2012/2330 train_time:116774ms step_avg:58.04ms
step:2013/2330 train_time:116830ms step_avg:58.04ms
step:2014/2330 train_time:116890ms step_avg:58.04ms
step:2015/2330 train_time:116946ms step_avg:58.04ms
step:2016/2330 train_time:117005ms step_avg:58.04ms
step:2017/2330 train_time:117062ms step_avg:58.04ms
step:2018/2330 train_time:117122ms step_avg:58.04ms
step:2019/2330 train_time:117180ms step_avg:58.04ms
step:2020/2330 train_time:117242ms step_avg:58.04ms
step:2021/2330 train_time:117301ms step_avg:58.04ms
step:2022/2330 train_time:117362ms step_avg:58.04ms
step:2023/2330 train_time:117418ms step_avg:58.04ms
step:2024/2330 train_time:117480ms step_avg:58.04ms
step:2025/2330 train_time:117537ms step_avg:58.04ms
step:2026/2330 train_time:117597ms step_avg:58.04ms
step:2027/2330 train_time:117654ms step_avg:58.04ms
step:2028/2330 train_time:117714ms step_avg:58.04ms
step:2029/2330 train_time:117770ms step_avg:58.04ms
step:2030/2330 train_time:117830ms step_avg:58.04ms
step:2031/2330 train_time:117886ms step_avg:58.04ms
step:2032/2330 train_time:117946ms step_avg:58.04ms
step:2033/2330 train_time:118003ms step_avg:58.04ms
step:2034/2330 train_time:118062ms step_avg:58.04ms
step:2035/2330 train_time:118119ms step_avg:58.04ms
step:2036/2330 train_time:118181ms step_avg:58.05ms
step:2037/2330 train_time:118238ms step_avg:58.05ms
step:2038/2330 train_time:118300ms step_avg:58.05ms
step:2039/2330 train_time:118358ms step_avg:58.05ms
step:2040/2330 train_time:118419ms step_avg:58.05ms
step:2041/2330 train_time:118476ms step_avg:58.05ms
step:2042/2330 train_time:118536ms step_avg:58.05ms
step:2043/2330 train_time:118594ms step_avg:58.05ms
step:2044/2330 train_time:118654ms step_avg:58.05ms
step:2045/2330 train_time:118711ms step_avg:58.05ms
step:2046/2330 train_time:118772ms step_avg:58.05ms
step:2047/2330 train_time:118829ms step_avg:58.05ms
step:2048/2330 train_time:118888ms step_avg:58.05ms
step:2049/2330 train_time:118945ms step_avg:58.05ms
step:2050/2330 train_time:119006ms step_avg:58.05ms
step:2051/2330 train_time:119062ms step_avg:58.05ms
step:2052/2330 train_time:119123ms step_avg:58.05ms
step:2053/2330 train_time:119180ms step_avg:58.05ms
step:2054/2330 train_time:119241ms step_avg:58.05ms
step:2055/2330 train_time:119298ms step_avg:58.05ms
step:2056/2330 train_time:119360ms step_avg:58.05ms
step:2057/2330 train_time:119417ms step_avg:58.05ms
step:2058/2330 train_time:119478ms step_avg:58.06ms
step:2059/2330 train_time:119535ms step_avg:58.05ms
step:2060/2330 train_time:119596ms step_avg:58.06ms
step:2061/2330 train_time:119654ms step_avg:58.06ms
step:2062/2330 train_time:119713ms step_avg:58.06ms
step:2063/2330 train_time:119771ms step_avg:58.06ms
step:2064/2330 train_time:119831ms step_avg:58.06ms
step:2065/2330 train_time:119889ms step_avg:58.06ms
step:2066/2330 train_time:119949ms step_avg:58.06ms
step:2067/2330 train_time:120006ms step_avg:58.06ms
step:2068/2330 train_time:120068ms step_avg:58.06ms
step:2069/2330 train_time:120125ms step_avg:58.06ms
step:2070/2330 train_time:120187ms step_avg:58.06ms
step:2071/2330 train_time:120243ms step_avg:58.06ms
step:2072/2330 train_time:120306ms step_avg:58.06ms
step:2073/2330 train_time:120362ms step_avg:58.06ms
step:2074/2330 train_time:120423ms step_avg:58.06ms
step:2075/2330 train_time:120480ms step_avg:58.06ms
step:2076/2330 train_time:120542ms step_avg:58.06ms
step:2077/2330 train_time:120598ms step_avg:58.06ms
step:2078/2330 train_time:120659ms step_avg:58.06ms
step:2079/2330 train_time:120715ms step_avg:58.06ms
step:2080/2330 train_time:120777ms step_avg:58.07ms
step:2081/2330 train_time:120834ms step_avg:58.07ms
step:2082/2330 train_time:120895ms step_avg:58.07ms
step:2083/2330 train_time:120954ms step_avg:58.07ms
step:2084/2330 train_time:121014ms step_avg:58.07ms
step:2085/2330 train_time:121071ms step_avg:58.07ms
step:2086/2330 train_time:121132ms step_avg:58.07ms
step:2087/2330 train_time:121189ms step_avg:58.07ms
step:2088/2330 train_time:121250ms step_avg:58.07ms
step:2089/2330 train_time:121307ms step_avg:58.07ms
step:2090/2330 train_time:121368ms step_avg:58.07ms
step:2091/2330 train_time:121425ms step_avg:58.07ms
step:2092/2330 train_time:121487ms step_avg:58.07ms
step:2093/2330 train_time:121543ms step_avg:58.07ms
step:2094/2330 train_time:121606ms step_avg:58.07ms
step:2095/2330 train_time:121661ms step_avg:58.07ms
step:2096/2330 train_time:121724ms step_avg:58.07ms
step:2097/2330 train_time:121780ms step_avg:58.07ms
step:2098/2330 train_time:121841ms step_avg:58.08ms
step:2099/2330 train_time:121898ms step_avg:58.07ms
step:2100/2330 train_time:121958ms step_avg:58.08ms
step:2101/2330 train_time:122015ms step_avg:58.07ms
step:2102/2330 train_time:122075ms step_avg:58.08ms
step:2103/2330 train_time:122132ms step_avg:58.08ms
step:2104/2330 train_time:122195ms step_avg:58.08ms
step:2105/2330 train_time:122253ms step_avg:58.08ms
step:2106/2330 train_time:122313ms step_avg:58.08ms
step:2107/2330 train_time:122371ms step_avg:58.08ms
step:2108/2330 train_time:122431ms step_avg:58.08ms
step:2109/2330 train_time:122490ms step_avg:58.08ms
step:2110/2330 train_time:122550ms step_avg:58.08ms
step:2111/2330 train_time:122607ms step_avg:58.08ms
step:2112/2330 train_time:122669ms step_avg:58.08ms
step:2113/2330 train_time:122725ms step_avg:58.08ms
step:2114/2330 train_time:122785ms step_avg:58.08ms
step:2115/2330 train_time:122842ms step_avg:58.08ms
step:2116/2330 train_time:122903ms step_avg:58.08ms
step:2117/2330 train_time:122959ms step_avg:58.08ms
step:2118/2330 train_time:123020ms step_avg:58.08ms
step:2119/2330 train_time:123076ms step_avg:58.08ms
step:2120/2330 train_time:123137ms step_avg:58.08ms
step:2121/2330 train_time:123194ms step_avg:58.08ms
step:2122/2330 train_time:123254ms step_avg:58.08ms
step:2123/2330 train_time:123313ms step_avg:58.08ms
step:2124/2330 train_time:123373ms step_avg:58.09ms
step:2125/2330 train_time:123431ms step_avg:58.09ms
step:2126/2330 train_time:123492ms step_avg:58.09ms
step:2127/2330 train_time:123549ms step_avg:58.09ms
step:2128/2330 train_time:123611ms step_avg:58.09ms
step:2129/2330 train_time:123668ms step_avg:58.09ms
step:2130/2330 train_time:123729ms step_avg:58.09ms
step:2131/2330 train_time:123786ms step_avg:58.09ms
step:2132/2330 train_time:123849ms step_avg:58.09ms
step:2133/2330 train_time:123906ms step_avg:58.09ms
step:2134/2330 train_time:123968ms step_avg:58.09ms
step:2135/2330 train_time:124024ms step_avg:58.09ms
step:2136/2330 train_time:124085ms step_avg:58.09ms
step:2137/2330 train_time:124142ms step_avg:58.09ms
step:2138/2330 train_time:124202ms step_avg:58.09ms
step:2139/2330 train_time:124258ms step_avg:58.09ms
step:2140/2330 train_time:124319ms step_avg:58.09ms
step:2141/2330 train_time:124375ms step_avg:58.09ms
step:2142/2330 train_time:124436ms step_avg:58.09ms
step:2143/2330 train_time:124493ms step_avg:58.09ms
step:2144/2330 train_time:124554ms step_avg:58.09ms
step:2145/2330 train_time:124612ms step_avg:58.09ms
step:2146/2330 train_time:124673ms step_avg:58.10ms
step:2147/2330 train_time:124730ms step_avg:58.10ms
step:2148/2330 train_time:124792ms step_avg:58.10ms
step:2149/2330 train_time:124850ms step_avg:58.10ms
step:2150/2330 train_time:124911ms step_avg:58.10ms
step:2151/2330 train_time:124968ms step_avg:58.10ms
step:2152/2330 train_time:125028ms step_avg:58.10ms
step:2153/2330 train_time:125085ms step_avg:58.10ms
step:2154/2330 train_time:125145ms step_avg:58.10ms
step:2155/2330 train_time:125202ms step_avg:58.10ms
step:2156/2330 train_time:125262ms step_avg:58.10ms
step:2157/2330 train_time:125318ms step_avg:58.10ms
step:2158/2330 train_time:125380ms step_avg:58.10ms
step:2159/2330 train_time:125436ms step_avg:58.10ms
step:2160/2330 train_time:125497ms step_avg:58.10ms
step:2161/2330 train_time:125554ms step_avg:58.10ms
step:2162/2330 train_time:125614ms step_avg:58.10ms
step:2163/2330 train_time:125671ms step_avg:58.10ms
step:2164/2330 train_time:125732ms step_avg:58.10ms
step:2165/2330 train_time:125790ms step_avg:58.10ms
step:2166/2330 train_time:125851ms step_avg:58.10ms
step:2167/2330 train_time:125910ms step_avg:58.10ms
step:2168/2330 train_time:125970ms step_avg:58.10ms
step:2169/2330 train_time:126027ms step_avg:58.10ms
step:2170/2330 train_time:126089ms step_avg:58.11ms
step:2171/2330 train_time:126145ms step_avg:58.10ms
step:2172/2330 train_time:126207ms step_avg:58.11ms
step:2173/2330 train_time:126263ms step_avg:58.11ms
step:2174/2330 train_time:126324ms step_avg:58.11ms
step:2175/2330 train_time:126380ms step_avg:58.11ms
step:2176/2330 train_time:126442ms step_avg:58.11ms
step:2177/2330 train_time:126498ms step_avg:58.11ms
step:2178/2330 train_time:126558ms step_avg:58.11ms
step:2179/2330 train_time:126614ms step_avg:58.11ms
step:2180/2330 train_time:126675ms step_avg:58.11ms
step:2181/2330 train_time:126732ms step_avg:58.11ms
step:2182/2330 train_time:126794ms step_avg:58.11ms
step:2183/2330 train_time:126852ms step_avg:58.11ms
step:2184/2330 train_time:126913ms step_avg:58.11ms
step:2185/2330 train_time:126970ms step_avg:58.11ms
step:2186/2330 train_time:127031ms step_avg:58.11ms
step:2187/2330 train_time:127089ms step_avg:58.11ms
step:2188/2330 train_time:127150ms step_avg:58.11ms
step:2189/2330 train_time:127208ms step_avg:58.11ms
step:2190/2330 train_time:127268ms step_avg:58.11ms
step:2191/2330 train_time:127325ms step_avg:58.11ms
step:2192/2330 train_time:127387ms step_avg:58.11ms
step:2193/2330 train_time:127444ms step_avg:58.11ms
step:2194/2330 train_time:127505ms step_avg:58.12ms
step:2195/2330 train_time:127561ms step_avg:58.11ms
step:2196/2330 train_time:127622ms step_avg:58.12ms
step:2197/2330 train_time:127678ms step_avg:58.11ms
step:2198/2330 train_time:127739ms step_avg:58.12ms
step:2199/2330 train_time:127796ms step_avg:58.12ms
step:2200/2330 train_time:127857ms step_avg:58.12ms
step:2201/2330 train_time:127914ms step_avg:58.12ms
step:2202/2330 train_time:127975ms step_avg:58.12ms
step:2203/2330 train_time:128032ms step_avg:58.12ms
step:2204/2330 train_time:128095ms step_avg:58.12ms
step:2205/2330 train_time:128153ms step_avg:58.12ms
step:2206/2330 train_time:128213ms step_avg:58.12ms
step:2207/2330 train_time:128271ms step_avg:58.12ms
step:2208/2330 train_time:128331ms step_avg:58.12ms
step:2209/2330 train_time:128389ms step_avg:58.12ms
step:2210/2330 train_time:128451ms step_avg:58.12ms
step:2211/2330 train_time:128508ms step_avg:58.12ms
step:2212/2330 train_time:128569ms step_avg:58.12ms
step:2213/2330 train_time:128625ms step_avg:58.12ms
step:2214/2330 train_time:128687ms step_avg:58.12ms
step:2215/2330 train_time:128744ms step_avg:58.12ms
step:2216/2330 train_time:128806ms step_avg:58.13ms
step:2217/2330 train_time:128862ms step_avg:58.12ms
step:2218/2330 train_time:128923ms step_avg:58.13ms
step:2219/2330 train_time:128979ms step_avg:58.12ms
step:2220/2330 train_time:129040ms step_avg:58.13ms
step:2221/2330 train_time:129096ms step_avg:58.12ms
step:2222/2330 train_time:129157ms step_avg:58.13ms
step:2223/2330 train_time:129215ms step_avg:58.13ms
step:2224/2330 train_time:129276ms step_avg:58.13ms
step:2225/2330 train_time:129333ms step_avg:58.13ms
step:2226/2330 train_time:129395ms step_avg:58.13ms
step:2227/2330 train_time:129452ms step_avg:58.13ms
step:2228/2330 train_time:129513ms step_avg:58.13ms
step:2229/2330 train_time:129572ms step_avg:58.13ms
step:2230/2330 train_time:129632ms step_avg:58.13ms
step:2231/2330 train_time:129689ms step_avg:58.13ms
step:2232/2330 train_time:129750ms step_avg:58.13ms
step:2233/2330 train_time:129807ms step_avg:58.13ms
step:2234/2330 train_time:129869ms step_avg:58.13ms
step:2235/2330 train_time:129925ms step_avg:58.13ms
step:2236/2330 train_time:129986ms step_avg:58.13ms
step:2237/2330 train_time:130042ms step_avg:58.13ms
step:2238/2330 train_time:130104ms step_avg:58.13ms
step:2239/2330 train_time:130160ms step_avg:58.13ms
step:2240/2330 train_time:130221ms step_avg:58.13ms
step:2241/2330 train_time:130277ms step_avg:58.13ms
step:2242/2330 train_time:130338ms step_avg:58.13ms
step:2243/2330 train_time:130394ms step_avg:58.13ms
step:2244/2330 train_time:130455ms step_avg:58.14ms
step:2245/2330 train_time:130512ms step_avg:58.13ms
step:2246/2330 train_time:130574ms step_avg:58.14ms
step:2247/2330 train_time:130632ms step_avg:58.14ms
step:2248/2330 train_time:130693ms step_avg:58.14ms
step:2249/2330 train_time:130752ms step_avg:58.14ms
step:2250/2330 train_time:130812ms step_avg:58.14ms
step:2250/2330 val_loss:4.2725 train_time:130893ms step_avg:58.17ms
step:2251/2330 train_time:130913ms step_avg:58.16ms
step:2252/2330 train_time:130933ms step_avg:58.14ms
step:2253/2330 train_time:130994ms step_avg:58.14ms
step:2254/2330 train_time:131058ms step_avg:58.14ms
step:2255/2330 train_time:131116ms step_avg:58.14ms
step:2256/2330 train_time:131175ms step_avg:58.15ms
step:2257/2330 train_time:131231ms step_avg:58.14ms
step:2258/2330 train_time:131292ms step_avg:58.15ms
step:2259/2330 train_time:131348ms step_avg:58.14ms
step:2260/2330 train_time:131408ms step_avg:58.15ms
step:2261/2330 train_time:131465ms step_avg:58.14ms
step:2262/2330 train_time:131524ms step_avg:58.14ms
step:2263/2330 train_time:131580ms step_avg:58.14ms
step:2264/2330 train_time:131640ms step_avg:58.14ms
step:2265/2330 train_time:131696ms step_avg:58.14ms
step:2266/2330 train_time:131756ms step_avg:58.14ms
step:2267/2330 train_time:131812ms step_avg:58.14ms
step:2268/2330 train_time:131873ms step_avg:58.15ms
step:2269/2330 train_time:131932ms step_avg:58.15ms
step:2270/2330 train_time:131994ms step_avg:58.15ms
step:2271/2330 train_time:132051ms step_avg:58.15ms
step:2272/2330 train_time:132114ms step_avg:58.15ms
step:2273/2330 train_time:132171ms step_avg:58.15ms
step:2274/2330 train_time:132232ms step_avg:58.15ms
step:2275/2330 train_time:132288ms step_avg:58.15ms
step:2276/2330 train_time:132347ms step_avg:58.15ms
step:2277/2330 train_time:132405ms step_avg:58.15ms
step:2278/2330 train_time:132464ms step_avg:58.15ms
step:2279/2330 train_time:132520ms step_avg:58.15ms
step:2280/2330 train_time:132580ms step_avg:58.15ms
step:2281/2330 train_time:132636ms step_avg:58.15ms
step:2282/2330 train_time:132696ms step_avg:58.15ms
step:2283/2330 train_time:132752ms step_avg:58.15ms
step:2284/2330 train_time:132813ms step_avg:58.15ms
step:2285/2330 train_time:132870ms step_avg:58.15ms
step:2286/2330 train_time:132931ms step_avg:58.15ms
step:2287/2330 train_time:132990ms step_avg:58.15ms
step:2288/2330 train_time:133051ms step_avg:58.15ms
step:2289/2330 train_time:133110ms step_avg:58.15ms
step:2290/2330 train_time:133170ms step_avg:58.15ms
step:2291/2330 train_time:133227ms step_avg:58.15ms
step:2292/2330 train_time:133288ms step_avg:58.15ms
step:2293/2330 train_time:133345ms step_avg:58.15ms
step:2294/2330 train_time:133406ms step_avg:58.15ms
step:2295/2330 train_time:133465ms step_avg:58.15ms
step:2296/2330 train_time:133525ms step_avg:58.16ms
step:2297/2330 train_time:133582ms step_avg:58.16ms
step:2298/2330 train_time:133642ms step_avg:58.16ms
step:2299/2330 train_time:133699ms step_avg:58.16ms
step:2300/2330 train_time:133758ms step_avg:58.16ms
step:2301/2330 train_time:133815ms step_avg:58.15ms
step:2302/2330 train_time:133875ms step_avg:58.16ms
step:2303/2330 train_time:133932ms step_avg:58.16ms
step:2304/2330 train_time:133993ms step_avg:58.16ms
step:2305/2330 train_time:134050ms step_avg:58.16ms
step:2306/2330 train_time:134111ms step_avg:58.16ms
step:2307/2330 train_time:134168ms step_avg:58.16ms
step:2308/2330 train_time:134229ms step_avg:58.16ms
step:2309/2330 train_time:134286ms step_avg:58.16ms
step:2310/2330 train_time:134347ms step_avg:58.16ms
step:2311/2330 train_time:134404ms step_avg:58.16ms
step:2312/2330 train_time:134465ms step_avg:58.16ms
step:2313/2330 train_time:134522ms step_avg:58.16ms
step:2314/2330 train_time:134583ms step_avg:58.16ms
step:2315/2330 train_time:134640ms step_avg:58.16ms
step:2316/2330 train_time:134700ms step_avg:58.16ms
step:2317/2330 train_time:134757ms step_avg:58.16ms
step:2318/2330 train_time:134816ms step_avg:58.16ms
step:2319/2330 train_time:134873ms step_avg:58.16ms
step:2320/2330 train_time:134934ms step_avg:58.16ms
step:2321/2330 train_time:134991ms step_avg:58.16ms
step:2322/2330 train_time:135052ms step_avg:58.16ms
step:2323/2330 train_time:135109ms step_avg:58.16ms
step:2324/2330 train_time:135169ms step_avg:58.16ms
step:2325/2330 train_time:135226ms step_avg:58.16ms
step:2326/2330 train_time:135286ms step_avg:58.16ms
step:2327/2330 train_time:135343ms step_avg:58.16ms
step:2328/2330 train_time:135404ms step_avg:58.16ms
step:2329/2330 train_time:135461ms step_avg:58.16ms
step:2330/2330 train_time:135522ms step_avg:58.16ms
step:2330/2330 val_loss:4.2564 train_time:135603ms step_avg:58.20ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
