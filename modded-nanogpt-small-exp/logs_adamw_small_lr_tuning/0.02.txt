import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--adamw_lr', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_lr_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_lr_tuning/{args2.adamw_lr}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=float(args2.adamw_lr),  betas=(0.85, 0.85), eps=1e-10, weight_decay=0.0, fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_lr_tuning/{args2.adamw_lr}", exist_ok=True)
            torch.save(log, f"logs_adamw_small_lr_tuning/{args2.adamw_lr}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 02:19:13 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   26C    P0             116W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   25C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   27C    P0             113W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   25C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   26C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:85ms step_avg:85.32ms
step:2/2330 train_time:175ms step_avg:87.47ms
step:3/2330 train_time:195ms step_avg:64.98ms
step:4/2330 train_time:216ms step_avg:54.01ms
step:5/2330 train_time:266ms step_avg:53.26ms
step:6/2330 train_time:324ms step_avg:53.94ms
step:7/2330 train_time:378ms step_avg:53.95ms
step:8/2330 train_time:435ms step_avg:54.32ms
step:9/2330 train_time:489ms step_avg:54.30ms
step:10/2330 train_time:546ms step_avg:54.58ms
step:11/2330 train_time:600ms step_avg:54.55ms
step:12/2330 train_time:657ms step_avg:54.77ms
step:13/2330 train_time:711ms step_avg:54.72ms
step:14/2330 train_time:768ms step_avg:54.88ms
step:15/2330 train_time:823ms step_avg:54.84ms
step:16/2330 train_time:880ms step_avg:54.98ms
step:17/2330 train_time:934ms step_avg:54.93ms
step:18/2330 train_time:990ms step_avg:55.02ms
step:19/2330 train_time:1044ms step_avg:54.97ms
step:20/2330 train_time:1103ms step_avg:55.15ms
step:21/2330 train_time:1158ms step_avg:55.13ms
step:22/2330 train_time:1217ms step_avg:55.30ms
step:23/2330 train_time:1271ms step_avg:55.26ms
step:24/2330 train_time:1329ms step_avg:55.36ms
step:25/2330 train_time:1383ms step_avg:55.34ms
step:26/2330 train_time:1441ms step_avg:55.44ms
step:27/2330 train_time:1496ms step_avg:55.40ms
step:28/2330 train_time:1554ms step_avg:55.49ms
step:29/2330 train_time:1608ms step_avg:55.44ms
step:30/2330 train_time:1665ms step_avg:55.51ms
step:31/2330 train_time:1720ms step_avg:55.48ms
step:32/2330 train_time:1777ms step_avg:55.54ms
step:33/2330 train_time:1831ms step_avg:55.50ms
step:34/2330 train_time:1889ms step_avg:55.55ms
step:35/2330 train_time:1943ms step_avg:55.52ms
step:36/2330 train_time:2000ms step_avg:55.56ms
step:37/2330 train_time:2055ms step_avg:55.55ms
step:38/2330 train_time:2113ms step_avg:55.60ms
step:39/2330 train_time:2168ms step_avg:55.58ms
step:40/2330 train_time:2225ms step_avg:55.62ms
step:41/2330 train_time:2280ms step_avg:55.61ms
step:42/2330 train_time:2337ms step_avg:55.65ms
step:43/2330 train_time:2392ms step_avg:55.62ms
step:44/2330 train_time:2450ms step_avg:55.69ms
step:45/2330 train_time:2505ms step_avg:55.67ms
step:46/2330 train_time:2564ms step_avg:55.74ms
step:47/2330 train_time:2619ms step_avg:55.72ms
step:48/2330 train_time:2677ms step_avg:55.77ms
step:49/2330 train_time:2732ms step_avg:55.75ms
step:50/2330 train_time:2789ms step_avg:55.79ms
step:51/2330 train_time:2844ms step_avg:55.76ms
step:52/2330 train_time:2901ms step_avg:55.80ms
step:53/2330 train_time:2956ms step_avg:55.77ms
step:54/2330 train_time:3014ms step_avg:55.81ms
step:55/2330 train_time:3069ms step_avg:55.80ms
step:56/2330 train_time:3126ms step_avg:55.82ms
step:57/2330 train_time:3181ms step_avg:55.80ms
step:58/2330 train_time:3239ms step_avg:55.84ms
step:59/2330 train_time:3294ms step_avg:55.83ms
step:60/2330 train_time:3352ms step_avg:55.87ms
step:61/2330 train_time:3407ms step_avg:55.85ms
step:62/2330 train_time:3464ms step_avg:55.88ms
step:63/2330 train_time:3520ms step_avg:55.87ms
step:64/2330 train_time:3578ms step_avg:55.90ms
step:65/2330 train_time:3633ms step_avg:55.89ms
step:66/2330 train_time:3691ms step_avg:55.92ms
step:67/2330 train_time:3745ms step_avg:55.90ms
step:68/2330 train_time:3803ms step_avg:55.93ms
step:69/2330 train_time:3858ms step_avg:55.92ms
step:70/2330 train_time:3916ms step_avg:55.94ms
step:71/2330 train_time:3970ms step_avg:55.92ms
step:72/2330 train_time:4028ms step_avg:55.95ms
step:73/2330 train_time:4083ms step_avg:55.93ms
step:74/2330 train_time:4140ms step_avg:55.95ms
step:75/2330 train_time:4195ms step_avg:55.94ms
step:76/2330 train_time:4253ms step_avg:55.96ms
step:77/2330 train_time:4308ms step_avg:55.95ms
step:78/2330 train_time:4366ms step_avg:55.97ms
step:79/2330 train_time:4421ms step_avg:55.96ms
step:80/2330 train_time:4479ms step_avg:55.98ms
step:81/2330 train_time:4534ms step_avg:55.97ms
step:82/2330 train_time:4591ms step_avg:55.99ms
step:83/2330 train_time:4647ms step_avg:55.98ms
step:84/2330 train_time:4705ms step_avg:56.01ms
step:85/2330 train_time:4760ms step_avg:56.00ms
step:86/2330 train_time:4818ms step_avg:56.02ms
step:87/2330 train_time:4874ms step_avg:56.02ms
step:88/2330 train_time:4931ms step_avg:56.03ms
step:89/2330 train_time:4986ms step_avg:56.02ms
step:90/2330 train_time:5044ms step_avg:56.04ms
step:91/2330 train_time:5098ms step_avg:56.03ms
step:92/2330 train_time:5156ms step_avg:56.05ms
step:93/2330 train_time:5211ms step_avg:56.03ms
step:94/2330 train_time:5269ms step_avg:56.06ms
step:95/2330 train_time:5324ms step_avg:56.04ms
step:96/2330 train_time:5382ms step_avg:56.06ms
step:97/2330 train_time:5437ms step_avg:56.05ms
step:98/2330 train_time:5495ms step_avg:56.07ms
step:99/2330 train_time:5550ms step_avg:56.06ms
step:100/2330 train_time:5608ms step_avg:56.08ms
step:101/2330 train_time:5663ms step_avg:56.07ms
step:102/2330 train_time:5721ms step_avg:56.09ms
step:103/2330 train_time:5777ms step_avg:56.08ms
step:104/2330 train_time:5835ms step_avg:56.10ms
step:105/2330 train_time:5890ms step_avg:56.09ms
step:106/2330 train_time:5947ms step_avg:56.11ms
step:107/2330 train_time:6003ms step_avg:56.10ms
step:108/2330 train_time:6061ms step_avg:56.12ms
step:109/2330 train_time:6117ms step_avg:56.12ms
step:110/2330 train_time:6175ms step_avg:56.13ms
step:111/2330 train_time:6230ms step_avg:56.12ms
step:112/2330 train_time:6288ms step_avg:56.15ms
step:113/2330 train_time:6344ms step_avg:56.14ms
step:114/2330 train_time:6402ms step_avg:56.16ms
step:115/2330 train_time:6458ms step_avg:56.15ms
step:116/2330 train_time:6516ms step_avg:56.17ms
step:117/2330 train_time:6571ms step_avg:56.16ms
step:118/2330 train_time:6628ms step_avg:56.17ms
step:119/2330 train_time:6683ms step_avg:56.16ms
step:120/2330 train_time:6742ms step_avg:56.18ms
step:121/2330 train_time:6797ms step_avg:56.18ms
step:122/2330 train_time:6855ms step_avg:56.19ms
step:123/2330 train_time:6910ms step_avg:56.18ms
step:124/2330 train_time:6968ms step_avg:56.19ms
step:125/2330 train_time:7023ms step_avg:56.19ms
step:126/2330 train_time:7081ms step_avg:56.20ms
step:127/2330 train_time:7136ms step_avg:56.19ms
step:128/2330 train_time:7196ms step_avg:56.22ms
step:129/2330 train_time:7251ms step_avg:56.21ms
step:130/2330 train_time:7310ms step_avg:56.23ms
step:131/2330 train_time:7365ms step_avg:56.22ms
step:132/2330 train_time:7424ms step_avg:56.24ms
step:133/2330 train_time:7479ms step_avg:56.23ms
step:134/2330 train_time:7537ms step_avg:56.25ms
step:135/2330 train_time:7592ms step_avg:56.24ms
step:136/2330 train_time:7651ms step_avg:56.26ms
step:137/2330 train_time:7706ms step_avg:56.25ms
step:138/2330 train_time:7765ms step_avg:56.27ms
step:139/2330 train_time:7821ms step_avg:56.26ms
step:140/2330 train_time:7878ms step_avg:56.27ms
step:141/2330 train_time:7934ms step_avg:56.27ms
step:142/2330 train_time:7991ms step_avg:56.28ms
step:143/2330 train_time:8046ms step_avg:56.26ms
step:144/2330 train_time:8105ms step_avg:56.28ms
step:145/2330 train_time:8160ms step_avg:56.28ms
step:146/2330 train_time:8219ms step_avg:56.30ms
step:147/2330 train_time:8275ms step_avg:56.29ms
step:148/2330 train_time:8333ms step_avg:56.31ms
step:149/2330 train_time:8388ms step_avg:56.30ms
step:150/2330 train_time:8447ms step_avg:56.31ms
step:151/2330 train_time:8502ms step_avg:56.30ms
step:152/2330 train_time:8560ms step_avg:56.32ms
step:153/2330 train_time:8615ms step_avg:56.31ms
step:154/2330 train_time:8674ms step_avg:56.32ms
step:155/2330 train_time:8729ms step_avg:56.31ms
step:156/2330 train_time:8787ms step_avg:56.32ms
step:157/2330 train_time:8842ms step_avg:56.32ms
step:158/2330 train_time:8900ms step_avg:56.33ms
step:159/2330 train_time:8955ms step_avg:56.32ms
step:160/2330 train_time:9014ms step_avg:56.34ms
step:161/2330 train_time:9069ms step_avg:56.33ms
step:162/2330 train_time:9127ms step_avg:56.34ms
step:163/2330 train_time:9182ms step_avg:56.33ms
step:164/2330 train_time:9242ms step_avg:56.35ms
step:165/2330 train_time:9297ms step_avg:56.35ms
step:166/2330 train_time:9356ms step_avg:56.36ms
step:167/2330 train_time:9412ms step_avg:56.36ms
step:168/2330 train_time:9471ms step_avg:56.38ms
step:169/2330 train_time:9526ms step_avg:56.37ms
step:170/2330 train_time:9584ms step_avg:56.38ms
step:171/2330 train_time:9640ms step_avg:56.37ms
step:172/2330 train_time:9698ms step_avg:56.38ms
step:173/2330 train_time:9753ms step_avg:56.38ms
step:174/2330 train_time:9811ms step_avg:56.39ms
step:175/2330 train_time:9866ms step_avg:56.38ms
step:176/2330 train_time:9925ms step_avg:56.39ms
step:177/2330 train_time:9981ms step_avg:56.39ms
step:178/2330 train_time:10039ms step_avg:56.40ms
step:179/2330 train_time:10094ms step_avg:56.39ms
step:180/2330 train_time:10153ms step_avg:56.41ms
step:181/2330 train_time:10208ms step_avg:56.40ms
step:182/2330 train_time:10266ms step_avg:56.41ms
step:183/2330 train_time:10322ms step_avg:56.40ms
step:184/2330 train_time:10381ms step_avg:56.42ms
step:185/2330 train_time:10436ms step_avg:56.41ms
step:186/2330 train_time:10495ms step_avg:56.43ms
step:187/2330 train_time:10551ms step_avg:56.42ms
step:188/2330 train_time:10608ms step_avg:56.43ms
step:189/2330 train_time:10664ms step_avg:56.42ms
step:190/2330 train_time:10723ms step_avg:56.44ms
step:191/2330 train_time:10778ms step_avg:56.43ms
step:192/2330 train_time:10836ms step_avg:56.44ms
step:193/2330 train_time:10891ms step_avg:56.43ms
step:194/2330 train_time:10950ms step_avg:56.44ms
step:195/2330 train_time:11005ms step_avg:56.44ms
step:196/2330 train_time:11063ms step_avg:56.45ms
step:197/2330 train_time:11120ms step_avg:56.44ms
step:198/2330 train_time:11178ms step_avg:56.45ms
step:199/2330 train_time:11233ms step_avg:56.45ms
step:200/2330 train_time:11290ms step_avg:56.45ms
step:201/2330 train_time:11346ms step_avg:56.45ms
step:202/2330 train_time:11405ms step_avg:56.46ms
step:203/2330 train_time:11460ms step_avg:56.45ms
step:204/2330 train_time:11519ms step_avg:56.47ms
step:205/2330 train_time:11574ms step_avg:56.46ms
step:206/2330 train_time:11634ms step_avg:56.47ms
step:207/2330 train_time:11689ms step_avg:56.47ms
step:208/2330 train_time:11747ms step_avg:56.48ms
step:209/2330 train_time:11802ms step_avg:56.47ms
step:210/2330 train_time:11862ms step_avg:56.48ms
step:211/2330 train_time:11917ms step_avg:56.48ms
step:212/2330 train_time:11976ms step_avg:56.49ms
step:213/2330 train_time:12031ms step_avg:56.48ms
step:214/2330 train_time:12090ms step_avg:56.50ms
step:215/2330 train_time:12146ms step_avg:56.49ms
step:216/2330 train_time:12205ms step_avg:56.51ms
step:217/2330 train_time:12260ms step_avg:56.50ms
step:218/2330 train_time:12320ms step_avg:56.51ms
step:219/2330 train_time:12376ms step_avg:56.51ms
step:220/2330 train_time:12434ms step_avg:56.52ms
step:221/2330 train_time:12489ms step_avg:56.51ms
step:222/2330 train_time:12548ms step_avg:56.52ms
step:223/2330 train_time:12603ms step_avg:56.52ms
step:224/2330 train_time:12663ms step_avg:56.53ms
step:225/2330 train_time:12719ms step_avg:56.53ms
step:226/2330 train_time:12777ms step_avg:56.53ms
step:227/2330 train_time:12832ms step_avg:56.53ms
step:228/2330 train_time:12891ms step_avg:56.54ms
step:229/2330 train_time:12946ms step_avg:56.53ms
step:230/2330 train_time:13006ms step_avg:56.55ms
step:231/2330 train_time:13061ms step_avg:56.54ms
step:232/2330 train_time:13120ms step_avg:56.55ms
step:233/2330 train_time:13175ms step_avg:56.54ms
step:234/2330 train_time:13233ms step_avg:56.55ms
step:235/2330 train_time:13288ms step_avg:56.54ms
step:236/2330 train_time:13346ms step_avg:56.55ms
step:237/2330 train_time:13402ms step_avg:56.55ms
step:238/2330 train_time:13461ms step_avg:56.56ms
step:239/2330 train_time:13517ms step_avg:56.56ms
step:240/2330 train_time:13576ms step_avg:56.57ms
step:241/2330 train_time:13631ms step_avg:56.56ms
step:242/2330 train_time:13690ms step_avg:56.57ms
step:243/2330 train_time:13745ms step_avg:56.56ms
step:244/2330 train_time:13804ms step_avg:56.58ms
step:245/2330 train_time:13860ms step_avg:56.57ms
step:246/2330 train_time:13919ms step_avg:56.58ms
step:247/2330 train_time:13975ms step_avg:56.58ms
step:248/2330 train_time:14034ms step_avg:56.59ms
step:249/2330 train_time:14088ms step_avg:56.58ms
step:250/2330 train_time:14147ms step_avg:56.59ms
step:250/2330 val_loss:6.3092 train_time:14226ms step_avg:56.90ms
step:251/2330 train_time:14246ms step_avg:56.76ms
step:252/2330 train_time:14266ms step_avg:56.61ms
step:253/2330 train_time:14321ms step_avg:56.60ms
step:254/2330 train_time:14382ms step_avg:56.62ms
step:255/2330 train_time:14438ms step_avg:56.62ms
step:256/2330 train_time:14500ms step_avg:56.64ms
step:257/2330 train_time:14555ms step_avg:56.64ms
step:258/2330 train_time:14614ms step_avg:56.64ms
step:259/2330 train_time:14670ms step_avg:56.64ms
step:260/2330 train_time:14728ms step_avg:56.65ms
step:261/2330 train_time:14783ms step_avg:56.64ms
step:262/2330 train_time:14841ms step_avg:56.65ms
step:263/2330 train_time:14896ms step_avg:56.64ms
step:264/2330 train_time:14954ms step_avg:56.65ms
step:265/2330 train_time:15010ms step_avg:56.64ms
step:266/2330 train_time:15068ms step_avg:56.64ms
step:267/2330 train_time:15122ms step_avg:56.64ms
step:268/2330 train_time:15180ms step_avg:56.64ms
step:269/2330 train_time:15236ms step_avg:56.64ms
step:270/2330 train_time:15296ms step_avg:56.65ms
step:271/2330 train_time:15352ms step_avg:56.65ms
step:272/2330 train_time:15413ms step_avg:56.66ms
step:273/2330 train_time:15469ms step_avg:56.66ms
step:274/2330 train_time:15528ms step_avg:56.67ms
step:275/2330 train_time:15584ms step_avg:56.67ms
step:276/2330 train_time:15642ms step_avg:56.67ms
step:277/2330 train_time:15697ms step_avg:56.67ms
step:278/2330 train_time:15755ms step_avg:56.67ms
step:279/2330 train_time:15811ms step_avg:56.67ms
step:280/2330 train_time:15870ms step_avg:56.68ms
step:281/2330 train_time:15925ms step_avg:56.67ms
step:282/2330 train_time:15982ms step_avg:56.67ms
step:283/2330 train_time:16037ms step_avg:56.67ms
step:284/2330 train_time:16096ms step_avg:56.67ms
step:285/2330 train_time:16152ms step_avg:56.67ms
step:286/2330 train_time:16209ms step_avg:56.68ms
step:287/2330 train_time:16265ms step_avg:56.67ms
step:288/2330 train_time:16324ms step_avg:56.68ms
step:289/2330 train_time:16379ms step_avg:56.68ms
step:290/2330 train_time:16439ms step_avg:56.69ms
step:291/2330 train_time:16494ms step_avg:56.68ms
step:292/2330 train_time:16555ms step_avg:56.69ms
step:293/2330 train_time:16611ms step_avg:56.69ms
step:294/2330 train_time:16669ms step_avg:56.70ms
step:295/2330 train_time:16724ms step_avg:56.69ms
step:296/2330 train_time:16782ms step_avg:56.70ms
step:297/2330 train_time:16838ms step_avg:56.69ms
step:298/2330 train_time:16896ms step_avg:56.70ms
step:299/2330 train_time:16952ms step_avg:56.69ms
step:300/2330 train_time:17010ms step_avg:56.70ms
step:301/2330 train_time:17065ms step_avg:56.69ms
step:302/2330 train_time:17124ms step_avg:56.70ms
step:303/2330 train_time:17179ms step_avg:56.70ms
step:304/2330 train_time:17238ms step_avg:56.70ms
step:305/2330 train_time:17294ms step_avg:56.70ms
step:306/2330 train_time:17352ms step_avg:56.71ms
step:307/2330 train_time:17408ms step_avg:56.70ms
step:308/2330 train_time:17468ms step_avg:56.71ms
step:309/2330 train_time:17523ms step_avg:56.71ms
step:310/2330 train_time:17583ms step_avg:56.72ms
step:311/2330 train_time:17638ms step_avg:56.71ms
step:312/2330 train_time:17696ms step_avg:56.72ms
step:313/2330 train_time:17752ms step_avg:56.72ms
step:314/2330 train_time:17810ms step_avg:56.72ms
step:315/2330 train_time:17865ms step_avg:56.71ms
step:316/2330 train_time:17923ms step_avg:56.72ms
step:317/2330 train_time:17987ms step_avg:56.74ms
step:318/2330 train_time:18038ms step_avg:56.72ms
step:319/2330 train_time:18093ms step_avg:56.72ms
step:320/2330 train_time:18152ms step_avg:56.73ms
step:321/2330 train_time:18207ms step_avg:56.72ms
step:322/2330 train_time:18267ms step_avg:56.73ms
step:323/2330 train_time:18322ms step_avg:56.73ms
step:324/2330 train_time:18380ms step_avg:56.73ms
step:325/2330 train_time:18436ms step_avg:56.72ms
step:326/2330 train_time:18495ms step_avg:56.73ms
step:327/2330 train_time:18550ms step_avg:56.73ms
step:328/2330 train_time:18611ms step_avg:56.74ms
step:329/2330 train_time:18666ms step_avg:56.74ms
step:330/2330 train_time:18725ms step_avg:56.74ms
step:331/2330 train_time:18781ms step_avg:56.74ms
step:332/2330 train_time:18838ms step_avg:56.74ms
step:333/2330 train_time:18894ms step_avg:56.74ms
step:334/2330 train_time:18952ms step_avg:56.74ms
step:335/2330 train_time:19008ms step_avg:56.74ms
step:336/2330 train_time:19066ms step_avg:56.74ms
step:337/2330 train_time:19121ms step_avg:56.74ms
step:338/2330 train_time:19180ms step_avg:56.74ms
step:339/2330 train_time:19235ms step_avg:56.74ms
step:340/2330 train_time:19295ms step_avg:56.75ms
step:341/2330 train_time:19351ms step_avg:56.75ms
step:342/2330 train_time:19410ms step_avg:56.75ms
step:343/2330 train_time:19466ms step_avg:56.75ms
step:344/2330 train_time:19525ms step_avg:56.76ms
step:345/2330 train_time:19580ms step_avg:56.75ms
step:346/2330 train_time:19639ms step_avg:56.76ms
step:347/2330 train_time:19694ms step_avg:56.75ms
step:348/2330 train_time:19753ms step_avg:56.76ms
step:349/2330 train_time:19810ms step_avg:56.76ms
step:350/2330 train_time:19868ms step_avg:56.77ms
step:351/2330 train_time:19924ms step_avg:56.76ms
step:352/2330 train_time:19981ms step_avg:56.77ms
step:353/2330 train_time:20036ms step_avg:56.76ms
step:354/2330 train_time:20096ms step_avg:56.77ms
step:355/2330 train_time:20151ms step_avg:56.76ms
step:356/2330 train_time:20210ms step_avg:56.77ms
step:357/2330 train_time:20265ms step_avg:56.77ms
step:358/2330 train_time:20324ms step_avg:56.77ms
step:359/2330 train_time:20380ms step_avg:56.77ms
step:360/2330 train_time:20438ms step_avg:56.77ms
step:361/2330 train_time:20494ms step_avg:56.77ms
step:362/2330 train_time:20553ms step_avg:56.78ms
step:363/2330 train_time:20609ms step_avg:56.78ms
step:364/2330 train_time:20668ms step_avg:56.78ms
step:365/2330 train_time:20723ms step_avg:56.77ms
step:366/2330 train_time:20782ms step_avg:56.78ms
step:367/2330 train_time:20837ms step_avg:56.78ms
step:368/2330 train_time:20896ms step_avg:56.78ms
step:369/2330 train_time:20952ms step_avg:56.78ms
step:370/2330 train_time:21010ms step_avg:56.78ms
step:371/2330 train_time:21066ms step_avg:56.78ms
step:372/2330 train_time:21124ms step_avg:56.78ms
step:373/2330 train_time:21179ms step_avg:56.78ms
step:374/2330 train_time:21237ms step_avg:56.78ms
step:375/2330 train_time:21293ms step_avg:56.78ms
step:376/2330 train_time:21352ms step_avg:56.79ms
step:377/2330 train_time:21408ms step_avg:56.78ms
step:378/2330 train_time:21467ms step_avg:56.79ms
step:379/2330 train_time:21522ms step_avg:56.79ms
step:380/2330 train_time:21581ms step_avg:56.79ms
step:381/2330 train_time:21637ms step_avg:56.79ms
step:382/2330 train_time:21696ms step_avg:56.80ms
step:383/2330 train_time:21751ms step_avg:56.79ms
step:384/2330 train_time:21810ms step_avg:56.80ms
step:385/2330 train_time:21866ms step_avg:56.79ms
step:386/2330 train_time:21923ms step_avg:56.80ms
step:387/2330 train_time:21979ms step_avg:56.79ms
step:388/2330 train_time:22037ms step_avg:56.80ms
step:389/2330 train_time:22093ms step_avg:56.79ms
step:390/2330 train_time:22151ms step_avg:56.80ms
step:391/2330 train_time:22207ms step_avg:56.80ms
step:392/2330 train_time:22266ms step_avg:56.80ms
step:393/2330 train_time:22321ms step_avg:56.80ms
step:394/2330 train_time:22380ms step_avg:56.80ms
step:395/2330 train_time:22436ms step_avg:56.80ms
step:396/2330 train_time:22495ms step_avg:56.81ms
step:397/2330 train_time:22552ms step_avg:56.80ms
step:398/2330 train_time:22610ms step_avg:56.81ms
step:399/2330 train_time:22666ms step_avg:56.81ms
step:400/2330 train_time:22724ms step_avg:56.81ms
step:401/2330 train_time:22780ms step_avg:56.81ms
step:402/2330 train_time:22839ms step_avg:56.81ms
step:403/2330 train_time:22894ms step_avg:56.81ms
step:404/2330 train_time:22953ms step_avg:56.81ms
step:405/2330 train_time:23009ms step_avg:56.81ms
step:406/2330 train_time:23068ms step_avg:56.82ms
step:407/2330 train_time:23123ms step_avg:56.81ms
step:408/2330 train_time:23181ms step_avg:56.82ms
step:409/2330 train_time:23237ms step_avg:56.81ms
step:410/2330 train_time:23296ms step_avg:56.82ms
step:411/2330 train_time:23351ms step_avg:56.82ms
step:412/2330 train_time:23411ms step_avg:56.82ms
step:413/2330 train_time:23467ms step_avg:56.82ms
step:414/2330 train_time:23526ms step_avg:56.83ms
step:415/2330 train_time:23581ms step_avg:56.82ms
step:416/2330 train_time:23641ms step_avg:56.83ms
step:417/2330 train_time:23696ms step_avg:56.83ms
step:418/2330 train_time:23755ms step_avg:56.83ms
step:419/2330 train_time:23811ms step_avg:56.83ms
step:420/2330 train_time:23870ms step_avg:56.83ms
step:421/2330 train_time:23925ms step_avg:56.83ms
step:422/2330 train_time:23983ms step_avg:56.83ms
step:423/2330 train_time:24038ms step_avg:56.83ms
step:424/2330 train_time:24097ms step_avg:56.83ms
step:425/2330 train_time:24152ms step_avg:56.83ms
step:426/2330 train_time:24212ms step_avg:56.83ms
step:427/2330 train_time:24267ms step_avg:56.83ms
step:428/2330 train_time:24325ms step_avg:56.83ms
step:429/2330 train_time:24381ms step_avg:56.83ms
step:430/2330 train_time:24440ms step_avg:56.84ms
step:431/2330 train_time:24495ms step_avg:56.83ms
step:432/2330 train_time:24554ms step_avg:56.84ms
step:433/2330 train_time:24610ms step_avg:56.84ms
step:434/2330 train_time:24668ms step_avg:56.84ms
step:435/2330 train_time:24724ms step_avg:56.84ms
step:436/2330 train_time:24782ms step_avg:56.84ms
step:437/2330 train_time:24837ms step_avg:56.84ms
step:438/2330 train_time:24896ms step_avg:56.84ms
step:439/2330 train_time:24952ms step_avg:56.84ms
step:440/2330 train_time:25011ms step_avg:56.84ms
step:441/2330 train_time:25066ms step_avg:56.84ms
step:442/2330 train_time:25125ms step_avg:56.84ms
step:443/2330 train_time:25180ms step_avg:56.84ms
step:444/2330 train_time:25238ms step_avg:56.84ms
step:445/2330 train_time:25294ms step_avg:56.84ms
step:446/2330 train_time:25352ms step_avg:56.84ms
step:447/2330 train_time:25408ms step_avg:56.84ms
step:448/2330 train_time:25467ms step_avg:56.85ms
step:449/2330 train_time:25522ms step_avg:56.84ms
step:450/2330 train_time:25582ms step_avg:56.85ms
step:451/2330 train_time:25637ms step_avg:56.84ms
step:452/2330 train_time:25696ms step_avg:56.85ms
step:453/2330 train_time:25751ms step_avg:56.85ms
step:454/2330 train_time:25811ms step_avg:56.85ms
step:455/2330 train_time:25866ms step_avg:56.85ms
step:456/2330 train_time:25925ms step_avg:56.85ms
step:457/2330 train_time:25980ms step_avg:56.85ms
step:458/2330 train_time:26039ms step_avg:56.85ms
step:459/2330 train_time:26095ms step_avg:56.85ms
step:460/2330 train_time:26155ms step_avg:56.86ms
step:461/2330 train_time:26211ms step_avg:56.86ms
step:462/2330 train_time:26269ms step_avg:56.86ms
step:463/2330 train_time:26325ms step_avg:56.86ms
step:464/2330 train_time:26383ms step_avg:56.86ms
step:465/2330 train_time:26439ms step_avg:56.86ms
step:466/2330 train_time:26497ms step_avg:56.86ms
step:467/2330 train_time:26554ms step_avg:56.86ms
step:468/2330 train_time:26612ms step_avg:56.86ms
step:469/2330 train_time:26668ms step_avg:56.86ms
step:470/2330 train_time:26727ms step_avg:56.87ms
step:471/2330 train_time:26782ms step_avg:56.86ms
step:472/2330 train_time:26841ms step_avg:56.87ms
step:473/2330 train_time:26896ms step_avg:56.86ms
step:474/2330 train_time:26955ms step_avg:56.87ms
step:475/2330 train_time:27011ms step_avg:56.86ms
step:476/2330 train_time:27069ms step_avg:56.87ms
step:477/2330 train_time:27125ms step_avg:56.86ms
step:478/2330 train_time:27184ms step_avg:56.87ms
step:479/2330 train_time:27239ms step_avg:56.87ms
step:480/2330 train_time:27298ms step_avg:56.87ms
step:481/2330 train_time:27354ms step_avg:56.87ms
step:482/2330 train_time:27413ms step_avg:56.87ms
step:483/2330 train_time:27470ms step_avg:56.87ms
step:484/2330 train_time:27528ms step_avg:56.88ms
step:485/2330 train_time:27584ms step_avg:56.87ms
step:486/2330 train_time:27642ms step_avg:56.88ms
step:487/2330 train_time:27697ms step_avg:56.87ms
step:488/2330 train_time:27756ms step_avg:56.88ms
step:489/2330 train_time:27812ms step_avg:56.88ms
step:490/2330 train_time:27870ms step_avg:56.88ms
step:491/2330 train_time:27926ms step_avg:56.87ms
step:492/2330 train_time:27984ms step_avg:56.88ms
step:493/2330 train_time:28039ms step_avg:56.88ms
step:494/2330 train_time:28099ms step_avg:56.88ms
step:495/2330 train_time:28154ms step_avg:56.88ms
step:496/2330 train_time:28214ms step_avg:56.88ms
step:497/2330 train_time:28270ms step_avg:56.88ms
step:498/2330 train_time:28328ms step_avg:56.88ms
step:499/2330 train_time:28383ms step_avg:56.88ms
step:500/2330 train_time:28441ms step_avg:56.88ms
step:500/2330 val_loss:5.4474 train_time:28521ms step_avg:57.04ms
step:501/2330 train_time:28541ms step_avg:56.97ms
step:502/2330 train_time:28561ms step_avg:56.89ms
step:503/2330 train_time:28616ms step_avg:56.89ms
step:504/2330 train_time:28676ms step_avg:56.90ms
step:505/2330 train_time:28733ms step_avg:56.90ms
step:506/2330 train_time:28792ms step_avg:56.90ms
step:507/2330 train_time:28848ms step_avg:56.90ms
step:508/2330 train_time:28906ms step_avg:56.90ms
step:509/2330 train_time:28963ms step_avg:56.90ms
step:510/2330 train_time:29021ms step_avg:56.90ms
step:511/2330 train_time:29076ms step_avg:56.90ms
step:512/2330 train_time:29133ms step_avg:56.90ms
step:513/2330 train_time:29188ms step_avg:56.90ms
step:514/2330 train_time:29246ms step_avg:56.90ms
step:515/2330 train_time:29301ms step_avg:56.90ms
step:516/2330 train_time:29359ms step_avg:56.90ms
step:517/2330 train_time:29414ms step_avg:56.89ms
step:518/2330 train_time:29473ms step_avg:56.90ms
step:519/2330 train_time:29529ms step_avg:56.90ms
step:520/2330 train_time:29589ms step_avg:56.90ms
step:521/2330 train_time:29645ms step_avg:56.90ms
step:522/2330 train_time:29706ms step_avg:56.91ms
step:523/2330 train_time:29762ms step_avg:56.91ms
step:524/2330 train_time:29821ms step_avg:56.91ms
step:525/2330 train_time:29877ms step_avg:56.91ms
step:526/2330 train_time:29935ms step_avg:56.91ms
step:527/2330 train_time:29991ms step_avg:56.91ms
step:528/2330 train_time:30049ms step_avg:56.91ms
step:529/2330 train_time:30105ms step_avg:56.91ms
step:530/2330 train_time:30163ms step_avg:56.91ms
step:531/2330 train_time:30219ms step_avg:56.91ms
step:532/2330 train_time:30277ms step_avg:56.91ms
step:533/2330 train_time:30333ms step_avg:56.91ms
step:534/2330 train_time:30391ms step_avg:56.91ms
step:535/2330 train_time:30446ms step_avg:56.91ms
step:536/2330 train_time:30505ms step_avg:56.91ms
step:537/2330 train_time:30561ms step_avg:56.91ms
step:538/2330 train_time:30620ms step_avg:56.91ms
step:539/2330 train_time:30676ms step_avg:56.91ms
step:540/2330 train_time:30734ms step_avg:56.92ms
step:541/2330 train_time:30791ms step_avg:56.91ms
step:542/2330 train_time:30850ms step_avg:56.92ms
step:543/2330 train_time:30906ms step_avg:56.92ms
step:544/2330 train_time:30964ms step_avg:56.92ms
step:545/2330 train_time:31020ms step_avg:56.92ms
step:546/2330 train_time:31079ms step_avg:56.92ms
step:547/2330 train_time:31134ms step_avg:56.92ms
step:548/2330 train_time:31192ms step_avg:56.92ms
step:549/2330 train_time:31247ms step_avg:56.92ms
step:550/2330 train_time:31307ms step_avg:56.92ms
step:551/2330 train_time:31362ms step_avg:56.92ms
step:552/2330 train_time:31420ms step_avg:56.92ms
step:553/2330 train_time:31476ms step_avg:56.92ms
step:554/2330 train_time:31535ms step_avg:56.92ms
step:555/2330 train_time:31590ms step_avg:56.92ms
step:556/2330 train_time:31650ms step_avg:56.92ms
step:557/2330 train_time:31706ms step_avg:56.92ms
step:558/2330 train_time:31765ms step_avg:56.93ms
step:559/2330 train_time:31821ms step_avg:56.93ms
step:560/2330 train_time:31880ms step_avg:56.93ms
step:561/2330 train_time:31936ms step_avg:56.93ms
step:562/2330 train_time:31995ms step_avg:56.93ms
step:563/2330 train_time:32050ms step_avg:56.93ms
step:564/2330 train_time:32109ms step_avg:56.93ms
step:565/2330 train_time:32165ms step_avg:56.93ms
step:566/2330 train_time:32224ms step_avg:56.93ms
step:567/2330 train_time:32279ms step_avg:56.93ms
step:568/2330 train_time:32338ms step_avg:56.93ms
step:569/2330 train_time:32393ms step_avg:56.93ms
step:570/2330 train_time:32452ms step_avg:56.93ms
step:571/2330 train_time:32507ms step_avg:56.93ms
step:572/2330 train_time:32566ms step_avg:56.93ms
step:573/2330 train_time:32622ms step_avg:56.93ms
step:574/2330 train_time:32681ms step_avg:56.94ms
step:575/2330 train_time:32736ms step_avg:56.93ms
step:576/2330 train_time:32796ms step_avg:56.94ms
step:577/2330 train_time:32852ms step_avg:56.94ms
step:578/2330 train_time:32910ms step_avg:56.94ms
step:579/2330 train_time:32965ms step_avg:56.94ms
step:580/2330 train_time:33024ms step_avg:56.94ms
step:581/2330 train_time:33080ms step_avg:56.94ms
step:582/2330 train_time:33139ms step_avg:56.94ms
step:583/2330 train_time:33194ms step_avg:56.94ms
step:584/2330 train_time:33253ms step_avg:56.94ms
step:585/2330 train_time:33309ms step_avg:56.94ms
step:586/2330 train_time:33368ms step_avg:56.94ms
step:587/2330 train_time:33423ms step_avg:56.94ms
step:588/2330 train_time:33482ms step_avg:56.94ms
step:589/2330 train_time:33537ms step_avg:56.94ms
step:590/2330 train_time:33596ms step_avg:56.94ms
step:591/2330 train_time:33652ms step_avg:56.94ms
step:592/2330 train_time:33711ms step_avg:56.94ms
step:593/2330 train_time:33767ms step_avg:56.94ms
step:594/2330 train_time:33825ms step_avg:56.95ms
step:595/2330 train_time:33881ms step_avg:56.94ms
step:596/2330 train_time:33940ms step_avg:56.95ms
step:597/2330 train_time:33996ms step_avg:56.94ms
step:598/2330 train_time:34054ms step_avg:56.95ms
step:599/2330 train_time:34110ms step_avg:56.94ms
step:600/2330 train_time:34169ms step_avg:56.95ms
step:601/2330 train_time:34225ms step_avg:56.95ms
step:602/2330 train_time:34284ms step_avg:56.95ms
step:603/2330 train_time:34340ms step_avg:56.95ms
step:604/2330 train_time:34398ms step_avg:56.95ms
step:605/2330 train_time:34454ms step_avg:56.95ms
step:606/2330 train_time:34512ms step_avg:56.95ms
step:607/2330 train_time:34568ms step_avg:56.95ms
step:608/2330 train_time:34627ms step_avg:56.95ms
step:609/2330 train_time:34684ms step_avg:56.95ms
step:610/2330 train_time:34742ms step_avg:56.95ms
step:611/2330 train_time:34798ms step_avg:56.95ms
step:612/2330 train_time:34857ms step_avg:56.96ms
step:613/2330 train_time:34913ms step_avg:56.95ms
step:614/2330 train_time:34971ms step_avg:56.96ms
step:615/2330 train_time:35027ms step_avg:56.95ms
step:616/2330 train_time:35086ms step_avg:56.96ms
step:617/2330 train_time:35142ms step_avg:56.96ms
step:618/2330 train_time:35200ms step_avg:56.96ms
step:619/2330 train_time:35256ms step_avg:56.96ms
step:620/2330 train_time:35314ms step_avg:56.96ms
step:621/2330 train_time:35370ms step_avg:56.96ms
step:622/2330 train_time:35429ms step_avg:56.96ms
step:623/2330 train_time:35485ms step_avg:56.96ms
step:624/2330 train_time:35543ms step_avg:56.96ms
step:625/2330 train_time:35599ms step_avg:56.96ms
step:626/2330 train_time:35658ms step_avg:56.96ms
step:627/2330 train_time:35713ms step_avg:56.96ms
step:628/2330 train_time:35772ms step_avg:56.96ms
step:629/2330 train_time:35828ms step_avg:56.96ms
step:630/2330 train_time:35887ms step_avg:56.96ms
step:631/2330 train_time:35942ms step_avg:56.96ms
step:632/2330 train_time:36001ms step_avg:56.96ms
step:633/2330 train_time:36058ms step_avg:56.96ms
step:634/2330 train_time:36116ms step_avg:56.97ms
step:635/2330 train_time:36172ms step_avg:56.96ms
step:636/2330 train_time:36230ms step_avg:56.97ms
step:637/2330 train_time:36286ms step_avg:56.96ms
step:638/2330 train_time:36345ms step_avg:56.97ms
step:639/2330 train_time:36401ms step_avg:56.97ms
step:640/2330 train_time:36460ms step_avg:56.97ms
step:641/2330 train_time:36516ms step_avg:56.97ms
step:642/2330 train_time:36575ms step_avg:56.97ms
step:643/2330 train_time:36630ms step_avg:56.97ms
step:644/2330 train_time:36689ms step_avg:56.97ms
step:645/2330 train_time:36745ms step_avg:56.97ms
step:646/2330 train_time:36804ms step_avg:56.97ms
step:647/2330 train_time:36859ms step_avg:56.97ms
step:648/2330 train_time:36919ms step_avg:56.97ms
step:649/2330 train_time:36974ms step_avg:56.97ms
step:650/2330 train_time:37032ms step_avg:56.97ms
step:651/2330 train_time:37088ms step_avg:56.97ms
step:652/2330 train_time:37147ms step_avg:56.97ms
step:653/2330 train_time:37202ms step_avg:56.97ms
step:654/2330 train_time:37262ms step_avg:56.98ms
step:655/2330 train_time:37317ms step_avg:56.97ms
step:656/2330 train_time:37376ms step_avg:56.98ms
step:657/2330 train_time:37432ms step_avg:56.97ms
step:658/2330 train_time:37492ms step_avg:56.98ms
step:659/2330 train_time:37548ms step_avg:56.98ms
step:660/2330 train_time:37606ms step_avg:56.98ms
step:661/2330 train_time:37662ms step_avg:56.98ms
step:662/2330 train_time:37721ms step_avg:56.98ms
step:663/2330 train_time:37777ms step_avg:56.98ms
step:664/2330 train_time:37835ms step_avg:56.98ms
step:665/2330 train_time:37890ms step_avg:56.98ms
step:666/2330 train_time:37949ms step_avg:56.98ms
step:667/2330 train_time:38005ms step_avg:56.98ms
step:668/2330 train_time:38064ms step_avg:56.98ms
step:669/2330 train_time:38120ms step_avg:56.98ms
step:670/2330 train_time:38179ms step_avg:56.98ms
step:671/2330 train_time:38234ms step_avg:56.98ms
step:672/2330 train_time:38292ms step_avg:56.98ms
step:673/2330 train_time:38347ms step_avg:56.98ms
step:674/2330 train_time:38408ms step_avg:56.98ms
step:675/2330 train_time:38463ms step_avg:56.98ms
step:676/2330 train_time:38522ms step_avg:56.99ms
step:677/2330 train_time:38577ms step_avg:56.98ms
step:678/2330 train_time:38636ms step_avg:56.99ms
step:679/2330 train_time:38693ms step_avg:56.98ms
step:680/2330 train_time:38751ms step_avg:56.99ms
step:681/2330 train_time:38806ms step_avg:56.98ms
step:682/2330 train_time:38866ms step_avg:56.99ms
step:683/2330 train_time:38922ms step_avg:56.99ms
step:684/2330 train_time:38981ms step_avg:56.99ms
step:685/2330 train_time:39037ms step_avg:56.99ms
step:686/2330 train_time:39096ms step_avg:56.99ms
step:687/2330 train_time:39151ms step_avg:56.99ms
step:688/2330 train_time:39210ms step_avg:56.99ms
step:689/2330 train_time:39266ms step_avg:56.99ms
step:690/2330 train_time:39324ms step_avg:56.99ms
step:691/2330 train_time:39380ms step_avg:56.99ms
step:692/2330 train_time:39438ms step_avg:56.99ms
step:693/2330 train_time:39494ms step_avg:56.99ms
step:694/2330 train_time:39553ms step_avg:56.99ms
step:695/2330 train_time:39608ms step_avg:56.99ms
step:696/2330 train_time:39668ms step_avg:56.99ms
step:697/2330 train_time:39724ms step_avg:56.99ms
step:698/2330 train_time:39783ms step_avg:57.00ms
step:699/2330 train_time:39838ms step_avg:56.99ms
step:700/2330 train_time:39897ms step_avg:57.00ms
step:701/2330 train_time:39953ms step_avg:56.99ms
step:702/2330 train_time:40012ms step_avg:57.00ms
step:703/2330 train_time:40068ms step_avg:57.00ms
step:704/2330 train_time:40127ms step_avg:57.00ms
step:705/2330 train_time:40182ms step_avg:57.00ms
step:706/2330 train_time:40241ms step_avg:57.00ms
step:707/2330 train_time:40297ms step_avg:57.00ms
step:708/2330 train_time:40355ms step_avg:57.00ms
step:709/2330 train_time:40412ms step_avg:57.00ms
step:710/2330 train_time:40470ms step_avg:57.00ms
step:711/2330 train_time:40526ms step_avg:57.00ms
step:712/2330 train_time:40584ms step_avg:57.00ms
step:713/2330 train_time:40640ms step_avg:57.00ms
step:714/2330 train_time:40699ms step_avg:57.00ms
step:715/2330 train_time:40755ms step_avg:57.00ms
step:716/2330 train_time:40814ms step_avg:57.00ms
step:717/2330 train_time:40869ms step_avg:57.00ms
step:718/2330 train_time:40929ms step_avg:57.00ms
step:719/2330 train_time:40985ms step_avg:57.00ms
step:720/2330 train_time:41044ms step_avg:57.01ms
step:721/2330 train_time:41099ms step_avg:57.00ms
step:722/2330 train_time:41159ms step_avg:57.01ms
step:723/2330 train_time:41214ms step_avg:57.00ms
step:724/2330 train_time:41272ms step_avg:57.01ms
step:725/2330 train_time:41327ms step_avg:57.00ms
step:726/2330 train_time:41386ms step_avg:57.01ms
step:727/2330 train_time:41441ms step_avg:57.00ms
step:728/2330 train_time:41501ms step_avg:57.01ms
step:729/2330 train_time:41557ms step_avg:57.01ms
step:730/2330 train_time:41616ms step_avg:57.01ms
step:731/2330 train_time:41672ms step_avg:57.01ms
step:732/2330 train_time:41730ms step_avg:57.01ms
step:733/2330 train_time:41786ms step_avg:57.01ms
step:734/2330 train_time:41846ms step_avg:57.01ms
step:735/2330 train_time:41902ms step_avg:57.01ms
step:736/2330 train_time:41960ms step_avg:57.01ms
step:737/2330 train_time:42016ms step_avg:57.01ms
step:738/2330 train_time:42074ms step_avg:57.01ms
step:739/2330 train_time:42129ms step_avg:57.01ms
step:740/2330 train_time:42189ms step_avg:57.01ms
step:741/2330 train_time:42245ms step_avg:57.01ms
step:742/2330 train_time:42303ms step_avg:57.01ms
step:743/2330 train_time:42359ms step_avg:57.01ms
step:744/2330 train_time:42419ms step_avg:57.02ms
step:745/2330 train_time:42474ms step_avg:57.01ms
step:746/2330 train_time:42533ms step_avg:57.01ms
step:747/2330 train_time:42588ms step_avg:57.01ms
step:748/2330 train_time:42648ms step_avg:57.02ms
step:749/2330 train_time:42705ms step_avg:57.02ms
step:750/2330 train_time:42763ms step_avg:57.02ms
step:750/2330 val_loss:5.0057 train_time:42843ms step_avg:57.12ms
step:751/2330 train_time:42860ms step_avg:57.07ms
step:752/2330 train_time:42881ms step_avg:57.02ms
step:753/2330 train_time:42937ms step_avg:57.02ms
step:754/2330 train_time:43000ms step_avg:57.03ms
step:755/2330 train_time:43056ms step_avg:57.03ms
step:756/2330 train_time:43118ms step_avg:57.03ms
step:757/2330 train_time:43173ms step_avg:57.03ms
step:758/2330 train_time:43233ms step_avg:57.03ms
step:759/2330 train_time:43288ms step_avg:57.03ms
step:760/2330 train_time:43347ms step_avg:57.04ms
step:761/2330 train_time:43402ms step_avg:57.03ms
step:762/2330 train_time:43460ms step_avg:57.03ms
step:763/2330 train_time:43515ms step_avg:57.03ms
step:764/2330 train_time:43573ms step_avg:57.03ms
step:765/2330 train_time:43630ms step_avg:57.03ms
step:766/2330 train_time:43687ms step_avg:57.03ms
step:767/2330 train_time:43743ms step_avg:57.03ms
step:768/2330 train_time:43802ms step_avg:57.03ms
step:769/2330 train_time:43859ms step_avg:57.03ms
step:770/2330 train_time:43919ms step_avg:57.04ms
step:771/2330 train_time:43976ms step_avg:57.04ms
step:772/2330 train_time:44037ms step_avg:57.04ms
step:773/2330 train_time:44094ms step_avg:57.04ms
step:774/2330 train_time:44155ms step_avg:57.05ms
step:775/2330 train_time:44211ms step_avg:57.05ms
step:776/2330 train_time:44270ms step_avg:57.05ms
step:777/2330 train_time:44327ms step_avg:57.05ms
step:778/2330 train_time:44386ms step_avg:57.05ms
step:779/2330 train_time:44441ms step_avg:57.05ms
step:780/2330 train_time:44501ms step_avg:57.05ms
step:781/2330 train_time:44557ms step_avg:57.05ms
step:782/2330 train_time:44617ms step_avg:57.05ms
step:783/2330 train_time:44673ms step_avg:57.05ms
step:784/2330 train_time:44733ms step_avg:57.06ms
step:785/2330 train_time:44789ms step_avg:57.06ms
step:786/2330 train_time:44849ms step_avg:57.06ms
step:787/2330 train_time:44906ms step_avg:57.06ms
step:788/2330 train_time:44966ms step_avg:57.06ms
step:789/2330 train_time:45023ms step_avg:57.06ms
step:790/2330 train_time:45082ms step_avg:57.07ms
step:791/2330 train_time:45137ms step_avg:57.06ms
step:792/2330 train_time:45198ms step_avg:57.07ms
step:793/2330 train_time:45254ms step_avg:57.07ms
step:794/2330 train_time:45314ms step_avg:57.07ms
step:795/2330 train_time:45371ms step_avg:57.07ms
step:796/2330 train_time:45431ms step_avg:57.07ms
step:797/2330 train_time:45487ms step_avg:57.07ms
step:798/2330 train_time:45547ms step_avg:57.08ms
step:799/2330 train_time:45602ms step_avg:57.07ms
step:800/2330 train_time:45662ms step_avg:57.08ms
step:801/2330 train_time:45718ms step_avg:57.08ms
step:802/2330 train_time:45777ms step_avg:57.08ms
step:803/2330 train_time:45834ms step_avg:57.08ms
step:804/2330 train_time:45894ms step_avg:57.08ms
step:805/2330 train_time:45951ms step_avg:57.08ms
step:806/2330 train_time:46011ms step_avg:57.09ms
step:807/2330 train_time:46068ms step_avg:57.09ms
step:808/2330 train_time:46128ms step_avg:57.09ms
step:809/2330 train_time:46184ms step_avg:57.09ms
step:810/2330 train_time:46244ms step_avg:57.09ms
step:811/2330 train_time:46300ms step_avg:57.09ms
step:812/2330 train_time:46360ms step_avg:57.09ms
step:813/2330 train_time:46416ms step_avg:57.09ms
step:814/2330 train_time:46477ms step_avg:57.10ms
step:815/2330 train_time:46533ms step_avg:57.10ms
step:816/2330 train_time:46592ms step_avg:57.10ms
step:817/2330 train_time:46648ms step_avg:57.10ms
step:818/2330 train_time:46708ms step_avg:57.10ms
step:819/2330 train_time:46764ms step_avg:57.10ms
step:820/2330 train_time:46824ms step_avg:57.10ms
step:821/2330 train_time:46880ms step_avg:57.10ms
step:822/2330 train_time:46939ms step_avg:57.10ms
step:823/2330 train_time:46995ms step_avg:57.10ms
step:824/2330 train_time:47055ms step_avg:57.11ms
step:825/2330 train_time:47112ms step_avg:57.11ms
step:826/2330 train_time:47172ms step_avg:57.11ms
step:827/2330 train_time:47229ms step_avg:57.11ms
step:828/2330 train_time:47289ms step_avg:57.11ms
step:829/2330 train_time:47345ms step_avg:57.11ms
step:830/2330 train_time:47405ms step_avg:57.11ms
step:831/2330 train_time:47460ms step_avg:57.11ms
step:832/2330 train_time:47520ms step_avg:57.11ms
step:833/2330 train_time:47576ms step_avg:57.11ms
step:834/2330 train_time:47635ms step_avg:57.12ms
step:835/2330 train_time:47691ms step_avg:57.11ms
step:836/2330 train_time:47751ms step_avg:57.12ms
step:837/2330 train_time:47808ms step_avg:57.12ms
step:838/2330 train_time:47868ms step_avg:57.12ms
step:839/2330 train_time:47924ms step_avg:57.12ms
step:840/2330 train_time:47983ms step_avg:57.12ms
step:841/2330 train_time:48039ms step_avg:57.12ms
step:842/2330 train_time:48099ms step_avg:57.12ms
step:843/2330 train_time:48155ms step_avg:57.12ms
step:844/2330 train_time:48215ms step_avg:57.13ms
step:845/2330 train_time:48272ms step_avg:57.13ms
step:846/2330 train_time:48332ms step_avg:57.13ms
step:847/2330 train_time:48388ms step_avg:57.13ms
step:848/2330 train_time:48448ms step_avg:57.13ms
step:849/2330 train_time:48505ms step_avg:57.13ms
step:850/2330 train_time:48564ms step_avg:57.13ms
step:851/2330 train_time:48620ms step_avg:57.13ms
step:852/2330 train_time:48679ms step_avg:57.13ms
step:853/2330 train_time:48735ms step_avg:57.13ms
step:854/2330 train_time:48795ms step_avg:57.14ms
step:855/2330 train_time:48852ms step_avg:57.14ms
step:856/2330 train_time:48912ms step_avg:57.14ms
step:857/2330 train_time:48969ms step_avg:57.14ms
step:858/2330 train_time:49028ms step_avg:57.14ms
step:859/2330 train_time:49084ms step_avg:57.14ms
step:860/2330 train_time:49143ms step_avg:57.14ms
step:861/2330 train_time:49199ms step_avg:57.14ms
step:862/2330 train_time:49259ms step_avg:57.15ms
step:863/2330 train_time:49315ms step_avg:57.14ms
step:864/2330 train_time:49375ms step_avg:57.15ms
step:865/2330 train_time:49432ms step_avg:57.15ms
step:866/2330 train_time:49491ms step_avg:57.15ms
step:867/2330 train_time:49548ms step_avg:57.15ms
step:868/2330 train_time:49608ms step_avg:57.15ms
step:869/2330 train_time:49663ms step_avg:57.15ms
step:870/2330 train_time:49723ms step_avg:57.15ms
step:871/2330 train_time:49778ms step_avg:57.15ms
step:872/2330 train_time:49839ms step_avg:57.15ms
step:873/2330 train_time:49895ms step_avg:57.15ms
step:874/2330 train_time:49955ms step_avg:57.16ms
step:875/2330 train_time:50012ms step_avg:57.16ms
step:876/2330 train_time:50072ms step_avg:57.16ms
step:877/2330 train_time:50128ms step_avg:57.16ms
step:878/2330 train_time:50188ms step_avg:57.16ms
step:879/2330 train_time:50244ms step_avg:57.16ms
step:880/2330 train_time:50304ms step_avg:57.16ms
step:881/2330 train_time:50360ms step_avg:57.16ms
step:882/2330 train_time:50419ms step_avg:57.16ms
step:883/2330 train_time:50475ms step_avg:57.16ms
step:884/2330 train_time:50534ms step_avg:57.17ms
step:885/2330 train_time:50590ms step_avg:57.16ms
step:886/2330 train_time:50651ms step_avg:57.17ms
step:887/2330 train_time:50708ms step_avg:57.17ms
step:888/2330 train_time:50768ms step_avg:57.17ms
step:889/2330 train_time:50824ms step_avg:57.17ms
step:890/2330 train_time:50883ms step_avg:57.17ms
step:891/2330 train_time:50939ms step_avg:57.17ms
step:892/2330 train_time:50998ms step_avg:57.17ms
step:893/2330 train_time:51054ms step_avg:57.17ms
step:894/2330 train_time:51114ms step_avg:57.17ms
step:895/2330 train_time:51171ms step_avg:57.17ms
step:896/2330 train_time:51233ms step_avg:57.18ms
step:897/2330 train_time:51289ms step_avg:57.18ms
step:898/2330 train_time:51349ms step_avg:57.18ms
step:899/2330 train_time:51405ms step_avg:57.18ms
step:900/2330 train_time:51465ms step_avg:57.18ms
step:901/2330 train_time:51521ms step_avg:57.18ms
step:902/2330 train_time:51580ms step_avg:57.18ms
step:903/2330 train_time:51636ms step_avg:57.18ms
step:904/2330 train_time:51697ms step_avg:57.19ms
step:905/2330 train_time:51753ms step_avg:57.19ms
step:906/2330 train_time:51813ms step_avg:57.19ms
step:907/2330 train_time:51870ms step_avg:57.19ms
step:908/2330 train_time:51929ms step_avg:57.19ms
step:909/2330 train_time:51985ms step_avg:57.19ms
step:910/2330 train_time:52044ms step_avg:57.19ms
step:911/2330 train_time:52100ms step_avg:57.19ms
step:912/2330 train_time:52160ms step_avg:57.19ms
step:913/2330 train_time:52216ms step_avg:57.19ms
step:914/2330 train_time:52276ms step_avg:57.20ms
step:915/2330 train_time:52333ms step_avg:57.19ms
step:916/2330 train_time:52393ms step_avg:57.20ms
step:917/2330 train_time:52449ms step_avg:57.20ms
step:918/2330 train_time:52510ms step_avg:57.20ms
step:919/2330 train_time:52566ms step_avg:57.20ms
step:920/2330 train_time:52625ms step_avg:57.20ms
step:921/2330 train_time:52681ms step_avg:57.20ms
step:922/2330 train_time:52741ms step_avg:57.20ms
step:923/2330 train_time:52797ms step_avg:57.20ms
step:924/2330 train_time:52857ms step_avg:57.20ms
step:925/2330 train_time:52913ms step_avg:57.20ms
step:926/2330 train_time:52973ms step_avg:57.21ms
step:927/2330 train_time:53030ms step_avg:57.21ms
step:928/2330 train_time:53090ms step_avg:57.21ms
step:929/2330 train_time:53146ms step_avg:57.21ms
step:930/2330 train_time:53205ms step_avg:57.21ms
step:931/2330 train_time:53262ms step_avg:57.21ms
step:932/2330 train_time:53321ms step_avg:57.21ms
step:933/2330 train_time:53376ms step_avg:57.21ms
step:934/2330 train_time:53437ms step_avg:57.21ms
step:935/2330 train_time:53493ms step_avg:57.21ms
step:936/2330 train_time:53553ms step_avg:57.21ms
step:937/2330 train_time:53610ms step_avg:57.21ms
step:938/2330 train_time:53669ms step_avg:57.22ms
step:939/2330 train_time:53725ms step_avg:57.22ms
step:940/2330 train_time:53785ms step_avg:57.22ms
step:941/2330 train_time:53840ms step_avg:57.22ms
step:942/2330 train_time:53899ms step_avg:57.22ms
step:943/2330 train_time:53955ms step_avg:57.22ms
step:944/2330 train_time:54015ms step_avg:57.22ms
step:945/2330 train_time:54072ms step_avg:57.22ms
step:946/2330 train_time:54133ms step_avg:57.22ms
step:947/2330 train_time:54189ms step_avg:57.22ms
step:948/2330 train_time:54249ms step_avg:57.22ms
step:949/2330 train_time:54305ms step_avg:57.22ms
step:950/2330 train_time:54364ms step_avg:57.23ms
step:951/2330 train_time:54420ms step_avg:57.22ms
step:952/2330 train_time:54480ms step_avg:57.23ms
step:953/2330 train_time:54536ms step_avg:57.23ms
step:954/2330 train_time:54596ms step_avg:57.23ms
step:955/2330 train_time:54652ms step_avg:57.23ms
step:956/2330 train_time:54713ms step_avg:57.23ms
step:957/2330 train_time:54769ms step_avg:57.23ms
step:958/2330 train_time:54829ms step_avg:57.23ms
step:959/2330 train_time:54885ms step_avg:57.23ms
step:960/2330 train_time:54944ms step_avg:57.23ms
step:961/2330 train_time:55000ms step_avg:57.23ms
step:962/2330 train_time:55060ms step_avg:57.24ms
step:963/2330 train_time:55116ms step_avg:57.23ms
step:964/2330 train_time:55177ms step_avg:57.24ms
step:965/2330 train_time:55234ms step_avg:57.24ms
step:966/2330 train_time:55294ms step_avg:57.24ms
step:967/2330 train_time:55351ms step_avg:57.24ms
step:968/2330 train_time:55410ms step_avg:57.24ms
step:969/2330 train_time:55466ms step_avg:57.24ms
step:970/2330 train_time:55525ms step_avg:57.24ms
step:971/2330 train_time:55581ms step_avg:57.24ms
step:972/2330 train_time:55641ms step_avg:57.24ms
step:973/2330 train_time:55697ms step_avg:57.24ms
step:974/2330 train_time:55757ms step_avg:57.24ms
step:975/2330 train_time:55813ms step_avg:57.24ms
step:976/2330 train_time:55873ms step_avg:57.25ms
step:977/2330 train_time:55930ms step_avg:57.25ms
step:978/2330 train_time:55990ms step_avg:57.25ms
step:979/2330 train_time:56045ms step_avg:57.25ms
step:980/2330 train_time:56105ms step_avg:57.25ms
step:981/2330 train_time:56161ms step_avg:57.25ms
step:982/2330 train_time:56221ms step_avg:57.25ms
step:983/2330 train_time:56277ms step_avg:57.25ms
step:984/2330 train_time:56338ms step_avg:57.25ms
step:985/2330 train_time:56394ms step_avg:57.25ms
step:986/2330 train_time:56454ms step_avg:57.26ms
step:987/2330 train_time:56510ms step_avg:57.25ms
step:988/2330 train_time:56570ms step_avg:57.26ms
step:989/2330 train_time:56627ms step_avg:57.26ms
step:990/2330 train_time:56687ms step_avg:57.26ms
step:991/2330 train_time:56743ms step_avg:57.26ms
step:992/2330 train_time:56802ms step_avg:57.26ms
step:993/2330 train_time:56858ms step_avg:57.26ms
step:994/2330 train_time:56918ms step_avg:57.26ms
step:995/2330 train_time:56974ms step_avg:57.26ms
step:996/2330 train_time:57035ms step_avg:57.26ms
step:997/2330 train_time:57091ms step_avg:57.26ms
step:998/2330 train_time:57151ms step_avg:57.27ms
step:999/2330 train_time:57208ms step_avg:57.26ms
step:1000/2330 train_time:57268ms step_avg:57.27ms
step:1000/2330 val_loss:4.6849 train_time:57348ms step_avg:57.35ms
step:1001/2330 train_time:57364ms step_avg:57.31ms
step:1002/2330 train_time:57385ms step_avg:57.27ms
step:1003/2330 train_time:57439ms step_avg:57.27ms
step:1004/2330 train_time:57508ms step_avg:57.28ms
step:1005/2330 train_time:57563ms step_avg:57.28ms
step:1006/2330 train_time:57627ms step_avg:57.28ms
step:1007/2330 train_time:57683ms step_avg:57.28ms
step:1008/2330 train_time:57745ms step_avg:57.29ms
step:1009/2330 train_time:57800ms step_avg:57.28ms
step:1010/2330 train_time:57860ms step_avg:57.29ms
step:1011/2330 train_time:57916ms step_avg:57.29ms
step:1012/2330 train_time:57976ms step_avg:57.29ms
step:1013/2330 train_time:58032ms step_avg:57.29ms
step:1014/2330 train_time:58090ms step_avg:57.29ms
step:1015/2330 train_time:58146ms step_avg:57.29ms
step:1016/2330 train_time:58204ms step_avg:57.29ms
step:1017/2330 train_time:58260ms step_avg:57.29ms
step:1018/2330 train_time:58321ms step_avg:57.29ms
step:1019/2330 train_time:58378ms step_avg:57.29ms
step:1020/2330 train_time:58439ms step_avg:57.29ms
step:1021/2330 train_time:58495ms step_avg:57.29ms
step:1022/2330 train_time:58555ms step_avg:57.29ms
step:1023/2330 train_time:58612ms step_avg:57.29ms
step:1024/2330 train_time:58673ms step_avg:57.30ms
step:1025/2330 train_time:58729ms step_avg:57.30ms
step:1026/2330 train_time:58788ms step_avg:57.30ms
step:1027/2330 train_time:58843ms step_avg:57.30ms
step:1028/2330 train_time:58904ms step_avg:57.30ms
step:1029/2330 train_time:58959ms step_avg:57.30ms
step:1030/2330 train_time:59020ms step_avg:57.30ms
step:1031/2330 train_time:59076ms step_avg:57.30ms
step:1032/2330 train_time:59136ms step_avg:57.30ms
step:1033/2330 train_time:59193ms step_avg:57.30ms
step:1034/2330 train_time:59252ms step_avg:57.30ms
step:1035/2330 train_time:59308ms step_avg:57.30ms
step:1036/2330 train_time:59367ms step_avg:57.30ms
step:1037/2330 train_time:59423ms step_avg:57.30ms
step:1038/2330 train_time:59483ms step_avg:57.31ms
step:1039/2330 train_time:59541ms step_avg:57.31ms
step:1040/2330 train_time:59601ms step_avg:57.31ms
step:1041/2330 train_time:59658ms step_avg:57.31ms
step:1042/2330 train_time:59719ms step_avg:57.31ms
step:1043/2330 train_time:59775ms step_avg:57.31ms
step:1044/2330 train_time:59835ms step_avg:57.31ms
step:1045/2330 train_time:59891ms step_avg:57.31ms
step:1046/2330 train_time:59950ms step_avg:57.31ms
step:1047/2330 train_time:60006ms step_avg:57.31ms
step:1048/2330 train_time:60066ms step_avg:57.31ms
step:1049/2330 train_time:60121ms step_avg:57.31ms
step:1050/2330 train_time:60181ms step_avg:57.32ms
step:1051/2330 train_time:60238ms step_avg:57.31ms
step:1052/2330 train_time:60297ms step_avg:57.32ms
step:1053/2330 train_time:60353ms step_avg:57.32ms
step:1054/2330 train_time:60413ms step_avg:57.32ms
step:1055/2330 train_time:60470ms step_avg:57.32ms
step:1056/2330 train_time:60530ms step_avg:57.32ms
step:1057/2330 train_time:60586ms step_avg:57.32ms
step:1058/2330 train_time:60646ms step_avg:57.32ms
step:1059/2330 train_time:60702ms step_avg:57.32ms
step:1060/2330 train_time:60762ms step_avg:57.32ms
step:1061/2330 train_time:60819ms step_avg:57.32ms
step:1062/2330 train_time:60879ms step_avg:57.32ms
step:1063/2330 train_time:60936ms step_avg:57.32ms
step:1064/2330 train_time:60995ms step_avg:57.33ms
step:1065/2330 train_time:61050ms step_avg:57.32ms
step:1066/2330 train_time:61110ms step_avg:57.33ms
step:1067/2330 train_time:61165ms step_avg:57.32ms
step:1068/2330 train_time:61225ms step_avg:57.33ms
step:1069/2330 train_time:61281ms step_avg:57.33ms
step:1070/2330 train_time:61341ms step_avg:57.33ms
step:1071/2330 train_time:61397ms step_avg:57.33ms
step:1072/2330 train_time:61457ms step_avg:57.33ms
step:1073/2330 train_time:61513ms step_avg:57.33ms
step:1074/2330 train_time:61573ms step_avg:57.33ms
step:1075/2330 train_time:61630ms step_avg:57.33ms
step:1076/2330 train_time:61690ms step_avg:57.33ms
step:1077/2330 train_time:61746ms step_avg:57.33ms
step:1078/2330 train_time:61806ms step_avg:57.33ms
step:1079/2330 train_time:61862ms step_avg:57.33ms
step:1080/2330 train_time:61921ms step_avg:57.33ms
step:1081/2330 train_time:61978ms step_avg:57.33ms
step:1082/2330 train_time:62038ms step_avg:57.34ms
step:1083/2330 train_time:62094ms step_avg:57.34ms
step:1084/2330 train_time:62153ms step_avg:57.34ms
step:1085/2330 train_time:62209ms step_avg:57.34ms
step:1086/2330 train_time:62269ms step_avg:57.34ms
step:1087/2330 train_time:62324ms step_avg:57.34ms
step:1088/2330 train_time:62385ms step_avg:57.34ms
step:1089/2330 train_time:62441ms step_avg:57.34ms
step:1090/2330 train_time:62501ms step_avg:57.34ms
step:1091/2330 train_time:62557ms step_avg:57.34ms
step:1092/2330 train_time:62618ms step_avg:57.34ms
step:1093/2330 train_time:62674ms step_avg:57.34ms
step:1094/2330 train_time:62736ms step_avg:57.35ms
step:1095/2330 train_time:62792ms step_avg:57.34ms
step:1096/2330 train_time:62853ms step_avg:57.35ms
step:1097/2330 train_time:62909ms step_avg:57.35ms
step:1098/2330 train_time:62967ms step_avg:57.35ms
step:1099/2330 train_time:63023ms step_avg:57.35ms
step:1100/2330 train_time:63084ms step_avg:57.35ms
step:1101/2330 train_time:63140ms step_avg:57.35ms
step:1102/2330 train_time:63200ms step_avg:57.35ms
step:1103/2330 train_time:63256ms step_avg:57.35ms
step:1104/2330 train_time:63316ms step_avg:57.35ms
step:1105/2330 train_time:63373ms step_avg:57.35ms
step:1106/2330 train_time:63432ms step_avg:57.35ms
step:1107/2330 train_time:63488ms step_avg:57.35ms
step:1108/2330 train_time:63547ms step_avg:57.35ms
step:1109/2330 train_time:63604ms step_avg:57.35ms
step:1110/2330 train_time:63665ms step_avg:57.36ms
step:1111/2330 train_time:63721ms step_avg:57.35ms
step:1112/2330 train_time:63781ms step_avg:57.36ms
step:1113/2330 train_time:63837ms step_avg:57.36ms
step:1114/2330 train_time:63897ms step_avg:57.36ms
step:1115/2330 train_time:63953ms step_avg:57.36ms
step:1116/2330 train_time:64013ms step_avg:57.36ms
step:1117/2330 train_time:64070ms step_avg:57.36ms
step:1118/2330 train_time:64128ms step_avg:57.36ms
step:1119/2330 train_time:64184ms step_avg:57.36ms
step:1120/2330 train_time:64243ms step_avg:57.36ms
step:1121/2330 train_time:64301ms step_avg:57.36ms
step:1122/2330 train_time:64361ms step_avg:57.36ms
step:1123/2330 train_time:64417ms step_avg:57.36ms
step:1124/2330 train_time:64477ms step_avg:57.36ms
step:1125/2330 train_time:64533ms step_avg:57.36ms
step:1126/2330 train_time:64594ms step_avg:57.37ms
step:1127/2330 train_time:64650ms step_avg:57.36ms
step:1128/2330 train_time:64710ms step_avg:57.37ms
step:1129/2330 train_time:64765ms step_avg:57.37ms
step:1130/2330 train_time:64825ms step_avg:57.37ms
step:1131/2330 train_time:64881ms step_avg:57.37ms
step:1132/2330 train_time:64942ms step_avg:57.37ms
step:1133/2330 train_time:64998ms step_avg:57.37ms
step:1134/2330 train_time:65059ms step_avg:57.37ms
step:1135/2330 train_time:65115ms step_avg:57.37ms
step:1136/2330 train_time:65175ms step_avg:57.37ms
step:1137/2330 train_time:65231ms step_avg:57.37ms
step:1138/2330 train_time:65290ms step_avg:57.37ms
step:1139/2330 train_time:65346ms step_avg:57.37ms
step:1140/2330 train_time:65406ms step_avg:57.37ms
step:1141/2330 train_time:65462ms step_avg:57.37ms
step:1142/2330 train_time:65522ms step_avg:57.37ms
step:1143/2330 train_time:65579ms step_avg:57.37ms
step:1144/2330 train_time:65638ms step_avg:57.38ms
step:1145/2330 train_time:65695ms step_avg:57.38ms
step:1146/2330 train_time:65755ms step_avg:57.38ms
step:1147/2330 train_time:65811ms step_avg:57.38ms
step:1148/2330 train_time:65871ms step_avg:57.38ms
step:1149/2330 train_time:65928ms step_avg:57.38ms
step:1150/2330 train_time:65987ms step_avg:57.38ms
step:1151/2330 train_time:66042ms step_avg:57.38ms
step:1152/2330 train_time:66102ms step_avg:57.38ms
step:1153/2330 train_time:66158ms step_avg:57.38ms
step:1154/2330 train_time:66219ms step_avg:57.38ms
step:1155/2330 train_time:66275ms step_avg:57.38ms
step:1156/2330 train_time:66335ms step_avg:57.38ms
step:1157/2330 train_time:66392ms step_avg:57.38ms
step:1158/2330 train_time:66450ms step_avg:57.38ms
step:1159/2330 train_time:66508ms step_avg:57.38ms
step:1160/2330 train_time:66567ms step_avg:57.39ms
step:1161/2330 train_time:66623ms step_avg:57.38ms
step:1162/2330 train_time:66683ms step_avg:57.39ms
step:1163/2330 train_time:66739ms step_avg:57.39ms
step:1164/2330 train_time:66800ms step_avg:57.39ms
step:1165/2330 train_time:66856ms step_avg:57.39ms
step:1166/2330 train_time:66916ms step_avg:57.39ms
step:1167/2330 train_time:66972ms step_avg:57.39ms
step:1168/2330 train_time:67033ms step_avg:57.39ms
step:1169/2330 train_time:67089ms step_avg:57.39ms
step:1170/2330 train_time:67148ms step_avg:57.39ms
step:1171/2330 train_time:67204ms step_avg:57.39ms
step:1172/2330 train_time:67264ms step_avg:57.39ms
step:1173/2330 train_time:67321ms step_avg:57.39ms
step:1174/2330 train_time:67380ms step_avg:57.39ms
step:1175/2330 train_time:67437ms step_avg:57.39ms
step:1176/2330 train_time:67497ms step_avg:57.40ms
step:1177/2330 train_time:67554ms step_avg:57.39ms
step:1178/2330 train_time:67612ms step_avg:57.40ms
step:1179/2330 train_time:67668ms step_avg:57.39ms
step:1180/2330 train_time:67727ms step_avg:57.40ms
step:1181/2330 train_time:67784ms step_avg:57.40ms
step:1182/2330 train_time:67842ms step_avg:57.40ms
step:1183/2330 train_time:67899ms step_avg:57.40ms
step:1184/2330 train_time:67959ms step_avg:57.40ms
step:1185/2330 train_time:68016ms step_avg:57.40ms
step:1186/2330 train_time:68076ms step_avg:57.40ms
step:1187/2330 train_time:68132ms step_avg:57.40ms
step:1188/2330 train_time:68192ms step_avg:57.40ms
step:1189/2330 train_time:68248ms step_avg:57.40ms
step:1190/2330 train_time:68307ms step_avg:57.40ms
step:1191/2330 train_time:68363ms step_avg:57.40ms
step:1192/2330 train_time:68423ms step_avg:57.40ms
step:1193/2330 train_time:68479ms step_avg:57.40ms
step:1194/2330 train_time:68539ms step_avg:57.40ms
step:1195/2330 train_time:68596ms step_avg:57.40ms
step:1196/2330 train_time:68655ms step_avg:57.40ms
step:1197/2330 train_time:68712ms step_avg:57.40ms
step:1198/2330 train_time:68770ms step_avg:57.40ms
step:1199/2330 train_time:68827ms step_avg:57.40ms
step:1200/2330 train_time:68886ms step_avg:57.40ms
step:1201/2330 train_time:68941ms step_avg:57.40ms
step:1202/2330 train_time:69002ms step_avg:57.41ms
step:1203/2330 train_time:69058ms step_avg:57.40ms
step:1204/2330 train_time:69119ms step_avg:57.41ms
step:1205/2330 train_time:69175ms step_avg:57.41ms
step:1206/2330 train_time:69236ms step_avg:57.41ms
step:1207/2330 train_time:69292ms step_avg:57.41ms
step:1208/2330 train_time:69352ms step_avg:57.41ms
step:1209/2330 train_time:69408ms step_avg:57.41ms
step:1210/2330 train_time:69467ms step_avg:57.41ms
step:1211/2330 train_time:69523ms step_avg:57.41ms
step:1212/2330 train_time:69583ms step_avg:57.41ms
step:1213/2330 train_time:69639ms step_avg:57.41ms
step:1214/2330 train_time:69700ms step_avg:57.41ms
step:1215/2330 train_time:69757ms step_avg:57.41ms
step:1216/2330 train_time:69816ms step_avg:57.41ms
step:1217/2330 train_time:69872ms step_avg:57.41ms
step:1218/2330 train_time:69932ms step_avg:57.42ms
step:1219/2330 train_time:69988ms step_avg:57.41ms
step:1220/2330 train_time:70047ms step_avg:57.42ms
step:1221/2330 train_time:70103ms step_avg:57.41ms
step:1222/2330 train_time:70163ms step_avg:57.42ms
step:1223/2330 train_time:70219ms step_avg:57.42ms
step:1224/2330 train_time:70279ms step_avg:57.42ms
step:1225/2330 train_time:70336ms step_avg:57.42ms
step:1226/2330 train_time:70396ms step_avg:57.42ms
step:1227/2330 train_time:70451ms step_avg:57.42ms
step:1228/2330 train_time:70511ms step_avg:57.42ms
step:1229/2330 train_time:70567ms step_avg:57.42ms
step:1230/2330 train_time:70627ms step_avg:57.42ms
step:1231/2330 train_time:70683ms step_avg:57.42ms
step:1232/2330 train_time:70742ms step_avg:57.42ms
step:1233/2330 train_time:70799ms step_avg:57.42ms
step:1234/2330 train_time:70859ms step_avg:57.42ms
step:1235/2330 train_time:70915ms step_avg:57.42ms
step:1236/2330 train_time:70975ms step_avg:57.42ms
step:1237/2330 train_time:71031ms step_avg:57.42ms
step:1238/2330 train_time:71092ms step_avg:57.42ms
step:1239/2330 train_time:71148ms step_avg:57.42ms
step:1240/2330 train_time:71207ms step_avg:57.43ms
step:1241/2330 train_time:71263ms step_avg:57.42ms
step:1242/2330 train_time:71323ms step_avg:57.43ms
step:1243/2330 train_time:71379ms step_avg:57.43ms
step:1244/2330 train_time:71439ms step_avg:57.43ms
step:1245/2330 train_time:71495ms step_avg:57.43ms
step:1246/2330 train_time:71555ms step_avg:57.43ms
step:1247/2330 train_time:71611ms step_avg:57.43ms
step:1248/2330 train_time:71671ms step_avg:57.43ms
step:1249/2330 train_time:71728ms step_avg:57.43ms
step:1250/2330 train_time:71786ms step_avg:57.43ms
step:1250/2330 val_loss:4.4938 train_time:71866ms step_avg:57.49ms
step:1251/2330 train_time:71884ms step_avg:57.46ms
step:1252/2330 train_time:71905ms step_avg:57.43ms
step:1253/2330 train_time:71963ms step_avg:57.43ms
step:1254/2330 train_time:72028ms step_avg:57.44ms
step:1255/2330 train_time:72084ms step_avg:57.44ms
step:1256/2330 train_time:72147ms step_avg:57.44ms
step:1257/2330 train_time:72203ms step_avg:57.44ms
step:1258/2330 train_time:72263ms step_avg:57.44ms
step:1259/2330 train_time:72319ms step_avg:57.44ms
step:1260/2330 train_time:72379ms step_avg:57.44ms
step:1261/2330 train_time:72435ms step_avg:57.44ms
step:1262/2330 train_time:72494ms step_avg:57.44ms
step:1263/2330 train_time:72550ms step_avg:57.44ms
step:1264/2330 train_time:72608ms step_avg:57.44ms
step:1265/2330 train_time:72664ms step_avg:57.44ms
step:1266/2330 train_time:72723ms step_avg:57.44ms
step:1267/2330 train_time:72778ms step_avg:57.44ms
step:1268/2330 train_time:72838ms step_avg:57.44ms
step:1269/2330 train_time:72895ms step_avg:57.44ms
step:1270/2330 train_time:72956ms step_avg:57.45ms
step:1271/2330 train_time:73014ms step_avg:57.45ms
step:1272/2330 train_time:73075ms step_avg:57.45ms
step:1273/2330 train_time:73132ms step_avg:57.45ms
step:1274/2330 train_time:73192ms step_avg:57.45ms
step:1275/2330 train_time:73248ms step_avg:57.45ms
step:1276/2330 train_time:73308ms step_avg:57.45ms
step:1277/2330 train_time:73364ms step_avg:57.45ms
step:1278/2330 train_time:73423ms step_avg:57.45ms
step:1279/2330 train_time:73479ms step_avg:57.45ms
step:1280/2330 train_time:73539ms step_avg:57.45ms
step:1281/2330 train_time:73596ms step_avg:57.45ms
step:1282/2330 train_time:73655ms step_avg:57.45ms
step:1283/2330 train_time:73711ms step_avg:57.45ms
step:1284/2330 train_time:73769ms step_avg:57.45ms
step:1285/2330 train_time:73826ms step_avg:57.45ms
step:1286/2330 train_time:73885ms step_avg:57.45ms
step:1287/2330 train_time:73942ms step_avg:57.45ms
step:1288/2330 train_time:74002ms step_avg:57.45ms
step:1289/2330 train_time:74059ms step_avg:57.45ms
step:1290/2330 train_time:74119ms step_avg:57.46ms
step:1291/2330 train_time:74176ms step_avg:57.46ms
step:1292/2330 train_time:74236ms step_avg:57.46ms
step:1293/2330 train_time:74293ms step_avg:57.46ms
step:1294/2330 train_time:74352ms step_avg:57.46ms
step:1295/2330 train_time:74408ms step_avg:57.46ms
step:1296/2330 train_time:74468ms step_avg:57.46ms
step:1297/2330 train_time:74523ms step_avg:57.46ms
step:1298/2330 train_time:74582ms step_avg:57.46ms
step:1299/2330 train_time:74638ms step_avg:57.46ms
step:1300/2330 train_time:74697ms step_avg:57.46ms
step:1301/2330 train_time:74753ms step_avg:57.46ms
step:1302/2330 train_time:74813ms step_avg:57.46ms
step:1303/2330 train_time:74869ms step_avg:57.46ms
step:1304/2330 train_time:74929ms step_avg:57.46ms
step:1305/2330 train_time:74985ms step_avg:57.46ms
step:1306/2330 train_time:75045ms step_avg:57.46ms
step:1307/2330 train_time:75102ms step_avg:57.46ms
step:1308/2330 train_time:75162ms step_avg:57.46ms
step:1309/2330 train_time:75219ms step_avg:57.46ms
step:1310/2330 train_time:75278ms step_avg:57.46ms
step:1311/2330 train_time:75335ms step_avg:57.46ms
step:1312/2330 train_time:75395ms step_avg:57.47ms
step:1313/2330 train_time:75451ms step_avg:57.46ms
step:1314/2330 train_time:75510ms step_avg:57.47ms
step:1315/2330 train_time:75567ms step_avg:57.47ms
step:1316/2330 train_time:75625ms step_avg:57.47ms
step:1317/2330 train_time:75681ms step_avg:57.46ms
step:1318/2330 train_time:75740ms step_avg:57.47ms
step:1319/2330 train_time:75796ms step_avg:57.46ms
step:1320/2330 train_time:75857ms step_avg:57.47ms
step:1321/2330 train_time:75913ms step_avg:57.47ms
step:1322/2330 train_time:75974ms step_avg:57.47ms
step:1323/2330 train_time:76031ms step_avg:57.47ms
step:1324/2330 train_time:76090ms step_avg:57.47ms
step:1325/2330 train_time:76147ms step_avg:57.47ms
step:1326/2330 train_time:76206ms step_avg:57.47ms
step:1327/2330 train_time:76262ms step_avg:57.47ms
step:1328/2330 train_time:76322ms step_avg:57.47ms
step:1329/2330 train_time:76379ms step_avg:57.47ms
step:1330/2330 train_time:76438ms step_avg:57.47ms
step:1331/2330 train_time:76495ms step_avg:57.47ms
step:1332/2330 train_time:76554ms step_avg:57.47ms
step:1333/2330 train_time:76611ms step_avg:57.47ms
step:1334/2330 train_time:76670ms step_avg:57.47ms
step:1335/2330 train_time:76726ms step_avg:57.47ms
step:1336/2330 train_time:76784ms step_avg:57.47ms
step:1337/2330 train_time:76840ms step_avg:57.47ms
step:1338/2330 train_time:76900ms step_avg:57.47ms
step:1339/2330 train_time:76956ms step_avg:57.47ms
step:1340/2330 train_time:77016ms step_avg:57.47ms
step:1341/2330 train_time:77073ms step_avg:57.47ms
step:1342/2330 train_time:77133ms step_avg:57.48ms
step:1343/2330 train_time:77190ms step_avg:57.48ms
step:1344/2330 train_time:77249ms step_avg:57.48ms
step:1345/2330 train_time:77306ms step_avg:57.48ms
step:1346/2330 train_time:77365ms step_avg:57.48ms
step:1347/2330 train_time:77421ms step_avg:57.48ms
step:1348/2330 train_time:77481ms step_avg:57.48ms
step:1349/2330 train_time:77537ms step_avg:57.48ms
step:1350/2330 train_time:77598ms step_avg:57.48ms
step:1351/2330 train_time:77654ms step_avg:57.48ms
step:1352/2330 train_time:77713ms step_avg:57.48ms
step:1353/2330 train_time:77770ms step_avg:57.48ms
step:1354/2330 train_time:77829ms step_avg:57.48ms
step:1355/2330 train_time:77885ms step_avg:57.48ms
step:1356/2330 train_time:77944ms step_avg:57.48ms
step:1357/2330 train_time:78000ms step_avg:57.48ms
step:1358/2330 train_time:78060ms step_avg:57.48ms
step:1359/2330 train_time:78117ms step_avg:57.48ms
step:1360/2330 train_time:78177ms step_avg:57.48ms
step:1361/2330 train_time:78234ms step_avg:57.48ms
step:1362/2330 train_time:78293ms step_avg:57.48ms
step:1363/2330 train_time:78349ms step_avg:57.48ms
step:1364/2330 train_time:78409ms step_avg:57.48ms
step:1365/2330 train_time:78465ms step_avg:57.48ms
step:1366/2330 train_time:78525ms step_avg:57.49ms
step:1367/2330 train_time:78580ms step_avg:57.48ms
step:1368/2330 train_time:78641ms step_avg:57.49ms
step:1369/2330 train_time:78697ms step_avg:57.49ms
step:1370/2330 train_time:78758ms step_avg:57.49ms
step:1371/2330 train_time:78814ms step_avg:57.49ms
step:1372/2330 train_time:78874ms step_avg:57.49ms
step:1373/2330 train_time:78931ms step_avg:57.49ms
step:1374/2330 train_time:78990ms step_avg:57.49ms
step:1375/2330 train_time:79046ms step_avg:57.49ms
step:1376/2330 train_time:79105ms step_avg:57.49ms
step:1377/2330 train_time:79162ms step_avg:57.49ms
step:1378/2330 train_time:79221ms step_avg:57.49ms
step:1379/2330 train_time:79278ms step_avg:57.49ms
step:1380/2330 train_time:79338ms step_avg:57.49ms
step:1381/2330 train_time:79395ms step_avg:57.49ms
step:1382/2330 train_time:79455ms step_avg:57.49ms
step:1383/2330 train_time:79511ms step_avg:57.49ms
step:1384/2330 train_time:79571ms step_avg:57.49ms
step:1385/2330 train_time:79627ms step_avg:57.49ms
step:1386/2330 train_time:79686ms step_avg:57.49ms
step:1387/2330 train_time:79742ms step_avg:57.49ms
step:1388/2330 train_time:79802ms step_avg:57.49ms
step:1389/2330 train_time:79859ms step_avg:57.49ms
step:1390/2330 train_time:79918ms step_avg:57.49ms
step:1391/2330 train_time:79975ms step_avg:57.49ms
step:1392/2330 train_time:80034ms step_avg:57.50ms
step:1393/2330 train_time:80090ms step_avg:57.49ms
step:1394/2330 train_time:80150ms step_avg:57.50ms
step:1395/2330 train_time:80206ms step_avg:57.50ms
step:1396/2330 train_time:80265ms step_avg:57.50ms
step:1397/2330 train_time:80321ms step_avg:57.50ms
step:1398/2330 train_time:80381ms step_avg:57.50ms
step:1399/2330 train_time:80437ms step_avg:57.50ms
step:1400/2330 train_time:80497ms step_avg:57.50ms
step:1401/2330 train_time:80555ms step_avg:57.50ms
step:1402/2330 train_time:80614ms step_avg:57.50ms
step:1403/2330 train_time:80671ms step_avg:57.50ms
step:1404/2330 train_time:80730ms step_avg:57.50ms
step:1405/2330 train_time:80785ms step_avg:57.50ms
step:1406/2330 train_time:80845ms step_avg:57.50ms
step:1407/2330 train_time:80900ms step_avg:57.50ms
step:1408/2330 train_time:80960ms step_avg:57.50ms
step:1409/2330 train_time:81016ms step_avg:57.50ms
step:1410/2330 train_time:81076ms step_avg:57.50ms
step:1411/2330 train_time:81132ms step_avg:57.50ms
step:1412/2330 train_time:81192ms step_avg:57.50ms
step:1413/2330 train_time:81249ms step_avg:57.50ms
step:1414/2330 train_time:81308ms step_avg:57.50ms
step:1415/2330 train_time:81365ms step_avg:57.50ms
step:1416/2330 train_time:81424ms step_avg:57.50ms
step:1417/2330 train_time:81480ms step_avg:57.50ms
step:1418/2330 train_time:81540ms step_avg:57.50ms
step:1419/2330 train_time:81596ms step_avg:57.50ms
step:1420/2330 train_time:81658ms step_avg:57.51ms
step:1421/2330 train_time:81715ms step_avg:57.51ms
step:1422/2330 train_time:81775ms step_avg:57.51ms
step:1423/2330 train_time:81831ms step_avg:57.51ms
step:1424/2330 train_time:81890ms step_avg:57.51ms
step:1425/2330 train_time:81945ms step_avg:57.51ms
step:1426/2330 train_time:82005ms step_avg:57.51ms
step:1427/2330 train_time:82061ms step_avg:57.51ms
step:1428/2330 train_time:82121ms step_avg:57.51ms
step:1429/2330 train_time:82178ms step_avg:57.51ms
step:1430/2330 train_time:82237ms step_avg:57.51ms
step:1431/2330 train_time:82294ms step_avg:57.51ms
step:1432/2330 train_time:82353ms step_avg:57.51ms
step:1433/2330 train_time:82409ms step_avg:57.51ms
step:1434/2330 train_time:82469ms step_avg:57.51ms
step:1435/2330 train_time:82525ms step_avg:57.51ms
step:1436/2330 train_time:82585ms step_avg:57.51ms
step:1437/2330 train_time:82641ms step_avg:57.51ms
step:1438/2330 train_time:82701ms step_avg:57.51ms
step:1439/2330 train_time:82757ms step_avg:57.51ms
step:1440/2330 train_time:82818ms step_avg:57.51ms
step:1441/2330 train_time:82875ms step_avg:57.51ms
step:1442/2330 train_time:82934ms step_avg:57.51ms
step:1443/2330 train_time:82990ms step_avg:57.51ms
step:1444/2330 train_time:83050ms step_avg:57.51ms
step:1445/2330 train_time:83107ms step_avg:57.51ms
step:1446/2330 train_time:83166ms step_avg:57.51ms
step:1447/2330 train_time:83222ms step_avg:57.51ms
step:1448/2330 train_time:83281ms step_avg:57.51ms
step:1449/2330 train_time:83338ms step_avg:57.51ms
step:1450/2330 train_time:83398ms step_avg:57.52ms
step:1451/2330 train_time:83455ms step_avg:57.52ms
step:1452/2330 train_time:83514ms step_avg:57.52ms
step:1453/2330 train_time:83569ms step_avg:57.52ms
step:1454/2330 train_time:83629ms step_avg:57.52ms
step:1455/2330 train_time:83685ms step_avg:57.52ms
step:1456/2330 train_time:83745ms step_avg:57.52ms
step:1457/2330 train_time:83801ms step_avg:57.52ms
step:1458/2330 train_time:83861ms step_avg:57.52ms
step:1459/2330 train_time:83917ms step_avg:57.52ms
step:1460/2330 train_time:83978ms step_avg:57.52ms
step:1461/2330 train_time:84035ms step_avg:57.52ms
step:1462/2330 train_time:84094ms step_avg:57.52ms
step:1463/2330 train_time:84151ms step_avg:57.52ms
step:1464/2330 train_time:84211ms step_avg:57.52ms
step:1465/2330 train_time:84267ms step_avg:57.52ms
step:1466/2330 train_time:84327ms step_avg:57.52ms
step:1467/2330 train_time:84383ms step_avg:57.52ms
step:1468/2330 train_time:84442ms step_avg:57.52ms
step:1469/2330 train_time:84499ms step_avg:57.52ms
step:1470/2330 train_time:84559ms step_avg:57.52ms
step:1471/2330 train_time:84616ms step_avg:57.52ms
step:1472/2330 train_time:84675ms step_avg:57.52ms
step:1473/2330 train_time:84732ms step_avg:57.52ms
step:1474/2330 train_time:84791ms step_avg:57.52ms
step:1475/2330 train_time:84847ms step_avg:57.52ms
step:1476/2330 train_time:84907ms step_avg:57.53ms
step:1477/2330 train_time:84963ms step_avg:57.52ms
step:1478/2330 train_time:85022ms step_avg:57.53ms
step:1479/2330 train_time:85079ms step_avg:57.52ms
step:1480/2330 train_time:85138ms step_avg:57.53ms
step:1481/2330 train_time:85195ms step_avg:57.53ms
step:1482/2330 train_time:85254ms step_avg:57.53ms
step:1483/2330 train_time:85311ms step_avg:57.53ms
step:1484/2330 train_time:85370ms step_avg:57.53ms
step:1485/2330 train_time:85426ms step_avg:57.53ms
step:1486/2330 train_time:85485ms step_avg:57.53ms
step:1487/2330 train_time:85541ms step_avg:57.53ms
step:1488/2330 train_time:85601ms step_avg:57.53ms
step:1489/2330 train_time:85657ms step_avg:57.53ms
step:1490/2330 train_time:85717ms step_avg:57.53ms
step:1491/2330 train_time:85774ms step_avg:57.53ms
step:1492/2330 train_time:85834ms step_avg:57.53ms
step:1493/2330 train_time:85890ms step_avg:57.53ms
step:1494/2330 train_time:85949ms step_avg:57.53ms
step:1495/2330 train_time:86006ms step_avg:57.53ms
step:1496/2330 train_time:86065ms step_avg:57.53ms
step:1497/2330 train_time:86121ms step_avg:57.53ms
step:1498/2330 train_time:86181ms step_avg:57.53ms
step:1499/2330 train_time:86238ms step_avg:57.53ms
step:1500/2330 train_time:86297ms step_avg:57.53ms
step:1500/2330 val_loss:4.3596 train_time:86378ms step_avg:57.59ms
step:1501/2330 train_time:86397ms step_avg:57.56ms
step:1502/2330 train_time:86417ms step_avg:57.53ms
step:1503/2330 train_time:86475ms step_avg:57.53ms
step:1504/2330 train_time:86536ms step_avg:57.54ms
step:1505/2330 train_time:86594ms step_avg:57.54ms
step:1506/2330 train_time:86656ms step_avg:57.54ms
step:1507/2330 train_time:86712ms step_avg:57.54ms
step:1508/2330 train_time:86772ms step_avg:57.54ms
step:1509/2330 train_time:86828ms step_avg:57.54ms
step:1510/2330 train_time:86887ms step_avg:57.54ms
step:1511/2330 train_time:86942ms step_avg:57.54ms
step:1512/2330 train_time:87002ms step_avg:57.54ms
step:1513/2330 train_time:87057ms step_avg:57.54ms
step:1514/2330 train_time:87116ms step_avg:57.54ms
step:1515/2330 train_time:87172ms step_avg:57.54ms
step:1516/2330 train_time:87231ms step_avg:57.54ms
step:1517/2330 train_time:87287ms step_avg:57.54ms
step:1518/2330 train_time:87346ms step_avg:57.54ms
step:1519/2330 train_time:87403ms step_avg:57.54ms
step:1520/2330 train_time:87464ms step_avg:57.54ms
step:1521/2330 train_time:87521ms step_avg:57.54ms
step:1522/2330 train_time:87585ms step_avg:57.55ms
step:1523/2330 train_time:87642ms step_avg:57.55ms
step:1524/2330 train_time:87702ms step_avg:57.55ms
step:1525/2330 train_time:87759ms step_avg:57.55ms
step:1526/2330 train_time:87818ms step_avg:57.55ms
step:1527/2330 train_time:87874ms step_avg:57.55ms
step:1528/2330 train_time:87933ms step_avg:57.55ms
step:1529/2330 train_time:87991ms step_avg:57.55ms
step:1530/2330 train_time:88048ms step_avg:57.55ms
step:1531/2330 train_time:88104ms step_avg:57.55ms
step:1532/2330 train_time:88164ms step_avg:57.55ms
step:1533/2330 train_time:88220ms step_avg:57.55ms
step:1534/2330 train_time:88282ms step_avg:57.55ms
step:1535/2330 train_time:88338ms step_avg:57.55ms
step:1536/2330 train_time:88399ms step_avg:57.55ms
step:1537/2330 train_time:88455ms step_avg:57.55ms
step:1538/2330 train_time:88517ms step_avg:57.55ms
step:1539/2330 train_time:88574ms step_avg:57.55ms
step:1540/2330 train_time:88635ms step_avg:57.56ms
step:1541/2330 train_time:88692ms step_avg:57.55ms
step:1542/2330 train_time:88752ms step_avg:57.56ms
step:1543/2330 train_time:88808ms step_avg:57.56ms
step:1544/2330 train_time:88869ms step_avg:57.56ms
step:1545/2330 train_time:88926ms step_avg:57.56ms
step:1546/2330 train_time:88986ms step_avg:57.56ms
step:1547/2330 train_time:89043ms step_avg:57.56ms
step:1548/2330 train_time:89102ms step_avg:57.56ms
step:1549/2330 train_time:89159ms step_avg:57.56ms
step:1550/2330 train_time:89219ms step_avg:57.56ms
step:1551/2330 train_time:89275ms step_avg:57.56ms
step:1552/2330 train_time:89335ms step_avg:57.56ms
step:1553/2330 train_time:89392ms step_avg:57.56ms
step:1554/2330 train_time:89452ms step_avg:57.56ms
step:1555/2330 train_time:89509ms step_avg:57.56ms
step:1556/2330 train_time:89570ms step_avg:57.56ms
step:1557/2330 train_time:89628ms step_avg:57.56ms
step:1558/2330 train_time:89688ms step_avg:57.57ms
step:1559/2330 train_time:89744ms step_avg:57.57ms
step:1560/2330 train_time:89805ms step_avg:57.57ms
step:1561/2330 train_time:89862ms step_avg:57.57ms
step:1562/2330 train_time:89922ms step_avg:57.57ms
step:1563/2330 train_time:89979ms step_avg:57.57ms
step:1564/2330 train_time:90038ms step_avg:57.57ms
step:1565/2330 train_time:90094ms step_avg:57.57ms
step:1566/2330 train_time:90155ms step_avg:57.57ms
step:1567/2330 train_time:90211ms step_avg:57.57ms
step:1568/2330 train_time:90270ms step_avg:57.57ms
step:1569/2330 train_time:90327ms step_avg:57.57ms
step:1570/2330 train_time:90388ms step_avg:57.57ms
step:1571/2330 train_time:90444ms step_avg:57.57ms
step:1572/2330 train_time:90504ms step_avg:57.57ms
step:1573/2330 train_time:90561ms step_avg:57.57ms
step:1574/2330 train_time:90621ms step_avg:57.57ms
step:1575/2330 train_time:90678ms step_avg:57.57ms
step:1576/2330 train_time:90739ms step_avg:57.58ms
step:1577/2330 train_time:90796ms step_avg:57.58ms
step:1578/2330 train_time:90856ms step_avg:57.58ms
step:1579/2330 train_time:90913ms step_avg:57.58ms
step:1580/2330 train_time:90972ms step_avg:57.58ms
step:1581/2330 train_time:91028ms step_avg:57.58ms
step:1582/2330 train_time:91089ms step_avg:57.58ms
step:1583/2330 train_time:91145ms step_avg:57.58ms
step:1584/2330 train_time:91206ms step_avg:57.58ms
step:1585/2330 train_time:91263ms step_avg:57.58ms
step:1586/2330 train_time:91322ms step_avg:57.58ms
step:1587/2330 train_time:91379ms step_avg:57.58ms
step:1588/2330 train_time:91439ms step_avg:57.58ms
step:1589/2330 train_time:91496ms step_avg:57.58ms
step:1590/2330 train_time:91556ms step_avg:57.58ms
step:1591/2330 train_time:91613ms step_avg:57.58ms
step:1592/2330 train_time:91673ms step_avg:57.58ms
step:1593/2330 train_time:91730ms step_avg:57.58ms
step:1594/2330 train_time:91789ms step_avg:57.58ms
step:1595/2330 train_time:91846ms step_avg:57.58ms
step:1596/2330 train_time:91907ms step_avg:57.59ms
step:1597/2330 train_time:91963ms step_avg:57.58ms
step:1598/2330 train_time:92023ms step_avg:57.59ms
step:1599/2330 train_time:92080ms step_avg:57.59ms
step:1600/2330 train_time:92141ms step_avg:57.59ms
step:1601/2330 train_time:92198ms step_avg:57.59ms
step:1602/2330 train_time:92258ms step_avg:57.59ms
step:1603/2330 train_time:92314ms step_avg:57.59ms
step:1604/2330 train_time:92374ms step_avg:57.59ms
step:1605/2330 train_time:92431ms step_avg:57.59ms
step:1606/2330 train_time:92491ms step_avg:57.59ms
step:1607/2330 train_time:92547ms step_avg:57.59ms
step:1608/2330 train_time:92607ms step_avg:57.59ms
step:1609/2330 train_time:92664ms step_avg:57.59ms
step:1610/2330 train_time:92725ms step_avg:57.59ms
step:1611/2330 train_time:92782ms step_avg:57.59ms
step:1612/2330 train_time:92842ms step_avg:57.59ms
step:1613/2330 train_time:92899ms step_avg:57.59ms
step:1614/2330 train_time:92959ms step_avg:57.60ms
step:1615/2330 train_time:93016ms step_avg:57.60ms
step:1616/2330 train_time:93076ms step_avg:57.60ms
step:1617/2330 train_time:93133ms step_avg:57.60ms
step:1618/2330 train_time:93193ms step_avg:57.60ms
step:1619/2330 train_time:93250ms step_avg:57.60ms
step:1620/2330 train_time:93309ms step_avg:57.60ms
step:1621/2330 train_time:93365ms step_avg:57.60ms
step:1622/2330 train_time:93426ms step_avg:57.60ms
step:1623/2330 train_time:93483ms step_avg:57.60ms
step:1624/2330 train_time:93543ms step_avg:57.60ms
step:1625/2330 train_time:93600ms step_avg:57.60ms
step:1626/2330 train_time:93660ms step_avg:57.60ms
step:1627/2330 train_time:93717ms step_avg:57.60ms
step:1628/2330 train_time:93777ms step_avg:57.60ms
step:1629/2330 train_time:93835ms step_avg:57.60ms
step:1630/2330 train_time:93894ms step_avg:57.60ms
step:1631/2330 train_time:93951ms step_avg:57.60ms
step:1632/2330 train_time:94010ms step_avg:57.60ms
step:1633/2330 train_time:94067ms step_avg:57.60ms
step:1634/2330 train_time:94127ms step_avg:57.61ms
step:1635/2330 train_time:94184ms step_avg:57.61ms
step:1636/2330 train_time:94244ms step_avg:57.61ms
step:1637/2330 train_time:94301ms step_avg:57.61ms
step:1638/2330 train_time:94361ms step_avg:57.61ms
step:1639/2330 train_time:94418ms step_avg:57.61ms
step:1640/2330 train_time:94478ms step_avg:57.61ms
step:1641/2330 train_time:94535ms step_avg:57.61ms
step:1642/2330 train_time:94595ms step_avg:57.61ms
step:1643/2330 train_time:94650ms step_avg:57.61ms
step:1644/2330 train_time:94712ms step_avg:57.61ms
step:1645/2330 train_time:94769ms step_avg:57.61ms
step:1646/2330 train_time:94829ms step_avg:57.61ms
step:1647/2330 train_time:94886ms step_avg:57.61ms
step:1648/2330 train_time:94945ms step_avg:57.61ms
step:1649/2330 train_time:95003ms step_avg:57.61ms
step:1650/2330 train_time:95062ms step_avg:57.61ms
step:1651/2330 train_time:95119ms step_avg:57.61ms
step:1652/2330 train_time:95181ms step_avg:57.62ms
step:1653/2330 train_time:95238ms step_avg:57.62ms
step:1654/2330 train_time:95298ms step_avg:57.62ms
step:1655/2330 train_time:95355ms step_avg:57.62ms
step:1656/2330 train_time:95415ms step_avg:57.62ms
step:1657/2330 train_time:95472ms step_avg:57.62ms
step:1658/2330 train_time:95531ms step_avg:57.62ms
step:1659/2330 train_time:95588ms step_avg:57.62ms
step:1660/2330 train_time:95647ms step_avg:57.62ms
step:1661/2330 train_time:95704ms step_avg:57.62ms
step:1662/2330 train_time:95764ms step_avg:57.62ms
step:1663/2330 train_time:95820ms step_avg:57.62ms
step:1664/2330 train_time:95882ms step_avg:57.62ms
step:1665/2330 train_time:95939ms step_avg:57.62ms
step:1666/2330 train_time:96000ms step_avg:57.62ms
step:1667/2330 train_time:96057ms step_avg:57.62ms
step:1668/2330 train_time:96116ms step_avg:57.62ms
step:1669/2330 train_time:96173ms step_avg:57.62ms
step:1670/2330 train_time:96232ms step_avg:57.62ms
step:1671/2330 train_time:96289ms step_avg:57.62ms
step:1672/2330 train_time:96350ms step_avg:57.63ms
step:1673/2330 train_time:96407ms step_avg:57.63ms
step:1674/2330 train_time:96467ms step_avg:57.63ms
step:1675/2330 train_time:96523ms step_avg:57.63ms
step:1676/2330 train_time:96584ms step_avg:57.63ms
step:1677/2330 train_time:96641ms step_avg:57.63ms
step:1678/2330 train_time:96701ms step_avg:57.63ms
step:1679/2330 train_time:96757ms step_avg:57.63ms
step:1680/2330 train_time:96818ms step_avg:57.63ms
step:1681/2330 train_time:96874ms step_avg:57.63ms
step:1682/2330 train_time:96935ms step_avg:57.63ms
step:1683/2330 train_time:96991ms step_avg:57.63ms
step:1684/2330 train_time:97051ms step_avg:57.63ms
step:1685/2330 train_time:97108ms step_avg:57.63ms
step:1686/2330 train_time:97168ms step_avg:57.63ms
step:1687/2330 train_time:97225ms step_avg:57.63ms
step:1688/2330 train_time:97286ms step_avg:57.63ms
step:1689/2330 train_time:97343ms step_avg:57.63ms
step:1690/2330 train_time:97403ms step_avg:57.63ms
step:1691/2330 train_time:97460ms step_avg:57.63ms
step:1692/2330 train_time:97520ms step_avg:57.64ms
step:1693/2330 train_time:97577ms step_avg:57.64ms
step:1694/2330 train_time:97636ms step_avg:57.64ms
step:1695/2330 train_time:97693ms step_avg:57.64ms
step:1696/2330 train_time:97753ms step_avg:57.64ms
step:1697/2330 train_time:97810ms step_avg:57.64ms
step:1698/2330 train_time:97870ms step_avg:57.64ms
step:1699/2330 train_time:97927ms step_avg:57.64ms
step:1700/2330 train_time:97988ms step_avg:57.64ms
step:1701/2330 train_time:98043ms step_avg:57.64ms
step:1702/2330 train_time:98105ms step_avg:57.64ms
step:1703/2330 train_time:98162ms step_avg:57.64ms
step:1704/2330 train_time:98222ms step_avg:57.64ms
step:1705/2330 train_time:98280ms step_avg:57.64ms
step:1706/2330 train_time:98339ms step_avg:57.64ms
step:1707/2330 train_time:98396ms step_avg:57.64ms
step:1708/2330 train_time:98457ms step_avg:57.64ms
step:1709/2330 train_time:98513ms step_avg:57.64ms
step:1710/2330 train_time:98573ms step_avg:57.65ms
step:1711/2330 train_time:98630ms step_avg:57.64ms
step:1712/2330 train_time:98690ms step_avg:57.65ms
step:1713/2330 train_time:98747ms step_avg:57.65ms
step:1714/2330 train_time:98808ms step_avg:57.65ms
step:1715/2330 train_time:98865ms step_avg:57.65ms
step:1716/2330 train_time:98925ms step_avg:57.65ms
step:1717/2330 train_time:98982ms step_avg:57.65ms
step:1718/2330 train_time:99042ms step_avg:57.65ms
step:1719/2330 train_time:99099ms step_avg:57.65ms
step:1720/2330 train_time:99158ms step_avg:57.65ms
step:1721/2330 train_time:99214ms step_avg:57.65ms
step:1722/2330 train_time:99274ms step_avg:57.65ms
step:1723/2330 train_time:99331ms step_avg:57.65ms
step:1724/2330 train_time:99391ms step_avg:57.65ms
step:1725/2330 train_time:99447ms step_avg:57.65ms
step:1726/2330 train_time:99508ms step_avg:57.65ms
step:1727/2330 train_time:99565ms step_avg:57.65ms
step:1728/2330 train_time:99625ms step_avg:57.65ms
step:1729/2330 train_time:99682ms step_avg:57.65ms
step:1730/2330 train_time:99743ms step_avg:57.65ms
step:1731/2330 train_time:99800ms step_avg:57.65ms
step:1732/2330 train_time:99860ms step_avg:57.66ms
step:1733/2330 train_time:99917ms step_avg:57.66ms
step:1734/2330 train_time:99978ms step_avg:57.66ms
step:1735/2330 train_time:100034ms step_avg:57.66ms
step:1736/2330 train_time:100094ms step_avg:57.66ms
step:1737/2330 train_time:100151ms step_avg:57.66ms
step:1738/2330 train_time:100210ms step_avg:57.66ms
step:1739/2330 train_time:100267ms step_avg:57.66ms
step:1740/2330 train_time:100327ms step_avg:57.66ms
step:1741/2330 train_time:100384ms step_avg:57.66ms
step:1742/2330 train_time:100444ms step_avg:57.66ms
step:1743/2330 train_time:100501ms step_avg:57.66ms
step:1744/2330 train_time:100561ms step_avg:57.66ms
step:1745/2330 train_time:100618ms step_avg:57.66ms
step:1746/2330 train_time:100678ms step_avg:57.66ms
step:1747/2330 train_time:100735ms step_avg:57.66ms
step:1748/2330 train_time:100795ms step_avg:57.66ms
step:1749/2330 train_time:100852ms step_avg:57.66ms
step:1750/2330 train_time:100911ms step_avg:57.66ms
step:1750/2330 val_loss:4.2527 train_time:100992ms step_avg:57.71ms
step:1751/2330 train_time:101011ms step_avg:57.69ms
step:1752/2330 train_time:101031ms step_avg:57.67ms
step:1753/2330 train_time:101086ms step_avg:57.66ms
step:1754/2330 train_time:101152ms step_avg:57.67ms
step:1755/2330 train_time:101207ms step_avg:57.67ms
step:1756/2330 train_time:101272ms step_avg:57.67ms
step:1757/2330 train_time:101328ms step_avg:57.67ms
step:1758/2330 train_time:101387ms step_avg:57.67ms
step:1759/2330 train_time:101443ms step_avg:57.67ms
step:1760/2330 train_time:101502ms step_avg:57.67ms
step:1761/2330 train_time:101558ms step_avg:57.67ms
step:1762/2330 train_time:101618ms step_avg:57.67ms
step:1763/2330 train_time:101674ms step_avg:57.67ms
step:1764/2330 train_time:101733ms step_avg:57.67ms
step:1765/2330 train_time:101789ms step_avg:57.67ms
step:1766/2330 train_time:101848ms step_avg:57.67ms
step:1767/2330 train_time:101908ms step_avg:57.67ms
step:1768/2330 train_time:101969ms step_avg:57.67ms
step:1769/2330 train_time:102027ms step_avg:57.68ms
step:1770/2330 train_time:102087ms step_avg:57.68ms
step:1771/2330 train_time:102144ms step_avg:57.68ms
step:1772/2330 train_time:102204ms step_avg:57.68ms
step:1773/2330 train_time:102261ms step_avg:57.68ms
step:1774/2330 train_time:102323ms step_avg:57.68ms
step:1775/2330 train_time:102380ms step_avg:57.68ms
step:1776/2330 train_time:102439ms step_avg:57.68ms
step:1777/2330 train_time:102496ms step_avg:57.68ms
step:1778/2330 train_time:102556ms step_avg:57.68ms
step:1779/2330 train_time:102613ms step_avg:57.68ms
step:1780/2330 train_time:102672ms step_avg:57.68ms
step:1781/2330 train_time:102728ms step_avg:57.68ms
step:1782/2330 train_time:102787ms step_avg:57.68ms
step:1783/2330 train_time:102844ms step_avg:57.68ms
step:1784/2330 train_time:102905ms step_avg:57.68ms
step:1785/2330 train_time:102962ms step_avg:57.68ms
step:1786/2330 train_time:103024ms step_avg:57.68ms
step:1787/2330 train_time:103081ms step_avg:57.68ms
step:1788/2330 train_time:103141ms step_avg:57.69ms
step:1789/2330 train_time:103199ms step_avg:57.69ms
step:1790/2330 train_time:103259ms step_avg:57.69ms
step:1791/2330 train_time:103316ms step_avg:57.69ms
step:1792/2330 train_time:103377ms step_avg:57.69ms
step:1793/2330 train_time:103433ms step_avg:57.69ms
step:1794/2330 train_time:103493ms step_avg:57.69ms
step:1795/2330 train_time:103549ms step_avg:57.69ms
step:1796/2330 train_time:103609ms step_avg:57.69ms
step:1797/2330 train_time:103665ms step_avg:57.69ms
step:1798/2330 train_time:103725ms step_avg:57.69ms
step:1799/2330 train_time:103781ms step_avg:57.69ms
step:1800/2330 train_time:103841ms step_avg:57.69ms
step:1801/2330 train_time:103899ms step_avg:57.69ms
step:1802/2330 train_time:103960ms step_avg:57.69ms
step:1803/2330 train_time:104018ms step_avg:57.69ms
step:1804/2330 train_time:104078ms step_avg:57.69ms
step:1805/2330 train_time:104135ms step_avg:57.69ms
step:1806/2330 train_time:104196ms step_avg:57.69ms
step:1807/2330 train_time:104253ms step_avg:57.69ms
step:1808/2330 train_time:104313ms step_avg:57.70ms
step:1809/2330 train_time:104369ms step_avg:57.69ms
step:1810/2330 train_time:104429ms step_avg:57.70ms
step:1811/2330 train_time:104486ms step_avg:57.70ms
step:1812/2330 train_time:104545ms step_avg:57.70ms
step:1813/2330 train_time:104602ms step_avg:57.70ms
step:1814/2330 train_time:104662ms step_avg:57.70ms
step:1815/2330 train_time:104718ms step_avg:57.70ms
step:1816/2330 train_time:104778ms step_avg:57.70ms
step:1817/2330 train_time:104835ms step_avg:57.70ms
step:1818/2330 train_time:104894ms step_avg:57.70ms
step:1819/2330 train_time:104950ms step_avg:57.70ms
step:1820/2330 train_time:105012ms step_avg:57.70ms
step:1821/2330 train_time:105069ms step_avg:57.70ms
step:1822/2330 train_time:105129ms step_avg:57.70ms
step:1823/2330 train_time:105186ms step_avg:57.70ms
step:1824/2330 train_time:105246ms step_avg:57.70ms
step:1825/2330 train_time:105303ms step_avg:57.70ms
step:1826/2330 train_time:105363ms step_avg:57.70ms
step:1827/2330 train_time:105420ms step_avg:57.70ms
step:1828/2330 train_time:105480ms step_avg:57.70ms
step:1829/2330 train_time:105537ms step_avg:57.70ms
step:1830/2330 train_time:105598ms step_avg:57.70ms
step:1831/2330 train_time:105653ms step_avg:57.70ms
step:1832/2330 train_time:105715ms step_avg:57.70ms
step:1833/2330 train_time:105772ms step_avg:57.70ms
step:1834/2330 train_time:105832ms step_avg:57.71ms
step:1835/2330 train_time:105889ms step_avg:57.70ms
step:1836/2330 train_time:105949ms step_avg:57.71ms
step:1837/2330 train_time:106005ms step_avg:57.71ms
step:1838/2330 train_time:106065ms step_avg:57.71ms
step:1839/2330 train_time:106122ms step_avg:57.71ms
step:1840/2330 train_time:106183ms step_avg:57.71ms
step:1841/2330 train_time:106240ms step_avg:57.71ms
step:1842/2330 train_time:106301ms step_avg:57.71ms
step:1843/2330 train_time:106357ms step_avg:57.71ms
step:1844/2330 train_time:106417ms step_avg:57.71ms
step:1845/2330 train_time:106474ms step_avg:57.71ms
step:1846/2330 train_time:106533ms step_avg:57.71ms
step:1847/2330 train_time:106590ms step_avg:57.71ms
step:1848/2330 train_time:106650ms step_avg:57.71ms
step:1849/2330 train_time:106706ms step_avg:57.71ms
step:1850/2330 train_time:106767ms step_avg:57.71ms
step:1851/2330 train_time:106824ms step_avg:57.71ms
step:1852/2330 train_time:106883ms step_avg:57.71ms
step:1853/2330 train_time:106940ms step_avg:57.71ms
step:1854/2330 train_time:107000ms step_avg:57.71ms
step:1855/2330 train_time:107056ms step_avg:57.71ms
step:1856/2330 train_time:107116ms step_avg:57.71ms
step:1857/2330 train_time:107173ms step_avg:57.71ms
step:1858/2330 train_time:107233ms step_avg:57.71ms
step:1859/2330 train_time:107290ms step_avg:57.71ms
step:1860/2330 train_time:107349ms step_avg:57.71ms
step:1861/2330 train_time:107406ms step_avg:57.71ms
step:1862/2330 train_time:107467ms step_avg:57.72ms
step:1863/2330 train_time:107524ms step_avg:57.72ms
step:1864/2330 train_time:107583ms step_avg:57.72ms
step:1865/2330 train_time:107640ms step_avg:57.72ms
step:1866/2330 train_time:107701ms step_avg:57.72ms
step:1867/2330 train_time:107758ms step_avg:57.72ms
step:1868/2330 train_time:107818ms step_avg:57.72ms
step:1869/2330 train_time:107875ms step_avg:57.72ms
step:1870/2330 train_time:107935ms step_avg:57.72ms
step:1871/2330 train_time:107991ms step_avg:57.72ms
step:1872/2330 train_time:108051ms step_avg:57.72ms
step:1873/2330 train_time:108108ms step_avg:57.72ms
step:1874/2330 train_time:108167ms step_avg:57.72ms
step:1875/2330 train_time:108225ms step_avg:57.72ms
step:1876/2330 train_time:108284ms step_avg:57.72ms
step:1877/2330 train_time:108341ms step_avg:57.72ms
step:1878/2330 train_time:108400ms step_avg:57.72ms
step:1879/2330 train_time:108457ms step_avg:57.72ms
step:1880/2330 train_time:108517ms step_avg:57.72ms
step:1881/2330 train_time:108573ms step_avg:57.72ms
step:1882/2330 train_time:108635ms step_avg:57.72ms
step:1883/2330 train_time:108691ms step_avg:57.72ms
step:1884/2330 train_time:108751ms step_avg:57.72ms
step:1885/2330 train_time:108808ms step_avg:57.72ms
step:1886/2330 train_time:108867ms step_avg:57.72ms
step:1887/2330 train_time:108924ms step_avg:57.72ms
step:1888/2330 train_time:108984ms step_avg:57.72ms
step:1889/2330 train_time:109041ms step_avg:57.72ms
step:1890/2330 train_time:109102ms step_avg:57.73ms
step:1891/2330 train_time:109158ms step_avg:57.73ms
step:1892/2330 train_time:109219ms step_avg:57.73ms
step:1893/2330 train_time:109276ms step_avg:57.73ms
step:1894/2330 train_time:109336ms step_avg:57.73ms
step:1895/2330 train_time:109392ms step_avg:57.73ms
step:1896/2330 train_time:109452ms step_avg:57.73ms
step:1897/2330 train_time:109508ms step_avg:57.73ms
step:1898/2330 train_time:109569ms step_avg:57.73ms
step:1899/2330 train_time:109625ms step_avg:57.73ms
step:1900/2330 train_time:109686ms step_avg:57.73ms
step:1901/2330 train_time:109743ms step_avg:57.73ms
step:1902/2330 train_time:109804ms step_avg:57.73ms
step:1903/2330 train_time:109860ms step_avg:57.73ms
step:1904/2330 train_time:109920ms step_avg:57.73ms
step:1905/2330 train_time:109977ms step_avg:57.73ms
step:1906/2330 train_time:110038ms step_avg:57.73ms
step:1907/2330 train_time:110094ms step_avg:57.73ms
step:1908/2330 train_time:110154ms step_avg:57.73ms
step:1909/2330 train_time:110210ms step_avg:57.73ms
step:1910/2330 train_time:110271ms step_avg:57.73ms
step:1911/2330 train_time:110328ms step_avg:57.73ms
step:1912/2330 train_time:110387ms step_avg:57.73ms
step:1913/2330 train_time:110444ms step_avg:57.73ms
step:1914/2330 train_time:110504ms step_avg:57.73ms
step:1915/2330 train_time:110561ms step_avg:57.73ms
step:1916/2330 train_time:110622ms step_avg:57.74ms
step:1917/2330 train_time:110679ms step_avg:57.74ms
step:1918/2330 train_time:110739ms step_avg:57.74ms
step:1919/2330 train_time:110796ms step_avg:57.74ms
step:1920/2330 train_time:110855ms step_avg:57.74ms
step:1921/2330 train_time:110913ms step_avg:57.74ms
step:1922/2330 train_time:110972ms step_avg:57.74ms
step:1923/2330 train_time:111030ms step_avg:57.74ms
step:1924/2330 train_time:111090ms step_avg:57.74ms
step:1925/2330 train_time:111146ms step_avg:57.74ms
step:1926/2330 train_time:111207ms step_avg:57.74ms
step:1927/2330 train_time:111264ms step_avg:57.74ms
step:1928/2330 train_time:111324ms step_avg:57.74ms
step:1929/2330 train_time:111382ms step_avg:57.74ms
step:1930/2330 train_time:111442ms step_avg:57.74ms
step:1931/2330 train_time:111499ms step_avg:57.74ms
step:1932/2330 train_time:111559ms step_avg:57.74ms
step:1933/2330 train_time:111617ms step_avg:57.74ms
step:1934/2330 train_time:111676ms step_avg:57.74ms
step:1935/2330 train_time:111733ms step_avg:57.74ms
step:1936/2330 train_time:111793ms step_avg:57.74ms
step:1937/2330 train_time:111850ms step_avg:57.74ms
step:1938/2330 train_time:111909ms step_avg:57.74ms
step:1939/2330 train_time:111965ms step_avg:57.74ms
step:1940/2330 train_time:112027ms step_avg:57.75ms
step:1941/2330 train_time:112084ms step_avg:57.75ms
step:1942/2330 train_time:112143ms step_avg:57.75ms
step:1943/2330 train_time:112200ms step_avg:57.75ms
step:1944/2330 train_time:112260ms step_avg:57.75ms
step:1945/2330 train_time:112317ms step_avg:57.75ms
step:1946/2330 train_time:112379ms step_avg:57.75ms
step:1947/2330 train_time:112435ms step_avg:57.75ms
step:1948/2330 train_time:112495ms step_avg:57.75ms
step:1949/2330 train_time:112551ms step_avg:57.75ms
step:1950/2330 train_time:112611ms step_avg:57.75ms
step:1951/2330 train_time:112667ms step_avg:57.75ms
step:1952/2330 train_time:112728ms step_avg:57.75ms
step:1953/2330 train_time:112784ms step_avg:57.75ms
step:1954/2330 train_time:112844ms step_avg:57.75ms
step:1955/2330 train_time:112901ms step_avg:57.75ms
step:1956/2330 train_time:112961ms step_avg:57.75ms
step:1957/2330 train_time:113019ms step_avg:57.75ms
step:1958/2330 train_time:113078ms step_avg:57.75ms
step:1959/2330 train_time:113136ms step_avg:57.75ms
step:1960/2330 train_time:113196ms step_avg:57.75ms
step:1961/2330 train_time:113252ms step_avg:57.75ms
step:1962/2330 train_time:113313ms step_avg:57.75ms
step:1963/2330 train_time:113369ms step_avg:57.75ms
step:1964/2330 train_time:113429ms step_avg:57.75ms
step:1965/2330 train_time:113486ms step_avg:57.75ms
step:1966/2330 train_time:113546ms step_avg:57.75ms
step:1967/2330 train_time:113603ms step_avg:57.75ms
step:1968/2330 train_time:113663ms step_avg:57.76ms
step:1969/2330 train_time:113720ms step_avg:57.76ms
step:1970/2330 train_time:113779ms step_avg:57.76ms
step:1971/2330 train_time:113836ms step_avg:57.76ms
step:1972/2330 train_time:113896ms step_avg:57.76ms
step:1973/2330 train_time:113952ms step_avg:57.76ms
step:1974/2330 train_time:114013ms step_avg:57.76ms
step:1975/2330 train_time:114069ms step_avg:57.76ms
step:1976/2330 train_time:114129ms step_avg:57.76ms
step:1977/2330 train_time:114186ms step_avg:57.76ms
step:1978/2330 train_time:114247ms step_avg:57.76ms
step:1979/2330 train_time:114303ms step_avg:57.76ms
step:1980/2330 train_time:114363ms step_avg:57.76ms
step:1981/2330 train_time:114421ms step_avg:57.76ms
step:1982/2330 train_time:114481ms step_avg:57.76ms
step:1983/2330 train_time:114538ms step_avg:57.76ms
step:1984/2330 train_time:114598ms step_avg:57.76ms
step:1985/2330 train_time:114654ms step_avg:57.76ms
step:1986/2330 train_time:114714ms step_avg:57.76ms
step:1987/2330 train_time:114771ms step_avg:57.76ms
step:1988/2330 train_time:114830ms step_avg:57.76ms
step:1989/2330 train_time:114887ms step_avg:57.76ms
step:1990/2330 train_time:114947ms step_avg:57.76ms
step:1991/2330 train_time:115005ms step_avg:57.76ms
step:1992/2330 train_time:115065ms step_avg:57.76ms
step:1993/2330 train_time:115122ms step_avg:57.76ms
step:1994/2330 train_time:115182ms step_avg:57.76ms
step:1995/2330 train_time:115239ms step_avg:57.76ms
step:1996/2330 train_time:115300ms step_avg:57.77ms
step:1997/2330 train_time:115356ms step_avg:57.76ms
step:1998/2330 train_time:115418ms step_avg:57.77ms
step:1999/2330 train_time:115474ms step_avg:57.77ms
step:2000/2330 train_time:115535ms step_avg:57.77ms
step:2000/2330 val_loss:4.1819 train_time:115615ms step_avg:57.81ms
step:2001/2330 train_time:115633ms step_avg:57.79ms
step:2002/2330 train_time:115655ms step_avg:57.77ms
step:2003/2330 train_time:115713ms step_avg:57.77ms
step:2004/2330 train_time:115774ms step_avg:57.77ms
step:2005/2330 train_time:115831ms step_avg:57.77ms
step:2006/2330 train_time:115891ms step_avg:57.77ms
step:2007/2330 train_time:115947ms step_avg:57.77ms
step:2008/2330 train_time:116008ms step_avg:57.77ms
step:2009/2330 train_time:116064ms step_avg:57.77ms
step:2010/2330 train_time:116123ms step_avg:57.77ms
step:2011/2330 train_time:116179ms step_avg:57.77ms
step:2012/2330 train_time:116239ms step_avg:57.77ms
step:2013/2330 train_time:116295ms step_avg:57.77ms
step:2014/2330 train_time:116354ms step_avg:57.77ms
step:2015/2330 train_time:116411ms step_avg:57.77ms
step:2016/2330 train_time:116469ms step_avg:57.77ms
step:2017/2330 train_time:116526ms step_avg:57.77ms
step:2018/2330 train_time:116587ms step_avg:57.77ms
step:2019/2330 train_time:116645ms step_avg:57.77ms
step:2020/2330 train_time:116706ms step_avg:57.78ms
step:2021/2330 train_time:116765ms step_avg:57.78ms
step:2022/2330 train_time:116826ms step_avg:57.78ms
step:2023/2330 train_time:116883ms step_avg:57.78ms
step:2024/2330 train_time:116945ms step_avg:57.78ms
step:2025/2330 train_time:117001ms step_avg:57.78ms
step:2026/2330 train_time:117061ms step_avg:57.78ms
step:2027/2330 train_time:117118ms step_avg:57.78ms
step:2028/2330 train_time:117178ms step_avg:57.78ms
step:2029/2330 train_time:117234ms step_avg:57.78ms
step:2030/2330 train_time:117293ms step_avg:57.78ms
step:2031/2330 train_time:117349ms step_avg:57.78ms
step:2032/2330 train_time:117408ms step_avg:57.78ms
step:2033/2330 train_time:117465ms step_avg:57.78ms
step:2034/2330 train_time:117525ms step_avg:57.78ms
step:2035/2330 train_time:117582ms step_avg:57.78ms
step:2036/2330 train_time:117643ms step_avg:57.78ms
step:2037/2330 train_time:117701ms step_avg:57.78ms
step:2038/2330 train_time:117762ms step_avg:57.78ms
step:2039/2330 train_time:117819ms step_avg:57.78ms
step:2040/2330 train_time:117879ms step_avg:57.78ms
step:2041/2330 train_time:117936ms step_avg:57.78ms
step:2042/2330 train_time:117997ms step_avg:57.79ms
step:2043/2330 train_time:118055ms step_avg:57.79ms
step:2044/2330 train_time:118114ms step_avg:57.79ms
step:2045/2330 train_time:118171ms step_avg:57.79ms
step:2046/2330 train_time:118231ms step_avg:57.79ms
step:2047/2330 train_time:118287ms step_avg:57.79ms
step:2048/2330 train_time:118347ms step_avg:57.79ms
step:2049/2330 train_time:118403ms step_avg:57.79ms
step:2050/2330 train_time:118463ms step_avg:57.79ms
step:2051/2330 train_time:118520ms step_avg:57.79ms
step:2052/2330 train_time:118581ms step_avg:57.79ms
step:2053/2330 train_time:118637ms step_avg:57.79ms
step:2054/2330 train_time:118698ms step_avg:57.79ms
step:2055/2330 train_time:118755ms step_avg:57.79ms
step:2056/2330 train_time:118816ms step_avg:57.79ms
step:2057/2330 train_time:118872ms step_avg:57.79ms
step:2058/2330 train_time:118933ms step_avg:57.79ms
step:2059/2330 train_time:118990ms step_avg:57.79ms
step:2060/2330 train_time:119051ms step_avg:57.79ms
step:2061/2330 train_time:119107ms step_avg:57.79ms
step:2062/2330 train_time:119167ms step_avg:57.79ms
step:2063/2330 train_time:119224ms step_avg:57.79ms
step:2064/2330 train_time:119284ms step_avg:57.79ms
step:2065/2330 train_time:119341ms step_avg:57.79ms
step:2066/2330 train_time:119400ms step_avg:57.79ms
step:2067/2330 train_time:119457ms step_avg:57.79ms
step:2068/2330 train_time:119517ms step_avg:57.79ms
step:2069/2330 train_time:119574ms step_avg:57.79ms
step:2070/2330 train_time:119634ms step_avg:57.79ms
step:2071/2330 train_time:119691ms step_avg:57.79ms
step:2072/2330 train_time:119752ms step_avg:57.80ms
step:2073/2330 train_time:119809ms step_avg:57.79ms
step:2074/2330 train_time:119868ms step_avg:57.80ms
step:2075/2330 train_time:119926ms step_avg:57.80ms
step:2076/2330 train_time:119986ms step_avg:57.80ms
step:2077/2330 train_time:120043ms step_avg:57.80ms
step:2078/2330 train_time:120103ms step_avg:57.80ms
step:2079/2330 train_time:120160ms step_avg:57.80ms
step:2080/2330 train_time:120220ms step_avg:57.80ms
step:2081/2330 train_time:120276ms step_avg:57.80ms
step:2082/2330 train_time:120337ms step_avg:57.80ms
step:2083/2330 train_time:120394ms step_avg:57.80ms
step:2084/2330 train_time:120453ms step_avg:57.80ms
step:2085/2330 train_time:120509ms step_avg:57.80ms
step:2086/2330 train_time:120570ms step_avg:57.80ms
step:2087/2330 train_time:120626ms step_avg:57.80ms
step:2088/2330 train_time:120687ms step_avg:57.80ms
step:2089/2330 train_time:120745ms step_avg:57.80ms
step:2090/2330 train_time:120805ms step_avg:57.80ms
step:2091/2330 train_time:120861ms step_avg:57.80ms
step:2092/2330 train_time:120922ms step_avg:57.80ms
step:2093/2330 train_time:120979ms step_avg:57.80ms
step:2094/2330 train_time:121039ms step_avg:57.80ms
step:2095/2330 train_time:121096ms step_avg:57.80ms
step:2096/2330 train_time:121156ms step_avg:57.80ms
step:2097/2330 train_time:121213ms step_avg:57.80ms
step:2098/2330 train_time:121272ms step_avg:57.80ms
step:2099/2330 train_time:121328ms step_avg:57.80ms
step:2100/2330 train_time:121389ms step_avg:57.80ms
step:2101/2330 train_time:121445ms step_avg:57.80ms
step:2102/2330 train_time:121506ms step_avg:57.80ms
step:2103/2330 train_time:121562ms step_avg:57.80ms
step:2104/2330 train_time:121623ms step_avg:57.81ms
step:2105/2330 train_time:121680ms step_avg:57.81ms
step:2106/2330 train_time:121741ms step_avg:57.81ms
step:2107/2330 train_time:121797ms step_avg:57.81ms
step:2108/2330 train_time:121858ms step_avg:57.81ms
step:2109/2330 train_time:121915ms step_avg:57.81ms
step:2110/2330 train_time:121974ms step_avg:57.81ms
step:2111/2330 train_time:122032ms step_avg:57.81ms
step:2112/2330 train_time:122092ms step_avg:57.81ms
step:2113/2330 train_time:122149ms step_avg:57.81ms
step:2114/2330 train_time:122209ms step_avg:57.81ms
step:2115/2330 train_time:122266ms step_avg:57.81ms
step:2116/2330 train_time:122325ms step_avg:57.81ms
step:2117/2330 train_time:122382ms step_avg:57.81ms
step:2118/2330 train_time:122442ms step_avg:57.81ms
step:2119/2330 train_time:122498ms step_avg:57.81ms
step:2120/2330 train_time:122558ms step_avg:57.81ms
step:2121/2330 train_time:122614ms step_avg:57.81ms
step:2122/2330 train_time:122675ms step_avg:57.81ms
step:2123/2330 train_time:122731ms step_avg:57.81ms
step:2124/2330 train_time:122791ms step_avg:57.81ms
step:2125/2330 train_time:122848ms step_avg:57.81ms
step:2126/2330 train_time:122909ms step_avg:57.81ms
step:2127/2330 train_time:122965ms step_avg:57.81ms
step:2128/2330 train_time:123027ms step_avg:57.81ms
step:2129/2330 train_time:123083ms step_avg:57.81ms
step:2130/2330 train_time:123145ms step_avg:57.81ms
step:2131/2330 train_time:123201ms step_avg:57.81ms
step:2132/2330 train_time:123261ms step_avg:57.81ms
step:2133/2330 train_time:123318ms step_avg:57.81ms
step:2134/2330 train_time:123378ms step_avg:57.82ms
step:2135/2330 train_time:123434ms step_avg:57.81ms
step:2136/2330 train_time:123494ms step_avg:57.82ms
step:2137/2330 train_time:123550ms step_avg:57.81ms
step:2138/2330 train_time:123610ms step_avg:57.82ms
step:2139/2330 train_time:123666ms step_avg:57.82ms
step:2140/2330 train_time:123728ms step_avg:57.82ms
step:2141/2330 train_time:123784ms step_avg:57.82ms
step:2142/2330 train_time:123845ms step_avg:57.82ms
step:2143/2330 train_time:123903ms step_avg:57.82ms
step:2144/2330 train_time:123963ms step_avg:57.82ms
step:2145/2330 train_time:124019ms step_avg:57.82ms
step:2146/2330 train_time:124079ms step_avg:57.82ms
step:2147/2330 train_time:124136ms step_avg:57.82ms
step:2148/2330 train_time:124196ms step_avg:57.82ms
step:2149/2330 train_time:124252ms step_avg:57.82ms
step:2150/2330 train_time:124312ms step_avg:57.82ms
step:2151/2330 train_time:124368ms step_avg:57.82ms
step:2152/2330 train_time:124429ms step_avg:57.82ms
step:2153/2330 train_time:124485ms step_avg:57.82ms
step:2154/2330 train_time:124545ms step_avg:57.82ms
step:2155/2330 train_time:124602ms step_avg:57.82ms
step:2156/2330 train_time:124662ms step_avg:57.82ms
step:2157/2330 train_time:124719ms step_avg:57.82ms
step:2158/2330 train_time:124780ms step_avg:57.82ms
step:2159/2330 train_time:124837ms step_avg:57.82ms
step:2160/2330 train_time:124897ms step_avg:57.82ms
step:2161/2330 train_time:124954ms step_avg:57.82ms
step:2162/2330 train_time:125014ms step_avg:57.82ms
step:2163/2330 train_time:125070ms step_avg:57.82ms
step:2164/2330 train_time:125130ms step_avg:57.82ms
step:2165/2330 train_time:125187ms step_avg:57.82ms
step:2166/2330 train_time:125247ms step_avg:57.82ms
step:2167/2330 train_time:125304ms step_avg:57.82ms
step:2168/2330 train_time:125364ms step_avg:57.82ms
step:2169/2330 train_time:125420ms step_avg:57.82ms
step:2170/2330 train_time:125481ms step_avg:57.83ms
step:2171/2330 train_time:125538ms step_avg:57.82ms
step:2172/2330 train_time:125598ms step_avg:57.83ms
step:2173/2330 train_time:125655ms step_avg:57.83ms
step:2174/2330 train_time:125715ms step_avg:57.83ms
step:2175/2330 train_time:125772ms step_avg:57.83ms
step:2176/2330 train_time:125833ms step_avg:57.83ms
step:2177/2330 train_time:125889ms step_avg:57.83ms
step:2178/2330 train_time:125950ms step_avg:57.83ms
step:2179/2330 train_time:126007ms step_avg:57.83ms
step:2180/2330 train_time:126067ms step_avg:57.83ms
step:2181/2330 train_time:126124ms step_avg:57.83ms
step:2182/2330 train_time:126184ms step_avg:57.83ms
step:2183/2330 train_time:126241ms step_avg:57.83ms
step:2184/2330 train_time:126302ms step_avg:57.83ms
step:2185/2330 train_time:126359ms step_avg:57.83ms
step:2186/2330 train_time:126418ms step_avg:57.83ms
step:2187/2330 train_time:126475ms step_avg:57.83ms
step:2188/2330 train_time:126536ms step_avg:57.83ms
step:2189/2330 train_time:126593ms step_avg:57.83ms
step:2190/2330 train_time:126652ms step_avg:57.83ms
step:2191/2330 train_time:126708ms step_avg:57.83ms
step:2192/2330 train_time:126769ms step_avg:57.83ms
step:2193/2330 train_time:126825ms step_avg:57.83ms
step:2194/2330 train_time:126886ms step_avg:57.83ms
step:2195/2330 train_time:126943ms step_avg:57.83ms
step:2196/2330 train_time:127003ms step_avg:57.83ms
step:2197/2330 train_time:127060ms step_avg:57.83ms
step:2198/2330 train_time:127120ms step_avg:57.83ms
step:2199/2330 train_time:127177ms step_avg:57.83ms
step:2200/2330 train_time:127237ms step_avg:57.83ms
step:2201/2330 train_time:127294ms step_avg:57.83ms
step:2202/2330 train_time:127354ms step_avg:57.84ms
step:2203/2330 train_time:127410ms step_avg:57.83ms
step:2204/2330 train_time:127470ms step_avg:57.84ms
step:2205/2330 train_time:127527ms step_avg:57.84ms
step:2206/2330 train_time:127587ms step_avg:57.84ms
step:2207/2330 train_time:127644ms step_avg:57.84ms
step:2208/2330 train_time:127705ms step_avg:57.84ms
step:2209/2330 train_time:127761ms step_avg:57.84ms
step:2210/2330 train_time:127822ms step_avg:57.84ms
step:2211/2330 train_time:127880ms step_avg:57.84ms
step:2212/2330 train_time:127939ms step_avg:57.84ms
step:2213/2330 train_time:127996ms step_avg:57.84ms
step:2214/2330 train_time:128057ms step_avg:57.84ms
step:2215/2330 train_time:128114ms step_avg:57.84ms
step:2216/2330 train_time:128173ms step_avg:57.84ms
step:2217/2330 train_time:128230ms step_avg:57.84ms
step:2218/2330 train_time:128290ms step_avg:57.84ms
step:2219/2330 train_time:128346ms step_avg:57.84ms
step:2220/2330 train_time:128407ms step_avg:57.84ms
step:2221/2330 train_time:128464ms step_avg:57.84ms
step:2222/2330 train_time:128524ms step_avg:57.84ms
step:2223/2330 train_time:128581ms step_avg:57.84ms
step:2224/2330 train_time:128641ms step_avg:57.84ms
step:2225/2330 train_time:128698ms step_avg:57.84ms
step:2226/2330 train_time:128758ms step_avg:57.84ms
step:2227/2330 train_time:128815ms step_avg:57.84ms
step:2228/2330 train_time:128874ms step_avg:57.84ms
step:2229/2330 train_time:128931ms step_avg:57.84ms
step:2230/2330 train_time:128990ms step_avg:57.84ms
step:2231/2330 train_time:129047ms step_avg:57.84ms
step:2232/2330 train_time:129107ms step_avg:57.84ms
step:2233/2330 train_time:129164ms step_avg:57.84ms
step:2234/2330 train_time:129225ms step_avg:57.84ms
step:2235/2330 train_time:129281ms step_avg:57.84ms
step:2236/2330 train_time:129342ms step_avg:57.85ms
step:2237/2330 train_time:129399ms step_avg:57.84ms
step:2238/2330 train_time:129460ms step_avg:57.85ms
step:2239/2330 train_time:129516ms step_avg:57.85ms
step:2240/2330 train_time:129576ms step_avg:57.85ms
step:2241/2330 train_time:129633ms step_avg:57.85ms
step:2242/2330 train_time:129692ms step_avg:57.85ms
step:2243/2330 train_time:129749ms step_avg:57.85ms
step:2244/2330 train_time:129810ms step_avg:57.85ms
step:2245/2330 train_time:129867ms step_avg:57.85ms
step:2246/2330 train_time:129927ms step_avg:57.85ms
step:2247/2330 train_time:129984ms step_avg:57.85ms
step:2248/2330 train_time:130045ms step_avg:57.85ms
step:2249/2330 train_time:130102ms step_avg:57.85ms
step:2250/2330 train_time:130161ms step_avg:57.85ms
step:2250/2330 val_loss:4.1301 train_time:130242ms step_avg:57.89ms
step:2251/2330 train_time:130260ms step_avg:57.87ms
step:2252/2330 train_time:130281ms step_avg:57.85ms
step:2253/2330 train_time:130340ms step_avg:57.85ms
step:2254/2330 train_time:130404ms step_avg:57.85ms
step:2255/2330 train_time:130460ms step_avg:57.85ms
step:2256/2330 train_time:130523ms step_avg:57.86ms
step:2257/2330 train_time:130579ms step_avg:57.86ms
step:2258/2330 train_time:130640ms step_avg:57.86ms
step:2259/2330 train_time:130697ms step_avg:57.86ms
step:2260/2330 train_time:130757ms step_avg:57.86ms
step:2261/2330 train_time:130813ms step_avg:57.86ms
step:2262/2330 train_time:130872ms step_avg:57.86ms
step:2263/2330 train_time:130928ms step_avg:57.86ms
step:2264/2330 train_time:130988ms step_avg:57.86ms
step:2265/2330 train_time:131044ms step_avg:57.86ms
step:2266/2330 train_time:131104ms step_avg:57.86ms
step:2267/2330 train_time:131159ms step_avg:57.86ms
step:2268/2330 train_time:131220ms step_avg:57.86ms
step:2269/2330 train_time:131278ms step_avg:57.86ms
step:2270/2330 train_time:131340ms step_avg:57.86ms
step:2271/2330 train_time:131397ms step_avg:57.86ms
step:2272/2330 train_time:131459ms step_avg:57.86ms
step:2273/2330 train_time:131515ms step_avg:57.86ms
step:2274/2330 train_time:131577ms step_avg:57.86ms
step:2275/2330 train_time:131633ms step_avg:57.86ms
step:2276/2330 train_time:131692ms step_avg:57.86ms
step:2277/2330 train_time:131749ms step_avg:57.86ms
step:2278/2330 train_time:131809ms step_avg:57.86ms
step:2279/2330 train_time:131865ms step_avg:57.86ms
step:2280/2330 train_time:131925ms step_avg:57.86ms
step:2281/2330 train_time:131980ms step_avg:57.86ms
step:2282/2330 train_time:132040ms step_avg:57.86ms
step:2283/2330 train_time:132097ms step_avg:57.86ms
step:2284/2330 train_time:132156ms step_avg:57.86ms
step:2285/2330 train_time:132213ms step_avg:57.86ms
step:2286/2330 train_time:132273ms step_avg:57.86ms
step:2287/2330 train_time:132330ms step_avg:57.86ms
step:2288/2330 train_time:132390ms step_avg:57.86ms
step:2289/2330 train_time:132447ms step_avg:57.86ms
step:2290/2330 train_time:132508ms step_avg:57.86ms
step:2291/2330 train_time:132565ms step_avg:57.86ms
step:2292/2330 train_time:132627ms step_avg:57.87ms
step:2293/2330 train_time:132684ms step_avg:57.86ms
step:2294/2330 train_time:132744ms step_avg:57.87ms
step:2295/2330 train_time:132800ms step_avg:57.87ms
step:2296/2330 train_time:132860ms step_avg:57.87ms
step:2297/2330 train_time:132918ms step_avg:57.87ms
step:2298/2330 train_time:132977ms step_avg:57.87ms
step:2299/2330 train_time:133033ms step_avg:57.87ms
step:2300/2330 train_time:133092ms step_avg:57.87ms
step:2301/2330 train_time:133149ms step_avg:57.87ms
step:2302/2330 train_time:133209ms step_avg:57.87ms
step:2303/2330 train_time:133266ms step_avg:57.87ms
step:2304/2330 train_time:133325ms step_avg:57.87ms
step:2305/2330 train_time:133383ms step_avg:57.87ms
step:2306/2330 train_time:133443ms step_avg:57.87ms
step:2307/2330 train_time:133500ms step_avg:57.87ms
step:2308/2330 train_time:133562ms step_avg:57.87ms
step:2309/2330 train_time:133619ms step_avg:57.87ms
step:2310/2330 train_time:133679ms step_avg:57.87ms
step:2311/2330 train_time:133736ms step_avg:57.87ms
step:2312/2330 train_time:133795ms step_avg:57.87ms
step:2313/2330 train_time:133851ms step_avg:57.87ms
step:2314/2330 train_time:133912ms step_avg:57.87ms
step:2315/2330 train_time:133969ms step_avg:57.87ms
step:2316/2330 train_time:134028ms step_avg:57.87ms
step:2317/2330 train_time:134084ms step_avg:57.87ms
step:2318/2330 train_time:134144ms step_avg:57.87ms
step:2319/2330 train_time:134201ms step_avg:57.87ms
step:2320/2330 train_time:134261ms step_avg:57.87ms
step:2321/2330 train_time:134318ms step_avg:57.87ms
step:2322/2330 train_time:134378ms step_avg:57.87ms
step:2323/2330 train_time:134436ms step_avg:57.87ms
step:2324/2330 train_time:134495ms step_avg:57.87ms
step:2325/2330 train_time:134553ms step_avg:57.87ms
step:2326/2330 train_time:134613ms step_avg:57.87ms
step:2327/2330 train_time:134670ms step_avg:57.87ms
step:2328/2330 train_time:134729ms step_avg:57.87ms
step:2329/2330 train_time:134785ms step_avg:57.87ms
step:2330/2330 train_time:134847ms step_avg:57.87ms
step:2330/2330 val_loss:4.1144 train_time:134929ms step_avg:57.91ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
