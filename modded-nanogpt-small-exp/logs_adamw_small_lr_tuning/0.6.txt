import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--adamw_lr', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_lr_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_lr_tuning/{args2.adamw_lr}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=float(args2.adamw_lr),  betas=(0.85, 0.85), eps=1e-10, weight_decay=0.0, fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_lr_tuning/{args2.adamw_lr}", exist_ok=True)
            torch.save(log, f"logs_adamw_small_lr_tuning/{args2.adamw_lr}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 03:15:14 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   26C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   26C    P0             116W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   25C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   27C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:87ms step_avg:87.05ms
step:2/2330 train_time:190ms step_avg:95.05ms
step:3/2330 train_time:209ms step_avg:69.73ms
step:4/2330 train_time:229ms step_avg:57.26ms
step:5/2330 train_time:281ms step_avg:56.20ms
step:6/2330 train_time:338ms step_avg:56.39ms
step:7/2330 train_time:392ms step_avg:56.02ms
step:8/2330 train_time:449ms step_avg:56.11ms
step:9/2330 train_time:503ms step_avg:55.87ms
step:10/2330 train_time:560ms step_avg:55.96ms
step:11/2330 train_time:613ms step_avg:55.77ms
step:12/2330 train_time:670ms step_avg:55.84ms
step:13/2330 train_time:724ms step_avg:55.70ms
step:14/2330 train_time:781ms step_avg:55.77ms
step:15/2330 train_time:835ms step_avg:55.65ms
step:16/2330 train_time:891ms step_avg:55.72ms
step:17/2330 train_time:945ms step_avg:55.61ms
step:18/2330 train_time:1002ms step_avg:55.68ms
step:19/2330 train_time:1056ms step_avg:55.59ms
step:20/2330 train_time:1113ms step_avg:55.64ms
step:21/2330 train_time:1167ms step_avg:55.57ms
step:22/2330 train_time:1226ms step_avg:55.73ms
step:23/2330 train_time:1280ms step_avg:55.66ms
step:24/2330 train_time:1338ms step_avg:55.75ms
step:25/2330 train_time:1392ms step_avg:55.70ms
step:26/2330 train_time:1449ms step_avg:55.75ms
step:27/2330 train_time:1503ms step_avg:55.68ms
step:28/2330 train_time:1561ms step_avg:55.74ms
step:29/2330 train_time:1615ms step_avg:55.69ms
step:30/2330 train_time:1672ms step_avg:55.73ms
step:31/2330 train_time:1726ms step_avg:55.68ms
step:32/2330 train_time:1784ms step_avg:55.75ms
step:33/2330 train_time:1839ms step_avg:55.72ms
step:34/2330 train_time:1896ms step_avg:55.76ms
step:35/2330 train_time:1950ms step_avg:55.72ms
step:36/2330 train_time:2007ms step_avg:55.75ms
step:37/2330 train_time:2061ms step_avg:55.70ms
step:38/2330 train_time:2119ms step_avg:55.76ms
step:39/2330 train_time:2173ms step_avg:55.73ms
step:40/2330 train_time:2231ms step_avg:55.79ms
step:41/2330 train_time:2286ms step_avg:55.75ms
step:42/2330 train_time:2345ms step_avg:55.84ms
step:43/2330 train_time:2399ms step_avg:55.80ms
step:44/2330 train_time:2458ms step_avg:55.85ms
step:45/2330 train_time:2512ms step_avg:55.82ms
step:46/2330 train_time:2570ms step_avg:55.87ms
step:47/2330 train_time:2624ms step_avg:55.83ms
step:48/2330 train_time:2682ms step_avg:55.88ms
step:49/2330 train_time:2736ms step_avg:55.84ms
step:50/2330 train_time:2793ms step_avg:55.87ms
step:51/2330 train_time:2848ms step_avg:55.84ms
step:52/2330 train_time:2906ms step_avg:55.88ms
step:53/2330 train_time:2961ms step_avg:55.86ms
step:54/2330 train_time:3018ms step_avg:55.89ms
step:55/2330 train_time:3072ms step_avg:55.86ms
step:56/2330 train_time:3130ms step_avg:55.89ms
step:57/2330 train_time:3184ms step_avg:55.86ms
step:58/2330 train_time:3241ms step_avg:55.88ms
step:59/2330 train_time:3296ms step_avg:55.86ms
step:60/2330 train_time:3353ms step_avg:55.88ms
step:61/2330 train_time:3407ms step_avg:55.86ms
step:62/2330 train_time:3465ms step_avg:55.88ms
step:63/2330 train_time:3519ms step_avg:55.85ms
step:64/2330 train_time:3576ms step_avg:55.88ms
step:65/2330 train_time:3631ms step_avg:55.86ms
step:66/2330 train_time:3689ms step_avg:55.90ms
step:67/2330 train_time:3743ms step_avg:55.87ms
step:68/2330 train_time:3801ms step_avg:55.90ms
step:69/2330 train_time:3855ms step_avg:55.88ms
step:70/2330 train_time:3914ms step_avg:55.91ms
step:71/2330 train_time:3968ms step_avg:55.88ms
step:72/2330 train_time:4026ms step_avg:55.92ms
step:73/2330 train_time:4081ms step_avg:55.91ms
step:74/2330 train_time:4139ms step_avg:55.94ms
step:75/2330 train_time:4194ms step_avg:55.92ms
step:76/2330 train_time:4252ms step_avg:55.94ms
step:77/2330 train_time:4306ms step_avg:55.93ms
step:78/2330 train_time:4364ms step_avg:55.94ms
step:79/2330 train_time:4418ms step_avg:55.93ms
step:80/2330 train_time:4476ms step_avg:55.95ms
step:81/2330 train_time:4531ms step_avg:55.93ms
step:82/2330 train_time:4589ms step_avg:55.96ms
step:83/2330 train_time:4643ms step_avg:55.94ms
step:84/2330 train_time:4701ms step_avg:55.96ms
step:85/2330 train_time:4756ms step_avg:55.95ms
step:86/2330 train_time:4814ms step_avg:55.97ms
step:87/2330 train_time:4868ms step_avg:55.96ms
step:88/2330 train_time:4926ms step_avg:55.98ms
step:89/2330 train_time:4982ms step_avg:55.97ms
step:90/2330 train_time:5039ms step_avg:55.99ms
step:91/2330 train_time:5094ms step_avg:55.98ms
step:92/2330 train_time:5152ms step_avg:56.00ms
step:93/2330 train_time:5207ms step_avg:55.99ms
step:94/2330 train_time:5265ms step_avg:56.01ms
step:95/2330 train_time:5320ms step_avg:55.99ms
step:96/2330 train_time:5378ms step_avg:56.02ms
step:97/2330 train_time:5433ms step_avg:56.01ms
step:98/2330 train_time:5490ms step_avg:56.02ms
step:99/2330 train_time:5544ms step_avg:56.00ms
step:100/2330 train_time:5604ms step_avg:56.04ms
step:101/2330 train_time:5658ms step_avg:56.02ms
step:102/2330 train_time:5717ms step_avg:56.05ms
step:103/2330 train_time:5771ms step_avg:56.03ms
step:104/2330 train_time:5829ms step_avg:56.05ms
step:105/2330 train_time:5884ms step_avg:56.03ms
step:106/2330 train_time:5942ms step_avg:56.06ms
step:107/2330 train_time:5997ms step_avg:56.05ms
step:108/2330 train_time:6055ms step_avg:56.07ms
step:109/2330 train_time:6110ms step_avg:56.06ms
step:110/2330 train_time:6167ms step_avg:56.07ms
step:111/2330 train_time:6222ms step_avg:56.06ms
step:112/2330 train_time:6280ms step_avg:56.07ms
step:113/2330 train_time:6335ms step_avg:56.06ms
step:114/2330 train_time:6393ms step_avg:56.08ms
step:115/2330 train_time:6448ms step_avg:56.07ms
step:116/2330 train_time:6505ms step_avg:56.08ms
step:117/2330 train_time:6560ms step_avg:56.07ms
step:118/2330 train_time:6618ms step_avg:56.09ms
step:119/2330 train_time:6674ms step_avg:56.08ms
step:120/2330 train_time:6731ms step_avg:56.09ms
step:121/2330 train_time:6786ms step_avg:56.08ms
step:122/2330 train_time:6845ms step_avg:56.11ms
step:123/2330 train_time:6900ms step_avg:56.09ms
step:124/2330 train_time:6959ms step_avg:56.12ms
step:125/2330 train_time:7014ms step_avg:56.11ms
step:126/2330 train_time:7071ms step_avg:56.12ms
step:127/2330 train_time:7126ms step_avg:56.11ms
step:128/2330 train_time:7185ms step_avg:56.13ms
step:129/2330 train_time:7240ms step_avg:56.12ms
step:130/2330 train_time:7298ms step_avg:56.14ms
step:131/2330 train_time:7353ms step_avg:56.13ms
step:132/2330 train_time:7412ms step_avg:56.15ms
step:133/2330 train_time:7467ms step_avg:56.14ms
step:134/2330 train_time:7526ms step_avg:56.16ms
step:135/2330 train_time:7581ms step_avg:56.15ms
step:136/2330 train_time:7639ms step_avg:56.17ms
step:137/2330 train_time:7694ms step_avg:56.16ms
step:138/2330 train_time:7751ms step_avg:56.17ms
step:139/2330 train_time:7807ms step_avg:56.16ms
step:140/2330 train_time:7865ms step_avg:56.18ms
step:141/2330 train_time:7919ms step_avg:56.17ms
step:142/2330 train_time:7979ms step_avg:56.19ms
step:143/2330 train_time:8034ms step_avg:56.18ms
step:144/2330 train_time:8092ms step_avg:56.19ms
step:145/2330 train_time:8147ms step_avg:56.19ms
step:146/2330 train_time:8206ms step_avg:56.21ms
step:147/2330 train_time:8261ms step_avg:56.20ms
step:148/2330 train_time:8321ms step_avg:56.22ms
step:149/2330 train_time:8376ms step_avg:56.21ms
step:150/2330 train_time:8435ms step_avg:56.23ms
step:151/2330 train_time:8490ms step_avg:56.22ms
step:152/2330 train_time:8549ms step_avg:56.25ms
step:153/2330 train_time:8604ms step_avg:56.23ms
step:154/2330 train_time:8663ms step_avg:56.26ms
step:155/2330 train_time:8718ms step_avg:56.25ms
step:156/2330 train_time:8777ms step_avg:56.26ms
step:157/2330 train_time:8833ms step_avg:56.26ms
step:158/2330 train_time:8891ms step_avg:56.27ms
step:159/2330 train_time:8946ms step_avg:56.26ms
step:160/2330 train_time:9005ms step_avg:56.28ms
step:161/2330 train_time:9061ms step_avg:56.28ms
step:162/2330 train_time:9120ms step_avg:56.29ms
step:163/2330 train_time:9175ms step_avg:56.29ms
step:164/2330 train_time:9233ms step_avg:56.30ms
step:165/2330 train_time:9288ms step_avg:56.29ms
step:166/2330 train_time:9347ms step_avg:56.31ms
step:167/2330 train_time:9402ms step_avg:56.30ms
step:168/2330 train_time:9461ms step_avg:56.32ms
step:169/2330 train_time:9517ms step_avg:56.32ms
step:170/2330 train_time:9575ms step_avg:56.33ms
step:171/2330 train_time:9630ms step_avg:56.32ms
step:172/2330 train_time:9689ms step_avg:56.33ms
step:173/2330 train_time:9744ms step_avg:56.33ms
step:174/2330 train_time:9803ms step_avg:56.34ms
step:175/2330 train_time:9859ms step_avg:56.34ms
step:176/2330 train_time:9918ms step_avg:56.35ms
step:177/2330 train_time:9973ms step_avg:56.35ms
step:178/2330 train_time:10031ms step_avg:56.36ms
step:179/2330 train_time:10087ms step_avg:56.35ms
step:180/2330 train_time:10146ms step_avg:56.37ms
step:181/2330 train_time:10201ms step_avg:56.36ms
step:182/2330 train_time:10260ms step_avg:56.37ms
step:183/2330 train_time:10315ms step_avg:56.37ms
step:184/2330 train_time:10374ms step_avg:56.38ms
step:185/2330 train_time:10430ms step_avg:56.38ms
step:186/2330 train_time:10489ms step_avg:56.39ms
step:187/2330 train_time:10544ms step_avg:56.39ms
step:188/2330 train_time:10604ms step_avg:56.40ms
step:189/2330 train_time:10659ms step_avg:56.40ms
step:190/2330 train_time:10718ms step_avg:56.41ms
step:191/2330 train_time:10774ms step_avg:56.41ms
step:192/2330 train_time:10832ms step_avg:56.42ms
step:193/2330 train_time:10888ms step_avg:56.41ms
step:194/2330 train_time:10946ms step_avg:56.42ms
step:195/2330 train_time:11001ms step_avg:56.42ms
step:196/2330 train_time:11060ms step_avg:56.43ms
step:197/2330 train_time:11116ms step_avg:56.43ms
step:198/2330 train_time:11175ms step_avg:56.44ms
step:199/2330 train_time:11231ms step_avg:56.44ms
step:200/2330 train_time:11289ms step_avg:56.45ms
step:201/2330 train_time:11345ms step_avg:56.44ms
step:202/2330 train_time:11403ms step_avg:56.45ms
step:203/2330 train_time:11459ms step_avg:56.45ms
step:204/2330 train_time:11518ms step_avg:56.46ms
step:205/2330 train_time:11574ms step_avg:56.46ms
step:206/2330 train_time:11632ms step_avg:56.47ms
step:207/2330 train_time:11688ms step_avg:56.46ms
step:208/2330 train_time:11746ms step_avg:56.47ms
step:209/2330 train_time:11802ms step_avg:56.47ms
step:210/2330 train_time:11861ms step_avg:56.48ms
step:211/2330 train_time:11917ms step_avg:56.48ms
step:212/2330 train_time:11975ms step_avg:56.49ms
step:213/2330 train_time:12031ms step_avg:56.48ms
step:214/2330 train_time:12089ms step_avg:56.49ms
step:215/2330 train_time:12145ms step_avg:56.49ms
step:216/2330 train_time:12205ms step_avg:56.50ms
step:217/2330 train_time:12260ms step_avg:56.50ms
step:218/2330 train_time:12318ms step_avg:56.51ms
step:219/2330 train_time:12374ms step_avg:56.50ms
step:220/2330 train_time:12433ms step_avg:56.51ms
step:221/2330 train_time:12488ms step_avg:56.51ms
step:222/2330 train_time:12547ms step_avg:56.52ms
step:223/2330 train_time:12603ms step_avg:56.51ms
step:224/2330 train_time:12662ms step_avg:56.53ms
step:225/2330 train_time:12717ms step_avg:56.52ms
step:226/2330 train_time:12776ms step_avg:56.53ms
step:227/2330 train_time:12832ms step_avg:56.53ms
step:228/2330 train_time:12891ms step_avg:56.54ms
step:229/2330 train_time:12946ms step_avg:56.53ms
step:230/2330 train_time:13006ms step_avg:56.55ms
step:231/2330 train_time:13061ms step_avg:56.54ms
step:232/2330 train_time:13121ms step_avg:56.56ms
step:233/2330 train_time:13176ms step_avg:56.55ms
step:234/2330 train_time:13235ms step_avg:56.56ms
step:235/2330 train_time:13290ms step_avg:56.55ms
step:236/2330 train_time:13349ms step_avg:56.56ms
step:237/2330 train_time:13404ms step_avg:56.56ms
step:238/2330 train_time:13464ms step_avg:56.57ms
step:239/2330 train_time:13520ms step_avg:56.57ms
step:240/2330 train_time:13578ms step_avg:56.58ms
step:241/2330 train_time:13633ms step_avg:56.57ms
step:242/2330 train_time:13693ms step_avg:56.58ms
step:243/2330 train_time:13748ms step_avg:56.58ms
step:244/2330 train_time:13808ms step_avg:56.59ms
step:245/2330 train_time:13863ms step_avg:56.59ms
step:246/2330 train_time:13923ms step_avg:56.60ms
step:247/2330 train_time:13978ms step_avg:56.59ms
step:248/2330 train_time:14037ms step_avg:56.60ms
step:249/2330 train_time:14093ms step_avg:56.60ms
step:250/2330 train_time:14151ms step_avg:56.60ms
step:250/2330 val_loss:6.6450 train_time:14230ms step_avg:56.92ms
step:251/2330 train_time:14248ms step_avg:56.77ms
step:252/2330 train_time:14268ms step_avg:56.62ms
step:253/2330 train_time:14323ms step_avg:56.61ms
step:254/2330 train_time:14385ms step_avg:56.63ms
step:255/2330 train_time:14440ms step_avg:56.63ms
step:256/2330 train_time:14507ms step_avg:56.67ms
step:257/2330 train_time:14563ms step_avg:56.66ms
step:258/2330 train_time:14623ms step_avg:56.68ms
step:259/2330 train_time:14678ms step_avg:56.67ms
step:260/2330 train_time:14738ms step_avg:56.68ms
step:261/2330 train_time:14793ms step_avg:56.68ms
step:262/2330 train_time:14851ms step_avg:56.68ms
step:263/2330 train_time:14906ms step_avg:56.68ms
step:264/2330 train_time:14965ms step_avg:56.68ms
step:265/2330 train_time:15020ms step_avg:56.68ms
step:266/2330 train_time:15078ms step_avg:56.68ms
step:267/2330 train_time:15133ms step_avg:56.68ms
step:268/2330 train_time:15192ms step_avg:56.69ms
step:269/2330 train_time:15249ms step_avg:56.69ms
step:270/2330 train_time:15308ms step_avg:56.70ms
step:271/2330 train_time:15364ms step_avg:56.69ms
step:272/2330 train_time:15425ms step_avg:56.71ms
step:273/2330 train_time:15481ms step_avg:56.71ms
step:274/2330 train_time:15542ms step_avg:56.72ms
step:275/2330 train_time:15598ms step_avg:56.72ms
step:276/2330 train_time:15658ms step_avg:56.73ms
step:277/2330 train_time:15713ms step_avg:56.73ms
step:278/2330 train_time:15772ms step_avg:56.73ms
step:279/2330 train_time:15828ms step_avg:56.73ms
step:280/2330 train_time:15887ms step_avg:56.74ms
step:281/2330 train_time:15942ms step_avg:56.73ms
step:282/2330 train_time:16001ms step_avg:56.74ms
step:283/2330 train_time:16057ms step_avg:56.74ms
step:284/2330 train_time:16116ms step_avg:56.75ms
step:285/2330 train_time:16171ms step_avg:56.74ms
step:286/2330 train_time:16230ms step_avg:56.75ms
step:287/2330 train_time:16286ms step_avg:56.75ms
step:288/2330 train_time:16346ms step_avg:56.76ms
step:289/2330 train_time:16402ms step_avg:56.76ms
step:290/2330 train_time:16462ms step_avg:56.76ms
step:291/2330 train_time:16517ms step_avg:56.76ms
step:292/2330 train_time:16577ms step_avg:56.77ms
step:293/2330 train_time:16634ms step_avg:56.77ms
step:294/2330 train_time:16692ms step_avg:56.78ms
step:295/2330 train_time:16748ms step_avg:56.77ms
step:296/2330 train_time:16807ms step_avg:56.78ms
step:297/2330 train_time:16863ms step_avg:56.78ms
step:298/2330 train_time:16922ms step_avg:56.79ms
step:299/2330 train_time:16978ms step_avg:56.78ms
step:300/2330 train_time:18404ms step_avg:61.35ms
step:301/2330 train_time:18420ms step_avg:61.20ms
step:302/2330 train_time:18477ms step_avg:61.18ms
step:303/2330 train_time:18532ms step_avg:61.16ms
step:304/2330 train_time:18590ms step_avg:61.15ms
step:305/2330 train_time:18645ms step_avg:61.13ms
step:306/2330 train_time:18704ms step_avg:61.12ms
step:307/2330 train_time:18759ms step_avg:61.10ms
step:308/2330 train_time:18817ms step_avg:61.09ms
step:309/2330 train_time:18872ms step_avg:61.07ms
step:310/2330 train_time:18930ms step_avg:61.06ms
step:311/2330 train_time:18985ms step_avg:61.05ms
step:312/2330 train_time:19044ms step_avg:61.04ms
step:313/2330 train_time:19099ms step_avg:61.02ms
step:314/2330 train_time:19157ms step_avg:61.01ms
step:315/2330 train_time:19213ms step_avg:60.99ms
step:316/2330 train_time:19271ms step_avg:60.98ms
step:317/2330 train_time:19332ms step_avg:60.98ms
step:318/2330 train_time:19393ms step_avg:60.98ms
step:319/2330 train_time:19450ms step_avg:60.97ms
step:320/2330 train_time:19511ms step_avg:60.97ms
step:321/2330 train_time:19567ms step_avg:60.96ms
step:322/2330 train_time:19627ms step_avg:60.95ms
step:323/2330 train_time:19682ms step_avg:60.94ms
step:324/2330 train_time:19742ms step_avg:60.93ms
step:325/2330 train_time:19797ms step_avg:60.91ms
step:326/2330 train_time:19856ms step_avg:60.91ms
step:327/2330 train_time:19911ms step_avg:60.89ms
step:328/2330 train_time:19969ms step_avg:60.88ms
step:329/2330 train_time:20024ms step_avg:60.86ms
step:330/2330 train_time:20083ms step_avg:60.86ms
step:331/2330 train_time:20138ms step_avg:60.84ms
step:332/2330 train_time:20197ms step_avg:60.83ms
step:333/2330 train_time:20253ms step_avg:60.82ms
step:334/2330 train_time:20312ms step_avg:60.81ms
step:335/2330 train_time:20368ms step_avg:60.80ms
step:336/2330 train_time:20429ms step_avg:60.80ms
step:337/2330 train_time:20485ms step_avg:60.79ms
step:338/2330 train_time:20546ms step_avg:60.79ms
step:339/2330 train_time:20601ms step_avg:60.77ms
step:340/2330 train_time:20661ms step_avg:60.77ms
step:341/2330 train_time:20717ms step_avg:60.75ms
step:342/2330 train_time:20776ms step_avg:60.75ms
step:343/2330 train_time:20832ms step_avg:60.74ms
step:344/2330 train_time:20891ms step_avg:60.73ms
step:345/2330 train_time:20946ms step_avg:60.71ms
step:346/2330 train_time:21005ms step_avg:60.71ms
step:347/2330 train_time:21060ms step_avg:60.69ms
step:348/2330 train_time:21118ms step_avg:60.68ms
step:349/2330 train_time:21173ms step_avg:60.67ms
step:350/2330 train_time:21233ms step_avg:60.67ms
step:351/2330 train_time:21289ms step_avg:60.65ms
step:352/2330 train_time:21348ms step_avg:60.65ms
step:353/2330 train_time:21404ms step_avg:60.63ms
step:354/2330 train_time:21464ms step_avg:60.63ms
step:355/2330 train_time:21520ms step_avg:60.62ms
step:356/2330 train_time:21579ms step_avg:60.61ms
step:357/2330 train_time:21635ms step_avg:60.60ms
step:358/2330 train_time:21694ms step_avg:60.60ms
step:359/2330 train_time:21750ms step_avg:60.58ms
step:360/2330 train_time:21809ms step_avg:60.58ms
step:361/2330 train_time:21865ms step_avg:60.57ms
step:362/2330 train_time:21924ms step_avg:60.56ms
step:363/2330 train_time:21980ms step_avg:60.55ms
step:364/2330 train_time:22038ms step_avg:60.55ms
step:365/2330 train_time:22094ms step_avg:60.53ms
step:366/2330 train_time:22153ms step_avg:60.53ms
step:367/2330 train_time:22209ms step_avg:60.51ms
step:368/2330 train_time:22267ms step_avg:60.51ms
step:369/2330 train_time:22322ms step_avg:60.49ms
step:370/2330 train_time:22383ms step_avg:60.49ms
step:371/2330 train_time:22439ms step_avg:60.48ms
step:372/2330 train_time:22498ms step_avg:60.48ms
step:373/2330 train_time:22554ms step_avg:60.47ms
step:374/2330 train_time:22613ms step_avg:60.46ms
step:375/2330 train_time:22669ms step_avg:60.45ms
step:376/2330 train_time:22728ms step_avg:60.45ms
step:377/2330 train_time:22784ms step_avg:60.43ms
step:378/2330 train_time:22844ms step_avg:60.43ms
step:379/2330 train_time:22899ms step_avg:60.42ms
step:380/2330 train_time:22958ms step_avg:60.42ms
step:381/2330 train_time:23013ms step_avg:60.40ms
step:382/2330 train_time:23073ms step_avg:60.40ms
step:383/2330 train_time:23129ms step_avg:60.39ms
step:384/2330 train_time:23188ms step_avg:60.39ms
step:385/2330 train_time:23244ms step_avg:60.37ms
step:386/2330 train_time:23303ms step_avg:60.37ms
step:387/2330 train_time:23360ms step_avg:60.36ms
step:388/2330 train_time:23418ms step_avg:60.36ms
step:389/2330 train_time:23475ms step_avg:60.35ms
step:390/2330 train_time:23533ms step_avg:60.34ms
step:391/2330 train_time:23590ms step_avg:60.33ms
step:392/2330 train_time:23648ms step_avg:60.33ms
step:393/2330 train_time:23705ms step_avg:60.32ms
step:394/2330 train_time:23764ms step_avg:60.31ms
step:395/2330 train_time:23820ms step_avg:60.30ms
step:396/2330 train_time:23878ms step_avg:60.30ms
step:397/2330 train_time:23934ms step_avg:60.29ms
step:398/2330 train_time:23993ms step_avg:60.29ms
step:399/2330 train_time:24049ms step_avg:60.27ms
step:400/2330 train_time:24108ms step_avg:60.27ms
step:401/2330 train_time:24163ms step_avg:60.26ms
step:402/2330 train_time:24223ms step_avg:60.26ms
step:403/2330 train_time:24278ms step_avg:60.24ms
step:404/2330 train_time:24338ms step_avg:60.24ms
step:405/2330 train_time:24393ms step_avg:60.23ms
step:406/2330 train_time:24452ms step_avg:60.23ms
step:407/2330 train_time:24508ms step_avg:60.22ms
step:408/2330 train_time:24567ms step_avg:60.21ms
step:409/2330 train_time:24623ms step_avg:60.20ms
step:410/2330 train_time:24683ms step_avg:60.20ms
step:411/2330 train_time:24739ms step_avg:60.19ms
step:412/2330 train_time:24798ms step_avg:60.19ms
step:413/2330 train_time:24854ms step_avg:60.18ms
step:414/2330 train_time:24912ms step_avg:60.17ms
step:415/2330 train_time:24968ms step_avg:60.16ms
step:416/2330 train_time:25027ms step_avg:60.16ms
step:417/2330 train_time:25082ms step_avg:60.15ms
step:418/2330 train_time:25142ms step_avg:60.15ms
step:419/2330 train_time:25198ms step_avg:60.14ms
step:420/2330 train_time:25257ms step_avg:60.13ms
step:421/2330 train_time:25313ms step_avg:60.13ms
step:422/2330 train_time:25372ms step_avg:60.12ms
step:423/2330 train_time:25428ms step_avg:60.11ms
step:424/2330 train_time:25487ms step_avg:60.11ms
step:425/2330 train_time:25543ms step_avg:60.10ms
step:426/2330 train_time:25603ms step_avg:60.10ms
step:427/2330 train_time:25659ms step_avg:60.09ms
step:428/2330 train_time:25717ms step_avg:60.09ms
step:429/2330 train_time:25773ms step_avg:60.08ms
step:430/2330 train_time:25832ms step_avg:60.08ms
step:431/2330 train_time:25889ms step_avg:60.07ms
step:432/2330 train_time:25948ms step_avg:60.07ms
step:433/2330 train_time:26004ms step_avg:60.06ms
step:434/2330 train_time:26063ms step_avg:60.05ms
step:435/2330 train_time:26119ms step_avg:60.04ms
step:436/2330 train_time:26179ms step_avg:60.04ms
step:437/2330 train_time:26234ms step_avg:60.03ms
step:438/2330 train_time:26293ms step_avg:60.03ms
step:439/2330 train_time:26349ms step_avg:60.02ms
step:440/2330 train_time:26408ms step_avg:60.02ms
step:441/2330 train_time:26464ms step_avg:60.01ms
step:442/2330 train_time:26523ms step_avg:60.01ms
step:443/2330 train_time:26580ms step_avg:60.00ms
step:444/2330 train_time:26639ms step_avg:60.00ms
step:445/2330 train_time:26694ms step_avg:59.99ms
step:446/2330 train_time:26754ms step_avg:59.99ms
step:447/2330 train_time:26810ms step_avg:59.98ms
step:448/2330 train_time:26868ms step_avg:59.97ms
step:449/2330 train_time:26924ms step_avg:59.96ms
step:450/2330 train_time:26983ms step_avg:59.96ms
step:451/2330 train_time:27039ms step_avg:59.95ms
step:452/2330 train_time:27097ms step_avg:59.95ms
step:453/2330 train_time:27154ms step_avg:59.94ms
step:454/2330 train_time:27212ms step_avg:59.94ms
step:455/2330 train_time:27268ms step_avg:59.93ms
step:456/2330 train_time:27327ms step_avg:59.93ms
step:457/2330 train_time:27383ms step_avg:59.92ms
step:458/2330 train_time:27442ms step_avg:59.92ms
step:459/2330 train_time:27498ms step_avg:59.91ms
step:460/2330 train_time:27557ms step_avg:59.91ms
step:461/2330 train_time:27613ms step_avg:59.90ms
step:462/2330 train_time:27672ms step_avg:59.90ms
step:463/2330 train_time:27728ms step_avg:59.89ms
step:464/2330 train_time:27787ms step_avg:59.88ms
step:465/2330 train_time:27843ms step_avg:59.88ms
step:466/2330 train_time:27901ms step_avg:59.87ms
step:467/2330 train_time:27957ms step_avg:59.86ms
step:468/2330 train_time:28016ms step_avg:59.86ms
step:469/2330 train_time:28071ms step_avg:59.85ms
step:470/2330 train_time:28131ms step_avg:59.85ms
step:471/2330 train_time:28187ms step_avg:59.84ms
step:472/2330 train_time:28246ms step_avg:59.84ms
step:473/2330 train_time:28302ms step_avg:59.83ms
step:474/2330 train_time:28361ms step_avg:59.83ms
step:475/2330 train_time:28416ms step_avg:59.82ms
step:476/2330 train_time:28476ms step_avg:59.82ms
step:477/2330 train_time:28533ms step_avg:59.82ms
step:478/2330 train_time:28592ms step_avg:59.82ms
step:479/2330 train_time:28649ms step_avg:59.81ms
step:480/2330 train_time:28708ms step_avg:59.81ms
step:481/2330 train_time:28764ms step_avg:59.80ms
step:482/2330 train_time:28822ms step_avg:59.80ms
step:483/2330 train_time:28878ms step_avg:59.79ms
step:484/2330 train_time:28938ms step_avg:59.79ms
step:485/2330 train_time:28994ms step_avg:59.78ms
step:486/2330 train_time:29052ms step_avg:59.78ms
step:487/2330 train_time:29109ms step_avg:59.77ms
step:488/2330 train_time:29168ms step_avg:59.77ms
step:489/2330 train_time:29225ms step_avg:59.76ms
step:490/2330 train_time:29283ms step_avg:59.76ms
step:491/2330 train_time:29338ms step_avg:59.75ms
step:492/2330 train_time:29398ms step_avg:59.75ms
step:493/2330 train_time:29454ms step_avg:59.74ms
step:494/2330 train_time:29513ms step_avg:59.74ms
step:495/2330 train_time:29568ms step_avg:59.73ms
step:496/2330 train_time:29627ms step_avg:59.73ms
step:497/2330 train_time:29683ms step_avg:59.72ms
step:498/2330 train_time:29744ms step_avg:59.73ms
step:499/2330 train_time:29799ms step_avg:59.72ms
step:500/2330 train_time:29858ms step_avg:59.72ms
step:500/2330 val_loss:6.0811 train_time:29937ms step_avg:59.87ms
step:501/2330 train_time:29955ms step_avg:59.79ms
step:502/2330 train_time:29977ms step_avg:59.72ms
step:503/2330 train_time:30033ms step_avg:59.71ms
step:504/2330 train_time:30096ms step_avg:59.71ms
step:505/2330 train_time:30152ms step_avg:59.71ms
step:506/2330 train_time:30217ms step_avg:59.72ms
step:507/2330 train_time:30272ms step_avg:59.71ms
step:508/2330 train_time:30332ms step_avg:59.71ms
step:509/2330 train_time:30387ms step_avg:59.70ms
step:510/2330 train_time:30447ms step_avg:59.70ms
step:511/2330 train_time:30502ms step_avg:59.69ms
step:512/2330 train_time:30562ms step_avg:59.69ms
step:513/2330 train_time:30617ms step_avg:59.68ms
step:514/2330 train_time:30676ms step_avg:59.68ms
step:515/2330 train_time:30731ms step_avg:59.67ms
step:516/2330 train_time:30790ms step_avg:59.67ms
step:517/2330 train_time:30845ms step_avg:59.66ms
step:518/2330 train_time:30905ms step_avg:59.66ms
step:519/2330 train_time:30961ms step_avg:59.65ms
step:520/2330 train_time:31021ms step_avg:59.66ms
step:521/2330 train_time:31077ms step_avg:59.65ms
step:522/2330 train_time:31137ms step_avg:59.65ms
step:523/2330 train_time:31194ms step_avg:59.64ms
step:524/2330 train_time:31253ms step_avg:59.64ms
step:525/2330 train_time:31309ms step_avg:59.64ms
step:526/2330 train_time:31369ms step_avg:59.64ms
step:527/2330 train_time:31425ms step_avg:59.63ms
step:528/2330 train_time:31484ms step_avg:59.63ms
step:529/2330 train_time:31540ms step_avg:59.62ms
step:530/2330 train_time:31599ms step_avg:59.62ms
step:531/2330 train_time:31654ms step_avg:59.61ms
step:532/2330 train_time:31713ms step_avg:59.61ms
step:533/2330 train_time:31769ms step_avg:59.60ms
step:534/2330 train_time:31828ms step_avg:59.60ms
step:535/2330 train_time:31883ms step_avg:59.59ms
step:536/2330 train_time:31943ms step_avg:59.59ms
step:537/2330 train_time:31998ms step_avg:59.59ms
step:538/2330 train_time:32058ms step_avg:59.59ms
step:539/2330 train_time:32114ms step_avg:59.58ms
step:540/2330 train_time:32175ms step_avg:59.58ms
step:541/2330 train_time:32231ms step_avg:59.58ms
step:542/2330 train_time:32291ms step_avg:59.58ms
step:543/2330 train_time:32346ms step_avg:59.57ms
step:544/2330 train_time:32406ms step_avg:59.57ms
step:545/2330 train_time:32462ms step_avg:59.56ms
step:546/2330 train_time:32521ms step_avg:59.56ms
step:547/2330 train_time:32577ms step_avg:59.56ms
step:548/2330 train_time:32635ms step_avg:59.55ms
step:549/2330 train_time:32691ms step_avg:59.55ms
step:550/2330 train_time:32749ms step_avg:59.54ms
step:551/2330 train_time:32805ms step_avg:59.54ms
step:552/2330 train_time:32864ms step_avg:59.54ms
step:553/2330 train_time:32920ms step_avg:59.53ms
step:554/2330 train_time:32980ms step_avg:59.53ms
step:555/2330 train_time:33036ms step_avg:59.52ms
step:556/2330 train_time:33095ms step_avg:59.52ms
step:557/2330 train_time:33151ms step_avg:59.52ms
step:558/2330 train_time:33210ms step_avg:59.52ms
step:559/2330 train_time:33266ms step_avg:59.51ms
step:560/2330 train_time:33326ms step_avg:59.51ms
step:561/2330 train_time:33382ms step_avg:59.50ms
step:562/2330 train_time:33441ms step_avg:59.50ms
step:563/2330 train_time:33496ms step_avg:59.50ms
step:564/2330 train_time:33556ms step_avg:59.50ms
step:565/2330 train_time:33612ms step_avg:59.49ms
step:566/2330 train_time:33671ms step_avg:59.49ms
step:567/2330 train_time:33727ms step_avg:59.48ms
step:568/2330 train_time:33785ms step_avg:59.48ms
step:569/2330 train_time:33841ms step_avg:59.47ms
step:570/2330 train_time:33900ms step_avg:59.47ms
step:571/2330 train_time:33955ms step_avg:59.47ms
step:572/2330 train_time:34015ms step_avg:59.47ms
step:573/2330 train_time:34071ms step_avg:59.46ms
step:574/2330 train_time:34130ms step_avg:59.46ms
step:575/2330 train_time:34187ms step_avg:59.46ms
step:576/2330 train_time:34247ms step_avg:59.46ms
step:577/2330 train_time:34302ms step_avg:59.45ms
step:578/2330 train_time:34362ms step_avg:59.45ms
step:579/2330 train_time:34417ms step_avg:59.44ms
step:580/2330 train_time:34477ms step_avg:59.44ms
step:581/2330 train_time:34533ms step_avg:59.44ms
step:582/2330 train_time:34591ms step_avg:59.44ms
step:583/2330 train_time:34647ms step_avg:59.43ms
step:584/2330 train_time:34706ms step_avg:59.43ms
step:585/2330 train_time:34762ms step_avg:59.42ms
step:586/2330 train_time:34821ms step_avg:59.42ms
step:587/2330 train_time:34877ms step_avg:59.42ms
step:588/2330 train_time:34936ms step_avg:59.42ms
step:589/2330 train_time:34992ms step_avg:59.41ms
step:590/2330 train_time:35051ms step_avg:59.41ms
step:591/2330 train_time:35107ms step_avg:59.40ms
step:592/2330 train_time:35167ms step_avg:59.40ms
step:593/2330 train_time:35223ms step_avg:59.40ms
step:594/2330 train_time:35283ms step_avg:59.40ms
step:595/2330 train_time:35338ms step_avg:59.39ms
step:596/2330 train_time:35397ms step_avg:59.39ms
step:597/2330 train_time:35453ms step_avg:59.39ms
step:598/2330 train_time:35512ms step_avg:59.38ms
step:599/2330 train_time:35568ms step_avg:59.38ms
step:600/2330 train_time:35627ms step_avg:59.38ms
step:601/2330 train_time:35683ms step_avg:59.37ms
step:602/2330 train_time:35742ms step_avg:59.37ms
step:603/2330 train_time:35798ms step_avg:59.37ms
step:604/2330 train_time:35857ms step_avg:59.37ms
step:605/2330 train_time:35913ms step_avg:59.36ms
step:606/2330 train_time:35971ms step_avg:59.36ms
step:607/2330 train_time:36027ms step_avg:59.35ms
step:608/2330 train_time:36087ms step_avg:59.35ms
step:609/2330 train_time:36142ms step_avg:59.35ms
step:610/2330 train_time:36203ms step_avg:59.35ms
step:611/2330 train_time:36258ms step_avg:59.34ms
step:612/2330 train_time:36318ms step_avg:59.34ms
step:613/2330 train_time:36374ms step_avg:59.34ms
step:614/2330 train_time:36433ms step_avg:59.34ms
step:615/2330 train_time:36490ms step_avg:59.33ms
step:616/2330 train_time:36549ms step_avg:59.33ms
step:617/2330 train_time:36605ms step_avg:59.33ms
step:618/2330 train_time:36665ms step_avg:59.33ms
step:619/2330 train_time:36720ms step_avg:59.32ms
step:620/2330 train_time:36781ms step_avg:59.32ms
step:621/2330 train_time:36837ms step_avg:59.32ms
step:622/2330 train_time:36896ms step_avg:59.32ms
step:623/2330 train_time:36952ms step_avg:59.31ms
step:624/2330 train_time:37011ms step_avg:59.31ms
step:625/2330 train_time:37067ms step_avg:59.31ms
step:626/2330 train_time:37126ms step_avg:59.31ms
step:627/2330 train_time:37182ms step_avg:59.30ms
step:628/2330 train_time:37243ms step_avg:59.30ms
step:629/2330 train_time:37298ms step_avg:59.30ms
step:630/2330 train_time:37358ms step_avg:59.30ms
step:631/2330 train_time:37414ms step_avg:59.29ms
step:632/2330 train_time:37473ms step_avg:59.29ms
step:633/2330 train_time:37529ms step_avg:59.29ms
step:634/2330 train_time:37589ms step_avg:59.29ms
step:635/2330 train_time:37644ms step_avg:59.28ms
step:636/2330 train_time:37704ms step_avg:59.28ms
step:637/2330 train_time:37759ms step_avg:59.28ms
step:638/2330 train_time:37820ms step_avg:59.28ms
step:639/2330 train_time:37875ms step_avg:59.27ms
step:640/2330 train_time:37934ms step_avg:59.27ms
step:641/2330 train_time:37990ms step_avg:59.27ms
step:642/2330 train_time:38049ms step_avg:59.27ms
step:643/2330 train_time:38104ms step_avg:59.26ms
step:644/2330 train_time:38164ms step_avg:59.26ms
step:645/2330 train_time:38220ms step_avg:59.26ms
step:646/2330 train_time:38279ms step_avg:59.26ms
step:647/2330 train_time:38336ms step_avg:59.25ms
step:648/2330 train_time:38395ms step_avg:59.25ms
step:649/2330 train_time:38451ms step_avg:59.25ms
step:650/2330 train_time:38510ms step_avg:59.25ms
step:651/2330 train_time:38566ms step_avg:59.24ms
step:652/2330 train_time:38625ms step_avg:59.24ms
step:653/2330 train_time:38681ms step_avg:59.24ms
step:654/2330 train_time:38741ms step_avg:59.24ms
step:655/2330 train_time:38796ms step_avg:59.23ms
step:656/2330 train_time:38856ms step_avg:59.23ms
step:657/2330 train_time:38911ms step_avg:59.23ms
step:658/2330 train_time:38970ms step_avg:59.23ms
step:659/2330 train_time:39027ms step_avg:59.22ms
step:660/2330 train_time:39086ms step_avg:59.22ms
step:661/2330 train_time:39142ms step_avg:59.22ms
step:662/2330 train_time:39201ms step_avg:59.22ms
step:663/2330 train_time:39257ms step_avg:59.21ms
step:664/2330 train_time:39317ms step_avg:59.21ms
step:665/2330 train_time:39372ms step_avg:59.21ms
step:666/2330 train_time:39431ms step_avg:59.21ms
step:667/2330 train_time:39487ms step_avg:59.20ms
step:668/2330 train_time:39546ms step_avg:59.20ms
step:669/2330 train_time:39602ms step_avg:59.20ms
step:670/2330 train_time:39662ms step_avg:59.20ms
step:671/2330 train_time:39717ms step_avg:59.19ms
step:672/2330 train_time:39777ms step_avg:59.19ms
step:673/2330 train_time:39833ms step_avg:59.19ms
step:674/2330 train_time:39893ms step_avg:59.19ms
step:675/2330 train_time:39949ms step_avg:59.18ms
step:676/2330 train_time:40008ms step_avg:59.18ms
step:677/2330 train_time:40064ms step_avg:59.18ms
step:678/2330 train_time:40123ms step_avg:59.18ms
step:679/2330 train_time:40179ms step_avg:59.17ms
step:680/2330 train_time:40239ms step_avg:59.17ms
step:681/2330 train_time:40295ms step_avg:59.17ms
step:682/2330 train_time:40353ms step_avg:59.17ms
step:683/2330 train_time:40409ms step_avg:59.16ms
step:684/2330 train_time:40468ms step_avg:59.16ms
step:685/2330 train_time:40525ms step_avg:59.16ms
step:686/2330 train_time:40584ms step_avg:59.16ms
step:687/2330 train_time:40640ms step_avg:59.16ms
step:688/2330 train_time:40699ms step_avg:59.16ms
step:689/2330 train_time:40755ms step_avg:59.15ms
step:690/2330 train_time:40814ms step_avg:59.15ms
step:691/2330 train_time:40869ms step_avg:59.14ms
step:692/2330 train_time:40929ms step_avg:59.15ms
step:693/2330 train_time:40985ms step_avg:59.14ms
step:694/2330 train_time:41044ms step_avg:59.14ms
step:695/2330 train_time:41099ms step_avg:59.14ms
step:696/2330 train_time:41160ms step_avg:59.14ms
step:697/2330 train_time:41215ms step_avg:59.13ms
step:698/2330 train_time:41275ms step_avg:59.13ms
step:699/2330 train_time:41331ms step_avg:59.13ms
step:700/2330 train_time:41390ms step_avg:59.13ms
step:701/2330 train_time:41446ms step_avg:59.12ms
step:702/2330 train_time:41506ms step_avg:59.13ms
step:703/2330 train_time:41562ms step_avg:59.12ms
step:704/2330 train_time:41621ms step_avg:59.12ms
step:705/2330 train_time:41677ms step_avg:59.12ms
step:706/2330 train_time:41736ms step_avg:59.12ms
step:707/2330 train_time:41792ms step_avg:59.11ms
step:708/2330 train_time:41851ms step_avg:59.11ms
step:709/2330 train_time:41906ms step_avg:59.11ms
step:710/2330 train_time:41966ms step_avg:59.11ms
step:711/2330 train_time:42022ms step_avg:59.10ms
step:712/2330 train_time:42081ms step_avg:59.10ms
step:713/2330 train_time:42137ms step_avg:59.10ms
step:714/2330 train_time:42196ms step_avg:59.10ms
step:715/2330 train_time:42252ms step_avg:59.09ms
step:716/2330 train_time:42311ms step_avg:59.09ms
step:717/2330 train_time:42367ms step_avg:59.09ms
step:718/2330 train_time:42428ms step_avg:59.09ms
step:719/2330 train_time:42484ms step_avg:59.09ms
step:720/2330 train_time:42543ms step_avg:59.09ms
step:721/2330 train_time:42599ms step_avg:59.08ms
step:722/2330 train_time:42659ms step_avg:59.08ms
step:723/2330 train_time:42714ms step_avg:59.08ms
step:724/2330 train_time:42773ms step_avg:59.08ms
step:725/2330 train_time:42829ms step_avg:59.07ms
step:726/2330 train_time:42888ms step_avg:59.07ms
step:727/2330 train_time:42943ms step_avg:59.07ms
step:728/2330 train_time:43003ms step_avg:59.07ms
step:729/2330 train_time:43058ms step_avg:59.06ms
step:730/2330 train_time:43119ms step_avg:59.07ms
step:731/2330 train_time:43174ms step_avg:59.06ms
step:732/2330 train_time:43234ms step_avg:59.06ms
step:733/2330 train_time:43290ms step_avg:59.06ms
step:734/2330 train_time:43349ms step_avg:59.06ms
step:735/2330 train_time:43405ms step_avg:59.05ms
step:736/2330 train_time:43464ms step_avg:59.05ms
step:737/2330 train_time:43520ms step_avg:59.05ms
step:738/2330 train_time:43579ms step_avg:59.05ms
step:739/2330 train_time:43635ms step_avg:59.05ms
step:740/2330 train_time:43694ms step_avg:59.05ms
step:741/2330 train_time:43750ms step_avg:59.04ms
step:742/2330 train_time:43810ms step_avg:59.04ms
step:743/2330 train_time:43866ms step_avg:59.04ms
step:744/2330 train_time:43926ms step_avg:59.04ms
step:745/2330 train_time:43981ms step_avg:59.04ms
step:746/2330 train_time:44041ms step_avg:59.04ms
step:747/2330 train_time:44097ms step_avg:59.03ms
step:748/2330 train_time:44156ms step_avg:59.03ms
step:749/2330 train_time:44212ms step_avg:59.03ms
step:750/2330 train_time:44271ms step_avg:59.03ms
step:750/2330 val_loss:5.9010 train_time:44350ms step_avg:59.13ms
step:751/2330 train_time:44368ms step_avg:59.08ms
step:752/2330 train_time:44388ms step_avg:59.03ms
step:753/2330 train_time:44444ms step_avg:59.02ms
step:754/2330 train_time:44511ms step_avg:59.03ms
step:755/2330 train_time:44567ms step_avg:59.03ms
step:756/2330 train_time:44628ms step_avg:59.03ms
step:757/2330 train_time:44683ms step_avg:59.03ms
step:758/2330 train_time:44744ms step_avg:59.03ms
step:759/2330 train_time:44799ms step_avg:59.02ms
step:760/2330 train_time:44858ms step_avg:59.02ms
step:761/2330 train_time:44913ms step_avg:59.02ms
step:762/2330 train_time:44972ms step_avg:59.02ms
step:763/2330 train_time:45028ms step_avg:59.01ms
step:764/2330 train_time:45087ms step_avg:59.01ms
step:765/2330 train_time:45143ms step_avg:59.01ms
step:766/2330 train_time:45201ms step_avg:59.01ms
step:767/2330 train_time:45257ms step_avg:59.01ms
step:768/2330 train_time:45317ms step_avg:59.01ms
step:769/2330 train_time:45373ms step_avg:59.00ms
step:770/2330 train_time:45435ms step_avg:59.01ms
step:771/2330 train_time:45493ms step_avg:59.00ms
step:772/2330 train_time:45553ms step_avg:59.01ms
step:773/2330 train_time:45612ms step_avg:59.01ms
step:774/2330 train_time:45673ms step_avg:59.01ms
step:775/2330 train_time:45731ms step_avg:59.01ms
step:776/2330 train_time:45790ms step_avg:59.01ms
step:777/2330 train_time:45848ms step_avg:59.01ms
step:778/2330 train_time:45907ms step_avg:59.01ms
step:779/2330 train_time:45964ms step_avg:59.00ms
step:780/2330 train_time:46024ms step_avg:59.00ms
step:781/2330 train_time:46080ms step_avg:59.00ms
step:782/2330 train_time:46139ms step_avg:59.00ms
step:783/2330 train_time:46195ms step_avg:59.00ms
step:784/2330 train_time:46254ms step_avg:59.00ms
step:785/2330 train_time:46310ms step_avg:58.99ms
step:786/2330 train_time:46370ms step_avg:59.00ms
step:787/2330 train_time:46428ms step_avg:58.99ms
step:788/2330 train_time:46489ms step_avg:59.00ms
step:789/2330 train_time:46546ms step_avg:58.99ms
step:790/2330 train_time:46606ms step_avg:59.00ms
step:791/2330 train_time:46664ms step_avg:58.99ms
step:792/2330 train_time:46725ms step_avg:59.00ms
step:793/2330 train_time:46781ms step_avg:58.99ms
step:794/2330 train_time:46842ms step_avg:59.00ms
step:795/2330 train_time:46899ms step_avg:58.99ms
step:796/2330 train_time:46958ms step_avg:58.99ms
step:797/2330 train_time:47015ms step_avg:58.99ms
step:798/2330 train_time:47074ms step_avg:58.99ms
step:799/2330 train_time:47131ms step_avg:58.99ms
step:800/2330 train_time:47190ms step_avg:58.99ms
step:801/2330 train_time:47247ms step_avg:58.98ms
step:802/2330 train_time:47307ms step_avg:58.99ms
step:803/2330 train_time:47363ms step_avg:58.98ms
step:804/2330 train_time:47424ms step_avg:58.99ms
step:805/2330 train_time:47481ms step_avg:58.98ms
step:806/2330 train_time:47542ms step_avg:58.99ms
step:807/2330 train_time:47599ms step_avg:58.98ms
step:808/2330 train_time:47659ms step_avg:58.98ms
step:809/2330 train_time:47716ms step_avg:58.98ms
step:810/2330 train_time:47776ms step_avg:58.98ms
step:811/2330 train_time:47833ms step_avg:58.98ms
step:812/2330 train_time:47893ms step_avg:58.98ms
step:813/2330 train_time:47950ms step_avg:58.98ms
step:814/2330 train_time:48010ms step_avg:58.98ms
step:815/2330 train_time:48067ms step_avg:58.98ms
step:816/2330 train_time:48126ms step_avg:58.98ms
step:817/2330 train_time:48182ms step_avg:58.97ms
step:818/2330 train_time:48243ms step_avg:58.98ms
step:819/2330 train_time:48299ms step_avg:58.97ms
step:820/2330 train_time:48359ms step_avg:58.97ms
step:821/2330 train_time:48416ms step_avg:58.97ms
step:822/2330 train_time:48476ms step_avg:58.97ms
step:823/2330 train_time:48533ms step_avg:58.97ms
step:824/2330 train_time:48592ms step_avg:58.97ms
step:825/2330 train_time:48649ms step_avg:58.97ms
step:826/2330 train_time:48709ms step_avg:58.97ms
step:827/2330 train_time:48767ms step_avg:58.97ms
step:828/2330 train_time:48827ms step_avg:58.97ms
step:829/2330 train_time:48884ms step_avg:58.97ms
step:830/2330 train_time:48944ms step_avg:58.97ms
step:831/2330 train_time:49001ms step_avg:58.97ms
step:832/2330 train_time:49061ms step_avg:58.97ms
step:833/2330 train_time:49117ms step_avg:58.96ms
step:834/2330 train_time:49177ms step_avg:58.97ms
step:835/2330 train_time:49233ms step_avg:58.96ms
step:836/2330 train_time:49294ms step_avg:58.96ms
step:837/2330 train_time:49350ms step_avg:58.96ms
step:838/2330 train_time:49410ms step_avg:58.96ms
step:839/2330 train_time:49467ms step_avg:58.96ms
step:840/2330 train_time:49528ms step_avg:58.96ms
step:841/2330 train_time:49585ms step_avg:58.96ms
step:842/2330 train_time:49645ms step_avg:58.96ms
step:843/2330 train_time:49702ms step_avg:58.96ms
step:844/2330 train_time:49763ms step_avg:58.96ms
step:845/2330 train_time:49819ms step_avg:58.96ms
step:846/2330 train_time:49880ms step_avg:58.96ms
step:847/2330 train_time:49936ms step_avg:58.96ms
step:848/2330 train_time:49997ms step_avg:58.96ms
step:849/2330 train_time:50053ms step_avg:58.96ms
step:850/2330 train_time:50113ms step_avg:58.96ms
step:851/2330 train_time:50169ms step_avg:58.95ms
step:852/2330 train_time:50229ms step_avg:58.95ms
step:853/2330 train_time:50286ms step_avg:58.95ms
step:854/2330 train_time:50346ms step_avg:58.95ms
step:855/2330 train_time:50403ms step_avg:58.95ms
step:856/2330 train_time:50463ms step_avg:58.95ms
step:857/2330 train_time:50519ms step_avg:58.95ms
step:858/2330 train_time:50581ms step_avg:58.95ms
step:859/2330 train_time:50637ms step_avg:58.95ms
step:860/2330 train_time:50698ms step_avg:58.95ms
step:861/2330 train_time:50755ms step_avg:58.95ms
step:862/2330 train_time:50815ms step_avg:58.95ms
step:863/2330 train_time:50872ms step_avg:58.95ms
step:864/2330 train_time:50931ms step_avg:58.95ms
step:865/2330 train_time:50988ms step_avg:58.95ms
step:866/2330 train_time:51048ms step_avg:58.95ms
step:867/2330 train_time:51105ms step_avg:58.94ms
step:868/2330 train_time:51165ms step_avg:58.95ms
step:869/2330 train_time:51222ms step_avg:58.94ms
step:870/2330 train_time:51282ms step_avg:58.94ms
step:871/2330 train_time:51338ms step_avg:58.94ms
step:872/2330 train_time:51399ms step_avg:58.94ms
step:873/2330 train_time:51455ms step_avg:58.94ms
step:874/2330 train_time:51515ms step_avg:58.94ms
step:875/2330 train_time:51571ms step_avg:58.94ms
step:876/2330 train_time:51632ms step_avg:58.94ms
step:877/2330 train_time:51689ms step_avg:58.94ms
step:878/2330 train_time:51750ms step_avg:58.94ms
step:879/2330 train_time:51807ms step_avg:58.94ms
step:880/2330 train_time:51867ms step_avg:58.94ms
step:881/2330 train_time:51924ms step_avg:58.94ms
step:882/2330 train_time:51985ms step_avg:58.94ms
step:883/2330 train_time:52041ms step_avg:58.94ms
step:884/2330 train_time:52101ms step_avg:58.94ms
step:885/2330 train_time:52158ms step_avg:58.94ms
step:886/2330 train_time:52218ms step_avg:58.94ms
step:887/2330 train_time:52275ms step_avg:58.93ms
step:888/2330 train_time:52335ms step_avg:58.94ms
step:889/2330 train_time:52392ms step_avg:58.93ms
step:890/2330 train_time:52451ms step_avg:58.93ms
step:891/2330 train_time:52508ms step_avg:58.93ms
step:892/2330 train_time:52568ms step_avg:58.93ms
step:893/2330 train_time:52625ms step_avg:58.93ms
step:894/2330 train_time:52686ms step_avg:58.93ms
step:895/2330 train_time:52742ms step_avg:58.93ms
step:896/2330 train_time:52804ms step_avg:58.93ms
step:897/2330 train_time:52861ms step_avg:58.93ms
step:898/2330 train_time:52921ms step_avg:58.93ms
step:899/2330 train_time:52977ms step_avg:58.93ms
step:900/2330 train_time:53038ms step_avg:58.93ms
step:901/2330 train_time:53094ms step_avg:58.93ms
step:902/2330 train_time:53155ms step_avg:58.93ms
step:903/2330 train_time:53212ms step_avg:58.93ms
step:904/2330 train_time:53272ms step_avg:58.93ms
step:905/2330 train_time:53329ms step_avg:58.93ms
step:906/2330 train_time:53388ms step_avg:58.93ms
step:907/2330 train_time:53445ms step_avg:58.92ms
step:908/2330 train_time:53505ms step_avg:58.93ms
step:909/2330 train_time:53562ms step_avg:58.92ms
step:910/2330 train_time:53621ms step_avg:58.92ms
step:911/2330 train_time:53677ms step_avg:58.92ms
step:912/2330 train_time:53738ms step_avg:58.92ms
step:913/2330 train_time:53795ms step_avg:58.92ms
step:914/2330 train_time:53854ms step_avg:58.92ms
step:915/2330 train_time:53911ms step_avg:58.92ms
step:916/2330 train_time:53971ms step_avg:58.92ms
step:917/2330 train_time:54028ms step_avg:58.92ms
step:918/2330 train_time:54088ms step_avg:58.92ms
step:919/2330 train_time:54145ms step_avg:58.92ms
step:920/2330 train_time:54205ms step_avg:58.92ms
step:921/2330 train_time:54261ms step_avg:58.92ms
step:922/2330 train_time:54321ms step_avg:58.92ms
step:923/2330 train_time:54378ms step_avg:58.91ms
step:924/2330 train_time:54438ms step_avg:58.92ms
step:925/2330 train_time:54495ms step_avg:58.91ms
step:926/2330 train_time:54555ms step_avg:58.92ms
step:927/2330 train_time:54612ms step_avg:58.91ms
step:928/2330 train_time:54672ms step_avg:58.91ms
step:929/2330 train_time:54729ms step_avg:58.91ms
step:930/2330 train_time:54789ms step_avg:58.91ms
step:931/2330 train_time:54846ms step_avg:58.91ms
step:932/2330 train_time:54906ms step_avg:58.91ms
step:933/2330 train_time:54963ms step_avg:58.91ms
step:934/2330 train_time:55023ms step_avg:58.91ms
step:935/2330 train_time:55079ms step_avg:58.91ms
step:936/2330 train_time:55140ms step_avg:58.91ms
step:937/2330 train_time:55197ms step_avg:58.91ms
step:938/2330 train_time:55256ms step_avg:58.91ms
step:939/2330 train_time:55313ms step_avg:58.91ms
step:940/2330 train_time:55372ms step_avg:58.91ms
step:941/2330 train_time:55430ms step_avg:58.91ms
step:942/2330 train_time:55489ms step_avg:58.91ms
step:943/2330 train_time:55546ms step_avg:58.90ms
step:944/2330 train_time:55607ms step_avg:58.91ms
step:945/2330 train_time:55664ms step_avg:58.90ms
step:946/2330 train_time:55724ms step_avg:58.91ms
step:947/2330 train_time:55781ms step_avg:58.90ms
step:948/2330 train_time:55841ms step_avg:58.90ms
step:949/2330 train_time:55898ms step_avg:58.90ms
step:950/2330 train_time:55957ms step_avg:58.90ms
step:951/2330 train_time:56014ms step_avg:58.90ms
step:952/2330 train_time:56073ms step_avg:58.90ms
step:953/2330 train_time:56130ms step_avg:58.90ms
step:954/2330 train_time:56191ms step_avg:58.90ms
step:955/2330 train_time:56248ms step_avg:58.90ms
step:956/2330 train_time:56307ms step_avg:58.90ms
step:957/2330 train_time:56364ms step_avg:58.90ms
step:958/2330 train_time:56424ms step_avg:58.90ms
step:959/2330 train_time:56481ms step_avg:58.90ms
step:960/2330 train_time:56542ms step_avg:58.90ms
step:961/2330 train_time:56598ms step_avg:58.89ms
step:962/2330 train_time:56658ms step_avg:58.90ms
step:963/2330 train_time:56715ms step_avg:58.89ms
step:964/2330 train_time:56775ms step_avg:58.90ms
step:965/2330 train_time:56832ms step_avg:58.89ms
step:966/2330 train_time:56892ms step_avg:58.89ms
step:967/2330 train_time:56949ms step_avg:58.89ms
step:968/2330 train_time:57009ms step_avg:58.89ms
step:969/2330 train_time:57066ms step_avg:58.89ms
step:970/2330 train_time:57126ms step_avg:58.89ms
step:971/2330 train_time:57182ms step_avg:58.89ms
step:972/2330 train_time:57244ms step_avg:58.89ms
step:973/2330 train_time:57300ms step_avg:58.89ms
step:974/2330 train_time:57361ms step_avg:58.89ms
step:975/2330 train_time:57418ms step_avg:58.89ms
step:976/2330 train_time:57478ms step_avg:58.89ms
step:977/2330 train_time:57535ms step_avg:58.89ms
step:978/2330 train_time:57594ms step_avg:58.89ms
step:979/2330 train_time:57651ms step_avg:58.89ms
step:980/2330 train_time:57711ms step_avg:58.89ms
step:981/2330 train_time:57768ms step_avg:58.89ms
step:982/2330 train_time:57828ms step_avg:58.89ms
step:983/2330 train_time:57886ms step_avg:58.89ms
step:984/2330 train_time:57945ms step_avg:58.89ms
step:985/2330 train_time:58002ms step_avg:58.88ms
step:986/2330 train_time:58061ms step_avg:58.89ms
step:987/2330 train_time:58118ms step_avg:58.88ms
step:988/2330 train_time:58178ms step_avg:58.88ms
step:989/2330 train_time:58235ms step_avg:58.88ms
step:990/2330 train_time:58294ms step_avg:58.88ms
step:991/2330 train_time:58351ms step_avg:58.88ms
step:992/2330 train_time:58411ms step_avg:58.88ms
step:993/2330 train_time:58469ms step_avg:58.88ms
step:994/2330 train_time:58528ms step_avg:58.88ms
step:995/2330 train_time:58585ms step_avg:58.88ms
step:996/2330 train_time:58644ms step_avg:58.88ms
step:997/2330 train_time:58701ms step_avg:58.88ms
step:998/2330 train_time:58761ms step_avg:58.88ms
step:999/2330 train_time:58818ms step_avg:58.88ms
step:1000/2330 train_time:58878ms step_avg:58.88ms
step:1000/2330 val_loss:5.5896 train_time:58958ms step_avg:58.96ms
step:1001/2330 train_time:58978ms step_avg:58.92ms
step:1002/2330 train_time:58999ms step_avg:58.88ms
step:1003/2330 train_time:59051ms step_avg:58.87ms
step:1004/2330 train_time:59111ms step_avg:58.88ms
step:1005/2330 train_time:59168ms step_avg:58.87ms
step:1006/2330 train_time:59229ms step_avg:58.88ms
step:1007/2330 train_time:59285ms step_avg:58.87ms
step:1008/2330 train_time:59344ms step_avg:58.87ms
step:1009/2330 train_time:59400ms step_avg:58.87ms
step:1010/2330 train_time:59459ms step_avg:58.87ms
step:1011/2330 train_time:59515ms step_avg:58.87ms
step:1012/2330 train_time:59575ms step_avg:58.87ms
step:1013/2330 train_time:59631ms step_avg:58.87ms
step:1014/2330 train_time:59690ms step_avg:58.87ms
step:1015/2330 train_time:59746ms step_avg:58.86ms
step:1016/2330 train_time:59805ms step_avg:58.86ms
step:1017/2330 train_time:59863ms step_avg:58.86ms
step:1018/2330 train_time:59926ms step_avg:58.87ms
step:1019/2330 train_time:59984ms step_avg:58.87ms
step:1020/2330 train_time:60047ms step_avg:58.87ms
step:1021/2330 train_time:60103ms step_avg:58.87ms
step:1022/2330 train_time:60164ms step_avg:58.87ms
step:1023/2330 train_time:60220ms step_avg:58.87ms
step:1024/2330 train_time:60281ms step_avg:58.87ms
step:1025/2330 train_time:60337ms step_avg:58.87ms
step:1026/2330 train_time:60397ms step_avg:58.87ms
step:1027/2330 train_time:60453ms step_avg:58.86ms
step:1028/2330 train_time:60512ms step_avg:58.86ms
step:1029/2330 train_time:60568ms step_avg:58.86ms
step:1030/2330 train_time:60627ms step_avg:58.86ms
step:1031/2330 train_time:60683ms step_avg:58.86ms
step:1032/2330 train_time:60743ms step_avg:58.86ms
step:1033/2330 train_time:60800ms step_avg:58.86ms
step:1034/2330 train_time:60860ms step_avg:58.86ms
step:1035/2330 train_time:60917ms step_avg:58.86ms
step:1036/2330 train_time:60979ms step_avg:58.86ms
step:1037/2330 train_time:61036ms step_avg:58.86ms
step:1038/2330 train_time:61097ms step_avg:58.86ms
step:1039/2330 train_time:61155ms step_avg:58.86ms
step:1040/2330 train_time:61214ms step_avg:58.86ms
step:1041/2330 train_time:61271ms step_avg:58.86ms
step:1042/2330 train_time:61331ms step_avg:58.86ms
step:1043/2330 train_time:61387ms step_avg:58.86ms
step:1044/2330 train_time:61447ms step_avg:58.86ms
step:1045/2330 train_time:61503ms step_avg:58.85ms
step:1046/2330 train_time:61563ms step_avg:58.86ms
step:1047/2330 train_time:61620ms step_avg:58.85ms
step:1048/2330 train_time:61680ms step_avg:58.85ms
step:1049/2330 train_time:61736ms step_avg:58.85ms
step:1050/2330 train_time:61796ms step_avg:58.85ms
step:1051/2330 train_time:61853ms step_avg:58.85ms
step:1052/2330 train_time:61912ms step_avg:58.85ms
step:1053/2330 train_time:61970ms step_avg:58.85ms
step:1054/2330 train_time:62031ms step_avg:58.85ms
step:1055/2330 train_time:62088ms step_avg:58.85ms
step:1056/2330 train_time:62148ms step_avg:58.85ms
step:1057/2330 train_time:62205ms step_avg:58.85ms
step:1058/2330 train_time:62265ms step_avg:58.85ms
step:1059/2330 train_time:62321ms step_avg:58.85ms
step:1060/2330 train_time:62382ms step_avg:58.85ms
step:1061/2330 train_time:62438ms step_avg:58.85ms
step:1062/2330 train_time:62500ms step_avg:58.85ms
step:1063/2330 train_time:62556ms step_avg:58.85ms
step:1064/2330 train_time:62616ms step_avg:58.85ms
step:1065/2330 train_time:62672ms step_avg:58.85ms
step:1066/2330 train_time:62731ms step_avg:58.85ms
step:1067/2330 train_time:62788ms step_avg:58.85ms
step:1068/2330 train_time:62848ms step_avg:58.85ms
step:1069/2330 train_time:62904ms step_avg:58.84ms
step:1070/2330 train_time:62965ms step_avg:58.85ms
step:1071/2330 train_time:63021ms step_avg:58.84ms
step:1072/2330 train_time:63083ms step_avg:58.85ms
step:1073/2330 train_time:63139ms step_avg:58.84ms
step:1074/2330 train_time:63200ms step_avg:58.85ms
step:1075/2330 train_time:63257ms step_avg:58.84ms
step:1076/2330 train_time:63317ms step_avg:58.85ms
step:1077/2330 train_time:63374ms step_avg:58.84ms
step:1078/2330 train_time:63434ms step_avg:58.84ms
step:1079/2330 train_time:63491ms step_avg:58.84ms
step:1080/2330 train_time:63551ms step_avg:58.84ms
step:1081/2330 train_time:63608ms step_avg:58.84ms
step:1082/2330 train_time:63668ms step_avg:58.84ms
step:1083/2330 train_time:63724ms step_avg:58.84ms
step:1084/2330 train_time:63785ms step_avg:58.84ms
step:1085/2330 train_time:63842ms step_avg:58.84ms
step:1086/2330 train_time:63902ms step_avg:58.84ms
step:1087/2330 train_time:63958ms step_avg:58.84ms
step:1088/2330 train_time:64018ms step_avg:58.84ms
step:1089/2330 train_time:64075ms step_avg:58.84ms
step:1090/2330 train_time:64135ms step_avg:58.84ms
step:1091/2330 train_time:64192ms step_avg:58.84ms
step:1092/2330 train_time:64252ms step_avg:58.84ms
step:1093/2330 train_time:64309ms step_avg:58.84ms
step:1094/2330 train_time:64368ms step_avg:58.84ms
step:1095/2330 train_time:64425ms step_avg:58.84ms
step:1096/2330 train_time:64486ms step_avg:58.84ms
step:1097/2330 train_time:64542ms step_avg:58.84ms
step:1098/2330 train_time:64602ms step_avg:58.84ms
step:1099/2330 train_time:64658ms step_avg:58.83ms
step:1100/2330 train_time:64719ms step_avg:58.84ms
step:1101/2330 train_time:64776ms step_avg:58.83ms
step:1102/2330 train_time:64835ms step_avg:58.83ms
step:1103/2330 train_time:64892ms step_avg:58.83ms
step:1104/2330 train_time:64951ms step_avg:58.83ms
step:1105/2330 train_time:65009ms step_avg:58.83ms
step:1106/2330 train_time:65068ms step_avg:58.83ms
step:1107/2330 train_time:65125ms step_avg:58.83ms
step:1108/2330 train_time:65185ms step_avg:58.83ms
step:1109/2330 train_time:65242ms step_avg:58.83ms
step:1110/2330 train_time:65303ms step_avg:58.83ms
step:1111/2330 train_time:65360ms step_avg:58.83ms
step:1112/2330 train_time:65419ms step_avg:58.83ms
step:1113/2330 train_time:65476ms step_avg:58.83ms
step:1114/2330 train_time:65535ms step_avg:58.83ms
step:1115/2330 train_time:65592ms step_avg:58.83ms
step:1116/2330 train_time:65652ms step_avg:58.83ms
step:1117/2330 train_time:65709ms step_avg:58.83ms
step:1118/2330 train_time:65769ms step_avg:58.83ms
step:1119/2330 train_time:65826ms step_avg:58.83ms
step:1120/2330 train_time:65886ms step_avg:58.83ms
step:1121/2330 train_time:65943ms step_avg:58.83ms
step:1122/2330 train_time:66004ms step_avg:58.83ms
step:1123/2330 train_time:66060ms step_avg:58.82ms
step:1124/2330 train_time:66121ms step_avg:58.83ms
step:1125/2330 train_time:66178ms step_avg:58.83ms
step:1126/2330 train_time:66238ms step_avg:58.83ms
step:1127/2330 train_time:66295ms step_avg:58.82ms
step:1128/2330 train_time:66355ms step_avg:58.83ms
step:1129/2330 train_time:66412ms step_avg:58.82ms
step:1130/2330 train_time:66471ms step_avg:58.82ms
step:1131/2330 train_time:66528ms step_avg:58.82ms
step:1132/2330 train_time:66588ms step_avg:58.82ms
step:1133/2330 train_time:66645ms step_avg:58.82ms
step:1134/2330 train_time:66705ms step_avg:58.82ms
step:1135/2330 train_time:66762ms step_avg:58.82ms
step:1136/2330 train_time:66822ms step_avg:58.82ms
step:1137/2330 train_time:66879ms step_avg:58.82ms
step:1138/2330 train_time:66938ms step_avg:58.82ms
step:1139/2330 train_time:66995ms step_avg:58.82ms
step:1140/2330 train_time:67056ms step_avg:58.82ms
step:1141/2330 train_time:67113ms step_avg:58.82ms
step:1142/2330 train_time:67173ms step_avg:58.82ms
step:1143/2330 train_time:67230ms step_avg:58.82ms
step:1144/2330 train_time:67290ms step_avg:58.82ms
step:1145/2330 train_time:67348ms step_avg:58.82ms
step:1146/2330 train_time:67407ms step_avg:58.82ms
step:1147/2330 train_time:67464ms step_avg:58.82ms
step:1148/2330 train_time:67524ms step_avg:58.82ms
step:1149/2330 train_time:67582ms step_avg:58.82ms
step:1150/2330 train_time:67642ms step_avg:58.82ms
step:1151/2330 train_time:67698ms step_avg:58.82ms
step:1152/2330 train_time:67758ms step_avg:58.82ms
step:1153/2330 train_time:67814ms step_avg:58.82ms
step:1154/2330 train_time:67874ms step_avg:58.82ms
step:1155/2330 train_time:67931ms step_avg:58.81ms
step:1156/2330 train_time:67991ms step_avg:58.82ms
step:1157/2330 train_time:68049ms step_avg:58.82ms
step:1158/2330 train_time:68109ms step_avg:58.82ms
step:1159/2330 train_time:68166ms step_avg:58.81ms
step:1160/2330 train_time:68226ms step_avg:58.82ms
step:1161/2330 train_time:68282ms step_avg:58.81ms
step:1162/2330 train_time:68343ms step_avg:58.82ms
step:1163/2330 train_time:68400ms step_avg:58.81ms
step:1164/2330 train_time:68460ms step_avg:58.81ms
step:1165/2330 train_time:68516ms step_avg:58.81ms
step:1166/2330 train_time:68576ms step_avg:58.81ms
step:1167/2330 train_time:68633ms step_avg:58.81ms
step:1168/2330 train_time:68693ms step_avg:58.81ms
step:1169/2330 train_time:68750ms step_avg:58.81ms
step:1170/2330 train_time:68809ms step_avg:58.81ms
step:1171/2330 train_time:68866ms step_avg:58.81ms
step:1172/2330 train_time:68927ms step_avg:58.81ms
step:1173/2330 train_time:68984ms step_avg:58.81ms
step:1174/2330 train_time:69044ms step_avg:58.81ms
step:1175/2330 train_time:69102ms step_avg:58.81ms
step:1176/2330 train_time:69161ms step_avg:58.81ms
step:1177/2330 train_time:69218ms step_avg:58.81ms
step:1178/2330 train_time:69278ms step_avg:58.81ms
step:1179/2330 train_time:69335ms step_avg:58.81ms
step:1180/2330 train_time:69395ms step_avg:58.81ms
step:1181/2330 train_time:69452ms step_avg:58.81ms
step:1182/2330 train_time:69512ms step_avg:58.81ms
step:1183/2330 train_time:69568ms step_avg:58.81ms
step:1184/2330 train_time:69627ms step_avg:58.81ms
step:1185/2330 train_time:69684ms step_avg:58.80ms
step:1186/2330 train_time:69744ms step_avg:58.81ms
step:1187/2330 train_time:69801ms step_avg:58.80ms
step:1188/2330 train_time:69861ms step_avg:58.81ms
step:1189/2330 train_time:69918ms step_avg:58.80ms
step:1190/2330 train_time:69978ms step_avg:58.80ms
step:1191/2330 train_time:70035ms step_avg:58.80ms
step:1192/2330 train_time:70095ms step_avg:58.80ms
step:1193/2330 train_time:70153ms step_avg:58.80ms
step:1194/2330 train_time:70213ms step_avg:58.80ms
step:1195/2330 train_time:70270ms step_avg:58.80ms
step:1196/2330 train_time:70329ms step_avg:58.80ms
step:1197/2330 train_time:70386ms step_avg:58.80ms
step:1198/2330 train_time:70446ms step_avg:58.80ms
step:1199/2330 train_time:70503ms step_avg:58.80ms
step:1200/2330 train_time:70563ms step_avg:58.80ms
step:1201/2330 train_time:70619ms step_avg:58.80ms
step:1202/2330 train_time:70680ms step_avg:58.80ms
step:1203/2330 train_time:70737ms step_avg:58.80ms
step:1204/2330 train_time:70797ms step_avg:58.80ms
step:1205/2330 train_time:70854ms step_avg:58.80ms
step:1206/2330 train_time:70913ms step_avg:58.80ms
step:1207/2330 train_time:70970ms step_avg:58.80ms
step:1208/2330 train_time:71031ms step_avg:58.80ms
step:1209/2330 train_time:71088ms step_avg:58.80ms
step:1210/2330 train_time:71147ms step_avg:58.80ms
step:1211/2330 train_time:71205ms step_avg:58.80ms
step:1212/2330 train_time:71265ms step_avg:58.80ms
step:1213/2330 train_time:71322ms step_avg:58.80ms
step:1214/2330 train_time:71382ms step_avg:58.80ms
step:1215/2330 train_time:71439ms step_avg:58.80ms
step:1216/2330 train_time:71499ms step_avg:58.80ms
step:1217/2330 train_time:71556ms step_avg:58.80ms
step:1218/2330 train_time:71615ms step_avg:58.80ms
step:1219/2330 train_time:71672ms step_avg:58.80ms
step:1220/2330 train_time:71732ms step_avg:58.80ms
step:1221/2330 train_time:71789ms step_avg:58.79ms
step:1222/2330 train_time:71848ms step_avg:58.80ms
step:1223/2330 train_time:71905ms step_avg:58.79ms
step:1224/2330 train_time:71965ms step_avg:58.80ms
step:1225/2330 train_time:72022ms step_avg:58.79ms
step:1226/2330 train_time:72082ms step_avg:58.79ms
step:1227/2330 train_time:72139ms step_avg:58.79ms
step:1228/2330 train_time:72199ms step_avg:58.79ms
step:1229/2330 train_time:72256ms step_avg:58.79ms
step:1230/2330 train_time:72316ms step_avg:58.79ms
step:1231/2330 train_time:72373ms step_avg:58.79ms
step:1232/2330 train_time:72432ms step_avg:58.79ms
step:1233/2330 train_time:72489ms step_avg:58.79ms
step:1234/2330 train_time:72549ms step_avg:58.79ms
step:1235/2330 train_time:72606ms step_avg:58.79ms
step:1236/2330 train_time:72665ms step_avg:58.79ms
step:1237/2330 train_time:72722ms step_avg:58.79ms
step:1238/2330 train_time:72783ms step_avg:58.79ms
step:1239/2330 train_time:72839ms step_avg:58.79ms
step:1240/2330 train_time:72900ms step_avg:58.79ms
step:1241/2330 train_time:72955ms step_avg:58.79ms
step:1242/2330 train_time:73016ms step_avg:58.79ms
step:1243/2330 train_time:73072ms step_avg:58.79ms
step:1244/2330 train_time:73132ms step_avg:58.79ms
step:1245/2330 train_time:73189ms step_avg:58.79ms
step:1246/2330 train_time:73249ms step_avg:58.79ms
step:1247/2330 train_time:73307ms step_avg:58.79ms
step:1248/2330 train_time:73365ms step_avg:58.79ms
step:1249/2330 train_time:73422ms step_avg:58.78ms
step:1250/2330 train_time:73482ms step_avg:58.79ms
step:1250/2330 val_loss:5.4999 train_time:73563ms step_avg:58.85ms
step:1251/2330 train_time:73583ms step_avg:58.82ms
step:1252/2330 train_time:73604ms step_avg:58.79ms
step:1253/2330 train_time:73661ms step_avg:58.79ms
step:1254/2330 train_time:73727ms step_avg:58.79ms
step:1255/2330 train_time:73785ms step_avg:58.79ms
step:1256/2330 train_time:73845ms step_avg:58.79ms
step:1257/2330 train_time:73902ms step_avg:58.79ms
step:1258/2330 train_time:73962ms step_avg:58.79ms
step:1259/2330 train_time:74019ms step_avg:58.79ms
step:1260/2330 train_time:74078ms step_avg:58.79ms
step:1261/2330 train_time:74135ms step_avg:58.79ms
step:1262/2330 train_time:74194ms step_avg:58.79ms
step:1263/2330 train_time:74250ms step_avg:58.79ms
step:1264/2330 train_time:74309ms step_avg:58.79ms
step:1265/2330 train_time:74365ms step_avg:58.79ms
step:1266/2330 train_time:74424ms step_avg:58.79ms
step:1267/2330 train_time:74481ms step_avg:58.79ms
step:1268/2330 train_time:74541ms step_avg:58.79ms
step:1269/2330 train_time:74598ms step_avg:58.78ms
step:1270/2330 train_time:74660ms step_avg:58.79ms
step:1271/2330 train_time:74717ms step_avg:58.79ms
step:1272/2330 train_time:74779ms step_avg:58.79ms
step:1273/2330 train_time:74837ms step_avg:58.79ms
step:1274/2330 train_time:74897ms step_avg:58.79ms
step:1275/2330 train_time:74954ms step_avg:58.79ms
step:1276/2330 train_time:75014ms step_avg:58.79ms
step:1277/2330 train_time:75071ms step_avg:58.79ms
step:1278/2330 train_time:75130ms step_avg:58.79ms
step:1279/2330 train_time:75187ms step_avg:58.79ms
step:1280/2330 train_time:75246ms step_avg:58.79ms
step:1281/2330 train_time:75303ms step_avg:58.78ms
step:1282/2330 train_time:75362ms step_avg:58.79ms
step:1283/2330 train_time:75419ms step_avg:58.78ms
step:1284/2330 train_time:75478ms step_avg:58.78ms
step:1285/2330 train_time:75535ms step_avg:58.78ms
step:1286/2330 train_time:75596ms step_avg:58.78ms
step:1287/2330 train_time:75652ms step_avg:58.78ms
step:1288/2330 train_time:75714ms step_avg:58.78ms
step:1289/2330 train_time:75770ms step_avg:58.78ms
step:1290/2330 train_time:75832ms step_avg:58.78ms
step:1291/2330 train_time:75888ms step_avg:58.78ms
step:1292/2330 train_time:75949ms step_avg:58.78ms
step:1293/2330 train_time:76005ms step_avg:58.78ms
step:1294/2330 train_time:76065ms step_avg:58.78ms
step:1295/2330 train_time:76122ms step_avg:58.78ms
step:1296/2330 train_time:76181ms step_avg:58.78ms
step:1297/2330 train_time:76238ms step_avg:58.78ms
step:1298/2330 train_time:76297ms step_avg:58.78ms
step:1299/2330 train_time:76353ms step_avg:58.78ms
step:1300/2330 train_time:76413ms step_avg:58.78ms
step:1301/2330 train_time:76470ms step_avg:58.78ms
step:1302/2330 train_time:76529ms step_avg:58.78ms
step:1303/2330 train_time:76586ms step_avg:58.78ms
step:1304/2330 train_time:76647ms step_avg:58.78ms
step:1305/2330 train_time:76704ms step_avg:58.78ms
step:1306/2330 train_time:76764ms step_avg:58.78ms
step:1307/2330 train_time:76821ms step_avg:58.78ms
step:1308/2330 train_time:76882ms step_avg:58.78ms
step:1309/2330 train_time:76939ms step_avg:58.78ms
step:1310/2330 train_time:77000ms step_avg:58.78ms
step:1311/2330 train_time:77057ms step_avg:58.78ms
step:1312/2330 train_time:77116ms step_avg:58.78ms
step:1313/2330 train_time:77173ms step_avg:58.78ms
step:1314/2330 train_time:77232ms step_avg:58.78ms
step:1315/2330 train_time:77289ms step_avg:58.77ms
step:1316/2330 train_time:77348ms step_avg:58.78ms
step:1317/2330 train_time:77405ms step_avg:58.77ms
step:1318/2330 train_time:77464ms step_avg:58.77ms
step:1319/2330 train_time:77521ms step_avg:58.77ms
step:1320/2330 train_time:77581ms step_avg:58.77ms
step:1321/2330 train_time:77638ms step_avg:58.77ms
step:1322/2330 train_time:77698ms step_avg:58.77ms
step:1323/2330 train_time:77755ms step_avg:58.77ms
step:1324/2330 train_time:77816ms step_avg:58.77ms
step:1325/2330 train_time:77872ms step_avg:58.77ms
step:1326/2330 train_time:77933ms step_avg:58.77ms
step:1327/2330 train_time:77990ms step_avg:58.77ms
step:1328/2330 train_time:78051ms step_avg:58.77ms
step:1329/2330 train_time:78107ms step_avg:58.77ms
step:1330/2330 train_time:78167ms step_avg:58.77ms
step:1331/2330 train_time:78223ms step_avg:58.77ms
step:1332/2330 train_time:78283ms step_avg:58.77ms
step:1333/2330 train_time:78340ms step_avg:58.77ms
step:1334/2330 train_time:78399ms step_avg:58.77ms
step:1335/2330 train_time:78456ms step_avg:58.77ms
step:1336/2330 train_time:78517ms step_avg:58.77ms
step:1337/2330 train_time:78574ms step_avg:58.77ms
step:1338/2330 train_time:78634ms step_avg:58.77ms
step:1339/2330 train_time:78691ms step_avg:58.77ms
step:1340/2330 train_time:78752ms step_avg:58.77ms
step:1341/2330 train_time:78808ms step_avg:58.77ms
step:1342/2330 train_time:78869ms step_avg:58.77ms
step:1343/2330 train_time:78926ms step_avg:58.77ms
step:1344/2330 train_time:78985ms step_avg:58.77ms
step:1345/2330 train_time:79041ms step_avg:58.77ms
step:1346/2330 train_time:79102ms step_avg:58.77ms
step:1347/2330 train_time:79159ms step_avg:58.77ms
step:1348/2330 train_time:79218ms step_avg:58.77ms
step:1349/2330 train_time:79275ms step_avg:58.77ms
step:1350/2330 train_time:79335ms step_avg:58.77ms
step:1351/2330 train_time:79391ms step_avg:58.76ms
step:1352/2330 train_time:79451ms step_avg:58.77ms
step:1353/2330 train_time:79508ms step_avg:58.76ms
step:1354/2330 train_time:79567ms step_avg:58.76ms
step:1355/2330 train_time:79625ms step_avg:58.76ms
step:1356/2330 train_time:79684ms step_avg:58.76ms
step:1357/2330 train_time:79742ms step_avg:58.76ms
step:1358/2330 train_time:79802ms step_avg:58.76ms
step:1359/2330 train_time:79859ms step_avg:58.76ms
step:1360/2330 train_time:79920ms step_avg:58.76ms
step:1361/2330 train_time:79977ms step_avg:58.76ms
step:1362/2330 train_time:80037ms step_avg:58.76ms
step:1363/2330 train_time:80094ms step_avg:58.76ms
step:1364/2330 train_time:80154ms step_avg:58.76ms
step:1365/2330 train_time:80211ms step_avg:58.76ms
step:1366/2330 train_time:80271ms step_avg:58.76ms
step:1367/2330 train_time:80328ms step_avg:58.76ms
step:1368/2330 train_time:80387ms step_avg:58.76ms
step:1369/2330 train_time:80444ms step_avg:58.76ms
step:1370/2330 train_time:80504ms step_avg:58.76ms
step:1371/2330 train_time:80561ms step_avg:58.76ms
step:1372/2330 train_time:80621ms step_avg:58.76ms
step:1373/2330 train_time:80678ms step_avg:58.76ms
step:1374/2330 train_time:80738ms step_avg:58.76ms
step:1375/2330 train_time:80795ms step_avg:58.76ms
step:1376/2330 train_time:80855ms step_avg:58.76ms
step:1377/2330 train_time:80912ms step_avg:58.76ms
step:1378/2330 train_time:80973ms step_avg:58.76ms
step:1379/2330 train_time:81030ms step_avg:58.76ms
step:1380/2330 train_time:81089ms step_avg:58.76ms
step:1381/2330 train_time:81146ms step_avg:58.76ms
step:1382/2330 train_time:81206ms step_avg:58.76ms
step:1383/2330 train_time:81262ms step_avg:58.76ms
step:1384/2330 train_time:81322ms step_avg:58.76ms
step:1385/2330 train_time:81379ms step_avg:58.76ms
step:1386/2330 train_time:81439ms step_avg:58.76ms
step:1387/2330 train_time:81496ms step_avg:58.76ms
step:1388/2330 train_time:81556ms step_avg:58.76ms
step:1389/2330 train_time:81612ms step_avg:58.76ms
step:1390/2330 train_time:81672ms step_avg:58.76ms
step:1391/2330 train_time:81729ms step_avg:58.76ms
step:1392/2330 train_time:81789ms step_avg:58.76ms
step:1393/2330 train_time:81846ms step_avg:58.76ms
step:1394/2330 train_time:81906ms step_avg:58.76ms
step:1395/2330 train_time:81963ms step_avg:58.75ms
step:1396/2330 train_time:82023ms step_avg:58.76ms
step:1397/2330 train_time:82080ms step_avg:58.75ms
step:1398/2330 train_time:82140ms step_avg:58.76ms
step:1399/2330 train_time:82196ms step_avg:58.75ms
step:1400/2330 train_time:82256ms step_avg:58.75ms
step:1401/2330 train_time:82312ms step_avg:58.75ms
step:1402/2330 train_time:82373ms step_avg:58.75ms
step:1403/2330 train_time:82430ms step_avg:58.75ms
step:1404/2330 train_time:82490ms step_avg:58.75ms
step:1405/2330 train_time:82547ms step_avg:58.75ms
step:1406/2330 train_time:82607ms step_avg:58.75ms
step:1407/2330 train_time:82664ms step_avg:58.75ms
step:1408/2330 train_time:82723ms step_avg:58.75ms
step:1409/2330 train_time:82780ms step_avg:58.75ms
step:1410/2330 train_time:82840ms step_avg:58.75ms
step:1411/2330 train_time:82897ms step_avg:58.75ms
step:1412/2330 train_time:82958ms step_avg:58.75ms
step:1413/2330 train_time:83014ms step_avg:58.75ms
step:1414/2330 train_time:83075ms step_avg:58.75ms
step:1415/2330 train_time:83131ms step_avg:58.75ms
step:1416/2330 train_time:83192ms step_avg:58.75ms
step:1417/2330 train_time:83248ms step_avg:58.75ms
step:1418/2330 train_time:83308ms step_avg:58.75ms
step:1419/2330 train_time:83365ms step_avg:58.75ms
step:1420/2330 train_time:83425ms step_avg:58.75ms
step:1421/2330 train_time:83481ms step_avg:58.75ms
step:1422/2330 train_time:83542ms step_avg:58.75ms
step:1423/2330 train_time:83599ms step_avg:58.75ms
step:1424/2330 train_time:83659ms step_avg:58.75ms
step:1425/2330 train_time:83717ms step_avg:58.75ms
step:1426/2330 train_time:83776ms step_avg:58.75ms
step:1427/2330 train_time:83833ms step_avg:58.75ms
step:1428/2330 train_time:83893ms step_avg:58.75ms
step:1429/2330 train_time:83950ms step_avg:58.75ms
step:1430/2330 train_time:84011ms step_avg:58.75ms
step:1431/2330 train_time:84068ms step_avg:58.75ms
step:1432/2330 train_time:84127ms step_avg:58.75ms
step:1433/2330 train_time:84184ms step_avg:58.75ms
step:1434/2330 train_time:84244ms step_avg:58.75ms
step:1435/2330 train_time:84301ms step_avg:58.75ms
step:1436/2330 train_time:84361ms step_avg:58.75ms
step:1437/2330 train_time:84418ms step_avg:58.75ms
step:1438/2330 train_time:84478ms step_avg:58.75ms
step:1439/2330 train_time:84534ms step_avg:58.75ms
step:1440/2330 train_time:84594ms step_avg:58.75ms
step:1441/2330 train_time:84651ms step_avg:58.74ms
step:1442/2330 train_time:84711ms step_avg:58.75ms
step:1443/2330 train_time:84768ms step_avg:58.74ms
step:1444/2330 train_time:84828ms step_avg:58.75ms
step:1445/2330 train_time:84885ms step_avg:58.74ms
step:1446/2330 train_time:84944ms step_avg:58.74ms
step:1447/2330 train_time:85002ms step_avg:58.74ms
step:1448/2330 train_time:85061ms step_avg:58.74ms
step:1449/2330 train_time:85118ms step_avg:58.74ms
step:1450/2330 train_time:85178ms step_avg:58.74ms
step:1451/2330 train_time:85234ms step_avg:58.74ms
step:1452/2330 train_time:85294ms step_avg:58.74ms
step:1453/2330 train_time:85350ms step_avg:58.74ms
step:1454/2330 train_time:85412ms step_avg:58.74ms
step:1455/2330 train_time:85468ms step_avg:58.74ms
step:1456/2330 train_time:85529ms step_avg:58.74ms
step:1457/2330 train_time:85586ms step_avg:58.74ms
step:1458/2330 train_time:85645ms step_avg:58.74ms
step:1459/2330 train_time:85702ms step_avg:58.74ms
step:1460/2330 train_time:85762ms step_avg:58.74ms
step:1461/2330 train_time:85820ms step_avg:58.74ms
step:1462/2330 train_time:85880ms step_avg:58.74ms
step:1463/2330 train_time:85937ms step_avg:58.74ms
step:1464/2330 train_time:85998ms step_avg:58.74ms
step:1465/2330 train_time:86054ms step_avg:58.74ms
step:1466/2330 train_time:86114ms step_avg:58.74ms
step:1467/2330 train_time:86172ms step_avg:58.74ms
step:1468/2330 train_time:86231ms step_avg:58.74ms
step:1469/2330 train_time:86288ms step_avg:58.74ms
step:1470/2330 train_time:86348ms step_avg:58.74ms
step:1471/2330 train_time:86405ms step_avg:58.74ms
step:1472/2330 train_time:86464ms step_avg:58.74ms
step:1473/2330 train_time:86522ms step_avg:58.74ms
step:1474/2330 train_time:86582ms step_avg:58.74ms
step:1475/2330 train_time:86639ms step_avg:58.74ms
step:1476/2330 train_time:86699ms step_avg:58.74ms
step:1477/2330 train_time:86756ms step_avg:58.74ms
step:1478/2330 train_time:86815ms step_avg:58.74ms
step:1479/2330 train_time:86873ms step_avg:58.74ms
step:1480/2330 train_time:86933ms step_avg:58.74ms
step:1481/2330 train_time:86989ms step_avg:58.74ms
step:1482/2330 train_time:87050ms step_avg:58.74ms
step:1483/2330 train_time:87107ms step_avg:58.74ms
step:1484/2330 train_time:87167ms step_avg:58.74ms
step:1485/2330 train_time:87224ms step_avg:58.74ms
step:1486/2330 train_time:87283ms step_avg:58.74ms
step:1487/2330 train_time:87340ms step_avg:58.74ms
step:1488/2330 train_time:87401ms step_avg:58.74ms
step:1489/2330 train_time:87458ms step_avg:58.74ms
step:1490/2330 train_time:87518ms step_avg:58.74ms
step:1491/2330 train_time:87575ms step_avg:58.74ms
step:1492/2330 train_time:87635ms step_avg:58.74ms
step:1493/2330 train_time:87692ms step_avg:58.74ms
step:1494/2330 train_time:87753ms step_avg:58.74ms
step:1495/2330 train_time:87809ms step_avg:58.74ms
step:1496/2330 train_time:87869ms step_avg:58.74ms
step:1497/2330 train_time:87926ms step_avg:58.73ms
step:1498/2330 train_time:87987ms step_avg:58.74ms
step:1499/2330 train_time:88044ms step_avg:58.73ms
step:1500/2330 train_time:88104ms step_avg:58.74ms
step:1500/2330 val_loss:5.4303 train_time:88183ms step_avg:58.79ms
step:1501/2330 train_time:88203ms step_avg:58.76ms
step:1502/2330 train_time:88223ms step_avg:58.74ms
step:1503/2330 train_time:88280ms step_avg:58.74ms
step:1504/2330 train_time:88348ms step_avg:58.74ms
step:1505/2330 train_time:88405ms step_avg:58.74ms
step:1506/2330 train_time:88467ms step_avg:58.74ms
step:1507/2330 train_time:88524ms step_avg:58.74ms
step:1508/2330 train_time:88584ms step_avg:58.74ms
step:1509/2330 train_time:88640ms step_avg:58.74ms
step:1510/2330 train_time:88700ms step_avg:58.74ms
step:1511/2330 train_time:88756ms step_avg:58.74ms
step:1512/2330 train_time:88815ms step_avg:58.74ms
step:1513/2330 train_time:88872ms step_avg:58.74ms
step:1514/2330 train_time:88931ms step_avg:58.74ms
step:1515/2330 train_time:88987ms step_avg:58.74ms
step:1516/2330 train_time:89047ms step_avg:58.74ms
step:1517/2330 train_time:89103ms step_avg:58.74ms
step:1518/2330 train_time:89164ms step_avg:58.74ms
step:1519/2330 train_time:89221ms step_avg:58.74ms
step:1520/2330 train_time:89284ms step_avg:58.74ms
step:1521/2330 train_time:89342ms step_avg:58.74ms
step:1522/2330 train_time:89402ms step_avg:58.74ms
step:1523/2330 train_time:89459ms step_avg:58.74ms
step:1524/2330 train_time:89519ms step_avg:58.74ms
step:1525/2330 train_time:89576ms step_avg:58.74ms
step:1526/2330 train_time:89635ms step_avg:58.74ms
step:1527/2330 train_time:89691ms step_avg:58.74ms
step:1528/2330 train_time:89751ms step_avg:58.74ms
step:1529/2330 train_time:89809ms step_avg:58.74ms
step:1530/2330 train_time:89868ms step_avg:58.74ms
step:1531/2330 train_time:89925ms step_avg:58.74ms
step:1532/2330 train_time:89984ms step_avg:58.74ms
step:1533/2330 train_time:90041ms step_avg:58.73ms
step:1534/2330 train_time:90101ms step_avg:58.74ms
step:1535/2330 train_time:90158ms step_avg:58.73ms
step:1536/2330 train_time:90219ms step_avg:58.74ms
step:1537/2330 train_time:90277ms step_avg:58.74ms
step:1538/2330 train_time:90338ms step_avg:58.74ms
step:1539/2330 train_time:90396ms step_avg:58.74ms
step:1540/2330 train_time:90457ms step_avg:58.74ms
step:1541/2330 train_time:90515ms step_avg:58.74ms
step:1542/2330 train_time:90575ms step_avg:58.74ms
step:1543/2330 train_time:90632ms step_avg:58.74ms
step:1544/2330 train_time:90693ms step_avg:58.74ms
step:1545/2330 train_time:90750ms step_avg:58.74ms
step:1546/2330 train_time:90811ms step_avg:58.74ms
step:1547/2330 train_time:90869ms step_avg:58.74ms
step:1548/2330 train_time:90928ms step_avg:58.74ms
step:1549/2330 train_time:90985ms step_avg:58.74ms
step:1550/2330 train_time:91046ms step_avg:58.74ms
step:1551/2330 train_time:91103ms step_avg:58.74ms
step:1552/2330 train_time:91164ms step_avg:58.74ms
step:1553/2330 train_time:91221ms step_avg:58.74ms
step:1554/2330 train_time:91282ms step_avg:58.74ms
step:1555/2330 train_time:91339ms step_avg:58.74ms
step:1556/2330 train_time:91401ms step_avg:58.74ms
step:1557/2330 train_time:91458ms step_avg:58.74ms
step:1558/2330 train_time:91518ms step_avg:58.74ms
step:1559/2330 train_time:91576ms step_avg:58.74ms
step:1560/2330 train_time:91636ms step_avg:58.74ms
step:1561/2330 train_time:91693ms step_avg:58.74ms
step:1562/2330 train_time:91754ms step_avg:58.74ms
step:1563/2330 train_time:91812ms step_avg:58.74ms
step:1564/2330 train_time:91873ms step_avg:58.74ms
step:1565/2330 train_time:91930ms step_avg:58.74ms
step:1566/2330 train_time:91990ms step_avg:58.74ms
step:1567/2330 train_time:92048ms step_avg:58.74ms
step:1568/2330 train_time:92108ms step_avg:58.74ms
step:1569/2330 train_time:92165ms step_avg:58.74ms
step:1570/2330 train_time:92226ms step_avg:58.74ms
step:1571/2330 train_time:92282ms step_avg:58.74ms
step:1572/2330 train_time:92346ms step_avg:58.74ms
step:1573/2330 train_time:92403ms step_avg:58.74ms
step:1574/2330 train_time:92465ms step_avg:58.75ms
step:1575/2330 train_time:92522ms step_avg:58.74ms
step:1576/2330 train_time:92583ms step_avg:58.75ms
step:1577/2330 train_time:92640ms step_avg:58.74ms
step:1578/2330 train_time:92701ms step_avg:58.75ms
step:1579/2330 train_time:92758ms step_avg:58.74ms
step:1580/2330 train_time:92817ms step_avg:58.75ms
step:1581/2330 train_time:92875ms step_avg:58.74ms
step:1582/2330 train_time:92934ms step_avg:58.74ms
step:1583/2330 train_time:92991ms step_avg:58.74ms
step:1584/2330 train_time:93052ms step_avg:58.74ms
step:1585/2330 train_time:93109ms step_avg:58.74ms
step:1586/2330 train_time:93170ms step_avg:58.75ms
step:1587/2330 train_time:93227ms step_avg:58.74ms
step:1588/2330 train_time:93288ms step_avg:58.75ms
step:1589/2330 train_time:93345ms step_avg:58.74ms
step:1590/2330 train_time:93408ms step_avg:58.75ms
step:1591/2330 train_time:93464ms step_avg:58.75ms
step:1592/2330 train_time:93527ms step_avg:58.75ms
step:1593/2330 train_time:93584ms step_avg:58.75ms
step:1594/2330 train_time:93647ms step_avg:58.75ms
step:1595/2330 train_time:93704ms step_avg:58.75ms
step:1596/2330 train_time:93765ms step_avg:58.75ms
step:1597/2330 train_time:93821ms step_avg:58.75ms
step:1598/2330 train_time:93882ms step_avg:58.75ms
step:1599/2330 train_time:93938ms step_avg:58.75ms
step:1600/2330 train_time:93999ms step_avg:58.75ms
step:1601/2330 train_time:94056ms step_avg:58.75ms
step:1602/2330 train_time:94115ms step_avg:58.75ms
step:1603/2330 train_time:94173ms step_avg:58.75ms
step:1604/2330 train_time:94234ms step_avg:58.75ms
step:1605/2330 train_time:94292ms step_avg:58.75ms
step:1606/2330 train_time:94353ms step_avg:58.75ms
step:1607/2330 train_time:94411ms step_avg:58.75ms
step:1608/2330 train_time:94471ms step_avg:58.75ms
step:1609/2330 train_time:94529ms step_avg:58.75ms
step:1610/2330 train_time:94591ms step_avg:58.75ms
step:1611/2330 train_time:94648ms step_avg:58.75ms
step:1612/2330 train_time:94709ms step_avg:58.75ms
step:1613/2330 train_time:94766ms step_avg:58.75ms
step:1614/2330 train_time:94828ms step_avg:58.75ms
step:1615/2330 train_time:94884ms step_avg:58.75ms
step:1616/2330 train_time:94946ms step_avg:58.75ms
step:1617/2330 train_time:95002ms step_avg:58.75ms
step:1618/2330 train_time:95064ms step_avg:58.75ms
step:1619/2330 train_time:95120ms step_avg:58.75ms
step:1620/2330 train_time:95181ms step_avg:58.75ms
step:1621/2330 train_time:95238ms step_avg:58.75ms
step:1622/2330 train_time:95298ms step_avg:58.75ms
step:1623/2330 train_time:95355ms step_avg:58.75ms
step:1624/2330 train_time:95415ms step_avg:58.75ms
step:1625/2330 train_time:95473ms step_avg:58.75ms
step:1626/2330 train_time:95533ms step_avg:58.75ms
step:1627/2330 train_time:95592ms step_avg:58.75ms
step:1628/2330 train_time:95652ms step_avg:58.75ms
step:1629/2330 train_time:95710ms step_avg:58.75ms
step:1630/2330 train_time:95771ms step_avg:58.75ms
step:1631/2330 train_time:95827ms step_avg:58.75ms
step:1632/2330 train_time:95889ms step_avg:58.76ms
step:1633/2330 train_time:95945ms step_avg:58.75ms
step:1634/2330 train_time:96008ms step_avg:58.76ms
step:1635/2330 train_time:96064ms step_avg:58.75ms
step:1636/2330 train_time:96126ms step_avg:58.76ms
step:1637/2330 train_time:96182ms step_avg:58.76ms
step:1638/2330 train_time:96245ms step_avg:58.76ms
step:1639/2330 train_time:96301ms step_avg:58.76ms
step:1640/2330 train_time:96363ms step_avg:58.76ms
step:1641/2330 train_time:96419ms step_avg:58.76ms
step:1642/2330 train_time:96480ms step_avg:58.76ms
step:1643/2330 train_time:96536ms step_avg:58.76ms
step:1644/2330 train_time:96597ms step_avg:58.76ms
step:1645/2330 train_time:96655ms step_avg:58.76ms
step:1646/2330 train_time:96715ms step_avg:58.76ms
step:1647/2330 train_time:96773ms step_avg:58.76ms
step:1648/2330 train_time:96834ms step_avg:58.76ms
step:1649/2330 train_time:96891ms step_avg:58.76ms
step:1650/2330 train_time:96951ms step_avg:58.76ms
step:1651/2330 train_time:97010ms step_avg:58.76ms
step:1652/2330 train_time:97071ms step_avg:58.76ms
step:1653/2330 train_time:97128ms step_avg:58.76ms
step:1654/2330 train_time:97189ms step_avg:58.76ms
step:1655/2330 train_time:97245ms step_avg:58.76ms
step:1656/2330 train_time:97307ms step_avg:58.76ms
step:1657/2330 train_time:97363ms step_avg:58.76ms
step:1658/2330 train_time:97426ms step_avg:58.76ms
step:1659/2330 train_time:97483ms step_avg:58.76ms
step:1660/2330 train_time:97544ms step_avg:58.76ms
step:1661/2330 train_time:97600ms step_avg:58.76ms
step:1662/2330 train_time:97661ms step_avg:58.76ms
step:1663/2330 train_time:97718ms step_avg:58.76ms
step:1664/2330 train_time:97778ms step_avg:58.76ms
step:1665/2330 train_time:97834ms step_avg:58.76ms
step:1666/2330 train_time:97895ms step_avg:58.76ms
step:1667/2330 train_time:97952ms step_avg:58.76ms
step:1668/2330 train_time:98013ms step_avg:58.76ms
step:1669/2330 train_time:98072ms step_avg:58.76ms
step:1670/2330 train_time:98131ms step_avg:58.76ms
step:1671/2330 train_time:98188ms step_avg:58.76ms
step:1672/2330 train_time:98251ms step_avg:58.76ms
step:1673/2330 train_time:98308ms step_avg:58.76ms
step:1674/2330 train_time:98369ms step_avg:58.76ms
step:1675/2330 train_time:98425ms step_avg:58.76ms
step:1676/2330 train_time:98488ms step_avg:58.76ms
step:1677/2330 train_time:98545ms step_avg:58.76ms
step:1678/2330 train_time:98607ms step_avg:58.76ms
step:1679/2330 train_time:98663ms step_avg:58.76ms
step:1680/2330 train_time:98726ms step_avg:58.77ms
step:1681/2330 train_time:98783ms step_avg:58.76ms
step:1682/2330 train_time:98844ms step_avg:58.77ms
step:1683/2330 train_time:98900ms step_avg:58.76ms
step:1684/2330 train_time:98962ms step_avg:58.77ms
step:1685/2330 train_time:99018ms step_avg:58.76ms
step:1686/2330 train_time:99078ms step_avg:58.77ms
step:1687/2330 train_time:99136ms step_avg:58.76ms
step:1688/2330 train_time:99195ms step_avg:58.76ms
step:1689/2330 train_time:99252ms step_avg:58.76ms
step:1690/2330 train_time:99313ms step_avg:58.77ms
step:1691/2330 train_time:99371ms step_avg:58.76ms
step:1692/2330 train_time:99431ms step_avg:58.77ms
step:1693/2330 train_time:99489ms step_avg:58.76ms
step:1694/2330 train_time:99551ms step_avg:58.77ms
step:1695/2330 train_time:99608ms step_avg:58.77ms
step:1696/2330 train_time:99669ms step_avg:58.77ms
step:1697/2330 train_time:99726ms step_avg:58.77ms
step:1698/2330 train_time:99787ms step_avg:58.77ms
step:1699/2330 train_time:99844ms step_avg:58.77ms
step:1700/2330 train_time:99906ms step_avg:58.77ms
step:1701/2330 train_time:99962ms step_avg:58.77ms
step:1702/2330 train_time:100024ms step_avg:58.77ms
step:1703/2330 train_time:100080ms step_avg:58.77ms
step:1704/2330 train_time:100141ms step_avg:58.77ms
step:1705/2330 train_time:100197ms step_avg:58.77ms
step:1706/2330 train_time:100258ms step_avg:58.77ms
step:1707/2330 train_time:100315ms step_avg:58.77ms
step:1708/2330 train_time:100374ms step_avg:58.77ms
step:1709/2330 train_time:100432ms step_avg:58.77ms
step:1710/2330 train_time:100493ms step_avg:58.77ms
step:1711/2330 train_time:100551ms step_avg:58.77ms
step:1712/2330 train_time:100611ms step_avg:58.77ms
step:1713/2330 train_time:100669ms step_avg:58.77ms
step:1714/2330 train_time:100729ms step_avg:58.77ms
step:1715/2330 train_time:100787ms step_avg:58.77ms
step:1716/2330 train_time:100849ms step_avg:58.77ms
step:1717/2330 train_time:100906ms step_avg:58.77ms
step:1718/2330 train_time:100968ms step_avg:58.77ms
step:1719/2330 train_time:101024ms step_avg:58.77ms
step:1720/2330 train_time:101086ms step_avg:58.77ms
step:1721/2330 train_time:101143ms step_avg:58.77ms
step:1722/2330 train_time:101204ms step_avg:58.77ms
step:1723/2330 train_time:101261ms step_avg:58.77ms
step:1724/2330 train_time:101322ms step_avg:58.77ms
step:1725/2330 train_time:101378ms step_avg:58.77ms
step:1726/2330 train_time:101439ms step_avg:58.77ms
step:1727/2330 train_time:101496ms step_avg:58.77ms
step:1728/2330 train_time:101556ms step_avg:58.77ms
step:1729/2330 train_time:101614ms step_avg:58.77ms
step:1730/2330 train_time:101674ms step_avg:58.77ms
step:1731/2330 train_time:101732ms step_avg:58.77ms
step:1732/2330 train_time:101792ms step_avg:58.77ms
step:1733/2330 train_time:101850ms step_avg:58.77ms
step:1734/2330 train_time:101911ms step_avg:58.77ms
step:1735/2330 train_time:101969ms step_avg:58.77ms
step:1736/2330 train_time:102029ms step_avg:58.77ms
step:1737/2330 train_time:102087ms step_avg:58.77ms
step:1738/2330 train_time:102148ms step_avg:58.77ms
step:1739/2330 train_time:102204ms step_avg:58.77ms
step:1740/2330 train_time:102267ms step_avg:58.77ms
step:1741/2330 train_time:102323ms step_avg:58.77ms
step:1742/2330 train_time:102384ms step_avg:58.77ms
step:1743/2330 train_time:102441ms step_avg:58.77ms
step:1744/2330 train_time:102502ms step_avg:58.77ms
step:1745/2330 train_time:102558ms step_avg:58.77ms
step:1746/2330 train_time:102619ms step_avg:58.77ms
step:1747/2330 train_time:102676ms step_avg:58.77ms
step:1748/2330 train_time:102737ms step_avg:58.77ms
step:1749/2330 train_time:102794ms step_avg:58.77ms
step:1750/2330 train_time:102854ms step_avg:58.77ms
step:1750/2330 val_loss:5.2493 train_time:102936ms step_avg:58.82ms
step:1751/2330 train_time:102957ms step_avg:58.80ms
step:1752/2330 train_time:102978ms step_avg:58.78ms
step:1753/2330 train_time:103039ms step_avg:58.78ms
step:1754/2330 train_time:103103ms step_avg:58.78ms
step:1755/2330 train_time:103162ms step_avg:58.78ms
step:1756/2330 train_time:103222ms step_avg:58.78ms
step:1757/2330 train_time:103279ms step_avg:58.78ms
step:1758/2330 train_time:103338ms step_avg:58.78ms
step:1759/2330 train_time:103394ms step_avg:58.78ms
step:1760/2330 train_time:103454ms step_avg:58.78ms
step:1761/2330 train_time:103511ms step_avg:58.78ms
step:1762/2330 train_time:103570ms step_avg:58.78ms
step:1763/2330 train_time:103627ms step_avg:58.78ms
step:1764/2330 train_time:103687ms step_avg:58.78ms
step:1765/2330 train_time:103743ms step_avg:58.78ms
step:1766/2330 train_time:103802ms step_avg:58.78ms
step:1767/2330 train_time:103859ms step_avg:58.78ms
step:1768/2330 train_time:103923ms step_avg:58.78ms
step:1769/2330 train_time:103980ms step_avg:58.78ms
step:1770/2330 train_time:104042ms step_avg:58.78ms
step:1771/2330 train_time:104100ms step_avg:58.78ms
step:1772/2330 train_time:104161ms step_avg:58.78ms
step:1773/2330 train_time:104219ms step_avg:58.78ms
step:1774/2330 train_time:104278ms step_avg:58.78ms
step:1775/2330 train_time:104335ms step_avg:58.78ms
step:1776/2330 train_time:104395ms step_avg:58.78ms
step:1777/2330 train_time:104453ms step_avg:58.78ms
step:1778/2330 train_time:104513ms step_avg:58.78ms
step:1779/2330 train_time:104571ms step_avg:58.78ms
step:1780/2330 train_time:104631ms step_avg:58.78ms
step:1781/2330 train_time:104688ms step_avg:58.78ms
step:1782/2330 train_time:104747ms step_avg:58.78ms
step:1783/2330 train_time:104804ms step_avg:58.78ms
step:1784/2330 train_time:104866ms step_avg:58.78ms
step:1785/2330 train_time:104923ms step_avg:58.78ms
step:1786/2330 train_time:104984ms step_avg:58.78ms
step:1787/2330 train_time:105041ms step_avg:58.78ms
step:1788/2330 train_time:105104ms step_avg:58.78ms
step:1789/2330 train_time:105161ms step_avg:58.78ms
step:1790/2330 train_time:105221ms step_avg:58.78ms
step:1791/2330 train_time:105278ms step_avg:58.78ms
step:1792/2330 train_time:105338ms step_avg:58.78ms
step:1793/2330 train_time:105395ms step_avg:58.78ms
step:1794/2330 train_time:105454ms step_avg:58.78ms
step:1795/2330 train_time:105512ms step_avg:58.78ms
step:1796/2330 train_time:105572ms step_avg:58.78ms
step:1797/2330 train_time:105629ms step_avg:58.78ms
step:1798/2330 train_time:105689ms step_avg:58.78ms
step:1799/2330 train_time:105746ms step_avg:58.78ms
step:1800/2330 train_time:105806ms step_avg:58.78ms
step:1801/2330 train_time:105864ms step_avg:58.78ms
step:1802/2330 train_time:105925ms step_avg:58.78ms
step:1803/2330 train_time:105982ms step_avg:58.78ms
step:1804/2330 train_time:106043ms step_avg:58.78ms
step:1805/2330 train_time:106100ms step_avg:58.78ms
step:1806/2330 train_time:106161ms step_avg:58.78ms
step:1807/2330 train_time:106219ms step_avg:58.78ms
step:1808/2330 train_time:106278ms step_avg:58.78ms
step:1809/2330 train_time:106335ms step_avg:58.78ms
step:1810/2330 train_time:106396ms step_avg:58.78ms
step:1811/2330 train_time:106453ms step_avg:58.78ms
step:1812/2330 train_time:106513ms step_avg:58.78ms
step:1813/2330 train_time:106570ms step_avg:58.78ms
step:1814/2330 train_time:106630ms step_avg:58.78ms
step:1815/2330 train_time:106688ms step_avg:58.78ms
step:1816/2330 train_time:106748ms step_avg:58.78ms
step:1817/2330 train_time:106804ms step_avg:58.78ms
step:1818/2330 train_time:106866ms step_avg:58.78ms
step:1819/2330 train_time:106924ms step_avg:58.78ms
step:1820/2330 train_time:106985ms step_avg:58.78ms
step:1821/2330 train_time:107042ms step_avg:58.78ms
step:1822/2330 train_time:107103ms step_avg:58.78ms
step:1823/2330 train_time:107160ms step_avg:58.78ms
step:1824/2330 train_time:107221ms step_avg:58.78ms
step:1825/2330 train_time:107277ms step_avg:58.78ms
step:1826/2330 train_time:107337ms step_avg:58.78ms
step:1827/2330 train_time:107394ms step_avg:58.78ms
step:1828/2330 train_time:107454ms step_avg:58.78ms
step:1829/2330 train_time:107512ms step_avg:58.78ms
step:1830/2330 train_time:107572ms step_avg:58.78ms
step:1831/2330 train_time:107630ms step_avg:58.78ms
step:1832/2330 train_time:107689ms step_avg:58.78ms
step:1833/2330 train_time:107747ms step_avg:58.78ms
step:1834/2330 train_time:107808ms step_avg:58.78ms
step:1835/2330 train_time:107865ms step_avg:58.78ms
step:1836/2330 train_time:107925ms step_avg:58.78ms
step:1837/2330 train_time:107983ms step_avg:58.78ms
step:1838/2330 train_time:108044ms step_avg:58.78ms
step:1839/2330 train_time:108101ms step_avg:58.78ms
step:1840/2330 train_time:108164ms step_avg:58.78ms
step:1841/2330 train_time:108220ms step_avg:58.78ms
step:1842/2330 train_time:108281ms step_avg:58.78ms
step:1843/2330 train_time:108338ms step_avg:58.78ms
step:1844/2330 train_time:108399ms step_avg:58.78ms
step:1845/2330 train_time:108456ms step_avg:58.78ms
step:1846/2330 train_time:108516ms step_avg:58.78ms
step:1847/2330 train_time:108575ms step_avg:58.78ms
step:1848/2330 train_time:108634ms step_avg:58.78ms
step:1849/2330 train_time:108692ms step_avg:58.78ms
step:1850/2330 train_time:108752ms step_avg:58.79ms
step:1851/2330 train_time:108811ms step_avg:58.78ms
step:1852/2330 train_time:108871ms step_avg:58.79ms
step:1853/2330 train_time:108928ms step_avg:58.78ms
step:1854/2330 train_time:108989ms step_avg:58.79ms
step:1855/2330 train_time:109045ms step_avg:58.78ms
step:1856/2330 train_time:109109ms step_avg:58.79ms
step:1857/2330 train_time:109165ms step_avg:58.79ms
step:1858/2330 train_time:109228ms step_avg:58.79ms
step:1859/2330 train_time:109284ms step_avg:58.79ms
step:1860/2330 train_time:109347ms step_avg:58.79ms
step:1861/2330 train_time:109404ms step_avg:58.79ms
step:1862/2330 train_time:109467ms step_avg:58.79ms
step:1863/2330 train_time:109523ms step_avg:58.79ms
step:1864/2330 train_time:109583ms step_avg:58.79ms
step:1865/2330 train_time:109640ms step_avg:58.79ms
step:1866/2330 train_time:109700ms step_avg:58.79ms
step:1867/2330 train_time:109757ms step_avg:58.79ms
step:1868/2330 train_time:109817ms step_avg:58.79ms
step:1869/2330 train_time:109876ms step_avg:58.79ms
step:1870/2330 train_time:109935ms step_avg:58.79ms
step:1871/2330 train_time:109993ms step_avg:58.79ms
step:1872/2330 train_time:110053ms step_avg:58.79ms
step:1873/2330 train_time:110111ms step_avg:58.79ms
step:1874/2330 train_time:110172ms step_avg:58.79ms
step:1875/2330 train_time:110229ms step_avg:58.79ms
step:1876/2330 train_time:110291ms step_avg:58.79ms
step:1877/2330 train_time:110347ms step_avg:58.79ms
step:1878/2330 train_time:110410ms step_avg:58.79ms
step:1879/2330 train_time:110466ms step_avg:58.79ms
step:1880/2330 train_time:110528ms step_avg:58.79ms
step:1881/2330 train_time:110584ms step_avg:58.79ms
step:1882/2330 train_time:110646ms step_avg:58.79ms
step:1883/2330 train_time:110702ms step_avg:58.79ms
step:1884/2330 train_time:110765ms step_avg:58.79ms
step:1885/2330 train_time:110821ms step_avg:58.79ms
step:1886/2330 train_time:110882ms step_avg:58.79ms
step:1887/2330 train_time:110938ms step_avg:58.79ms
step:1888/2330 train_time:110998ms step_avg:58.79ms
step:1889/2330 train_time:111056ms step_avg:58.79ms
step:1890/2330 train_time:111116ms step_avg:58.79ms
step:1891/2330 train_time:111173ms step_avg:58.79ms
step:1892/2330 train_time:111234ms step_avg:58.79ms
step:1893/2330 train_time:111293ms step_avg:58.79ms
step:1894/2330 train_time:111353ms step_avg:58.79ms
step:1895/2330 train_time:111411ms step_avg:58.79ms
step:1896/2330 train_time:111471ms step_avg:58.79ms
step:1897/2330 train_time:111528ms step_avg:58.79ms
step:1898/2330 train_time:111590ms step_avg:58.79ms
step:1899/2330 train_time:111647ms step_avg:58.79ms
step:1900/2330 train_time:111708ms step_avg:58.79ms
step:1901/2330 train_time:111764ms step_avg:58.79ms
step:1902/2330 train_time:111827ms step_avg:58.79ms
step:1903/2330 train_time:111883ms step_avg:58.79ms
step:1904/2330 train_time:111944ms step_avg:58.79ms
step:1905/2330 train_time:112001ms step_avg:58.79ms
step:1906/2330 train_time:112062ms step_avg:58.79ms
step:1907/2330 train_time:112118ms step_avg:58.79ms
step:1908/2330 train_time:112179ms step_avg:58.79ms
step:1909/2330 train_time:112237ms step_avg:58.79ms
step:1910/2330 train_time:112296ms step_avg:58.79ms
step:1911/2330 train_time:112353ms step_avg:58.79ms
step:1912/2330 train_time:112415ms step_avg:58.79ms
step:1913/2330 train_time:112473ms step_avg:58.79ms
step:1914/2330 train_time:112534ms step_avg:58.80ms
step:1915/2330 train_time:112593ms step_avg:58.80ms
step:1916/2330 train_time:112653ms step_avg:58.80ms
step:1917/2330 train_time:112711ms step_avg:58.80ms
step:1918/2330 train_time:112772ms step_avg:58.80ms
step:1919/2330 train_time:112829ms step_avg:58.80ms
step:1920/2330 train_time:112890ms step_avg:58.80ms
step:1921/2330 train_time:112947ms step_avg:58.80ms
step:1922/2330 train_time:113009ms step_avg:58.80ms
step:1923/2330 train_time:113065ms step_avg:58.80ms
step:1924/2330 train_time:113127ms step_avg:58.80ms
step:1925/2330 train_time:113183ms step_avg:58.80ms
step:1926/2330 train_time:113244ms step_avg:58.80ms
step:1927/2330 train_time:113300ms step_avg:58.80ms
step:1928/2330 train_time:113362ms step_avg:58.80ms
step:1929/2330 train_time:113419ms step_avg:58.80ms
step:1930/2330 train_time:113479ms step_avg:58.80ms
step:1931/2330 train_time:113536ms step_avg:58.80ms
step:1932/2330 train_time:113596ms step_avg:58.80ms
step:1933/2330 train_time:113654ms step_avg:58.80ms
step:1934/2330 train_time:113714ms step_avg:58.80ms
step:1935/2330 train_time:113773ms step_avg:58.80ms
step:1936/2330 train_time:113833ms step_avg:58.80ms
step:1937/2330 train_time:113892ms step_avg:58.80ms
step:1938/2330 train_time:113952ms step_avg:58.80ms
step:1939/2330 train_time:114010ms step_avg:58.80ms
step:1940/2330 train_time:114070ms step_avg:58.80ms
step:1941/2330 train_time:114127ms step_avg:58.80ms
step:1942/2330 train_time:114188ms step_avg:58.80ms
step:1943/2330 train_time:114245ms step_avg:58.80ms
step:1944/2330 train_time:114308ms step_avg:58.80ms
step:1945/2330 train_time:114364ms step_avg:58.80ms
step:1946/2330 train_time:114426ms step_avg:58.80ms
step:1947/2330 train_time:114483ms step_avg:58.80ms
step:1948/2330 train_time:114543ms step_avg:58.80ms
step:1949/2330 train_time:114600ms step_avg:58.80ms
step:1950/2330 train_time:114661ms step_avg:58.80ms
step:1951/2330 train_time:114717ms step_avg:58.80ms
step:1952/2330 train_time:114778ms step_avg:58.80ms
step:1953/2330 train_time:114835ms step_avg:58.80ms
step:1954/2330 train_time:114895ms step_avg:58.80ms
step:1955/2330 train_time:114952ms step_avg:58.80ms
step:1956/2330 train_time:115012ms step_avg:58.80ms
step:1957/2330 train_time:115071ms step_avg:58.80ms
step:1958/2330 train_time:115131ms step_avg:58.80ms
step:1959/2330 train_time:115188ms step_avg:58.80ms
step:1960/2330 train_time:115249ms step_avg:58.80ms
step:1961/2330 train_time:115306ms step_avg:58.80ms
step:1962/2330 train_time:115367ms step_avg:58.80ms
step:1963/2330 train_time:115424ms step_avg:58.80ms
step:1964/2330 train_time:115485ms step_avg:58.80ms
step:1965/2330 train_time:115542ms step_avg:58.80ms
step:1966/2330 train_time:115603ms step_avg:58.80ms
step:1967/2330 train_time:115660ms step_avg:58.80ms
step:1968/2330 train_time:115721ms step_avg:58.80ms
step:1969/2330 train_time:115777ms step_avg:58.80ms
step:1970/2330 train_time:115838ms step_avg:58.80ms
step:1971/2330 train_time:115895ms step_avg:58.80ms
step:1972/2330 train_time:115955ms step_avg:58.80ms
step:1973/2330 train_time:116013ms step_avg:58.80ms
step:1974/2330 train_time:116074ms step_avg:58.80ms
step:1975/2330 train_time:116133ms step_avg:58.80ms
step:1976/2330 train_time:116193ms step_avg:58.80ms
step:1977/2330 train_time:116251ms step_avg:58.80ms
step:1978/2330 train_time:116311ms step_avg:58.80ms
step:1979/2330 train_time:116369ms step_avg:58.80ms
step:1980/2330 train_time:116430ms step_avg:58.80ms
step:1981/2330 train_time:116486ms step_avg:58.80ms
step:1982/2330 train_time:116549ms step_avg:58.80ms
step:1983/2330 train_time:116605ms step_avg:58.80ms
step:1984/2330 train_time:116667ms step_avg:58.80ms
step:1985/2330 train_time:116724ms step_avg:58.80ms
step:1986/2330 train_time:116785ms step_avg:58.80ms
step:1987/2330 train_time:116842ms step_avg:58.80ms
step:1988/2330 train_time:116903ms step_avg:58.80ms
step:1989/2330 train_time:116959ms step_avg:58.80ms
step:1990/2330 train_time:117020ms step_avg:58.80ms
step:1991/2330 train_time:117077ms step_avg:58.80ms
step:1992/2330 train_time:117138ms step_avg:58.80ms
step:1993/2330 train_time:117195ms step_avg:58.80ms
step:1994/2330 train_time:117256ms step_avg:58.80ms
step:1995/2330 train_time:117315ms step_avg:58.80ms
step:1996/2330 train_time:117374ms step_avg:58.80ms
step:1997/2330 train_time:117432ms step_avg:58.80ms
step:1998/2330 train_time:117493ms step_avg:58.81ms
step:1999/2330 train_time:117551ms step_avg:58.80ms
step:2000/2330 train_time:117611ms step_avg:58.81ms
step:2000/2330 val_loss:5.1210 train_time:117693ms step_avg:58.85ms
step:2001/2330 train_time:117713ms step_avg:58.83ms
step:2002/2330 train_time:117735ms step_avg:58.81ms
step:2003/2330 train_time:117792ms step_avg:58.81ms
step:2004/2330 train_time:117859ms step_avg:58.81ms
step:2005/2330 train_time:117917ms step_avg:58.81ms
step:2006/2330 train_time:117980ms step_avg:58.81ms
step:2007/2330 train_time:118037ms step_avg:58.81ms
step:2008/2330 train_time:118097ms step_avg:58.81ms
step:2009/2330 train_time:118154ms step_avg:58.81ms
step:2010/2330 train_time:118213ms step_avg:58.81ms
step:2011/2330 train_time:118270ms step_avg:58.81ms
step:2012/2330 train_time:118330ms step_avg:58.81ms
step:2013/2330 train_time:118386ms step_avg:58.81ms
step:2014/2330 train_time:118447ms step_avg:58.81ms
step:2015/2330 train_time:118503ms step_avg:58.81ms
step:2016/2330 train_time:118562ms step_avg:58.81ms
step:2017/2330 train_time:118619ms step_avg:58.81ms
step:2018/2330 train_time:118680ms step_avg:58.81ms
step:2019/2330 train_time:118738ms step_avg:58.81ms
step:2020/2330 train_time:118801ms step_avg:58.81ms
step:2021/2330 train_time:118858ms step_avg:58.81ms
step:2022/2330 train_time:118919ms step_avg:58.81ms
step:2023/2330 train_time:118977ms step_avg:58.81ms
step:2024/2330 train_time:119038ms step_avg:58.81ms
step:2025/2330 train_time:119095ms step_avg:58.81ms
step:2026/2330 train_time:119155ms step_avg:58.81ms
step:2027/2330 train_time:119213ms step_avg:58.81ms
step:2028/2330 train_time:119272ms step_avg:58.81ms
step:2029/2330 train_time:119329ms step_avg:58.81ms
step:2030/2330 train_time:119389ms step_avg:58.81ms
step:2031/2330 train_time:119446ms step_avg:58.81ms
step:2032/2330 train_time:119506ms step_avg:58.81ms
step:2033/2330 train_time:119563ms step_avg:58.81ms
step:2034/2330 train_time:119622ms step_avg:58.81ms
step:2035/2330 train_time:119680ms step_avg:58.81ms
step:2036/2330 train_time:119740ms step_avg:58.81ms
step:2037/2330 train_time:119798ms step_avg:58.81ms
step:2038/2330 train_time:119858ms step_avg:58.81ms
step:2039/2330 train_time:119916ms step_avg:58.81ms
step:2040/2330 train_time:119977ms step_avg:58.81ms
step:2041/2330 train_time:120036ms step_avg:58.81ms
step:2042/2330 train_time:120096ms step_avg:58.81ms
step:2043/2330 train_time:120155ms step_avg:58.81ms
step:2044/2330 train_time:120215ms step_avg:58.81ms
step:2045/2330 train_time:120272ms step_avg:58.81ms
step:2046/2330 train_time:120332ms step_avg:58.81ms
step:2047/2330 train_time:120390ms step_avg:58.81ms
step:2048/2330 train_time:120450ms step_avg:58.81ms
step:2049/2330 train_time:120507ms step_avg:58.81ms
step:2050/2330 train_time:120567ms step_avg:58.81ms
step:2051/2330 train_time:120624ms step_avg:58.81ms
step:2052/2330 train_time:120685ms step_avg:58.81ms
step:2053/2330 train_time:120742ms step_avg:58.81ms
step:2054/2330 train_time:120803ms step_avg:58.81ms
step:2055/2330 train_time:120860ms step_avg:58.81ms
step:2056/2330 train_time:120920ms step_avg:58.81ms
step:2057/2330 train_time:120977ms step_avg:58.81ms
step:2058/2330 train_time:121038ms step_avg:58.81ms
step:2059/2330 train_time:121095ms step_avg:58.81ms
step:2060/2330 train_time:121155ms step_avg:58.81ms
step:2061/2330 train_time:121213ms step_avg:58.81ms
step:2062/2330 train_time:121273ms step_avg:58.81ms
step:2063/2330 train_time:121331ms step_avg:58.81ms
step:2064/2330 train_time:121391ms step_avg:58.81ms
step:2065/2330 train_time:121449ms step_avg:58.81ms
step:2066/2330 train_time:121509ms step_avg:58.81ms
step:2067/2330 train_time:121565ms step_avg:58.81ms
step:2068/2330 train_time:121627ms step_avg:58.81ms
step:2069/2330 train_time:121684ms step_avg:58.81ms
step:2070/2330 train_time:121745ms step_avg:58.81ms
step:2071/2330 train_time:121802ms step_avg:58.81ms
step:2072/2330 train_time:121863ms step_avg:58.81ms
step:2073/2330 train_time:121919ms step_avg:58.81ms
step:2074/2330 train_time:121980ms step_avg:58.81ms
step:2075/2330 train_time:122037ms step_avg:58.81ms
step:2076/2330 train_time:122098ms step_avg:58.81ms
step:2077/2330 train_time:122155ms step_avg:58.81ms
step:2078/2330 train_time:122215ms step_avg:58.81ms
step:2079/2330 train_time:122272ms step_avg:58.81ms
step:2080/2330 train_time:122332ms step_avg:58.81ms
step:2081/2330 train_time:122390ms step_avg:58.81ms
step:2082/2330 train_time:122450ms step_avg:58.81ms
step:2083/2330 train_time:122508ms step_avg:58.81ms
step:2084/2330 train_time:122568ms step_avg:58.81ms
step:2085/2330 train_time:122625ms step_avg:58.81ms
step:2086/2330 train_time:122686ms step_avg:58.81ms
step:2087/2330 train_time:122743ms step_avg:58.81ms
step:2088/2330 train_time:122805ms step_avg:58.81ms
step:2089/2330 train_time:122861ms step_avg:58.81ms
step:2090/2330 train_time:122923ms step_avg:58.81ms
step:2091/2330 train_time:122979ms step_avg:58.81ms
step:2092/2330 train_time:123039ms step_avg:58.81ms
step:2093/2330 train_time:123096ms step_avg:58.81ms
step:2094/2330 train_time:123156ms step_avg:58.81ms
step:2095/2330 train_time:123214ms step_avg:58.81ms
step:2096/2330 train_time:123274ms step_avg:58.81ms
step:2097/2330 train_time:123331ms step_avg:58.81ms
step:2098/2330 train_time:123391ms step_avg:58.81ms
step:2099/2330 train_time:123448ms step_avg:58.81ms
step:2100/2330 train_time:123509ms step_avg:58.81ms
step:2101/2330 train_time:123567ms step_avg:58.81ms
step:2102/2330 train_time:123628ms step_avg:58.81ms
step:2103/2330 train_time:123686ms step_avg:58.81ms
step:2104/2330 train_time:123746ms step_avg:58.81ms
step:2105/2330 train_time:123803ms step_avg:58.81ms
step:2106/2330 train_time:123864ms step_avg:58.81ms
step:2107/2330 train_time:123920ms step_avg:58.81ms
step:2108/2330 train_time:123981ms step_avg:58.81ms
step:2109/2330 train_time:124038ms step_avg:58.81ms
step:2110/2330 train_time:124098ms step_avg:58.81ms
step:2111/2330 train_time:124155ms step_avg:58.81ms
step:2112/2330 train_time:124216ms step_avg:58.81ms
step:2113/2330 train_time:124273ms step_avg:58.81ms
step:2114/2330 train_time:124334ms step_avg:58.81ms
step:2115/2330 train_time:124392ms step_avg:58.81ms
step:2116/2330 train_time:124452ms step_avg:58.81ms
step:2117/2330 train_time:124510ms step_avg:58.81ms
step:2118/2330 train_time:124570ms step_avg:58.81ms
step:2119/2330 train_time:124628ms step_avg:58.81ms
step:2120/2330 train_time:124689ms step_avg:58.82ms
step:2121/2330 train_time:124746ms step_avg:58.81ms
step:2122/2330 train_time:124808ms step_avg:58.82ms
step:2123/2330 train_time:124864ms step_avg:58.81ms
step:2124/2330 train_time:124928ms step_avg:58.82ms
step:2125/2330 train_time:124984ms step_avg:58.82ms
step:2126/2330 train_time:125045ms step_avg:58.82ms
step:2127/2330 train_time:125101ms step_avg:58.82ms
step:2128/2330 train_time:125163ms step_avg:58.82ms
step:2129/2330 train_time:125219ms step_avg:58.82ms
step:2130/2330 train_time:125281ms step_avg:58.82ms
step:2131/2330 train_time:125337ms step_avg:58.82ms
step:2132/2330 train_time:125397ms step_avg:58.82ms
step:2133/2330 train_time:125456ms step_avg:58.82ms
step:2134/2330 train_time:125516ms step_avg:58.82ms
step:2135/2330 train_time:125574ms step_avg:58.82ms
step:2136/2330 train_time:125635ms step_avg:58.82ms
step:2137/2330 train_time:125693ms step_avg:58.82ms
step:2138/2330 train_time:125753ms step_avg:58.82ms
step:2139/2330 train_time:125810ms step_avg:58.82ms
step:2140/2330 train_time:125872ms step_avg:58.82ms
step:2141/2330 train_time:125929ms step_avg:58.82ms
step:2142/2330 train_time:125991ms step_avg:58.82ms
step:2143/2330 train_time:126048ms step_avg:58.82ms
step:2144/2330 train_time:126109ms step_avg:58.82ms
step:2145/2330 train_time:126165ms step_avg:58.82ms
step:2146/2330 train_time:126226ms step_avg:58.82ms
step:2147/2330 train_time:126283ms step_avg:58.82ms
step:2148/2330 train_time:126344ms step_avg:58.82ms
step:2149/2330 train_time:126401ms step_avg:58.82ms
step:2150/2330 train_time:126461ms step_avg:58.82ms
step:2151/2330 train_time:126518ms step_avg:58.82ms
step:2152/2330 train_time:126577ms step_avg:58.82ms
step:2153/2330 train_time:126635ms step_avg:58.82ms
step:2154/2330 train_time:126696ms step_avg:58.82ms
step:2155/2330 train_time:126754ms step_avg:58.82ms
step:2156/2330 train_time:126814ms step_avg:58.82ms
step:2157/2330 train_time:126872ms step_avg:58.82ms
step:2158/2330 train_time:126932ms step_avg:58.82ms
step:2159/2330 train_time:126990ms step_avg:58.82ms
step:2160/2330 train_time:127051ms step_avg:58.82ms
step:2161/2330 train_time:127107ms step_avg:58.82ms
step:2162/2330 train_time:127170ms step_avg:58.82ms
step:2163/2330 train_time:127227ms step_avg:58.82ms
step:2164/2330 train_time:127288ms step_avg:58.82ms
step:2165/2330 train_time:127345ms step_avg:58.82ms
step:2166/2330 train_time:127406ms step_avg:58.82ms
step:2167/2330 train_time:127462ms step_avg:58.82ms
step:2168/2330 train_time:127522ms step_avg:58.82ms
step:2169/2330 train_time:127578ms step_avg:58.82ms
step:2170/2330 train_time:127640ms step_avg:58.82ms
step:2171/2330 train_time:127697ms step_avg:58.82ms
step:2172/2330 train_time:127757ms step_avg:58.82ms
step:2173/2330 train_time:127815ms step_avg:58.82ms
step:2174/2330 train_time:127875ms step_avg:58.82ms
step:2175/2330 train_time:127933ms step_avg:58.82ms
step:2176/2330 train_time:127994ms step_avg:58.82ms
step:2177/2330 train_time:128051ms step_avg:58.82ms
step:2178/2330 train_time:128113ms step_avg:58.82ms
step:2179/2330 train_time:128170ms step_avg:58.82ms
step:2180/2330 train_time:128231ms step_avg:58.82ms
step:2181/2330 train_time:128288ms step_avg:58.82ms
step:2182/2330 train_time:128350ms step_avg:58.82ms
step:2183/2330 train_time:128407ms step_avg:58.82ms
step:2184/2330 train_time:128468ms step_avg:58.82ms
step:2185/2330 train_time:128525ms step_avg:58.82ms
step:2186/2330 train_time:128586ms step_avg:58.82ms
step:2187/2330 train_time:128643ms step_avg:58.82ms
step:2188/2330 train_time:128704ms step_avg:58.82ms
step:2189/2330 train_time:128760ms step_avg:58.82ms
step:2190/2330 train_time:128821ms step_avg:58.82ms
step:2191/2330 train_time:128878ms step_avg:58.82ms
step:2192/2330 train_time:128938ms step_avg:58.82ms
step:2193/2330 train_time:128996ms step_avg:58.82ms
step:2194/2330 train_time:129056ms step_avg:58.82ms
step:2195/2330 train_time:129114ms step_avg:58.82ms
step:2196/2330 train_time:129174ms step_avg:58.82ms
step:2197/2330 train_time:129232ms step_avg:58.82ms
step:2198/2330 train_time:129292ms step_avg:58.82ms
step:2199/2330 train_time:129350ms step_avg:58.82ms
step:2200/2330 train_time:129410ms step_avg:58.82ms
step:2201/2330 train_time:129467ms step_avg:58.82ms
step:2202/2330 train_time:129528ms step_avg:58.82ms
step:2203/2330 train_time:129584ms step_avg:58.82ms
step:2204/2330 train_time:129646ms step_avg:58.82ms
step:2205/2330 train_time:129703ms step_avg:58.82ms
step:2206/2330 train_time:129764ms step_avg:58.82ms
step:2207/2330 train_time:129820ms step_avg:58.82ms
step:2208/2330 train_time:129880ms step_avg:58.82ms
step:2209/2330 train_time:129938ms step_avg:58.82ms
step:2210/2330 train_time:129998ms step_avg:58.82ms
step:2211/2330 train_time:130056ms step_avg:58.82ms
step:2212/2330 train_time:130116ms step_avg:58.82ms
step:2213/2330 train_time:130174ms step_avg:58.82ms
step:2214/2330 train_time:130235ms step_avg:58.82ms
step:2215/2330 train_time:130293ms step_avg:58.82ms
step:2216/2330 train_time:130353ms step_avg:58.82ms
step:2217/2330 train_time:130412ms step_avg:58.82ms
step:2218/2330 train_time:130471ms step_avg:58.82ms
step:2219/2330 train_time:130528ms step_avg:58.82ms
step:2220/2330 train_time:130590ms step_avg:58.82ms
step:2221/2330 train_time:130647ms step_avg:58.82ms
step:2222/2330 train_time:130708ms step_avg:58.82ms
step:2223/2330 train_time:130764ms step_avg:58.82ms
step:2224/2330 train_time:130826ms step_avg:58.82ms
step:2225/2330 train_time:130883ms step_avg:58.82ms
step:2226/2330 train_time:130944ms step_avg:58.82ms
step:2227/2330 train_time:131000ms step_avg:58.82ms
step:2228/2330 train_time:131061ms step_avg:58.82ms
step:2229/2330 train_time:131118ms step_avg:58.82ms
step:2230/2330 train_time:131179ms step_avg:58.82ms
step:2231/2330 train_time:131237ms step_avg:58.82ms
step:2232/2330 train_time:131297ms step_avg:58.82ms
step:2233/2330 train_time:131354ms step_avg:58.82ms
step:2234/2330 train_time:131414ms step_avg:58.82ms
step:2235/2330 train_time:131472ms step_avg:58.82ms
step:2236/2330 train_time:131532ms step_avg:58.82ms
step:2237/2330 train_time:131590ms step_avg:58.82ms
step:2238/2330 train_time:131652ms step_avg:58.83ms
step:2239/2330 train_time:131709ms step_avg:58.82ms
step:2240/2330 train_time:131769ms step_avg:58.83ms
step:2241/2330 train_time:131826ms step_avg:58.82ms
step:2242/2330 train_time:131888ms step_avg:58.83ms
step:2243/2330 train_time:131944ms step_avg:58.82ms
step:2244/2330 train_time:132006ms step_avg:58.83ms
step:2245/2330 train_time:132062ms step_avg:58.83ms
step:2246/2330 train_time:132124ms step_avg:58.83ms
step:2247/2330 train_time:132181ms step_avg:58.83ms
step:2248/2330 train_time:132242ms step_avg:58.83ms
step:2249/2330 train_time:132299ms step_avg:58.83ms
step:2250/2330 train_time:132359ms step_avg:58.83ms
step:2250/2330 val_loss:5.0497 train_time:132440ms step_avg:58.86ms
step:2251/2330 train_time:132460ms step_avg:58.85ms
step:2252/2330 train_time:132481ms step_avg:58.83ms
step:2253/2330 train_time:132539ms step_avg:58.83ms
step:2254/2330 train_time:132602ms step_avg:58.83ms
step:2255/2330 train_time:132660ms step_avg:58.83ms
step:2256/2330 train_time:132723ms step_avg:58.83ms
step:2257/2330 train_time:132780ms step_avg:58.83ms
step:2258/2330 train_time:132840ms step_avg:58.83ms
step:2259/2330 train_time:132898ms step_avg:58.83ms
step:2260/2330 train_time:132957ms step_avg:58.83ms
step:2261/2330 train_time:133015ms step_avg:58.83ms
step:2262/2330 train_time:133074ms step_avg:58.83ms
step:2263/2330 train_time:133131ms step_avg:58.83ms
step:2264/2330 train_time:133192ms step_avg:58.83ms
step:2265/2330 train_time:133248ms step_avg:58.83ms
step:2266/2330 train_time:133310ms step_avg:58.83ms
step:2267/2330 train_time:133367ms step_avg:58.83ms
step:2268/2330 train_time:133427ms step_avg:58.83ms
step:2269/2330 train_time:133485ms step_avg:58.83ms
step:2270/2330 train_time:133547ms step_avg:58.83ms
step:2271/2330 train_time:133605ms step_avg:58.83ms
step:2272/2330 train_time:133667ms step_avg:58.83ms
step:2273/2330 train_time:133723ms step_avg:58.83ms
step:2274/2330 train_time:133785ms step_avg:58.83ms
step:2275/2330 train_time:133841ms step_avg:58.83ms
step:2276/2330 train_time:133901ms step_avg:58.83ms
step:2277/2330 train_time:133958ms step_avg:58.83ms
step:2278/2330 train_time:134018ms step_avg:58.83ms
step:2279/2330 train_time:134076ms step_avg:58.83ms
step:2280/2330 train_time:134136ms step_avg:58.83ms
step:2281/2330 train_time:134193ms step_avg:58.83ms
step:2282/2330 train_time:134253ms step_avg:58.83ms
step:2283/2330 train_time:134311ms step_avg:58.83ms
step:2284/2330 train_time:134371ms step_avg:58.83ms
step:2285/2330 train_time:134428ms step_avg:58.83ms
step:2286/2330 train_time:134489ms step_avg:58.83ms
step:2287/2330 train_time:134546ms step_avg:58.83ms
step:2288/2330 train_time:134608ms step_avg:58.83ms
step:2289/2330 train_time:134665ms step_avg:58.83ms
step:2290/2330 train_time:134727ms step_avg:58.83ms
step:2291/2330 train_time:134783ms step_avg:58.83ms
step:2292/2330 train_time:134845ms step_avg:58.83ms
step:2293/2330 train_time:134901ms step_avg:58.83ms
step:2294/2330 train_time:134962ms step_avg:58.83ms
step:2295/2330 train_time:135019ms step_avg:58.83ms
step:2296/2330 train_time:135080ms step_avg:58.83ms
step:2297/2330 train_time:135137ms step_avg:58.83ms
step:2298/2330 train_time:135198ms step_avg:58.83ms
step:2299/2330 train_time:135255ms step_avg:58.83ms
step:2300/2330 train_time:135315ms step_avg:58.83ms
step:2301/2330 train_time:135373ms step_avg:58.83ms
step:2302/2330 train_time:135433ms step_avg:58.83ms
step:2303/2330 train_time:135491ms step_avg:58.83ms
step:2304/2330 train_time:135552ms step_avg:58.83ms
step:2305/2330 train_time:135609ms step_avg:58.83ms
step:2306/2330 train_time:135673ms step_avg:58.83ms
step:2307/2330 train_time:135730ms step_avg:58.83ms
step:2308/2330 train_time:135792ms step_avg:58.84ms
step:2309/2330 train_time:135849ms step_avg:58.83ms
step:2310/2330 train_time:135911ms step_avg:58.84ms
step:2311/2330 train_time:135968ms step_avg:58.84ms
step:2312/2330 train_time:136029ms step_avg:58.84ms
step:2313/2330 train_time:136085ms step_avg:58.83ms
step:2314/2330 train_time:136146ms step_avg:58.84ms
step:2315/2330 train_time:136202ms step_avg:58.83ms
step:2316/2330 train_time:136265ms step_avg:58.84ms
step:2317/2330 train_time:136321ms step_avg:58.84ms
step:2318/2330 train_time:136381ms step_avg:58.84ms
step:2319/2330 train_time:136439ms step_avg:58.84ms
step:2320/2330 train_time:136499ms step_avg:58.84ms
step:2321/2330 train_time:136557ms step_avg:58.84ms
step:2322/2330 train_time:136618ms step_avg:58.84ms
step:2323/2330 train_time:136675ms step_avg:58.84ms
step:2324/2330 train_time:136736ms step_avg:58.84ms
step:2325/2330 train_time:136794ms step_avg:58.84ms
step:2326/2330 train_time:136856ms step_avg:58.84ms
step:2327/2330 train_time:136914ms step_avg:58.84ms
step:2328/2330 train_time:136974ms step_avg:58.84ms
step:2329/2330 train_time:137032ms step_avg:58.84ms
step:2330/2330 train_time:137092ms step_avg:58.84ms
step:2330/2330 val_loss:5.0345 train_time:137174ms step_avg:58.87ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
