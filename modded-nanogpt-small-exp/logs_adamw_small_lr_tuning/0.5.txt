import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--adamw_lr', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_lr_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_lr_tuning/{args2.adamw_lr}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=float(args2.adamw_lr),  betas=(0.85, 0.85), eps=1e-10, weight_decay=0.0, fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_lr_tuning/{args2.adamw_lr}", exist_ok=True)
            torch.save(log, f"logs_adamw_small_lr_tuning/{args2.adamw_lr}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 03:10:43 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   35C    P0             117W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   34C    P0             117W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             118W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   28C    P0             115W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   34C    P0             118W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   32C    P0             114W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:86ms step_avg:85.73ms
step:2/2330 train_time:181ms step_avg:90.44ms
step:3/2330 train_time:200ms step_avg:66.59ms
step:4/2330 train_time:219ms step_avg:54.75ms
step:5/2330 train_time:271ms step_avg:54.28ms
step:6/2330 train_time:329ms step_avg:54.86ms
step:7/2330 train_time:383ms step_avg:54.68ms
step:8/2330 train_time:440ms step_avg:54.97ms
step:9/2330 train_time:494ms step_avg:54.85ms
step:10/2330 train_time:550ms step_avg:55.03ms
step:11/2330 train_time:604ms step_avg:54.94ms
step:12/2330 train_time:661ms step_avg:55.08ms
step:13/2330 train_time:715ms step_avg:54.98ms
step:14/2330 train_time:772ms step_avg:55.11ms
step:15/2330 train_time:825ms step_avg:55.03ms
step:16/2330 train_time:882ms step_avg:55.14ms
step:17/2330 train_time:936ms step_avg:55.07ms
step:18/2330 train_time:993ms step_avg:55.15ms
step:19/2330 train_time:1046ms step_avg:55.08ms
step:20/2330 train_time:1104ms step_avg:55.21ms
step:21/2330 train_time:1158ms step_avg:55.16ms
step:22/2330 train_time:1218ms step_avg:55.35ms
step:23/2330 train_time:1272ms step_avg:55.31ms
step:24/2330 train_time:1330ms step_avg:55.41ms
step:25/2330 train_time:1384ms step_avg:55.36ms
step:26/2330 train_time:1441ms step_avg:55.43ms
step:27/2330 train_time:1495ms step_avg:55.37ms
step:28/2330 train_time:1552ms step_avg:55.44ms
step:29/2330 train_time:1606ms step_avg:55.39ms
step:30/2330 train_time:1664ms step_avg:55.46ms
step:31/2330 train_time:1718ms step_avg:55.41ms
step:32/2330 train_time:1775ms step_avg:55.47ms
step:33/2330 train_time:1829ms step_avg:55.42ms
step:34/2330 train_time:1887ms step_avg:55.49ms
step:35/2330 train_time:1941ms step_avg:55.46ms
step:36/2330 train_time:1997ms step_avg:55.48ms
step:37/2330 train_time:2051ms step_avg:55.44ms
step:38/2330 train_time:2110ms step_avg:55.53ms
step:39/2330 train_time:2164ms step_avg:55.49ms
step:40/2330 train_time:2221ms step_avg:55.53ms
step:41/2330 train_time:2276ms step_avg:55.51ms
step:42/2330 train_time:2333ms step_avg:55.55ms
step:43/2330 train_time:2387ms step_avg:55.52ms
step:44/2330 train_time:2446ms step_avg:55.59ms
step:45/2330 train_time:2500ms step_avg:55.55ms
step:46/2330 train_time:2558ms step_avg:55.61ms
step:47/2330 train_time:2613ms step_avg:55.60ms
step:48/2330 train_time:2671ms step_avg:55.64ms
step:49/2330 train_time:2725ms step_avg:55.61ms
step:50/2330 train_time:2784ms step_avg:55.67ms
step:51/2330 train_time:2838ms step_avg:55.65ms
step:52/2330 train_time:2895ms step_avg:55.68ms
step:53/2330 train_time:2950ms step_avg:55.66ms
step:54/2330 train_time:3009ms step_avg:55.71ms
step:55/2330 train_time:3063ms step_avg:55.69ms
step:56/2330 train_time:3120ms step_avg:55.72ms
step:57/2330 train_time:3175ms step_avg:55.69ms
step:58/2330 train_time:3233ms step_avg:55.75ms
step:59/2330 train_time:3288ms step_avg:55.72ms
step:60/2330 train_time:3346ms step_avg:55.77ms
step:61/2330 train_time:3401ms step_avg:55.75ms
step:62/2330 train_time:3459ms step_avg:55.80ms
step:63/2330 train_time:3514ms step_avg:55.78ms
step:64/2330 train_time:3572ms step_avg:55.82ms
step:65/2330 train_time:3627ms step_avg:55.80ms
step:66/2330 train_time:3685ms step_avg:55.83ms
step:67/2330 train_time:3740ms step_avg:55.82ms
step:68/2330 train_time:3797ms step_avg:55.85ms
step:69/2330 train_time:3852ms step_avg:55.83ms
step:70/2330 train_time:3910ms step_avg:55.86ms
step:71/2330 train_time:3965ms step_avg:55.84ms
step:72/2330 train_time:4022ms step_avg:55.87ms
step:73/2330 train_time:4077ms step_avg:55.85ms
step:74/2330 train_time:4135ms step_avg:55.87ms
step:75/2330 train_time:4189ms step_avg:55.86ms
step:76/2330 train_time:4247ms step_avg:55.88ms
step:77/2330 train_time:4302ms step_avg:55.87ms
step:78/2330 train_time:4360ms step_avg:55.89ms
step:79/2330 train_time:4414ms step_avg:55.88ms
step:80/2330 train_time:4473ms step_avg:55.91ms
step:81/2330 train_time:4528ms step_avg:55.90ms
step:82/2330 train_time:4586ms step_avg:55.93ms
step:83/2330 train_time:4640ms step_avg:55.91ms
step:84/2330 train_time:4699ms step_avg:55.94ms
step:85/2330 train_time:4754ms step_avg:55.93ms
step:86/2330 train_time:4812ms step_avg:55.95ms
step:87/2330 train_time:4867ms step_avg:55.94ms
step:88/2330 train_time:4925ms step_avg:55.96ms
step:89/2330 train_time:4979ms step_avg:55.95ms
step:90/2330 train_time:5038ms step_avg:55.97ms
step:91/2330 train_time:5093ms step_avg:55.97ms
step:92/2330 train_time:5151ms step_avg:55.99ms
step:93/2330 train_time:5206ms step_avg:55.98ms
step:94/2330 train_time:5264ms step_avg:56.00ms
step:95/2330 train_time:5319ms step_avg:55.99ms
step:96/2330 train_time:5377ms step_avg:56.01ms
step:97/2330 train_time:5431ms step_avg:55.99ms
step:98/2330 train_time:5490ms step_avg:56.02ms
step:99/2330 train_time:5545ms step_avg:56.01ms
step:100/2330 train_time:5603ms step_avg:56.03ms
step:101/2330 train_time:5658ms step_avg:56.02ms
step:102/2330 train_time:5717ms step_avg:56.05ms
step:103/2330 train_time:5772ms step_avg:56.04ms
step:104/2330 train_time:5830ms step_avg:56.06ms
step:105/2330 train_time:5884ms step_avg:56.04ms
step:106/2330 train_time:5943ms step_avg:56.07ms
step:107/2330 train_time:5998ms step_avg:56.05ms
step:108/2330 train_time:6057ms step_avg:56.08ms
step:109/2330 train_time:6112ms step_avg:56.07ms
step:110/2330 train_time:6170ms step_avg:56.09ms
step:111/2330 train_time:6224ms step_avg:56.08ms
step:112/2330 train_time:6283ms step_avg:56.10ms
step:113/2330 train_time:6338ms step_avg:56.09ms
step:114/2330 train_time:6396ms step_avg:56.10ms
step:115/2330 train_time:6451ms step_avg:56.10ms
step:116/2330 train_time:6509ms step_avg:56.11ms
step:117/2330 train_time:6564ms step_avg:56.10ms
step:118/2330 train_time:6622ms step_avg:56.12ms
step:119/2330 train_time:6676ms step_avg:56.10ms
step:120/2330 train_time:6735ms step_avg:56.13ms
step:121/2330 train_time:6791ms step_avg:56.12ms
step:122/2330 train_time:6849ms step_avg:56.14ms
step:123/2330 train_time:6904ms step_avg:56.13ms
step:124/2330 train_time:6962ms step_avg:56.15ms
step:125/2330 train_time:7017ms step_avg:56.14ms
step:126/2330 train_time:7075ms step_avg:56.15ms
step:127/2330 train_time:7131ms step_avg:56.15ms
step:128/2330 train_time:7189ms step_avg:56.16ms
step:129/2330 train_time:7244ms step_avg:56.15ms
step:130/2330 train_time:7303ms step_avg:56.18ms
step:131/2330 train_time:7358ms step_avg:56.17ms
step:132/2330 train_time:7416ms step_avg:56.18ms
step:133/2330 train_time:7472ms step_avg:56.18ms
step:134/2330 train_time:7530ms step_avg:56.19ms
step:135/2330 train_time:7585ms step_avg:56.19ms
step:136/2330 train_time:7643ms step_avg:56.20ms
step:137/2330 train_time:7698ms step_avg:56.19ms
step:138/2330 train_time:7757ms step_avg:56.21ms
step:139/2330 train_time:7812ms step_avg:56.20ms
step:140/2330 train_time:7870ms step_avg:56.22ms
step:141/2330 train_time:7925ms step_avg:56.21ms
step:142/2330 train_time:7983ms step_avg:56.22ms
step:143/2330 train_time:8037ms step_avg:56.20ms
step:144/2330 train_time:8096ms step_avg:56.22ms
step:145/2330 train_time:8152ms step_avg:56.22ms
step:146/2330 train_time:8210ms step_avg:56.23ms
step:147/2330 train_time:8264ms step_avg:56.22ms
step:148/2330 train_time:8322ms step_avg:56.23ms
step:149/2330 train_time:8377ms step_avg:56.22ms
step:150/2330 train_time:8436ms step_avg:56.24ms
step:151/2330 train_time:8491ms step_avg:56.23ms
step:152/2330 train_time:8550ms step_avg:56.25ms
step:153/2330 train_time:8605ms step_avg:56.24ms
step:154/2330 train_time:8665ms step_avg:56.26ms
step:155/2330 train_time:8719ms step_avg:56.25ms
step:156/2330 train_time:8778ms step_avg:56.27ms
step:157/2330 train_time:8834ms step_avg:56.26ms
step:158/2330 train_time:8892ms step_avg:56.28ms
step:159/2330 train_time:8948ms step_avg:56.27ms
step:160/2330 train_time:9006ms step_avg:56.29ms
step:161/2330 train_time:9060ms step_avg:56.28ms
step:162/2330 train_time:9119ms step_avg:56.29ms
step:163/2330 train_time:9175ms step_avg:56.29ms
step:164/2330 train_time:9233ms step_avg:56.30ms
step:165/2330 train_time:9288ms step_avg:56.29ms
step:166/2330 train_time:9346ms step_avg:56.30ms
step:167/2330 train_time:9401ms step_avg:56.29ms
step:168/2330 train_time:9460ms step_avg:56.31ms
step:169/2330 train_time:9515ms step_avg:56.30ms
step:170/2330 train_time:9574ms step_avg:56.32ms
step:171/2330 train_time:9629ms step_avg:56.31ms
step:172/2330 train_time:9688ms step_avg:56.33ms
step:173/2330 train_time:9743ms step_avg:56.32ms
step:174/2330 train_time:9802ms step_avg:56.33ms
step:175/2330 train_time:9857ms step_avg:56.33ms
step:176/2330 train_time:9915ms step_avg:56.34ms
step:177/2330 train_time:9970ms step_avg:56.33ms
step:178/2330 train_time:10029ms step_avg:56.34ms
step:179/2330 train_time:10084ms step_avg:56.33ms
step:180/2330 train_time:10143ms step_avg:56.35ms
step:181/2330 train_time:10197ms step_avg:56.34ms
step:182/2330 train_time:10256ms step_avg:56.35ms
step:183/2330 train_time:10311ms step_avg:56.35ms
step:184/2330 train_time:10370ms step_avg:56.36ms
step:185/2330 train_time:10426ms step_avg:56.36ms
step:186/2330 train_time:10484ms step_avg:56.37ms
step:187/2330 train_time:10539ms step_avg:56.36ms
step:188/2330 train_time:10598ms step_avg:56.37ms
step:189/2330 train_time:10654ms step_avg:56.37ms
step:190/2330 train_time:10711ms step_avg:56.38ms
step:191/2330 train_time:10767ms step_avg:56.37ms
step:192/2330 train_time:10826ms step_avg:56.38ms
step:193/2330 train_time:10880ms step_avg:56.37ms
step:194/2330 train_time:10939ms step_avg:56.39ms
step:195/2330 train_time:10995ms step_avg:56.38ms
step:196/2330 train_time:11053ms step_avg:56.39ms
step:197/2330 train_time:11108ms step_avg:56.39ms
step:198/2330 train_time:11167ms step_avg:56.40ms
step:199/2330 train_time:11222ms step_avg:56.39ms
step:200/2330 train_time:11281ms step_avg:56.41ms
step:201/2330 train_time:11337ms step_avg:56.40ms
step:202/2330 train_time:11395ms step_avg:56.41ms
step:203/2330 train_time:11450ms step_avg:56.41ms
step:204/2330 train_time:11509ms step_avg:56.42ms
step:205/2330 train_time:11564ms step_avg:56.41ms
step:206/2330 train_time:11623ms step_avg:56.42ms
step:207/2330 train_time:11678ms step_avg:56.42ms
step:208/2330 train_time:11737ms step_avg:56.43ms
step:209/2330 train_time:11793ms step_avg:56.42ms
step:210/2330 train_time:11851ms step_avg:56.44ms
step:211/2330 train_time:11906ms step_avg:56.43ms
step:212/2330 train_time:11965ms step_avg:56.44ms
step:213/2330 train_time:12020ms step_avg:56.43ms
step:214/2330 train_time:12080ms step_avg:56.45ms
step:215/2330 train_time:12135ms step_avg:56.44ms
step:216/2330 train_time:12194ms step_avg:56.45ms
step:217/2330 train_time:12249ms step_avg:56.45ms
step:218/2330 train_time:12308ms step_avg:56.46ms
step:219/2330 train_time:12363ms step_avg:56.45ms
step:220/2330 train_time:12423ms step_avg:56.47ms
step:221/2330 train_time:12478ms step_avg:56.46ms
step:222/2330 train_time:12537ms step_avg:56.47ms
step:223/2330 train_time:12593ms step_avg:56.47ms
step:224/2330 train_time:12651ms step_avg:56.48ms
step:225/2330 train_time:12706ms step_avg:56.47ms
step:226/2330 train_time:12765ms step_avg:56.48ms
step:227/2330 train_time:12820ms step_avg:56.48ms
step:228/2330 train_time:12879ms step_avg:56.49ms
step:229/2330 train_time:12935ms step_avg:56.48ms
step:230/2330 train_time:12993ms step_avg:56.49ms
step:231/2330 train_time:13048ms step_avg:56.49ms
step:232/2330 train_time:13107ms step_avg:56.49ms
step:233/2330 train_time:13162ms step_avg:56.49ms
step:234/2330 train_time:13221ms step_avg:56.50ms
step:235/2330 train_time:13277ms step_avg:56.50ms
step:236/2330 train_time:13335ms step_avg:56.51ms
step:237/2330 train_time:13390ms step_avg:56.50ms
step:238/2330 train_time:13450ms step_avg:56.51ms
step:239/2330 train_time:13504ms step_avg:56.50ms
step:240/2330 train_time:13563ms step_avg:56.51ms
step:241/2330 train_time:13619ms step_avg:56.51ms
step:242/2330 train_time:13678ms step_avg:56.52ms
step:243/2330 train_time:13733ms step_avg:56.52ms
step:244/2330 train_time:13792ms step_avg:56.53ms
step:245/2330 train_time:13847ms step_avg:56.52ms
step:246/2330 train_time:13908ms step_avg:56.54ms
step:247/2330 train_time:13963ms step_avg:56.53ms
step:248/2330 train_time:14022ms step_avg:56.54ms
step:249/2330 train_time:14077ms step_avg:56.53ms
step:250/2330 train_time:14137ms step_avg:56.55ms
step:250/2330 val_loss:6.5473 train_time:14216ms step_avg:56.86ms
step:251/2330 train_time:14234ms step_avg:56.71ms
step:252/2330 train_time:14254ms step_avg:56.56ms
step:253/2330 train_time:14309ms step_avg:56.56ms
step:254/2330 train_time:14372ms step_avg:56.58ms
step:255/2330 train_time:14429ms step_avg:56.59ms
step:256/2330 train_time:14489ms step_avg:56.60ms
step:257/2330 train_time:14545ms step_avg:56.60ms
step:258/2330 train_time:14605ms step_avg:56.61ms
step:259/2330 train_time:14661ms step_avg:56.61ms
step:260/2330 train_time:14719ms step_avg:56.61ms
step:261/2330 train_time:14774ms step_avg:56.61ms
step:262/2330 train_time:14832ms step_avg:56.61ms
step:263/2330 train_time:14887ms step_avg:56.60ms
step:264/2330 train_time:14945ms step_avg:56.61ms
step:265/2330 train_time:15001ms step_avg:56.61ms
step:266/2330 train_time:15059ms step_avg:56.61ms
step:267/2330 train_time:15114ms step_avg:56.61ms
step:268/2330 train_time:15172ms step_avg:56.61ms
step:269/2330 train_time:15228ms step_avg:56.61ms
step:270/2330 train_time:15288ms step_avg:56.62ms
step:271/2330 train_time:15344ms step_avg:56.62ms
step:272/2330 train_time:15405ms step_avg:56.64ms
step:273/2330 train_time:15461ms step_avg:56.63ms
step:274/2330 train_time:15521ms step_avg:56.65ms
step:275/2330 train_time:15577ms step_avg:56.64ms
step:276/2330 train_time:15638ms step_avg:56.66ms
step:277/2330 train_time:15693ms step_avg:56.65ms
step:278/2330 train_time:15751ms step_avg:56.66ms
step:279/2330 train_time:15806ms step_avg:56.65ms
step:280/2330 train_time:15865ms step_avg:56.66ms
step:281/2330 train_time:15920ms step_avg:56.66ms
step:282/2330 train_time:15979ms step_avg:56.66ms
step:283/2330 train_time:16034ms step_avg:56.66ms
step:284/2330 train_time:16093ms step_avg:56.67ms
step:285/2330 train_time:16148ms step_avg:56.66ms
step:286/2330 train_time:16208ms step_avg:56.67ms
step:287/2330 train_time:16265ms step_avg:56.67ms
step:288/2330 train_time:16325ms step_avg:56.68ms
step:289/2330 train_time:16381ms step_avg:56.68ms
step:290/2330 train_time:16441ms step_avg:56.69ms
step:291/2330 train_time:16497ms step_avg:56.69ms
step:292/2330 train_time:16558ms step_avg:56.70ms
step:293/2330 train_time:16614ms step_avg:56.70ms
step:294/2330 train_time:16673ms step_avg:56.71ms
step:295/2330 train_time:16729ms step_avg:56.71ms
step:296/2330 train_time:16787ms step_avg:56.71ms
step:297/2330 train_time:16843ms step_avg:56.71ms
step:298/2330 train_time:16902ms step_avg:56.72ms
step:299/2330 train_time:16958ms step_avg:56.71ms
step:300/2330 train_time:17016ms step_avg:56.72ms
step:301/2330 train_time:17072ms step_avg:56.72ms
step:302/2330 train_time:17130ms step_avg:56.72ms
step:303/2330 train_time:17186ms step_avg:56.72ms
step:304/2330 train_time:17245ms step_avg:56.73ms
step:305/2330 train_time:17301ms step_avg:56.72ms
step:306/2330 train_time:17360ms step_avg:56.73ms
step:307/2330 train_time:17417ms step_avg:56.73ms
step:308/2330 train_time:17476ms step_avg:56.74ms
step:309/2330 train_time:17531ms step_avg:56.74ms
step:310/2330 train_time:17591ms step_avg:56.74ms
step:311/2330 train_time:17647ms step_avg:56.74ms
step:312/2330 train_time:17705ms step_avg:56.75ms
step:313/2330 train_time:17761ms step_avg:56.74ms
step:314/2330 train_time:17820ms step_avg:56.75ms
step:315/2330 train_time:17875ms step_avg:56.75ms
step:316/2330 train_time:17934ms step_avg:56.75ms
step:317/2330 train_time:17989ms step_avg:56.75ms
step:318/2330 train_time:18048ms step_avg:56.75ms
step:319/2330 train_time:18103ms step_avg:56.75ms
step:320/2330 train_time:18162ms step_avg:56.76ms
step:321/2330 train_time:18217ms step_avg:56.75ms
step:322/2330 train_time:18277ms step_avg:56.76ms
step:323/2330 train_time:18333ms step_avg:56.76ms
step:324/2330 train_time:18393ms step_avg:56.77ms
step:325/2330 train_time:18449ms step_avg:56.77ms
step:326/2330 train_time:18509ms step_avg:56.78ms
step:327/2330 train_time:18566ms step_avg:56.78ms
step:328/2330 train_time:18624ms step_avg:56.78ms
step:329/2330 train_time:18680ms step_avg:56.78ms
step:330/2330 train_time:18739ms step_avg:56.78ms
step:331/2330 train_time:18795ms step_avg:56.78ms
step:332/2330 train_time:18854ms step_avg:56.79ms
step:333/2330 train_time:18910ms step_avg:56.79ms
step:334/2330 train_time:18968ms step_avg:56.79ms
step:335/2330 train_time:19024ms step_avg:56.79ms
step:336/2330 train_time:19082ms step_avg:56.79ms
step:337/2330 train_time:19137ms step_avg:56.79ms
step:338/2330 train_time:19197ms step_avg:56.80ms
step:339/2330 train_time:19253ms step_avg:56.79ms
step:340/2330 train_time:19312ms step_avg:56.80ms
step:341/2330 train_time:19368ms step_avg:56.80ms
step:342/2330 train_time:19427ms step_avg:56.80ms
step:343/2330 train_time:19482ms step_avg:56.80ms
step:344/2330 train_time:19541ms step_avg:56.81ms
step:345/2330 train_time:19597ms step_avg:56.80ms
step:346/2330 train_time:19658ms step_avg:56.81ms
step:347/2330 train_time:19712ms step_avg:56.81ms
step:348/2330 train_time:19773ms step_avg:56.82ms
step:349/2330 train_time:19828ms step_avg:56.81ms
step:350/2330 train_time:19887ms step_avg:56.82ms
step:351/2330 train_time:19942ms step_avg:56.82ms
step:352/2330 train_time:20001ms step_avg:56.82ms
step:353/2330 train_time:20057ms step_avg:56.82ms
step:354/2330 train_time:20117ms step_avg:56.83ms
step:355/2330 train_time:20172ms step_avg:56.82ms
step:356/2330 train_time:20232ms step_avg:56.83ms
step:357/2330 train_time:20288ms step_avg:56.83ms
step:358/2330 train_time:20346ms step_avg:56.83ms
step:359/2330 train_time:20402ms step_avg:56.83ms
step:360/2330 train_time:20461ms step_avg:56.84ms
step:361/2330 train_time:20516ms step_avg:56.83ms
step:362/2330 train_time:20575ms step_avg:56.84ms
step:363/2330 train_time:20631ms step_avg:56.83ms
step:364/2330 train_time:20690ms step_avg:56.84ms
step:365/2330 train_time:20745ms step_avg:56.84ms
step:366/2330 train_time:20804ms step_avg:56.84ms
step:367/2330 train_time:20860ms step_avg:56.84ms
step:368/2330 train_time:20919ms step_avg:56.85ms
step:369/2330 train_time:20975ms step_avg:56.84ms
step:370/2330 train_time:21034ms step_avg:56.85ms
step:371/2330 train_time:21089ms step_avg:56.84ms
step:372/2330 train_time:21148ms step_avg:56.85ms
step:373/2330 train_time:21204ms step_avg:56.85ms
step:374/2330 train_time:21263ms step_avg:56.85ms
step:375/2330 train_time:21319ms step_avg:56.85ms
step:376/2330 train_time:21378ms step_avg:56.86ms
step:377/2330 train_time:21434ms step_avg:56.85ms
step:378/2330 train_time:21493ms step_avg:56.86ms
step:379/2330 train_time:21548ms step_avg:56.86ms
step:380/2330 train_time:21608ms step_avg:56.86ms
step:381/2330 train_time:21663ms step_avg:56.86ms
step:382/2330 train_time:21722ms step_avg:56.86ms
step:383/2330 train_time:21778ms step_avg:56.86ms
step:384/2330 train_time:21837ms step_avg:56.87ms
step:385/2330 train_time:21892ms step_avg:56.86ms
step:386/2330 train_time:21952ms step_avg:56.87ms
step:387/2330 train_time:22007ms step_avg:56.87ms
step:388/2330 train_time:22067ms step_avg:56.87ms
step:389/2330 train_time:22123ms step_avg:56.87ms
step:390/2330 train_time:22182ms step_avg:56.88ms
step:391/2330 train_time:22237ms step_avg:56.87ms
step:392/2330 train_time:22298ms step_avg:56.88ms
step:393/2330 train_time:22354ms step_avg:56.88ms
step:394/2330 train_time:22414ms step_avg:56.89ms
step:395/2330 train_time:22469ms step_avg:56.88ms
step:396/2330 train_time:22528ms step_avg:56.89ms
step:397/2330 train_time:22583ms step_avg:56.89ms
step:398/2330 train_time:22642ms step_avg:56.89ms
step:399/2330 train_time:22698ms step_avg:56.89ms
step:400/2330 train_time:22758ms step_avg:56.89ms
step:401/2330 train_time:22813ms step_avg:56.89ms
step:402/2330 train_time:22873ms step_avg:56.90ms
step:403/2330 train_time:22928ms step_avg:56.89ms
step:404/2330 train_time:22987ms step_avg:56.90ms
step:405/2330 train_time:23042ms step_avg:56.89ms
step:406/2330 train_time:23101ms step_avg:56.90ms
step:407/2330 train_time:23156ms step_avg:56.89ms
step:408/2330 train_time:23215ms step_avg:56.90ms
step:409/2330 train_time:23270ms step_avg:56.89ms
step:410/2330 train_time:23329ms step_avg:56.90ms
step:411/2330 train_time:23385ms step_avg:56.90ms
step:412/2330 train_time:23443ms step_avg:56.90ms
step:413/2330 train_time:23499ms step_avg:56.90ms
step:414/2330 train_time:23558ms step_avg:56.90ms
step:415/2330 train_time:23613ms step_avg:56.90ms
step:416/2330 train_time:23671ms step_avg:56.90ms
step:417/2330 train_time:23727ms step_avg:56.90ms
step:418/2330 train_time:23786ms step_avg:56.90ms
step:419/2330 train_time:23842ms step_avg:56.90ms
step:420/2330 train_time:23901ms step_avg:56.91ms
step:421/2330 train_time:23956ms step_avg:56.90ms
step:422/2330 train_time:24016ms step_avg:56.91ms
step:423/2330 train_time:24070ms step_avg:56.90ms
step:424/2330 train_time:24130ms step_avg:56.91ms
step:425/2330 train_time:24185ms step_avg:56.91ms
step:426/2330 train_time:24245ms step_avg:56.91ms
step:427/2330 train_time:24300ms step_avg:56.91ms
step:428/2330 train_time:24361ms step_avg:56.92ms
step:429/2330 train_time:24416ms step_avg:56.91ms
step:430/2330 train_time:24477ms step_avg:56.92ms
step:431/2330 train_time:24531ms step_avg:56.92ms
step:432/2330 train_time:24592ms step_avg:56.92ms
step:433/2330 train_time:24647ms step_avg:56.92ms
step:434/2330 train_time:24706ms step_avg:56.93ms
step:435/2330 train_time:24761ms step_avg:56.92ms
step:436/2330 train_time:24821ms step_avg:56.93ms
step:437/2330 train_time:24876ms step_avg:56.92ms
step:438/2330 train_time:24936ms step_avg:56.93ms
step:439/2330 train_time:24991ms step_avg:56.93ms
step:440/2330 train_time:25051ms step_avg:56.93ms
step:441/2330 train_time:25106ms step_avg:56.93ms
step:442/2330 train_time:25166ms step_avg:56.94ms
step:443/2330 train_time:25221ms step_avg:56.93ms
step:444/2330 train_time:25281ms step_avg:56.94ms
step:445/2330 train_time:25336ms step_avg:56.94ms
step:446/2330 train_time:25395ms step_avg:56.94ms
step:447/2330 train_time:25450ms step_avg:56.94ms
step:448/2330 train_time:25509ms step_avg:56.94ms
step:449/2330 train_time:25565ms step_avg:56.94ms
step:450/2330 train_time:25624ms step_avg:56.94ms
step:451/2330 train_time:25679ms step_avg:56.94ms
step:452/2330 train_time:25739ms step_avg:56.95ms
step:453/2330 train_time:25795ms step_avg:56.94ms
step:454/2330 train_time:25854ms step_avg:56.95ms
step:455/2330 train_time:25909ms step_avg:56.94ms
step:456/2330 train_time:25969ms step_avg:56.95ms
step:457/2330 train_time:26025ms step_avg:56.95ms
step:458/2330 train_time:26083ms step_avg:56.95ms
step:459/2330 train_time:26138ms step_avg:56.95ms
step:460/2330 train_time:26199ms step_avg:56.95ms
step:461/2330 train_time:26254ms step_avg:56.95ms
step:462/2330 train_time:26315ms step_avg:56.96ms
step:463/2330 train_time:26370ms step_avg:56.95ms
step:464/2330 train_time:26429ms step_avg:56.96ms
step:465/2330 train_time:26485ms step_avg:56.96ms
step:466/2330 train_time:26543ms step_avg:56.96ms
step:467/2330 train_time:26598ms step_avg:56.96ms
step:468/2330 train_time:26658ms step_avg:56.96ms
step:469/2330 train_time:26713ms step_avg:56.96ms
step:470/2330 train_time:26772ms step_avg:56.96ms
step:471/2330 train_time:26827ms step_avg:56.96ms
step:472/2330 train_time:26887ms step_avg:56.96ms
step:473/2330 train_time:26942ms step_avg:56.96ms
step:474/2330 train_time:27002ms step_avg:56.97ms
step:475/2330 train_time:27057ms step_avg:56.96ms
step:476/2330 train_time:27118ms step_avg:56.97ms
step:477/2330 train_time:27173ms step_avg:56.97ms
step:478/2330 train_time:27233ms step_avg:56.97ms
step:479/2330 train_time:27288ms step_avg:56.97ms
step:480/2330 train_time:27347ms step_avg:56.97ms
step:481/2330 train_time:27402ms step_avg:56.97ms
step:482/2330 train_time:27463ms step_avg:56.98ms
step:483/2330 train_time:27518ms step_avg:56.97ms
step:484/2330 train_time:27579ms step_avg:56.98ms
step:485/2330 train_time:27635ms step_avg:56.98ms
step:486/2330 train_time:27695ms step_avg:56.99ms
step:487/2330 train_time:27750ms step_avg:56.98ms
step:488/2330 train_time:27809ms step_avg:56.99ms
step:489/2330 train_time:27865ms step_avg:56.98ms
step:490/2330 train_time:27924ms step_avg:56.99ms
step:491/2330 train_time:27979ms step_avg:56.98ms
step:492/2330 train_time:28038ms step_avg:56.99ms
step:493/2330 train_time:28094ms step_avg:56.99ms
step:494/2330 train_time:28153ms step_avg:56.99ms
step:495/2330 train_time:28209ms step_avg:56.99ms
step:496/2330 train_time:28268ms step_avg:56.99ms
step:497/2330 train_time:28324ms step_avg:56.99ms
step:498/2330 train_time:28384ms step_avg:57.00ms
step:499/2330 train_time:28439ms step_avg:56.99ms
step:500/2330 train_time:28498ms step_avg:57.00ms
step:500/2330 val_loss:6.1115 train_time:28578ms step_avg:57.16ms
step:501/2330 train_time:28598ms step_avg:57.08ms
step:502/2330 train_time:28618ms step_avg:57.01ms
step:503/2330 train_time:28673ms step_avg:57.00ms
step:504/2330 train_time:28736ms step_avg:57.02ms
step:505/2330 train_time:28792ms step_avg:57.01ms
step:506/2330 train_time:28854ms step_avg:57.02ms
step:507/2330 train_time:28910ms step_avg:57.02ms
step:508/2330 train_time:28968ms step_avg:57.02ms
step:509/2330 train_time:29023ms step_avg:57.02ms
step:510/2330 train_time:29083ms step_avg:57.03ms
step:511/2330 train_time:29138ms step_avg:57.02ms
step:512/2330 train_time:29197ms step_avg:57.03ms
step:513/2330 train_time:29252ms step_avg:57.02ms
step:514/2330 train_time:29311ms step_avg:57.03ms
step:515/2330 train_time:29366ms step_avg:57.02ms
step:516/2330 train_time:29425ms step_avg:57.02ms
step:517/2330 train_time:29480ms step_avg:57.02ms
step:518/2330 train_time:29539ms step_avg:57.02ms
step:519/2330 train_time:29594ms step_avg:57.02ms
step:520/2330 train_time:29655ms step_avg:57.03ms
step:521/2330 train_time:29711ms step_avg:57.03ms
step:522/2330 train_time:29772ms step_avg:57.03ms
step:523/2330 train_time:29828ms step_avg:57.03ms
step:524/2330 train_time:29888ms step_avg:57.04ms
step:525/2330 train_time:29944ms step_avg:57.04ms
step:526/2330 train_time:30003ms step_avg:57.04ms
step:527/2330 train_time:30059ms step_avg:57.04ms
step:528/2330 train_time:30118ms step_avg:57.04ms
step:529/2330 train_time:30173ms step_avg:57.04ms
step:530/2330 train_time:30232ms step_avg:57.04ms
step:531/2330 train_time:30287ms step_avg:57.04ms
step:532/2330 train_time:30346ms step_avg:57.04ms
step:533/2330 train_time:30402ms step_avg:57.04ms
step:534/2330 train_time:30460ms step_avg:57.04ms
step:535/2330 train_time:30515ms step_avg:57.04ms
step:536/2330 train_time:30575ms step_avg:57.04ms
step:537/2330 train_time:30630ms step_avg:57.04ms
step:538/2330 train_time:30691ms step_avg:57.05ms
step:539/2330 train_time:30746ms step_avg:57.04ms
step:540/2330 train_time:30807ms step_avg:57.05ms
step:541/2330 train_time:30863ms step_avg:57.05ms
step:542/2330 train_time:30922ms step_avg:57.05ms
step:543/2330 train_time:30978ms step_avg:57.05ms
step:544/2330 train_time:31038ms step_avg:57.05ms
step:545/2330 train_time:31093ms step_avg:57.05ms
step:546/2330 train_time:31153ms step_avg:57.06ms
step:547/2330 train_time:31208ms step_avg:57.05ms
step:548/2330 train_time:31268ms step_avg:57.06ms
step:549/2330 train_time:31323ms step_avg:57.06ms
step:550/2330 train_time:31381ms step_avg:57.06ms
step:551/2330 train_time:31437ms step_avg:57.05ms
step:552/2330 train_time:31496ms step_avg:57.06ms
step:553/2330 train_time:31552ms step_avg:57.06ms
step:554/2330 train_time:31610ms step_avg:57.06ms
step:555/2330 train_time:31666ms step_avg:57.06ms
step:556/2330 train_time:31726ms step_avg:57.06ms
step:557/2330 train_time:31782ms step_avg:57.06ms
step:558/2330 train_time:31842ms step_avg:57.06ms
step:559/2330 train_time:31898ms step_avg:57.06ms
step:560/2330 train_time:31957ms step_avg:57.07ms
step:561/2330 train_time:32012ms step_avg:57.06ms
step:562/2330 train_time:32073ms step_avg:57.07ms
step:563/2330 train_time:32128ms step_avg:57.07ms
step:564/2330 train_time:32189ms step_avg:57.07ms
step:565/2330 train_time:32244ms step_avg:57.07ms
step:566/2330 train_time:32304ms step_avg:57.07ms
step:567/2330 train_time:32359ms step_avg:57.07ms
step:568/2330 train_time:32418ms step_avg:57.07ms
step:569/2330 train_time:32474ms step_avg:57.07ms
step:570/2330 train_time:32532ms step_avg:57.07ms
step:571/2330 train_time:32587ms step_avg:57.07ms
step:572/2330 train_time:32647ms step_avg:57.08ms
step:573/2330 train_time:32703ms step_avg:57.07ms
step:574/2330 train_time:32761ms step_avg:57.08ms
step:575/2330 train_time:32817ms step_avg:57.07ms
step:576/2330 train_time:32877ms step_avg:57.08ms
step:577/2330 train_time:32932ms step_avg:57.08ms
step:578/2330 train_time:32992ms step_avg:57.08ms
step:579/2330 train_time:33048ms step_avg:57.08ms
step:580/2330 train_time:33108ms step_avg:57.08ms
step:581/2330 train_time:33164ms step_avg:57.08ms
step:582/2330 train_time:33224ms step_avg:57.09ms
step:583/2330 train_time:33279ms step_avg:57.08ms
step:584/2330 train_time:33338ms step_avg:57.09ms
step:585/2330 train_time:33394ms step_avg:57.08ms
step:586/2330 train_time:33454ms step_avg:57.09ms
step:587/2330 train_time:33510ms step_avg:57.09ms
step:588/2330 train_time:33568ms step_avg:57.09ms
step:589/2330 train_time:33624ms step_avg:57.09ms
step:590/2330 train_time:33682ms step_avg:57.09ms
step:591/2330 train_time:33738ms step_avg:57.09ms
step:592/2330 train_time:33798ms step_avg:57.09ms
step:593/2330 train_time:33854ms step_avg:57.09ms
step:594/2330 train_time:33913ms step_avg:57.09ms
step:595/2330 train_time:33968ms step_avg:57.09ms
step:596/2330 train_time:34028ms step_avg:57.09ms
step:597/2330 train_time:34084ms step_avg:57.09ms
step:598/2330 train_time:34143ms step_avg:57.10ms
step:599/2330 train_time:34199ms step_avg:57.09ms
step:600/2330 train_time:34258ms step_avg:57.10ms
step:601/2330 train_time:34314ms step_avg:57.09ms
step:602/2330 train_time:34373ms step_avg:57.10ms
step:603/2330 train_time:34429ms step_avg:57.10ms
step:604/2330 train_time:34488ms step_avg:57.10ms
step:605/2330 train_time:34543ms step_avg:57.10ms
step:606/2330 train_time:34603ms step_avg:57.10ms
step:607/2330 train_time:34659ms step_avg:57.10ms
step:608/2330 train_time:34719ms step_avg:57.10ms
step:609/2330 train_time:34775ms step_avg:57.10ms
step:610/2330 train_time:34835ms step_avg:57.11ms
step:611/2330 train_time:34890ms step_avg:57.10ms
step:612/2330 train_time:34950ms step_avg:57.11ms
step:613/2330 train_time:35005ms step_avg:57.11ms
step:614/2330 train_time:35065ms step_avg:57.11ms
step:615/2330 train_time:35121ms step_avg:57.11ms
step:616/2330 train_time:35180ms step_avg:57.11ms
step:617/2330 train_time:35236ms step_avg:57.11ms
step:618/2330 train_time:35296ms step_avg:57.11ms
step:619/2330 train_time:35352ms step_avg:57.11ms
step:620/2330 train_time:35411ms step_avg:57.11ms
step:621/2330 train_time:35466ms step_avg:57.11ms
step:622/2330 train_time:35527ms step_avg:57.12ms
step:623/2330 train_time:35582ms step_avg:57.11ms
step:624/2330 train_time:35642ms step_avg:57.12ms
step:625/2330 train_time:35699ms step_avg:57.12ms
step:626/2330 train_time:35758ms step_avg:57.12ms
step:627/2330 train_time:35814ms step_avg:57.12ms
step:628/2330 train_time:35873ms step_avg:57.12ms
step:629/2330 train_time:35928ms step_avg:57.12ms
step:630/2330 train_time:35988ms step_avg:57.12ms
step:631/2330 train_time:36043ms step_avg:57.12ms
step:632/2330 train_time:36102ms step_avg:57.12ms
step:633/2330 train_time:36158ms step_avg:57.12ms
step:634/2330 train_time:36219ms step_avg:57.13ms
step:635/2330 train_time:36274ms step_avg:57.12ms
step:636/2330 train_time:36333ms step_avg:57.13ms
step:637/2330 train_time:36389ms step_avg:57.13ms
step:638/2330 train_time:36449ms step_avg:57.13ms
step:639/2330 train_time:36504ms step_avg:57.13ms
step:640/2330 train_time:36563ms step_avg:57.13ms
step:641/2330 train_time:36619ms step_avg:57.13ms
step:642/2330 train_time:36678ms step_avg:57.13ms
step:643/2330 train_time:36733ms step_avg:57.13ms
step:644/2330 train_time:36794ms step_avg:57.13ms
step:645/2330 train_time:36849ms step_avg:57.13ms
step:646/2330 train_time:36909ms step_avg:57.13ms
step:647/2330 train_time:36964ms step_avg:57.13ms
step:648/2330 train_time:37024ms step_avg:57.14ms
step:649/2330 train_time:37080ms step_avg:57.13ms
step:650/2330 train_time:37139ms step_avg:57.14ms
step:651/2330 train_time:37195ms step_avg:57.13ms
step:652/2330 train_time:37255ms step_avg:57.14ms
step:653/2330 train_time:37310ms step_avg:57.14ms
step:654/2330 train_time:37370ms step_avg:57.14ms
step:655/2330 train_time:37425ms step_avg:57.14ms
step:656/2330 train_time:37484ms step_avg:57.14ms
step:657/2330 train_time:37540ms step_avg:57.14ms
step:658/2330 train_time:37599ms step_avg:57.14ms
step:659/2330 train_time:37656ms step_avg:57.14ms
step:660/2330 train_time:37715ms step_avg:57.14ms
step:661/2330 train_time:37770ms step_avg:57.14ms
step:662/2330 train_time:37830ms step_avg:57.15ms
step:663/2330 train_time:37885ms step_avg:57.14ms
step:664/2330 train_time:37945ms step_avg:57.15ms
step:665/2330 train_time:38000ms step_avg:57.14ms
step:666/2330 train_time:38060ms step_avg:57.15ms
step:667/2330 train_time:38115ms step_avg:57.14ms
step:668/2330 train_time:38175ms step_avg:57.15ms
step:669/2330 train_time:38230ms step_avg:57.15ms
step:670/2330 train_time:38289ms step_avg:57.15ms
step:671/2330 train_time:38345ms step_avg:57.15ms
step:672/2330 train_time:38404ms step_avg:57.15ms
step:673/2330 train_time:38460ms step_avg:57.15ms
step:674/2330 train_time:38519ms step_avg:57.15ms
step:675/2330 train_time:38575ms step_avg:57.15ms
step:676/2330 train_time:38635ms step_avg:57.15ms
step:677/2330 train_time:38690ms step_avg:57.15ms
step:678/2330 train_time:38750ms step_avg:57.15ms
step:679/2330 train_time:38805ms step_avg:57.15ms
step:680/2330 train_time:38864ms step_avg:57.15ms
step:681/2330 train_time:38920ms step_avg:57.15ms
step:682/2330 train_time:38980ms step_avg:57.15ms
step:683/2330 train_time:39035ms step_avg:57.15ms
step:684/2330 train_time:39095ms step_avg:57.16ms
step:685/2330 train_time:39151ms step_avg:57.15ms
step:686/2330 train_time:39210ms step_avg:57.16ms
step:687/2330 train_time:39265ms step_avg:57.15ms
step:688/2330 train_time:39325ms step_avg:57.16ms
step:689/2330 train_time:39380ms step_avg:57.16ms
step:690/2330 train_time:39439ms step_avg:57.16ms
step:691/2330 train_time:39495ms step_avg:57.16ms
step:692/2330 train_time:39555ms step_avg:57.16ms
step:693/2330 train_time:39610ms step_avg:57.16ms
step:694/2330 train_time:39669ms step_avg:57.16ms
step:695/2330 train_time:39725ms step_avg:57.16ms
step:696/2330 train_time:39784ms step_avg:57.16ms
step:697/2330 train_time:39840ms step_avg:57.16ms
step:698/2330 train_time:39899ms step_avg:57.16ms
step:699/2330 train_time:39955ms step_avg:57.16ms
step:700/2330 train_time:40015ms step_avg:57.16ms
step:701/2330 train_time:40070ms step_avg:57.16ms
step:702/2330 train_time:40130ms step_avg:57.17ms
step:703/2330 train_time:40186ms step_avg:57.16ms
step:704/2330 train_time:40245ms step_avg:57.17ms
step:705/2330 train_time:40302ms step_avg:57.17ms
step:706/2330 train_time:40360ms step_avg:57.17ms
step:707/2330 train_time:40416ms step_avg:57.17ms
step:708/2330 train_time:40475ms step_avg:57.17ms
step:709/2330 train_time:40531ms step_avg:57.17ms
step:710/2330 train_time:40590ms step_avg:57.17ms
step:711/2330 train_time:40646ms step_avg:57.17ms
step:712/2330 train_time:40705ms step_avg:57.17ms
step:713/2330 train_time:40761ms step_avg:57.17ms
step:714/2330 train_time:40820ms step_avg:57.17ms
step:715/2330 train_time:40875ms step_avg:57.17ms
step:716/2330 train_time:40935ms step_avg:57.17ms
step:717/2330 train_time:40990ms step_avg:57.17ms
step:718/2330 train_time:41050ms step_avg:57.17ms
step:719/2330 train_time:41105ms step_avg:57.17ms
step:720/2330 train_time:41166ms step_avg:57.17ms
step:721/2330 train_time:41221ms step_avg:57.17ms
step:722/2330 train_time:41280ms step_avg:57.17ms
step:723/2330 train_time:41336ms step_avg:57.17ms
step:724/2330 train_time:41395ms step_avg:57.18ms
step:725/2330 train_time:41451ms step_avg:57.17ms
step:726/2330 train_time:41510ms step_avg:57.18ms
step:727/2330 train_time:41565ms step_avg:57.17ms
step:728/2330 train_time:41625ms step_avg:57.18ms
step:729/2330 train_time:41681ms step_avg:57.18ms
step:730/2330 train_time:41740ms step_avg:57.18ms
step:731/2330 train_time:41796ms step_avg:57.18ms
step:732/2330 train_time:41855ms step_avg:57.18ms
step:733/2330 train_time:41911ms step_avg:57.18ms
step:734/2330 train_time:41969ms step_avg:57.18ms
step:735/2330 train_time:42025ms step_avg:57.18ms
step:736/2330 train_time:42084ms step_avg:57.18ms
step:737/2330 train_time:42140ms step_avg:57.18ms
step:738/2330 train_time:42200ms step_avg:57.18ms
step:739/2330 train_time:42256ms step_avg:57.18ms
step:740/2330 train_time:42315ms step_avg:57.18ms
step:741/2330 train_time:42370ms step_avg:57.18ms
step:742/2330 train_time:42430ms step_avg:57.18ms
step:743/2330 train_time:42485ms step_avg:57.18ms
step:744/2330 train_time:42545ms step_avg:57.18ms
step:745/2330 train_time:42601ms step_avg:57.18ms
step:746/2330 train_time:42660ms step_avg:57.18ms
step:747/2330 train_time:42716ms step_avg:57.18ms
step:748/2330 train_time:42775ms step_avg:57.19ms
step:749/2330 train_time:42830ms step_avg:57.18ms
step:750/2330 train_time:42889ms step_avg:57.19ms
step:750/2330 val_loss:5.6834 train_time:42969ms step_avg:57.29ms
step:751/2330 train_time:42988ms step_avg:57.24ms
step:752/2330 train_time:43008ms step_avg:57.19ms
step:753/2330 train_time:43065ms step_avg:57.19ms
step:754/2330 train_time:43129ms step_avg:57.20ms
step:755/2330 train_time:43185ms step_avg:57.20ms
step:756/2330 train_time:43245ms step_avg:57.20ms
step:757/2330 train_time:43301ms step_avg:57.20ms
step:758/2330 train_time:43359ms step_avg:57.20ms
step:759/2330 train_time:43414ms step_avg:57.20ms
step:760/2330 train_time:43474ms step_avg:57.20ms
step:761/2330 train_time:43529ms step_avg:57.20ms
step:762/2330 train_time:43587ms step_avg:57.20ms
step:763/2330 train_time:43642ms step_avg:57.20ms
step:764/2330 train_time:43701ms step_avg:57.20ms
step:765/2330 train_time:43758ms step_avg:57.20ms
step:766/2330 train_time:43815ms step_avg:57.20ms
step:767/2330 train_time:43872ms step_avg:57.20ms
step:768/2330 train_time:43932ms step_avg:57.20ms
step:769/2330 train_time:43988ms step_avg:57.20ms
step:770/2330 train_time:44051ms step_avg:57.21ms
step:771/2330 train_time:44107ms step_avg:57.21ms
step:772/2330 train_time:44171ms step_avg:57.22ms
step:773/2330 train_time:44228ms step_avg:57.22ms
step:774/2330 train_time:44288ms step_avg:57.22ms
step:775/2330 train_time:44344ms step_avg:57.22ms
step:776/2330 train_time:44404ms step_avg:57.22ms
step:777/2330 train_time:44460ms step_avg:57.22ms
step:778/2330 train_time:44520ms step_avg:57.22ms
step:779/2330 train_time:44577ms step_avg:57.22ms
step:780/2330 train_time:44636ms step_avg:57.23ms
step:781/2330 train_time:44692ms step_avg:57.22ms
step:782/2330 train_time:44751ms step_avg:57.23ms
step:783/2330 train_time:44807ms step_avg:57.22ms
step:784/2330 train_time:44866ms step_avg:57.23ms
step:785/2330 train_time:44923ms step_avg:57.23ms
step:786/2330 train_time:44982ms step_avg:57.23ms
step:787/2330 train_time:45039ms step_avg:57.23ms
step:788/2330 train_time:45101ms step_avg:57.23ms
step:789/2330 train_time:45157ms step_avg:57.23ms
step:790/2330 train_time:45219ms step_avg:57.24ms
step:791/2330 train_time:45276ms step_avg:57.24ms
step:792/2330 train_time:45336ms step_avg:57.24ms
step:793/2330 train_time:45393ms step_avg:57.24ms
step:794/2330 train_time:45452ms step_avg:57.24ms
step:795/2330 train_time:45508ms step_avg:57.24ms
step:796/2330 train_time:45568ms step_avg:57.25ms
step:797/2330 train_time:45624ms step_avg:57.24ms
step:798/2330 train_time:45683ms step_avg:57.25ms
step:799/2330 train_time:45739ms step_avg:57.25ms
step:800/2330 train_time:45798ms step_avg:57.25ms
step:801/2330 train_time:45855ms step_avg:57.25ms
step:802/2330 train_time:45914ms step_avg:57.25ms
step:803/2330 train_time:45971ms step_avg:57.25ms
step:804/2330 train_time:46030ms step_avg:57.25ms
step:805/2330 train_time:46087ms step_avg:57.25ms
step:806/2330 train_time:46147ms step_avg:57.25ms
step:807/2330 train_time:46203ms step_avg:57.25ms
step:808/2330 train_time:46263ms step_avg:57.26ms
step:809/2330 train_time:46320ms step_avg:57.26ms
step:810/2330 train_time:46380ms step_avg:57.26ms
step:811/2330 train_time:46438ms step_avg:57.26ms
step:812/2330 train_time:46497ms step_avg:57.26ms
step:813/2330 train_time:46553ms step_avg:57.26ms
step:814/2330 train_time:46614ms step_avg:57.27ms
step:815/2330 train_time:46670ms step_avg:57.26ms
step:816/2330 train_time:46730ms step_avg:57.27ms
step:817/2330 train_time:46786ms step_avg:57.27ms
step:818/2330 train_time:46846ms step_avg:57.27ms
step:819/2330 train_time:46902ms step_avg:57.27ms
step:820/2330 train_time:46962ms step_avg:57.27ms
step:821/2330 train_time:47018ms step_avg:57.27ms
step:822/2330 train_time:47078ms step_avg:57.27ms
step:823/2330 train_time:47134ms step_avg:57.27ms
step:824/2330 train_time:47195ms step_avg:57.28ms
step:825/2330 train_time:47251ms step_avg:57.27ms
step:826/2330 train_time:47312ms step_avg:57.28ms
step:827/2330 train_time:47368ms step_avg:57.28ms
step:828/2330 train_time:47429ms step_avg:57.28ms
step:829/2330 train_time:47485ms step_avg:57.28ms
step:830/2330 train_time:47545ms step_avg:57.28ms
step:831/2330 train_time:47602ms step_avg:57.28ms
step:832/2330 train_time:47661ms step_avg:57.28ms
step:833/2330 train_time:47717ms step_avg:57.28ms
step:834/2330 train_time:47776ms step_avg:57.29ms
step:835/2330 train_time:47833ms step_avg:57.28ms
step:836/2330 train_time:47893ms step_avg:57.29ms
step:837/2330 train_time:47949ms step_avg:57.29ms
step:838/2330 train_time:48009ms step_avg:57.29ms
step:839/2330 train_time:48066ms step_avg:57.29ms
step:840/2330 train_time:48126ms step_avg:57.29ms
step:841/2330 train_time:48182ms step_avg:57.29ms
step:842/2330 train_time:48242ms step_avg:57.29ms
step:843/2330 train_time:48299ms step_avg:57.29ms
step:844/2330 train_time:48360ms step_avg:57.30ms
step:845/2330 train_time:48416ms step_avg:57.30ms
step:846/2330 train_time:48476ms step_avg:57.30ms
step:847/2330 train_time:48533ms step_avg:57.30ms
step:848/2330 train_time:48592ms step_avg:57.30ms
step:849/2330 train_time:48648ms step_avg:57.30ms
step:850/2330 train_time:48708ms step_avg:57.30ms
step:851/2330 train_time:48763ms step_avg:57.30ms
step:852/2330 train_time:48824ms step_avg:57.31ms
step:853/2330 train_time:48880ms step_avg:57.30ms
step:854/2330 train_time:48940ms step_avg:57.31ms
step:855/2330 train_time:48997ms step_avg:57.31ms
step:856/2330 train_time:49057ms step_avg:57.31ms
step:857/2330 train_time:49113ms step_avg:57.31ms
step:858/2330 train_time:49174ms step_avg:57.31ms
step:859/2330 train_time:49231ms step_avg:57.31ms
step:860/2330 train_time:49291ms step_avg:57.32ms
step:861/2330 train_time:49348ms step_avg:57.31ms
step:862/2330 train_time:49408ms step_avg:57.32ms
step:863/2330 train_time:49464ms step_avg:57.32ms
step:864/2330 train_time:49524ms step_avg:57.32ms
step:865/2330 train_time:49581ms step_avg:57.32ms
step:866/2330 train_time:49640ms step_avg:57.32ms
step:867/2330 train_time:49696ms step_avg:57.32ms
step:868/2330 train_time:49756ms step_avg:57.32ms
step:869/2330 train_time:49813ms step_avg:57.32ms
step:870/2330 train_time:49873ms step_avg:57.33ms
step:871/2330 train_time:49929ms step_avg:57.32ms
step:872/2330 train_time:49989ms step_avg:57.33ms
step:873/2330 train_time:50045ms step_avg:57.33ms
step:874/2330 train_time:50105ms step_avg:57.33ms
step:875/2330 train_time:50162ms step_avg:57.33ms
step:876/2330 train_time:50221ms step_avg:57.33ms
step:877/2330 train_time:50278ms step_avg:57.33ms
step:878/2330 train_time:50338ms step_avg:57.33ms
step:879/2330 train_time:50395ms step_avg:57.33ms
step:880/2330 train_time:50455ms step_avg:57.33ms
step:881/2330 train_time:50511ms step_avg:57.33ms
step:882/2330 train_time:50572ms step_avg:57.34ms
step:883/2330 train_time:50628ms step_avg:57.34ms
step:884/2330 train_time:50688ms step_avg:57.34ms
step:885/2330 train_time:50745ms step_avg:57.34ms
step:886/2330 train_time:50804ms step_avg:57.34ms
step:887/2330 train_time:50861ms step_avg:57.34ms
step:888/2330 train_time:50920ms step_avg:57.34ms
step:889/2330 train_time:50977ms step_avg:57.34ms
step:890/2330 train_time:51037ms step_avg:57.34ms
step:891/2330 train_time:51094ms step_avg:57.34ms
step:892/2330 train_time:51154ms step_avg:57.35ms
step:893/2330 train_time:51210ms step_avg:57.35ms
step:894/2330 train_time:51270ms step_avg:57.35ms
step:895/2330 train_time:51326ms step_avg:57.35ms
step:896/2330 train_time:51386ms step_avg:57.35ms
step:897/2330 train_time:51442ms step_avg:57.35ms
step:898/2330 train_time:51502ms step_avg:57.35ms
step:899/2330 train_time:51558ms step_avg:57.35ms
step:900/2330 train_time:51618ms step_avg:57.35ms
step:901/2330 train_time:51674ms step_avg:57.35ms
step:902/2330 train_time:51734ms step_avg:57.35ms
step:903/2330 train_time:51791ms step_avg:57.35ms
step:904/2330 train_time:51851ms step_avg:57.36ms
step:905/2330 train_time:51907ms step_avg:57.36ms
step:906/2330 train_time:51967ms step_avg:57.36ms
step:907/2330 train_time:52023ms step_avg:57.36ms
step:908/2330 train_time:52082ms step_avg:57.36ms
step:909/2330 train_time:52139ms step_avg:57.36ms
step:910/2330 train_time:52199ms step_avg:57.36ms
step:911/2330 train_time:52255ms step_avg:57.36ms
step:912/2330 train_time:52315ms step_avg:57.36ms
step:913/2330 train_time:52372ms step_avg:57.36ms
step:914/2330 train_time:52432ms step_avg:57.37ms
step:915/2330 train_time:52488ms step_avg:57.36ms
step:916/2330 train_time:52549ms step_avg:57.37ms
step:917/2330 train_time:52604ms step_avg:57.37ms
step:918/2330 train_time:52665ms step_avg:57.37ms
step:919/2330 train_time:52721ms step_avg:57.37ms
step:920/2330 train_time:52781ms step_avg:57.37ms
step:921/2330 train_time:52838ms step_avg:57.37ms
step:922/2330 train_time:52898ms step_avg:57.37ms
step:923/2330 train_time:52954ms step_avg:57.37ms
step:924/2330 train_time:53013ms step_avg:57.37ms
step:925/2330 train_time:53069ms step_avg:57.37ms
step:926/2330 train_time:53129ms step_avg:57.37ms
step:927/2330 train_time:53185ms step_avg:57.37ms
step:928/2330 train_time:53245ms step_avg:57.38ms
step:929/2330 train_time:53301ms step_avg:57.37ms
step:930/2330 train_time:53361ms step_avg:57.38ms
step:931/2330 train_time:53418ms step_avg:57.38ms
step:932/2330 train_time:53478ms step_avg:57.38ms
step:933/2330 train_time:53535ms step_avg:57.38ms
step:934/2330 train_time:53594ms step_avg:57.38ms
step:935/2330 train_time:53650ms step_avg:57.38ms
step:936/2330 train_time:53710ms step_avg:57.38ms
step:937/2330 train_time:53766ms step_avg:57.38ms
step:938/2330 train_time:53826ms step_avg:57.38ms
step:939/2330 train_time:53882ms step_avg:57.38ms
step:940/2330 train_time:53942ms step_avg:57.38ms
step:941/2330 train_time:53999ms step_avg:57.38ms
step:942/2330 train_time:54057ms step_avg:57.39ms
step:943/2330 train_time:54114ms step_avg:57.38ms
step:944/2330 train_time:54174ms step_avg:57.39ms
step:945/2330 train_time:54230ms step_avg:57.39ms
step:946/2330 train_time:54291ms step_avg:57.39ms
step:947/2330 train_time:54348ms step_avg:57.39ms
step:948/2330 train_time:54407ms step_avg:57.39ms
step:949/2330 train_time:54463ms step_avg:57.39ms
step:950/2330 train_time:54523ms step_avg:57.39ms
step:951/2330 train_time:54580ms step_avg:57.39ms
step:952/2330 train_time:54640ms step_avg:57.39ms
step:953/2330 train_time:54697ms step_avg:57.39ms
step:954/2330 train_time:54758ms step_avg:57.40ms
step:955/2330 train_time:54813ms step_avg:57.40ms
step:956/2330 train_time:54874ms step_avg:57.40ms
step:957/2330 train_time:54930ms step_avg:57.40ms
step:958/2330 train_time:54991ms step_avg:57.40ms
step:959/2330 train_time:55048ms step_avg:57.40ms
step:960/2330 train_time:55107ms step_avg:57.40ms
step:961/2330 train_time:55163ms step_avg:57.40ms
step:962/2330 train_time:55223ms step_avg:57.40ms
step:963/2330 train_time:55279ms step_avg:57.40ms
step:964/2330 train_time:55339ms step_avg:57.41ms
step:965/2330 train_time:55395ms step_avg:57.40ms
step:966/2330 train_time:55455ms step_avg:57.41ms
step:967/2330 train_time:55511ms step_avg:57.41ms
step:968/2330 train_time:55572ms step_avg:57.41ms
step:969/2330 train_time:55628ms step_avg:57.41ms
step:970/2330 train_time:55689ms step_avg:57.41ms
step:971/2330 train_time:55745ms step_avg:57.41ms
step:972/2330 train_time:55806ms step_avg:57.41ms
step:973/2330 train_time:55862ms step_avg:57.41ms
step:974/2330 train_time:55922ms step_avg:57.41ms
step:975/2330 train_time:55978ms step_avg:57.41ms
step:976/2330 train_time:56038ms step_avg:57.42ms
step:977/2330 train_time:56094ms step_avg:57.41ms
step:978/2330 train_time:56155ms step_avg:57.42ms
step:979/2330 train_time:56211ms step_avg:57.42ms
step:980/2330 train_time:56271ms step_avg:57.42ms
step:981/2330 train_time:56327ms step_avg:57.42ms
step:982/2330 train_time:56387ms step_avg:57.42ms
step:983/2330 train_time:56444ms step_avg:57.42ms
step:984/2330 train_time:56503ms step_avg:57.42ms
step:985/2330 train_time:56560ms step_avg:57.42ms
step:986/2330 train_time:56620ms step_avg:57.42ms
step:987/2330 train_time:56677ms step_avg:57.42ms
step:988/2330 train_time:56737ms step_avg:57.43ms
step:989/2330 train_time:56794ms step_avg:57.43ms
step:990/2330 train_time:56853ms step_avg:57.43ms
step:991/2330 train_time:56910ms step_avg:57.43ms
step:992/2330 train_time:56970ms step_avg:57.43ms
step:993/2330 train_time:57026ms step_avg:57.43ms
step:994/2330 train_time:57086ms step_avg:57.43ms
step:995/2330 train_time:57142ms step_avg:57.43ms
step:996/2330 train_time:57202ms step_avg:57.43ms
step:997/2330 train_time:57259ms step_avg:57.43ms
step:998/2330 train_time:57319ms step_avg:57.43ms
step:999/2330 train_time:57376ms step_avg:57.43ms
step:1000/2330 train_time:57436ms step_avg:57.44ms
step:1000/2330 val_loss:5.6354 train_time:57516ms step_avg:57.52ms
step:1001/2330 train_time:57538ms step_avg:57.48ms
step:1002/2330 train_time:57559ms step_avg:57.44ms
step:1003/2330 train_time:57614ms step_avg:57.44ms
step:1004/2330 train_time:57677ms step_avg:57.45ms
step:1005/2330 train_time:57733ms step_avg:57.45ms
step:1006/2330 train_time:57794ms step_avg:57.45ms
step:1007/2330 train_time:57850ms step_avg:57.45ms
step:1008/2330 train_time:57909ms step_avg:57.45ms
step:1009/2330 train_time:57965ms step_avg:57.45ms
step:1010/2330 train_time:58024ms step_avg:57.45ms
step:1011/2330 train_time:58080ms step_avg:57.45ms
step:1012/2330 train_time:58139ms step_avg:57.45ms
step:1013/2330 train_time:58194ms step_avg:57.45ms
step:1014/2330 train_time:58254ms step_avg:57.45ms
step:1015/2330 train_time:58309ms step_avg:57.45ms
step:1016/2330 train_time:58368ms step_avg:57.45ms
step:1017/2330 train_time:58425ms step_avg:57.45ms
step:1018/2330 train_time:58487ms step_avg:57.45ms
step:1019/2330 train_time:58543ms step_avg:57.45ms
step:1020/2330 train_time:58606ms step_avg:57.46ms
step:1021/2330 train_time:58663ms step_avg:57.46ms
step:1022/2330 train_time:58724ms step_avg:57.46ms
step:1023/2330 train_time:58782ms step_avg:57.46ms
step:1024/2330 train_time:58842ms step_avg:57.46ms
step:1025/2330 train_time:58898ms step_avg:57.46ms
step:1026/2330 train_time:58958ms step_avg:57.46ms
step:1027/2330 train_time:59014ms step_avg:57.46ms
step:1028/2330 train_time:59074ms step_avg:57.46ms
step:1029/2330 train_time:59130ms step_avg:57.46ms
step:1030/2330 train_time:59188ms step_avg:57.46ms
step:1031/2330 train_time:59245ms step_avg:57.46ms
step:1032/2330 train_time:59303ms step_avg:57.46ms
step:1033/2330 train_time:59360ms step_avg:57.46ms
step:1034/2330 train_time:59420ms step_avg:57.47ms
step:1035/2330 train_time:59476ms step_avg:57.46ms
step:1036/2330 train_time:59538ms step_avg:57.47ms
step:1037/2330 train_time:59594ms step_avg:57.47ms
step:1038/2330 train_time:59656ms step_avg:57.47ms
step:1039/2330 train_time:59713ms step_avg:57.47ms
step:1040/2330 train_time:59774ms step_avg:57.47ms
step:1041/2330 train_time:59830ms step_avg:57.47ms
step:1042/2330 train_time:59889ms step_avg:57.48ms
step:1043/2330 train_time:59946ms step_avg:57.47ms
step:1044/2330 train_time:60005ms step_avg:57.48ms
step:1045/2330 train_time:60062ms step_avg:57.48ms
step:1046/2330 train_time:60122ms step_avg:57.48ms
step:1047/2330 train_time:60177ms step_avg:57.48ms
step:1048/2330 train_time:60238ms step_avg:57.48ms
step:1049/2330 train_time:60294ms step_avg:57.48ms
step:1050/2330 train_time:60353ms step_avg:57.48ms
step:1051/2330 train_time:60409ms step_avg:57.48ms
step:1052/2330 train_time:60470ms step_avg:57.48ms
step:1053/2330 train_time:60526ms step_avg:57.48ms
step:1054/2330 train_time:60588ms step_avg:57.48ms
step:1055/2330 train_time:60643ms step_avg:57.48ms
step:1056/2330 train_time:60705ms step_avg:57.49ms
step:1057/2330 train_time:60762ms step_avg:57.49ms
step:1058/2330 train_time:60822ms step_avg:57.49ms
step:1059/2330 train_time:60879ms step_avg:57.49ms
step:1060/2330 train_time:60940ms step_avg:57.49ms
step:1061/2330 train_time:60996ms step_avg:57.49ms
step:1062/2330 train_time:61057ms step_avg:57.49ms
step:1063/2330 train_time:61113ms step_avg:57.49ms
step:1064/2330 train_time:61173ms step_avg:57.49ms
step:1065/2330 train_time:61228ms step_avg:57.49ms
step:1066/2330 train_time:61288ms step_avg:57.49ms
step:1067/2330 train_time:61345ms step_avg:57.49ms
step:1068/2330 train_time:61405ms step_avg:57.50ms
step:1069/2330 train_time:61461ms step_avg:57.49ms
step:1070/2330 train_time:61522ms step_avg:57.50ms
step:1071/2330 train_time:61578ms step_avg:57.50ms
step:1072/2330 train_time:61639ms step_avg:57.50ms
step:1073/2330 train_time:61694ms step_avg:57.50ms
step:1074/2330 train_time:61756ms step_avg:57.50ms
step:1075/2330 train_time:61812ms step_avg:57.50ms
step:1076/2330 train_time:61873ms step_avg:57.50ms
step:1077/2330 train_time:61930ms step_avg:57.50ms
step:1078/2330 train_time:61988ms step_avg:57.50ms
step:1079/2330 train_time:62045ms step_avg:57.50ms
step:1080/2330 train_time:62105ms step_avg:57.50ms
step:1081/2330 train_time:62161ms step_avg:57.50ms
step:1082/2330 train_time:62222ms step_avg:57.51ms
step:1083/2330 train_time:62277ms step_avg:57.50ms
step:1084/2330 train_time:62338ms step_avg:57.51ms
step:1085/2330 train_time:62394ms step_avg:57.51ms
step:1086/2330 train_time:62455ms step_avg:57.51ms
step:1087/2330 train_time:62511ms step_avg:57.51ms
step:1088/2330 train_time:62571ms step_avg:57.51ms
step:1089/2330 train_time:62628ms step_avg:57.51ms
step:1090/2330 train_time:62687ms step_avg:57.51ms
step:1091/2330 train_time:62744ms step_avg:57.51ms
step:1092/2330 train_time:62805ms step_avg:57.51ms
step:1093/2330 train_time:62861ms step_avg:57.51ms
step:1094/2330 train_time:62922ms step_avg:57.52ms
step:1095/2330 train_time:62978ms step_avg:57.51ms
step:1096/2330 train_time:63038ms step_avg:57.52ms
step:1097/2330 train_time:63094ms step_avg:57.52ms
step:1098/2330 train_time:63155ms step_avg:57.52ms
step:1099/2330 train_time:63211ms step_avg:57.52ms
step:1100/2330 train_time:63270ms step_avg:57.52ms
step:1101/2330 train_time:63327ms step_avg:57.52ms
step:1102/2330 train_time:63386ms step_avg:57.52ms
step:1103/2330 train_time:63442ms step_avg:57.52ms
step:1104/2330 train_time:63502ms step_avg:57.52ms
step:1105/2330 train_time:63558ms step_avg:57.52ms
step:1106/2330 train_time:63619ms step_avg:57.52ms
step:1107/2330 train_time:63675ms step_avg:57.52ms
step:1108/2330 train_time:63736ms step_avg:57.52ms
step:1109/2330 train_time:63792ms step_avg:57.52ms
step:1110/2330 train_time:63853ms step_avg:57.53ms
step:1111/2330 train_time:63909ms step_avg:57.52ms
step:1112/2330 train_time:63968ms step_avg:57.53ms
step:1113/2330 train_time:64025ms step_avg:57.52ms
step:1114/2330 train_time:64085ms step_avg:57.53ms
step:1115/2330 train_time:64141ms step_avg:57.53ms
step:1116/2330 train_time:64201ms step_avg:57.53ms
step:1117/2330 train_time:64257ms step_avg:57.53ms
step:1118/2330 train_time:64317ms step_avg:57.53ms
step:1119/2330 train_time:64373ms step_avg:57.53ms
step:1120/2330 train_time:64433ms step_avg:57.53ms
step:1121/2330 train_time:64490ms step_avg:57.53ms
step:1122/2330 train_time:64550ms step_avg:57.53ms
step:1123/2330 train_time:64605ms step_avg:57.53ms
step:1124/2330 train_time:64667ms step_avg:57.53ms
step:1125/2330 train_time:64724ms step_avg:57.53ms
step:1126/2330 train_time:64785ms step_avg:57.54ms
step:1127/2330 train_time:64841ms step_avg:57.53ms
step:1128/2330 train_time:64902ms step_avg:57.54ms
step:1129/2330 train_time:64959ms step_avg:57.54ms
step:1130/2330 train_time:65019ms step_avg:57.54ms
step:1131/2330 train_time:65075ms step_avg:57.54ms
step:1132/2330 train_time:65136ms step_avg:57.54ms
step:1133/2330 train_time:65192ms step_avg:57.54ms
step:1134/2330 train_time:65252ms step_avg:57.54ms
step:1135/2330 train_time:65308ms step_avg:57.54ms
step:1136/2330 train_time:65368ms step_avg:57.54ms
step:1137/2330 train_time:65424ms step_avg:57.54ms
step:1138/2330 train_time:65484ms step_avg:57.54ms
step:1139/2330 train_time:65540ms step_avg:57.54ms
step:1140/2330 train_time:65601ms step_avg:57.54ms
step:1141/2330 train_time:65657ms step_avg:57.54ms
step:1142/2330 train_time:65719ms step_avg:57.55ms
step:1143/2330 train_time:65774ms step_avg:57.55ms
step:1144/2330 train_time:65836ms step_avg:57.55ms
step:1145/2330 train_time:65892ms step_avg:57.55ms
step:1146/2330 train_time:65952ms step_avg:57.55ms
step:1147/2330 train_time:66008ms step_avg:57.55ms
step:1148/2330 train_time:66068ms step_avg:57.55ms
step:1149/2330 train_time:66125ms step_avg:57.55ms
step:1150/2330 train_time:66185ms step_avg:57.55ms
step:1151/2330 train_time:66241ms step_avg:57.55ms
step:1152/2330 train_time:66301ms step_avg:57.55ms
step:1153/2330 train_time:66357ms step_avg:57.55ms
step:1154/2330 train_time:66416ms step_avg:57.55ms
step:1155/2330 train_time:66473ms step_avg:57.55ms
step:1156/2330 train_time:66533ms step_avg:57.55ms
step:1157/2330 train_time:66589ms step_avg:57.55ms
step:1158/2330 train_time:66649ms step_avg:57.56ms
step:1159/2330 train_time:66705ms step_avg:57.55ms
step:1160/2330 train_time:66766ms step_avg:57.56ms
step:1161/2330 train_time:66822ms step_avg:57.56ms
step:1162/2330 train_time:66883ms step_avg:57.56ms
step:1163/2330 train_time:66939ms step_avg:57.56ms
step:1164/2330 train_time:67000ms step_avg:57.56ms
step:1165/2330 train_time:67055ms step_avg:57.56ms
step:1166/2330 train_time:67117ms step_avg:57.56ms
step:1167/2330 train_time:67173ms step_avg:57.56ms
step:1168/2330 train_time:67233ms step_avg:57.56ms
step:1169/2330 train_time:67289ms step_avg:57.56ms
step:1170/2330 train_time:67349ms step_avg:57.56ms
step:1171/2330 train_time:67405ms step_avg:57.56ms
step:1172/2330 train_time:67465ms step_avg:57.56ms
step:1173/2330 train_time:67522ms step_avg:57.56ms
step:1174/2330 train_time:67583ms step_avg:57.57ms
step:1175/2330 train_time:67638ms step_avg:57.56ms
step:1176/2330 train_time:67699ms step_avg:57.57ms
step:1177/2330 train_time:67756ms step_avg:57.57ms
step:1178/2330 train_time:67817ms step_avg:57.57ms
step:1179/2330 train_time:67873ms step_avg:57.57ms
step:1180/2330 train_time:67934ms step_avg:57.57ms
step:1181/2330 train_time:67990ms step_avg:57.57ms
step:1182/2330 train_time:68051ms step_avg:57.57ms
step:1183/2330 train_time:68107ms step_avg:57.57ms
step:1184/2330 train_time:68168ms step_avg:57.57ms
step:1185/2330 train_time:68224ms step_avg:57.57ms
step:1186/2330 train_time:68285ms step_avg:57.58ms
step:1187/2330 train_time:68340ms step_avg:57.57ms
step:1188/2330 train_time:68401ms step_avg:57.58ms
step:1189/2330 train_time:68457ms step_avg:57.58ms
step:1190/2330 train_time:68517ms step_avg:57.58ms
step:1191/2330 train_time:68573ms step_avg:57.58ms
step:1192/2330 train_time:68633ms step_avg:57.58ms
step:1193/2330 train_time:68690ms step_avg:57.58ms
step:1194/2330 train_time:68749ms step_avg:57.58ms
step:1195/2330 train_time:68805ms step_avg:57.58ms
step:1196/2330 train_time:68866ms step_avg:57.58ms
step:1197/2330 train_time:68923ms step_avg:57.58ms
step:1198/2330 train_time:68984ms step_avg:57.58ms
step:1199/2330 train_time:69040ms step_avg:57.58ms
step:1200/2330 train_time:69101ms step_avg:57.58ms
step:1201/2330 train_time:69157ms step_avg:57.58ms
step:1202/2330 train_time:69219ms step_avg:57.59ms
step:1203/2330 train_time:69275ms step_avg:57.58ms
step:1204/2330 train_time:69335ms step_avg:57.59ms
step:1205/2330 train_time:69391ms step_avg:57.59ms
step:1206/2330 train_time:69452ms step_avg:57.59ms
step:1207/2330 train_time:69508ms step_avg:57.59ms
step:1208/2330 train_time:69568ms step_avg:57.59ms
step:1209/2330 train_time:69625ms step_avg:57.59ms
step:1210/2330 train_time:69685ms step_avg:57.59ms
step:1211/2330 train_time:69741ms step_avg:57.59ms
step:1212/2330 train_time:69802ms step_avg:57.59ms
step:1213/2330 train_time:69859ms step_avg:57.59ms
step:1214/2330 train_time:69920ms step_avg:57.59ms
step:1215/2330 train_time:69976ms step_avg:57.59ms
step:1216/2330 train_time:70037ms step_avg:57.60ms
step:1217/2330 train_time:70093ms step_avg:57.59ms
step:1218/2330 train_time:70153ms step_avg:57.60ms
step:1219/2330 train_time:70209ms step_avg:57.60ms
step:1220/2330 train_time:70269ms step_avg:57.60ms
step:1221/2330 train_time:70325ms step_avg:57.60ms
step:1222/2330 train_time:70386ms step_avg:57.60ms
step:1223/2330 train_time:70441ms step_avg:57.60ms
step:1224/2330 train_time:70502ms step_avg:57.60ms
step:1225/2330 train_time:70558ms step_avg:57.60ms
step:1226/2330 train_time:70620ms step_avg:57.60ms
step:1227/2330 train_time:70676ms step_avg:57.60ms
step:1228/2330 train_time:70736ms step_avg:57.60ms
step:1229/2330 train_time:70791ms step_avg:57.60ms
step:1230/2330 train_time:70853ms step_avg:57.60ms
step:1231/2330 train_time:70909ms step_avg:57.60ms
step:1232/2330 train_time:70969ms step_avg:57.61ms
step:1233/2330 train_time:71026ms step_avg:57.60ms
step:1234/2330 train_time:71085ms step_avg:57.61ms
step:1235/2330 train_time:71141ms step_avg:57.60ms
step:1236/2330 train_time:71202ms step_avg:57.61ms
step:1237/2330 train_time:71257ms step_avg:57.60ms
step:1238/2330 train_time:71320ms step_avg:57.61ms
step:1239/2330 train_time:71376ms step_avg:57.61ms
step:1240/2330 train_time:71437ms step_avg:57.61ms
step:1241/2330 train_time:71492ms step_avg:57.61ms
step:1242/2330 train_time:71553ms step_avg:57.61ms
step:1243/2330 train_time:71609ms step_avg:57.61ms
step:1244/2330 train_time:71669ms step_avg:57.61ms
step:1245/2330 train_time:71726ms step_avg:57.61ms
step:1246/2330 train_time:71786ms step_avg:57.61ms
step:1247/2330 train_time:71842ms step_avg:57.61ms
step:1248/2330 train_time:71903ms step_avg:57.61ms
step:1249/2330 train_time:71959ms step_avg:57.61ms
step:1250/2330 train_time:72020ms step_avg:57.62ms
step:1250/2330 val_loss:5.8541 train_time:72100ms step_avg:57.68ms
step:1251/2330 train_time:72121ms step_avg:57.65ms
step:1252/2330 train_time:72143ms step_avg:57.62ms
step:1253/2330 train_time:72197ms step_avg:57.62ms
step:1254/2330 train_time:72259ms step_avg:57.62ms
step:1255/2330 train_time:72315ms step_avg:57.62ms
step:1256/2330 train_time:72379ms step_avg:57.63ms
step:1257/2330 train_time:72434ms step_avg:57.62ms
step:1258/2330 train_time:72495ms step_avg:57.63ms
step:1259/2330 train_time:72551ms step_avg:57.63ms
step:1260/2330 train_time:72610ms step_avg:57.63ms
step:1261/2330 train_time:72666ms step_avg:57.63ms
step:1262/2330 train_time:72726ms step_avg:57.63ms
step:1263/2330 train_time:72782ms step_avg:57.63ms
step:1264/2330 train_time:72841ms step_avg:57.63ms
step:1265/2330 train_time:72897ms step_avg:57.63ms
step:1266/2330 train_time:72956ms step_avg:57.63ms
step:1267/2330 train_time:73012ms step_avg:57.63ms
step:1268/2330 train_time:73071ms step_avg:57.63ms
step:1269/2330 train_time:73129ms step_avg:57.63ms
step:1270/2330 train_time:73190ms step_avg:57.63ms
step:1271/2330 train_time:73247ms step_avg:57.63ms
step:1272/2330 train_time:73308ms step_avg:57.63ms
step:1273/2330 train_time:73364ms step_avg:57.63ms
step:1274/2330 train_time:73425ms step_avg:57.63ms
step:1275/2330 train_time:73481ms step_avg:57.63ms
step:1276/2330 train_time:73543ms step_avg:57.64ms
step:1277/2330 train_time:73598ms step_avg:57.63ms
step:1278/2330 train_time:73658ms step_avg:57.64ms
step:1279/2330 train_time:73714ms step_avg:57.63ms
step:1280/2330 train_time:73774ms step_avg:57.64ms
step:1281/2330 train_time:73830ms step_avg:57.63ms
step:1282/2330 train_time:73890ms step_avg:57.64ms
step:1283/2330 train_time:73946ms step_avg:57.64ms
step:1284/2330 train_time:74006ms step_avg:57.64ms
step:1285/2330 train_time:74062ms step_avg:57.64ms
step:1286/2330 train_time:74122ms step_avg:57.64ms
step:1287/2330 train_time:74179ms step_avg:57.64ms
step:1288/2330 train_time:74240ms step_avg:57.64ms
step:1289/2330 train_time:74296ms step_avg:57.64ms
step:1290/2330 train_time:74356ms step_avg:57.64ms
step:1291/2330 train_time:74413ms step_avg:57.64ms
step:1292/2330 train_time:74473ms step_avg:57.64ms
step:1293/2330 train_time:74529ms step_avg:57.64ms
step:1294/2330 train_time:74589ms step_avg:57.64ms
step:1295/2330 train_time:74646ms step_avg:57.64ms
step:1296/2330 train_time:74706ms step_avg:57.64ms
step:1297/2330 train_time:74761ms step_avg:57.64ms
step:1298/2330 train_time:74820ms step_avg:57.64ms
step:1299/2330 train_time:74876ms step_avg:57.64ms
step:1300/2330 train_time:74936ms step_avg:57.64ms
step:1301/2330 train_time:74992ms step_avg:57.64ms
step:1302/2330 train_time:75051ms step_avg:57.64ms
step:1303/2330 train_time:75108ms step_avg:57.64ms
step:1304/2330 train_time:75169ms step_avg:57.64ms
step:1305/2330 train_time:75225ms step_avg:57.64ms
step:1306/2330 train_time:75286ms step_avg:57.65ms
step:1307/2330 train_time:75343ms step_avg:57.65ms
step:1308/2330 train_time:75405ms step_avg:57.65ms
step:1309/2330 train_time:75461ms step_avg:57.65ms
step:1310/2330 train_time:75522ms step_avg:57.65ms
step:1311/2330 train_time:75578ms step_avg:57.65ms
step:1312/2330 train_time:75638ms step_avg:57.65ms
step:1313/2330 train_time:75693ms step_avg:57.65ms
step:1314/2330 train_time:75753ms step_avg:57.65ms
step:1315/2330 train_time:75810ms step_avg:57.65ms
step:1316/2330 train_time:75869ms step_avg:57.65ms
step:1317/2330 train_time:75925ms step_avg:57.65ms
step:1318/2330 train_time:75985ms step_avg:57.65ms
step:1319/2330 train_time:76042ms step_avg:57.65ms
step:1320/2330 train_time:76102ms step_avg:57.65ms
step:1321/2330 train_time:76158ms step_avg:57.65ms
step:1322/2330 train_time:76218ms step_avg:57.65ms
step:1323/2330 train_time:76274ms step_avg:57.65ms
step:1324/2330 train_time:76334ms step_avg:57.65ms
step:1325/2330 train_time:76391ms step_avg:57.65ms
step:1326/2330 train_time:76451ms step_avg:57.66ms
step:1327/2330 train_time:76508ms step_avg:57.65ms
step:1328/2330 train_time:76568ms step_avg:57.66ms
step:1329/2330 train_time:76625ms step_avg:57.66ms
step:1330/2330 train_time:76685ms step_avg:57.66ms
step:1331/2330 train_time:76741ms step_avg:57.66ms
step:1332/2330 train_time:76801ms step_avg:57.66ms
step:1333/2330 train_time:76858ms step_avg:57.66ms
step:1334/2330 train_time:76917ms step_avg:57.66ms
step:1335/2330 train_time:76973ms step_avg:57.66ms
step:1336/2330 train_time:77033ms step_avg:57.66ms
step:1337/2330 train_time:77090ms step_avg:57.66ms
step:1338/2330 train_time:77149ms step_avg:57.66ms
step:1339/2330 train_time:77205ms step_avg:57.66ms
step:1340/2330 train_time:77265ms step_avg:57.66ms
step:1341/2330 train_time:77322ms step_avg:57.66ms
step:1342/2330 train_time:77382ms step_avg:57.66ms
step:1343/2330 train_time:77438ms step_avg:57.66ms
step:1344/2330 train_time:77500ms step_avg:57.66ms
step:1345/2330 train_time:77555ms step_avg:57.66ms
step:1346/2330 train_time:77616ms step_avg:57.66ms
step:1347/2330 train_time:77672ms step_avg:57.66ms
step:1348/2330 train_time:77732ms step_avg:57.66ms
step:1349/2330 train_time:77790ms step_avg:57.66ms
step:1350/2330 train_time:77849ms step_avg:57.67ms
step:1351/2330 train_time:77905ms step_avg:57.66ms
step:1352/2330 train_time:77965ms step_avg:57.67ms
step:1353/2330 train_time:78021ms step_avg:57.66ms
step:1354/2330 train_time:78082ms step_avg:57.67ms
step:1355/2330 train_time:78138ms step_avg:57.67ms
step:1356/2330 train_time:78198ms step_avg:57.67ms
step:1357/2330 train_time:78255ms step_avg:57.67ms
step:1358/2330 train_time:78315ms step_avg:57.67ms
step:1359/2330 train_time:78372ms step_avg:57.67ms
step:1360/2330 train_time:78432ms step_avg:57.67ms
step:1361/2330 train_time:78489ms step_avg:57.67ms
step:1362/2330 train_time:78548ms step_avg:57.67ms
step:1363/2330 train_time:78605ms step_avg:57.67ms
step:1364/2330 train_time:78665ms step_avg:57.67ms
step:1365/2330 train_time:78721ms step_avg:57.67ms
step:1366/2330 train_time:78781ms step_avg:57.67ms
step:1367/2330 train_time:78837ms step_avg:57.67ms
step:1368/2330 train_time:78896ms step_avg:57.67ms
step:1369/2330 train_time:78953ms step_avg:57.67ms
step:1370/2330 train_time:79013ms step_avg:57.67ms
step:1371/2330 train_time:79069ms step_avg:57.67ms
step:1372/2330 train_time:79129ms step_avg:57.67ms
step:1373/2330 train_time:79186ms step_avg:57.67ms
step:1374/2330 train_time:79245ms step_avg:57.67ms
step:1375/2330 train_time:79301ms step_avg:57.67ms
step:1376/2330 train_time:79362ms step_avg:57.68ms
step:1377/2330 train_time:79418ms step_avg:57.67ms
step:1378/2330 train_time:79478ms step_avg:57.68ms
step:1379/2330 train_time:79535ms step_avg:57.68ms
step:1380/2330 train_time:79594ms step_avg:57.68ms
step:1381/2330 train_time:79650ms step_avg:57.68ms
step:1382/2330 train_time:79710ms step_avg:57.68ms
step:1383/2330 train_time:79768ms step_avg:57.68ms
step:1384/2330 train_time:79827ms step_avg:57.68ms
step:1385/2330 train_time:79884ms step_avg:57.68ms
step:1386/2330 train_time:79943ms step_avg:57.68ms
step:1387/2330 train_time:79999ms step_avg:57.68ms
step:1388/2330 train_time:80059ms step_avg:57.68ms
step:1389/2330 train_time:80115ms step_avg:57.68ms
step:1390/2330 train_time:80176ms step_avg:57.68ms
step:1391/2330 train_time:80232ms step_avg:57.68ms
step:1392/2330 train_time:80291ms step_avg:57.68ms
step:1393/2330 train_time:80348ms step_avg:57.68ms
step:1394/2330 train_time:80409ms step_avg:57.68ms
step:1395/2330 train_time:80465ms step_avg:57.68ms
step:1396/2330 train_time:80524ms step_avg:57.68ms
step:1397/2330 train_time:80581ms step_avg:57.68ms
step:1398/2330 train_time:80641ms step_avg:57.68ms
step:1399/2330 train_time:80697ms step_avg:57.68ms
step:1400/2330 train_time:80758ms step_avg:57.68ms
step:1401/2330 train_time:80813ms step_avg:57.68ms
step:1402/2330 train_time:80873ms step_avg:57.68ms
step:1403/2330 train_time:80929ms step_avg:57.68ms
step:1404/2330 train_time:80989ms step_avg:57.68ms
step:1405/2330 train_time:81046ms step_avg:57.68ms
step:1406/2330 train_time:81106ms step_avg:57.69ms
step:1407/2330 train_time:81162ms step_avg:57.68ms
step:1408/2330 train_time:81222ms step_avg:57.69ms
step:1409/2330 train_time:81278ms step_avg:57.68ms
step:1410/2330 train_time:81339ms step_avg:57.69ms
step:1411/2330 train_time:81395ms step_avg:57.69ms
step:1412/2330 train_time:81455ms step_avg:57.69ms
step:1413/2330 train_time:81512ms step_avg:57.69ms
step:1414/2330 train_time:81571ms step_avg:57.69ms
step:1415/2330 train_time:81627ms step_avg:57.69ms
step:1416/2330 train_time:81688ms step_avg:57.69ms
step:1417/2330 train_time:81744ms step_avg:57.69ms
step:1418/2330 train_time:81804ms step_avg:57.69ms
step:1419/2330 train_time:81860ms step_avg:57.69ms
step:1420/2330 train_time:81920ms step_avg:57.69ms
step:1421/2330 train_time:81976ms step_avg:57.69ms
step:1422/2330 train_time:82037ms step_avg:57.69ms
step:1423/2330 train_time:82093ms step_avg:57.69ms
step:1424/2330 train_time:82152ms step_avg:57.69ms
step:1425/2330 train_time:82209ms step_avg:57.69ms
step:1426/2330 train_time:82269ms step_avg:57.69ms
step:1427/2330 train_time:82325ms step_avg:57.69ms
step:1428/2330 train_time:82386ms step_avg:57.69ms
step:1429/2330 train_time:82442ms step_avg:57.69ms
step:1430/2330 train_time:82503ms step_avg:57.69ms
step:1431/2330 train_time:82559ms step_avg:57.69ms
step:1432/2330 train_time:82619ms step_avg:57.69ms
step:1433/2330 train_time:82675ms step_avg:57.69ms
step:1434/2330 train_time:82735ms step_avg:57.70ms
step:1435/2330 train_time:82792ms step_avg:57.69ms
step:1436/2330 train_time:82851ms step_avg:57.70ms
step:1437/2330 train_time:82907ms step_avg:57.69ms
step:1438/2330 train_time:82968ms step_avg:57.70ms
step:1439/2330 train_time:83024ms step_avg:57.70ms
step:1440/2330 train_time:83085ms step_avg:57.70ms
step:1441/2330 train_time:83141ms step_avg:57.70ms
step:1442/2330 train_time:83200ms step_avg:57.70ms
step:1443/2330 train_time:83257ms step_avg:57.70ms
step:1444/2330 train_time:83316ms step_avg:57.70ms
step:1445/2330 train_time:83373ms step_avg:57.70ms
step:1446/2330 train_time:83431ms step_avg:57.70ms
step:1447/2330 train_time:83488ms step_avg:57.70ms
step:1448/2330 train_time:83548ms step_avg:57.70ms
step:1449/2330 train_time:83604ms step_avg:57.70ms
step:1450/2330 train_time:83665ms step_avg:57.70ms
step:1451/2330 train_time:83721ms step_avg:57.70ms
step:1452/2330 train_time:83780ms step_avg:57.70ms
step:1453/2330 train_time:83837ms step_avg:57.70ms
step:1454/2330 train_time:83896ms step_avg:57.70ms
step:1455/2330 train_time:83952ms step_avg:57.70ms
step:1456/2330 train_time:84012ms step_avg:57.70ms
step:1457/2330 train_time:84068ms step_avg:57.70ms
step:1458/2330 train_time:84128ms step_avg:57.70ms
step:1459/2330 train_time:84184ms step_avg:57.70ms
step:1460/2330 train_time:84245ms step_avg:57.70ms
step:1461/2330 train_time:84301ms step_avg:57.70ms
step:1462/2330 train_time:84361ms step_avg:57.70ms
step:1463/2330 train_time:84417ms step_avg:57.70ms
step:1464/2330 train_time:84477ms step_avg:57.70ms
step:1465/2330 train_time:84533ms step_avg:57.70ms
step:1466/2330 train_time:84594ms step_avg:57.70ms
step:1467/2330 train_time:84651ms step_avg:57.70ms
step:1468/2330 train_time:84710ms step_avg:57.70ms
step:1469/2330 train_time:84766ms step_avg:57.70ms
step:1470/2330 train_time:84827ms step_avg:57.71ms
step:1471/2330 train_time:84882ms step_avg:57.70ms
step:1472/2330 train_time:84944ms step_avg:57.71ms
step:1473/2330 train_time:85000ms step_avg:57.71ms
step:1474/2330 train_time:85061ms step_avg:57.71ms
step:1475/2330 train_time:85117ms step_avg:57.71ms
step:1476/2330 train_time:85177ms step_avg:57.71ms
step:1477/2330 train_time:85233ms step_avg:57.71ms
step:1478/2330 train_time:85293ms step_avg:57.71ms
step:1479/2330 train_time:85350ms step_avg:57.71ms
step:1480/2330 train_time:85410ms step_avg:57.71ms
step:1481/2330 train_time:85467ms step_avg:57.71ms
step:1482/2330 train_time:85527ms step_avg:57.71ms
step:1483/2330 train_time:85582ms step_avg:57.71ms
step:1484/2330 train_time:85644ms step_avg:57.71ms
step:1485/2330 train_time:85700ms step_avg:57.71ms
step:1486/2330 train_time:85759ms step_avg:57.71ms
step:1487/2330 train_time:85815ms step_avg:57.71ms
step:1488/2330 train_time:85876ms step_avg:57.71ms
step:1489/2330 train_time:85932ms step_avg:57.71ms
step:1490/2330 train_time:85992ms step_avg:57.71ms
step:1491/2330 train_time:86049ms step_avg:57.71ms
step:1492/2330 train_time:86108ms step_avg:57.71ms
step:1493/2330 train_time:86164ms step_avg:57.71ms
step:1494/2330 train_time:86225ms step_avg:57.71ms
step:1495/2330 train_time:86280ms step_avg:57.71ms
step:1496/2330 train_time:86341ms step_avg:57.71ms
step:1497/2330 train_time:86397ms step_avg:57.71ms
step:1498/2330 train_time:86457ms step_avg:57.72ms
step:1499/2330 train_time:86513ms step_avg:57.71ms
step:1500/2330 train_time:86573ms step_avg:57.72ms
step:1500/2330 val_loss:5.2113 train_time:86652ms step_avg:57.77ms
step:1501/2330 train_time:86674ms step_avg:57.74ms
step:1502/2330 train_time:86696ms step_avg:57.72ms
step:1503/2330 train_time:86750ms step_avg:57.72ms
step:1504/2330 train_time:86815ms step_avg:57.72ms
step:1505/2330 train_time:86872ms step_avg:57.72ms
step:1506/2330 train_time:86935ms step_avg:57.73ms
step:1507/2330 train_time:86991ms step_avg:57.72ms
step:1508/2330 train_time:87051ms step_avg:57.73ms
step:1509/2330 train_time:87107ms step_avg:57.72ms
step:1510/2330 train_time:87166ms step_avg:57.73ms
step:1511/2330 train_time:87222ms step_avg:57.72ms
step:1512/2330 train_time:87281ms step_avg:57.73ms
step:1513/2330 train_time:87337ms step_avg:57.72ms
step:1514/2330 train_time:87396ms step_avg:57.73ms
step:1515/2330 train_time:87452ms step_avg:57.72ms
step:1516/2330 train_time:87511ms step_avg:57.72ms
step:1517/2330 train_time:87567ms step_avg:57.72ms
step:1518/2330 train_time:87626ms step_avg:57.72ms
step:1519/2330 train_time:87683ms step_avg:57.72ms
step:1520/2330 train_time:87744ms step_avg:57.73ms
step:1521/2330 train_time:87801ms step_avg:57.73ms
step:1522/2330 train_time:87862ms step_avg:57.73ms
step:1523/2330 train_time:87919ms step_avg:57.73ms
step:1524/2330 train_time:87979ms step_avg:57.73ms
step:1525/2330 train_time:88036ms step_avg:57.73ms
step:1526/2330 train_time:88096ms step_avg:57.73ms
step:1527/2330 train_time:88153ms step_avg:57.73ms
step:1528/2330 train_time:88212ms step_avg:57.73ms
step:1529/2330 train_time:88270ms step_avg:57.73ms
step:1530/2330 train_time:88328ms step_avg:57.73ms
step:1531/2330 train_time:88384ms step_avg:57.73ms
step:1532/2330 train_time:88443ms step_avg:57.73ms
step:1533/2330 train_time:88500ms step_avg:57.73ms
step:1534/2330 train_time:88560ms step_avg:57.73ms
step:1535/2330 train_time:88616ms step_avg:57.73ms
step:1536/2330 train_time:88676ms step_avg:57.73ms
step:1537/2330 train_time:88733ms step_avg:57.73ms
step:1538/2330 train_time:88795ms step_avg:57.73ms
step:1539/2330 train_time:88853ms step_avg:57.73ms
step:1540/2330 train_time:88914ms step_avg:57.74ms
step:1541/2330 train_time:88971ms step_avg:57.74ms
step:1542/2330 train_time:89032ms step_avg:57.74ms
step:1543/2330 train_time:89088ms step_avg:57.74ms
step:1544/2330 train_time:89151ms step_avg:57.74ms
step:1545/2330 train_time:89207ms step_avg:57.74ms
step:1546/2330 train_time:89268ms step_avg:57.74ms
step:1547/2330 train_time:89325ms step_avg:57.74ms
step:1548/2330 train_time:89385ms step_avg:57.74ms
step:1549/2330 train_time:89441ms step_avg:57.74ms
step:1550/2330 train_time:89502ms step_avg:57.74ms
step:1551/2330 train_time:89558ms step_avg:57.74ms
step:1552/2330 train_time:89618ms step_avg:57.74ms
step:1553/2330 train_time:89675ms step_avg:57.74ms
step:1554/2330 train_time:89735ms step_avg:57.74ms
step:1555/2330 train_time:89793ms step_avg:57.74ms
step:1556/2330 train_time:89854ms step_avg:57.75ms
step:1557/2330 train_time:89911ms step_avg:57.75ms
step:1558/2330 train_time:89972ms step_avg:57.75ms
step:1559/2330 train_time:90029ms step_avg:57.75ms
step:1560/2330 train_time:90090ms step_avg:57.75ms
step:1561/2330 train_time:90146ms step_avg:57.75ms
step:1562/2330 train_time:90207ms step_avg:57.75ms
step:1563/2330 train_time:90264ms step_avg:57.75ms
step:1564/2330 train_time:90324ms step_avg:57.75ms
step:1565/2330 train_time:90380ms step_avg:57.75ms
step:1566/2330 train_time:90441ms step_avg:57.75ms
step:1567/2330 train_time:90498ms step_avg:57.75ms
step:1568/2330 train_time:90557ms step_avg:57.75ms
step:1569/2330 train_time:90614ms step_avg:57.75ms
step:1570/2330 train_time:90674ms step_avg:57.75ms
step:1571/2330 train_time:90731ms step_avg:57.75ms
step:1572/2330 train_time:90792ms step_avg:57.76ms
step:1573/2330 train_time:90849ms step_avg:57.76ms
step:1574/2330 train_time:90909ms step_avg:57.76ms
step:1575/2330 train_time:90966ms step_avg:57.76ms
step:1576/2330 train_time:91029ms step_avg:57.76ms
step:1577/2330 train_time:91085ms step_avg:57.76ms
step:1578/2330 train_time:91145ms step_avg:57.76ms
step:1579/2330 train_time:91202ms step_avg:57.76ms
step:1580/2330 train_time:91263ms step_avg:57.76ms
step:1581/2330 train_time:91319ms step_avg:57.76ms
step:1582/2330 train_time:91379ms step_avg:57.76ms
step:1583/2330 train_time:91436ms step_avg:57.76ms
step:1584/2330 train_time:91498ms step_avg:57.76ms
step:1585/2330 train_time:91554ms step_avg:57.76ms
step:1586/2330 train_time:91614ms step_avg:57.76ms
step:1587/2330 train_time:91670ms step_avg:57.76ms
step:1588/2330 train_time:91731ms step_avg:57.77ms
step:1589/2330 train_time:91789ms step_avg:57.76ms
step:1590/2330 train_time:91849ms step_avg:57.77ms
step:1591/2330 train_time:91906ms step_avg:57.77ms
step:1592/2330 train_time:91967ms step_avg:57.77ms
step:1593/2330 train_time:92023ms step_avg:57.77ms
step:1594/2330 train_time:92086ms step_avg:57.77ms
step:1595/2330 train_time:92142ms step_avg:57.77ms
step:1596/2330 train_time:92202ms step_avg:57.77ms
step:1597/2330 train_time:92259ms step_avg:57.77ms
step:1598/2330 train_time:92319ms step_avg:57.77ms
step:1599/2330 train_time:92375ms step_avg:57.77ms
step:1600/2330 train_time:92436ms step_avg:57.77ms
step:1601/2330 train_time:92493ms step_avg:57.77ms
step:1602/2330 train_time:92553ms step_avg:57.77ms
step:1603/2330 train_time:92610ms step_avg:57.77ms
step:1604/2330 train_time:92670ms step_avg:57.77ms
step:1605/2330 train_time:92726ms step_avg:57.77ms
step:1606/2330 train_time:92787ms step_avg:57.78ms
step:1607/2330 train_time:92843ms step_avg:57.77ms
step:1608/2330 train_time:92904ms step_avg:57.78ms
step:1609/2330 train_time:92961ms step_avg:57.78ms
step:1610/2330 train_time:93022ms step_avg:57.78ms
step:1611/2330 train_time:93079ms step_avg:57.78ms
step:1612/2330 train_time:93139ms step_avg:57.78ms
step:1613/2330 train_time:93195ms step_avg:57.78ms
step:1614/2330 train_time:93256ms step_avg:57.78ms
step:1615/2330 train_time:93313ms step_avg:57.78ms
step:1616/2330 train_time:93374ms step_avg:57.78ms
step:1617/2330 train_time:93430ms step_avg:57.78ms
step:1618/2330 train_time:93492ms step_avg:57.78ms
step:1619/2330 train_time:93548ms step_avg:57.78ms
step:1620/2330 train_time:93609ms step_avg:57.78ms
step:1621/2330 train_time:93665ms step_avg:57.78ms
step:1622/2330 train_time:93726ms step_avg:57.78ms
step:1623/2330 train_time:93783ms step_avg:57.78ms
step:1624/2330 train_time:93843ms step_avg:57.78ms
step:1625/2330 train_time:93900ms step_avg:57.78ms
step:1626/2330 train_time:93959ms step_avg:57.79ms
step:1627/2330 train_time:94016ms step_avg:57.79ms
step:1628/2330 train_time:94076ms step_avg:57.79ms
step:1629/2330 train_time:94133ms step_avg:57.79ms
step:1630/2330 train_time:94194ms step_avg:57.79ms
step:1631/2330 train_time:94250ms step_avg:57.79ms
step:1632/2330 train_time:94311ms step_avg:57.79ms
step:1633/2330 train_time:94367ms step_avg:57.79ms
step:1634/2330 train_time:94428ms step_avg:57.79ms
step:1635/2330 train_time:94485ms step_avg:57.79ms
step:1636/2330 train_time:94545ms step_avg:57.79ms
step:1637/2330 train_time:94601ms step_avg:57.79ms
step:1638/2330 train_time:94662ms step_avg:57.79ms
step:1639/2330 train_time:94718ms step_avg:57.79ms
step:1640/2330 train_time:94779ms step_avg:57.79ms
step:1641/2330 train_time:94836ms step_avg:57.79ms
step:1642/2330 train_time:94896ms step_avg:57.79ms
step:1643/2330 train_time:94953ms step_avg:57.79ms
step:1644/2330 train_time:95014ms step_avg:57.79ms
step:1645/2330 train_time:95071ms step_avg:57.79ms
step:1646/2330 train_time:95132ms step_avg:57.80ms
step:1647/2330 train_time:95189ms step_avg:57.80ms
step:1648/2330 train_time:95249ms step_avg:57.80ms
step:1649/2330 train_time:95306ms step_avg:57.80ms
step:1650/2330 train_time:95366ms step_avg:57.80ms
step:1651/2330 train_time:95423ms step_avg:57.80ms
step:1652/2330 train_time:95484ms step_avg:57.80ms
step:1653/2330 train_time:95540ms step_avg:57.80ms
step:1654/2330 train_time:95601ms step_avg:57.80ms
step:1655/2330 train_time:95658ms step_avg:57.80ms
step:1656/2330 train_time:95718ms step_avg:57.80ms
step:1657/2330 train_time:95775ms step_avg:57.80ms
step:1658/2330 train_time:95835ms step_avg:57.80ms
step:1659/2330 train_time:95892ms step_avg:57.80ms
step:1660/2330 train_time:95952ms step_avg:57.80ms
step:1661/2330 train_time:96008ms step_avg:57.80ms
step:1662/2330 train_time:96070ms step_avg:57.80ms
step:1663/2330 train_time:96126ms step_avg:57.80ms
step:1664/2330 train_time:96186ms step_avg:57.80ms
step:1665/2330 train_time:96242ms step_avg:57.80ms
step:1666/2330 train_time:96304ms step_avg:57.81ms
step:1667/2330 train_time:96361ms step_avg:57.80ms
step:1668/2330 train_time:96421ms step_avg:57.81ms
step:1669/2330 train_time:96478ms step_avg:57.81ms
step:1670/2330 train_time:96538ms step_avg:57.81ms
step:1671/2330 train_time:96595ms step_avg:57.81ms
step:1672/2330 train_time:96656ms step_avg:57.81ms
step:1673/2330 train_time:96713ms step_avg:57.81ms
step:1674/2330 train_time:96773ms step_avg:57.81ms
step:1675/2330 train_time:96830ms step_avg:57.81ms
step:1676/2330 train_time:96890ms step_avg:57.81ms
step:1677/2330 train_time:96946ms step_avg:57.81ms
step:1678/2330 train_time:97008ms step_avg:57.81ms
step:1679/2330 train_time:97064ms step_avg:57.81ms
step:1680/2330 train_time:97125ms step_avg:57.81ms
step:1681/2330 train_time:97181ms step_avg:57.81ms
step:1682/2330 train_time:97242ms step_avg:57.81ms
step:1683/2330 train_time:97298ms step_avg:57.81ms
step:1684/2330 train_time:97359ms step_avg:57.81ms
step:1685/2330 train_time:97416ms step_avg:57.81ms
step:1686/2330 train_time:97476ms step_avg:57.81ms
step:1687/2330 train_time:97532ms step_avg:57.81ms
step:1688/2330 train_time:97593ms step_avg:57.82ms
step:1689/2330 train_time:97650ms step_avg:57.82ms
step:1690/2330 train_time:97710ms step_avg:57.82ms
step:1691/2330 train_time:97767ms step_avg:57.82ms
step:1692/2330 train_time:97828ms step_avg:57.82ms
step:1693/2330 train_time:97885ms step_avg:57.82ms
step:1694/2330 train_time:97944ms step_avg:57.82ms
step:1695/2330 train_time:98001ms step_avg:57.82ms
step:1696/2330 train_time:98063ms step_avg:57.82ms
step:1697/2330 train_time:98119ms step_avg:57.82ms
step:1698/2330 train_time:98179ms step_avg:57.82ms
step:1699/2330 train_time:98236ms step_avg:57.82ms
step:1700/2330 train_time:98297ms step_avg:57.82ms
step:1701/2330 train_time:98353ms step_avg:57.82ms
step:1702/2330 train_time:98414ms step_avg:57.82ms
step:1703/2330 train_time:98471ms step_avg:57.82ms
step:1704/2330 train_time:98532ms step_avg:57.82ms
step:1705/2330 train_time:98588ms step_avg:57.82ms
step:1706/2330 train_time:98650ms step_avg:57.83ms
step:1707/2330 train_time:98706ms step_avg:57.82ms
step:1708/2330 train_time:98767ms step_avg:57.83ms
step:1709/2330 train_time:98823ms step_avg:57.83ms
step:1710/2330 train_time:98884ms step_avg:57.83ms
step:1711/2330 train_time:98940ms step_avg:57.83ms
step:1712/2330 train_time:99001ms step_avg:57.83ms
step:1713/2330 train_time:99057ms step_avg:57.83ms
step:1714/2330 train_time:99118ms step_avg:57.83ms
step:1715/2330 train_time:99175ms step_avg:57.83ms
step:1716/2330 train_time:99235ms step_avg:57.83ms
step:1717/2330 train_time:99292ms step_avg:57.83ms
step:1718/2330 train_time:99353ms step_avg:57.83ms
step:1719/2330 train_time:99409ms step_avg:57.83ms
step:1720/2330 train_time:99470ms step_avg:57.83ms
step:1721/2330 train_time:99526ms step_avg:57.83ms
step:1722/2330 train_time:99588ms step_avg:57.83ms
step:1723/2330 train_time:99645ms step_avg:57.83ms
step:1724/2330 train_time:99706ms step_avg:57.83ms
step:1725/2330 train_time:99762ms step_avg:57.83ms
step:1726/2330 train_time:99824ms step_avg:57.84ms
step:1727/2330 train_time:99880ms step_avg:57.83ms
step:1728/2330 train_time:99940ms step_avg:57.84ms
step:1729/2330 train_time:99996ms step_avg:57.83ms
step:1730/2330 train_time:100057ms step_avg:57.84ms
step:1731/2330 train_time:100114ms step_avg:57.84ms
step:1732/2330 train_time:100174ms step_avg:57.84ms
step:1733/2330 train_time:100230ms step_avg:57.84ms
step:1734/2330 train_time:100291ms step_avg:57.84ms
step:1735/2330 train_time:100347ms step_avg:57.84ms
step:1736/2330 train_time:100409ms step_avg:57.84ms
step:1737/2330 train_time:100466ms step_avg:57.84ms
step:1738/2330 train_time:100527ms step_avg:57.84ms
step:1739/2330 train_time:100583ms step_avg:57.84ms
step:1740/2330 train_time:100643ms step_avg:57.84ms
step:1741/2330 train_time:100699ms step_avg:57.84ms
step:1742/2330 train_time:100761ms step_avg:57.84ms
step:1743/2330 train_time:100817ms step_avg:57.84ms
step:1744/2330 train_time:100878ms step_avg:57.84ms
step:1745/2330 train_time:100934ms step_avg:57.84ms
step:1746/2330 train_time:100995ms step_avg:57.84ms
step:1747/2330 train_time:101051ms step_avg:57.84ms
step:1748/2330 train_time:101112ms step_avg:57.84ms
step:1749/2330 train_time:101169ms step_avg:57.84ms
step:1750/2330 train_time:101229ms step_avg:57.85ms
step:1750/2330 val_loss:5.0981 train_time:101310ms step_avg:57.89ms
step:1751/2330 train_time:101330ms step_avg:57.87ms
step:1752/2330 train_time:101351ms step_avg:57.85ms
step:1753/2330 train_time:101403ms step_avg:57.85ms
step:1754/2330 train_time:101470ms step_avg:57.85ms
step:1755/2330 train_time:101526ms step_avg:57.85ms
step:1756/2330 train_time:101592ms step_avg:57.85ms
step:1757/2330 train_time:101648ms step_avg:57.85ms
step:1758/2330 train_time:101711ms step_avg:57.86ms
step:1759/2330 train_time:101767ms step_avg:57.86ms
step:1760/2330 train_time:101829ms step_avg:57.86ms
step:1761/2330 train_time:101885ms step_avg:57.86ms
step:1762/2330 train_time:101945ms step_avg:57.86ms
step:1763/2330 train_time:102001ms step_avg:57.86ms
step:1764/2330 train_time:102061ms step_avg:57.86ms
step:1765/2330 train_time:102117ms step_avg:57.86ms
step:1766/2330 train_time:102176ms step_avg:57.86ms
step:1767/2330 train_time:102233ms step_avg:57.86ms
step:1768/2330 train_time:102293ms step_avg:57.86ms
step:1769/2330 train_time:102350ms step_avg:57.86ms
step:1770/2330 train_time:102411ms step_avg:57.86ms
step:1771/2330 train_time:102467ms step_avg:57.86ms
step:1772/2330 train_time:102531ms step_avg:57.86ms
step:1773/2330 train_time:102587ms step_avg:57.86ms
step:1774/2330 train_time:102651ms step_avg:57.86ms
step:1775/2330 train_time:102708ms step_avg:57.86ms
step:1776/2330 train_time:102769ms step_avg:57.87ms
step:1777/2330 train_time:102825ms step_avg:57.86ms
step:1778/2330 train_time:102887ms step_avg:57.87ms
step:1779/2330 train_time:102944ms step_avg:57.87ms
step:1780/2330 train_time:103004ms step_avg:57.87ms
step:1781/2330 train_time:103061ms step_avg:57.87ms
step:1782/2330 train_time:103120ms step_avg:57.87ms
step:1783/2330 train_time:103176ms step_avg:57.87ms
step:1784/2330 train_time:103236ms step_avg:57.87ms
step:1785/2330 train_time:103292ms step_avg:57.87ms
step:1786/2330 train_time:103352ms step_avg:57.87ms
step:1787/2330 train_time:103409ms step_avg:57.87ms
step:1788/2330 train_time:103470ms step_avg:57.87ms
step:1789/2330 train_time:103527ms step_avg:57.87ms
step:1790/2330 train_time:103588ms step_avg:57.87ms
step:1791/2330 train_time:103644ms step_avg:57.87ms
step:1792/2330 train_time:103706ms step_avg:57.87ms
step:1793/2330 train_time:103763ms step_avg:57.87ms
step:1794/2330 train_time:103823ms step_avg:57.87ms
step:1795/2330 train_time:103879ms step_avg:57.87ms
step:1796/2330 train_time:103941ms step_avg:57.87ms
step:1797/2330 train_time:103997ms step_avg:57.87ms
step:1798/2330 train_time:104057ms step_avg:57.87ms
step:1799/2330 train_time:104114ms step_avg:57.87ms
step:1800/2330 train_time:104174ms step_avg:57.87ms
step:1801/2330 train_time:104231ms step_avg:57.87ms
step:1802/2330 train_time:104290ms step_avg:57.87ms
step:1803/2330 train_time:104348ms step_avg:57.87ms
step:1804/2330 train_time:104407ms step_avg:57.88ms
step:1805/2330 train_time:104464ms step_avg:57.87ms
step:1806/2330 train_time:104525ms step_avg:57.88ms
step:1807/2330 train_time:104582ms step_avg:57.88ms
step:1808/2330 train_time:104644ms step_avg:57.88ms
step:1809/2330 train_time:104701ms step_avg:57.88ms
step:1810/2330 train_time:104761ms step_avg:57.88ms
step:1811/2330 train_time:104818ms step_avg:57.88ms
step:1812/2330 train_time:104879ms step_avg:57.88ms
step:1813/2330 train_time:104936ms step_avg:57.88ms
step:1814/2330 train_time:104996ms step_avg:57.88ms
step:1815/2330 train_time:105054ms step_avg:57.88ms
step:1816/2330 train_time:105114ms step_avg:57.88ms
step:1817/2330 train_time:105171ms step_avg:57.88ms
step:1818/2330 train_time:105230ms step_avg:57.88ms
step:1819/2330 train_time:105287ms step_avg:57.88ms
step:1820/2330 train_time:105348ms step_avg:57.88ms
step:1821/2330 train_time:105405ms step_avg:57.88ms
step:1822/2330 train_time:105465ms step_avg:57.88ms
step:1823/2330 train_time:105522ms step_avg:57.88ms
step:1824/2330 train_time:105582ms step_avg:57.89ms
step:1825/2330 train_time:105639ms step_avg:57.88ms
step:1826/2330 train_time:105700ms step_avg:57.89ms
step:1827/2330 train_time:105757ms step_avg:57.89ms
step:1828/2330 train_time:105817ms step_avg:57.89ms
step:1829/2330 train_time:105874ms step_avg:57.89ms
step:1830/2330 train_time:105935ms step_avg:57.89ms
step:1831/2330 train_time:105991ms step_avg:57.89ms
step:1832/2330 train_time:106052ms step_avg:57.89ms
step:1833/2330 train_time:106109ms step_avg:57.89ms
step:1834/2330 train_time:106169ms step_avg:57.89ms
step:1835/2330 train_time:106226ms step_avg:57.89ms
step:1836/2330 train_time:106286ms step_avg:57.89ms
step:1837/2330 train_time:106343ms step_avg:57.89ms
step:1838/2330 train_time:106403ms step_avg:57.89ms
step:1839/2330 train_time:106459ms step_avg:57.89ms
step:1840/2330 train_time:106520ms step_avg:57.89ms
step:1841/2330 train_time:106577ms step_avg:57.89ms
step:1842/2330 train_time:106638ms step_avg:57.89ms
step:1843/2330 train_time:106695ms step_avg:57.89ms
step:1844/2330 train_time:106756ms step_avg:57.89ms
step:1845/2330 train_time:106814ms step_avg:57.89ms
step:1846/2330 train_time:106873ms step_avg:57.89ms
step:1847/2330 train_time:106930ms step_avg:57.89ms
step:1848/2330 train_time:106991ms step_avg:57.90ms
step:1849/2330 train_time:107048ms step_avg:57.89ms
step:1850/2330 train_time:107109ms step_avg:57.90ms
step:1851/2330 train_time:107165ms step_avg:57.90ms
step:1852/2330 train_time:107227ms step_avg:57.90ms
step:1853/2330 train_time:107283ms step_avg:57.90ms
step:1854/2330 train_time:107345ms step_avg:57.90ms
step:1855/2330 train_time:107401ms step_avg:57.90ms
step:1856/2330 train_time:107460ms step_avg:57.90ms
step:1857/2330 train_time:107517ms step_avg:57.90ms
step:1858/2330 train_time:107578ms step_avg:57.90ms
step:1859/2330 train_time:107634ms step_avg:57.90ms
step:1860/2330 train_time:107697ms step_avg:57.90ms
step:1861/2330 train_time:107753ms step_avg:57.90ms
step:1862/2330 train_time:107815ms step_avg:57.90ms
step:1863/2330 train_time:107872ms step_avg:57.90ms
step:1864/2330 train_time:107933ms step_avg:57.90ms
step:1865/2330 train_time:107990ms step_avg:57.90ms
step:1866/2330 train_time:108051ms step_avg:57.91ms
step:1867/2330 train_time:108108ms step_avg:57.90ms
step:1868/2330 train_time:108169ms step_avg:57.91ms
step:1869/2330 train_time:108225ms step_avg:57.91ms
step:1870/2330 train_time:108286ms step_avg:57.91ms
step:1871/2330 train_time:108342ms step_avg:57.91ms
step:1872/2330 train_time:108402ms step_avg:57.91ms
step:1873/2330 train_time:108459ms step_avg:57.91ms
step:1874/2330 train_time:108518ms step_avg:57.91ms
step:1875/2330 train_time:108575ms step_avg:57.91ms
step:1876/2330 train_time:108636ms step_avg:57.91ms
step:1877/2330 train_time:108694ms step_avg:57.91ms
step:1878/2330 train_time:108754ms step_avg:57.91ms
step:1879/2330 train_time:108811ms step_avg:57.91ms
step:1880/2330 train_time:108871ms step_avg:57.91ms
step:1881/2330 train_time:108928ms step_avg:57.91ms
step:1882/2330 train_time:108989ms step_avg:57.91ms
step:1883/2330 train_time:109046ms step_avg:57.91ms
step:1884/2330 train_time:109106ms step_avg:57.91ms
step:1885/2330 train_time:109163ms step_avg:57.91ms
step:1886/2330 train_time:109223ms step_avg:57.91ms
step:1887/2330 train_time:109280ms step_avg:57.91ms
step:1888/2330 train_time:109340ms step_avg:57.91ms
step:1889/2330 train_time:109397ms step_avg:57.91ms
step:1890/2330 train_time:109457ms step_avg:57.91ms
step:1891/2330 train_time:109514ms step_avg:57.91ms
step:1892/2330 train_time:109574ms step_avg:57.91ms
step:1893/2330 train_time:109632ms step_avg:57.91ms
step:1894/2330 train_time:109693ms step_avg:57.92ms
step:1895/2330 train_time:109750ms step_avg:57.92ms
step:1896/2330 train_time:109810ms step_avg:57.92ms
step:1897/2330 train_time:109867ms step_avg:57.92ms
step:1898/2330 train_time:109928ms step_avg:57.92ms
step:1899/2330 train_time:109984ms step_avg:57.92ms
step:1900/2330 train_time:110045ms step_avg:57.92ms
step:1901/2330 train_time:110102ms step_avg:57.92ms
step:1902/2330 train_time:110162ms step_avg:57.92ms
step:1903/2330 train_time:110218ms step_avg:57.92ms
step:1904/2330 train_time:110278ms step_avg:57.92ms
step:1905/2330 train_time:110335ms step_avg:57.92ms
step:1906/2330 train_time:110396ms step_avg:57.92ms
step:1907/2330 train_time:110452ms step_avg:57.92ms
step:1908/2330 train_time:110514ms step_avg:57.92ms
step:1909/2330 train_time:110570ms step_avg:57.92ms
step:1910/2330 train_time:110631ms step_avg:57.92ms
step:1911/2330 train_time:110687ms step_avg:57.92ms
step:1912/2330 train_time:110749ms step_avg:57.92ms
step:1913/2330 train_time:110806ms step_avg:57.92ms
step:1914/2330 train_time:110867ms step_avg:57.92ms
step:1915/2330 train_time:110924ms step_avg:57.92ms
step:1916/2330 train_time:110985ms step_avg:57.93ms
step:1917/2330 train_time:111042ms step_avg:57.92ms
step:1918/2330 train_time:111102ms step_avg:57.93ms
step:1919/2330 train_time:111159ms step_avg:57.93ms
step:1920/2330 train_time:111219ms step_avg:57.93ms
step:1921/2330 train_time:111275ms step_avg:57.93ms
step:1922/2330 train_time:111336ms step_avg:57.93ms
step:1923/2330 train_time:111393ms step_avg:57.93ms
step:1924/2330 train_time:111454ms step_avg:57.93ms
step:1925/2330 train_time:111510ms step_avg:57.93ms
step:1926/2330 train_time:111571ms step_avg:57.93ms
step:1927/2330 train_time:111628ms step_avg:57.93ms
step:1928/2330 train_time:111688ms step_avg:57.93ms
step:1929/2330 train_time:111745ms step_avg:57.93ms
step:1930/2330 train_time:111806ms step_avg:57.93ms
step:1931/2330 train_time:111863ms step_avg:57.93ms
step:1932/2330 train_time:111923ms step_avg:57.93ms
step:1933/2330 train_time:111980ms step_avg:57.93ms
step:1934/2330 train_time:112040ms step_avg:57.93ms
step:1935/2330 train_time:112097ms step_avg:57.93ms
step:1936/2330 train_time:112157ms step_avg:57.93ms
step:1937/2330 train_time:112214ms step_avg:57.93ms
step:1938/2330 train_time:112274ms step_avg:57.93ms
step:1939/2330 train_time:112330ms step_avg:57.93ms
step:1940/2330 train_time:112392ms step_avg:57.93ms
step:1941/2330 train_time:112449ms step_avg:57.93ms
step:1942/2330 train_time:112509ms step_avg:57.93ms
step:1943/2330 train_time:112566ms step_avg:57.93ms
step:1944/2330 train_time:112628ms step_avg:57.94ms
step:1945/2330 train_time:112683ms step_avg:57.93ms
step:1946/2330 train_time:112746ms step_avg:57.94ms
step:1947/2330 train_time:112802ms step_avg:57.94ms
step:1948/2330 train_time:112863ms step_avg:57.94ms
step:1949/2330 train_time:112919ms step_avg:57.94ms
step:1950/2330 train_time:112979ms step_avg:57.94ms
step:1951/2330 train_time:113036ms step_avg:57.94ms
step:1952/2330 train_time:113097ms step_avg:57.94ms
step:1953/2330 train_time:113154ms step_avg:57.94ms
step:1954/2330 train_time:113213ms step_avg:57.94ms
step:1955/2330 train_time:113270ms step_avg:57.94ms
step:1956/2330 train_time:113332ms step_avg:57.94ms
step:1957/2330 train_time:113388ms step_avg:57.94ms
step:1958/2330 train_time:113449ms step_avg:57.94ms
step:1959/2330 train_time:113505ms step_avg:57.94ms
step:1960/2330 train_time:113567ms step_avg:57.94ms
step:1961/2330 train_time:113623ms step_avg:57.94ms
step:1962/2330 train_time:113684ms step_avg:57.94ms
step:1963/2330 train_time:113741ms step_avg:57.94ms
step:1964/2330 train_time:113801ms step_avg:57.94ms
step:1965/2330 train_time:113858ms step_avg:57.94ms
step:1966/2330 train_time:113919ms step_avg:57.94ms
step:1967/2330 train_time:113975ms step_avg:57.94ms
step:1968/2330 train_time:114036ms step_avg:57.95ms
step:1969/2330 train_time:114092ms step_avg:57.94ms
step:1970/2330 train_time:114154ms step_avg:57.95ms
step:1971/2330 train_time:114211ms step_avg:57.95ms
step:1972/2330 train_time:114271ms step_avg:57.95ms
step:1973/2330 train_time:114328ms step_avg:57.95ms
step:1974/2330 train_time:114389ms step_avg:57.95ms
step:1975/2330 train_time:114445ms step_avg:57.95ms
step:1976/2330 train_time:114507ms step_avg:57.95ms
step:1977/2330 train_time:114563ms step_avg:57.95ms
step:1978/2330 train_time:114624ms step_avg:57.95ms
step:1979/2330 train_time:114680ms step_avg:57.95ms
step:1980/2330 train_time:114742ms step_avg:57.95ms
step:1981/2330 train_time:114798ms step_avg:57.95ms
step:1982/2330 train_time:114858ms step_avg:57.95ms
step:1983/2330 train_time:114915ms step_avg:57.95ms
step:1984/2330 train_time:114975ms step_avg:57.95ms
step:1985/2330 train_time:115031ms step_avg:57.95ms
step:1986/2330 train_time:115093ms step_avg:57.95ms
step:1987/2330 train_time:115150ms step_avg:57.95ms
step:1988/2330 train_time:115210ms step_avg:57.95ms
step:1989/2330 train_time:115267ms step_avg:57.95ms
step:1990/2330 train_time:115328ms step_avg:57.95ms
step:1991/2330 train_time:115385ms step_avg:57.95ms
step:1992/2330 train_time:115447ms step_avg:57.96ms
step:1993/2330 train_time:115503ms step_avg:57.95ms
step:1994/2330 train_time:115563ms step_avg:57.96ms
step:1995/2330 train_time:115620ms step_avg:57.95ms
step:1996/2330 train_time:115681ms step_avg:57.96ms
step:1997/2330 train_time:115738ms step_avg:57.96ms
step:1998/2330 train_time:115798ms step_avg:57.96ms
step:1999/2330 train_time:115855ms step_avg:57.96ms
step:2000/2330 train_time:115916ms step_avg:57.96ms
step:2000/2330 val_loss:5.0095 train_time:115997ms step_avg:58.00ms
step:2001/2330 train_time:116018ms step_avg:57.98ms
step:2002/2330 train_time:116040ms step_avg:57.96ms
step:2003/2330 train_time:116096ms step_avg:57.96ms
step:2004/2330 train_time:116161ms step_avg:57.96ms
step:2005/2330 train_time:116218ms step_avg:57.96ms
step:2006/2330 train_time:116280ms step_avg:57.97ms
step:2007/2330 train_time:116337ms step_avg:57.97ms
step:2008/2330 train_time:116397ms step_avg:57.97ms
step:2009/2330 train_time:116454ms step_avg:57.97ms
step:2010/2330 train_time:116514ms step_avg:57.97ms
step:2011/2330 train_time:116570ms step_avg:57.97ms
step:2012/2330 train_time:116630ms step_avg:57.97ms
step:2013/2330 train_time:116685ms step_avg:57.97ms
step:2014/2330 train_time:116745ms step_avg:57.97ms
step:2015/2330 train_time:116801ms step_avg:57.97ms
step:2016/2330 train_time:116861ms step_avg:57.97ms
step:2017/2330 train_time:116918ms step_avg:57.97ms
step:2018/2330 train_time:116979ms step_avg:57.97ms
step:2019/2330 train_time:117037ms step_avg:57.97ms
step:2020/2330 train_time:117101ms step_avg:57.97ms
step:2021/2330 train_time:117158ms step_avg:57.97ms
step:2022/2330 train_time:117221ms step_avg:57.97ms
step:2023/2330 train_time:117278ms step_avg:57.97ms
step:2024/2330 train_time:117338ms step_avg:57.97ms
step:2025/2330 train_time:117396ms step_avg:57.97ms
step:2026/2330 train_time:117455ms step_avg:57.97ms
step:2027/2330 train_time:117513ms step_avg:57.97ms
step:2028/2330 train_time:117572ms step_avg:57.97ms
step:2029/2330 train_time:117628ms step_avg:57.97ms
step:2030/2330 train_time:117688ms step_avg:57.97ms
step:2031/2330 train_time:117744ms step_avg:57.97ms
step:2032/2330 train_time:117803ms step_avg:57.97ms
step:2033/2330 train_time:117860ms step_avg:57.97ms
step:2034/2330 train_time:117920ms step_avg:57.97ms
step:2035/2330 train_time:117977ms step_avg:57.97ms
step:2036/2330 train_time:118038ms step_avg:57.98ms
step:2037/2330 train_time:118095ms step_avg:57.98ms
step:2038/2330 train_time:118158ms step_avg:57.98ms
step:2039/2330 train_time:118215ms step_avg:57.98ms
step:2040/2330 train_time:118277ms step_avg:57.98ms
step:2041/2330 train_time:118333ms step_avg:57.98ms
step:2042/2330 train_time:118395ms step_avg:57.98ms
step:2043/2330 train_time:118451ms step_avg:57.98ms
step:2044/2330 train_time:118512ms step_avg:57.98ms
step:2045/2330 train_time:118569ms step_avg:57.98ms
step:2046/2330 train_time:118629ms step_avg:57.98ms
step:2047/2330 train_time:118685ms step_avg:57.98ms
step:2048/2330 train_time:118746ms step_avg:57.98ms
step:2049/2330 train_time:118802ms step_avg:57.98ms
step:2050/2330 train_time:118863ms step_avg:57.98ms
step:2051/2330 train_time:118920ms step_avg:57.98ms
step:2052/2330 train_time:118980ms step_avg:57.98ms
step:2053/2330 train_time:119037ms step_avg:57.98ms
step:2054/2330 train_time:119098ms step_avg:57.98ms
step:2055/2330 train_time:119155ms step_avg:57.98ms
step:2056/2330 train_time:119217ms step_avg:57.98ms
step:2057/2330 train_time:119274ms step_avg:57.98ms
step:2058/2330 train_time:119335ms step_avg:57.99ms
step:2059/2330 train_time:119392ms step_avg:57.99ms
step:2060/2330 train_time:119453ms step_avg:57.99ms
step:2061/2330 train_time:119510ms step_avg:57.99ms
step:2062/2330 train_time:119571ms step_avg:57.99ms
step:2063/2330 train_time:119628ms step_avg:57.99ms
step:2064/2330 train_time:119687ms step_avg:57.99ms
step:2065/2330 train_time:119744ms step_avg:57.99ms
step:2066/2330 train_time:119804ms step_avg:57.99ms
step:2067/2330 train_time:119861ms step_avg:57.99ms
step:2068/2330 train_time:119921ms step_avg:57.99ms
step:2069/2330 train_time:119978ms step_avg:57.99ms
step:2070/2330 train_time:120038ms step_avg:57.99ms
step:2071/2330 train_time:120094ms step_avg:57.99ms
step:2072/2330 train_time:120156ms step_avg:57.99ms
step:2073/2330 train_time:120213ms step_avg:57.99ms
step:2074/2330 train_time:120274ms step_avg:57.99ms
step:2075/2330 train_time:120330ms step_avg:57.99ms
step:2076/2330 train_time:120391ms step_avg:57.99ms
step:2077/2330 train_time:120449ms step_avg:57.99ms
step:2078/2330 train_time:120509ms step_avg:57.99ms
step:2079/2330 train_time:120565ms step_avg:57.99ms
step:2080/2330 train_time:120626ms step_avg:57.99ms
step:2081/2330 train_time:120683ms step_avg:57.99ms
step:2082/2330 train_time:120743ms step_avg:57.99ms
step:2083/2330 train_time:120799ms step_avg:57.99ms
step:2084/2330 train_time:120859ms step_avg:57.99ms
step:2085/2330 train_time:120916ms step_avg:57.99ms
step:2086/2330 train_time:120978ms step_avg:58.00ms
step:2087/2330 train_time:121034ms step_avg:57.99ms
step:2088/2330 train_time:121095ms step_avg:58.00ms
step:2089/2330 train_time:121151ms step_avg:57.99ms
step:2090/2330 train_time:121213ms step_avg:58.00ms
step:2091/2330 train_time:121269ms step_avg:58.00ms
step:2092/2330 train_time:121331ms step_avg:58.00ms
step:2093/2330 train_time:121388ms step_avg:58.00ms
step:2094/2330 train_time:121451ms step_avg:58.00ms
step:2095/2330 train_time:121507ms step_avg:58.00ms
step:2096/2330 train_time:121569ms step_avg:58.00ms
step:2097/2330 train_time:121625ms step_avg:58.00ms
step:2098/2330 train_time:121686ms step_avg:58.00ms
step:2099/2330 train_time:121742ms step_avg:58.00ms
step:2100/2330 train_time:121803ms step_avg:58.00ms
step:2101/2330 train_time:121859ms step_avg:58.00ms
step:2102/2330 train_time:121920ms step_avg:58.00ms
step:2103/2330 train_time:121976ms step_avg:58.00ms
step:2104/2330 train_time:122037ms step_avg:58.00ms
step:2105/2330 train_time:122094ms step_avg:58.00ms
step:2106/2330 train_time:122156ms step_avg:58.00ms
step:2107/2330 train_time:122213ms step_avg:58.00ms
step:2108/2330 train_time:122273ms step_avg:58.00ms
step:2109/2330 train_time:122330ms step_avg:58.00ms
step:2110/2330 train_time:122391ms step_avg:58.01ms
step:2111/2330 train_time:122447ms step_avg:58.00ms
step:2112/2330 train_time:122509ms step_avg:58.01ms
step:2113/2330 train_time:122565ms step_avg:58.01ms
step:2114/2330 train_time:122626ms step_avg:58.01ms
step:2115/2330 train_time:122682ms step_avg:58.01ms
step:2116/2330 train_time:122742ms step_avg:58.01ms
step:2117/2330 train_time:122798ms step_avg:58.01ms
step:2118/2330 train_time:122859ms step_avg:58.01ms
step:2119/2330 train_time:122916ms step_avg:58.01ms
step:2120/2330 train_time:122976ms step_avg:58.01ms
step:2121/2330 train_time:123033ms step_avg:58.01ms
step:2122/2330 train_time:123093ms step_avg:58.01ms
step:2123/2330 train_time:123150ms step_avg:58.01ms
step:2124/2330 train_time:123212ms step_avg:58.01ms
step:2125/2330 train_time:123269ms step_avg:58.01ms
step:2126/2330 train_time:123330ms step_avg:58.01ms
step:2127/2330 train_time:123385ms step_avg:58.01ms
step:2128/2330 train_time:123448ms step_avg:58.01ms
step:2129/2330 train_time:123504ms step_avg:58.01ms
step:2130/2330 train_time:123567ms step_avg:58.01ms
step:2131/2330 train_time:123623ms step_avg:58.01ms
step:2132/2330 train_time:123684ms step_avg:58.01ms
step:2133/2330 train_time:123740ms step_avg:58.01ms
step:2134/2330 train_time:123801ms step_avg:58.01ms
step:2135/2330 train_time:123857ms step_avg:58.01ms
step:2136/2330 train_time:123919ms step_avg:58.01ms
step:2137/2330 train_time:123975ms step_avg:58.01ms
step:2138/2330 train_time:124036ms step_avg:58.02ms
step:2139/2330 train_time:124092ms step_avg:58.01ms
step:2140/2330 train_time:124153ms step_avg:58.02ms
step:2141/2330 train_time:124210ms step_avg:58.02ms
step:2142/2330 train_time:124271ms step_avg:58.02ms
step:2143/2330 train_time:124327ms step_avg:58.02ms
step:2144/2330 train_time:124388ms step_avg:58.02ms
step:2145/2330 train_time:124444ms step_avg:58.02ms
step:2146/2330 train_time:124506ms step_avg:58.02ms
step:2147/2330 train_time:124561ms step_avg:58.02ms
step:2148/2330 train_time:124623ms step_avg:58.02ms
step:2149/2330 train_time:124679ms step_avg:58.02ms
step:2150/2330 train_time:124740ms step_avg:58.02ms
step:2151/2330 train_time:124796ms step_avg:58.02ms
step:2152/2330 train_time:124856ms step_avg:58.02ms
step:2153/2330 train_time:124913ms step_avg:58.02ms
step:2154/2330 train_time:124974ms step_avg:58.02ms
step:2155/2330 train_time:125031ms step_avg:58.02ms
step:2156/2330 train_time:125091ms step_avg:58.02ms
step:2157/2330 train_time:125148ms step_avg:58.02ms
step:2158/2330 train_time:125208ms step_avg:58.02ms
step:2159/2330 train_time:125265ms step_avg:58.02ms
step:2160/2330 train_time:125326ms step_avg:58.02ms
step:2161/2330 train_time:125382ms step_avg:58.02ms
step:2162/2330 train_time:125443ms step_avg:58.02ms
step:2163/2330 train_time:125500ms step_avg:58.02ms
step:2164/2330 train_time:125560ms step_avg:58.02ms
step:2165/2330 train_time:125617ms step_avg:58.02ms
step:2166/2330 train_time:125677ms step_avg:58.02ms
step:2167/2330 train_time:125734ms step_avg:58.02ms
step:2168/2330 train_time:125794ms step_avg:58.02ms
step:2169/2330 train_time:125850ms step_avg:58.02ms
step:2170/2330 train_time:125912ms step_avg:58.02ms
step:2171/2330 train_time:125968ms step_avg:58.02ms
step:2172/2330 train_time:126029ms step_avg:58.02ms
step:2173/2330 train_time:126085ms step_avg:58.02ms
step:2174/2330 train_time:126147ms step_avg:58.03ms
step:2175/2330 train_time:126204ms step_avg:58.02ms
step:2176/2330 train_time:126264ms step_avg:58.03ms
step:2177/2330 train_time:126321ms step_avg:58.03ms
step:2178/2330 train_time:126381ms step_avg:58.03ms
step:2179/2330 train_time:126438ms step_avg:58.03ms
step:2180/2330 train_time:126499ms step_avg:58.03ms
step:2181/2330 train_time:126557ms step_avg:58.03ms
step:2182/2330 train_time:126617ms step_avg:58.03ms
step:2183/2330 train_time:126674ms step_avg:58.03ms
step:2184/2330 train_time:126734ms step_avg:58.03ms
step:2185/2330 train_time:126791ms step_avg:58.03ms
step:2186/2330 train_time:126851ms step_avg:58.03ms
step:2187/2330 train_time:126908ms step_avg:58.03ms
step:2188/2330 train_time:126970ms step_avg:58.03ms
step:2189/2330 train_time:127027ms step_avg:58.03ms
step:2190/2330 train_time:127088ms step_avg:58.03ms
step:2191/2330 train_time:127144ms step_avg:58.03ms
step:2192/2330 train_time:127206ms step_avg:58.03ms
step:2193/2330 train_time:127262ms step_avg:58.03ms
step:2194/2330 train_time:127323ms step_avg:58.03ms
step:2195/2330 train_time:127380ms step_avg:58.03ms
step:2196/2330 train_time:127440ms step_avg:58.03ms
step:2197/2330 train_time:127497ms step_avg:58.03ms
step:2198/2330 train_time:127558ms step_avg:58.03ms
step:2199/2330 train_time:127616ms step_avg:58.03ms
step:2200/2330 train_time:127676ms step_avg:58.03ms
step:2201/2330 train_time:127734ms step_avg:58.03ms
step:2202/2330 train_time:127794ms step_avg:58.04ms
step:2203/2330 train_time:127851ms step_avg:58.03ms
step:2204/2330 train_time:127912ms step_avg:58.04ms
step:2205/2330 train_time:127969ms step_avg:58.04ms
step:2206/2330 train_time:128030ms step_avg:58.04ms
step:2207/2330 train_time:128086ms step_avg:58.04ms
step:2208/2330 train_time:128149ms step_avg:58.04ms
step:2209/2330 train_time:128206ms step_avg:58.04ms
step:2210/2330 train_time:128268ms step_avg:58.04ms
step:2211/2330 train_time:128324ms step_avg:58.04ms
step:2212/2330 train_time:128385ms step_avg:58.04ms
step:2213/2330 train_time:128441ms step_avg:58.04ms
step:2214/2330 train_time:128501ms step_avg:58.04ms
step:2215/2330 train_time:128557ms step_avg:58.04ms
step:2216/2330 train_time:128618ms step_avg:58.04ms
step:2217/2330 train_time:128675ms step_avg:58.04ms
step:2218/2330 train_time:128736ms step_avg:58.04ms
step:2219/2330 train_time:128792ms step_avg:58.04ms
step:2220/2330 train_time:128852ms step_avg:58.04ms
step:2221/2330 train_time:128909ms step_avg:58.04ms
step:2222/2330 train_time:128970ms step_avg:58.04ms
step:2223/2330 train_time:129027ms step_avg:58.04ms
step:2224/2330 train_time:129088ms step_avg:58.04ms
step:2225/2330 train_time:129144ms step_avg:58.04ms
step:2226/2330 train_time:129205ms step_avg:58.04ms
step:2227/2330 train_time:129261ms step_avg:58.04ms
step:2228/2330 train_time:129323ms step_avg:58.04ms
step:2229/2330 train_time:129379ms step_avg:58.04ms
step:2230/2330 train_time:129440ms step_avg:58.04ms
step:2231/2330 train_time:129496ms step_avg:58.04ms
step:2232/2330 train_time:129557ms step_avg:58.05ms
step:2233/2330 train_time:129614ms step_avg:58.04ms
step:2234/2330 train_time:129674ms step_avg:58.05ms
step:2235/2330 train_time:129730ms step_avg:58.04ms
step:2236/2330 train_time:129791ms step_avg:58.05ms
step:2237/2330 train_time:129848ms step_avg:58.05ms
step:2238/2330 train_time:129909ms step_avg:58.05ms
step:2239/2330 train_time:129966ms step_avg:58.05ms
step:2240/2330 train_time:130027ms step_avg:58.05ms
step:2241/2330 train_time:130083ms step_avg:58.05ms
step:2242/2330 train_time:130143ms step_avg:58.05ms
step:2243/2330 train_time:130200ms step_avg:58.05ms
step:2244/2330 train_time:130261ms step_avg:58.05ms
step:2245/2330 train_time:130317ms step_avg:58.05ms
step:2246/2330 train_time:130378ms step_avg:58.05ms
step:2247/2330 train_time:130435ms step_avg:58.05ms
step:2248/2330 train_time:130496ms step_avg:58.05ms
step:2249/2330 train_time:130552ms step_avg:58.05ms
step:2250/2330 train_time:130613ms step_avg:58.05ms
step:2250/2330 val_loss:4.9421 train_time:130694ms step_avg:58.09ms
step:2251/2330 train_time:130714ms step_avg:58.07ms
step:2252/2330 train_time:130735ms step_avg:58.05ms
step:2253/2330 train_time:130793ms step_avg:58.05ms
step:2254/2330 train_time:130859ms step_avg:58.06ms
step:2255/2330 train_time:130916ms step_avg:58.06ms
step:2256/2330 train_time:130979ms step_avg:58.06ms
step:2257/2330 train_time:131035ms step_avg:58.06ms
step:2258/2330 train_time:131096ms step_avg:58.06ms
step:2259/2330 train_time:131153ms step_avg:58.06ms
step:2260/2330 train_time:131212ms step_avg:58.06ms
step:2261/2330 train_time:131268ms step_avg:58.06ms
step:2262/2330 train_time:131328ms step_avg:58.06ms
step:2263/2330 train_time:131384ms step_avg:58.06ms
step:2264/2330 train_time:131444ms step_avg:58.06ms
step:2265/2330 train_time:131500ms step_avg:58.06ms
step:2266/2330 train_time:131560ms step_avg:58.06ms
step:2267/2330 train_time:131616ms step_avg:58.06ms
step:2268/2330 train_time:131677ms step_avg:58.06ms
step:2269/2330 train_time:131735ms step_avg:58.06ms
step:2270/2330 train_time:131797ms step_avg:58.06ms
step:2271/2330 train_time:131855ms step_avg:58.06ms
step:2272/2330 train_time:131917ms step_avg:58.06ms
step:2273/2330 train_time:131974ms step_avg:58.06ms
step:2274/2330 train_time:132036ms step_avg:58.06ms
step:2275/2330 train_time:132093ms step_avg:58.06ms
step:2276/2330 train_time:132153ms step_avg:58.06ms
step:2277/2330 train_time:132210ms step_avg:58.06ms
step:2278/2330 train_time:132270ms step_avg:58.06ms
step:2279/2330 train_time:132326ms step_avg:58.06ms
step:2280/2330 train_time:132387ms step_avg:58.06ms
step:2281/2330 train_time:132443ms step_avg:58.06ms
step:2282/2330 train_time:132504ms step_avg:58.06ms
step:2283/2330 train_time:132560ms step_avg:58.06ms
step:2284/2330 train_time:132620ms step_avg:58.06ms
step:2285/2330 train_time:132676ms step_avg:58.06ms
step:2286/2330 train_time:132738ms step_avg:58.07ms
step:2287/2330 train_time:132795ms step_avg:58.07ms
step:2288/2330 train_time:132857ms step_avg:58.07ms
step:2289/2330 train_time:132914ms step_avg:58.07ms
step:2290/2330 train_time:132976ms step_avg:58.07ms
step:2291/2330 train_time:133034ms step_avg:58.07ms
step:2292/2330 train_time:133094ms step_avg:58.07ms
step:2293/2330 train_time:133151ms step_avg:58.07ms
step:2294/2330 train_time:133211ms step_avg:58.07ms
step:2295/2330 train_time:133267ms step_avg:58.07ms
step:2296/2330 train_time:133328ms step_avg:58.07ms
step:2297/2330 train_time:133384ms step_avg:58.07ms
step:2298/2330 train_time:133445ms step_avg:58.07ms
step:2299/2330 train_time:133501ms step_avg:58.07ms
step:2300/2330 train_time:133560ms step_avg:58.07ms
step:2301/2330 train_time:133617ms step_avg:58.07ms
step:2302/2330 train_time:133677ms step_avg:58.07ms
step:2303/2330 train_time:133734ms step_avg:58.07ms
step:2304/2330 train_time:133796ms step_avg:58.07ms
step:2305/2330 train_time:133852ms step_avg:58.07ms
step:2306/2330 train_time:133914ms step_avg:58.07ms
step:2307/2330 train_time:133970ms step_avg:58.07ms
step:2308/2330 train_time:134033ms step_avg:58.07ms
step:2309/2330 train_time:134090ms step_avg:58.07ms
step:2310/2330 train_time:134151ms step_avg:58.07ms
step:2311/2330 train_time:134207ms step_avg:58.07ms
step:2312/2330 train_time:134269ms step_avg:58.07ms
step:2313/2330 train_time:134325ms step_avg:58.07ms
step:2314/2330 train_time:134386ms step_avg:58.08ms
step:2315/2330 train_time:134442ms step_avg:58.07ms
step:2316/2330 train_time:134503ms step_avg:58.08ms
step:2317/2330 train_time:134559ms step_avg:58.07ms
step:2318/2330 train_time:134619ms step_avg:58.08ms
step:2319/2330 train_time:134676ms step_avg:58.08ms
step:2320/2330 train_time:134737ms step_avg:58.08ms
step:2321/2330 train_time:134794ms step_avg:58.08ms
step:2322/2330 train_time:134856ms step_avg:58.08ms
step:2323/2330 train_time:134913ms step_avg:58.08ms
step:2324/2330 train_time:134974ms step_avg:58.08ms
step:2325/2330 train_time:135032ms step_avg:58.08ms
step:2326/2330 train_time:135092ms step_avg:58.08ms
step:2327/2330 train_time:135149ms step_avg:58.08ms
step:2328/2330 train_time:135210ms step_avg:58.08ms
step:2329/2330 train_time:135266ms step_avg:58.08ms
step:2330/2330 train_time:135328ms step_avg:58.08ms
step:2330/2330 val_loss:4.9257 train_time:135411ms step_avg:58.12ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
