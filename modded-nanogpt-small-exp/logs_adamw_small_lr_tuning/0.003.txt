import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--adamw_lr', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_lr_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_lr_tuning/{args2.adamw_lr}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=float(args2.adamw_lr),  betas=(0.85, 0.85), eps=1e-10, weight_decay=0.0, fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_lr_tuning/{args2.adamw_lr}", exist_ok=True)
            torch.save(log, f"logs_adamw_small_lr_tuning/{args2.adamw_lr}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 01:48:14 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   27C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   26C    P0             116W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   25C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   27C    P0             113W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   25C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   26C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:86ms step_avg:86.24ms
step:2/2330 train_time:181ms step_avg:90.37ms
step:3/2330 train_time:199ms step_avg:66.37ms
step:4/2330 train_time:218ms step_avg:54.45ms
step:5/2330 train_time:271ms step_avg:54.28ms
step:6/2330 train_time:329ms step_avg:54.81ms
step:7/2330 train_time:383ms step_avg:54.73ms
step:8/2330 train_time:441ms step_avg:55.11ms
step:9/2330 train_time:496ms step_avg:55.06ms
step:10/2330 train_time:553ms step_avg:55.30ms
step:11/2330 train_time:607ms step_avg:55.19ms
step:12/2330 train_time:665ms step_avg:55.41ms
step:13/2330 train_time:719ms step_avg:55.33ms
step:14/2330 train_time:777ms step_avg:55.47ms
step:15/2330 train_time:831ms step_avg:55.41ms
step:16/2330 train_time:888ms step_avg:55.52ms
step:17/2330 train_time:943ms step_avg:55.45ms
step:18/2330 train_time:1000ms step_avg:55.57ms
step:19/2330 train_time:1056ms step_avg:55.55ms
step:20/2330 train_time:1116ms step_avg:55.81ms
step:21/2330 train_time:1172ms step_avg:55.81ms
step:22/2330 train_time:1233ms step_avg:56.03ms
step:23/2330 train_time:1288ms step_avg:55.98ms
step:24/2330 train_time:1346ms step_avg:56.07ms
step:25/2330 train_time:1400ms step_avg:56.01ms
step:26/2330 train_time:1459ms step_avg:56.10ms
step:27/2330 train_time:1514ms step_avg:56.06ms
step:28/2330 train_time:1571ms step_avg:56.11ms
step:29/2330 train_time:1625ms step_avg:56.05ms
step:30/2330 train_time:1684ms step_avg:56.15ms
step:31/2330 train_time:1739ms step_avg:56.10ms
step:32/2330 train_time:1797ms step_avg:56.16ms
step:33/2330 train_time:1852ms step_avg:56.12ms
step:34/2330 train_time:1909ms step_avg:56.16ms
step:35/2330 train_time:1965ms step_avg:56.14ms
step:36/2330 train_time:2024ms step_avg:56.22ms
step:37/2330 train_time:2079ms step_avg:56.19ms
step:38/2330 train_time:2139ms step_avg:56.29ms
step:39/2330 train_time:2195ms step_avg:56.29ms
step:40/2330 train_time:2254ms step_avg:56.36ms
step:41/2330 train_time:2310ms step_avg:56.35ms
step:42/2330 train_time:2368ms step_avg:56.39ms
step:43/2330 train_time:2424ms step_avg:56.36ms
step:44/2330 train_time:2481ms step_avg:56.40ms
step:45/2330 train_time:2537ms step_avg:56.37ms
step:46/2330 train_time:2595ms step_avg:56.41ms
step:47/2330 train_time:2650ms step_avg:56.38ms
step:48/2330 train_time:2707ms step_avg:56.40ms
step:49/2330 train_time:2762ms step_avg:56.36ms
step:50/2330 train_time:2820ms step_avg:56.41ms
step:51/2330 train_time:2875ms step_avg:56.38ms
step:52/2330 train_time:2934ms step_avg:56.42ms
step:53/2330 train_time:2989ms step_avg:56.39ms
step:54/2330 train_time:3048ms step_avg:56.44ms
step:55/2330 train_time:3103ms step_avg:56.42ms
step:56/2330 train_time:3162ms step_avg:56.47ms
step:57/2330 train_time:3218ms step_avg:56.45ms
step:58/2330 train_time:3277ms step_avg:56.49ms
step:59/2330 train_time:3332ms step_avg:56.48ms
step:60/2330 train_time:3391ms step_avg:56.51ms
step:61/2330 train_time:3446ms step_avg:56.49ms
step:62/2330 train_time:3504ms step_avg:56.51ms
step:63/2330 train_time:3559ms step_avg:56.50ms
step:64/2330 train_time:3618ms step_avg:56.52ms
step:65/2330 train_time:3673ms step_avg:56.51ms
step:66/2330 train_time:3731ms step_avg:56.53ms
step:67/2330 train_time:3786ms step_avg:56.51ms
step:68/2330 train_time:3844ms step_avg:56.53ms
step:69/2330 train_time:3899ms step_avg:56.51ms
step:70/2330 train_time:3958ms step_avg:56.54ms
step:71/2330 train_time:4013ms step_avg:56.53ms
step:72/2330 train_time:4072ms step_avg:56.56ms
step:73/2330 train_time:4128ms step_avg:56.54ms
step:74/2330 train_time:4186ms step_avg:56.57ms
step:75/2330 train_time:4242ms step_avg:56.55ms
step:76/2330 train_time:4301ms step_avg:56.59ms
step:77/2330 train_time:4357ms step_avg:56.58ms
step:78/2330 train_time:4415ms step_avg:56.60ms
step:79/2330 train_time:4470ms step_avg:56.59ms
step:80/2330 train_time:4529ms step_avg:56.61ms
step:81/2330 train_time:4584ms step_avg:56.59ms
step:82/2330 train_time:4642ms step_avg:56.62ms
step:83/2330 train_time:4699ms step_avg:56.61ms
step:84/2330 train_time:4757ms step_avg:56.63ms
step:85/2330 train_time:4812ms step_avg:56.62ms
step:86/2330 train_time:4870ms step_avg:56.63ms
step:87/2330 train_time:4926ms step_avg:56.62ms
step:88/2330 train_time:4984ms step_avg:56.64ms
step:89/2330 train_time:5039ms step_avg:56.62ms
step:90/2330 train_time:5098ms step_avg:56.64ms
step:91/2330 train_time:5153ms step_avg:56.63ms
step:92/2330 train_time:5212ms step_avg:56.65ms
step:93/2330 train_time:5267ms step_avg:56.63ms
step:94/2330 train_time:5326ms step_avg:56.66ms
step:95/2330 train_time:5381ms step_avg:56.64ms
step:96/2330 train_time:5440ms step_avg:56.67ms
step:97/2330 train_time:5496ms step_avg:56.66ms
step:98/2330 train_time:5554ms step_avg:56.68ms
step:99/2330 train_time:5610ms step_avg:56.66ms
step:100/2330 train_time:5669ms step_avg:56.69ms
step:101/2330 train_time:5724ms step_avg:56.67ms
step:102/2330 train_time:5783ms step_avg:56.70ms
step:103/2330 train_time:5838ms step_avg:56.68ms
step:104/2330 train_time:5896ms step_avg:56.70ms
step:105/2330 train_time:5952ms step_avg:56.68ms
step:106/2330 train_time:6010ms step_avg:56.70ms
step:107/2330 train_time:6065ms step_avg:56.68ms
step:108/2330 train_time:6124ms step_avg:56.70ms
step:109/2330 train_time:6179ms step_avg:56.69ms
step:110/2330 train_time:6238ms step_avg:56.71ms
step:111/2330 train_time:6294ms step_avg:56.70ms
step:112/2330 train_time:6352ms step_avg:56.72ms
step:113/2330 train_time:6407ms step_avg:56.70ms
step:114/2330 train_time:6466ms step_avg:56.72ms
step:115/2330 train_time:6521ms step_avg:56.70ms
step:116/2330 train_time:6581ms step_avg:56.73ms
step:117/2330 train_time:6635ms step_avg:56.71ms
step:118/2330 train_time:6694ms step_avg:56.73ms
step:119/2330 train_time:6749ms step_avg:56.71ms
step:120/2330 train_time:6808ms step_avg:56.73ms
step:121/2330 train_time:6863ms step_avg:56.72ms
step:122/2330 train_time:6922ms step_avg:56.74ms
step:123/2330 train_time:6977ms step_avg:56.72ms
step:124/2330 train_time:7036ms step_avg:56.74ms
step:125/2330 train_time:7091ms step_avg:56.72ms
step:126/2330 train_time:7149ms step_avg:56.74ms
step:127/2330 train_time:7205ms step_avg:56.73ms
step:128/2330 train_time:7263ms step_avg:56.74ms
step:129/2330 train_time:7319ms step_avg:56.73ms
step:130/2330 train_time:7377ms step_avg:56.75ms
step:131/2330 train_time:7432ms step_avg:56.74ms
step:132/2330 train_time:7491ms step_avg:56.75ms
step:133/2330 train_time:7546ms step_avg:56.74ms
step:134/2330 train_time:7605ms step_avg:56.75ms
step:135/2330 train_time:7660ms step_avg:56.74ms
step:136/2330 train_time:7719ms step_avg:56.76ms
step:137/2330 train_time:7774ms step_avg:56.75ms
step:138/2330 train_time:7834ms step_avg:56.77ms
step:139/2330 train_time:7889ms step_avg:56.75ms
step:140/2330 train_time:7948ms step_avg:56.77ms
step:141/2330 train_time:8002ms step_avg:56.76ms
step:142/2330 train_time:8061ms step_avg:56.77ms
step:143/2330 train_time:8117ms step_avg:56.76ms
step:144/2330 train_time:8176ms step_avg:56.77ms
step:145/2330 train_time:8231ms step_avg:56.77ms
step:146/2330 train_time:8290ms step_avg:56.78ms
step:147/2330 train_time:8345ms step_avg:56.77ms
step:148/2330 train_time:8403ms step_avg:56.78ms
step:149/2330 train_time:8458ms step_avg:56.77ms
step:150/2330 train_time:8517ms step_avg:56.78ms
step:151/2330 train_time:8573ms step_avg:56.78ms
step:152/2330 train_time:8631ms step_avg:56.78ms
step:153/2330 train_time:8686ms step_avg:56.77ms
step:154/2330 train_time:8745ms step_avg:56.79ms
step:155/2330 train_time:8800ms step_avg:56.78ms
step:156/2330 train_time:8861ms step_avg:56.80ms
step:157/2330 train_time:8918ms step_avg:56.80ms
step:158/2330 train_time:8976ms step_avg:56.81ms
step:159/2330 train_time:9032ms step_avg:56.80ms
step:160/2330 train_time:9090ms step_avg:56.81ms
step:161/2330 train_time:9145ms step_avg:56.80ms
step:162/2330 train_time:9203ms step_avg:56.81ms
step:163/2330 train_time:9258ms step_avg:56.80ms
step:164/2330 train_time:9316ms step_avg:56.81ms
step:165/2330 train_time:9371ms step_avg:56.80ms
step:166/2330 train_time:9430ms step_avg:56.81ms
step:167/2330 train_time:9485ms step_avg:56.79ms
step:168/2330 train_time:9544ms step_avg:56.81ms
step:169/2330 train_time:9598ms step_avg:56.80ms
step:170/2330 train_time:9657ms step_avg:56.81ms
step:171/2330 train_time:9713ms step_avg:56.80ms
step:172/2330 train_time:9771ms step_avg:56.81ms
step:173/2330 train_time:9826ms step_avg:56.80ms
step:174/2330 train_time:9886ms step_avg:56.81ms
step:175/2330 train_time:9940ms step_avg:56.80ms
step:176/2330 train_time:10000ms step_avg:56.82ms
step:177/2330 train_time:10055ms step_avg:56.81ms
step:178/2330 train_time:10113ms step_avg:56.81ms
step:179/2330 train_time:10168ms step_avg:56.80ms
step:180/2330 train_time:10226ms step_avg:56.81ms
step:181/2330 train_time:10281ms step_avg:56.80ms
step:182/2330 train_time:10341ms step_avg:56.82ms
step:183/2330 train_time:10397ms step_avg:56.81ms
step:184/2330 train_time:10455ms step_avg:56.82ms
step:185/2330 train_time:10510ms step_avg:56.81ms
step:186/2330 train_time:10568ms step_avg:56.82ms
step:187/2330 train_time:10623ms step_avg:56.81ms
step:188/2330 train_time:10681ms step_avg:56.81ms
step:189/2330 train_time:10737ms step_avg:56.81ms
step:190/2330 train_time:10795ms step_avg:56.82ms
step:191/2330 train_time:10851ms step_avg:56.81ms
step:192/2330 train_time:10910ms step_avg:56.82ms
step:193/2330 train_time:10965ms step_avg:56.82ms
step:194/2330 train_time:11024ms step_avg:56.82ms
step:195/2330 train_time:11079ms step_avg:56.82ms
step:196/2330 train_time:11138ms step_avg:56.83ms
step:197/2330 train_time:11193ms step_avg:56.82ms
step:198/2330 train_time:11251ms step_avg:56.82ms
step:199/2330 train_time:11306ms step_avg:56.82ms
step:200/2330 train_time:11366ms step_avg:56.83ms
step:201/2330 train_time:11421ms step_avg:56.82ms
step:202/2330 train_time:11479ms step_avg:56.83ms
step:203/2330 train_time:11535ms step_avg:56.82ms
step:204/2330 train_time:11593ms step_avg:56.83ms
step:205/2330 train_time:11648ms step_avg:56.82ms
step:206/2330 train_time:11706ms step_avg:56.83ms
step:207/2330 train_time:11762ms step_avg:56.82ms
step:208/2330 train_time:11821ms step_avg:56.83ms
step:209/2330 train_time:11876ms step_avg:56.82ms
step:210/2330 train_time:11935ms step_avg:56.83ms
step:211/2330 train_time:11990ms step_avg:56.82ms
step:212/2330 train_time:12049ms step_avg:56.83ms
step:213/2330 train_time:12104ms step_avg:56.83ms
step:214/2330 train_time:12163ms step_avg:56.84ms
step:215/2330 train_time:12219ms step_avg:56.83ms
step:216/2330 train_time:12277ms step_avg:56.84ms
step:217/2330 train_time:12332ms step_avg:56.83ms
step:218/2330 train_time:12390ms step_avg:56.84ms
step:219/2330 train_time:12445ms step_avg:56.83ms
step:220/2330 train_time:12504ms step_avg:56.83ms
step:221/2330 train_time:12559ms step_avg:56.83ms
step:222/2330 train_time:12618ms step_avg:56.84ms
step:223/2330 train_time:12674ms step_avg:56.83ms
step:224/2330 train_time:12732ms step_avg:56.84ms
step:225/2330 train_time:12787ms step_avg:56.83ms
step:226/2330 train_time:12845ms step_avg:56.84ms
step:227/2330 train_time:12900ms step_avg:56.83ms
step:228/2330 train_time:12959ms step_avg:56.84ms
step:229/2330 train_time:13015ms step_avg:56.83ms
step:230/2330 train_time:13073ms step_avg:56.84ms
step:231/2330 train_time:13129ms step_avg:56.83ms
step:232/2330 train_time:13187ms step_avg:56.84ms
step:233/2330 train_time:13242ms step_avg:56.83ms
step:234/2330 train_time:13301ms step_avg:56.84ms
step:235/2330 train_time:13357ms step_avg:56.84ms
step:236/2330 train_time:13415ms step_avg:56.84ms
step:237/2330 train_time:13470ms step_avg:56.84ms
step:238/2330 train_time:13529ms step_avg:56.85ms
step:239/2330 train_time:13584ms step_avg:56.84ms
step:240/2330 train_time:13643ms step_avg:56.85ms
step:241/2330 train_time:13699ms step_avg:56.84ms
step:242/2330 train_time:13758ms step_avg:56.85ms
step:243/2330 train_time:13813ms step_avg:56.84ms
step:244/2330 train_time:13871ms step_avg:56.85ms
step:245/2330 train_time:13927ms step_avg:56.84ms
step:246/2330 train_time:13985ms step_avg:56.85ms
step:247/2330 train_time:14040ms step_avg:56.84ms
step:248/2330 train_time:14100ms step_avg:56.85ms
step:249/2330 train_time:14155ms step_avg:56.85ms
step:250/2330 train_time:14213ms step_avg:56.85ms
step:250/2330 val_loss:4.9774 train_time:14291ms step_avg:57.16ms
step:251/2330 train_time:14308ms step_avg:57.01ms
step:252/2330 train_time:14329ms step_avg:56.86ms
step:253/2330 train_time:14385ms step_avg:56.86ms
step:254/2330 train_time:14447ms step_avg:56.88ms
step:255/2330 train_time:14503ms step_avg:56.87ms
step:256/2330 train_time:14562ms step_avg:56.88ms
step:257/2330 train_time:14617ms step_avg:56.88ms
step:258/2330 train_time:14676ms step_avg:56.89ms
step:259/2330 train_time:14731ms step_avg:56.88ms
step:260/2330 train_time:14790ms step_avg:56.89ms
step:261/2330 train_time:14845ms step_avg:56.88ms
step:262/2330 train_time:14903ms step_avg:56.88ms
step:263/2330 train_time:14958ms step_avg:56.87ms
step:264/2330 train_time:15016ms step_avg:56.88ms
step:265/2330 train_time:15071ms step_avg:56.87ms
step:266/2330 train_time:15129ms step_avg:56.88ms
step:267/2330 train_time:15184ms step_avg:56.87ms
step:268/2330 train_time:15242ms step_avg:56.87ms
step:269/2330 train_time:15298ms step_avg:56.87ms
step:270/2330 train_time:15358ms step_avg:56.88ms
step:271/2330 train_time:15414ms step_avg:56.88ms
step:272/2330 train_time:15474ms step_avg:56.89ms
step:273/2330 train_time:15530ms step_avg:56.89ms
step:274/2330 train_time:15590ms step_avg:56.90ms
step:275/2330 train_time:15645ms step_avg:56.89ms
step:276/2330 train_time:15704ms step_avg:56.90ms
step:277/2330 train_time:15759ms step_avg:56.89ms
step:278/2330 train_time:15817ms step_avg:56.90ms
step:279/2330 train_time:15873ms step_avg:56.89ms
step:280/2330 train_time:15931ms step_avg:56.90ms
step:281/2330 train_time:15986ms step_avg:56.89ms
step:282/2330 train_time:16044ms step_avg:56.90ms
step:283/2330 train_time:16100ms step_avg:56.89ms
step:284/2330 train_time:16158ms step_avg:56.89ms
step:285/2330 train_time:16213ms step_avg:56.89ms
step:286/2330 train_time:16271ms step_avg:56.89ms
step:287/2330 train_time:16326ms step_avg:56.89ms
step:288/2330 train_time:16386ms step_avg:56.90ms
step:289/2330 train_time:16442ms step_avg:56.89ms
step:290/2330 train_time:16501ms step_avg:56.90ms
step:291/2330 train_time:16558ms step_avg:56.90ms
step:292/2330 train_time:16616ms step_avg:56.91ms
step:293/2330 train_time:16672ms step_avg:56.90ms
step:294/2330 train_time:16731ms step_avg:56.91ms
step:295/2330 train_time:16787ms step_avg:56.90ms
step:296/2330 train_time:16845ms step_avg:56.91ms
step:297/2330 train_time:16900ms step_avg:56.90ms
step:298/2330 train_time:16959ms step_avg:56.91ms
step:299/2330 train_time:17014ms step_avg:56.90ms
step:300/2330 train_time:17072ms step_avg:56.91ms
step:301/2330 train_time:17128ms step_avg:56.90ms
step:302/2330 train_time:17187ms step_avg:56.91ms
step:303/2330 train_time:17242ms step_avg:56.90ms
step:304/2330 train_time:17300ms step_avg:56.91ms
step:305/2330 train_time:17355ms step_avg:56.90ms
step:306/2330 train_time:17414ms step_avg:56.91ms
step:307/2330 train_time:17470ms step_avg:56.90ms
step:308/2330 train_time:17529ms step_avg:56.91ms
step:309/2330 train_time:17584ms step_avg:56.91ms
step:310/2330 train_time:17643ms step_avg:56.91ms
step:311/2330 train_time:17699ms step_avg:56.91ms
step:312/2330 train_time:17758ms step_avg:56.92ms
step:313/2330 train_time:17814ms step_avg:56.91ms
step:314/2330 train_time:17872ms step_avg:56.92ms
step:315/2330 train_time:17928ms step_avg:56.91ms
step:316/2330 train_time:17986ms step_avg:56.92ms
step:317/2330 train_time:18041ms step_avg:56.91ms
step:318/2330 train_time:18100ms step_avg:56.92ms
step:319/2330 train_time:18156ms step_avg:56.91ms
step:320/2330 train_time:18213ms step_avg:56.92ms
step:321/2330 train_time:18269ms step_avg:56.91ms
step:322/2330 train_time:18327ms step_avg:56.92ms
step:323/2330 train_time:18382ms step_avg:56.91ms
step:324/2330 train_time:18442ms step_avg:56.92ms
step:325/2330 train_time:18497ms step_avg:56.91ms
step:326/2330 train_time:18556ms step_avg:56.92ms
step:327/2330 train_time:18611ms step_avg:56.92ms
step:328/2330 train_time:18670ms step_avg:56.92ms
step:329/2330 train_time:18725ms step_avg:56.91ms
step:330/2330 train_time:18784ms step_avg:56.92ms
step:331/2330 train_time:18840ms step_avg:56.92ms
step:332/2330 train_time:18899ms step_avg:56.93ms
step:333/2330 train_time:18955ms step_avg:56.92ms
step:334/2330 train_time:19014ms step_avg:56.93ms
step:335/2330 train_time:19070ms step_avg:56.92ms
step:336/2330 train_time:19128ms step_avg:56.93ms
step:337/2330 train_time:19184ms step_avg:56.93ms
step:338/2330 train_time:19242ms step_avg:56.93ms
step:339/2330 train_time:19297ms step_avg:56.92ms
step:340/2330 train_time:19356ms step_avg:56.93ms
step:341/2330 train_time:19411ms step_avg:56.92ms
step:342/2330 train_time:19469ms step_avg:56.93ms
step:343/2330 train_time:19524ms step_avg:56.92ms
step:344/2330 train_time:19584ms step_avg:56.93ms
step:345/2330 train_time:19639ms step_avg:56.93ms
step:346/2330 train_time:19699ms step_avg:56.93ms
step:347/2330 train_time:19755ms step_avg:56.93ms
step:348/2330 train_time:19814ms step_avg:56.94ms
step:349/2330 train_time:19869ms step_avg:56.93ms
step:350/2330 train_time:19928ms step_avg:56.94ms
step:351/2330 train_time:19983ms step_avg:56.93ms
step:352/2330 train_time:20042ms step_avg:56.94ms
step:353/2330 train_time:20098ms step_avg:56.93ms
step:354/2330 train_time:20156ms step_avg:56.94ms
step:355/2330 train_time:20211ms step_avg:56.93ms
step:356/2330 train_time:20270ms step_avg:56.94ms
step:357/2330 train_time:20326ms step_avg:56.93ms
step:358/2330 train_time:20384ms step_avg:56.94ms
step:359/2330 train_time:20439ms step_avg:56.93ms
step:360/2330 train_time:20498ms step_avg:56.94ms
step:361/2330 train_time:20553ms step_avg:56.93ms
step:362/2330 train_time:20612ms step_avg:56.94ms
step:363/2330 train_time:20667ms step_avg:56.94ms
step:364/2330 train_time:20727ms step_avg:56.94ms
step:365/2330 train_time:20782ms step_avg:56.94ms
step:366/2330 train_time:20842ms step_avg:56.94ms
step:367/2330 train_time:20897ms step_avg:56.94ms
step:368/2330 train_time:20956ms step_avg:56.95ms
step:369/2330 train_time:21012ms step_avg:56.94ms
step:370/2330 train_time:21070ms step_avg:56.95ms
step:371/2330 train_time:21126ms step_avg:56.94ms
step:372/2330 train_time:21185ms step_avg:56.95ms
step:373/2330 train_time:21240ms step_avg:56.94ms
step:374/2330 train_time:21299ms step_avg:56.95ms
step:375/2330 train_time:21354ms step_avg:56.94ms
step:376/2330 train_time:21412ms step_avg:56.95ms
step:377/2330 train_time:21468ms step_avg:56.94ms
step:378/2330 train_time:21527ms step_avg:56.95ms
step:379/2330 train_time:21582ms step_avg:56.94ms
step:380/2330 train_time:21642ms step_avg:56.95ms
step:381/2330 train_time:21697ms step_avg:56.95ms
step:382/2330 train_time:21756ms step_avg:56.95ms
step:383/2330 train_time:21810ms step_avg:56.95ms
step:384/2330 train_time:21870ms step_avg:56.95ms
step:385/2330 train_time:21925ms step_avg:56.95ms
step:386/2330 train_time:21985ms step_avg:56.96ms
step:387/2330 train_time:22040ms step_avg:56.95ms
step:388/2330 train_time:22099ms step_avg:56.96ms
step:389/2330 train_time:22155ms step_avg:56.95ms
step:390/2330 train_time:22213ms step_avg:56.96ms
step:391/2330 train_time:22269ms step_avg:56.95ms
step:392/2330 train_time:22327ms step_avg:56.96ms
step:393/2330 train_time:22383ms step_avg:56.95ms
step:394/2330 train_time:22441ms step_avg:56.96ms
step:395/2330 train_time:22497ms step_avg:56.95ms
step:396/2330 train_time:22556ms step_avg:56.96ms
step:397/2330 train_time:22611ms step_avg:56.95ms
step:398/2330 train_time:22670ms step_avg:56.96ms
step:399/2330 train_time:22725ms step_avg:56.96ms
step:400/2330 train_time:22785ms step_avg:56.96ms
step:401/2330 train_time:22840ms step_avg:56.96ms
step:402/2330 train_time:22899ms step_avg:56.96ms
step:403/2330 train_time:22955ms step_avg:56.96ms
step:404/2330 train_time:23013ms step_avg:56.96ms
step:405/2330 train_time:23069ms step_avg:56.96ms
step:406/2330 train_time:23128ms step_avg:56.97ms
step:407/2330 train_time:23183ms step_avg:56.96ms
step:408/2330 train_time:23243ms step_avg:56.97ms
step:409/2330 train_time:23299ms step_avg:56.96ms
step:410/2330 train_time:23356ms step_avg:56.97ms
step:411/2330 train_time:23411ms step_avg:56.96ms
step:412/2330 train_time:23470ms step_avg:56.97ms
step:413/2330 train_time:23526ms step_avg:56.96ms
step:414/2330 train_time:23585ms step_avg:56.97ms
step:415/2330 train_time:23640ms step_avg:56.96ms
step:416/2330 train_time:23699ms step_avg:56.97ms
step:417/2330 train_time:23755ms step_avg:56.97ms
step:418/2330 train_time:23814ms step_avg:56.97ms
step:419/2330 train_time:23869ms step_avg:56.97ms
step:420/2330 train_time:23928ms step_avg:56.97ms
step:421/2330 train_time:23983ms step_avg:56.97ms
step:422/2330 train_time:24042ms step_avg:56.97ms
step:423/2330 train_time:24098ms step_avg:56.97ms
step:424/2330 train_time:24156ms step_avg:56.97ms
step:425/2330 train_time:24211ms step_avg:56.97ms
step:426/2330 train_time:24270ms step_avg:56.97ms
step:427/2330 train_time:24326ms step_avg:56.97ms
step:428/2330 train_time:24386ms step_avg:56.98ms
step:429/2330 train_time:24441ms step_avg:56.97ms
step:430/2330 train_time:24500ms step_avg:56.98ms
step:431/2330 train_time:24556ms step_avg:56.97ms
step:432/2330 train_time:24614ms step_avg:56.98ms
step:433/2330 train_time:24670ms step_avg:56.97ms
step:434/2330 train_time:24729ms step_avg:56.98ms
step:435/2330 train_time:24785ms step_avg:56.98ms
step:436/2330 train_time:24843ms step_avg:56.98ms
step:437/2330 train_time:24899ms step_avg:56.98ms
step:438/2330 train_time:24958ms step_avg:56.98ms
step:439/2330 train_time:25013ms step_avg:56.98ms
step:440/2330 train_time:25072ms step_avg:56.98ms
step:441/2330 train_time:25127ms step_avg:56.98ms
step:442/2330 train_time:25186ms step_avg:56.98ms
step:443/2330 train_time:25241ms step_avg:56.98ms
step:444/2330 train_time:25300ms step_avg:56.98ms
step:445/2330 train_time:25356ms step_avg:56.98ms
step:446/2330 train_time:25414ms step_avg:56.98ms
step:447/2330 train_time:25469ms step_avg:56.98ms
step:448/2330 train_time:25528ms step_avg:56.98ms
step:449/2330 train_time:25583ms step_avg:56.98ms
step:450/2330 train_time:25642ms step_avg:56.98ms
step:451/2330 train_time:25698ms step_avg:56.98ms
step:452/2330 train_time:25757ms step_avg:56.99ms
step:453/2330 train_time:25813ms step_avg:56.98ms
step:454/2330 train_time:25872ms step_avg:56.99ms
step:455/2330 train_time:25927ms step_avg:56.98ms
step:456/2330 train_time:25987ms step_avg:56.99ms
step:457/2330 train_time:26042ms step_avg:56.99ms
step:458/2330 train_time:26102ms step_avg:56.99ms
step:459/2330 train_time:26157ms step_avg:56.99ms
step:460/2330 train_time:26216ms step_avg:56.99ms
step:461/2330 train_time:26271ms step_avg:56.99ms
step:462/2330 train_time:26330ms step_avg:56.99ms
step:463/2330 train_time:26385ms step_avg:56.99ms
step:464/2330 train_time:26443ms step_avg:56.99ms
step:465/2330 train_time:26499ms step_avg:56.99ms
step:466/2330 train_time:26558ms step_avg:56.99ms
step:467/2330 train_time:26613ms step_avg:56.99ms
step:468/2330 train_time:26674ms step_avg:56.99ms
step:469/2330 train_time:26729ms step_avg:56.99ms
step:470/2330 train_time:26788ms step_avg:57.00ms
step:471/2330 train_time:26844ms step_avg:56.99ms
step:472/2330 train_time:26904ms step_avg:57.00ms
step:473/2330 train_time:26959ms step_avg:57.00ms
step:474/2330 train_time:27017ms step_avg:57.00ms
step:475/2330 train_time:27073ms step_avg:57.00ms
step:476/2330 train_time:27131ms step_avg:57.00ms
step:477/2330 train_time:27187ms step_avg:57.00ms
step:478/2330 train_time:27246ms step_avg:57.00ms
step:479/2330 train_time:27301ms step_avg:57.00ms
step:480/2330 train_time:27361ms step_avg:57.00ms
step:481/2330 train_time:27416ms step_avg:57.00ms
step:482/2330 train_time:27476ms step_avg:57.00ms
step:483/2330 train_time:27531ms step_avg:57.00ms
step:484/2330 train_time:27591ms step_avg:57.01ms
step:485/2330 train_time:27646ms step_avg:57.00ms
step:486/2330 train_time:27705ms step_avg:57.01ms
step:487/2330 train_time:27760ms step_avg:57.00ms
step:488/2330 train_time:27822ms step_avg:57.01ms
step:489/2330 train_time:27878ms step_avg:57.01ms
step:490/2330 train_time:27936ms step_avg:57.01ms
step:491/2330 train_time:27992ms step_avg:57.01ms
step:492/2330 train_time:28050ms step_avg:57.01ms
step:493/2330 train_time:28105ms step_avg:57.01ms
step:494/2330 train_time:28164ms step_avg:57.01ms
step:495/2330 train_time:28220ms step_avg:57.01ms
step:496/2330 train_time:28278ms step_avg:57.01ms
step:497/2330 train_time:28334ms step_avg:57.01ms
step:498/2330 train_time:28392ms step_avg:57.01ms
step:499/2330 train_time:28447ms step_avg:57.01ms
step:500/2330 train_time:28506ms step_avg:57.01ms
step:500/2330 val_loss:4.4679 train_time:28585ms step_avg:57.17ms
step:501/2330 train_time:28603ms step_avg:57.09ms
step:502/2330 train_time:28623ms step_avg:57.02ms
step:503/2330 train_time:28681ms step_avg:57.02ms
step:504/2330 train_time:28743ms step_avg:57.03ms
step:505/2330 train_time:28800ms step_avg:57.03ms
step:506/2330 train_time:28859ms step_avg:57.03ms
step:507/2330 train_time:28914ms step_avg:57.03ms
step:508/2330 train_time:28974ms step_avg:57.04ms
step:509/2330 train_time:29029ms step_avg:57.03ms
step:510/2330 train_time:29087ms step_avg:57.03ms
step:511/2330 train_time:29142ms step_avg:57.03ms
step:512/2330 train_time:29201ms step_avg:57.03ms
step:513/2330 train_time:29256ms step_avg:57.03ms
step:514/2330 train_time:29314ms step_avg:57.03ms
step:515/2330 train_time:29368ms step_avg:57.03ms
step:516/2330 train_time:29427ms step_avg:57.03ms
step:517/2330 train_time:29482ms step_avg:57.03ms
step:518/2330 train_time:29541ms step_avg:57.03ms
step:519/2330 train_time:29596ms step_avg:57.03ms
step:520/2330 train_time:29656ms step_avg:57.03ms
step:521/2330 train_time:29711ms step_avg:57.03ms
step:522/2330 train_time:29773ms step_avg:57.04ms
step:523/2330 train_time:29829ms step_avg:57.03ms
step:524/2330 train_time:29889ms step_avg:57.04ms
step:525/2330 train_time:29944ms step_avg:57.04ms
step:526/2330 train_time:30004ms step_avg:57.04ms
step:527/2330 train_time:30059ms step_avg:57.04ms
step:528/2330 train_time:30118ms step_avg:57.04ms
step:529/2330 train_time:30173ms step_avg:57.04ms
step:530/2330 train_time:30232ms step_avg:57.04ms
step:531/2330 train_time:30287ms step_avg:57.04ms
step:532/2330 train_time:30346ms step_avg:57.04ms
step:533/2330 train_time:30401ms step_avg:57.04ms
step:534/2330 train_time:30459ms step_avg:57.04ms
step:535/2330 train_time:30514ms step_avg:57.04ms
step:536/2330 train_time:30573ms step_avg:57.04ms
step:537/2330 train_time:30629ms step_avg:57.04ms
step:538/2330 train_time:30688ms step_avg:57.04ms
step:539/2330 train_time:30744ms step_avg:57.04ms
step:540/2330 train_time:30804ms step_avg:57.05ms
step:541/2330 train_time:30860ms step_avg:57.04ms
step:542/2330 train_time:30921ms step_avg:57.05ms
step:543/2330 train_time:30977ms step_avg:57.05ms
step:544/2330 train_time:31036ms step_avg:57.05ms
step:545/2330 train_time:31091ms step_avg:57.05ms
step:546/2330 train_time:31150ms step_avg:57.05ms
step:547/2330 train_time:31205ms step_avg:57.05ms
step:548/2330 train_time:31264ms step_avg:57.05ms
step:549/2330 train_time:31320ms step_avg:57.05ms
step:550/2330 train_time:31378ms step_avg:57.05ms
step:551/2330 train_time:31434ms step_avg:57.05ms
step:552/2330 train_time:31492ms step_avg:57.05ms
step:553/2330 train_time:31547ms step_avg:57.05ms
step:554/2330 train_time:31606ms step_avg:57.05ms
step:555/2330 train_time:31662ms step_avg:57.05ms
step:556/2330 train_time:31722ms step_avg:57.05ms
step:557/2330 train_time:31777ms step_avg:57.05ms
step:558/2330 train_time:31837ms step_avg:57.06ms
step:559/2330 train_time:31893ms step_avg:57.05ms
step:560/2330 train_time:31952ms step_avg:57.06ms
step:561/2330 train_time:32008ms step_avg:57.05ms
step:562/2330 train_time:32067ms step_avg:57.06ms
step:563/2330 train_time:32123ms step_avg:57.06ms
step:564/2330 train_time:32182ms step_avg:57.06ms
step:565/2330 train_time:32237ms step_avg:57.06ms
step:566/2330 train_time:32296ms step_avg:57.06ms
step:567/2330 train_time:32351ms step_avg:57.06ms
step:568/2330 train_time:32410ms step_avg:57.06ms
step:569/2330 train_time:32465ms step_avg:57.06ms
step:570/2330 train_time:32524ms step_avg:57.06ms
step:571/2330 train_time:32580ms step_avg:57.06ms
step:572/2330 train_time:32639ms step_avg:57.06ms
step:573/2330 train_time:32694ms step_avg:57.06ms
step:574/2330 train_time:32753ms step_avg:57.06ms
step:575/2330 train_time:32809ms step_avg:57.06ms
step:576/2330 train_time:32868ms step_avg:57.06ms
step:577/2330 train_time:32923ms step_avg:57.06ms
step:578/2330 train_time:32983ms step_avg:57.06ms
step:579/2330 train_time:33038ms step_avg:57.06ms
step:580/2330 train_time:33098ms step_avg:57.07ms
step:581/2330 train_time:33153ms step_avg:57.06ms
step:582/2330 train_time:33212ms step_avg:57.07ms
step:583/2330 train_time:33268ms step_avg:57.06ms
step:584/2330 train_time:33327ms step_avg:57.07ms
step:585/2330 train_time:33383ms step_avg:57.06ms
step:586/2330 train_time:33441ms step_avg:57.07ms
step:587/2330 train_time:33498ms step_avg:57.07ms
step:588/2330 train_time:33557ms step_avg:57.07ms
step:589/2330 train_time:33612ms step_avg:57.07ms
step:590/2330 train_time:33671ms step_avg:57.07ms
step:591/2330 train_time:33728ms step_avg:57.07ms
step:592/2330 train_time:33786ms step_avg:57.07ms
step:593/2330 train_time:33842ms step_avg:57.07ms
step:594/2330 train_time:33901ms step_avg:57.07ms
step:595/2330 train_time:33957ms step_avg:57.07ms
step:596/2330 train_time:34015ms step_avg:57.07ms
step:597/2330 train_time:34071ms step_avg:57.07ms
step:598/2330 train_time:34130ms step_avg:57.07ms
step:599/2330 train_time:34185ms step_avg:57.07ms
step:600/2330 train_time:34244ms step_avg:57.07ms
step:601/2330 train_time:34299ms step_avg:57.07ms
step:602/2330 train_time:34357ms step_avg:57.07ms
step:603/2330 train_time:34413ms step_avg:57.07ms
step:604/2330 train_time:34471ms step_avg:57.07ms
step:605/2330 train_time:34528ms step_avg:57.07ms
step:606/2330 train_time:34586ms step_avg:57.07ms
step:607/2330 train_time:34642ms step_avg:57.07ms
step:608/2330 train_time:34701ms step_avg:57.07ms
step:609/2330 train_time:34756ms step_avg:57.07ms
step:610/2330 train_time:34815ms step_avg:57.07ms
step:611/2330 train_time:34870ms step_avg:57.07ms
step:612/2330 train_time:34930ms step_avg:57.08ms
step:613/2330 train_time:34985ms step_avg:57.07ms
step:614/2330 train_time:35044ms step_avg:57.08ms
step:615/2330 train_time:35100ms step_avg:57.07ms
step:616/2330 train_time:35159ms step_avg:57.08ms
step:617/2330 train_time:35215ms step_avg:57.07ms
step:618/2330 train_time:35274ms step_avg:57.08ms
step:619/2330 train_time:35329ms step_avg:57.07ms
step:620/2330 train_time:35388ms step_avg:57.08ms
step:621/2330 train_time:35443ms step_avg:57.07ms
step:622/2330 train_time:35503ms step_avg:57.08ms
step:623/2330 train_time:35559ms step_avg:57.08ms
step:624/2330 train_time:35617ms step_avg:57.08ms
step:625/2330 train_time:35673ms step_avg:57.08ms
step:626/2330 train_time:35732ms step_avg:57.08ms
step:627/2330 train_time:35788ms step_avg:57.08ms
step:628/2330 train_time:35847ms step_avg:57.08ms
step:629/2330 train_time:35903ms step_avg:57.08ms
step:630/2330 train_time:35962ms step_avg:57.08ms
step:631/2330 train_time:36018ms step_avg:57.08ms
step:632/2330 train_time:36076ms step_avg:57.08ms
step:633/2330 train_time:36132ms step_avg:57.08ms
step:634/2330 train_time:36190ms step_avg:57.08ms
step:635/2330 train_time:36245ms step_avg:57.08ms
step:636/2330 train_time:36304ms step_avg:57.08ms
step:637/2330 train_time:36360ms step_avg:57.08ms
step:638/2330 train_time:36419ms step_avg:57.08ms
step:639/2330 train_time:36475ms step_avg:57.08ms
step:640/2330 train_time:36534ms step_avg:57.08ms
step:641/2330 train_time:36589ms step_avg:57.08ms
step:642/2330 train_time:36648ms step_avg:57.08ms
step:643/2330 train_time:36704ms step_avg:57.08ms
step:644/2330 train_time:36762ms step_avg:57.08ms
step:645/2330 train_time:36819ms step_avg:57.08ms
step:646/2330 train_time:36878ms step_avg:57.09ms
step:647/2330 train_time:36933ms step_avg:57.08ms
step:648/2330 train_time:36992ms step_avg:57.09ms
step:649/2330 train_time:37047ms step_avg:57.08ms
step:650/2330 train_time:37107ms step_avg:57.09ms
step:651/2330 train_time:37162ms step_avg:57.08ms
step:652/2330 train_time:37222ms step_avg:57.09ms
step:653/2330 train_time:37277ms step_avg:57.09ms
step:654/2330 train_time:37337ms step_avg:57.09ms
step:655/2330 train_time:37392ms step_avg:57.09ms
step:656/2330 train_time:37450ms step_avg:57.09ms
step:657/2330 train_time:37506ms step_avg:57.09ms
step:658/2330 train_time:37565ms step_avg:57.09ms
step:659/2330 train_time:37621ms step_avg:57.09ms
step:660/2330 train_time:37680ms step_avg:57.09ms
step:661/2330 train_time:37736ms step_avg:57.09ms
step:662/2330 train_time:37794ms step_avg:57.09ms
step:663/2330 train_time:37850ms step_avg:57.09ms
step:664/2330 train_time:37909ms step_avg:57.09ms
step:665/2330 train_time:37964ms step_avg:57.09ms
step:666/2330 train_time:38024ms step_avg:57.09ms
step:667/2330 train_time:38080ms step_avg:57.09ms
step:668/2330 train_time:38138ms step_avg:57.09ms
step:669/2330 train_time:38194ms step_avg:57.09ms
step:670/2330 train_time:38253ms step_avg:57.09ms
step:671/2330 train_time:38308ms step_avg:57.09ms
step:672/2330 train_time:38367ms step_avg:57.09ms
step:673/2330 train_time:38423ms step_avg:57.09ms
step:674/2330 train_time:38482ms step_avg:57.10ms
step:675/2330 train_time:38538ms step_avg:57.09ms
step:676/2330 train_time:38597ms step_avg:57.10ms
step:677/2330 train_time:38653ms step_avg:57.09ms
step:678/2330 train_time:38712ms step_avg:57.10ms
step:679/2330 train_time:38768ms step_avg:57.10ms
step:680/2330 train_time:38826ms step_avg:57.10ms
step:681/2330 train_time:38882ms step_avg:57.10ms
step:682/2330 train_time:38941ms step_avg:57.10ms
step:683/2330 train_time:38997ms step_avg:57.10ms
step:684/2330 train_time:39056ms step_avg:57.10ms
step:685/2330 train_time:39112ms step_avg:57.10ms
step:686/2330 train_time:39170ms step_avg:57.10ms
step:687/2330 train_time:39226ms step_avg:57.10ms
step:688/2330 train_time:39285ms step_avg:57.10ms
step:689/2330 train_time:39341ms step_avg:57.10ms
step:690/2330 train_time:39400ms step_avg:57.10ms
step:691/2330 train_time:39456ms step_avg:57.10ms
step:692/2330 train_time:39514ms step_avg:57.10ms
step:693/2330 train_time:39570ms step_avg:57.10ms
step:694/2330 train_time:39629ms step_avg:57.10ms
step:695/2330 train_time:39684ms step_avg:57.10ms
step:696/2330 train_time:39744ms step_avg:57.10ms
step:697/2330 train_time:39801ms step_avg:57.10ms
step:698/2330 train_time:39859ms step_avg:57.10ms
step:699/2330 train_time:39915ms step_avg:57.10ms
step:700/2330 train_time:39973ms step_avg:57.10ms
step:701/2330 train_time:40028ms step_avg:57.10ms
step:702/2330 train_time:40088ms step_avg:57.10ms
step:703/2330 train_time:40143ms step_avg:57.10ms
step:704/2330 train_time:40202ms step_avg:57.11ms
step:705/2330 train_time:40258ms step_avg:57.10ms
step:706/2330 train_time:40317ms step_avg:57.11ms
step:707/2330 train_time:40373ms step_avg:57.10ms
step:708/2330 train_time:40432ms step_avg:57.11ms
step:709/2330 train_time:40488ms step_avg:57.11ms
step:710/2330 train_time:40547ms step_avg:57.11ms
step:711/2330 train_time:40603ms step_avg:57.11ms
step:712/2330 train_time:40662ms step_avg:57.11ms
step:713/2330 train_time:40717ms step_avg:57.11ms
step:714/2330 train_time:40777ms step_avg:57.11ms
step:715/2330 train_time:40832ms step_avg:57.11ms
step:716/2330 train_time:40891ms step_avg:57.11ms
step:717/2330 train_time:40946ms step_avg:57.11ms
step:718/2330 train_time:41005ms step_avg:57.11ms
step:719/2330 train_time:41061ms step_avg:57.11ms
step:720/2330 train_time:41121ms step_avg:57.11ms
step:721/2330 train_time:41176ms step_avg:57.11ms
step:722/2330 train_time:41236ms step_avg:57.11ms
step:723/2330 train_time:41291ms step_avg:57.11ms
step:724/2330 train_time:41350ms step_avg:57.11ms
step:725/2330 train_time:41406ms step_avg:57.11ms
step:726/2330 train_time:41465ms step_avg:57.11ms
step:727/2330 train_time:41521ms step_avg:57.11ms
step:728/2330 train_time:41581ms step_avg:57.12ms
step:729/2330 train_time:41637ms step_avg:57.12ms
step:730/2330 train_time:41695ms step_avg:57.12ms
step:731/2330 train_time:41751ms step_avg:57.11ms
step:732/2330 train_time:41809ms step_avg:57.12ms
step:733/2330 train_time:41865ms step_avg:57.12ms
step:734/2330 train_time:41925ms step_avg:57.12ms
step:735/2330 train_time:41981ms step_avg:57.12ms
step:736/2330 train_time:42040ms step_avg:57.12ms
step:737/2330 train_time:42096ms step_avg:57.12ms
step:738/2330 train_time:42154ms step_avg:57.12ms
step:739/2330 train_time:42210ms step_avg:57.12ms
step:740/2330 train_time:42269ms step_avg:57.12ms
step:741/2330 train_time:42324ms step_avg:57.12ms
step:742/2330 train_time:42384ms step_avg:57.12ms
step:743/2330 train_time:42440ms step_avg:57.12ms
step:744/2330 train_time:42499ms step_avg:57.12ms
step:745/2330 train_time:42554ms step_avg:57.12ms
step:746/2330 train_time:42613ms step_avg:57.12ms
step:747/2330 train_time:42668ms step_avg:57.12ms
step:748/2330 train_time:42727ms step_avg:57.12ms
step:749/2330 train_time:42783ms step_avg:57.12ms
step:750/2330 train_time:42842ms step_avg:57.12ms
step:750/2330 val_loss:4.2685 train_time:42921ms step_avg:57.23ms
step:751/2330 train_time:42937ms step_avg:57.17ms
step:752/2330 train_time:42959ms step_avg:57.13ms
step:753/2330 train_time:43015ms step_avg:57.12ms
step:754/2330 train_time:43077ms step_avg:57.13ms
step:755/2330 train_time:43133ms step_avg:57.13ms
step:756/2330 train_time:43193ms step_avg:57.13ms
step:757/2330 train_time:43249ms step_avg:57.13ms
step:758/2330 train_time:43308ms step_avg:57.13ms
step:759/2330 train_time:43364ms step_avg:57.13ms
step:760/2330 train_time:43422ms step_avg:57.13ms
step:761/2330 train_time:43478ms step_avg:57.13ms
step:762/2330 train_time:43535ms step_avg:57.13ms
step:763/2330 train_time:43591ms step_avg:57.13ms
step:764/2330 train_time:43650ms step_avg:57.13ms
step:765/2330 train_time:43706ms step_avg:57.13ms
step:766/2330 train_time:43764ms step_avg:57.13ms
step:767/2330 train_time:43820ms step_avg:57.13ms
step:768/2330 train_time:43880ms step_avg:57.13ms
step:769/2330 train_time:43938ms step_avg:57.14ms
step:770/2330 train_time:43997ms step_avg:57.14ms
step:771/2330 train_time:44055ms step_avg:57.14ms
step:772/2330 train_time:44115ms step_avg:57.14ms
step:773/2330 train_time:44171ms step_avg:57.14ms
step:774/2330 train_time:44232ms step_avg:57.15ms
step:775/2330 train_time:44288ms step_avg:57.15ms
step:776/2330 train_time:44347ms step_avg:57.15ms
step:777/2330 train_time:44404ms step_avg:57.15ms
step:778/2330 train_time:44463ms step_avg:57.15ms
step:779/2330 train_time:44520ms step_avg:57.15ms
step:780/2330 train_time:44580ms step_avg:57.15ms
step:781/2330 train_time:44636ms step_avg:57.15ms
step:782/2330 train_time:44695ms step_avg:57.15ms
step:783/2330 train_time:44751ms step_avg:57.15ms
step:784/2330 train_time:44810ms step_avg:57.16ms
step:785/2330 train_time:44867ms step_avg:57.16ms
step:786/2330 train_time:44926ms step_avg:57.16ms
step:787/2330 train_time:44983ms step_avg:57.16ms
step:788/2330 train_time:45044ms step_avg:57.16ms
step:789/2330 train_time:45100ms step_avg:57.16ms
step:790/2330 train_time:45162ms step_avg:57.17ms
step:791/2330 train_time:45220ms step_avg:57.17ms
step:792/2330 train_time:45279ms step_avg:57.17ms
step:793/2330 train_time:45337ms step_avg:57.17ms
step:794/2330 train_time:45396ms step_avg:57.17ms
step:795/2330 train_time:45453ms step_avg:57.17ms
step:796/2330 train_time:45512ms step_avg:57.18ms
step:797/2330 train_time:45568ms step_avg:57.17ms
step:798/2330 train_time:45627ms step_avg:57.18ms
step:799/2330 train_time:45683ms step_avg:57.18ms
step:800/2330 train_time:45743ms step_avg:57.18ms
step:801/2330 train_time:45799ms step_avg:57.18ms
step:802/2330 train_time:45859ms step_avg:57.18ms
step:803/2330 train_time:45916ms step_avg:57.18ms
step:804/2330 train_time:45976ms step_avg:57.18ms
step:805/2330 train_time:46032ms step_avg:57.18ms
step:806/2330 train_time:46092ms step_avg:57.19ms
step:807/2330 train_time:46149ms step_avg:57.19ms
step:808/2330 train_time:46209ms step_avg:57.19ms
step:809/2330 train_time:46265ms step_avg:57.19ms
step:810/2330 train_time:46325ms step_avg:57.19ms
step:811/2330 train_time:46381ms step_avg:57.19ms
step:812/2330 train_time:46442ms step_avg:57.19ms
step:813/2330 train_time:46499ms step_avg:57.19ms
step:814/2330 train_time:46559ms step_avg:57.20ms
step:815/2330 train_time:46616ms step_avg:57.20ms
step:816/2330 train_time:46675ms step_avg:57.20ms
step:817/2330 train_time:46731ms step_avg:57.20ms
step:818/2330 train_time:46790ms step_avg:57.20ms
step:819/2330 train_time:46846ms step_avg:57.20ms
step:820/2330 train_time:46906ms step_avg:57.20ms
step:821/2330 train_time:46962ms step_avg:57.20ms
step:822/2330 train_time:47022ms step_avg:57.20ms
step:823/2330 train_time:47079ms step_avg:57.20ms
step:824/2330 train_time:47139ms step_avg:57.21ms
step:825/2330 train_time:47197ms step_avg:57.21ms
step:826/2330 train_time:47257ms step_avg:57.21ms
step:827/2330 train_time:47315ms step_avg:57.21ms
step:828/2330 train_time:47375ms step_avg:57.22ms
step:829/2330 train_time:47431ms step_avg:57.22ms
step:830/2330 train_time:47491ms step_avg:57.22ms
step:831/2330 train_time:47547ms step_avg:57.22ms
step:832/2330 train_time:47606ms step_avg:57.22ms
step:833/2330 train_time:47663ms step_avg:57.22ms
step:834/2330 train_time:47722ms step_avg:57.22ms
step:835/2330 train_time:47778ms step_avg:57.22ms
step:836/2330 train_time:47838ms step_avg:57.22ms
step:837/2330 train_time:47894ms step_avg:57.22ms
step:838/2330 train_time:47954ms step_avg:57.22ms
step:839/2330 train_time:48011ms step_avg:57.22ms
step:840/2330 train_time:48070ms step_avg:57.23ms
step:841/2330 train_time:48127ms step_avg:57.23ms
step:842/2330 train_time:48186ms step_avg:57.23ms
step:843/2330 train_time:48242ms step_avg:57.23ms
step:844/2330 train_time:48302ms step_avg:57.23ms
step:845/2330 train_time:48359ms step_avg:57.23ms
step:846/2330 train_time:48420ms step_avg:57.23ms
step:847/2330 train_time:48476ms step_avg:57.23ms
step:848/2330 train_time:48537ms step_avg:57.24ms
step:849/2330 train_time:48594ms step_avg:57.24ms
step:850/2330 train_time:48653ms step_avg:57.24ms
step:851/2330 train_time:48710ms step_avg:57.24ms
step:852/2330 train_time:48768ms step_avg:57.24ms
step:853/2330 train_time:48825ms step_avg:57.24ms
step:854/2330 train_time:48885ms step_avg:57.24ms
step:855/2330 train_time:48941ms step_avg:57.24ms
step:856/2330 train_time:49000ms step_avg:57.24ms
step:857/2330 train_time:49058ms step_avg:57.24ms
step:858/2330 train_time:49118ms step_avg:57.25ms
step:859/2330 train_time:49175ms step_avg:57.25ms
step:860/2330 train_time:49234ms step_avg:57.25ms
step:861/2330 train_time:49291ms step_avg:57.25ms
step:862/2330 train_time:49350ms step_avg:57.25ms
step:863/2330 train_time:49407ms step_avg:57.25ms
step:864/2330 train_time:49466ms step_avg:57.25ms
step:865/2330 train_time:49522ms step_avg:57.25ms
step:866/2330 train_time:49583ms step_avg:57.25ms
step:867/2330 train_time:49639ms step_avg:57.25ms
step:868/2330 train_time:49700ms step_avg:57.26ms
step:869/2330 train_time:49757ms step_avg:57.26ms
step:870/2330 train_time:49817ms step_avg:57.26ms
step:871/2330 train_time:49873ms step_avg:57.26ms
step:872/2330 train_time:49932ms step_avg:57.26ms
step:873/2330 train_time:49989ms step_avg:57.26ms
step:874/2330 train_time:50048ms step_avg:57.26ms
step:875/2330 train_time:50105ms step_avg:57.26ms
step:876/2330 train_time:50165ms step_avg:57.27ms
step:877/2330 train_time:50221ms step_avg:57.26ms
step:878/2330 train_time:50282ms step_avg:57.27ms
step:879/2330 train_time:50339ms step_avg:57.27ms
step:880/2330 train_time:50398ms step_avg:57.27ms
step:881/2330 train_time:50456ms step_avg:57.27ms
step:882/2330 train_time:50515ms step_avg:57.27ms
step:883/2330 train_time:50572ms step_avg:57.27ms
step:884/2330 train_time:50631ms step_avg:57.28ms
step:885/2330 train_time:50688ms step_avg:57.27ms
step:886/2330 train_time:50747ms step_avg:57.28ms
step:887/2330 train_time:50803ms step_avg:57.28ms
step:888/2330 train_time:50863ms step_avg:57.28ms
step:889/2330 train_time:50920ms step_avg:57.28ms
step:890/2330 train_time:50979ms step_avg:57.28ms
step:891/2330 train_time:51036ms step_avg:57.28ms
step:892/2330 train_time:51096ms step_avg:57.28ms
step:893/2330 train_time:51153ms step_avg:57.28ms
step:894/2330 train_time:51212ms step_avg:57.28ms
step:895/2330 train_time:51269ms step_avg:57.28ms
step:896/2330 train_time:51328ms step_avg:57.29ms
step:897/2330 train_time:51384ms step_avg:57.28ms
step:898/2330 train_time:51444ms step_avg:57.29ms
step:899/2330 train_time:51500ms step_avg:57.29ms
step:900/2330 train_time:51561ms step_avg:57.29ms
step:901/2330 train_time:51618ms step_avg:57.29ms
step:902/2330 train_time:51678ms step_avg:57.29ms
step:903/2330 train_time:51734ms step_avg:57.29ms
step:904/2330 train_time:51793ms step_avg:57.29ms
step:905/2330 train_time:51850ms step_avg:57.29ms
step:906/2330 train_time:51910ms step_avg:57.30ms
step:907/2330 train_time:51966ms step_avg:57.29ms
step:908/2330 train_time:52026ms step_avg:57.30ms
step:909/2330 train_time:52082ms step_avg:57.30ms
step:910/2330 train_time:52142ms step_avg:57.30ms
step:911/2330 train_time:52198ms step_avg:57.30ms
step:912/2330 train_time:52259ms step_avg:57.30ms
step:913/2330 train_time:52315ms step_avg:57.30ms
step:914/2330 train_time:52376ms step_avg:57.30ms
step:915/2330 train_time:52433ms step_avg:57.30ms
step:916/2330 train_time:52493ms step_avg:57.31ms
step:917/2330 train_time:52550ms step_avg:57.31ms
step:918/2330 train_time:52609ms step_avg:57.31ms
step:919/2330 train_time:52665ms step_avg:57.31ms
step:920/2330 train_time:52725ms step_avg:57.31ms
step:921/2330 train_time:52780ms step_avg:57.31ms
step:922/2330 train_time:52841ms step_avg:57.31ms
step:923/2330 train_time:52898ms step_avg:57.31ms
step:924/2330 train_time:52958ms step_avg:57.31ms
step:925/2330 train_time:53015ms step_avg:57.31ms
step:926/2330 train_time:53074ms step_avg:57.32ms
step:927/2330 train_time:53130ms step_avg:57.31ms
step:928/2330 train_time:53190ms step_avg:57.32ms
step:929/2330 train_time:53245ms step_avg:57.31ms
step:930/2330 train_time:53306ms step_avg:57.32ms
step:931/2330 train_time:53362ms step_avg:57.32ms
step:932/2330 train_time:53423ms step_avg:57.32ms
step:933/2330 train_time:53479ms step_avg:57.32ms
step:934/2330 train_time:53540ms step_avg:57.32ms
step:935/2330 train_time:53596ms step_avg:57.32ms
step:936/2330 train_time:53656ms step_avg:57.32ms
step:937/2330 train_time:53713ms step_avg:57.32ms
step:938/2330 train_time:53772ms step_avg:57.33ms
step:939/2330 train_time:53828ms step_avg:57.32ms
step:940/2330 train_time:53888ms step_avg:57.33ms
step:941/2330 train_time:53945ms step_avg:57.33ms
step:942/2330 train_time:54005ms step_avg:57.33ms
step:943/2330 train_time:54061ms step_avg:57.33ms
step:944/2330 train_time:54122ms step_avg:57.33ms
step:945/2330 train_time:54178ms step_avg:57.33ms
step:946/2330 train_time:54239ms step_avg:57.33ms
step:947/2330 train_time:54295ms step_avg:57.33ms
step:948/2330 train_time:54356ms step_avg:57.34ms
step:949/2330 train_time:54413ms step_avg:57.34ms
step:950/2330 train_time:54473ms step_avg:57.34ms
step:951/2330 train_time:54529ms step_avg:57.34ms
step:952/2330 train_time:54588ms step_avg:57.34ms
step:953/2330 train_time:54645ms step_avg:57.34ms
step:954/2330 train_time:54704ms step_avg:57.34ms
step:955/2330 train_time:54760ms step_avg:57.34ms
step:956/2330 train_time:54821ms step_avg:57.34ms
step:957/2330 train_time:54877ms step_avg:57.34ms
step:958/2330 train_time:54937ms step_avg:57.35ms
step:959/2330 train_time:54993ms step_avg:57.34ms
step:960/2330 train_time:55052ms step_avg:57.35ms
step:961/2330 train_time:55109ms step_avg:57.35ms
step:962/2330 train_time:55169ms step_avg:57.35ms
step:963/2330 train_time:55225ms step_avg:57.35ms
step:964/2330 train_time:55285ms step_avg:57.35ms
step:965/2330 train_time:55341ms step_avg:57.35ms
step:966/2330 train_time:55402ms step_avg:57.35ms
step:967/2330 train_time:55458ms step_avg:57.35ms
step:968/2330 train_time:55518ms step_avg:57.35ms
step:969/2330 train_time:55575ms step_avg:57.35ms
step:970/2330 train_time:55634ms step_avg:57.36ms
step:971/2330 train_time:55691ms step_avg:57.35ms
step:972/2330 train_time:55751ms step_avg:57.36ms
step:973/2330 train_time:55808ms step_avg:57.36ms
step:974/2330 train_time:55868ms step_avg:57.36ms
step:975/2330 train_time:55924ms step_avg:57.36ms
step:976/2330 train_time:55983ms step_avg:57.36ms
step:977/2330 train_time:56040ms step_avg:57.36ms
step:978/2330 train_time:56100ms step_avg:57.36ms
step:979/2330 train_time:56157ms step_avg:57.36ms
step:980/2330 train_time:56217ms step_avg:57.36ms
step:981/2330 train_time:56274ms step_avg:57.36ms
step:982/2330 train_time:56333ms step_avg:57.37ms
step:983/2330 train_time:56389ms step_avg:57.36ms
step:984/2330 train_time:56449ms step_avg:57.37ms
step:985/2330 train_time:56505ms step_avg:57.37ms
step:986/2330 train_time:56566ms step_avg:57.37ms
step:987/2330 train_time:56622ms step_avg:57.37ms
step:988/2330 train_time:56682ms step_avg:57.37ms
step:989/2330 train_time:56738ms step_avg:57.37ms
step:990/2330 train_time:56798ms step_avg:57.37ms
step:991/2330 train_time:56855ms step_avg:57.37ms
step:992/2330 train_time:56915ms step_avg:57.37ms
step:993/2330 train_time:56971ms step_avg:57.37ms
step:994/2330 train_time:57030ms step_avg:57.37ms
step:995/2330 train_time:57087ms step_avg:57.37ms
step:996/2330 train_time:57147ms step_avg:57.38ms
step:997/2330 train_time:57203ms step_avg:57.37ms
step:998/2330 train_time:57264ms step_avg:57.38ms
step:999/2330 train_time:57321ms step_avg:57.38ms
step:1000/2330 train_time:57380ms step_avg:57.38ms
step:1000/2330 val_loss:4.1277 train_time:57460ms step_avg:57.46ms
step:1001/2330 train_time:57478ms step_avg:57.42ms
step:1002/2330 train_time:57499ms step_avg:57.38ms
step:1003/2330 train_time:57557ms step_avg:57.38ms
step:1004/2330 train_time:57618ms step_avg:57.39ms
step:1005/2330 train_time:57675ms step_avg:57.39ms
step:1006/2330 train_time:57735ms step_avg:57.39ms
step:1007/2330 train_time:57792ms step_avg:57.39ms
step:1008/2330 train_time:57851ms step_avg:57.39ms
step:1009/2330 train_time:57908ms step_avg:57.39ms
step:1010/2330 train_time:57967ms step_avg:57.39ms
step:1011/2330 train_time:58023ms step_avg:57.39ms
step:1012/2330 train_time:58082ms step_avg:57.39ms
step:1013/2330 train_time:58138ms step_avg:57.39ms
step:1014/2330 train_time:58197ms step_avg:57.39ms
step:1015/2330 train_time:58253ms step_avg:57.39ms
step:1016/2330 train_time:58312ms step_avg:57.39ms
step:1017/2330 train_time:58368ms step_avg:57.39ms
step:1018/2330 train_time:58431ms step_avg:57.40ms
step:1019/2330 train_time:58488ms step_avg:57.40ms
step:1020/2330 train_time:58552ms step_avg:57.40ms
step:1021/2330 train_time:58609ms step_avg:57.40ms
step:1022/2330 train_time:58669ms step_avg:57.41ms
step:1023/2330 train_time:58726ms step_avg:57.41ms
step:1024/2330 train_time:58786ms step_avg:57.41ms
step:1025/2330 train_time:58843ms step_avg:57.41ms
step:1026/2330 train_time:58902ms step_avg:57.41ms
step:1027/2330 train_time:58958ms step_avg:57.41ms
step:1028/2330 train_time:59017ms step_avg:57.41ms
step:1029/2330 train_time:59073ms step_avg:57.41ms
step:1030/2330 train_time:59132ms step_avg:57.41ms
step:1031/2330 train_time:59189ms step_avg:57.41ms
step:1032/2330 train_time:59248ms step_avg:57.41ms
step:1033/2330 train_time:59304ms step_avg:57.41ms
step:1034/2330 train_time:59363ms step_avg:57.41ms
step:1035/2330 train_time:59420ms step_avg:57.41ms
step:1036/2330 train_time:59481ms step_avg:57.41ms
step:1037/2330 train_time:59538ms step_avg:57.41ms
step:1038/2330 train_time:59599ms step_avg:57.42ms
step:1039/2330 train_time:59656ms step_avg:57.42ms
step:1040/2330 train_time:59716ms step_avg:57.42ms
step:1041/2330 train_time:59773ms step_avg:57.42ms
step:1042/2330 train_time:59833ms step_avg:57.42ms
step:1043/2330 train_time:59890ms step_avg:57.42ms
step:1044/2330 train_time:59950ms step_avg:57.42ms
step:1045/2330 train_time:60007ms step_avg:57.42ms
step:1046/2330 train_time:60066ms step_avg:57.42ms
step:1047/2330 train_time:60122ms step_avg:57.42ms
step:1048/2330 train_time:60181ms step_avg:57.42ms
step:1049/2330 train_time:60237ms step_avg:57.42ms
step:1050/2330 train_time:60296ms step_avg:57.43ms
step:1051/2330 train_time:60352ms step_avg:57.42ms
step:1052/2330 train_time:60413ms step_avg:57.43ms
step:1053/2330 train_time:60470ms step_avg:57.43ms
step:1054/2330 train_time:60530ms step_avg:57.43ms
step:1055/2330 train_time:60587ms step_avg:57.43ms
step:1056/2330 train_time:60648ms step_avg:57.43ms
step:1057/2330 train_time:60704ms step_avg:57.43ms
step:1058/2330 train_time:60765ms step_avg:57.43ms
step:1059/2330 train_time:60821ms step_avg:57.43ms
step:1060/2330 train_time:60881ms step_avg:57.44ms
step:1061/2330 train_time:60937ms step_avg:57.43ms
step:1062/2330 train_time:60998ms step_avg:57.44ms
step:1063/2330 train_time:61054ms step_avg:57.44ms
step:1064/2330 train_time:61113ms step_avg:57.44ms
step:1065/2330 train_time:61169ms step_avg:57.44ms
step:1066/2330 train_time:61230ms step_avg:57.44ms
step:1067/2330 train_time:61286ms step_avg:57.44ms
step:1068/2330 train_time:61346ms step_avg:57.44ms
step:1069/2330 train_time:61402ms step_avg:57.44ms
step:1070/2330 train_time:61462ms step_avg:57.44ms
step:1071/2330 train_time:61518ms step_avg:57.44ms
step:1072/2330 train_time:61578ms step_avg:57.44ms
step:1073/2330 train_time:61634ms step_avg:57.44ms
step:1074/2330 train_time:61695ms step_avg:57.44ms
step:1075/2330 train_time:61752ms step_avg:57.44ms
step:1076/2330 train_time:61812ms step_avg:57.45ms
step:1077/2330 train_time:61869ms step_avg:57.45ms
step:1078/2330 train_time:61928ms step_avg:57.45ms
step:1079/2330 train_time:61985ms step_avg:57.45ms
step:1080/2330 train_time:62044ms step_avg:57.45ms
step:1081/2330 train_time:62100ms step_avg:57.45ms
step:1082/2330 train_time:62160ms step_avg:57.45ms
step:1083/2330 train_time:62216ms step_avg:57.45ms
step:1084/2330 train_time:62276ms step_avg:57.45ms
step:1085/2330 train_time:62332ms step_avg:57.45ms
step:1086/2330 train_time:62393ms step_avg:57.45ms
step:1087/2330 train_time:62450ms step_avg:57.45ms
step:1088/2330 train_time:62510ms step_avg:57.45ms
step:1089/2330 train_time:62566ms step_avg:57.45ms
step:1090/2330 train_time:62625ms step_avg:57.45ms
step:1091/2330 train_time:62682ms step_avg:57.45ms
step:1092/2330 train_time:62742ms step_avg:57.46ms
step:1093/2330 train_time:62799ms step_avg:57.46ms
step:1094/2330 train_time:62858ms step_avg:57.46ms
step:1095/2330 train_time:62914ms step_avg:57.46ms
step:1096/2330 train_time:62974ms step_avg:57.46ms
step:1097/2330 train_time:63031ms step_avg:57.46ms
step:1098/2330 train_time:63092ms step_avg:57.46ms
step:1099/2330 train_time:63148ms step_avg:57.46ms
step:1100/2330 train_time:63208ms step_avg:57.46ms
step:1101/2330 train_time:63264ms step_avg:57.46ms
step:1102/2330 train_time:63324ms step_avg:57.46ms
step:1103/2330 train_time:63380ms step_avg:57.46ms
step:1104/2330 train_time:63440ms step_avg:57.46ms
step:1105/2330 train_time:63497ms step_avg:57.46ms
step:1106/2330 train_time:63557ms step_avg:57.47ms
step:1107/2330 train_time:63613ms step_avg:57.46ms
step:1108/2330 train_time:63673ms step_avg:57.47ms
step:1109/2330 train_time:63729ms step_avg:57.47ms
step:1110/2330 train_time:63791ms step_avg:57.47ms
step:1111/2330 train_time:63848ms step_avg:57.47ms
step:1112/2330 train_time:63907ms step_avg:57.47ms
step:1113/2330 train_time:63964ms step_avg:57.47ms
step:1114/2330 train_time:64023ms step_avg:57.47ms
step:1115/2330 train_time:64079ms step_avg:57.47ms
step:1116/2330 train_time:64139ms step_avg:57.47ms
step:1117/2330 train_time:64196ms step_avg:57.47ms
step:1118/2330 train_time:64255ms step_avg:57.47ms
step:1119/2330 train_time:64311ms step_avg:57.47ms
step:1120/2330 train_time:64371ms step_avg:57.47ms
step:1121/2330 train_time:64428ms step_avg:57.47ms
step:1122/2330 train_time:64488ms step_avg:57.48ms
step:1123/2330 train_time:64545ms step_avg:57.48ms
step:1124/2330 train_time:64605ms step_avg:57.48ms
step:1125/2330 train_time:64661ms step_avg:57.48ms
step:1126/2330 train_time:64721ms step_avg:57.48ms
step:1127/2330 train_time:64777ms step_avg:57.48ms
step:1128/2330 train_time:64837ms step_avg:57.48ms
step:1129/2330 train_time:64894ms step_avg:57.48ms
step:1130/2330 train_time:64954ms step_avg:57.48ms
step:1131/2330 train_time:65010ms step_avg:57.48ms
step:1132/2330 train_time:65070ms step_avg:57.48ms
step:1133/2330 train_time:65127ms step_avg:57.48ms
step:1134/2330 train_time:65186ms step_avg:57.48ms
step:1135/2330 train_time:65242ms step_avg:57.48ms
step:1136/2330 train_time:65304ms step_avg:57.49ms
step:1137/2330 train_time:65359ms step_avg:57.48ms
step:1138/2330 train_time:65419ms step_avg:57.49ms
step:1139/2330 train_time:65475ms step_avg:57.48ms
step:1140/2330 train_time:65535ms step_avg:57.49ms
step:1141/2330 train_time:65592ms step_avg:57.49ms
step:1142/2330 train_time:65652ms step_avg:57.49ms
step:1143/2330 train_time:65708ms step_avg:57.49ms
step:1144/2330 train_time:65768ms step_avg:57.49ms
step:1145/2330 train_time:65825ms step_avg:57.49ms
step:1146/2330 train_time:65884ms step_avg:57.49ms
step:1147/2330 train_time:65940ms step_avg:57.49ms
step:1148/2330 train_time:66000ms step_avg:57.49ms
step:1149/2330 train_time:66057ms step_avg:57.49ms
step:1150/2330 train_time:66116ms step_avg:57.49ms
step:1151/2330 train_time:66173ms step_avg:57.49ms
step:1152/2330 train_time:66233ms step_avg:57.49ms
step:1153/2330 train_time:66290ms step_avg:57.49ms
step:1154/2330 train_time:66350ms step_avg:57.50ms
step:1155/2330 train_time:66407ms step_avg:57.50ms
step:1156/2330 train_time:66467ms step_avg:57.50ms
step:1157/2330 train_time:66524ms step_avg:57.50ms
step:1158/2330 train_time:66583ms step_avg:57.50ms
step:1159/2330 train_time:66639ms step_avg:57.50ms
step:1160/2330 train_time:66699ms step_avg:57.50ms
step:1161/2330 train_time:66755ms step_avg:57.50ms
step:1162/2330 train_time:66816ms step_avg:57.50ms
step:1163/2330 train_time:66872ms step_avg:57.50ms
step:1164/2330 train_time:66932ms step_avg:57.50ms
step:1165/2330 train_time:66989ms step_avg:57.50ms
step:1166/2330 train_time:67049ms step_avg:57.50ms
step:1167/2330 train_time:67106ms step_avg:57.50ms
step:1168/2330 train_time:67165ms step_avg:57.50ms
step:1169/2330 train_time:67222ms step_avg:57.50ms
step:1170/2330 train_time:67281ms step_avg:57.51ms
step:1171/2330 train_time:67337ms step_avg:57.50ms
step:1172/2330 train_time:67397ms step_avg:57.51ms
step:1173/2330 train_time:67453ms step_avg:57.51ms
step:1174/2330 train_time:67514ms step_avg:57.51ms
step:1175/2330 train_time:67570ms step_avg:57.51ms
step:1176/2330 train_time:67630ms step_avg:57.51ms
step:1177/2330 train_time:67687ms step_avg:57.51ms
step:1178/2330 train_time:67747ms step_avg:57.51ms
step:1179/2330 train_time:67804ms step_avg:57.51ms
step:1180/2330 train_time:67863ms step_avg:57.51ms
step:1181/2330 train_time:67919ms step_avg:57.51ms
step:1182/2330 train_time:67980ms step_avg:57.51ms
step:1183/2330 train_time:68036ms step_avg:57.51ms
step:1184/2330 train_time:68096ms step_avg:57.51ms
step:1185/2330 train_time:68153ms step_avg:57.51ms
step:1186/2330 train_time:68213ms step_avg:57.52ms
step:1187/2330 train_time:68270ms step_avg:57.51ms
step:1188/2330 train_time:68329ms step_avg:57.52ms
step:1189/2330 train_time:68385ms step_avg:57.52ms
step:1190/2330 train_time:68445ms step_avg:57.52ms
step:1191/2330 train_time:68502ms step_avg:57.52ms
step:1192/2330 train_time:68561ms step_avg:57.52ms
step:1193/2330 train_time:68617ms step_avg:57.52ms
step:1194/2330 train_time:68678ms step_avg:57.52ms
step:1195/2330 train_time:68734ms step_avg:57.52ms
step:1196/2330 train_time:68795ms step_avg:57.52ms
step:1197/2330 train_time:68851ms step_avg:57.52ms
step:1198/2330 train_time:68911ms step_avg:57.52ms
step:1199/2330 train_time:68968ms step_avg:57.52ms
step:1200/2330 train_time:69028ms step_avg:57.52ms
step:1201/2330 train_time:69084ms step_avg:57.52ms
step:1202/2330 train_time:69144ms step_avg:57.52ms
step:1203/2330 train_time:69200ms step_avg:57.52ms
step:1204/2330 train_time:69261ms step_avg:57.53ms
step:1205/2330 train_time:69317ms step_avg:57.52ms
step:1206/2330 train_time:69377ms step_avg:57.53ms
step:1207/2330 train_time:69434ms step_avg:57.53ms
step:1208/2330 train_time:69494ms step_avg:57.53ms
step:1209/2330 train_time:69551ms step_avg:57.53ms
step:1210/2330 train_time:69611ms step_avg:57.53ms
step:1211/2330 train_time:69668ms step_avg:57.53ms
step:1212/2330 train_time:69728ms step_avg:57.53ms
step:1213/2330 train_time:69784ms step_avg:57.53ms
step:1214/2330 train_time:69844ms step_avg:57.53ms
step:1215/2330 train_time:69900ms step_avg:57.53ms
step:1216/2330 train_time:69959ms step_avg:57.53ms
step:1217/2330 train_time:70015ms step_avg:57.53ms
step:1218/2330 train_time:70076ms step_avg:57.53ms
step:1219/2330 train_time:70132ms step_avg:57.53ms
step:1220/2330 train_time:70193ms step_avg:57.54ms
step:1221/2330 train_time:70250ms step_avg:57.53ms
step:1222/2330 train_time:70311ms step_avg:57.54ms
step:1223/2330 train_time:70367ms step_avg:57.54ms
step:1224/2330 train_time:70427ms step_avg:57.54ms
step:1225/2330 train_time:70483ms step_avg:57.54ms
step:1226/2330 train_time:70543ms step_avg:57.54ms
step:1227/2330 train_time:70599ms step_avg:57.54ms
step:1228/2330 train_time:70659ms step_avg:57.54ms
step:1229/2330 train_time:70715ms step_avg:57.54ms
step:1230/2330 train_time:70776ms step_avg:57.54ms
step:1231/2330 train_time:70832ms step_avg:57.54ms
step:1232/2330 train_time:70892ms step_avg:57.54ms
step:1233/2330 train_time:70948ms step_avg:57.54ms
step:1234/2330 train_time:71009ms step_avg:57.54ms
step:1235/2330 train_time:71065ms step_avg:57.54ms
step:1236/2330 train_time:71126ms step_avg:57.54ms
step:1237/2330 train_time:71181ms step_avg:57.54ms
step:1238/2330 train_time:71242ms step_avg:57.55ms
step:1239/2330 train_time:71298ms step_avg:57.54ms
step:1240/2330 train_time:71358ms step_avg:57.55ms
step:1241/2330 train_time:71414ms step_avg:57.55ms
step:1242/2330 train_time:71474ms step_avg:57.55ms
step:1243/2330 train_time:71531ms step_avg:57.55ms
step:1244/2330 train_time:71590ms step_avg:57.55ms
step:1245/2330 train_time:71647ms step_avg:57.55ms
step:1246/2330 train_time:71707ms step_avg:57.55ms
step:1247/2330 train_time:71763ms step_avg:57.55ms
step:1248/2330 train_time:71822ms step_avg:57.55ms
step:1249/2330 train_time:71879ms step_avg:57.55ms
step:1250/2330 train_time:71939ms step_avg:57.55ms
step:1250/2330 val_loss:4.0483 train_time:72019ms step_avg:57.62ms
step:1251/2330 train_time:72038ms step_avg:57.58ms
step:1252/2330 train_time:72058ms step_avg:57.55ms
step:1253/2330 train_time:72116ms step_avg:57.55ms
step:1254/2330 train_time:72179ms step_avg:57.56ms
step:1255/2330 train_time:72235ms step_avg:57.56ms
step:1256/2330 train_time:72297ms step_avg:57.56ms
step:1257/2330 train_time:72353ms step_avg:57.56ms
step:1258/2330 train_time:72413ms step_avg:57.56ms
step:1259/2330 train_time:72469ms step_avg:57.56ms
step:1260/2330 train_time:72528ms step_avg:57.56ms
step:1261/2330 train_time:72584ms step_avg:57.56ms
step:1262/2330 train_time:72643ms step_avg:57.56ms
step:1263/2330 train_time:72699ms step_avg:57.56ms
step:1264/2330 train_time:72758ms step_avg:57.56ms
step:1265/2330 train_time:72814ms step_avg:57.56ms
step:1266/2330 train_time:72873ms step_avg:57.56ms
step:1267/2330 train_time:72929ms step_avg:57.56ms
step:1268/2330 train_time:72988ms step_avg:57.56ms
step:1269/2330 train_time:73046ms step_avg:57.56ms
step:1270/2330 train_time:73108ms step_avg:57.57ms
step:1271/2330 train_time:73166ms step_avg:57.57ms
step:1272/2330 train_time:73228ms step_avg:57.57ms
step:1273/2330 train_time:73285ms step_avg:57.57ms
step:1274/2330 train_time:73344ms step_avg:57.57ms
step:1275/2330 train_time:73400ms step_avg:57.57ms
step:1276/2330 train_time:73460ms step_avg:57.57ms
step:1277/2330 train_time:73516ms step_avg:57.57ms
step:1278/2330 train_time:73576ms step_avg:57.57ms
step:1279/2330 train_time:73633ms step_avg:57.57ms
step:1280/2330 train_time:73691ms step_avg:57.57ms
step:1281/2330 train_time:73747ms step_avg:57.57ms
step:1282/2330 train_time:73806ms step_avg:57.57ms
step:1283/2330 train_time:73862ms step_avg:57.57ms
step:1284/2330 train_time:73922ms step_avg:57.57ms
step:1285/2330 train_time:73978ms step_avg:57.57ms
step:1286/2330 train_time:74038ms step_avg:57.57ms
step:1287/2330 train_time:74095ms step_avg:57.57ms
step:1288/2330 train_time:74155ms step_avg:57.57ms
step:1289/2330 train_time:74212ms step_avg:57.57ms
step:1290/2330 train_time:74272ms step_avg:57.58ms
step:1291/2330 train_time:74329ms step_avg:57.57ms
step:1292/2330 train_time:74390ms step_avg:57.58ms
step:1293/2330 train_time:74445ms step_avg:57.58ms
step:1294/2330 train_time:74506ms step_avg:57.58ms
step:1295/2330 train_time:74562ms step_avg:57.58ms
step:1296/2330 train_time:74623ms step_avg:57.58ms
step:1297/2330 train_time:74679ms step_avg:57.58ms
step:1298/2330 train_time:74739ms step_avg:57.58ms
step:1299/2330 train_time:74795ms step_avg:57.58ms
step:1300/2330 train_time:74854ms step_avg:57.58ms
step:1301/2330 train_time:74910ms step_avg:57.58ms
step:1302/2330 train_time:74970ms step_avg:57.58ms
step:1303/2330 train_time:75026ms step_avg:57.58ms
step:1304/2330 train_time:75087ms step_avg:57.58ms
step:1305/2330 train_time:75145ms step_avg:57.58ms
step:1306/2330 train_time:75204ms step_avg:57.58ms
step:1307/2330 train_time:75261ms step_avg:57.58ms
step:1308/2330 train_time:75322ms step_avg:57.59ms
step:1309/2330 train_time:75379ms step_avg:57.59ms
step:1310/2330 train_time:75440ms step_avg:57.59ms
step:1311/2330 train_time:75496ms step_avg:57.59ms
step:1312/2330 train_time:75555ms step_avg:57.59ms
step:1313/2330 train_time:75612ms step_avg:57.59ms
step:1314/2330 train_time:75672ms step_avg:57.59ms
step:1315/2330 train_time:75728ms step_avg:57.59ms
step:1316/2330 train_time:75787ms step_avg:57.59ms
step:1317/2330 train_time:75843ms step_avg:57.59ms
step:1318/2330 train_time:75903ms step_avg:57.59ms
step:1319/2330 train_time:75959ms step_avg:57.59ms
step:1320/2330 train_time:76020ms step_avg:57.59ms
step:1321/2330 train_time:76077ms step_avg:57.59ms
step:1322/2330 train_time:76136ms step_avg:57.59ms
step:1323/2330 train_time:76193ms step_avg:57.59ms
step:1324/2330 train_time:76253ms step_avg:57.59ms
step:1325/2330 train_time:76309ms step_avg:57.59ms
step:1326/2330 train_time:76370ms step_avg:57.59ms
step:1327/2330 train_time:76426ms step_avg:57.59ms
step:1328/2330 train_time:76488ms step_avg:57.60ms
step:1329/2330 train_time:76544ms step_avg:57.60ms
step:1330/2330 train_time:76603ms step_avg:57.60ms
step:1331/2330 train_time:76660ms step_avg:57.60ms
step:1332/2330 train_time:76719ms step_avg:57.60ms
step:1333/2330 train_time:76776ms step_avg:57.60ms
step:1334/2330 train_time:76834ms step_avg:57.60ms
step:1335/2330 train_time:76890ms step_avg:57.60ms
step:1336/2330 train_time:76951ms step_avg:57.60ms
step:1337/2330 train_time:77007ms step_avg:57.60ms
step:1338/2330 train_time:77068ms step_avg:57.60ms
step:1339/2330 train_time:77125ms step_avg:57.60ms
step:1340/2330 train_time:77185ms step_avg:57.60ms
step:1341/2330 train_time:77242ms step_avg:57.60ms
step:1342/2330 train_time:77302ms step_avg:57.60ms
step:1343/2330 train_time:77358ms step_avg:57.60ms
step:1344/2330 train_time:77418ms step_avg:57.60ms
step:1345/2330 train_time:77474ms step_avg:57.60ms
step:1346/2330 train_time:77535ms step_avg:57.60ms
step:1347/2330 train_time:77591ms step_avg:57.60ms
step:1348/2330 train_time:77651ms step_avg:57.60ms
step:1349/2330 train_time:77707ms step_avg:57.60ms
step:1350/2330 train_time:77768ms step_avg:57.61ms
step:1351/2330 train_time:77824ms step_avg:57.60ms
step:1352/2330 train_time:77883ms step_avg:57.61ms
step:1353/2330 train_time:77940ms step_avg:57.61ms
step:1354/2330 train_time:78000ms step_avg:57.61ms
step:1355/2330 train_time:78056ms step_avg:57.61ms
step:1356/2330 train_time:78116ms step_avg:57.61ms
step:1357/2330 train_time:78172ms step_avg:57.61ms
step:1358/2330 train_time:78232ms step_avg:57.61ms
step:1359/2330 train_time:78288ms step_avg:57.61ms
step:1360/2330 train_time:78348ms step_avg:57.61ms
step:1361/2330 train_time:78404ms step_avg:57.61ms
step:1362/2330 train_time:78465ms step_avg:57.61ms
step:1363/2330 train_time:78521ms step_avg:57.61ms
step:1364/2330 train_time:78581ms step_avg:57.61ms
step:1365/2330 train_time:78638ms step_avg:57.61ms
step:1366/2330 train_time:78697ms step_avg:57.61ms
step:1367/2330 train_time:78753ms step_avg:57.61ms
step:1368/2330 train_time:78813ms step_avg:57.61ms
step:1369/2330 train_time:78869ms step_avg:57.61ms
step:1370/2330 train_time:78929ms step_avg:57.61ms
step:1371/2330 train_time:78985ms step_avg:57.61ms
step:1372/2330 train_time:79045ms step_avg:57.61ms
step:1373/2330 train_time:79101ms step_avg:57.61ms
step:1374/2330 train_time:79162ms step_avg:57.61ms
step:1375/2330 train_time:79218ms step_avg:57.61ms
step:1376/2330 train_time:79278ms step_avg:57.61ms
step:1377/2330 train_time:79335ms step_avg:57.61ms
step:1378/2330 train_time:79394ms step_avg:57.62ms
step:1379/2330 train_time:79451ms step_avg:57.61ms
step:1380/2330 train_time:79511ms step_avg:57.62ms
step:1381/2330 train_time:79567ms step_avg:57.62ms
step:1382/2330 train_time:79627ms step_avg:57.62ms
step:1383/2330 train_time:79683ms step_avg:57.62ms
step:1384/2330 train_time:79743ms step_avg:57.62ms
step:1385/2330 train_time:79800ms step_avg:57.62ms
step:1386/2330 train_time:79860ms step_avg:57.62ms
step:1387/2330 train_time:79917ms step_avg:57.62ms
step:1388/2330 train_time:79977ms step_avg:57.62ms
step:1389/2330 train_time:80033ms step_avg:57.62ms
step:1390/2330 train_time:80093ms step_avg:57.62ms
step:1391/2330 train_time:80149ms step_avg:57.62ms
step:1392/2330 train_time:80209ms step_avg:57.62ms
step:1393/2330 train_time:80265ms step_avg:57.62ms
step:1394/2330 train_time:80325ms step_avg:57.62ms
step:1395/2330 train_time:80382ms step_avg:57.62ms
step:1396/2330 train_time:80442ms step_avg:57.62ms
step:1397/2330 train_time:80499ms step_avg:57.62ms
step:1398/2330 train_time:80559ms step_avg:57.62ms
step:1399/2330 train_time:80615ms step_avg:57.62ms
step:1400/2330 train_time:80675ms step_avg:57.62ms
step:1401/2330 train_time:80732ms step_avg:57.62ms
step:1402/2330 train_time:80791ms step_avg:57.63ms
step:1403/2330 train_time:80847ms step_avg:57.62ms
step:1404/2330 train_time:80907ms step_avg:57.63ms
step:1405/2330 train_time:80963ms step_avg:57.63ms
step:1406/2330 train_time:81023ms step_avg:57.63ms
step:1407/2330 train_time:81080ms step_avg:57.63ms
step:1408/2330 train_time:81140ms step_avg:57.63ms
step:1409/2330 train_time:81196ms step_avg:57.63ms
step:1410/2330 train_time:81255ms step_avg:57.63ms
step:1411/2330 train_time:81312ms step_avg:57.63ms
step:1412/2330 train_time:81371ms step_avg:57.63ms
step:1413/2330 train_time:81428ms step_avg:57.63ms
step:1414/2330 train_time:81488ms step_avg:57.63ms
step:1415/2330 train_time:81545ms step_avg:57.63ms
step:1416/2330 train_time:81604ms step_avg:57.63ms
step:1417/2330 train_time:81661ms step_avg:57.63ms
step:1418/2330 train_time:81721ms step_avg:57.63ms
step:1419/2330 train_time:81779ms step_avg:57.63ms
step:1420/2330 train_time:81838ms step_avg:57.63ms
step:1421/2330 train_time:81894ms step_avg:57.63ms
step:1422/2330 train_time:81954ms step_avg:57.63ms
step:1423/2330 train_time:82010ms step_avg:57.63ms
step:1424/2330 train_time:82069ms step_avg:57.63ms
step:1425/2330 train_time:82126ms step_avg:57.63ms
step:1426/2330 train_time:82185ms step_avg:57.63ms
step:1427/2330 train_time:82242ms step_avg:57.63ms
step:1428/2330 train_time:82302ms step_avg:57.63ms
step:1429/2330 train_time:82358ms step_avg:57.63ms
step:1430/2330 train_time:82419ms step_avg:57.64ms
step:1431/2330 train_time:82475ms step_avg:57.63ms
step:1432/2330 train_time:82535ms step_avg:57.64ms
step:1433/2330 train_time:82591ms step_avg:57.63ms
step:1434/2330 train_time:82651ms step_avg:57.64ms
step:1435/2330 train_time:82707ms step_avg:57.64ms
step:1436/2330 train_time:82768ms step_avg:57.64ms
step:1437/2330 train_time:82825ms step_avg:57.64ms
step:1438/2330 train_time:82885ms step_avg:57.64ms
step:1439/2330 train_time:82942ms step_avg:57.64ms
step:1440/2330 train_time:83001ms step_avg:57.64ms
step:1441/2330 train_time:83058ms step_avg:57.64ms
step:1442/2330 train_time:83118ms step_avg:57.64ms
step:1443/2330 train_time:83175ms step_avg:57.64ms
step:1444/2330 train_time:83235ms step_avg:57.64ms
step:1445/2330 train_time:83291ms step_avg:57.64ms
step:1446/2330 train_time:83350ms step_avg:57.64ms
step:1447/2330 train_time:83406ms step_avg:57.64ms
step:1448/2330 train_time:83466ms step_avg:57.64ms
step:1449/2330 train_time:83523ms step_avg:57.64ms
step:1450/2330 train_time:83584ms step_avg:57.64ms
step:1451/2330 train_time:83640ms step_avg:57.64ms
step:1452/2330 train_time:83702ms step_avg:57.65ms
step:1453/2330 train_time:83758ms step_avg:57.65ms
step:1454/2330 train_time:83819ms step_avg:57.65ms
step:1455/2330 train_time:83876ms step_avg:57.65ms
step:1456/2330 train_time:83935ms step_avg:57.65ms
step:1457/2330 train_time:83991ms step_avg:57.65ms
step:1458/2330 train_time:84051ms step_avg:57.65ms
step:1459/2330 train_time:84107ms step_avg:57.65ms
step:1460/2330 train_time:84168ms step_avg:57.65ms
step:1461/2330 train_time:84225ms step_avg:57.65ms
step:1462/2330 train_time:84284ms step_avg:57.65ms
step:1463/2330 train_time:84341ms step_avg:57.65ms
step:1464/2330 train_time:84400ms step_avg:57.65ms
step:1465/2330 train_time:84457ms step_avg:57.65ms
step:1466/2330 train_time:84517ms step_avg:57.65ms
step:1467/2330 train_time:84573ms step_avg:57.65ms
step:1468/2330 train_time:84634ms step_avg:57.65ms
step:1469/2330 train_time:84690ms step_avg:57.65ms
step:1470/2330 train_time:84749ms step_avg:57.65ms
step:1471/2330 train_time:84806ms step_avg:57.65ms
step:1472/2330 train_time:84867ms step_avg:57.65ms
step:1473/2330 train_time:84923ms step_avg:57.65ms
step:1474/2330 train_time:84983ms step_avg:57.65ms
step:1475/2330 train_time:85040ms step_avg:57.65ms
step:1476/2330 train_time:85100ms step_avg:57.66ms
step:1477/2330 train_time:85157ms step_avg:57.66ms
step:1478/2330 train_time:85216ms step_avg:57.66ms
step:1479/2330 train_time:85273ms step_avg:57.66ms
step:1480/2330 train_time:85332ms step_avg:57.66ms
step:1481/2330 train_time:85388ms step_avg:57.66ms
step:1482/2330 train_time:85447ms step_avg:57.66ms
step:1483/2330 train_time:85504ms step_avg:57.66ms
step:1484/2330 train_time:85564ms step_avg:57.66ms
step:1485/2330 train_time:85621ms step_avg:57.66ms
step:1486/2330 train_time:85681ms step_avg:57.66ms
step:1487/2330 train_time:85738ms step_avg:57.66ms
step:1488/2330 train_time:85797ms step_avg:57.66ms
step:1489/2330 train_time:85853ms step_avg:57.66ms
step:1490/2330 train_time:85913ms step_avg:57.66ms
step:1491/2330 train_time:85969ms step_avg:57.66ms
step:1492/2330 train_time:86030ms step_avg:57.66ms
step:1493/2330 train_time:86086ms step_avg:57.66ms
step:1494/2330 train_time:86146ms step_avg:57.66ms
step:1495/2330 train_time:86203ms step_avg:57.66ms
step:1496/2330 train_time:86264ms step_avg:57.66ms
step:1497/2330 train_time:86321ms step_avg:57.66ms
step:1498/2330 train_time:86380ms step_avg:57.66ms
step:1499/2330 train_time:86436ms step_avg:57.66ms
step:1500/2330 train_time:86496ms step_avg:57.66ms
step:1500/2330 val_loss:3.9719 train_time:86576ms step_avg:57.72ms
step:1501/2330 train_time:86593ms step_avg:57.69ms
step:1502/2330 train_time:86616ms step_avg:57.67ms
step:1503/2330 train_time:86675ms step_avg:57.67ms
step:1504/2330 train_time:86738ms step_avg:57.67ms
step:1505/2330 train_time:86796ms step_avg:57.67ms
step:1506/2330 train_time:86858ms step_avg:57.67ms
step:1507/2330 train_time:86914ms step_avg:57.67ms
step:1508/2330 train_time:86974ms step_avg:57.68ms
step:1509/2330 train_time:87030ms step_avg:57.67ms
step:1510/2330 train_time:87089ms step_avg:57.67ms
step:1511/2330 train_time:87145ms step_avg:57.67ms
step:1512/2330 train_time:87204ms step_avg:57.67ms
step:1513/2330 train_time:87260ms step_avg:57.67ms
step:1514/2330 train_time:87319ms step_avg:57.67ms
step:1515/2330 train_time:87375ms step_avg:57.67ms
step:1516/2330 train_time:87433ms step_avg:57.67ms
step:1517/2330 train_time:87489ms step_avg:57.67ms
step:1518/2330 train_time:87549ms step_avg:57.67ms
step:1519/2330 train_time:87607ms step_avg:57.67ms
step:1520/2330 train_time:87667ms step_avg:57.68ms
step:1521/2330 train_time:87725ms step_avg:57.68ms
step:1522/2330 train_time:87786ms step_avg:57.68ms
step:1523/2330 train_time:87842ms step_avg:57.68ms
step:1524/2330 train_time:87904ms step_avg:57.68ms
step:1525/2330 train_time:87961ms step_avg:57.68ms
step:1526/2330 train_time:88021ms step_avg:57.68ms
step:1527/2330 train_time:88077ms step_avg:57.68ms
step:1528/2330 train_time:88137ms step_avg:57.68ms
step:1529/2330 train_time:88195ms step_avg:57.68ms
step:1530/2330 train_time:88253ms step_avg:57.68ms
step:1531/2330 train_time:88309ms step_avg:57.68ms
step:1532/2330 train_time:88370ms step_avg:57.68ms
step:1533/2330 train_time:88426ms step_avg:57.68ms
step:1534/2330 train_time:88486ms step_avg:57.68ms
step:1535/2330 train_time:88543ms step_avg:57.68ms
step:1536/2330 train_time:88602ms step_avg:57.68ms
step:1537/2330 train_time:88659ms step_avg:57.68ms
step:1538/2330 train_time:88721ms step_avg:57.69ms
step:1539/2330 train_time:88778ms step_avg:57.69ms
step:1540/2330 train_time:88840ms step_avg:57.69ms
step:1541/2330 train_time:88897ms step_avg:57.69ms
step:1542/2330 train_time:88958ms step_avg:57.69ms
step:1543/2330 train_time:89015ms step_avg:57.69ms
step:1544/2330 train_time:89076ms step_avg:57.69ms
step:1545/2330 train_time:89133ms step_avg:57.69ms
step:1546/2330 train_time:89193ms step_avg:57.69ms
step:1547/2330 train_time:89249ms step_avg:57.69ms
step:1548/2330 train_time:89309ms step_avg:57.69ms
step:1549/2330 train_time:89365ms step_avg:57.69ms
step:1550/2330 train_time:89425ms step_avg:57.69ms
step:1551/2330 train_time:89481ms step_avg:57.69ms
step:1552/2330 train_time:89542ms step_avg:57.69ms
step:1553/2330 train_time:89599ms step_avg:57.69ms
step:1554/2330 train_time:89659ms step_avg:57.70ms
step:1555/2330 train_time:89716ms step_avg:57.70ms
step:1556/2330 train_time:89777ms step_avg:57.70ms
step:1557/2330 train_time:89835ms step_avg:57.70ms
step:1558/2330 train_time:89896ms step_avg:57.70ms
step:1559/2330 train_time:89953ms step_avg:57.70ms
step:1560/2330 train_time:90013ms step_avg:57.70ms
step:1561/2330 train_time:90071ms step_avg:57.70ms
step:1562/2330 train_time:90131ms step_avg:57.70ms
step:1563/2330 train_time:90188ms step_avg:57.70ms
step:1564/2330 train_time:90248ms step_avg:57.70ms
step:1565/2330 train_time:90305ms step_avg:57.70ms
step:1566/2330 train_time:90364ms step_avg:57.70ms
step:1567/2330 train_time:90421ms step_avg:57.70ms
step:1568/2330 train_time:90481ms step_avg:57.70ms
step:1569/2330 train_time:90537ms step_avg:57.70ms
step:1570/2330 train_time:90597ms step_avg:57.71ms
step:1571/2330 train_time:90655ms step_avg:57.70ms
step:1572/2330 train_time:90715ms step_avg:57.71ms
step:1573/2330 train_time:90773ms step_avg:57.71ms
step:1574/2330 train_time:90833ms step_avg:57.71ms
step:1575/2330 train_time:90891ms step_avg:57.71ms
step:1576/2330 train_time:90952ms step_avg:57.71ms
step:1577/2330 train_time:91009ms step_avg:57.71ms
step:1578/2330 train_time:91069ms step_avg:57.71ms
step:1579/2330 train_time:91128ms step_avg:57.71ms
step:1580/2330 train_time:91187ms step_avg:57.71ms
step:1581/2330 train_time:91245ms step_avg:57.71ms
step:1582/2330 train_time:91304ms step_avg:57.71ms
step:1583/2330 train_time:91361ms step_avg:57.71ms
step:1584/2330 train_time:91422ms step_avg:57.72ms
step:1585/2330 train_time:91477ms step_avg:57.71ms
step:1586/2330 train_time:91538ms step_avg:57.72ms
step:1587/2330 train_time:91594ms step_avg:57.72ms
step:1588/2330 train_time:91656ms step_avg:57.72ms
step:1589/2330 train_time:91712ms step_avg:57.72ms
step:1590/2330 train_time:91774ms step_avg:57.72ms
step:1591/2330 train_time:91831ms step_avg:57.72ms
step:1592/2330 train_time:91891ms step_avg:57.72ms
step:1593/2330 train_time:91948ms step_avg:57.72ms
step:1594/2330 train_time:92009ms step_avg:57.72ms
step:1595/2330 train_time:92067ms step_avg:57.72ms
step:1596/2330 train_time:92127ms step_avg:57.72ms
step:1597/2330 train_time:92184ms step_avg:57.72ms
step:1598/2330 train_time:92244ms step_avg:57.72ms
step:1599/2330 train_time:92301ms step_avg:57.72ms
step:1600/2330 train_time:92361ms step_avg:57.73ms
step:1601/2330 train_time:92418ms step_avg:57.72ms
step:1602/2330 train_time:92478ms step_avg:57.73ms
step:1603/2330 train_time:92534ms step_avg:57.73ms
step:1604/2330 train_time:92594ms step_avg:57.73ms
step:1605/2330 train_time:92650ms step_avg:57.73ms
step:1606/2330 train_time:92711ms step_avg:57.73ms
step:1607/2330 train_time:92768ms step_avg:57.73ms
step:1608/2330 train_time:92829ms step_avg:57.73ms
step:1609/2330 train_time:92885ms step_avg:57.73ms
step:1610/2330 train_time:92946ms step_avg:57.73ms
step:1611/2330 train_time:93003ms step_avg:57.73ms
step:1612/2330 train_time:93064ms step_avg:57.73ms
step:1613/2330 train_time:93121ms step_avg:57.73ms
step:1614/2330 train_time:93182ms step_avg:57.73ms
step:1615/2330 train_time:93238ms step_avg:57.73ms
step:1616/2330 train_time:93300ms step_avg:57.73ms
step:1617/2330 train_time:93357ms step_avg:57.73ms
step:1618/2330 train_time:93417ms step_avg:57.74ms
step:1619/2330 train_time:93474ms step_avg:57.74ms
step:1620/2330 train_time:93534ms step_avg:57.74ms
step:1621/2330 train_time:93590ms step_avg:57.74ms
step:1622/2330 train_time:93652ms step_avg:57.74ms
step:1623/2330 train_time:93709ms step_avg:57.74ms
step:1624/2330 train_time:93769ms step_avg:57.74ms
step:1625/2330 train_time:93826ms step_avg:57.74ms
step:1626/2330 train_time:93886ms step_avg:57.74ms
step:1627/2330 train_time:93944ms step_avg:57.74ms
step:1628/2330 train_time:94004ms step_avg:57.74ms
step:1629/2330 train_time:94061ms step_avg:57.74ms
step:1630/2330 train_time:94121ms step_avg:57.74ms
step:1631/2330 train_time:94177ms step_avg:57.74ms
step:1632/2330 train_time:94237ms step_avg:57.74ms
step:1633/2330 train_time:94294ms step_avg:57.74ms
step:1634/2330 train_time:94356ms step_avg:57.75ms
step:1635/2330 train_time:94413ms step_avg:57.75ms
step:1636/2330 train_time:94473ms step_avg:57.75ms
step:1637/2330 train_time:94530ms step_avg:57.75ms
step:1638/2330 train_time:94589ms step_avg:57.75ms
step:1639/2330 train_time:94646ms step_avg:57.75ms
step:1640/2330 train_time:94706ms step_avg:57.75ms
step:1641/2330 train_time:94763ms step_avg:57.75ms
step:1642/2330 train_time:94823ms step_avg:57.75ms
step:1643/2330 train_time:94880ms step_avg:57.75ms
step:1644/2330 train_time:94941ms step_avg:57.75ms
step:1645/2330 train_time:94998ms step_avg:57.75ms
step:1646/2330 train_time:95059ms step_avg:57.75ms
step:1647/2330 train_time:95116ms step_avg:57.75ms
step:1648/2330 train_time:95177ms step_avg:57.75ms
step:1649/2330 train_time:95234ms step_avg:57.75ms
step:1650/2330 train_time:95294ms step_avg:57.75ms
step:1651/2330 train_time:95351ms step_avg:57.75ms
step:1652/2330 train_time:95411ms step_avg:57.75ms
step:1653/2330 train_time:95468ms step_avg:57.75ms
step:1654/2330 train_time:95528ms step_avg:57.76ms
step:1655/2330 train_time:95585ms step_avg:57.76ms
step:1656/2330 train_time:95644ms step_avg:57.76ms
step:1657/2330 train_time:95701ms step_avg:57.76ms
step:1658/2330 train_time:95762ms step_avg:57.76ms
step:1659/2330 train_time:95818ms step_avg:57.76ms
step:1660/2330 train_time:95879ms step_avg:57.76ms
step:1661/2330 train_time:95936ms step_avg:57.76ms
step:1662/2330 train_time:95996ms step_avg:57.76ms
step:1663/2330 train_time:96053ms step_avg:57.76ms
step:1664/2330 train_time:96114ms step_avg:57.76ms
step:1665/2330 train_time:96170ms step_avg:57.76ms
step:1666/2330 train_time:96232ms step_avg:57.76ms
step:1667/2330 train_time:96289ms step_avg:57.76ms
step:1668/2330 train_time:96349ms step_avg:57.76ms
step:1669/2330 train_time:96406ms step_avg:57.76ms
step:1670/2330 train_time:96466ms step_avg:57.76ms
step:1671/2330 train_time:96523ms step_avg:57.76ms
step:1672/2330 train_time:96582ms step_avg:57.76ms
step:1673/2330 train_time:96640ms step_avg:57.76ms
step:1674/2330 train_time:96700ms step_avg:57.77ms
step:1675/2330 train_time:96757ms step_avg:57.77ms
step:1676/2330 train_time:96818ms step_avg:57.77ms
step:1677/2330 train_time:96874ms step_avg:57.77ms
step:1678/2330 train_time:96935ms step_avg:57.77ms
step:1679/2330 train_time:96991ms step_avg:57.77ms
step:1680/2330 train_time:97052ms step_avg:57.77ms
step:1681/2330 train_time:97110ms step_avg:57.77ms
step:1682/2330 train_time:97170ms step_avg:57.77ms
step:1683/2330 train_time:97227ms step_avg:57.77ms
step:1684/2330 train_time:97287ms step_avg:57.77ms
step:1685/2330 train_time:97344ms step_avg:57.77ms
step:1686/2330 train_time:97404ms step_avg:57.77ms
step:1687/2330 train_time:97461ms step_avg:57.77ms
step:1688/2330 train_time:97521ms step_avg:57.77ms
step:1689/2330 train_time:97577ms step_avg:57.77ms
step:1690/2330 train_time:97638ms step_avg:57.77ms
step:1691/2330 train_time:97695ms step_avg:57.77ms
step:1692/2330 train_time:97757ms step_avg:57.78ms
step:1693/2330 train_time:97814ms step_avg:57.78ms
step:1694/2330 train_time:97874ms step_avg:57.78ms
step:1695/2330 train_time:97932ms step_avg:57.78ms
step:1696/2330 train_time:97992ms step_avg:57.78ms
step:1697/2330 train_time:98049ms step_avg:57.78ms
step:1698/2330 train_time:98109ms step_avg:57.78ms
step:1699/2330 train_time:98168ms step_avg:57.78ms
step:1700/2330 train_time:98227ms step_avg:57.78ms
step:1701/2330 train_time:98284ms step_avg:57.78ms
step:1702/2330 train_time:98344ms step_avg:57.78ms
step:1703/2330 train_time:98402ms step_avg:57.78ms
step:1704/2330 train_time:98461ms step_avg:57.78ms
step:1705/2330 train_time:98518ms step_avg:57.78ms
step:1706/2330 train_time:98578ms step_avg:57.78ms
step:1707/2330 train_time:98635ms step_avg:57.78ms
step:1708/2330 train_time:98696ms step_avg:57.78ms
step:1709/2330 train_time:98753ms step_avg:57.78ms
step:1710/2330 train_time:98815ms step_avg:57.79ms
step:1711/2330 train_time:98872ms step_avg:57.79ms
step:1712/2330 train_time:98933ms step_avg:57.79ms
step:1713/2330 train_time:98989ms step_avg:57.79ms
step:1714/2330 train_time:99050ms step_avg:57.79ms
step:1715/2330 train_time:99108ms step_avg:57.79ms
step:1716/2330 train_time:99169ms step_avg:57.79ms
step:1717/2330 train_time:99227ms step_avg:57.79ms
step:1718/2330 train_time:99286ms step_avg:57.79ms
step:1719/2330 train_time:99342ms step_avg:57.79ms
step:1720/2330 train_time:99403ms step_avg:57.79ms
step:1721/2330 train_time:99459ms step_avg:57.79ms
step:1722/2330 train_time:99520ms step_avg:57.79ms
step:1723/2330 train_time:99576ms step_avg:57.79ms
step:1724/2330 train_time:99637ms step_avg:57.79ms
step:1725/2330 train_time:99694ms step_avg:57.79ms
step:1726/2330 train_time:99756ms step_avg:57.80ms
step:1727/2330 train_time:99813ms step_avg:57.80ms
step:1728/2330 train_time:99873ms step_avg:57.80ms
step:1729/2330 train_time:99931ms step_avg:57.80ms
step:1730/2330 train_time:99991ms step_avg:57.80ms
step:1731/2330 train_time:100048ms step_avg:57.80ms
step:1732/2330 train_time:100108ms step_avg:57.80ms
step:1733/2330 train_time:100166ms step_avg:57.80ms
step:1734/2330 train_time:100226ms step_avg:57.80ms
step:1735/2330 train_time:100284ms step_avg:57.80ms
step:1736/2330 train_time:100343ms step_avg:57.80ms
step:1737/2330 train_time:100400ms step_avg:57.80ms
step:1738/2330 train_time:100461ms step_avg:57.80ms
step:1739/2330 train_time:100517ms step_avg:57.80ms
step:1740/2330 train_time:100578ms step_avg:57.80ms
step:1741/2330 train_time:100635ms step_avg:57.80ms
step:1742/2330 train_time:100696ms step_avg:57.81ms
step:1743/2330 train_time:100753ms step_avg:57.80ms
step:1744/2330 train_time:100814ms step_avg:57.81ms
step:1745/2330 train_time:100871ms step_avg:57.81ms
step:1746/2330 train_time:100932ms step_avg:57.81ms
step:1747/2330 train_time:100989ms step_avg:57.81ms
step:1748/2330 train_time:101049ms step_avg:57.81ms
step:1749/2330 train_time:101107ms step_avg:57.81ms
step:1750/2330 train_time:101167ms step_avg:57.81ms
step:1750/2330 val_loss:3.8908 train_time:101247ms step_avg:57.86ms
step:1751/2330 train_time:101266ms step_avg:57.83ms
step:1752/2330 train_time:101285ms step_avg:57.81ms
step:1753/2330 train_time:101340ms step_avg:57.81ms
step:1754/2330 train_time:101403ms step_avg:57.81ms
step:1755/2330 train_time:101459ms step_avg:57.81ms
step:1756/2330 train_time:101522ms step_avg:57.81ms
step:1757/2330 train_time:101578ms step_avg:57.81ms
step:1758/2330 train_time:101637ms step_avg:57.81ms
step:1759/2330 train_time:101694ms step_avg:57.81ms
step:1760/2330 train_time:101753ms step_avg:57.81ms
step:1761/2330 train_time:101809ms step_avg:57.81ms
step:1762/2330 train_time:101869ms step_avg:57.81ms
step:1763/2330 train_time:101925ms step_avg:57.81ms
step:1764/2330 train_time:101985ms step_avg:57.81ms
step:1765/2330 train_time:102041ms step_avg:57.81ms
step:1766/2330 train_time:102100ms step_avg:57.81ms
step:1767/2330 train_time:102157ms step_avg:57.81ms
step:1768/2330 train_time:102222ms step_avg:57.82ms
step:1769/2330 train_time:102279ms step_avg:57.82ms
step:1770/2330 train_time:102342ms step_avg:57.82ms
step:1771/2330 train_time:102400ms step_avg:57.82ms
step:1772/2330 train_time:102461ms step_avg:57.82ms
step:1773/2330 train_time:102517ms step_avg:57.82ms
step:1774/2330 train_time:102577ms step_avg:57.82ms
step:1775/2330 train_time:102634ms step_avg:57.82ms
step:1776/2330 train_time:102694ms step_avg:57.82ms
step:1777/2330 train_time:102751ms step_avg:57.82ms
step:1778/2330 train_time:102810ms step_avg:57.82ms
step:1779/2330 train_time:102867ms step_avg:57.82ms
step:1780/2330 train_time:102926ms step_avg:57.82ms
step:1781/2330 train_time:102983ms step_avg:57.82ms
step:1782/2330 train_time:103042ms step_avg:57.82ms
step:1783/2330 train_time:103100ms step_avg:57.82ms
step:1784/2330 train_time:103161ms step_avg:57.83ms
step:1785/2330 train_time:103219ms step_avg:57.83ms
step:1786/2330 train_time:103278ms step_avg:57.83ms
step:1787/2330 train_time:103336ms step_avg:57.83ms
step:1788/2330 train_time:103397ms step_avg:57.83ms
step:1789/2330 train_time:103453ms step_avg:57.83ms
step:1790/2330 train_time:103514ms step_avg:57.83ms
step:1791/2330 train_time:103571ms step_avg:57.83ms
step:1792/2330 train_time:103631ms step_avg:57.83ms
step:1793/2330 train_time:103688ms step_avg:57.83ms
step:1794/2330 train_time:103748ms step_avg:57.83ms
step:1795/2330 train_time:103806ms step_avg:57.83ms
step:1796/2330 train_time:103864ms step_avg:57.83ms
step:1797/2330 train_time:103921ms step_avg:57.83ms
step:1798/2330 train_time:103981ms step_avg:57.83ms
step:1799/2330 train_time:104037ms step_avg:57.83ms
step:1800/2330 train_time:104097ms step_avg:57.83ms
step:1801/2330 train_time:104155ms step_avg:57.83ms
step:1802/2330 train_time:104215ms step_avg:57.83ms
step:1803/2330 train_time:104272ms step_avg:57.83ms
step:1804/2330 train_time:104333ms step_avg:57.83ms
step:1805/2330 train_time:104390ms step_avg:57.83ms
step:1806/2330 train_time:104451ms step_avg:57.84ms
step:1807/2330 train_time:104507ms step_avg:57.83ms
step:1808/2330 train_time:104569ms step_avg:57.84ms
step:1809/2330 train_time:104625ms step_avg:57.84ms
step:1810/2330 train_time:104686ms step_avg:57.84ms
step:1811/2330 train_time:104743ms step_avg:57.84ms
step:1812/2330 train_time:104803ms step_avg:57.84ms
step:1813/2330 train_time:104859ms step_avg:57.84ms
step:1814/2330 train_time:104919ms step_avg:57.84ms
step:1815/2330 train_time:104976ms step_avg:57.84ms
step:1816/2330 train_time:105035ms step_avg:57.84ms
step:1817/2330 train_time:105093ms step_avg:57.84ms
step:1818/2330 train_time:105153ms step_avg:57.84ms
step:1819/2330 train_time:105211ms step_avg:57.84ms
step:1820/2330 train_time:105271ms step_avg:57.84ms
step:1821/2330 train_time:105328ms step_avg:57.84ms
step:1822/2330 train_time:105388ms step_avg:57.84ms
step:1823/2330 train_time:105445ms step_avg:57.84ms
step:1824/2330 train_time:105507ms step_avg:57.84ms
step:1825/2330 train_time:105564ms step_avg:57.84ms
step:1826/2330 train_time:105624ms step_avg:57.84ms
step:1827/2330 train_time:105681ms step_avg:57.84ms
step:1828/2330 train_time:105741ms step_avg:57.85ms
step:1829/2330 train_time:105798ms step_avg:57.84ms
step:1830/2330 train_time:105858ms step_avg:57.85ms
step:1831/2330 train_time:105915ms step_avg:57.85ms
step:1832/2330 train_time:105974ms step_avg:57.85ms
step:1833/2330 train_time:106031ms step_avg:57.85ms
step:1834/2330 train_time:106091ms step_avg:57.85ms
step:1835/2330 train_time:106148ms step_avg:57.85ms
step:1836/2330 train_time:106209ms step_avg:57.85ms
step:1837/2330 train_time:106267ms step_avg:57.85ms
step:1838/2330 train_time:106327ms step_avg:57.85ms
step:1839/2330 train_time:106384ms step_avg:57.85ms
step:1840/2330 train_time:106445ms step_avg:57.85ms
step:1841/2330 train_time:106502ms step_avg:57.85ms
step:1842/2330 train_time:106562ms step_avg:57.85ms
step:1843/2330 train_time:106619ms step_avg:57.85ms
step:1844/2330 train_time:106679ms step_avg:57.85ms
step:1845/2330 train_time:106736ms step_avg:57.85ms
step:1846/2330 train_time:106796ms step_avg:57.85ms
step:1847/2330 train_time:106853ms step_avg:57.85ms
step:1848/2330 train_time:106913ms step_avg:57.85ms
step:1849/2330 train_time:106970ms step_avg:57.85ms
step:1850/2330 train_time:107030ms step_avg:57.85ms
step:1851/2330 train_time:107087ms step_avg:57.85ms
step:1852/2330 train_time:107148ms step_avg:57.86ms
step:1853/2330 train_time:107205ms step_avg:57.85ms
step:1854/2330 train_time:107265ms step_avg:57.86ms
step:1855/2330 train_time:107322ms step_avg:57.86ms
step:1856/2330 train_time:107383ms step_avg:57.86ms
step:1857/2330 train_time:107440ms step_avg:57.86ms
step:1858/2330 train_time:107501ms step_avg:57.86ms
step:1859/2330 train_time:107558ms step_avg:57.86ms
step:1860/2330 train_time:107620ms step_avg:57.86ms
step:1861/2330 train_time:107677ms step_avg:57.86ms
step:1862/2330 train_time:107737ms step_avg:57.86ms
step:1863/2330 train_time:107795ms step_avg:57.86ms
step:1864/2330 train_time:107854ms step_avg:57.86ms
step:1865/2330 train_time:107911ms step_avg:57.86ms
step:1866/2330 train_time:107970ms step_avg:57.86ms
step:1867/2330 train_time:108026ms step_avg:57.86ms
step:1868/2330 train_time:108088ms step_avg:57.86ms
step:1869/2330 train_time:108145ms step_avg:57.86ms
step:1870/2330 train_time:108205ms step_avg:57.86ms
step:1871/2330 train_time:108262ms step_avg:57.86ms
step:1872/2330 train_time:108322ms step_avg:57.86ms
step:1873/2330 train_time:108379ms step_avg:57.86ms
step:1874/2330 train_time:108439ms step_avg:57.87ms
step:1875/2330 train_time:108496ms step_avg:57.86ms
step:1876/2330 train_time:108557ms step_avg:57.87ms
step:1877/2330 train_time:108615ms step_avg:57.87ms
step:1878/2330 train_time:108674ms step_avg:57.87ms
step:1879/2330 train_time:108732ms step_avg:57.87ms
step:1880/2330 train_time:108791ms step_avg:57.87ms
step:1881/2330 train_time:108848ms step_avg:57.87ms
step:1882/2330 train_time:108908ms step_avg:57.87ms
step:1883/2330 train_time:108965ms step_avg:57.87ms
step:1884/2330 train_time:109025ms step_avg:57.87ms
step:1885/2330 train_time:109082ms step_avg:57.87ms
step:1886/2330 train_time:109142ms step_avg:57.87ms
step:1887/2330 train_time:109200ms step_avg:57.87ms
step:1888/2330 train_time:109260ms step_avg:57.87ms
step:1889/2330 train_time:109317ms step_avg:57.87ms
step:1890/2330 train_time:109377ms step_avg:57.87ms
step:1891/2330 train_time:109434ms step_avg:57.87ms
step:1892/2330 train_time:109494ms step_avg:57.87ms
step:1893/2330 train_time:109552ms step_avg:57.87ms
step:1894/2330 train_time:109611ms step_avg:57.87ms
step:1895/2330 train_time:109669ms step_avg:57.87ms
step:1896/2330 train_time:109729ms step_avg:57.87ms
step:1897/2330 train_time:109787ms step_avg:57.87ms
step:1898/2330 train_time:109847ms step_avg:57.87ms
step:1899/2330 train_time:109904ms step_avg:57.87ms
step:1900/2330 train_time:109964ms step_avg:57.88ms
step:1901/2330 train_time:110021ms step_avg:57.88ms
step:1902/2330 train_time:110081ms step_avg:57.88ms
step:1903/2330 train_time:110138ms step_avg:57.88ms
step:1904/2330 train_time:110198ms step_avg:57.88ms
step:1905/2330 train_time:110255ms step_avg:57.88ms
step:1906/2330 train_time:110315ms step_avg:57.88ms
step:1907/2330 train_time:110371ms step_avg:57.88ms
step:1908/2330 train_time:110434ms step_avg:57.88ms
step:1909/2330 train_time:110490ms step_avg:57.88ms
step:1910/2330 train_time:110551ms step_avg:57.88ms
step:1911/2330 train_time:110608ms step_avg:57.88ms
step:1912/2330 train_time:110669ms step_avg:57.88ms
step:1913/2330 train_time:110725ms step_avg:57.88ms
step:1914/2330 train_time:110785ms step_avg:57.88ms
step:1915/2330 train_time:110842ms step_avg:57.88ms
step:1916/2330 train_time:110903ms step_avg:57.88ms
step:1917/2330 train_time:110960ms step_avg:57.88ms
step:1918/2330 train_time:111020ms step_avg:57.88ms
step:1919/2330 train_time:111077ms step_avg:57.88ms
step:1920/2330 train_time:111137ms step_avg:57.88ms
step:1921/2330 train_time:111194ms step_avg:57.88ms
step:1922/2330 train_time:111254ms step_avg:57.88ms
step:1923/2330 train_time:111310ms step_avg:57.88ms
step:1924/2330 train_time:111371ms step_avg:57.88ms
step:1925/2330 train_time:111427ms step_avg:57.88ms
step:1926/2330 train_time:111488ms step_avg:57.89ms
step:1927/2330 train_time:111544ms step_avg:57.89ms
step:1928/2330 train_time:111606ms step_avg:57.89ms
step:1929/2330 train_time:111663ms step_avg:57.89ms
step:1930/2330 train_time:111725ms step_avg:57.89ms
step:1931/2330 train_time:111781ms step_avg:57.89ms
step:1932/2330 train_time:111842ms step_avg:57.89ms
step:1933/2330 train_time:111900ms step_avg:57.89ms
step:1934/2330 train_time:111960ms step_avg:57.89ms
step:1935/2330 train_time:112016ms step_avg:57.89ms
step:1936/2330 train_time:112077ms step_avg:57.89ms
step:1937/2330 train_time:112134ms step_avg:57.89ms
step:1938/2330 train_time:112194ms step_avg:57.89ms
step:1939/2330 train_time:112251ms step_avg:57.89ms
step:1940/2330 train_time:112310ms step_avg:57.89ms
step:1941/2330 train_time:112368ms step_avg:57.89ms
step:1942/2330 train_time:112428ms step_avg:57.89ms
step:1943/2330 train_time:112484ms step_avg:57.89ms
step:1944/2330 train_time:112546ms step_avg:57.89ms
step:1945/2330 train_time:112602ms step_avg:57.89ms
step:1946/2330 train_time:112663ms step_avg:57.89ms
step:1947/2330 train_time:112720ms step_avg:57.89ms
step:1948/2330 train_time:112780ms step_avg:57.90ms
step:1949/2330 train_time:112837ms step_avg:57.89ms
step:1950/2330 train_time:112896ms step_avg:57.90ms
step:1951/2330 train_time:112953ms step_avg:57.89ms
step:1952/2330 train_time:113013ms step_avg:57.90ms
step:1953/2330 train_time:113070ms step_avg:57.90ms
step:1954/2330 train_time:113130ms step_avg:57.90ms
step:1955/2330 train_time:113186ms step_avg:57.90ms
step:1956/2330 train_time:113248ms step_avg:57.90ms
step:1957/2330 train_time:113306ms step_avg:57.90ms
step:1958/2330 train_time:113366ms step_avg:57.90ms
step:1959/2330 train_time:113423ms step_avg:57.90ms
step:1960/2330 train_time:113483ms step_avg:57.90ms
step:1961/2330 train_time:113540ms step_avg:57.90ms
step:1962/2330 train_time:113599ms step_avg:57.90ms
step:1963/2330 train_time:113656ms step_avg:57.90ms
step:1964/2330 train_time:113716ms step_avg:57.90ms
step:1965/2330 train_time:113772ms step_avg:57.90ms
step:1966/2330 train_time:113833ms step_avg:57.90ms
step:1967/2330 train_time:113890ms step_avg:57.90ms
step:1968/2330 train_time:113951ms step_avg:57.90ms
step:1969/2330 train_time:114007ms step_avg:57.90ms
step:1970/2330 train_time:114068ms step_avg:57.90ms
step:1971/2330 train_time:114124ms step_avg:57.90ms
step:1972/2330 train_time:114185ms step_avg:57.90ms
step:1973/2330 train_time:114242ms step_avg:57.90ms
step:1974/2330 train_time:114303ms step_avg:57.90ms
step:1975/2330 train_time:114360ms step_avg:57.90ms
step:1976/2330 train_time:114420ms step_avg:57.90ms
step:1977/2330 train_time:114477ms step_avg:57.90ms
step:1978/2330 train_time:114537ms step_avg:57.91ms
step:1979/2330 train_time:114594ms step_avg:57.91ms
step:1980/2330 train_time:114655ms step_avg:57.91ms
step:1981/2330 train_time:114712ms step_avg:57.91ms
step:1982/2330 train_time:114771ms step_avg:57.91ms
step:1983/2330 train_time:114828ms step_avg:57.91ms
step:1984/2330 train_time:114889ms step_avg:57.91ms
step:1985/2330 train_time:114946ms step_avg:57.91ms
step:1986/2330 train_time:115006ms step_avg:57.91ms
step:1987/2330 train_time:115063ms step_avg:57.91ms
step:1988/2330 train_time:115123ms step_avg:57.91ms
step:1989/2330 train_time:115180ms step_avg:57.91ms
step:1990/2330 train_time:115240ms step_avg:57.91ms
step:1991/2330 train_time:115297ms step_avg:57.91ms
step:1992/2330 train_time:115358ms step_avg:57.91ms
step:1993/2330 train_time:115415ms step_avg:57.91ms
step:1994/2330 train_time:115475ms step_avg:57.91ms
step:1995/2330 train_time:115531ms step_avg:57.91ms
step:1996/2330 train_time:115592ms step_avg:57.91ms
step:1997/2330 train_time:115650ms step_avg:57.91ms
step:1998/2330 train_time:115709ms step_avg:57.91ms
step:1999/2330 train_time:115767ms step_avg:57.91ms
step:2000/2330 train_time:115827ms step_avg:57.91ms
step:2000/2330 val_loss:3.8322 train_time:115908ms step_avg:57.95ms
step:2001/2330 train_time:115925ms step_avg:57.93ms
step:2002/2330 train_time:115946ms step_avg:57.92ms
step:2003/2330 train_time:116004ms step_avg:57.92ms
step:2004/2330 train_time:116074ms step_avg:57.92ms
step:2005/2330 train_time:116131ms step_avg:57.92ms
step:2006/2330 train_time:116191ms step_avg:57.92ms
step:2007/2330 train_time:116247ms step_avg:57.92ms
step:2008/2330 train_time:116309ms step_avg:57.92ms
step:2009/2330 train_time:116365ms step_avg:57.92ms
step:2010/2330 train_time:116426ms step_avg:57.92ms
step:2011/2330 train_time:116482ms step_avg:57.92ms
step:2012/2330 train_time:116542ms step_avg:57.92ms
step:2013/2330 train_time:116598ms step_avg:57.92ms
step:2014/2330 train_time:116658ms step_avg:57.92ms
step:2015/2330 train_time:116714ms step_avg:57.92ms
step:2016/2330 train_time:116773ms step_avg:57.92ms
step:2017/2330 train_time:116829ms step_avg:57.92ms
step:2018/2330 train_time:116889ms step_avg:57.92ms
step:2019/2330 train_time:116947ms step_avg:57.92ms
step:2020/2330 train_time:117011ms step_avg:57.93ms
step:2021/2330 train_time:117068ms step_avg:57.93ms
step:2022/2330 train_time:117131ms step_avg:57.93ms
step:2023/2330 train_time:117187ms step_avg:57.93ms
step:2024/2330 train_time:117250ms step_avg:57.93ms
step:2025/2330 train_time:117306ms step_avg:57.93ms
step:2026/2330 train_time:117367ms step_avg:57.93ms
step:2027/2330 train_time:117424ms step_avg:57.93ms
step:2028/2330 train_time:117483ms step_avg:57.93ms
step:2029/2330 train_time:117540ms step_avg:57.93ms
step:2030/2330 train_time:117599ms step_avg:57.93ms
step:2031/2330 train_time:117655ms step_avg:57.93ms
step:2032/2330 train_time:117715ms step_avg:57.93ms
step:2033/2330 train_time:117772ms step_avg:57.93ms
step:2034/2330 train_time:117830ms step_avg:57.93ms
step:2035/2330 train_time:117888ms step_avg:57.93ms
step:2036/2330 train_time:117949ms step_avg:57.93ms
step:2037/2330 train_time:118007ms step_avg:57.93ms
step:2038/2330 train_time:118068ms step_avg:57.93ms
step:2039/2330 train_time:118126ms step_avg:57.93ms
step:2040/2330 train_time:118186ms step_avg:57.93ms
step:2041/2330 train_time:118243ms step_avg:57.93ms
step:2042/2330 train_time:118303ms step_avg:57.93ms
step:2043/2330 train_time:118360ms step_avg:57.93ms
step:2044/2330 train_time:118420ms step_avg:57.94ms
step:2045/2330 train_time:118477ms step_avg:57.93ms
step:2046/2330 train_time:118537ms step_avg:57.94ms
step:2047/2330 train_time:118593ms step_avg:57.93ms
step:2048/2330 train_time:118652ms step_avg:57.94ms
step:2049/2330 train_time:118708ms step_avg:57.93ms
step:2050/2330 train_time:118770ms step_avg:57.94ms
step:2051/2330 train_time:118826ms step_avg:57.94ms
step:2052/2330 train_time:118886ms step_avg:57.94ms
step:2053/2330 train_time:118944ms step_avg:57.94ms
step:2054/2330 train_time:119003ms step_avg:57.94ms
step:2055/2330 train_time:119061ms step_avg:57.94ms
step:2056/2330 train_time:119122ms step_avg:57.94ms
step:2057/2330 train_time:119179ms step_avg:57.94ms
step:2058/2330 train_time:119241ms step_avg:57.94ms
step:2059/2330 train_time:119297ms step_avg:57.94ms
step:2060/2330 train_time:119358ms step_avg:57.94ms
step:2061/2330 train_time:119415ms step_avg:57.94ms
step:2062/2330 train_time:119475ms step_avg:57.94ms
step:2063/2330 train_time:119533ms step_avg:57.94ms
step:2064/2330 train_time:119592ms step_avg:57.94ms
step:2065/2330 train_time:119648ms step_avg:57.94ms
step:2066/2330 train_time:119708ms step_avg:57.94ms
step:2067/2330 train_time:119765ms step_avg:57.94ms
step:2068/2330 train_time:119825ms step_avg:57.94ms
step:2069/2330 train_time:119883ms step_avg:57.94ms
step:2070/2330 train_time:119943ms step_avg:57.94ms
step:2071/2330 train_time:120001ms step_avg:57.94ms
step:2072/2330 train_time:120060ms step_avg:57.94ms
step:2073/2330 train_time:120118ms step_avg:57.94ms
step:2074/2330 train_time:120180ms step_avg:57.95ms
step:2075/2330 train_time:120236ms step_avg:57.95ms
step:2076/2330 train_time:120296ms step_avg:57.95ms
step:2077/2330 train_time:120353ms step_avg:57.95ms
step:2078/2330 train_time:120413ms step_avg:57.95ms
step:2079/2330 train_time:120470ms step_avg:57.95ms
step:2080/2330 train_time:120530ms step_avg:57.95ms
step:2081/2330 train_time:120586ms step_avg:57.95ms
step:2082/2330 train_time:120647ms step_avg:57.95ms
step:2083/2330 train_time:120704ms step_avg:57.95ms
step:2084/2330 train_time:120764ms step_avg:57.95ms
step:2085/2330 train_time:120820ms step_avg:57.95ms
step:2086/2330 train_time:120881ms step_avg:57.95ms
step:2087/2330 train_time:120938ms step_avg:57.95ms
step:2088/2330 train_time:120998ms step_avg:57.95ms
step:2089/2330 train_time:121054ms step_avg:57.95ms
step:2090/2330 train_time:121115ms step_avg:57.95ms
step:2091/2330 train_time:121172ms step_avg:57.95ms
step:2092/2330 train_time:121232ms step_avg:57.95ms
step:2093/2330 train_time:121289ms step_avg:57.95ms
step:2094/2330 train_time:121351ms step_avg:57.95ms
step:2095/2330 train_time:121407ms step_avg:57.95ms
step:2096/2330 train_time:121468ms step_avg:57.95ms
step:2097/2330 train_time:121524ms step_avg:57.95ms
step:2098/2330 train_time:121585ms step_avg:57.95ms
step:2099/2330 train_time:121641ms step_avg:57.95ms
step:2100/2330 train_time:121702ms step_avg:57.95ms
step:2101/2330 train_time:121759ms step_avg:57.95ms
step:2102/2330 train_time:121820ms step_avg:57.95ms
step:2103/2330 train_time:121877ms step_avg:57.95ms
step:2104/2330 train_time:121937ms step_avg:57.95ms
step:2105/2330 train_time:121994ms step_avg:57.95ms
step:2106/2330 train_time:122054ms step_avg:57.96ms
step:2107/2330 train_time:122110ms step_avg:57.95ms
step:2108/2330 train_time:122171ms step_avg:57.96ms
step:2109/2330 train_time:122229ms step_avg:57.96ms
step:2110/2330 train_time:122288ms step_avg:57.96ms
step:2111/2330 train_time:122346ms step_avg:57.96ms
step:2112/2330 train_time:122406ms step_avg:57.96ms
step:2113/2330 train_time:122463ms step_avg:57.96ms
step:2114/2330 train_time:122523ms step_avg:57.96ms
step:2115/2330 train_time:122581ms step_avg:57.96ms
step:2116/2330 train_time:122640ms step_avg:57.96ms
step:2117/2330 train_time:122696ms step_avg:57.96ms
step:2118/2330 train_time:122757ms step_avg:57.96ms
step:2119/2330 train_time:122813ms step_avg:57.96ms
step:2120/2330 train_time:122874ms step_avg:57.96ms
step:2121/2330 train_time:122930ms step_avg:57.96ms
step:2122/2330 train_time:122991ms step_avg:57.96ms
step:2123/2330 train_time:123048ms step_avg:57.96ms
step:2124/2330 train_time:123108ms step_avg:57.96ms
step:2125/2330 train_time:123166ms step_avg:57.96ms
step:2126/2330 train_time:123226ms step_avg:57.96ms
step:2127/2330 train_time:123283ms step_avg:57.96ms
step:2128/2330 train_time:123344ms step_avg:57.96ms
step:2129/2330 train_time:123401ms step_avg:57.96ms
step:2130/2330 train_time:123461ms step_avg:57.96ms
step:2131/2330 train_time:123518ms step_avg:57.96ms
step:2132/2330 train_time:123579ms step_avg:57.96ms
step:2133/2330 train_time:123636ms step_avg:57.96ms
step:2134/2330 train_time:123696ms step_avg:57.96ms
step:2135/2330 train_time:123752ms step_avg:57.96ms
step:2136/2330 train_time:123813ms step_avg:57.96ms
step:2137/2330 train_time:123870ms step_avg:57.96ms
step:2138/2330 train_time:123931ms step_avg:57.97ms
step:2139/2330 train_time:123988ms step_avg:57.97ms
step:2140/2330 train_time:124048ms step_avg:57.97ms
step:2141/2330 train_time:124105ms step_avg:57.97ms
step:2142/2330 train_time:124166ms step_avg:57.97ms
step:2143/2330 train_time:124223ms step_avg:57.97ms
step:2144/2330 train_time:124283ms step_avg:57.97ms
step:2145/2330 train_time:124340ms step_avg:57.97ms
step:2146/2330 train_time:124400ms step_avg:57.97ms
step:2147/2330 train_time:124458ms step_avg:57.97ms
step:2148/2330 train_time:124518ms step_avg:57.97ms
step:2149/2330 train_time:124575ms step_avg:57.97ms
step:2150/2330 train_time:124635ms step_avg:57.97ms
step:2151/2330 train_time:124692ms step_avg:57.97ms
step:2152/2330 train_time:124753ms step_avg:57.97ms
step:2153/2330 train_time:124810ms step_avg:57.97ms
step:2154/2330 train_time:124870ms step_avg:57.97ms
step:2155/2330 train_time:124927ms step_avg:57.97ms
step:2156/2330 train_time:124987ms step_avg:57.97ms
step:2157/2330 train_time:125045ms step_avg:57.97ms
step:2158/2330 train_time:125105ms step_avg:57.97ms
step:2159/2330 train_time:125162ms step_avg:57.97ms
step:2160/2330 train_time:125223ms step_avg:57.97ms
step:2161/2330 train_time:125280ms step_avg:57.97ms
step:2162/2330 train_time:125342ms step_avg:57.97ms
step:2163/2330 train_time:125399ms step_avg:57.97ms
step:2164/2330 train_time:125460ms step_avg:57.98ms
step:2165/2330 train_time:125517ms step_avg:57.98ms
step:2166/2330 train_time:125577ms step_avg:57.98ms
step:2167/2330 train_time:125634ms step_avg:57.98ms
step:2168/2330 train_time:125694ms step_avg:57.98ms
step:2169/2330 train_time:125750ms step_avg:57.98ms
step:2170/2330 train_time:125811ms step_avg:57.98ms
step:2171/2330 train_time:125867ms step_avg:57.98ms
step:2172/2330 train_time:125928ms step_avg:57.98ms
step:2173/2330 train_time:125985ms step_avg:57.98ms
step:2174/2330 train_time:126046ms step_avg:57.98ms
step:2175/2330 train_time:126103ms step_avg:57.98ms
step:2176/2330 train_time:126165ms step_avg:57.98ms
step:2177/2330 train_time:126221ms step_avg:57.98ms
step:2178/2330 train_time:126282ms step_avg:57.98ms
step:2179/2330 train_time:126339ms step_avg:57.98ms
step:2180/2330 train_time:126400ms step_avg:57.98ms
step:2181/2330 train_time:126456ms step_avg:57.98ms
step:2182/2330 train_time:126517ms step_avg:57.98ms
step:2183/2330 train_time:126575ms step_avg:57.98ms
step:2184/2330 train_time:126634ms step_avg:57.98ms
step:2185/2330 train_time:126691ms step_avg:57.98ms
step:2186/2330 train_time:126752ms step_avg:57.98ms
step:2187/2330 train_time:126808ms step_avg:57.98ms
step:2188/2330 train_time:126870ms step_avg:57.98ms
step:2189/2330 train_time:126926ms step_avg:57.98ms
step:2190/2330 train_time:126986ms step_avg:57.98ms
step:2191/2330 train_time:127043ms step_avg:57.98ms
step:2192/2330 train_time:127104ms step_avg:57.99ms
step:2193/2330 train_time:127160ms step_avg:57.98ms
step:2194/2330 train_time:127222ms step_avg:57.99ms
step:2195/2330 train_time:127279ms step_avg:57.99ms
step:2196/2330 train_time:127338ms step_avg:57.99ms
step:2197/2330 train_time:127395ms step_avg:57.99ms
step:2198/2330 train_time:127456ms step_avg:57.99ms
step:2199/2330 train_time:127513ms step_avg:57.99ms
step:2200/2330 train_time:127574ms step_avg:57.99ms
step:2201/2330 train_time:127630ms step_avg:57.99ms
step:2202/2330 train_time:127691ms step_avg:57.99ms
step:2203/2330 train_time:127747ms step_avg:57.99ms
step:2204/2330 train_time:127808ms step_avg:57.99ms
step:2205/2330 train_time:127864ms step_avg:57.99ms
step:2206/2330 train_time:127925ms step_avg:57.99ms
step:2207/2330 train_time:127981ms step_avg:57.99ms
step:2208/2330 train_time:128041ms step_avg:57.99ms
step:2209/2330 train_time:128098ms step_avg:57.99ms
step:2210/2330 train_time:128159ms step_avg:57.99ms
step:2211/2330 train_time:128216ms step_avg:57.99ms
step:2212/2330 train_time:128276ms step_avg:57.99ms
step:2213/2330 train_time:128333ms step_avg:57.99ms
step:2214/2330 train_time:128394ms step_avg:57.99ms
step:2215/2330 train_time:128450ms step_avg:57.99ms
step:2216/2330 train_time:128511ms step_avg:57.99ms
step:2217/2330 train_time:128568ms step_avg:57.99ms
step:2218/2330 train_time:128629ms step_avg:57.99ms
step:2219/2330 train_time:128686ms step_avg:57.99ms
step:2220/2330 train_time:128745ms step_avg:57.99ms
step:2221/2330 train_time:128801ms step_avg:57.99ms
step:2222/2330 train_time:128863ms step_avg:57.99ms
step:2223/2330 train_time:128919ms step_avg:57.99ms
step:2224/2330 train_time:128980ms step_avg:57.99ms
step:2225/2330 train_time:129036ms step_avg:57.99ms
step:2226/2330 train_time:129096ms step_avg:57.99ms
step:2227/2330 train_time:129153ms step_avg:57.99ms
step:2228/2330 train_time:129214ms step_avg:58.00ms
step:2229/2330 train_time:129271ms step_avg:57.99ms
step:2230/2330 train_time:129331ms step_avg:58.00ms
step:2231/2330 train_time:129387ms step_avg:58.00ms
step:2232/2330 train_time:129449ms step_avg:58.00ms
step:2233/2330 train_time:129506ms step_avg:58.00ms
step:2234/2330 train_time:129567ms step_avg:58.00ms
step:2235/2330 train_time:129624ms step_avg:58.00ms
step:2236/2330 train_time:129685ms step_avg:58.00ms
step:2237/2330 train_time:129742ms step_avg:58.00ms
step:2238/2330 train_time:129802ms step_avg:58.00ms
step:2239/2330 train_time:129860ms step_avg:58.00ms
step:2240/2330 train_time:129920ms step_avg:58.00ms
step:2241/2330 train_time:129978ms step_avg:58.00ms
step:2242/2330 train_time:130037ms step_avg:58.00ms
step:2243/2330 train_time:130094ms step_avg:58.00ms
step:2244/2330 train_time:130154ms step_avg:58.00ms
step:2245/2330 train_time:130210ms step_avg:58.00ms
step:2246/2330 train_time:130272ms step_avg:58.00ms
step:2247/2330 train_time:130329ms step_avg:58.00ms
step:2248/2330 train_time:130390ms step_avg:58.00ms
step:2249/2330 train_time:130446ms step_avg:58.00ms
step:2250/2330 train_time:130507ms step_avg:58.00ms
step:2250/2330 val_loss:3.7875 train_time:130589ms step_avg:58.04ms
step:2251/2330 train_time:130607ms step_avg:58.02ms
step:2252/2330 train_time:130628ms step_avg:58.01ms
step:2253/2330 train_time:130688ms step_avg:58.01ms
step:2254/2330 train_time:130753ms step_avg:58.01ms
step:2255/2330 train_time:130809ms step_avg:58.01ms
step:2256/2330 train_time:130871ms step_avg:58.01ms
step:2257/2330 train_time:130928ms step_avg:58.01ms
step:2258/2330 train_time:130989ms step_avg:58.01ms
step:2259/2330 train_time:131045ms step_avg:58.01ms
step:2260/2330 train_time:131105ms step_avg:58.01ms
step:2261/2330 train_time:131162ms step_avg:58.01ms
step:2262/2330 train_time:131221ms step_avg:58.01ms
step:2263/2330 train_time:131277ms step_avg:58.01ms
step:2264/2330 train_time:131337ms step_avg:58.01ms
step:2265/2330 train_time:131393ms step_avg:58.01ms
step:2266/2330 train_time:131452ms step_avg:58.01ms
step:2267/2330 train_time:131508ms step_avg:58.01ms
step:2268/2330 train_time:131569ms step_avg:58.01ms
step:2269/2330 train_time:131627ms step_avg:58.01ms
step:2270/2330 train_time:131691ms step_avg:58.01ms
step:2271/2330 train_time:131748ms step_avg:58.01ms
step:2272/2330 train_time:131809ms step_avg:58.01ms
step:2273/2330 train_time:131867ms step_avg:58.01ms
step:2274/2330 train_time:131927ms step_avg:58.02ms
step:2275/2330 train_time:131984ms step_avg:58.02ms
step:2276/2330 train_time:132044ms step_avg:58.02ms
step:2277/2330 train_time:132101ms step_avg:58.02ms
step:2278/2330 train_time:132160ms step_avg:58.02ms
step:2279/2330 train_time:132217ms step_avg:58.02ms
step:2280/2330 train_time:132276ms step_avg:58.02ms
step:2281/2330 train_time:132332ms step_avg:58.01ms
step:2282/2330 train_time:132393ms step_avg:58.02ms
step:2283/2330 train_time:132449ms step_avg:58.02ms
step:2284/2330 train_time:132509ms step_avg:58.02ms
step:2285/2330 train_time:132566ms step_avg:58.02ms
step:2286/2330 train_time:132626ms step_avg:58.02ms
step:2287/2330 train_time:132684ms step_avg:58.02ms
step:2288/2330 train_time:132745ms step_avg:58.02ms
step:2289/2330 train_time:132802ms step_avg:58.02ms
step:2290/2330 train_time:132863ms step_avg:58.02ms
step:2291/2330 train_time:132920ms step_avg:58.02ms
step:2292/2330 train_time:132980ms step_avg:58.02ms
step:2293/2330 train_time:133037ms step_avg:58.02ms
step:2294/2330 train_time:133097ms step_avg:58.02ms
step:2295/2330 train_time:133155ms step_avg:58.02ms
step:2296/2330 train_time:133214ms step_avg:58.02ms
step:2297/2330 train_time:133271ms step_avg:58.02ms
step:2298/2330 train_time:133330ms step_avg:58.02ms
step:2299/2330 train_time:133386ms step_avg:58.02ms
step:2300/2330 train_time:133446ms step_avg:58.02ms
step:2301/2330 train_time:133503ms step_avg:58.02ms
step:2302/2330 train_time:133563ms step_avg:58.02ms
step:2303/2330 train_time:133621ms step_avg:58.02ms
step:2304/2330 train_time:133681ms step_avg:58.02ms
step:2305/2330 train_time:133738ms step_avg:58.02ms
step:2306/2330 train_time:133799ms step_avg:58.02ms
step:2307/2330 train_time:133856ms step_avg:58.02ms
step:2308/2330 train_time:133917ms step_avg:58.02ms
step:2309/2330 train_time:133974ms step_avg:58.02ms
step:2310/2330 train_time:134035ms step_avg:58.02ms
step:2311/2330 train_time:134091ms step_avg:58.02ms
step:2312/2330 train_time:134151ms step_avg:58.02ms
step:2313/2330 train_time:134208ms step_avg:58.02ms
step:2314/2330 train_time:134268ms step_avg:58.02ms
step:2315/2330 train_time:134325ms step_avg:58.02ms
step:2316/2330 train_time:134384ms step_avg:58.02ms
step:2317/2330 train_time:134441ms step_avg:58.02ms
step:2318/2330 train_time:134501ms step_avg:58.02ms
step:2319/2330 train_time:134557ms step_avg:58.02ms
step:2320/2330 train_time:134618ms step_avg:58.02ms
step:2321/2330 train_time:134674ms step_avg:58.02ms
step:2322/2330 train_time:134735ms step_avg:58.03ms
step:2323/2330 train_time:134793ms step_avg:58.03ms
step:2324/2330 train_time:134853ms step_avg:58.03ms
step:2325/2330 train_time:134910ms step_avg:58.03ms
step:2326/2330 train_time:134971ms step_avg:58.03ms
step:2327/2330 train_time:135027ms step_avg:58.03ms
step:2328/2330 train_time:135088ms step_avg:58.03ms
step:2329/2330 train_time:135146ms step_avg:58.03ms
step:2330/2330 train_time:135205ms step_avg:58.03ms
step:2330/2330 val_loss:3.7729 train_time:135287ms step_avg:58.06ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
