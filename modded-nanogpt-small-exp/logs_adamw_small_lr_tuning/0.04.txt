import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--adamw_lr', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_lr_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_lr_tuning/{args2.adamw_lr}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=float(args2.adamw_lr),  betas=(0.85, 0.85), eps=1e-10, weight_decay=0.0, fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_lr_tuning/{args2.adamw_lr}", exist_ok=True)
            torch.save(log, f"logs_adamw_small_lr_tuning/{args2.adamw_lr}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 02:29:13 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   26C    P0             116W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   25C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   27C    P0             114W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   26C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   26C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:92ms step_avg:91.62ms
step:2/2330 train_time:181ms step_avg:90.45ms
step:3/2330 train_time:200ms step_avg:66.78ms
step:4/2330 train_time:220ms step_avg:55.03ms
step:5/2330 train_time:272ms step_avg:54.41ms
step:6/2330 train_time:329ms step_avg:54.91ms
step:7/2330 train_time:383ms step_avg:54.73ms
step:8/2330 train_time:441ms step_avg:55.08ms
step:9/2330 train_time:495ms step_avg:54.96ms
step:10/2330 train_time:552ms step_avg:55.16ms
step:11/2330 train_time:606ms step_avg:55.07ms
step:12/2330 train_time:663ms step_avg:55.22ms
step:13/2330 train_time:717ms step_avg:55.13ms
step:14/2330 train_time:773ms step_avg:55.25ms
step:15/2330 train_time:828ms step_avg:55.18ms
step:16/2330 train_time:885ms step_avg:55.30ms
step:17/2330 train_time:939ms step_avg:55.21ms
step:18/2330 train_time:996ms step_avg:55.32ms
step:19/2330 train_time:1050ms step_avg:55.25ms
step:20/2330 train_time:1107ms step_avg:55.35ms
step:21/2330 train_time:1162ms step_avg:55.34ms
step:22/2330 train_time:1219ms step_avg:55.42ms
step:23/2330 train_time:1274ms step_avg:55.37ms
step:24/2330 train_time:1331ms step_avg:55.45ms
step:25/2330 train_time:1385ms step_avg:55.41ms
step:26/2330 train_time:1443ms step_avg:55.50ms
step:27/2330 train_time:1498ms step_avg:55.47ms
step:28/2330 train_time:1555ms step_avg:55.53ms
step:29/2330 train_time:1609ms step_avg:55.47ms
step:30/2330 train_time:1666ms step_avg:55.54ms
step:31/2330 train_time:1721ms step_avg:55.51ms
step:32/2330 train_time:1778ms step_avg:55.55ms
step:33/2330 train_time:1832ms step_avg:55.51ms
step:34/2330 train_time:1889ms step_avg:55.56ms
step:35/2330 train_time:1944ms step_avg:55.53ms
step:36/2330 train_time:2001ms step_avg:55.57ms
step:37/2330 train_time:2055ms step_avg:55.55ms
step:38/2330 train_time:2112ms step_avg:55.59ms
step:39/2330 train_time:2167ms step_avg:55.55ms
step:40/2330 train_time:2225ms step_avg:55.61ms
step:41/2330 train_time:2279ms step_avg:55.58ms
step:42/2330 train_time:2337ms step_avg:55.65ms
step:43/2330 train_time:2392ms step_avg:55.62ms
step:44/2330 train_time:2449ms step_avg:55.66ms
step:45/2330 train_time:2504ms step_avg:55.64ms
step:46/2330 train_time:2561ms step_avg:55.68ms
step:47/2330 train_time:2615ms step_avg:55.65ms
step:48/2330 train_time:2673ms step_avg:55.68ms
step:49/2330 train_time:2727ms step_avg:55.65ms
step:50/2330 train_time:2785ms step_avg:55.69ms
step:51/2330 train_time:2839ms step_avg:55.66ms
step:52/2330 train_time:2897ms step_avg:55.70ms
step:53/2330 train_time:2951ms step_avg:55.69ms
step:54/2330 train_time:3009ms step_avg:55.71ms
step:55/2330 train_time:3063ms step_avg:55.69ms
step:56/2330 train_time:3121ms step_avg:55.73ms
step:57/2330 train_time:3176ms step_avg:55.71ms
step:58/2330 train_time:3234ms step_avg:55.76ms
step:59/2330 train_time:3289ms step_avg:55.74ms
step:60/2330 train_time:3347ms step_avg:55.78ms
step:61/2330 train_time:3402ms step_avg:55.76ms
step:62/2330 train_time:3460ms step_avg:55.81ms
step:63/2330 train_time:3515ms step_avg:55.79ms
step:64/2330 train_time:3572ms step_avg:55.82ms
step:65/2330 train_time:3627ms step_avg:55.80ms
step:66/2330 train_time:3685ms step_avg:55.83ms
step:67/2330 train_time:3740ms step_avg:55.82ms
step:68/2330 train_time:3798ms step_avg:55.85ms
step:69/2330 train_time:3853ms step_avg:55.84ms
step:70/2330 train_time:3911ms step_avg:55.87ms
step:71/2330 train_time:3966ms step_avg:55.86ms
step:72/2330 train_time:4023ms step_avg:55.88ms
step:73/2330 train_time:4078ms step_avg:55.87ms
step:74/2330 train_time:4137ms step_avg:55.90ms
step:75/2330 train_time:4192ms step_avg:55.89ms
step:76/2330 train_time:4249ms step_avg:55.91ms
step:77/2330 train_time:4305ms step_avg:55.90ms
step:78/2330 train_time:4363ms step_avg:55.94ms
step:79/2330 train_time:4418ms step_avg:55.92ms
step:80/2330 train_time:4475ms step_avg:55.94ms
step:81/2330 train_time:4530ms step_avg:55.92ms
step:82/2330 train_time:4588ms step_avg:55.95ms
step:83/2330 train_time:4642ms step_avg:55.93ms
step:84/2330 train_time:4700ms step_avg:55.96ms
step:85/2330 train_time:4755ms step_avg:55.94ms
step:86/2330 train_time:4813ms step_avg:55.97ms
step:87/2330 train_time:4868ms step_avg:55.95ms
step:88/2330 train_time:4927ms step_avg:55.99ms
step:89/2330 train_time:4982ms step_avg:55.97ms
step:90/2330 train_time:5040ms step_avg:56.00ms
step:91/2330 train_time:5095ms step_avg:55.99ms
step:92/2330 train_time:5152ms step_avg:56.00ms
step:93/2330 train_time:5207ms step_avg:55.99ms
step:94/2330 train_time:5265ms step_avg:56.01ms
step:95/2330 train_time:5320ms step_avg:56.00ms
step:96/2330 train_time:5379ms step_avg:56.03ms
step:97/2330 train_time:5434ms step_avg:56.02ms
step:98/2330 train_time:5493ms step_avg:56.05ms
step:99/2330 train_time:5547ms step_avg:56.03ms
step:100/2330 train_time:5606ms step_avg:56.06ms
step:101/2330 train_time:5661ms step_avg:56.05ms
step:102/2330 train_time:5720ms step_avg:56.07ms
step:103/2330 train_time:5774ms step_avg:56.06ms
step:104/2330 train_time:5833ms step_avg:56.09ms
step:105/2330 train_time:5888ms step_avg:56.07ms
step:106/2330 train_time:5946ms step_avg:56.10ms
step:107/2330 train_time:6002ms step_avg:56.09ms
step:108/2330 train_time:6060ms step_avg:56.11ms
step:109/2330 train_time:6115ms step_avg:56.10ms
step:110/2330 train_time:6173ms step_avg:56.12ms
step:111/2330 train_time:6228ms step_avg:56.10ms
step:112/2330 train_time:6286ms step_avg:56.12ms
step:113/2330 train_time:6341ms step_avg:56.11ms
step:114/2330 train_time:6400ms step_avg:56.14ms
step:115/2330 train_time:6455ms step_avg:56.13ms
step:116/2330 train_time:6512ms step_avg:56.14ms
step:117/2330 train_time:6568ms step_avg:56.13ms
step:118/2330 train_time:6625ms step_avg:56.15ms
step:119/2330 train_time:6680ms step_avg:56.14ms
step:120/2330 train_time:6738ms step_avg:56.15ms
step:121/2330 train_time:6793ms step_avg:56.14ms
step:122/2330 train_time:6852ms step_avg:56.16ms
step:123/2330 train_time:6907ms step_avg:56.15ms
step:124/2330 train_time:6964ms step_avg:56.17ms
step:125/2330 train_time:7020ms step_avg:56.16ms
step:126/2330 train_time:7078ms step_avg:56.17ms
step:127/2330 train_time:7133ms step_avg:56.17ms
step:128/2330 train_time:7191ms step_avg:56.18ms
step:129/2330 train_time:7246ms step_avg:56.17ms
step:130/2330 train_time:7305ms step_avg:56.19ms
step:131/2330 train_time:7361ms step_avg:56.19ms
step:132/2330 train_time:7418ms step_avg:56.20ms
step:133/2330 train_time:7473ms step_avg:56.19ms
step:134/2330 train_time:7532ms step_avg:56.21ms
step:135/2330 train_time:7587ms step_avg:56.20ms
step:136/2330 train_time:7646ms step_avg:56.22ms
step:137/2330 train_time:7701ms step_avg:56.21ms
step:138/2330 train_time:7759ms step_avg:56.23ms
step:139/2330 train_time:7815ms step_avg:56.22ms
step:140/2330 train_time:7872ms step_avg:56.23ms
step:141/2330 train_time:7928ms step_avg:56.22ms
step:142/2330 train_time:7986ms step_avg:56.24ms
step:143/2330 train_time:8041ms step_avg:56.23ms
step:144/2330 train_time:8099ms step_avg:56.24ms
step:145/2330 train_time:8154ms step_avg:56.23ms
step:146/2330 train_time:8211ms step_avg:56.24ms
step:147/2330 train_time:8267ms step_avg:56.24ms
step:148/2330 train_time:8325ms step_avg:56.25ms
step:149/2330 train_time:8380ms step_avg:56.24ms
step:150/2330 train_time:8439ms step_avg:56.26ms
step:151/2330 train_time:8494ms step_avg:56.25ms
step:152/2330 train_time:8551ms step_avg:56.26ms
step:153/2330 train_time:8606ms step_avg:56.25ms
step:154/2330 train_time:8664ms step_avg:56.26ms
step:155/2330 train_time:8720ms step_avg:56.26ms
step:156/2330 train_time:8779ms step_avg:56.27ms
step:157/2330 train_time:8834ms step_avg:56.27ms
step:158/2330 train_time:8892ms step_avg:56.28ms
step:159/2330 train_time:8947ms step_avg:56.27ms
step:160/2330 train_time:9005ms step_avg:56.28ms
step:161/2330 train_time:9061ms step_avg:56.28ms
step:162/2330 train_time:9119ms step_avg:56.29ms
step:163/2330 train_time:9174ms step_avg:56.28ms
step:164/2330 train_time:9232ms step_avg:56.29ms
step:165/2330 train_time:9287ms step_avg:56.28ms
step:166/2330 train_time:9345ms step_avg:56.30ms
step:167/2330 train_time:9401ms step_avg:56.29ms
step:168/2330 train_time:9460ms step_avg:56.31ms
step:169/2330 train_time:9514ms step_avg:56.30ms
step:170/2330 train_time:9574ms step_avg:56.32ms
step:171/2330 train_time:9628ms step_avg:56.31ms
step:172/2330 train_time:9687ms step_avg:56.32ms
step:173/2330 train_time:9743ms step_avg:56.32ms
step:174/2330 train_time:9801ms step_avg:56.33ms
step:175/2330 train_time:9856ms step_avg:56.32ms
step:176/2330 train_time:9915ms step_avg:56.33ms
step:177/2330 train_time:9970ms step_avg:56.33ms
step:178/2330 train_time:10028ms step_avg:56.34ms
step:179/2330 train_time:10084ms step_avg:56.34ms
step:180/2330 train_time:10143ms step_avg:56.35ms
step:181/2330 train_time:10198ms step_avg:56.34ms
step:182/2330 train_time:10256ms step_avg:56.35ms
step:183/2330 train_time:10311ms step_avg:56.35ms
step:184/2330 train_time:10370ms step_avg:56.36ms
step:185/2330 train_time:10426ms step_avg:56.36ms
step:186/2330 train_time:10484ms step_avg:56.36ms
step:187/2330 train_time:10538ms step_avg:56.36ms
step:188/2330 train_time:10597ms step_avg:56.37ms
step:189/2330 train_time:10652ms step_avg:56.36ms
step:190/2330 train_time:10710ms step_avg:56.37ms
step:191/2330 train_time:10765ms step_avg:56.36ms
step:192/2330 train_time:10824ms step_avg:56.37ms
step:193/2330 train_time:10879ms step_avg:56.37ms
step:194/2330 train_time:10937ms step_avg:56.38ms
step:195/2330 train_time:10992ms step_avg:56.37ms
step:196/2330 train_time:11050ms step_avg:56.38ms
step:197/2330 train_time:11106ms step_avg:56.37ms
step:198/2330 train_time:11164ms step_avg:56.38ms
step:199/2330 train_time:11219ms step_avg:56.38ms
step:200/2330 train_time:11278ms step_avg:56.39ms
step:201/2330 train_time:11333ms step_avg:56.38ms
step:202/2330 train_time:11391ms step_avg:56.39ms
step:203/2330 train_time:11446ms step_avg:56.38ms
step:204/2330 train_time:11505ms step_avg:56.39ms
step:205/2330 train_time:11560ms step_avg:56.39ms
step:206/2330 train_time:11618ms step_avg:56.40ms
step:207/2330 train_time:11673ms step_avg:56.39ms
step:208/2330 train_time:11732ms step_avg:56.41ms
step:209/2330 train_time:11787ms step_avg:56.40ms
step:210/2330 train_time:11847ms step_avg:56.41ms
step:211/2330 train_time:11902ms step_avg:56.41ms
step:212/2330 train_time:11961ms step_avg:56.42ms
step:213/2330 train_time:12016ms step_avg:56.41ms
step:214/2330 train_time:12075ms step_avg:56.42ms
step:215/2330 train_time:12130ms step_avg:56.42ms
step:216/2330 train_time:12189ms step_avg:56.43ms
step:217/2330 train_time:12245ms step_avg:56.43ms
step:218/2330 train_time:12303ms step_avg:56.43ms
step:219/2330 train_time:12358ms step_avg:56.43ms
step:220/2330 train_time:12417ms step_avg:56.44ms
step:221/2330 train_time:12472ms step_avg:56.43ms
step:222/2330 train_time:12531ms step_avg:56.44ms
step:223/2330 train_time:12586ms step_avg:56.44ms
step:224/2330 train_time:12644ms step_avg:56.45ms
step:225/2330 train_time:12700ms step_avg:56.44ms
step:226/2330 train_time:12759ms step_avg:56.45ms
step:227/2330 train_time:12814ms step_avg:56.45ms
step:228/2330 train_time:12872ms step_avg:56.46ms
step:229/2330 train_time:12927ms step_avg:56.45ms
step:230/2330 train_time:12986ms step_avg:56.46ms
step:231/2330 train_time:13042ms step_avg:56.46ms
step:232/2330 train_time:13100ms step_avg:56.47ms
step:233/2330 train_time:13155ms step_avg:56.46ms
step:234/2330 train_time:13215ms step_avg:56.47ms
step:235/2330 train_time:13270ms step_avg:56.47ms
step:236/2330 train_time:13329ms step_avg:56.48ms
step:237/2330 train_time:13385ms step_avg:56.48ms
step:238/2330 train_time:13443ms step_avg:56.48ms
step:239/2330 train_time:13499ms step_avg:56.48ms
step:240/2330 train_time:13558ms step_avg:56.49ms
step:241/2330 train_time:13613ms step_avg:56.49ms
step:242/2330 train_time:13671ms step_avg:56.49ms
step:243/2330 train_time:13727ms step_avg:56.49ms
step:244/2330 train_time:13785ms step_avg:56.50ms
step:245/2330 train_time:13841ms step_avg:56.49ms
step:246/2330 train_time:13900ms step_avg:56.50ms
step:247/2330 train_time:13955ms step_avg:56.50ms
step:248/2330 train_time:14015ms step_avg:56.51ms
step:249/2330 train_time:14070ms step_avg:56.51ms
step:250/2330 train_time:14128ms step_avg:56.51ms
step:250/2330 val_loss:6.4329 train_time:14207ms step_avg:56.83ms
step:251/2330 train_time:14225ms step_avg:56.67ms
step:252/2330 train_time:14244ms step_avg:56.53ms
step:253/2330 train_time:14298ms step_avg:56.52ms
step:254/2330 train_time:14363ms step_avg:56.55ms
step:255/2330 train_time:14417ms step_avg:56.54ms
step:256/2330 train_time:14482ms step_avg:56.57ms
step:257/2330 train_time:14537ms step_avg:56.56ms
step:258/2330 train_time:14596ms step_avg:56.57ms
step:259/2330 train_time:14651ms step_avg:56.57ms
step:260/2330 train_time:14710ms step_avg:56.58ms
step:261/2330 train_time:14765ms step_avg:56.57ms
step:262/2330 train_time:14823ms step_avg:56.58ms
step:263/2330 train_time:14879ms step_avg:56.57ms
step:264/2330 train_time:14936ms step_avg:56.58ms
step:265/2330 train_time:14991ms step_avg:56.57ms
step:266/2330 train_time:15049ms step_avg:56.58ms
step:267/2330 train_time:15105ms step_avg:56.57ms
step:268/2330 train_time:15163ms step_avg:56.58ms
step:269/2330 train_time:15219ms step_avg:56.58ms
step:270/2330 train_time:15278ms step_avg:56.58ms
step:271/2330 train_time:15334ms step_avg:56.58ms
step:272/2330 train_time:15395ms step_avg:56.60ms
step:273/2330 train_time:15450ms step_avg:56.59ms
step:274/2330 train_time:15510ms step_avg:56.61ms
step:275/2330 train_time:15565ms step_avg:56.60ms
step:276/2330 train_time:15625ms step_avg:56.61ms
step:277/2330 train_time:15680ms step_avg:56.61ms
step:278/2330 train_time:15738ms step_avg:56.61ms
step:279/2330 train_time:15793ms step_avg:56.61ms
step:280/2330 train_time:15852ms step_avg:56.61ms
step:281/2330 train_time:15907ms step_avg:56.61ms
step:282/2330 train_time:15965ms step_avg:56.61ms
step:283/2330 train_time:16021ms step_avg:56.61ms
step:284/2330 train_time:16078ms step_avg:56.61ms
step:285/2330 train_time:16134ms step_avg:56.61ms
step:286/2330 train_time:16192ms step_avg:56.62ms
step:287/2330 train_time:16248ms step_avg:56.61ms
step:288/2330 train_time:16308ms step_avg:56.62ms
step:289/2330 train_time:16364ms step_avg:56.62ms
step:290/2330 train_time:16424ms step_avg:56.63ms
step:291/2330 train_time:16479ms step_avg:56.63ms
step:292/2330 train_time:16539ms step_avg:56.64ms
step:293/2330 train_time:16594ms step_avg:56.63ms
step:294/2330 train_time:16653ms step_avg:56.64ms
step:295/2330 train_time:16708ms step_avg:56.64ms
step:296/2330 train_time:16768ms step_avg:56.65ms
step:297/2330 train_time:16823ms step_avg:56.64ms
step:298/2330 train_time:16882ms step_avg:56.65ms
step:299/2330 train_time:16937ms step_avg:56.65ms
step:300/2330 train_time:16996ms step_avg:56.65ms
step:301/2330 train_time:17051ms step_avg:56.65ms
step:302/2330 train_time:17110ms step_avg:56.65ms
step:303/2330 train_time:17165ms step_avg:56.65ms
step:304/2330 train_time:17224ms step_avg:56.66ms
step:305/2330 train_time:17280ms step_avg:56.66ms
step:306/2330 train_time:17339ms step_avg:56.66ms
step:307/2330 train_time:17395ms step_avg:56.66ms
step:308/2330 train_time:17454ms step_avg:56.67ms
step:309/2330 train_time:17509ms step_avg:56.66ms
step:310/2330 train_time:17569ms step_avg:56.67ms
step:311/2330 train_time:17625ms step_avg:56.67ms
step:312/2330 train_time:17683ms step_avg:56.68ms
step:313/2330 train_time:17739ms step_avg:56.67ms
step:314/2330 train_time:17797ms step_avg:56.68ms
step:315/2330 train_time:17852ms step_avg:56.67ms
step:316/2330 train_time:17911ms step_avg:56.68ms
step:317/2330 train_time:17967ms step_avg:56.68ms
step:318/2330 train_time:18025ms step_avg:56.68ms
step:319/2330 train_time:18080ms step_avg:56.68ms
step:320/2330 train_time:18138ms step_avg:56.68ms
step:321/2330 train_time:18194ms step_avg:56.68ms
step:322/2330 train_time:18253ms step_avg:56.69ms
step:323/2330 train_time:18308ms step_avg:56.68ms
step:324/2330 train_time:18368ms step_avg:56.69ms
step:325/2330 train_time:18423ms step_avg:56.69ms
step:326/2330 train_time:18483ms step_avg:56.70ms
step:327/2330 train_time:18539ms step_avg:56.69ms
step:328/2330 train_time:18598ms step_avg:56.70ms
step:329/2330 train_time:18653ms step_avg:56.70ms
step:330/2330 train_time:18712ms step_avg:56.70ms
step:331/2330 train_time:18767ms step_avg:56.70ms
step:332/2330 train_time:18826ms step_avg:56.71ms
step:333/2330 train_time:18882ms step_avg:56.70ms
step:334/2330 train_time:18940ms step_avg:56.71ms
step:335/2330 train_time:18995ms step_avg:56.70ms
step:336/2330 train_time:19055ms step_avg:56.71ms
step:337/2330 train_time:19110ms step_avg:56.71ms
step:338/2330 train_time:19169ms step_avg:56.71ms
step:339/2330 train_time:19224ms step_avg:56.71ms
step:340/2330 train_time:19284ms step_avg:56.72ms
step:341/2330 train_time:19339ms step_avg:56.71ms
step:342/2330 train_time:19398ms step_avg:56.72ms
step:343/2330 train_time:19453ms step_avg:56.71ms
step:344/2330 train_time:19513ms step_avg:56.72ms
step:345/2330 train_time:19568ms step_avg:56.72ms
step:346/2330 train_time:19627ms step_avg:56.73ms
step:347/2330 train_time:19683ms step_avg:56.72ms
step:348/2330 train_time:19741ms step_avg:56.73ms
step:349/2330 train_time:19796ms step_avg:56.72ms
step:350/2330 train_time:19855ms step_avg:56.73ms
step:351/2330 train_time:19911ms step_avg:56.73ms
step:352/2330 train_time:19969ms step_avg:56.73ms
step:353/2330 train_time:20024ms step_avg:56.73ms
step:354/2330 train_time:20084ms step_avg:56.73ms
step:355/2330 train_time:20140ms step_avg:56.73ms
step:356/2330 train_time:20199ms step_avg:56.74ms
step:357/2330 train_time:20255ms step_avg:56.74ms
step:358/2330 train_time:20313ms step_avg:56.74ms
step:359/2330 train_time:20368ms step_avg:56.73ms
step:360/2330 train_time:20427ms step_avg:56.74ms
step:361/2330 train_time:20482ms step_avg:56.74ms
step:362/2330 train_time:20541ms step_avg:56.74ms
step:363/2330 train_time:20597ms step_avg:56.74ms
step:364/2330 train_time:20656ms step_avg:56.75ms
step:365/2330 train_time:20711ms step_avg:56.74ms
step:366/2330 train_time:20770ms step_avg:56.75ms
step:367/2330 train_time:20825ms step_avg:56.75ms
step:368/2330 train_time:20884ms step_avg:56.75ms
step:369/2330 train_time:20940ms step_avg:56.75ms
step:370/2330 train_time:20999ms step_avg:56.75ms
step:371/2330 train_time:21054ms step_avg:56.75ms
step:372/2330 train_time:21114ms step_avg:56.76ms
step:373/2330 train_time:21169ms step_avg:56.75ms
step:374/2330 train_time:21228ms step_avg:56.76ms
step:375/2330 train_time:21282ms step_avg:56.75ms
step:376/2330 train_time:21343ms step_avg:56.76ms
step:377/2330 train_time:21399ms step_avg:56.76ms
step:378/2330 train_time:21458ms step_avg:56.77ms
step:379/2330 train_time:21513ms step_avg:56.76ms
step:380/2330 train_time:21572ms step_avg:56.77ms
step:381/2330 train_time:21627ms step_avg:56.76ms
step:382/2330 train_time:21687ms step_avg:56.77ms
step:383/2330 train_time:21742ms step_avg:56.77ms
step:384/2330 train_time:21801ms step_avg:56.77ms
step:385/2330 train_time:21857ms step_avg:56.77ms
step:386/2330 train_time:21916ms step_avg:56.78ms
step:387/2330 train_time:21971ms step_avg:56.77ms
step:388/2330 train_time:22032ms step_avg:56.78ms
step:389/2330 train_time:22087ms step_avg:56.78ms
step:390/2330 train_time:22145ms step_avg:56.78ms
step:391/2330 train_time:22201ms step_avg:56.78ms
step:392/2330 train_time:22259ms step_avg:56.78ms
step:393/2330 train_time:22314ms step_avg:56.78ms
step:394/2330 train_time:22373ms step_avg:56.79ms
step:395/2330 train_time:22429ms step_avg:56.78ms
step:396/2330 train_time:22489ms step_avg:56.79ms
step:397/2330 train_time:22543ms step_avg:56.78ms
step:398/2330 train_time:22603ms step_avg:56.79ms
step:399/2330 train_time:22659ms step_avg:56.79ms
step:400/2330 train_time:22718ms step_avg:56.80ms
step:401/2330 train_time:22774ms step_avg:56.79ms
step:402/2330 train_time:22833ms step_avg:56.80ms
step:403/2330 train_time:22888ms step_avg:56.79ms
step:404/2330 train_time:22947ms step_avg:56.80ms
step:405/2330 train_time:23002ms step_avg:56.80ms
step:406/2330 train_time:23062ms step_avg:56.80ms
step:407/2330 train_time:23118ms step_avg:56.80ms
step:408/2330 train_time:23177ms step_avg:56.81ms
step:409/2330 train_time:23232ms step_avg:56.80ms
step:410/2330 train_time:23291ms step_avg:56.81ms
step:411/2330 train_time:23346ms step_avg:56.80ms
step:412/2330 train_time:23405ms step_avg:56.81ms
step:413/2330 train_time:23461ms step_avg:56.81ms
step:414/2330 train_time:23520ms step_avg:56.81ms
step:415/2330 train_time:23575ms step_avg:56.81ms
step:416/2330 train_time:23634ms step_avg:56.81ms
step:417/2330 train_time:23689ms step_avg:56.81ms
step:418/2330 train_time:23748ms step_avg:56.81ms
step:419/2330 train_time:23803ms step_avg:56.81ms
step:420/2330 train_time:23862ms step_avg:56.82ms
step:421/2330 train_time:23919ms step_avg:56.81ms
step:422/2330 train_time:23977ms step_avg:56.82ms
step:423/2330 train_time:24032ms step_avg:56.81ms
step:424/2330 train_time:24092ms step_avg:56.82ms
step:425/2330 train_time:24147ms step_avg:56.82ms
step:426/2330 train_time:24206ms step_avg:56.82ms
step:427/2330 train_time:24261ms step_avg:56.82ms
step:428/2330 train_time:24320ms step_avg:56.82ms
step:429/2330 train_time:24376ms step_avg:56.82ms
step:430/2330 train_time:24435ms step_avg:56.82ms
step:431/2330 train_time:24490ms step_avg:56.82ms
step:432/2330 train_time:24550ms step_avg:56.83ms
step:433/2330 train_time:24605ms step_avg:56.82ms
step:434/2330 train_time:24665ms step_avg:56.83ms
step:435/2330 train_time:24720ms step_avg:56.83ms
step:436/2330 train_time:24779ms step_avg:56.83ms
step:437/2330 train_time:24835ms step_avg:56.83ms
step:438/2330 train_time:24894ms step_avg:56.84ms
step:439/2330 train_time:24950ms step_avg:56.83ms
step:440/2330 train_time:25009ms step_avg:56.84ms
step:441/2330 train_time:25064ms step_avg:56.83ms
step:442/2330 train_time:25123ms step_avg:56.84ms
step:443/2330 train_time:25178ms step_avg:56.84ms
step:444/2330 train_time:25237ms step_avg:56.84ms
step:445/2330 train_time:25292ms step_avg:56.84ms
step:446/2330 train_time:25350ms step_avg:56.84ms
step:447/2330 train_time:25406ms step_avg:56.84ms
step:448/2330 train_time:25466ms step_avg:56.84ms
step:449/2330 train_time:25522ms step_avg:56.84ms
step:450/2330 train_time:25580ms step_avg:56.84ms
step:451/2330 train_time:25635ms step_avg:56.84ms
step:452/2330 train_time:25694ms step_avg:56.85ms
step:453/2330 train_time:25749ms step_avg:56.84ms
step:454/2330 train_time:25808ms step_avg:56.85ms
step:455/2330 train_time:25863ms step_avg:56.84ms
step:456/2330 train_time:25923ms step_avg:56.85ms
step:457/2330 train_time:25979ms step_avg:56.85ms
step:458/2330 train_time:26039ms step_avg:56.85ms
step:459/2330 train_time:26095ms step_avg:56.85ms
step:460/2330 train_time:26154ms step_avg:56.86ms
step:461/2330 train_time:26209ms step_avg:56.85ms
step:462/2330 train_time:26269ms step_avg:56.86ms
step:463/2330 train_time:26325ms step_avg:56.86ms
step:464/2330 train_time:26383ms step_avg:56.86ms
step:465/2330 train_time:26439ms step_avg:56.86ms
step:466/2330 train_time:26499ms step_avg:56.86ms
step:467/2330 train_time:26554ms step_avg:56.86ms
step:468/2330 train_time:26613ms step_avg:56.87ms
step:469/2330 train_time:26668ms step_avg:56.86ms
step:470/2330 train_time:26728ms step_avg:56.87ms
step:471/2330 train_time:26783ms step_avg:56.86ms
step:472/2330 train_time:26842ms step_avg:56.87ms
step:473/2330 train_time:26898ms step_avg:56.87ms
step:474/2330 train_time:26957ms step_avg:56.87ms
step:475/2330 train_time:27013ms step_avg:56.87ms
step:476/2330 train_time:27073ms step_avg:56.88ms
step:477/2330 train_time:27128ms step_avg:56.87ms
step:478/2330 train_time:27187ms step_avg:56.88ms
step:479/2330 train_time:27243ms step_avg:56.87ms
step:480/2330 train_time:27302ms step_avg:56.88ms
step:481/2330 train_time:27357ms step_avg:56.88ms
step:482/2330 train_time:27415ms step_avg:56.88ms
step:483/2330 train_time:27471ms step_avg:56.88ms
step:484/2330 train_time:27530ms step_avg:56.88ms
step:485/2330 train_time:27585ms step_avg:56.88ms
step:486/2330 train_time:27645ms step_avg:56.88ms
step:487/2330 train_time:27700ms step_avg:56.88ms
step:488/2330 train_time:27759ms step_avg:56.88ms
step:489/2330 train_time:27814ms step_avg:56.88ms
step:490/2330 train_time:27874ms step_avg:56.89ms
step:491/2330 train_time:27929ms step_avg:56.88ms
step:492/2330 train_time:27989ms step_avg:56.89ms
step:493/2330 train_time:28044ms step_avg:56.89ms
step:494/2330 train_time:28104ms step_avg:56.89ms
step:495/2330 train_time:28160ms step_avg:56.89ms
step:496/2330 train_time:28219ms step_avg:56.89ms
step:497/2330 train_time:28274ms step_avg:56.89ms
step:498/2330 train_time:28334ms step_avg:56.90ms
step:499/2330 train_time:28389ms step_avg:56.89ms
step:500/2330 train_time:28448ms step_avg:56.90ms
step:500/2330 val_loss:5.6240 train_time:28527ms step_avg:57.05ms
step:501/2330 train_time:28545ms step_avg:56.98ms
step:502/2330 train_time:28565ms step_avg:56.90ms
step:503/2330 train_time:28621ms step_avg:56.90ms
step:504/2330 train_time:28684ms step_avg:56.91ms
step:505/2330 train_time:28741ms step_avg:56.91ms
step:506/2330 train_time:28801ms step_avg:56.92ms
step:507/2330 train_time:28857ms step_avg:56.92ms
step:508/2330 train_time:28916ms step_avg:56.92ms
step:509/2330 train_time:28971ms step_avg:56.92ms
step:510/2330 train_time:29030ms step_avg:56.92ms
step:511/2330 train_time:29085ms step_avg:56.92ms
step:512/2330 train_time:29143ms step_avg:56.92ms
step:513/2330 train_time:29198ms step_avg:56.92ms
step:514/2330 train_time:29256ms step_avg:56.92ms
step:515/2330 train_time:29311ms step_avg:56.91ms
step:516/2330 train_time:29370ms step_avg:56.92ms
step:517/2330 train_time:29426ms step_avg:56.92ms
step:518/2330 train_time:29484ms step_avg:56.92ms
step:519/2330 train_time:29540ms step_avg:56.92ms
step:520/2330 train_time:29599ms step_avg:56.92ms
step:521/2330 train_time:29655ms step_avg:56.92ms
step:522/2330 train_time:29715ms step_avg:56.92ms
step:523/2330 train_time:29771ms step_avg:56.92ms
step:524/2330 train_time:29831ms step_avg:56.93ms
step:525/2330 train_time:29887ms step_avg:56.93ms
step:526/2330 train_time:29946ms step_avg:56.93ms
step:527/2330 train_time:30002ms step_avg:56.93ms
step:528/2330 train_time:30060ms step_avg:56.93ms
step:529/2330 train_time:30115ms step_avg:56.93ms
step:530/2330 train_time:30174ms step_avg:56.93ms
step:531/2330 train_time:30228ms step_avg:56.93ms
step:532/2330 train_time:30288ms step_avg:56.93ms
step:533/2330 train_time:30343ms step_avg:56.93ms
step:534/2330 train_time:30403ms step_avg:56.93ms
step:535/2330 train_time:30458ms step_avg:56.93ms
step:536/2330 train_time:30517ms step_avg:56.94ms
step:537/2330 train_time:30573ms step_avg:56.93ms
step:538/2330 train_time:30632ms step_avg:56.94ms
step:539/2330 train_time:30688ms step_avg:56.93ms
step:540/2330 train_time:30748ms step_avg:56.94ms
step:541/2330 train_time:30804ms step_avg:56.94ms
step:542/2330 train_time:30863ms step_avg:56.94ms
step:543/2330 train_time:30919ms step_avg:56.94ms
step:544/2330 train_time:30978ms step_avg:56.95ms
step:545/2330 train_time:31033ms step_avg:56.94ms
step:546/2330 train_time:31093ms step_avg:56.95ms
step:547/2330 train_time:31148ms step_avg:56.94ms
step:548/2330 train_time:31207ms step_avg:56.95ms
step:549/2330 train_time:31262ms step_avg:56.94ms
step:550/2330 train_time:31321ms step_avg:56.95ms
step:551/2330 train_time:31376ms step_avg:56.94ms
step:552/2330 train_time:31435ms step_avg:56.95ms
step:553/2330 train_time:31489ms step_avg:56.94ms
step:554/2330 train_time:31550ms step_avg:56.95ms
step:555/2330 train_time:31606ms step_avg:56.95ms
step:556/2330 train_time:31665ms step_avg:56.95ms
step:557/2330 train_time:31720ms step_avg:56.95ms
step:558/2330 train_time:31780ms step_avg:56.95ms
step:559/2330 train_time:31835ms step_avg:56.95ms
step:560/2330 train_time:31894ms step_avg:56.95ms
step:561/2330 train_time:31950ms step_avg:56.95ms
step:562/2330 train_time:32009ms step_avg:56.96ms
step:563/2330 train_time:32065ms step_avg:56.95ms
step:564/2330 train_time:32124ms step_avg:56.96ms
step:565/2330 train_time:32179ms step_avg:56.95ms
step:566/2330 train_time:32240ms step_avg:56.96ms
step:567/2330 train_time:32295ms step_avg:56.96ms
step:568/2330 train_time:32355ms step_avg:56.96ms
step:569/2330 train_time:32410ms step_avg:56.96ms
step:570/2330 train_time:32469ms step_avg:56.96ms
step:571/2330 train_time:32525ms step_avg:56.96ms
step:572/2330 train_time:32584ms step_avg:56.96ms
step:573/2330 train_time:32639ms step_avg:56.96ms
step:574/2330 train_time:32698ms step_avg:56.97ms
step:575/2330 train_time:32754ms step_avg:56.96ms
step:576/2330 train_time:32813ms step_avg:56.97ms
step:577/2330 train_time:32868ms step_avg:56.96ms
step:578/2330 train_time:32927ms step_avg:56.97ms
step:579/2330 train_time:32983ms step_avg:56.97ms
step:580/2330 train_time:33042ms step_avg:56.97ms
step:581/2330 train_time:33098ms step_avg:56.97ms
step:582/2330 train_time:33157ms step_avg:56.97ms
step:583/2330 train_time:33213ms step_avg:56.97ms
step:584/2330 train_time:33271ms step_avg:56.97ms
step:585/2330 train_time:33327ms step_avg:56.97ms
step:586/2330 train_time:33386ms step_avg:56.97ms
step:587/2330 train_time:33441ms step_avg:56.97ms
step:588/2330 train_time:33500ms step_avg:56.97ms
step:589/2330 train_time:33555ms step_avg:56.97ms
step:590/2330 train_time:33616ms step_avg:56.98ms
step:591/2330 train_time:33671ms step_avg:56.97ms
step:592/2330 train_time:33730ms step_avg:56.98ms
step:593/2330 train_time:33786ms step_avg:56.97ms
step:594/2330 train_time:33845ms step_avg:56.98ms
step:595/2330 train_time:33900ms step_avg:56.98ms
step:596/2330 train_time:33960ms step_avg:56.98ms
step:597/2330 train_time:34016ms step_avg:56.98ms
step:598/2330 train_time:34075ms step_avg:56.98ms
step:599/2330 train_time:34131ms step_avg:56.98ms
step:600/2330 train_time:34190ms step_avg:56.98ms
step:601/2330 train_time:34245ms step_avg:56.98ms
step:602/2330 train_time:34305ms step_avg:56.98ms
step:603/2330 train_time:34360ms step_avg:56.98ms
step:604/2330 train_time:34419ms step_avg:56.99ms
step:605/2330 train_time:34475ms step_avg:56.98ms
step:606/2330 train_time:34534ms step_avg:56.99ms
step:607/2330 train_time:34589ms step_avg:56.98ms
step:608/2330 train_time:34649ms step_avg:56.99ms
step:609/2330 train_time:34704ms step_avg:56.99ms
step:610/2330 train_time:34764ms step_avg:56.99ms
step:611/2330 train_time:34819ms step_avg:56.99ms
step:612/2330 train_time:34880ms step_avg:56.99ms
step:613/2330 train_time:34936ms step_avg:56.99ms
step:614/2330 train_time:34995ms step_avg:57.00ms
step:615/2330 train_time:35051ms step_avg:56.99ms
step:616/2330 train_time:35110ms step_avg:57.00ms
step:617/2330 train_time:35166ms step_avg:56.99ms
step:618/2330 train_time:35224ms step_avg:57.00ms
step:619/2330 train_time:35280ms step_avg:56.99ms
step:620/2330 train_time:35339ms step_avg:57.00ms
step:621/2330 train_time:35394ms step_avg:56.99ms
step:622/2330 train_time:35453ms step_avg:57.00ms
step:623/2330 train_time:35508ms step_avg:57.00ms
step:624/2330 train_time:35567ms step_avg:57.00ms
step:625/2330 train_time:35623ms step_avg:57.00ms
step:626/2330 train_time:35682ms step_avg:57.00ms
step:627/2330 train_time:35738ms step_avg:57.00ms
step:628/2330 train_time:35797ms step_avg:57.00ms
step:629/2330 train_time:35852ms step_avg:57.00ms
step:630/2330 train_time:35912ms step_avg:57.00ms
step:631/2330 train_time:35968ms step_avg:57.00ms
step:632/2330 train_time:36026ms step_avg:57.00ms
step:633/2330 train_time:36082ms step_avg:57.00ms
step:634/2330 train_time:36141ms step_avg:57.00ms
step:635/2330 train_time:36196ms step_avg:57.00ms
step:636/2330 train_time:36257ms step_avg:57.01ms
step:637/2330 train_time:36312ms step_avg:57.00ms
step:638/2330 train_time:36372ms step_avg:57.01ms
step:639/2330 train_time:36427ms step_avg:57.01ms
step:640/2330 train_time:36486ms step_avg:57.01ms
step:641/2330 train_time:36542ms step_avg:57.01ms
step:642/2330 train_time:36601ms step_avg:57.01ms
step:643/2330 train_time:36656ms step_avg:57.01ms
step:644/2330 train_time:36715ms step_avg:57.01ms
step:645/2330 train_time:36770ms step_avg:57.01ms
step:646/2330 train_time:36830ms step_avg:57.01ms
step:647/2330 train_time:36886ms step_avg:57.01ms
step:648/2330 train_time:36945ms step_avg:57.01ms
step:649/2330 train_time:37001ms step_avg:57.01ms
step:650/2330 train_time:37060ms step_avg:57.02ms
step:651/2330 train_time:37115ms step_avg:57.01ms
step:652/2330 train_time:37176ms step_avg:57.02ms
step:653/2330 train_time:37231ms step_avg:57.02ms
step:654/2330 train_time:37289ms step_avg:57.02ms
step:655/2330 train_time:37345ms step_avg:57.01ms
step:656/2330 train_time:37403ms step_avg:57.02ms
step:657/2330 train_time:37459ms step_avg:57.01ms
step:658/2330 train_time:37519ms step_avg:57.02ms
step:659/2330 train_time:37575ms step_avg:57.02ms
step:660/2330 train_time:37633ms step_avg:57.02ms
step:661/2330 train_time:37688ms step_avg:57.02ms
step:662/2330 train_time:37748ms step_avg:57.02ms
step:663/2330 train_time:37803ms step_avg:57.02ms
step:664/2330 train_time:37862ms step_avg:57.02ms
step:665/2330 train_time:37918ms step_avg:57.02ms
step:666/2330 train_time:37977ms step_avg:57.02ms
step:667/2330 train_time:38032ms step_avg:57.02ms
step:668/2330 train_time:38091ms step_avg:57.02ms
step:669/2330 train_time:38147ms step_avg:57.02ms
step:670/2330 train_time:38207ms step_avg:57.02ms
step:671/2330 train_time:38263ms step_avg:57.02ms
step:672/2330 train_time:38321ms step_avg:57.03ms
step:673/2330 train_time:38376ms step_avg:57.02ms
step:674/2330 train_time:38437ms step_avg:57.03ms
step:675/2330 train_time:38492ms step_avg:57.03ms
step:676/2330 train_time:38552ms step_avg:57.03ms
step:677/2330 train_time:38607ms step_avg:57.03ms
step:678/2330 train_time:38666ms step_avg:57.03ms
step:679/2330 train_time:38722ms step_avg:57.03ms
step:680/2330 train_time:38781ms step_avg:57.03ms
step:681/2330 train_time:38837ms step_avg:57.03ms
step:682/2330 train_time:38895ms step_avg:57.03ms
step:683/2330 train_time:38951ms step_avg:57.03ms
step:684/2330 train_time:39010ms step_avg:57.03ms
step:685/2330 train_time:39066ms step_avg:57.03ms
step:686/2330 train_time:39125ms step_avg:57.03ms
step:687/2330 train_time:39181ms step_avg:57.03ms
step:688/2330 train_time:39240ms step_avg:57.03ms
step:689/2330 train_time:39295ms step_avg:57.03ms
step:690/2330 train_time:39355ms step_avg:57.04ms
step:691/2330 train_time:39410ms step_avg:57.03ms
step:692/2330 train_time:39470ms step_avg:57.04ms
step:693/2330 train_time:39526ms step_avg:57.04ms
step:694/2330 train_time:39584ms step_avg:57.04ms
step:695/2330 train_time:39640ms step_avg:57.04ms
step:696/2330 train_time:39700ms step_avg:57.04ms
step:697/2330 train_time:39755ms step_avg:57.04ms
step:698/2330 train_time:39815ms step_avg:57.04ms
step:699/2330 train_time:39870ms step_avg:57.04ms
step:700/2330 train_time:39929ms step_avg:57.04ms
step:701/2330 train_time:39985ms step_avg:57.04ms
step:702/2330 train_time:40043ms step_avg:57.04ms
step:703/2330 train_time:40099ms step_avg:57.04ms
step:704/2330 train_time:40159ms step_avg:57.04ms
step:705/2330 train_time:40215ms step_avg:57.04ms
step:706/2330 train_time:40273ms step_avg:57.04ms
step:707/2330 train_time:40329ms step_avg:57.04ms
step:708/2330 train_time:40387ms step_avg:57.04ms
step:709/2330 train_time:40443ms step_avg:57.04ms
step:710/2330 train_time:40503ms step_avg:57.05ms
step:711/2330 train_time:40558ms step_avg:57.04ms
step:712/2330 train_time:40618ms step_avg:57.05ms
step:713/2330 train_time:40673ms step_avg:57.04ms
step:714/2330 train_time:40733ms step_avg:57.05ms
step:715/2330 train_time:40788ms step_avg:57.05ms
step:716/2330 train_time:40847ms step_avg:57.05ms
step:717/2330 train_time:40903ms step_avg:57.05ms
step:718/2330 train_time:40963ms step_avg:57.05ms
step:719/2330 train_time:41018ms step_avg:57.05ms
step:720/2330 train_time:41078ms step_avg:57.05ms
step:721/2330 train_time:41133ms step_avg:57.05ms
step:722/2330 train_time:41194ms step_avg:57.05ms
step:723/2330 train_time:41249ms step_avg:57.05ms
step:724/2330 train_time:41308ms step_avg:57.06ms
step:725/2330 train_time:41364ms step_avg:57.05ms
step:726/2330 train_time:41423ms step_avg:57.06ms
step:727/2330 train_time:41478ms step_avg:57.05ms
step:728/2330 train_time:41538ms step_avg:57.06ms
step:729/2330 train_time:41593ms step_avg:57.05ms
step:730/2330 train_time:41652ms step_avg:57.06ms
step:731/2330 train_time:41707ms step_avg:57.06ms
step:732/2330 train_time:41766ms step_avg:57.06ms
step:733/2330 train_time:41822ms step_avg:57.06ms
step:734/2330 train_time:41881ms step_avg:57.06ms
step:735/2330 train_time:41937ms step_avg:57.06ms
step:736/2330 train_time:41996ms step_avg:57.06ms
step:737/2330 train_time:42051ms step_avg:57.06ms
step:738/2330 train_time:42110ms step_avg:57.06ms
step:739/2330 train_time:42166ms step_avg:57.06ms
step:740/2330 train_time:42225ms step_avg:57.06ms
step:741/2330 train_time:42281ms step_avg:57.06ms
step:742/2330 train_time:42340ms step_avg:57.06ms
step:743/2330 train_time:42396ms step_avg:57.06ms
step:744/2330 train_time:42456ms step_avg:57.06ms
step:745/2330 train_time:42512ms step_avg:57.06ms
step:746/2330 train_time:42570ms step_avg:57.06ms
step:747/2330 train_time:42625ms step_avg:57.06ms
step:748/2330 train_time:42685ms step_avg:57.07ms
step:749/2330 train_time:42740ms step_avg:57.06ms
step:750/2330 train_time:42800ms step_avg:57.07ms
step:750/2330 val_loss:5.1710 train_time:42879ms step_avg:57.17ms
step:751/2330 train_time:42897ms step_avg:57.12ms
step:752/2330 train_time:42917ms step_avg:57.07ms
step:753/2330 train_time:42974ms step_avg:57.07ms
step:754/2330 train_time:43040ms step_avg:57.08ms
step:755/2330 train_time:43096ms step_avg:57.08ms
step:756/2330 train_time:43156ms step_avg:57.09ms
step:757/2330 train_time:43212ms step_avg:57.08ms
step:758/2330 train_time:43272ms step_avg:57.09ms
step:759/2330 train_time:43328ms step_avg:57.09ms
step:760/2330 train_time:43387ms step_avg:57.09ms
step:761/2330 train_time:43442ms step_avg:57.09ms
step:762/2330 train_time:43500ms step_avg:57.09ms
step:763/2330 train_time:43555ms step_avg:57.08ms
step:764/2330 train_time:43613ms step_avg:57.08ms
step:765/2330 train_time:43669ms step_avg:57.08ms
step:766/2330 train_time:43727ms step_avg:57.08ms
step:767/2330 train_time:43783ms step_avg:57.08ms
step:768/2330 train_time:43842ms step_avg:57.09ms
step:769/2330 train_time:43898ms step_avg:57.08ms
step:770/2330 train_time:43959ms step_avg:57.09ms
step:771/2330 train_time:44016ms step_avg:57.09ms
step:772/2330 train_time:44078ms step_avg:57.10ms
step:773/2330 train_time:44135ms step_avg:57.10ms
step:774/2330 train_time:44195ms step_avg:57.10ms
step:775/2330 train_time:44252ms step_avg:57.10ms
step:776/2330 train_time:44313ms step_avg:57.10ms
step:777/2330 train_time:44369ms step_avg:57.10ms
step:778/2330 train_time:44429ms step_avg:57.11ms
step:779/2330 train_time:44485ms step_avg:57.10ms
step:780/2330 train_time:44543ms step_avg:57.11ms
step:781/2330 train_time:44599ms step_avg:57.11ms
step:782/2330 train_time:44659ms step_avg:57.11ms
step:783/2330 train_time:44715ms step_avg:57.11ms
step:784/2330 train_time:44775ms step_avg:57.11ms
step:785/2330 train_time:44832ms step_avg:57.11ms
step:786/2330 train_time:44891ms step_avg:57.11ms
step:787/2330 train_time:44947ms step_avg:57.11ms
step:788/2330 train_time:45007ms step_avg:57.12ms
step:789/2330 train_time:45063ms step_avg:57.11ms
step:790/2330 train_time:45126ms step_avg:57.12ms
step:791/2330 train_time:45182ms step_avg:57.12ms
step:792/2330 train_time:45242ms step_avg:57.12ms
step:793/2330 train_time:45299ms step_avg:57.12ms
step:794/2330 train_time:45359ms step_avg:57.13ms
step:795/2330 train_time:45416ms step_avg:57.13ms
step:796/2330 train_time:45475ms step_avg:57.13ms
step:797/2330 train_time:45531ms step_avg:57.13ms
step:798/2330 train_time:45590ms step_avg:57.13ms
step:799/2330 train_time:45646ms step_avg:57.13ms
step:800/2330 train_time:45706ms step_avg:57.13ms
step:801/2330 train_time:45762ms step_avg:57.13ms
step:802/2330 train_time:45823ms step_avg:57.14ms
step:803/2330 train_time:45878ms step_avg:57.13ms
step:804/2330 train_time:45939ms step_avg:57.14ms
step:805/2330 train_time:45996ms step_avg:57.14ms
step:806/2330 train_time:46057ms step_avg:57.14ms
step:807/2330 train_time:46114ms step_avg:57.14ms
step:808/2330 train_time:46174ms step_avg:57.15ms
step:809/2330 train_time:46231ms step_avg:57.15ms
step:810/2330 train_time:46290ms step_avg:57.15ms
step:811/2330 train_time:46347ms step_avg:57.15ms
step:812/2330 train_time:46406ms step_avg:57.15ms
step:813/2330 train_time:46463ms step_avg:57.15ms
step:814/2330 train_time:46522ms step_avg:57.15ms
step:815/2330 train_time:46579ms step_avg:57.15ms
step:816/2330 train_time:46637ms step_avg:57.15ms
step:817/2330 train_time:46694ms step_avg:57.15ms
step:818/2330 train_time:46754ms step_avg:57.16ms
step:819/2330 train_time:46811ms step_avg:57.16ms
step:820/2330 train_time:46870ms step_avg:57.16ms
step:821/2330 train_time:46926ms step_avg:57.16ms
step:822/2330 train_time:46987ms step_avg:57.16ms
step:823/2330 train_time:47043ms step_avg:57.16ms
step:824/2330 train_time:47103ms step_avg:57.16ms
step:825/2330 train_time:47160ms step_avg:57.16ms
step:826/2330 train_time:47219ms step_avg:57.17ms
step:827/2330 train_time:47276ms step_avg:57.17ms
step:828/2330 train_time:47336ms step_avg:57.17ms
step:829/2330 train_time:47393ms step_avg:57.17ms
step:830/2330 train_time:47453ms step_avg:57.17ms
step:831/2330 train_time:47509ms step_avg:57.17ms
step:832/2330 train_time:47568ms step_avg:57.17ms
step:833/2330 train_time:47625ms step_avg:57.17ms
step:834/2330 train_time:47684ms step_avg:57.18ms
step:835/2330 train_time:47740ms step_avg:57.17ms
step:836/2330 train_time:47800ms step_avg:57.18ms
step:837/2330 train_time:47856ms step_avg:57.18ms
step:838/2330 train_time:47916ms step_avg:57.18ms
step:839/2330 train_time:47972ms step_avg:57.18ms
step:840/2330 train_time:48032ms step_avg:57.18ms
step:841/2330 train_time:48089ms step_avg:57.18ms
step:842/2330 train_time:48149ms step_avg:57.18ms
step:843/2330 train_time:48205ms step_avg:57.18ms
step:844/2330 train_time:48266ms step_avg:57.19ms
step:845/2330 train_time:48323ms step_avg:57.19ms
step:846/2330 train_time:48382ms step_avg:57.19ms
step:847/2330 train_time:48438ms step_avg:57.19ms
step:848/2330 train_time:48498ms step_avg:57.19ms
step:849/2330 train_time:48554ms step_avg:57.19ms
step:850/2330 train_time:48614ms step_avg:57.19ms
step:851/2330 train_time:48670ms step_avg:57.19ms
step:852/2330 train_time:48730ms step_avg:57.19ms
step:853/2330 train_time:48786ms step_avg:57.19ms
step:854/2330 train_time:48846ms step_avg:57.20ms
step:855/2330 train_time:48902ms step_avg:57.20ms
step:856/2330 train_time:48962ms step_avg:57.20ms
step:857/2330 train_time:49018ms step_avg:57.20ms
step:858/2330 train_time:49078ms step_avg:57.20ms
step:859/2330 train_time:49135ms step_avg:57.20ms
step:860/2330 train_time:49196ms step_avg:57.20ms
step:861/2330 train_time:49252ms step_avg:57.20ms
step:862/2330 train_time:49313ms step_avg:57.21ms
step:863/2330 train_time:49369ms step_avg:57.21ms
step:864/2330 train_time:49429ms step_avg:57.21ms
step:865/2330 train_time:49485ms step_avg:57.21ms
step:866/2330 train_time:49545ms step_avg:57.21ms
step:867/2330 train_time:49601ms step_avg:57.21ms
step:868/2330 train_time:49661ms step_avg:57.21ms
step:869/2330 train_time:49717ms step_avg:57.21ms
step:870/2330 train_time:49777ms step_avg:57.21ms
step:871/2330 train_time:49833ms step_avg:57.21ms
step:872/2330 train_time:49893ms step_avg:57.22ms
step:873/2330 train_time:49949ms step_avg:57.22ms
step:874/2330 train_time:50009ms step_avg:57.22ms
step:875/2330 train_time:50065ms step_avg:57.22ms
step:876/2330 train_time:50125ms step_avg:57.22ms
step:877/2330 train_time:50181ms step_avg:57.22ms
step:878/2330 train_time:50241ms step_avg:57.22ms
step:879/2330 train_time:50297ms step_avg:57.22ms
step:880/2330 train_time:50357ms step_avg:57.22ms
step:881/2330 train_time:50414ms step_avg:57.22ms
step:882/2330 train_time:50475ms step_avg:57.23ms
step:883/2330 train_time:50532ms step_avg:57.23ms
step:884/2330 train_time:50590ms step_avg:57.23ms
step:885/2330 train_time:50646ms step_avg:57.23ms
step:886/2330 train_time:50707ms step_avg:57.23ms
step:887/2330 train_time:50763ms step_avg:57.23ms
step:888/2330 train_time:50823ms step_avg:57.23ms
step:889/2330 train_time:50879ms step_avg:57.23ms
step:890/2330 train_time:50939ms step_avg:57.23ms
step:891/2330 train_time:50996ms step_avg:57.23ms
step:892/2330 train_time:51056ms step_avg:57.24ms
step:893/2330 train_time:51113ms step_avg:57.24ms
step:894/2330 train_time:51172ms step_avg:57.24ms
step:895/2330 train_time:51229ms step_avg:57.24ms
step:896/2330 train_time:51290ms step_avg:57.24ms
step:897/2330 train_time:51346ms step_avg:57.24ms
step:898/2330 train_time:51406ms step_avg:57.25ms
step:899/2330 train_time:51462ms step_avg:57.24ms
step:900/2330 train_time:51521ms step_avg:57.25ms
step:901/2330 train_time:51578ms step_avg:57.25ms
step:902/2330 train_time:51638ms step_avg:57.25ms
step:903/2330 train_time:51695ms step_avg:57.25ms
step:904/2330 train_time:51754ms step_avg:57.25ms
step:905/2330 train_time:51810ms step_avg:57.25ms
step:906/2330 train_time:51871ms step_avg:57.25ms
step:907/2330 train_time:51927ms step_avg:57.25ms
step:908/2330 train_time:51986ms step_avg:57.25ms
step:909/2330 train_time:52042ms step_avg:57.25ms
step:910/2330 train_time:52102ms step_avg:57.26ms
step:911/2330 train_time:52158ms step_avg:57.25ms
step:912/2330 train_time:52219ms step_avg:57.26ms
step:913/2330 train_time:52275ms step_avg:57.26ms
step:914/2330 train_time:52336ms step_avg:57.26ms
step:915/2330 train_time:52392ms step_avg:57.26ms
step:916/2330 train_time:52452ms step_avg:57.26ms
step:917/2330 train_time:52508ms step_avg:57.26ms
step:918/2330 train_time:52568ms step_avg:57.26ms
step:919/2330 train_time:52624ms step_avg:57.26ms
step:920/2330 train_time:52684ms step_avg:57.27ms
step:921/2330 train_time:52740ms step_avg:57.26ms
step:922/2330 train_time:52801ms step_avg:57.27ms
step:923/2330 train_time:52857ms step_avg:57.27ms
step:924/2330 train_time:52917ms step_avg:57.27ms
step:925/2330 train_time:52973ms step_avg:57.27ms
step:926/2330 train_time:53033ms step_avg:57.27ms
step:927/2330 train_time:53090ms step_avg:57.27ms
step:928/2330 train_time:53150ms step_avg:57.27ms
step:929/2330 train_time:53207ms step_avg:57.27ms
step:930/2330 train_time:53266ms step_avg:57.28ms
step:931/2330 train_time:53322ms step_avg:57.27ms
step:932/2330 train_time:53382ms step_avg:57.28ms
step:933/2330 train_time:53439ms step_avg:57.28ms
step:934/2330 train_time:53498ms step_avg:57.28ms
step:935/2330 train_time:53555ms step_avg:57.28ms
step:936/2330 train_time:53615ms step_avg:57.28ms
step:937/2330 train_time:53672ms step_avg:57.28ms
step:938/2330 train_time:53731ms step_avg:57.28ms
step:939/2330 train_time:53787ms step_avg:57.28ms
step:940/2330 train_time:53847ms step_avg:57.28ms
step:941/2330 train_time:53903ms step_avg:57.28ms
step:942/2330 train_time:53963ms step_avg:57.29ms
step:943/2330 train_time:54019ms step_avg:57.28ms
step:944/2330 train_time:54079ms step_avg:57.29ms
step:945/2330 train_time:54136ms step_avg:57.29ms
step:946/2330 train_time:54196ms step_avg:57.29ms
step:947/2330 train_time:54252ms step_avg:57.29ms
step:948/2330 train_time:54312ms step_avg:57.29ms
step:949/2330 train_time:54369ms step_avg:57.29ms
step:950/2330 train_time:54429ms step_avg:57.29ms
step:951/2330 train_time:54485ms step_avg:57.29ms
step:952/2330 train_time:54544ms step_avg:57.29ms
step:953/2330 train_time:54601ms step_avg:57.29ms
step:954/2330 train_time:54660ms step_avg:57.30ms
step:955/2330 train_time:54717ms step_avg:57.29ms
step:956/2330 train_time:54776ms step_avg:57.30ms
step:957/2330 train_time:54832ms step_avg:57.30ms
step:958/2330 train_time:54893ms step_avg:57.30ms
step:959/2330 train_time:54949ms step_avg:57.30ms
step:960/2330 train_time:55009ms step_avg:57.30ms
step:961/2330 train_time:55065ms step_avg:57.30ms
step:962/2330 train_time:55125ms step_avg:57.30ms
step:963/2330 train_time:55181ms step_avg:57.30ms
step:964/2330 train_time:55241ms step_avg:57.30ms
step:965/2330 train_time:55297ms step_avg:57.30ms
step:966/2330 train_time:55357ms step_avg:57.31ms
step:967/2330 train_time:55413ms step_avg:57.30ms
step:968/2330 train_time:55473ms step_avg:57.31ms
step:969/2330 train_time:55529ms step_avg:57.31ms
step:970/2330 train_time:55590ms step_avg:57.31ms
step:971/2330 train_time:55646ms step_avg:57.31ms
step:972/2330 train_time:55706ms step_avg:57.31ms
step:973/2330 train_time:55762ms step_avg:57.31ms
step:974/2330 train_time:55821ms step_avg:57.31ms
step:975/2330 train_time:55877ms step_avg:57.31ms
step:976/2330 train_time:55937ms step_avg:57.31ms
step:977/2330 train_time:55994ms step_avg:57.31ms
step:978/2330 train_time:56053ms step_avg:57.31ms
step:979/2330 train_time:56109ms step_avg:57.31ms
step:980/2330 train_time:56169ms step_avg:57.32ms
step:981/2330 train_time:56226ms step_avg:57.31ms
step:982/2330 train_time:56287ms step_avg:57.32ms
step:983/2330 train_time:56343ms step_avg:57.32ms
step:984/2330 train_time:56403ms step_avg:57.32ms
step:985/2330 train_time:56459ms step_avg:57.32ms
step:986/2330 train_time:56518ms step_avg:57.32ms
step:987/2330 train_time:56574ms step_avg:57.32ms
step:988/2330 train_time:56635ms step_avg:57.32ms
step:989/2330 train_time:56692ms step_avg:57.32ms
step:990/2330 train_time:56751ms step_avg:57.32ms
step:991/2330 train_time:56808ms step_avg:57.32ms
step:992/2330 train_time:56867ms step_avg:57.33ms
step:993/2330 train_time:56923ms step_avg:57.32ms
step:994/2330 train_time:56983ms step_avg:57.33ms
step:995/2330 train_time:57039ms step_avg:57.33ms
step:996/2330 train_time:57099ms step_avg:57.33ms
step:997/2330 train_time:57156ms step_avg:57.33ms
step:998/2330 train_time:57216ms step_avg:57.33ms
step:999/2330 train_time:57273ms step_avg:57.33ms
step:1000/2330 train_time:57333ms step_avg:57.33ms
step:1000/2330 val_loss:4.8121 train_time:57413ms step_avg:57.41ms
step:1001/2330 train_time:57432ms step_avg:57.37ms
step:1002/2330 train_time:57452ms step_avg:57.34ms
step:1003/2330 train_time:57510ms step_avg:57.34ms
step:1004/2330 train_time:57574ms step_avg:57.34ms
step:1005/2330 train_time:57632ms step_avg:57.35ms
step:1006/2330 train_time:57692ms step_avg:57.35ms
step:1007/2330 train_time:57747ms step_avg:57.35ms
step:1008/2330 train_time:57806ms step_avg:57.35ms
step:1009/2330 train_time:57862ms step_avg:57.35ms
step:1010/2330 train_time:57920ms step_avg:57.35ms
step:1011/2330 train_time:57976ms step_avg:57.34ms
step:1012/2330 train_time:58035ms step_avg:57.35ms
step:1013/2330 train_time:58091ms step_avg:57.35ms
step:1014/2330 train_time:58150ms step_avg:57.35ms
step:1015/2330 train_time:58206ms step_avg:57.35ms
step:1016/2330 train_time:58265ms step_avg:57.35ms
step:1017/2330 train_time:58320ms step_avg:57.35ms
step:1018/2330 train_time:58383ms step_avg:57.35ms
step:1019/2330 train_time:58438ms step_avg:57.35ms
step:1020/2330 train_time:58504ms step_avg:57.36ms
step:1021/2330 train_time:58560ms step_avg:57.36ms
step:1022/2330 train_time:58622ms step_avg:57.36ms
step:1023/2330 train_time:58677ms step_avg:57.36ms
step:1024/2330 train_time:58738ms step_avg:57.36ms
step:1025/2330 train_time:58794ms step_avg:57.36ms
step:1026/2330 train_time:58854ms step_avg:57.36ms
step:1027/2330 train_time:58910ms step_avg:57.36ms
step:1028/2330 train_time:58969ms step_avg:57.36ms
step:1029/2330 train_time:59025ms step_avg:57.36ms
step:1030/2330 train_time:59084ms step_avg:57.36ms
step:1031/2330 train_time:59140ms step_avg:57.36ms
step:1032/2330 train_time:59200ms step_avg:57.36ms
step:1033/2330 train_time:59257ms step_avg:57.36ms
step:1034/2330 train_time:59316ms step_avg:57.37ms
step:1035/2330 train_time:59372ms step_avg:57.36ms
step:1036/2330 train_time:59433ms step_avg:57.37ms
step:1037/2330 train_time:59490ms step_avg:57.37ms
step:1038/2330 train_time:59551ms step_avg:57.37ms
step:1039/2330 train_time:59608ms step_avg:57.37ms
step:1040/2330 train_time:59668ms step_avg:57.37ms
step:1041/2330 train_time:59725ms step_avg:57.37ms
step:1042/2330 train_time:59784ms step_avg:57.37ms
step:1043/2330 train_time:59840ms step_avg:57.37ms
step:1044/2330 train_time:59900ms step_avg:57.38ms
step:1045/2330 train_time:59956ms step_avg:57.37ms
step:1046/2330 train_time:60015ms step_avg:57.38ms
step:1047/2330 train_time:60070ms step_avg:57.37ms
step:1048/2330 train_time:60130ms step_avg:57.38ms
step:1049/2330 train_time:60187ms step_avg:57.38ms
step:1050/2330 train_time:60246ms step_avg:57.38ms
step:1051/2330 train_time:60303ms step_avg:57.38ms
step:1052/2330 train_time:60363ms step_avg:57.38ms
step:1053/2330 train_time:60419ms step_avg:57.38ms
step:1054/2330 train_time:60479ms step_avg:57.38ms
step:1055/2330 train_time:60536ms step_avg:57.38ms
step:1056/2330 train_time:60596ms step_avg:57.38ms
step:1057/2330 train_time:60652ms step_avg:57.38ms
step:1058/2330 train_time:60713ms step_avg:57.38ms
step:1059/2330 train_time:60770ms step_avg:57.38ms
step:1060/2330 train_time:60829ms step_avg:57.39ms
step:1061/2330 train_time:60886ms step_avg:57.39ms
step:1062/2330 train_time:60945ms step_avg:57.39ms
step:1063/2330 train_time:61001ms step_avg:57.39ms
step:1064/2330 train_time:61062ms step_avg:57.39ms
step:1065/2330 train_time:61117ms step_avg:57.39ms
step:1066/2330 train_time:61177ms step_avg:57.39ms
step:1067/2330 train_time:61233ms step_avg:57.39ms
step:1068/2330 train_time:61293ms step_avg:57.39ms
step:1069/2330 train_time:61350ms step_avg:57.39ms
step:1070/2330 train_time:61409ms step_avg:57.39ms
step:1071/2330 train_time:61466ms step_avg:57.39ms
step:1072/2330 train_time:61527ms step_avg:57.39ms
step:1073/2330 train_time:61583ms step_avg:57.39ms
step:1074/2330 train_time:61645ms step_avg:57.40ms
step:1075/2330 train_time:61700ms step_avg:57.40ms
step:1076/2330 train_time:61761ms step_avg:57.40ms
step:1077/2330 train_time:61817ms step_avg:57.40ms
step:1078/2330 train_time:61877ms step_avg:57.40ms
step:1079/2330 train_time:61933ms step_avg:57.40ms
step:1080/2330 train_time:61992ms step_avg:57.40ms
step:1081/2330 train_time:62049ms step_avg:57.40ms
step:1082/2330 train_time:62109ms step_avg:57.40ms
step:1083/2330 train_time:62165ms step_avg:57.40ms
step:1084/2330 train_time:62225ms step_avg:57.40ms
step:1085/2330 train_time:62281ms step_avg:57.40ms
step:1086/2330 train_time:62341ms step_avg:57.40ms
step:1087/2330 train_time:62397ms step_avg:57.40ms
step:1088/2330 train_time:62457ms step_avg:57.41ms
step:1089/2330 train_time:62513ms step_avg:57.40ms
step:1090/2330 train_time:62574ms step_avg:57.41ms
step:1091/2330 train_time:62630ms step_avg:57.41ms
step:1092/2330 train_time:62690ms step_avg:57.41ms
step:1093/2330 train_time:62747ms step_avg:57.41ms
step:1094/2330 train_time:62807ms step_avg:57.41ms
step:1095/2330 train_time:62863ms step_avg:57.41ms
step:1096/2330 train_time:62924ms step_avg:57.41ms
step:1097/2330 train_time:62980ms step_avg:57.41ms
step:1098/2330 train_time:63040ms step_avg:57.41ms
step:1099/2330 train_time:63096ms step_avg:57.41ms
step:1100/2330 train_time:63157ms step_avg:57.42ms
step:1101/2330 train_time:63213ms step_avg:57.41ms
step:1102/2330 train_time:63272ms step_avg:57.42ms
step:1103/2330 train_time:63328ms step_avg:57.41ms
step:1104/2330 train_time:63388ms step_avg:57.42ms
step:1105/2330 train_time:63445ms step_avg:57.42ms
step:1106/2330 train_time:63505ms step_avg:57.42ms
step:1107/2330 train_time:63562ms step_avg:57.42ms
step:1108/2330 train_time:63621ms step_avg:57.42ms
step:1109/2330 train_time:63677ms step_avg:57.42ms
step:1110/2330 train_time:63737ms step_avg:57.42ms
step:1111/2330 train_time:63793ms step_avg:57.42ms
step:1112/2330 train_time:63854ms step_avg:57.42ms
step:1113/2330 train_time:63910ms step_avg:57.42ms
step:1114/2330 train_time:63970ms step_avg:57.42ms
step:1115/2330 train_time:64026ms step_avg:57.42ms
step:1116/2330 train_time:64086ms step_avg:57.42ms
step:1117/2330 train_time:64142ms step_avg:57.42ms
step:1118/2330 train_time:64202ms step_avg:57.43ms
step:1119/2330 train_time:64258ms step_avg:57.42ms
step:1120/2330 train_time:64319ms step_avg:57.43ms
step:1121/2330 train_time:64374ms step_avg:57.43ms
step:1122/2330 train_time:64435ms step_avg:57.43ms
step:1123/2330 train_time:64491ms step_avg:57.43ms
step:1124/2330 train_time:64552ms step_avg:57.43ms
step:1125/2330 train_time:64609ms step_avg:57.43ms
step:1126/2330 train_time:64668ms step_avg:57.43ms
step:1127/2330 train_time:64725ms step_avg:57.43ms
step:1128/2330 train_time:64785ms step_avg:57.43ms
step:1129/2330 train_time:64841ms step_avg:57.43ms
step:1130/2330 train_time:64900ms step_avg:57.43ms
step:1131/2330 train_time:64956ms step_avg:57.43ms
step:1132/2330 train_time:65016ms step_avg:57.43ms
step:1133/2330 train_time:65071ms step_avg:57.43ms
step:1134/2330 train_time:65132ms step_avg:57.44ms
step:1135/2330 train_time:65188ms step_avg:57.43ms
step:1136/2330 train_time:65248ms step_avg:57.44ms
step:1137/2330 train_time:65304ms step_avg:57.44ms
step:1138/2330 train_time:65364ms step_avg:57.44ms
step:1139/2330 train_time:65420ms step_avg:57.44ms
step:1140/2330 train_time:65482ms step_avg:57.44ms
step:1141/2330 train_time:65538ms step_avg:57.44ms
step:1142/2330 train_time:65598ms step_avg:57.44ms
step:1143/2330 train_time:65654ms step_avg:57.44ms
step:1144/2330 train_time:65715ms step_avg:57.44ms
step:1145/2330 train_time:65771ms step_avg:57.44ms
step:1146/2330 train_time:65831ms step_avg:57.44ms
step:1147/2330 train_time:65887ms step_avg:57.44ms
step:1148/2330 train_time:65948ms step_avg:57.45ms
step:1149/2330 train_time:66005ms step_avg:57.45ms
step:1150/2330 train_time:66064ms step_avg:57.45ms
step:1151/2330 train_time:66120ms step_avg:57.45ms
step:1152/2330 train_time:66180ms step_avg:57.45ms
step:1153/2330 train_time:66236ms step_avg:57.45ms
step:1154/2330 train_time:66296ms step_avg:57.45ms
step:1155/2330 train_time:66353ms step_avg:57.45ms
step:1156/2330 train_time:66412ms step_avg:57.45ms
step:1157/2330 train_time:66469ms step_avg:57.45ms
step:1158/2330 train_time:66529ms step_avg:57.45ms
step:1159/2330 train_time:66585ms step_avg:57.45ms
step:1160/2330 train_time:66646ms step_avg:57.45ms
step:1161/2330 train_time:66702ms step_avg:57.45ms
step:1162/2330 train_time:66762ms step_avg:57.45ms
step:1163/2330 train_time:66818ms step_avg:57.45ms
step:1164/2330 train_time:66878ms step_avg:57.46ms
step:1165/2330 train_time:66934ms step_avg:57.45ms
step:1166/2330 train_time:66994ms step_avg:57.46ms
step:1167/2330 train_time:67050ms step_avg:57.46ms
step:1168/2330 train_time:67109ms step_avg:57.46ms
step:1169/2330 train_time:67166ms step_avg:57.46ms
step:1170/2330 train_time:67226ms step_avg:57.46ms
step:1171/2330 train_time:67282ms step_avg:57.46ms
step:1172/2330 train_time:67341ms step_avg:57.46ms
step:1173/2330 train_time:67398ms step_avg:57.46ms
step:1174/2330 train_time:67458ms step_avg:57.46ms
step:1175/2330 train_time:67513ms step_avg:57.46ms
step:1176/2330 train_time:67574ms step_avg:57.46ms
step:1177/2330 train_time:67631ms step_avg:57.46ms
step:1178/2330 train_time:67690ms step_avg:57.46ms
step:1179/2330 train_time:67747ms step_avg:57.46ms
step:1180/2330 train_time:67807ms step_avg:57.46ms
step:1181/2330 train_time:67863ms step_avg:57.46ms
step:1182/2330 train_time:67923ms step_avg:57.46ms
step:1183/2330 train_time:67978ms step_avg:57.46ms
step:1184/2330 train_time:68039ms step_avg:57.47ms
step:1185/2330 train_time:68095ms step_avg:57.46ms
step:1186/2330 train_time:68156ms step_avg:57.47ms
step:1187/2330 train_time:68211ms step_avg:57.47ms
step:1188/2330 train_time:68271ms step_avg:57.47ms
step:1189/2330 train_time:68328ms step_avg:57.47ms
step:1190/2330 train_time:68388ms step_avg:57.47ms
step:1191/2330 train_time:68445ms step_avg:57.47ms
step:1192/2330 train_time:68504ms step_avg:57.47ms
step:1193/2330 train_time:68560ms step_avg:57.47ms
step:1194/2330 train_time:68620ms step_avg:57.47ms
step:1195/2330 train_time:68675ms step_avg:57.47ms
step:1196/2330 train_time:68736ms step_avg:57.47ms
step:1197/2330 train_time:68793ms step_avg:57.47ms
step:1198/2330 train_time:68853ms step_avg:57.47ms
step:1199/2330 train_time:68909ms step_avg:57.47ms
step:1200/2330 train_time:68968ms step_avg:57.47ms
step:1201/2330 train_time:69024ms step_avg:57.47ms
step:1202/2330 train_time:69085ms step_avg:57.47ms
step:1203/2330 train_time:69140ms step_avg:57.47ms
step:1204/2330 train_time:69201ms step_avg:57.48ms
step:1205/2330 train_time:69258ms step_avg:57.48ms
step:1206/2330 train_time:69318ms step_avg:57.48ms
step:1207/2330 train_time:69374ms step_avg:57.48ms
step:1208/2330 train_time:69433ms step_avg:57.48ms
step:1209/2330 train_time:69490ms step_avg:57.48ms
step:1210/2330 train_time:69549ms step_avg:57.48ms
step:1211/2330 train_time:69606ms step_avg:57.48ms
step:1212/2330 train_time:69667ms step_avg:57.48ms
step:1213/2330 train_time:69722ms step_avg:57.48ms
step:1214/2330 train_time:69783ms step_avg:57.48ms
step:1215/2330 train_time:69839ms step_avg:57.48ms
step:1216/2330 train_time:69899ms step_avg:57.48ms
step:1217/2330 train_time:69954ms step_avg:57.48ms
step:1218/2330 train_time:70015ms step_avg:57.48ms
step:1219/2330 train_time:70071ms step_avg:57.48ms
step:1220/2330 train_time:70131ms step_avg:57.48ms
step:1221/2330 train_time:70187ms step_avg:57.48ms
step:1222/2330 train_time:70248ms step_avg:57.49ms
step:1223/2330 train_time:70305ms step_avg:57.49ms
step:1224/2330 train_time:70365ms step_avg:57.49ms
step:1225/2330 train_time:70421ms step_avg:57.49ms
step:1226/2330 train_time:70481ms step_avg:57.49ms
step:1227/2330 train_time:70537ms step_avg:57.49ms
step:1228/2330 train_time:70597ms step_avg:57.49ms
step:1229/2330 train_time:70653ms step_avg:57.49ms
step:1230/2330 train_time:70714ms step_avg:57.49ms
step:1231/2330 train_time:70770ms step_avg:57.49ms
step:1232/2330 train_time:70829ms step_avg:57.49ms
step:1233/2330 train_time:70885ms step_avg:57.49ms
step:1234/2330 train_time:70945ms step_avg:57.49ms
step:1235/2330 train_time:71002ms step_avg:57.49ms
step:1236/2330 train_time:71061ms step_avg:57.49ms
step:1237/2330 train_time:71117ms step_avg:57.49ms
step:1238/2330 train_time:71177ms step_avg:57.49ms
step:1239/2330 train_time:71234ms step_avg:57.49ms
step:1240/2330 train_time:71294ms step_avg:57.50ms
step:1241/2330 train_time:71350ms step_avg:57.49ms
step:1242/2330 train_time:71409ms step_avg:57.50ms
step:1243/2330 train_time:71466ms step_avg:57.49ms
step:1244/2330 train_time:71526ms step_avg:57.50ms
step:1245/2330 train_time:71582ms step_avg:57.50ms
step:1246/2330 train_time:71644ms step_avg:57.50ms
step:1247/2330 train_time:71700ms step_avg:57.50ms
step:1248/2330 train_time:71760ms step_avg:57.50ms
step:1249/2330 train_time:71816ms step_avg:57.50ms
step:1250/2330 train_time:71877ms step_avg:57.50ms
step:1250/2330 val_loss:4.5881 train_time:71957ms step_avg:57.57ms
step:1251/2330 train_time:71975ms step_avg:57.53ms
step:1252/2330 train_time:71995ms step_avg:57.50ms
step:1253/2330 train_time:72054ms step_avg:57.50ms
step:1254/2330 train_time:72118ms step_avg:57.51ms
step:1255/2330 train_time:72175ms step_avg:57.51ms
step:1256/2330 train_time:72235ms step_avg:57.51ms
step:1257/2330 train_time:72291ms step_avg:57.51ms
step:1258/2330 train_time:72351ms step_avg:57.51ms
step:1259/2330 train_time:72407ms step_avg:57.51ms
step:1260/2330 train_time:72467ms step_avg:57.51ms
step:1261/2330 train_time:72522ms step_avg:57.51ms
step:1262/2330 train_time:72581ms step_avg:57.51ms
step:1263/2330 train_time:72637ms step_avg:57.51ms
step:1264/2330 train_time:72696ms step_avg:57.51ms
step:1265/2330 train_time:72751ms step_avg:57.51ms
step:1266/2330 train_time:72810ms step_avg:57.51ms
step:1267/2330 train_time:72866ms step_avg:57.51ms
step:1268/2330 train_time:72927ms step_avg:57.51ms
step:1269/2330 train_time:72983ms step_avg:57.51ms
step:1270/2330 train_time:73045ms step_avg:57.52ms
step:1271/2330 train_time:73101ms step_avg:57.51ms
step:1272/2330 train_time:73162ms step_avg:57.52ms
step:1273/2330 train_time:73219ms step_avg:57.52ms
step:1274/2330 train_time:73280ms step_avg:57.52ms
step:1275/2330 train_time:73336ms step_avg:57.52ms
step:1276/2330 train_time:73396ms step_avg:57.52ms
step:1277/2330 train_time:73452ms step_avg:57.52ms
step:1278/2330 train_time:73513ms step_avg:57.52ms
step:1279/2330 train_time:73568ms step_avg:57.52ms
step:1280/2330 train_time:73629ms step_avg:57.52ms
step:1281/2330 train_time:73684ms step_avg:57.52ms
step:1282/2330 train_time:73744ms step_avg:57.52ms
step:1283/2330 train_time:73800ms step_avg:57.52ms
step:1284/2330 train_time:73859ms step_avg:57.52ms
step:1285/2330 train_time:73915ms step_avg:57.52ms
step:1286/2330 train_time:73976ms step_avg:57.52ms
step:1287/2330 train_time:74032ms step_avg:57.52ms
step:1288/2330 train_time:74094ms step_avg:57.53ms
step:1289/2330 train_time:74150ms step_avg:57.53ms
step:1290/2330 train_time:74211ms step_avg:57.53ms
step:1291/2330 train_time:74267ms step_avg:57.53ms
step:1292/2330 train_time:74326ms step_avg:57.53ms
step:1293/2330 train_time:74383ms step_avg:57.53ms
step:1294/2330 train_time:74443ms step_avg:57.53ms
step:1295/2330 train_time:74499ms step_avg:57.53ms
step:1296/2330 train_time:74558ms step_avg:57.53ms
step:1297/2330 train_time:74615ms step_avg:57.53ms
step:1298/2330 train_time:74675ms step_avg:57.53ms
step:1299/2330 train_time:74731ms step_avg:57.53ms
step:1300/2330 train_time:74792ms step_avg:57.53ms
step:1301/2330 train_time:74848ms step_avg:57.53ms
step:1302/2330 train_time:74907ms step_avg:57.53ms
step:1303/2330 train_time:74963ms step_avg:57.53ms
step:1304/2330 train_time:75023ms step_avg:57.53ms
step:1305/2330 train_time:75080ms step_avg:57.53ms
step:1306/2330 train_time:75140ms step_avg:57.53ms
step:1307/2330 train_time:75198ms step_avg:57.53ms
step:1308/2330 train_time:75258ms step_avg:57.54ms
step:1309/2330 train_time:75314ms step_avg:57.54ms
step:1310/2330 train_time:75374ms step_avg:57.54ms
step:1311/2330 train_time:75430ms step_avg:57.54ms
step:1312/2330 train_time:75491ms step_avg:57.54ms
step:1313/2330 train_time:75547ms step_avg:57.54ms
step:1314/2330 train_time:75606ms step_avg:57.54ms
step:1315/2330 train_time:75662ms step_avg:57.54ms
step:1316/2330 train_time:75722ms step_avg:57.54ms
step:1317/2330 train_time:75778ms step_avg:57.54ms
step:1318/2330 train_time:75837ms step_avg:57.54ms
step:1319/2330 train_time:75894ms step_avg:57.54ms
step:1320/2330 train_time:75954ms step_avg:57.54ms
step:1321/2330 train_time:76010ms step_avg:57.54ms
step:1322/2330 train_time:76070ms step_avg:57.54ms
step:1323/2330 train_time:76126ms step_avg:57.54ms
step:1324/2330 train_time:76188ms step_avg:57.54ms
step:1325/2330 train_time:76243ms step_avg:57.54ms
step:1326/2330 train_time:76305ms step_avg:57.55ms
step:1327/2330 train_time:76361ms step_avg:57.54ms
step:1328/2330 train_time:76421ms step_avg:57.55ms
step:1329/2330 train_time:76478ms step_avg:57.55ms
step:1330/2330 train_time:76538ms step_avg:57.55ms
step:1331/2330 train_time:76595ms step_avg:57.55ms
step:1332/2330 train_time:76654ms step_avg:57.55ms
step:1333/2330 train_time:76710ms step_avg:57.55ms
step:1334/2330 train_time:76769ms step_avg:57.55ms
step:1335/2330 train_time:76825ms step_avg:57.55ms
step:1336/2330 train_time:76885ms step_avg:57.55ms
step:1337/2330 train_time:76941ms step_avg:57.55ms
step:1338/2330 train_time:77001ms step_avg:57.55ms
step:1339/2330 train_time:77057ms step_avg:57.55ms
step:1340/2330 train_time:77118ms step_avg:57.55ms
step:1341/2330 train_time:77174ms step_avg:57.55ms
step:1342/2330 train_time:77235ms step_avg:57.55ms
step:1343/2330 train_time:77291ms step_avg:57.55ms
step:1344/2330 train_time:77351ms step_avg:57.55ms
step:1345/2330 train_time:77407ms step_avg:57.55ms
step:1346/2330 train_time:77467ms step_avg:57.55ms
step:1347/2330 train_time:77523ms step_avg:57.55ms
step:1348/2330 train_time:77583ms step_avg:57.55ms
step:1349/2330 train_time:77639ms step_avg:57.55ms
step:1350/2330 train_time:77698ms step_avg:57.55ms
step:1351/2330 train_time:77754ms step_avg:57.55ms
step:1352/2330 train_time:77815ms step_avg:57.56ms
step:1353/2330 train_time:77871ms step_avg:57.55ms
step:1354/2330 train_time:77931ms step_avg:57.56ms
step:1355/2330 train_time:77987ms step_avg:57.56ms
step:1356/2330 train_time:78047ms step_avg:57.56ms
step:1357/2330 train_time:78103ms step_avg:57.56ms
step:1358/2330 train_time:78163ms step_avg:57.56ms
step:1359/2330 train_time:78219ms step_avg:57.56ms
step:1360/2330 train_time:78280ms step_avg:57.56ms
step:1361/2330 train_time:78336ms step_avg:57.56ms
step:1362/2330 train_time:78396ms step_avg:57.56ms
step:1363/2330 train_time:78452ms step_avg:57.56ms
step:1364/2330 train_time:78512ms step_avg:57.56ms
step:1365/2330 train_time:78568ms step_avg:57.56ms
step:1366/2330 train_time:78628ms step_avg:57.56ms
step:1367/2330 train_time:78684ms step_avg:57.56ms
step:1368/2330 train_time:78743ms step_avg:57.56ms
step:1369/2330 train_time:78800ms step_avg:57.56ms
step:1370/2330 train_time:78860ms step_avg:57.56ms
step:1371/2330 train_time:78917ms step_avg:57.56ms
step:1372/2330 train_time:78977ms step_avg:57.56ms
step:1373/2330 train_time:79033ms step_avg:57.56ms
step:1374/2330 train_time:79092ms step_avg:57.56ms
step:1375/2330 train_time:79149ms step_avg:57.56ms
step:1376/2330 train_time:79210ms step_avg:57.57ms
step:1377/2330 train_time:79266ms step_avg:57.56ms
step:1378/2330 train_time:79326ms step_avg:57.57ms
step:1379/2330 train_time:79382ms step_avg:57.56ms
step:1380/2330 train_time:79442ms step_avg:57.57ms
step:1381/2330 train_time:79498ms step_avg:57.57ms
step:1382/2330 train_time:79558ms step_avg:57.57ms
step:1383/2330 train_time:79614ms step_avg:57.57ms
step:1384/2330 train_time:79674ms step_avg:57.57ms
step:1385/2330 train_time:79730ms step_avg:57.57ms
step:1386/2330 train_time:79789ms step_avg:57.57ms
step:1387/2330 train_time:79846ms step_avg:57.57ms
step:1388/2330 train_time:79905ms step_avg:57.57ms
step:1389/2330 train_time:79961ms step_avg:57.57ms
step:1390/2330 train_time:80021ms step_avg:57.57ms
step:1391/2330 train_time:80078ms step_avg:57.57ms
step:1392/2330 train_time:80138ms step_avg:57.57ms
step:1393/2330 train_time:80195ms step_avg:57.57ms
step:1394/2330 train_time:80255ms step_avg:57.57ms
step:1395/2330 train_time:80311ms step_avg:57.57ms
step:1396/2330 train_time:80371ms step_avg:57.57ms
step:1397/2330 train_time:80427ms step_avg:57.57ms
step:1398/2330 train_time:80488ms step_avg:57.57ms
step:1399/2330 train_time:80544ms step_avg:57.57ms
step:1400/2330 train_time:80604ms step_avg:57.57ms
step:1401/2330 train_time:80660ms step_avg:57.57ms
step:1402/2330 train_time:80720ms step_avg:57.57ms
step:1403/2330 train_time:80776ms step_avg:57.57ms
step:1404/2330 train_time:80836ms step_avg:57.58ms
step:1405/2330 train_time:80892ms step_avg:57.57ms
step:1406/2330 train_time:80953ms step_avg:57.58ms
step:1407/2330 train_time:81009ms step_avg:57.58ms
step:1408/2330 train_time:81070ms step_avg:57.58ms
step:1409/2330 train_time:81126ms step_avg:57.58ms
step:1410/2330 train_time:81185ms step_avg:57.58ms
step:1411/2330 train_time:81241ms step_avg:57.58ms
step:1412/2330 train_time:81301ms step_avg:57.58ms
step:1413/2330 train_time:81358ms step_avg:57.58ms
step:1414/2330 train_time:81418ms step_avg:57.58ms
step:1415/2330 train_time:81475ms step_avg:57.58ms
step:1416/2330 train_time:81534ms step_avg:57.58ms
step:1417/2330 train_time:81591ms step_avg:57.58ms
step:1418/2330 train_time:81650ms step_avg:57.58ms
step:1419/2330 train_time:81706ms step_avg:57.58ms
step:1420/2330 train_time:81767ms step_avg:57.58ms
step:1421/2330 train_time:81823ms step_avg:57.58ms
step:1422/2330 train_time:81884ms step_avg:57.58ms
step:1423/2330 train_time:81940ms step_avg:57.58ms
step:1424/2330 train_time:81999ms step_avg:57.58ms
step:1425/2330 train_time:82056ms step_avg:57.58ms
step:1426/2330 train_time:82116ms step_avg:57.59ms
step:1427/2330 train_time:82172ms step_avg:57.58ms
step:1428/2330 train_time:82232ms step_avg:57.59ms
step:1429/2330 train_time:82288ms step_avg:57.58ms
step:1430/2330 train_time:82349ms step_avg:57.59ms
step:1431/2330 train_time:82405ms step_avg:57.59ms
step:1432/2330 train_time:82465ms step_avg:57.59ms
step:1433/2330 train_time:82522ms step_avg:57.59ms
step:1434/2330 train_time:82582ms step_avg:57.59ms
step:1435/2330 train_time:82638ms step_avg:57.59ms
step:1436/2330 train_time:82698ms step_avg:57.59ms
step:1437/2330 train_time:82755ms step_avg:57.59ms
step:1438/2330 train_time:82814ms step_avg:57.59ms
step:1439/2330 train_time:82870ms step_avg:57.59ms
step:1440/2330 train_time:82930ms step_avg:57.59ms
step:1441/2330 train_time:82986ms step_avg:57.59ms
step:1442/2330 train_time:83046ms step_avg:57.59ms
step:1443/2330 train_time:83101ms step_avg:57.59ms
step:1444/2330 train_time:83162ms step_avg:57.59ms
step:1445/2330 train_time:83218ms step_avg:57.59ms
step:1446/2330 train_time:83279ms step_avg:57.59ms
step:1447/2330 train_time:83336ms step_avg:57.59ms
step:1448/2330 train_time:83396ms step_avg:57.59ms
step:1449/2330 train_time:83453ms step_avg:57.59ms
step:1450/2330 train_time:83513ms step_avg:57.60ms
step:1451/2330 train_time:83569ms step_avg:57.59ms
step:1452/2330 train_time:83629ms step_avg:57.60ms
step:1453/2330 train_time:83686ms step_avg:57.60ms
step:1454/2330 train_time:83745ms step_avg:57.60ms
step:1455/2330 train_time:83801ms step_avg:57.60ms
step:1456/2330 train_time:83861ms step_avg:57.60ms
step:1457/2330 train_time:83918ms step_avg:57.60ms
step:1458/2330 train_time:83977ms step_avg:57.60ms
step:1459/2330 train_time:84034ms step_avg:57.60ms
step:1460/2330 train_time:84094ms step_avg:57.60ms
step:1461/2330 train_time:84150ms step_avg:57.60ms
step:1462/2330 train_time:84210ms step_avg:57.60ms
step:1463/2330 train_time:84266ms step_avg:57.60ms
step:1464/2330 train_time:84326ms step_avg:57.60ms
step:1465/2330 train_time:84382ms step_avg:57.60ms
step:1466/2330 train_time:84443ms step_avg:57.60ms
step:1467/2330 train_time:84499ms step_avg:57.60ms
step:1468/2330 train_time:84559ms step_avg:57.60ms
step:1469/2330 train_time:84615ms step_avg:57.60ms
step:1470/2330 train_time:84676ms step_avg:57.60ms
step:1471/2330 train_time:84733ms step_avg:57.60ms
step:1472/2330 train_time:84793ms step_avg:57.60ms
step:1473/2330 train_time:84849ms step_avg:57.60ms
step:1474/2330 train_time:84909ms step_avg:57.60ms
step:1475/2330 train_time:84965ms step_avg:57.60ms
step:1476/2330 train_time:85025ms step_avg:57.60ms
step:1477/2330 train_time:85080ms step_avg:57.60ms
step:1478/2330 train_time:85141ms step_avg:57.61ms
step:1479/2330 train_time:85197ms step_avg:57.60ms
step:1480/2330 train_time:85257ms step_avg:57.61ms
step:1481/2330 train_time:85313ms step_avg:57.60ms
step:1482/2330 train_time:85374ms step_avg:57.61ms
step:1483/2330 train_time:85430ms step_avg:57.61ms
step:1484/2330 train_time:85490ms step_avg:57.61ms
step:1485/2330 train_time:85546ms step_avg:57.61ms
step:1486/2330 train_time:85607ms step_avg:57.61ms
step:1487/2330 train_time:85663ms step_avg:57.61ms
step:1488/2330 train_time:85724ms step_avg:57.61ms
step:1489/2330 train_time:85780ms step_avg:57.61ms
step:1490/2330 train_time:85840ms step_avg:57.61ms
step:1491/2330 train_time:85897ms step_avg:57.61ms
step:1492/2330 train_time:85957ms step_avg:57.61ms
step:1493/2330 train_time:86012ms step_avg:57.61ms
step:1494/2330 train_time:86073ms step_avg:57.61ms
step:1495/2330 train_time:86129ms step_avg:57.61ms
step:1496/2330 train_time:86190ms step_avg:57.61ms
step:1497/2330 train_time:86246ms step_avg:57.61ms
step:1498/2330 train_time:86306ms step_avg:57.61ms
step:1499/2330 train_time:86362ms step_avg:57.61ms
step:1500/2330 train_time:86422ms step_avg:57.61ms
step:1500/2330 val_loss:4.4302 train_time:86502ms step_avg:57.67ms
step:1501/2330 train_time:86522ms step_avg:57.64ms
step:1502/2330 train_time:86543ms step_avg:57.62ms
step:1503/2330 train_time:86598ms step_avg:57.62ms
step:1504/2330 train_time:86663ms step_avg:57.62ms
step:1505/2330 train_time:86721ms step_avg:57.62ms
step:1506/2330 train_time:86780ms step_avg:57.62ms
step:1507/2330 train_time:86837ms step_avg:57.62ms
step:1508/2330 train_time:86896ms step_avg:57.62ms
step:1509/2330 train_time:86952ms step_avg:57.62ms
step:1510/2330 train_time:87012ms step_avg:57.62ms
step:1511/2330 train_time:87069ms step_avg:57.62ms
step:1512/2330 train_time:87128ms step_avg:57.62ms
step:1513/2330 train_time:87184ms step_avg:57.62ms
step:1514/2330 train_time:87242ms step_avg:57.62ms
step:1515/2330 train_time:87298ms step_avg:57.62ms
step:1516/2330 train_time:87357ms step_avg:57.62ms
step:1517/2330 train_time:87413ms step_avg:57.62ms
step:1518/2330 train_time:87473ms step_avg:57.62ms
step:1519/2330 train_time:87529ms step_avg:57.62ms
step:1520/2330 train_time:87591ms step_avg:57.63ms
step:1521/2330 train_time:87648ms step_avg:57.63ms
step:1522/2330 train_time:87709ms step_avg:57.63ms
step:1523/2330 train_time:87766ms step_avg:57.63ms
step:1524/2330 train_time:87826ms step_avg:57.63ms
step:1525/2330 train_time:87882ms step_avg:57.63ms
step:1526/2330 train_time:87942ms step_avg:57.63ms
step:1527/2330 train_time:87998ms step_avg:57.63ms
step:1528/2330 train_time:88058ms step_avg:57.63ms
step:1529/2330 train_time:88116ms step_avg:57.63ms
step:1530/2330 train_time:88174ms step_avg:57.63ms
step:1531/2330 train_time:88231ms step_avg:57.63ms
step:1532/2330 train_time:88290ms step_avg:57.63ms
step:1533/2330 train_time:88347ms step_avg:57.63ms
step:1534/2330 train_time:88406ms step_avg:57.63ms
step:1535/2330 train_time:88462ms step_avg:57.63ms
step:1536/2330 train_time:88522ms step_avg:57.63ms
step:1537/2330 train_time:88579ms step_avg:57.63ms
step:1538/2330 train_time:88641ms step_avg:57.63ms
step:1539/2330 train_time:88698ms step_avg:57.63ms
step:1540/2330 train_time:88760ms step_avg:57.64ms
step:1541/2330 train_time:88817ms step_avg:57.64ms
step:1542/2330 train_time:88877ms step_avg:57.64ms
step:1543/2330 train_time:88934ms step_avg:57.64ms
step:1544/2330 train_time:88994ms step_avg:57.64ms
step:1545/2330 train_time:89050ms step_avg:57.64ms
step:1546/2330 train_time:89111ms step_avg:57.64ms
step:1547/2330 train_time:89168ms step_avg:57.64ms
step:1548/2330 train_time:89227ms step_avg:57.64ms
step:1549/2330 train_time:89284ms step_avg:57.64ms
step:1550/2330 train_time:89343ms step_avg:57.64ms
step:1551/2330 train_time:89399ms step_avg:57.64ms
step:1552/2330 train_time:89461ms step_avg:57.64ms
step:1553/2330 train_time:89517ms step_avg:57.64ms
step:1554/2330 train_time:89577ms step_avg:57.64ms
step:1555/2330 train_time:89633ms step_avg:57.64ms
step:1556/2330 train_time:89694ms step_avg:57.64ms
step:1557/2330 train_time:89750ms step_avg:57.64ms
step:1558/2330 train_time:89813ms step_avg:57.65ms
step:1559/2330 train_time:89870ms step_avg:57.65ms
step:1560/2330 train_time:89931ms step_avg:57.65ms
step:1561/2330 train_time:89987ms step_avg:57.65ms
step:1562/2330 train_time:90048ms step_avg:57.65ms
step:1563/2330 train_time:90105ms step_avg:57.65ms
step:1564/2330 train_time:90165ms step_avg:57.65ms
step:1565/2330 train_time:90222ms step_avg:57.65ms
step:1566/2330 train_time:90282ms step_avg:57.65ms
step:1567/2330 train_time:90338ms step_avg:57.65ms
step:1568/2330 train_time:90398ms step_avg:57.65ms
step:1569/2330 train_time:90454ms step_avg:57.65ms
step:1570/2330 train_time:90514ms step_avg:57.65ms
step:1571/2330 train_time:90571ms step_avg:57.65ms
step:1572/2330 train_time:90631ms step_avg:57.65ms
step:1573/2330 train_time:90689ms step_avg:57.65ms
step:1574/2330 train_time:90749ms step_avg:57.65ms
step:1575/2330 train_time:90805ms step_avg:57.65ms
step:1576/2330 train_time:90866ms step_avg:57.66ms
step:1577/2330 train_time:90922ms step_avg:57.66ms
step:1578/2330 train_time:90982ms step_avg:57.66ms
step:1579/2330 train_time:91039ms step_avg:57.66ms
step:1580/2330 train_time:91099ms step_avg:57.66ms
step:1581/2330 train_time:91156ms step_avg:57.66ms
step:1582/2330 train_time:91217ms step_avg:57.66ms
step:1583/2330 train_time:91274ms step_avg:57.66ms
step:1584/2330 train_time:91334ms step_avg:57.66ms
step:1585/2330 train_time:91390ms step_avg:57.66ms
step:1586/2330 train_time:91450ms step_avg:57.66ms
step:1587/2330 train_time:91506ms step_avg:57.66ms
step:1588/2330 train_time:91567ms step_avg:57.66ms
step:1589/2330 train_time:91623ms step_avg:57.66ms
step:1590/2330 train_time:91684ms step_avg:57.66ms
step:1591/2330 train_time:91740ms step_avg:57.66ms
step:1592/2330 train_time:91801ms step_avg:57.66ms
step:1593/2330 train_time:91857ms step_avg:57.66ms
step:1594/2330 train_time:91918ms step_avg:57.66ms
step:1595/2330 train_time:91975ms step_avg:57.66ms
step:1596/2330 train_time:92035ms step_avg:57.67ms
step:1597/2330 train_time:92092ms step_avg:57.67ms
step:1598/2330 train_time:92152ms step_avg:57.67ms
step:1599/2330 train_time:92209ms step_avg:57.67ms
step:1600/2330 train_time:92269ms step_avg:57.67ms
step:1601/2330 train_time:92326ms step_avg:57.67ms
step:1602/2330 train_time:92386ms step_avg:57.67ms
step:1603/2330 train_time:92442ms step_avg:57.67ms
step:1604/2330 train_time:92503ms step_avg:57.67ms
step:1605/2330 train_time:92559ms step_avg:57.67ms
step:1606/2330 train_time:92621ms step_avg:57.67ms
step:1607/2330 train_time:92677ms step_avg:57.67ms
step:1608/2330 train_time:92738ms step_avg:57.67ms
step:1609/2330 train_time:92795ms step_avg:57.67ms
step:1610/2330 train_time:92855ms step_avg:57.67ms
step:1611/2330 train_time:92912ms step_avg:57.67ms
step:1612/2330 train_time:92971ms step_avg:57.67ms
step:1613/2330 train_time:93028ms step_avg:57.67ms
step:1614/2330 train_time:93090ms step_avg:57.68ms
step:1615/2330 train_time:93147ms step_avg:57.68ms
step:1616/2330 train_time:93207ms step_avg:57.68ms
step:1617/2330 train_time:93264ms step_avg:57.68ms
step:1618/2330 train_time:93324ms step_avg:57.68ms
step:1619/2330 train_time:93381ms step_avg:57.68ms
step:1620/2330 train_time:93440ms step_avg:57.68ms
step:1621/2330 train_time:93497ms step_avg:57.68ms
step:1622/2330 train_time:93557ms step_avg:57.68ms
step:1623/2330 train_time:93615ms step_avg:57.68ms
step:1624/2330 train_time:93673ms step_avg:57.68ms
step:1625/2330 train_time:93730ms step_avg:57.68ms
step:1626/2330 train_time:93791ms step_avg:57.68ms
step:1627/2330 train_time:93848ms step_avg:57.68ms
step:1628/2330 train_time:93908ms step_avg:57.68ms
step:1629/2330 train_time:93965ms step_avg:57.68ms
step:1630/2330 train_time:94026ms step_avg:57.68ms
step:1631/2330 train_time:94082ms step_avg:57.68ms
step:1632/2330 train_time:94143ms step_avg:57.69ms
step:1633/2330 train_time:94200ms step_avg:57.68ms
step:1634/2330 train_time:94260ms step_avg:57.69ms
step:1635/2330 train_time:94318ms step_avg:57.69ms
step:1636/2330 train_time:94377ms step_avg:57.69ms
step:1637/2330 train_time:94434ms step_avg:57.69ms
step:1638/2330 train_time:94494ms step_avg:57.69ms
step:1639/2330 train_time:94550ms step_avg:57.69ms
step:1640/2330 train_time:94612ms step_avg:57.69ms
step:1641/2330 train_time:94669ms step_avg:57.69ms
step:1642/2330 train_time:94728ms step_avg:57.69ms
step:1643/2330 train_time:94785ms step_avg:57.69ms
step:1644/2330 train_time:94845ms step_avg:57.69ms
step:1645/2330 train_time:94902ms step_avg:57.69ms
step:1646/2330 train_time:94962ms step_avg:57.69ms
step:1647/2330 train_time:95018ms step_avg:57.69ms
step:1648/2330 train_time:95079ms step_avg:57.69ms
step:1649/2330 train_time:95135ms step_avg:57.69ms
step:1650/2330 train_time:95196ms step_avg:57.69ms
step:1651/2330 train_time:95253ms step_avg:57.69ms
step:1652/2330 train_time:95314ms step_avg:57.70ms
step:1653/2330 train_time:95371ms step_avg:57.70ms
step:1654/2330 train_time:95431ms step_avg:57.70ms
step:1655/2330 train_time:95488ms step_avg:57.70ms
step:1656/2330 train_time:95547ms step_avg:57.70ms
step:1657/2330 train_time:95604ms step_avg:57.70ms
step:1658/2330 train_time:95664ms step_avg:57.70ms
step:1659/2330 train_time:95720ms step_avg:57.70ms
step:1660/2330 train_time:95781ms step_avg:57.70ms
step:1661/2330 train_time:95837ms step_avg:57.70ms
step:1662/2330 train_time:95898ms step_avg:57.70ms
step:1663/2330 train_time:95954ms step_avg:57.70ms
step:1664/2330 train_time:96015ms step_avg:57.70ms
step:1665/2330 train_time:96072ms step_avg:57.70ms
step:1666/2330 train_time:96132ms step_avg:57.70ms
step:1667/2330 train_time:96189ms step_avg:57.70ms
step:1668/2330 train_time:96250ms step_avg:57.70ms
step:1669/2330 train_time:96307ms step_avg:57.70ms
step:1670/2330 train_time:96367ms step_avg:57.71ms
step:1671/2330 train_time:96424ms step_avg:57.70ms
step:1672/2330 train_time:96485ms step_avg:57.71ms
step:1673/2330 train_time:96542ms step_avg:57.71ms
step:1674/2330 train_time:96602ms step_avg:57.71ms
step:1675/2330 train_time:96658ms step_avg:57.71ms
step:1676/2330 train_time:96718ms step_avg:57.71ms
step:1677/2330 train_time:96774ms step_avg:57.71ms
step:1678/2330 train_time:96834ms step_avg:57.71ms
step:1679/2330 train_time:96891ms step_avg:57.71ms
step:1680/2330 train_time:96952ms step_avg:57.71ms
step:1681/2330 train_time:97009ms step_avg:57.71ms
step:1682/2330 train_time:97069ms step_avg:57.71ms
step:1683/2330 train_time:97125ms step_avg:57.71ms
step:1684/2330 train_time:97186ms step_avg:57.71ms
step:1685/2330 train_time:97243ms step_avg:57.71ms
step:1686/2330 train_time:97303ms step_avg:57.71ms
step:1687/2330 train_time:97359ms step_avg:57.71ms
step:1688/2330 train_time:97420ms step_avg:57.71ms
step:1689/2330 train_time:97477ms step_avg:57.71ms
step:1690/2330 train_time:97538ms step_avg:57.71ms
step:1691/2330 train_time:97595ms step_avg:57.71ms
step:1692/2330 train_time:97655ms step_avg:57.72ms
step:1693/2330 train_time:97712ms step_avg:57.72ms
step:1694/2330 train_time:97772ms step_avg:57.72ms
step:1695/2330 train_time:97830ms step_avg:57.72ms
step:1696/2330 train_time:97889ms step_avg:57.72ms
step:1697/2330 train_time:97945ms step_avg:57.72ms
step:1698/2330 train_time:98006ms step_avg:57.72ms
step:1699/2330 train_time:98063ms step_avg:57.72ms
step:1700/2330 train_time:98123ms step_avg:57.72ms
step:1701/2330 train_time:98179ms step_avg:57.72ms
step:1702/2330 train_time:98240ms step_avg:57.72ms
step:1703/2330 train_time:98296ms step_avg:57.72ms
step:1704/2330 train_time:98357ms step_avg:57.72ms
step:1705/2330 train_time:98414ms step_avg:57.72ms
step:1706/2330 train_time:98474ms step_avg:57.72ms
step:1707/2330 train_time:98531ms step_avg:57.72ms
step:1708/2330 train_time:98591ms step_avg:57.72ms
step:1709/2330 train_time:98648ms step_avg:57.72ms
step:1710/2330 train_time:98708ms step_avg:57.72ms
step:1711/2330 train_time:98764ms step_avg:57.72ms
step:1712/2330 train_time:98825ms step_avg:57.72ms
step:1713/2330 train_time:98882ms step_avg:57.72ms
step:1714/2330 train_time:98941ms step_avg:57.73ms
step:1715/2330 train_time:98998ms step_avg:57.72ms
step:1716/2330 train_time:99059ms step_avg:57.73ms
step:1717/2330 train_time:99115ms step_avg:57.73ms
step:1718/2330 train_time:99175ms step_avg:57.73ms
step:1719/2330 train_time:99231ms step_avg:57.73ms
step:1720/2330 train_time:99293ms step_avg:57.73ms
step:1721/2330 train_time:99349ms step_avg:57.73ms
step:1722/2330 train_time:99410ms step_avg:57.73ms
step:1723/2330 train_time:99466ms step_avg:57.73ms
step:1724/2330 train_time:99527ms step_avg:57.73ms
step:1725/2330 train_time:99584ms step_avg:57.73ms
step:1726/2330 train_time:99644ms step_avg:57.73ms
step:1727/2330 train_time:99701ms step_avg:57.73ms
step:1728/2330 train_time:99762ms step_avg:57.73ms
step:1729/2330 train_time:99818ms step_avg:57.73ms
step:1730/2330 train_time:99878ms step_avg:57.73ms
step:1731/2330 train_time:99934ms step_avg:57.73ms
step:1732/2330 train_time:99996ms step_avg:57.73ms
step:1733/2330 train_time:100052ms step_avg:57.73ms
step:1734/2330 train_time:100113ms step_avg:57.74ms
step:1735/2330 train_time:100170ms step_avg:57.73ms
step:1736/2330 train_time:100230ms step_avg:57.74ms
step:1737/2330 train_time:100287ms step_avg:57.74ms
step:1738/2330 train_time:100346ms step_avg:57.74ms
step:1739/2330 train_time:100403ms step_avg:57.74ms
step:1740/2330 train_time:100463ms step_avg:57.74ms
step:1741/2330 train_time:100520ms step_avg:57.74ms
step:1742/2330 train_time:100580ms step_avg:57.74ms
step:1743/2330 train_time:100637ms step_avg:57.74ms
step:1744/2330 train_time:100698ms step_avg:57.74ms
step:1745/2330 train_time:100755ms step_avg:57.74ms
step:1746/2330 train_time:100815ms step_avg:57.74ms
step:1747/2330 train_time:100873ms step_avg:57.74ms
step:1748/2330 train_time:100932ms step_avg:57.74ms
step:1749/2330 train_time:100990ms step_avg:57.74ms
step:1750/2330 train_time:101049ms step_avg:57.74ms
step:1750/2330 val_loss:4.3099 train_time:101131ms step_avg:57.79ms
step:1751/2330 train_time:101150ms step_avg:57.77ms
step:1752/2330 train_time:101169ms step_avg:57.74ms
step:1753/2330 train_time:101223ms step_avg:57.74ms
step:1754/2330 train_time:101292ms step_avg:57.75ms
step:1755/2330 train_time:101347ms step_avg:57.75ms
step:1756/2330 train_time:101411ms step_avg:57.75ms
step:1757/2330 train_time:101467ms step_avg:57.75ms
step:1758/2330 train_time:101526ms step_avg:57.75ms
step:1759/2330 train_time:101583ms step_avg:57.75ms
step:1760/2330 train_time:101642ms step_avg:57.75ms
step:1761/2330 train_time:101699ms step_avg:57.75ms
step:1762/2330 train_time:101758ms step_avg:57.75ms
step:1763/2330 train_time:101814ms step_avg:57.75ms
step:1764/2330 train_time:101873ms step_avg:57.75ms
step:1765/2330 train_time:101929ms step_avg:57.75ms
step:1766/2330 train_time:101988ms step_avg:57.75ms
step:1767/2330 train_time:102049ms step_avg:57.75ms
step:1768/2330 train_time:102110ms step_avg:57.75ms
step:1769/2330 train_time:102168ms step_avg:57.75ms
step:1770/2330 train_time:102229ms step_avg:57.76ms
step:1771/2330 train_time:102287ms step_avg:57.76ms
step:1772/2330 train_time:102346ms step_avg:57.76ms
step:1773/2330 train_time:102403ms step_avg:57.76ms
step:1774/2330 train_time:102463ms step_avg:57.76ms
step:1775/2330 train_time:102519ms step_avg:57.76ms
step:1776/2330 train_time:102580ms step_avg:57.76ms
step:1777/2330 train_time:102636ms step_avg:57.76ms
step:1778/2330 train_time:102696ms step_avg:57.76ms
step:1779/2330 train_time:102752ms step_avg:57.76ms
step:1780/2330 train_time:102811ms step_avg:57.76ms
step:1781/2330 train_time:102867ms step_avg:57.76ms
step:1782/2330 train_time:102927ms step_avg:57.76ms
step:1783/2330 train_time:102984ms step_avg:57.76ms
step:1784/2330 train_time:103044ms step_avg:57.76ms
step:1785/2330 train_time:103101ms step_avg:57.76ms
step:1786/2330 train_time:103163ms step_avg:57.76ms
step:1787/2330 train_time:103220ms step_avg:57.76ms
step:1788/2330 train_time:103282ms step_avg:57.76ms
step:1789/2330 train_time:103338ms step_avg:57.76ms
step:1790/2330 train_time:103400ms step_avg:57.77ms
step:1791/2330 train_time:103456ms step_avg:57.76ms
step:1792/2330 train_time:103518ms step_avg:57.77ms
step:1793/2330 train_time:103574ms step_avg:57.77ms
step:1794/2330 train_time:103634ms step_avg:57.77ms
step:1795/2330 train_time:103689ms step_avg:57.77ms
step:1796/2330 train_time:103751ms step_avg:57.77ms
step:1797/2330 train_time:103807ms step_avg:57.77ms
step:1798/2330 train_time:103866ms step_avg:57.77ms
step:1799/2330 train_time:103923ms step_avg:57.77ms
step:1800/2330 train_time:103983ms step_avg:57.77ms
step:1801/2330 train_time:104040ms step_avg:57.77ms
step:1802/2330 train_time:104101ms step_avg:57.77ms
step:1803/2330 train_time:104159ms step_avg:57.77ms
step:1804/2330 train_time:104218ms step_avg:57.77ms
step:1805/2330 train_time:104276ms step_avg:57.77ms
step:1806/2330 train_time:104336ms step_avg:57.77ms
step:1807/2330 train_time:104393ms step_avg:57.77ms
step:1808/2330 train_time:104454ms step_avg:57.77ms
step:1809/2330 train_time:104510ms step_avg:57.77ms
step:1810/2330 train_time:104571ms step_avg:57.77ms
step:1811/2330 train_time:104627ms step_avg:57.77ms
step:1812/2330 train_time:104687ms step_avg:57.77ms
step:1813/2330 train_time:104743ms step_avg:57.77ms
step:1814/2330 train_time:104804ms step_avg:57.78ms
step:1815/2330 train_time:104860ms step_avg:57.77ms
step:1816/2330 train_time:104921ms step_avg:57.78ms
step:1817/2330 train_time:104978ms step_avg:57.78ms
step:1818/2330 train_time:105038ms step_avg:57.78ms
step:1819/2330 train_time:105094ms step_avg:57.78ms
step:1820/2330 train_time:105155ms step_avg:57.78ms
step:1821/2330 train_time:105212ms step_avg:57.78ms
step:1822/2330 train_time:105273ms step_avg:57.78ms
step:1823/2330 train_time:105330ms step_avg:57.78ms
step:1824/2330 train_time:105389ms step_avg:57.78ms
step:1825/2330 train_time:105446ms step_avg:57.78ms
step:1826/2330 train_time:105506ms step_avg:57.78ms
step:1827/2330 train_time:105563ms step_avg:57.78ms
step:1828/2330 train_time:105623ms step_avg:57.78ms
step:1829/2330 train_time:105679ms step_avg:57.78ms
step:1830/2330 train_time:105741ms step_avg:57.78ms
step:1831/2330 train_time:105797ms step_avg:57.78ms
step:1832/2330 train_time:105858ms step_avg:57.78ms
step:1833/2330 train_time:105915ms step_avg:57.78ms
step:1834/2330 train_time:105975ms step_avg:57.78ms
step:1835/2330 train_time:106032ms step_avg:57.78ms
step:1836/2330 train_time:106092ms step_avg:57.78ms
step:1837/2330 train_time:106149ms step_avg:57.78ms
step:1838/2330 train_time:106209ms step_avg:57.78ms
step:1839/2330 train_time:106266ms step_avg:57.78ms
step:1840/2330 train_time:106327ms step_avg:57.79ms
step:1841/2330 train_time:106384ms step_avg:57.79ms
step:1842/2330 train_time:106444ms step_avg:57.79ms
step:1843/2330 train_time:106501ms step_avg:57.79ms
step:1844/2330 train_time:106562ms step_avg:57.79ms
step:1845/2330 train_time:106620ms step_avg:57.79ms
step:1846/2330 train_time:106679ms step_avg:57.79ms
step:1847/2330 train_time:106736ms step_avg:57.79ms
step:1848/2330 train_time:106795ms step_avg:57.79ms
step:1849/2330 train_time:106852ms step_avg:57.79ms
step:1850/2330 train_time:106912ms step_avg:57.79ms
step:1851/2330 train_time:106968ms step_avg:57.79ms
step:1852/2330 train_time:107028ms step_avg:57.79ms
step:1853/2330 train_time:107085ms step_avg:57.79ms
step:1854/2330 train_time:107146ms step_avg:57.79ms
step:1855/2330 train_time:107203ms step_avg:57.79ms
step:1856/2330 train_time:107264ms step_avg:57.79ms
step:1857/2330 train_time:107322ms step_avg:57.79ms
step:1858/2330 train_time:107382ms step_avg:57.79ms
step:1859/2330 train_time:107439ms step_avg:57.79ms
step:1860/2330 train_time:107499ms step_avg:57.79ms
step:1861/2330 train_time:107555ms step_avg:57.79ms
step:1862/2330 train_time:107616ms step_avg:57.80ms
step:1863/2330 train_time:107672ms step_avg:57.80ms
step:1864/2330 train_time:107732ms step_avg:57.80ms
step:1865/2330 train_time:107788ms step_avg:57.80ms
step:1866/2330 train_time:107848ms step_avg:57.80ms
step:1867/2330 train_time:107905ms step_avg:57.80ms
step:1868/2330 train_time:107965ms step_avg:57.80ms
step:1869/2330 train_time:108022ms step_avg:57.80ms
step:1870/2330 train_time:108083ms step_avg:57.80ms
step:1871/2330 train_time:108139ms step_avg:57.80ms
step:1872/2330 train_time:108200ms step_avg:57.80ms
step:1873/2330 train_time:108256ms step_avg:57.80ms
step:1874/2330 train_time:108317ms step_avg:57.80ms
step:1875/2330 train_time:108373ms step_avg:57.80ms
step:1876/2330 train_time:108433ms step_avg:57.80ms
step:1877/2330 train_time:108490ms step_avg:57.80ms
step:1878/2330 train_time:108550ms step_avg:57.80ms
step:1879/2330 train_time:108608ms step_avg:57.80ms
step:1880/2330 train_time:108668ms step_avg:57.80ms
step:1881/2330 train_time:108725ms step_avg:57.80ms
step:1882/2330 train_time:108784ms step_avg:57.80ms
step:1883/2330 train_time:108841ms step_avg:57.80ms
step:1884/2330 train_time:108901ms step_avg:57.80ms
step:1885/2330 train_time:108957ms step_avg:57.80ms
step:1886/2330 train_time:109018ms step_avg:57.80ms
step:1887/2330 train_time:109075ms step_avg:57.80ms
step:1888/2330 train_time:109135ms step_avg:57.80ms
step:1889/2330 train_time:109192ms step_avg:57.80ms
step:1890/2330 train_time:109252ms step_avg:57.81ms
step:1891/2330 train_time:109309ms step_avg:57.80ms
step:1892/2330 train_time:109369ms step_avg:57.81ms
step:1893/2330 train_time:109425ms step_avg:57.81ms
step:1894/2330 train_time:109486ms step_avg:57.81ms
step:1895/2330 train_time:109543ms step_avg:57.81ms
step:1896/2330 train_time:109603ms step_avg:57.81ms
step:1897/2330 train_time:109660ms step_avg:57.81ms
step:1898/2330 train_time:109720ms step_avg:57.81ms
step:1899/2330 train_time:109777ms step_avg:57.81ms
step:1900/2330 train_time:109837ms step_avg:57.81ms
step:1901/2330 train_time:109894ms step_avg:57.81ms
step:1902/2330 train_time:109954ms step_avg:57.81ms
step:1903/2330 train_time:110011ms step_avg:57.81ms
step:1904/2330 train_time:110071ms step_avg:57.81ms
step:1905/2330 train_time:110128ms step_avg:57.81ms
step:1906/2330 train_time:110188ms step_avg:57.81ms
step:1907/2330 train_time:110244ms step_avg:57.81ms
step:1908/2330 train_time:110305ms step_avg:57.81ms
step:1909/2330 train_time:110361ms step_avg:57.81ms
step:1910/2330 train_time:110421ms step_avg:57.81ms
step:1911/2330 train_time:110478ms step_avg:57.81ms
step:1912/2330 train_time:110538ms step_avg:57.81ms
step:1913/2330 train_time:110595ms step_avg:57.81ms
step:1914/2330 train_time:110656ms step_avg:57.81ms
step:1915/2330 train_time:110713ms step_avg:57.81ms
step:1916/2330 train_time:110773ms step_avg:57.81ms
step:1917/2330 train_time:110829ms step_avg:57.81ms
step:1918/2330 train_time:110890ms step_avg:57.82ms
step:1919/2330 train_time:110946ms step_avg:57.81ms
step:1920/2330 train_time:111007ms step_avg:57.82ms
step:1921/2330 train_time:111064ms step_avg:57.82ms
step:1922/2330 train_time:111124ms step_avg:57.82ms
step:1923/2330 train_time:111181ms step_avg:57.82ms
step:1924/2330 train_time:111241ms step_avg:57.82ms
step:1925/2330 train_time:111298ms step_avg:57.82ms
step:1926/2330 train_time:111359ms step_avg:57.82ms
step:1927/2330 train_time:111415ms step_avg:57.82ms
step:1928/2330 train_time:111476ms step_avg:57.82ms
step:1929/2330 train_time:111533ms step_avg:57.82ms
step:1930/2330 train_time:111593ms step_avg:57.82ms
step:1931/2330 train_time:111649ms step_avg:57.82ms
step:1932/2330 train_time:111709ms step_avg:57.82ms
step:1933/2330 train_time:111766ms step_avg:57.82ms
step:1934/2330 train_time:111826ms step_avg:57.82ms
step:1935/2330 train_time:111882ms step_avg:57.82ms
step:1936/2330 train_time:111942ms step_avg:57.82ms
step:1937/2330 train_time:111999ms step_avg:57.82ms
step:1938/2330 train_time:112059ms step_avg:57.82ms
step:1939/2330 train_time:112116ms step_avg:57.82ms
step:1940/2330 train_time:112177ms step_avg:57.82ms
step:1941/2330 train_time:112233ms step_avg:57.82ms
step:1942/2330 train_time:112294ms step_avg:57.82ms
step:1943/2330 train_time:112350ms step_avg:57.82ms
step:1944/2330 train_time:112411ms step_avg:57.82ms
step:1945/2330 train_time:112467ms step_avg:57.82ms
step:1946/2330 train_time:112527ms step_avg:57.82ms
step:1947/2330 train_time:112584ms step_avg:57.82ms
step:1948/2330 train_time:112645ms step_avg:57.83ms
step:1949/2330 train_time:112702ms step_avg:57.83ms
step:1950/2330 train_time:112761ms step_avg:57.83ms
step:1951/2330 train_time:112818ms step_avg:57.83ms
step:1952/2330 train_time:112878ms step_avg:57.83ms
step:1953/2330 train_time:112934ms step_avg:57.83ms
step:1954/2330 train_time:112995ms step_avg:57.83ms
step:1955/2330 train_time:113052ms step_avg:57.83ms
step:1956/2330 train_time:113112ms step_avg:57.83ms
step:1957/2330 train_time:113169ms step_avg:57.83ms
step:1958/2330 train_time:113229ms step_avg:57.83ms
step:1959/2330 train_time:113286ms step_avg:57.83ms
step:1960/2330 train_time:113346ms step_avg:57.83ms
step:1961/2330 train_time:113403ms step_avg:57.83ms
step:1962/2330 train_time:113463ms step_avg:57.83ms
step:1963/2330 train_time:113520ms step_avg:57.83ms
step:1964/2330 train_time:113580ms step_avg:57.83ms
step:1965/2330 train_time:113637ms step_avg:57.83ms
step:1966/2330 train_time:113697ms step_avg:57.83ms
step:1967/2330 train_time:113755ms step_avg:57.83ms
step:1968/2330 train_time:113814ms step_avg:57.83ms
step:1969/2330 train_time:113871ms step_avg:57.83ms
step:1970/2330 train_time:113931ms step_avg:57.83ms
step:1971/2330 train_time:113988ms step_avg:57.83ms
step:1972/2330 train_time:114047ms step_avg:57.83ms
step:1973/2330 train_time:114103ms step_avg:57.83ms
step:1974/2330 train_time:114165ms step_avg:57.83ms
step:1975/2330 train_time:114222ms step_avg:57.83ms
step:1976/2330 train_time:114283ms step_avg:57.84ms
step:1977/2330 train_time:114339ms step_avg:57.83ms
step:1978/2330 train_time:114399ms step_avg:57.84ms
step:1979/2330 train_time:114456ms step_avg:57.84ms
step:1980/2330 train_time:114517ms step_avg:57.84ms
step:1981/2330 train_time:114574ms step_avg:57.84ms
step:1982/2330 train_time:114634ms step_avg:57.84ms
step:1983/2330 train_time:114691ms step_avg:57.84ms
step:1984/2330 train_time:114750ms step_avg:57.84ms
step:1985/2330 train_time:114807ms step_avg:57.84ms
step:1986/2330 train_time:114868ms step_avg:57.84ms
step:1987/2330 train_time:114925ms step_avg:57.84ms
step:1988/2330 train_time:114984ms step_avg:57.84ms
step:1989/2330 train_time:115042ms step_avg:57.84ms
step:1990/2330 train_time:115101ms step_avg:57.84ms
step:1991/2330 train_time:115158ms step_avg:57.84ms
step:1992/2330 train_time:115218ms step_avg:57.84ms
step:1993/2330 train_time:115274ms step_avg:57.84ms
step:1994/2330 train_time:115335ms step_avg:57.84ms
step:1995/2330 train_time:115391ms step_avg:57.84ms
step:1996/2330 train_time:115452ms step_avg:57.84ms
step:1997/2330 train_time:115509ms step_avg:57.84ms
step:1998/2330 train_time:115570ms step_avg:57.84ms
step:1999/2330 train_time:115627ms step_avg:57.84ms
step:2000/2330 train_time:115687ms step_avg:57.84ms
step:2000/2330 val_loss:4.2307 train_time:115768ms step_avg:57.88ms
step:2001/2330 train_time:115786ms step_avg:57.86ms
step:2002/2330 train_time:115807ms step_avg:57.85ms
step:2003/2330 train_time:115867ms step_avg:57.85ms
step:2004/2330 train_time:115931ms step_avg:57.85ms
step:2005/2330 train_time:115989ms step_avg:57.85ms
step:2006/2330 train_time:116050ms step_avg:57.85ms
step:2007/2330 train_time:116107ms step_avg:57.85ms
step:2008/2330 train_time:116166ms step_avg:57.85ms
step:2009/2330 train_time:116222ms step_avg:57.85ms
step:2010/2330 train_time:116282ms step_avg:57.85ms
step:2011/2330 train_time:116338ms step_avg:57.85ms
step:2012/2330 train_time:116398ms step_avg:57.85ms
step:2013/2330 train_time:116453ms step_avg:57.85ms
step:2014/2330 train_time:116514ms step_avg:57.85ms
step:2015/2330 train_time:116570ms step_avg:57.85ms
step:2016/2330 train_time:116629ms step_avg:57.85ms
step:2017/2330 train_time:116685ms step_avg:57.85ms
step:2018/2330 train_time:116745ms step_avg:57.85ms
step:2019/2330 train_time:116804ms step_avg:57.85ms
step:2020/2330 train_time:116865ms step_avg:57.85ms
step:2021/2330 train_time:116923ms step_avg:57.85ms
step:2022/2330 train_time:116986ms step_avg:57.86ms
step:2023/2330 train_time:117044ms step_avg:57.86ms
step:2024/2330 train_time:117105ms step_avg:57.86ms
step:2025/2330 train_time:117162ms step_avg:57.86ms
step:2026/2330 train_time:117222ms step_avg:57.86ms
step:2027/2330 train_time:117278ms step_avg:57.86ms
step:2028/2330 train_time:117337ms step_avg:57.86ms
step:2029/2330 train_time:117395ms step_avg:57.86ms
step:2030/2330 train_time:117453ms step_avg:57.86ms
step:2031/2330 train_time:117509ms step_avg:57.86ms
step:2032/2330 train_time:117569ms step_avg:57.86ms
step:2033/2330 train_time:117625ms step_avg:57.86ms
step:2034/2330 train_time:117685ms step_avg:57.86ms
step:2035/2330 train_time:117742ms step_avg:57.86ms
step:2036/2330 train_time:117802ms step_avg:57.86ms
step:2037/2330 train_time:117859ms step_avg:57.86ms
step:2038/2330 train_time:117921ms step_avg:57.86ms
step:2039/2330 train_time:117977ms step_avg:57.86ms
step:2040/2330 train_time:118039ms step_avg:57.86ms
step:2041/2330 train_time:118096ms step_avg:57.86ms
step:2042/2330 train_time:118157ms step_avg:57.86ms
step:2043/2330 train_time:118213ms step_avg:57.86ms
step:2044/2330 train_time:118274ms step_avg:57.86ms
step:2045/2330 train_time:118330ms step_avg:57.86ms
step:2046/2330 train_time:118392ms step_avg:57.86ms
step:2047/2330 train_time:118449ms step_avg:57.86ms
step:2048/2330 train_time:118508ms step_avg:57.87ms
step:2049/2330 train_time:118564ms step_avg:57.86ms
step:2050/2330 train_time:118624ms step_avg:57.87ms
step:2051/2330 train_time:118680ms step_avg:57.86ms
step:2052/2330 train_time:118741ms step_avg:57.87ms
step:2053/2330 train_time:118799ms step_avg:57.87ms
step:2054/2330 train_time:118859ms step_avg:57.87ms
step:2055/2330 train_time:118915ms step_avg:57.87ms
step:2056/2330 train_time:118977ms step_avg:57.87ms
step:2057/2330 train_time:119034ms step_avg:57.87ms
step:2058/2330 train_time:119094ms step_avg:57.87ms
step:2059/2330 train_time:119150ms step_avg:57.87ms
step:2060/2330 train_time:119211ms step_avg:57.87ms
step:2061/2330 train_time:119267ms step_avg:57.87ms
step:2062/2330 train_time:119328ms step_avg:57.87ms
step:2063/2330 train_time:119385ms step_avg:57.87ms
step:2064/2330 train_time:119445ms step_avg:57.87ms
step:2065/2330 train_time:119502ms step_avg:57.87ms
step:2066/2330 train_time:119561ms step_avg:57.87ms
step:2067/2330 train_time:119617ms step_avg:57.87ms
step:2068/2330 train_time:119677ms step_avg:57.87ms
step:2069/2330 train_time:119734ms step_avg:57.87ms
step:2070/2330 train_time:119795ms step_avg:57.87ms
step:2071/2330 train_time:119851ms step_avg:57.87ms
step:2072/2330 train_time:119912ms step_avg:57.87ms
step:2073/2330 train_time:119970ms step_avg:57.87ms
step:2074/2330 train_time:120030ms step_avg:57.87ms
step:2075/2330 train_time:120087ms step_avg:57.87ms
step:2076/2330 train_time:120147ms step_avg:57.87ms
step:2077/2330 train_time:120204ms step_avg:57.87ms
step:2078/2330 train_time:120265ms step_avg:57.88ms
step:2079/2330 train_time:120321ms step_avg:57.87ms
step:2080/2330 train_time:120382ms step_avg:57.88ms
step:2081/2330 train_time:120438ms step_avg:57.88ms
step:2082/2330 train_time:120498ms step_avg:57.88ms
step:2083/2330 train_time:120555ms step_avg:57.88ms
step:2084/2330 train_time:120614ms step_avg:57.88ms
step:2085/2330 train_time:120671ms step_avg:57.88ms
step:2086/2330 train_time:120731ms step_avg:57.88ms
step:2087/2330 train_time:120788ms step_avg:57.88ms
step:2088/2330 train_time:120849ms step_avg:57.88ms
step:2089/2330 train_time:120906ms step_avg:57.88ms
step:2090/2330 train_time:120967ms step_avg:57.88ms
step:2091/2330 train_time:121024ms step_avg:57.88ms
step:2092/2330 train_time:121085ms step_avg:57.88ms
step:2093/2330 train_time:121142ms step_avg:57.88ms
step:2094/2330 train_time:121202ms step_avg:57.88ms
step:2095/2330 train_time:121259ms step_avg:57.88ms
step:2096/2330 train_time:121319ms step_avg:57.88ms
step:2097/2330 train_time:121375ms step_avg:57.88ms
step:2098/2330 train_time:121435ms step_avg:57.88ms
step:2099/2330 train_time:121491ms step_avg:57.88ms
step:2100/2330 train_time:121551ms step_avg:57.88ms
step:2101/2330 train_time:121608ms step_avg:57.88ms
step:2102/2330 train_time:121668ms step_avg:57.88ms
step:2103/2330 train_time:121725ms step_avg:57.88ms
step:2104/2330 train_time:121785ms step_avg:57.88ms
step:2105/2330 train_time:121843ms step_avg:57.88ms
step:2106/2330 train_time:121903ms step_avg:57.88ms
step:2107/2330 train_time:121959ms step_avg:57.88ms
step:2108/2330 train_time:122020ms step_avg:57.88ms
step:2109/2330 train_time:122077ms step_avg:57.88ms
step:2110/2330 train_time:122137ms step_avg:57.88ms
step:2111/2330 train_time:122194ms step_avg:57.88ms
step:2112/2330 train_time:122254ms step_avg:57.89ms
step:2113/2330 train_time:122311ms step_avg:57.88ms
step:2114/2330 train_time:122371ms step_avg:57.89ms
step:2115/2330 train_time:122428ms step_avg:57.89ms
step:2116/2330 train_time:122487ms step_avg:57.89ms
step:2117/2330 train_time:122544ms step_avg:57.89ms
step:2118/2330 train_time:122604ms step_avg:57.89ms
step:2119/2330 train_time:122661ms step_avg:57.89ms
step:2120/2330 train_time:122721ms step_avg:57.89ms
step:2121/2330 train_time:122778ms step_avg:57.89ms
step:2122/2330 train_time:122838ms step_avg:57.89ms
step:2123/2330 train_time:122895ms step_avg:57.89ms
step:2124/2330 train_time:122955ms step_avg:57.89ms
step:2125/2330 train_time:123011ms step_avg:57.89ms
step:2126/2330 train_time:123072ms step_avg:57.89ms
step:2127/2330 train_time:123129ms step_avg:57.89ms
step:2128/2330 train_time:123190ms step_avg:57.89ms
step:2129/2330 train_time:123247ms step_avg:57.89ms
step:2130/2330 train_time:123307ms step_avg:57.89ms
step:2131/2330 train_time:123364ms step_avg:57.89ms
step:2132/2330 train_time:123425ms step_avg:57.89ms
step:2133/2330 train_time:123481ms step_avg:57.89ms
step:2134/2330 train_time:123541ms step_avg:57.89ms
step:2135/2330 train_time:123598ms step_avg:57.89ms
step:2136/2330 train_time:123658ms step_avg:57.89ms
step:2137/2330 train_time:123713ms step_avg:57.89ms
step:2138/2330 train_time:123775ms step_avg:57.89ms
step:2139/2330 train_time:123831ms step_avg:57.89ms
step:2140/2330 train_time:123892ms step_avg:57.89ms
step:2141/2330 train_time:123949ms step_avg:57.89ms
step:2142/2330 train_time:124009ms step_avg:57.89ms
step:2143/2330 train_time:124066ms step_avg:57.89ms
step:2144/2330 train_time:124126ms step_avg:57.89ms
step:2145/2330 train_time:124183ms step_avg:57.89ms
step:2146/2330 train_time:124242ms step_avg:57.89ms
step:2147/2330 train_time:124299ms step_avg:57.89ms
step:2148/2330 train_time:124359ms step_avg:57.90ms
step:2149/2330 train_time:124416ms step_avg:57.89ms
step:2150/2330 train_time:124477ms step_avg:57.90ms
step:2151/2330 train_time:124534ms step_avg:57.90ms
step:2152/2330 train_time:124594ms step_avg:57.90ms
step:2153/2330 train_time:124650ms step_avg:57.90ms
step:2154/2330 train_time:124711ms step_avg:57.90ms
step:2155/2330 train_time:124768ms step_avg:57.90ms
step:2156/2330 train_time:124827ms step_avg:57.90ms
step:2157/2330 train_time:124884ms step_avg:57.90ms
step:2158/2330 train_time:124945ms step_avg:57.90ms
step:2159/2330 train_time:125002ms step_avg:57.90ms
step:2160/2330 train_time:125062ms step_avg:57.90ms
step:2161/2330 train_time:125119ms step_avg:57.90ms
step:2162/2330 train_time:125180ms step_avg:57.90ms
step:2163/2330 train_time:125237ms step_avg:57.90ms
step:2164/2330 train_time:125297ms step_avg:57.90ms
step:2165/2330 train_time:125353ms step_avg:57.90ms
step:2166/2330 train_time:125414ms step_avg:57.90ms
step:2167/2330 train_time:125471ms step_avg:57.90ms
step:2168/2330 train_time:125531ms step_avg:57.90ms
step:2169/2330 train_time:125588ms step_avg:57.90ms
step:2170/2330 train_time:125648ms step_avg:57.90ms
step:2171/2330 train_time:125705ms step_avg:57.90ms
step:2172/2330 train_time:125765ms step_avg:57.90ms
step:2173/2330 train_time:125822ms step_avg:57.90ms
step:2174/2330 train_time:125882ms step_avg:57.90ms
step:2175/2330 train_time:125939ms step_avg:57.90ms
step:2176/2330 train_time:125999ms step_avg:57.90ms
step:2177/2330 train_time:126056ms step_avg:57.90ms
step:2178/2330 train_time:126116ms step_avg:57.90ms
step:2179/2330 train_time:126173ms step_avg:57.90ms
step:2180/2330 train_time:126233ms step_avg:57.90ms
step:2181/2330 train_time:126290ms step_avg:57.90ms
step:2182/2330 train_time:126350ms step_avg:57.91ms
step:2183/2330 train_time:126407ms step_avg:57.91ms
step:2184/2330 train_time:126467ms step_avg:57.91ms
step:2185/2330 train_time:126524ms step_avg:57.91ms
step:2186/2330 train_time:126585ms step_avg:57.91ms
step:2187/2330 train_time:126642ms step_avg:57.91ms
step:2188/2330 train_time:126703ms step_avg:57.91ms
step:2189/2330 train_time:126760ms step_avg:57.91ms
step:2190/2330 train_time:126819ms step_avg:57.91ms
step:2191/2330 train_time:126876ms step_avg:57.91ms
step:2192/2330 train_time:126936ms step_avg:57.91ms
step:2193/2330 train_time:126993ms step_avg:57.91ms
step:2194/2330 train_time:127053ms step_avg:57.91ms
step:2195/2330 train_time:127110ms step_avg:57.91ms
step:2196/2330 train_time:127170ms step_avg:57.91ms
step:2197/2330 train_time:127225ms step_avg:57.91ms
step:2198/2330 train_time:127287ms step_avg:57.91ms
step:2199/2330 train_time:127344ms step_avg:57.91ms
step:2200/2330 train_time:127404ms step_avg:57.91ms
step:2201/2330 train_time:127461ms step_avg:57.91ms
step:2202/2330 train_time:127521ms step_avg:57.91ms
step:2203/2330 train_time:127577ms step_avg:57.91ms
step:2204/2330 train_time:127637ms step_avg:57.91ms
step:2205/2330 train_time:127694ms step_avg:57.91ms
step:2206/2330 train_time:127754ms step_avg:57.91ms
step:2207/2330 train_time:127810ms step_avg:57.91ms
step:2208/2330 train_time:127871ms step_avg:57.91ms
step:2209/2330 train_time:127928ms step_avg:57.91ms
step:2210/2330 train_time:127988ms step_avg:57.91ms
step:2211/2330 train_time:128045ms step_avg:57.91ms
step:2212/2330 train_time:128105ms step_avg:57.91ms
step:2213/2330 train_time:128163ms step_avg:57.91ms
step:2214/2330 train_time:128223ms step_avg:57.91ms
step:2215/2330 train_time:128279ms step_avg:57.91ms
step:2216/2330 train_time:128340ms step_avg:57.92ms
step:2217/2330 train_time:128396ms step_avg:57.91ms
step:2218/2330 train_time:128457ms step_avg:57.92ms
step:2219/2330 train_time:128513ms step_avg:57.91ms
step:2220/2330 train_time:128574ms step_avg:57.92ms
step:2221/2330 train_time:128630ms step_avg:57.92ms
step:2222/2330 train_time:128690ms step_avg:57.92ms
step:2223/2330 train_time:128747ms step_avg:57.92ms
step:2224/2330 train_time:128807ms step_avg:57.92ms
step:2225/2330 train_time:128864ms step_avg:57.92ms
step:2226/2330 train_time:128925ms step_avg:57.92ms
step:2227/2330 train_time:128982ms step_avg:57.92ms
step:2228/2330 train_time:129042ms step_avg:57.92ms
step:2229/2330 train_time:129098ms step_avg:57.92ms
step:2230/2330 train_time:129158ms step_avg:57.92ms
step:2231/2330 train_time:129214ms step_avg:57.92ms
step:2232/2330 train_time:129276ms step_avg:57.92ms
step:2233/2330 train_time:129332ms step_avg:57.92ms
step:2234/2330 train_time:129393ms step_avg:57.92ms
step:2235/2330 train_time:129449ms step_avg:57.92ms
step:2236/2330 train_time:129509ms step_avg:57.92ms
step:2237/2330 train_time:129566ms step_avg:57.92ms
step:2238/2330 train_time:129627ms step_avg:57.92ms
step:2239/2330 train_time:129684ms step_avg:57.92ms
step:2240/2330 train_time:129744ms step_avg:57.92ms
step:2241/2330 train_time:129800ms step_avg:57.92ms
step:2242/2330 train_time:129861ms step_avg:57.92ms
step:2243/2330 train_time:129917ms step_avg:57.92ms
step:2244/2330 train_time:129977ms step_avg:57.92ms
step:2245/2330 train_time:130034ms step_avg:57.92ms
step:2246/2330 train_time:130094ms step_avg:57.92ms
step:2247/2330 train_time:130152ms step_avg:57.92ms
step:2248/2330 train_time:130212ms step_avg:57.92ms
step:2249/2330 train_time:130269ms step_avg:57.92ms
step:2250/2330 train_time:130329ms step_avg:57.92ms
step:2250/2330 val_loss:4.1717 train_time:130410ms step_avg:57.96ms
step:2251/2330 train_time:130429ms step_avg:57.94ms
step:2252/2330 train_time:130449ms step_avg:57.93ms
step:2253/2330 train_time:130509ms step_avg:57.93ms
step:2254/2330 train_time:130572ms step_avg:57.93ms
step:2255/2330 train_time:130629ms step_avg:57.93ms
step:2256/2330 train_time:130692ms step_avg:57.93ms
step:2257/2330 train_time:130747ms step_avg:57.93ms
step:2258/2330 train_time:130808ms step_avg:57.93ms
step:2259/2330 train_time:130865ms step_avg:57.93ms
step:2260/2330 train_time:130924ms step_avg:57.93ms
step:2261/2330 train_time:130980ms step_avg:57.93ms
step:2262/2330 train_time:131039ms step_avg:57.93ms
step:2263/2330 train_time:131096ms step_avg:57.93ms
step:2264/2330 train_time:131156ms step_avg:57.93ms
step:2265/2330 train_time:131212ms step_avg:57.93ms
step:2266/2330 train_time:131272ms step_avg:57.93ms
step:2267/2330 train_time:131329ms step_avg:57.93ms
step:2268/2330 train_time:131389ms step_avg:57.93ms
step:2269/2330 train_time:131447ms step_avg:57.93ms
step:2270/2330 train_time:131510ms step_avg:57.93ms
step:2271/2330 train_time:131568ms step_avg:57.93ms
step:2272/2330 train_time:131631ms step_avg:57.94ms
step:2273/2330 train_time:131687ms step_avg:57.94ms
step:2274/2330 train_time:131748ms step_avg:57.94ms
step:2275/2330 train_time:131805ms step_avg:57.94ms
step:2276/2330 train_time:131865ms step_avg:57.94ms
step:2277/2330 train_time:131921ms step_avg:57.94ms
step:2278/2330 train_time:131980ms step_avg:57.94ms
step:2279/2330 train_time:132036ms step_avg:57.94ms
step:2280/2330 train_time:132097ms step_avg:57.94ms
step:2281/2330 train_time:132152ms step_avg:57.94ms
step:2282/2330 train_time:132213ms step_avg:57.94ms
step:2283/2330 train_time:132269ms step_avg:57.94ms
step:2284/2330 train_time:132329ms step_avg:57.94ms
step:2285/2330 train_time:132387ms step_avg:57.94ms
step:2286/2330 train_time:132446ms step_avg:57.94ms
step:2287/2330 train_time:132505ms step_avg:57.94ms
step:2288/2330 train_time:132565ms step_avg:57.94ms
step:2289/2330 train_time:132622ms step_avg:57.94ms
step:2290/2330 train_time:132683ms step_avg:57.94ms
step:2291/2330 train_time:132740ms step_avg:57.94ms
step:2292/2330 train_time:132800ms step_avg:57.94ms
step:2293/2330 train_time:132857ms step_avg:57.94ms
step:2294/2330 train_time:132917ms step_avg:57.94ms
step:2295/2330 train_time:132974ms step_avg:57.94ms
step:2296/2330 train_time:133033ms step_avg:57.94ms
step:2297/2330 train_time:133090ms step_avg:57.94ms
step:2298/2330 train_time:133149ms step_avg:57.94ms
step:2299/2330 train_time:133205ms step_avg:57.94ms
step:2300/2330 train_time:133264ms step_avg:57.94ms
step:2301/2330 train_time:133321ms step_avg:57.94ms
step:2302/2330 train_time:133382ms step_avg:57.94ms
step:2303/2330 train_time:133438ms step_avg:57.94ms
step:2304/2330 train_time:133498ms step_avg:57.94ms
step:2305/2330 train_time:133556ms step_avg:57.94ms
step:2306/2330 train_time:133618ms step_avg:57.94ms
step:2307/2330 train_time:133675ms step_avg:57.94ms
step:2308/2330 train_time:133735ms step_avg:57.94ms
step:2309/2330 train_time:133792ms step_avg:57.94ms
step:2310/2330 train_time:133852ms step_avg:57.94ms
step:2311/2330 train_time:133909ms step_avg:57.94ms
step:2312/2330 train_time:133970ms step_avg:57.95ms
step:2313/2330 train_time:134026ms step_avg:57.94ms
step:2314/2330 train_time:134087ms step_avg:57.95ms
step:2315/2330 train_time:134143ms step_avg:57.95ms
step:2316/2330 train_time:134203ms step_avg:57.95ms
step:2317/2330 train_time:134259ms step_avg:57.95ms
step:2318/2330 train_time:134320ms step_avg:57.95ms
step:2319/2330 train_time:134376ms step_avg:57.95ms
step:2320/2330 train_time:134436ms step_avg:57.95ms
step:2321/2330 train_time:134493ms step_avg:57.95ms
step:2322/2330 train_time:134554ms step_avg:57.95ms
step:2323/2330 train_time:134612ms step_avg:57.95ms
step:2324/2330 train_time:134672ms step_avg:57.95ms
step:2325/2330 train_time:134730ms step_avg:57.95ms
step:2326/2330 train_time:134789ms step_avg:57.95ms
step:2327/2330 train_time:134846ms step_avg:57.95ms
step:2328/2330 train_time:134906ms step_avg:57.95ms
step:2329/2330 train_time:134964ms step_avg:57.95ms
step:2330/2330 train_time:135024ms step_avg:57.95ms
step:2330/2330 val_loss:4.1556 train_time:135105ms step_avg:57.98ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
