import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--adamw_lr', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_lr_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_lr_tuning/{args2.adamw_lr}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=float(args2.adamw_lr),  betas=(0.85, 0.85), eps=1e-10, weight_decay=0.0, fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_lr_tuning/{args2.adamw_lr}", exist_ok=True)
            torch.save(log, f"logs_adamw_small_lr_tuning/{args2.adamw_lr}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 02:46:30 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   35C    P0             117W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   34C    P0             117W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   29C    P0             118W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   28C    P0             115W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   34C    P0             118W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             114W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:88ms step_avg:87.86ms
step:2/2330 train_time:178ms step_avg:89.14ms
step:3/2330 train_time:197ms step_avg:65.66ms
step:4/2330 train_time:216ms step_avg:54.06ms
step:5/2330 train_time:269ms step_avg:53.82ms
step:6/2330 train_time:326ms step_avg:54.41ms
step:7/2330 train_time:380ms step_avg:54.30ms
step:8/2330 train_time:437ms step_avg:54.67ms
step:9/2330 train_time:491ms step_avg:54.56ms
step:10/2330 train_time:548ms step_avg:54.81ms
step:11/2330 train_time:602ms step_avg:54.74ms
step:12/2330 train_time:659ms step_avg:54.90ms
step:13/2330 train_time:713ms step_avg:54.83ms
step:14/2330 train_time:769ms step_avg:54.95ms
step:15/2330 train_time:823ms step_avg:54.88ms
step:16/2330 train_time:880ms step_avg:55.02ms
step:17/2330 train_time:934ms step_avg:54.95ms
step:18/2330 train_time:991ms step_avg:55.06ms
step:19/2330 train_time:1045ms step_avg:55.00ms
step:20/2330 train_time:1103ms step_avg:55.13ms
step:21/2330 train_time:1158ms step_avg:55.15ms
step:22/2330 train_time:1215ms step_avg:55.23ms
step:23/2330 train_time:1270ms step_avg:55.23ms
step:24/2330 train_time:1328ms step_avg:55.32ms
step:25/2330 train_time:1383ms step_avg:55.31ms
step:26/2330 train_time:1440ms step_avg:55.39ms
step:27/2330 train_time:1494ms step_avg:55.34ms
step:28/2330 train_time:1551ms step_avg:55.40ms
step:29/2330 train_time:1606ms step_avg:55.38ms
step:30/2330 train_time:1663ms step_avg:55.42ms
step:31/2330 train_time:1717ms step_avg:55.39ms
step:32/2330 train_time:1774ms step_avg:55.44ms
step:33/2330 train_time:1828ms step_avg:55.40ms
step:34/2330 train_time:1886ms step_avg:55.46ms
step:35/2330 train_time:1940ms step_avg:55.42ms
step:36/2330 train_time:1997ms step_avg:55.49ms
step:37/2330 train_time:2052ms step_avg:55.45ms
step:38/2330 train_time:2110ms step_avg:55.52ms
step:39/2330 train_time:2164ms step_avg:55.49ms
step:40/2330 train_time:2222ms step_avg:55.55ms
step:41/2330 train_time:2277ms step_avg:55.53ms
step:42/2330 train_time:2334ms step_avg:55.57ms
step:43/2330 train_time:2389ms step_avg:55.55ms
step:44/2330 train_time:2446ms step_avg:55.59ms
step:45/2330 train_time:2501ms step_avg:55.57ms
step:46/2330 train_time:2558ms step_avg:55.61ms
step:47/2330 train_time:2612ms step_avg:55.57ms
step:48/2330 train_time:2670ms step_avg:55.62ms
step:49/2330 train_time:2724ms step_avg:55.60ms
step:50/2330 train_time:2782ms step_avg:55.64ms
step:51/2330 train_time:2837ms step_avg:55.62ms
step:52/2330 train_time:2894ms step_avg:55.65ms
step:53/2330 train_time:2948ms step_avg:55.62ms
step:54/2330 train_time:3006ms step_avg:55.67ms
step:55/2330 train_time:3061ms step_avg:55.66ms
step:56/2330 train_time:3119ms step_avg:55.70ms
step:57/2330 train_time:3174ms step_avg:55.68ms
step:58/2330 train_time:3231ms step_avg:55.71ms
step:59/2330 train_time:3286ms step_avg:55.70ms
step:60/2330 train_time:3344ms step_avg:55.73ms
step:61/2330 train_time:3399ms step_avg:55.72ms
step:62/2330 train_time:3456ms step_avg:55.75ms
step:63/2330 train_time:3511ms step_avg:55.73ms
step:64/2330 train_time:3571ms step_avg:55.80ms
step:65/2330 train_time:3626ms step_avg:55.79ms
step:66/2330 train_time:3684ms step_avg:55.82ms
step:67/2330 train_time:3739ms step_avg:55.80ms
step:68/2330 train_time:3796ms step_avg:55.83ms
step:69/2330 train_time:3851ms step_avg:55.81ms
step:70/2330 train_time:3909ms step_avg:55.84ms
step:71/2330 train_time:3963ms step_avg:55.82ms
step:72/2330 train_time:4022ms step_avg:55.86ms
step:73/2330 train_time:4076ms step_avg:55.84ms
step:74/2330 train_time:4134ms step_avg:55.87ms
step:75/2330 train_time:4189ms step_avg:55.86ms
step:76/2330 train_time:4247ms step_avg:55.88ms
step:77/2330 train_time:4301ms step_avg:55.86ms
step:78/2330 train_time:4359ms step_avg:55.89ms
step:79/2330 train_time:4414ms step_avg:55.87ms
step:80/2330 train_time:4472ms step_avg:55.90ms
step:81/2330 train_time:4527ms step_avg:55.89ms
step:82/2330 train_time:4586ms step_avg:55.92ms
step:83/2330 train_time:4641ms step_avg:55.91ms
step:84/2330 train_time:4699ms step_avg:55.94ms
step:85/2330 train_time:4753ms step_avg:55.92ms
step:86/2330 train_time:4812ms step_avg:55.96ms
step:87/2330 train_time:4867ms step_avg:55.94ms
step:88/2330 train_time:4925ms step_avg:55.96ms
step:89/2330 train_time:4980ms step_avg:55.95ms
step:90/2330 train_time:5037ms step_avg:55.97ms
step:91/2330 train_time:5092ms step_avg:55.95ms
step:92/2330 train_time:5150ms step_avg:55.97ms
step:93/2330 train_time:5204ms step_avg:55.96ms
step:94/2330 train_time:5262ms step_avg:55.98ms
step:95/2330 train_time:5317ms step_avg:55.97ms
step:96/2330 train_time:5375ms step_avg:55.99ms
step:97/2330 train_time:5430ms step_avg:55.98ms
step:98/2330 train_time:5487ms step_avg:55.99ms
step:99/2330 train_time:5543ms step_avg:55.99ms
step:100/2330 train_time:5601ms step_avg:56.01ms
step:101/2330 train_time:5656ms step_avg:56.00ms
step:102/2330 train_time:5714ms step_avg:56.02ms
step:103/2330 train_time:5769ms step_avg:56.01ms
step:104/2330 train_time:5829ms step_avg:56.04ms
step:105/2330 train_time:5884ms step_avg:56.04ms
step:106/2330 train_time:5942ms step_avg:56.06ms
step:107/2330 train_time:5997ms step_avg:56.05ms
step:108/2330 train_time:6054ms step_avg:56.06ms
step:109/2330 train_time:6109ms step_avg:56.04ms
step:110/2330 train_time:6167ms step_avg:56.07ms
step:111/2330 train_time:6222ms step_avg:56.05ms
step:112/2330 train_time:6282ms step_avg:56.09ms
step:113/2330 train_time:6336ms step_avg:56.07ms
step:114/2330 train_time:6394ms step_avg:56.08ms
step:115/2330 train_time:6448ms step_avg:56.07ms
step:116/2330 train_time:6506ms step_avg:56.09ms
step:117/2330 train_time:6561ms step_avg:56.08ms
step:118/2330 train_time:6619ms step_avg:56.09ms
step:119/2330 train_time:6673ms step_avg:56.08ms
step:120/2330 train_time:6732ms step_avg:56.10ms
step:121/2330 train_time:6787ms step_avg:56.09ms
step:122/2330 train_time:6845ms step_avg:56.11ms
step:123/2330 train_time:6901ms step_avg:56.10ms
step:124/2330 train_time:6959ms step_avg:56.12ms
step:125/2330 train_time:7014ms step_avg:56.11ms
step:126/2330 train_time:7072ms step_avg:56.12ms
step:127/2330 train_time:7127ms step_avg:56.12ms
step:128/2330 train_time:7185ms step_avg:56.13ms
step:129/2330 train_time:7240ms step_avg:56.12ms
step:130/2330 train_time:7298ms step_avg:56.14ms
step:131/2330 train_time:7352ms step_avg:56.12ms
step:132/2330 train_time:7412ms step_avg:56.15ms
step:133/2330 train_time:7467ms step_avg:56.14ms
step:134/2330 train_time:7524ms step_avg:56.15ms
step:135/2330 train_time:7580ms step_avg:56.15ms
step:136/2330 train_time:7638ms step_avg:56.16ms
step:137/2330 train_time:7693ms step_avg:56.15ms
step:138/2330 train_time:7752ms step_avg:56.17ms
step:139/2330 train_time:7807ms step_avg:56.17ms
step:140/2330 train_time:7865ms step_avg:56.18ms
step:141/2330 train_time:7921ms step_avg:56.18ms
step:142/2330 train_time:7979ms step_avg:56.19ms
step:143/2330 train_time:8034ms step_avg:56.18ms
step:144/2330 train_time:8092ms step_avg:56.20ms
step:145/2330 train_time:8148ms step_avg:56.19ms
step:146/2330 train_time:8206ms step_avg:56.20ms
step:147/2330 train_time:8261ms step_avg:56.20ms
step:148/2330 train_time:8319ms step_avg:56.21ms
step:149/2330 train_time:8375ms step_avg:56.21ms
step:150/2330 train_time:8433ms step_avg:56.22ms
step:151/2330 train_time:8488ms step_avg:56.21ms
step:152/2330 train_time:8546ms step_avg:56.22ms
step:153/2330 train_time:8601ms step_avg:56.21ms
step:154/2330 train_time:8660ms step_avg:56.23ms
step:155/2330 train_time:8715ms step_avg:56.22ms
step:156/2330 train_time:8774ms step_avg:56.24ms
step:157/2330 train_time:8829ms step_avg:56.23ms
step:158/2330 train_time:8888ms step_avg:56.25ms
step:159/2330 train_time:8943ms step_avg:56.24ms
step:160/2330 train_time:9001ms step_avg:56.26ms
step:161/2330 train_time:9057ms step_avg:56.25ms
step:162/2330 train_time:9116ms step_avg:56.27ms
step:163/2330 train_time:9170ms step_avg:56.26ms
step:164/2330 train_time:9230ms step_avg:56.28ms
step:165/2330 train_time:9285ms step_avg:56.27ms
step:166/2330 train_time:9344ms step_avg:56.29ms
step:167/2330 train_time:9399ms step_avg:56.28ms
step:168/2330 train_time:9458ms step_avg:56.30ms
step:169/2330 train_time:9513ms step_avg:56.29ms
step:170/2330 train_time:9572ms step_avg:56.30ms
step:171/2330 train_time:9627ms step_avg:56.30ms
step:172/2330 train_time:9685ms step_avg:56.31ms
step:173/2330 train_time:9740ms step_avg:56.30ms
step:174/2330 train_time:9799ms step_avg:56.32ms
step:175/2330 train_time:9855ms step_avg:56.31ms
step:176/2330 train_time:9912ms step_avg:56.32ms
step:177/2330 train_time:9968ms step_avg:56.31ms
step:178/2330 train_time:10026ms step_avg:56.33ms
step:179/2330 train_time:10081ms step_avg:56.32ms
step:180/2330 train_time:10140ms step_avg:56.33ms
step:181/2330 train_time:10195ms step_avg:56.33ms
step:182/2330 train_time:10254ms step_avg:56.34ms
step:183/2330 train_time:10309ms step_avg:56.33ms
step:184/2330 train_time:10368ms step_avg:56.35ms
step:185/2330 train_time:10423ms step_avg:56.34ms
step:186/2330 train_time:10482ms step_avg:56.36ms
step:187/2330 train_time:10537ms step_avg:56.35ms
step:188/2330 train_time:10596ms step_avg:56.36ms
step:189/2330 train_time:10650ms step_avg:56.35ms
step:190/2330 train_time:10710ms step_avg:56.37ms
step:191/2330 train_time:10765ms step_avg:56.36ms
step:192/2330 train_time:10824ms step_avg:56.37ms
step:193/2330 train_time:10879ms step_avg:56.37ms
step:194/2330 train_time:10937ms step_avg:56.38ms
step:195/2330 train_time:10992ms step_avg:56.37ms
step:196/2330 train_time:11051ms step_avg:56.38ms
step:197/2330 train_time:11107ms step_avg:56.38ms
step:198/2330 train_time:11165ms step_avg:56.39ms
step:199/2330 train_time:11220ms step_avg:56.38ms
step:200/2330 train_time:11278ms step_avg:56.39ms
step:201/2330 train_time:11333ms step_avg:56.38ms
step:202/2330 train_time:11392ms step_avg:56.40ms
step:203/2330 train_time:11448ms step_avg:56.39ms
step:204/2330 train_time:11507ms step_avg:56.41ms
step:205/2330 train_time:11562ms step_avg:56.40ms
step:206/2330 train_time:11621ms step_avg:56.41ms
step:207/2330 train_time:11676ms step_avg:56.41ms
step:208/2330 train_time:11735ms step_avg:56.42ms
step:209/2330 train_time:11790ms step_avg:56.41ms
step:210/2330 train_time:11850ms step_avg:56.43ms
step:211/2330 train_time:11906ms step_avg:56.42ms
step:212/2330 train_time:11965ms step_avg:56.44ms
step:213/2330 train_time:12020ms step_avg:56.43ms
step:214/2330 train_time:12079ms step_avg:56.44ms
step:215/2330 train_time:12134ms step_avg:56.44ms
step:216/2330 train_time:12192ms step_avg:56.44ms
step:217/2330 train_time:12248ms step_avg:56.44ms
step:218/2330 train_time:12305ms step_avg:56.45ms
step:219/2330 train_time:12361ms step_avg:56.44ms
step:220/2330 train_time:12420ms step_avg:56.45ms
step:221/2330 train_time:12475ms step_avg:56.45ms
step:222/2330 train_time:12533ms step_avg:56.46ms
step:223/2330 train_time:12589ms step_avg:56.45ms
step:224/2330 train_time:12648ms step_avg:56.46ms
step:225/2330 train_time:12703ms step_avg:56.46ms
step:226/2330 train_time:12763ms step_avg:56.47ms
step:227/2330 train_time:12818ms step_avg:56.47ms
step:228/2330 train_time:12876ms step_avg:56.47ms
step:229/2330 train_time:12931ms step_avg:56.47ms
step:230/2330 train_time:12990ms step_avg:56.48ms
step:231/2330 train_time:13046ms step_avg:56.47ms
step:232/2330 train_time:13105ms step_avg:56.48ms
step:233/2330 train_time:13160ms step_avg:56.48ms
step:234/2330 train_time:13219ms step_avg:56.49ms
step:235/2330 train_time:13273ms step_avg:56.48ms
step:236/2330 train_time:13332ms step_avg:56.49ms
step:237/2330 train_time:13388ms step_avg:56.49ms
step:238/2330 train_time:13446ms step_avg:56.50ms
step:239/2330 train_time:13502ms step_avg:56.49ms
step:240/2330 train_time:13561ms step_avg:56.50ms
step:241/2330 train_time:13616ms step_avg:56.50ms
step:242/2330 train_time:13675ms step_avg:56.51ms
step:243/2330 train_time:13731ms step_avg:56.50ms
step:244/2330 train_time:13789ms step_avg:56.51ms
step:245/2330 train_time:13845ms step_avg:56.51ms
step:246/2330 train_time:13903ms step_avg:56.52ms
step:247/2330 train_time:13958ms step_avg:56.51ms
step:248/2330 train_time:14017ms step_avg:56.52ms
step:249/2330 train_time:14072ms step_avg:56.51ms
step:250/2330 train_time:14131ms step_avg:56.52ms
step:250/2330 val_loss:6.4180 train_time:14209ms step_avg:56.84ms
step:251/2330 train_time:14228ms step_avg:56.69ms
step:252/2330 train_time:14247ms step_avg:56.54ms
step:253/2330 train_time:14303ms step_avg:56.53ms
step:254/2330 train_time:14364ms step_avg:56.55ms
step:255/2330 train_time:14420ms step_avg:56.55ms
step:256/2330 train_time:14485ms step_avg:56.58ms
step:257/2330 train_time:14539ms step_avg:56.57ms
step:258/2330 train_time:14599ms step_avg:56.59ms
step:259/2330 train_time:14655ms step_avg:56.58ms
step:260/2330 train_time:14713ms step_avg:56.59ms
step:261/2330 train_time:14768ms step_avg:56.58ms
step:262/2330 train_time:14825ms step_avg:56.59ms
step:263/2330 train_time:14880ms step_avg:56.58ms
step:264/2330 train_time:14939ms step_avg:56.59ms
step:265/2330 train_time:14994ms step_avg:56.58ms
step:266/2330 train_time:15052ms step_avg:56.59ms
step:267/2330 train_time:15107ms step_avg:56.58ms
step:268/2330 train_time:15165ms step_avg:56.59ms
step:269/2330 train_time:15221ms step_avg:56.58ms
step:270/2330 train_time:15281ms step_avg:56.60ms
step:271/2330 train_time:15337ms step_avg:56.60ms
step:272/2330 train_time:15398ms step_avg:56.61ms
step:273/2330 train_time:15454ms step_avg:56.61ms
step:274/2330 train_time:15514ms step_avg:56.62ms
step:275/2330 train_time:15570ms step_avg:56.62ms
step:276/2330 train_time:15629ms step_avg:56.63ms
step:277/2330 train_time:15684ms step_avg:56.62ms
step:278/2330 train_time:15744ms step_avg:56.63ms
step:279/2330 train_time:15799ms step_avg:56.63ms
step:280/2330 train_time:15857ms step_avg:56.63ms
step:281/2330 train_time:15912ms step_avg:56.63ms
step:282/2330 train_time:15971ms step_avg:56.64ms
step:283/2330 train_time:16027ms step_avg:56.63ms
step:284/2330 train_time:16085ms step_avg:56.64ms
step:285/2330 train_time:16141ms step_avg:56.63ms
step:286/2330 train_time:16200ms step_avg:56.64ms
step:287/2330 train_time:16256ms step_avg:56.64ms
step:288/2330 train_time:16316ms step_avg:56.65ms
step:289/2330 train_time:16371ms step_avg:56.65ms
step:290/2330 train_time:16431ms step_avg:56.66ms
step:291/2330 train_time:16487ms step_avg:56.66ms
step:292/2330 train_time:16547ms step_avg:56.67ms
step:293/2330 train_time:16602ms step_avg:56.66ms
step:294/2330 train_time:16663ms step_avg:56.68ms
step:295/2330 train_time:16718ms step_avg:56.67ms
step:296/2330 train_time:16777ms step_avg:56.68ms
step:297/2330 train_time:16832ms step_avg:56.67ms
step:298/2330 train_time:16892ms step_avg:56.68ms
step:299/2330 train_time:16947ms step_avg:56.68ms
step:300/2330 train_time:17006ms step_avg:56.69ms
step:301/2330 train_time:17062ms step_avg:56.68ms
step:302/2330 train_time:17120ms step_avg:56.69ms
step:303/2330 train_time:17175ms step_avg:56.68ms
step:304/2330 train_time:17235ms step_avg:56.69ms
step:305/2330 train_time:17291ms step_avg:56.69ms
step:306/2330 train_time:17351ms step_avg:56.70ms
step:307/2330 train_time:17407ms step_avg:56.70ms
step:308/2330 train_time:17467ms step_avg:56.71ms
step:309/2330 train_time:17523ms step_avg:56.71ms
step:310/2330 train_time:17584ms step_avg:56.72ms
step:311/2330 train_time:17639ms step_avg:56.72ms
step:312/2330 train_time:17699ms step_avg:56.73ms
step:313/2330 train_time:17754ms step_avg:56.72ms
step:314/2330 train_time:17812ms step_avg:56.73ms
step:315/2330 train_time:17868ms step_avg:56.73ms
step:316/2330 train_time:17926ms step_avg:56.73ms
step:317/2330 train_time:17982ms step_avg:56.73ms
step:318/2330 train_time:18042ms step_avg:56.74ms
step:319/2330 train_time:18098ms step_avg:56.73ms
step:320/2330 train_time:18157ms step_avg:56.74ms
step:321/2330 train_time:18212ms step_avg:56.73ms
step:322/2330 train_time:18272ms step_avg:56.75ms
step:323/2330 train_time:18328ms step_avg:56.74ms
step:324/2330 train_time:18387ms step_avg:56.75ms
step:325/2330 train_time:18442ms step_avg:56.75ms
step:326/2330 train_time:18502ms step_avg:56.75ms
step:327/2330 train_time:18557ms step_avg:56.75ms
step:328/2330 train_time:18617ms step_avg:56.76ms
step:329/2330 train_time:18672ms step_avg:56.76ms
step:330/2330 train_time:18731ms step_avg:56.76ms
step:331/2330 train_time:18787ms step_avg:56.76ms
step:332/2330 train_time:18846ms step_avg:56.77ms
step:333/2330 train_time:18902ms step_avg:56.76ms
step:334/2330 train_time:18961ms step_avg:56.77ms
step:335/2330 train_time:19017ms step_avg:56.77ms
step:336/2330 train_time:19076ms step_avg:56.77ms
step:337/2330 train_time:19131ms step_avg:56.77ms
step:338/2330 train_time:19190ms step_avg:56.77ms
step:339/2330 train_time:19245ms step_avg:56.77ms
step:340/2330 train_time:19305ms step_avg:56.78ms
step:341/2330 train_time:19361ms step_avg:56.78ms
step:342/2330 train_time:19420ms step_avg:56.78ms
step:343/2330 train_time:19476ms step_avg:56.78ms
step:344/2330 train_time:19535ms step_avg:56.79ms
step:345/2330 train_time:19591ms step_avg:56.79ms
step:346/2330 train_time:19651ms step_avg:56.79ms
step:347/2330 train_time:19707ms step_avg:56.79ms
step:348/2330 train_time:19766ms step_avg:56.80ms
step:349/2330 train_time:19821ms step_avg:56.79ms
step:350/2330 train_time:19881ms step_avg:56.80ms
step:351/2330 train_time:19937ms step_avg:56.80ms
step:352/2330 train_time:19995ms step_avg:56.80ms
step:353/2330 train_time:20051ms step_avg:56.80ms
step:354/2330 train_time:20110ms step_avg:56.81ms
step:355/2330 train_time:20166ms step_avg:56.81ms
step:356/2330 train_time:20226ms step_avg:56.81ms
step:357/2330 train_time:20282ms step_avg:56.81ms
step:358/2330 train_time:20340ms step_avg:56.82ms
step:359/2330 train_time:20397ms step_avg:56.82ms
step:360/2330 train_time:20456ms step_avg:56.82ms
step:361/2330 train_time:20512ms step_avg:56.82ms
step:362/2330 train_time:20571ms step_avg:56.83ms
step:363/2330 train_time:20627ms step_avg:56.82ms
step:364/2330 train_time:20686ms step_avg:56.83ms
step:365/2330 train_time:20742ms step_avg:56.83ms
step:366/2330 train_time:20802ms step_avg:56.84ms
step:367/2330 train_time:20858ms step_avg:56.83ms
step:368/2330 train_time:20916ms step_avg:56.84ms
step:369/2330 train_time:20972ms step_avg:56.83ms
step:370/2330 train_time:21030ms step_avg:56.84ms
step:371/2330 train_time:21087ms step_avg:56.84ms
step:372/2330 train_time:21146ms step_avg:56.84ms
step:373/2330 train_time:21202ms step_avg:56.84ms
step:374/2330 train_time:21261ms step_avg:56.85ms
step:375/2330 train_time:21317ms step_avg:56.85ms
step:376/2330 train_time:21376ms step_avg:56.85ms
step:377/2330 train_time:21432ms step_avg:56.85ms
step:378/2330 train_time:21491ms step_avg:56.85ms
step:379/2330 train_time:21546ms step_avg:56.85ms
step:380/2330 train_time:21606ms step_avg:56.86ms
step:381/2330 train_time:21662ms step_avg:56.86ms
step:382/2330 train_time:21721ms step_avg:56.86ms
step:383/2330 train_time:21777ms step_avg:56.86ms
step:384/2330 train_time:21836ms step_avg:56.87ms
step:385/2330 train_time:21892ms step_avg:56.86ms
step:386/2330 train_time:21950ms step_avg:56.87ms
step:387/2330 train_time:22006ms step_avg:56.86ms
step:388/2330 train_time:22066ms step_avg:56.87ms
step:389/2330 train_time:22122ms step_avg:56.87ms
step:390/2330 train_time:22181ms step_avg:56.87ms
step:391/2330 train_time:22237ms step_avg:56.87ms
step:392/2330 train_time:22295ms step_avg:56.88ms
step:393/2330 train_time:22351ms step_avg:56.87ms
step:394/2330 train_time:22410ms step_avg:56.88ms
step:395/2330 train_time:22466ms step_avg:56.88ms
step:396/2330 train_time:22526ms step_avg:56.88ms
step:397/2330 train_time:22581ms step_avg:56.88ms
step:398/2330 train_time:22641ms step_avg:56.89ms
step:399/2330 train_time:22696ms step_avg:56.88ms
step:400/2330 train_time:22756ms step_avg:56.89ms
step:401/2330 train_time:22812ms step_avg:56.89ms
step:402/2330 train_time:22871ms step_avg:56.89ms
step:403/2330 train_time:22927ms step_avg:56.89ms
step:404/2330 train_time:22986ms step_avg:56.90ms
step:405/2330 train_time:23042ms step_avg:56.89ms
step:406/2330 train_time:23102ms step_avg:56.90ms
step:407/2330 train_time:23157ms step_avg:56.90ms
step:408/2330 train_time:23217ms step_avg:56.90ms
step:409/2330 train_time:23272ms step_avg:56.90ms
step:410/2330 train_time:23332ms step_avg:56.91ms
step:411/2330 train_time:23388ms step_avg:56.90ms
step:412/2330 train_time:23446ms step_avg:56.91ms
step:413/2330 train_time:23503ms step_avg:56.91ms
step:414/2330 train_time:23561ms step_avg:56.91ms
step:415/2330 train_time:23616ms step_avg:56.91ms
step:416/2330 train_time:23675ms step_avg:56.91ms
step:417/2330 train_time:23731ms step_avg:56.91ms
step:418/2330 train_time:23791ms step_avg:56.92ms
step:419/2330 train_time:23846ms step_avg:56.91ms
step:420/2330 train_time:23906ms step_avg:56.92ms
step:421/2330 train_time:23961ms step_avg:56.92ms
step:422/2330 train_time:24021ms step_avg:56.92ms
step:423/2330 train_time:24077ms step_avg:56.92ms
step:424/2330 train_time:24135ms step_avg:56.92ms
step:425/2330 train_time:24191ms step_avg:56.92ms
step:426/2330 train_time:24250ms step_avg:56.93ms
step:427/2330 train_time:24307ms step_avg:56.92ms
step:428/2330 train_time:24365ms step_avg:56.93ms
step:429/2330 train_time:24421ms step_avg:56.93ms
step:430/2330 train_time:24481ms step_avg:56.93ms
step:431/2330 train_time:24536ms step_avg:56.93ms
step:432/2330 train_time:24595ms step_avg:56.93ms
step:433/2330 train_time:24650ms step_avg:56.93ms
step:434/2330 train_time:24711ms step_avg:56.94ms
step:435/2330 train_time:24767ms step_avg:56.94ms
step:436/2330 train_time:24826ms step_avg:56.94ms
step:437/2330 train_time:24881ms step_avg:56.94ms
step:438/2330 train_time:24940ms step_avg:56.94ms
step:439/2330 train_time:24995ms step_avg:56.94ms
step:440/2330 train_time:25055ms step_avg:56.94ms
step:441/2330 train_time:25111ms step_avg:56.94ms
step:442/2330 train_time:25170ms step_avg:56.95ms
step:443/2330 train_time:25226ms step_avg:56.94ms
step:444/2330 train_time:25285ms step_avg:56.95ms
step:445/2330 train_time:25341ms step_avg:56.95ms
step:446/2330 train_time:25400ms step_avg:56.95ms
step:447/2330 train_time:25456ms step_avg:56.95ms
step:448/2330 train_time:25515ms step_avg:56.95ms
step:449/2330 train_time:25570ms step_avg:56.95ms
step:450/2330 train_time:25629ms step_avg:56.95ms
step:451/2330 train_time:25685ms step_avg:56.95ms
step:452/2330 train_time:25744ms step_avg:56.96ms
step:453/2330 train_time:25800ms step_avg:56.95ms
step:454/2330 train_time:25860ms step_avg:56.96ms
step:455/2330 train_time:25916ms step_avg:56.96ms
step:456/2330 train_time:25974ms step_avg:56.96ms
step:457/2330 train_time:26030ms step_avg:56.96ms
step:458/2330 train_time:26088ms step_avg:56.96ms
step:459/2330 train_time:26144ms step_avg:56.96ms
step:460/2330 train_time:26204ms step_avg:56.96ms
step:461/2330 train_time:26260ms step_avg:56.96ms
step:462/2330 train_time:26319ms step_avg:56.97ms
step:463/2330 train_time:26374ms step_avg:56.96ms
step:464/2330 train_time:26433ms step_avg:56.97ms
step:465/2330 train_time:26489ms step_avg:56.97ms
step:466/2330 train_time:26549ms step_avg:56.97ms
step:467/2330 train_time:26604ms step_avg:56.97ms
step:468/2330 train_time:26664ms step_avg:56.97ms
step:469/2330 train_time:26719ms step_avg:56.97ms
step:470/2330 train_time:26779ms step_avg:56.98ms
step:471/2330 train_time:26834ms step_avg:56.97ms
step:472/2330 train_time:26893ms step_avg:56.98ms
step:473/2330 train_time:26949ms step_avg:56.97ms
step:474/2330 train_time:27008ms step_avg:56.98ms
step:475/2330 train_time:27063ms step_avg:56.98ms
step:476/2330 train_time:27123ms step_avg:56.98ms
step:477/2330 train_time:27178ms step_avg:56.98ms
step:478/2330 train_time:27237ms step_avg:56.98ms
step:479/2330 train_time:27292ms step_avg:56.98ms
step:480/2330 train_time:27352ms step_avg:56.98ms
step:481/2330 train_time:27407ms step_avg:56.98ms
step:482/2330 train_time:27467ms step_avg:56.99ms
step:483/2330 train_time:27523ms step_avg:56.98ms
step:484/2330 train_time:27582ms step_avg:56.99ms
step:485/2330 train_time:27638ms step_avg:56.99ms
step:486/2330 train_time:27697ms step_avg:56.99ms
step:487/2330 train_time:27752ms step_avg:56.99ms
step:488/2330 train_time:27811ms step_avg:56.99ms
step:489/2330 train_time:27867ms step_avg:56.99ms
step:490/2330 train_time:27927ms step_avg:56.99ms
step:491/2330 train_time:27982ms step_avg:56.99ms
step:492/2330 train_time:28041ms step_avg:56.99ms
step:493/2330 train_time:28097ms step_avg:56.99ms
step:494/2330 train_time:28156ms step_avg:57.00ms
step:495/2330 train_time:28212ms step_avg:56.99ms
step:496/2330 train_time:28271ms step_avg:57.00ms
step:497/2330 train_time:28327ms step_avg:57.00ms
step:498/2330 train_time:28386ms step_avg:57.00ms
step:499/2330 train_time:28442ms step_avg:57.00ms
step:500/2330 train_time:28501ms step_avg:57.00ms
step:500/2330 val_loss:5.7944 train_time:28579ms step_avg:57.16ms
step:501/2330 train_time:28599ms step_avg:57.08ms
step:502/2330 train_time:28620ms step_avg:57.01ms
step:503/2330 train_time:28672ms step_avg:57.00ms
step:504/2330 train_time:28738ms step_avg:57.02ms
step:505/2330 train_time:28794ms step_avg:57.02ms
step:506/2330 train_time:28854ms step_avg:57.02ms
step:507/2330 train_time:28911ms step_avg:57.02ms
step:508/2330 train_time:28970ms step_avg:57.03ms
step:509/2330 train_time:29025ms step_avg:57.02ms
step:510/2330 train_time:29083ms step_avg:57.02ms
step:511/2330 train_time:29138ms step_avg:57.02ms
step:512/2330 train_time:29196ms step_avg:57.02ms
step:513/2330 train_time:29252ms step_avg:57.02ms
step:514/2330 train_time:29310ms step_avg:57.02ms
step:515/2330 train_time:29365ms step_avg:57.02ms
step:516/2330 train_time:29423ms step_avg:57.02ms
step:517/2330 train_time:29478ms step_avg:57.02ms
step:518/2330 train_time:29537ms step_avg:57.02ms
step:519/2330 train_time:29593ms step_avg:57.02ms
step:520/2330 train_time:29655ms step_avg:57.03ms
step:521/2330 train_time:29711ms step_avg:57.03ms
step:522/2330 train_time:29772ms step_avg:57.03ms
step:523/2330 train_time:29828ms step_avg:57.03ms
step:524/2330 train_time:29888ms step_avg:57.04ms
step:525/2330 train_time:29943ms step_avg:57.03ms
step:526/2330 train_time:30002ms step_avg:57.04ms
step:527/2330 train_time:30057ms step_avg:57.03ms
step:528/2330 train_time:30116ms step_avg:57.04ms
step:529/2330 train_time:30172ms step_avg:57.04ms
step:530/2330 train_time:30230ms step_avg:57.04ms
step:531/2330 train_time:30285ms step_avg:57.03ms
step:532/2330 train_time:30344ms step_avg:57.04ms
step:533/2330 train_time:30400ms step_avg:57.04ms
step:534/2330 train_time:30459ms step_avg:57.04ms
step:535/2330 train_time:30514ms step_avg:57.04ms
step:536/2330 train_time:30573ms step_avg:57.04ms
step:537/2330 train_time:30629ms step_avg:57.04ms
step:538/2330 train_time:30688ms step_avg:57.04ms
step:539/2330 train_time:30744ms step_avg:57.04ms
step:540/2330 train_time:30804ms step_avg:57.04ms
step:541/2330 train_time:30860ms step_avg:57.04ms
step:542/2330 train_time:30919ms step_avg:57.05ms
step:543/2330 train_time:30976ms step_avg:57.05ms
step:544/2330 train_time:31035ms step_avg:57.05ms
step:545/2330 train_time:31091ms step_avg:57.05ms
step:546/2330 train_time:31149ms step_avg:57.05ms
step:547/2330 train_time:31205ms step_avg:57.05ms
step:548/2330 train_time:31263ms step_avg:57.05ms
step:549/2330 train_time:31318ms step_avg:57.05ms
step:550/2330 train_time:31377ms step_avg:57.05ms
step:551/2330 train_time:31433ms step_avg:57.05ms
step:552/2330 train_time:31491ms step_avg:57.05ms
step:553/2330 train_time:31547ms step_avg:57.05ms
step:554/2330 train_time:31607ms step_avg:57.05ms
step:555/2330 train_time:31662ms step_avg:57.05ms
step:556/2330 train_time:31722ms step_avg:57.05ms
step:557/2330 train_time:31778ms step_avg:57.05ms
step:558/2330 train_time:31838ms step_avg:57.06ms
step:559/2330 train_time:31894ms step_avg:57.06ms
step:560/2330 train_time:31952ms step_avg:57.06ms
step:561/2330 train_time:32008ms step_avg:57.06ms
step:562/2330 train_time:32067ms step_avg:57.06ms
step:563/2330 train_time:32123ms step_avg:57.06ms
step:564/2330 train_time:32182ms step_avg:57.06ms
step:565/2330 train_time:32238ms step_avg:57.06ms
step:566/2330 train_time:32297ms step_avg:57.06ms
step:567/2330 train_time:32353ms step_avg:57.06ms
step:568/2330 train_time:32412ms step_avg:57.06ms
step:569/2330 train_time:32468ms step_avg:57.06ms
step:570/2330 train_time:32526ms step_avg:57.06ms
step:571/2330 train_time:32582ms step_avg:57.06ms
step:572/2330 train_time:32642ms step_avg:57.07ms
step:573/2330 train_time:32698ms step_avg:57.06ms
step:574/2330 train_time:32757ms step_avg:57.07ms
step:575/2330 train_time:32813ms step_avg:57.07ms
step:576/2330 train_time:32873ms step_avg:57.07ms
step:577/2330 train_time:32928ms step_avg:57.07ms
step:578/2330 train_time:32988ms step_avg:57.07ms
step:579/2330 train_time:33044ms step_avg:57.07ms
step:580/2330 train_time:33102ms step_avg:57.07ms
step:581/2330 train_time:33157ms step_avg:57.07ms
step:582/2330 train_time:33217ms step_avg:57.07ms
step:583/2330 train_time:33273ms step_avg:57.07ms
step:584/2330 train_time:33332ms step_avg:57.08ms
step:585/2330 train_time:33388ms step_avg:57.07ms
step:586/2330 train_time:33447ms step_avg:57.08ms
step:587/2330 train_time:33503ms step_avg:57.07ms
step:588/2330 train_time:33562ms step_avg:57.08ms
step:589/2330 train_time:33618ms step_avg:57.08ms
step:590/2330 train_time:33677ms step_avg:57.08ms
step:591/2330 train_time:33733ms step_avg:57.08ms
step:592/2330 train_time:33793ms step_avg:57.08ms
step:593/2330 train_time:33849ms step_avg:57.08ms
step:594/2330 train_time:33909ms step_avg:57.09ms
step:595/2330 train_time:33965ms step_avg:57.08ms
step:596/2330 train_time:34023ms step_avg:57.09ms
step:597/2330 train_time:34078ms step_avg:57.08ms
step:598/2330 train_time:34137ms step_avg:57.09ms
step:599/2330 train_time:34194ms step_avg:57.08ms
step:600/2330 train_time:34253ms step_avg:57.09ms
step:601/2330 train_time:34309ms step_avg:57.09ms
step:602/2330 train_time:34368ms step_avg:57.09ms
step:603/2330 train_time:34423ms step_avg:57.09ms
step:604/2330 train_time:34482ms step_avg:57.09ms
step:605/2330 train_time:34538ms step_avg:57.09ms
step:606/2330 train_time:34597ms step_avg:57.09ms
step:607/2330 train_time:34653ms step_avg:57.09ms
step:608/2330 train_time:34712ms step_avg:57.09ms
step:609/2330 train_time:34769ms step_avg:57.09ms
step:610/2330 train_time:34828ms step_avg:57.09ms
step:611/2330 train_time:34883ms step_avg:57.09ms
step:612/2330 train_time:34943ms step_avg:57.10ms
step:613/2330 train_time:34998ms step_avg:57.09ms
step:614/2330 train_time:35058ms step_avg:57.10ms
step:615/2330 train_time:35115ms step_avg:57.10ms
step:616/2330 train_time:35173ms step_avg:57.10ms
step:617/2330 train_time:35229ms step_avg:57.10ms
step:618/2330 train_time:35288ms step_avg:57.10ms
step:619/2330 train_time:35344ms step_avg:57.10ms
step:620/2330 train_time:35403ms step_avg:57.10ms
step:621/2330 train_time:35458ms step_avg:57.10ms
step:622/2330 train_time:35517ms step_avg:57.10ms
step:623/2330 train_time:35573ms step_avg:57.10ms
step:624/2330 train_time:35632ms step_avg:57.10ms
step:625/2330 train_time:35689ms step_avg:57.10ms
step:626/2330 train_time:35748ms step_avg:57.11ms
step:627/2330 train_time:35805ms step_avg:57.10ms
step:628/2330 train_time:35863ms step_avg:57.11ms
step:629/2330 train_time:35919ms step_avg:57.11ms
step:630/2330 train_time:35978ms step_avg:57.11ms
step:631/2330 train_time:36035ms step_avg:57.11ms
step:632/2330 train_time:36093ms step_avg:57.11ms
step:633/2330 train_time:36149ms step_avg:57.11ms
step:634/2330 train_time:36209ms step_avg:57.11ms
step:635/2330 train_time:36264ms step_avg:57.11ms
step:636/2330 train_time:36324ms step_avg:57.11ms
step:637/2330 train_time:36379ms step_avg:57.11ms
step:638/2330 train_time:36439ms step_avg:57.11ms
step:639/2330 train_time:36495ms step_avg:57.11ms
step:640/2330 train_time:36555ms step_avg:57.12ms
step:641/2330 train_time:36610ms step_avg:57.11ms
step:642/2330 train_time:36670ms step_avg:57.12ms
step:643/2330 train_time:36726ms step_avg:57.12ms
step:644/2330 train_time:36785ms step_avg:57.12ms
step:645/2330 train_time:36841ms step_avg:57.12ms
step:646/2330 train_time:36899ms step_avg:57.12ms
step:647/2330 train_time:36955ms step_avg:57.12ms
step:648/2330 train_time:37014ms step_avg:57.12ms
step:649/2330 train_time:37070ms step_avg:57.12ms
step:650/2330 train_time:37130ms step_avg:57.12ms
step:651/2330 train_time:37186ms step_avg:57.12ms
step:652/2330 train_time:37246ms step_avg:57.13ms
step:653/2330 train_time:37301ms step_avg:57.12ms
step:654/2330 train_time:37362ms step_avg:57.13ms
step:655/2330 train_time:37417ms step_avg:57.13ms
step:656/2330 train_time:37476ms step_avg:57.13ms
step:657/2330 train_time:37532ms step_avg:57.13ms
step:658/2330 train_time:37591ms step_avg:57.13ms
step:659/2330 train_time:37647ms step_avg:57.13ms
step:660/2330 train_time:37706ms step_avg:57.13ms
step:661/2330 train_time:37762ms step_avg:57.13ms
step:662/2330 train_time:37821ms step_avg:57.13ms
step:663/2330 train_time:37877ms step_avg:57.13ms
step:664/2330 train_time:37936ms step_avg:57.13ms
step:665/2330 train_time:37993ms step_avg:57.13ms
step:666/2330 train_time:38051ms step_avg:57.13ms
step:667/2330 train_time:38107ms step_avg:57.13ms
step:668/2330 train_time:38167ms step_avg:57.14ms
step:669/2330 train_time:38222ms step_avg:57.13ms
step:670/2330 train_time:38282ms step_avg:57.14ms
step:671/2330 train_time:38338ms step_avg:57.14ms
step:672/2330 train_time:38397ms step_avg:57.14ms
step:673/2330 train_time:38453ms step_avg:57.14ms
step:674/2330 train_time:38513ms step_avg:57.14ms
step:675/2330 train_time:38569ms step_avg:57.14ms
step:676/2330 train_time:38627ms step_avg:57.14ms
step:677/2330 train_time:38683ms step_avg:57.14ms
step:678/2330 train_time:38742ms step_avg:57.14ms
step:679/2330 train_time:38798ms step_avg:57.14ms
step:680/2330 train_time:38857ms step_avg:57.14ms
step:681/2330 train_time:38913ms step_avg:57.14ms
step:682/2330 train_time:38973ms step_avg:57.14ms
step:683/2330 train_time:39029ms step_avg:57.14ms
step:684/2330 train_time:39088ms step_avg:57.15ms
step:685/2330 train_time:39144ms step_avg:57.14ms
step:686/2330 train_time:39204ms step_avg:57.15ms
step:687/2330 train_time:39259ms step_avg:57.15ms
step:688/2330 train_time:39318ms step_avg:57.15ms
step:689/2330 train_time:39374ms step_avg:57.15ms
step:690/2330 train_time:39433ms step_avg:57.15ms
step:691/2330 train_time:39489ms step_avg:57.15ms
step:692/2330 train_time:39549ms step_avg:57.15ms
step:693/2330 train_time:39604ms step_avg:57.15ms
step:694/2330 train_time:39664ms step_avg:57.15ms
step:695/2330 train_time:39719ms step_avg:57.15ms
step:696/2330 train_time:39778ms step_avg:57.15ms
step:697/2330 train_time:39834ms step_avg:57.15ms
step:698/2330 train_time:39893ms step_avg:57.15ms
step:699/2330 train_time:39950ms step_avg:57.15ms
step:700/2330 train_time:40009ms step_avg:57.16ms
step:701/2330 train_time:40065ms step_avg:57.15ms
step:702/2330 train_time:40124ms step_avg:57.16ms
step:703/2330 train_time:40180ms step_avg:57.15ms
step:704/2330 train_time:40240ms step_avg:57.16ms
step:705/2330 train_time:40296ms step_avg:57.16ms
step:706/2330 train_time:40355ms step_avg:57.16ms
step:707/2330 train_time:40412ms step_avg:57.16ms
step:708/2330 train_time:40471ms step_avg:57.16ms
step:709/2330 train_time:40527ms step_avg:57.16ms
step:710/2330 train_time:40587ms step_avg:57.16ms
step:711/2330 train_time:40643ms step_avg:57.16ms
step:712/2330 train_time:40701ms step_avg:57.16ms
step:713/2330 train_time:40757ms step_avg:57.16ms
step:714/2330 train_time:40817ms step_avg:57.17ms
step:715/2330 train_time:40873ms step_avg:57.17ms
step:716/2330 train_time:40933ms step_avg:57.17ms
step:717/2330 train_time:40989ms step_avg:57.17ms
step:718/2330 train_time:41049ms step_avg:57.17ms
step:719/2330 train_time:41105ms step_avg:57.17ms
step:720/2330 train_time:41164ms step_avg:57.17ms
step:721/2330 train_time:41220ms step_avg:57.17ms
step:722/2330 train_time:41279ms step_avg:57.17ms
step:723/2330 train_time:41335ms step_avg:57.17ms
step:724/2330 train_time:41394ms step_avg:57.17ms
step:725/2330 train_time:41450ms step_avg:57.17ms
step:726/2330 train_time:41511ms step_avg:57.18ms
step:727/2330 train_time:41567ms step_avg:57.18ms
step:728/2330 train_time:41625ms step_avg:57.18ms
step:729/2330 train_time:41681ms step_avg:57.18ms
step:730/2330 train_time:41741ms step_avg:57.18ms
step:731/2330 train_time:41797ms step_avg:57.18ms
step:732/2330 train_time:41856ms step_avg:57.18ms
step:733/2330 train_time:41912ms step_avg:57.18ms
step:734/2330 train_time:41972ms step_avg:57.18ms
step:735/2330 train_time:42028ms step_avg:57.18ms
step:736/2330 train_time:42088ms step_avg:57.18ms
step:737/2330 train_time:42143ms step_avg:57.18ms
step:738/2330 train_time:42202ms step_avg:57.18ms
step:739/2330 train_time:42258ms step_avg:57.18ms
step:740/2330 train_time:42317ms step_avg:57.19ms
step:741/2330 train_time:42373ms step_avg:57.18ms
step:742/2330 train_time:42433ms step_avg:57.19ms
step:743/2330 train_time:42489ms step_avg:57.19ms
step:744/2330 train_time:42549ms step_avg:57.19ms
step:745/2330 train_time:42605ms step_avg:57.19ms
step:746/2330 train_time:42664ms step_avg:57.19ms
step:747/2330 train_time:42719ms step_avg:57.19ms
step:748/2330 train_time:42780ms step_avg:57.19ms
step:749/2330 train_time:42836ms step_avg:57.19ms
step:750/2330 train_time:42895ms step_avg:57.19ms
step:750/2330 val_loss:5.5614 train_time:42975ms step_avg:57.30ms
step:751/2330 train_time:42995ms step_avg:57.25ms
step:752/2330 train_time:43014ms step_avg:57.20ms
step:753/2330 train_time:43071ms step_avg:57.20ms
step:754/2330 train_time:43133ms step_avg:57.21ms
step:755/2330 train_time:43190ms step_avg:57.21ms
step:756/2330 train_time:43251ms step_avg:57.21ms
step:757/2330 train_time:43308ms step_avg:57.21ms
step:758/2330 train_time:43367ms step_avg:57.21ms
step:759/2330 train_time:43423ms step_avg:57.21ms
step:760/2330 train_time:43481ms step_avg:57.21ms
step:761/2330 train_time:43536ms step_avg:57.21ms
step:762/2330 train_time:43595ms step_avg:57.21ms
step:763/2330 train_time:43650ms step_avg:57.21ms
step:764/2330 train_time:43708ms step_avg:57.21ms
step:765/2330 train_time:43764ms step_avg:57.21ms
step:766/2330 train_time:43822ms step_avg:57.21ms
step:767/2330 train_time:43878ms step_avg:57.21ms
step:768/2330 train_time:43937ms step_avg:57.21ms
step:769/2330 train_time:43994ms step_avg:57.21ms
step:770/2330 train_time:44056ms step_avg:57.22ms
step:771/2330 train_time:44114ms step_avg:57.22ms
step:772/2330 train_time:44175ms step_avg:57.22ms
step:773/2330 train_time:44233ms step_avg:57.22ms
step:774/2330 train_time:44293ms step_avg:57.23ms
step:775/2330 train_time:44350ms step_avg:57.23ms
step:776/2330 train_time:44410ms step_avg:57.23ms
step:777/2330 train_time:44466ms step_avg:57.23ms
step:778/2330 train_time:44526ms step_avg:57.23ms
step:779/2330 train_time:44582ms step_avg:57.23ms
step:780/2330 train_time:44642ms step_avg:57.23ms
step:781/2330 train_time:44698ms step_avg:57.23ms
step:782/2330 train_time:44757ms step_avg:57.23ms
step:783/2330 train_time:44813ms step_avg:57.23ms
step:784/2330 train_time:44873ms step_avg:57.24ms
step:785/2330 train_time:44929ms step_avg:57.23ms
step:786/2330 train_time:44989ms step_avg:57.24ms
step:787/2330 train_time:45046ms step_avg:57.24ms
step:788/2330 train_time:45106ms step_avg:57.24ms
step:789/2330 train_time:45163ms step_avg:57.24ms
step:790/2330 train_time:45224ms step_avg:57.25ms
step:791/2330 train_time:45281ms step_avg:57.25ms
step:792/2330 train_time:45342ms step_avg:57.25ms
step:793/2330 train_time:45399ms step_avg:57.25ms
step:794/2330 train_time:45458ms step_avg:57.25ms
step:795/2330 train_time:45515ms step_avg:57.25ms
step:796/2330 train_time:45574ms step_avg:57.25ms
step:797/2330 train_time:45630ms step_avg:57.25ms
step:798/2330 train_time:45690ms step_avg:57.26ms
step:799/2330 train_time:45746ms step_avg:57.25ms
step:800/2330 train_time:45806ms step_avg:57.26ms
step:801/2330 train_time:45862ms step_avg:57.26ms
step:802/2330 train_time:45922ms step_avg:57.26ms
step:803/2330 train_time:45978ms step_avg:57.26ms
step:804/2330 train_time:46039ms step_avg:57.26ms
step:805/2330 train_time:46096ms step_avg:57.26ms
step:806/2330 train_time:46157ms step_avg:57.27ms
step:807/2330 train_time:46214ms step_avg:57.27ms
step:808/2330 train_time:46275ms step_avg:57.27ms
step:809/2330 train_time:46332ms step_avg:57.27ms
step:810/2330 train_time:46393ms step_avg:57.28ms
step:811/2330 train_time:46450ms step_avg:57.28ms
step:812/2330 train_time:46510ms step_avg:57.28ms
step:813/2330 train_time:46566ms step_avg:57.28ms
step:814/2330 train_time:46626ms step_avg:57.28ms
step:815/2330 train_time:46682ms step_avg:57.28ms
step:816/2330 train_time:46741ms step_avg:57.28ms
step:817/2330 train_time:46798ms step_avg:57.28ms
step:818/2330 train_time:46858ms step_avg:57.28ms
step:819/2330 train_time:46914ms step_avg:57.28ms
step:820/2330 train_time:46975ms step_avg:57.29ms
step:821/2330 train_time:47031ms step_avg:57.29ms
step:822/2330 train_time:47091ms step_avg:57.29ms
step:823/2330 train_time:47148ms step_avg:57.29ms
step:824/2330 train_time:47208ms step_avg:57.29ms
step:825/2330 train_time:47264ms step_avg:57.29ms
step:826/2330 train_time:47325ms step_avg:57.29ms
step:827/2330 train_time:47381ms step_avg:57.29ms
step:828/2330 train_time:47442ms step_avg:57.30ms
step:829/2330 train_time:47500ms step_avg:57.30ms
step:830/2330 train_time:47560ms step_avg:57.30ms
step:831/2330 train_time:47617ms step_avg:57.30ms
step:832/2330 train_time:47676ms step_avg:57.30ms
step:833/2330 train_time:47733ms step_avg:57.30ms
step:834/2330 train_time:47792ms step_avg:57.30ms
step:835/2330 train_time:47849ms step_avg:57.30ms
step:836/2330 train_time:47908ms step_avg:57.31ms
step:837/2330 train_time:47965ms step_avg:57.31ms
step:838/2330 train_time:48024ms step_avg:57.31ms
step:839/2330 train_time:48081ms step_avg:57.31ms
step:840/2330 train_time:48142ms step_avg:57.31ms
step:841/2330 train_time:48199ms step_avg:57.31ms
step:842/2330 train_time:48259ms step_avg:57.31ms
step:843/2330 train_time:48316ms step_avg:57.31ms
step:844/2330 train_time:48376ms step_avg:57.32ms
step:845/2330 train_time:48433ms step_avg:57.32ms
step:846/2330 train_time:48493ms step_avg:57.32ms
step:847/2330 train_time:48550ms step_avg:57.32ms
step:848/2330 train_time:48609ms step_avg:57.32ms
step:849/2330 train_time:48666ms step_avg:57.32ms
step:850/2330 train_time:48725ms step_avg:57.32ms
step:851/2330 train_time:48781ms step_avg:57.32ms
step:852/2330 train_time:48841ms step_avg:57.33ms
step:853/2330 train_time:48898ms step_avg:57.32ms
step:854/2330 train_time:48958ms step_avg:57.33ms
step:855/2330 train_time:49014ms step_avg:57.33ms
step:856/2330 train_time:49075ms step_avg:57.33ms
step:857/2330 train_time:49131ms step_avg:57.33ms
step:858/2330 train_time:49191ms step_avg:57.33ms
step:859/2330 train_time:49248ms step_avg:57.33ms
step:860/2330 train_time:49308ms step_avg:57.33ms
step:861/2330 train_time:49365ms step_avg:57.33ms
step:862/2330 train_time:49425ms step_avg:57.34ms
step:863/2330 train_time:49481ms step_avg:57.34ms
step:864/2330 train_time:49542ms step_avg:57.34ms
step:865/2330 train_time:49599ms step_avg:57.34ms
step:866/2330 train_time:49658ms step_avg:57.34ms
step:867/2330 train_time:49715ms step_avg:57.34ms
step:868/2330 train_time:49775ms step_avg:57.34ms
step:869/2330 train_time:49831ms step_avg:57.34ms
step:870/2330 train_time:49891ms step_avg:57.35ms
step:871/2330 train_time:49948ms step_avg:57.35ms
step:872/2330 train_time:50007ms step_avg:57.35ms
step:873/2330 train_time:50064ms step_avg:57.35ms
step:874/2330 train_time:50124ms step_avg:57.35ms
step:875/2330 train_time:50180ms step_avg:57.35ms
step:876/2330 train_time:50241ms step_avg:57.35ms
step:877/2330 train_time:50298ms step_avg:57.35ms
step:878/2330 train_time:50359ms step_avg:57.36ms
step:879/2330 train_time:50415ms step_avg:57.36ms
step:880/2330 train_time:50476ms step_avg:57.36ms
step:881/2330 train_time:50533ms step_avg:57.36ms
step:882/2330 train_time:50593ms step_avg:57.36ms
step:883/2330 train_time:50649ms step_avg:57.36ms
step:884/2330 train_time:50709ms step_avg:57.36ms
step:885/2330 train_time:50765ms step_avg:57.36ms
step:886/2330 train_time:50825ms step_avg:57.36ms
step:887/2330 train_time:50881ms step_avg:57.36ms
step:888/2330 train_time:50941ms step_avg:57.37ms
step:889/2330 train_time:50998ms step_avg:57.37ms
step:890/2330 train_time:51057ms step_avg:57.37ms
step:891/2330 train_time:51114ms step_avg:57.37ms
step:892/2330 train_time:51174ms step_avg:57.37ms
step:893/2330 train_time:51231ms step_avg:57.37ms
step:894/2330 train_time:51291ms step_avg:57.37ms
step:895/2330 train_time:51348ms step_avg:57.37ms
step:896/2330 train_time:51407ms step_avg:57.37ms
step:897/2330 train_time:51464ms step_avg:57.37ms
step:898/2330 train_time:51524ms step_avg:57.38ms
step:899/2330 train_time:51581ms step_avg:57.38ms
step:900/2330 train_time:51641ms step_avg:57.38ms
step:901/2330 train_time:51698ms step_avg:57.38ms
step:902/2330 train_time:51759ms step_avg:57.38ms
step:903/2330 train_time:51815ms step_avg:57.38ms
step:904/2330 train_time:51876ms step_avg:57.38ms
step:905/2330 train_time:51932ms step_avg:57.38ms
step:906/2330 train_time:51993ms step_avg:57.39ms
step:907/2330 train_time:52050ms step_avg:57.39ms
step:908/2330 train_time:52110ms step_avg:57.39ms
step:909/2330 train_time:52167ms step_avg:57.39ms
step:910/2330 train_time:52226ms step_avg:57.39ms
step:911/2330 train_time:52282ms step_avg:57.39ms
step:912/2330 train_time:52342ms step_avg:57.39ms
step:913/2330 train_time:52399ms step_avg:57.39ms
step:914/2330 train_time:52458ms step_avg:57.39ms
step:915/2330 train_time:52515ms step_avg:57.39ms
step:916/2330 train_time:52576ms step_avg:57.40ms
step:917/2330 train_time:52632ms step_avg:57.40ms
step:918/2330 train_time:52693ms step_avg:57.40ms
step:919/2330 train_time:52749ms step_avg:57.40ms
step:920/2330 train_time:52809ms step_avg:57.40ms
step:921/2330 train_time:52866ms step_avg:57.40ms
step:922/2330 train_time:52925ms step_avg:57.40ms
step:923/2330 train_time:52981ms step_avg:57.40ms
step:924/2330 train_time:53042ms step_avg:57.40ms
step:925/2330 train_time:53099ms step_avg:57.40ms
step:926/2330 train_time:53159ms step_avg:57.41ms
step:927/2330 train_time:53216ms step_avg:57.41ms
step:928/2330 train_time:53276ms step_avg:57.41ms
step:929/2330 train_time:53333ms step_avg:57.41ms
step:930/2330 train_time:53392ms step_avg:57.41ms
step:931/2330 train_time:53448ms step_avg:57.41ms
step:932/2330 train_time:53509ms step_avg:57.41ms
step:933/2330 train_time:53566ms step_avg:57.41ms
step:934/2330 train_time:53625ms step_avg:57.41ms
step:935/2330 train_time:53682ms step_avg:57.41ms
step:936/2330 train_time:53742ms step_avg:57.42ms
step:937/2330 train_time:53799ms step_avg:57.42ms
step:938/2330 train_time:53859ms step_avg:57.42ms
step:939/2330 train_time:53916ms step_avg:57.42ms
step:940/2330 train_time:53976ms step_avg:57.42ms
step:941/2330 train_time:54033ms step_avg:57.42ms
step:942/2330 train_time:54093ms step_avg:57.42ms
step:943/2330 train_time:54150ms step_avg:57.42ms
step:944/2330 train_time:54210ms step_avg:57.43ms
step:945/2330 train_time:54266ms step_avg:57.42ms
step:946/2330 train_time:54325ms step_avg:57.43ms
step:947/2330 train_time:54383ms step_avg:57.43ms
step:948/2330 train_time:54442ms step_avg:57.43ms
step:949/2330 train_time:54499ms step_avg:57.43ms
step:950/2330 train_time:54559ms step_avg:57.43ms
step:951/2330 train_time:54616ms step_avg:57.43ms
step:952/2330 train_time:54675ms step_avg:57.43ms
step:953/2330 train_time:54732ms step_avg:57.43ms
step:954/2330 train_time:54792ms step_avg:57.43ms
step:955/2330 train_time:54848ms step_avg:57.43ms
step:956/2330 train_time:54908ms step_avg:57.44ms
step:957/2330 train_time:54965ms step_avg:57.43ms
step:958/2330 train_time:55025ms step_avg:57.44ms
step:959/2330 train_time:55082ms step_avg:57.44ms
step:960/2330 train_time:55142ms step_avg:57.44ms
step:961/2330 train_time:55198ms step_avg:57.44ms
step:962/2330 train_time:55258ms step_avg:57.44ms
step:963/2330 train_time:55315ms step_avg:57.44ms
step:964/2330 train_time:55376ms step_avg:57.44ms
step:965/2330 train_time:55432ms step_avg:57.44ms
step:966/2330 train_time:55494ms step_avg:57.45ms
step:967/2330 train_time:55551ms step_avg:57.45ms
step:968/2330 train_time:55611ms step_avg:57.45ms
step:969/2330 train_time:55667ms step_avg:57.45ms
step:970/2330 train_time:55727ms step_avg:57.45ms
step:971/2330 train_time:55783ms step_avg:57.45ms
step:972/2330 train_time:55844ms step_avg:57.45ms
step:973/2330 train_time:55900ms step_avg:57.45ms
step:974/2330 train_time:55960ms step_avg:57.45ms
step:975/2330 train_time:56017ms step_avg:57.45ms
step:976/2330 train_time:56077ms step_avg:57.46ms
step:977/2330 train_time:56133ms step_avg:57.45ms
step:978/2330 train_time:56193ms step_avg:57.46ms
step:979/2330 train_time:56250ms step_avg:57.46ms
step:980/2330 train_time:56310ms step_avg:57.46ms
step:981/2330 train_time:56366ms step_avg:57.46ms
step:982/2330 train_time:56426ms step_avg:57.46ms
step:983/2330 train_time:56483ms step_avg:57.46ms
step:984/2330 train_time:56542ms step_avg:57.46ms
step:985/2330 train_time:56599ms step_avg:57.46ms
step:986/2330 train_time:56659ms step_avg:57.46ms
step:987/2330 train_time:56716ms step_avg:57.46ms
step:988/2330 train_time:56776ms step_avg:57.47ms
step:989/2330 train_time:56832ms step_avg:57.46ms
step:990/2330 train_time:56893ms step_avg:57.47ms
step:991/2330 train_time:56950ms step_avg:57.47ms
step:992/2330 train_time:57009ms step_avg:57.47ms
step:993/2330 train_time:57066ms step_avg:57.47ms
step:994/2330 train_time:57126ms step_avg:57.47ms
step:995/2330 train_time:57182ms step_avg:57.47ms
step:996/2330 train_time:57242ms step_avg:57.47ms
step:997/2330 train_time:57299ms step_avg:57.47ms
step:998/2330 train_time:57358ms step_avg:57.47ms
step:999/2330 train_time:57415ms step_avg:57.47ms
step:1000/2330 train_time:57475ms step_avg:57.48ms
step:1000/2330 val_loss:5.0666 train_time:57557ms step_avg:57.56ms
step:1001/2330 train_time:57577ms step_avg:57.52ms
step:1002/2330 train_time:57596ms step_avg:57.48ms
step:1003/2330 train_time:57650ms step_avg:57.48ms
step:1004/2330 train_time:57715ms step_avg:57.49ms
step:1005/2330 train_time:57771ms step_avg:57.48ms
step:1006/2330 train_time:57838ms step_avg:57.49ms
step:1007/2330 train_time:57894ms step_avg:57.49ms
step:1008/2330 train_time:57955ms step_avg:57.50ms
step:1009/2330 train_time:58011ms step_avg:57.49ms
step:1010/2330 train_time:58070ms step_avg:57.50ms
step:1011/2330 train_time:58127ms step_avg:57.49ms
step:1012/2330 train_time:58186ms step_avg:57.50ms
step:1013/2330 train_time:58242ms step_avg:57.49ms
step:1014/2330 train_time:58301ms step_avg:57.50ms
step:1015/2330 train_time:58357ms step_avg:57.49ms
step:1016/2330 train_time:58417ms step_avg:57.50ms
step:1017/2330 train_time:58474ms step_avg:57.50ms
step:1018/2330 train_time:58534ms step_avg:57.50ms
step:1019/2330 train_time:58592ms step_avg:57.50ms
step:1020/2330 train_time:58652ms step_avg:57.50ms
step:1021/2330 train_time:58709ms step_avg:57.50ms
step:1022/2330 train_time:58772ms step_avg:57.51ms
step:1023/2330 train_time:58829ms step_avg:57.51ms
step:1024/2330 train_time:58889ms step_avg:57.51ms
step:1025/2330 train_time:58946ms step_avg:57.51ms
step:1026/2330 train_time:59005ms step_avg:57.51ms
step:1027/2330 train_time:59062ms step_avg:57.51ms
step:1028/2330 train_time:59121ms step_avg:57.51ms
step:1029/2330 train_time:59178ms step_avg:57.51ms
step:1030/2330 train_time:59237ms step_avg:57.51ms
step:1031/2330 train_time:59294ms step_avg:57.51ms
step:1032/2330 train_time:59353ms step_avg:57.51ms
step:1033/2330 train_time:59409ms step_avg:57.51ms
step:1034/2330 train_time:59469ms step_avg:57.51ms
step:1035/2330 train_time:59526ms step_avg:57.51ms
step:1036/2330 train_time:59587ms step_avg:57.52ms
step:1037/2330 train_time:59645ms step_avg:57.52ms
step:1038/2330 train_time:59705ms step_avg:57.52ms
step:1039/2330 train_time:59763ms step_avg:57.52ms
step:1040/2330 train_time:59824ms step_avg:57.52ms
step:1041/2330 train_time:59881ms step_avg:57.52ms
step:1042/2330 train_time:59940ms step_avg:57.52ms
step:1043/2330 train_time:59997ms step_avg:57.52ms
step:1044/2330 train_time:60057ms step_avg:57.53ms
step:1045/2330 train_time:60114ms step_avg:57.53ms
step:1046/2330 train_time:60174ms step_avg:57.53ms
step:1047/2330 train_time:60230ms step_avg:57.53ms
step:1048/2330 train_time:60289ms step_avg:57.53ms
step:1049/2330 train_time:60346ms step_avg:57.53ms
step:1050/2330 train_time:60405ms step_avg:57.53ms
step:1051/2330 train_time:60462ms step_avg:57.53ms
step:1052/2330 train_time:60522ms step_avg:57.53ms
step:1053/2330 train_time:60579ms step_avg:57.53ms
step:1054/2330 train_time:60639ms step_avg:57.53ms
step:1055/2330 train_time:60696ms step_avg:57.53ms
step:1056/2330 train_time:60757ms step_avg:57.54ms
step:1057/2330 train_time:60814ms step_avg:57.53ms
step:1058/2330 train_time:60876ms step_avg:57.54ms
step:1059/2330 train_time:60932ms step_avg:57.54ms
step:1060/2330 train_time:60994ms step_avg:57.54ms
step:1061/2330 train_time:61050ms step_avg:57.54ms
step:1062/2330 train_time:61110ms step_avg:57.54ms
step:1063/2330 train_time:61167ms step_avg:57.54ms
step:1064/2330 train_time:61226ms step_avg:57.54ms
step:1065/2330 train_time:61283ms step_avg:57.54ms
step:1066/2330 train_time:61342ms step_avg:57.54ms
step:1067/2330 train_time:61398ms step_avg:57.54ms
step:1068/2330 train_time:61459ms step_avg:57.55ms
step:1069/2330 train_time:61515ms step_avg:57.54ms
step:1070/2330 train_time:61575ms step_avg:57.55ms
step:1071/2330 train_time:61633ms step_avg:57.55ms
step:1072/2330 train_time:61692ms step_avg:57.55ms
step:1073/2330 train_time:61750ms step_avg:57.55ms
step:1074/2330 train_time:61809ms step_avg:57.55ms
step:1075/2330 train_time:61867ms step_avg:57.55ms
step:1076/2330 train_time:61927ms step_avg:57.55ms
step:1077/2330 train_time:61984ms step_avg:57.55ms
step:1078/2330 train_time:62044ms step_avg:57.55ms
step:1079/2330 train_time:62101ms step_avg:57.55ms
step:1080/2330 train_time:62160ms step_avg:57.56ms
step:1081/2330 train_time:62217ms step_avg:57.55ms
step:1082/2330 train_time:62276ms step_avg:57.56ms
step:1083/2330 train_time:62333ms step_avg:57.56ms
step:1084/2330 train_time:62392ms step_avg:57.56ms
step:1085/2330 train_time:62449ms step_avg:57.56ms
step:1086/2330 train_time:62508ms step_avg:57.56ms
step:1087/2330 train_time:62565ms step_avg:57.56ms
step:1088/2330 train_time:62624ms step_avg:57.56ms
step:1089/2330 train_time:62682ms step_avg:57.56ms
step:1090/2330 train_time:62742ms step_avg:57.56ms
step:1091/2330 train_time:62799ms step_avg:57.56ms
step:1092/2330 train_time:62860ms step_avg:57.56ms
step:1093/2330 train_time:62916ms step_avg:57.56ms
step:1094/2330 train_time:62978ms step_avg:57.57ms
step:1095/2330 train_time:63034ms step_avg:57.57ms
step:1096/2330 train_time:63095ms step_avg:57.57ms
step:1097/2330 train_time:63151ms step_avg:57.57ms
step:1098/2330 train_time:63212ms step_avg:57.57ms
step:1099/2330 train_time:63268ms step_avg:57.57ms
step:1100/2330 train_time:63328ms step_avg:57.57ms
step:1101/2330 train_time:63384ms step_avg:57.57ms
step:1102/2330 train_time:63444ms step_avg:57.57ms
step:1103/2330 train_time:63501ms step_avg:57.57ms
step:1104/2330 train_time:63561ms step_avg:57.57ms
step:1105/2330 train_time:63618ms step_avg:57.57ms
step:1106/2330 train_time:63678ms step_avg:57.57ms
step:1107/2330 train_time:63735ms step_avg:57.57ms
step:1108/2330 train_time:63794ms step_avg:57.58ms
step:1109/2330 train_time:63851ms step_avg:57.58ms
step:1110/2330 train_time:63911ms step_avg:57.58ms
step:1111/2330 train_time:63968ms step_avg:57.58ms
step:1112/2330 train_time:64028ms step_avg:57.58ms
step:1113/2330 train_time:64085ms step_avg:57.58ms
step:1114/2330 train_time:64144ms step_avg:57.58ms
step:1115/2330 train_time:64201ms step_avg:57.58ms
step:1116/2330 train_time:64262ms step_avg:57.58ms
step:1117/2330 train_time:64319ms step_avg:57.58ms
step:1118/2330 train_time:64378ms step_avg:57.58ms
step:1119/2330 train_time:64435ms step_avg:57.58ms
step:1120/2330 train_time:64494ms step_avg:57.58ms
step:1121/2330 train_time:64551ms step_avg:57.58ms
step:1122/2330 train_time:64611ms step_avg:57.59ms
step:1123/2330 train_time:64667ms step_avg:57.58ms
step:1124/2330 train_time:64727ms step_avg:57.59ms
step:1125/2330 train_time:64784ms step_avg:57.59ms
step:1126/2330 train_time:64845ms step_avg:57.59ms
step:1127/2330 train_time:64902ms step_avg:57.59ms
step:1128/2330 train_time:64962ms step_avg:57.59ms
step:1129/2330 train_time:65019ms step_avg:57.59ms
step:1130/2330 train_time:65079ms step_avg:57.59ms
step:1131/2330 train_time:65136ms step_avg:57.59ms
step:1132/2330 train_time:65194ms step_avg:57.59ms
step:1133/2330 train_time:65251ms step_avg:57.59ms
step:1134/2330 train_time:65310ms step_avg:57.59ms
step:1135/2330 train_time:65367ms step_avg:57.59ms
step:1136/2330 train_time:65426ms step_avg:57.59ms
step:1137/2330 train_time:65483ms step_avg:57.59ms
step:1138/2330 train_time:65543ms step_avg:57.60ms
step:1139/2330 train_time:65600ms step_avg:57.59ms
step:1140/2330 train_time:65660ms step_avg:57.60ms
step:1141/2330 train_time:65717ms step_avg:57.60ms
step:1142/2330 train_time:65778ms step_avg:57.60ms
step:1143/2330 train_time:65835ms step_avg:57.60ms
step:1144/2330 train_time:65896ms step_avg:57.60ms
step:1145/2330 train_time:65953ms step_avg:57.60ms
step:1146/2330 train_time:66012ms step_avg:57.60ms
step:1147/2330 train_time:66069ms step_avg:57.60ms
step:1148/2330 train_time:66129ms step_avg:57.60ms
step:1149/2330 train_time:66186ms step_avg:57.60ms
step:1150/2330 train_time:66245ms step_avg:57.60ms
step:1151/2330 train_time:66303ms step_avg:57.60ms
step:1152/2330 train_time:66362ms step_avg:57.61ms
step:1153/2330 train_time:66419ms step_avg:57.61ms
step:1154/2330 train_time:66478ms step_avg:57.61ms
step:1155/2330 train_time:66534ms step_avg:57.61ms
step:1156/2330 train_time:66594ms step_avg:57.61ms
step:1157/2330 train_time:66651ms step_avg:57.61ms
step:1158/2330 train_time:66711ms step_avg:57.61ms
step:1159/2330 train_time:66767ms step_avg:57.61ms
step:1160/2330 train_time:66828ms step_avg:57.61ms
step:1161/2330 train_time:66885ms step_avg:57.61ms
step:1162/2330 train_time:66945ms step_avg:57.61ms
step:1163/2330 train_time:67001ms step_avg:57.61ms
step:1164/2330 train_time:67063ms step_avg:57.61ms
step:1165/2330 train_time:67119ms step_avg:57.61ms
step:1166/2330 train_time:67180ms step_avg:57.62ms
step:1167/2330 train_time:67236ms step_avg:57.61ms
step:1168/2330 train_time:67297ms step_avg:57.62ms
step:1169/2330 train_time:67353ms step_avg:57.62ms
step:1170/2330 train_time:67413ms step_avg:57.62ms
step:1171/2330 train_time:67470ms step_avg:57.62ms
step:1172/2330 train_time:67530ms step_avg:57.62ms
step:1173/2330 train_time:67586ms step_avg:57.62ms
step:1174/2330 train_time:67646ms step_avg:57.62ms
step:1175/2330 train_time:67703ms step_avg:57.62ms
step:1176/2330 train_time:67763ms step_avg:57.62ms
step:1177/2330 train_time:67820ms step_avg:57.62ms
step:1178/2330 train_time:67880ms step_avg:57.62ms
step:1179/2330 train_time:67937ms step_avg:57.62ms
step:1180/2330 train_time:67997ms step_avg:57.62ms
step:1181/2330 train_time:68053ms step_avg:57.62ms
step:1182/2330 train_time:68114ms step_avg:57.63ms
step:1183/2330 train_time:68170ms step_avg:57.62ms
step:1184/2330 train_time:68231ms step_avg:57.63ms
step:1185/2330 train_time:68287ms step_avg:57.63ms
step:1186/2330 train_time:68348ms step_avg:57.63ms
step:1187/2330 train_time:68405ms step_avg:57.63ms
step:1188/2330 train_time:68464ms step_avg:57.63ms
step:1189/2330 train_time:68521ms step_avg:57.63ms
step:1190/2330 train_time:68581ms step_avg:57.63ms
step:1191/2330 train_time:68637ms step_avg:57.63ms
step:1192/2330 train_time:68698ms step_avg:57.63ms
step:1193/2330 train_time:68755ms step_avg:57.63ms
step:1194/2330 train_time:68814ms step_avg:57.63ms
step:1195/2330 train_time:68870ms step_avg:57.63ms
step:1196/2330 train_time:68930ms step_avg:57.63ms
step:1197/2330 train_time:68987ms step_avg:57.63ms
step:1198/2330 train_time:69046ms step_avg:57.63ms
step:1199/2330 train_time:69103ms step_avg:57.63ms
step:1200/2330 train_time:69163ms step_avg:57.64ms
step:1201/2330 train_time:69220ms step_avg:57.64ms
step:1202/2330 train_time:69281ms step_avg:57.64ms
step:1203/2330 train_time:69337ms step_avg:57.64ms
step:1204/2330 train_time:69397ms step_avg:57.64ms
step:1205/2330 train_time:69453ms step_avg:57.64ms
step:1206/2330 train_time:69514ms step_avg:57.64ms
step:1207/2330 train_time:69570ms step_avg:57.64ms
step:1208/2330 train_time:69631ms step_avg:57.64ms
step:1209/2330 train_time:69687ms step_avg:57.64ms
step:1210/2330 train_time:69747ms step_avg:57.64ms
step:1211/2330 train_time:69805ms step_avg:57.64ms
step:1212/2330 train_time:69864ms step_avg:57.64ms
step:1213/2330 train_time:69921ms step_avg:57.64ms
step:1214/2330 train_time:69981ms step_avg:57.65ms
step:1215/2330 train_time:70038ms step_avg:57.64ms
step:1216/2330 train_time:70098ms step_avg:57.65ms
step:1217/2330 train_time:70154ms step_avg:57.65ms
step:1218/2330 train_time:70214ms step_avg:57.65ms
step:1219/2330 train_time:70271ms step_avg:57.65ms
step:1220/2330 train_time:70330ms step_avg:57.65ms
step:1221/2330 train_time:70387ms step_avg:57.65ms
step:1222/2330 train_time:70447ms step_avg:57.65ms
step:1223/2330 train_time:70503ms step_avg:57.65ms
step:1224/2330 train_time:70564ms step_avg:57.65ms
step:1225/2330 train_time:70621ms step_avg:57.65ms
step:1226/2330 train_time:70681ms step_avg:57.65ms
step:1227/2330 train_time:70738ms step_avg:57.65ms
step:1228/2330 train_time:70798ms step_avg:57.65ms
step:1229/2330 train_time:70854ms step_avg:57.65ms
step:1230/2330 train_time:70916ms step_avg:57.66ms
step:1231/2330 train_time:70973ms step_avg:57.65ms
step:1232/2330 train_time:71033ms step_avg:57.66ms
step:1233/2330 train_time:71089ms step_avg:57.66ms
step:1234/2330 train_time:71150ms step_avg:57.66ms
step:1235/2330 train_time:71207ms step_avg:57.66ms
step:1236/2330 train_time:71266ms step_avg:57.66ms
step:1237/2330 train_time:71324ms step_avg:57.66ms
step:1238/2330 train_time:71383ms step_avg:57.66ms
step:1239/2330 train_time:71440ms step_avg:57.66ms
step:1240/2330 train_time:71501ms step_avg:57.66ms
step:1241/2330 train_time:71557ms step_avg:57.66ms
step:1242/2330 train_time:71617ms step_avg:57.66ms
step:1243/2330 train_time:71674ms step_avg:57.66ms
step:1244/2330 train_time:71734ms step_avg:57.66ms
step:1245/2330 train_time:71790ms step_avg:57.66ms
step:1246/2330 train_time:71851ms step_avg:57.66ms
step:1247/2330 train_time:71907ms step_avg:57.66ms
step:1248/2330 train_time:71967ms step_avg:57.67ms
step:1249/2330 train_time:72024ms step_avg:57.67ms
step:1250/2330 train_time:72084ms step_avg:57.67ms
step:1250/2330 val_loss:4.7997 train_time:72164ms step_avg:57.73ms
step:1251/2330 train_time:72184ms step_avg:57.70ms
step:1252/2330 train_time:72204ms step_avg:57.67ms
step:1253/2330 train_time:72262ms step_avg:57.67ms
step:1254/2330 train_time:72327ms step_avg:57.68ms
step:1255/2330 train_time:72383ms step_avg:57.68ms
step:1256/2330 train_time:72446ms step_avg:57.68ms
step:1257/2330 train_time:72502ms step_avg:57.68ms
step:1258/2330 train_time:72563ms step_avg:57.68ms
step:1259/2330 train_time:72619ms step_avg:57.68ms
step:1260/2330 train_time:72679ms step_avg:57.68ms
step:1261/2330 train_time:72736ms step_avg:57.68ms
step:1262/2330 train_time:72794ms step_avg:57.68ms
step:1263/2330 train_time:72851ms step_avg:57.68ms
step:1264/2330 train_time:72909ms step_avg:57.68ms
step:1265/2330 train_time:72966ms step_avg:57.68ms
step:1266/2330 train_time:73024ms step_avg:57.68ms
step:1267/2330 train_time:73080ms step_avg:57.68ms
step:1268/2330 train_time:73141ms step_avg:57.68ms
step:1269/2330 train_time:73199ms step_avg:57.68ms
step:1270/2330 train_time:73260ms step_avg:57.68ms
step:1271/2330 train_time:73318ms step_avg:57.68ms
step:1272/2330 train_time:73378ms step_avg:57.69ms
step:1273/2330 train_time:73435ms step_avg:57.69ms
step:1274/2330 train_time:73496ms step_avg:57.69ms
step:1275/2330 train_time:73553ms step_avg:57.69ms
step:1276/2330 train_time:73613ms step_avg:57.69ms
step:1277/2330 train_time:73669ms step_avg:57.69ms
step:1278/2330 train_time:73728ms step_avg:57.69ms
step:1279/2330 train_time:73785ms step_avg:57.69ms
step:1280/2330 train_time:73844ms step_avg:57.69ms
step:1281/2330 train_time:73900ms step_avg:57.69ms
step:1282/2330 train_time:73960ms step_avg:57.69ms
step:1283/2330 train_time:74016ms step_avg:57.69ms
step:1284/2330 train_time:74076ms step_avg:57.69ms
step:1285/2330 train_time:74132ms step_avg:57.69ms
step:1286/2330 train_time:74194ms step_avg:57.69ms
step:1287/2330 train_time:74250ms step_avg:57.69ms
step:1288/2330 train_time:74313ms step_avg:57.70ms
step:1289/2330 train_time:74370ms step_avg:57.70ms
step:1290/2330 train_time:74429ms step_avg:57.70ms
step:1291/2330 train_time:74486ms step_avg:57.70ms
step:1292/2330 train_time:74546ms step_avg:57.70ms
step:1293/2330 train_time:74602ms step_avg:57.70ms
step:1294/2330 train_time:74663ms step_avg:57.70ms
step:1295/2330 train_time:74719ms step_avg:57.70ms
step:1296/2330 train_time:74779ms step_avg:57.70ms
step:1297/2330 train_time:74835ms step_avg:57.70ms
step:1298/2330 train_time:74896ms step_avg:57.70ms
step:1299/2330 train_time:74952ms step_avg:57.70ms
step:1300/2330 train_time:75011ms step_avg:57.70ms
step:1301/2330 train_time:75068ms step_avg:57.70ms
step:1302/2330 train_time:75127ms step_avg:57.70ms
step:1303/2330 train_time:75184ms step_avg:57.70ms
step:1304/2330 train_time:75244ms step_avg:57.70ms
step:1305/2330 train_time:75301ms step_avg:57.70ms
step:1306/2330 train_time:75361ms step_avg:57.70ms
step:1307/2330 train_time:75419ms step_avg:57.70ms
step:1308/2330 train_time:75479ms step_avg:57.71ms
step:1309/2330 train_time:75536ms step_avg:57.70ms
step:1310/2330 train_time:75596ms step_avg:57.71ms
step:1311/2330 train_time:75652ms step_avg:57.71ms
step:1312/2330 train_time:75712ms step_avg:57.71ms
step:1313/2330 train_time:75769ms step_avg:57.71ms
step:1314/2330 train_time:75828ms step_avg:57.71ms
step:1315/2330 train_time:75885ms step_avg:57.71ms
step:1316/2330 train_time:75944ms step_avg:57.71ms
step:1317/2330 train_time:76000ms step_avg:57.71ms
step:1318/2330 train_time:76060ms step_avg:57.71ms
step:1319/2330 train_time:76116ms step_avg:57.71ms
step:1320/2330 train_time:76177ms step_avg:57.71ms
step:1321/2330 train_time:76234ms step_avg:57.71ms
step:1322/2330 train_time:76294ms step_avg:57.71ms
step:1323/2330 train_time:76351ms step_avg:57.71ms
step:1324/2330 train_time:76411ms step_avg:57.71ms
step:1325/2330 train_time:76468ms step_avg:57.71ms
step:1326/2330 train_time:76527ms step_avg:57.71ms
step:1327/2330 train_time:76584ms step_avg:57.71ms
step:1328/2330 train_time:76644ms step_avg:57.71ms
step:1329/2330 train_time:76700ms step_avg:57.71ms
step:1330/2330 train_time:76761ms step_avg:57.71ms
step:1331/2330 train_time:76817ms step_avg:57.71ms
step:1332/2330 train_time:76877ms step_avg:57.72ms
step:1333/2330 train_time:76934ms step_avg:57.72ms
step:1334/2330 train_time:76993ms step_avg:57.72ms
step:1335/2330 train_time:77050ms step_avg:57.72ms
step:1336/2330 train_time:77110ms step_avg:57.72ms
step:1337/2330 train_time:77166ms step_avg:57.72ms
step:1338/2330 train_time:77227ms step_avg:57.72ms
step:1339/2330 train_time:77284ms step_avg:57.72ms
step:1340/2330 train_time:77343ms step_avg:57.72ms
step:1341/2330 train_time:77400ms step_avg:57.72ms
step:1342/2330 train_time:77460ms step_avg:57.72ms
step:1343/2330 train_time:77517ms step_avg:57.72ms
step:1344/2330 train_time:77576ms step_avg:57.72ms
step:1345/2330 train_time:77634ms step_avg:57.72ms
step:1346/2330 train_time:77693ms step_avg:57.72ms
step:1347/2330 train_time:77750ms step_avg:57.72ms
step:1348/2330 train_time:77810ms step_avg:57.72ms
step:1349/2330 train_time:77866ms step_avg:57.72ms
step:1350/2330 train_time:77926ms step_avg:57.72ms
step:1351/2330 train_time:77982ms step_avg:57.72ms
step:1352/2330 train_time:78042ms step_avg:57.72ms
step:1353/2330 train_time:78099ms step_avg:57.72ms
step:1354/2330 train_time:78159ms step_avg:57.72ms
step:1355/2330 train_time:78216ms step_avg:57.72ms
step:1356/2330 train_time:78275ms step_avg:57.73ms
step:1357/2330 train_time:78332ms step_avg:57.72ms
step:1358/2330 train_time:78393ms step_avg:57.73ms
step:1359/2330 train_time:78450ms step_avg:57.73ms
step:1360/2330 train_time:78509ms step_avg:57.73ms
step:1361/2330 train_time:78565ms step_avg:57.73ms
step:1362/2330 train_time:78625ms step_avg:57.73ms
step:1363/2330 train_time:78682ms step_avg:57.73ms
step:1364/2330 train_time:78742ms step_avg:57.73ms
step:1365/2330 train_time:78799ms step_avg:57.73ms
step:1366/2330 train_time:78858ms step_avg:57.73ms
step:1367/2330 train_time:78915ms step_avg:57.73ms
step:1368/2330 train_time:78975ms step_avg:57.73ms
step:1369/2330 train_time:79032ms step_avg:57.73ms
step:1370/2330 train_time:79092ms step_avg:57.73ms
step:1371/2330 train_time:79149ms step_avg:57.73ms
step:1372/2330 train_time:79209ms step_avg:57.73ms
step:1373/2330 train_time:79265ms step_avg:57.73ms
step:1374/2330 train_time:79325ms step_avg:57.73ms
step:1375/2330 train_time:79381ms step_avg:57.73ms
step:1376/2330 train_time:79441ms step_avg:57.73ms
step:1377/2330 train_time:79499ms step_avg:57.73ms
step:1378/2330 train_time:79559ms step_avg:57.73ms
step:1379/2330 train_time:79616ms step_avg:57.73ms
step:1380/2330 train_time:79675ms step_avg:57.74ms
step:1381/2330 train_time:79732ms step_avg:57.73ms
step:1382/2330 train_time:79791ms step_avg:57.74ms
step:1383/2330 train_time:79848ms step_avg:57.74ms
step:1384/2330 train_time:79907ms step_avg:57.74ms
step:1385/2330 train_time:79965ms step_avg:57.74ms
step:1386/2330 train_time:80024ms step_avg:57.74ms
step:1387/2330 train_time:80081ms step_avg:57.74ms
step:1388/2330 train_time:80141ms step_avg:57.74ms
step:1389/2330 train_time:80198ms step_avg:57.74ms
step:1390/2330 train_time:80258ms step_avg:57.74ms
step:1391/2330 train_time:80315ms step_avg:57.74ms
step:1392/2330 train_time:80374ms step_avg:57.74ms
step:1393/2330 train_time:80431ms step_avg:57.74ms
step:1394/2330 train_time:80490ms step_avg:57.74ms
step:1395/2330 train_time:80547ms step_avg:57.74ms
step:1396/2330 train_time:80607ms step_avg:57.74ms
step:1397/2330 train_time:80663ms step_avg:57.74ms
step:1398/2330 train_time:80723ms step_avg:57.74ms
step:1399/2330 train_time:80780ms step_avg:57.74ms
step:1400/2330 train_time:80840ms step_avg:57.74ms
step:1401/2330 train_time:80897ms step_avg:57.74ms
step:1402/2330 train_time:80957ms step_avg:57.74ms
step:1403/2330 train_time:81014ms step_avg:57.74ms
step:1404/2330 train_time:81075ms step_avg:57.75ms
step:1405/2330 train_time:81132ms step_avg:57.75ms
step:1406/2330 train_time:81192ms step_avg:57.75ms
step:1407/2330 train_time:81248ms step_avg:57.75ms
step:1408/2330 train_time:81308ms step_avg:57.75ms
step:1409/2330 train_time:81365ms step_avg:57.75ms
step:1410/2330 train_time:81424ms step_avg:57.75ms
step:1411/2330 train_time:81481ms step_avg:57.75ms
step:1412/2330 train_time:81540ms step_avg:57.75ms
step:1413/2330 train_time:81598ms step_avg:57.75ms
step:1414/2330 train_time:81657ms step_avg:57.75ms
step:1415/2330 train_time:81715ms step_avg:57.75ms
step:1416/2330 train_time:81774ms step_avg:57.75ms
step:1417/2330 train_time:81831ms step_avg:57.75ms
step:1418/2330 train_time:81891ms step_avg:57.75ms
step:1419/2330 train_time:81947ms step_avg:57.75ms
step:1420/2330 train_time:82007ms step_avg:57.75ms
step:1421/2330 train_time:82063ms step_avg:57.75ms
step:1422/2330 train_time:82124ms step_avg:57.75ms
step:1423/2330 train_time:82181ms step_avg:57.75ms
step:1424/2330 train_time:82240ms step_avg:57.75ms
step:1425/2330 train_time:82297ms step_avg:57.75ms
step:1426/2330 train_time:82357ms step_avg:57.75ms
step:1427/2330 train_time:82414ms step_avg:57.75ms
step:1428/2330 train_time:82474ms step_avg:57.76ms
step:1429/2330 train_time:82531ms step_avg:57.75ms
step:1430/2330 train_time:82591ms step_avg:57.76ms
step:1431/2330 train_time:82648ms step_avg:57.76ms
step:1432/2330 train_time:82708ms step_avg:57.76ms
step:1433/2330 train_time:82764ms step_avg:57.76ms
step:1434/2330 train_time:82824ms step_avg:57.76ms
step:1435/2330 train_time:82881ms step_avg:57.76ms
step:1436/2330 train_time:82941ms step_avg:57.76ms
step:1437/2330 train_time:82999ms step_avg:57.76ms
step:1438/2330 train_time:83058ms step_avg:57.76ms
step:1439/2330 train_time:83115ms step_avg:57.76ms
step:1440/2330 train_time:83175ms step_avg:57.76ms
step:1441/2330 train_time:83232ms step_avg:57.76ms
step:1442/2330 train_time:83291ms step_avg:57.76ms
step:1443/2330 train_time:83347ms step_avg:57.76ms
step:1444/2330 train_time:83407ms step_avg:57.76ms
step:1445/2330 train_time:83464ms step_avg:57.76ms
step:1446/2330 train_time:83524ms step_avg:57.76ms
step:1447/2330 train_time:83581ms step_avg:57.76ms
step:1448/2330 train_time:83640ms step_avg:57.76ms
step:1449/2330 train_time:83697ms step_avg:57.76ms
step:1450/2330 train_time:83758ms step_avg:57.76ms
step:1451/2330 train_time:83814ms step_avg:57.76ms
step:1452/2330 train_time:83875ms step_avg:57.76ms
step:1453/2330 train_time:83931ms step_avg:57.76ms
step:1454/2330 train_time:83991ms step_avg:57.77ms
step:1455/2330 train_time:84048ms step_avg:57.76ms
step:1456/2330 train_time:84107ms step_avg:57.77ms
step:1457/2330 train_time:84164ms step_avg:57.77ms
step:1458/2330 train_time:84224ms step_avg:57.77ms
step:1459/2330 train_time:84280ms step_avg:57.77ms
step:1460/2330 train_time:84340ms step_avg:57.77ms
step:1461/2330 train_time:84397ms step_avg:57.77ms
step:1462/2330 train_time:84458ms step_avg:57.77ms
step:1463/2330 train_time:84514ms step_avg:57.77ms
step:1464/2330 train_time:84574ms step_avg:57.77ms
step:1465/2330 train_time:84631ms step_avg:57.77ms
step:1466/2330 train_time:84691ms step_avg:57.77ms
step:1467/2330 train_time:84748ms step_avg:57.77ms
step:1468/2330 train_time:84807ms step_avg:57.77ms
step:1469/2330 train_time:84863ms step_avg:57.77ms
step:1470/2330 train_time:84924ms step_avg:57.77ms
step:1471/2330 train_time:84980ms step_avg:57.77ms
step:1472/2330 train_time:85040ms step_avg:57.77ms
step:1473/2330 train_time:85097ms step_avg:57.77ms
step:1474/2330 train_time:85157ms step_avg:57.77ms
step:1475/2330 train_time:85214ms step_avg:57.77ms
step:1476/2330 train_time:85274ms step_avg:57.77ms
step:1477/2330 train_time:85331ms step_avg:57.77ms
step:1478/2330 train_time:85390ms step_avg:57.77ms
step:1479/2330 train_time:85445ms step_avg:57.77ms
step:1480/2330 train_time:85506ms step_avg:57.77ms
step:1481/2330 train_time:85563ms step_avg:57.77ms
step:1482/2330 train_time:85623ms step_avg:57.78ms
step:1483/2330 train_time:85680ms step_avg:57.77ms
step:1484/2330 train_time:85740ms step_avg:57.78ms
step:1485/2330 train_time:85797ms step_avg:57.78ms
step:1486/2330 train_time:85856ms step_avg:57.78ms
step:1487/2330 train_time:85913ms step_avg:57.78ms
step:1488/2330 train_time:85973ms step_avg:57.78ms
step:1489/2330 train_time:86030ms step_avg:57.78ms
step:1490/2330 train_time:86090ms step_avg:57.78ms
step:1491/2330 train_time:86146ms step_avg:57.78ms
step:1492/2330 train_time:86207ms step_avg:57.78ms
step:1493/2330 train_time:86263ms step_avg:57.78ms
step:1494/2330 train_time:86323ms step_avg:57.78ms
step:1495/2330 train_time:86380ms step_avg:57.78ms
step:1496/2330 train_time:86439ms step_avg:57.78ms
step:1497/2330 train_time:86496ms step_avg:57.78ms
step:1498/2330 train_time:86556ms step_avg:57.78ms
step:1499/2330 train_time:86614ms step_avg:57.78ms
step:1500/2330 train_time:86673ms step_avg:57.78ms
step:1500/2330 val_loss:4.5757 train_time:86753ms step_avg:57.84ms
step:1501/2330 train_time:86772ms step_avg:57.81ms
step:1502/2330 train_time:86793ms step_avg:57.78ms
step:1503/2330 train_time:86853ms step_avg:57.79ms
step:1504/2330 train_time:86915ms step_avg:57.79ms
step:1505/2330 train_time:86972ms step_avg:57.79ms
step:1506/2330 train_time:87033ms step_avg:57.79ms
step:1507/2330 train_time:87089ms step_avg:57.79ms
step:1508/2330 train_time:87149ms step_avg:57.79ms
step:1509/2330 train_time:87205ms step_avg:57.79ms
step:1510/2330 train_time:87265ms step_avg:57.79ms
step:1511/2330 train_time:87321ms step_avg:57.79ms
step:1512/2330 train_time:87380ms step_avg:57.79ms
step:1513/2330 train_time:87435ms step_avg:57.79ms
step:1514/2330 train_time:87495ms step_avg:57.79ms
step:1515/2330 train_time:87551ms step_avg:57.79ms
step:1516/2330 train_time:87610ms step_avg:57.79ms
step:1517/2330 train_time:87666ms step_avg:57.79ms
step:1518/2330 train_time:87726ms step_avg:57.79ms
step:1519/2330 train_time:87783ms step_avg:57.79ms
step:1520/2330 train_time:87845ms step_avg:57.79ms
step:1521/2330 train_time:87902ms step_avg:57.79ms
step:1522/2330 train_time:87964ms step_avg:57.80ms
step:1523/2330 train_time:88022ms step_avg:57.79ms
step:1524/2330 train_time:88082ms step_avg:57.80ms
step:1525/2330 train_time:88138ms step_avg:57.80ms
step:1526/2330 train_time:88199ms step_avg:57.80ms
step:1527/2330 train_time:88256ms step_avg:57.80ms
step:1528/2330 train_time:88316ms step_avg:57.80ms
step:1529/2330 train_time:88373ms step_avg:57.80ms
step:1530/2330 train_time:88431ms step_avg:57.80ms
step:1531/2330 train_time:88488ms step_avg:57.80ms
step:1532/2330 train_time:88548ms step_avg:57.80ms
step:1533/2330 train_time:88604ms step_avg:57.80ms
step:1534/2330 train_time:88664ms step_avg:57.80ms
step:1535/2330 train_time:88720ms step_avg:57.80ms
step:1536/2330 train_time:88781ms step_avg:57.80ms
step:1537/2330 train_time:88838ms step_avg:57.80ms
step:1538/2330 train_time:88901ms step_avg:57.80ms
step:1539/2330 train_time:88959ms step_avg:57.80ms
step:1540/2330 train_time:89020ms step_avg:57.81ms
step:1541/2330 train_time:89078ms step_avg:57.81ms
step:1542/2330 train_time:89139ms step_avg:57.81ms
step:1543/2330 train_time:89197ms step_avg:57.81ms
step:1544/2330 train_time:89257ms step_avg:57.81ms
step:1545/2330 train_time:89313ms step_avg:57.81ms
step:1546/2330 train_time:89374ms step_avg:57.81ms
step:1547/2330 train_time:89430ms step_avg:57.81ms
step:1548/2330 train_time:89491ms step_avg:57.81ms
step:1549/2330 train_time:89547ms step_avg:57.81ms
step:1550/2330 train_time:89607ms step_avg:57.81ms
step:1551/2330 train_time:89664ms step_avg:57.81ms
step:1552/2330 train_time:89724ms step_avg:57.81ms
step:1553/2330 train_time:89780ms step_avg:57.81ms
step:1554/2330 train_time:89841ms step_avg:57.81ms
step:1555/2330 train_time:89899ms step_avg:57.81ms
step:1556/2330 train_time:89960ms step_avg:57.82ms
step:1557/2330 train_time:90018ms step_avg:57.81ms
step:1558/2330 train_time:90080ms step_avg:57.82ms
step:1559/2330 train_time:90138ms step_avg:57.82ms
step:1560/2330 train_time:90198ms step_avg:57.82ms
step:1561/2330 train_time:90255ms step_avg:57.82ms
step:1562/2330 train_time:90316ms step_avg:57.82ms
step:1563/2330 train_time:90373ms step_avg:57.82ms
step:1564/2330 train_time:90433ms step_avg:57.82ms
step:1565/2330 train_time:90489ms step_avg:57.82ms
step:1566/2330 train_time:90550ms step_avg:57.82ms
step:1567/2330 train_time:90607ms step_avg:57.82ms
step:1568/2330 train_time:90667ms step_avg:57.82ms
step:1569/2330 train_time:90724ms step_avg:57.82ms
step:1570/2330 train_time:90783ms step_avg:57.82ms
step:1571/2330 train_time:90840ms step_avg:57.82ms
step:1572/2330 train_time:90902ms step_avg:57.83ms
step:1573/2330 train_time:90959ms step_avg:57.83ms
step:1574/2330 train_time:91021ms step_avg:57.83ms
step:1575/2330 train_time:91079ms step_avg:57.83ms
step:1576/2330 train_time:91140ms step_avg:57.83ms
step:1577/2330 train_time:91198ms step_avg:57.83ms
step:1578/2330 train_time:91258ms step_avg:57.83ms
step:1579/2330 train_time:91316ms step_avg:57.83ms
step:1580/2330 train_time:91376ms step_avg:57.83ms
step:1581/2330 train_time:91435ms step_avg:57.83ms
step:1582/2330 train_time:91494ms step_avg:57.83ms
step:1583/2330 train_time:91551ms step_avg:57.83ms
step:1584/2330 train_time:91612ms step_avg:57.84ms
step:1585/2330 train_time:91668ms step_avg:57.83ms
step:1586/2330 train_time:91729ms step_avg:57.84ms
step:1587/2330 train_time:91786ms step_avg:57.84ms
step:1588/2330 train_time:91847ms step_avg:57.84ms
step:1589/2330 train_time:91902ms step_avg:57.84ms
step:1590/2330 train_time:91965ms step_avg:57.84ms
step:1591/2330 train_time:92022ms step_avg:57.84ms
step:1592/2330 train_time:92082ms step_avg:57.84ms
step:1593/2330 train_time:92139ms step_avg:57.84ms
step:1594/2330 train_time:92200ms step_avg:57.84ms
step:1595/2330 train_time:92257ms step_avg:57.84ms
step:1596/2330 train_time:92319ms step_avg:57.84ms
step:1597/2330 train_time:92377ms step_avg:57.84ms
step:1598/2330 train_time:92437ms step_avg:57.85ms
step:1599/2330 train_time:92494ms step_avg:57.85ms
step:1600/2330 train_time:92555ms step_avg:57.85ms
step:1601/2330 train_time:92613ms step_avg:57.85ms
step:1602/2330 train_time:92673ms step_avg:57.85ms
step:1603/2330 train_time:92729ms step_avg:57.85ms
step:1604/2330 train_time:92791ms step_avg:57.85ms
step:1605/2330 train_time:92847ms step_avg:57.85ms
step:1606/2330 train_time:92908ms step_avg:57.85ms
step:1607/2330 train_time:92964ms step_avg:57.85ms
step:1608/2330 train_time:93025ms step_avg:57.85ms
step:1609/2330 train_time:93082ms step_avg:57.85ms
step:1610/2330 train_time:93142ms step_avg:57.85ms
step:1611/2330 train_time:93199ms step_avg:57.85ms
step:1612/2330 train_time:93260ms step_avg:57.85ms
step:1613/2330 train_time:93317ms step_avg:57.85ms
step:1614/2330 train_time:93377ms step_avg:57.85ms
step:1615/2330 train_time:93434ms step_avg:57.85ms
step:1616/2330 train_time:93496ms step_avg:57.86ms
step:1617/2330 train_time:93554ms step_avg:57.86ms
step:1618/2330 train_time:93615ms step_avg:57.86ms
step:1619/2330 train_time:93673ms step_avg:57.86ms
step:1620/2330 train_time:93733ms step_avg:57.86ms
step:1621/2330 train_time:93789ms step_avg:57.86ms
step:1622/2330 train_time:93851ms step_avg:57.86ms
step:1623/2330 train_time:93907ms step_avg:57.86ms
step:1624/2330 train_time:93968ms step_avg:57.86ms
step:1625/2330 train_time:94024ms step_avg:57.86ms
step:1626/2330 train_time:94085ms step_avg:57.86ms
step:1627/2330 train_time:94141ms step_avg:57.86ms
step:1628/2330 train_time:94201ms step_avg:57.86ms
step:1629/2330 train_time:94258ms step_avg:57.86ms
step:1630/2330 train_time:94320ms step_avg:57.86ms
step:1631/2330 train_time:94377ms step_avg:57.86ms
step:1632/2330 train_time:94437ms step_avg:57.87ms
step:1633/2330 train_time:94495ms step_avg:57.87ms
step:1634/2330 train_time:94557ms step_avg:57.87ms
step:1635/2330 train_time:94615ms step_avg:57.87ms
step:1636/2330 train_time:94676ms step_avg:57.87ms
step:1637/2330 train_time:94734ms step_avg:57.87ms
step:1638/2330 train_time:94794ms step_avg:57.87ms
step:1639/2330 train_time:94851ms step_avg:57.87ms
step:1640/2330 train_time:94912ms step_avg:57.87ms
step:1641/2330 train_time:94969ms step_avg:57.87ms
step:1642/2330 train_time:95030ms step_avg:57.87ms
step:1643/2330 train_time:95086ms step_avg:57.87ms
step:1644/2330 train_time:95148ms step_avg:57.88ms
step:1645/2330 train_time:95204ms step_avg:57.87ms
step:1646/2330 train_time:95264ms step_avg:57.88ms
step:1647/2330 train_time:95320ms step_avg:57.88ms
step:1648/2330 train_time:95381ms step_avg:57.88ms
step:1649/2330 train_time:95438ms step_avg:57.88ms
step:1650/2330 train_time:95500ms step_avg:57.88ms
step:1651/2330 train_time:95557ms step_avg:57.88ms
step:1652/2330 train_time:95619ms step_avg:57.88ms
step:1653/2330 train_time:95676ms step_avg:57.88ms
step:1654/2330 train_time:95737ms step_avg:57.88ms
step:1655/2330 train_time:95795ms step_avg:57.88ms
step:1656/2330 train_time:95855ms step_avg:57.88ms
step:1657/2330 train_time:95913ms step_avg:57.88ms
step:1658/2330 train_time:95974ms step_avg:57.89ms
step:1659/2330 train_time:96031ms step_avg:57.88ms
step:1660/2330 train_time:96091ms step_avg:57.89ms
step:1661/2330 train_time:96147ms step_avg:57.89ms
step:1662/2330 train_time:96209ms step_avg:57.89ms
step:1663/2330 train_time:96265ms step_avg:57.89ms
step:1664/2330 train_time:96325ms step_avg:57.89ms
step:1665/2330 train_time:96381ms step_avg:57.89ms
step:1666/2330 train_time:96442ms step_avg:57.89ms
step:1667/2330 train_time:96499ms step_avg:57.89ms
step:1668/2330 train_time:96562ms step_avg:57.89ms
step:1669/2330 train_time:96620ms step_avg:57.89ms
step:1670/2330 train_time:96680ms step_avg:57.89ms
step:1671/2330 train_time:96739ms step_avg:57.89ms
step:1672/2330 train_time:96799ms step_avg:57.89ms
step:1673/2330 train_time:96857ms step_avg:57.89ms
step:1674/2330 train_time:96919ms step_avg:57.90ms
step:1675/2330 train_time:96978ms step_avg:57.90ms
step:1676/2330 train_time:97038ms step_avg:57.90ms
step:1677/2330 train_time:97095ms step_avg:57.90ms
step:1678/2330 train_time:97156ms step_avg:57.90ms
step:1679/2330 train_time:97213ms step_avg:57.90ms
step:1680/2330 train_time:97274ms step_avg:57.90ms
step:1681/2330 train_time:97331ms step_avg:57.90ms
step:1682/2330 train_time:97392ms step_avg:57.90ms
step:1683/2330 train_time:97448ms step_avg:57.90ms
step:1684/2330 train_time:97510ms step_avg:57.90ms
step:1685/2330 train_time:97566ms step_avg:57.90ms
step:1686/2330 train_time:97627ms step_avg:57.90ms
step:1687/2330 train_time:97684ms step_avg:57.90ms
step:1688/2330 train_time:97744ms step_avg:57.91ms
step:1689/2330 train_time:97801ms step_avg:57.90ms
step:1690/2330 train_time:97863ms step_avg:57.91ms
step:1691/2330 train_time:97920ms step_avg:57.91ms
step:1692/2330 train_time:97981ms step_avg:57.91ms
step:1693/2330 train_time:98038ms step_avg:57.91ms
step:1694/2330 train_time:98100ms step_avg:57.91ms
step:1695/2330 train_time:98157ms step_avg:57.91ms
step:1696/2330 train_time:98218ms step_avg:57.91ms
step:1697/2330 train_time:98275ms step_avg:57.91ms
step:1698/2330 train_time:98336ms step_avg:57.91ms
step:1699/2330 train_time:98392ms step_avg:57.91ms
step:1700/2330 train_time:98454ms step_avg:57.91ms
step:1701/2330 train_time:98509ms step_avg:57.91ms
step:1702/2330 train_time:98572ms step_avg:57.92ms
step:1703/2330 train_time:98629ms step_avg:57.91ms
step:1704/2330 train_time:98689ms step_avg:57.92ms
step:1705/2330 train_time:98745ms step_avg:57.91ms
step:1706/2330 train_time:98806ms step_avg:57.92ms
step:1707/2330 train_time:98862ms step_avg:57.92ms
step:1708/2330 train_time:98925ms step_avg:57.92ms
step:1709/2330 train_time:98982ms step_avg:57.92ms
step:1710/2330 train_time:99042ms step_avg:57.92ms
step:1711/2330 train_time:99099ms step_avg:57.92ms
step:1712/2330 train_time:99159ms step_avg:57.92ms
step:1713/2330 train_time:99217ms step_avg:57.92ms
step:1714/2330 train_time:99277ms step_avg:57.92ms
step:1715/2330 train_time:99335ms step_avg:57.92ms
step:1716/2330 train_time:99395ms step_avg:57.92ms
step:1717/2330 train_time:99453ms step_avg:57.92ms
step:1718/2330 train_time:99513ms step_avg:57.92ms
step:1719/2330 train_time:99570ms step_avg:57.92ms
step:1720/2330 train_time:99632ms step_avg:57.93ms
step:1721/2330 train_time:99688ms step_avg:57.92ms
step:1722/2330 train_time:99748ms step_avg:57.93ms
step:1723/2330 train_time:99804ms step_avg:57.92ms
step:1724/2330 train_time:99865ms step_avg:57.93ms
step:1725/2330 train_time:99922ms step_avg:57.93ms
step:1726/2330 train_time:99983ms step_avg:57.93ms
step:1727/2330 train_time:100040ms step_avg:57.93ms
step:1728/2330 train_time:100099ms step_avg:57.93ms
step:1729/2330 train_time:100156ms step_avg:57.93ms
step:1730/2330 train_time:100218ms step_avg:57.93ms
step:1731/2330 train_time:100275ms step_avg:57.93ms
step:1732/2330 train_time:100336ms step_avg:57.93ms
step:1733/2330 train_time:100393ms step_avg:57.93ms
step:1734/2330 train_time:100455ms step_avg:57.93ms
step:1735/2330 train_time:100513ms step_avg:57.93ms
step:1736/2330 train_time:100575ms step_avg:57.93ms
step:1737/2330 train_time:100632ms step_avg:57.93ms
step:1738/2330 train_time:100693ms step_avg:57.94ms
step:1739/2330 train_time:100749ms step_avg:57.94ms
step:1740/2330 train_time:100811ms step_avg:57.94ms
step:1741/2330 train_time:100867ms step_avg:57.94ms
step:1742/2330 train_time:100928ms step_avg:57.94ms
step:1743/2330 train_time:100984ms step_avg:57.94ms
step:1744/2330 train_time:101045ms step_avg:57.94ms
step:1745/2330 train_time:101101ms step_avg:57.94ms
step:1746/2330 train_time:101162ms step_avg:57.94ms
step:1747/2330 train_time:101219ms step_avg:57.94ms
step:1748/2330 train_time:101280ms step_avg:57.94ms
step:1749/2330 train_time:101338ms step_avg:57.94ms
step:1750/2330 train_time:101399ms step_avg:57.94ms
step:1750/2330 val_loss:4.4386 train_time:101480ms step_avg:57.99ms
step:1751/2330 train_time:101500ms step_avg:57.97ms
step:1752/2330 train_time:101520ms step_avg:57.95ms
step:1753/2330 train_time:101573ms step_avg:57.94ms
step:1754/2330 train_time:101637ms step_avg:57.95ms
step:1755/2330 train_time:101693ms step_avg:57.94ms
step:1756/2330 train_time:101764ms step_avg:57.95ms
step:1757/2330 train_time:101820ms step_avg:57.95ms
step:1758/2330 train_time:101883ms step_avg:57.95ms
step:1759/2330 train_time:101939ms step_avg:57.95ms
step:1760/2330 train_time:101999ms step_avg:57.95ms
step:1761/2330 train_time:102056ms step_avg:57.95ms
step:1762/2330 train_time:102115ms step_avg:57.95ms
step:1763/2330 train_time:102171ms step_avg:57.95ms
step:1764/2330 train_time:102231ms step_avg:57.95ms
step:1765/2330 train_time:102287ms step_avg:57.95ms
step:1766/2330 train_time:102346ms step_avg:57.95ms
step:1767/2330 train_time:102405ms step_avg:57.95ms
step:1768/2330 train_time:102467ms step_avg:57.96ms
step:1769/2330 train_time:102525ms step_avg:57.96ms
step:1770/2330 train_time:102587ms step_avg:57.96ms
step:1771/2330 train_time:102646ms step_avg:57.96ms
step:1772/2330 train_time:102707ms step_avg:57.96ms
step:1773/2330 train_time:102764ms step_avg:57.96ms
step:1774/2330 train_time:102825ms step_avg:57.96ms
step:1775/2330 train_time:102882ms step_avg:57.96ms
step:1776/2330 train_time:102944ms step_avg:57.96ms
step:1777/2330 train_time:103000ms step_avg:57.96ms
step:1778/2330 train_time:103060ms step_avg:57.96ms
step:1779/2330 train_time:103117ms step_avg:57.96ms
step:1780/2330 train_time:103176ms step_avg:57.96ms
step:1781/2330 train_time:103232ms step_avg:57.96ms
step:1782/2330 train_time:103292ms step_avg:57.96ms
step:1783/2330 train_time:103348ms step_avg:57.96ms
step:1784/2330 train_time:103410ms step_avg:57.97ms
step:1785/2330 train_time:103467ms step_avg:57.96ms
step:1786/2330 train_time:103529ms step_avg:57.97ms
step:1787/2330 train_time:103586ms step_avg:57.97ms
step:1788/2330 train_time:103651ms step_avg:57.97ms
step:1789/2330 train_time:103709ms step_avg:57.97ms
step:1790/2330 train_time:103769ms step_avg:57.97ms
step:1791/2330 train_time:103827ms step_avg:57.97ms
step:1792/2330 train_time:103888ms step_avg:57.97ms
step:1793/2330 train_time:103946ms step_avg:57.97ms
step:1794/2330 train_time:104007ms step_avg:57.97ms
step:1795/2330 train_time:104063ms step_avg:57.97ms
step:1796/2330 train_time:104123ms step_avg:57.97ms
step:1797/2330 train_time:104180ms step_avg:57.97ms
step:1798/2330 train_time:104240ms step_avg:57.98ms
step:1799/2330 train_time:104296ms step_avg:57.97ms
step:1800/2330 train_time:104358ms step_avg:57.98ms
step:1801/2330 train_time:104414ms step_avg:57.98ms
step:1802/2330 train_time:104475ms step_avg:57.98ms
step:1803/2330 train_time:104532ms step_avg:57.98ms
step:1804/2330 train_time:104593ms step_avg:57.98ms
step:1805/2330 train_time:104650ms step_avg:57.98ms
step:1806/2330 train_time:104711ms step_avg:57.98ms
step:1807/2330 train_time:104768ms step_avg:57.98ms
step:1808/2330 train_time:104829ms step_avg:57.98ms
step:1809/2330 train_time:104887ms step_avg:57.98ms
step:1810/2330 train_time:104948ms step_avg:57.98ms
step:1811/2330 train_time:105007ms step_avg:57.98ms
step:1812/2330 train_time:105067ms step_avg:57.98ms
step:1813/2330 train_time:105124ms step_avg:57.98ms
step:1814/2330 train_time:105184ms step_avg:57.98ms
step:1815/2330 train_time:105241ms step_avg:57.98ms
step:1816/2330 train_time:105302ms step_avg:57.99ms
step:1817/2330 train_time:105359ms step_avg:57.99ms
step:1818/2330 train_time:105420ms step_avg:57.99ms
step:1819/2330 train_time:105477ms step_avg:57.99ms
step:1820/2330 train_time:105539ms step_avg:57.99ms
step:1821/2330 train_time:105595ms step_avg:57.99ms
step:1822/2330 train_time:105657ms step_avg:57.99ms
step:1823/2330 train_time:105714ms step_avg:57.99ms
step:1824/2330 train_time:105775ms step_avg:57.99ms
step:1825/2330 train_time:105831ms step_avg:57.99ms
step:1826/2330 train_time:105893ms step_avg:57.99ms
step:1827/2330 train_time:105950ms step_avg:57.99ms
step:1828/2330 train_time:106012ms step_avg:57.99ms
step:1829/2330 train_time:106070ms step_avg:57.99ms
step:1830/2330 train_time:106130ms step_avg:57.99ms
step:1831/2330 train_time:106188ms step_avg:57.99ms
step:1832/2330 train_time:106248ms step_avg:58.00ms
step:1833/2330 train_time:106306ms step_avg:58.00ms
step:1834/2330 train_time:106366ms step_avg:58.00ms
step:1835/2330 train_time:106423ms step_avg:58.00ms
step:1836/2330 train_time:106484ms step_avg:58.00ms
step:1837/2330 train_time:106541ms step_avg:58.00ms
step:1838/2330 train_time:106602ms step_avg:58.00ms
step:1839/2330 train_time:106658ms step_avg:58.00ms
step:1840/2330 train_time:106722ms step_avg:58.00ms
step:1841/2330 train_time:106778ms step_avg:58.00ms
step:1842/2330 train_time:106839ms step_avg:58.00ms
step:1843/2330 train_time:106895ms step_avg:58.00ms
step:1844/2330 train_time:106955ms step_avg:58.00ms
step:1845/2330 train_time:107012ms step_avg:58.00ms
step:1846/2330 train_time:107073ms step_avg:58.00ms
step:1847/2330 train_time:107130ms step_avg:58.00ms
step:1848/2330 train_time:107192ms step_avg:58.00ms
step:1849/2330 train_time:107250ms step_avg:58.00ms
step:1850/2330 train_time:107309ms step_avg:58.00ms
step:1851/2330 train_time:107366ms step_avg:58.00ms
step:1852/2330 train_time:107428ms step_avg:58.01ms
step:1853/2330 train_time:107485ms step_avg:58.01ms
step:1854/2330 train_time:107546ms step_avg:58.01ms
step:1855/2330 train_time:107604ms step_avg:58.01ms
step:1856/2330 train_time:107665ms step_avg:58.01ms
step:1857/2330 train_time:107722ms step_avg:58.01ms
step:1858/2330 train_time:107783ms step_avg:58.01ms
step:1859/2330 train_time:107840ms step_avg:58.01ms
step:1860/2330 train_time:107902ms step_avg:58.01ms
step:1861/2330 train_time:107959ms step_avg:58.01ms
step:1862/2330 train_time:108020ms step_avg:58.01ms
step:1863/2330 train_time:108076ms step_avg:58.01ms
step:1864/2330 train_time:108137ms step_avg:58.01ms
step:1865/2330 train_time:108194ms step_avg:58.01ms
step:1866/2330 train_time:108253ms step_avg:58.01ms
step:1867/2330 train_time:108310ms step_avg:58.01ms
step:1868/2330 train_time:108371ms step_avg:58.01ms
step:1869/2330 train_time:108428ms step_avg:58.01ms
step:1870/2330 train_time:108489ms step_avg:58.02ms
step:1871/2330 train_time:108547ms step_avg:58.02ms
step:1872/2330 train_time:108607ms step_avg:58.02ms
step:1873/2330 train_time:108664ms step_avg:58.02ms
step:1874/2330 train_time:108726ms step_avg:58.02ms
step:1875/2330 train_time:108785ms step_avg:58.02ms
step:1876/2330 train_time:108845ms step_avg:58.02ms
step:1877/2330 train_time:108902ms step_avg:58.02ms
step:1878/2330 train_time:108964ms step_avg:58.02ms
step:1879/2330 train_time:109021ms step_avg:58.02ms
step:1880/2330 train_time:109082ms step_avg:58.02ms
step:1881/2330 train_time:109139ms step_avg:58.02ms
step:1882/2330 train_time:109200ms step_avg:58.02ms
step:1883/2330 train_time:109256ms step_avg:58.02ms
step:1884/2330 train_time:109316ms step_avg:58.02ms
step:1885/2330 train_time:109373ms step_avg:58.02ms
step:1886/2330 train_time:109434ms step_avg:58.02ms
step:1887/2330 train_time:109490ms step_avg:58.02ms
step:1888/2330 train_time:109551ms step_avg:58.03ms
step:1889/2330 train_time:109608ms step_avg:58.02ms
step:1890/2330 train_time:109669ms step_avg:58.03ms
step:1891/2330 train_time:109728ms step_avg:58.03ms
step:1892/2330 train_time:109789ms step_avg:58.03ms
step:1893/2330 train_time:109847ms step_avg:58.03ms
step:1894/2330 train_time:109908ms step_avg:58.03ms
step:1895/2330 train_time:109966ms step_avg:58.03ms
step:1896/2330 train_time:110026ms step_avg:58.03ms
step:1897/2330 train_time:110084ms step_avg:58.03ms
step:1898/2330 train_time:110144ms step_avg:58.03ms
step:1899/2330 train_time:110201ms step_avg:58.03ms
step:1900/2330 train_time:110263ms step_avg:58.03ms
step:1901/2330 train_time:110319ms step_avg:58.03ms
step:1902/2330 train_time:110381ms step_avg:58.03ms
step:1903/2330 train_time:110437ms step_avg:58.03ms
step:1904/2330 train_time:110499ms step_avg:58.04ms
step:1905/2330 train_time:110555ms step_avg:58.03ms
step:1906/2330 train_time:110616ms step_avg:58.04ms
step:1907/2330 train_time:110672ms step_avg:58.03ms
step:1908/2330 train_time:110734ms step_avg:58.04ms
step:1909/2330 train_time:110791ms step_avg:58.04ms
step:1910/2330 train_time:110852ms step_avg:58.04ms
step:1911/2330 train_time:110910ms step_avg:58.04ms
step:1912/2330 train_time:110970ms step_avg:58.04ms
step:1913/2330 train_time:111027ms step_avg:58.04ms
step:1914/2330 train_time:111088ms step_avg:58.04ms
step:1915/2330 train_time:111145ms step_avg:58.04ms
step:1916/2330 train_time:111207ms step_avg:58.04ms
step:1917/2330 train_time:111265ms step_avg:58.04ms
step:1918/2330 train_time:111325ms step_avg:58.04ms
step:1919/2330 train_time:111382ms step_avg:58.04ms
step:1920/2330 train_time:111443ms step_avg:58.04ms
step:1921/2330 train_time:111499ms step_avg:58.04ms
step:1922/2330 train_time:111560ms step_avg:58.04ms
step:1923/2330 train_time:111616ms step_avg:58.04ms
step:1924/2330 train_time:111678ms step_avg:58.04ms
step:1925/2330 train_time:111735ms step_avg:58.04ms
step:1926/2330 train_time:111796ms step_avg:58.05ms
step:1927/2330 train_time:111852ms step_avg:58.04ms
step:1928/2330 train_time:111914ms step_avg:58.05ms
step:1929/2330 train_time:111971ms step_avg:58.05ms
step:1930/2330 train_time:112032ms step_avg:58.05ms
step:1931/2330 train_time:112090ms step_avg:58.05ms
step:1932/2330 train_time:112150ms step_avg:58.05ms
step:1933/2330 train_time:112208ms step_avg:58.05ms
step:1934/2330 train_time:112269ms step_avg:58.05ms
step:1935/2330 train_time:112326ms step_avg:58.05ms
step:1936/2330 train_time:112387ms step_avg:58.05ms
step:1937/2330 train_time:112445ms step_avg:58.05ms
step:1938/2330 train_time:112505ms step_avg:58.05ms
step:1939/2330 train_time:112563ms step_avg:58.05ms
step:1940/2330 train_time:112623ms step_avg:58.05ms
step:1941/2330 train_time:112681ms step_avg:58.05ms
step:1942/2330 train_time:112741ms step_avg:58.05ms
step:1943/2330 train_time:112798ms step_avg:58.05ms
step:1944/2330 train_time:112861ms step_avg:58.06ms
step:1945/2330 train_time:112917ms step_avg:58.05ms
step:1946/2330 train_time:112978ms step_avg:58.06ms
step:1947/2330 train_time:113035ms step_avg:58.06ms
step:1948/2330 train_time:113095ms step_avg:58.06ms
step:1949/2330 train_time:113152ms step_avg:58.06ms
step:1950/2330 train_time:113212ms step_avg:58.06ms
step:1951/2330 train_time:113269ms step_avg:58.06ms
step:1952/2330 train_time:113330ms step_avg:58.06ms
step:1953/2330 train_time:113387ms step_avg:58.06ms
step:1954/2330 train_time:113448ms step_avg:58.06ms
step:1955/2330 train_time:113506ms step_avg:58.06ms
step:1956/2330 train_time:113566ms step_avg:58.06ms
step:1957/2330 train_time:113623ms step_avg:58.06ms
step:1958/2330 train_time:113683ms step_avg:58.06ms
step:1959/2330 train_time:113741ms step_avg:58.06ms
step:1960/2330 train_time:113801ms step_avg:58.06ms
step:1961/2330 train_time:113858ms step_avg:58.06ms
step:1962/2330 train_time:113919ms step_avg:58.06ms
step:1963/2330 train_time:113976ms step_avg:58.06ms
step:1964/2330 train_time:114037ms step_avg:58.06ms
step:1965/2330 train_time:114093ms step_avg:58.06ms
step:1966/2330 train_time:114153ms step_avg:58.06ms
step:1967/2330 train_time:114210ms step_avg:58.06ms
step:1968/2330 train_time:114271ms step_avg:58.06ms
step:1969/2330 train_time:114328ms step_avg:58.06ms
step:1970/2330 train_time:114390ms step_avg:58.07ms
step:1971/2330 train_time:114447ms step_avg:58.07ms
step:1972/2330 train_time:114508ms step_avg:58.07ms
step:1973/2330 train_time:114566ms step_avg:58.07ms
step:1974/2330 train_time:114626ms step_avg:58.07ms
step:1975/2330 train_time:114685ms step_avg:58.07ms
step:1976/2330 train_time:114745ms step_avg:58.07ms
step:1977/2330 train_time:114804ms step_avg:58.07ms
step:1978/2330 train_time:114864ms step_avg:58.07ms
step:1979/2330 train_time:114921ms step_avg:58.07ms
step:1980/2330 train_time:114981ms step_avg:58.07ms
step:1981/2330 train_time:115037ms step_avg:58.07ms
step:1982/2330 train_time:115099ms step_avg:58.07ms
step:1983/2330 train_time:115155ms step_avg:58.07ms
step:1984/2330 train_time:115215ms step_avg:58.07ms
step:1985/2330 train_time:115272ms step_avg:58.07ms
step:1986/2330 train_time:115332ms step_avg:58.07ms
step:1987/2330 train_time:115389ms step_avg:58.07ms
step:1988/2330 train_time:115450ms step_avg:58.07ms
step:1989/2330 train_time:115507ms step_avg:58.07ms
step:1990/2330 train_time:115568ms step_avg:58.07ms
step:1991/2330 train_time:115626ms step_avg:58.07ms
step:1992/2330 train_time:115687ms step_avg:58.08ms
step:1993/2330 train_time:115746ms step_avg:58.08ms
step:1994/2330 train_time:115806ms step_avg:58.08ms
step:1995/2330 train_time:115864ms step_avg:58.08ms
step:1996/2330 train_time:115924ms step_avg:58.08ms
step:1997/2330 train_time:115981ms step_avg:58.08ms
step:1998/2330 train_time:116043ms step_avg:58.08ms
step:1999/2330 train_time:116099ms step_avg:58.08ms
step:2000/2330 train_time:116161ms step_avg:58.08ms
step:2000/2330 val_loss:4.3417 train_time:116242ms step_avg:58.12ms
step:2001/2330 train_time:116262ms step_avg:58.10ms
step:2002/2330 train_time:116282ms step_avg:58.08ms
step:2003/2330 train_time:116344ms step_avg:58.09ms
step:2004/2330 train_time:116409ms step_avg:58.09ms
step:2005/2330 train_time:116467ms step_avg:58.09ms
step:2006/2330 train_time:116528ms step_avg:58.09ms
step:2007/2330 train_time:116586ms step_avg:58.09ms
step:2008/2330 train_time:116645ms step_avg:58.09ms
step:2009/2330 train_time:116701ms step_avg:58.09ms
step:2010/2330 train_time:116761ms step_avg:58.09ms
step:2011/2330 train_time:116817ms step_avg:58.09ms
step:2012/2330 train_time:116879ms step_avg:58.09ms
step:2013/2330 train_time:116935ms step_avg:58.09ms
step:2014/2330 train_time:116995ms step_avg:58.09ms
step:2015/2330 train_time:117051ms step_avg:58.09ms
step:2016/2330 train_time:117112ms step_avg:58.09ms
step:2017/2330 train_time:117168ms step_avg:58.09ms
step:2018/2330 train_time:117231ms step_avg:58.09ms
step:2019/2330 train_time:117288ms step_avg:58.09ms
step:2020/2330 train_time:117351ms step_avg:58.09ms
step:2021/2330 train_time:117408ms step_avg:58.09ms
step:2022/2330 train_time:117469ms step_avg:58.10ms
step:2023/2330 train_time:117527ms step_avg:58.10ms
step:2024/2330 train_time:117588ms step_avg:58.10ms
step:2025/2330 train_time:117644ms step_avg:58.10ms
step:2026/2330 train_time:117704ms step_avg:58.10ms
step:2027/2330 train_time:117762ms step_avg:58.10ms
step:2028/2330 train_time:117821ms step_avg:58.10ms
step:2029/2330 train_time:117879ms step_avg:58.10ms
step:2030/2330 train_time:117938ms step_avg:58.10ms
step:2031/2330 train_time:117995ms step_avg:58.10ms
step:2032/2330 train_time:118055ms step_avg:58.10ms
step:2033/2330 train_time:118113ms step_avg:58.10ms
step:2034/2330 train_time:118173ms step_avg:58.10ms
step:2035/2330 train_time:118229ms step_avg:58.10ms
step:2036/2330 train_time:118291ms step_avg:58.10ms
step:2037/2330 train_time:118349ms step_avg:58.10ms
step:2038/2330 train_time:118410ms step_avg:58.10ms
step:2039/2330 train_time:118467ms step_avg:58.10ms
step:2040/2330 train_time:118527ms step_avg:58.10ms
step:2041/2330 train_time:118584ms step_avg:58.10ms
step:2042/2330 train_time:118645ms step_avg:58.10ms
step:2043/2330 train_time:118702ms step_avg:58.10ms
step:2044/2330 train_time:118762ms step_avg:58.10ms
step:2045/2330 train_time:118820ms step_avg:58.10ms
step:2046/2330 train_time:118879ms step_avg:58.10ms
step:2047/2330 train_time:118937ms step_avg:58.10ms
step:2048/2330 train_time:118996ms step_avg:58.10ms
step:2049/2330 train_time:119053ms step_avg:58.10ms
step:2050/2330 train_time:119115ms step_avg:58.10ms
step:2051/2330 train_time:119172ms step_avg:58.10ms
step:2052/2330 train_time:119232ms step_avg:58.11ms
step:2053/2330 train_time:119288ms step_avg:58.10ms
step:2054/2330 train_time:119350ms step_avg:58.11ms
step:2055/2330 train_time:119406ms step_avg:58.11ms
step:2056/2330 train_time:119468ms step_avg:58.11ms
step:2057/2330 train_time:119524ms step_avg:58.11ms
step:2058/2330 train_time:119585ms step_avg:58.11ms
step:2059/2330 train_time:119642ms step_avg:58.11ms
step:2060/2330 train_time:119703ms step_avg:58.11ms
step:2061/2330 train_time:119760ms step_avg:58.11ms
step:2062/2330 train_time:119821ms step_avg:58.11ms
step:2063/2330 train_time:119879ms step_avg:58.11ms
step:2064/2330 train_time:119939ms step_avg:58.11ms
step:2065/2330 train_time:119996ms step_avg:58.11ms
step:2066/2330 train_time:120057ms step_avg:58.11ms
step:2067/2330 train_time:120114ms step_avg:58.11ms
step:2068/2330 train_time:120176ms step_avg:58.11ms
step:2069/2330 train_time:120233ms step_avg:58.11ms
step:2070/2330 train_time:120294ms step_avg:58.11ms
step:2071/2330 train_time:120350ms step_avg:58.11ms
step:2072/2330 train_time:120412ms step_avg:58.11ms
step:2073/2330 train_time:120468ms step_avg:58.11ms
step:2074/2330 train_time:120529ms step_avg:58.11ms
step:2075/2330 train_time:120586ms step_avg:58.11ms
step:2076/2330 train_time:120647ms step_avg:58.12ms
step:2077/2330 train_time:120704ms step_avg:58.11ms
step:2078/2330 train_time:120765ms step_avg:58.12ms
step:2079/2330 train_time:120822ms step_avg:58.12ms
step:2080/2330 train_time:120883ms step_avg:58.12ms
step:2081/2330 train_time:120940ms step_avg:58.12ms
step:2082/2330 train_time:121001ms step_avg:58.12ms
step:2083/2330 train_time:121060ms step_avg:58.12ms
step:2084/2330 train_time:121120ms step_avg:58.12ms
step:2085/2330 train_time:121178ms step_avg:58.12ms
step:2086/2330 train_time:121239ms step_avg:58.12ms
step:2087/2330 train_time:121296ms step_avg:58.12ms
step:2088/2330 train_time:121359ms step_avg:58.12ms
step:2089/2330 train_time:121414ms step_avg:58.12ms
step:2090/2330 train_time:121478ms step_avg:58.12ms
step:2091/2330 train_time:121534ms step_avg:58.12ms
step:2092/2330 train_time:121596ms step_avg:58.12ms
step:2093/2330 train_time:121652ms step_avg:58.12ms
step:2094/2330 train_time:121714ms step_avg:58.12ms
step:2095/2330 train_time:121770ms step_avg:58.12ms
step:2096/2330 train_time:121831ms step_avg:58.13ms
step:2097/2330 train_time:121887ms step_avg:58.12ms
step:2098/2330 train_time:121949ms step_avg:58.13ms
step:2099/2330 train_time:122006ms step_avg:58.13ms
step:2100/2330 train_time:122066ms step_avg:58.13ms
step:2101/2330 train_time:122123ms step_avg:58.13ms
step:2102/2330 train_time:122185ms step_avg:58.13ms
step:2103/2330 train_time:122242ms step_avg:58.13ms
step:2104/2330 train_time:122304ms step_avg:58.13ms
step:2105/2330 train_time:122362ms step_avg:58.13ms
step:2106/2330 train_time:122423ms step_avg:58.13ms
step:2107/2330 train_time:122481ms step_avg:58.13ms
step:2108/2330 train_time:122542ms step_avg:58.13ms
step:2109/2330 train_time:122598ms step_avg:58.13ms
step:2110/2330 train_time:122659ms step_avg:58.13ms
step:2111/2330 train_time:122717ms step_avg:58.13ms
step:2112/2330 train_time:122777ms step_avg:58.13ms
step:2113/2330 train_time:122834ms step_avg:58.13ms
step:2114/2330 train_time:122894ms step_avg:58.13ms
step:2115/2330 train_time:122951ms step_avg:58.13ms
step:2116/2330 train_time:123011ms step_avg:58.13ms
step:2117/2330 train_time:123067ms step_avg:58.13ms
step:2118/2330 train_time:123128ms step_avg:58.13ms
step:2119/2330 train_time:123185ms step_avg:58.13ms
step:2120/2330 train_time:123247ms step_avg:58.14ms
step:2121/2330 train_time:123304ms step_avg:58.13ms
step:2122/2330 train_time:123365ms step_avg:58.14ms
step:2123/2330 train_time:123424ms step_avg:58.14ms
step:2124/2330 train_time:123483ms step_avg:58.14ms
step:2125/2330 train_time:123541ms step_avg:58.14ms
step:2126/2330 train_time:123601ms step_avg:58.14ms
step:2127/2330 train_time:123658ms step_avg:58.14ms
step:2128/2330 train_time:123720ms step_avg:58.14ms
step:2129/2330 train_time:123777ms step_avg:58.14ms
step:2130/2330 train_time:123838ms step_avg:58.14ms
step:2131/2330 train_time:123895ms step_avg:58.14ms
step:2132/2330 train_time:123958ms step_avg:58.14ms
step:2133/2330 train_time:124014ms step_avg:58.14ms
step:2134/2330 train_time:124075ms step_avg:58.14ms
step:2135/2330 train_time:124132ms step_avg:58.14ms
step:2136/2330 train_time:124193ms step_avg:58.14ms
step:2137/2330 train_time:124249ms step_avg:58.14ms
step:2138/2330 train_time:124310ms step_avg:58.14ms
step:2139/2330 train_time:124367ms step_avg:58.14ms
step:2140/2330 train_time:124429ms step_avg:58.14ms
step:2141/2330 train_time:124486ms step_avg:58.14ms
step:2142/2330 train_time:124547ms step_avg:58.15ms
step:2143/2330 train_time:124605ms step_avg:58.14ms
step:2144/2330 train_time:124664ms step_avg:58.15ms
step:2145/2330 train_time:124721ms step_avg:58.15ms
step:2146/2330 train_time:124783ms step_avg:58.15ms
step:2147/2330 train_time:124840ms step_avg:58.15ms
step:2148/2330 train_time:124901ms step_avg:58.15ms
step:2149/2330 train_time:124959ms step_avg:58.15ms
step:2150/2330 train_time:125020ms step_avg:58.15ms
step:2151/2330 train_time:125077ms step_avg:58.15ms
step:2152/2330 train_time:125137ms step_avg:58.15ms
step:2153/2330 train_time:125193ms step_avg:58.15ms
step:2154/2330 train_time:125256ms step_avg:58.15ms
step:2155/2330 train_time:125312ms step_avg:58.15ms
step:2156/2330 train_time:125373ms step_avg:58.15ms
step:2157/2330 train_time:125429ms step_avg:58.15ms
step:2158/2330 train_time:125490ms step_avg:58.15ms
step:2159/2330 train_time:125546ms step_avg:58.15ms
step:2160/2330 train_time:125608ms step_avg:58.15ms
step:2161/2330 train_time:125665ms step_avg:58.15ms
step:2162/2330 train_time:125726ms step_avg:58.15ms
step:2163/2330 train_time:125783ms step_avg:58.15ms
step:2164/2330 train_time:125844ms step_avg:58.15ms
step:2165/2330 train_time:125901ms step_avg:58.15ms
step:2166/2330 train_time:125961ms step_avg:58.15ms
step:2167/2330 train_time:126020ms step_avg:58.15ms
step:2168/2330 train_time:126079ms step_avg:58.15ms
step:2169/2330 train_time:126136ms step_avg:58.15ms
step:2170/2330 train_time:126198ms step_avg:58.16ms
step:2171/2330 train_time:126256ms step_avg:58.16ms
step:2172/2330 train_time:126318ms step_avg:58.16ms
step:2173/2330 train_time:126374ms step_avg:58.16ms
step:2174/2330 train_time:126437ms step_avg:58.16ms
step:2175/2330 train_time:126494ms step_avg:58.16ms
step:2176/2330 train_time:126556ms step_avg:58.16ms
step:2177/2330 train_time:126612ms step_avg:58.16ms
step:2178/2330 train_time:126672ms step_avg:58.16ms
step:2179/2330 train_time:126728ms step_avg:58.16ms
step:2180/2330 train_time:126791ms step_avg:58.16ms
step:2181/2330 train_time:126847ms step_avg:58.16ms
step:2182/2330 train_time:126909ms step_avg:58.16ms
step:2183/2330 train_time:126965ms step_avg:58.16ms
step:2184/2330 train_time:127027ms step_avg:58.16ms
step:2185/2330 train_time:127085ms step_avg:58.16ms
step:2186/2330 train_time:127145ms step_avg:58.16ms
step:2187/2330 train_time:127203ms step_avg:58.16ms
step:2188/2330 train_time:127264ms step_avg:58.16ms
step:2189/2330 train_time:127322ms step_avg:58.16ms
step:2190/2330 train_time:127383ms step_avg:58.17ms
step:2191/2330 train_time:127440ms step_avg:58.17ms
step:2192/2330 train_time:127501ms step_avg:58.17ms
step:2193/2330 train_time:127558ms step_avg:58.17ms
step:2194/2330 train_time:127620ms step_avg:58.17ms
step:2195/2330 train_time:127675ms step_avg:58.17ms
step:2196/2330 train_time:127737ms step_avg:58.17ms
step:2197/2330 train_time:127793ms step_avg:58.17ms
step:2198/2330 train_time:127855ms step_avg:58.17ms
step:2199/2330 train_time:127912ms step_avg:58.17ms
step:2200/2330 train_time:127973ms step_avg:58.17ms
step:2201/2330 train_time:128029ms step_avg:58.17ms
step:2202/2330 train_time:128090ms step_avg:58.17ms
step:2203/2330 train_time:128146ms step_avg:58.17ms
step:2204/2330 train_time:128207ms step_avg:58.17ms
step:2205/2330 train_time:128265ms step_avg:58.17ms
step:2206/2330 train_time:128326ms step_avg:58.17ms
step:2207/2330 train_time:128383ms step_avg:58.17ms
step:2208/2330 train_time:128444ms step_avg:58.17ms
step:2209/2330 train_time:128501ms step_avg:58.17ms
step:2210/2330 train_time:128562ms step_avg:58.17ms
step:2211/2330 train_time:128621ms step_avg:58.17ms
step:2212/2330 train_time:128681ms step_avg:58.17ms
step:2213/2330 train_time:128737ms step_avg:58.17ms
step:2214/2330 train_time:128799ms step_avg:58.17ms
step:2215/2330 train_time:128855ms step_avg:58.17ms
step:2216/2330 train_time:128917ms step_avg:58.18ms
step:2217/2330 train_time:128974ms step_avg:58.17ms
step:2218/2330 train_time:129035ms step_avg:58.18ms
step:2219/2330 train_time:129091ms step_avg:58.18ms
step:2220/2330 train_time:129153ms step_avg:58.18ms
step:2221/2330 train_time:129209ms step_avg:58.18ms
step:2222/2330 train_time:129271ms step_avg:58.18ms
step:2223/2330 train_time:129327ms step_avg:58.18ms
step:2224/2330 train_time:129388ms step_avg:58.18ms
step:2225/2330 train_time:129444ms step_avg:58.18ms
step:2226/2330 train_time:129506ms step_avg:58.18ms
step:2227/2330 train_time:129564ms step_avg:58.18ms
step:2228/2330 train_time:129625ms step_avg:58.18ms
step:2229/2330 train_time:129684ms step_avg:58.18ms
step:2230/2330 train_time:129743ms step_avg:58.18ms
step:2231/2330 train_time:129802ms step_avg:58.18ms
step:2232/2330 train_time:129862ms step_avg:58.18ms
step:2233/2330 train_time:129919ms step_avg:58.18ms
step:2234/2330 train_time:129981ms step_avg:58.18ms
step:2235/2330 train_time:130038ms step_avg:58.18ms
step:2236/2330 train_time:130099ms step_avg:58.18ms
step:2237/2330 train_time:130155ms step_avg:58.18ms
step:2238/2330 train_time:130217ms step_avg:58.18ms
step:2239/2330 train_time:130273ms step_avg:58.18ms
step:2240/2330 train_time:130335ms step_avg:58.19ms
step:2241/2330 train_time:130391ms step_avg:58.18ms
step:2242/2330 train_time:130453ms step_avg:58.19ms
step:2243/2330 train_time:130509ms step_avg:58.19ms
step:2244/2330 train_time:130569ms step_avg:58.19ms
step:2245/2330 train_time:130626ms step_avg:58.19ms
step:2246/2330 train_time:130688ms step_avg:58.19ms
step:2247/2330 train_time:130745ms step_avg:58.19ms
step:2248/2330 train_time:130807ms step_avg:58.19ms
step:2249/2330 train_time:130864ms step_avg:58.19ms
step:2250/2330 train_time:130925ms step_avg:58.19ms
step:2250/2330 val_loss:4.2743 train_time:131007ms step_avg:58.23ms
step:2251/2330 train_time:131027ms step_avg:58.21ms
step:2252/2330 train_time:131048ms step_avg:58.19ms
step:2253/2330 train_time:131109ms step_avg:58.19ms
step:2254/2330 train_time:131171ms step_avg:58.19ms
step:2255/2330 train_time:131228ms step_avg:58.19ms
step:2256/2330 train_time:131292ms step_avg:58.20ms
step:2257/2330 train_time:131349ms step_avg:58.20ms
step:2258/2330 train_time:131409ms step_avg:58.20ms
step:2259/2330 train_time:131465ms step_avg:58.20ms
step:2260/2330 train_time:131526ms step_avg:58.20ms
step:2261/2330 train_time:131583ms step_avg:58.20ms
step:2262/2330 train_time:131643ms step_avg:58.20ms
step:2263/2330 train_time:131699ms step_avg:58.20ms
step:2264/2330 train_time:131759ms step_avg:58.20ms
step:2265/2330 train_time:131815ms step_avg:58.20ms
step:2266/2330 train_time:131875ms step_avg:58.20ms
step:2267/2330 train_time:131932ms step_avg:58.20ms
step:2268/2330 train_time:131994ms step_avg:58.20ms
step:2269/2330 train_time:132053ms step_avg:58.20ms
step:2270/2330 train_time:132114ms step_avg:58.20ms
step:2271/2330 train_time:132172ms step_avg:58.20ms
step:2272/2330 train_time:132234ms step_avg:58.20ms
step:2273/2330 train_time:132291ms step_avg:58.20ms
step:2274/2330 train_time:132350ms step_avg:58.20ms
step:2275/2330 train_time:132408ms step_avg:58.20ms
step:2276/2330 train_time:132468ms step_avg:58.20ms
step:2277/2330 train_time:132524ms step_avg:58.20ms
step:2278/2330 train_time:132584ms step_avg:58.20ms
step:2279/2330 train_time:132641ms step_avg:58.20ms
step:2280/2330 train_time:132701ms step_avg:58.20ms
step:2281/2330 train_time:132756ms step_avg:58.20ms
step:2282/2330 train_time:132818ms step_avg:58.20ms
step:2283/2330 train_time:132874ms step_avg:58.20ms
step:2284/2330 train_time:132935ms step_avg:58.20ms
step:2285/2330 train_time:132992ms step_avg:58.20ms
step:2286/2330 train_time:133053ms step_avg:58.20ms
step:2287/2330 train_time:133111ms step_avg:58.20ms
step:2288/2330 train_time:133173ms step_avg:58.20ms
step:2289/2330 train_time:133230ms step_avg:58.20ms
step:2290/2330 train_time:133291ms step_avg:58.21ms
step:2291/2330 train_time:133349ms step_avg:58.21ms
step:2292/2330 train_time:133410ms step_avg:58.21ms
step:2293/2330 train_time:133466ms step_avg:58.21ms
step:2294/2330 train_time:133527ms step_avg:58.21ms
step:2295/2330 train_time:133584ms step_avg:58.21ms
step:2296/2330 train_time:133644ms step_avg:58.21ms
step:2297/2330 train_time:133701ms step_avg:58.21ms
step:2298/2330 train_time:133761ms step_avg:58.21ms
step:2299/2330 train_time:133817ms step_avg:58.21ms
step:2300/2330 train_time:133877ms step_avg:58.21ms
step:2301/2330 train_time:133934ms step_avg:58.21ms
step:2302/2330 train_time:133995ms step_avg:58.21ms
step:2303/2330 train_time:134052ms step_avg:58.21ms
step:2304/2330 train_time:134113ms step_avg:58.21ms
step:2305/2330 train_time:134170ms step_avg:58.21ms
step:2306/2330 train_time:134232ms step_avg:58.21ms
step:2307/2330 train_time:134289ms step_avg:58.21ms
step:2308/2330 train_time:134350ms step_avg:58.21ms
step:2309/2330 train_time:134408ms step_avg:58.21ms
step:2310/2330 train_time:134469ms step_avg:58.21ms
step:2311/2330 train_time:134526ms step_avg:58.21ms
step:2312/2330 train_time:134586ms step_avg:58.21ms
step:2313/2330 train_time:134643ms step_avg:58.21ms
step:2314/2330 train_time:134703ms step_avg:58.21ms
step:2315/2330 train_time:134760ms step_avg:58.21ms
step:2316/2330 train_time:134821ms step_avg:58.21ms
step:2317/2330 train_time:134878ms step_avg:58.21ms
step:2318/2330 train_time:134938ms step_avg:58.21ms
step:2319/2330 train_time:134995ms step_avg:58.21ms
step:2320/2330 train_time:135056ms step_avg:58.21ms
step:2321/2330 train_time:135113ms step_avg:58.21ms
step:2322/2330 train_time:135175ms step_avg:58.21ms
step:2323/2330 train_time:135231ms step_avg:58.21ms
step:2324/2330 train_time:135292ms step_avg:58.22ms
step:2325/2330 train_time:135349ms step_avg:58.21ms
step:2326/2330 train_time:135410ms step_avg:58.22ms
step:2327/2330 train_time:135467ms step_avg:58.22ms
step:2328/2330 train_time:135529ms step_avg:58.22ms
step:2329/2330 train_time:135586ms step_avg:58.22ms
step:2330/2330 train_time:135647ms step_avg:58.22ms
step:2330/2330 val_loss:4.2567 train_time:135729ms step_avg:58.25ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
