import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--adamw_lr', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_lr_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_lr_tuning/{args2.adamw_lr}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=float(args2.adamw_lr),  betas=(0.85, 0.85), eps=1e-10, weight_decay=0.0, fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_lr_tuning/{args2.adamw_lr}", exist_ok=True)
            torch.save(log, f"logs_adamw_small_lr_tuning/{args2.adamw_lr}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 02:58:14 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   29C    P0             114W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   26C    P0             115W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   25C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   27C    P0             114W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   25C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   26C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.05ms
step:1/2330 train_time:88ms step_avg:88.14ms
step:2/2330 train_time:181ms step_avg:90.45ms
step:3/2330 train_time:200ms step_avg:66.55ms
step:4/2330 train_time:219ms step_avg:54.83ms
step:5/2330 train_time:272ms step_avg:54.36ms
step:6/2330 train_time:329ms step_avg:54.88ms
step:7/2330 train_time:383ms step_avg:54.72ms
step:8/2330 train_time:440ms step_avg:55.00ms
step:9/2330 train_time:494ms step_avg:54.86ms
step:10/2330 train_time:551ms step_avg:55.08ms
step:11/2330 train_time:605ms step_avg:54.96ms
step:12/2330 train_time:662ms step_avg:55.14ms
step:13/2330 train_time:715ms step_avg:55.04ms
step:14/2330 train_time:772ms step_avg:55.17ms
step:15/2330 train_time:826ms step_avg:55.10ms
step:16/2330 train_time:883ms step_avg:55.19ms
step:17/2330 train_time:937ms step_avg:55.12ms
step:18/2330 train_time:994ms step_avg:55.20ms
step:19/2330 train_time:1048ms step_avg:55.16ms
step:20/2330 train_time:1106ms step_avg:55.32ms
step:21/2330 train_time:1161ms step_avg:55.31ms
step:22/2330 train_time:1219ms step_avg:55.42ms
step:23/2330 train_time:1274ms step_avg:55.38ms
step:24/2330 train_time:1331ms step_avg:55.47ms
step:25/2330 train_time:1385ms step_avg:55.42ms
step:26/2330 train_time:1442ms step_avg:55.48ms
step:27/2330 train_time:1497ms step_avg:55.44ms
step:28/2330 train_time:1554ms step_avg:55.51ms
step:29/2330 train_time:1609ms step_avg:55.48ms
step:30/2330 train_time:1666ms step_avg:55.53ms
step:31/2330 train_time:1720ms step_avg:55.50ms
step:32/2330 train_time:1777ms step_avg:55.54ms
step:33/2330 train_time:1832ms step_avg:55.50ms
step:34/2330 train_time:1888ms step_avg:55.54ms
step:35/2330 train_time:1942ms step_avg:55.49ms
step:36/2330 train_time:2000ms step_avg:55.54ms
step:37/2330 train_time:2054ms step_avg:55.51ms
step:38/2330 train_time:2111ms step_avg:55.56ms
step:39/2330 train_time:2166ms step_avg:55.53ms
step:40/2330 train_time:2223ms step_avg:55.57ms
step:41/2330 train_time:2278ms step_avg:55.56ms
step:42/2330 train_time:2336ms step_avg:55.61ms
step:43/2330 train_time:2390ms step_avg:55.59ms
step:44/2330 train_time:2448ms step_avg:55.64ms
step:45/2330 train_time:2503ms step_avg:55.62ms
step:46/2330 train_time:2561ms step_avg:55.67ms
step:47/2330 train_time:2615ms step_avg:55.65ms
step:48/2330 train_time:2672ms step_avg:55.67ms
step:49/2330 train_time:2727ms step_avg:55.65ms
step:50/2330 train_time:2785ms step_avg:55.70ms
step:51/2330 train_time:2839ms step_avg:55.67ms
step:52/2330 train_time:2897ms step_avg:55.71ms
step:53/2330 train_time:2951ms step_avg:55.68ms
step:54/2330 train_time:3008ms step_avg:55.71ms
step:55/2330 train_time:3062ms step_avg:55.68ms
step:56/2330 train_time:3120ms step_avg:55.72ms
step:57/2330 train_time:3174ms step_avg:55.69ms
step:58/2330 train_time:3233ms step_avg:55.73ms
step:59/2330 train_time:3287ms step_avg:55.71ms
step:60/2330 train_time:3345ms step_avg:55.75ms
step:61/2330 train_time:3400ms step_avg:55.74ms
step:62/2330 train_time:3458ms step_avg:55.78ms
step:63/2330 train_time:3513ms step_avg:55.76ms
step:64/2330 train_time:3572ms step_avg:55.81ms
step:65/2330 train_time:3626ms step_avg:55.79ms
step:66/2330 train_time:3685ms step_avg:55.83ms
step:67/2330 train_time:3740ms step_avg:55.82ms
step:68/2330 train_time:3797ms step_avg:55.84ms
step:69/2330 train_time:3853ms step_avg:55.83ms
step:70/2330 train_time:3910ms step_avg:55.86ms
step:71/2330 train_time:3965ms step_avg:55.85ms
step:72/2330 train_time:4022ms step_avg:55.87ms
step:73/2330 train_time:4077ms step_avg:55.85ms
step:74/2330 train_time:4135ms step_avg:55.88ms
step:75/2330 train_time:4190ms step_avg:55.87ms
step:76/2330 train_time:4249ms step_avg:55.91ms
step:77/2330 train_time:4304ms step_avg:55.89ms
step:78/2330 train_time:4361ms step_avg:55.92ms
step:79/2330 train_time:4416ms step_avg:55.90ms
step:80/2330 train_time:4475ms step_avg:55.94ms
step:81/2330 train_time:4530ms step_avg:55.93ms
step:82/2330 train_time:4589ms step_avg:55.96ms
step:83/2330 train_time:4644ms step_avg:55.95ms
step:84/2330 train_time:4702ms step_avg:55.98ms
step:85/2330 train_time:4758ms step_avg:55.97ms
step:86/2330 train_time:4816ms step_avg:56.00ms
step:87/2330 train_time:4871ms step_avg:55.99ms
step:88/2330 train_time:4929ms step_avg:56.01ms
step:89/2330 train_time:4983ms step_avg:55.99ms
step:90/2330 train_time:5041ms step_avg:56.01ms
step:91/2330 train_time:5096ms step_avg:56.00ms
step:92/2330 train_time:5154ms step_avg:56.02ms
step:93/2330 train_time:5209ms step_avg:56.01ms
step:94/2330 train_time:5268ms step_avg:56.04ms
step:95/2330 train_time:5323ms step_avg:56.03ms
step:96/2330 train_time:5380ms step_avg:56.04ms
step:97/2330 train_time:5435ms step_avg:56.03ms
step:98/2330 train_time:5493ms step_avg:56.05ms
step:99/2330 train_time:5549ms step_avg:56.05ms
step:100/2330 train_time:5606ms step_avg:56.06ms
step:101/2330 train_time:5662ms step_avg:56.06ms
step:102/2330 train_time:5719ms step_avg:56.07ms
step:103/2330 train_time:5775ms step_avg:56.06ms
step:104/2330 train_time:5833ms step_avg:56.08ms
step:105/2330 train_time:5888ms step_avg:56.08ms
step:106/2330 train_time:5946ms step_avg:56.10ms
step:107/2330 train_time:6002ms step_avg:56.09ms
step:108/2330 train_time:6060ms step_avg:56.11ms
step:109/2330 train_time:6116ms step_avg:56.11ms
step:110/2330 train_time:6173ms step_avg:56.12ms
step:111/2330 train_time:6229ms step_avg:56.11ms
step:112/2330 train_time:6287ms step_avg:56.13ms
step:113/2330 train_time:6342ms step_avg:56.12ms
step:114/2330 train_time:6400ms step_avg:56.14ms
step:115/2330 train_time:6455ms step_avg:56.13ms
step:116/2330 train_time:6513ms step_avg:56.15ms
step:117/2330 train_time:6569ms step_avg:56.15ms
step:118/2330 train_time:6627ms step_avg:56.16ms
step:119/2330 train_time:6682ms step_avg:56.15ms
step:120/2330 train_time:6740ms step_avg:56.17ms
step:121/2330 train_time:6795ms step_avg:56.15ms
step:122/2330 train_time:6853ms step_avg:56.17ms
step:123/2330 train_time:6908ms step_avg:56.17ms
step:124/2330 train_time:6966ms step_avg:56.18ms
step:125/2330 train_time:7022ms step_avg:56.17ms
step:126/2330 train_time:7080ms step_avg:56.19ms
step:127/2330 train_time:7135ms step_avg:56.18ms
step:128/2330 train_time:7194ms step_avg:56.21ms
step:129/2330 train_time:7250ms step_avg:56.20ms
step:130/2330 train_time:7309ms step_avg:56.22ms
step:131/2330 train_time:7364ms step_avg:56.22ms
step:132/2330 train_time:7422ms step_avg:56.23ms
step:133/2330 train_time:7477ms step_avg:56.22ms
step:134/2330 train_time:7535ms step_avg:56.23ms
step:135/2330 train_time:7591ms step_avg:56.23ms
step:136/2330 train_time:7649ms step_avg:56.24ms
step:137/2330 train_time:7704ms step_avg:56.23ms
step:138/2330 train_time:7762ms step_avg:56.25ms
step:139/2330 train_time:7817ms step_avg:56.24ms
step:140/2330 train_time:7876ms step_avg:56.26ms
step:141/2330 train_time:7932ms step_avg:56.25ms
step:142/2330 train_time:7990ms step_avg:56.26ms
step:143/2330 train_time:8045ms step_avg:56.26ms
step:144/2330 train_time:8102ms step_avg:56.26ms
step:145/2330 train_time:8157ms step_avg:56.25ms
step:146/2330 train_time:8215ms step_avg:56.27ms
step:147/2330 train_time:8271ms step_avg:56.27ms
step:148/2330 train_time:8329ms step_avg:56.28ms
step:149/2330 train_time:8385ms step_avg:56.28ms
step:150/2330 train_time:8443ms step_avg:56.29ms
step:151/2330 train_time:8498ms step_avg:56.28ms
step:152/2330 train_time:8557ms step_avg:56.29ms
step:153/2330 train_time:8612ms step_avg:56.29ms
step:154/2330 train_time:8670ms step_avg:56.30ms
step:155/2330 train_time:8726ms step_avg:56.29ms
step:156/2330 train_time:8783ms step_avg:56.30ms
step:157/2330 train_time:8838ms step_avg:56.29ms
step:158/2330 train_time:8897ms step_avg:56.31ms
step:159/2330 train_time:8952ms step_avg:56.30ms
step:160/2330 train_time:9011ms step_avg:56.32ms
step:161/2330 train_time:9066ms step_avg:56.31ms
step:162/2330 train_time:9124ms step_avg:56.32ms
step:163/2330 train_time:9180ms step_avg:56.32ms
step:164/2330 train_time:9238ms step_avg:56.33ms
step:165/2330 train_time:9294ms step_avg:56.33ms
step:166/2330 train_time:9352ms step_avg:56.34ms
step:167/2330 train_time:9408ms step_avg:56.34ms
step:168/2330 train_time:9467ms step_avg:56.35ms
step:169/2330 train_time:9522ms step_avg:56.34ms
step:170/2330 train_time:9580ms step_avg:56.35ms
step:171/2330 train_time:9635ms step_avg:56.35ms
step:172/2330 train_time:9694ms step_avg:56.36ms
step:173/2330 train_time:9750ms step_avg:56.36ms
step:174/2330 train_time:9809ms step_avg:56.37ms
step:175/2330 train_time:9863ms step_avg:56.36ms
step:176/2330 train_time:9922ms step_avg:56.37ms
step:177/2330 train_time:9977ms step_avg:56.37ms
step:178/2330 train_time:10035ms step_avg:56.38ms
step:179/2330 train_time:10091ms step_avg:56.37ms
step:180/2330 train_time:10148ms step_avg:56.38ms
step:181/2330 train_time:10204ms step_avg:56.37ms
step:182/2330 train_time:10262ms step_avg:56.39ms
step:183/2330 train_time:10317ms step_avg:56.38ms
step:184/2330 train_time:10376ms step_avg:56.39ms
step:185/2330 train_time:10433ms step_avg:56.39ms
step:186/2330 train_time:10491ms step_avg:56.40ms
step:187/2330 train_time:10547ms step_avg:56.40ms
step:188/2330 train_time:10605ms step_avg:56.41ms
step:189/2330 train_time:10661ms step_avg:56.41ms
step:190/2330 train_time:10720ms step_avg:56.42ms
step:191/2330 train_time:10776ms step_avg:56.42ms
step:192/2330 train_time:10835ms step_avg:56.43ms
step:193/2330 train_time:10891ms step_avg:56.43ms
step:194/2330 train_time:10949ms step_avg:56.44ms
step:195/2330 train_time:11004ms step_avg:56.43ms
step:196/2330 train_time:11063ms step_avg:56.44ms
step:197/2330 train_time:11118ms step_avg:56.44ms
step:198/2330 train_time:11177ms step_avg:56.45ms
step:199/2330 train_time:11233ms step_avg:56.45ms
step:200/2330 train_time:11292ms step_avg:56.46ms
step:201/2330 train_time:11347ms step_avg:56.45ms
step:202/2330 train_time:11406ms step_avg:56.47ms
step:203/2330 train_time:11462ms step_avg:56.47ms
step:204/2330 train_time:11521ms step_avg:56.47ms
step:205/2330 train_time:11576ms step_avg:56.47ms
step:206/2330 train_time:11636ms step_avg:56.48ms
step:207/2330 train_time:11692ms step_avg:56.48ms
step:208/2330 train_time:11751ms step_avg:56.49ms
step:209/2330 train_time:11807ms step_avg:56.49ms
step:210/2330 train_time:11866ms step_avg:56.50ms
step:211/2330 train_time:11922ms step_avg:56.50ms
step:212/2330 train_time:11980ms step_avg:56.51ms
step:213/2330 train_time:12036ms step_avg:56.51ms
step:214/2330 train_time:12094ms step_avg:56.51ms
step:215/2330 train_time:12150ms step_avg:56.51ms
step:216/2330 train_time:12209ms step_avg:56.52ms
step:217/2330 train_time:12265ms step_avg:56.52ms
step:218/2330 train_time:12324ms step_avg:56.53ms
step:219/2330 train_time:12379ms step_avg:56.53ms
step:220/2330 train_time:12438ms step_avg:56.54ms
step:221/2330 train_time:12494ms step_avg:56.53ms
step:222/2330 train_time:12552ms step_avg:56.54ms
step:223/2330 train_time:12608ms step_avg:56.54ms
step:224/2330 train_time:12666ms step_avg:56.54ms
step:225/2330 train_time:12721ms step_avg:56.54ms
step:226/2330 train_time:12780ms step_avg:56.55ms
step:227/2330 train_time:12836ms step_avg:56.55ms
step:228/2330 train_time:12896ms step_avg:56.56ms
step:229/2330 train_time:12952ms step_avg:56.56ms
step:230/2330 train_time:13011ms step_avg:56.57ms
step:231/2330 train_time:13067ms step_avg:56.57ms
step:232/2330 train_time:13127ms step_avg:56.58ms
step:233/2330 train_time:13182ms step_avg:56.57ms
step:234/2330 train_time:13240ms step_avg:56.58ms
step:235/2330 train_time:13296ms step_avg:56.58ms
step:236/2330 train_time:13356ms step_avg:56.59ms
step:237/2330 train_time:13412ms step_avg:56.59ms
step:238/2330 train_time:13471ms step_avg:56.60ms
step:239/2330 train_time:13526ms step_avg:56.60ms
step:240/2330 train_time:13585ms step_avg:56.60ms
step:241/2330 train_time:13641ms step_avg:56.60ms
step:242/2330 train_time:13699ms step_avg:56.61ms
step:243/2330 train_time:13755ms step_avg:56.60ms
step:244/2330 train_time:13814ms step_avg:56.61ms
step:245/2330 train_time:13870ms step_avg:56.61ms
step:246/2330 train_time:13929ms step_avg:56.62ms
step:247/2330 train_time:13985ms step_avg:56.62ms
step:248/2330 train_time:14045ms step_avg:56.63ms
step:249/2330 train_time:14101ms step_avg:56.63ms
step:250/2330 train_time:14159ms step_avg:56.64ms
step:250/2330 val_loss:6.4885 train_time:14238ms step_avg:56.95ms
step:251/2330 train_time:14257ms step_avg:56.80ms
step:252/2330 train_time:14276ms step_avg:56.65ms
step:253/2330 train_time:14330ms step_avg:56.64ms
step:254/2330 train_time:14395ms step_avg:56.67ms
step:255/2330 train_time:14449ms step_avg:56.66ms
step:256/2330 train_time:14512ms step_avg:56.69ms
step:257/2330 train_time:14568ms step_avg:56.68ms
step:258/2330 train_time:14628ms step_avg:56.70ms
step:259/2330 train_time:14683ms step_avg:56.69ms
step:260/2330 train_time:14741ms step_avg:56.70ms
step:261/2330 train_time:14796ms step_avg:56.69ms
step:262/2330 train_time:14855ms step_avg:56.70ms
step:263/2330 train_time:14909ms step_avg:56.69ms
step:264/2330 train_time:14969ms step_avg:56.70ms
step:265/2330 train_time:15024ms step_avg:56.69ms
step:266/2330 train_time:15082ms step_avg:56.70ms
step:267/2330 train_time:15138ms step_avg:56.69ms
step:268/2330 train_time:15197ms step_avg:56.70ms
step:269/2330 train_time:15253ms step_avg:56.70ms
step:270/2330 train_time:15313ms step_avg:56.72ms
step:271/2330 train_time:15370ms step_avg:56.71ms
step:272/2330 train_time:15429ms step_avg:56.72ms
step:273/2330 train_time:15485ms step_avg:56.72ms
step:274/2330 train_time:15544ms step_avg:56.73ms
step:275/2330 train_time:15600ms step_avg:56.73ms
step:276/2330 train_time:15660ms step_avg:56.74ms
step:277/2330 train_time:15715ms step_avg:56.73ms
step:278/2330 train_time:15774ms step_avg:56.74ms
step:279/2330 train_time:15830ms step_avg:56.74ms
step:280/2330 train_time:15888ms step_avg:56.74ms
step:281/2330 train_time:15943ms step_avg:56.74ms
step:282/2330 train_time:16001ms step_avg:56.74ms
step:283/2330 train_time:16057ms step_avg:56.74ms
step:284/2330 train_time:16115ms step_avg:56.74ms
step:285/2330 train_time:16171ms step_avg:56.74ms
step:286/2330 train_time:16230ms step_avg:56.75ms
step:287/2330 train_time:16285ms step_avg:56.74ms
step:288/2330 train_time:16345ms step_avg:56.75ms
step:289/2330 train_time:16402ms step_avg:56.75ms
step:290/2330 train_time:16461ms step_avg:56.76ms
step:291/2330 train_time:16517ms step_avg:56.76ms
step:292/2330 train_time:16577ms step_avg:56.77ms
step:293/2330 train_time:16632ms step_avg:56.77ms
step:294/2330 train_time:16691ms step_avg:56.77ms
step:295/2330 train_time:16747ms step_avg:56.77ms
step:296/2330 train_time:16806ms step_avg:56.78ms
step:297/2330 train_time:16862ms step_avg:56.77ms
step:298/2330 train_time:16921ms step_avg:56.78ms
step:299/2330 train_time:16976ms step_avg:56.78ms
step:300/2330 train_time:17035ms step_avg:56.78ms
step:301/2330 train_time:17091ms step_avg:56.78ms
step:302/2330 train_time:17149ms step_avg:56.78ms
step:303/2330 train_time:17204ms step_avg:56.78ms
step:304/2330 train_time:17264ms step_avg:56.79ms
step:305/2330 train_time:17320ms step_avg:56.79ms
step:306/2330 train_time:17380ms step_avg:56.80ms
step:307/2330 train_time:17436ms step_avg:56.79ms
step:308/2330 train_time:17496ms step_avg:56.81ms
step:309/2330 train_time:17551ms step_avg:56.80ms
step:310/2330 train_time:17611ms step_avg:56.81ms
step:311/2330 train_time:17667ms step_avg:56.81ms
step:312/2330 train_time:17726ms step_avg:56.82ms
step:313/2330 train_time:17782ms step_avg:56.81ms
step:314/2330 train_time:17841ms step_avg:56.82ms
step:315/2330 train_time:17896ms step_avg:56.81ms
step:316/2330 train_time:17955ms step_avg:56.82ms
step:317/2330 train_time:18011ms step_avg:56.82ms
step:318/2330 train_time:18069ms step_avg:56.82ms
step:319/2330 train_time:18125ms step_avg:56.82ms
step:320/2330 train_time:18183ms step_avg:56.82ms
step:321/2330 train_time:18239ms step_avg:56.82ms
step:322/2330 train_time:18298ms step_avg:56.83ms
step:323/2330 train_time:18354ms step_avg:56.82ms
step:324/2330 train_time:18413ms step_avg:56.83ms
step:325/2330 train_time:18469ms step_avg:56.83ms
step:326/2330 train_time:18528ms step_avg:56.83ms
step:327/2330 train_time:18584ms step_avg:56.83ms
step:328/2330 train_time:18643ms step_avg:56.84ms
step:329/2330 train_time:18698ms step_avg:56.83ms
step:330/2330 train_time:18758ms step_avg:56.84ms
step:331/2330 train_time:18814ms step_avg:56.84ms
step:332/2330 train_time:18872ms step_avg:56.84ms
step:333/2330 train_time:18928ms step_avg:56.84ms
step:334/2330 train_time:18987ms step_avg:56.85ms
step:335/2330 train_time:19043ms step_avg:56.85ms
step:336/2330 train_time:19101ms step_avg:56.85ms
step:337/2330 train_time:19158ms step_avg:56.85ms
step:338/2330 train_time:19216ms step_avg:56.85ms
step:339/2330 train_time:19272ms step_avg:56.85ms
step:340/2330 train_time:19331ms step_avg:56.86ms
step:341/2330 train_time:19386ms step_avg:56.85ms
step:342/2330 train_time:19446ms step_avg:56.86ms
step:343/2330 train_time:19503ms step_avg:56.86ms
step:344/2330 train_time:19562ms step_avg:56.87ms
step:345/2330 train_time:19619ms step_avg:56.87ms
step:346/2330 train_time:19678ms step_avg:56.87ms
step:347/2330 train_time:19734ms step_avg:56.87ms
step:348/2330 train_time:19792ms step_avg:56.87ms
step:349/2330 train_time:19848ms step_avg:56.87ms
step:350/2330 train_time:19907ms step_avg:56.88ms
step:351/2330 train_time:19963ms step_avg:56.88ms
step:352/2330 train_time:20022ms step_avg:56.88ms
step:353/2330 train_time:20078ms step_avg:56.88ms
step:354/2330 train_time:20136ms step_avg:56.88ms
step:355/2330 train_time:20193ms step_avg:56.88ms
step:356/2330 train_time:20251ms step_avg:56.88ms
step:357/2330 train_time:20307ms step_avg:56.88ms
step:358/2330 train_time:20365ms step_avg:56.89ms
step:359/2330 train_time:20421ms step_avg:56.88ms
step:360/2330 train_time:20480ms step_avg:56.89ms
step:361/2330 train_time:20536ms step_avg:56.89ms
step:362/2330 train_time:20595ms step_avg:56.89ms
step:363/2330 train_time:20651ms step_avg:56.89ms
step:364/2330 train_time:20710ms step_avg:56.90ms
step:365/2330 train_time:20766ms step_avg:56.89ms
step:366/2330 train_time:20826ms step_avg:56.90ms
step:367/2330 train_time:20882ms step_avg:56.90ms
step:368/2330 train_time:20941ms step_avg:56.90ms
step:369/2330 train_time:20997ms step_avg:56.90ms
step:370/2330 train_time:21055ms step_avg:56.91ms
step:371/2330 train_time:21111ms step_avg:56.90ms
step:372/2330 train_time:21169ms step_avg:56.91ms
step:373/2330 train_time:21225ms step_avg:56.90ms
step:374/2330 train_time:21284ms step_avg:56.91ms
step:375/2330 train_time:21341ms step_avg:56.91ms
step:376/2330 train_time:21399ms step_avg:56.91ms
step:377/2330 train_time:21455ms step_avg:56.91ms
step:378/2330 train_time:21514ms step_avg:56.91ms
step:379/2330 train_time:21570ms step_avg:56.91ms
step:380/2330 train_time:21628ms step_avg:56.92ms
step:381/2330 train_time:21684ms step_avg:56.91ms
step:382/2330 train_time:21743ms step_avg:56.92ms
step:383/2330 train_time:21799ms step_avg:56.92ms
step:384/2330 train_time:21858ms step_avg:56.92ms
step:385/2330 train_time:21914ms step_avg:56.92ms
step:386/2330 train_time:21973ms step_avg:56.93ms
step:387/2330 train_time:22029ms step_avg:56.92ms
step:388/2330 train_time:22088ms step_avg:56.93ms
step:389/2330 train_time:22145ms step_avg:56.93ms
step:390/2330 train_time:22203ms step_avg:56.93ms
step:391/2330 train_time:22258ms step_avg:56.93ms
step:392/2330 train_time:22318ms step_avg:56.93ms
step:393/2330 train_time:22374ms step_avg:56.93ms
step:394/2330 train_time:22432ms step_avg:56.94ms
step:395/2330 train_time:22489ms step_avg:56.93ms
step:396/2330 train_time:22547ms step_avg:56.94ms
step:397/2330 train_time:22603ms step_avg:56.93ms
step:398/2330 train_time:22662ms step_avg:56.94ms
step:399/2330 train_time:22719ms step_avg:56.94ms
step:400/2330 train_time:22778ms step_avg:56.95ms
step:401/2330 train_time:22834ms step_avg:56.94ms
step:402/2330 train_time:22894ms step_avg:56.95ms
step:403/2330 train_time:22949ms step_avg:56.95ms
step:404/2330 train_time:23008ms step_avg:56.95ms
step:405/2330 train_time:23064ms step_avg:56.95ms
step:406/2330 train_time:23123ms step_avg:56.95ms
step:407/2330 train_time:23180ms step_avg:56.95ms
step:408/2330 train_time:23239ms step_avg:56.96ms
step:409/2330 train_time:23295ms step_avg:56.95ms
step:410/2330 train_time:23353ms step_avg:56.96ms
step:411/2330 train_time:23409ms step_avg:56.96ms
step:412/2330 train_time:23468ms step_avg:56.96ms
step:413/2330 train_time:23524ms step_avg:56.96ms
step:414/2330 train_time:23582ms step_avg:56.96ms
step:415/2330 train_time:23639ms step_avg:56.96ms
step:416/2330 train_time:23698ms step_avg:56.97ms
step:417/2330 train_time:23754ms step_avg:56.96ms
step:418/2330 train_time:23813ms step_avg:56.97ms
step:419/2330 train_time:23869ms step_avg:56.97ms
step:420/2330 train_time:23928ms step_avg:56.97ms
step:421/2330 train_time:23985ms step_avg:56.97ms
step:422/2330 train_time:24043ms step_avg:56.97ms
step:423/2330 train_time:24099ms step_avg:56.97ms
step:424/2330 train_time:24158ms step_avg:56.98ms
step:425/2330 train_time:24214ms step_avg:56.97ms
step:426/2330 train_time:24272ms step_avg:56.98ms
step:427/2330 train_time:24328ms step_avg:56.97ms
step:428/2330 train_time:24387ms step_avg:56.98ms
step:429/2330 train_time:24444ms step_avg:56.98ms
step:430/2330 train_time:24502ms step_avg:56.98ms
step:431/2330 train_time:24557ms step_avg:56.98ms
step:432/2330 train_time:24617ms step_avg:56.98ms
step:433/2330 train_time:24674ms step_avg:56.98ms
step:434/2330 train_time:24732ms step_avg:56.99ms
step:435/2330 train_time:24788ms step_avg:56.98ms
step:436/2330 train_time:24847ms step_avg:56.99ms
step:437/2330 train_time:24904ms step_avg:56.99ms
step:438/2330 train_time:24962ms step_avg:56.99ms
step:439/2330 train_time:25018ms step_avg:56.99ms
step:440/2330 train_time:25077ms step_avg:56.99ms
step:441/2330 train_time:25133ms step_avg:56.99ms
step:442/2330 train_time:25192ms step_avg:56.99ms
step:443/2330 train_time:25247ms step_avg:56.99ms
step:444/2330 train_time:25307ms step_avg:57.00ms
step:445/2330 train_time:25363ms step_avg:57.00ms
step:446/2330 train_time:25421ms step_avg:57.00ms
step:447/2330 train_time:25477ms step_avg:57.00ms
step:448/2330 train_time:25537ms step_avg:57.00ms
step:449/2330 train_time:25592ms step_avg:57.00ms
step:450/2330 train_time:25651ms step_avg:57.00ms
step:451/2330 train_time:25707ms step_avg:57.00ms
step:452/2330 train_time:25766ms step_avg:57.00ms
step:453/2330 train_time:25822ms step_avg:57.00ms
step:454/2330 train_time:25880ms step_avg:57.01ms
step:455/2330 train_time:25936ms step_avg:57.00ms
step:456/2330 train_time:25995ms step_avg:57.01ms
step:457/2330 train_time:26051ms step_avg:57.00ms
step:458/2330 train_time:26109ms step_avg:57.01ms
step:459/2330 train_time:26165ms step_avg:57.01ms
step:460/2330 train_time:26224ms step_avg:57.01ms
step:461/2330 train_time:26280ms step_avg:57.01ms
step:462/2330 train_time:26339ms step_avg:57.01ms
step:463/2330 train_time:26395ms step_avg:57.01ms
step:464/2330 train_time:26455ms step_avg:57.02ms
step:465/2330 train_time:26511ms step_avg:57.01ms
step:466/2330 train_time:26571ms step_avg:57.02ms
step:467/2330 train_time:26626ms step_avg:57.01ms
step:468/2330 train_time:26685ms step_avg:57.02ms
step:469/2330 train_time:26741ms step_avg:57.02ms
step:470/2330 train_time:26800ms step_avg:57.02ms
step:471/2330 train_time:26856ms step_avg:57.02ms
step:472/2330 train_time:26914ms step_avg:57.02ms
step:473/2330 train_time:26970ms step_avg:57.02ms
step:474/2330 train_time:27029ms step_avg:57.02ms
step:475/2330 train_time:27085ms step_avg:57.02ms
step:476/2330 train_time:27143ms step_avg:57.02ms
step:477/2330 train_time:27199ms step_avg:57.02ms
step:478/2330 train_time:27258ms step_avg:57.03ms
step:479/2330 train_time:27315ms step_avg:57.03ms
step:480/2330 train_time:27375ms step_avg:57.03ms
step:481/2330 train_time:27431ms step_avg:57.03ms
step:482/2330 train_time:27489ms step_avg:57.03ms
step:483/2330 train_time:27545ms step_avg:57.03ms
step:484/2330 train_time:27604ms step_avg:57.03ms
step:485/2330 train_time:27661ms step_avg:57.03ms
step:486/2330 train_time:27720ms step_avg:57.04ms
step:487/2330 train_time:27776ms step_avg:57.04ms
step:488/2330 train_time:27835ms step_avg:57.04ms
step:489/2330 train_time:27891ms step_avg:57.04ms
step:490/2330 train_time:27949ms step_avg:57.04ms
step:491/2330 train_time:28005ms step_avg:57.04ms
step:492/2330 train_time:28064ms step_avg:57.04ms
step:493/2330 train_time:28120ms step_avg:57.04ms
step:494/2330 train_time:28179ms step_avg:57.04ms
step:495/2330 train_time:28235ms step_avg:57.04ms
step:496/2330 train_time:28294ms step_avg:57.04ms
step:497/2330 train_time:28350ms step_avg:57.04ms
step:498/2330 train_time:28409ms step_avg:57.05ms
step:499/2330 train_time:28465ms step_avg:57.04ms
step:500/2330 train_time:28524ms step_avg:57.05ms
step:500/2330 val_loss:5.8194 train_time:28603ms step_avg:57.21ms
step:501/2330 train_time:28622ms step_avg:57.13ms
step:502/2330 train_time:28642ms step_avg:57.06ms
step:503/2330 train_time:28699ms step_avg:57.06ms
step:504/2330 train_time:28760ms step_avg:57.06ms
step:505/2330 train_time:28816ms step_avg:57.06ms
step:506/2330 train_time:28878ms step_avg:57.07ms
step:507/2330 train_time:28934ms step_avg:57.07ms
step:508/2330 train_time:28993ms step_avg:57.07ms
step:509/2330 train_time:29049ms step_avg:57.07ms
step:510/2330 train_time:29107ms step_avg:57.07ms
step:511/2330 train_time:29162ms step_avg:57.07ms
step:512/2330 train_time:29221ms step_avg:57.07ms
step:513/2330 train_time:29277ms step_avg:57.07ms
step:514/2330 train_time:29335ms step_avg:57.07ms
step:515/2330 train_time:29391ms step_avg:57.07ms
step:516/2330 train_time:29449ms step_avg:57.07ms
step:517/2330 train_time:29505ms step_avg:57.07ms
step:518/2330 train_time:29564ms step_avg:57.07ms
step:519/2330 train_time:29620ms step_avg:57.07ms
step:520/2330 train_time:29681ms step_avg:57.08ms
step:521/2330 train_time:29738ms step_avg:57.08ms
step:522/2330 train_time:29797ms step_avg:57.08ms
step:523/2330 train_time:29854ms step_avg:57.08ms
step:524/2330 train_time:29913ms step_avg:57.09ms
step:525/2330 train_time:29970ms step_avg:57.09ms
step:526/2330 train_time:30029ms step_avg:57.09ms
step:527/2330 train_time:30085ms step_avg:57.09ms
step:528/2330 train_time:30144ms step_avg:57.09ms
step:529/2330 train_time:30199ms step_avg:57.09ms
step:530/2330 train_time:30258ms step_avg:57.09ms
step:531/2330 train_time:30313ms step_avg:57.09ms
step:532/2330 train_time:30372ms step_avg:57.09ms
step:533/2330 train_time:30428ms step_avg:57.09ms
step:534/2330 train_time:30486ms step_avg:57.09ms
step:535/2330 train_time:30542ms step_avg:57.09ms
step:536/2330 train_time:30602ms step_avg:57.09ms
step:537/2330 train_time:30659ms step_avg:57.09ms
step:538/2330 train_time:30719ms step_avg:57.10ms
step:539/2330 train_time:30775ms step_avg:57.10ms
step:540/2330 train_time:30834ms step_avg:57.10ms
step:541/2330 train_time:30891ms step_avg:57.10ms
step:542/2330 train_time:30949ms step_avg:57.10ms
step:543/2330 train_time:31005ms step_avg:57.10ms
step:544/2330 train_time:31065ms step_avg:57.10ms
step:545/2330 train_time:31121ms step_avg:57.10ms
step:546/2330 train_time:31180ms step_avg:57.11ms
step:547/2330 train_time:31236ms step_avg:57.10ms
step:548/2330 train_time:31294ms step_avg:57.11ms
step:549/2330 train_time:31350ms step_avg:57.10ms
step:550/2330 train_time:31409ms step_avg:57.11ms
step:551/2330 train_time:31466ms step_avg:57.11ms
step:552/2330 train_time:31524ms step_avg:57.11ms
step:553/2330 train_time:31581ms step_avg:57.11ms
step:554/2330 train_time:31640ms step_avg:57.11ms
step:555/2330 train_time:31696ms step_avg:57.11ms
step:556/2330 train_time:31756ms step_avg:57.11ms
step:557/2330 train_time:31812ms step_avg:57.11ms
step:558/2330 train_time:31871ms step_avg:57.12ms
step:559/2330 train_time:31928ms step_avg:57.12ms
step:560/2330 train_time:31987ms step_avg:57.12ms
step:561/2330 train_time:32044ms step_avg:57.12ms
step:562/2330 train_time:32103ms step_avg:57.12ms
step:563/2330 train_time:32159ms step_avg:57.12ms
step:564/2330 train_time:32217ms step_avg:57.12ms
step:565/2330 train_time:32273ms step_avg:57.12ms
step:566/2330 train_time:32332ms step_avg:57.12ms
step:567/2330 train_time:32388ms step_avg:57.12ms
step:568/2330 train_time:32447ms step_avg:57.13ms
step:569/2330 train_time:32503ms step_avg:57.12ms
step:570/2330 train_time:32563ms step_avg:57.13ms
step:571/2330 train_time:32620ms step_avg:57.13ms
step:572/2330 train_time:32677ms step_avg:57.13ms
step:573/2330 train_time:32733ms step_avg:57.13ms
step:574/2330 train_time:32792ms step_avg:57.13ms
step:575/2330 train_time:32848ms step_avg:57.13ms
step:576/2330 train_time:32908ms step_avg:57.13ms
step:577/2330 train_time:32964ms step_avg:57.13ms
step:578/2330 train_time:33024ms step_avg:57.14ms
step:579/2330 train_time:33080ms step_avg:57.13ms
step:580/2330 train_time:33140ms step_avg:57.14ms
step:581/2330 train_time:33195ms step_avg:57.13ms
step:582/2330 train_time:33254ms step_avg:57.14ms
step:583/2330 train_time:33310ms step_avg:57.14ms
step:584/2330 train_time:33369ms step_avg:57.14ms
step:585/2330 train_time:33426ms step_avg:57.14ms
step:586/2330 train_time:33484ms step_avg:57.14ms
step:587/2330 train_time:33541ms step_avg:57.14ms
step:588/2330 train_time:33599ms step_avg:57.14ms
step:589/2330 train_time:33655ms step_avg:57.14ms
step:590/2330 train_time:33714ms step_avg:57.14ms
step:591/2330 train_time:33771ms step_avg:57.14ms
step:592/2330 train_time:33829ms step_avg:57.14ms
step:593/2330 train_time:33885ms step_avg:57.14ms
step:594/2330 train_time:33945ms step_avg:57.15ms
step:595/2330 train_time:34001ms step_avg:57.14ms
step:596/2330 train_time:34061ms step_avg:57.15ms
step:597/2330 train_time:34117ms step_avg:57.15ms
step:598/2330 train_time:34177ms step_avg:57.15ms
step:599/2330 train_time:34232ms step_avg:57.15ms
step:600/2330 train_time:34291ms step_avg:57.15ms
step:601/2330 train_time:34348ms step_avg:57.15ms
step:602/2330 train_time:34407ms step_avg:57.15ms
step:603/2330 train_time:34463ms step_avg:57.15ms
step:604/2330 train_time:34521ms step_avg:57.15ms
step:605/2330 train_time:34577ms step_avg:57.15ms
step:606/2330 train_time:34636ms step_avg:57.16ms
step:607/2330 train_time:34692ms step_avg:57.15ms
step:608/2330 train_time:34751ms step_avg:57.16ms
step:609/2330 train_time:34808ms step_avg:57.16ms
step:610/2330 train_time:34867ms step_avg:57.16ms
step:611/2330 train_time:34923ms step_avg:57.16ms
step:612/2330 train_time:34983ms step_avg:57.16ms
step:613/2330 train_time:35039ms step_avg:57.16ms
step:614/2330 train_time:35097ms step_avg:57.16ms
step:615/2330 train_time:35153ms step_avg:57.16ms
step:616/2330 train_time:35213ms step_avg:57.16ms
step:617/2330 train_time:35269ms step_avg:57.16ms
step:618/2330 train_time:35327ms step_avg:57.16ms
step:619/2330 train_time:35383ms step_avg:57.16ms
step:620/2330 train_time:35443ms step_avg:57.17ms
step:621/2330 train_time:35499ms step_avg:57.16ms
step:622/2330 train_time:35558ms step_avg:57.17ms
step:623/2330 train_time:35614ms step_avg:57.16ms
step:624/2330 train_time:35673ms step_avg:57.17ms
step:625/2330 train_time:35730ms step_avg:57.17ms
step:626/2330 train_time:35789ms step_avg:57.17ms
step:627/2330 train_time:35845ms step_avg:57.17ms
step:628/2330 train_time:35905ms step_avg:57.17ms
step:629/2330 train_time:35961ms step_avg:57.17ms
step:630/2330 train_time:36020ms step_avg:57.18ms
step:631/2330 train_time:36077ms step_avg:57.17ms
step:632/2330 train_time:36136ms step_avg:57.18ms
step:633/2330 train_time:36192ms step_avg:57.18ms
step:634/2330 train_time:36251ms step_avg:57.18ms
step:635/2330 train_time:36306ms step_avg:57.18ms
step:636/2330 train_time:36367ms step_avg:57.18ms
step:637/2330 train_time:36424ms step_avg:57.18ms
step:638/2330 train_time:36483ms step_avg:57.18ms
step:639/2330 train_time:36539ms step_avg:57.18ms
step:640/2330 train_time:36599ms step_avg:57.19ms
step:641/2330 train_time:36654ms step_avg:57.18ms
step:642/2330 train_time:36714ms step_avg:57.19ms
step:643/2330 train_time:36771ms step_avg:57.19ms
step:644/2330 train_time:36830ms step_avg:57.19ms
step:645/2330 train_time:36887ms step_avg:57.19ms
step:646/2330 train_time:36946ms step_avg:57.19ms
step:647/2330 train_time:37002ms step_avg:57.19ms
step:648/2330 train_time:37062ms step_avg:57.19ms
step:649/2330 train_time:37118ms step_avg:57.19ms
step:650/2330 train_time:37177ms step_avg:57.20ms
step:651/2330 train_time:37232ms step_avg:57.19ms
step:652/2330 train_time:37292ms step_avg:57.20ms
step:653/2330 train_time:37349ms step_avg:57.20ms
step:654/2330 train_time:37408ms step_avg:57.20ms
step:655/2330 train_time:37464ms step_avg:57.20ms
step:656/2330 train_time:37523ms step_avg:57.20ms
step:657/2330 train_time:37579ms step_avg:57.20ms
step:658/2330 train_time:37638ms step_avg:57.20ms
step:659/2330 train_time:37693ms step_avg:57.20ms
step:660/2330 train_time:37754ms step_avg:57.20ms
step:661/2330 train_time:37811ms step_avg:57.20ms
step:662/2330 train_time:37870ms step_avg:57.21ms
step:663/2330 train_time:37927ms step_avg:57.20ms
step:664/2330 train_time:37986ms step_avg:57.21ms
step:665/2330 train_time:38043ms step_avg:57.21ms
step:666/2330 train_time:38102ms step_avg:57.21ms
step:667/2330 train_time:38157ms step_avg:57.21ms
step:668/2330 train_time:38217ms step_avg:57.21ms
step:669/2330 train_time:38273ms step_avg:57.21ms
step:670/2330 train_time:38332ms step_avg:57.21ms
step:671/2330 train_time:38388ms step_avg:57.21ms
step:672/2330 train_time:38448ms step_avg:57.21ms
step:673/2330 train_time:38503ms step_avg:57.21ms
step:674/2330 train_time:38563ms step_avg:57.22ms
step:675/2330 train_time:38619ms step_avg:57.21ms
step:676/2330 train_time:38679ms step_avg:57.22ms
step:677/2330 train_time:38734ms step_avg:57.21ms
step:678/2330 train_time:38794ms step_avg:57.22ms
step:679/2330 train_time:38850ms step_avg:57.22ms
step:680/2330 train_time:38909ms step_avg:57.22ms
step:681/2330 train_time:38966ms step_avg:57.22ms
step:682/2330 train_time:39025ms step_avg:57.22ms
step:683/2330 train_time:39081ms step_avg:57.22ms
step:684/2330 train_time:39141ms step_avg:57.22ms
step:685/2330 train_time:39197ms step_avg:57.22ms
step:686/2330 train_time:39256ms step_avg:57.22ms
step:687/2330 train_time:39311ms step_avg:57.22ms
step:688/2330 train_time:39371ms step_avg:57.23ms
step:689/2330 train_time:39427ms step_avg:57.22ms
step:690/2330 train_time:39487ms step_avg:57.23ms
step:691/2330 train_time:39542ms step_avg:57.22ms
step:692/2330 train_time:39604ms step_avg:57.23ms
step:693/2330 train_time:39659ms step_avg:57.23ms
step:694/2330 train_time:39720ms step_avg:57.23ms
step:695/2330 train_time:39775ms step_avg:57.23ms
step:696/2330 train_time:39835ms step_avg:57.23ms
step:697/2330 train_time:39891ms step_avg:57.23ms
step:698/2330 train_time:39951ms step_avg:57.24ms
step:699/2330 train_time:40007ms step_avg:57.23ms
step:700/2330 train_time:40067ms step_avg:57.24ms
step:701/2330 train_time:40123ms step_avg:57.24ms
step:702/2330 train_time:40183ms step_avg:57.24ms
step:703/2330 train_time:40239ms step_avg:57.24ms
step:704/2330 train_time:40299ms step_avg:57.24ms
step:705/2330 train_time:40354ms step_avg:57.24ms
step:706/2330 train_time:40414ms step_avg:57.24ms
step:707/2330 train_time:40470ms step_avg:57.24ms
step:708/2330 train_time:40529ms step_avg:57.25ms
step:709/2330 train_time:40586ms step_avg:57.24ms
step:710/2330 train_time:40646ms step_avg:57.25ms
step:711/2330 train_time:40702ms step_avg:57.25ms
step:712/2330 train_time:40761ms step_avg:57.25ms
step:713/2330 train_time:40817ms step_avg:57.25ms
step:714/2330 train_time:40877ms step_avg:57.25ms
step:715/2330 train_time:40933ms step_avg:57.25ms
step:716/2330 train_time:40992ms step_avg:57.25ms
step:717/2330 train_time:41049ms step_avg:57.25ms
step:718/2330 train_time:41108ms step_avg:57.25ms
step:719/2330 train_time:41165ms step_avg:57.25ms
step:720/2330 train_time:41224ms step_avg:57.26ms
step:721/2330 train_time:41280ms step_avg:57.25ms
step:722/2330 train_time:41340ms step_avg:57.26ms
step:723/2330 train_time:41395ms step_avg:57.26ms
step:724/2330 train_time:41456ms step_avg:57.26ms
step:725/2330 train_time:41512ms step_avg:57.26ms
step:726/2330 train_time:41571ms step_avg:57.26ms
step:727/2330 train_time:41627ms step_avg:57.26ms
step:728/2330 train_time:41687ms step_avg:57.26ms
step:729/2330 train_time:41743ms step_avg:57.26ms
step:730/2330 train_time:41802ms step_avg:57.26ms
step:731/2330 train_time:41858ms step_avg:57.26ms
step:732/2330 train_time:41917ms step_avg:57.26ms
step:733/2330 train_time:41972ms step_avg:57.26ms
step:734/2330 train_time:42032ms step_avg:57.26ms
step:735/2330 train_time:42089ms step_avg:57.26ms
step:736/2330 train_time:42148ms step_avg:57.27ms
step:737/2330 train_time:42204ms step_avg:57.26ms
step:738/2330 train_time:42264ms step_avg:57.27ms
step:739/2330 train_time:42320ms step_avg:57.27ms
step:740/2330 train_time:42379ms step_avg:57.27ms
step:741/2330 train_time:42435ms step_avg:57.27ms
step:742/2330 train_time:42494ms step_avg:57.27ms
step:743/2330 train_time:42550ms step_avg:57.27ms
step:744/2330 train_time:42609ms step_avg:57.27ms
step:745/2330 train_time:42665ms step_avg:57.27ms
step:746/2330 train_time:42724ms step_avg:57.27ms
step:747/2330 train_time:42780ms step_avg:57.27ms
step:748/2330 train_time:42841ms step_avg:57.27ms
step:749/2330 train_time:42897ms step_avg:57.27ms
step:750/2330 train_time:42957ms step_avg:57.28ms
step:750/2330 val_loss:5.4750 train_time:43037ms step_avg:57.38ms
step:751/2330 train_time:43056ms step_avg:57.33ms
step:752/2330 train_time:43076ms step_avg:57.28ms
step:753/2330 train_time:43133ms step_avg:57.28ms
step:754/2330 train_time:43197ms step_avg:57.29ms
step:755/2330 train_time:43253ms step_avg:57.29ms
step:756/2330 train_time:43313ms step_avg:57.29ms
step:757/2330 train_time:43370ms step_avg:57.29ms
step:758/2330 train_time:43429ms step_avg:57.29ms
step:759/2330 train_time:43485ms step_avg:57.29ms
step:760/2330 train_time:43544ms step_avg:57.29ms
step:761/2330 train_time:43599ms step_avg:57.29ms
step:762/2330 train_time:43658ms step_avg:57.29ms
step:763/2330 train_time:43713ms step_avg:57.29ms
step:764/2330 train_time:43772ms step_avg:57.29ms
step:765/2330 train_time:43829ms step_avg:57.29ms
step:766/2330 train_time:43886ms step_avg:57.29ms
step:767/2330 train_time:43943ms step_avg:57.29ms
step:768/2330 train_time:44003ms step_avg:57.29ms
step:769/2330 train_time:44060ms step_avg:57.29ms
step:770/2330 train_time:44124ms step_avg:57.30ms
step:771/2330 train_time:44181ms step_avg:57.30ms
step:772/2330 train_time:44244ms step_avg:57.31ms
step:773/2330 train_time:44301ms step_avg:57.31ms
step:774/2330 train_time:44363ms step_avg:57.32ms
step:775/2330 train_time:44419ms step_avg:57.32ms
step:776/2330 train_time:44480ms step_avg:57.32ms
step:777/2330 train_time:44537ms step_avg:57.32ms
step:778/2330 train_time:44597ms step_avg:57.32ms
step:779/2330 train_time:44653ms step_avg:57.32ms
step:780/2330 train_time:44712ms step_avg:57.32ms
step:781/2330 train_time:44769ms step_avg:57.32ms
step:782/2330 train_time:44828ms step_avg:57.32ms
step:783/2330 train_time:44885ms step_avg:57.32ms
step:784/2330 train_time:44944ms step_avg:57.33ms
step:785/2330 train_time:45001ms step_avg:57.33ms
step:786/2330 train_time:45062ms step_avg:57.33ms
step:787/2330 train_time:45120ms step_avg:57.33ms
step:788/2330 train_time:45180ms step_avg:57.34ms
step:789/2330 train_time:45237ms step_avg:57.33ms
step:790/2330 train_time:45299ms step_avg:57.34ms
step:791/2330 train_time:45356ms step_avg:57.34ms
step:792/2330 train_time:45417ms step_avg:57.34ms
step:793/2330 train_time:45473ms step_avg:57.34ms
step:794/2330 train_time:45533ms step_avg:57.35ms
step:795/2330 train_time:45590ms step_avg:57.35ms
step:796/2330 train_time:45649ms step_avg:57.35ms
step:797/2330 train_time:45706ms step_avg:57.35ms
step:798/2330 train_time:45766ms step_avg:57.35ms
step:799/2330 train_time:45822ms step_avg:57.35ms
step:800/2330 train_time:45882ms step_avg:57.35ms
step:801/2330 train_time:45938ms step_avg:57.35ms
step:802/2330 train_time:45998ms step_avg:57.35ms
step:803/2330 train_time:46054ms step_avg:57.35ms
step:804/2330 train_time:46115ms step_avg:57.36ms
step:805/2330 train_time:46173ms step_avg:57.36ms
step:806/2330 train_time:46233ms step_avg:57.36ms
step:807/2330 train_time:46290ms step_avg:57.36ms
step:808/2330 train_time:46351ms step_avg:57.36ms
step:809/2330 train_time:46408ms step_avg:57.36ms
step:810/2330 train_time:46468ms step_avg:57.37ms
step:811/2330 train_time:46525ms step_avg:57.37ms
step:812/2330 train_time:46585ms step_avg:57.37ms
step:813/2330 train_time:46642ms step_avg:57.37ms
step:814/2330 train_time:46702ms step_avg:57.37ms
step:815/2330 train_time:46758ms step_avg:57.37ms
step:816/2330 train_time:46819ms step_avg:57.38ms
step:817/2330 train_time:46875ms step_avg:57.37ms
step:818/2330 train_time:46935ms step_avg:57.38ms
step:819/2330 train_time:46991ms step_avg:57.38ms
step:820/2330 train_time:47052ms step_avg:57.38ms
step:821/2330 train_time:47109ms step_avg:57.38ms
step:822/2330 train_time:47170ms step_avg:57.38ms
step:823/2330 train_time:47228ms step_avg:57.39ms
step:824/2330 train_time:47288ms step_avg:57.39ms
step:825/2330 train_time:47344ms step_avg:57.39ms
step:826/2330 train_time:47405ms step_avg:57.39ms
step:827/2330 train_time:47461ms step_avg:57.39ms
step:828/2330 train_time:47524ms step_avg:57.40ms
step:829/2330 train_time:47580ms step_avg:57.39ms
step:830/2330 train_time:47641ms step_avg:57.40ms
step:831/2330 train_time:47698ms step_avg:57.40ms
step:832/2330 train_time:47758ms step_avg:57.40ms
step:833/2330 train_time:47815ms step_avg:57.40ms
step:834/2330 train_time:47874ms step_avg:57.40ms
step:835/2330 train_time:47930ms step_avg:57.40ms
step:836/2330 train_time:47991ms step_avg:57.41ms
step:837/2330 train_time:48047ms step_avg:57.40ms
step:838/2330 train_time:48108ms step_avg:57.41ms
step:839/2330 train_time:48166ms step_avg:57.41ms
step:840/2330 train_time:48226ms step_avg:57.41ms
step:841/2330 train_time:48284ms step_avg:57.41ms
step:842/2330 train_time:48344ms step_avg:57.42ms
step:843/2330 train_time:48401ms step_avg:57.41ms
step:844/2330 train_time:48462ms step_avg:57.42ms
step:845/2330 train_time:48519ms step_avg:57.42ms
step:846/2330 train_time:48579ms step_avg:57.42ms
step:847/2330 train_time:48635ms step_avg:57.42ms
step:848/2330 train_time:48696ms step_avg:57.42ms
step:849/2330 train_time:48752ms step_avg:57.42ms
step:850/2330 train_time:48813ms step_avg:57.43ms
step:851/2330 train_time:48870ms step_avg:57.43ms
step:852/2330 train_time:48930ms step_avg:57.43ms
step:853/2330 train_time:48986ms step_avg:57.43ms
step:854/2330 train_time:49047ms step_avg:57.43ms
step:855/2330 train_time:49104ms step_avg:57.43ms
step:856/2330 train_time:49165ms step_avg:57.44ms
step:857/2330 train_time:49222ms step_avg:57.44ms
step:858/2330 train_time:49283ms step_avg:57.44ms
step:859/2330 train_time:49340ms step_avg:57.44ms
step:860/2330 train_time:49400ms step_avg:57.44ms
step:861/2330 train_time:49457ms step_avg:57.44ms
step:862/2330 train_time:49518ms step_avg:57.45ms
step:863/2330 train_time:49574ms step_avg:57.44ms
step:864/2330 train_time:49635ms step_avg:57.45ms
step:865/2330 train_time:49692ms step_avg:57.45ms
step:866/2330 train_time:49752ms step_avg:57.45ms
step:867/2330 train_time:49809ms step_avg:57.45ms
step:868/2330 train_time:49869ms step_avg:57.45ms
step:869/2330 train_time:49926ms step_avg:57.45ms
step:870/2330 train_time:49985ms step_avg:57.45ms
step:871/2330 train_time:50042ms step_avg:57.45ms
step:872/2330 train_time:50103ms step_avg:57.46ms
step:873/2330 train_time:50160ms step_avg:57.46ms
step:874/2330 train_time:50221ms step_avg:57.46ms
step:875/2330 train_time:50277ms step_avg:57.46ms
step:876/2330 train_time:50338ms step_avg:57.46ms
step:877/2330 train_time:50395ms step_avg:57.46ms
step:878/2330 train_time:50455ms step_avg:57.47ms
step:879/2330 train_time:50512ms step_avg:57.47ms
step:880/2330 train_time:50572ms step_avg:57.47ms
step:881/2330 train_time:50629ms step_avg:57.47ms
step:882/2330 train_time:50689ms step_avg:57.47ms
step:883/2330 train_time:50746ms step_avg:57.47ms
step:884/2330 train_time:50806ms step_avg:57.47ms
step:885/2330 train_time:50862ms step_avg:57.47ms
step:886/2330 train_time:50924ms step_avg:57.48ms
step:887/2330 train_time:50981ms step_avg:57.48ms
step:888/2330 train_time:51040ms step_avg:57.48ms
step:889/2330 train_time:51098ms step_avg:57.48ms
step:890/2330 train_time:51157ms step_avg:57.48ms
step:891/2330 train_time:51214ms step_avg:57.48ms
step:892/2330 train_time:51274ms step_avg:57.48ms
step:893/2330 train_time:51330ms step_avg:57.48ms
step:894/2330 train_time:51391ms step_avg:57.48ms
step:895/2330 train_time:51448ms step_avg:57.48ms
step:896/2330 train_time:51509ms step_avg:57.49ms
step:897/2330 train_time:51566ms step_avg:57.49ms
step:898/2330 train_time:51626ms step_avg:57.49ms
step:899/2330 train_time:51683ms step_avg:57.49ms
step:900/2330 train_time:51743ms step_avg:57.49ms
step:901/2330 train_time:51800ms step_avg:57.49ms
step:902/2330 train_time:51859ms step_avg:57.49ms
step:903/2330 train_time:51915ms step_avg:57.49ms
step:904/2330 train_time:51976ms step_avg:57.50ms
step:905/2330 train_time:52032ms step_avg:57.49ms
step:906/2330 train_time:52092ms step_avg:57.50ms
step:907/2330 train_time:52149ms step_avg:57.50ms
step:908/2330 train_time:52209ms step_avg:57.50ms
step:909/2330 train_time:52267ms step_avg:57.50ms
step:910/2330 train_time:52326ms step_avg:57.50ms
step:911/2330 train_time:52384ms step_avg:57.50ms
step:912/2330 train_time:52443ms step_avg:57.50ms
step:913/2330 train_time:52500ms step_avg:57.50ms
step:914/2330 train_time:52561ms step_avg:57.51ms
step:915/2330 train_time:52619ms step_avg:57.51ms
step:916/2330 train_time:52679ms step_avg:57.51ms
step:917/2330 train_time:52736ms step_avg:57.51ms
step:918/2330 train_time:52795ms step_avg:57.51ms
step:919/2330 train_time:52852ms step_avg:57.51ms
step:920/2330 train_time:52913ms step_avg:57.51ms
step:921/2330 train_time:52970ms step_avg:57.51ms
step:922/2330 train_time:53029ms step_avg:57.52ms
step:923/2330 train_time:53087ms step_avg:57.52ms
step:924/2330 train_time:53147ms step_avg:57.52ms
step:925/2330 train_time:53204ms step_avg:57.52ms
step:926/2330 train_time:53264ms step_avg:57.52ms
step:927/2330 train_time:53320ms step_avg:57.52ms
step:928/2330 train_time:53380ms step_avg:57.52ms
step:929/2330 train_time:53437ms step_avg:57.52ms
step:930/2330 train_time:53497ms step_avg:57.52ms
step:931/2330 train_time:53554ms step_avg:57.52ms
step:932/2330 train_time:53615ms step_avg:57.53ms
step:933/2330 train_time:53672ms step_avg:57.53ms
step:934/2330 train_time:53732ms step_avg:57.53ms
step:935/2330 train_time:53789ms step_avg:57.53ms
step:936/2330 train_time:53849ms step_avg:57.53ms
step:937/2330 train_time:53906ms step_avg:57.53ms
step:938/2330 train_time:53966ms step_avg:57.53ms
step:939/2330 train_time:54023ms step_avg:57.53ms
step:940/2330 train_time:54083ms step_avg:57.53ms
step:941/2330 train_time:54139ms step_avg:57.53ms
step:942/2330 train_time:54199ms step_avg:57.54ms
step:943/2330 train_time:54255ms step_avg:57.53ms
step:944/2330 train_time:54316ms step_avg:57.54ms
step:945/2330 train_time:54373ms step_avg:57.54ms
step:946/2330 train_time:54433ms step_avg:57.54ms
step:947/2330 train_time:54491ms step_avg:57.54ms
step:948/2330 train_time:54550ms step_avg:57.54ms
step:949/2330 train_time:54608ms step_avg:57.54ms
step:950/2330 train_time:54667ms step_avg:57.54ms
step:951/2330 train_time:54724ms step_avg:57.54ms
step:952/2330 train_time:54784ms step_avg:57.55ms
step:953/2330 train_time:54841ms step_avg:57.55ms
step:954/2330 train_time:54903ms step_avg:57.55ms
step:955/2330 train_time:54960ms step_avg:57.55ms
step:956/2330 train_time:55021ms step_avg:57.55ms
step:957/2330 train_time:55077ms step_avg:57.55ms
step:958/2330 train_time:55137ms step_avg:57.55ms
step:959/2330 train_time:55194ms step_avg:57.55ms
step:960/2330 train_time:55253ms step_avg:57.56ms
step:961/2330 train_time:55310ms step_avg:57.55ms
step:962/2330 train_time:55370ms step_avg:57.56ms
step:963/2330 train_time:55427ms step_avg:57.56ms
step:964/2330 train_time:55488ms step_avg:57.56ms
step:965/2330 train_time:55545ms step_avg:57.56ms
step:966/2330 train_time:55605ms step_avg:57.56ms
step:967/2330 train_time:55662ms step_avg:57.56ms
step:968/2330 train_time:55723ms step_avg:57.57ms
step:969/2330 train_time:55779ms step_avg:57.56ms
step:970/2330 train_time:55841ms step_avg:57.57ms
step:971/2330 train_time:55898ms step_avg:57.57ms
step:972/2330 train_time:55957ms step_avg:57.57ms
step:973/2330 train_time:56014ms step_avg:57.57ms
step:974/2330 train_time:56074ms step_avg:57.57ms
step:975/2330 train_time:56131ms step_avg:57.57ms
step:976/2330 train_time:56191ms step_avg:57.57ms
step:977/2330 train_time:56248ms step_avg:57.57ms
step:978/2330 train_time:56308ms step_avg:57.57ms
step:979/2330 train_time:56366ms step_avg:57.57ms
step:980/2330 train_time:56426ms step_avg:57.58ms
step:981/2330 train_time:56482ms step_avg:57.58ms
step:982/2330 train_time:56543ms step_avg:57.58ms
step:983/2330 train_time:56600ms step_avg:57.58ms
step:984/2330 train_time:56661ms step_avg:57.58ms
step:985/2330 train_time:56718ms step_avg:57.58ms
step:986/2330 train_time:56777ms step_avg:57.58ms
step:987/2330 train_time:56833ms step_avg:57.58ms
step:988/2330 train_time:56894ms step_avg:57.59ms
step:989/2330 train_time:56951ms step_avg:57.58ms
step:990/2330 train_time:57012ms step_avg:57.59ms
step:991/2330 train_time:57069ms step_avg:57.59ms
step:992/2330 train_time:57129ms step_avg:57.59ms
step:993/2330 train_time:57185ms step_avg:57.59ms
step:994/2330 train_time:57245ms step_avg:57.59ms
step:995/2330 train_time:57302ms step_avg:57.59ms
step:996/2330 train_time:57363ms step_avg:57.59ms
step:997/2330 train_time:57419ms step_avg:57.59ms
step:998/2330 train_time:57479ms step_avg:57.59ms
step:999/2330 train_time:57536ms step_avg:57.59ms
step:1000/2330 train_time:57596ms step_avg:57.60ms
step:1000/2330 val_loss:5.2507 train_time:57676ms step_avg:57.68ms
step:1001/2330 train_time:57695ms step_avg:57.64ms
step:1002/2330 train_time:57715ms step_avg:57.60ms
step:1003/2330 train_time:57776ms step_avg:57.60ms
step:1004/2330 train_time:57842ms step_avg:57.61ms
step:1005/2330 train_time:57901ms step_avg:57.61ms
step:1006/2330 train_time:57961ms step_avg:57.62ms
step:1007/2330 train_time:58018ms step_avg:57.61ms
step:1008/2330 train_time:58077ms step_avg:57.62ms
step:1009/2330 train_time:58134ms step_avg:57.62ms
step:1010/2330 train_time:58194ms step_avg:57.62ms
step:1011/2330 train_time:58250ms step_avg:57.62ms
step:1012/2330 train_time:58310ms step_avg:57.62ms
step:1013/2330 train_time:58365ms step_avg:57.62ms
step:1014/2330 train_time:58425ms step_avg:57.62ms
step:1015/2330 train_time:58481ms step_avg:57.62ms
step:1016/2330 train_time:58540ms step_avg:57.62ms
step:1017/2330 train_time:58596ms step_avg:57.62ms
step:1018/2330 train_time:58656ms step_avg:57.62ms
step:1019/2330 train_time:58713ms step_avg:57.62ms
step:1020/2330 train_time:58776ms step_avg:57.62ms
step:1021/2330 train_time:58835ms step_avg:57.62ms
step:1022/2330 train_time:58896ms step_avg:57.63ms
step:1023/2330 train_time:58953ms step_avg:57.63ms
step:1024/2330 train_time:59014ms step_avg:57.63ms
step:1025/2330 train_time:59071ms step_avg:57.63ms
step:1026/2330 train_time:59131ms step_avg:57.63ms
step:1027/2330 train_time:59187ms step_avg:57.63ms
step:1028/2330 train_time:59247ms step_avg:57.63ms
step:1029/2330 train_time:59303ms step_avg:57.63ms
step:1030/2330 train_time:59363ms step_avg:57.63ms
step:1031/2330 train_time:59419ms step_avg:57.63ms
step:1032/2330 train_time:59479ms step_avg:57.63ms
step:1033/2330 train_time:59536ms step_avg:57.63ms
step:1034/2330 train_time:59595ms step_avg:57.64ms
step:1035/2330 train_time:59652ms step_avg:57.64ms
step:1036/2330 train_time:59713ms step_avg:57.64ms
step:1037/2330 train_time:59770ms step_avg:57.64ms
step:1038/2330 train_time:59832ms step_avg:57.64ms
step:1039/2330 train_time:59889ms step_avg:57.64ms
step:1040/2330 train_time:59950ms step_avg:57.64ms
step:1041/2330 train_time:60007ms step_avg:57.64ms
step:1042/2330 train_time:60068ms step_avg:57.65ms
step:1043/2330 train_time:60125ms step_avg:57.65ms
step:1044/2330 train_time:60186ms step_avg:57.65ms
step:1045/2330 train_time:60242ms step_avg:57.65ms
step:1046/2330 train_time:60301ms step_avg:57.65ms
step:1047/2330 train_time:60357ms step_avg:57.65ms
step:1048/2330 train_time:60418ms step_avg:57.65ms
step:1049/2330 train_time:60474ms step_avg:57.65ms
step:1050/2330 train_time:60534ms step_avg:57.65ms
step:1051/2330 train_time:60591ms step_avg:57.65ms
step:1052/2330 train_time:60651ms step_avg:57.65ms
step:1053/2330 train_time:60708ms step_avg:57.65ms
step:1054/2330 train_time:60769ms step_avg:57.66ms
step:1055/2330 train_time:60826ms step_avg:57.65ms
step:1056/2330 train_time:60887ms step_avg:57.66ms
step:1057/2330 train_time:60943ms step_avg:57.66ms
step:1058/2330 train_time:61005ms step_avg:57.66ms
step:1059/2330 train_time:61061ms step_avg:57.66ms
step:1060/2330 train_time:61122ms step_avg:57.66ms
step:1061/2330 train_time:61179ms step_avg:57.66ms
step:1062/2330 train_time:61238ms step_avg:57.66ms
step:1063/2330 train_time:61295ms step_avg:57.66ms
step:1064/2330 train_time:61354ms step_avg:57.66ms
step:1065/2330 train_time:61411ms step_avg:57.66ms
step:1066/2330 train_time:61472ms step_avg:57.67ms
step:1067/2330 train_time:61528ms step_avg:57.66ms
step:1068/2330 train_time:61589ms step_avg:57.67ms
step:1069/2330 train_time:61645ms step_avg:57.67ms
step:1070/2330 train_time:61706ms step_avg:57.67ms
step:1071/2330 train_time:61762ms step_avg:57.67ms
step:1072/2330 train_time:61823ms step_avg:57.67ms
step:1073/2330 train_time:61880ms step_avg:57.67ms
step:1074/2330 train_time:61940ms step_avg:57.67ms
step:1075/2330 train_time:61998ms step_avg:57.67ms
step:1076/2330 train_time:62057ms step_avg:57.67ms
step:1077/2330 train_time:62114ms step_avg:57.67ms
step:1078/2330 train_time:62174ms step_avg:57.68ms
step:1079/2330 train_time:62231ms step_avg:57.68ms
step:1080/2330 train_time:62291ms step_avg:57.68ms
step:1081/2330 train_time:62347ms step_avg:57.68ms
step:1082/2330 train_time:62408ms step_avg:57.68ms
step:1083/2330 train_time:62465ms step_avg:57.68ms
step:1084/2330 train_time:62525ms step_avg:57.68ms
step:1085/2330 train_time:62582ms step_avg:57.68ms
step:1086/2330 train_time:62641ms step_avg:57.68ms
step:1087/2330 train_time:62699ms step_avg:57.68ms
step:1088/2330 train_time:62758ms step_avg:57.68ms
step:1089/2330 train_time:62815ms step_avg:57.68ms
step:1090/2330 train_time:62875ms step_avg:57.68ms
step:1091/2330 train_time:62932ms step_avg:57.68ms
step:1092/2330 train_time:62994ms step_avg:57.69ms
step:1093/2330 train_time:63050ms step_avg:57.69ms
step:1094/2330 train_time:63113ms step_avg:57.69ms
step:1095/2330 train_time:63170ms step_avg:57.69ms
step:1096/2330 train_time:63230ms step_avg:57.69ms
step:1097/2330 train_time:63287ms step_avg:57.69ms
step:1098/2330 train_time:63347ms step_avg:57.69ms
step:1099/2330 train_time:63405ms step_avg:57.69ms
step:1100/2330 train_time:63464ms step_avg:57.69ms
step:1101/2330 train_time:63521ms step_avg:57.69ms
step:1102/2330 train_time:63581ms step_avg:57.70ms
step:1103/2330 train_time:63637ms step_avg:57.69ms
step:1104/2330 train_time:63697ms step_avg:57.70ms
step:1105/2330 train_time:63754ms step_avg:57.70ms
step:1106/2330 train_time:63815ms step_avg:57.70ms
step:1107/2330 train_time:63872ms step_avg:57.70ms
step:1108/2330 train_time:63932ms step_avg:57.70ms
step:1109/2330 train_time:63989ms step_avg:57.70ms
step:1110/2330 train_time:64049ms step_avg:57.70ms
step:1111/2330 train_time:64106ms step_avg:57.70ms
step:1112/2330 train_time:64167ms step_avg:57.70ms
step:1113/2330 train_time:64223ms step_avg:57.70ms
step:1114/2330 train_time:64283ms step_avg:57.70ms
step:1115/2330 train_time:64339ms step_avg:57.70ms
step:1116/2330 train_time:64399ms step_avg:57.71ms
step:1117/2330 train_time:64456ms step_avg:57.70ms
step:1118/2330 train_time:64516ms step_avg:57.71ms
step:1119/2330 train_time:64574ms step_avg:57.71ms
step:1120/2330 train_time:64634ms step_avg:57.71ms
step:1121/2330 train_time:64690ms step_avg:57.71ms
step:1122/2330 train_time:64750ms step_avg:57.71ms
step:1123/2330 train_time:64806ms step_avg:57.71ms
step:1124/2330 train_time:64868ms step_avg:57.71ms
step:1125/2330 train_time:64925ms step_avg:57.71ms
step:1126/2330 train_time:64986ms step_avg:57.71ms
step:1127/2330 train_time:65042ms step_avg:57.71ms
step:1128/2330 train_time:65104ms step_avg:57.72ms
step:1129/2330 train_time:65160ms step_avg:57.72ms
step:1130/2330 train_time:65221ms step_avg:57.72ms
step:1131/2330 train_time:65277ms step_avg:57.72ms
step:1132/2330 train_time:65338ms step_avg:57.72ms
step:1133/2330 train_time:65395ms step_avg:57.72ms
step:1134/2330 train_time:65454ms step_avg:57.72ms
step:1135/2330 train_time:65511ms step_avg:57.72ms
step:1136/2330 train_time:65572ms step_avg:57.72ms
step:1137/2330 train_time:65628ms step_avg:57.72ms
step:1138/2330 train_time:65688ms step_avg:57.72ms
step:1139/2330 train_time:65744ms step_avg:57.72ms
step:1140/2330 train_time:65805ms step_avg:57.72ms
step:1141/2330 train_time:65862ms step_avg:57.72ms
step:1142/2330 train_time:65923ms step_avg:57.73ms
step:1143/2330 train_time:65980ms step_avg:57.73ms
step:1144/2330 train_time:66040ms step_avg:57.73ms
step:1145/2330 train_time:66098ms step_avg:57.73ms
step:1146/2330 train_time:66157ms step_avg:57.73ms
step:1147/2330 train_time:66214ms step_avg:57.73ms
step:1148/2330 train_time:66274ms step_avg:57.73ms
step:1149/2330 train_time:66331ms step_avg:57.73ms
step:1150/2330 train_time:66392ms step_avg:57.73ms
step:1151/2330 train_time:66448ms step_avg:57.73ms
step:1152/2330 train_time:66509ms step_avg:57.73ms
step:1153/2330 train_time:66564ms step_avg:57.73ms
step:1154/2330 train_time:66625ms step_avg:57.73ms
step:1155/2330 train_time:66681ms step_avg:57.73ms
step:1156/2330 train_time:66743ms step_avg:57.74ms
step:1157/2330 train_time:66800ms step_avg:57.74ms
step:1158/2330 train_time:66859ms step_avg:57.74ms
step:1159/2330 train_time:66917ms step_avg:57.74ms
step:1160/2330 train_time:66976ms step_avg:57.74ms
step:1161/2330 train_time:67033ms step_avg:57.74ms
step:1162/2330 train_time:67094ms step_avg:57.74ms
step:1163/2330 train_time:67150ms step_avg:57.74ms
step:1164/2330 train_time:67211ms step_avg:57.74ms
step:1165/2330 train_time:67268ms step_avg:57.74ms
step:1166/2330 train_time:67329ms step_avg:57.74ms
step:1167/2330 train_time:67386ms step_avg:57.74ms
step:1168/2330 train_time:67446ms step_avg:57.74ms
step:1169/2330 train_time:67503ms step_avg:57.74ms
step:1170/2330 train_time:67562ms step_avg:57.75ms
step:1171/2330 train_time:67619ms step_avg:57.74ms
step:1172/2330 train_time:67679ms step_avg:57.75ms
step:1173/2330 train_time:67735ms step_avg:57.75ms
step:1174/2330 train_time:67795ms step_avg:57.75ms
step:1175/2330 train_time:67853ms step_avg:57.75ms
step:1176/2330 train_time:67913ms step_avg:57.75ms
step:1177/2330 train_time:67970ms step_avg:57.75ms
step:1178/2330 train_time:68030ms step_avg:57.75ms
step:1179/2330 train_time:68086ms step_avg:57.75ms
step:1180/2330 train_time:68146ms step_avg:57.75ms
step:1181/2330 train_time:68203ms step_avg:57.75ms
step:1182/2330 train_time:68264ms step_avg:57.75ms
step:1183/2330 train_time:68320ms step_avg:57.75ms
step:1184/2330 train_time:68381ms step_avg:57.75ms
step:1185/2330 train_time:68438ms step_avg:57.75ms
step:1186/2330 train_time:68497ms step_avg:57.75ms
step:1187/2330 train_time:68554ms step_avg:57.75ms
step:1188/2330 train_time:68615ms step_avg:57.76ms
step:1189/2330 train_time:68672ms step_avg:57.76ms
step:1190/2330 train_time:68733ms step_avg:57.76ms
step:1191/2330 train_time:68789ms step_avg:57.76ms
step:1192/2330 train_time:68849ms step_avg:57.76ms
step:1193/2330 train_time:68905ms step_avg:57.76ms
step:1194/2330 train_time:68967ms step_avg:57.76ms
step:1195/2330 train_time:69023ms step_avg:57.76ms
step:1196/2330 train_time:69085ms step_avg:57.76ms
step:1197/2330 train_time:69141ms step_avg:57.76ms
step:1198/2330 train_time:69201ms step_avg:57.76ms
step:1199/2330 train_time:69258ms step_avg:57.76ms
step:1200/2330 train_time:69318ms step_avg:57.77ms
step:1201/2330 train_time:69375ms step_avg:57.76ms
step:1202/2330 train_time:69435ms step_avg:57.77ms
step:1203/2330 train_time:69491ms step_avg:57.77ms
step:1204/2330 train_time:69553ms step_avg:57.77ms
step:1205/2330 train_time:69609ms step_avg:57.77ms
step:1206/2330 train_time:69671ms step_avg:57.77ms
step:1207/2330 train_time:69727ms step_avg:57.77ms
step:1208/2330 train_time:69788ms step_avg:57.77ms
step:1209/2330 train_time:69844ms step_avg:57.77ms
step:1210/2330 train_time:69905ms step_avg:57.77ms
step:1211/2330 train_time:69961ms step_avg:57.77ms
step:1212/2330 train_time:70021ms step_avg:57.77ms
step:1213/2330 train_time:70078ms step_avg:57.77ms
step:1214/2330 train_time:70138ms step_avg:57.77ms
step:1215/2330 train_time:70195ms step_avg:57.77ms
step:1216/2330 train_time:70255ms step_avg:57.78ms
step:1217/2330 train_time:70311ms step_avg:57.77ms
step:1218/2330 train_time:70372ms step_avg:57.78ms
step:1219/2330 train_time:70429ms step_avg:57.78ms
step:1220/2330 train_time:70490ms step_avg:57.78ms
step:1221/2330 train_time:70547ms step_avg:57.78ms
step:1222/2330 train_time:70608ms step_avg:57.78ms
step:1223/2330 train_time:70664ms step_avg:57.78ms
step:1224/2330 train_time:70726ms step_avg:57.78ms
step:1225/2330 train_time:70782ms step_avg:57.78ms
step:1226/2330 train_time:70842ms step_avg:57.78ms
step:1227/2330 train_time:70899ms step_avg:57.78ms
step:1228/2330 train_time:70959ms step_avg:57.78ms
step:1229/2330 train_time:71015ms step_avg:57.78ms
step:1230/2330 train_time:71076ms step_avg:57.79ms
step:1231/2330 train_time:71133ms step_avg:57.78ms
step:1232/2330 train_time:71193ms step_avg:57.79ms
step:1233/2330 train_time:71250ms step_avg:57.79ms
step:1234/2330 train_time:71311ms step_avg:57.79ms
step:1235/2330 train_time:71367ms step_avg:57.79ms
step:1236/2330 train_time:71428ms step_avg:57.79ms
step:1237/2330 train_time:71484ms step_avg:57.79ms
step:1238/2330 train_time:71545ms step_avg:57.79ms
step:1239/2330 train_time:71601ms step_avg:57.79ms
step:1240/2330 train_time:71663ms step_avg:57.79ms
step:1241/2330 train_time:71719ms step_avg:57.79ms
step:1242/2330 train_time:71779ms step_avg:57.79ms
step:1243/2330 train_time:71836ms step_avg:57.79ms
step:1244/2330 train_time:71896ms step_avg:57.79ms
step:1245/2330 train_time:71952ms step_avg:57.79ms
step:1246/2330 train_time:72014ms step_avg:57.80ms
step:1247/2330 train_time:72071ms step_avg:57.80ms
step:1248/2330 train_time:72131ms step_avg:57.80ms
step:1249/2330 train_time:72187ms step_avg:57.80ms
step:1250/2330 train_time:72249ms step_avg:57.80ms
step:1250/2330 val_loss:5.0916 train_time:72330ms step_avg:57.86ms
step:1251/2330 train_time:72350ms step_avg:57.83ms
step:1252/2330 train_time:72371ms step_avg:57.80ms
step:1253/2330 train_time:72429ms step_avg:57.80ms
step:1254/2330 train_time:72494ms step_avg:57.81ms
step:1255/2330 train_time:72551ms step_avg:57.81ms
step:1256/2330 train_time:72614ms step_avg:57.81ms
step:1257/2330 train_time:72670ms step_avg:57.81ms
step:1258/2330 train_time:72732ms step_avg:57.82ms
step:1259/2330 train_time:72788ms step_avg:57.81ms
step:1260/2330 train_time:72848ms step_avg:57.82ms
step:1261/2330 train_time:72904ms step_avg:57.81ms
step:1262/2330 train_time:72963ms step_avg:57.82ms
step:1263/2330 train_time:73019ms step_avg:57.81ms
step:1264/2330 train_time:73079ms step_avg:57.82ms
step:1265/2330 train_time:73135ms step_avg:57.81ms
step:1266/2330 train_time:73194ms step_avg:57.82ms
step:1267/2330 train_time:73251ms step_avg:57.81ms
step:1268/2330 train_time:73311ms step_avg:57.82ms
step:1269/2330 train_time:73369ms step_avg:57.82ms
step:1270/2330 train_time:73432ms step_avg:57.82ms
step:1271/2330 train_time:73489ms step_avg:57.82ms
step:1272/2330 train_time:73551ms step_avg:57.82ms
step:1273/2330 train_time:73608ms step_avg:57.82ms
step:1274/2330 train_time:73669ms step_avg:57.82ms
step:1275/2330 train_time:73725ms step_avg:57.82ms
step:1276/2330 train_time:73786ms step_avg:57.83ms
step:1277/2330 train_time:73842ms step_avg:57.82ms
step:1278/2330 train_time:73902ms step_avg:57.83ms
step:1279/2330 train_time:73959ms step_avg:57.83ms
step:1280/2330 train_time:74018ms step_avg:57.83ms
step:1281/2330 train_time:74074ms step_avg:57.83ms
step:1282/2330 train_time:74134ms step_avg:57.83ms
step:1283/2330 train_time:74191ms step_avg:57.83ms
step:1284/2330 train_time:74250ms step_avg:57.83ms
step:1285/2330 train_time:74307ms step_avg:57.83ms
step:1286/2330 train_time:74368ms step_avg:57.83ms
step:1287/2330 train_time:74425ms step_avg:57.83ms
step:1288/2330 train_time:74486ms step_avg:57.83ms
step:1289/2330 train_time:74543ms step_avg:57.83ms
step:1290/2330 train_time:74604ms step_avg:57.83ms
step:1291/2330 train_time:74661ms step_avg:57.83ms
step:1292/2330 train_time:74721ms step_avg:57.83ms
step:1293/2330 train_time:74778ms step_avg:57.83ms
step:1294/2330 train_time:74838ms step_avg:57.83ms
step:1295/2330 train_time:74895ms step_avg:57.83ms
step:1296/2330 train_time:74956ms step_avg:57.84ms
step:1297/2330 train_time:75012ms step_avg:57.83ms
step:1298/2330 train_time:75071ms step_avg:57.84ms
step:1299/2330 train_time:75128ms step_avg:57.83ms
step:1300/2330 train_time:75187ms step_avg:57.84ms
step:1301/2330 train_time:75243ms step_avg:57.84ms
step:1302/2330 train_time:75304ms step_avg:57.84ms
step:1303/2330 train_time:75361ms step_avg:57.84ms
step:1304/2330 train_time:75421ms step_avg:57.84ms
step:1305/2330 train_time:75479ms step_avg:57.84ms
step:1306/2330 train_time:75540ms step_avg:57.84ms
step:1307/2330 train_time:75598ms step_avg:57.84ms
step:1308/2330 train_time:75658ms step_avg:57.84ms
step:1309/2330 train_time:75715ms step_avg:57.84ms
step:1310/2330 train_time:75775ms step_avg:57.84ms
step:1311/2330 train_time:75832ms step_avg:57.84ms
step:1312/2330 train_time:75892ms step_avg:57.84ms
step:1313/2330 train_time:75949ms step_avg:57.84ms
step:1314/2330 train_time:76008ms step_avg:57.85ms
step:1315/2330 train_time:76065ms step_avg:57.84ms
step:1316/2330 train_time:76126ms step_avg:57.85ms
step:1317/2330 train_time:76183ms step_avg:57.85ms
step:1318/2330 train_time:76242ms step_avg:57.85ms
step:1319/2330 train_time:76299ms step_avg:57.85ms
step:1320/2330 train_time:76358ms step_avg:57.85ms
step:1321/2330 train_time:76415ms step_avg:57.85ms
step:1322/2330 train_time:76477ms step_avg:57.85ms
step:1323/2330 train_time:76534ms step_avg:57.85ms
step:1324/2330 train_time:76595ms step_avg:57.85ms
step:1325/2330 train_time:76652ms step_avg:57.85ms
step:1326/2330 train_time:76712ms step_avg:57.85ms
step:1327/2330 train_time:76769ms step_avg:57.85ms
step:1328/2330 train_time:76830ms step_avg:57.85ms
step:1329/2330 train_time:76886ms step_avg:57.85ms
step:1330/2330 train_time:76946ms step_avg:57.85ms
step:1331/2330 train_time:77003ms step_avg:57.85ms
step:1332/2330 train_time:77062ms step_avg:57.85ms
step:1333/2330 train_time:77119ms step_avg:57.85ms
step:1334/2330 train_time:77178ms step_avg:57.85ms
step:1335/2330 train_time:77236ms step_avg:57.85ms
step:1336/2330 train_time:77296ms step_avg:57.86ms
step:1337/2330 train_time:77353ms step_avg:57.86ms
step:1338/2330 train_time:77414ms step_avg:57.86ms
step:1339/2330 train_time:77471ms step_avg:57.86ms
step:1340/2330 train_time:77531ms step_avg:57.86ms
step:1341/2330 train_time:77587ms step_avg:57.86ms
step:1342/2330 train_time:77648ms step_avg:57.86ms
step:1343/2330 train_time:77705ms step_avg:57.86ms
step:1344/2330 train_time:77765ms step_avg:57.86ms
step:1345/2330 train_time:77821ms step_avg:57.86ms
step:1346/2330 train_time:77881ms step_avg:57.86ms
step:1347/2330 train_time:77939ms step_avg:57.86ms
step:1348/2330 train_time:77999ms step_avg:57.86ms
step:1349/2330 train_time:78056ms step_avg:57.86ms
step:1350/2330 train_time:78116ms step_avg:57.86ms
step:1351/2330 train_time:78173ms step_avg:57.86ms
step:1352/2330 train_time:78232ms step_avg:57.86ms
step:1353/2330 train_time:78290ms step_avg:57.86ms
step:1354/2330 train_time:78350ms step_avg:57.87ms
step:1355/2330 train_time:78407ms step_avg:57.86ms
step:1356/2330 train_time:78468ms step_avg:57.87ms
step:1357/2330 train_time:78524ms step_avg:57.87ms
step:1358/2330 train_time:78584ms step_avg:57.87ms
step:1359/2330 train_time:78641ms step_avg:57.87ms
step:1360/2330 train_time:78701ms step_avg:57.87ms
step:1361/2330 train_time:78759ms step_avg:57.87ms
step:1362/2330 train_time:78819ms step_avg:57.87ms
step:1363/2330 train_time:78876ms step_avg:57.87ms
step:1364/2330 train_time:78937ms step_avg:57.87ms
step:1365/2330 train_time:78994ms step_avg:57.87ms
step:1366/2330 train_time:79053ms step_avg:57.87ms
step:1367/2330 train_time:79110ms step_avg:57.87ms
step:1368/2330 train_time:79170ms step_avg:57.87ms
step:1369/2330 train_time:79227ms step_avg:57.87ms
step:1370/2330 train_time:79287ms step_avg:57.87ms
step:1371/2330 train_time:79344ms step_avg:57.87ms
step:1372/2330 train_time:79404ms step_avg:57.87ms
step:1373/2330 train_time:79461ms step_avg:57.87ms
step:1374/2330 train_time:79521ms step_avg:57.88ms
step:1375/2330 train_time:79577ms step_avg:57.87ms
step:1376/2330 train_time:79637ms step_avg:57.88ms
step:1377/2330 train_time:79694ms step_avg:57.88ms
step:1378/2330 train_time:79755ms step_avg:57.88ms
step:1379/2330 train_time:79811ms step_avg:57.88ms
step:1380/2330 train_time:79872ms step_avg:57.88ms
step:1381/2330 train_time:79928ms step_avg:57.88ms
step:1382/2330 train_time:79990ms step_avg:57.88ms
step:1383/2330 train_time:80047ms step_avg:57.88ms
step:1384/2330 train_time:80108ms step_avg:57.88ms
step:1385/2330 train_time:80165ms step_avg:57.88ms
step:1386/2330 train_time:80224ms step_avg:57.88ms
step:1387/2330 train_time:80281ms step_avg:57.88ms
step:1388/2330 train_time:80341ms step_avg:57.88ms
step:1389/2330 train_time:80399ms step_avg:57.88ms
step:1390/2330 train_time:80458ms step_avg:57.88ms
step:1391/2330 train_time:80516ms step_avg:57.88ms
step:1392/2330 train_time:80575ms step_avg:57.88ms
step:1393/2330 train_time:80632ms step_avg:57.88ms
step:1394/2330 train_time:80693ms step_avg:57.89ms
step:1395/2330 train_time:80750ms step_avg:57.89ms
step:1396/2330 train_time:80810ms step_avg:57.89ms
step:1397/2330 train_time:80867ms step_avg:57.89ms
step:1398/2330 train_time:80927ms step_avg:57.89ms
step:1399/2330 train_time:80983ms step_avg:57.89ms
step:1400/2330 train_time:81045ms step_avg:57.89ms
step:1401/2330 train_time:81101ms step_avg:57.89ms
step:1402/2330 train_time:81160ms step_avg:57.89ms
step:1403/2330 train_time:81217ms step_avg:57.89ms
step:1404/2330 train_time:81278ms step_avg:57.89ms
step:1405/2330 train_time:81335ms step_avg:57.89ms
step:1406/2330 train_time:81395ms step_avg:57.89ms
step:1407/2330 train_time:81453ms step_avg:57.89ms
step:1408/2330 train_time:81512ms step_avg:57.89ms
step:1409/2330 train_time:81568ms step_avg:57.89ms
step:1410/2330 train_time:81629ms step_avg:57.89ms
step:1411/2330 train_time:81687ms step_avg:57.89ms
step:1412/2330 train_time:81746ms step_avg:57.89ms
step:1413/2330 train_time:81803ms step_avg:57.89ms
step:1414/2330 train_time:81863ms step_avg:57.89ms
step:1415/2330 train_time:81920ms step_avg:57.89ms
step:1416/2330 train_time:81980ms step_avg:57.90ms
step:1417/2330 train_time:82037ms step_avg:57.90ms
step:1418/2330 train_time:82097ms step_avg:57.90ms
step:1419/2330 train_time:82154ms step_avg:57.90ms
step:1420/2330 train_time:82214ms step_avg:57.90ms
step:1421/2330 train_time:82271ms step_avg:57.90ms
step:1422/2330 train_time:82332ms step_avg:57.90ms
step:1423/2330 train_time:82389ms step_avg:57.90ms
step:1424/2330 train_time:82450ms step_avg:57.90ms
step:1425/2330 train_time:82507ms step_avg:57.90ms
step:1426/2330 train_time:82568ms step_avg:57.90ms
step:1427/2330 train_time:82624ms step_avg:57.90ms
step:1428/2330 train_time:82685ms step_avg:57.90ms
step:1429/2330 train_time:82741ms step_avg:57.90ms
step:1430/2330 train_time:82802ms step_avg:57.90ms
step:1431/2330 train_time:82859ms step_avg:57.90ms
step:1432/2330 train_time:82919ms step_avg:57.90ms
step:1433/2330 train_time:82976ms step_avg:57.90ms
step:1434/2330 train_time:83036ms step_avg:57.91ms
step:1435/2330 train_time:83093ms step_avg:57.90ms
step:1436/2330 train_time:83153ms step_avg:57.91ms
step:1437/2330 train_time:83210ms step_avg:57.91ms
step:1438/2330 train_time:83269ms step_avg:57.91ms
step:1439/2330 train_time:83326ms step_avg:57.91ms
step:1440/2330 train_time:83386ms step_avg:57.91ms
step:1441/2330 train_time:83443ms step_avg:57.91ms
step:1442/2330 train_time:83503ms step_avg:57.91ms
step:1443/2330 train_time:83560ms step_avg:57.91ms
step:1444/2330 train_time:83619ms step_avg:57.91ms
step:1445/2330 train_time:83676ms step_avg:57.91ms
step:1446/2330 train_time:83737ms step_avg:57.91ms
step:1447/2330 train_time:83794ms step_avg:57.91ms
step:1448/2330 train_time:83854ms step_avg:57.91ms
step:1449/2330 train_time:83911ms step_avg:57.91ms
step:1450/2330 train_time:83971ms step_avg:57.91ms
step:1451/2330 train_time:84028ms step_avg:57.91ms
step:1452/2330 train_time:84088ms step_avg:57.91ms
step:1453/2330 train_time:84146ms step_avg:57.91ms
step:1454/2330 train_time:84205ms step_avg:57.91ms
step:1455/2330 train_time:84262ms step_avg:57.91ms
step:1456/2330 train_time:84322ms step_avg:57.91ms
step:1457/2330 train_time:84378ms step_avg:57.91ms
step:1458/2330 train_time:84439ms step_avg:57.91ms
step:1459/2330 train_time:84496ms step_avg:57.91ms
step:1460/2330 train_time:84556ms step_avg:57.92ms
step:1461/2330 train_time:84613ms step_avg:57.91ms
step:1462/2330 train_time:84673ms step_avg:57.92ms
step:1463/2330 train_time:84730ms step_avg:57.92ms
step:1464/2330 train_time:84791ms step_avg:57.92ms
step:1465/2330 train_time:84847ms step_avg:57.92ms
step:1466/2330 train_time:84908ms step_avg:57.92ms
step:1467/2330 train_time:84965ms step_avg:57.92ms
step:1468/2330 train_time:85024ms step_avg:57.92ms
step:1469/2330 train_time:85081ms step_avg:57.92ms
step:1470/2330 train_time:85140ms step_avg:57.92ms
step:1471/2330 train_time:85197ms step_avg:57.92ms
step:1472/2330 train_time:85257ms step_avg:57.92ms
step:1473/2330 train_time:85314ms step_avg:57.92ms
step:1474/2330 train_time:85374ms step_avg:57.92ms
step:1475/2330 train_time:85431ms step_avg:57.92ms
step:1476/2330 train_time:85491ms step_avg:57.92ms
step:1477/2330 train_time:85548ms step_avg:57.92ms
step:1478/2330 train_time:85608ms step_avg:57.92ms
step:1479/2330 train_time:85664ms step_avg:57.92ms
step:1480/2330 train_time:85724ms step_avg:57.92ms
step:1481/2330 train_time:85781ms step_avg:57.92ms
step:1482/2330 train_time:85841ms step_avg:57.92ms
step:1483/2330 train_time:85898ms step_avg:57.92ms
step:1484/2330 train_time:85958ms step_avg:57.92ms
step:1485/2330 train_time:86015ms step_avg:57.92ms
step:1486/2330 train_time:86075ms step_avg:57.92ms
step:1487/2330 train_time:86133ms step_avg:57.92ms
step:1488/2330 train_time:86192ms step_avg:57.93ms
step:1489/2330 train_time:86249ms step_avg:57.92ms
step:1490/2330 train_time:86309ms step_avg:57.93ms
step:1491/2330 train_time:86366ms step_avg:57.92ms
step:1492/2330 train_time:86426ms step_avg:57.93ms
step:1493/2330 train_time:86483ms step_avg:57.93ms
step:1494/2330 train_time:86544ms step_avg:57.93ms
step:1495/2330 train_time:86600ms step_avg:57.93ms
step:1496/2330 train_time:86660ms step_avg:57.93ms
step:1497/2330 train_time:86717ms step_avg:57.93ms
step:1498/2330 train_time:86777ms step_avg:57.93ms
step:1499/2330 train_time:86834ms step_avg:57.93ms
step:1500/2330 train_time:86894ms step_avg:57.93ms
step:1500/2330 val_loss:4.9309 train_time:86975ms step_avg:57.98ms
step:1501/2330 train_time:86994ms step_avg:57.96ms
step:1502/2330 train_time:87014ms step_avg:57.93ms
step:1503/2330 train_time:87070ms step_avg:57.93ms
step:1504/2330 train_time:87135ms step_avg:57.94ms
step:1505/2330 train_time:87192ms step_avg:57.94ms
step:1506/2330 train_time:87253ms step_avg:57.94ms
step:1507/2330 train_time:87310ms step_avg:57.94ms
step:1508/2330 train_time:87370ms step_avg:57.94ms
step:1509/2330 train_time:87426ms step_avg:57.94ms
step:1510/2330 train_time:87486ms step_avg:57.94ms
step:1511/2330 train_time:87542ms step_avg:57.94ms
step:1512/2330 train_time:87601ms step_avg:57.94ms
step:1513/2330 train_time:87657ms step_avg:57.94ms
step:1514/2330 train_time:87716ms step_avg:57.94ms
step:1515/2330 train_time:87772ms step_avg:57.94ms
step:1516/2330 train_time:87832ms step_avg:57.94ms
step:1517/2330 train_time:87889ms step_avg:57.94ms
step:1518/2330 train_time:87950ms step_avg:57.94ms
step:1519/2330 train_time:88008ms step_avg:57.94ms
step:1520/2330 train_time:88071ms step_avg:57.94ms
step:1521/2330 train_time:88129ms step_avg:57.94ms
step:1522/2330 train_time:88192ms step_avg:57.94ms
step:1523/2330 train_time:88249ms step_avg:57.94ms
step:1524/2330 train_time:88310ms step_avg:57.95ms
step:1525/2330 train_time:88367ms step_avg:57.95ms
step:1526/2330 train_time:88427ms step_avg:57.95ms
step:1527/2330 train_time:88484ms step_avg:57.95ms
step:1528/2330 train_time:88544ms step_avg:57.95ms
step:1529/2330 train_time:88601ms step_avg:57.95ms
step:1530/2330 train_time:88661ms step_avg:57.95ms
step:1531/2330 train_time:88717ms step_avg:57.95ms
step:1532/2330 train_time:88778ms step_avg:57.95ms
step:1533/2330 train_time:88835ms step_avg:57.95ms
step:1534/2330 train_time:88895ms step_avg:57.95ms
step:1535/2330 train_time:88952ms step_avg:57.95ms
step:1536/2330 train_time:89013ms step_avg:57.95ms
step:1537/2330 train_time:89071ms step_avg:57.95ms
step:1538/2330 train_time:89133ms step_avg:57.95ms
step:1539/2330 train_time:89191ms step_avg:57.95ms
step:1540/2330 train_time:89251ms step_avg:57.96ms
step:1541/2330 train_time:89309ms step_avg:57.96ms
step:1542/2330 train_time:89370ms step_avg:57.96ms
step:1543/2330 train_time:89428ms step_avg:57.96ms
step:1544/2330 train_time:89489ms step_avg:57.96ms
step:1545/2330 train_time:89546ms step_avg:57.96ms
step:1546/2330 train_time:89607ms step_avg:57.96ms
step:1547/2330 train_time:89665ms step_avg:57.96ms
step:1548/2330 train_time:89726ms step_avg:57.96ms
step:1549/2330 train_time:89782ms step_avg:57.96ms
step:1550/2330 train_time:89844ms step_avg:57.96ms
step:1551/2330 train_time:89900ms step_avg:57.96ms
step:1552/2330 train_time:89963ms step_avg:57.97ms
step:1553/2330 train_time:90020ms step_avg:57.97ms
step:1554/2330 train_time:90083ms step_avg:57.97ms
step:1555/2330 train_time:90140ms step_avg:57.97ms
step:1556/2330 train_time:90202ms step_avg:57.97ms
step:1557/2330 train_time:90259ms step_avg:57.97ms
step:1558/2330 train_time:90321ms step_avg:57.97ms
step:1559/2330 train_time:90377ms step_avg:57.97ms
step:1560/2330 train_time:90438ms step_avg:57.97ms
step:1561/2330 train_time:90495ms step_avg:57.97ms
step:1562/2330 train_time:90556ms step_avg:57.97ms
step:1563/2330 train_time:90614ms step_avg:57.97ms
step:1564/2330 train_time:90674ms step_avg:57.98ms
step:1565/2330 train_time:90732ms step_avg:57.98ms
step:1566/2330 train_time:90792ms step_avg:57.98ms
step:1567/2330 train_time:90850ms step_avg:57.98ms
step:1568/2330 train_time:90911ms step_avg:57.98ms
step:1569/2330 train_time:90968ms step_avg:57.98ms
step:1570/2330 train_time:91030ms step_avg:57.98ms
step:1571/2330 train_time:91087ms step_avg:57.98ms
step:1572/2330 train_time:91149ms step_avg:57.98ms
step:1573/2330 train_time:91206ms step_avg:57.98ms
step:1574/2330 train_time:91268ms step_avg:57.98ms
step:1575/2330 train_time:91325ms step_avg:57.98ms
step:1576/2330 train_time:91387ms step_avg:57.99ms
step:1577/2330 train_time:91443ms step_avg:57.99ms
step:1578/2330 train_time:91505ms step_avg:57.99ms
step:1579/2330 train_time:91561ms step_avg:57.99ms
step:1580/2330 train_time:91623ms step_avg:57.99ms
step:1581/2330 train_time:91679ms step_avg:57.99ms
step:1582/2330 train_time:91741ms step_avg:57.99ms
step:1583/2330 train_time:91797ms step_avg:57.99ms
step:1584/2330 train_time:91859ms step_avg:57.99ms
step:1585/2330 train_time:91915ms step_avg:57.99ms
step:1586/2330 train_time:91976ms step_avg:57.99ms
step:1587/2330 train_time:92032ms step_avg:57.99ms
step:1588/2330 train_time:92093ms step_avg:57.99ms
step:1589/2330 train_time:92150ms step_avg:57.99ms
step:1590/2330 train_time:92212ms step_avg:57.99ms
step:1591/2330 train_time:92269ms step_avg:57.99ms
step:1592/2330 train_time:92331ms step_avg:58.00ms
step:1593/2330 train_time:92390ms step_avg:58.00ms
step:1594/2330 train_time:92451ms step_avg:58.00ms
step:1595/2330 train_time:92508ms step_avg:58.00ms
step:1596/2330 train_time:92570ms step_avg:58.00ms
step:1597/2330 train_time:92627ms step_avg:58.00ms
step:1598/2330 train_time:92687ms step_avg:58.00ms
step:1599/2330 train_time:92744ms step_avg:58.00ms
step:1600/2330 train_time:92805ms step_avg:58.00ms
step:1601/2330 train_time:92862ms step_avg:58.00ms
step:1602/2330 train_time:92924ms step_avg:58.01ms
step:1603/2330 train_time:92980ms step_avg:58.00ms
step:1604/2330 train_time:93042ms step_avg:58.01ms
step:1605/2330 train_time:93098ms step_avg:58.01ms
step:1606/2330 train_time:93160ms step_avg:58.01ms
step:1607/2330 train_time:93216ms step_avg:58.01ms
step:1608/2330 train_time:93279ms step_avg:58.01ms
step:1609/2330 train_time:93335ms step_avg:58.01ms
step:1610/2330 train_time:93396ms step_avg:58.01ms
step:1611/2330 train_time:93452ms step_avg:58.01ms
step:1612/2330 train_time:93514ms step_avg:58.01ms
step:1613/2330 train_time:93571ms step_avg:58.01ms
step:1614/2330 train_time:93632ms step_avg:58.01ms
step:1615/2330 train_time:93689ms step_avg:58.01ms
step:1616/2330 train_time:93750ms step_avg:58.01ms
step:1617/2330 train_time:93807ms step_avg:58.01ms
step:1618/2330 train_time:93870ms step_avg:58.02ms
step:1619/2330 train_time:93927ms step_avg:58.02ms
step:1620/2330 train_time:93988ms step_avg:58.02ms
step:1621/2330 train_time:94045ms step_avg:58.02ms
step:1622/2330 train_time:94107ms step_avg:58.02ms
step:1623/2330 train_time:94164ms step_avg:58.02ms
step:1624/2330 train_time:94226ms step_avg:58.02ms
step:1625/2330 train_time:94283ms step_avg:58.02ms
step:1626/2330 train_time:94344ms step_avg:58.02ms
step:1627/2330 train_time:94400ms step_avg:58.02ms
step:1628/2330 train_time:94464ms step_avg:58.02ms
step:1629/2330 train_time:94520ms step_avg:58.02ms
step:1630/2330 train_time:94582ms step_avg:58.03ms
step:1631/2330 train_time:94638ms step_avg:58.02ms
step:1632/2330 train_time:94700ms step_avg:58.03ms
step:1633/2330 train_time:94756ms step_avg:58.03ms
step:1634/2330 train_time:94818ms step_avg:58.03ms
step:1635/2330 train_time:94874ms step_avg:58.03ms
step:1636/2330 train_time:94936ms step_avg:58.03ms
step:1637/2330 train_time:94993ms step_avg:58.03ms
step:1638/2330 train_time:95054ms step_avg:58.03ms
step:1639/2330 train_time:95112ms step_avg:58.03ms
step:1640/2330 train_time:95173ms step_avg:58.03ms
step:1641/2330 train_time:95231ms step_avg:58.03ms
step:1642/2330 train_time:95291ms step_avg:58.03ms
step:1643/2330 train_time:95349ms step_avg:58.03ms
step:1644/2330 train_time:95410ms step_avg:58.04ms
step:1645/2330 train_time:95468ms step_avg:58.04ms
step:1646/2330 train_time:95529ms step_avg:58.04ms
step:1647/2330 train_time:95587ms step_avg:58.04ms
step:1648/2330 train_time:95647ms step_avg:58.04ms
step:1649/2330 train_time:95703ms step_avg:58.04ms
step:1650/2330 train_time:95765ms step_avg:58.04ms
step:1651/2330 train_time:95822ms step_avg:58.04ms
step:1652/2330 train_time:95884ms step_avg:58.04ms
step:1653/2330 train_time:95941ms step_avg:58.04ms
step:1654/2330 train_time:96002ms step_avg:58.04ms
step:1655/2330 train_time:96059ms step_avg:58.04ms
step:1656/2330 train_time:96120ms step_avg:58.04ms
step:1657/2330 train_time:96176ms step_avg:58.04ms
step:1658/2330 train_time:96237ms step_avg:58.04ms
step:1659/2330 train_time:96294ms step_avg:58.04ms
step:1660/2330 train_time:96355ms step_avg:58.04ms
step:1661/2330 train_time:96412ms step_avg:58.04ms
step:1662/2330 train_time:96472ms step_avg:58.05ms
step:1663/2330 train_time:96530ms step_avg:58.05ms
step:1664/2330 train_time:96591ms step_avg:58.05ms
step:1665/2330 train_time:96649ms step_avg:58.05ms
step:1666/2330 train_time:96710ms step_avg:58.05ms
step:1667/2330 train_time:96768ms step_avg:58.05ms
step:1668/2330 train_time:96829ms step_avg:58.05ms
step:1669/2330 train_time:96887ms step_avg:58.05ms
step:1670/2330 train_time:96947ms step_avg:58.05ms
step:1671/2330 train_time:97004ms step_avg:58.05ms
step:1672/2330 train_time:97066ms step_avg:58.05ms
step:1673/2330 train_time:97123ms step_avg:58.05ms
step:1674/2330 train_time:97184ms step_avg:58.06ms
step:1675/2330 train_time:97240ms step_avg:58.05ms
step:1676/2330 train_time:97301ms step_avg:58.06ms
step:1677/2330 train_time:97357ms step_avg:58.05ms
step:1678/2330 train_time:97420ms step_avg:58.06ms
step:1679/2330 train_time:97476ms step_avg:58.06ms
step:1680/2330 train_time:97537ms step_avg:58.06ms
step:1681/2330 train_time:97593ms step_avg:58.06ms
step:1682/2330 train_time:97654ms step_avg:58.06ms
step:1683/2330 train_time:97711ms step_avg:58.06ms
step:1684/2330 train_time:97773ms step_avg:58.06ms
step:1685/2330 train_time:97831ms step_avg:58.06ms
step:1686/2330 train_time:97892ms step_avg:58.06ms
step:1687/2330 train_time:97950ms step_avg:58.06ms
step:1688/2330 train_time:98010ms step_avg:58.06ms
step:1689/2330 train_time:98069ms step_avg:58.06ms
step:1690/2330 train_time:98129ms step_avg:58.06ms
step:1691/2330 train_time:98188ms step_avg:58.07ms
step:1692/2330 train_time:98248ms step_avg:58.07ms
step:1693/2330 train_time:98306ms step_avg:58.07ms
step:1694/2330 train_time:98366ms step_avg:58.07ms
step:1695/2330 train_time:98423ms step_avg:58.07ms
step:1696/2330 train_time:98485ms step_avg:58.07ms
step:1697/2330 train_time:98541ms step_avg:58.07ms
step:1698/2330 train_time:98604ms step_avg:58.07ms
step:1699/2330 train_time:98660ms step_avg:58.07ms
step:1700/2330 train_time:98721ms step_avg:58.07ms
step:1701/2330 train_time:98777ms step_avg:58.07ms
step:1702/2330 train_time:98839ms step_avg:58.07ms
step:1703/2330 train_time:98896ms step_avg:58.07ms
step:1704/2330 train_time:98957ms step_avg:58.07ms
step:1705/2330 train_time:99014ms step_avg:58.07ms
step:1706/2330 train_time:99075ms step_avg:58.07ms
step:1707/2330 train_time:99133ms step_avg:58.07ms
step:1708/2330 train_time:99193ms step_avg:58.08ms
step:1709/2330 train_time:99250ms step_avg:58.08ms
step:1710/2330 train_time:99312ms step_avg:58.08ms
step:1711/2330 train_time:99369ms step_avg:58.08ms
step:1712/2330 train_time:99430ms step_avg:58.08ms
step:1713/2330 train_time:99489ms step_avg:58.08ms
step:1714/2330 train_time:99550ms step_avg:58.08ms
step:1715/2330 train_time:99607ms step_avg:58.08ms
step:1716/2330 train_time:99668ms step_avg:58.08ms
step:1717/2330 train_time:99725ms step_avg:58.08ms
step:1718/2330 train_time:99786ms step_avg:58.08ms
step:1719/2330 train_time:99843ms step_avg:58.08ms
step:1720/2330 train_time:99905ms step_avg:58.08ms
step:1721/2330 train_time:99961ms step_avg:58.08ms
step:1722/2330 train_time:100024ms step_avg:58.09ms
step:1723/2330 train_time:100081ms step_avg:58.09ms
step:1724/2330 train_time:100144ms step_avg:58.09ms
step:1725/2330 train_time:100200ms step_avg:58.09ms
step:1726/2330 train_time:100263ms step_avg:58.09ms
step:1727/2330 train_time:100320ms step_avg:58.09ms
step:1728/2330 train_time:100382ms step_avg:58.09ms
step:1729/2330 train_time:100438ms step_avg:58.09ms
step:1730/2330 train_time:100499ms step_avg:58.09ms
step:1731/2330 train_time:100556ms step_avg:58.09ms
step:1732/2330 train_time:100617ms step_avg:58.09ms
step:1733/2330 train_time:100674ms step_avg:58.09ms
step:1734/2330 train_time:100735ms step_avg:58.09ms
step:1735/2330 train_time:100793ms step_avg:58.09ms
step:1736/2330 train_time:100852ms step_avg:58.09ms
step:1737/2330 train_time:100910ms step_avg:58.09ms
step:1738/2330 train_time:100971ms step_avg:58.10ms
step:1739/2330 train_time:101028ms step_avg:58.10ms
step:1740/2330 train_time:101089ms step_avg:58.10ms
step:1741/2330 train_time:101147ms step_avg:58.10ms
step:1742/2330 train_time:101209ms step_avg:58.10ms
step:1743/2330 train_time:101267ms step_avg:58.10ms
step:1744/2330 train_time:101328ms step_avg:58.10ms
step:1745/2330 train_time:101385ms step_avg:58.10ms
step:1746/2330 train_time:101447ms step_avg:58.10ms
step:1747/2330 train_time:101503ms step_avg:58.10ms
step:1748/2330 train_time:101565ms step_avg:58.10ms
step:1749/2330 train_time:101622ms step_avg:58.10ms
step:1750/2330 train_time:101683ms step_avg:58.10ms
step:1750/2330 val_loss:4.7786 train_time:101765ms step_avg:58.15ms
step:1751/2330 train_time:101784ms step_avg:58.13ms
step:1752/2330 train_time:101804ms step_avg:58.11ms
step:1753/2330 train_time:101864ms step_avg:58.11ms
step:1754/2330 train_time:101933ms step_avg:58.11ms
step:1755/2330 train_time:101991ms step_avg:58.11ms
step:1756/2330 train_time:102051ms step_avg:58.12ms
step:1757/2330 train_time:102108ms step_avg:58.11ms
step:1758/2330 train_time:102167ms step_avg:58.12ms
step:1759/2330 train_time:102223ms step_avg:58.11ms
step:1760/2330 train_time:102284ms step_avg:58.12ms
step:1761/2330 train_time:102340ms step_avg:58.11ms
step:1762/2330 train_time:102401ms step_avg:58.12ms
step:1763/2330 train_time:102457ms step_avg:58.12ms
step:1764/2330 train_time:102517ms step_avg:58.12ms
step:1765/2330 train_time:102573ms step_avg:58.12ms
step:1766/2330 train_time:102632ms step_avg:58.12ms
step:1767/2330 train_time:102689ms step_avg:58.11ms
step:1768/2330 train_time:102753ms step_avg:58.12ms
step:1769/2330 train_time:102811ms step_avg:58.12ms
step:1770/2330 train_time:102875ms step_avg:58.12ms
step:1771/2330 train_time:102932ms step_avg:58.12ms
step:1772/2330 train_time:102994ms step_avg:58.12ms
step:1773/2330 train_time:103051ms step_avg:58.12ms
step:1774/2330 train_time:103113ms step_avg:58.12ms
step:1775/2330 train_time:103169ms step_avg:58.12ms
step:1776/2330 train_time:103229ms step_avg:58.12ms
step:1777/2330 train_time:103285ms step_avg:58.12ms
step:1778/2330 train_time:103345ms step_avg:58.12ms
step:1779/2330 train_time:103403ms step_avg:58.12ms
step:1780/2330 train_time:103462ms step_avg:58.12ms
step:1781/2330 train_time:103519ms step_avg:58.12ms
step:1782/2330 train_time:103578ms step_avg:58.12ms
step:1783/2330 train_time:103635ms step_avg:58.12ms
step:1784/2330 train_time:103696ms step_avg:58.13ms
step:1785/2330 train_time:103754ms step_avg:58.13ms
step:1786/2330 train_time:103816ms step_avg:58.13ms
step:1787/2330 train_time:103873ms step_avg:58.13ms
step:1788/2330 train_time:103937ms step_avg:58.13ms
step:1789/2330 train_time:103994ms step_avg:58.13ms
step:1790/2330 train_time:104055ms step_avg:58.13ms
step:1791/2330 train_time:104113ms step_avg:58.13ms
step:1792/2330 train_time:104175ms step_avg:58.13ms
step:1793/2330 train_time:104231ms step_avg:58.13ms
step:1794/2330 train_time:104293ms step_avg:58.13ms
step:1795/2330 train_time:104349ms step_avg:58.13ms
step:1796/2330 train_time:104409ms step_avg:58.13ms
step:1797/2330 train_time:104465ms step_avg:58.13ms
step:1798/2330 train_time:104526ms step_avg:58.13ms
step:1799/2330 train_time:104583ms step_avg:58.13ms
step:1800/2330 train_time:104642ms step_avg:58.13ms
step:1801/2330 train_time:104699ms step_avg:58.13ms
step:1802/2330 train_time:104762ms step_avg:58.14ms
step:1803/2330 train_time:104820ms step_avg:58.14ms
step:1804/2330 train_time:104882ms step_avg:58.14ms
step:1805/2330 train_time:104940ms step_avg:58.14ms
step:1806/2330 train_time:105001ms step_avg:58.14ms
step:1807/2330 train_time:105060ms step_avg:58.14ms
step:1808/2330 train_time:105122ms step_avg:58.14ms
step:1809/2330 train_time:105179ms step_avg:58.14ms
step:1810/2330 train_time:105239ms step_avg:58.14ms
step:1811/2330 train_time:105297ms step_avg:58.14ms
step:1812/2330 train_time:105357ms step_avg:58.14ms
step:1813/2330 train_time:105413ms step_avg:58.14ms
step:1814/2330 train_time:105476ms step_avg:58.15ms
step:1815/2330 train_time:105532ms step_avg:58.14ms
step:1816/2330 train_time:105593ms step_avg:58.15ms
step:1817/2330 train_time:105649ms step_avg:58.14ms
step:1818/2330 train_time:105711ms step_avg:58.15ms
step:1819/2330 train_time:105767ms step_avg:58.15ms
step:1820/2330 train_time:105828ms step_avg:58.15ms
step:1821/2330 train_time:105885ms step_avg:58.15ms
step:1822/2330 train_time:105946ms step_avg:58.15ms
step:1823/2330 train_time:106004ms step_avg:58.15ms
step:1824/2330 train_time:106064ms step_avg:58.15ms
step:1825/2330 train_time:106122ms step_avg:58.15ms
step:1826/2330 train_time:106183ms step_avg:58.15ms
step:1827/2330 train_time:106241ms step_avg:58.15ms
step:1828/2330 train_time:106301ms step_avg:58.15ms
step:1829/2330 train_time:106359ms step_avg:58.15ms
step:1830/2330 train_time:106419ms step_avg:58.15ms
step:1831/2330 train_time:106476ms step_avg:58.15ms
step:1832/2330 train_time:106537ms step_avg:58.15ms
step:1833/2330 train_time:106594ms step_avg:58.15ms
step:1834/2330 train_time:106656ms step_avg:58.15ms
step:1835/2330 train_time:106712ms step_avg:58.15ms
step:1836/2330 train_time:106774ms step_avg:58.16ms
step:1837/2330 train_time:106830ms step_avg:58.15ms
step:1838/2330 train_time:106892ms step_avg:58.16ms
step:1839/2330 train_time:106948ms step_avg:58.16ms
step:1840/2330 train_time:107010ms step_avg:58.16ms
step:1841/2330 train_time:107067ms step_avg:58.16ms
step:1842/2330 train_time:107127ms step_avg:58.16ms
step:1843/2330 train_time:107185ms step_avg:58.16ms
step:1844/2330 train_time:107245ms step_avg:58.16ms
step:1845/2330 train_time:107303ms step_avg:58.16ms
step:1846/2330 train_time:107363ms step_avg:58.16ms
step:1847/2330 train_time:107422ms step_avg:58.16ms
step:1848/2330 train_time:107482ms step_avg:58.16ms
step:1849/2330 train_time:107540ms step_avg:58.16ms
step:1850/2330 train_time:107600ms step_avg:58.16ms
step:1851/2330 train_time:107659ms step_avg:58.16ms
step:1852/2330 train_time:107719ms step_avg:58.16ms
step:1853/2330 train_time:107776ms step_avg:58.16ms
step:1854/2330 train_time:107837ms step_avg:58.16ms
step:1855/2330 train_time:107894ms step_avg:58.16ms
step:1856/2330 train_time:107956ms step_avg:58.17ms
step:1857/2330 train_time:108013ms step_avg:58.17ms
step:1858/2330 train_time:108075ms step_avg:58.17ms
step:1859/2330 train_time:108131ms step_avg:58.17ms
step:1860/2330 train_time:108193ms step_avg:58.17ms
step:1861/2330 train_time:108250ms step_avg:58.17ms
step:1862/2330 train_time:108312ms step_avg:58.17ms
step:1863/2330 train_time:108368ms step_avg:58.17ms
step:1864/2330 train_time:108428ms step_avg:58.17ms
step:1865/2330 train_time:108485ms step_avg:58.17ms
step:1866/2330 train_time:108545ms step_avg:58.17ms
step:1867/2330 train_time:108603ms step_avg:58.17ms
step:1868/2330 train_time:108663ms step_avg:58.17ms
step:1869/2330 train_time:108721ms step_avg:58.17ms
step:1870/2330 train_time:108781ms step_avg:58.17ms
step:1871/2330 train_time:108839ms step_avg:58.17ms
step:1872/2330 train_time:108899ms step_avg:58.17ms
step:1873/2330 train_time:108957ms step_avg:58.17ms
step:1874/2330 train_time:109017ms step_avg:58.17ms
step:1875/2330 train_time:109074ms step_avg:58.17ms
step:1876/2330 train_time:109135ms step_avg:58.17ms
step:1877/2330 train_time:109192ms step_avg:58.17ms
step:1878/2330 train_time:109254ms step_avg:58.18ms
step:1879/2330 train_time:109311ms step_avg:58.17ms
step:1880/2330 train_time:109372ms step_avg:58.18ms
step:1881/2330 train_time:109428ms step_avg:58.18ms
step:1882/2330 train_time:109490ms step_avg:58.18ms
step:1883/2330 train_time:109546ms step_avg:58.18ms
step:1884/2330 train_time:109607ms step_avg:58.18ms
step:1885/2330 train_time:109664ms step_avg:58.18ms
step:1886/2330 train_time:109725ms step_avg:58.18ms
step:1887/2330 train_time:109782ms step_avg:58.18ms
step:1888/2330 train_time:109843ms step_avg:58.18ms
step:1889/2330 train_time:109901ms step_avg:58.18ms
step:1890/2330 train_time:109962ms step_avg:58.18ms
step:1891/2330 train_time:110019ms step_avg:58.18ms
step:1892/2330 train_time:110080ms step_avg:58.18ms
step:1893/2330 train_time:110138ms step_avg:58.18ms
step:1894/2330 train_time:110200ms step_avg:58.18ms
step:1895/2330 train_time:110257ms step_avg:58.18ms
step:1896/2330 train_time:110319ms step_avg:58.19ms
step:1897/2330 train_time:110376ms step_avg:58.18ms
step:1898/2330 train_time:110437ms step_avg:58.19ms
step:1899/2330 train_time:110494ms step_avg:58.19ms
step:1900/2330 train_time:110555ms step_avg:58.19ms
step:1901/2330 train_time:110612ms step_avg:58.19ms
step:1902/2330 train_time:110673ms step_avg:58.19ms
step:1903/2330 train_time:110729ms step_avg:58.19ms
step:1904/2330 train_time:110790ms step_avg:58.19ms
step:1905/2330 train_time:110847ms step_avg:58.19ms
step:1906/2330 train_time:110907ms step_avg:58.19ms
step:1907/2330 train_time:110965ms step_avg:58.19ms
step:1908/2330 train_time:111024ms step_avg:58.19ms
step:1909/2330 train_time:111082ms step_avg:58.19ms
step:1910/2330 train_time:111143ms step_avg:58.19ms
step:1911/2330 train_time:111202ms step_avg:58.19ms
step:1912/2330 train_time:111263ms step_avg:58.19ms
step:1913/2330 train_time:111320ms step_avg:58.19ms
step:1914/2330 train_time:111382ms step_avg:58.19ms
step:1915/2330 train_time:111439ms step_avg:58.19ms
step:1916/2330 train_time:111501ms step_avg:58.19ms
step:1917/2330 train_time:111558ms step_avg:58.19ms
step:1918/2330 train_time:111618ms step_avg:58.20ms
step:1919/2330 train_time:111676ms step_avg:58.19ms
step:1920/2330 train_time:111736ms step_avg:58.20ms
step:1921/2330 train_time:111792ms step_avg:58.19ms
step:1922/2330 train_time:111854ms step_avg:58.20ms
step:1923/2330 train_time:111910ms step_avg:58.20ms
step:1924/2330 train_time:111973ms step_avg:58.20ms
step:1925/2330 train_time:112029ms step_avg:58.20ms
step:1926/2330 train_time:112091ms step_avg:58.20ms
step:1927/2330 train_time:112147ms step_avg:58.20ms
step:1928/2330 train_time:112209ms step_avg:58.20ms
step:1929/2330 train_time:112266ms step_avg:58.20ms
step:1930/2330 train_time:112326ms step_avg:58.20ms
step:1931/2330 train_time:112384ms step_avg:58.20ms
step:1932/2330 train_time:112444ms step_avg:58.20ms
step:1933/2330 train_time:112502ms step_avg:58.20ms
step:1934/2330 train_time:112562ms step_avg:58.20ms
step:1935/2330 train_time:112620ms step_avg:58.20ms
step:1936/2330 train_time:112681ms step_avg:58.20ms
step:1937/2330 train_time:112738ms step_avg:58.20ms
step:1938/2330 train_time:112799ms step_avg:58.20ms
step:1939/2330 train_time:112856ms step_avg:58.20ms
step:1940/2330 train_time:112917ms step_avg:58.20ms
step:1941/2330 train_time:112975ms step_avg:58.20ms
step:1942/2330 train_time:113035ms step_avg:58.21ms
step:1943/2330 train_time:113092ms step_avg:58.20ms
step:1944/2330 train_time:113154ms step_avg:58.21ms
step:1945/2330 train_time:113211ms step_avg:58.21ms
step:1946/2330 train_time:113274ms step_avg:58.21ms
step:1947/2330 train_time:113331ms step_avg:58.21ms
step:1948/2330 train_time:113392ms step_avg:58.21ms
step:1949/2330 train_time:113448ms step_avg:58.21ms
step:1950/2330 train_time:113510ms step_avg:58.21ms
step:1951/2330 train_time:113566ms step_avg:58.21ms
step:1952/2330 train_time:113627ms step_avg:58.21ms
step:1953/2330 train_time:113683ms step_avg:58.21ms
step:1954/2330 train_time:113744ms step_avg:58.21ms
step:1955/2330 train_time:113802ms step_avg:58.21ms
step:1956/2330 train_time:113862ms step_avg:58.21ms
step:1957/2330 train_time:113921ms step_avg:58.21ms
step:1958/2330 train_time:113982ms step_avg:58.21ms
step:1959/2330 train_time:114040ms step_avg:58.21ms
step:1960/2330 train_time:114100ms step_avg:58.21ms
step:1961/2330 train_time:114158ms step_avg:58.21ms
step:1962/2330 train_time:114219ms step_avg:58.22ms
step:1963/2330 train_time:114276ms step_avg:58.21ms
step:1964/2330 train_time:114337ms step_avg:58.22ms
step:1965/2330 train_time:114393ms step_avg:58.22ms
step:1966/2330 train_time:114456ms step_avg:58.22ms
step:1967/2330 train_time:114513ms step_avg:58.22ms
step:1968/2330 train_time:114575ms step_avg:58.22ms
step:1969/2330 train_time:114630ms step_avg:58.22ms
step:1970/2330 train_time:114692ms step_avg:58.22ms
step:1971/2330 train_time:114749ms step_avg:58.22ms
step:1972/2330 train_time:114809ms step_avg:58.22ms
step:1973/2330 train_time:114866ms step_avg:58.22ms
step:1974/2330 train_time:114927ms step_avg:58.22ms
step:1975/2330 train_time:114984ms step_avg:58.22ms
step:1976/2330 train_time:115044ms step_avg:58.22ms
step:1977/2330 train_time:115102ms step_avg:58.22ms
step:1978/2330 train_time:115162ms step_avg:58.22ms
step:1979/2330 train_time:115220ms step_avg:58.22ms
step:1980/2330 train_time:115281ms step_avg:58.22ms
step:1981/2330 train_time:115339ms step_avg:58.22ms
step:1982/2330 train_time:115400ms step_avg:58.22ms
step:1983/2330 train_time:115458ms step_avg:58.22ms
step:1984/2330 train_time:115519ms step_avg:58.23ms
step:1985/2330 train_time:115576ms step_avg:58.22ms
step:1986/2330 train_time:115637ms step_avg:58.23ms
step:1987/2330 train_time:115694ms step_avg:58.23ms
step:1988/2330 train_time:115754ms step_avg:58.23ms
step:1989/2330 train_time:115810ms step_avg:58.23ms
step:1990/2330 train_time:115872ms step_avg:58.23ms
step:1991/2330 train_time:115928ms step_avg:58.23ms
step:1992/2330 train_time:115989ms step_avg:58.23ms
step:1993/2330 train_time:116046ms step_avg:58.23ms
step:1994/2330 train_time:116107ms step_avg:58.23ms
step:1995/2330 train_time:116164ms step_avg:58.23ms
step:1996/2330 train_time:116224ms step_avg:58.23ms
step:1997/2330 train_time:116282ms step_avg:58.23ms
step:1998/2330 train_time:116343ms step_avg:58.23ms
step:1999/2330 train_time:116401ms step_avg:58.23ms
step:2000/2330 train_time:116462ms step_avg:58.23ms
step:2000/2330 val_loss:4.6325 train_time:116543ms step_avg:58.27ms
step:2001/2330 train_time:116563ms step_avg:58.25ms
step:2002/2330 train_time:116584ms step_avg:58.23ms
step:2003/2330 train_time:116641ms step_avg:58.23ms
step:2004/2330 train_time:116706ms step_avg:58.24ms
step:2005/2330 train_time:116763ms step_avg:58.24ms
step:2006/2330 train_time:116826ms step_avg:58.24ms
step:2007/2330 train_time:116883ms step_avg:58.24ms
step:2008/2330 train_time:116943ms step_avg:58.24ms
step:2009/2330 train_time:116999ms step_avg:58.24ms
step:2010/2330 train_time:117060ms step_avg:58.24ms
step:2011/2330 train_time:117117ms step_avg:58.24ms
step:2012/2330 train_time:117176ms step_avg:58.24ms
step:2013/2330 train_time:117233ms step_avg:58.24ms
step:2014/2330 train_time:117293ms step_avg:58.24ms
step:2015/2330 train_time:117349ms step_avg:58.24ms
step:2016/2330 train_time:117409ms step_avg:58.24ms
step:2017/2330 train_time:117465ms step_avg:58.24ms
step:2018/2330 train_time:117527ms step_avg:58.24ms
step:2019/2330 train_time:117586ms step_avg:58.24ms
step:2020/2330 train_time:117648ms step_avg:58.24ms
step:2021/2330 train_time:117705ms step_avg:58.24ms
step:2022/2330 train_time:117767ms step_avg:58.24ms
step:2023/2330 train_time:117826ms step_avg:58.24ms
step:2024/2330 train_time:117887ms step_avg:58.24ms
step:2025/2330 train_time:117945ms step_avg:58.24ms
step:2026/2330 train_time:118005ms step_avg:58.25ms
step:2027/2330 train_time:118063ms step_avg:58.25ms
step:2028/2330 train_time:118122ms step_avg:58.25ms
step:2029/2330 train_time:118179ms step_avg:58.24ms
step:2030/2330 train_time:118238ms step_avg:58.25ms
step:2031/2330 train_time:118295ms step_avg:58.24ms
step:2032/2330 train_time:118355ms step_avg:58.25ms
step:2033/2330 train_time:118411ms step_avg:58.24ms
step:2034/2330 train_time:118471ms step_avg:58.25ms
step:2035/2330 train_time:118529ms step_avg:58.25ms
step:2036/2330 train_time:118590ms step_avg:58.25ms
step:2037/2330 train_time:118647ms step_avg:58.25ms
step:2038/2330 train_time:118711ms step_avg:58.25ms
step:2039/2330 train_time:118768ms step_avg:58.25ms
step:2040/2330 train_time:118829ms step_avg:58.25ms
step:2041/2330 train_time:118887ms step_avg:58.25ms
step:2042/2330 train_time:118948ms step_avg:58.25ms
step:2043/2330 train_time:119006ms step_avg:58.25ms
step:2044/2330 train_time:119065ms step_avg:58.25ms
step:2045/2330 train_time:119123ms step_avg:58.25ms
step:2046/2330 train_time:119184ms step_avg:58.25ms
step:2047/2330 train_time:119241ms step_avg:58.25ms
step:2048/2330 train_time:119301ms step_avg:58.25ms
step:2049/2330 train_time:119358ms step_avg:58.25ms
step:2050/2330 train_time:119418ms step_avg:58.25ms
step:2051/2330 train_time:119475ms step_avg:58.25ms
step:2052/2330 train_time:119536ms step_avg:58.25ms
step:2053/2330 train_time:119593ms step_avg:58.25ms
step:2054/2330 train_time:119654ms step_avg:58.25ms
step:2055/2330 train_time:119710ms step_avg:58.25ms
step:2056/2330 train_time:119772ms step_avg:58.25ms
step:2057/2330 train_time:119829ms step_avg:58.25ms
step:2058/2330 train_time:119891ms step_avg:58.26ms
step:2059/2330 train_time:119948ms step_avg:58.26ms
step:2060/2330 train_time:120008ms step_avg:58.26ms
step:2061/2330 train_time:120066ms step_avg:58.26ms
step:2062/2330 train_time:120127ms step_avg:58.26ms
step:2063/2330 train_time:120185ms step_avg:58.26ms
step:2064/2330 train_time:120245ms step_avg:58.26ms
step:2065/2330 train_time:120302ms step_avg:58.26ms
step:2066/2330 train_time:120363ms step_avg:58.26ms
step:2067/2330 train_time:120421ms step_avg:58.26ms
step:2068/2330 train_time:120482ms step_avg:58.26ms
step:2069/2330 train_time:120539ms step_avg:58.26ms
step:2070/2330 train_time:120601ms step_avg:58.26ms
step:2071/2330 train_time:120657ms step_avg:58.26ms
step:2072/2330 train_time:120720ms step_avg:58.26ms
step:2073/2330 train_time:120776ms step_avg:58.26ms
step:2074/2330 train_time:120839ms step_avg:58.26ms
step:2075/2330 train_time:120895ms step_avg:58.26ms
step:2076/2330 train_time:120957ms step_avg:58.26ms
step:2077/2330 train_time:121013ms step_avg:58.26ms
step:2078/2330 train_time:121074ms step_avg:58.26ms
step:2079/2330 train_time:121130ms step_avg:58.26ms
step:2080/2330 train_time:121191ms step_avg:58.26ms
step:2081/2330 train_time:121247ms step_avg:58.26ms
step:2082/2330 train_time:121308ms step_avg:58.27ms
step:2083/2330 train_time:121366ms step_avg:58.26ms
step:2084/2330 train_time:121427ms step_avg:58.27ms
step:2085/2330 train_time:121484ms step_avg:58.27ms
step:2086/2330 train_time:121545ms step_avg:58.27ms
step:2087/2330 train_time:121602ms step_avg:58.27ms
step:2088/2330 train_time:121663ms step_avg:58.27ms
step:2089/2330 train_time:121721ms step_avg:58.27ms
step:2090/2330 train_time:121783ms step_avg:58.27ms
step:2091/2330 train_time:121840ms step_avg:58.27ms
step:2092/2330 train_time:121902ms step_avg:58.27ms
step:2093/2330 train_time:121958ms step_avg:58.27ms
step:2094/2330 train_time:122020ms step_avg:58.27ms
step:2095/2330 train_time:122077ms step_avg:58.27ms
step:2096/2330 train_time:122138ms step_avg:58.27ms
step:2097/2330 train_time:122195ms step_avg:58.27ms
step:2098/2330 train_time:122255ms step_avg:58.27ms
step:2099/2330 train_time:122311ms step_avg:58.27ms
step:2100/2330 train_time:122373ms step_avg:58.27ms
step:2101/2330 train_time:122430ms step_avg:58.27ms
step:2102/2330 train_time:122490ms step_avg:58.27ms
step:2103/2330 train_time:122548ms step_avg:58.27ms
step:2104/2330 train_time:122608ms step_avg:58.27ms
step:2105/2330 train_time:122666ms step_avg:58.27ms
step:2106/2330 train_time:122726ms step_avg:58.27ms
step:2107/2330 train_time:122785ms step_avg:58.27ms
step:2108/2330 train_time:122846ms step_avg:58.28ms
step:2109/2330 train_time:122905ms step_avg:58.28ms
step:2110/2330 train_time:122965ms step_avg:58.28ms
step:2111/2330 train_time:123024ms step_avg:58.28ms
step:2112/2330 train_time:123085ms step_avg:58.28ms
step:2113/2330 train_time:123143ms step_avg:58.28ms
step:2114/2330 train_time:123203ms step_avg:58.28ms
step:2115/2330 train_time:123260ms step_avg:58.28ms
step:2116/2330 train_time:123321ms step_avg:58.28ms
step:2117/2330 train_time:123378ms step_avg:58.28ms
step:2118/2330 train_time:123438ms step_avg:58.28ms
step:2119/2330 train_time:123495ms step_avg:58.28ms
step:2120/2330 train_time:123555ms step_avg:58.28ms
step:2121/2330 train_time:123612ms step_avg:58.28ms
step:2122/2330 train_time:123674ms step_avg:58.28ms
step:2123/2330 train_time:123731ms step_avg:58.28ms
step:2124/2330 train_time:123791ms step_avg:58.28ms
step:2125/2330 train_time:123849ms step_avg:58.28ms
step:2126/2330 train_time:123909ms step_avg:58.28ms
step:2127/2330 train_time:123966ms step_avg:58.28ms
step:2128/2330 train_time:124028ms step_avg:58.28ms
step:2129/2330 train_time:124086ms step_avg:58.28ms
step:2130/2330 train_time:124146ms step_avg:58.28ms
step:2131/2330 train_time:124204ms step_avg:58.28ms
step:2132/2330 train_time:124264ms step_avg:58.29ms
step:2133/2330 train_time:124322ms step_avg:58.29ms
step:2134/2330 train_time:124384ms step_avg:58.29ms
step:2135/2330 train_time:124441ms step_avg:58.29ms
step:2136/2330 train_time:124502ms step_avg:58.29ms
step:2137/2330 train_time:124559ms step_avg:58.29ms
step:2138/2330 train_time:124620ms step_avg:58.29ms
step:2139/2330 train_time:124676ms step_avg:58.29ms
step:2140/2330 train_time:124738ms step_avg:58.29ms
step:2141/2330 train_time:124794ms step_avg:58.29ms
step:2142/2330 train_time:124855ms step_avg:58.29ms
step:2143/2330 train_time:124911ms step_avg:58.29ms
step:2144/2330 train_time:124973ms step_avg:58.29ms
step:2145/2330 train_time:125029ms step_avg:58.29ms
step:2146/2330 train_time:125090ms step_avg:58.29ms
step:2147/2330 train_time:125147ms step_avg:58.29ms
step:2148/2330 train_time:125208ms step_avg:58.29ms
step:2149/2330 train_time:125266ms step_avg:58.29ms
step:2150/2330 train_time:125327ms step_avg:58.29ms
step:2151/2330 train_time:125385ms step_avg:58.29ms
step:2152/2330 train_time:125445ms step_avg:58.29ms
step:2153/2330 train_time:125503ms step_avg:58.29ms
step:2154/2330 train_time:125563ms step_avg:58.29ms
step:2155/2330 train_time:125620ms step_avg:58.29ms
step:2156/2330 train_time:125682ms step_avg:58.29ms
step:2157/2330 train_time:125738ms step_avg:58.29ms
step:2158/2330 train_time:125801ms step_avg:58.30ms
step:2159/2330 train_time:125857ms step_avg:58.29ms
step:2160/2330 train_time:125921ms step_avg:58.30ms
step:2161/2330 train_time:125977ms step_avg:58.30ms
step:2162/2330 train_time:126039ms step_avg:58.30ms
step:2163/2330 train_time:126096ms step_avg:58.30ms
step:2164/2330 train_time:126157ms step_avg:58.30ms
step:2165/2330 train_time:126213ms step_avg:58.30ms
step:2166/2330 train_time:126274ms step_avg:58.30ms
step:2167/2330 train_time:126331ms step_avg:58.30ms
step:2168/2330 train_time:126391ms step_avg:58.30ms
step:2169/2330 train_time:126448ms step_avg:58.30ms
step:2170/2330 train_time:126509ms step_avg:58.30ms
step:2171/2330 train_time:126567ms step_avg:58.30ms
step:2172/2330 train_time:126628ms step_avg:58.30ms
step:2173/2330 train_time:126686ms step_avg:58.30ms
step:2174/2330 train_time:126746ms step_avg:58.30ms
step:2175/2330 train_time:126803ms step_avg:58.30ms
step:2176/2330 train_time:126865ms step_avg:58.30ms
step:2177/2330 train_time:126923ms step_avg:58.30ms
step:2178/2330 train_time:126983ms step_avg:58.30ms
step:2179/2330 train_time:127040ms step_avg:58.30ms
step:2180/2330 train_time:127102ms step_avg:58.30ms
step:2181/2330 train_time:127159ms step_avg:58.30ms
step:2182/2330 train_time:127221ms step_avg:58.30ms
step:2183/2330 train_time:127277ms step_avg:58.30ms
step:2184/2330 train_time:127339ms step_avg:58.31ms
step:2185/2330 train_time:127396ms step_avg:58.30ms
step:2186/2330 train_time:127458ms step_avg:58.31ms
step:2187/2330 train_time:127514ms step_avg:58.31ms
step:2188/2330 train_time:127577ms step_avg:58.31ms
step:2189/2330 train_time:127633ms step_avg:58.31ms
step:2190/2330 train_time:127694ms step_avg:58.31ms
step:2191/2330 train_time:127750ms step_avg:58.31ms
step:2192/2330 train_time:127812ms step_avg:58.31ms
step:2193/2330 train_time:127869ms step_avg:58.31ms
step:2194/2330 train_time:127930ms step_avg:58.31ms
step:2195/2330 train_time:127987ms step_avg:58.31ms
step:2196/2330 train_time:128047ms step_avg:58.31ms
step:2197/2330 train_time:128105ms step_avg:58.31ms
step:2198/2330 train_time:128165ms step_avg:58.31ms
step:2199/2330 train_time:128223ms step_avg:58.31ms
step:2200/2330 train_time:128284ms step_avg:58.31ms
step:2201/2330 train_time:128343ms step_avg:58.31ms
step:2202/2330 train_time:128404ms step_avg:58.31ms
step:2203/2330 train_time:128462ms step_avg:58.31ms
step:2204/2330 train_time:128522ms step_avg:58.31ms
step:2205/2330 train_time:128579ms step_avg:58.31ms
step:2206/2330 train_time:128641ms step_avg:58.31ms
step:2207/2330 train_time:128697ms step_avg:58.31ms
step:2208/2330 train_time:128760ms step_avg:58.32ms
step:2209/2330 train_time:128816ms step_avg:58.31ms
step:2210/2330 train_time:128879ms step_avg:58.32ms
step:2211/2330 train_time:128935ms step_avg:58.32ms
step:2212/2330 train_time:128997ms step_avg:58.32ms
step:2213/2330 train_time:129053ms step_avg:58.32ms
step:2214/2330 train_time:129115ms step_avg:58.32ms
step:2215/2330 train_time:129171ms step_avg:58.32ms
step:2216/2330 train_time:129231ms step_avg:58.32ms
step:2217/2330 train_time:129288ms step_avg:58.32ms
step:2218/2330 train_time:129349ms step_avg:58.32ms
step:2219/2330 train_time:129406ms step_avg:58.32ms
step:2220/2330 train_time:129467ms step_avg:58.32ms
step:2221/2330 train_time:129525ms step_avg:58.32ms
step:2222/2330 train_time:129585ms step_avg:58.32ms
step:2223/2330 train_time:129643ms step_avg:58.32ms
step:2224/2330 train_time:129704ms step_avg:58.32ms
step:2225/2330 train_time:129761ms step_avg:58.32ms
step:2226/2330 train_time:129822ms step_avg:58.32ms
step:2227/2330 train_time:129879ms step_avg:58.32ms
step:2228/2330 train_time:129941ms step_avg:58.32ms
step:2229/2330 train_time:129998ms step_avg:58.32ms
step:2230/2330 train_time:130059ms step_avg:58.32ms
step:2231/2330 train_time:130116ms step_avg:58.32ms
step:2232/2330 train_time:130178ms step_avg:58.32ms
step:2233/2330 train_time:130235ms step_avg:58.32ms
step:2234/2330 train_time:130296ms step_avg:58.32ms
step:2235/2330 train_time:130352ms step_avg:58.32ms
step:2236/2330 train_time:130414ms step_avg:58.32ms
step:2237/2330 train_time:130470ms step_avg:58.32ms
step:2238/2330 train_time:130532ms step_avg:58.33ms
step:2239/2330 train_time:130589ms step_avg:58.32ms
step:2240/2330 train_time:130650ms step_avg:58.33ms
step:2241/2330 train_time:130708ms step_avg:58.33ms
step:2242/2330 train_time:130768ms step_avg:58.33ms
step:2243/2330 train_time:130826ms step_avg:58.33ms
step:2244/2330 train_time:130887ms step_avg:58.33ms
step:2245/2330 train_time:130945ms step_avg:58.33ms
step:2246/2330 train_time:131006ms step_avg:58.33ms
step:2247/2330 train_time:131064ms step_avg:58.33ms
step:2248/2330 train_time:131125ms step_avg:58.33ms
step:2249/2330 train_time:131182ms step_avg:58.33ms
step:2250/2330 train_time:131243ms step_avg:58.33ms
step:2250/2330 val_loss:4.5373 train_time:131325ms step_avg:58.37ms
step:2251/2330 train_time:131344ms step_avg:58.35ms
step:2252/2330 train_time:131365ms step_avg:58.33ms
step:2253/2330 train_time:131424ms step_avg:58.33ms
step:2254/2330 train_time:131486ms step_avg:58.33ms
step:2255/2330 train_time:131544ms step_avg:58.33ms
step:2256/2330 train_time:131606ms step_avg:58.34ms
step:2257/2330 train_time:131662ms step_avg:58.33ms
step:2258/2330 train_time:131723ms step_avg:58.34ms
step:2259/2330 train_time:131779ms step_avg:58.34ms
step:2260/2330 train_time:131839ms step_avg:58.34ms
step:2261/2330 train_time:131896ms step_avg:58.34ms
step:2262/2330 train_time:131956ms step_avg:58.34ms
step:2263/2330 train_time:132012ms step_avg:58.33ms
step:2264/2330 train_time:132072ms step_avg:58.34ms
step:2265/2330 train_time:132128ms step_avg:58.33ms
step:2266/2330 train_time:132188ms step_avg:58.34ms
step:2267/2330 train_time:132245ms step_avg:58.33ms
step:2268/2330 train_time:132307ms step_avg:58.34ms
step:2269/2330 train_time:132366ms step_avg:58.34ms
step:2270/2330 train_time:132428ms step_avg:58.34ms
step:2271/2330 train_time:132486ms step_avg:58.34ms
step:2272/2330 train_time:132550ms step_avg:58.34ms
step:2273/2330 train_time:132606ms step_avg:58.34ms
step:2274/2330 train_time:132668ms step_avg:58.34ms
step:2275/2330 train_time:132725ms step_avg:58.34ms
step:2276/2330 train_time:132785ms step_avg:58.34ms
step:2277/2330 train_time:132842ms step_avg:58.34ms
step:2278/2330 train_time:132903ms step_avg:58.34ms
step:2279/2330 train_time:132959ms step_avg:58.34ms
step:2280/2330 train_time:133020ms step_avg:58.34ms
step:2281/2330 train_time:133076ms step_avg:58.34ms
step:2282/2330 train_time:133136ms step_avg:58.34ms
step:2283/2330 train_time:133193ms step_avg:58.34ms
step:2284/2330 train_time:133253ms step_avg:58.34ms
step:2285/2330 train_time:133311ms step_avg:58.34ms
step:2286/2330 train_time:133370ms step_avg:58.34ms
step:2287/2330 train_time:133429ms step_avg:58.34ms
step:2288/2330 train_time:133490ms step_avg:58.34ms
step:2289/2330 train_time:133547ms step_avg:58.34ms
step:2290/2330 train_time:133609ms step_avg:58.34ms
step:2291/2330 train_time:133666ms step_avg:58.34ms
step:2292/2330 train_time:133726ms step_avg:58.34ms
step:2293/2330 train_time:133783ms step_avg:58.34ms
step:2294/2330 train_time:133844ms step_avg:58.35ms
step:2295/2330 train_time:133902ms step_avg:58.35ms
step:2296/2330 train_time:133963ms step_avg:58.35ms
step:2297/2330 train_time:134019ms step_avg:58.35ms
step:2298/2330 train_time:134080ms step_avg:58.35ms
step:2299/2330 train_time:134137ms step_avg:58.35ms
step:2300/2330 train_time:134196ms step_avg:58.35ms
step:2301/2330 train_time:134253ms step_avg:58.35ms
step:2302/2330 train_time:134314ms step_avg:58.35ms
step:2303/2330 train_time:134371ms step_avg:58.35ms
step:2304/2330 train_time:134431ms step_avg:58.35ms
step:2305/2330 train_time:134489ms step_avg:58.35ms
step:2306/2330 train_time:134550ms step_avg:58.35ms
step:2307/2330 train_time:134607ms step_avg:58.35ms
step:2308/2330 train_time:134667ms step_avg:58.35ms
step:2309/2330 train_time:134724ms step_avg:58.35ms
step:2310/2330 train_time:134785ms step_avg:58.35ms
step:2311/2330 train_time:134844ms step_avg:58.35ms
step:2312/2330 train_time:134904ms step_avg:58.35ms
step:2313/2330 train_time:134961ms step_avg:58.35ms
step:2314/2330 train_time:135021ms step_avg:58.35ms
step:2315/2330 train_time:135078ms step_avg:58.35ms
step:2316/2330 train_time:135138ms step_avg:58.35ms
step:2317/2330 train_time:135195ms step_avg:58.35ms
step:2318/2330 train_time:135255ms step_avg:58.35ms
step:2319/2330 train_time:135311ms step_avg:58.35ms
step:2320/2330 train_time:135373ms step_avg:58.35ms
step:2321/2330 train_time:135430ms step_avg:58.35ms
step:2322/2330 train_time:135491ms step_avg:58.35ms
step:2323/2330 train_time:135548ms step_avg:58.35ms
step:2324/2330 train_time:135608ms step_avg:58.35ms
step:2325/2330 train_time:135665ms step_avg:58.35ms
step:2326/2330 train_time:135726ms step_avg:58.35ms
step:2327/2330 train_time:135784ms step_avg:58.35ms
step:2328/2330 train_time:135844ms step_avg:58.35ms
step:2329/2330 train_time:135902ms step_avg:58.35ms
step:2330/2330 train_time:135962ms step_avg:58.35ms
step:2330/2330 val_loss:4.5164 train_time:136044ms step_avg:58.39ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
