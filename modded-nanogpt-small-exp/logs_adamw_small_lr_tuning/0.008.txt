import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--adamw_lr', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_lr_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_lr_tuning/{args2.adamw_lr}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=float(args2.adamw_lr),  betas=(0.85, 0.85), eps=1e-10, weight_decay=0.0, fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_lr_tuning/{args2.adamw_lr}", exist_ok=True)
            torch.save(log, f"logs_adamw_small_lr_tuning/{args2.adamw_lr}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 02:05:40 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   36C    P0             117W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   35C    P0             117W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             117W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   28C    P0             115W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   34C    P0             118W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             114W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:88ms step_avg:88.20ms
step:2/2330 train_time:179ms step_avg:89.37ms
step:3/2330 train_time:198ms step_avg:66.00ms
step:4/2330 train_time:218ms step_avg:54.43ms
step:5/2330 train_time:269ms step_avg:53.87ms
step:6/2330 train_time:326ms step_avg:54.40ms
step:7/2330 train_time:380ms step_avg:54.29ms
step:8/2330 train_time:439ms step_avg:54.83ms
step:9/2330 train_time:493ms step_avg:54.73ms
step:10/2330 train_time:550ms step_avg:55.01ms
step:11/2330 train_time:604ms step_avg:54.94ms
step:12/2330 train_time:662ms step_avg:55.16ms
step:13/2330 train_time:717ms step_avg:55.14ms
step:14/2330 train_time:774ms step_avg:55.32ms
step:15/2330 train_time:829ms step_avg:55.28ms
step:16/2330 train_time:887ms step_avg:55.41ms
step:17/2330 train_time:941ms step_avg:55.35ms
step:18/2330 train_time:999ms step_avg:55.48ms
step:19/2330 train_time:1056ms step_avg:55.56ms
step:20/2330 train_time:1116ms step_avg:55.78ms
step:21/2330 train_time:1172ms step_avg:55.82ms
step:22/2330 train_time:1231ms step_avg:55.93ms
step:23/2330 train_time:1285ms step_avg:55.88ms
step:24/2330 train_time:1344ms step_avg:55.99ms
step:25/2330 train_time:1399ms step_avg:55.95ms
step:26/2330 train_time:1458ms step_avg:56.08ms
step:27/2330 train_time:1513ms step_avg:56.02ms
step:28/2330 train_time:1570ms step_avg:56.09ms
step:29/2330 train_time:1625ms step_avg:56.03ms
step:30/2330 train_time:1683ms step_avg:56.10ms
step:31/2330 train_time:1738ms step_avg:56.05ms
step:32/2330 train_time:1796ms step_avg:56.13ms
step:33/2330 train_time:1851ms step_avg:56.09ms
step:34/2330 train_time:1908ms step_avg:56.12ms
step:35/2330 train_time:1963ms step_avg:56.07ms
step:36/2330 train_time:2021ms step_avg:56.14ms
step:37/2330 train_time:2077ms step_avg:56.15ms
step:38/2330 train_time:2136ms step_avg:56.21ms
step:39/2330 train_time:2191ms step_avg:56.18ms
step:40/2330 train_time:2250ms step_avg:56.26ms
step:41/2330 train_time:2306ms step_avg:56.23ms
step:42/2330 train_time:2364ms step_avg:56.30ms
step:43/2330 train_time:2420ms step_avg:56.27ms
step:44/2330 train_time:2478ms step_avg:56.33ms
step:45/2330 train_time:2534ms step_avg:56.31ms
step:46/2330 train_time:2592ms step_avg:56.35ms
step:47/2330 train_time:2647ms step_avg:56.31ms
step:48/2330 train_time:2704ms step_avg:56.33ms
step:49/2330 train_time:2759ms step_avg:56.30ms
step:50/2330 train_time:2817ms step_avg:56.33ms
step:51/2330 train_time:2871ms step_avg:56.30ms
step:52/2330 train_time:2929ms step_avg:56.32ms
step:53/2330 train_time:2984ms step_avg:56.30ms
step:54/2330 train_time:3042ms step_avg:56.34ms
step:55/2330 train_time:3098ms step_avg:56.33ms
step:56/2330 train_time:3156ms step_avg:56.36ms
step:57/2330 train_time:3211ms step_avg:56.34ms
step:58/2330 train_time:3269ms step_avg:56.37ms
step:59/2330 train_time:3324ms step_avg:56.35ms
step:60/2330 train_time:3383ms step_avg:56.39ms
step:61/2330 train_time:3439ms step_avg:56.38ms
step:62/2330 train_time:3497ms step_avg:56.41ms
step:63/2330 train_time:3552ms step_avg:56.39ms
step:64/2330 train_time:3611ms step_avg:56.42ms
step:65/2330 train_time:3666ms step_avg:56.40ms
step:66/2330 train_time:3723ms step_avg:56.41ms
step:67/2330 train_time:3779ms step_avg:56.40ms
step:68/2330 train_time:3837ms step_avg:56.43ms
step:69/2330 train_time:3892ms step_avg:56.41ms
step:70/2330 train_time:3950ms step_avg:56.43ms
step:71/2330 train_time:4005ms step_avg:56.41ms
step:72/2330 train_time:4064ms step_avg:56.44ms
step:73/2330 train_time:4119ms step_avg:56.43ms
step:74/2330 train_time:4178ms step_avg:56.46ms
step:75/2330 train_time:4234ms step_avg:56.46ms
step:76/2330 train_time:4293ms step_avg:56.49ms
step:77/2330 train_time:4349ms step_avg:56.48ms
step:78/2330 train_time:4407ms step_avg:56.50ms
step:79/2330 train_time:4462ms step_avg:56.48ms
step:80/2330 train_time:4522ms step_avg:56.53ms
step:81/2330 train_time:4578ms step_avg:56.52ms
step:82/2330 train_time:4637ms step_avg:56.54ms
step:83/2330 train_time:4692ms step_avg:56.53ms
step:84/2330 train_time:4750ms step_avg:56.55ms
step:85/2330 train_time:4805ms step_avg:56.53ms
step:86/2330 train_time:4863ms step_avg:56.55ms
step:87/2330 train_time:4919ms step_avg:56.54ms
step:88/2330 train_time:4978ms step_avg:56.57ms
step:89/2330 train_time:5034ms step_avg:56.56ms
step:90/2330 train_time:5092ms step_avg:56.58ms
step:91/2330 train_time:5147ms step_avg:56.56ms
step:92/2330 train_time:5205ms step_avg:56.58ms
step:93/2330 train_time:5260ms step_avg:56.56ms
step:94/2330 train_time:5320ms step_avg:56.60ms
step:95/2330 train_time:5376ms step_avg:56.59ms
step:96/2330 train_time:5434ms step_avg:56.60ms
step:97/2330 train_time:5489ms step_avg:56.59ms
step:98/2330 train_time:5548ms step_avg:56.61ms
step:99/2330 train_time:5603ms step_avg:56.59ms
step:100/2330 train_time:5662ms step_avg:56.62ms
step:101/2330 train_time:5717ms step_avg:56.61ms
step:102/2330 train_time:5776ms step_avg:56.63ms
step:103/2330 train_time:5831ms step_avg:56.62ms
step:104/2330 train_time:5889ms step_avg:56.63ms
step:105/2330 train_time:5945ms step_avg:56.62ms
step:106/2330 train_time:6003ms step_avg:56.63ms
step:107/2330 train_time:6059ms step_avg:56.63ms
step:108/2330 train_time:6118ms step_avg:56.65ms
step:109/2330 train_time:6173ms step_avg:56.63ms
step:110/2330 train_time:6231ms step_avg:56.65ms
step:111/2330 train_time:6286ms step_avg:56.63ms
step:112/2330 train_time:6345ms step_avg:56.65ms
step:113/2330 train_time:6400ms step_avg:56.64ms
step:114/2330 train_time:6460ms step_avg:56.67ms
step:115/2330 train_time:6516ms step_avg:56.66ms
step:116/2330 train_time:6574ms step_avg:56.67ms
step:117/2330 train_time:6630ms step_avg:56.66ms
step:118/2330 train_time:6688ms step_avg:56.67ms
step:119/2330 train_time:6743ms step_avg:56.66ms
step:120/2330 train_time:6802ms step_avg:56.68ms
step:121/2330 train_time:6858ms step_avg:56.67ms
step:122/2330 train_time:6916ms step_avg:56.69ms
step:123/2330 train_time:6972ms step_avg:56.68ms
step:124/2330 train_time:7030ms step_avg:56.69ms
step:125/2330 train_time:7086ms step_avg:56.69ms
step:126/2330 train_time:7144ms step_avg:56.70ms
step:127/2330 train_time:7199ms step_avg:56.69ms
step:128/2330 train_time:7257ms step_avg:56.70ms
step:129/2330 train_time:7312ms step_avg:56.69ms
step:130/2330 train_time:7371ms step_avg:56.70ms
step:131/2330 train_time:7427ms step_avg:56.69ms
step:132/2330 train_time:7485ms step_avg:56.71ms
step:133/2330 train_time:7541ms step_avg:56.70ms
step:134/2330 train_time:7600ms step_avg:56.71ms
step:135/2330 train_time:7655ms step_avg:56.71ms
step:136/2330 train_time:7714ms step_avg:56.72ms
step:137/2330 train_time:7770ms step_avg:56.71ms
step:138/2330 train_time:7828ms step_avg:56.72ms
step:139/2330 train_time:7883ms step_avg:56.71ms
step:140/2330 train_time:7942ms step_avg:56.73ms
step:141/2330 train_time:7998ms step_avg:56.72ms
step:142/2330 train_time:8057ms step_avg:56.74ms
step:143/2330 train_time:8112ms step_avg:56.73ms
step:144/2330 train_time:8170ms step_avg:56.74ms
step:145/2330 train_time:8225ms step_avg:56.73ms
step:146/2330 train_time:8284ms step_avg:56.74ms
step:147/2330 train_time:8339ms step_avg:56.73ms
step:148/2330 train_time:8398ms step_avg:56.74ms
step:149/2330 train_time:8453ms step_avg:56.73ms
step:150/2330 train_time:8511ms step_avg:56.74ms
step:151/2330 train_time:8566ms step_avg:56.73ms
step:152/2330 train_time:8625ms step_avg:56.74ms
step:153/2330 train_time:8680ms step_avg:56.73ms
step:154/2330 train_time:8739ms step_avg:56.74ms
step:155/2330 train_time:8794ms step_avg:56.74ms
step:156/2330 train_time:8852ms step_avg:56.75ms
step:157/2330 train_time:8908ms step_avg:56.74ms
step:158/2330 train_time:8966ms step_avg:56.75ms
step:159/2330 train_time:9021ms step_avg:56.73ms
step:160/2330 train_time:9081ms step_avg:56.76ms
step:161/2330 train_time:9137ms step_avg:56.75ms
step:162/2330 train_time:9195ms step_avg:56.76ms
step:163/2330 train_time:9250ms step_avg:56.75ms
step:164/2330 train_time:9308ms step_avg:56.76ms
step:165/2330 train_time:9364ms step_avg:56.75ms
step:166/2330 train_time:9423ms step_avg:56.76ms
step:167/2330 train_time:9479ms step_avg:56.76ms
step:168/2330 train_time:9537ms step_avg:56.77ms
step:169/2330 train_time:9593ms step_avg:56.76ms
step:170/2330 train_time:9651ms step_avg:56.77ms
step:171/2330 train_time:9706ms step_avg:56.76ms
step:172/2330 train_time:9765ms step_avg:56.77ms
step:173/2330 train_time:9820ms step_avg:56.76ms
step:174/2330 train_time:9879ms step_avg:56.78ms
step:175/2330 train_time:9934ms step_avg:56.77ms
step:176/2330 train_time:9993ms step_avg:56.78ms
step:177/2330 train_time:10049ms step_avg:56.77ms
step:178/2330 train_time:10107ms step_avg:56.78ms
step:179/2330 train_time:10162ms step_avg:56.77ms
step:180/2330 train_time:10221ms step_avg:56.78ms
step:181/2330 train_time:10277ms step_avg:56.78ms
step:182/2330 train_time:10335ms step_avg:56.79ms
step:183/2330 train_time:10390ms step_avg:56.78ms
step:184/2330 train_time:10448ms step_avg:56.78ms
step:185/2330 train_time:10503ms step_avg:56.77ms
step:186/2330 train_time:10563ms step_avg:56.79ms
step:187/2330 train_time:10618ms step_avg:56.78ms
step:188/2330 train_time:10677ms step_avg:56.79ms
step:189/2330 train_time:10733ms step_avg:56.79ms
step:190/2330 train_time:10791ms step_avg:56.79ms
step:191/2330 train_time:10846ms step_avg:56.79ms
step:192/2330 train_time:10904ms step_avg:56.79ms
step:193/2330 train_time:10960ms step_avg:56.79ms
step:194/2330 train_time:11019ms step_avg:56.80ms
step:195/2330 train_time:11075ms step_avg:56.80ms
step:196/2330 train_time:11133ms step_avg:56.80ms
step:197/2330 train_time:11188ms step_avg:56.79ms
step:198/2330 train_time:11247ms step_avg:56.80ms
step:199/2330 train_time:11302ms step_avg:56.80ms
step:200/2330 train_time:11361ms step_avg:56.80ms
step:201/2330 train_time:11416ms step_avg:56.80ms
step:202/2330 train_time:11475ms step_avg:56.81ms
step:203/2330 train_time:11530ms step_avg:56.80ms
step:204/2330 train_time:11589ms step_avg:56.81ms
step:205/2330 train_time:11644ms step_avg:56.80ms
step:206/2330 train_time:11703ms step_avg:56.81ms
step:207/2330 train_time:11758ms step_avg:56.80ms
step:208/2330 train_time:11817ms step_avg:56.81ms
step:209/2330 train_time:11873ms step_avg:56.81ms
step:210/2330 train_time:11931ms step_avg:56.81ms
step:211/2330 train_time:11987ms step_avg:56.81ms
step:212/2330 train_time:12045ms step_avg:56.82ms
step:213/2330 train_time:12100ms step_avg:56.81ms
step:214/2330 train_time:12159ms step_avg:56.82ms
step:215/2330 train_time:12214ms step_avg:56.81ms
step:216/2330 train_time:12273ms step_avg:56.82ms
step:217/2330 train_time:12329ms step_avg:56.81ms
step:218/2330 train_time:12386ms step_avg:56.82ms
step:219/2330 train_time:12441ms step_avg:56.81ms
step:220/2330 train_time:12500ms step_avg:56.82ms
step:221/2330 train_time:12556ms step_avg:56.81ms
step:222/2330 train_time:12615ms step_avg:56.82ms
step:223/2330 train_time:12671ms step_avg:56.82ms
step:224/2330 train_time:12728ms step_avg:56.82ms
step:225/2330 train_time:12784ms step_avg:56.82ms
step:226/2330 train_time:12843ms step_avg:56.83ms
step:227/2330 train_time:12898ms step_avg:56.82ms
step:228/2330 train_time:12957ms step_avg:56.83ms
step:229/2330 train_time:13013ms step_avg:56.83ms
step:230/2330 train_time:13071ms step_avg:56.83ms
step:231/2330 train_time:13126ms step_avg:56.82ms
step:232/2330 train_time:13185ms step_avg:56.83ms
step:233/2330 train_time:13241ms step_avg:56.83ms
step:234/2330 train_time:13300ms step_avg:56.84ms
step:235/2330 train_time:13355ms step_avg:56.83ms
step:236/2330 train_time:13413ms step_avg:56.83ms
step:237/2330 train_time:13468ms step_avg:56.83ms
step:238/2330 train_time:13526ms step_avg:56.83ms
step:239/2330 train_time:13581ms step_avg:56.83ms
step:240/2330 train_time:13640ms step_avg:56.84ms
step:241/2330 train_time:13696ms step_avg:56.83ms
step:242/2330 train_time:13754ms step_avg:56.84ms
step:243/2330 train_time:13810ms step_avg:56.83ms
step:244/2330 train_time:13869ms step_avg:56.84ms
step:245/2330 train_time:13924ms step_avg:56.83ms
step:246/2330 train_time:13983ms step_avg:56.84ms
step:247/2330 train_time:14039ms step_avg:56.84ms
step:248/2330 train_time:14097ms step_avg:56.84ms
step:249/2330 train_time:14153ms step_avg:56.84ms
step:250/2330 train_time:14211ms step_avg:56.85ms
step:250/2330 val_loss:5.8575 train_time:14289ms step_avg:57.16ms
step:251/2330 train_time:14309ms step_avg:57.01ms
step:252/2330 train_time:14328ms step_avg:56.86ms
step:253/2330 train_time:14381ms step_avg:56.84ms
step:254/2330 train_time:14446ms step_avg:56.87ms
step:255/2330 train_time:14500ms step_avg:56.86ms
step:256/2330 train_time:14564ms step_avg:56.89ms
step:257/2330 train_time:14618ms step_avg:56.88ms
step:258/2330 train_time:14678ms step_avg:56.89ms
step:259/2330 train_time:14733ms step_avg:56.88ms
step:260/2330 train_time:14792ms step_avg:56.89ms
step:261/2330 train_time:14848ms step_avg:56.89ms
step:262/2330 train_time:14906ms step_avg:56.89ms
step:263/2330 train_time:14960ms step_avg:56.88ms
step:264/2330 train_time:15019ms step_avg:56.89ms
step:265/2330 train_time:15074ms step_avg:56.88ms
step:266/2330 train_time:15132ms step_avg:56.89ms
step:267/2330 train_time:15187ms step_avg:56.88ms
step:268/2330 train_time:15246ms step_avg:56.89ms
step:269/2330 train_time:15302ms step_avg:56.88ms
step:270/2330 train_time:15361ms step_avg:56.89ms
step:271/2330 train_time:15416ms step_avg:56.89ms
step:272/2330 train_time:15476ms step_avg:56.90ms
step:273/2330 train_time:15532ms step_avg:56.89ms
step:274/2330 train_time:15591ms step_avg:56.90ms
step:275/2330 train_time:15647ms step_avg:56.90ms
step:276/2330 train_time:15705ms step_avg:56.90ms
step:277/2330 train_time:15760ms step_avg:56.89ms
step:278/2330 train_time:15819ms step_avg:56.90ms
step:279/2330 train_time:15874ms step_avg:56.90ms
step:280/2330 train_time:15933ms step_avg:56.90ms
step:281/2330 train_time:15988ms step_avg:56.90ms
step:282/2330 train_time:16046ms step_avg:56.90ms
step:283/2330 train_time:16101ms step_avg:56.89ms
step:284/2330 train_time:16159ms step_avg:56.90ms
step:285/2330 train_time:16214ms step_avg:56.89ms
step:286/2330 train_time:16273ms step_avg:56.90ms
step:287/2330 train_time:16329ms step_avg:56.90ms
step:288/2330 train_time:16388ms step_avg:56.90ms
step:289/2330 train_time:16444ms step_avg:56.90ms
step:290/2330 train_time:16502ms step_avg:56.90ms
step:291/2330 train_time:16558ms step_avg:56.90ms
step:292/2330 train_time:16618ms step_avg:56.91ms
step:293/2330 train_time:16674ms step_avg:56.91ms
step:294/2330 train_time:16733ms step_avg:56.91ms
step:295/2330 train_time:16789ms step_avg:56.91ms
step:296/2330 train_time:16847ms step_avg:56.92ms
step:297/2330 train_time:16902ms step_avg:56.91ms
step:298/2330 train_time:16960ms step_avg:56.91ms
step:299/2330 train_time:17015ms step_avg:56.91ms
step:300/2330 train_time:17074ms step_avg:56.91ms
step:301/2330 train_time:17129ms step_avg:56.91ms
step:302/2330 train_time:17188ms step_avg:56.91ms
step:303/2330 train_time:17243ms step_avg:56.91ms
step:304/2330 train_time:17302ms step_avg:56.91ms
step:305/2330 train_time:17358ms step_avg:56.91ms
step:306/2330 train_time:17417ms step_avg:56.92ms
step:307/2330 train_time:17472ms step_avg:56.91ms
step:308/2330 train_time:17531ms step_avg:56.92ms
step:309/2330 train_time:17587ms step_avg:56.91ms
step:310/2330 train_time:17645ms step_avg:56.92ms
step:311/2330 train_time:17700ms step_avg:56.91ms
step:312/2330 train_time:17760ms step_avg:56.92ms
step:313/2330 train_time:17815ms step_avg:56.92ms
step:314/2330 train_time:17876ms step_avg:56.93ms
step:315/2330 train_time:17932ms step_avg:56.93ms
step:316/2330 train_time:17990ms step_avg:56.93ms
step:317/2330 train_time:18046ms step_avg:56.93ms
step:318/2330 train_time:18104ms step_avg:56.93ms
step:319/2330 train_time:18159ms step_avg:56.92ms
step:320/2330 train_time:18218ms step_avg:56.93ms
step:321/2330 train_time:18273ms step_avg:56.92ms
step:322/2330 train_time:18332ms step_avg:56.93ms
step:323/2330 train_time:18388ms step_avg:56.93ms
step:324/2330 train_time:18446ms step_avg:56.93ms
step:325/2330 train_time:18502ms step_avg:56.93ms
step:326/2330 train_time:18560ms step_avg:56.93ms
step:327/2330 train_time:18615ms step_avg:56.93ms
step:328/2330 train_time:18675ms step_avg:56.93ms
step:329/2330 train_time:18730ms step_avg:56.93ms
step:330/2330 train_time:18789ms step_avg:56.94ms
step:331/2330 train_time:18844ms step_avg:56.93ms
step:332/2330 train_time:18903ms step_avg:56.94ms
step:333/2330 train_time:18958ms step_avg:56.93ms
step:334/2330 train_time:19017ms step_avg:56.94ms
step:335/2330 train_time:19072ms step_avg:56.93ms
step:336/2330 train_time:19131ms step_avg:56.94ms
step:337/2330 train_time:19186ms step_avg:56.93ms
step:338/2330 train_time:19244ms step_avg:56.93ms
step:339/2330 train_time:19299ms step_avg:56.93ms
step:340/2330 train_time:19358ms step_avg:56.94ms
step:341/2330 train_time:19414ms step_avg:56.93ms
step:342/2330 train_time:19473ms step_avg:56.94ms
step:343/2330 train_time:19529ms step_avg:56.94ms
step:344/2330 train_time:19588ms step_avg:56.94ms
step:345/2330 train_time:19643ms step_avg:56.94ms
step:346/2330 train_time:19702ms step_avg:56.94ms
step:347/2330 train_time:19757ms step_avg:56.94ms
step:348/2330 train_time:19817ms step_avg:56.94ms
step:349/2330 train_time:19872ms step_avg:56.94ms
step:350/2330 train_time:19931ms step_avg:56.95ms
step:351/2330 train_time:19986ms step_avg:56.94ms
step:352/2330 train_time:20046ms step_avg:56.95ms
step:353/2330 train_time:20101ms step_avg:56.94ms
step:354/2330 train_time:20159ms step_avg:56.95ms
step:355/2330 train_time:20214ms step_avg:56.94ms
step:356/2330 train_time:20274ms step_avg:56.95ms
step:357/2330 train_time:20329ms step_avg:56.95ms
step:358/2330 train_time:20388ms step_avg:56.95ms
step:359/2330 train_time:20443ms step_avg:56.94ms
step:360/2330 train_time:20502ms step_avg:56.95ms
step:361/2330 train_time:20556ms step_avg:56.94ms
step:362/2330 train_time:20616ms step_avg:56.95ms
step:363/2330 train_time:20672ms step_avg:56.95ms
step:364/2330 train_time:20731ms step_avg:56.95ms
step:365/2330 train_time:20786ms step_avg:56.95ms
step:366/2330 train_time:20845ms step_avg:56.95ms
step:367/2330 train_time:20900ms step_avg:56.95ms
step:368/2330 train_time:20959ms step_avg:56.95ms
step:369/2330 train_time:21015ms step_avg:56.95ms
step:370/2330 train_time:21073ms step_avg:56.96ms
step:371/2330 train_time:21129ms step_avg:56.95ms
step:372/2330 train_time:21188ms step_avg:56.96ms
step:373/2330 train_time:21243ms step_avg:56.95ms
step:374/2330 train_time:21301ms step_avg:56.95ms
step:375/2330 train_time:21356ms step_avg:56.95ms
step:376/2330 train_time:21415ms step_avg:56.96ms
step:377/2330 train_time:21471ms step_avg:56.95ms
step:378/2330 train_time:21530ms step_avg:56.96ms
step:379/2330 train_time:21586ms step_avg:56.95ms
step:380/2330 train_time:21643ms step_avg:56.96ms
step:381/2330 train_time:21699ms step_avg:56.95ms
step:382/2330 train_time:21758ms step_avg:56.96ms
step:383/2330 train_time:21813ms step_avg:56.95ms
step:384/2330 train_time:21873ms step_avg:56.96ms
step:385/2330 train_time:21928ms step_avg:56.96ms
step:386/2330 train_time:21987ms step_avg:56.96ms
step:387/2330 train_time:22042ms step_avg:56.96ms
step:388/2330 train_time:22101ms step_avg:56.96ms
step:389/2330 train_time:22156ms step_avg:56.96ms
step:390/2330 train_time:22215ms step_avg:56.96ms
step:391/2330 train_time:22271ms step_avg:56.96ms
step:392/2330 train_time:22329ms step_avg:56.96ms
step:393/2330 train_time:22384ms step_avg:56.96ms
step:394/2330 train_time:22442ms step_avg:56.96ms
step:395/2330 train_time:22498ms step_avg:56.96ms
step:396/2330 train_time:22556ms step_avg:56.96ms
step:397/2330 train_time:22612ms step_avg:56.96ms
step:398/2330 train_time:22671ms step_avg:56.96ms
step:399/2330 train_time:22727ms step_avg:56.96ms
step:400/2330 train_time:22785ms step_avg:56.96ms
step:401/2330 train_time:22841ms step_avg:56.96ms
step:402/2330 train_time:22900ms step_avg:56.96ms
step:403/2330 train_time:22955ms step_avg:56.96ms
step:404/2330 train_time:23014ms step_avg:56.97ms
step:405/2330 train_time:23069ms step_avg:56.96ms
step:406/2330 train_time:23128ms step_avg:56.97ms
step:407/2330 train_time:23184ms step_avg:56.96ms
step:408/2330 train_time:23242ms step_avg:56.97ms
step:409/2330 train_time:23298ms step_avg:56.96ms
step:410/2330 train_time:23357ms step_avg:56.97ms
step:411/2330 train_time:23412ms step_avg:56.96ms
step:412/2330 train_time:23472ms step_avg:56.97ms
step:413/2330 train_time:23527ms step_avg:56.97ms
step:414/2330 train_time:23585ms step_avg:56.97ms
step:415/2330 train_time:23641ms step_avg:56.97ms
step:416/2330 train_time:23699ms step_avg:56.97ms
step:417/2330 train_time:23755ms step_avg:56.97ms
step:418/2330 train_time:23813ms step_avg:56.97ms
step:419/2330 train_time:23869ms step_avg:56.97ms
step:420/2330 train_time:23929ms step_avg:56.97ms
step:421/2330 train_time:23983ms step_avg:56.97ms
step:422/2330 train_time:24043ms step_avg:56.97ms
step:423/2330 train_time:24099ms step_avg:56.97ms
step:424/2330 train_time:24157ms step_avg:56.97ms
step:425/2330 train_time:24212ms step_avg:56.97ms
step:426/2330 train_time:24271ms step_avg:56.97ms
step:427/2330 train_time:24327ms step_avg:56.97ms
step:428/2330 train_time:24385ms step_avg:56.97ms
step:429/2330 train_time:24440ms step_avg:56.97ms
step:430/2330 train_time:24499ms step_avg:56.97ms
step:431/2330 train_time:24554ms step_avg:56.97ms
step:432/2330 train_time:24613ms step_avg:56.98ms
step:433/2330 train_time:24669ms step_avg:56.97ms
step:434/2330 train_time:24727ms step_avg:56.98ms
step:435/2330 train_time:24783ms step_avg:56.97ms
step:436/2330 train_time:24842ms step_avg:56.98ms
step:437/2330 train_time:24897ms step_avg:56.97ms
step:438/2330 train_time:24957ms step_avg:56.98ms
step:439/2330 train_time:25012ms step_avg:56.97ms
step:440/2330 train_time:25071ms step_avg:56.98ms
step:441/2330 train_time:25126ms step_avg:56.98ms
step:442/2330 train_time:25184ms step_avg:56.98ms
step:443/2330 train_time:25240ms step_avg:56.97ms
step:444/2330 train_time:25298ms step_avg:56.98ms
step:445/2330 train_time:25354ms step_avg:56.97ms
step:446/2330 train_time:25412ms step_avg:56.98ms
step:447/2330 train_time:25468ms step_avg:56.98ms
step:448/2330 train_time:25527ms step_avg:56.98ms
step:449/2330 train_time:25582ms step_avg:56.98ms
step:450/2330 train_time:25641ms step_avg:56.98ms
step:451/2330 train_time:25696ms step_avg:56.98ms
step:452/2330 train_time:25755ms step_avg:56.98ms
step:453/2330 train_time:25811ms step_avg:56.98ms
step:454/2330 train_time:25869ms step_avg:56.98ms
step:455/2330 train_time:25925ms step_avg:56.98ms
step:456/2330 train_time:25983ms step_avg:56.98ms
step:457/2330 train_time:26039ms step_avg:56.98ms
step:458/2330 train_time:26098ms step_avg:56.98ms
step:459/2330 train_time:26153ms step_avg:56.98ms
step:460/2330 train_time:26213ms step_avg:56.98ms
step:461/2330 train_time:26268ms step_avg:56.98ms
step:462/2330 train_time:26327ms step_avg:56.98ms
step:463/2330 train_time:26382ms step_avg:56.98ms
step:464/2330 train_time:26441ms step_avg:56.98ms
step:465/2330 train_time:26496ms step_avg:56.98ms
step:466/2330 train_time:26556ms step_avg:56.99ms
step:467/2330 train_time:26611ms step_avg:56.98ms
step:468/2330 train_time:26669ms step_avg:56.99ms
step:469/2330 train_time:26725ms step_avg:56.98ms
step:470/2330 train_time:26783ms step_avg:56.99ms
step:471/2330 train_time:26839ms step_avg:56.98ms
step:472/2330 train_time:26897ms step_avg:56.99ms
step:473/2330 train_time:26953ms step_avg:56.98ms
step:474/2330 train_time:27012ms step_avg:56.99ms
step:475/2330 train_time:27067ms step_avg:56.98ms
step:476/2330 train_time:27127ms step_avg:56.99ms
step:477/2330 train_time:27182ms step_avg:56.98ms
step:478/2330 train_time:27240ms step_avg:56.99ms
step:479/2330 train_time:27295ms step_avg:56.98ms
step:480/2330 train_time:27354ms step_avg:56.99ms
step:481/2330 train_time:27410ms step_avg:56.99ms
step:482/2330 train_time:27469ms step_avg:56.99ms
step:483/2330 train_time:27524ms step_avg:56.99ms
step:484/2330 train_time:27583ms step_avg:56.99ms
step:485/2330 train_time:27638ms step_avg:56.98ms
step:486/2330 train_time:27698ms step_avg:56.99ms
step:487/2330 train_time:27753ms step_avg:56.99ms
step:488/2330 train_time:27812ms step_avg:56.99ms
step:489/2330 train_time:27867ms step_avg:56.99ms
step:490/2330 train_time:27926ms step_avg:56.99ms
step:491/2330 train_time:27982ms step_avg:56.99ms
step:492/2330 train_time:28040ms step_avg:56.99ms
step:493/2330 train_time:28095ms step_avg:56.99ms
step:494/2330 train_time:28155ms step_avg:56.99ms
step:495/2330 train_time:28211ms step_avg:56.99ms
step:496/2330 train_time:28269ms step_avg:56.99ms
step:497/2330 train_time:28325ms step_avg:56.99ms
step:498/2330 train_time:28383ms step_avg:56.99ms
step:499/2330 train_time:28438ms step_avg:56.99ms
step:500/2330 train_time:28497ms step_avg:56.99ms
step:500/2330 val_loss:5.1363 train_time:28576ms step_avg:57.15ms
step:501/2330 train_time:28595ms step_avg:57.08ms
step:502/2330 train_time:28615ms step_avg:57.00ms
step:503/2330 train_time:28669ms step_avg:57.00ms
step:504/2330 train_time:28733ms step_avg:57.01ms
step:505/2330 train_time:28788ms step_avg:57.01ms
step:506/2330 train_time:28850ms step_avg:57.02ms
step:507/2330 train_time:28905ms step_avg:57.01ms
step:508/2330 train_time:28964ms step_avg:57.02ms
step:509/2330 train_time:29019ms step_avg:57.01ms
step:510/2330 train_time:29078ms step_avg:57.02ms
step:511/2330 train_time:29133ms step_avg:57.01ms
step:512/2330 train_time:29191ms step_avg:57.01ms
step:513/2330 train_time:29246ms step_avg:57.01ms
step:514/2330 train_time:29305ms step_avg:57.01ms
step:515/2330 train_time:29360ms step_avg:57.01ms
step:516/2330 train_time:29418ms step_avg:57.01ms
step:517/2330 train_time:29473ms step_avg:57.01ms
step:518/2330 train_time:29531ms step_avg:57.01ms
step:519/2330 train_time:29586ms step_avg:57.01ms
step:520/2330 train_time:29646ms step_avg:57.01ms
step:521/2330 train_time:29703ms step_avg:57.01ms
step:522/2330 train_time:29763ms step_avg:57.02ms
step:523/2330 train_time:29818ms step_avg:57.01ms
step:524/2330 train_time:29877ms step_avg:57.02ms
step:525/2330 train_time:29932ms step_avg:57.01ms
step:526/2330 train_time:29991ms step_avg:57.02ms
step:527/2330 train_time:30047ms step_avg:57.01ms
step:528/2330 train_time:30105ms step_avg:57.02ms
step:529/2330 train_time:30160ms step_avg:57.01ms
step:530/2330 train_time:30218ms step_avg:57.02ms
step:531/2330 train_time:30274ms step_avg:57.01ms
step:532/2330 train_time:30332ms step_avg:57.01ms
step:533/2330 train_time:30387ms step_avg:57.01ms
step:534/2330 train_time:30446ms step_avg:57.02ms
step:535/2330 train_time:30502ms step_avg:57.01ms
step:536/2330 train_time:30560ms step_avg:57.02ms
step:537/2330 train_time:30616ms step_avg:57.01ms
step:538/2330 train_time:30675ms step_avg:57.02ms
step:539/2330 train_time:30731ms step_avg:57.01ms
step:540/2330 train_time:30791ms step_avg:57.02ms
step:541/2330 train_time:30846ms step_avg:57.02ms
step:542/2330 train_time:30906ms step_avg:57.02ms
step:543/2330 train_time:30961ms step_avg:57.02ms
step:544/2330 train_time:31020ms step_avg:57.02ms
step:545/2330 train_time:31076ms step_avg:57.02ms
step:546/2330 train_time:31134ms step_avg:57.02ms
step:547/2330 train_time:31190ms step_avg:57.02ms
step:548/2330 train_time:31248ms step_avg:57.02ms
step:549/2330 train_time:31304ms step_avg:57.02ms
step:550/2330 train_time:31362ms step_avg:57.02ms
step:551/2330 train_time:31417ms step_avg:57.02ms
step:552/2330 train_time:31475ms step_avg:57.02ms
step:553/2330 train_time:31530ms step_avg:57.02ms
step:554/2330 train_time:31589ms step_avg:57.02ms
step:555/2330 train_time:31645ms step_avg:57.02ms
step:556/2330 train_time:31704ms step_avg:57.02ms
step:557/2330 train_time:31759ms step_avg:57.02ms
step:558/2330 train_time:31819ms step_avg:57.02ms
step:559/2330 train_time:31874ms step_avg:57.02ms
step:560/2330 train_time:31933ms step_avg:57.02ms
step:561/2330 train_time:31988ms step_avg:57.02ms
step:562/2330 train_time:32048ms step_avg:57.02ms
step:563/2330 train_time:32104ms step_avg:57.02ms
step:564/2330 train_time:32162ms step_avg:57.02ms
step:565/2330 train_time:32217ms step_avg:57.02ms
step:566/2330 train_time:32276ms step_avg:57.02ms
step:567/2330 train_time:32331ms step_avg:57.02ms
step:568/2330 train_time:32389ms step_avg:57.02ms
step:569/2330 train_time:32444ms step_avg:57.02ms
step:570/2330 train_time:32503ms step_avg:57.02ms
step:571/2330 train_time:32558ms step_avg:57.02ms
step:572/2330 train_time:32616ms step_avg:57.02ms
step:573/2330 train_time:32672ms step_avg:57.02ms
step:574/2330 train_time:32731ms step_avg:57.02ms
step:575/2330 train_time:32787ms step_avg:57.02ms
step:576/2330 train_time:32847ms step_avg:57.03ms
step:577/2330 train_time:32903ms step_avg:57.02ms
step:578/2330 train_time:32962ms step_avg:57.03ms
step:579/2330 train_time:33017ms step_avg:57.02ms
step:580/2330 train_time:33075ms step_avg:57.03ms
step:581/2330 train_time:33131ms step_avg:57.02ms
step:582/2330 train_time:33190ms step_avg:57.03ms
step:583/2330 train_time:33245ms step_avg:57.02ms
step:584/2330 train_time:33304ms step_avg:57.03ms
step:585/2330 train_time:33360ms step_avg:57.03ms
step:586/2330 train_time:33418ms step_avg:57.03ms
step:587/2330 train_time:33472ms step_avg:57.02ms
step:588/2330 train_time:33531ms step_avg:57.03ms
step:589/2330 train_time:33586ms step_avg:57.02ms
step:590/2330 train_time:33646ms step_avg:57.03ms
step:591/2330 train_time:33702ms step_avg:57.02ms
step:592/2330 train_time:33760ms step_avg:57.03ms
step:593/2330 train_time:33815ms step_avg:57.02ms
step:594/2330 train_time:33875ms step_avg:57.03ms
step:595/2330 train_time:33930ms step_avg:57.03ms
step:596/2330 train_time:33990ms step_avg:57.03ms
step:597/2330 train_time:34045ms step_avg:57.03ms
step:598/2330 train_time:34104ms step_avg:57.03ms
step:599/2330 train_time:34160ms step_avg:57.03ms
step:600/2330 train_time:34218ms step_avg:57.03ms
step:601/2330 train_time:34273ms step_avg:57.03ms
step:602/2330 train_time:34332ms step_avg:57.03ms
step:603/2330 train_time:34387ms step_avg:57.03ms
step:604/2330 train_time:34446ms step_avg:57.03ms
step:605/2330 train_time:34502ms step_avg:57.03ms
step:606/2330 train_time:34560ms step_avg:57.03ms
step:607/2330 train_time:34616ms step_avg:57.03ms
step:608/2330 train_time:34674ms step_avg:57.03ms
step:609/2330 train_time:34729ms step_avg:57.03ms
step:610/2330 train_time:34788ms step_avg:57.03ms
step:611/2330 train_time:34844ms step_avg:57.03ms
step:612/2330 train_time:34903ms step_avg:57.03ms
step:613/2330 train_time:34958ms step_avg:57.03ms
step:614/2330 train_time:35017ms step_avg:57.03ms
step:615/2330 train_time:35072ms step_avg:57.03ms
step:616/2330 train_time:35131ms step_avg:57.03ms
step:617/2330 train_time:35187ms step_avg:57.03ms
step:618/2330 train_time:35246ms step_avg:57.03ms
step:619/2330 train_time:35301ms step_avg:57.03ms
step:620/2330 train_time:35360ms step_avg:57.03ms
step:621/2330 train_time:35415ms step_avg:57.03ms
step:622/2330 train_time:35473ms step_avg:57.03ms
step:623/2330 train_time:35528ms step_avg:57.03ms
step:624/2330 train_time:35588ms step_avg:57.03ms
step:625/2330 train_time:35643ms step_avg:57.03ms
step:626/2330 train_time:35702ms step_avg:57.03ms
step:627/2330 train_time:35757ms step_avg:57.03ms
step:628/2330 train_time:35817ms step_avg:57.03ms
step:629/2330 train_time:35872ms step_avg:57.03ms
step:630/2330 train_time:35931ms step_avg:57.03ms
step:631/2330 train_time:35986ms step_avg:57.03ms
step:632/2330 train_time:36046ms step_avg:57.03ms
step:633/2330 train_time:36102ms step_avg:57.03ms
step:634/2330 train_time:36160ms step_avg:57.03ms
step:635/2330 train_time:36215ms step_avg:57.03ms
step:636/2330 train_time:36274ms step_avg:57.03ms
step:637/2330 train_time:36329ms step_avg:57.03ms
step:638/2330 train_time:36389ms step_avg:57.04ms
step:639/2330 train_time:36444ms step_avg:57.03ms
step:640/2330 train_time:36503ms step_avg:57.04ms
step:641/2330 train_time:36557ms step_avg:57.03ms
step:642/2330 train_time:36616ms step_avg:57.03ms
step:643/2330 train_time:36672ms step_avg:57.03ms
step:644/2330 train_time:36731ms step_avg:57.04ms
step:645/2330 train_time:36786ms step_avg:57.03ms
step:646/2330 train_time:36845ms step_avg:57.04ms
step:647/2330 train_time:36901ms step_avg:57.03ms
step:648/2330 train_time:36959ms step_avg:57.04ms
step:649/2330 train_time:37014ms step_avg:57.03ms
step:650/2330 train_time:37073ms step_avg:57.04ms
step:651/2330 train_time:37128ms step_avg:57.03ms
step:652/2330 train_time:37188ms step_avg:57.04ms
step:653/2330 train_time:37244ms step_avg:57.03ms
step:654/2330 train_time:37302ms step_avg:57.04ms
step:655/2330 train_time:37357ms step_avg:57.03ms
step:656/2330 train_time:37415ms step_avg:57.04ms
step:657/2330 train_time:37471ms step_avg:57.03ms
step:658/2330 train_time:37530ms step_avg:57.04ms
step:659/2330 train_time:37585ms step_avg:57.03ms
step:660/2330 train_time:37645ms step_avg:57.04ms
step:661/2330 train_time:37700ms step_avg:57.04ms
step:662/2330 train_time:37759ms step_avg:57.04ms
step:663/2330 train_time:37814ms step_avg:57.03ms
step:664/2330 train_time:37872ms step_avg:57.04ms
step:665/2330 train_time:37928ms step_avg:57.03ms
step:666/2330 train_time:37987ms step_avg:57.04ms
step:667/2330 train_time:38043ms step_avg:57.04ms
step:668/2330 train_time:38101ms step_avg:57.04ms
step:669/2330 train_time:38157ms step_avg:57.04ms
step:670/2330 train_time:38214ms step_avg:57.04ms
step:671/2330 train_time:38269ms step_avg:57.03ms
step:672/2330 train_time:38329ms step_avg:57.04ms
step:673/2330 train_time:38384ms step_avg:57.03ms
step:674/2330 train_time:38444ms step_avg:57.04ms
step:675/2330 train_time:38499ms step_avg:57.04ms
step:676/2330 train_time:38557ms step_avg:57.04ms
step:677/2330 train_time:38612ms step_avg:57.03ms
step:678/2330 train_time:38671ms step_avg:57.04ms
step:679/2330 train_time:38726ms step_avg:57.03ms
step:680/2330 train_time:38785ms step_avg:57.04ms
step:681/2330 train_time:38840ms step_avg:57.03ms
step:682/2330 train_time:38900ms step_avg:57.04ms
step:683/2330 train_time:38955ms step_avg:57.04ms
step:684/2330 train_time:39014ms step_avg:57.04ms
step:685/2330 train_time:39069ms step_avg:57.04ms
step:686/2330 train_time:39129ms step_avg:57.04ms
step:687/2330 train_time:39186ms step_avg:57.04ms
step:688/2330 train_time:39244ms step_avg:57.04ms
step:689/2330 train_time:39300ms step_avg:57.04ms
step:690/2330 train_time:39359ms step_avg:57.04ms
step:691/2330 train_time:39413ms step_avg:57.04ms
step:692/2330 train_time:39473ms step_avg:57.04ms
step:693/2330 train_time:39527ms step_avg:57.04ms
step:694/2330 train_time:39588ms step_avg:57.04ms
step:695/2330 train_time:39643ms step_avg:57.04ms
step:696/2330 train_time:39702ms step_avg:57.04ms
step:697/2330 train_time:39758ms step_avg:57.04ms
step:698/2330 train_time:39816ms step_avg:57.04ms
step:699/2330 train_time:39871ms step_avg:57.04ms
step:700/2330 train_time:39930ms step_avg:57.04ms
step:701/2330 train_time:39986ms step_avg:57.04ms
step:702/2330 train_time:40046ms step_avg:57.05ms
step:703/2330 train_time:40101ms step_avg:57.04ms
step:704/2330 train_time:40160ms step_avg:57.05ms
step:705/2330 train_time:40215ms step_avg:57.04ms
step:706/2330 train_time:40274ms step_avg:57.05ms
step:707/2330 train_time:40329ms step_avg:57.04ms
step:708/2330 train_time:40389ms step_avg:57.05ms
step:709/2330 train_time:40444ms step_avg:57.04ms
step:710/2330 train_time:40504ms step_avg:57.05ms
step:711/2330 train_time:40559ms step_avg:57.05ms
step:712/2330 train_time:40617ms step_avg:57.05ms
step:713/2330 train_time:40673ms step_avg:57.04ms
step:714/2330 train_time:40732ms step_avg:57.05ms
step:715/2330 train_time:40788ms step_avg:57.05ms
step:716/2330 train_time:40846ms step_avg:57.05ms
step:717/2330 train_time:40902ms step_avg:57.05ms
step:718/2330 train_time:40961ms step_avg:57.05ms
step:719/2330 train_time:41016ms step_avg:57.05ms
step:720/2330 train_time:41075ms step_avg:57.05ms
step:721/2330 train_time:41130ms step_avg:57.05ms
step:722/2330 train_time:41189ms step_avg:57.05ms
step:723/2330 train_time:41245ms step_avg:57.05ms
step:724/2330 train_time:41303ms step_avg:57.05ms
step:725/2330 train_time:41359ms step_avg:57.05ms
step:726/2330 train_time:41417ms step_avg:57.05ms
step:727/2330 train_time:41473ms step_avg:57.05ms
step:728/2330 train_time:41531ms step_avg:57.05ms
step:729/2330 train_time:41587ms step_avg:57.05ms
step:730/2330 train_time:41646ms step_avg:57.05ms
step:731/2330 train_time:41701ms step_avg:57.05ms
step:732/2330 train_time:41760ms step_avg:57.05ms
step:733/2330 train_time:41815ms step_avg:57.05ms
step:734/2330 train_time:41874ms step_avg:57.05ms
step:735/2330 train_time:41929ms step_avg:57.05ms
step:736/2330 train_time:41989ms step_avg:57.05ms
step:737/2330 train_time:42045ms step_avg:57.05ms
step:738/2330 train_time:42103ms step_avg:57.05ms
step:739/2330 train_time:42158ms step_avg:57.05ms
step:740/2330 train_time:42217ms step_avg:57.05ms
step:741/2330 train_time:42272ms step_avg:57.05ms
step:742/2330 train_time:42332ms step_avg:57.05ms
step:743/2330 train_time:42387ms step_avg:57.05ms
step:744/2330 train_time:42446ms step_avg:57.05ms
step:745/2330 train_time:42501ms step_avg:57.05ms
step:746/2330 train_time:42560ms step_avg:57.05ms
step:747/2330 train_time:42615ms step_avg:57.05ms
step:748/2330 train_time:42675ms step_avg:57.05ms
step:749/2330 train_time:42731ms step_avg:57.05ms
step:750/2330 train_time:42790ms step_avg:57.05ms
step:750/2330 val_loss:4.6947 train_time:42868ms step_avg:57.16ms
step:751/2330 train_time:42885ms step_avg:57.10ms
step:752/2330 train_time:42906ms step_avg:57.06ms
step:753/2330 train_time:42961ms step_avg:57.05ms
step:754/2330 train_time:43024ms step_avg:57.06ms
step:755/2330 train_time:43080ms step_avg:57.06ms
step:756/2330 train_time:43141ms step_avg:57.06ms
step:757/2330 train_time:43197ms step_avg:57.06ms
step:758/2330 train_time:43255ms step_avg:57.06ms
step:759/2330 train_time:43310ms step_avg:57.06ms
step:760/2330 train_time:43368ms step_avg:57.06ms
step:761/2330 train_time:43423ms step_avg:57.06ms
step:762/2330 train_time:43481ms step_avg:57.06ms
step:763/2330 train_time:43537ms step_avg:57.06ms
step:764/2330 train_time:43595ms step_avg:57.06ms
step:765/2330 train_time:43650ms step_avg:57.06ms
step:766/2330 train_time:43709ms step_avg:57.06ms
step:767/2330 train_time:43764ms step_avg:57.06ms
step:768/2330 train_time:43823ms step_avg:57.06ms
step:769/2330 train_time:43880ms step_avg:57.06ms
step:770/2330 train_time:43941ms step_avg:57.07ms
step:771/2330 train_time:43998ms step_avg:57.07ms
step:772/2330 train_time:44058ms step_avg:57.07ms
step:773/2330 train_time:44115ms step_avg:57.07ms
step:774/2330 train_time:44176ms step_avg:57.07ms
step:775/2330 train_time:44232ms step_avg:57.07ms
step:776/2330 train_time:44292ms step_avg:57.08ms
step:777/2330 train_time:44348ms step_avg:57.08ms
step:778/2330 train_time:44407ms step_avg:57.08ms
step:779/2330 train_time:44463ms step_avg:57.08ms
step:780/2330 train_time:44522ms step_avg:57.08ms
step:781/2330 train_time:44578ms step_avg:57.08ms
step:782/2330 train_time:44638ms step_avg:57.08ms
step:783/2330 train_time:44695ms step_avg:57.08ms
step:784/2330 train_time:44753ms step_avg:57.08ms
step:785/2330 train_time:44809ms step_avg:57.08ms
step:786/2330 train_time:44868ms step_avg:57.08ms
step:787/2330 train_time:44925ms step_avg:57.08ms
step:788/2330 train_time:44985ms step_avg:57.09ms
step:789/2330 train_time:45041ms step_avg:57.09ms
step:790/2330 train_time:45102ms step_avg:57.09ms
step:791/2330 train_time:45158ms step_avg:57.09ms
step:792/2330 train_time:45219ms step_avg:57.09ms
step:793/2330 train_time:45275ms step_avg:57.09ms
step:794/2330 train_time:45335ms step_avg:57.10ms
step:795/2330 train_time:45392ms step_avg:57.10ms
step:796/2330 train_time:45451ms step_avg:57.10ms
step:797/2330 train_time:45507ms step_avg:57.10ms
step:798/2330 train_time:45565ms step_avg:57.10ms
step:799/2330 train_time:45622ms step_avg:57.10ms
step:800/2330 train_time:45681ms step_avg:57.10ms
step:801/2330 train_time:45738ms step_avg:57.10ms
step:802/2330 train_time:45797ms step_avg:57.10ms
step:803/2330 train_time:45854ms step_avg:57.10ms
step:804/2330 train_time:45913ms step_avg:57.11ms
step:805/2330 train_time:45969ms step_avg:57.10ms
step:806/2330 train_time:46029ms step_avg:57.11ms
step:807/2330 train_time:46085ms step_avg:57.11ms
step:808/2330 train_time:46145ms step_avg:57.11ms
step:809/2330 train_time:46201ms step_avg:57.11ms
step:810/2330 train_time:46261ms step_avg:57.11ms
step:811/2330 train_time:46318ms step_avg:57.11ms
step:812/2330 train_time:46378ms step_avg:57.12ms
step:813/2330 train_time:46435ms step_avg:57.12ms
step:814/2330 train_time:46495ms step_avg:57.12ms
step:815/2330 train_time:46552ms step_avg:57.12ms
step:816/2330 train_time:46610ms step_avg:57.12ms
step:817/2330 train_time:46666ms step_avg:57.12ms
step:818/2330 train_time:46725ms step_avg:57.12ms
step:819/2330 train_time:46781ms step_avg:57.12ms
step:820/2330 train_time:46841ms step_avg:57.12ms
step:821/2330 train_time:46898ms step_avg:57.12ms
step:822/2330 train_time:46958ms step_avg:57.13ms
step:823/2330 train_time:47014ms step_avg:57.13ms
step:824/2330 train_time:47074ms step_avg:57.13ms
step:825/2330 train_time:47130ms step_avg:57.13ms
step:826/2330 train_time:47190ms step_avg:57.13ms
step:827/2330 train_time:47246ms step_avg:57.13ms
step:828/2330 train_time:47305ms step_avg:57.13ms
step:829/2330 train_time:47361ms step_avg:57.13ms
step:830/2330 train_time:47421ms step_avg:57.13ms
step:831/2330 train_time:47477ms step_avg:57.13ms
step:832/2330 train_time:47538ms step_avg:57.14ms
step:833/2330 train_time:47594ms step_avg:57.14ms
step:834/2330 train_time:47654ms step_avg:57.14ms
step:835/2330 train_time:47709ms step_avg:57.14ms
step:836/2330 train_time:47768ms step_avg:57.14ms
step:837/2330 train_time:47825ms step_avg:57.14ms
step:838/2330 train_time:47884ms step_avg:57.14ms
step:839/2330 train_time:47940ms step_avg:57.14ms
step:840/2330 train_time:48000ms step_avg:57.14ms
step:841/2330 train_time:48057ms step_avg:57.14ms
step:842/2330 train_time:48118ms step_avg:57.15ms
step:843/2330 train_time:48174ms step_avg:57.15ms
step:844/2330 train_time:48233ms step_avg:57.15ms
step:845/2330 train_time:48290ms step_avg:57.15ms
step:846/2330 train_time:48349ms step_avg:57.15ms
step:847/2330 train_time:48405ms step_avg:57.15ms
step:848/2330 train_time:48464ms step_avg:57.15ms
step:849/2330 train_time:48521ms step_avg:57.15ms
step:850/2330 train_time:48581ms step_avg:57.15ms
step:851/2330 train_time:48637ms step_avg:57.15ms
step:852/2330 train_time:48697ms step_avg:57.16ms
step:853/2330 train_time:48754ms step_avg:57.16ms
step:854/2330 train_time:48813ms step_avg:57.16ms
step:855/2330 train_time:48869ms step_avg:57.16ms
step:856/2330 train_time:48928ms step_avg:57.16ms
step:857/2330 train_time:48984ms step_avg:57.16ms
step:858/2330 train_time:49043ms step_avg:57.16ms
step:859/2330 train_time:49099ms step_avg:57.16ms
step:860/2330 train_time:49160ms step_avg:57.16ms
step:861/2330 train_time:49217ms step_avg:57.16ms
step:862/2330 train_time:49276ms step_avg:57.16ms
step:863/2330 train_time:49332ms step_avg:57.16ms
step:864/2330 train_time:49392ms step_avg:57.17ms
step:865/2330 train_time:49447ms step_avg:57.16ms
step:866/2330 train_time:49506ms step_avg:57.17ms
step:867/2330 train_time:49562ms step_avg:57.16ms
step:868/2330 train_time:49622ms step_avg:57.17ms
step:869/2330 train_time:49678ms step_avg:57.17ms
step:870/2330 train_time:49739ms step_avg:57.17ms
step:871/2330 train_time:49795ms step_avg:57.17ms
step:872/2330 train_time:49855ms step_avg:57.17ms
step:873/2330 train_time:49912ms step_avg:57.17ms
step:874/2330 train_time:49972ms step_avg:57.18ms
step:875/2330 train_time:50028ms step_avg:57.17ms
step:876/2330 train_time:50087ms step_avg:57.18ms
step:877/2330 train_time:50142ms step_avg:57.17ms
step:878/2330 train_time:50202ms step_avg:57.18ms
step:879/2330 train_time:50259ms step_avg:57.18ms
step:880/2330 train_time:50319ms step_avg:57.18ms
step:881/2330 train_time:50375ms step_avg:57.18ms
step:882/2330 train_time:50436ms step_avg:57.18ms
step:883/2330 train_time:50492ms step_avg:57.18ms
step:884/2330 train_time:50552ms step_avg:57.19ms
step:885/2330 train_time:50608ms step_avg:57.18ms
step:886/2330 train_time:50667ms step_avg:57.19ms
step:887/2330 train_time:50723ms step_avg:57.18ms
step:888/2330 train_time:50782ms step_avg:57.19ms
step:889/2330 train_time:50839ms step_avg:57.19ms
step:890/2330 train_time:50899ms step_avg:57.19ms
step:891/2330 train_time:50955ms step_avg:57.19ms
step:892/2330 train_time:51014ms step_avg:57.19ms
step:893/2330 train_time:51071ms step_avg:57.19ms
step:894/2330 train_time:51129ms step_avg:57.19ms
step:895/2330 train_time:51185ms step_avg:57.19ms
step:896/2330 train_time:51245ms step_avg:57.19ms
step:897/2330 train_time:51301ms step_avg:57.19ms
step:898/2330 train_time:51361ms step_avg:57.19ms
step:899/2330 train_time:51417ms step_avg:57.19ms
step:900/2330 train_time:51477ms step_avg:57.20ms
step:901/2330 train_time:51534ms step_avg:57.20ms
step:902/2330 train_time:51593ms step_avg:57.20ms
step:903/2330 train_time:51650ms step_avg:57.20ms
step:904/2330 train_time:51709ms step_avg:57.20ms
step:905/2330 train_time:51766ms step_avg:57.20ms
step:906/2330 train_time:51824ms step_avg:57.20ms
step:907/2330 train_time:51881ms step_avg:57.20ms
step:908/2330 train_time:51941ms step_avg:57.20ms
step:909/2330 train_time:51997ms step_avg:57.20ms
step:910/2330 train_time:52057ms step_avg:57.21ms
step:911/2330 train_time:52113ms step_avg:57.20ms
step:912/2330 train_time:52172ms step_avg:57.21ms
step:913/2330 train_time:52228ms step_avg:57.21ms
step:914/2330 train_time:52287ms step_avg:57.21ms
step:915/2330 train_time:52344ms step_avg:57.21ms
step:916/2330 train_time:52402ms step_avg:57.21ms
step:917/2330 train_time:52458ms step_avg:57.21ms
step:918/2330 train_time:52519ms step_avg:57.21ms
step:919/2330 train_time:52576ms step_avg:57.21ms
step:920/2330 train_time:52635ms step_avg:57.21ms
step:921/2330 train_time:52692ms step_avg:57.21ms
step:922/2330 train_time:52751ms step_avg:57.21ms
step:923/2330 train_time:52807ms step_avg:57.21ms
step:924/2330 train_time:52866ms step_avg:57.21ms
step:925/2330 train_time:52922ms step_avg:57.21ms
step:926/2330 train_time:52982ms step_avg:57.22ms
step:927/2330 train_time:53038ms step_avg:57.22ms
step:928/2330 train_time:53099ms step_avg:57.22ms
step:929/2330 train_time:53155ms step_avg:57.22ms
step:930/2330 train_time:53215ms step_avg:57.22ms
step:931/2330 train_time:53271ms step_avg:57.22ms
step:932/2330 train_time:53330ms step_avg:57.22ms
step:933/2330 train_time:53387ms step_avg:57.22ms
step:934/2330 train_time:53446ms step_avg:57.22ms
step:935/2330 train_time:53501ms step_avg:57.22ms
step:936/2330 train_time:53561ms step_avg:57.22ms
step:937/2330 train_time:53617ms step_avg:57.22ms
step:938/2330 train_time:53678ms step_avg:57.23ms
step:939/2330 train_time:53735ms step_avg:57.23ms
step:940/2330 train_time:53794ms step_avg:57.23ms
step:941/2330 train_time:53851ms step_avg:57.23ms
step:942/2330 train_time:53910ms step_avg:57.23ms
step:943/2330 train_time:53966ms step_avg:57.23ms
step:944/2330 train_time:54026ms step_avg:57.23ms
step:945/2330 train_time:54082ms step_avg:57.23ms
step:946/2330 train_time:54142ms step_avg:57.23ms
step:947/2330 train_time:54198ms step_avg:57.23ms
step:948/2330 train_time:54258ms step_avg:57.23ms
step:949/2330 train_time:54315ms step_avg:57.23ms
step:950/2330 train_time:54374ms step_avg:57.24ms
step:951/2330 train_time:54430ms step_avg:57.23ms
step:952/2330 train_time:54489ms step_avg:57.24ms
step:953/2330 train_time:54545ms step_avg:57.24ms
step:954/2330 train_time:54604ms step_avg:57.24ms
step:955/2330 train_time:54660ms step_avg:57.24ms
step:956/2330 train_time:54721ms step_avg:57.24ms
step:957/2330 train_time:54776ms step_avg:57.24ms
step:958/2330 train_time:54838ms step_avg:57.24ms
step:959/2330 train_time:54894ms step_avg:57.24ms
step:960/2330 train_time:54954ms step_avg:57.24ms
step:961/2330 train_time:55010ms step_avg:57.24ms
step:962/2330 train_time:55070ms step_avg:57.25ms
step:963/2330 train_time:55125ms step_avg:57.24ms
step:964/2330 train_time:55185ms step_avg:57.25ms
step:965/2330 train_time:55241ms step_avg:57.24ms
step:966/2330 train_time:55301ms step_avg:57.25ms
step:967/2330 train_time:55357ms step_avg:57.25ms
step:968/2330 train_time:55417ms step_avg:57.25ms
step:969/2330 train_time:55473ms step_avg:57.25ms
step:970/2330 train_time:55533ms step_avg:57.25ms
step:971/2330 train_time:55589ms step_avg:57.25ms
step:972/2330 train_time:55648ms step_avg:57.25ms
step:973/2330 train_time:55704ms step_avg:57.25ms
step:974/2330 train_time:55763ms step_avg:57.25ms
step:975/2330 train_time:55819ms step_avg:57.25ms
step:976/2330 train_time:55879ms step_avg:57.25ms
step:977/2330 train_time:55936ms step_avg:57.25ms
step:978/2330 train_time:55996ms step_avg:57.26ms
step:979/2330 train_time:56053ms step_avg:57.25ms
step:980/2330 train_time:56112ms step_avg:57.26ms
step:981/2330 train_time:56168ms step_avg:57.26ms
step:982/2330 train_time:56228ms step_avg:57.26ms
step:983/2330 train_time:56283ms step_avg:57.26ms
step:984/2330 train_time:56343ms step_avg:57.26ms
step:985/2330 train_time:56399ms step_avg:57.26ms
step:986/2330 train_time:56458ms step_avg:57.26ms
step:987/2330 train_time:56515ms step_avg:57.26ms
step:988/2330 train_time:56576ms step_avg:57.26ms
step:989/2330 train_time:56633ms step_avg:57.26ms
step:990/2330 train_time:56692ms step_avg:57.26ms
step:991/2330 train_time:56749ms step_avg:57.26ms
step:992/2330 train_time:56807ms step_avg:57.27ms
step:993/2330 train_time:56863ms step_avg:57.26ms
step:994/2330 train_time:56923ms step_avg:57.27ms
step:995/2330 train_time:56979ms step_avg:57.27ms
step:996/2330 train_time:57039ms step_avg:57.27ms
step:997/2330 train_time:57096ms step_avg:57.27ms
step:998/2330 train_time:57155ms step_avg:57.27ms
step:999/2330 train_time:57213ms step_avg:57.27ms
step:1000/2330 train_time:57272ms step_avg:57.27ms
step:1000/2330 val_loss:4.4561 train_time:57351ms step_avg:57.35ms
step:1001/2330 train_time:57370ms step_avg:57.31ms
step:1002/2330 train_time:57391ms step_avg:57.28ms
step:1003/2330 train_time:57443ms step_avg:57.27ms
step:1004/2330 train_time:57509ms step_avg:57.28ms
step:1005/2330 train_time:57565ms step_avg:57.28ms
step:1006/2330 train_time:57629ms step_avg:57.29ms
step:1007/2330 train_time:57684ms step_avg:57.28ms
step:1008/2330 train_time:57744ms step_avg:57.29ms
step:1009/2330 train_time:57800ms step_avg:57.28ms
step:1010/2330 train_time:57859ms step_avg:57.29ms
step:1011/2330 train_time:57915ms step_avg:57.28ms
step:1012/2330 train_time:57973ms step_avg:57.29ms
step:1013/2330 train_time:58028ms step_avg:57.28ms
step:1014/2330 train_time:58087ms step_avg:57.28ms
step:1015/2330 train_time:58143ms step_avg:57.28ms
step:1016/2330 train_time:58201ms step_avg:57.28ms
step:1017/2330 train_time:58259ms step_avg:57.28ms
step:1018/2330 train_time:58319ms step_avg:57.29ms
step:1019/2330 train_time:58377ms step_avg:57.29ms
step:1020/2330 train_time:58437ms step_avg:57.29ms
step:1021/2330 train_time:58494ms step_avg:57.29ms
step:1022/2330 train_time:58553ms step_avg:57.29ms
step:1023/2330 train_time:58609ms step_avg:57.29ms
step:1024/2330 train_time:58669ms step_avg:57.29ms
step:1025/2330 train_time:58726ms step_avg:57.29ms
step:1026/2330 train_time:58785ms step_avg:57.30ms
step:1027/2330 train_time:58842ms step_avg:57.30ms
step:1028/2330 train_time:58902ms step_avg:57.30ms
step:1029/2330 train_time:58957ms step_avg:57.30ms
step:1030/2330 train_time:59016ms step_avg:57.30ms
step:1031/2330 train_time:59072ms step_avg:57.30ms
step:1032/2330 train_time:59130ms step_avg:57.30ms
step:1033/2330 train_time:59187ms step_avg:57.30ms
step:1034/2330 train_time:59247ms step_avg:57.30ms
step:1035/2330 train_time:59304ms step_avg:57.30ms
step:1036/2330 train_time:59365ms step_avg:57.30ms
step:1037/2330 train_time:59422ms step_avg:57.30ms
step:1038/2330 train_time:59482ms step_avg:57.30ms
step:1039/2330 train_time:59538ms step_avg:57.30ms
step:1040/2330 train_time:59598ms step_avg:57.31ms
step:1041/2330 train_time:59654ms step_avg:57.30ms
step:1042/2330 train_time:59714ms step_avg:57.31ms
step:1043/2330 train_time:59769ms step_avg:57.31ms
step:1044/2330 train_time:59829ms step_avg:57.31ms
step:1045/2330 train_time:59885ms step_avg:57.31ms
step:1046/2330 train_time:59944ms step_avg:57.31ms
step:1047/2330 train_time:60000ms step_avg:57.31ms
step:1048/2330 train_time:60061ms step_avg:57.31ms
step:1049/2330 train_time:60117ms step_avg:57.31ms
step:1050/2330 train_time:60176ms step_avg:57.31ms
step:1051/2330 train_time:60232ms step_avg:57.31ms
step:1052/2330 train_time:60291ms step_avg:57.31ms
step:1053/2330 train_time:60348ms step_avg:57.31ms
step:1054/2330 train_time:60408ms step_avg:57.31ms
step:1055/2330 train_time:60465ms step_avg:57.31ms
step:1056/2330 train_time:60525ms step_avg:57.32ms
step:1057/2330 train_time:60582ms step_avg:57.32ms
step:1058/2330 train_time:60642ms step_avg:57.32ms
step:1059/2330 train_time:60699ms step_avg:57.32ms
step:1060/2330 train_time:60758ms step_avg:57.32ms
step:1061/2330 train_time:60813ms step_avg:57.32ms
step:1062/2330 train_time:60873ms step_avg:57.32ms
step:1063/2330 train_time:60928ms step_avg:57.32ms
step:1064/2330 train_time:60990ms step_avg:57.32ms
step:1065/2330 train_time:61045ms step_avg:57.32ms
step:1066/2330 train_time:61106ms step_avg:57.32ms
step:1067/2330 train_time:61162ms step_avg:57.32ms
step:1068/2330 train_time:61221ms step_avg:57.32ms
step:1069/2330 train_time:61277ms step_avg:57.32ms
step:1070/2330 train_time:61336ms step_avg:57.32ms
step:1071/2330 train_time:61392ms step_avg:57.32ms
step:1072/2330 train_time:61452ms step_avg:57.32ms
step:1073/2330 train_time:61508ms step_avg:57.32ms
step:1074/2330 train_time:61569ms step_avg:57.33ms
step:1075/2330 train_time:61625ms step_avg:57.33ms
step:1076/2330 train_time:61685ms step_avg:57.33ms
step:1077/2330 train_time:61742ms step_avg:57.33ms
step:1078/2330 train_time:61802ms step_avg:57.33ms
step:1079/2330 train_time:61858ms step_avg:57.33ms
step:1080/2330 train_time:61918ms step_avg:57.33ms
step:1081/2330 train_time:61973ms step_avg:57.33ms
step:1082/2330 train_time:62032ms step_avg:57.33ms
step:1083/2330 train_time:62088ms step_avg:57.33ms
step:1084/2330 train_time:62150ms step_avg:57.33ms
step:1085/2330 train_time:62206ms step_avg:57.33ms
step:1086/2330 train_time:62266ms step_avg:57.33ms
step:1087/2330 train_time:62322ms step_avg:57.33ms
step:1088/2330 train_time:62382ms step_avg:57.34ms
step:1089/2330 train_time:62438ms step_avg:57.34ms
step:1090/2330 train_time:62498ms step_avg:57.34ms
step:1091/2330 train_time:62553ms step_avg:57.34ms
step:1092/2330 train_time:62613ms step_avg:57.34ms
step:1093/2330 train_time:62669ms step_avg:57.34ms
step:1094/2330 train_time:62729ms step_avg:57.34ms
step:1095/2330 train_time:62786ms step_avg:57.34ms
step:1096/2330 train_time:62846ms step_avg:57.34ms
step:1097/2330 train_time:62903ms step_avg:57.34ms
step:1098/2330 train_time:62964ms step_avg:57.34ms
step:1099/2330 train_time:63020ms step_avg:57.34ms
step:1100/2330 train_time:63080ms step_avg:57.35ms
step:1101/2330 train_time:63136ms step_avg:57.34ms
step:1102/2330 train_time:63195ms step_avg:57.35ms
step:1103/2330 train_time:63250ms step_avg:57.34ms
step:1104/2330 train_time:63312ms step_avg:57.35ms
step:1105/2330 train_time:63368ms step_avg:57.35ms
step:1106/2330 train_time:63427ms step_avg:57.35ms
step:1107/2330 train_time:63483ms step_avg:57.35ms
step:1108/2330 train_time:63543ms step_avg:57.35ms
step:1109/2330 train_time:63600ms step_avg:57.35ms
step:1110/2330 train_time:63659ms step_avg:57.35ms
step:1111/2330 train_time:63715ms step_avg:57.35ms
step:1112/2330 train_time:63774ms step_avg:57.35ms
step:1113/2330 train_time:63829ms step_avg:57.35ms
step:1114/2330 train_time:63890ms step_avg:57.35ms
step:1115/2330 train_time:63946ms step_avg:57.35ms
step:1116/2330 train_time:64007ms step_avg:57.35ms
step:1117/2330 train_time:64064ms step_avg:57.35ms
step:1118/2330 train_time:64123ms step_avg:57.36ms
step:1119/2330 train_time:64179ms step_avg:57.35ms
step:1120/2330 train_time:64238ms step_avg:57.36ms
step:1121/2330 train_time:64295ms step_avg:57.36ms
step:1122/2330 train_time:64354ms step_avg:57.36ms
step:1123/2330 train_time:64409ms step_avg:57.35ms
step:1124/2330 train_time:64470ms step_avg:57.36ms
step:1125/2330 train_time:64527ms step_avg:57.36ms
step:1126/2330 train_time:64587ms step_avg:57.36ms
step:1127/2330 train_time:64644ms step_avg:57.36ms
step:1128/2330 train_time:64704ms step_avg:57.36ms
step:1129/2330 train_time:64760ms step_avg:57.36ms
step:1130/2330 train_time:64820ms step_avg:57.36ms
step:1131/2330 train_time:64876ms step_avg:57.36ms
step:1132/2330 train_time:64935ms step_avg:57.36ms
step:1133/2330 train_time:64991ms step_avg:57.36ms
step:1134/2330 train_time:65050ms step_avg:57.36ms
step:1135/2330 train_time:65106ms step_avg:57.36ms
step:1136/2330 train_time:65166ms step_avg:57.36ms
step:1137/2330 train_time:65223ms step_avg:57.36ms
step:1138/2330 train_time:65282ms step_avg:57.37ms
step:1139/2330 train_time:65338ms step_avg:57.36ms
step:1140/2330 train_time:65397ms step_avg:57.37ms
step:1141/2330 train_time:65453ms step_avg:57.36ms
step:1142/2330 train_time:65512ms step_avg:57.37ms
step:1143/2330 train_time:65568ms step_avg:57.36ms
step:1144/2330 train_time:65628ms step_avg:57.37ms
step:1145/2330 train_time:65685ms step_avg:57.37ms
step:1146/2330 train_time:65744ms step_avg:57.37ms
step:1147/2330 train_time:65801ms step_avg:57.37ms
step:1148/2330 train_time:65861ms step_avg:57.37ms
step:1149/2330 train_time:65918ms step_avg:57.37ms
step:1150/2330 train_time:65977ms step_avg:57.37ms
step:1151/2330 train_time:66033ms step_avg:57.37ms
step:1152/2330 train_time:66092ms step_avg:57.37ms
step:1153/2330 train_time:66148ms step_avg:57.37ms
step:1154/2330 train_time:66208ms step_avg:57.37ms
step:1155/2330 train_time:66265ms step_avg:57.37ms
step:1156/2330 train_time:66324ms step_avg:57.37ms
step:1157/2330 train_time:66380ms step_avg:57.37ms
step:1158/2330 train_time:66439ms step_avg:57.37ms
step:1159/2330 train_time:66496ms step_avg:57.37ms
step:1160/2330 train_time:66555ms step_avg:57.37ms
step:1161/2330 train_time:66611ms step_avg:57.37ms
step:1162/2330 train_time:66670ms step_avg:57.38ms
step:1163/2330 train_time:66726ms step_avg:57.37ms
step:1164/2330 train_time:66786ms step_avg:57.38ms
step:1165/2330 train_time:66843ms step_avg:57.38ms
step:1166/2330 train_time:66903ms step_avg:57.38ms
step:1167/2330 train_time:66960ms step_avg:57.38ms
step:1168/2330 train_time:67020ms step_avg:57.38ms
step:1169/2330 train_time:67077ms step_avg:57.38ms
step:1170/2330 train_time:67135ms step_avg:57.38ms
step:1171/2330 train_time:67192ms step_avg:57.38ms
step:1172/2330 train_time:67251ms step_avg:57.38ms
step:1173/2330 train_time:67308ms step_avg:57.38ms
step:1174/2330 train_time:67368ms step_avg:57.38ms
step:1175/2330 train_time:67424ms step_avg:57.38ms
step:1176/2330 train_time:67483ms step_avg:57.38ms
step:1177/2330 train_time:67540ms step_avg:57.38ms
step:1178/2330 train_time:67598ms step_avg:57.38ms
step:1179/2330 train_time:67654ms step_avg:57.38ms
step:1180/2330 train_time:67714ms step_avg:57.38ms
step:1181/2330 train_time:67770ms step_avg:57.38ms
step:1182/2330 train_time:67830ms step_avg:57.39ms
step:1183/2330 train_time:67887ms step_avg:57.39ms
step:1184/2330 train_time:67947ms step_avg:57.39ms
step:1185/2330 train_time:68003ms step_avg:57.39ms
step:1186/2330 train_time:68063ms step_avg:57.39ms
step:1187/2330 train_time:68119ms step_avg:57.39ms
step:1188/2330 train_time:68179ms step_avg:57.39ms
step:1189/2330 train_time:68235ms step_avg:57.39ms
step:1190/2330 train_time:68295ms step_avg:57.39ms
step:1191/2330 train_time:68351ms step_avg:57.39ms
step:1192/2330 train_time:68411ms step_avg:57.39ms
step:1193/2330 train_time:68467ms step_avg:57.39ms
step:1194/2330 train_time:68527ms step_avg:57.39ms
step:1195/2330 train_time:68583ms step_avg:57.39ms
step:1196/2330 train_time:68642ms step_avg:57.39ms
step:1197/2330 train_time:68699ms step_avg:57.39ms
step:1198/2330 train_time:68758ms step_avg:57.39ms
step:1199/2330 train_time:68815ms step_avg:57.39ms
step:1200/2330 train_time:68873ms step_avg:57.39ms
step:1201/2330 train_time:68929ms step_avg:57.39ms
step:1202/2330 train_time:68989ms step_avg:57.40ms
step:1203/2330 train_time:69046ms step_avg:57.39ms
step:1204/2330 train_time:69107ms step_avg:57.40ms
step:1205/2330 train_time:69163ms step_avg:57.40ms
step:1206/2330 train_time:69223ms step_avg:57.40ms
step:1207/2330 train_time:69279ms step_avg:57.40ms
step:1208/2330 train_time:69338ms step_avg:57.40ms
step:1209/2330 train_time:69394ms step_avg:57.40ms
step:1210/2330 train_time:69453ms step_avg:57.40ms
step:1211/2330 train_time:69509ms step_avg:57.40ms
step:1212/2330 train_time:69568ms step_avg:57.40ms
step:1213/2330 train_time:69624ms step_avg:57.40ms
step:1214/2330 train_time:69684ms step_avg:57.40ms
step:1215/2330 train_time:69740ms step_avg:57.40ms
step:1216/2330 train_time:69800ms step_avg:57.40ms
step:1217/2330 train_time:69857ms step_avg:57.40ms
step:1218/2330 train_time:69916ms step_avg:57.40ms
step:1219/2330 train_time:69973ms step_avg:57.40ms
step:1220/2330 train_time:70032ms step_avg:57.40ms
step:1221/2330 train_time:70088ms step_avg:57.40ms
step:1222/2330 train_time:70148ms step_avg:57.40ms
step:1223/2330 train_time:70205ms step_avg:57.40ms
step:1224/2330 train_time:70265ms step_avg:57.41ms
step:1225/2330 train_time:70322ms step_avg:57.41ms
step:1226/2330 train_time:70382ms step_avg:57.41ms
step:1227/2330 train_time:70438ms step_avg:57.41ms
step:1228/2330 train_time:70497ms step_avg:57.41ms
step:1229/2330 train_time:70553ms step_avg:57.41ms
step:1230/2330 train_time:70612ms step_avg:57.41ms
step:1231/2330 train_time:70668ms step_avg:57.41ms
step:1232/2330 train_time:70729ms step_avg:57.41ms
step:1233/2330 train_time:70785ms step_avg:57.41ms
step:1234/2330 train_time:70845ms step_avg:57.41ms
step:1235/2330 train_time:70902ms step_avg:57.41ms
step:1236/2330 train_time:70961ms step_avg:57.41ms
step:1237/2330 train_time:71018ms step_avg:57.41ms
step:1238/2330 train_time:71077ms step_avg:57.41ms
step:1239/2330 train_time:71133ms step_avg:57.41ms
step:1240/2330 train_time:71192ms step_avg:57.41ms
step:1241/2330 train_time:71248ms step_avg:57.41ms
step:1242/2330 train_time:71309ms step_avg:57.41ms
step:1243/2330 train_time:71367ms step_avg:57.41ms
step:1244/2330 train_time:71426ms step_avg:57.42ms
step:1245/2330 train_time:71482ms step_avg:57.42ms
step:1246/2330 train_time:71541ms step_avg:57.42ms
step:1247/2330 train_time:71598ms step_avg:57.42ms
step:1248/2330 train_time:71657ms step_avg:57.42ms
step:1249/2330 train_time:71713ms step_avg:57.42ms
step:1250/2330 train_time:71772ms step_avg:57.42ms
step:1250/2330 val_loss:4.3329 train_time:71852ms step_avg:57.48ms
step:1251/2330 train_time:71871ms step_avg:57.45ms
step:1252/2330 train_time:71891ms step_avg:57.42ms
step:1253/2330 train_time:71946ms step_avg:57.42ms
step:1254/2330 train_time:72013ms step_avg:57.43ms
step:1255/2330 train_time:72068ms step_avg:57.43ms
step:1256/2330 train_time:72129ms step_avg:57.43ms
step:1257/2330 train_time:72185ms step_avg:57.43ms
step:1258/2330 train_time:72245ms step_avg:57.43ms
step:1259/2330 train_time:72301ms step_avg:57.43ms
step:1260/2330 train_time:72360ms step_avg:57.43ms
step:1261/2330 train_time:72415ms step_avg:57.43ms
step:1262/2330 train_time:72475ms step_avg:57.43ms
step:1263/2330 train_time:72530ms step_avg:57.43ms
step:1264/2330 train_time:72589ms step_avg:57.43ms
step:1265/2330 train_time:72645ms step_avg:57.43ms
step:1266/2330 train_time:72704ms step_avg:57.43ms
step:1267/2330 train_time:72760ms step_avg:57.43ms
step:1268/2330 train_time:72821ms step_avg:57.43ms
step:1269/2330 train_time:72879ms step_avg:57.43ms
step:1270/2330 train_time:72940ms step_avg:57.43ms
step:1271/2330 train_time:72998ms step_avg:57.43ms
step:1272/2330 train_time:73058ms step_avg:57.44ms
step:1273/2330 train_time:73114ms step_avg:57.43ms
step:1274/2330 train_time:73173ms step_avg:57.44ms
step:1275/2330 train_time:73230ms step_avg:57.43ms
step:1276/2330 train_time:73288ms step_avg:57.44ms
step:1277/2330 train_time:73344ms step_avg:57.43ms
step:1278/2330 train_time:73405ms step_avg:57.44ms
step:1279/2330 train_time:73460ms step_avg:57.44ms
step:1280/2330 train_time:73520ms step_avg:57.44ms
step:1281/2330 train_time:73577ms step_avg:57.44ms
step:1282/2330 train_time:73636ms step_avg:57.44ms
step:1283/2330 train_time:73693ms step_avg:57.44ms
step:1284/2330 train_time:73751ms step_avg:57.44ms
step:1285/2330 train_time:73808ms step_avg:57.44ms
step:1286/2330 train_time:73868ms step_avg:57.44ms
step:1287/2330 train_time:73925ms step_avg:57.44ms
step:1288/2330 train_time:73985ms step_avg:57.44ms
step:1289/2330 train_time:74042ms step_avg:57.44ms
step:1290/2330 train_time:74102ms step_avg:57.44ms
step:1291/2330 train_time:74159ms step_avg:57.44ms
step:1292/2330 train_time:74219ms step_avg:57.45ms
step:1293/2330 train_time:74276ms step_avg:57.44ms
step:1294/2330 train_time:74336ms step_avg:57.45ms
step:1295/2330 train_time:74391ms step_avg:57.44ms
step:1296/2330 train_time:74450ms step_avg:57.45ms
step:1297/2330 train_time:74506ms step_avg:57.44ms
step:1298/2330 train_time:74566ms step_avg:57.45ms
step:1299/2330 train_time:74621ms step_avg:57.45ms
step:1300/2330 train_time:74681ms step_avg:57.45ms
step:1301/2330 train_time:74738ms step_avg:57.45ms
step:1302/2330 train_time:74797ms step_avg:57.45ms
step:1303/2330 train_time:74853ms step_avg:57.45ms
step:1304/2330 train_time:74913ms step_avg:57.45ms
step:1305/2330 train_time:74969ms step_avg:57.45ms
step:1306/2330 train_time:75029ms step_avg:57.45ms
step:1307/2330 train_time:75086ms step_avg:57.45ms
step:1308/2330 train_time:75146ms step_avg:57.45ms
step:1309/2330 train_time:75202ms step_avg:57.45ms
step:1310/2330 train_time:75263ms step_avg:57.45ms
step:1311/2330 train_time:75320ms step_avg:57.45ms
step:1312/2330 train_time:75379ms step_avg:57.45ms
step:1313/2330 train_time:75435ms step_avg:57.45ms
step:1314/2330 train_time:75495ms step_avg:57.45ms
step:1315/2330 train_time:75550ms step_avg:57.45ms
step:1316/2330 train_time:75610ms step_avg:57.45ms
step:1317/2330 train_time:75665ms step_avg:57.45ms
step:1318/2330 train_time:75726ms step_avg:57.46ms
step:1319/2330 train_time:75782ms step_avg:57.45ms
step:1320/2330 train_time:75842ms step_avg:57.46ms
step:1321/2330 train_time:75899ms step_avg:57.46ms
step:1322/2330 train_time:75959ms step_avg:57.46ms
step:1323/2330 train_time:76016ms step_avg:57.46ms
step:1324/2330 train_time:76075ms step_avg:57.46ms
step:1325/2330 train_time:76131ms step_avg:57.46ms
step:1326/2330 train_time:76190ms step_avg:57.46ms
step:1327/2330 train_time:76246ms step_avg:57.46ms
step:1328/2330 train_time:76307ms step_avg:57.46ms
step:1329/2330 train_time:76363ms step_avg:57.46ms
step:1330/2330 train_time:76424ms step_avg:57.46ms
step:1331/2330 train_time:76481ms step_avg:57.46ms
step:1332/2330 train_time:76540ms step_avg:57.46ms
step:1333/2330 train_time:76596ms step_avg:57.46ms
step:1334/2330 train_time:76655ms step_avg:57.46ms
step:1335/2330 train_time:76712ms step_avg:57.46ms
step:1336/2330 train_time:76770ms step_avg:57.46ms
step:1337/2330 train_time:76826ms step_avg:57.46ms
step:1338/2330 train_time:76886ms step_avg:57.46ms
step:1339/2330 train_time:76943ms step_avg:57.46ms
step:1340/2330 train_time:77003ms step_avg:57.46ms
step:1341/2330 train_time:77059ms step_avg:57.46ms
step:1342/2330 train_time:77119ms step_avg:57.47ms
step:1343/2330 train_time:77176ms step_avg:57.47ms
step:1344/2330 train_time:77236ms step_avg:57.47ms
step:1345/2330 train_time:77291ms step_avg:57.47ms
step:1346/2330 train_time:77350ms step_avg:57.47ms
step:1347/2330 train_time:77406ms step_avg:57.47ms
step:1348/2330 train_time:77467ms step_avg:57.47ms
step:1349/2330 train_time:77524ms step_avg:57.47ms
step:1350/2330 train_time:77583ms step_avg:57.47ms
step:1351/2330 train_time:77639ms step_avg:57.47ms
step:1352/2330 train_time:77699ms step_avg:57.47ms
step:1353/2330 train_time:77755ms step_avg:57.47ms
step:1354/2330 train_time:77815ms step_avg:57.47ms
step:1355/2330 train_time:77870ms step_avg:57.47ms
step:1356/2330 train_time:77929ms step_avg:57.47ms
step:1357/2330 train_time:77985ms step_avg:57.47ms
step:1358/2330 train_time:78045ms step_avg:57.47ms
step:1359/2330 train_time:78102ms step_avg:57.47ms
step:1360/2330 train_time:78162ms step_avg:57.47ms
step:1361/2330 train_time:78219ms step_avg:57.47ms
step:1362/2330 train_time:78279ms step_avg:57.47ms
step:1363/2330 train_time:78336ms step_avg:57.47ms
step:1364/2330 train_time:78395ms step_avg:57.47ms
step:1365/2330 train_time:78451ms step_avg:57.47ms
step:1366/2330 train_time:78510ms step_avg:57.47ms
step:1367/2330 train_time:78566ms step_avg:57.47ms
step:1368/2330 train_time:78625ms step_avg:57.47ms
step:1369/2330 train_time:78681ms step_avg:57.47ms
step:1370/2330 train_time:78742ms step_avg:57.48ms
step:1371/2330 train_time:78799ms step_avg:57.48ms
step:1372/2330 train_time:78859ms step_avg:57.48ms
step:1373/2330 train_time:78915ms step_avg:57.48ms
step:1374/2330 train_time:78975ms step_avg:57.48ms
step:1375/2330 train_time:79031ms step_avg:57.48ms
step:1376/2330 train_time:79090ms step_avg:57.48ms
step:1377/2330 train_time:79146ms step_avg:57.48ms
step:1378/2330 train_time:79205ms step_avg:57.48ms
step:1379/2330 train_time:79261ms step_avg:57.48ms
step:1380/2330 train_time:79322ms step_avg:57.48ms
step:1381/2330 train_time:79379ms step_avg:57.48ms
step:1382/2330 train_time:79439ms step_avg:57.48ms
step:1383/2330 train_time:79496ms step_avg:57.48ms
step:1384/2330 train_time:79555ms step_avg:57.48ms
step:1385/2330 train_time:79611ms step_avg:57.48ms
step:1386/2330 train_time:79670ms step_avg:57.48ms
step:1387/2330 train_time:79726ms step_avg:57.48ms
step:1388/2330 train_time:79786ms step_avg:57.48ms
step:1389/2330 train_time:79842ms step_avg:57.48ms
step:1390/2330 train_time:79902ms step_avg:57.48ms
step:1391/2330 train_time:79958ms step_avg:57.48ms
step:1392/2330 train_time:80019ms step_avg:57.48ms
step:1393/2330 train_time:80075ms step_avg:57.48ms
step:1394/2330 train_time:80135ms step_avg:57.49ms
step:1395/2330 train_time:80191ms step_avg:57.48ms
step:1396/2330 train_time:80250ms step_avg:57.49ms
step:1397/2330 train_time:80306ms step_avg:57.48ms
step:1398/2330 train_time:80368ms step_avg:57.49ms
step:1399/2330 train_time:80424ms step_avg:57.49ms
step:1400/2330 train_time:80484ms step_avg:57.49ms
step:1401/2330 train_time:80540ms step_avg:57.49ms
step:1402/2330 train_time:80600ms step_avg:57.49ms
step:1403/2330 train_time:80656ms step_avg:57.49ms
step:1404/2330 train_time:80717ms step_avg:57.49ms
step:1405/2330 train_time:80773ms step_avg:57.49ms
step:1406/2330 train_time:80833ms step_avg:57.49ms
step:1407/2330 train_time:80889ms step_avg:57.49ms
step:1408/2330 train_time:80948ms step_avg:57.49ms
step:1409/2330 train_time:81004ms step_avg:57.49ms
step:1410/2330 train_time:81065ms step_avg:57.49ms
step:1411/2330 train_time:81122ms step_avg:57.49ms
step:1412/2330 train_time:81181ms step_avg:57.49ms
step:1413/2330 train_time:81238ms step_avg:57.49ms
step:1414/2330 train_time:81298ms step_avg:57.49ms
step:1415/2330 train_time:81354ms step_avg:57.49ms
step:1416/2330 train_time:81414ms step_avg:57.50ms
step:1417/2330 train_time:81470ms step_avg:57.49ms
step:1418/2330 train_time:81529ms step_avg:57.50ms
step:1419/2330 train_time:81585ms step_avg:57.49ms
step:1420/2330 train_time:81645ms step_avg:57.50ms
step:1421/2330 train_time:81702ms step_avg:57.50ms
step:1422/2330 train_time:81762ms step_avg:57.50ms
step:1423/2330 train_time:81819ms step_avg:57.50ms
step:1424/2330 train_time:81878ms step_avg:57.50ms
step:1425/2330 train_time:81934ms step_avg:57.50ms
step:1426/2330 train_time:81993ms step_avg:57.50ms
step:1427/2330 train_time:82049ms step_avg:57.50ms
step:1428/2330 train_time:82108ms step_avg:57.50ms
step:1429/2330 train_time:82164ms step_avg:57.50ms
step:1430/2330 train_time:82225ms step_avg:57.50ms
step:1431/2330 train_time:82281ms step_avg:57.50ms
step:1432/2330 train_time:82341ms step_avg:57.50ms
step:1433/2330 train_time:82398ms step_avg:57.50ms
step:1434/2330 train_time:82457ms step_avg:57.50ms
step:1435/2330 train_time:82514ms step_avg:57.50ms
step:1436/2330 train_time:82573ms step_avg:57.50ms
step:1437/2330 train_time:82630ms step_avg:57.50ms
step:1438/2330 train_time:82689ms step_avg:57.50ms
step:1439/2330 train_time:82745ms step_avg:57.50ms
step:1440/2330 train_time:82805ms step_avg:57.50ms
step:1441/2330 train_time:82861ms step_avg:57.50ms
step:1442/2330 train_time:82921ms step_avg:57.50ms
step:1443/2330 train_time:82977ms step_avg:57.50ms
step:1444/2330 train_time:83036ms step_avg:57.50ms
step:1445/2330 train_time:83093ms step_avg:57.50ms
step:1446/2330 train_time:83151ms step_avg:57.50ms
step:1447/2330 train_time:83207ms step_avg:57.50ms
step:1448/2330 train_time:83268ms step_avg:57.51ms
step:1449/2330 train_time:83324ms step_avg:57.50ms
step:1450/2330 train_time:83385ms step_avg:57.51ms
step:1451/2330 train_time:83442ms step_avg:57.51ms
step:1452/2330 train_time:83502ms step_avg:57.51ms
step:1453/2330 train_time:83559ms step_avg:57.51ms
step:1454/2330 train_time:83618ms step_avg:57.51ms
step:1455/2330 train_time:83675ms step_avg:57.51ms
step:1456/2330 train_time:83734ms step_avg:57.51ms
step:1457/2330 train_time:83790ms step_avg:57.51ms
step:1458/2330 train_time:83849ms step_avg:57.51ms
step:1459/2330 train_time:83906ms step_avg:57.51ms
step:1460/2330 train_time:83965ms step_avg:57.51ms
step:1461/2330 train_time:84022ms step_avg:57.51ms
step:1462/2330 train_time:84081ms step_avg:57.51ms
step:1463/2330 train_time:84138ms step_avg:57.51ms
step:1464/2330 train_time:84197ms step_avg:57.51ms
step:1465/2330 train_time:84253ms step_avg:57.51ms
step:1466/2330 train_time:84313ms step_avg:57.51ms
step:1467/2330 train_time:84369ms step_avg:57.51ms
step:1468/2330 train_time:84429ms step_avg:57.51ms
step:1469/2330 train_time:84485ms step_avg:57.51ms
step:1470/2330 train_time:84545ms step_avg:57.51ms
step:1471/2330 train_time:84602ms step_avg:57.51ms
step:1472/2330 train_time:84662ms step_avg:57.52ms
step:1473/2330 train_time:84719ms step_avg:57.51ms
step:1474/2330 train_time:84779ms step_avg:57.52ms
step:1475/2330 train_time:84835ms step_avg:57.52ms
step:1476/2330 train_time:84895ms step_avg:57.52ms
step:1477/2330 train_time:84951ms step_avg:57.52ms
step:1478/2330 train_time:85010ms step_avg:57.52ms
step:1479/2330 train_time:85066ms step_avg:57.52ms
step:1480/2330 train_time:85126ms step_avg:57.52ms
step:1481/2330 train_time:85183ms step_avg:57.52ms
step:1482/2330 train_time:85242ms step_avg:57.52ms
step:1483/2330 train_time:85299ms step_avg:57.52ms
step:1484/2330 train_time:85358ms step_avg:57.52ms
step:1485/2330 train_time:85415ms step_avg:57.52ms
step:1486/2330 train_time:85474ms step_avg:57.52ms
step:1487/2330 train_time:85530ms step_avg:57.52ms
step:1488/2330 train_time:85590ms step_avg:57.52ms
step:1489/2330 train_time:85646ms step_avg:57.52ms
step:1490/2330 train_time:85708ms step_avg:57.52ms
step:1491/2330 train_time:85764ms step_avg:57.52ms
step:1492/2330 train_time:85824ms step_avg:57.52ms
step:1493/2330 train_time:85881ms step_avg:57.52ms
step:1494/2330 train_time:85940ms step_avg:57.52ms
step:1495/2330 train_time:85997ms step_avg:57.52ms
step:1496/2330 train_time:86056ms step_avg:57.52ms
step:1497/2330 train_time:86112ms step_avg:57.52ms
step:1498/2330 train_time:86171ms step_avg:57.52ms
step:1499/2330 train_time:86228ms step_avg:57.52ms
step:1500/2330 train_time:86287ms step_avg:57.52ms
step:1500/2330 val_loss:4.2298 train_time:86367ms step_avg:57.58ms
step:1501/2330 train_time:86386ms step_avg:57.55ms
step:1502/2330 train_time:86406ms step_avg:57.53ms
step:1503/2330 train_time:86462ms step_avg:57.53ms
step:1504/2330 train_time:86527ms step_avg:57.53ms
step:1505/2330 train_time:86583ms step_avg:57.53ms
step:1506/2330 train_time:86643ms step_avg:57.53ms
step:1507/2330 train_time:86699ms step_avg:57.53ms
step:1508/2330 train_time:86758ms step_avg:57.53ms
step:1509/2330 train_time:86814ms step_avg:57.53ms
step:1510/2330 train_time:86874ms step_avg:57.53ms
step:1511/2330 train_time:86930ms step_avg:57.53ms
step:1512/2330 train_time:86989ms step_avg:57.53ms
step:1513/2330 train_time:87044ms step_avg:57.53ms
step:1514/2330 train_time:87104ms step_avg:57.53ms
step:1515/2330 train_time:87160ms step_avg:57.53ms
step:1516/2330 train_time:87219ms step_avg:57.53ms
step:1517/2330 train_time:87275ms step_avg:57.53ms
step:1518/2330 train_time:87335ms step_avg:57.53ms
step:1519/2330 train_time:87391ms step_avg:57.53ms
step:1520/2330 train_time:87454ms step_avg:57.54ms
step:1521/2330 train_time:87511ms step_avg:57.54ms
step:1522/2330 train_time:87572ms step_avg:57.54ms
step:1523/2330 train_time:87629ms step_avg:57.54ms
step:1524/2330 train_time:87689ms step_avg:57.54ms
step:1525/2330 train_time:87746ms step_avg:57.54ms
step:1526/2330 train_time:87805ms step_avg:57.54ms
step:1527/2330 train_time:87861ms step_avg:57.54ms
step:1528/2330 train_time:87920ms step_avg:57.54ms
step:1529/2330 train_time:87978ms step_avg:57.54ms
step:1530/2330 train_time:88036ms step_avg:57.54ms
step:1531/2330 train_time:88092ms step_avg:57.54ms
step:1532/2330 train_time:88152ms step_avg:57.54ms
step:1533/2330 train_time:88209ms step_avg:57.54ms
step:1534/2330 train_time:88269ms step_avg:57.54ms
step:1535/2330 train_time:88325ms step_avg:57.54ms
step:1536/2330 train_time:88385ms step_avg:57.54ms
step:1537/2330 train_time:88442ms step_avg:57.54ms
step:1538/2330 train_time:88503ms step_avg:57.54ms
step:1539/2330 train_time:88560ms step_avg:57.54ms
step:1540/2330 train_time:88619ms step_avg:57.54ms
step:1541/2330 train_time:88675ms step_avg:57.54ms
step:1542/2330 train_time:88737ms step_avg:57.55ms
step:1543/2330 train_time:88793ms step_avg:57.55ms
step:1544/2330 train_time:88854ms step_avg:57.55ms
step:1545/2330 train_time:88910ms step_avg:57.55ms
step:1546/2330 train_time:88970ms step_avg:57.55ms
step:1547/2330 train_time:89027ms step_avg:57.55ms
step:1548/2330 train_time:89086ms step_avg:57.55ms
step:1549/2330 train_time:89143ms step_avg:57.55ms
step:1550/2330 train_time:89202ms step_avg:57.55ms
step:1551/2330 train_time:89259ms step_avg:57.55ms
step:1552/2330 train_time:89318ms step_avg:57.55ms
step:1553/2330 train_time:89375ms step_avg:57.55ms
step:1554/2330 train_time:89436ms step_avg:57.55ms
step:1555/2330 train_time:89494ms step_avg:57.55ms
step:1556/2330 train_time:89554ms step_avg:57.55ms
step:1557/2330 train_time:89612ms step_avg:57.55ms
step:1558/2330 train_time:89672ms step_avg:57.56ms
step:1559/2330 train_time:89729ms step_avg:57.56ms
step:1560/2330 train_time:89789ms step_avg:57.56ms
step:1561/2330 train_time:89846ms step_avg:57.56ms
step:1562/2330 train_time:89907ms step_avg:57.56ms
step:1563/2330 train_time:89963ms step_avg:57.56ms
step:1564/2330 train_time:90023ms step_avg:57.56ms
step:1565/2330 train_time:90079ms step_avg:57.56ms
step:1566/2330 train_time:90139ms step_avg:57.56ms
step:1567/2330 train_time:90196ms step_avg:57.56ms
step:1568/2330 train_time:90256ms step_avg:57.56ms
step:1569/2330 train_time:90313ms step_avg:57.56ms
step:1570/2330 train_time:90373ms step_avg:57.56ms
step:1571/2330 train_time:90430ms step_avg:57.56ms
step:1572/2330 train_time:90491ms step_avg:57.56ms
step:1573/2330 train_time:90549ms step_avg:57.56ms
step:1574/2330 train_time:90609ms step_avg:57.57ms
step:1575/2330 train_time:90666ms step_avg:57.57ms
step:1576/2330 train_time:90725ms step_avg:57.57ms
step:1577/2330 train_time:90782ms step_avg:57.57ms
step:1578/2330 train_time:90843ms step_avg:57.57ms
step:1579/2330 train_time:90900ms step_avg:57.57ms
step:1580/2330 train_time:90959ms step_avg:57.57ms
step:1581/2330 train_time:91016ms step_avg:57.57ms
step:1582/2330 train_time:91076ms step_avg:57.57ms
step:1583/2330 train_time:91133ms step_avg:57.57ms
step:1584/2330 train_time:91192ms step_avg:57.57ms
step:1585/2330 train_time:91249ms step_avg:57.57ms
step:1586/2330 train_time:91309ms step_avg:57.57ms
step:1587/2330 train_time:91365ms step_avg:57.57ms
step:1588/2330 train_time:91425ms step_avg:57.57ms
step:1589/2330 train_time:91482ms step_avg:57.57ms
step:1590/2330 train_time:91542ms step_avg:57.57ms
step:1591/2330 train_time:91599ms step_avg:57.57ms
step:1592/2330 train_time:91660ms step_avg:57.58ms
step:1593/2330 train_time:91716ms step_avg:57.57ms
step:1594/2330 train_time:91777ms step_avg:57.58ms
step:1595/2330 train_time:91834ms step_avg:57.58ms
step:1596/2330 train_time:91894ms step_avg:57.58ms
step:1597/2330 train_time:91951ms step_avg:57.58ms
step:1598/2330 train_time:92011ms step_avg:57.58ms
step:1599/2330 train_time:92068ms step_avg:57.58ms
step:1600/2330 train_time:92128ms step_avg:57.58ms
step:1601/2330 train_time:92185ms step_avg:57.58ms
step:1602/2330 train_time:92244ms step_avg:57.58ms
step:1603/2330 train_time:92301ms step_avg:57.58ms
step:1604/2330 train_time:92360ms step_avg:57.58ms
step:1605/2330 train_time:92417ms step_avg:57.58ms
step:1606/2330 train_time:92477ms step_avg:57.58ms
step:1607/2330 train_time:92534ms step_avg:57.58ms
step:1608/2330 train_time:92594ms step_avg:57.58ms
step:1609/2330 train_time:92652ms step_avg:57.58ms
step:1610/2330 train_time:92712ms step_avg:57.59ms
step:1611/2330 train_time:92770ms step_avg:57.59ms
step:1612/2330 train_time:92830ms step_avg:57.59ms
step:1613/2330 train_time:92887ms step_avg:57.59ms
step:1614/2330 train_time:92947ms step_avg:57.59ms
step:1615/2330 train_time:93004ms step_avg:57.59ms
step:1616/2330 train_time:93064ms step_avg:57.59ms
step:1617/2330 train_time:93120ms step_avg:57.59ms
step:1618/2330 train_time:93180ms step_avg:57.59ms
step:1619/2330 train_time:93237ms step_avg:57.59ms
step:1620/2330 train_time:93297ms step_avg:57.59ms
step:1621/2330 train_time:93353ms step_avg:57.59ms
step:1622/2330 train_time:93414ms step_avg:57.59ms
step:1623/2330 train_time:93470ms step_avg:57.59ms
step:1624/2330 train_time:93531ms step_avg:57.59ms
step:1625/2330 train_time:93587ms step_avg:57.59ms
step:1626/2330 train_time:93648ms step_avg:57.59ms
step:1627/2330 train_time:93704ms step_avg:57.59ms
step:1628/2330 train_time:93765ms step_avg:57.60ms
step:1629/2330 train_time:93822ms step_avg:57.60ms
step:1630/2330 train_time:93882ms step_avg:57.60ms
step:1631/2330 train_time:93939ms step_avg:57.60ms
step:1632/2330 train_time:93999ms step_avg:57.60ms
step:1633/2330 train_time:94056ms step_avg:57.60ms
step:1634/2330 train_time:94116ms step_avg:57.60ms
step:1635/2330 train_time:94173ms step_avg:57.60ms
step:1636/2330 train_time:94234ms step_avg:57.60ms
step:1637/2330 train_time:94290ms step_avg:57.60ms
step:1638/2330 train_time:94351ms step_avg:57.60ms
step:1639/2330 train_time:94409ms step_avg:57.60ms
step:1640/2330 train_time:94468ms step_avg:57.60ms
step:1641/2330 train_time:94525ms step_avg:57.60ms
step:1642/2330 train_time:94585ms step_avg:57.60ms
step:1643/2330 train_time:94641ms step_avg:57.60ms
step:1644/2330 train_time:94702ms step_avg:57.60ms
step:1645/2330 train_time:94759ms step_avg:57.60ms
step:1646/2330 train_time:94818ms step_avg:57.61ms
step:1647/2330 train_time:94875ms step_avg:57.60ms
step:1648/2330 train_time:94936ms step_avg:57.61ms
step:1649/2330 train_time:94993ms step_avg:57.61ms
step:1650/2330 train_time:95053ms step_avg:57.61ms
step:1651/2330 train_time:95111ms step_avg:57.61ms
step:1652/2330 train_time:95171ms step_avg:57.61ms
step:1653/2330 train_time:95228ms step_avg:57.61ms
step:1654/2330 train_time:95288ms step_avg:57.61ms
step:1655/2330 train_time:95345ms step_avg:57.61ms
step:1656/2330 train_time:95405ms step_avg:57.61ms
step:1657/2330 train_time:95461ms step_avg:57.61ms
step:1658/2330 train_time:95522ms step_avg:57.61ms
step:1659/2330 train_time:95579ms step_avg:57.61ms
step:1660/2330 train_time:95639ms step_avg:57.61ms
step:1661/2330 train_time:95695ms step_avg:57.61ms
step:1662/2330 train_time:95756ms step_avg:57.61ms
step:1663/2330 train_time:95813ms step_avg:57.61ms
step:1664/2330 train_time:95872ms step_avg:57.62ms
step:1665/2330 train_time:95929ms step_avg:57.61ms
step:1666/2330 train_time:95988ms step_avg:57.62ms
step:1667/2330 train_time:96046ms step_avg:57.62ms
step:1668/2330 train_time:96106ms step_avg:57.62ms
step:1669/2330 train_time:96163ms step_avg:57.62ms
step:1670/2330 train_time:96222ms step_avg:57.62ms
step:1671/2330 train_time:96279ms step_avg:57.62ms
step:1672/2330 train_time:96339ms step_avg:57.62ms
step:1673/2330 train_time:96397ms step_avg:57.62ms
step:1674/2330 train_time:96457ms step_avg:57.62ms
step:1675/2330 train_time:96513ms step_avg:57.62ms
step:1676/2330 train_time:96573ms step_avg:57.62ms
step:1677/2330 train_time:96629ms step_avg:57.62ms
step:1678/2330 train_time:96689ms step_avg:57.62ms
step:1679/2330 train_time:96746ms step_avg:57.62ms
step:1680/2330 train_time:96806ms step_avg:57.62ms
step:1681/2330 train_time:96863ms step_avg:57.62ms
step:1682/2330 train_time:96922ms step_avg:57.62ms
step:1683/2330 train_time:96979ms step_avg:57.62ms
step:1684/2330 train_time:97039ms step_avg:57.62ms
step:1685/2330 train_time:97095ms step_avg:57.62ms
step:1686/2330 train_time:97155ms step_avg:57.62ms
step:1687/2330 train_time:97212ms step_avg:57.62ms
step:1688/2330 train_time:97273ms step_avg:57.63ms
step:1689/2330 train_time:97330ms step_avg:57.63ms
step:1690/2330 train_time:97390ms step_avg:57.63ms
step:1691/2330 train_time:97447ms step_avg:57.63ms
step:1692/2330 train_time:97508ms step_avg:57.63ms
step:1693/2330 train_time:97564ms step_avg:57.63ms
step:1694/2330 train_time:97625ms step_avg:57.63ms
step:1695/2330 train_time:97682ms step_avg:57.63ms
step:1696/2330 train_time:97741ms step_avg:57.63ms
step:1697/2330 train_time:97798ms step_avg:57.63ms
step:1698/2330 train_time:97858ms step_avg:57.63ms
step:1699/2330 train_time:97915ms step_avg:57.63ms
step:1700/2330 train_time:97974ms step_avg:57.63ms
step:1701/2330 train_time:98030ms step_avg:57.63ms
step:1702/2330 train_time:98092ms step_avg:57.63ms
step:1703/2330 train_time:98150ms step_avg:57.63ms
step:1704/2330 train_time:98209ms step_avg:57.63ms
step:1705/2330 train_time:98267ms step_avg:57.63ms
step:1706/2330 train_time:98326ms step_avg:57.64ms
step:1707/2330 train_time:98383ms step_avg:57.64ms
step:1708/2330 train_time:98443ms step_avg:57.64ms
step:1709/2330 train_time:98500ms step_avg:57.64ms
step:1710/2330 train_time:98559ms step_avg:57.64ms
step:1711/2330 train_time:98616ms step_avg:57.64ms
step:1712/2330 train_time:98677ms step_avg:57.64ms
step:1713/2330 train_time:98734ms step_avg:57.64ms
step:1714/2330 train_time:98794ms step_avg:57.64ms
step:1715/2330 train_time:98851ms step_avg:57.64ms
step:1716/2330 train_time:98912ms step_avg:57.64ms
step:1717/2330 train_time:98968ms step_avg:57.64ms
step:1718/2330 train_time:99028ms step_avg:57.64ms
step:1719/2330 train_time:99085ms step_avg:57.64ms
step:1720/2330 train_time:99145ms step_avg:57.64ms
step:1721/2330 train_time:99202ms step_avg:57.64ms
step:1722/2330 train_time:99261ms step_avg:57.64ms
step:1723/2330 train_time:99318ms step_avg:57.64ms
step:1724/2330 train_time:99378ms step_avg:57.64ms
step:1725/2330 train_time:99435ms step_avg:57.64ms
step:1726/2330 train_time:99496ms step_avg:57.65ms
step:1727/2330 train_time:99553ms step_avg:57.65ms
step:1728/2330 train_time:99613ms step_avg:57.65ms
step:1729/2330 train_time:99670ms step_avg:57.65ms
step:1730/2330 train_time:99730ms step_avg:57.65ms
step:1731/2330 train_time:99787ms step_avg:57.65ms
step:1732/2330 train_time:99847ms step_avg:57.65ms
step:1733/2330 train_time:99903ms step_avg:57.65ms
step:1734/2330 train_time:99964ms step_avg:57.65ms
step:1735/2330 train_time:100020ms step_avg:57.65ms
step:1736/2330 train_time:100081ms step_avg:57.65ms
step:1737/2330 train_time:100137ms step_avg:57.65ms
step:1738/2330 train_time:100197ms step_avg:57.65ms
step:1739/2330 train_time:100254ms step_avg:57.65ms
step:1740/2330 train_time:100315ms step_avg:57.65ms
step:1741/2330 train_time:100372ms step_avg:57.65ms
step:1742/2330 train_time:100431ms step_avg:57.65ms
step:1743/2330 train_time:100488ms step_avg:57.65ms
step:1744/2330 train_time:100549ms step_avg:57.65ms
step:1745/2330 train_time:100606ms step_avg:57.65ms
step:1746/2330 train_time:100666ms step_avg:57.66ms
step:1747/2330 train_time:100723ms step_avg:57.65ms
step:1748/2330 train_time:100782ms step_avg:57.66ms
step:1749/2330 train_time:100839ms step_avg:57.66ms
step:1750/2330 train_time:100898ms step_avg:57.66ms
step:1750/2330 val_loss:4.1373 train_time:100980ms step_avg:57.70ms
step:1751/2330 train_time:101000ms step_avg:57.68ms
step:1752/2330 train_time:101019ms step_avg:57.66ms
step:1753/2330 train_time:101073ms step_avg:57.66ms
step:1754/2330 train_time:101135ms step_avg:57.66ms
step:1755/2330 train_time:101191ms step_avg:57.66ms
step:1756/2330 train_time:101253ms step_avg:57.66ms
step:1757/2330 train_time:101310ms step_avg:57.66ms
step:1758/2330 train_time:101369ms step_avg:57.66ms
step:1759/2330 train_time:101425ms step_avg:57.66ms
step:1760/2330 train_time:101485ms step_avg:57.66ms
step:1761/2330 train_time:101541ms step_avg:57.66ms
step:1762/2330 train_time:101600ms step_avg:57.66ms
step:1763/2330 train_time:101656ms step_avg:57.66ms
step:1764/2330 train_time:101715ms step_avg:57.66ms
step:1765/2330 train_time:101772ms step_avg:57.66ms
step:1766/2330 train_time:101832ms step_avg:57.66ms
step:1767/2330 train_time:101890ms step_avg:57.66ms
step:1768/2330 train_time:101953ms step_avg:57.67ms
step:1769/2330 train_time:102012ms step_avg:57.67ms
step:1770/2330 train_time:102071ms step_avg:57.67ms
step:1771/2330 train_time:102129ms step_avg:57.67ms
step:1772/2330 train_time:102188ms step_avg:57.67ms
step:1773/2330 train_time:102244ms step_avg:57.67ms
step:1774/2330 train_time:102304ms step_avg:57.67ms
step:1775/2330 train_time:102360ms step_avg:57.67ms
step:1776/2330 train_time:102421ms step_avg:57.67ms
step:1777/2330 train_time:102477ms step_avg:57.67ms
step:1778/2330 train_time:102536ms step_avg:57.67ms
step:1779/2330 train_time:102593ms step_avg:57.67ms
step:1780/2330 train_time:102652ms step_avg:57.67ms
step:1781/2330 train_time:102708ms step_avg:57.67ms
step:1782/2330 train_time:102768ms step_avg:57.67ms
step:1783/2330 train_time:102824ms step_avg:57.67ms
step:1784/2330 train_time:102884ms step_avg:57.67ms
step:1785/2330 train_time:102942ms step_avg:57.67ms
step:1786/2330 train_time:103003ms step_avg:57.67ms
step:1787/2330 train_time:103061ms step_avg:57.67ms
step:1788/2330 train_time:103122ms step_avg:57.67ms
step:1789/2330 train_time:103179ms step_avg:57.67ms
step:1790/2330 train_time:103239ms step_avg:57.68ms
step:1791/2330 train_time:103295ms step_avg:57.67ms
step:1792/2330 train_time:103357ms step_avg:57.68ms
step:1793/2330 train_time:103413ms step_avg:57.68ms
step:1794/2330 train_time:103473ms step_avg:57.68ms
step:1795/2330 train_time:103530ms step_avg:57.68ms
step:1796/2330 train_time:103589ms step_avg:57.68ms
step:1797/2330 train_time:103645ms step_avg:57.68ms
step:1798/2330 train_time:103705ms step_avg:57.68ms
step:1799/2330 train_time:103761ms step_avg:57.68ms
step:1800/2330 train_time:103821ms step_avg:57.68ms
step:1801/2330 train_time:103878ms step_avg:57.68ms
step:1802/2330 train_time:103939ms step_avg:57.68ms
step:1803/2330 train_time:103997ms step_avg:57.68ms
step:1804/2330 train_time:104057ms step_avg:57.68ms
step:1805/2330 train_time:104115ms step_avg:57.68ms
step:1806/2330 train_time:104175ms step_avg:57.68ms
step:1807/2330 train_time:104232ms step_avg:57.68ms
step:1808/2330 train_time:104291ms step_avg:57.68ms
step:1809/2330 train_time:104348ms step_avg:57.68ms
step:1810/2330 train_time:104407ms step_avg:57.68ms
step:1811/2330 train_time:104464ms step_avg:57.68ms
step:1812/2330 train_time:104523ms step_avg:57.68ms
step:1813/2330 train_time:104580ms step_avg:57.68ms
step:1814/2330 train_time:104640ms step_avg:57.68ms
step:1815/2330 train_time:104697ms step_avg:57.68ms
step:1816/2330 train_time:104756ms step_avg:57.69ms
step:1817/2330 train_time:104814ms step_avg:57.68ms
step:1818/2330 train_time:104873ms step_avg:57.69ms
step:1819/2330 train_time:104931ms step_avg:57.69ms
step:1820/2330 train_time:104990ms step_avg:57.69ms
step:1821/2330 train_time:105047ms step_avg:57.69ms
step:1822/2330 train_time:105107ms step_avg:57.69ms
step:1823/2330 train_time:105164ms step_avg:57.69ms
step:1824/2330 train_time:105224ms step_avg:57.69ms
step:1825/2330 train_time:105281ms step_avg:57.69ms
step:1826/2330 train_time:105340ms step_avg:57.69ms
step:1827/2330 train_time:105397ms step_avg:57.69ms
step:1828/2330 train_time:105457ms step_avg:57.69ms
step:1829/2330 train_time:105514ms step_avg:57.69ms
step:1830/2330 train_time:105573ms step_avg:57.69ms
step:1831/2330 train_time:105629ms step_avg:57.69ms
step:1832/2330 train_time:105690ms step_avg:57.69ms
step:1833/2330 train_time:105747ms step_avg:57.69ms
step:1834/2330 train_time:105806ms step_avg:57.69ms
step:1835/2330 train_time:105863ms step_avg:57.69ms
step:1836/2330 train_time:105922ms step_avg:57.69ms
step:1837/2330 train_time:105979ms step_avg:57.69ms
step:1838/2330 train_time:106039ms step_avg:57.69ms
step:1839/2330 train_time:106096ms step_avg:57.69ms
step:1840/2330 train_time:106156ms step_avg:57.69ms
step:1841/2330 train_time:106213ms step_avg:57.69ms
step:1842/2330 train_time:106274ms step_avg:57.69ms
step:1843/2330 train_time:106331ms step_avg:57.69ms
step:1844/2330 train_time:106391ms step_avg:57.70ms
step:1845/2330 train_time:106449ms step_avg:57.70ms
step:1846/2330 train_time:106508ms step_avg:57.70ms
step:1847/2330 train_time:106565ms step_avg:57.70ms
step:1848/2330 train_time:106624ms step_avg:57.70ms
step:1849/2330 train_time:106681ms step_avg:57.70ms
step:1850/2330 train_time:106741ms step_avg:57.70ms
step:1851/2330 train_time:106797ms step_avg:57.70ms
step:1852/2330 train_time:106857ms step_avg:57.70ms
step:1853/2330 train_time:106914ms step_avg:57.70ms
step:1854/2330 train_time:106974ms step_avg:57.70ms
step:1855/2330 train_time:107031ms step_avg:57.70ms
step:1856/2330 train_time:107091ms step_avg:57.70ms
step:1857/2330 train_time:107148ms step_avg:57.70ms
step:1858/2330 train_time:107207ms step_avg:57.70ms
step:1859/2330 train_time:107263ms step_avg:57.70ms
step:1860/2330 train_time:107323ms step_avg:57.70ms
step:1861/2330 train_time:107380ms step_avg:57.70ms
step:1862/2330 train_time:107440ms step_avg:57.70ms
step:1863/2330 train_time:107497ms step_avg:57.70ms
step:1864/2330 train_time:107557ms step_avg:57.70ms
step:1865/2330 train_time:107613ms step_avg:57.70ms
step:1866/2330 train_time:107673ms step_avg:57.70ms
step:1867/2330 train_time:107730ms step_avg:57.70ms
step:1868/2330 train_time:107790ms step_avg:57.70ms
step:1869/2330 train_time:107847ms step_avg:57.70ms
step:1870/2330 train_time:107906ms step_avg:57.70ms
step:1871/2330 train_time:107962ms step_avg:57.70ms
step:1872/2330 train_time:108022ms step_avg:57.70ms
step:1873/2330 train_time:108078ms step_avg:57.70ms
step:1874/2330 train_time:108139ms step_avg:57.70ms
step:1875/2330 train_time:108196ms step_avg:57.70ms
step:1876/2330 train_time:108255ms step_avg:57.71ms
step:1877/2330 train_time:108312ms step_avg:57.70ms
step:1878/2330 train_time:108372ms step_avg:57.71ms
step:1879/2330 train_time:108429ms step_avg:57.71ms
step:1880/2330 train_time:108489ms step_avg:57.71ms
step:1881/2330 train_time:108545ms step_avg:57.71ms
step:1882/2330 train_time:108605ms step_avg:57.71ms
step:1883/2330 train_time:108662ms step_avg:57.71ms
step:1884/2330 train_time:108722ms step_avg:57.71ms
step:1885/2330 train_time:108779ms step_avg:57.71ms
step:1886/2330 train_time:108839ms step_avg:57.71ms
step:1887/2330 train_time:108896ms step_avg:57.71ms
step:1888/2330 train_time:108956ms step_avg:57.71ms
step:1889/2330 train_time:109013ms step_avg:57.71ms
step:1890/2330 train_time:109073ms step_avg:57.71ms
step:1891/2330 train_time:109131ms step_avg:57.71ms
step:1892/2330 train_time:109190ms step_avg:57.71ms
step:1893/2330 train_time:109247ms step_avg:57.71ms
step:1894/2330 train_time:109307ms step_avg:57.71ms
step:1895/2330 train_time:109363ms step_avg:57.71ms
step:1896/2330 train_time:109422ms step_avg:57.71ms
step:1897/2330 train_time:109479ms step_avg:57.71ms
step:1898/2330 train_time:109539ms step_avg:57.71ms
step:1899/2330 train_time:109596ms step_avg:57.71ms
step:1900/2330 train_time:109657ms step_avg:57.71ms
step:1901/2330 train_time:109714ms step_avg:57.71ms
step:1902/2330 train_time:109775ms step_avg:57.72ms
step:1903/2330 train_time:109831ms step_avg:57.71ms
step:1904/2330 train_time:109890ms step_avg:57.72ms
step:1905/2330 train_time:109947ms step_avg:57.72ms
step:1906/2330 train_time:110007ms step_avg:57.72ms
step:1907/2330 train_time:110063ms step_avg:57.72ms
step:1908/2330 train_time:110124ms step_avg:57.72ms
step:1909/2330 train_time:110180ms step_avg:57.72ms
step:1910/2330 train_time:110241ms step_avg:57.72ms
step:1911/2330 train_time:110297ms step_avg:57.72ms
step:1912/2330 train_time:110358ms step_avg:57.72ms
step:1913/2330 train_time:110415ms step_avg:57.72ms
step:1914/2330 train_time:110476ms step_avg:57.72ms
step:1915/2330 train_time:110533ms step_avg:57.72ms
step:1916/2330 train_time:110593ms step_avg:57.72ms
step:1917/2330 train_time:110649ms step_avg:57.72ms
step:1918/2330 train_time:110709ms step_avg:57.72ms
step:1919/2330 train_time:110766ms step_avg:57.72ms
step:1920/2330 train_time:110826ms step_avg:57.72ms
step:1921/2330 train_time:110883ms step_avg:57.72ms
step:1922/2330 train_time:110943ms step_avg:57.72ms
step:1923/2330 train_time:110999ms step_avg:57.72ms
step:1924/2330 train_time:111059ms step_avg:57.72ms
step:1925/2330 train_time:111116ms step_avg:57.72ms
step:1926/2330 train_time:111177ms step_avg:57.72ms
step:1927/2330 train_time:111234ms step_avg:57.72ms
step:1928/2330 train_time:111294ms step_avg:57.73ms
step:1929/2330 train_time:111351ms step_avg:57.72ms
step:1930/2330 train_time:111411ms step_avg:57.73ms
step:1931/2330 train_time:111468ms step_avg:57.73ms
step:1932/2330 train_time:111527ms step_avg:57.73ms
step:1933/2330 train_time:111584ms step_avg:57.73ms
step:1934/2330 train_time:111644ms step_avg:57.73ms
step:1935/2330 train_time:111700ms step_avg:57.73ms
step:1936/2330 train_time:111762ms step_avg:57.73ms
step:1937/2330 train_time:111818ms step_avg:57.73ms
step:1938/2330 train_time:111879ms step_avg:57.73ms
step:1939/2330 train_time:111935ms step_avg:57.73ms
step:1940/2330 train_time:111995ms step_avg:57.73ms
step:1941/2330 train_time:112052ms step_avg:57.73ms
step:1942/2330 train_time:112112ms step_avg:57.73ms
step:1943/2330 train_time:112169ms step_avg:57.73ms
step:1944/2330 train_time:112228ms step_avg:57.73ms
step:1945/2330 train_time:112284ms step_avg:57.73ms
step:1946/2330 train_time:112344ms step_avg:57.73ms
step:1947/2330 train_time:112400ms step_avg:57.73ms
step:1948/2330 train_time:112461ms step_avg:57.73ms
step:1949/2330 train_time:112518ms step_avg:57.73ms
step:1950/2330 train_time:112578ms step_avg:57.73ms
step:1951/2330 train_time:112635ms step_avg:57.73ms
step:1952/2330 train_time:112695ms step_avg:57.73ms
step:1953/2330 train_time:112753ms step_avg:57.73ms
step:1954/2330 train_time:112812ms step_avg:57.73ms
step:1955/2330 train_time:112869ms step_avg:57.73ms
step:1956/2330 train_time:112929ms step_avg:57.73ms
step:1957/2330 train_time:112987ms step_avg:57.73ms
step:1958/2330 train_time:113046ms step_avg:57.74ms
step:1959/2330 train_time:113103ms step_avg:57.73ms
step:1960/2330 train_time:113163ms step_avg:57.74ms
step:1961/2330 train_time:113219ms step_avg:57.74ms
step:1962/2330 train_time:113280ms step_avg:57.74ms
step:1963/2330 train_time:113336ms step_avg:57.74ms
step:1964/2330 train_time:113397ms step_avg:57.74ms
step:1965/2330 train_time:113453ms step_avg:57.74ms
step:1966/2330 train_time:113513ms step_avg:57.74ms
step:1967/2330 train_time:113571ms step_avg:57.74ms
step:1968/2330 train_time:113631ms step_avg:57.74ms
step:1969/2330 train_time:113687ms step_avg:57.74ms
step:1970/2330 train_time:113747ms step_avg:57.74ms
step:1971/2330 train_time:113804ms step_avg:57.74ms
step:1972/2330 train_time:113863ms step_avg:57.74ms
step:1973/2330 train_time:113920ms step_avg:57.74ms
step:1974/2330 train_time:113981ms step_avg:57.74ms
step:1975/2330 train_time:114038ms step_avg:57.74ms
step:1976/2330 train_time:114098ms step_avg:57.74ms
step:1977/2330 train_time:114155ms step_avg:57.74ms
step:1978/2330 train_time:114215ms step_avg:57.74ms
step:1979/2330 train_time:114272ms step_avg:57.74ms
step:1980/2330 train_time:114331ms step_avg:57.74ms
step:1981/2330 train_time:114388ms step_avg:57.74ms
step:1982/2330 train_time:114447ms step_avg:57.74ms
step:1983/2330 train_time:114504ms step_avg:57.74ms
step:1984/2330 train_time:114563ms step_avg:57.74ms
step:1985/2330 train_time:114620ms step_avg:57.74ms
step:1986/2330 train_time:114681ms step_avg:57.74ms
step:1987/2330 train_time:114737ms step_avg:57.74ms
step:1988/2330 train_time:114798ms step_avg:57.75ms
step:1989/2330 train_time:114854ms step_avg:57.74ms
step:1990/2330 train_time:114915ms step_avg:57.75ms
step:1991/2330 train_time:114973ms step_avg:57.75ms
step:1992/2330 train_time:115033ms step_avg:57.75ms
step:1993/2330 train_time:115091ms step_avg:57.75ms
step:1994/2330 train_time:115150ms step_avg:57.75ms
step:1995/2330 train_time:115208ms step_avg:57.75ms
step:1996/2330 train_time:115267ms step_avg:57.75ms
step:1997/2330 train_time:115324ms step_avg:57.75ms
step:1998/2330 train_time:115382ms step_avg:57.75ms
step:1999/2330 train_time:115438ms step_avg:57.75ms
step:2000/2330 train_time:115499ms step_avg:57.75ms
step:2000/2330 val_loss:4.0755 train_time:115580ms step_avg:57.79ms
step:2001/2330 train_time:115599ms step_avg:57.77ms
step:2002/2330 train_time:115619ms step_avg:57.75ms
step:2003/2330 train_time:115678ms step_avg:57.75ms
step:2004/2330 train_time:115744ms step_avg:57.76ms
step:2005/2330 train_time:115802ms step_avg:57.76ms
step:2006/2330 train_time:115862ms step_avg:57.76ms
step:2007/2330 train_time:115919ms step_avg:57.76ms
step:2008/2330 train_time:115979ms step_avg:57.76ms
step:2009/2330 train_time:116035ms step_avg:57.76ms
step:2010/2330 train_time:116096ms step_avg:57.76ms
step:2011/2330 train_time:116152ms step_avg:57.76ms
step:2012/2330 train_time:116211ms step_avg:57.76ms
step:2013/2330 train_time:116267ms step_avg:57.76ms
step:2014/2330 train_time:116327ms step_avg:57.76ms
step:2015/2330 train_time:116383ms step_avg:57.76ms
step:2016/2330 train_time:116443ms step_avg:57.76ms
step:2017/2330 train_time:116499ms step_avg:57.76ms
step:2018/2330 train_time:116558ms step_avg:57.76ms
step:2019/2330 train_time:116616ms step_avg:57.76ms
step:2020/2330 train_time:116679ms step_avg:57.76ms
step:2021/2330 train_time:116738ms step_avg:57.76ms
step:2022/2330 train_time:116798ms step_avg:57.76ms
step:2023/2330 train_time:116857ms step_avg:57.76ms
step:2024/2330 train_time:116916ms step_avg:57.76ms
step:2025/2330 train_time:116973ms step_avg:57.76ms
step:2026/2330 train_time:117032ms step_avg:57.77ms
step:2027/2330 train_time:117089ms step_avg:57.76ms
step:2028/2330 train_time:117148ms step_avg:57.77ms
step:2029/2330 train_time:117205ms step_avg:57.76ms
step:2030/2330 train_time:117264ms step_avg:57.77ms
step:2031/2330 train_time:117320ms step_avg:57.76ms
step:2032/2330 train_time:117380ms step_avg:57.77ms
step:2033/2330 train_time:117437ms step_avg:57.77ms
step:2034/2330 train_time:117496ms step_avg:57.77ms
step:2035/2330 train_time:117552ms step_avg:57.77ms
step:2036/2330 train_time:117612ms step_avg:57.77ms
step:2037/2330 train_time:117670ms step_avg:57.77ms
step:2038/2330 train_time:117730ms step_avg:57.77ms
step:2039/2330 train_time:117787ms step_avg:57.77ms
step:2040/2330 train_time:117848ms step_avg:57.77ms
step:2041/2330 train_time:117905ms step_avg:57.77ms
step:2042/2330 train_time:117965ms step_avg:57.77ms
step:2043/2330 train_time:118021ms step_avg:57.77ms
step:2044/2330 train_time:118082ms step_avg:57.77ms
step:2045/2330 train_time:118139ms step_avg:57.77ms
step:2046/2330 train_time:118199ms step_avg:57.77ms
step:2047/2330 train_time:118255ms step_avg:57.77ms
step:2048/2330 train_time:118315ms step_avg:57.77ms
step:2049/2330 train_time:118372ms step_avg:57.77ms
step:2050/2330 train_time:118431ms step_avg:57.77ms
step:2051/2330 train_time:118487ms step_avg:57.77ms
step:2052/2330 train_time:118547ms step_avg:57.77ms
step:2053/2330 train_time:118604ms step_avg:57.77ms
step:2054/2330 train_time:118664ms step_avg:57.77ms
step:2055/2330 train_time:118721ms step_avg:57.77ms
step:2056/2330 train_time:118782ms step_avg:57.77ms
step:2057/2330 train_time:118840ms step_avg:57.77ms
step:2058/2330 train_time:118901ms step_avg:57.77ms
step:2059/2330 train_time:118957ms step_avg:57.77ms
step:2060/2330 train_time:119018ms step_avg:57.78ms
step:2061/2330 train_time:119075ms step_avg:57.78ms
step:2062/2330 train_time:119135ms step_avg:57.78ms
step:2063/2330 train_time:119192ms step_avg:57.78ms
step:2064/2330 train_time:119252ms step_avg:57.78ms
step:2065/2330 train_time:119309ms step_avg:57.78ms
step:2066/2330 train_time:119368ms step_avg:57.78ms
step:2067/2330 train_time:119424ms step_avg:57.78ms
step:2068/2330 train_time:119484ms step_avg:57.78ms
step:2069/2330 train_time:119541ms step_avg:57.78ms
step:2070/2330 train_time:119600ms step_avg:57.78ms
step:2071/2330 train_time:119658ms step_avg:57.78ms
step:2072/2330 train_time:119718ms step_avg:57.78ms
step:2073/2330 train_time:119776ms step_avg:57.78ms
step:2074/2330 train_time:119836ms step_avg:57.78ms
step:2075/2330 train_time:119894ms step_avg:57.78ms
step:2076/2330 train_time:119954ms step_avg:57.78ms
step:2077/2330 train_time:120010ms step_avg:57.78ms
step:2078/2330 train_time:120070ms step_avg:57.78ms
step:2079/2330 train_time:120126ms step_avg:57.78ms
step:2080/2330 train_time:120187ms step_avg:57.78ms
step:2081/2330 train_time:120243ms step_avg:57.78ms
step:2082/2330 train_time:120303ms step_avg:57.78ms
step:2083/2330 train_time:120360ms step_avg:57.78ms
step:2084/2330 train_time:120420ms step_avg:57.78ms
step:2085/2330 train_time:120476ms step_avg:57.78ms
step:2086/2330 train_time:120537ms step_avg:57.78ms
step:2087/2330 train_time:120594ms step_avg:57.78ms
step:2088/2330 train_time:120654ms step_avg:57.78ms
step:2089/2330 train_time:120710ms step_avg:57.78ms
step:2090/2330 train_time:120770ms step_avg:57.78ms
step:2091/2330 train_time:120827ms step_avg:57.78ms
step:2092/2330 train_time:120886ms step_avg:57.79ms
step:2093/2330 train_time:120944ms step_avg:57.78ms
step:2094/2330 train_time:121004ms step_avg:57.79ms
step:2095/2330 train_time:121060ms step_avg:57.79ms
step:2096/2330 train_time:121121ms step_avg:57.79ms
step:2097/2330 train_time:121178ms step_avg:57.79ms
step:2098/2330 train_time:121238ms step_avg:57.79ms
step:2099/2330 train_time:121295ms step_avg:57.79ms
step:2100/2330 train_time:121355ms step_avg:57.79ms
step:2101/2330 train_time:121412ms step_avg:57.79ms
step:2102/2330 train_time:121472ms step_avg:57.79ms
step:2103/2330 train_time:121528ms step_avg:57.79ms
step:2104/2330 train_time:121588ms step_avg:57.79ms
step:2105/2330 train_time:121645ms step_avg:57.79ms
step:2106/2330 train_time:121704ms step_avg:57.79ms
step:2107/2330 train_time:121761ms step_avg:57.79ms
step:2108/2330 train_time:121823ms step_avg:57.79ms
step:2109/2330 train_time:121880ms step_avg:57.79ms
step:2110/2330 train_time:121940ms step_avg:57.79ms
step:2111/2330 train_time:121997ms step_avg:57.79ms
step:2112/2330 train_time:122057ms step_avg:57.79ms
step:2113/2330 train_time:122114ms step_avg:57.79ms
step:2114/2330 train_time:122173ms step_avg:57.79ms
step:2115/2330 train_time:122230ms step_avg:57.79ms
step:2116/2330 train_time:122289ms step_avg:57.79ms
step:2117/2330 train_time:122345ms step_avg:57.79ms
step:2118/2330 train_time:122406ms step_avg:57.79ms
step:2119/2330 train_time:122462ms step_avg:57.79ms
step:2120/2330 train_time:122522ms step_avg:57.79ms
step:2121/2330 train_time:122579ms step_avg:57.79ms
step:2122/2330 train_time:122639ms step_avg:57.79ms
step:2123/2330 train_time:122696ms step_avg:57.79ms
step:2124/2330 train_time:122757ms step_avg:57.80ms
step:2125/2330 train_time:122814ms step_avg:57.79ms
step:2126/2330 train_time:122874ms step_avg:57.80ms
step:2127/2330 train_time:122931ms step_avg:57.80ms
step:2128/2330 train_time:122990ms step_avg:57.80ms
step:2129/2330 train_time:123048ms step_avg:57.80ms
step:2130/2330 train_time:123107ms step_avg:57.80ms
step:2131/2330 train_time:123163ms step_avg:57.80ms
step:2132/2330 train_time:123224ms step_avg:57.80ms
step:2133/2330 train_time:123281ms step_avg:57.80ms
step:2134/2330 train_time:123342ms step_avg:57.80ms
step:2135/2330 train_time:123399ms step_avg:57.80ms
step:2136/2330 train_time:123458ms step_avg:57.80ms
step:2137/2330 train_time:123515ms step_avg:57.80ms
step:2138/2330 train_time:123575ms step_avg:57.80ms
step:2139/2330 train_time:123633ms step_avg:57.80ms
step:2140/2330 train_time:123693ms step_avg:57.80ms
step:2141/2330 train_time:123750ms step_avg:57.80ms
step:2142/2330 train_time:123809ms step_avg:57.80ms
step:2143/2330 train_time:123866ms step_avg:57.80ms
step:2144/2330 train_time:123925ms step_avg:57.80ms
step:2145/2330 train_time:123982ms step_avg:57.80ms
step:2146/2330 train_time:124042ms step_avg:57.80ms
step:2147/2330 train_time:124100ms step_avg:57.80ms
step:2148/2330 train_time:124160ms step_avg:57.80ms
step:2149/2330 train_time:124217ms step_avg:57.80ms
step:2150/2330 train_time:124277ms step_avg:57.80ms
step:2151/2330 train_time:124335ms step_avg:57.80ms
step:2152/2330 train_time:124395ms step_avg:57.80ms
step:2153/2330 train_time:124452ms step_avg:57.80ms
step:2154/2330 train_time:124511ms step_avg:57.80ms
step:2155/2330 train_time:124568ms step_avg:57.80ms
step:2156/2330 train_time:124628ms step_avg:57.81ms
step:2157/2330 train_time:124684ms step_avg:57.80ms
step:2158/2330 train_time:124744ms step_avg:57.81ms
step:2159/2330 train_time:124801ms step_avg:57.81ms
step:2160/2330 train_time:124861ms step_avg:57.81ms
step:2161/2330 train_time:124918ms step_avg:57.81ms
step:2162/2330 train_time:124979ms step_avg:57.81ms
step:2163/2330 train_time:125036ms step_avg:57.81ms
step:2164/2330 train_time:125097ms step_avg:57.81ms
step:2165/2330 train_time:125154ms step_avg:57.81ms
step:2166/2330 train_time:125214ms step_avg:57.81ms
step:2167/2330 train_time:125270ms step_avg:57.81ms
step:2168/2330 train_time:125330ms step_avg:57.81ms
step:2169/2330 train_time:125387ms step_avg:57.81ms
step:2170/2330 train_time:125446ms step_avg:57.81ms
step:2171/2330 train_time:125503ms step_avg:57.81ms
step:2172/2330 train_time:125563ms step_avg:57.81ms
step:2173/2330 train_time:125620ms step_avg:57.81ms
step:2174/2330 train_time:125680ms step_avg:57.81ms
step:2175/2330 train_time:125738ms step_avg:57.81ms
step:2176/2330 train_time:125798ms step_avg:57.81ms
step:2177/2330 train_time:125855ms step_avg:57.81ms
step:2178/2330 train_time:125914ms step_avg:57.81ms
step:2179/2330 train_time:125971ms step_avg:57.81ms
step:2180/2330 train_time:126032ms step_avg:57.81ms
step:2181/2330 train_time:126089ms step_avg:57.81ms
step:2182/2330 train_time:126149ms step_avg:57.81ms
step:2183/2330 train_time:126205ms step_avg:57.81ms
step:2184/2330 train_time:126265ms step_avg:57.81ms
step:2185/2330 train_time:126322ms step_avg:57.81ms
step:2186/2330 train_time:126382ms step_avg:57.81ms
step:2187/2330 train_time:126440ms step_avg:57.81ms
step:2188/2330 train_time:126500ms step_avg:57.82ms
step:2189/2330 train_time:126557ms step_avg:57.82ms
step:2190/2330 train_time:126617ms step_avg:57.82ms
step:2191/2330 train_time:126674ms step_avg:57.82ms
step:2192/2330 train_time:126733ms step_avg:57.82ms
step:2193/2330 train_time:126790ms step_avg:57.82ms
step:2194/2330 train_time:126850ms step_avg:57.82ms
step:2195/2330 train_time:126907ms step_avg:57.82ms
step:2196/2330 train_time:126966ms step_avg:57.82ms
step:2197/2330 train_time:127023ms step_avg:57.82ms
step:2198/2330 train_time:127085ms step_avg:57.82ms
step:2199/2330 train_time:127141ms step_avg:57.82ms
step:2200/2330 train_time:127201ms step_avg:57.82ms
step:2201/2330 train_time:127258ms step_avg:57.82ms
step:2202/2330 train_time:127319ms step_avg:57.82ms
step:2203/2330 train_time:127376ms step_avg:57.82ms
step:2204/2330 train_time:127436ms step_avg:57.82ms
step:2205/2330 train_time:127493ms step_avg:57.82ms
step:2206/2330 train_time:127554ms step_avg:57.82ms
step:2207/2330 train_time:127610ms step_avg:57.82ms
step:2208/2330 train_time:127670ms step_avg:57.82ms
step:2209/2330 train_time:127726ms step_avg:57.82ms
step:2210/2330 train_time:127786ms step_avg:57.82ms
step:2211/2330 train_time:127843ms step_avg:57.82ms
step:2212/2330 train_time:127903ms step_avg:57.82ms
step:2213/2330 train_time:127959ms step_avg:57.82ms
step:2214/2330 train_time:128020ms step_avg:57.82ms
step:2215/2330 train_time:128077ms step_avg:57.82ms
step:2216/2330 train_time:128138ms step_avg:57.82ms
step:2217/2330 train_time:128195ms step_avg:57.82ms
step:2218/2330 train_time:128254ms step_avg:57.82ms
step:2219/2330 train_time:128310ms step_avg:57.82ms
step:2220/2330 train_time:128370ms step_avg:57.82ms
step:2221/2330 train_time:128427ms step_avg:57.82ms
step:2222/2330 train_time:128487ms step_avg:57.82ms
step:2223/2330 train_time:128544ms step_avg:57.82ms
step:2224/2330 train_time:128604ms step_avg:57.83ms
step:2225/2330 train_time:128660ms step_avg:57.82ms
step:2226/2330 train_time:128720ms step_avg:57.83ms
step:2227/2330 train_time:128777ms step_avg:57.83ms
step:2228/2330 train_time:128838ms step_avg:57.83ms
step:2229/2330 train_time:128895ms step_avg:57.83ms
step:2230/2330 train_time:128955ms step_avg:57.83ms
step:2231/2330 train_time:129013ms step_avg:57.83ms
step:2232/2330 train_time:129072ms step_avg:57.83ms
step:2233/2330 train_time:129129ms step_avg:57.83ms
step:2234/2330 train_time:129188ms step_avg:57.83ms
step:2235/2330 train_time:129245ms step_avg:57.83ms
step:2236/2330 train_time:129305ms step_avg:57.83ms
step:2237/2330 train_time:129362ms step_avg:57.83ms
step:2238/2330 train_time:129423ms step_avg:57.83ms
step:2239/2330 train_time:129480ms step_avg:57.83ms
step:2240/2330 train_time:129540ms step_avg:57.83ms
step:2241/2330 train_time:129597ms step_avg:57.83ms
step:2242/2330 train_time:129657ms step_avg:57.83ms
step:2243/2330 train_time:129714ms step_avg:57.83ms
step:2244/2330 train_time:129773ms step_avg:57.83ms
step:2245/2330 train_time:129830ms step_avg:57.83ms
step:2246/2330 train_time:129889ms step_avg:57.83ms
step:2247/2330 train_time:129946ms step_avg:57.83ms
step:2248/2330 train_time:130006ms step_avg:57.83ms
step:2249/2330 train_time:130062ms step_avg:57.83ms
step:2250/2330 train_time:130123ms step_avg:57.83ms
step:2250/2330 val_loss:4.0275 train_time:130203ms step_avg:57.87ms
step:2251/2330 train_time:130223ms step_avg:57.85ms
step:2252/2330 train_time:130243ms step_avg:57.83ms
step:2253/2330 train_time:130301ms step_avg:57.83ms
step:2254/2330 train_time:130366ms step_avg:57.84ms
step:2255/2330 train_time:130422ms step_avg:57.84ms
step:2256/2330 train_time:130484ms step_avg:57.84ms
step:2257/2330 train_time:130541ms step_avg:57.84ms
step:2258/2330 train_time:130600ms step_avg:57.84ms
step:2259/2330 train_time:130656ms step_avg:57.84ms
step:2260/2330 train_time:130714ms step_avg:57.84ms
step:2261/2330 train_time:130770ms step_avg:57.84ms
step:2262/2330 train_time:130829ms step_avg:57.84ms
step:2263/2330 train_time:130886ms step_avg:57.84ms
step:2264/2330 train_time:130945ms step_avg:57.84ms
step:2265/2330 train_time:131001ms step_avg:57.84ms
step:2266/2330 train_time:131061ms step_avg:57.84ms
step:2267/2330 train_time:131117ms step_avg:57.84ms
step:2268/2330 train_time:131177ms step_avg:57.84ms
step:2269/2330 train_time:131234ms step_avg:57.84ms
step:2270/2330 train_time:131295ms step_avg:57.84ms
step:2271/2330 train_time:131352ms step_avg:57.84ms
step:2272/2330 train_time:131415ms step_avg:57.84ms
step:2273/2330 train_time:131471ms step_avg:57.84ms
step:2274/2330 train_time:131533ms step_avg:57.84ms
step:2275/2330 train_time:131590ms step_avg:57.84ms
step:2276/2330 train_time:131650ms step_avg:57.84ms
step:2277/2330 train_time:131706ms step_avg:57.84ms
step:2278/2330 train_time:131766ms step_avg:57.84ms
step:2279/2330 train_time:131822ms step_avg:57.84ms
step:2280/2330 train_time:131881ms step_avg:57.84ms
step:2281/2330 train_time:131937ms step_avg:57.84ms
step:2282/2330 train_time:131996ms step_avg:57.84ms
step:2283/2330 train_time:132052ms step_avg:57.84ms
step:2284/2330 train_time:132113ms step_avg:57.84ms
step:2285/2330 train_time:132171ms step_avg:57.84ms
step:2286/2330 train_time:132231ms step_avg:57.84ms
step:2287/2330 train_time:132287ms step_avg:57.84ms
step:2288/2330 train_time:132349ms step_avg:57.84ms
step:2289/2330 train_time:132405ms step_avg:57.84ms
step:2290/2330 train_time:132467ms step_avg:57.85ms
step:2291/2330 train_time:132524ms step_avg:57.85ms
step:2292/2330 train_time:132584ms step_avg:57.85ms
step:2293/2330 train_time:132640ms step_avg:57.85ms
step:2294/2330 train_time:132701ms step_avg:57.85ms
step:2295/2330 train_time:132757ms step_avg:57.85ms
step:2296/2330 train_time:132816ms step_avg:57.85ms
step:2297/2330 train_time:132873ms step_avg:57.85ms
step:2298/2330 train_time:132932ms step_avg:57.85ms
step:2299/2330 train_time:132988ms step_avg:57.85ms
step:2300/2330 train_time:133048ms step_avg:57.85ms
step:2301/2330 train_time:133104ms step_avg:57.85ms
step:2302/2330 train_time:133165ms step_avg:57.85ms
step:2303/2330 train_time:133222ms step_avg:57.85ms
step:2304/2330 train_time:133281ms step_avg:57.85ms
step:2305/2330 train_time:133338ms step_avg:57.85ms
step:2306/2330 train_time:133398ms step_avg:57.85ms
step:2307/2330 train_time:133455ms step_avg:57.85ms
step:2308/2330 train_time:133515ms step_avg:57.85ms
step:2309/2330 train_time:133572ms step_avg:57.85ms
step:2310/2330 train_time:133633ms step_avg:57.85ms
step:2311/2330 train_time:133690ms step_avg:57.85ms
step:2312/2330 train_time:133751ms step_avg:57.85ms
step:2313/2330 train_time:133807ms step_avg:57.85ms
step:2314/2330 train_time:133868ms step_avg:57.85ms
step:2315/2330 train_time:133925ms step_avg:57.85ms
step:2316/2330 train_time:133984ms step_avg:57.85ms
step:2317/2330 train_time:134040ms step_avg:57.85ms
step:2318/2330 train_time:134099ms step_avg:57.85ms
step:2319/2330 train_time:134156ms step_avg:57.85ms
step:2320/2330 train_time:134215ms step_avg:57.85ms
step:2321/2330 train_time:134272ms step_avg:57.85ms
step:2322/2330 train_time:134332ms step_avg:57.85ms
step:2323/2330 train_time:134389ms step_avg:57.85ms
step:2324/2330 train_time:134449ms step_avg:57.85ms
step:2325/2330 train_time:134506ms step_avg:57.85ms
step:2326/2330 train_time:134567ms step_avg:57.85ms
step:2327/2330 train_time:134624ms step_avg:57.85ms
step:2328/2330 train_time:134684ms step_avg:57.85ms
step:2329/2330 train_time:134741ms step_avg:57.85ms
step:2330/2330 train_time:134800ms step_avg:57.85ms
step:2330/2330 val_loss:4.0136 train_time:134880ms step_avg:57.89ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
