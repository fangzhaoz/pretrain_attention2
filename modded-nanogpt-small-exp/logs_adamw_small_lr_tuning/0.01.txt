import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--adamw_lr', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_lr_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_lr_tuning/{args2.adamw_lr}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=float(args2.adamw_lr),  betas=(0.85, 0.85), eps=1e-10, weight_decay=0.0, fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_lr_tuning/{args2.adamw_lr}", exist_ok=True)
            torch.save(log, f"logs_adamw_small_lr_tuning/{args2.adamw_lr}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 02:14:13 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   27C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   26C    P0             115W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   24C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   27C    P0             113W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   25C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   26C    P0             109W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:96ms step_avg:96.35ms
step:2/2330 train_time:186ms step_avg:92.76ms
step:3/2330 train_time:203ms step_avg:67.75ms
step:4/2330 train_time:222ms step_avg:55.59ms
step:5/2330 train_time:276ms step_avg:55.23ms
step:6/2330 train_time:334ms step_avg:55.60ms
step:7/2330 train_time:387ms step_avg:55.33ms
step:8/2330 train_time:445ms step_avg:55.59ms
step:9/2330 train_time:499ms step_avg:55.41ms
step:10/2330 train_time:556ms step_avg:55.64ms
step:11/2330 train_time:611ms step_avg:55.51ms
step:12/2330 train_time:668ms step_avg:55.67ms
step:13/2330 train_time:722ms step_avg:55.56ms
step:14/2330 train_time:779ms step_avg:55.66ms
step:15/2330 train_time:834ms step_avg:55.59ms
step:16/2330 train_time:891ms step_avg:55.69ms
step:17/2330 train_time:945ms step_avg:55.61ms
step:18/2330 train_time:1003ms step_avg:55.71ms
step:19/2330 train_time:1058ms step_avg:55.68ms
step:20/2330 train_time:1117ms step_avg:55.85ms
step:21/2330 train_time:1173ms step_avg:55.85ms
step:22/2330 train_time:1231ms step_avg:55.93ms
step:23/2330 train_time:1285ms step_avg:55.89ms
step:24/2330 train_time:1344ms step_avg:56.00ms
step:25/2330 train_time:1399ms step_avg:55.98ms
step:26/2330 train_time:1457ms step_avg:56.05ms
step:27/2330 train_time:1513ms step_avg:56.03ms
step:28/2330 train_time:1570ms step_avg:56.08ms
step:29/2330 train_time:1625ms step_avg:56.02ms
step:30/2330 train_time:1682ms step_avg:56.07ms
step:31/2330 train_time:1737ms step_avg:56.04ms
step:32/2330 train_time:1795ms step_avg:56.10ms
step:33/2330 train_time:1849ms step_avg:56.04ms
step:34/2330 train_time:1907ms step_avg:56.09ms
step:35/2330 train_time:1961ms step_avg:56.04ms
step:36/2330 train_time:2019ms step_avg:56.09ms
step:37/2330 train_time:2074ms step_avg:56.06ms
step:38/2330 train_time:2133ms step_avg:56.13ms
step:39/2330 train_time:2188ms step_avg:56.11ms
step:40/2330 train_time:2246ms step_avg:56.16ms
step:41/2330 train_time:2302ms step_avg:56.15ms
step:42/2330 train_time:2360ms step_avg:56.20ms
step:43/2330 train_time:2416ms step_avg:56.19ms
step:44/2330 train_time:2475ms step_avg:56.25ms
step:45/2330 train_time:2530ms step_avg:56.23ms
step:46/2330 train_time:2588ms step_avg:56.25ms
step:47/2330 train_time:2642ms step_avg:56.22ms
step:48/2330 train_time:2700ms step_avg:56.26ms
step:49/2330 train_time:2755ms step_avg:56.23ms
step:50/2330 train_time:2813ms step_avg:56.26ms
step:51/2330 train_time:2868ms step_avg:56.23ms
step:52/2330 train_time:2925ms step_avg:56.25ms
step:53/2330 train_time:2980ms step_avg:56.23ms
step:54/2330 train_time:3038ms step_avg:56.26ms
step:55/2330 train_time:3093ms step_avg:56.24ms
step:56/2330 train_time:3152ms step_avg:56.29ms
step:57/2330 train_time:3207ms step_avg:56.26ms
step:58/2330 train_time:3265ms step_avg:56.30ms
step:59/2330 train_time:3321ms step_avg:56.29ms
step:60/2330 train_time:3379ms step_avg:56.32ms
step:61/2330 train_time:3435ms step_avg:56.30ms
step:62/2330 train_time:3493ms step_avg:56.34ms
step:63/2330 train_time:3549ms step_avg:56.33ms
step:64/2330 train_time:3607ms step_avg:56.36ms
step:65/2330 train_time:3662ms step_avg:56.34ms
step:66/2330 train_time:3720ms step_avg:56.37ms
step:67/2330 train_time:3776ms step_avg:56.35ms
step:68/2330 train_time:3833ms step_avg:56.37ms
step:69/2330 train_time:3888ms step_avg:56.35ms
step:70/2330 train_time:3945ms step_avg:56.36ms
step:71/2330 train_time:4001ms step_avg:56.35ms
step:72/2330 train_time:4059ms step_avg:56.37ms
step:73/2330 train_time:4114ms step_avg:56.35ms
step:74/2330 train_time:4172ms step_avg:56.37ms
step:75/2330 train_time:4226ms step_avg:56.35ms
step:76/2330 train_time:4285ms step_avg:56.38ms
step:77/2330 train_time:4340ms step_avg:56.37ms
step:78/2330 train_time:4399ms step_avg:56.40ms
step:79/2330 train_time:4455ms step_avg:56.39ms
step:80/2330 train_time:4514ms step_avg:56.43ms
step:81/2330 train_time:4569ms step_avg:56.41ms
step:82/2330 train_time:4627ms step_avg:56.43ms
step:83/2330 train_time:4682ms step_avg:56.41ms
step:84/2330 train_time:4741ms step_avg:56.44ms
step:85/2330 train_time:4796ms step_avg:56.43ms
step:86/2330 train_time:4856ms step_avg:56.47ms
step:87/2330 train_time:4911ms step_avg:56.45ms
step:88/2330 train_time:4969ms step_avg:56.46ms
step:89/2330 train_time:5023ms step_avg:56.44ms
step:90/2330 train_time:5082ms step_avg:56.47ms
step:91/2330 train_time:5137ms step_avg:56.45ms
step:92/2330 train_time:5196ms step_avg:56.48ms
step:93/2330 train_time:5251ms step_avg:56.47ms
step:94/2330 train_time:5309ms step_avg:56.48ms
step:95/2330 train_time:5364ms step_avg:56.46ms
step:96/2330 train_time:5423ms step_avg:56.49ms
step:97/2330 train_time:5478ms step_avg:56.48ms
step:98/2330 train_time:5536ms step_avg:56.49ms
step:99/2330 train_time:5591ms step_avg:56.48ms
step:100/2330 train_time:5649ms step_avg:56.49ms
step:101/2330 train_time:5704ms step_avg:56.47ms
step:102/2330 train_time:5763ms step_avg:56.50ms
step:103/2330 train_time:5818ms step_avg:56.49ms
step:104/2330 train_time:5878ms step_avg:56.52ms
step:105/2330 train_time:5933ms step_avg:56.51ms
step:106/2330 train_time:5991ms step_avg:56.52ms
step:107/2330 train_time:6046ms step_avg:56.50ms
step:108/2330 train_time:6105ms step_avg:56.53ms
step:109/2330 train_time:6161ms step_avg:56.52ms
step:110/2330 train_time:6219ms step_avg:56.53ms
step:111/2330 train_time:6274ms step_avg:56.52ms
step:112/2330 train_time:6332ms step_avg:56.53ms
step:113/2330 train_time:6387ms step_avg:56.53ms
step:114/2330 train_time:6446ms step_avg:56.55ms
step:115/2330 train_time:6502ms step_avg:56.54ms
step:116/2330 train_time:6561ms step_avg:56.56ms
step:117/2330 train_time:6616ms step_avg:56.55ms
step:118/2330 train_time:6676ms step_avg:56.58ms
step:119/2330 train_time:6731ms step_avg:56.56ms
step:120/2330 train_time:6789ms step_avg:56.58ms
step:121/2330 train_time:6844ms step_avg:56.56ms
step:122/2330 train_time:6903ms step_avg:56.58ms
step:123/2330 train_time:6959ms step_avg:56.58ms
step:124/2330 train_time:7018ms step_avg:56.59ms
step:125/2330 train_time:7074ms step_avg:56.59ms
step:126/2330 train_time:7131ms step_avg:56.60ms
step:127/2330 train_time:7187ms step_avg:56.59ms
step:128/2330 train_time:7245ms step_avg:56.60ms
step:129/2330 train_time:7301ms step_avg:56.60ms
step:130/2330 train_time:7360ms step_avg:56.62ms
step:131/2330 train_time:7416ms step_avg:56.61ms
step:132/2330 train_time:7475ms step_avg:56.63ms
step:133/2330 train_time:7529ms step_avg:56.61ms
step:134/2330 train_time:7588ms step_avg:56.63ms
step:135/2330 train_time:7643ms step_avg:56.62ms
step:136/2330 train_time:7703ms step_avg:56.64ms
step:137/2330 train_time:7759ms step_avg:56.64ms
step:138/2330 train_time:7818ms step_avg:56.65ms
step:139/2330 train_time:7873ms step_avg:56.64ms
step:140/2330 train_time:7931ms step_avg:56.65ms
step:141/2330 train_time:7987ms step_avg:56.64ms
step:142/2330 train_time:8046ms step_avg:56.66ms
step:143/2330 train_time:8102ms step_avg:56.66ms
step:144/2330 train_time:8160ms step_avg:56.67ms
step:145/2330 train_time:8215ms step_avg:56.66ms
step:146/2330 train_time:8273ms step_avg:56.67ms
step:147/2330 train_time:8329ms step_avg:56.66ms
step:148/2330 train_time:8387ms step_avg:56.67ms
step:149/2330 train_time:8443ms step_avg:56.66ms
step:150/2330 train_time:8502ms step_avg:56.68ms
step:151/2330 train_time:8557ms step_avg:56.67ms
step:152/2330 train_time:8616ms step_avg:56.68ms
step:153/2330 train_time:8671ms step_avg:56.67ms
step:154/2330 train_time:8730ms step_avg:56.69ms
step:155/2330 train_time:8785ms step_avg:56.68ms
step:156/2330 train_time:8844ms step_avg:56.70ms
step:157/2330 train_time:8900ms step_avg:56.69ms
step:158/2330 train_time:8960ms step_avg:56.71ms
step:159/2330 train_time:9015ms step_avg:56.70ms
step:160/2330 train_time:9074ms step_avg:56.71ms
step:161/2330 train_time:9129ms step_avg:56.70ms
step:162/2330 train_time:9188ms step_avg:56.72ms
step:163/2330 train_time:9243ms step_avg:56.71ms
step:164/2330 train_time:9302ms step_avg:56.72ms
step:165/2330 train_time:9358ms step_avg:56.71ms
step:166/2330 train_time:9416ms step_avg:56.72ms
step:167/2330 train_time:9471ms step_avg:56.71ms
step:168/2330 train_time:9530ms step_avg:56.73ms
step:169/2330 train_time:9585ms step_avg:56.72ms
step:170/2330 train_time:9644ms step_avg:56.73ms
step:171/2330 train_time:9700ms step_avg:56.72ms
step:172/2330 train_time:9758ms step_avg:56.73ms
step:173/2330 train_time:9814ms step_avg:56.73ms
step:174/2330 train_time:9872ms step_avg:56.74ms
step:175/2330 train_time:9927ms step_avg:56.73ms
step:176/2330 train_time:9987ms step_avg:56.74ms
step:177/2330 train_time:10042ms step_avg:56.73ms
step:178/2330 train_time:10102ms step_avg:56.75ms
step:179/2330 train_time:10158ms step_avg:56.75ms
step:180/2330 train_time:10216ms step_avg:56.76ms
step:181/2330 train_time:10271ms step_avg:56.75ms
step:182/2330 train_time:10329ms step_avg:56.75ms
step:183/2330 train_time:10384ms step_avg:56.74ms
step:184/2330 train_time:10443ms step_avg:56.76ms
step:185/2330 train_time:10499ms step_avg:56.75ms
step:186/2330 train_time:10557ms step_avg:56.76ms
step:187/2330 train_time:10614ms step_avg:56.76ms
step:188/2330 train_time:10672ms step_avg:56.76ms
step:189/2330 train_time:10727ms step_avg:56.76ms
step:190/2330 train_time:10786ms step_avg:56.77ms
step:191/2330 train_time:10841ms step_avg:56.76ms
step:192/2330 train_time:10900ms step_avg:56.77ms
step:193/2330 train_time:10956ms step_avg:56.77ms
step:194/2330 train_time:11016ms step_avg:56.78ms
step:195/2330 train_time:11072ms step_avg:56.78ms
step:196/2330 train_time:11129ms step_avg:56.78ms
step:197/2330 train_time:11185ms step_avg:56.78ms
step:198/2330 train_time:11244ms step_avg:56.79ms
step:199/2330 train_time:11301ms step_avg:56.79ms
step:200/2330 train_time:11359ms step_avg:56.79ms
step:201/2330 train_time:11413ms step_avg:56.78ms
step:202/2330 train_time:11472ms step_avg:56.79ms
step:203/2330 train_time:11526ms step_avg:56.78ms
step:204/2330 train_time:11586ms step_avg:56.79ms
step:205/2330 train_time:11642ms step_avg:56.79ms
step:206/2330 train_time:11701ms step_avg:56.80ms
step:207/2330 train_time:11757ms step_avg:56.80ms
step:208/2330 train_time:11816ms step_avg:56.81ms
step:209/2330 train_time:11871ms step_avg:56.80ms
step:210/2330 train_time:11929ms step_avg:56.81ms
step:211/2330 train_time:11985ms step_avg:56.80ms
step:212/2330 train_time:12044ms step_avg:56.81ms
step:213/2330 train_time:12100ms step_avg:56.81ms
step:214/2330 train_time:12160ms step_avg:56.82ms
step:215/2330 train_time:12215ms step_avg:56.82ms
step:216/2330 train_time:12275ms step_avg:56.83ms
step:217/2330 train_time:12330ms step_avg:56.82ms
step:218/2330 train_time:12389ms step_avg:56.83ms
step:219/2330 train_time:12444ms step_avg:56.82ms
step:220/2330 train_time:12504ms step_avg:56.84ms
step:221/2330 train_time:12559ms step_avg:56.83ms
step:222/2330 train_time:12617ms step_avg:56.83ms
step:223/2330 train_time:12674ms step_avg:56.83ms
step:224/2330 train_time:12731ms step_avg:56.84ms
step:225/2330 train_time:12787ms step_avg:56.83ms
step:226/2330 train_time:12846ms step_avg:56.84ms
step:227/2330 train_time:12901ms step_avg:56.83ms
step:228/2330 train_time:12960ms step_avg:56.84ms
step:229/2330 train_time:13016ms step_avg:56.84ms
step:230/2330 train_time:13074ms step_avg:56.84ms
step:231/2330 train_time:13129ms step_avg:56.84ms
step:232/2330 train_time:13188ms step_avg:56.84ms
step:233/2330 train_time:13243ms step_avg:56.84ms
step:234/2330 train_time:13302ms step_avg:56.85ms
step:235/2330 train_time:13358ms step_avg:56.84ms
step:236/2330 train_time:13417ms step_avg:56.85ms
step:237/2330 train_time:13473ms step_avg:56.85ms
step:238/2330 train_time:13531ms step_avg:56.85ms
step:239/2330 train_time:13587ms step_avg:56.85ms
step:240/2330 train_time:13646ms step_avg:56.86ms
step:241/2330 train_time:13701ms step_avg:56.85ms
step:242/2330 train_time:13760ms step_avg:56.86ms
step:243/2330 train_time:13815ms step_avg:56.85ms
step:244/2330 train_time:13875ms step_avg:56.86ms
step:245/2330 train_time:13930ms step_avg:56.86ms
step:246/2330 train_time:13989ms step_avg:56.87ms
step:247/2330 train_time:14045ms step_avg:56.86ms
step:248/2330 train_time:14104ms step_avg:56.87ms
step:249/2330 train_time:14159ms step_avg:56.87ms
step:250/2330 train_time:14218ms step_avg:56.87ms
step:250/2330 val_loss:6.0921 train_time:14296ms step_avg:57.19ms
step:251/2330 train_time:14315ms step_avg:57.03ms
step:252/2330 train_time:14335ms step_avg:56.89ms
step:253/2330 train_time:14391ms step_avg:56.88ms
step:254/2330 train_time:14452ms step_avg:56.90ms
step:255/2330 train_time:14508ms step_avg:56.89ms
step:256/2330 train_time:14572ms step_avg:56.92ms
step:257/2330 train_time:14628ms step_avg:56.92ms
step:258/2330 train_time:14687ms step_avg:56.93ms
step:259/2330 train_time:14743ms step_avg:56.92ms
step:260/2330 train_time:14801ms step_avg:56.93ms
step:261/2330 train_time:14857ms step_avg:56.92ms
step:262/2330 train_time:14914ms step_avg:56.93ms
step:263/2330 train_time:14969ms step_avg:56.92ms
step:264/2330 train_time:15028ms step_avg:56.92ms
step:265/2330 train_time:15083ms step_avg:56.92ms
step:266/2330 train_time:15141ms step_avg:56.92ms
step:267/2330 train_time:15197ms step_avg:56.92ms
step:268/2330 train_time:15255ms step_avg:56.92ms
step:269/2330 train_time:15310ms step_avg:56.92ms
step:270/2330 train_time:15370ms step_avg:56.92ms
step:271/2330 train_time:15426ms step_avg:56.92ms
step:272/2330 train_time:15487ms step_avg:56.94ms
step:273/2330 train_time:15543ms step_avg:56.93ms
step:274/2330 train_time:15602ms step_avg:56.94ms
step:275/2330 train_time:15657ms step_avg:56.93ms
step:276/2330 train_time:15717ms step_avg:56.94ms
step:277/2330 train_time:15772ms step_avg:56.94ms
step:278/2330 train_time:15831ms step_avg:56.94ms
step:279/2330 train_time:15886ms step_avg:56.94ms
step:280/2330 train_time:15945ms step_avg:56.95ms
step:281/2330 train_time:16000ms step_avg:56.94ms
step:282/2330 train_time:16058ms step_avg:56.94ms
step:283/2330 train_time:16113ms step_avg:56.94ms
step:284/2330 train_time:16172ms step_avg:56.94ms
step:285/2330 train_time:16227ms step_avg:56.94ms
step:286/2330 train_time:16286ms step_avg:56.94ms
step:287/2330 train_time:16342ms step_avg:56.94ms
step:288/2330 train_time:16401ms step_avg:56.95ms
step:289/2330 train_time:16457ms step_avg:56.95ms
step:290/2330 train_time:16516ms step_avg:56.95ms
step:291/2330 train_time:16571ms step_avg:56.95ms
step:292/2330 train_time:16632ms step_avg:56.96ms
step:293/2330 train_time:16688ms step_avg:56.96ms
step:294/2330 train_time:16747ms step_avg:56.96ms
step:295/2330 train_time:16802ms step_avg:56.96ms
step:296/2330 train_time:16861ms step_avg:56.96ms
step:297/2330 train_time:16916ms step_avg:56.96ms
step:298/2330 train_time:16974ms step_avg:56.96ms
step:299/2330 train_time:17029ms step_avg:56.95ms
step:300/2330 train_time:17088ms step_avg:56.96ms
step:301/2330 train_time:17143ms step_avg:56.95ms
step:302/2330 train_time:17202ms step_avg:56.96ms
step:303/2330 train_time:17257ms step_avg:56.95ms
step:304/2330 train_time:17316ms step_avg:56.96ms
step:305/2330 train_time:17372ms step_avg:56.96ms
step:306/2330 train_time:17431ms step_avg:56.96ms
step:307/2330 train_time:17487ms step_avg:56.96ms
step:308/2330 train_time:17546ms step_avg:56.97ms
step:309/2330 train_time:17603ms step_avg:56.97ms
step:310/2330 train_time:17661ms step_avg:56.97ms
step:311/2330 train_time:17717ms step_avg:56.97ms
step:312/2330 train_time:17776ms step_avg:56.97ms
step:313/2330 train_time:17831ms step_avg:56.97ms
step:314/2330 train_time:17890ms step_avg:56.97ms
step:315/2330 train_time:17946ms step_avg:56.97ms
step:316/2330 train_time:18005ms step_avg:56.98ms
step:317/2330 train_time:18061ms step_avg:56.98ms
step:318/2330 train_time:18119ms step_avg:56.98ms
step:319/2330 train_time:18174ms step_avg:56.97ms
step:320/2330 train_time:18233ms step_avg:56.98ms
step:321/2330 train_time:18289ms step_avg:56.97ms
step:322/2330 train_time:18347ms step_avg:56.98ms
step:323/2330 train_time:18402ms step_avg:56.97ms
step:324/2330 train_time:18462ms step_avg:56.98ms
step:325/2330 train_time:18517ms step_avg:56.98ms
step:326/2330 train_time:18576ms step_avg:56.98ms
step:327/2330 train_time:18631ms step_avg:56.98ms
step:328/2330 train_time:18691ms step_avg:56.99ms
step:329/2330 train_time:18746ms step_avg:56.98ms
step:330/2330 train_time:18805ms step_avg:56.99ms
step:331/2330 train_time:18861ms step_avg:56.98ms
step:332/2330 train_time:18920ms step_avg:56.99ms
step:333/2330 train_time:18975ms step_avg:56.98ms
step:334/2330 train_time:19035ms step_avg:56.99ms
step:335/2330 train_time:19090ms step_avg:56.98ms
step:336/2330 train_time:19148ms step_avg:56.99ms
step:337/2330 train_time:19204ms step_avg:56.99ms
step:338/2330 train_time:19263ms step_avg:56.99ms
step:339/2330 train_time:19319ms step_avg:56.99ms
step:340/2330 train_time:19377ms step_avg:56.99ms
step:341/2330 train_time:19433ms step_avg:56.99ms
step:342/2330 train_time:19492ms step_avg:56.99ms
step:343/2330 train_time:19548ms step_avg:56.99ms
step:344/2330 train_time:19607ms step_avg:57.00ms
step:345/2330 train_time:19662ms step_avg:56.99ms
step:346/2330 train_time:19721ms step_avg:57.00ms
step:347/2330 train_time:19776ms step_avg:56.99ms
step:348/2330 train_time:19834ms step_avg:57.00ms
step:349/2330 train_time:19890ms step_avg:56.99ms
step:350/2330 train_time:19949ms step_avg:57.00ms
step:351/2330 train_time:20004ms step_avg:56.99ms
step:352/2330 train_time:20063ms step_avg:57.00ms
step:353/2330 train_time:20119ms step_avg:56.99ms
step:354/2330 train_time:20177ms step_avg:57.00ms
step:355/2330 train_time:20232ms step_avg:56.99ms
step:356/2330 train_time:20291ms step_avg:57.00ms
step:357/2330 train_time:20347ms step_avg:56.99ms
step:358/2330 train_time:20406ms step_avg:57.00ms
step:359/2330 train_time:20462ms step_avg:57.00ms
step:360/2330 train_time:20521ms step_avg:57.00ms
step:361/2330 train_time:20576ms step_avg:57.00ms
step:362/2330 train_time:20635ms step_avg:57.00ms
step:363/2330 train_time:20690ms step_avg:57.00ms
step:364/2330 train_time:20750ms step_avg:57.00ms
step:365/2330 train_time:20805ms step_avg:57.00ms
step:366/2330 train_time:20864ms step_avg:57.01ms
step:367/2330 train_time:20920ms step_avg:57.00ms
step:368/2330 train_time:20980ms step_avg:57.01ms
step:369/2330 train_time:21035ms step_avg:57.01ms
step:370/2330 train_time:21094ms step_avg:57.01ms
step:371/2330 train_time:21149ms step_avg:57.01ms
step:372/2330 train_time:21208ms step_avg:57.01ms
step:373/2330 train_time:21263ms step_avg:57.01ms
step:374/2330 train_time:21321ms step_avg:57.01ms
step:375/2330 train_time:21377ms step_avg:57.01ms
step:376/2330 train_time:21435ms step_avg:57.01ms
step:377/2330 train_time:21491ms step_avg:57.00ms
step:378/2330 train_time:21550ms step_avg:57.01ms
step:379/2330 train_time:21606ms step_avg:57.01ms
step:380/2330 train_time:21664ms step_avg:57.01ms
step:381/2330 train_time:21720ms step_avg:57.01ms
step:382/2330 train_time:21779ms step_avg:57.01ms
step:383/2330 train_time:21835ms step_avg:57.01ms
step:384/2330 train_time:21894ms step_avg:57.01ms
step:385/2330 train_time:21949ms step_avg:57.01ms
step:386/2330 train_time:22008ms step_avg:57.02ms
step:387/2330 train_time:22064ms step_avg:57.01ms
step:388/2330 train_time:22123ms step_avg:57.02ms
step:389/2330 train_time:22178ms step_avg:57.01ms
step:390/2330 train_time:22237ms step_avg:57.02ms
step:391/2330 train_time:22292ms step_avg:57.01ms
step:392/2330 train_time:22351ms step_avg:57.02ms
step:393/2330 train_time:22407ms step_avg:57.02ms
step:394/2330 train_time:22466ms step_avg:57.02ms
step:395/2330 train_time:22521ms step_avg:57.02ms
step:396/2330 train_time:22581ms step_avg:57.02ms
step:397/2330 train_time:22637ms step_avg:57.02ms
step:398/2330 train_time:22696ms step_avg:57.02ms
step:399/2330 train_time:22751ms step_avg:57.02ms
step:400/2330 train_time:22810ms step_avg:57.02ms
step:401/2330 train_time:22866ms step_avg:57.02ms
step:402/2330 train_time:22925ms step_avg:57.03ms
step:403/2330 train_time:22980ms step_avg:57.02ms
step:404/2330 train_time:23040ms step_avg:57.03ms
step:405/2330 train_time:23096ms step_avg:57.03ms
step:406/2330 train_time:23155ms step_avg:57.03ms
step:407/2330 train_time:23210ms step_avg:57.03ms
step:408/2330 train_time:23269ms step_avg:57.03ms
step:409/2330 train_time:23324ms step_avg:57.03ms
step:410/2330 train_time:23384ms step_avg:57.03ms
step:411/2330 train_time:23439ms step_avg:57.03ms
step:412/2330 train_time:23499ms step_avg:57.04ms
step:413/2330 train_time:23554ms step_avg:57.03ms
step:414/2330 train_time:23613ms step_avg:57.04ms
step:415/2330 train_time:23668ms step_avg:57.03ms
step:416/2330 train_time:23727ms step_avg:57.04ms
step:417/2330 train_time:23783ms step_avg:57.03ms
step:418/2330 train_time:23842ms step_avg:57.04ms
step:419/2330 train_time:23898ms step_avg:57.04ms
step:420/2330 train_time:23956ms step_avg:57.04ms
step:421/2330 train_time:24012ms step_avg:57.03ms
step:422/2330 train_time:24071ms step_avg:57.04ms
step:423/2330 train_time:24127ms step_avg:57.04ms
step:424/2330 train_time:24185ms step_avg:57.04ms
step:425/2330 train_time:24242ms step_avg:57.04ms
step:426/2330 train_time:24300ms step_avg:57.04ms
step:427/2330 train_time:24356ms step_avg:57.04ms
step:428/2330 train_time:24414ms step_avg:57.04ms
step:429/2330 train_time:24470ms step_avg:57.04ms
step:430/2330 train_time:24528ms step_avg:57.04ms
step:431/2330 train_time:24584ms step_avg:57.04ms
step:432/2330 train_time:24643ms step_avg:57.04ms
step:433/2330 train_time:24698ms step_avg:57.04ms
step:434/2330 train_time:24756ms step_avg:57.04ms
step:435/2330 train_time:24812ms step_avg:57.04ms
step:436/2330 train_time:24872ms step_avg:57.05ms
step:437/2330 train_time:24928ms step_avg:57.04ms
step:438/2330 train_time:24987ms step_avg:57.05ms
step:439/2330 train_time:25042ms step_avg:57.04ms
step:440/2330 train_time:25101ms step_avg:57.05ms
step:441/2330 train_time:25156ms step_avg:57.04ms
step:442/2330 train_time:25214ms step_avg:57.05ms
step:443/2330 train_time:25270ms step_avg:57.04ms
step:444/2330 train_time:25329ms step_avg:57.05ms
step:445/2330 train_time:25385ms step_avg:57.05ms
step:446/2330 train_time:25444ms step_avg:57.05ms
step:447/2330 train_time:25500ms step_avg:57.05ms
step:448/2330 train_time:25559ms step_avg:57.05ms
step:449/2330 train_time:25614ms step_avg:57.05ms
step:450/2330 train_time:25673ms step_avg:57.05ms
step:451/2330 train_time:25728ms step_avg:57.05ms
step:452/2330 train_time:25787ms step_avg:57.05ms
step:453/2330 train_time:25844ms step_avg:57.05ms
step:454/2330 train_time:25903ms step_avg:57.05ms
step:455/2330 train_time:25958ms step_avg:57.05ms
step:456/2330 train_time:26016ms step_avg:57.05ms
step:457/2330 train_time:26072ms step_avg:57.05ms
step:458/2330 train_time:26131ms step_avg:57.05ms
step:459/2330 train_time:26187ms step_avg:57.05ms
step:460/2330 train_time:26246ms step_avg:57.06ms
step:461/2330 train_time:26302ms step_avg:57.05ms
step:462/2330 train_time:26360ms step_avg:57.06ms
step:463/2330 train_time:26415ms step_avg:57.05ms
step:464/2330 train_time:26474ms step_avg:57.06ms
step:465/2330 train_time:26530ms step_avg:57.05ms
step:466/2330 train_time:26590ms step_avg:57.06ms
step:467/2330 train_time:26646ms step_avg:57.06ms
step:468/2330 train_time:26704ms step_avg:57.06ms
step:469/2330 train_time:26760ms step_avg:57.06ms
step:470/2330 train_time:26818ms step_avg:57.06ms
step:471/2330 train_time:26874ms step_avg:57.06ms
step:472/2330 train_time:26933ms step_avg:57.06ms
step:473/2330 train_time:26989ms step_avg:57.06ms
step:474/2330 train_time:27047ms step_avg:57.06ms
step:475/2330 train_time:27103ms step_avg:57.06ms
step:476/2330 train_time:27163ms step_avg:57.06ms
step:477/2330 train_time:27218ms step_avg:57.06ms
step:478/2330 train_time:27277ms step_avg:57.06ms
step:479/2330 train_time:27333ms step_avg:57.06ms
step:480/2330 train_time:27392ms step_avg:57.07ms
step:481/2330 train_time:27448ms step_avg:57.07ms
step:482/2330 train_time:27507ms step_avg:57.07ms
step:483/2330 train_time:27562ms step_avg:57.06ms
step:484/2330 train_time:27621ms step_avg:57.07ms
step:485/2330 train_time:27676ms step_avg:57.06ms
step:486/2330 train_time:27735ms step_avg:57.07ms
step:487/2330 train_time:27791ms step_avg:57.07ms
step:488/2330 train_time:27849ms step_avg:57.07ms
step:489/2330 train_time:27905ms step_avg:57.07ms
step:490/2330 train_time:27964ms step_avg:57.07ms
step:491/2330 train_time:28019ms step_avg:57.07ms
step:492/2330 train_time:28079ms step_avg:57.07ms
step:493/2330 train_time:28134ms step_avg:57.07ms
step:494/2330 train_time:28193ms step_avg:57.07ms
step:495/2330 train_time:28250ms step_avg:57.07ms
step:496/2330 train_time:28308ms step_avg:57.07ms
step:497/2330 train_time:28363ms step_avg:57.07ms
step:498/2330 train_time:28423ms step_avg:57.07ms
step:499/2330 train_time:28478ms step_avg:57.07ms
step:500/2330 train_time:28538ms step_avg:57.08ms
step:500/2330 val_loss:5.2722 train_time:28616ms step_avg:57.23ms
step:501/2330 train_time:28634ms step_avg:57.15ms
step:502/2330 train_time:28656ms step_avg:57.08ms
step:503/2330 train_time:28713ms step_avg:57.08ms
step:504/2330 train_time:28775ms step_avg:57.09ms
step:505/2330 train_time:28832ms step_avg:57.09ms
step:506/2330 train_time:28895ms step_avg:57.10ms
step:507/2330 train_time:28950ms step_avg:57.10ms
step:508/2330 train_time:29010ms step_avg:57.11ms
step:509/2330 train_time:29065ms step_avg:57.10ms
step:510/2330 train_time:29125ms step_avg:57.11ms
step:511/2330 train_time:29180ms step_avg:57.10ms
step:512/2330 train_time:29238ms step_avg:57.11ms
step:513/2330 train_time:29293ms step_avg:57.10ms
step:514/2330 train_time:29351ms step_avg:57.10ms
step:515/2330 train_time:29407ms step_avg:57.10ms
step:516/2330 train_time:29465ms step_avg:57.10ms
step:517/2330 train_time:29520ms step_avg:57.10ms
step:518/2330 train_time:29578ms step_avg:57.10ms
step:519/2330 train_time:29634ms step_avg:57.10ms
step:520/2330 train_time:29694ms step_avg:57.10ms
step:521/2330 train_time:29751ms step_avg:57.10ms
step:522/2330 train_time:29812ms step_avg:57.11ms
step:523/2330 train_time:29868ms step_avg:57.11ms
step:524/2330 train_time:29930ms step_avg:57.12ms
step:525/2330 train_time:29987ms step_avg:57.12ms
step:526/2330 train_time:30045ms step_avg:57.12ms
step:527/2330 train_time:30101ms step_avg:57.12ms
step:528/2330 train_time:30158ms step_avg:57.12ms
step:529/2330 train_time:30214ms step_avg:57.11ms
step:530/2330 train_time:30272ms step_avg:57.12ms
step:531/2330 train_time:30328ms step_avg:57.11ms
step:532/2330 train_time:30386ms step_avg:57.12ms
step:533/2330 train_time:30442ms step_avg:57.11ms
step:534/2330 train_time:30500ms step_avg:57.12ms
step:535/2330 train_time:30555ms step_avg:57.11ms
step:536/2330 train_time:30613ms step_avg:57.11ms
step:537/2330 train_time:30668ms step_avg:57.11ms
step:538/2330 train_time:30730ms step_avg:57.12ms
step:539/2330 train_time:30786ms step_avg:57.12ms
step:540/2330 train_time:30847ms step_avg:57.12ms
step:541/2330 train_time:30903ms step_avg:57.12ms
step:542/2330 train_time:30964ms step_avg:57.13ms
step:543/2330 train_time:31020ms step_avg:57.13ms
step:544/2330 train_time:31078ms step_avg:57.13ms
step:545/2330 train_time:31134ms step_avg:57.13ms
step:546/2330 train_time:31192ms step_avg:57.13ms
step:547/2330 train_time:31248ms step_avg:57.13ms
step:548/2330 train_time:31307ms step_avg:57.13ms
step:549/2330 train_time:31362ms step_avg:57.13ms
step:550/2330 train_time:31421ms step_avg:57.13ms
step:551/2330 train_time:31476ms step_avg:57.13ms
step:552/2330 train_time:31534ms step_avg:57.13ms
step:553/2330 train_time:31590ms step_avg:57.12ms
step:554/2330 train_time:31648ms step_avg:57.13ms
step:555/2330 train_time:31704ms step_avg:57.12ms
step:556/2330 train_time:31763ms step_avg:57.13ms
step:557/2330 train_time:31819ms step_avg:57.13ms
step:558/2330 train_time:31879ms step_avg:57.13ms
step:559/2330 train_time:31935ms step_avg:57.13ms
step:560/2330 train_time:31994ms step_avg:57.13ms
step:561/2330 train_time:32050ms step_avg:57.13ms
step:562/2330 train_time:32109ms step_avg:57.13ms
step:563/2330 train_time:32165ms step_avg:57.13ms
step:564/2330 train_time:32224ms step_avg:57.13ms
step:565/2330 train_time:32280ms step_avg:57.13ms
step:566/2330 train_time:32338ms step_avg:57.13ms
step:567/2330 train_time:32393ms step_avg:57.13ms
step:568/2330 train_time:32452ms step_avg:57.13ms
step:569/2330 train_time:32508ms step_avg:57.13ms
step:570/2330 train_time:32566ms step_avg:57.13ms
step:571/2330 train_time:32622ms step_avg:57.13ms
step:572/2330 train_time:32681ms step_avg:57.13ms
step:573/2330 train_time:32736ms step_avg:57.13ms
step:574/2330 train_time:32796ms step_avg:57.14ms
step:575/2330 train_time:32852ms step_avg:57.13ms
step:576/2330 train_time:32911ms step_avg:57.14ms
step:577/2330 train_time:32967ms step_avg:57.13ms
step:578/2330 train_time:33026ms step_avg:57.14ms
step:579/2330 train_time:33081ms step_avg:57.14ms
step:580/2330 train_time:33140ms step_avg:57.14ms
step:581/2330 train_time:33196ms step_avg:57.14ms
step:582/2330 train_time:33255ms step_avg:57.14ms
step:583/2330 train_time:33311ms step_avg:57.14ms
step:584/2330 train_time:33370ms step_avg:57.14ms
step:585/2330 train_time:33426ms step_avg:57.14ms
step:586/2330 train_time:33485ms step_avg:57.14ms
step:587/2330 train_time:33540ms step_avg:57.14ms
step:588/2330 train_time:33599ms step_avg:57.14ms
step:589/2330 train_time:33654ms step_avg:57.14ms
step:590/2330 train_time:33714ms step_avg:57.14ms
step:591/2330 train_time:33769ms step_avg:57.14ms
step:592/2330 train_time:33829ms step_avg:57.14ms
step:593/2330 train_time:33885ms step_avg:57.14ms
step:594/2330 train_time:33944ms step_avg:57.14ms
step:595/2330 train_time:34000ms step_avg:57.14ms
step:596/2330 train_time:34058ms step_avg:57.14ms
step:597/2330 train_time:34114ms step_avg:57.14ms
step:598/2330 train_time:34173ms step_avg:57.14ms
step:599/2330 train_time:34228ms step_avg:57.14ms
step:600/2330 train_time:34288ms step_avg:57.15ms
step:601/2330 train_time:34343ms step_avg:57.14ms
step:602/2330 train_time:34402ms step_avg:57.15ms
step:603/2330 train_time:34458ms step_avg:57.14ms
step:604/2330 train_time:34517ms step_avg:57.15ms
step:605/2330 train_time:34573ms step_avg:57.15ms
step:606/2330 train_time:34632ms step_avg:57.15ms
step:607/2330 train_time:34688ms step_avg:57.15ms
step:608/2330 train_time:34746ms step_avg:57.15ms
step:609/2330 train_time:34802ms step_avg:57.15ms
step:610/2330 train_time:34861ms step_avg:57.15ms
step:611/2330 train_time:34917ms step_avg:57.15ms
step:612/2330 train_time:34975ms step_avg:57.15ms
step:613/2330 train_time:35031ms step_avg:57.15ms
step:614/2330 train_time:35090ms step_avg:57.15ms
step:615/2330 train_time:35146ms step_avg:57.15ms
step:616/2330 train_time:35205ms step_avg:57.15ms
step:617/2330 train_time:35261ms step_avg:57.15ms
step:618/2330 train_time:35319ms step_avg:57.15ms
step:619/2330 train_time:35374ms step_avg:57.15ms
step:620/2330 train_time:35434ms step_avg:57.15ms
step:621/2330 train_time:35490ms step_avg:57.15ms
step:622/2330 train_time:35548ms step_avg:57.15ms
step:623/2330 train_time:35604ms step_avg:57.15ms
step:624/2330 train_time:35663ms step_avg:57.15ms
step:625/2330 train_time:35718ms step_avg:57.15ms
step:626/2330 train_time:35778ms step_avg:57.15ms
step:627/2330 train_time:35833ms step_avg:57.15ms
step:628/2330 train_time:35892ms step_avg:57.15ms
step:629/2330 train_time:35948ms step_avg:57.15ms
step:630/2330 train_time:36007ms step_avg:57.15ms
step:631/2330 train_time:36062ms step_avg:57.15ms
step:632/2330 train_time:36121ms step_avg:57.15ms
step:633/2330 train_time:36176ms step_avg:57.15ms
step:634/2330 train_time:36236ms step_avg:57.15ms
step:635/2330 train_time:36291ms step_avg:57.15ms
step:636/2330 train_time:36350ms step_avg:57.15ms
step:637/2330 train_time:36406ms step_avg:57.15ms
step:638/2330 train_time:36466ms step_avg:57.16ms
step:639/2330 train_time:36522ms step_avg:57.15ms
step:640/2330 train_time:36580ms step_avg:57.16ms
step:641/2330 train_time:36635ms step_avg:57.15ms
step:642/2330 train_time:36695ms step_avg:57.16ms
step:643/2330 train_time:36750ms step_avg:57.15ms
step:644/2330 train_time:36809ms step_avg:57.16ms
step:645/2330 train_time:36865ms step_avg:57.15ms
step:646/2330 train_time:36924ms step_avg:57.16ms
step:647/2330 train_time:36979ms step_avg:57.16ms
step:648/2330 train_time:37038ms step_avg:57.16ms
step:649/2330 train_time:37094ms step_avg:57.16ms
step:650/2330 train_time:37152ms step_avg:57.16ms
step:651/2330 train_time:37208ms step_avg:57.16ms
step:652/2330 train_time:37267ms step_avg:57.16ms
step:653/2330 train_time:37323ms step_avg:57.16ms
step:654/2330 train_time:37382ms step_avg:57.16ms
step:655/2330 train_time:37438ms step_avg:57.16ms
step:656/2330 train_time:37496ms step_avg:57.16ms
step:657/2330 train_time:37551ms step_avg:57.16ms
step:658/2330 train_time:37611ms step_avg:57.16ms
step:659/2330 train_time:37667ms step_avg:57.16ms
step:660/2330 train_time:37725ms step_avg:57.16ms
step:661/2330 train_time:37781ms step_avg:57.16ms
step:662/2330 train_time:37840ms step_avg:57.16ms
step:663/2330 train_time:37895ms step_avg:57.16ms
step:664/2330 train_time:37954ms step_avg:57.16ms
step:665/2330 train_time:38010ms step_avg:57.16ms
step:666/2330 train_time:38069ms step_avg:57.16ms
step:667/2330 train_time:38126ms step_avg:57.16ms
step:668/2330 train_time:38185ms step_avg:57.16ms
step:669/2330 train_time:38241ms step_avg:57.16ms
step:670/2330 train_time:38299ms step_avg:57.16ms
step:671/2330 train_time:38354ms step_avg:57.16ms
step:672/2330 train_time:38413ms step_avg:57.16ms
step:673/2330 train_time:38469ms step_avg:57.16ms
step:674/2330 train_time:38527ms step_avg:57.16ms
step:675/2330 train_time:38584ms step_avg:57.16ms
step:676/2330 train_time:38642ms step_avg:57.16ms
step:677/2330 train_time:38698ms step_avg:57.16ms
step:678/2330 train_time:38757ms step_avg:57.16ms
step:679/2330 train_time:38813ms step_avg:57.16ms
step:680/2330 train_time:38872ms step_avg:57.16ms
step:681/2330 train_time:38928ms step_avg:57.16ms
step:682/2330 train_time:38987ms step_avg:57.17ms
step:683/2330 train_time:39043ms step_avg:57.16ms
step:684/2330 train_time:39102ms step_avg:57.17ms
step:685/2330 train_time:39158ms step_avg:57.16ms
step:686/2330 train_time:39217ms step_avg:57.17ms
step:687/2330 train_time:39272ms step_avg:57.16ms
step:688/2330 train_time:39332ms step_avg:57.17ms
step:689/2330 train_time:39387ms step_avg:57.17ms
step:690/2330 train_time:39446ms step_avg:57.17ms
step:691/2330 train_time:39501ms step_avg:57.17ms
step:692/2330 train_time:39560ms step_avg:57.17ms
step:693/2330 train_time:39616ms step_avg:57.17ms
step:694/2330 train_time:39675ms step_avg:57.17ms
step:695/2330 train_time:39730ms step_avg:57.17ms
step:696/2330 train_time:39790ms step_avg:57.17ms
step:697/2330 train_time:39846ms step_avg:57.17ms
step:698/2330 train_time:39904ms step_avg:57.17ms
step:699/2330 train_time:39960ms step_avg:57.17ms
step:700/2330 train_time:40019ms step_avg:57.17ms
step:701/2330 train_time:40075ms step_avg:57.17ms
step:702/2330 train_time:40134ms step_avg:57.17ms
step:703/2330 train_time:40190ms step_avg:57.17ms
step:704/2330 train_time:40248ms step_avg:57.17ms
step:705/2330 train_time:40304ms step_avg:57.17ms
step:706/2330 train_time:40363ms step_avg:57.17ms
step:707/2330 train_time:40419ms step_avg:57.17ms
step:708/2330 train_time:40477ms step_avg:57.17ms
step:709/2330 train_time:40533ms step_avg:57.17ms
step:710/2330 train_time:40593ms step_avg:57.17ms
step:711/2330 train_time:40648ms step_avg:57.17ms
step:712/2330 train_time:40707ms step_avg:57.17ms
step:713/2330 train_time:40763ms step_avg:57.17ms
step:714/2330 train_time:40822ms step_avg:57.17ms
step:715/2330 train_time:40878ms step_avg:57.17ms
step:716/2330 train_time:40938ms step_avg:57.18ms
step:717/2330 train_time:40994ms step_avg:57.17ms
step:718/2330 train_time:41053ms step_avg:57.18ms
step:719/2330 train_time:41109ms step_avg:57.17ms
step:720/2330 train_time:41168ms step_avg:57.18ms
step:721/2330 train_time:41224ms step_avg:57.18ms
step:722/2330 train_time:41283ms step_avg:57.18ms
step:723/2330 train_time:41339ms step_avg:57.18ms
step:724/2330 train_time:41398ms step_avg:57.18ms
step:725/2330 train_time:41453ms step_avg:57.18ms
step:726/2330 train_time:41513ms step_avg:57.18ms
step:727/2330 train_time:41569ms step_avg:57.18ms
step:728/2330 train_time:41628ms step_avg:57.18ms
step:729/2330 train_time:41683ms step_avg:57.18ms
step:730/2330 train_time:41742ms step_avg:57.18ms
step:731/2330 train_time:41798ms step_avg:57.18ms
step:732/2330 train_time:41856ms step_avg:57.18ms
step:733/2330 train_time:41912ms step_avg:57.18ms
step:734/2330 train_time:41971ms step_avg:57.18ms
step:735/2330 train_time:42028ms step_avg:57.18ms
step:736/2330 train_time:42087ms step_avg:57.18ms
step:737/2330 train_time:42142ms step_avg:57.18ms
step:738/2330 train_time:42201ms step_avg:57.18ms
step:739/2330 train_time:42257ms step_avg:57.18ms
step:740/2330 train_time:42315ms step_avg:57.18ms
step:741/2330 train_time:42370ms step_avg:57.18ms
step:742/2330 train_time:42430ms step_avg:57.18ms
step:743/2330 train_time:42486ms step_avg:57.18ms
step:744/2330 train_time:42545ms step_avg:57.18ms
step:745/2330 train_time:42600ms step_avg:57.18ms
step:746/2330 train_time:42659ms step_avg:57.18ms
step:747/2330 train_time:42714ms step_avg:57.18ms
step:748/2330 train_time:42773ms step_avg:57.18ms
step:749/2330 train_time:42829ms step_avg:57.18ms
step:750/2330 train_time:42888ms step_avg:57.18ms
step:750/2330 val_loss:4.8204 train_time:42969ms step_avg:57.29ms
step:751/2330 train_time:42986ms step_avg:57.24ms
step:752/2330 train_time:43009ms step_avg:57.19ms
step:753/2330 train_time:43066ms step_avg:57.19ms
step:754/2330 train_time:43127ms step_avg:57.20ms
step:755/2330 train_time:43184ms step_avg:57.20ms
step:756/2330 train_time:43243ms step_avg:57.20ms
step:757/2330 train_time:43299ms step_avg:57.20ms
step:758/2330 train_time:43357ms step_avg:57.20ms
step:759/2330 train_time:43412ms step_avg:57.20ms
step:760/2330 train_time:43471ms step_avg:57.20ms
step:761/2330 train_time:43526ms step_avg:57.20ms
step:762/2330 train_time:43585ms step_avg:57.20ms
step:763/2330 train_time:43640ms step_avg:57.20ms
step:764/2330 train_time:43697ms step_avg:57.20ms
step:765/2330 train_time:43754ms step_avg:57.19ms
step:766/2330 train_time:43811ms step_avg:57.19ms
step:767/2330 train_time:43867ms step_avg:57.19ms
step:768/2330 train_time:43927ms step_avg:57.20ms
step:769/2330 train_time:43984ms step_avg:57.20ms
step:770/2330 train_time:44044ms step_avg:57.20ms
step:771/2330 train_time:44102ms step_avg:57.20ms
step:772/2330 train_time:44163ms step_avg:57.21ms
step:773/2330 train_time:44221ms step_avg:57.21ms
step:774/2330 train_time:44280ms step_avg:57.21ms
step:775/2330 train_time:44337ms step_avg:57.21ms
step:776/2330 train_time:44396ms step_avg:57.21ms
step:777/2330 train_time:44452ms step_avg:57.21ms
step:778/2330 train_time:44512ms step_avg:57.21ms
step:779/2330 train_time:44570ms step_avg:57.21ms
step:780/2330 train_time:44629ms step_avg:57.22ms
step:781/2330 train_time:44686ms step_avg:57.22ms
step:782/2330 train_time:44745ms step_avg:57.22ms
step:783/2330 train_time:44801ms step_avg:57.22ms
step:784/2330 train_time:44860ms step_avg:57.22ms
step:785/2330 train_time:44917ms step_avg:57.22ms
step:786/2330 train_time:44976ms step_avg:57.22ms
step:787/2330 train_time:45034ms step_avg:57.22ms
step:788/2330 train_time:45094ms step_avg:57.23ms
step:789/2330 train_time:45151ms step_avg:57.23ms
step:790/2330 train_time:45211ms step_avg:57.23ms
step:791/2330 train_time:45268ms step_avg:57.23ms
step:792/2330 train_time:45328ms step_avg:57.23ms
step:793/2330 train_time:45385ms step_avg:57.23ms
step:794/2330 train_time:45444ms step_avg:57.23ms
step:795/2330 train_time:45501ms step_avg:57.23ms
step:796/2330 train_time:45559ms step_avg:57.24ms
step:797/2330 train_time:45616ms step_avg:57.24ms
step:798/2330 train_time:45676ms step_avg:57.24ms
step:799/2330 train_time:45732ms step_avg:57.24ms
step:800/2330 train_time:45791ms step_avg:57.24ms
step:801/2330 train_time:45848ms step_avg:57.24ms
step:802/2330 train_time:45907ms step_avg:57.24ms
step:803/2330 train_time:45964ms step_avg:57.24ms
step:804/2330 train_time:46023ms step_avg:57.24ms
step:805/2330 train_time:46080ms step_avg:57.24ms
step:806/2330 train_time:46139ms step_avg:57.24ms
step:807/2330 train_time:46195ms step_avg:57.24ms
step:808/2330 train_time:46256ms step_avg:57.25ms
step:809/2330 train_time:46313ms step_avg:57.25ms
step:810/2330 train_time:46373ms step_avg:57.25ms
step:811/2330 train_time:46430ms step_avg:57.25ms
step:812/2330 train_time:46489ms step_avg:57.25ms
step:813/2330 train_time:46546ms step_avg:57.25ms
step:814/2330 train_time:46606ms step_avg:57.26ms
step:815/2330 train_time:46662ms step_avg:57.25ms
step:816/2330 train_time:46721ms step_avg:57.26ms
step:817/2330 train_time:46777ms step_avg:57.25ms
step:818/2330 train_time:46836ms step_avg:57.26ms
step:819/2330 train_time:46893ms step_avg:57.26ms
step:820/2330 train_time:46952ms step_avg:57.26ms
step:821/2330 train_time:47009ms step_avg:57.26ms
step:822/2330 train_time:47069ms step_avg:57.26ms
step:823/2330 train_time:47126ms step_avg:57.26ms
step:824/2330 train_time:47187ms step_avg:57.27ms
step:825/2330 train_time:47243ms step_avg:57.26ms
step:826/2330 train_time:47303ms step_avg:57.27ms
step:827/2330 train_time:47360ms step_avg:57.27ms
step:828/2330 train_time:47419ms step_avg:57.27ms
step:829/2330 train_time:47476ms step_avg:57.27ms
step:830/2330 train_time:47536ms step_avg:57.27ms
step:831/2330 train_time:47593ms step_avg:57.27ms
step:832/2330 train_time:47651ms step_avg:57.27ms
step:833/2330 train_time:47708ms step_avg:57.27ms
step:834/2330 train_time:47767ms step_avg:57.27ms
step:835/2330 train_time:47824ms step_avg:57.27ms
step:836/2330 train_time:47884ms step_avg:57.28ms
step:837/2330 train_time:47940ms step_avg:57.28ms
step:838/2330 train_time:47999ms step_avg:57.28ms
step:839/2330 train_time:48056ms step_avg:57.28ms
step:840/2330 train_time:48116ms step_avg:57.28ms
step:841/2330 train_time:48173ms step_avg:57.28ms
step:842/2330 train_time:48233ms step_avg:57.28ms
step:843/2330 train_time:48290ms step_avg:57.28ms
step:844/2330 train_time:48350ms step_avg:57.29ms
step:845/2330 train_time:48407ms step_avg:57.29ms
step:846/2330 train_time:48466ms step_avg:57.29ms
step:847/2330 train_time:48523ms step_avg:57.29ms
step:848/2330 train_time:48582ms step_avg:57.29ms
step:849/2330 train_time:48639ms step_avg:57.29ms
step:850/2330 train_time:48697ms step_avg:57.29ms
step:851/2330 train_time:48754ms step_avg:57.29ms
step:852/2330 train_time:48813ms step_avg:57.29ms
step:853/2330 train_time:48869ms step_avg:57.29ms
step:854/2330 train_time:48931ms step_avg:57.30ms
step:855/2330 train_time:48987ms step_avg:57.30ms
step:856/2330 train_time:49046ms step_avg:57.30ms
step:857/2330 train_time:49103ms step_avg:57.30ms
step:858/2330 train_time:49163ms step_avg:57.30ms
step:859/2330 train_time:49219ms step_avg:57.30ms
step:860/2330 train_time:49278ms step_avg:57.30ms
step:861/2330 train_time:49335ms step_avg:57.30ms
step:862/2330 train_time:49395ms step_avg:57.30ms
step:863/2330 train_time:49453ms step_avg:57.30ms
step:864/2330 train_time:49512ms step_avg:57.31ms
step:865/2330 train_time:49569ms step_avg:57.31ms
step:866/2330 train_time:49629ms step_avg:57.31ms
step:867/2330 train_time:49686ms step_avg:57.31ms
step:868/2330 train_time:49745ms step_avg:57.31ms
step:869/2330 train_time:49801ms step_avg:57.31ms
step:870/2330 train_time:49860ms step_avg:57.31ms
step:871/2330 train_time:49916ms step_avg:57.31ms
step:872/2330 train_time:49976ms step_avg:57.31ms
step:873/2330 train_time:50032ms step_avg:57.31ms
step:874/2330 train_time:50093ms step_avg:57.31ms
step:875/2330 train_time:50150ms step_avg:57.31ms
step:876/2330 train_time:50210ms step_avg:57.32ms
step:877/2330 train_time:50267ms step_avg:57.32ms
step:878/2330 train_time:50326ms step_avg:57.32ms
step:879/2330 train_time:50383ms step_avg:57.32ms
step:880/2330 train_time:50443ms step_avg:57.32ms
step:881/2330 train_time:50499ms step_avg:57.32ms
step:882/2330 train_time:50559ms step_avg:57.32ms
step:883/2330 train_time:50616ms step_avg:57.32ms
step:884/2330 train_time:50676ms step_avg:57.33ms
step:885/2330 train_time:50732ms step_avg:57.32ms
step:886/2330 train_time:50792ms step_avg:57.33ms
step:887/2330 train_time:50849ms step_avg:57.33ms
step:888/2330 train_time:50908ms step_avg:57.33ms
step:889/2330 train_time:50965ms step_avg:57.33ms
step:890/2330 train_time:51023ms step_avg:57.33ms
step:891/2330 train_time:51081ms step_avg:57.33ms
step:892/2330 train_time:51139ms step_avg:57.33ms
step:893/2330 train_time:51195ms step_avg:57.33ms
step:894/2330 train_time:51255ms step_avg:57.33ms
step:895/2330 train_time:51312ms step_avg:57.33ms
step:896/2330 train_time:51372ms step_avg:57.34ms
step:897/2330 train_time:51430ms step_avg:57.34ms
step:898/2330 train_time:51490ms step_avg:57.34ms
step:899/2330 train_time:51547ms step_avg:57.34ms
step:900/2330 train_time:51606ms step_avg:57.34ms
step:901/2330 train_time:51663ms step_avg:57.34ms
step:902/2330 train_time:51723ms step_avg:57.34ms
step:903/2330 train_time:51779ms step_avg:57.34ms
step:904/2330 train_time:51838ms step_avg:57.34ms
step:905/2330 train_time:51894ms step_avg:57.34ms
step:906/2330 train_time:51955ms step_avg:57.35ms
step:907/2330 train_time:52011ms step_avg:57.34ms
step:908/2330 train_time:52071ms step_avg:57.35ms
step:909/2330 train_time:52128ms step_avg:57.35ms
step:910/2330 train_time:52187ms step_avg:57.35ms
step:911/2330 train_time:52244ms step_avg:57.35ms
step:912/2330 train_time:52302ms step_avg:57.35ms
step:913/2330 train_time:52358ms step_avg:57.35ms
step:914/2330 train_time:52418ms step_avg:57.35ms
step:915/2330 train_time:52474ms step_avg:57.35ms
step:916/2330 train_time:52533ms step_avg:57.35ms
step:917/2330 train_time:52591ms step_avg:57.35ms
step:918/2330 train_time:52651ms step_avg:57.35ms
step:919/2330 train_time:52708ms step_avg:57.35ms
step:920/2330 train_time:52768ms step_avg:57.36ms
step:921/2330 train_time:52826ms step_avg:57.36ms
step:922/2330 train_time:52885ms step_avg:57.36ms
step:923/2330 train_time:52941ms step_avg:57.36ms
step:924/2330 train_time:53000ms step_avg:57.36ms
step:925/2330 train_time:53056ms step_avg:57.36ms
step:926/2330 train_time:53116ms step_avg:57.36ms
step:927/2330 train_time:53173ms step_avg:57.36ms
step:928/2330 train_time:53232ms step_avg:57.36ms
step:929/2330 train_time:53289ms step_avg:57.36ms
step:930/2330 train_time:53348ms step_avg:57.36ms
step:931/2330 train_time:53404ms step_avg:57.36ms
step:932/2330 train_time:53464ms step_avg:57.37ms
step:933/2330 train_time:53520ms step_avg:57.36ms
step:934/2330 train_time:53580ms step_avg:57.37ms
step:935/2330 train_time:53636ms step_avg:57.36ms
step:936/2330 train_time:53696ms step_avg:57.37ms
step:937/2330 train_time:53752ms step_avg:57.37ms
step:938/2330 train_time:53812ms step_avg:57.37ms
step:939/2330 train_time:53869ms step_avg:57.37ms
step:940/2330 train_time:53929ms step_avg:57.37ms
step:941/2330 train_time:53987ms step_avg:57.37ms
step:942/2330 train_time:54045ms step_avg:57.37ms
step:943/2330 train_time:54102ms step_avg:57.37ms
step:944/2330 train_time:54161ms step_avg:57.37ms
step:945/2330 train_time:54217ms step_avg:57.37ms
step:946/2330 train_time:54277ms step_avg:57.38ms
step:947/2330 train_time:54334ms step_avg:57.37ms
step:948/2330 train_time:54393ms step_avg:57.38ms
step:949/2330 train_time:54450ms step_avg:57.38ms
step:950/2330 train_time:54509ms step_avg:57.38ms
step:951/2330 train_time:54565ms step_avg:57.38ms
step:952/2330 train_time:54626ms step_avg:57.38ms
step:953/2330 train_time:54683ms step_avg:57.38ms
step:954/2330 train_time:54742ms step_avg:57.38ms
step:955/2330 train_time:54798ms step_avg:57.38ms
step:956/2330 train_time:54858ms step_avg:57.38ms
step:957/2330 train_time:54915ms step_avg:57.38ms
step:958/2330 train_time:54975ms step_avg:57.39ms
step:959/2330 train_time:55032ms step_avg:57.38ms
step:960/2330 train_time:55091ms step_avg:57.39ms
step:961/2330 train_time:55148ms step_avg:57.39ms
step:962/2330 train_time:55208ms step_avg:57.39ms
step:963/2330 train_time:55264ms step_avg:57.39ms
step:964/2330 train_time:55323ms step_avg:57.39ms
step:965/2330 train_time:55379ms step_avg:57.39ms
step:966/2330 train_time:55439ms step_avg:57.39ms
step:967/2330 train_time:55495ms step_avg:57.39ms
step:968/2330 train_time:55556ms step_avg:57.39ms
step:969/2330 train_time:55612ms step_avg:57.39ms
step:970/2330 train_time:55673ms step_avg:57.39ms
step:971/2330 train_time:55729ms step_avg:57.39ms
step:972/2330 train_time:55789ms step_avg:57.40ms
step:973/2330 train_time:55846ms step_avg:57.40ms
step:974/2330 train_time:55905ms step_avg:57.40ms
step:975/2330 train_time:55962ms step_avg:57.40ms
step:976/2330 train_time:56021ms step_avg:57.40ms
step:977/2330 train_time:56077ms step_avg:57.40ms
step:978/2330 train_time:56136ms step_avg:57.40ms
step:979/2330 train_time:56192ms step_avg:57.40ms
step:980/2330 train_time:56252ms step_avg:57.40ms
step:981/2330 train_time:56308ms step_avg:57.40ms
step:982/2330 train_time:56368ms step_avg:57.40ms
step:983/2330 train_time:56425ms step_avg:57.40ms
step:984/2330 train_time:56484ms step_avg:57.40ms
step:985/2330 train_time:56541ms step_avg:57.40ms
step:986/2330 train_time:56599ms step_avg:57.40ms
step:987/2330 train_time:56656ms step_avg:57.40ms
step:988/2330 train_time:56716ms step_avg:57.40ms
step:989/2330 train_time:56772ms step_avg:57.40ms
step:990/2330 train_time:56833ms step_avg:57.41ms
step:991/2330 train_time:56890ms step_avg:57.41ms
step:992/2330 train_time:56949ms step_avg:57.41ms
step:993/2330 train_time:57006ms step_avg:57.41ms
step:994/2330 train_time:57066ms step_avg:57.41ms
step:995/2330 train_time:57123ms step_avg:57.41ms
step:996/2330 train_time:57183ms step_avg:57.41ms
step:997/2330 train_time:57239ms step_avg:57.41ms
step:998/2330 train_time:57298ms step_avg:57.41ms
step:999/2330 train_time:57354ms step_avg:57.41ms
step:1000/2330 train_time:57414ms step_avg:57.41ms
step:1000/2330 val_loss:4.5284 train_time:57495ms step_avg:57.49ms
step:1001/2330 train_time:57512ms step_avg:57.45ms
step:1002/2330 train_time:57532ms step_avg:57.42ms
step:1003/2330 train_time:57587ms step_avg:57.42ms
step:1004/2330 train_time:57653ms step_avg:57.42ms
step:1005/2330 train_time:57709ms step_avg:57.42ms
step:1006/2330 train_time:57773ms step_avg:57.43ms
step:1007/2330 train_time:57828ms step_avg:57.43ms
step:1008/2330 train_time:57888ms step_avg:57.43ms
step:1009/2330 train_time:57944ms step_avg:57.43ms
step:1010/2330 train_time:58003ms step_avg:57.43ms
step:1011/2330 train_time:58060ms step_avg:57.43ms
step:1012/2330 train_time:58119ms step_avg:57.43ms
step:1013/2330 train_time:58175ms step_avg:57.43ms
step:1014/2330 train_time:58233ms step_avg:57.43ms
step:1015/2330 train_time:58289ms step_avg:57.43ms
step:1016/2330 train_time:58348ms step_avg:57.43ms
step:1017/2330 train_time:58405ms step_avg:57.43ms
step:1018/2330 train_time:58468ms step_avg:57.43ms
step:1019/2330 train_time:58526ms step_avg:57.43ms
step:1020/2330 train_time:58586ms step_avg:57.44ms
step:1021/2330 train_time:58644ms step_avg:57.44ms
step:1022/2330 train_time:58704ms step_avg:57.44ms
step:1023/2330 train_time:58762ms step_avg:57.44ms
step:1024/2330 train_time:58821ms step_avg:57.44ms
step:1025/2330 train_time:58877ms step_avg:57.44ms
step:1026/2330 train_time:58937ms step_avg:57.44ms
step:1027/2330 train_time:58993ms step_avg:57.44ms
step:1028/2330 train_time:59052ms step_avg:57.44ms
step:1029/2330 train_time:59108ms step_avg:57.44ms
step:1030/2330 train_time:59167ms step_avg:57.44ms
step:1031/2330 train_time:59224ms step_avg:57.44ms
step:1032/2330 train_time:59283ms step_avg:57.44ms
step:1033/2330 train_time:59340ms step_avg:57.44ms
step:1034/2330 train_time:59400ms step_avg:57.45ms
step:1035/2330 train_time:59457ms step_avg:57.45ms
step:1036/2330 train_time:59517ms step_avg:57.45ms
step:1037/2330 train_time:59574ms step_avg:57.45ms
step:1038/2330 train_time:59633ms step_avg:57.45ms
step:1039/2330 train_time:59690ms step_avg:57.45ms
step:1040/2330 train_time:59749ms step_avg:57.45ms
step:1041/2330 train_time:59805ms step_avg:57.45ms
step:1042/2330 train_time:59866ms step_avg:57.45ms
step:1043/2330 train_time:59923ms step_avg:57.45ms
step:1044/2330 train_time:59983ms step_avg:57.45ms
step:1045/2330 train_time:60039ms step_avg:57.45ms
step:1046/2330 train_time:60099ms step_avg:57.46ms
step:1047/2330 train_time:60155ms step_avg:57.45ms
step:1048/2330 train_time:60214ms step_avg:57.46ms
step:1049/2330 train_time:60270ms step_avg:57.45ms
step:1050/2330 train_time:60329ms step_avg:57.46ms
step:1051/2330 train_time:60386ms step_avg:57.46ms
step:1052/2330 train_time:60446ms step_avg:57.46ms
step:1053/2330 train_time:60504ms step_avg:57.46ms
step:1054/2330 train_time:60564ms step_avg:57.46ms
step:1055/2330 train_time:60622ms step_avg:57.46ms
step:1056/2330 train_time:60682ms step_avg:57.46ms
step:1057/2330 train_time:60738ms step_avg:57.46ms
step:1058/2330 train_time:60799ms step_avg:57.47ms
step:1059/2330 train_time:60855ms step_avg:57.46ms
step:1060/2330 train_time:60915ms step_avg:57.47ms
step:1061/2330 train_time:60971ms step_avg:57.47ms
step:1062/2330 train_time:61031ms step_avg:57.47ms
step:1063/2330 train_time:61087ms step_avg:57.47ms
step:1064/2330 train_time:61146ms step_avg:57.47ms
step:1065/2330 train_time:61202ms step_avg:57.47ms
step:1066/2330 train_time:61262ms step_avg:57.47ms
step:1067/2330 train_time:61318ms step_avg:57.47ms
step:1068/2330 train_time:61377ms step_avg:57.47ms
step:1069/2330 train_time:61434ms step_avg:57.47ms
step:1070/2330 train_time:61493ms step_avg:57.47ms
step:1071/2330 train_time:61550ms step_avg:57.47ms
step:1072/2330 train_time:61610ms step_avg:57.47ms
step:1073/2330 train_time:61667ms step_avg:57.47ms
step:1074/2330 train_time:61727ms step_avg:57.47ms
step:1075/2330 train_time:61784ms step_avg:57.47ms
step:1076/2330 train_time:61843ms step_avg:57.48ms
step:1077/2330 train_time:61901ms step_avg:57.48ms
step:1078/2330 train_time:61961ms step_avg:57.48ms
step:1079/2330 train_time:62017ms step_avg:57.48ms
step:1080/2330 train_time:62076ms step_avg:57.48ms
step:1081/2330 train_time:62132ms step_avg:57.48ms
step:1082/2330 train_time:62192ms step_avg:57.48ms
step:1083/2330 train_time:62247ms step_avg:57.48ms
step:1084/2330 train_time:62307ms step_avg:57.48ms
step:1085/2330 train_time:62364ms step_avg:57.48ms
step:1086/2330 train_time:62424ms step_avg:57.48ms
step:1087/2330 train_time:62481ms step_avg:57.48ms
step:1088/2330 train_time:62540ms step_avg:57.48ms
step:1089/2330 train_time:62597ms step_avg:57.48ms
step:1090/2330 train_time:62656ms step_avg:57.48ms
step:1091/2330 train_time:62712ms step_avg:57.48ms
step:1092/2330 train_time:62773ms step_avg:57.48ms
step:1093/2330 train_time:62829ms step_avg:57.48ms
step:1094/2330 train_time:62889ms step_avg:57.49ms
step:1095/2330 train_time:62945ms step_avg:57.48ms
step:1096/2330 train_time:63007ms step_avg:57.49ms
step:1097/2330 train_time:63064ms step_avg:57.49ms
step:1098/2330 train_time:63123ms step_avg:57.49ms
step:1099/2330 train_time:63180ms step_avg:57.49ms
step:1100/2330 train_time:63240ms step_avg:57.49ms
step:1101/2330 train_time:63296ms step_avg:57.49ms
step:1102/2330 train_time:63356ms step_avg:57.49ms
step:1103/2330 train_time:63412ms step_avg:57.49ms
step:1104/2330 train_time:63472ms step_avg:57.49ms
step:1105/2330 train_time:63528ms step_avg:57.49ms
step:1106/2330 train_time:63588ms step_avg:57.49ms
step:1107/2330 train_time:63644ms step_avg:57.49ms
step:1108/2330 train_time:63705ms step_avg:57.50ms
step:1109/2330 train_time:63762ms step_avg:57.50ms
step:1110/2330 train_time:63822ms step_avg:57.50ms
step:1111/2330 train_time:63879ms step_avg:57.50ms
step:1112/2330 train_time:63938ms step_avg:57.50ms
step:1113/2330 train_time:63995ms step_avg:57.50ms
step:1114/2330 train_time:64054ms step_avg:57.50ms
step:1115/2330 train_time:64111ms step_avg:57.50ms
step:1116/2330 train_time:64170ms step_avg:57.50ms
step:1117/2330 train_time:64226ms step_avg:57.50ms
step:1118/2330 train_time:64286ms step_avg:57.50ms
step:1119/2330 train_time:64343ms step_avg:57.50ms
step:1120/2330 train_time:64403ms step_avg:57.50ms
step:1121/2330 train_time:64460ms step_avg:57.50ms
step:1122/2330 train_time:64520ms step_avg:57.50ms
step:1123/2330 train_time:64577ms step_avg:57.50ms
step:1124/2330 train_time:64636ms step_avg:57.51ms
step:1125/2330 train_time:64693ms step_avg:57.50ms
step:1126/2330 train_time:64752ms step_avg:57.51ms
step:1127/2330 train_time:64809ms step_avg:57.51ms
step:1128/2330 train_time:64868ms step_avg:57.51ms
step:1129/2330 train_time:64925ms step_avg:57.51ms
step:1130/2330 train_time:64984ms step_avg:57.51ms
step:1131/2330 train_time:65041ms step_avg:57.51ms
step:1132/2330 train_time:65101ms step_avg:57.51ms
step:1133/2330 train_time:65158ms step_avg:57.51ms
step:1134/2330 train_time:65218ms step_avg:57.51ms
step:1135/2330 train_time:65274ms step_avg:57.51ms
step:1136/2330 train_time:65334ms step_avg:57.51ms
step:1137/2330 train_time:65390ms step_avg:57.51ms
step:1138/2330 train_time:65449ms step_avg:57.51ms
step:1139/2330 train_time:65506ms step_avg:57.51ms
step:1140/2330 train_time:65566ms step_avg:57.51ms
step:1141/2330 train_time:65623ms step_avg:57.51ms
step:1142/2330 train_time:65683ms step_avg:57.52ms
step:1143/2330 train_time:65740ms step_avg:57.52ms
step:1144/2330 train_time:65799ms step_avg:57.52ms
step:1145/2330 train_time:65856ms step_avg:57.52ms
step:1146/2330 train_time:65916ms step_avg:57.52ms
step:1147/2330 train_time:65972ms step_avg:57.52ms
step:1148/2330 train_time:66031ms step_avg:57.52ms
step:1149/2330 train_time:66087ms step_avg:57.52ms
step:1150/2330 train_time:66147ms step_avg:57.52ms
step:1151/2330 train_time:66204ms step_avg:57.52ms
step:1152/2330 train_time:66264ms step_avg:57.52ms
step:1153/2330 train_time:66321ms step_avg:57.52ms
step:1154/2330 train_time:66381ms step_avg:57.52ms
step:1155/2330 train_time:66437ms step_avg:57.52ms
step:1156/2330 train_time:66496ms step_avg:57.52ms
step:1157/2330 train_time:66552ms step_avg:57.52ms
step:1158/2330 train_time:66611ms step_avg:57.52ms
step:1159/2330 train_time:66667ms step_avg:57.52ms
step:1160/2330 train_time:66727ms step_avg:57.52ms
step:1161/2330 train_time:66784ms step_avg:57.52ms
step:1162/2330 train_time:66844ms step_avg:57.53ms
step:1163/2330 train_time:66902ms step_avg:57.53ms
step:1164/2330 train_time:66961ms step_avg:57.53ms
step:1165/2330 train_time:67018ms step_avg:57.53ms
step:1166/2330 train_time:67077ms step_avg:57.53ms
step:1167/2330 train_time:67134ms step_avg:57.53ms
step:1168/2330 train_time:67194ms step_avg:57.53ms
step:1169/2330 train_time:67251ms step_avg:57.53ms
step:1170/2330 train_time:67310ms step_avg:57.53ms
step:1171/2330 train_time:67366ms step_avg:57.53ms
step:1172/2330 train_time:67426ms step_avg:57.53ms
step:1173/2330 train_time:67482ms step_avg:57.53ms
step:1174/2330 train_time:67542ms step_avg:57.53ms
step:1175/2330 train_time:67599ms step_avg:57.53ms
step:1176/2330 train_time:67658ms step_avg:57.53ms
step:1177/2330 train_time:67715ms step_avg:57.53ms
step:1178/2330 train_time:67774ms step_avg:57.53ms
step:1179/2330 train_time:67830ms step_avg:57.53ms
step:1180/2330 train_time:67890ms step_avg:57.53ms
step:1181/2330 train_time:67946ms step_avg:57.53ms
step:1182/2330 train_time:68006ms step_avg:57.53ms
step:1183/2330 train_time:68062ms step_avg:57.53ms
step:1184/2330 train_time:68123ms step_avg:57.54ms
step:1185/2330 train_time:68180ms step_avg:57.54ms
step:1186/2330 train_time:68240ms step_avg:57.54ms
step:1187/2330 train_time:68297ms step_avg:57.54ms
step:1188/2330 train_time:68357ms step_avg:57.54ms
step:1189/2330 train_time:68414ms step_avg:57.54ms
step:1190/2330 train_time:68474ms step_avg:57.54ms
step:1191/2330 train_time:68530ms step_avg:57.54ms
step:1192/2330 train_time:68589ms step_avg:57.54ms
step:1193/2330 train_time:68645ms step_avg:57.54ms
step:1194/2330 train_time:68705ms step_avg:57.54ms
step:1195/2330 train_time:68762ms step_avg:57.54ms
step:1196/2330 train_time:68823ms step_avg:57.54ms
step:1197/2330 train_time:68880ms step_avg:57.54ms
step:1198/2330 train_time:68940ms step_avg:57.55ms
step:1199/2330 train_time:68996ms step_avg:57.54ms
step:1200/2330 train_time:69056ms step_avg:57.55ms
step:1201/2330 train_time:69112ms step_avg:57.55ms
step:1202/2330 train_time:69171ms step_avg:57.55ms
step:1203/2330 train_time:69227ms step_avg:57.55ms
step:1204/2330 train_time:69287ms step_avg:57.55ms
step:1205/2330 train_time:69344ms step_avg:57.55ms
step:1206/2330 train_time:69405ms step_avg:57.55ms
step:1207/2330 train_time:69462ms step_avg:57.55ms
step:1208/2330 train_time:69521ms step_avg:57.55ms
step:1209/2330 train_time:69578ms step_avg:57.55ms
step:1210/2330 train_time:69637ms step_avg:57.55ms
step:1211/2330 train_time:69693ms step_avg:57.55ms
step:1212/2330 train_time:69752ms step_avg:57.55ms
step:1213/2330 train_time:69808ms step_avg:57.55ms
step:1214/2330 train_time:69868ms step_avg:57.55ms
step:1215/2330 train_time:69925ms step_avg:57.55ms
step:1216/2330 train_time:69984ms step_avg:57.55ms
step:1217/2330 train_time:70041ms step_avg:57.55ms
step:1218/2330 train_time:70101ms step_avg:57.55ms
step:1219/2330 train_time:70157ms step_avg:57.55ms
step:1220/2330 train_time:70218ms step_avg:57.56ms
step:1221/2330 train_time:70274ms step_avg:57.55ms
step:1222/2330 train_time:70334ms step_avg:57.56ms
step:1223/2330 train_time:70390ms step_avg:57.56ms
step:1224/2330 train_time:70450ms step_avg:57.56ms
step:1225/2330 train_time:70507ms step_avg:57.56ms
step:1226/2330 train_time:70567ms step_avg:57.56ms
step:1227/2330 train_time:70624ms step_avg:57.56ms
step:1228/2330 train_time:70684ms step_avg:57.56ms
step:1229/2330 train_time:70740ms step_avg:57.56ms
step:1230/2330 train_time:70800ms step_avg:57.56ms
step:1231/2330 train_time:70856ms step_avg:57.56ms
step:1232/2330 train_time:70916ms step_avg:57.56ms
step:1233/2330 train_time:70972ms step_avg:57.56ms
step:1234/2330 train_time:71031ms step_avg:57.56ms
step:1235/2330 train_time:71088ms step_avg:57.56ms
step:1236/2330 train_time:71148ms step_avg:57.56ms
step:1237/2330 train_time:71205ms step_avg:57.56ms
step:1238/2330 train_time:71265ms step_avg:57.56ms
step:1239/2330 train_time:71322ms step_avg:57.56ms
step:1240/2330 train_time:71381ms step_avg:57.57ms
step:1241/2330 train_time:71437ms step_avg:57.56ms
step:1242/2330 train_time:71497ms step_avg:57.57ms
step:1243/2330 train_time:71554ms step_avg:57.57ms
step:1244/2330 train_time:71614ms step_avg:57.57ms
step:1245/2330 train_time:71671ms step_avg:57.57ms
step:1246/2330 train_time:71730ms step_avg:57.57ms
step:1247/2330 train_time:71787ms step_avg:57.57ms
step:1248/2330 train_time:71846ms step_avg:57.57ms
step:1249/2330 train_time:71904ms step_avg:57.57ms
step:1250/2330 train_time:71963ms step_avg:57.57ms
step:1250/2330 val_loss:4.3825 train_time:72043ms step_avg:57.63ms
step:1251/2330 train_time:72059ms step_avg:57.60ms
step:1252/2330 train_time:72081ms step_avg:57.57ms
step:1253/2330 train_time:72140ms step_avg:57.57ms
step:1254/2330 train_time:72204ms step_avg:57.58ms
step:1255/2330 train_time:72261ms step_avg:57.58ms
step:1256/2330 train_time:72322ms step_avg:57.58ms
step:1257/2330 train_time:72379ms step_avg:57.58ms
step:1258/2330 train_time:72437ms step_avg:57.58ms
step:1259/2330 train_time:72493ms step_avg:57.58ms
step:1260/2330 train_time:72554ms step_avg:57.58ms
step:1261/2330 train_time:72610ms step_avg:57.58ms
step:1262/2330 train_time:72670ms step_avg:57.58ms
step:1263/2330 train_time:72726ms step_avg:57.58ms
step:1264/2330 train_time:72786ms step_avg:57.58ms
step:1265/2330 train_time:72841ms step_avg:57.58ms
step:1266/2330 train_time:72900ms step_avg:57.58ms
step:1267/2330 train_time:72957ms step_avg:57.58ms
step:1268/2330 train_time:73016ms step_avg:57.58ms
step:1269/2330 train_time:73074ms step_avg:57.58ms
step:1270/2330 train_time:73135ms step_avg:57.59ms
step:1271/2330 train_time:73192ms step_avg:57.59ms
step:1272/2330 train_time:73255ms step_avg:57.59ms
step:1273/2330 train_time:73311ms step_avg:57.59ms
step:1274/2330 train_time:73371ms step_avg:57.59ms
step:1275/2330 train_time:73427ms step_avg:57.59ms
step:1276/2330 train_time:73486ms step_avg:57.59ms
step:1277/2330 train_time:73542ms step_avg:57.59ms
step:1278/2330 train_time:73602ms step_avg:57.59ms
step:1279/2330 train_time:73658ms step_avg:57.59ms
step:1280/2330 train_time:73718ms step_avg:57.59ms
step:1281/2330 train_time:73774ms step_avg:57.59ms
step:1282/2330 train_time:73836ms step_avg:57.59ms
step:1283/2330 train_time:73892ms step_avg:57.59ms
step:1284/2330 train_time:73951ms step_avg:57.59ms
step:1285/2330 train_time:74007ms step_avg:57.59ms
step:1286/2330 train_time:74067ms step_avg:57.59ms
step:1287/2330 train_time:74124ms step_avg:57.59ms
step:1288/2330 train_time:74184ms step_avg:57.60ms
step:1289/2330 train_time:74241ms step_avg:57.60ms
step:1290/2330 train_time:74301ms step_avg:57.60ms
step:1291/2330 train_time:74358ms step_avg:57.60ms
step:1292/2330 train_time:74418ms step_avg:57.60ms
step:1293/2330 train_time:74475ms step_avg:57.60ms
step:1294/2330 train_time:74535ms step_avg:57.60ms
step:1295/2330 train_time:74592ms step_avg:57.60ms
step:1296/2330 train_time:74651ms step_avg:57.60ms
step:1297/2330 train_time:74708ms step_avg:57.60ms
step:1298/2330 train_time:74766ms step_avg:57.60ms
step:1299/2330 train_time:74822ms step_avg:57.60ms
step:1300/2330 train_time:74882ms step_avg:57.60ms
step:1301/2330 train_time:74939ms step_avg:57.60ms
step:1302/2330 train_time:74998ms step_avg:57.60ms
step:1303/2330 train_time:75055ms step_avg:57.60ms
step:1304/2330 train_time:75115ms step_avg:57.60ms
step:1305/2330 train_time:75173ms step_avg:57.60ms
step:1306/2330 train_time:75233ms step_avg:57.61ms
step:1307/2330 train_time:75290ms step_avg:57.61ms
step:1308/2330 train_time:75350ms step_avg:57.61ms
step:1309/2330 train_time:75406ms step_avg:57.61ms
step:1310/2330 train_time:75466ms step_avg:57.61ms
step:1311/2330 train_time:75522ms step_avg:57.61ms
step:1312/2330 train_time:75582ms step_avg:57.61ms
step:1313/2330 train_time:75640ms step_avg:57.61ms
step:1314/2330 train_time:75698ms step_avg:57.61ms
step:1315/2330 train_time:75755ms step_avg:57.61ms
step:1316/2330 train_time:75815ms step_avg:57.61ms
step:1317/2330 train_time:75871ms step_avg:57.61ms
step:1318/2330 train_time:75931ms step_avg:57.61ms
step:1319/2330 train_time:75987ms step_avg:57.61ms
step:1320/2330 train_time:76046ms step_avg:57.61ms
step:1321/2330 train_time:76102ms step_avg:57.61ms
step:1322/2330 train_time:76163ms step_avg:57.61ms
step:1323/2330 train_time:76220ms step_avg:57.61ms
step:1324/2330 train_time:76280ms step_avg:57.61ms
step:1325/2330 train_time:76337ms step_avg:57.61ms
step:1326/2330 train_time:76397ms step_avg:57.61ms
step:1327/2330 train_time:76454ms step_avg:57.61ms
step:1328/2330 train_time:76514ms step_avg:57.62ms
step:1329/2330 train_time:76572ms step_avg:57.62ms
step:1330/2330 train_time:76631ms step_avg:57.62ms
step:1331/2330 train_time:76688ms step_avg:57.62ms
step:1332/2330 train_time:76747ms step_avg:57.62ms
step:1333/2330 train_time:76803ms step_avg:57.62ms
step:1334/2330 train_time:76863ms step_avg:57.62ms
step:1335/2330 train_time:76919ms step_avg:57.62ms
step:1336/2330 train_time:76979ms step_avg:57.62ms
step:1337/2330 train_time:77035ms step_avg:57.62ms
step:1338/2330 train_time:77095ms step_avg:57.62ms
step:1339/2330 train_time:77152ms step_avg:57.62ms
step:1340/2330 train_time:77211ms step_avg:57.62ms
step:1341/2330 train_time:77268ms step_avg:57.62ms
step:1342/2330 train_time:77327ms step_avg:57.62ms
step:1343/2330 train_time:77384ms step_avg:57.62ms
step:1344/2330 train_time:77444ms step_avg:57.62ms
step:1345/2330 train_time:77501ms step_avg:57.62ms
step:1346/2330 train_time:77560ms step_avg:57.62ms
step:1347/2330 train_time:77616ms step_avg:57.62ms
step:1348/2330 train_time:77677ms step_avg:57.62ms
step:1349/2330 train_time:77734ms step_avg:57.62ms
step:1350/2330 train_time:77794ms step_avg:57.63ms
step:1351/2330 train_time:77851ms step_avg:57.62ms
step:1352/2330 train_time:77909ms step_avg:57.63ms
step:1353/2330 train_time:77965ms step_avg:57.62ms
step:1354/2330 train_time:78025ms step_avg:57.63ms
step:1355/2330 train_time:78082ms step_avg:57.62ms
step:1356/2330 train_time:78142ms step_avg:57.63ms
step:1357/2330 train_time:78199ms step_avg:57.63ms
step:1358/2330 train_time:78258ms step_avg:57.63ms
step:1359/2330 train_time:78315ms step_avg:57.63ms
step:1360/2330 train_time:78375ms step_avg:57.63ms
step:1361/2330 train_time:78432ms step_avg:57.63ms
step:1362/2330 train_time:78491ms step_avg:57.63ms
step:1363/2330 train_time:78548ms step_avg:57.63ms
step:1364/2330 train_time:78609ms step_avg:57.63ms
step:1365/2330 train_time:78664ms step_avg:57.63ms
step:1366/2330 train_time:78724ms step_avg:57.63ms
step:1367/2330 train_time:78781ms step_avg:57.63ms
step:1368/2330 train_time:78841ms step_avg:57.63ms
step:1369/2330 train_time:78897ms step_avg:57.63ms
step:1370/2330 train_time:78957ms step_avg:57.63ms
step:1371/2330 train_time:79013ms step_avg:57.63ms
step:1372/2330 train_time:79073ms step_avg:57.63ms
step:1373/2330 train_time:79130ms step_avg:57.63ms
step:1374/2330 train_time:79188ms step_avg:57.63ms
step:1375/2330 train_time:79245ms step_avg:57.63ms
step:1376/2330 train_time:79304ms step_avg:57.63ms
step:1377/2330 train_time:79360ms step_avg:57.63ms
step:1378/2330 train_time:79420ms step_avg:57.63ms
step:1379/2330 train_time:79478ms step_avg:57.63ms
step:1380/2330 train_time:79539ms step_avg:57.64ms
step:1381/2330 train_time:79596ms step_avg:57.64ms
step:1382/2330 train_time:79656ms step_avg:57.64ms
step:1383/2330 train_time:79712ms step_avg:57.64ms
step:1384/2330 train_time:79772ms step_avg:57.64ms
step:1385/2330 train_time:79829ms step_avg:57.64ms
step:1386/2330 train_time:79888ms step_avg:57.64ms
step:1387/2330 train_time:79944ms step_avg:57.64ms
step:1388/2330 train_time:80004ms step_avg:57.64ms
step:1389/2330 train_time:80060ms step_avg:57.64ms
step:1390/2330 train_time:80121ms step_avg:57.64ms
step:1391/2330 train_time:80178ms step_avg:57.64ms
step:1392/2330 train_time:80238ms step_avg:57.64ms
step:1393/2330 train_time:80295ms step_avg:57.64ms
step:1394/2330 train_time:80354ms step_avg:57.64ms
step:1395/2330 train_time:80411ms step_avg:57.64ms
step:1396/2330 train_time:80470ms step_avg:57.64ms
step:1397/2330 train_time:80526ms step_avg:57.64ms
step:1398/2330 train_time:80587ms step_avg:57.64ms
step:1399/2330 train_time:80644ms step_avg:57.64ms
step:1400/2330 train_time:80703ms step_avg:57.65ms
step:1401/2330 train_time:80759ms step_avg:57.64ms
step:1402/2330 train_time:80820ms step_avg:57.65ms
step:1403/2330 train_time:80877ms step_avg:57.65ms
step:1404/2330 train_time:80936ms step_avg:57.65ms
step:1405/2330 train_time:80993ms step_avg:57.65ms
step:1406/2330 train_time:81054ms step_avg:57.65ms
step:1407/2330 train_time:81110ms step_avg:57.65ms
step:1408/2330 train_time:81170ms step_avg:57.65ms
step:1409/2330 train_time:81226ms step_avg:57.65ms
step:1410/2330 train_time:81285ms step_avg:57.65ms
step:1411/2330 train_time:81342ms step_avg:57.65ms
step:1412/2330 train_time:81402ms step_avg:57.65ms
step:1413/2330 train_time:81459ms step_avg:57.65ms
step:1414/2330 train_time:81518ms step_avg:57.65ms
step:1415/2330 train_time:81575ms step_avg:57.65ms
step:1416/2330 train_time:81635ms step_avg:57.65ms
step:1417/2330 train_time:81692ms step_avg:57.65ms
step:1418/2330 train_time:81751ms step_avg:57.65ms
step:1419/2330 train_time:81808ms step_avg:57.65ms
step:1420/2330 train_time:81866ms step_avg:57.65ms
step:1421/2330 train_time:81923ms step_avg:57.65ms
step:1422/2330 train_time:81983ms step_avg:57.65ms
step:1423/2330 train_time:82040ms step_avg:57.65ms
step:1424/2330 train_time:82099ms step_avg:57.65ms
step:1425/2330 train_time:82156ms step_avg:57.65ms
step:1426/2330 train_time:82216ms step_avg:57.66ms
step:1427/2330 train_time:82273ms step_avg:57.65ms
step:1428/2330 train_time:82332ms step_avg:57.66ms
step:1429/2330 train_time:82390ms step_avg:57.66ms
step:1430/2330 train_time:82449ms step_avg:57.66ms
step:1431/2330 train_time:82505ms step_avg:57.66ms
step:1432/2330 train_time:82564ms step_avg:57.66ms
step:1433/2330 train_time:82621ms step_avg:57.66ms
step:1434/2330 train_time:82681ms step_avg:57.66ms
step:1435/2330 train_time:82738ms step_avg:57.66ms
step:1436/2330 train_time:82798ms step_avg:57.66ms
step:1437/2330 train_time:82855ms step_avg:57.66ms
step:1438/2330 train_time:82915ms step_avg:57.66ms
step:1439/2330 train_time:82971ms step_avg:57.66ms
step:1440/2330 train_time:83031ms step_avg:57.66ms
step:1441/2330 train_time:83087ms step_avg:57.66ms
step:1442/2330 train_time:83147ms step_avg:57.66ms
step:1443/2330 train_time:83203ms step_avg:57.66ms
step:1444/2330 train_time:83263ms step_avg:57.66ms
step:1445/2330 train_time:83320ms step_avg:57.66ms
step:1446/2330 train_time:83379ms step_avg:57.66ms
step:1447/2330 train_time:83436ms step_avg:57.66ms
step:1448/2330 train_time:83496ms step_avg:57.66ms
step:1449/2330 train_time:83553ms step_avg:57.66ms
step:1450/2330 train_time:83612ms step_avg:57.66ms
step:1451/2330 train_time:83669ms step_avg:57.66ms
step:1452/2330 train_time:83729ms step_avg:57.66ms
step:1453/2330 train_time:83785ms step_avg:57.66ms
step:1454/2330 train_time:83845ms step_avg:57.66ms
step:1455/2330 train_time:83901ms step_avg:57.66ms
step:1456/2330 train_time:83961ms step_avg:57.67ms
step:1457/2330 train_time:84017ms step_avg:57.66ms
step:1458/2330 train_time:84078ms step_avg:57.67ms
step:1459/2330 train_time:84135ms step_avg:57.67ms
step:1460/2330 train_time:84196ms step_avg:57.67ms
step:1461/2330 train_time:84253ms step_avg:57.67ms
step:1462/2330 train_time:84312ms step_avg:57.67ms
step:1463/2330 train_time:84368ms step_avg:57.67ms
step:1464/2330 train_time:84428ms step_avg:57.67ms
step:1465/2330 train_time:84484ms step_avg:57.67ms
step:1466/2330 train_time:84544ms step_avg:57.67ms
step:1467/2330 train_time:84601ms step_avg:57.67ms
step:1468/2330 train_time:84660ms step_avg:57.67ms
step:1469/2330 train_time:84717ms step_avg:57.67ms
step:1470/2330 train_time:84777ms step_avg:57.67ms
step:1471/2330 train_time:84834ms step_avg:57.67ms
step:1472/2330 train_time:84893ms step_avg:57.67ms
step:1473/2330 train_time:84950ms step_avg:57.67ms
step:1474/2330 train_time:85009ms step_avg:57.67ms
step:1475/2330 train_time:85065ms step_avg:57.67ms
step:1476/2330 train_time:85124ms step_avg:57.67ms
step:1477/2330 train_time:85180ms step_avg:57.67ms
step:1478/2330 train_time:85240ms step_avg:57.67ms
step:1479/2330 train_time:85297ms step_avg:57.67ms
step:1480/2330 train_time:85357ms step_avg:57.67ms
step:1481/2330 train_time:85414ms step_avg:57.67ms
step:1482/2330 train_time:85473ms step_avg:57.67ms
step:1483/2330 train_time:85530ms step_avg:57.67ms
step:1484/2330 train_time:85589ms step_avg:57.67ms
step:1485/2330 train_time:85645ms step_avg:57.67ms
step:1486/2330 train_time:85705ms step_avg:57.68ms
step:1487/2330 train_time:85761ms step_avg:57.67ms
step:1488/2330 train_time:85822ms step_avg:57.68ms
step:1489/2330 train_time:85879ms step_avg:57.68ms
step:1490/2330 train_time:85939ms step_avg:57.68ms
step:1491/2330 train_time:85996ms step_avg:57.68ms
step:1492/2330 train_time:86055ms step_avg:57.68ms
step:1493/2330 train_time:86111ms step_avg:57.68ms
step:1494/2330 train_time:86171ms step_avg:57.68ms
step:1495/2330 train_time:86227ms step_avg:57.68ms
step:1496/2330 train_time:86287ms step_avg:57.68ms
step:1497/2330 train_time:86343ms step_avg:57.68ms
step:1498/2330 train_time:86404ms step_avg:57.68ms
step:1499/2330 train_time:86460ms step_avg:57.68ms
step:1500/2330 train_time:86520ms step_avg:57.68ms
step:1500/2330 val_loss:4.2644 train_time:86600ms step_avg:57.73ms
step:1501/2330 train_time:86619ms step_avg:57.71ms
step:1502/2330 train_time:86639ms step_avg:57.68ms
step:1503/2330 train_time:86695ms step_avg:57.68ms
step:1504/2330 train_time:86761ms step_avg:57.69ms
step:1505/2330 train_time:86818ms step_avg:57.69ms
step:1506/2330 train_time:86878ms step_avg:57.69ms
step:1507/2330 train_time:86935ms step_avg:57.69ms
step:1508/2330 train_time:86994ms step_avg:57.69ms
step:1509/2330 train_time:87049ms step_avg:57.69ms
step:1510/2330 train_time:87109ms step_avg:57.69ms
step:1511/2330 train_time:87165ms step_avg:57.69ms
step:1512/2330 train_time:87224ms step_avg:57.69ms
step:1513/2330 train_time:87280ms step_avg:57.69ms
step:1514/2330 train_time:87339ms step_avg:57.69ms
step:1515/2330 train_time:87395ms step_avg:57.69ms
step:1516/2330 train_time:87455ms step_avg:57.69ms
step:1517/2330 train_time:87511ms step_avg:57.69ms
step:1518/2330 train_time:87571ms step_avg:57.69ms
step:1519/2330 train_time:87628ms step_avg:57.69ms
step:1520/2330 train_time:87690ms step_avg:57.69ms
step:1521/2330 train_time:87748ms step_avg:57.69ms
step:1522/2330 train_time:87808ms step_avg:57.69ms
step:1523/2330 train_time:87865ms step_avg:57.69ms
step:1524/2330 train_time:87926ms step_avg:57.69ms
step:1525/2330 train_time:87983ms step_avg:57.69ms
step:1526/2330 train_time:88043ms step_avg:57.70ms
step:1527/2330 train_time:88098ms step_avg:57.69ms
step:1528/2330 train_time:88158ms step_avg:57.70ms
step:1529/2330 train_time:88215ms step_avg:57.69ms
step:1530/2330 train_time:88273ms step_avg:57.69ms
step:1531/2330 train_time:88329ms step_avg:57.69ms
step:1532/2330 train_time:88389ms step_avg:57.69ms
step:1533/2330 train_time:88446ms step_avg:57.69ms
step:1534/2330 train_time:88506ms step_avg:57.70ms
step:1535/2330 train_time:88563ms step_avg:57.70ms
step:1536/2330 train_time:88623ms step_avg:57.70ms
step:1537/2330 train_time:88681ms step_avg:57.70ms
step:1538/2330 train_time:88742ms step_avg:57.70ms
step:1539/2330 train_time:88800ms step_avg:57.70ms
step:1540/2330 train_time:88860ms step_avg:57.70ms
step:1541/2330 train_time:88917ms step_avg:57.70ms
step:1542/2330 train_time:88978ms step_avg:57.70ms
step:1543/2330 train_time:89035ms step_avg:57.70ms
step:1544/2330 train_time:89095ms step_avg:57.70ms
step:1545/2330 train_time:89151ms step_avg:57.70ms
step:1546/2330 train_time:89212ms step_avg:57.71ms
step:1547/2330 train_time:89268ms step_avg:57.70ms
step:1548/2330 train_time:89328ms step_avg:57.71ms
step:1549/2330 train_time:89385ms step_avg:57.71ms
step:1550/2330 train_time:89445ms step_avg:57.71ms
step:1551/2330 train_time:89502ms step_avg:57.71ms
step:1552/2330 train_time:89561ms step_avg:57.71ms
step:1553/2330 train_time:89619ms step_avg:57.71ms
step:1554/2330 train_time:89679ms step_avg:57.71ms
step:1555/2330 train_time:89736ms step_avg:57.71ms
step:1556/2330 train_time:89795ms step_avg:57.71ms
step:1557/2330 train_time:89853ms step_avg:57.71ms
step:1558/2330 train_time:89913ms step_avg:57.71ms
step:1559/2330 train_time:89970ms step_avg:57.71ms
step:1560/2330 train_time:90031ms step_avg:57.71ms
step:1561/2330 train_time:90088ms step_avg:57.71ms
step:1562/2330 train_time:90149ms step_avg:57.71ms
step:1563/2330 train_time:90206ms step_avg:57.71ms
step:1564/2330 train_time:90266ms step_avg:57.71ms
step:1565/2330 train_time:90323ms step_avg:57.71ms
step:1566/2330 train_time:90383ms step_avg:57.72ms
step:1567/2330 train_time:90440ms step_avg:57.72ms
step:1568/2330 train_time:90500ms step_avg:57.72ms
step:1569/2330 train_time:90556ms step_avg:57.72ms
step:1570/2330 train_time:90618ms step_avg:57.72ms
step:1571/2330 train_time:90675ms step_avg:57.72ms
step:1572/2330 train_time:90734ms step_avg:57.72ms
step:1573/2330 train_time:90791ms step_avg:57.72ms
step:1574/2330 train_time:90852ms step_avg:57.72ms
step:1575/2330 train_time:90910ms step_avg:57.72ms
step:1576/2330 train_time:90970ms step_avg:57.72ms
step:1577/2330 train_time:91027ms step_avg:57.72ms
step:1578/2330 train_time:91087ms step_avg:57.72ms
step:1579/2330 train_time:91144ms step_avg:57.72ms
step:1580/2330 train_time:91204ms step_avg:57.72ms
step:1581/2330 train_time:91260ms step_avg:57.72ms
step:1582/2330 train_time:91321ms step_avg:57.72ms
step:1583/2330 train_time:91378ms step_avg:57.72ms
step:1584/2330 train_time:91438ms step_avg:57.73ms
step:1585/2330 train_time:91494ms step_avg:57.72ms
step:1586/2330 train_time:91554ms step_avg:57.73ms
step:1587/2330 train_time:91610ms step_avg:57.73ms
step:1588/2330 train_time:91672ms step_avg:57.73ms
step:1589/2330 train_time:91728ms step_avg:57.73ms
step:1590/2330 train_time:91790ms step_avg:57.73ms
step:1591/2330 train_time:91847ms step_avg:57.73ms
step:1592/2330 train_time:91908ms step_avg:57.73ms
step:1593/2330 train_time:91965ms step_avg:57.73ms
step:1594/2330 train_time:92025ms step_avg:57.73ms
step:1595/2330 train_time:92083ms step_avg:57.73ms
step:1596/2330 train_time:92143ms step_avg:57.73ms
step:1597/2330 train_time:92200ms step_avg:57.73ms
step:1598/2330 train_time:92259ms step_avg:57.73ms
step:1599/2330 train_time:92316ms step_avg:57.73ms
step:1600/2330 train_time:92377ms step_avg:57.74ms
step:1601/2330 train_time:92434ms step_avg:57.74ms
step:1602/2330 train_time:92493ms step_avg:57.74ms
step:1603/2330 train_time:92550ms step_avg:57.74ms
step:1604/2330 train_time:92610ms step_avg:57.74ms
step:1605/2330 train_time:92667ms step_avg:57.74ms
step:1606/2330 train_time:92727ms step_avg:57.74ms
step:1607/2330 train_time:92784ms step_avg:57.74ms
step:1608/2330 train_time:92845ms step_avg:57.74ms
step:1609/2330 train_time:92901ms step_avg:57.74ms
step:1610/2330 train_time:92962ms step_avg:57.74ms
step:1611/2330 train_time:93019ms step_avg:57.74ms
step:1612/2330 train_time:93080ms step_avg:57.74ms
step:1613/2330 train_time:93136ms step_avg:57.74ms
step:1614/2330 train_time:93196ms step_avg:57.74ms
step:1615/2330 train_time:93253ms step_avg:57.74ms
step:1616/2330 train_time:93313ms step_avg:57.74ms
step:1617/2330 train_time:93369ms step_avg:57.74ms
step:1618/2330 train_time:93430ms step_avg:57.74ms
step:1619/2330 train_time:93487ms step_avg:57.74ms
step:1620/2330 train_time:93547ms step_avg:57.75ms
step:1621/2330 train_time:93604ms step_avg:57.74ms
step:1622/2330 train_time:93665ms step_avg:57.75ms
step:1623/2330 train_time:93722ms step_avg:57.75ms
step:1624/2330 train_time:93782ms step_avg:57.75ms
step:1625/2330 train_time:93839ms step_avg:57.75ms
step:1626/2330 train_time:93899ms step_avg:57.75ms
step:1627/2330 train_time:93956ms step_avg:57.75ms
step:1628/2330 train_time:94016ms step_avg:57.75ms
step:1629/2330 train_time:94073ms step_avg:57.75ms
step:1630/2330 train_time:94133ms step_avg:57.75ms
step:1631/2330 train_time:94189ms step_avg:57.75ms
step:1632/2330 train_time:94250ms step_avg:57.75ms
step:1633/2330 train_time:94306ms step_avg:57.75ms
step:1634/2330 train_time:94368ms step_avg:57.75ms
step:1635/2330 train_time:94425ms step_avg:57.75ms
step:1636/2330 train_time:94486ms step_avg:57.75ms
step:1637/2330 train_time:94543ms step_avg:57.75ms
step:1638/2330 train_time:94603ms step_avg:57.76ms
step:1639/2330 train_time:94659ms step_avg:57.75ms
step:1640/2330 train_time:94720ms step_avg:57.76ms
step:1641/2330 train_time:94777ms step_avg:57.76ms
step:1642/2330 train_time:94838ms step_avg:57.76ms
step:1643/2330 train_time:94894ms step_avg:57.76ms
step:1644/2330 train_time:94954ms step_avg:57.76ms
step:1645/2330 train_time:95011ms step_avg:57.76ms
step:1646/2330 train_time:95072ms step_avg:57.76ms
step:1647/2330 train_time:95129ms step_avg:57.76ms
step:1648/2330 train_time:95189ms step_avg:57.76ms
step:1649/2330 train_time:95246ms step_avg:57.76ms
step:1650/2330 train_time:95306ms step_avg:57.76ms
step:1651/2330 train_time:95363ms step_avg:57.76ms
step:1652/2330 train_time:95423ms step_avg:57.76ms
step:1653/2330 train_time:95480ms step_avg:57.76ms
step:1654/2330 train_time:95540ms step_avg:57.76ms
step:1655/2330 train_time:95597ms step_avg:57.76ms
step:1656/2330 train_time:95657ms step_avg:57.76ms
step:1657/2330 train_time:95713ms step_avg:57.76ms
step:1658/2330 train_time:95774ms step_avg:57.76ms
step:1659/2330 train_time:95831ms step_avg:57.76ms
step:1660/2330 train_time:95890ms step_avg:57.77ms
step:1661/2330 train_time:95947ms step_avg:57.76ms
step:1662/2330 train_time:96008ms step_avg:57.77ms
step:1663/2330 train_time:96065ms step_avg:57.77ms
step:1664/2330 train_time:96126ms step_avg:57.77ms
step:1665/2330 train_time:96182ms step_avg:57.77ms
step:1666/2330 train_time:96243ms step_avg:57.77ms
step:1667/2330 train_time:96300ms step_avg:57.77ms
step:1668/2330 train_time:96359ms step_avg:57.77ms
step:1669/2330 train_time:96417ms step_avg:57.77ms
step:1670/2330 train_time:96476ms step_avg:57.77ms
step:1671/2330 train_time:96533ms step_avg:57.77ms
step:1672/2330 train_time:96593ms step_avg:57.77ms
step:1673/2330 train_time:96650ms step_avg:57.77ms
step:1674/2330 train_time:96710ms step_avg:57.77ms
step:1675/2330 train_time:96767ms step_avg:57.77ms
step:1676/2330 train_time:96828ms step_avg:57.77ms
step:1677/2330 train_time:96885ms step_avg:57.77ms
step:1678/2330 train_time:96947ms step_avg:57.78ms
step:1679/2330 train_time:97004ms step_avg:57.77ms
step:1680/2330 train_time:97064ms step_avg:57.78ms
step:1681/2330 train_time:97121ms step_avg:57.78ms
step:1682/2330 train_time:97182ms step_avg:57.78ms
step:1683/2330 train_time:97238ms step_avg:57.78ms
step:1684/2330 train_time:97299ms step_avg:57.78ms
step:1685/2330 train_time:97355ms step_avg:57.78ms
step:1686/2330 train_time:97416ms step_avg:57.78ms
step:1687/2330 train_time:97473ms step_avg:57.78ms
step:1688/2330 train_time:97533ms step_avg:57.78ms
step:1689/2330 train_time:97589ms step_avg:57.78ms
step:1690/2330 train_time:97650ms step_avg:57.78ms
step:1691/2330 train_time:97707ms step_avg:57.78ms
step:1692/2330 train_time:97768ms step_avg:57.78ms
step:1693/2330 train_time:97825ms step_avg:57.78ms
step:1694/2330 train_time:97886ms step_avg:57.78ms
step:1695/2330 train_time:97944ms step_avg:57.78ms
step:1696/2330 train_time:98004ms step_avg:57.79ms
step:1697/2330 train_time:98062ms step_avg:57.79ms
step:1698/2330 train_time:98121ms step_avg:57.79ms
step:1699/2330 train_time:98179ms step_avg:57.79ms
step:1700/2330 train_time:98238ms step_avg:57.79ms
step:1701/2330 train_time:98295ms step_avg:57.79ms
step:1702/2330 train_time:98355ms step_avg:57.79ms
step:1703/2330 train_time:98411ms step_avg:57.79ms
step:1704/2330 train_time:98472ms step_avg:57.79ms
step:1705/2330 train_time:98529ms step_avg:57.79ms
step:1706/2330 train_time:98589ms step_avg:57.79ms
step:1707/2330 train_time:98646ms step_avg:57.79ms
step:1708/2330 train_time:98707ms step_avg:57.79ms
step:1709/2330 train_time:98763ms step_avg:57.79ms
step:1710/2330 train_time:98824ms step_avg:57.79ms
step:1711/2330 train_time:98881ms step_avg:57.79ms
step:1712/2330 train_time:98942ms step_avg:57.79ms
step:1713/2330 train_time:98998ms step_avg:57.79ms
step:1714/2330 train_time:99059ms step_avg:57.79ms
step:1715/2330 train_time:99116ms step_avg:57.79ms
step:1716/2330 train_time:99177ms step_avg:57.80ms
step:1717/2330 train_time:99234ms step_avg:57.80ms
step:1718/2330 train_time:99294ms step_avg:57.80ms
step:1719/2330 train_time:99350ms step_avg:57.80ms
step:1720/2330 train_time:99410ms step_avg:57.80ms
step:1721/2330 train_time:99467ms step_avg:57.80ms
step:1722/2330 train_time:99528ms step_avg:57.80ms
step:1723/2330 train_time:99584ms step_avg:57.80ms
step:1724/2330 train_time:99645ms step_avg:57.80ms
step:1725/2330 train_time:99701ms step_avg:57.80ms
step:1726/2330 train_time:99762ms step_avg:57.80ms
step:1727/2330 train_time:99820ms step_avg:57.80ms
step:1728/2330 train_time:99881ms step_avg:57.80ms
step:1729/2330 train_time:99937ms step_avg:57.80ms
step:1730/2330 train_time:99997ms step_avg:57.80ms
step:1731/2330 train_time:100054ms step_avg:57.80ms
step:1732/2330 train_time:100114ms step_avg:57.80ms
step:1733/2330 train_time:100171ms step_avg:57.80ms
step:1734/2330 train_time:100231ms step_avg:57.80ms
step:1735/2330 train_time:100288ms step_avg:57.80ms
step:1736/2330 train_time:100348ms step_avg:57.80ms
step:1737/2330 train_time:100405ms step_avg:57.80ms
step:1738/2330 train_time:100465ms step_avg:57.81ms
step:1739/2330 train_time:100522ms step_avg:57.80ms
step:1740/2330 train_time:100583ms step_avg:57.81ms
step:1741/2330 train_time:100639ms step_avg:57.81ms
step:1742/2330 train_time:100699ms step_avg:57.81ms
step:1743/2330 train_time:100754ms step_avg:57.81ms
step:1744/2330 train_time:100816ms step_avg:57.81ms
step:1745/2330 train_time:100872ms step_avg:57.81ms
step:1746/2330 train_time:100933ms step_avg:57.81ms
step:1747/2330 train_time:100990ms step_avg:57.81ms
step:1748/2330 train_time:101050ms step_avg:57.81ms
step:1749/2330 train_time:101107ms step_avg:57.81ms
step:1750/2330 train_time:101169ms step_avg:57.81ms
step:1750/2330 val_loss:4.1649 train_time:101250ms step_avg:57.86ms
step:1751/2330 train_time:101269ms step_avg:57.84ms
step:1752/2330 train_time:101290ms step_avg:57.81ms
step:1753/2330 train_time:101343ms step_avg:57.81ms
step:1754/2330 train_time:101405ms step_avg:57.81ms
step:1755/2330 train_time:101462ms step_avg:57.81ms
step:1756/2330 train_time:101523ms step_avg:57.81ms
step:1757/2330 train_time:101579ms step_avg:57.81ms
step:1758/2330 train_time:101639ms step_avg:57.81ms
step:1759/2330 train_time:101695ms step_avg:57.81ms
step:1760/2330 train_time:101754ms step_avg:57.81ms
step:1761/2330 train_time:101810ms step_avg:57.81ms
step:1762/2330 train_time:101870ms step_avg:57.81ms
step:1763/2330 train_time:101926ms step_avg:57.81ms
step:1764/2330 train_time:101985ms step_avg:57.81ms
step:1765/2330 train_time:102042ms step_avg:57.81ms
step:1766/2330 train_time:102101ms step_avg:57.81ms
step:1767/2330 train_time:102159ms step_avg:57.82ms
step:1768/2330 train_time:102224ms step_avg:57.82ms
step:1769/2330 train_time:102282ms step_avg:57.82ms
step:1770/2330 train_time:102343ms step_avg:57.82ms
step:1771/2330 train_time:102400ms step_avg:57.82ms
step:1772/2330 train_time:102460ms step_avg:57.82ms
step:1773/2330 train_time:102517ms step_avg:57.82ms
step:1774/2330 train_time:102577ms step_avg:57.82ms
step:1775/2330 train_time:102633ms step_avg:57.82ms
step:1776/2330 train_time:102692ms step_avg:57.82ms
step:1777/2330 train_time:102748ms step_avg:57.82ms
step:1778/2330 train_time:102809ms step_avg:57.82ms
step:1779/2330 train_time:102866ms step_avg:57.82ms
step:1780/2330 train_time:102926ms step_avg:57.82ms
step:1781/2330 train_time:102983ms step_avg:57.82ms
step:1782/2330 train_time:103042ms step_avg:57.82ms
step:1783/2330 train_time:103100ms step_avg:57.82ms
step:1784/2330 train_time:103160ms step_avg:57.82ms
step:1785/2330 train_time:103218ms step_avg:57.82ms
step:1786/2330 train_time:103277ms step_avg:57.83ms
step:1787/2330 train_time:103334ms step_avg:57.83ms
step:1788/2330 train_time:103395ms step_avg:57.83ms
step:1789/2330 train_time:103453ms step_avg:57.83ms
step:1790/2330 train_time:103513ms step_avg:57.83ms
step:1791/2330 train_time:103570ms step_avg:57.83ms
step:1792/2330 train_time:103630ms step_avg:57.83ms
step:1793/2330 train_time:103687ms step_avg:57.83ms
step:1794/2330 train_time:103747ms step_avg:57.83ms
step:1795/2330 train_time:103803ms step_avg:57.83ms
step:1796/2330 train_time:103862ms step_avg:57.83ms
step:1797/2330 train_time:103918ms step_avg:57.83ms
step:1798/2330 train_time:103978ms step_avg:57.83ms
step:1799/2330 train_time:104034ms step_avg:57.83ms
step:1800/2330 train_time:104095ms step_avg:57.83ms
step:1801/2330 train_time:104152ms step_avg:57.83ms
step:1802/2330 train_time:104213ms step_avg:57.83ms
step:1803/2330 train_time:104270ms step_avg:57.83ms
step:1804/2330 train_time:104330ms step_avg:57.83ms
step:1805/2330 train_time:104388ms step_avg:57.83ms
step:1806/2330 train_time:104448ms step_avg:57.83ms
step:1807/2330 train_time:104506ms step_avg:57.83ms
step:1808/2330 train_time:104566ms step_avg:57.84ms
step:1809/2330 train_time:104623ms step_avg:57.83ms
step:1810/2330 train_time:104684ms step_avg:57.84ms
step:1811/2330 train_time:104740ms step_avg:57.84ms
step:1812/2330 train_time:104800ms step_avg:57.84ms
step:1813/2330 train_time:104856ms step_avg:57.84ms
step:1814/2330 train_time:104916ms step_avg:57.84ms
step:1815/2330 train_time:104973ms step_avg:57.84ms
step:1816/2330 train_time:105032ms step_avg:57.84ms
step:1817/2330 train_time:105089ms step_avg:57.84ms
step:1818/2330 train_time:105150ms step_avg:57.84ms
step:1819/2330 train_time:105208ms step_avg:57.84ms
step:1820/2330 train_time:105268ms step_avg:57.84ms
step:1821/2330 train_time:105326ms step_avg:57.84ms
step:1822/2330 train_time:105386ms step_avg:57.84ms
step:1823/2330 train_time:105443ms step_avg:57.84ms
step:1824/2330 train_time:105504ms step_avg:57.84ms
step:1825/2330 train_time:105560ms step_avg:57.84ms
step:1826/2330 train_time:105620ms step_avg:57.84ms
step:1827/2330 train_time:105677ms step_avg:57.84ms
step:1828/2330 train_time:105736ms step_avg:57.84ms
step:1829/2330 train_time:105793ms step_avg:57.84ms
step:1830/2330 train_time:105852ms step_avg:57.84ms
step:1831/2330 train_time:105908ms step_avg:57.84ms
step:1832/2330 train_time:105969ms step_avg:57.84ms
step:1833/2330 train_time:106026ms step_avg:57.84ms
step:1834/2330 train_time:106085ms step_avg:57.84ms
step:1835/2330 train_time:106141ms step_avg:57.84ms
step:1836/2330 train_time:106202ms step_avg:57.84ms
step:1837/2330 train_time:106260ms step_avg:57.84ms
step:1838/2330 train_time:106319ms step_avg:57.85ms
step:1839/2330 train_time:106376ms step_avg:57.84ms
step:1840/2330 train_time:106436ms step_avg:57.85ms
step:1841/2330 train_time:106493ms step_avg:57.85ms
step:1842/2330 train_time:106553ms step_avg:57.85ms
step:1843/2330 train_time:106611ms step_avg:57.85ms
step:1844/2330 train_time:106671ms step_avg:57.85ms
step:1845/2330 train_time:106728ms step_avg:57.85ms
step:1846/2330 train_time:106789ms step_avg:57.85ms
step:1847/2330 train_time:106846ms step_avg:57.85ms
step:1848/2330 train_time:106906ms step_avg:57.85ms
step:1849/2330 train_time:106963ms step_avg:57.85ms
step:1850/2330 train_time:107023ms step_avg:57.85ms
step:1851/2330 train_time:107079ms step_avg:57.85ms
step:1852/2330 train_time:107139ms step_avg:57.85ms
step:1853/2330 train_time:107195ms step_avg:57.85ms
step:1854/2330 train_time:107255ms step_avg:57.85ms
step:1855/2330 train_time:107312ms step_avg:57.85ms
step:1856/2330 train_time:107373ms step_avg:57.85ms
step:1857/2330 train_time:107430ms step_avg:57.85ms
step:1858/2330 train_time:107490ms step_avg:57.85ms
step:1859/2330 train_time:107547ms step_avg:57.85ms
step:1860/2330 train_time:107608ms step_avg:57.85ms
step:1861/2330 train_time:107666ms step_avg:57.85ms
step:1862/2330 train_time:107726ms step_avg:57.86ms
step:1863/2330 train_time:107783ms step_avg:57.85ms
step:1864/2330 train_time:107843ms step_avg:57.86ms
step:1865/2330 train_time:107900ms step_avg:57.86ms
step:1866/2330 train_time:107960ms step_avg:57.86ms
step:1867/2330 train_time:108018ms step_avg:57.86ms
step:1868/2330 train_time:108077ms step_avg:57.86ms
step:1869/2330 train_time:108133ms step_avg:57.86ms
step:1870/2330 train_time:108193ms step_avg:57.86ms
step:1871/2330 train_time:108250ms step_avg:57.86ms
step:1872/2330 train_time:108311ms step_avg:57.86ms
step:1873/2330 train_time:108367ms step_avg:57.86ms
step:1874/2330 train_time:108428ms step_avg:57.86ms
step:1875/2330 train_time:108484ms step_avg:57.86ms
step:1876/2330 train_time:108544ms step_avg:57.86ms
step:1877/2330 train_time:108602ms step_avg:57.86ms
step:1878/2330 train_time:108662ms step_avg:57.86ms
step:1879/2330 train_time:108720ms step_avg:57.86ms
step:1880/2330 train_time:108779ms step_avg:57.86ms
step:1881/2330 train_time:108836ms step_avg:57.86ms
step:1882/2330 train_time:108895ms step_avg:57.86ms
step:1883/2330 train_time:108952ms step_avg:57.86ms
step:1884/2330 train_time:109012ms step_avg:57.86ms
step:1885/2330 train_time:109070ms step_avg:57.86ms
step:1886/2330 train_time:109129ms step_avg:57.86ms
step:1887/2330 train_time:109186ms step_avg:57.86ms
step:1888/2330 train_time:109246ms step_avg:57.86ms
step:1889/2330 train_time:109302ms step_avg:57.86ms
step:1890/2330 train_time:109364ms step_avg:57.86ms
step:1891/2330 train_time:109421ms step_avg:57.86ms
step:1892/2330 train_time:109481ms step_avg:57.86ms
step:1893/2330 train_time:109537ms step_avg:57.86ms
step:1894/2330 train_time:109597ms step_avg:57.87ms
step:1895/2330 train_time:109654ms step_avg:57.86ms
step:1896/2330 train_time:109714ms step_avg:57.87ms
step:1897/2330 train_time:109771ms step_avg:57.87ms
step:1898/2330 train_time:109831ms step_avg:57.87ms
step:1899/2330 train_time:109888ms step_avg:57.87ms
step:1900/2330 train_time:109948ms step_avg:57.87ms
step:1901/2330 train_time:110005ms step_avg:57.87ms
step:1902/2330 train_time:110066ms step_avg:57.87ms
step:1903/2330 train_time:110122ms step_avg:57.87ms
step:1904/2330 train_time:110182ms step_avg:57.87ms
step:1905/2330 train_time:110239ms step_avg:57.87ms
step:1906/2330 train_time:110299ms step_avg:57.87ms
step:1907/2330 train_time:110356ms step_avg:57.87ms
step:1908/2330 train_time:110416ms step_avg:57.87ms
step:1909/2330 train_time:110472ms step_avg:57.87ms
step:1910/2330 train_time:110532ms step_avg:57.87ms
step:1911/2330 train_time:110590ms step_avg:57.87ms
step:1912/2330 train_time:110649ms step_avg:57.87ms
step:1913/2330 train_time:110706ms step_avg:57.87ms
step:1914/2330 train_time:110767ms step_avg:57.87ms
step:1915/2330 train_time:110824ms step_avg:57.87ms
step:1916/2330 train_time:110883ms step_avg:57.87ms
step:1917/2330 train_time:110940ms step_avg:57.87ms
step:1918/2330 train_time:111000ms step_avg:57.87ms
step:1919/2330 train_time:111058ms step_avg:57.87ms
step:1920/2330 train_time:111118ms step_avg:57.87ms
step:1921/2330 train_time:111175ms step_avg:57.87ms
step:1922/2330 train_time:111234ms step_avg:57.87ms
step:1923/2330 train_time:111291ms step_avg:57.87ms
step:1924/2330 train_time:111351ms step_avg:57.87ms
step:1925/2330 train_time:111408ms step_avg:57.87ms
step:1926/2330 train_time:111468ms step_avg:57.88ms
step:1927/2330 train_time:111525ms step_avg:57.88ms
step:1928/2330 train_time:111586ms step_avg:57.88ms
step:1929/2330 train_time:111643ms step_avg:57.88ms
step:1930/2330 train_time:111704ms step_avg:57.88ms
step:1931/2330 train_time:111762ms step_avg:57.88ms
step:1932/2330 train_time:111821ms step_avg:57.88ms
step:1933/2330 train_time:111878ms step_avg:57.88ms
step:1934/2330 train_time:111937ms step_avg:57.88ms
step:1935/2330 train_time:111994ms step_avg:57.88ms
step:1936/2330 train_time:112054ms step_avg:57.88ms
step:1937/2330 train_time:112111ms step_avg:57.88ms
step:1938/2330 train_time:112171ms step_avg:57.88ms
step:1939/2330 train_time:112228ms step_avg:57.88ms
step:1940/2330 train_time:112288ms step_avg:57.88ms
step:1941/2330 train_time:112345ms step_avg:57.88ms
step:1942/2330 train_time:112407ms step_avg:57.88ms
step:1943/2330 train_time:112464ms step_avg:57.88ms
step:1944/2330 train_time:112524ms step_avg:57.88ms
step:1945/2330 train_time:112581ms step_avg:57.88ms
step:1946/2330 train_time:112641ms step_avg:57.88ms
step:1947/2330 train_time:112698ms step_avg:57.88ms
step:1948/2330 train_time:112758ms step_avg:57.88ms
step:1949/2330 train_time:112814ms step_avg:57.88ms
step:1950/2330 train_time:112873ms step_avg:57.88ms
step:1951/2330 train_time:112929ms step_avg:57.88ms
step:1952/2330 train_time:112990ms step_avg:57.88ms
step:1953/2330 train_time:113048ms step_avg:57.88ms
step:1954/2330 train_time:113107ms step_avg:57.89ms
step:1955/2330 train_time:113164ms step_avg:57.88ms
step:1956/2330 train_time:113224ms step_avg:57.89ms
step:1957/2330 train_time:113282ms step_avg:57.89ms
step:1958/2330 train_time:113342ms step_avg:57.89ms
step:1959/2330 train_time:113399ms step_avg:57.89ms
step:1960/2330 train_time:113459ms step_avg:57.89ms
step:1961/2330 train_time:113515ms step_avg:57.89ms
step:1962/2330 train_time:113575ms step_avg:57.89ms
step:1963/2330 train_time:113631ms step_avg:57.89ms
step:1964/2330 train_time:113692ms step_avg:57.89ms
step:1965/2330 train_time:113748ms step_avg:57.89ms
step:1966/2330 train_time:113808ms step_avg:57.89ms
step:1967/2330 train_time:113866ms step_avg:57.89ms
step:1968/2330 train_time:113926ms step_avg:57.89ms
step:1969/2330 train_time:113983ms step_avg:57.89ms
step:1970/2330 train_time:114042ms step_avg:57.89ms
step:1971/2330 train_time:114099ms step_avg:57.89ms
step:1972/2330 train_time:114159ms step_avg:57.89ms
step:1973/2330 train_time:114216ms step_avg:57.89ms
step:1974/2330 train_time:114275ms step_avg:57.89ms
step:1975/2330 train_time:114332ms step_avg:57.89ms
step:1976/2330 train_time:114393ms step_avg:57.89ms
step:1977/2330 train_time:114450ms step_avg:57.89ms
step:1978/2330 train_time:114510ms step_avg:57.89ms
step:1979/2330 train_time:114568ms step_avg:57.89ms
step:1980/2330 train_time:114628ms step_avg:57.89ms
step:1981/2330 train_time:114685ms step_avg:57.89ms
step:1982/2330 train_time:114744ms step_avg:57.89ms
step:1983/2330 train_time:114802ms step_avg:57.89ms
step:1984/2330 train_time:114861ms step_avg:57.89ms
step:1985/2330 train_time:114918ms step_avg:57.89ms
step:1986/2330 train_time:114978ms step_avg:57.89ms
step:1987/2330 train_time:115035ms step_avg:57.89ms
step:1988/2330 train_time:115094ms step_avg:57.89ms
step:1989/2330 train_time:115151ms step_avg:57.89ms
step:1990/2330 train_time:115211ms step_avg:57.89ms
step:1991/2330 train_time:115268ms step_avg:57.89ms
step:1992/2330 train_time:115328ms step_avg:57.90ms
step:1993/2330 train_time:115386ms step_avg:57.90ms
step:1994/2330 train_time:115446ms step_avg:57.90ms
step:1995/2330 train_time:115503ms step_avg:57.90ms
step:1996/2330 train_time:115564ms step_avg:57.90ms
step:1997/2330 train_time:115622ms step_avg:57.90ms
step:1998/2330 train_time:115681ms step_avg:57.90ms
step:1999/2330 train_time:115738ms step_avg:57.90ms
step:2000/2330 train_time:115798ms step_avg:57.90ms
step:2000/2330 val_loss:4.0973 train_time:115877ms step_avg:57.94ms
step:2001/2330 train_time:115895ms step_avg:57.92ms
step:2002/2330 train_time:115916ms step_avg:57.90ms
step:2003/2330 train_time:115973ms step_avg:57.90ms
step:2004/2330 train_time:116039ms step_avg:57.90ms
step:2005/2330 train_time:116096ms step_avg:57.90ms
step:2006/2330 train_time:116156ms step_avg:57.90ms
step:2007/2330 train_time:116212ms step_avg:57.90ms
step:2008/2330 train_time:116274ms step_avg:57.91ms
step:2009/2330 train_time:116329ms step_avg:57.90ms
step:2010/2330 train_time:116391ms step_avg:57.91ms
step:2011/2330 train_time:116447ms step_avg:57.91ms
step:2012/2330 train_time:116507ms step_avg:57.91ms
step:2013/2330 train_time:116563ms step_avg:57.90ms
step:2014/2330 train_time:116622ms step_avg:57.91ms
step:2015/2330 train_time:116678ms step_avg:57.90ms
step:2016/2330 train_time:116737ms step_avg:57.91ms
step:2017/2330 train_time:116793ms step_avg:57.90ms
step:2018/2330 train_time:116854ms step_avg:57.91ms
step:2019/2330 train_time:116912ms step_avg:57.91ms
step:2020/2330 train_time:116974ms step_avg:57.91ms
step:2021/2330 train_time:117033ms step_avg:57.91ms
step:2022/2330 train_time:117093ms step_avg:57.91ms
step:2023/2330 train_time:117151ms step_avg:57.91ms
step:2024/2330 train_time:117210ms step_avg:57.91ms
step:2025/2330 train_time:117267ms step_avg:57.91ms
step:2026/2330 train_time:117328ms step_avg:57.91ms
step:2027/2330 train_time:117384ms step_avg:57.91ms
step:2028/2330 train_time:117444ms step_avg:57.91ms
step:2029/2330 train_time:117500ms step_avg:57.91ms
step:2030/2330 train_time:117560ms step_avg:57.91ms
step:2031/2330 train_time:117616ms step_avg:57.91ms
step:2032/2330 train_time:117676ms step_avg:57.91ms
step:2033/2330 train_time:117732ms step_avg:57.91ms
step:2034/2330 train_time:117792ms step_avg:57.91ms
step:2035/2330 train_time:117849ms step_avg:57.91ms
step:2036/2330 train_time:117909ms step_avg:57.91ms
step:2037/2330 train_time:117967ms step_avg:57.91ms
step:2038/2330 train_time:118028ms step_avg:57.91ms
step:2039/2330 train_time:118086ms step_avg:57.91ms
step:2040/2330 train_time:118147ms step_avg:57.92ms
step:2041/2330 train_time:118204ms step_avg:57.91ms
step:2042/2330 train_time:118264ms step_avg:57.92ms
step:2043/2330 train_time:118321ms step_avg:57.92ms
step:2044/2330 train_time:118380ms step_avg:57.92ms
step:2045/2330 train_time:118436ms step_avg:57.92ms
step:2046/2330 train_time:118497ms step_avg:57.92ms
step:2047/2330 train_time:118553ms step_avg:57.92ms
step:2048/2330 train_time:118612ms step_avg:57.92ms
step:2049/2330 train_time:118669ms step_avg:57.92ms
step:2050/2330 train_time:118729ms step_avg:57.92ms
step:2051/2330 train_time:118786ms step_avg:57.92ms
step:2052/2330 train_time:118847ms step_avg:57.92ms
step:2053/2330 train_time:118904ms step_avg:57.92ms
step:2054/2330 train_time:118964ms step_avg:57.92ms
step:2055/2330 train_time:119022ms step_avg:57.92ms
step:2056/2330 train_time:119082ms step_avg:57.92ms
step:2057/2330 train_time:119138ms step_avg:57.92ms
step:2058/2330 train_time:119198ms step_avg:57.92ms
step:2059/2330 train_time:119254ms step_avg:57.92ms
step:2060/2330 train_time:119315ms step_avg:57.92ms
step:2061/2330 train_time:119371ms step_avg:57.92ms
step:2062/2330 train_time:119432ms step_avg:57.92ms
step:2063/2330 train_time:119490ms step_avg:57.92ms
step:2064/2330 train_time:119549ms step_avg:57.92ms
step:2065/2330 train_time:119606ms step_avg:57.92ms
step:2066/2330 train_time:119665ms step_avg:57.92ms
step:2067/2330 train_time:119722ms step_avg:57.92ms
step:2068/2330 train_time:119783ms step_avg:57.92ms
step:2069/2330 train_time:119840ms step_avg:57.92ms
step:2070/2330 train_time:119899ms step_avg:57.92ms
step:2071/2330 train_time:119955ms step_avg:57.92ms
step:2072/2330 train_time:120018ms step_avg:57.92ms
step:2073/2330 train_time:120075ms step_avg:57.92ms
step:2074/2330 train_time:120135ms step_avg:57.92ms
step:2075/2330 train_time:120192ms step_avg:57.92ms
step:2076/2330 train_time:120251ms step_avg:57.92ms
step:2077/2330 train_time:120308ms step_avg:57.92ms
step:2078/2330 train_time:120369ms step_avg:57.93ms
step:2079/2330 train_time:120425ms step_avg:57.92ms
step:2080/2330 train_time:120486ms step_avg:57.93ms
step:2081/2330 train_time:120543ms step_avg:57.93ms
step:2082/2330 train_time:120603ms step_avg:57.93ms
step:2083/2330 train_time:120659ms step_avg:57.93ms
step:2084/2330 train_time:120719ms step_avg:57.93ms
step:2085/2330 train_time:120775ms step_avg:57.93ms
step:2086/2330 train_time:120836ms step_avg:57.93ms
step:2087/2330 train_time:120893ms step_avg:57.93ms
step:2088/2330 train_time:120953ms step_avg:57.93ms
step:2089/2330 train_time:121010ms step_avg:57.93ms
step:2090/2330 train_time:121070ms step_avg:57.93ms
step:2091/2330 train_time:121128ms step_avg:57.93ms
step:2092/2330 train_time:121188ms step_avg:57.93ms
step:2093/2330 train_time:121245ms step_avg:57.93ms
step:2094/2330 train_time:121304ms step_avg:57.93ms
step:2095/2330 train_time:121360ms step_avg:57.93ms
step:2096/2330 train_time:121421ms step_avg:57.93ms
step:2097/2330 train_time:121477ms step_avg:57.93ms
step:2098/2330 train_time:121537ms step_avg:57.93ms
step:2099/2330 train_time:121593ms step_avg:57.93ms
step:2100/2330 train_time:121654ms step_avg:57.93ms
step:2101/2330 train_time:121711ms step_avg:57.93ms
step:2102/2330 train_time:121771ms step_avg:57.93ms
step:2103/2330 train_time:121828ms step_avg:57.93ms
step:2104/2330 train_time:121888ms step_avg:57.93ms
step:2105/2330 train_time:121945ms step_avg:57.93ms
step:2106/2330 train_time:122005ms step_avg:57.93ms
step:2107/2330 train_time:122062ms step_avg:57.93ms
step:2108/2330 train_time:122122ms step_avg:57.93ms
step:2109/2330 train_time:122179ms step_avg:57.93ms
step:2110/2330 train_time:122238ms step_avg:57.93ms
step:2111/2330 train_time:122295ms step_avg:57.93ms
step:2112/2330 train_time:122356ms step_avg:57.93ms
step:2113/2330 train_time:122413ms step_avg:57.93ms
step:2114/2330 train_time:122474ms step_avg:57.93ms
step:2115/2330 train_time:122531ms step_avg:57.93ms
step:2116/2330 train_time:122591ms step_avg:57.94ms
step:2117/2330 train_time:122648ms step_avg:57.93ms
step:2118/2330 train_time:122707ms step_avg:57.94ms
step:2119/2330 train_time:122764ms step_avg:57.94ms
step:2120/2330 train_time:122824ms step_avg:57.94ms
step:2121/2330 train_time:122881ms step_avg:57.94ms
step:2122/2330 train_time:122940ms step_avg:57.94ms
step:2123/2330 train_time:122997ms step_avg:57.94ms
step:2124/2330 train_time:123056ms step_avg:57.94ms
step:2125/2330 train_time:123113ms step_avg:57.94ms
step:2126/2330 train_time:123173ms step_avg:57.94ms
step:2127/2330 train_time:123230ms step_avg:57.94ms
step:2128/2330 train_time:123291ms step_avg:57.94ms
step:2129/2330 train_time:123348ms step_avg:57.94ms
step:2130/2330 train_time:123408ms step_avg:57.94ms
step:2131/2330 train_time:123465ms step_avg:57.94ms
step:2132/2330 train_time:123526ms step_avg:57.94ms
step:2133/2330 train_time:123583ms step_avg:57.94ms
step:2134/2330 train_time:123643ms step_avg:57.94ms
step:2135/2330 train_time:123699ms step_avg:57.94ms
step:2136/2330 train_time:123759ms step_avg:57.94ms
step:2137/2330 train_time:123815ms step_avg:57.94ms
step:2138/2330 train_time:123876ms step_avg:57.94ms
step:2139/2330 train_time:123932ms step_avg:57.94ms
step:2140/2330 train_time:123993ms step_avg:57.94ms
step:2141/2330 train_time:124050ms step_avg:57.94ms
step:2142/2330 train_time:124110ms step_avg:57.94ms
step:2143/2330 train_time:124167ms step_avg:57.94ms
step:2144/2330 train_time:124227ms step_avg:57.94ms
step:2145/2330 train_time:124284ms step_avg:57.94ms
step:2146/2330 train_time:124344ms step_avg:57.94ms
step:2147/2330 train_time:124401ms step_avg:57.94ms
step:2148/2330 train_time:124460ms step_avg:57.94ms
step:2149/2330 train_time:124517ms step_avg:57.94ms
step:2150/2330 train_time:124578ms step_avg:57.94ms
step:2151/2330 train_time:124635ms step_avg:57.94ms
step:2152/2330 train_time:124694ms step_avg:57.94ms
step:2153/2330 train_time:124751ms step_avg:57.94ms
step:2154/2330 train_time:124812ms step_avg:57.94ms
step:2155/2330 train_time:124868ms step_avg:57.94ms
step:2156/2330 train_time:124929ms step_avg:57.94ms
step:2157/2330 train_time:124986ms step_avg:57.94ms
step:2158/2330 train_time:125047ms step_avg:57.95ms
step:2159/2330 train_time:125104ms step_avg:57.95ms
step:2160/2330 train_time:125164ms step_avg:57.95ms
step:2161/2330 train_time:125220ms step_avg:57.95ms
step:2162/2330 train_time:125280ms step_avg:57.95ms
step:2163/2330 train_time:125337ms step_avg:57.95ms
step:2164/2330 train_time:125397ms step_avg:57.95ms
step:2165/2330 train_time:125454ms step_avg:57.95ms
step:2166/2330 train_time:125515ms step_avg:57.95ms
step:2167/2330 train_time:125572ms step_avg:57.95ms
step:2168/2330 train_time:125632ms step_avg:57.95ms
step:2169/2330 train_time:125689ms step_avg:57.95ms
step:2170/2330 train_time:125749ms step_avg:57.95ms
step:2171/2330 train_time:125807ms step_avg:57.95ms
step:2172/2330 train_time:125867ms step_avg:57.95ms
step:2173/2330 train_time:125924ms step_avg:57.95ms
step:2174/2330 train_time:125983ms step_avg:57.95ms
step:2175/2330 train_time:126040ms step_avg:57.95ms
step:2176/2330 train_time:126100ms step_avg:57.95ms
step:2177/2330 train_time:126155ms step_avg:57.95ms
step:2178/2330 train_time:126216ms step_avg:57.95ms
step:2179/2330 train_time:126273ms step_avg:57.95ms
step:2180/2330 train_time:126334ms step_avg:57.95ms
step:2181/2330 train_time:126391ms step_avg:57.95ms
step:2182/2330 train_time:126451ms step_avg:57.95ms
step:2183/2330 train_time:126508ms step_avg:57.95ms
step:2184/2330 train_time:126568ms step_avg:57.95ms
step:2185/2330 train_time:126625ms step_avg:57.95ms
step:2186/2330 train_time:126685ms step_avg:57.95ms
step:2187/2330 train_time:126742ms step_avg:57.95ms
step:2188/2330 train_time:126802ms step_avg:57.95ms
step:2189/2330 train_time:126859ms step_avg:57.95ms
step:2190/2330 train_time:126919ms step_avg:57.95ms
step:2191/2330 train_time:126975ms step_avg:57.95ms
step:2192/2330 train_time:127036ms step_avg:57.95ms
step:2193/2330 train_time:127092ms step_avg:57.95ms
step:2194/2330 train_time:127153ms step_avg:57.95ms
step:2195/2330 train_time:127209ms step_avg:57.95ms
step:2196/2330 train_time:127270ms step_avg:57.96ms
step:2197/2330 train_time:127327ms step_avg:57.95ms
step:2198/2330 train_time:127387ms step_avg:57.96ms
step:2199/2330 train_time:127444ms step_avg:57.96ms
step:2200/2330 train_time:127505ms step_avg:57.96ms
step:2201/2330 train_time:127561ms step_avg:57.96ms
step:2202/2330 train_time:127621ms step_avg:57.96ms
step:2203/2330 train_time:127678ms step_avg:57.96ms
step:2204/2330 train_time:127738ms step_avg:57.96ms
step:2205/2330 train_time:127794ms step_avg:57.96ms
step:2206/2330 train_time:127855ms step_avg:57.96ms
step:2207/2330 train_time:127911ms step_avg:57.96ms
step:2208/2330 train_time:127971ms step_avg:57.96ms
step:2209/2330 train_time:128029ms step_avg:57.96ms
step:2210/2330 train_time:128089ms step_avg:57.96ms
step:2211/2330 train_time:128147ms step_avg:57.96ms
step:2212/2330 train_time:128207ms step_avg:57.96ms
step:2213/2330 train_time:128262ms step_avg:57.96ms
step:2214/2330 train_time:128324ms step_avg:57.96ms
step:2215/2330 train_time:128380ms step_avg:57.96ms
step:2216/2330 train_time:128440ms step_avg:57.96ms
step:2217/2330 train_time:128497ms step_avg:57.96ms
step:2218/2330 train_time:128557ms step_avg:57.96ms
step:2219/2330 train_time:128613ms step_avg:57.96ms
step:2220/2330 train_time:128673ms step_avg:57.96ms
step:2221/2330 train_time:128730ms step_avg:57.96ms
step:2222/2330 train_time:128790ms step_avg:57.96ms
step:2223/2330 train_time:128848ms step_avg:57.96ms
step:2224/2330 train_time:128908ms step_avg:57.96ms
step:2225/2330 train_time:128965ms step_avg:57.96ms
step:2226/2330 train_time:129024ms step_avg:57.96ms
step:2227/2330 train_time:129081ms step_avg:57.96ms
step:2228/2330 train_time:129142ms step_avg:57.96ms
step:2229/2330 train_time:129198ms step_avg:57.96ms
step:2230/2330 train_time:129257ms step_avg:57.96ms
step:2231/2330 train_time:129313ms step_avg:57.96ms
step:2232/2330 train_time:129374ms step_avg:57.96ms
step:2233/2330 train_time:129431ms step_avg:57.96ms
step:2234/2330 train_time:129491ms step_avg:57.96ms
step:2235/2330 train_time:129548ms step_avg:57.96ms
step:2236/2330 train_time:129608ms step_avg:57.96ms
step:2237/2330 train_time:129665ms step_avg:57.96ms
step:2238/2330 train_time:129725ms step_avg:57.96ms
step:2239/2330 train_time:129782ms step_avg:57.96ms
step:2240/2330 train_time:129842ms step_avg:57.97ms
step:2241/2330 train_time:129899ms step_avg:57.96ms
step:2242/2330 train_time:129959ms step_avg:57.97ms
step:2243/2330 train_time:130016ms step_avg:57.97ms
step:2244/2330 train_time:130076ms step_avg:57.97ms
step:2245/2330 train_time:130133ms step_avg:57.97ms
step:2246/2330 train_time:130193ms step_avg:57.97ms
step:2247/2330 train_time:130249ms step_avg:57.97ms
step:2248/2330 train_time:130311ms step_avg:57.97ms
step:2249/2330 train_time:130367ms step_avg:57.97ms
step:2250/2330 train_time:130428ms step_avg:57.97ms
step:2250/2330 val_loss:4.0455 train_time:130508ms step_avg:58.00ms
step:2251/2330 train_time:130526ms step_avg:57.99ms
step:2252/2330 train_time:130547ms step_avg:57.97ms
step:2253/2330 train_time:130604ms step_avg:57.97ms
step:2254/2330 train_time:130670ms step_avg:57.97ms
step:2255/2330 train_time:130727ms step_avg:57.97ms
step:2256/2330 train_time:130791ms step_avg:57.97ms
step:2257/2330 train_time:130846ms step_avg:57.97ms
step:2258/2330 train_time:130908ms step_avg:57.97ms
step:2259/2330 train_time:130964ms step_avg:57.97ms
step:2260/2330 train_time:131023ms step_avg:57.97ms
step:2261/2330 train_time:131080ms step_avg:57.97ms
step:2262/2330 train_time:131138ms step_avg:57.97ms
step:2263/2330 train_time:131194ms step_avg:57.97ms
step:2264/2330 train_time:131255ms step_avg:57.97ms
step:2265/2330 train_time:131311ms step_avg:57.97ms
step:2266/2330 train_time:131369ms step_avg:57.97ms
step:2267/2330 train_time:131426ms step_avg:57.97ms
step:2268/2330 train_time:131486ms step_avg:57.97ms
step:2269/2330 train_time:131545ms step_avg:57.97ms
step:2270/2330 train_time:131606ms step_avg:57.98ms
step:2271/2330 train_time:131664ms step_avg:57.98ms
step:2272/2330 train_time:131726ms step_avg:57.98ms
step:2273/2330 train_time:131783ms step_avg:57.98ms
step:2274/2330 train_time:131844ms step_avg:57.98ms
step:2275/2330 train_time:131900ms step_avg:57.98ms
step:2276/2330 train_time:131961ms step_avg:57.98ms
step:2277/2330 train_time:132017ms step_avg:57.98ms
step:2278/2330 train_time:132077ms step_avg:57.98ms
step:2279/2330 train_time:132133ms step_avg:57.98ms
step:2280/2330 train_time:132193ms step_avg:57.98ms
step:2281/2330 train_time:132250ms step_avg:57.98ms
step:2282/2330 train_time:132308ms step_avg:57.98ms
step:2283/2330 train_time:132365ms step_avg:57.98ms
step:2284/2330 train_time:132424ms step_avg:57.98ms
step:2285/2330 train_time:132481ms step_avg:57.98ms
step:2286/2330 train_time:132541ms step_avg:57.98ms
step:2287/2330 train_time:132599ms step_avg:57.98ms
step:2288/2330 train_time:132661ms step_avg:57.98ms
step:2289/2330 train_time:132718ms step_avg:57.98ms
step:2290/2330 train_time:132779ms step_avg:57.98ms
step:2291/2330 train_time:132836ms step_avg:57.98ms
step:2292/2330 train_time:132896ms step_avg:57.98ms
step:2293/2330 train_time:132953ms step_avg:57.98ms
step:2294/2330 train_time:133014ms step_avg:57.98ms
step:2295/2330 train_time:133071ms step_avg:57.98ms
step:2296/2330 train_time:133130ms step_avg:57.98ms
step:2297/2330 train_time:133186ms step_avg:57.98ms
step:2298/2330 train_time:133246ms step_avg:57.98ms
step:2299/2330 train_time:133302ms step_avg:57.98ms
step:2300/2330 train_time:133362ms step_avg:57.98ms
step:2301/2330 train_time:133419ms step_avg:57.98ms
step:2302/2330 train_time:133479ms step_avg:57.98ms
step:2303/2330 train_time:133537ms step_avg:57.98ms
step:2304/2330 train_time:133597ms step_avg:57.98ms
step:2305/2330 train_time:133655ms step_avg:57.98ms
step:2306/2330 train_time:133714ms step_avg:57.99ms
step:2307/2330 train_time:133771ms step_avg:57.98ms
step:2308/2330 train_time:133833ms step_avg:57.99ms
step:2309/2330 train_time:133889ms step_avg:57.99ms
step:2310/2330 train_time:133950ms step_avg:57.99ms
step:2311/2330 train_time:134007ms step_avg:57.99ms
step:2312/2330 train_time:134067ms step_avg:57.99ms
step:2313/2330 train_time:134123ms step_avg:57.99ms
step:2314/2330 train_time:134183ms step_avg:57.99ms
step:2315/2330 train_time:134239ms step_avg:57.99ms
step:2316/2330 train_time:134299ms step_avg:57.99ms
step:2317/2330 train_time:134355ms step_avg:57.99ms
step:2318/2330 train_time:134415ms step_avg:57.99ms
step:2319/2330 train_time:134471ms step_avg:57.99ms
step:2320/2330 train_time:134531ms step_avg:57.99ms
step:2321/2330 train_time:134588ms step_avg:57.99ms
step:2322/2330 train_time:134648ms step_avg:57.99ms
step:2323/2330 train_time:134705ms step_avg:57.99ms
step:2324/2330 train_time:134766ms step_avg:57.99ms
step:2325/2330 train_time:134823ms step_avg:57.99ms
step:2326/2330 train_time:134883ms step_avg:57.99ms
step:2327/2330 train_time:134941ms step_avg:57.99ms
step:2328/2330 train_time:135001ms step_avg:57.99ms
step:2329/2330 train_time:135059ms step_avg:57.99ms
step:2330/2330 train_time:135118ms step_avg:57.99ms
step:2330/2330 val_loss:4.0302 train_time:135199ms step_avg:58.03ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
