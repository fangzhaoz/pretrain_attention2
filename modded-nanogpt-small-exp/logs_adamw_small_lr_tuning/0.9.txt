import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--adamw_lr', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_lr_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_lr_tuning/{args2.adamw_lr}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=float(args2.adamw_lr),  betas=(0.85, 0.85), eps=1e-10, weight_decay=0.0, fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_lr_tuning/{args2.adamw_lr}", exist_ok=True)
            torch.save(log, f"logs_adamw_small_lr_tuning/{args2.adamw_lr}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 03:26:13 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   31C    P0             115W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   27C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   27C    P0             117W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   26C    P0             114W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   30C    P0             115W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   27C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:84ms step_avg:83.69ms
step:2/2330 train_time:176ms step_avg:88.21ms
step:3/2330 train_time:194ms step_avg:64.67ms
step:4/2330 train_time:213ms step_avg:53.35ms
step:5/2330 train_time:267ms step_avg:53.44ms
step:6/2330 train_time:325ms step_avg:54.16ms
step:7/2330 train_time:379ms step_avg:54.12ms
step:8/2330 train_time:436ms step_avg:54.45ms
step:9/2330 train_time:489ms step_avg:54.36ms
step:10/2330 train_time:546ms step_avg:54.61ms
step:11/2330 train_time:600ms step_avg:54.56ms
step:12/2330 train_time:657ms step_avg:54.74ms
step:13/2330 train_time:711ms step_avg:54.69ms
step:14/2330 train_time:768ms step_avg:54.82ms
step:15/2330 train_time:822ms step_avg:54.77ms
step:16/2330 train_time:878ms step_avg:54.89ms
step:17/2330 train_time:932ms step_avg:54.84ms
step:18/2330 train_time:989ms step_avg:54.94ms
step:19/2330 train_time:1043ms step_avg:54.88ms
step:20/2330 train_time:1100ms step_avg:54.98ms
step:21/2330 train_time:1154ms step_avg:54.95ms
step:22/2330 train_time:1213ms step_avg:55.13ms
step:23/2330 train_time:1267ms step_avg:55.09ms
step:24/2330 train_time:1325ms step_avg:55.21ms
step:25/2330 train_time:1379ms step_avg:55.16ms
step:26/2330 train_time:1437ms step_avg:55.27ms
step:27/2330 train_time:1491ms step_avg:55.22ms
step:28/2330 train_time:1548ms step_avg:55.29ms
step:29/2330 train_time:1602ms step_avg:55.25ms
step:30/2330 train_time:1660ms step_avg:55.33ms
step:31/2330 train_time:1714ms step_avg:55.29ms
step:32/2330 train_time:1771ms step_avg:55.35ms
step:33/2330 train_time:1825ms step_avg:55.31ms
step:34/2330 train_time:1882ms step_avg:55.36ms
step:35/2330 train_time:1936ms step_avg:55.32ms
step:36/2330 train_time:1994ms step_avg:55.38ms
step:37/2330 train_time:2047ms step_avg:55.33ms
step:38/2330 train_time:2105ms step_avg:55.38ms
step:39/2330 train_time:2159ms step_avg:55.36ms
step:40/2330 train_time:2217ms step_avg:55.41ms
step:41/2330 train_time:2270ms step_avg:55.38ms
step:42/2330 train_time:2328ms step_avg:55.42ms
step:43/2330 train_time:2382ms step_avg:55.39ms
step:44/2330 train_time:2439ms step_avg:55.43ms
step:45/2330 train_time:2494ms step_avg:55.42ms
step:46/2330 train_time:2551ms step_avg:55.45ms
step:47/2330 train_time:2606ms step_avg:55.44ms
step:48/2330 train_time:2663ms step_avg:55.48ms
step:49/2330 train_time:2717ms step_avg:55.45ms
step:50/2330 train_time:2774ms step_avg:55.49ms
step:51/2330 train_time:2829ms step_avg:55.48ms
step:52/2330 train_time:2886ms step_avg:55.51ms
step:53/2330 train_time:2940ms step_avg:55.48ms
step:54/2330 train_time:2999ms step_avg:55.54ms
step:55/2330 train_time:3054ms step_avg:55.52ms
step:56/2330 train_time:3112ms step_avg:55.57ms
step:57/2330 train_time:3166ms step_avg:55.55ms
step:58/2330 train_time:3224ms step_avg:55.58ms
step:59/2330 train_time:3278ms step_avg:55.56ms
step:60/2330 train_time:3336ms step_avg:55.59ms
step:61/2330 train_time:3390ms step_avg:55.57ms
step:62/2330 train_time:3447ms step_avg:55.60ms
step:63/2330 train_time:3501ms step_avg:55.58ms
step:64/2330 train_time:3559ms step_avg:55.61ms
step:65/2330 train_time:3614ms step_avg:55.59ms
step:66/2330 train_time:3671ms step_avg:55.62ms
step:67/2330 train_time:3725ms step_avg:55.60ms
step:68/2330 train_time:3784ms step_avg:55.65ms
step:69/2330 train_time:3838ms step_avg:55.62ms
step:70/2330 train_time:3896ms step_avg:55.66ms
step:71/2330 train_time:3950ms step_avg:55.64ms
step:72/2330 train_time:4007ms step_avg:55.66ms
step:73/2330 train_time:4061ms step_avg:55.63ms
step:74/2330 train_time:4120ms step_avg:55.67ms
step:75/2330 train_time:4174ms step_avg:55.66ms
step:76/2330 train_time:4231ms step_avg:55.68ms
step:77/2330 train_time:4286ms step_avg:55.67ms
step:78/2330 train_time:4344ms step_avg:55.69ms
step:79/2330 train_time:4398ms step_avg:55.67ms
step:80/2330 train_time:4455ms step_avg:55.69ms
step:81/2330 train_time:4510ms step_avg:55.68ms
step:82/2330 train_time:4567ms step_avg:55.70ms
step:83/2330 train_time:4622ms step_avg:55.69ms
step:84/2330 train_time:4680ms step_avg:55.71ms
step:85/2330 train_time:4735ms step_avg:55.70ms
step:86/2330 train_time:4793ms step_avg:55.73ms
step:87/2330 train_time:4848ms step_avg:55.72ms
step:88/2330 train_time:4905ms step_avg:55.74ms
step:89/2330 train_time:4960ms step_avg:55.73ms
step:90/2330 train_time:5017ms step_avg:55.75ms
step:91/2330 train_time:5072ms step_avg:55.74ms
step:92/2330 train_time:5129ms step_avg:55.75ms
step:93/2330 train_time:5184ms step_avg:55.74ms
step:94/2330 train_time:5241ms step_avg:55.76ms
step:95/2330 train_time:5296ms step_avg:55.75ms
step:96/2330 train_time:5355ms step_avg:55.78ms
step:97/2330 train_time:5410ms step_avg:55.77ms
step:98/2330 train_time:5467ms step_avg:55.79ms
step:99/2330 train_time:5521ms step_avg:55.77ms
step:100/2330 train_time:5581ms step_avg:55.81ms
step:101/2330 train_time:5636ms step_avg:55.80ms
step:102/2330 train_time:5693ms step_avg:55.81ms
step:103/2330 train_time:5748ms step_avg:55.80ms
step:104/2330 train_time:5806ms step_avg:55.83ms
step:105/2330 train_time:5861ms step_avg:55.82ms
step:106/2330 train_time:5919ms step_avg:55.84ms
step:107/2330 train_time:5974ms step_avg:55.83ms
step:108/2330 train_time:6032ms step_avg:55.85ms
step:109/2330 train_time:6087ms step_avg:55.84ms
step:110/2330 train_time:6145ms step_avg:55.86ms
step:111/2330 train_time:6199ms step_avg:55.85ms
step:112/2330 train_time:6257ms step_avg:55.86ms
step:113/2330 train_time:6311ms step_avg:55.85ms
step:114/2330 train_time:6369ms step_avg:55.87ms
step:115/2330 train_time:6423ms step_avg:55.86ms
step:116/2330 train_time:6481ms step_avg:55.87ms
step:117/2330 train_time:6536ms step_avg:55.87ms
step:118/2330 train_time:6595ms step_avg:55.89ms
step:119/2330 train_time:6650ms step_avg:55.88ms
step:120/2330 train_time:6708ms step_avg:55.90ms
step:121/2330 train_time:6762ms step_avg:55.89ms
step:122/2330 train_time:6821ms step_avg:55.91ms
step:123/2330 train_time:6876ms step_avg:55.90ms
step:124/2330 train_time:6934ms step_avg:55.92ms
step:125/2330 train_time:6989ms step_avg:55.92ms
step:126/2330 train_time:7048ms step_avg:55.93ms
step:127/2330 train_time:7102ms step_avg:55.92ms
step:128/2330 train_time:7161ms step_avg:55.94ms
step:129/2330 train_time:7216ms step_avg:55.93ms
step:130/2330 train_time:7274ms step_avg:55.95ms
step:131/2330 train_time:7329ms step_avg:55.94ms
step:132/2330 train_time:7386ms step_avg:55.96ms
step:133/2330 train_time:7441ms step_avg:55.95ms
step:134/2330 train_time:7499ms step_avg:55.96ms
step:135/2330 train_time:7554ms step_avg:55.96ms
step:136/2330 train_time:7612ms step_avg:55.97ms
step:137/2330 train_time:7667ms step_avg:55.96ms
step:138/2330 train_time:7725ms step_avg:55.98ms
step:139/2330 train_time:7780ms step_avg:55.97ms
step:140/2330 train_time:7839ms step_avg:55.99ms
step:141/2330 train_time:7893ms step_avg:55.98ms
step:142/2330 train_time:7952ms step_avg:56.00ms
step:143/2330 train_time:8006ms step_avg:55.99ms
step:144/2330 train_time:8065ms step_avg:56.01ms
step:145/2330 train_time:8119ms step_avg:55.99ms
step:146/2330 train_time:8178ms step_avg:56.02ms
step:147/2330 train_time:8233ms step_avg:56.01ms
step:148/2330 train_time:8292ms step_avg:56.03ms
step:149/2330 train_time:8346ms step_avg:56.02ms
step:150/2330 train_time:8405ms step_avg:56.03ms
step:151/2330 train_time:8459ms step_avg:56.02ms
step:152/2330 train_time:8517ms step_avg:56.04ms
step:153/2330 train_time:8572ms step_avg:56.03ms
step:154/2330 train_time:8630ms step_avg:56.04ms
step:155/2330 train_time:8685ms step_avg:56.03ms
step:156/2330 train_time:8743ms step_avg:56.05ms
step:157/2330 train_time:8798ms step_avg:56.04ms
step:158/2330 train_time:8856ms step_avg:56.05ms
step:159/2330 train_time:8912ms step_avg:56.05ms
step:160/2330 train_time:8970ms step_avg:56.06ms
step:161/2330 train_time:9025ms step_avg:56.06ms
step:162/2330 train_time:9083ms step_avg:56.07ms
step:163/2330 train_time:9138ms step_avg:56.06ms
step:164/2330 train_time:9197ms step_avg:56.08ms
step:165/2330 train_time:9251ms step_avg:56.07ms
step:166/2330 train_time:9310ms step_avg:56.08ms
step:167/2330 train_time:9365ms step_avg:56.08ms
step:168/2330 train_time:9423ms step_avg:56.09ms
step:169/2330 train_time:9477ms step_avg:56.08ms
step:170/2330 train_time:9536ms step_avg:56.10ms
step:171/2330 train_time:9592ms step_avg:56.09ms
step:172/2330 train_time:9650ms step_avg:56.11ms
step:173/2330 train_time:9706ms step_avg:56.10ms
step:174/2330 train_time:9764ms step_avg:56.11ms
step:175/2330 train_time:9819ms step_avg:56.11ms
step:176/2330 train_time:9878ms step_avg:56.12ms
step:177/2330 train_time:9933ms step_avg:56.12ms
step:178/2330 train_time:9991ms step_avg:56.13ms
step:179/2330 train_time:10047ms step_avg:56.13ms
step:180/2330 train_time:10106ms step_avg:56.14ms
step:181/2330 train_time:10160ms step_avg:56.14ms
step:182/2330 train_time:10220ms step_avg:56.15ms
step:183/2330 train_time:10276ms step_avg:56.15ms
step:184/2330 train_time:10334ms step_avg:56.16ms
step:185/2330 train_time:10389ms step_avg:56.16ms
step:186/2330 train_time:10447ms step_avg:56.17ms
step:187/2330 train_time:10502ms step_avg:56.16ms
step:188/2330 train_time:10561ms step_avg:56.18ms
step:189/2330 train_time:10617ms step_avg:56.17ms
step:190/2330 train_time:10676ms step_avg:56.19ms
step:191/2330 train_time:10732ms step_avg:56.19ms
step:192/2330 train_time:10790ms step_avg:56.20ms
step:193/2330 train_time:10845ms step_avg:56.19ms
step:194/2330 train_time:10904ms step_avg:56.20ms
step:195/2330 train_time:10959ms step_avg:56.20ms
step:196/2330 train_time:11018ms step_avg:56.21ms
step:197/2330 train_time:11073ms step_avg:56.21ms
step:198/2330 train_time:11132ms step_avg:56.22ms
step:199/2330 train_time:11187ms step_avg:56.22ms
step:200/2330 train_time:11245ms step_avg:56.23ms
step:201/2330 train_time:11301ms step_avg:56.22ms
step:202/2330 train_time:11359ms step_avg:56.23ms
step:203/2330 train_time:11415ms step_avg:56.23ms
step:204/2330 train_time:11474ms step_avg:56.24ms
step:205/2330 train_time:11529ms step_avg:56.24ms
step:206/2330 train_time:11588ms step_avg:56.25ms
step:207/2330 train_time:11643ms step_avg:56.25ms
step:208/2330 train_time:11702ms step_avg:56.26ms
step:209/2330 train_time:11758ms step_avg:56.26ms
step:210/2330 train_time:11816ms step_avg:56.27ms
step:211/2330 train_time:11872ms step_avg:56.26ms
step:212/2330 train_time:11930ms step_avg:56.27ms
step:213/2330 train_time:11986ms step_avg:56.27ms
step:214/2330 train_time:12044ms step_avg:56.28ms
step:215/2330 train_time:12099ms step_avg:56.27ms
step:216/2330 train_time:12158ms step_avg:56.29ms
step:217/2330 train_time:12213ms step_avg:56.28ms
step:218/2330 train_time:12272ms step_avg:56.29ms
step:219/2330 train_time:12328ms step_avg:56.29ms
step:220/2330 train_time:12386ms step_avg:56.30ms
step:221/2330 train_time:12442ms step_avg:56.30ms
step:222/2330 train_time:12501ms step_avg:56.31ms
step:223/2330 train_time:12556ms step_avg:56.30ms
step:224/2330 train_time:12615ms step_avg:56.32ms
step:225/2330 train_time:12670ms step_avg:56.31ms
step:226/2330 train_time:12729ms step_avg:56.32ms
step:227/2330 train_time:12784ms step_avg:56.32ms
step:228/2330 train_time:12844ms step_avg:56.33ms
step:229/2330 train_time:12899ms step_avg:56.33ms
step:230/2330 train_time:12958ms step_avg:56.34ms
step:231/2330 train_time:13014ms step_avg:56.34ms
step:232/2330 train_time:13073ms step_avg:56.35ms
step:233/2330 train_time:13128ms step_avg:56.34ms
step:234/2330 train_time:13186ms step_avg:56.35ms
step:235/2330 train_time:13242ms step_avg:56.35ms
step:236/2330 train_time:13301ms step_avg:56.36ms
step:237/2330 train_time:13357ms step_avg:56.36ms
step:238/2330 train_time:13416ms step_avg:56.37ms
step:239/2330 train_time:13472ms step_avg:56.37ms
step:240/2330 train_time:13531ms step_avg:56.38ms
step:241/2330 train_time:13587ms step_avg:56.38ms
step:242/2330 train_time:13646ms step_avg:56.39ms
step:243/2330 train_time:13701ms step_avg:56.38ms
step:244/2330 train_time:13761ms step_avg:56.40ms
step:245/2330 train_time:13817ms step_avg:56.39ms
step:246/2330 train_time:13875ms step_avg:56.40ms
step:247/2330 train_time:13931ms step_avg:56.40ms
step:248/2330 train_time:13990ms step_avg:56.41ms
step:249/2330 train_time:14045ms step_avg:56.41ms
step:250/2330 train_time:14105ms step_avg:56.42ms
step:250/2330 val_loss:6.6562 train_time:14184ms step_avg:56.74ms
step:251/2330 train_time:14202ms step_avg:56.58ms
step:252/2330 train_time:14222ms step_avg:56.44ms
step:253/2330 train_time:14277ms step_avg:56.43ms
step:254/2330 train_time:14339ms step_avg:56.45ms
step:255/2330 train_time:14395ms step_avg:56.45ms
step:256/2330 train_time:14458ms step_avg:56.48ms
step:257/2330 train_time:14513ms step_avg:56.47ms
step:258/2330 train_time:14575ms step_avg:56.49ms
step:259/2330 train_time:14631ms step_avg:56.49ms
step:260/2330 train_time:14690ms step_avg:56.50ms
step:261/2330 train_time:14746ms step_avg:56.50ms
step:262/2330 train_time:14804ms step_avg:56.50ms
step:263/2330 train_time:14859ms step_avg:56.50ms
step:264/2330 train_time:14918ms step_avg:56.51ms
step:265/2330 train_time:14972ms step_avg:56.50ms
step:266/2330 train_time:15031ms step_avg:56.51ms
step:267/2330 train_time:15086ms step_avg:56.50ms
step:268/2330 train_time:15146ms step_avg:56.51ms
step:269/2330 train_time:15203ms step_avg:56.52ms
step:270/2330 train_time:15262ms step_avg:56.52ms
step:271/2330 train_time:15317ms step_avg:56.52ms
step:272/2330 train_time:15379ms step_avg:56.54ms
step:273/2330 train_time:15435ms step_avg:56.54ms
step:274/2330 train_time:15495ms step_avg:56.55ms
step:275/2330 train_time:15551ms step_avg:56.55ms
step:276/2330 train_time:15611ms step_avg:56.56ms
step:277/2330 train_time:15666ms step_avg:56.56ms
step:278/2330 train_time:15725ms step_avg:56.56ms
step:279/2330 train_time:15780ms step_avg:56.56ms
step:280/2330 train_time:15839ms step_avg:56.57ms
step:281/2330 train_time:15894ms step_avg:56.56ms
step:282/2330 train_time:15953ms step_avg:56.57ms
step:283/2330 train_time:16008ms step_avg:56.57ms
step:284/2330 train_time:16067ms step_avg:56.57ms
step:285/2330 train_time:16123ms step_avg:56.57ms
step:286/2330 train_time:16182ms step_avg:56.58ms
step:287/2330 train_time:16238ms step_avg:56.58ms
step:288/2330 train_time:16297ms step_avg:56.59ms
step:289/2330 train_time:16354ms step_avg:56.59ms
step:290/2330 train_time:16413ms step_avg:56.60ms
step:291/2330 train_time:16469ms step_avg:56.59ms
step:292/2330 train_time:16528ms step_avg:56.60ms
step:293/2330 train_time:16584ms step_avg:56.60ms
step:294/2330 train_time:16644ms step_avg:56.61ms
step:295/2330 train_time:16699ms step_avg:56.61ms
step:296/2330 train_time:16760ms step_avg:56.62ms
step:297/2330 train_time:16815ms step_avg:56.61ms
step:298/2330 train_time:16874ms step_avg:56.62ms
step:299/2330 train_time:16929ms step_avg:56.62ms
step:300/2330 train_time:16988ms step_avg:56.63ms
step:301/2330 train_time:17043ms step_avg:56.62ms
step:302/2330 train_time:17102ms step_avg:56.63ms
step:303/2330 train_time:17158ms step_avg:56.63ms
step:304/2330 train_time:17216ms step_avg:56.63ms
step:305/2330 train_time:17273ms step_avg:56.63ms
step:306/2330 train_time:17332ms step_avg:56.64ms
step:307/2330 train_time:17388ms step_avg:56.64ms
step:308/2330 train_time:17448ms step_avg:56.65ms
step:309/2330 train_time:17504ms step_avg:56.65ms
step:310/2330 train_time:17564ms step_avg:56.66ms
step:311/2330 train_time:17619ms step_avg:56.65ms
step:312/2330 train_time:17679ms step_avg:56.66ms
step:313/2330 train_time:17734ms step_avg:56.66ms
step:314/2330 train_time:17794ms step_avg:56.67ms
step:315/2330 train_time:17850ms step_avg:56.67ms
step:316/2330 train_time:17908ms step_avg:56.67ms
step:317/2330 train_time:17963ms step_avg:56.67ms
step:318/2330 train_time:18022ms step_avg:56.67ms
step:319/2330 train_time:18078ms step_avg:56.67ms
step:320/2330 train_time:18137ms step_avg:56.68ms
step:321/2330 train_time:18193ms step_avg:56.68ms
step:322/2330 train_time:18252ms step_avg:56.68ms
step:323/2330 train_time:18308ms step_avg:56.68ms
step:324/2330 train_time:18368ms step_avg:56.69ms
step:325/2330 train_time:18423ms step_avg:56.69ms
step:326/2330 train_time:18483ms step_avg:56.70ms
step:327/2330 train_time:18539ms step_avg:56.70ms
step:328/2330 train_time:18599ms step_avg:56.71ms
step:329/2330 train_time:18655ms step_avg:56.70ms
step:330/2330 train_time:18714ms step_avg:56.71ms
step:331/2330 train_time:18771ms step_avg:56.71ms
step:332/2330 train_time:18829ms step_avg:56.71ms
step:333/2330 train_time:18885ms step_avg:56.71ms
step:334/2330 train_time:18944ms step_avg:56.72ms
step:335/2330 train_time:19000ms step_avg:56.72ms
step:336/2330 train_time:19059ms step_avg:56.72ms
step:337/2330 train_time:19114ms step_avg:56.72ms
step:338/2330 train_time:19174ms step_avg:56.73ms
step:339/2330 train_time:19229ms step_avg:56.72ms
step:340/2330 train_time:19289ms step_avg:56.73ms
step:341/2330 train_time:19345ms step_avg:56.73ms
step:342/2330 train_time:19404ms step_avg:56.74ms
step:343/2330 train_time:19460ms step_avg:56.73ms
step:344/2330 train_time:19519ms step_avg:56.74ms
step:345/2330 train_time:19575ms step_avg:56.74ms
step:346/2330 train_time:19635ms step_avg:56.75ms
step:347/2330 train_time:19691ms step_avg:56.75ms
step:348/2330 train_time:19749ms step_avg:56.75ms
step:349/2330 train_time:19805ms step_avg:56.75ms
step:350/2330 train_time:19864ms step_avg:56.75ms
step:351/2330 train_time:19920ms step_avg:56.75ms
step:352/2330 train_time:19979ms step_avg:56.76ms
step:353/2330 train_time:20035ms step_avg:56.76ms
step:354/2330 train_time:20094ms step_avg:56.76ms
step:355/2330 train_time:20150ms step_avg:56.76ms
step:356/2330 train_time:20209ms step_avg:56.77ms
step:357/2330 train_time:20265ms step_avg:56.76ms
step:358/2330 train_time:20324ms step_avg:56.77ms
step:359/2330 train_time:20380ms step_avg:56.77ms
step:360/2330 train_time:20440ms step_avg:56.78ms
step:361/2330 train_time:20496ms step_avg:56.78ms
step:362/2330 train_time:20555ms step_avg:56.78ms
step:363/2330 train_time:20612ms step_avg:56.78ms
step:364/2330 train_time:20671ms step_avg:56.79ms
step:365/2330 train_time:20727ms step_avg:56.79ms
step:366/2330 train_time:20785ms step_avg:56.79ms
step:367/2330 train_time:20842ms step_avg:56.79ms
step:368/2330 train_time:20901ms step_avg:56.80ms
step:369/2330 train_time:20956ms step_avg:56.79ms
step:370/2330 train_time:21015ms step_avg:56.80ms
step:371/2330 train_time:21071ms step_avg:56.80ms
step:372/2330 train_time:21130ms step_avg:56.80ms
step:373/2330 train_time:21185ms step_avg:56.80ms
step:374/2330 train_time:21245ms step_avg:56.81ms
step:375/2330 train_time:21301ms step_avg:56.80ms
step:376/2330 train_time:21360ms step_avg:56.81ms
step:377/2330 train_time:21416ms step_avg:56.81ms
step:378/2330 train_time:21476ms step_avg:56.81ms
step:379/2330 train_time:21531ms step_avg:56.81ms
step:380/2330 train_time:21590ms step_avg:56.82ms
step:381/2330 train_time:21646ms step_avg:56.81ms
step:382/2330 train_time:21706ms step_avg:56.82ms
step:383/2330 train_time:21762ms step_avg:56.82ms
step:384/2330 train_time:21822ms step_avg:56.83ms
step:385/2330 train_time:21878ms step_avg:56.83ms
step:386/2330 train_time:21937ms step_avg:56.83ms
step:387/2330 train_time:21993ms step_avg:56.83ms
step:388/2330 train_time:22052ms step_avg:56.84ms
step:389/2330 train_time:22108ms step_avg:56.83ms
step:390/2330 train_time:22167ms step_avg:56.84ms
step:391/2330 train_time:22223ms step_avg:56.84ms
step:392/2330 train_time:22282ms step_avg:56.84ms
step:393/2330 train_time:22337ms step_avg:56.84ms
step:394/2330 train_time:22397ms step_avg:56.85ms
step:395/2330 train_time:22453ms step_avg:56.84ms
step:396/2330 train_time:22512ms step_avg:56.85ms
step:397/2330 train_time:22568ms step_avg:56.85ms
step:398/2330 train_time:22627ms step_avg:56.85ms
step:399/2330 train_time:22683ms step_avg:56.85ms
step:400/2330 train_time:22742ms step_avg:56.85ms
step:401/2330 train_time:22798ms step_avg:56.85ms
step:402/2330 train_time:22857ms step_avg:56.86ms
step:403/2330 train_time:22913ms step_avg:56.86ms
step:404/2330 train_time:22972ms step_avg:56.86ms
step:405/2330 train_time:23028ms step_avg:56.86ms
step:406/2330 train_time:23086ms step_avg:56.86ms
step:407/2330 train_time:23142ms step_avg:56.86ms
step:408/2330 train_time:23201ms step_avg:56.87ms
step:409/2330 train_time:23257ms step_avg:56.86ms
step:410/2330 train_time:23316ms step_avg:56.87ms
step:411/2330 train_time:23372ms step_avg:56.87ms
step:412/2330 train_time:23431ms step_avg:56.87ms
step:413/2330 train_time:23487ms step_avg:56.87ms
step:414/2330 train_time:23546ms step_avg:56.87ms
step:415/2330 train_time:23602ms step_avg:56.87ms
step:416/2330 train_time:23661ms step_avg:56.88ms
step:417/2330 train_time:23717ms step_avg:56.87ms
step:418/2330 train_time:23777ms step_avg:56.88ms
step:419/2330 train_time:23832ms step_avg:56.88ms
step:420/2330 train_time:23891ms step_avg:56.88ms
step:421/2330 train_time:23948ms step_avg:56.88ms
step:422/2330 train_time:24006ms step_avg:56.89ms
step:423/2330 train_time:24062ms step_avg:56.88ms
step:424/2330 train_time:24120ms step_avg:56.89ms
step:425/2330 train_time:24177ms step_avg:56.89ms
step:426/2330 train_time:24236ms step_avg:56.89ms
step:427/2330 train_time:24292ms step_avg:56.89ms
step:428/2330 train_time:24350ms step_avg:56.89ms
step:429/2330 train_time:24407ms step_avg:56.89ms
step:430/2330 train_time:24466ms step_avg:56.90ms
step:431/2330 train_time:24521ms step_avg:56.89ms
step:432/2330 train_time:24581ms step_avg:56.90ms
step:433/2330 train_time:24637ms step_avg:56.90ms
step:434/2330 train_time:24697ms step_avg:56.90ms
step:435/2330 train_time:24752ms step_avg:56.90ms
step:436/2330 train_time:24812ms step_avg:56.91ms
step:437/2330 train_time:24867ms step_avg:56.90ms
step:438/2330 train_time:24926ms step_avg:56.91ms
step:439/2330 train_time:24982ms step_avg:56.91ms
step:440/2330 train_time:25041ms step_avg:56.91ms
step:441/2330 train_time:25097ms step_avg:56.91ms
step:442/2330 train_time:25156ms step_avg:56.91ms
step:443/2330 train_time:25212ms step_avg:56.91ms
step:444/2330 train_time:25271ms step_avg:56.92ms
step:445/2330 train_time:25327ms step_avg:56.91ms
step:446/2330 train_time:25386ms step_avg:56.92ms
step:447/2330 train_time:25442ms step_avg:56.92ms
step:448/2330 train_time:25501ms step_avg:56.92ms
step:449/2330 train_time:25557ms step_avg:56.92ms
step:450/2330 train_time:25616ms step_avg:56.92ms
step:451/2330 train_time:25672ms step_avg:56.92ms
step:452/2330 train_time:25731ms step_avg:56.93ms
step:453/2330 train_time:25788ms step_avg:56.93ms
step:454/2330 train_time:25847ms step_avg:56.93ms
step:455/2330 train_time:25903ms step_avg:56.93ms
step:456/2330 train_time:25962ms step_avg:56.93ms
step:457/2330 train_time:26018ms step_avg:56.93ms
step:458/2330 train_time:26077ms step_avg:56.94ms
step:459/2330 train_time:26132ms step_avg:56.93ms
step:460/2330 train_time:26192ms step_avg:56.94ms
step:461/2330 train_time:26248ms step_avg:56.94ms
step:462/2330 train_time:26306ms step_avg:56.94ms
step:463/2330 train_time:26362ms step_avg:56.94ms
step:464/2330 train_time:26421ms step_avg:56.94ms
step:465/2330 train_time:26478ms step_avg:56.94ms
step:466/2330 train_time:26536ms step_avg:56.94ms
step:467/2330 train_time:26592ms step_avg:56.94ms
step:468/2330 train_time:26652ms step_avg:56.95ms
step:469/2330 train_time:26708ms step_avg:56.95ms
step:470/2330 train_time:26767ms step_avg:56.95ms
step:471/2330 train_time:26823ms step_avg:56.95ms
step:472/2330 train_time:26882ms step_avg:56.95ms
step:473/2330 train_time:26938ms step_avg:56.95ms
step:474/2330 train_time:26997ms step_avg:56.96ms
step:475/2330 train_time:27053ms step_avg:56.95ms
step:476/2330 train_time:27112ms step_avg:56.96ms
step:477/2330 train_time:27168ms step_avg:56.96ms
step:478/2330 train_time:27227ms step_avg:56.96ms
step:479/2330 train_time:27282ms step_avg:56.96ms
step:480/2330 train_time:27342ms step_avg:56.96ms
step:481/2330 train_time:27398ms step_avg:56.96ms
step:482/2330 train_time:27457ms step_avg:56.96ms
step:483/2330 train_time:27513ms step_avg:56.96ms
step:484/2330 train_time:27572ms step_avg:56.97ms
step:485/2330 train_time:27628ms step_avg:56.96ms
step:486/2330 train_time:27686ms step_avg:56.97ms
step:487/2330 train_time:27742ms step_avg:56.97ms
step:488/2330 train_time:27802ms step_avg:56.97ms
step:489/2330 train_time:27858ms step_avg:56.97ms
step:490/2330 train_time:27917ms step_avg:56.97ms
step:491/2330 train_time:27972ms step_avg:56.97ms
step:492/2330 train_time:28031ms step_avg:56.97ms
step:493/2330 train_time:28087ms step_avg:56.97ms
step:494/2330 train_time:28146ms step_avg:56.98ms
step:495/2330 train_time:28202ms step_avg:56.97ms
step:496/2330 train_time:28261ms step_avg:56.98ms
step:497/2330 train_time:28317ms step_avg:56.98ms
step:498/2330 train_time:28377ms step_avg:56.98ms
step:499/2330 train_time:28432ms step_avg:56.98ms
step:500/2330 train_time:28491ms step_avg:56.98ms
step:500/2330 val_loss:6.0178 train_time:28570ms step_avg:57.14ms
step:501/2330 train_time:28588ms step_avg:57.06ms
step:502/2330 train_time:28609ms step_avg:56.99ms
step:503/2330 train_time:28668ms step_avg:56.99ms
step:504/2330 train_time:28732ms step_avg:57.01ms
step:505/2330 train_time:28788ms step_avg:57.01ms
step:506/2330 train_time:28849ms step_avg:57.01ms
step:507/2330 train_time:28906ms step_avg:57.01ms
step:508/2330 train_time:28964ms step_avg:57.02ms
step:509/2330 train_time:29020ms step_avg:57.01ms
step:510/2330 train_time:29079ms step_avg:57.02ms
step:511/2330 train_time:29134ms step_avg:57.01ms
step:512/2330 train_time:29192ms step_avg:57.02ms
step:513/2330 train_time:29248ms step_avg:57.01ms
step:514/2330 train_time:29306ms step_avg:57.01ms
step:515/2330 train_time:29361ms step_avg:57.01ms
step:516/2330 train_time:29420ms step_avg:57.01ms
step:517/2330 train_time:29475ms step_avg:57.01ms
step:518/2330 train_time:29533ms step_avg:57.01ms
step:519/2330 train_time:29591ms step_avg:57.01ms
step:520/2330 train_time:29650ms step_avg:57.02ms
step:521/2330 train_time:29707ms step_avg:57.02ms
step:522/2330 train_time:29768ms step_avg:57.03ms
step:523/2330 train_time:29825ms step_avg:57.03ms
step:524/2330 train_time:29885ms step_avg:57.03ms
step:525/2330 train_time:29941ms step_avg:57.03ms
step:526/2330 train_time:30001ms step_avg:57.04ms
step:527/2330 train_time:30057ms step_avg:57.03ms
step:528/2330 train_time:30115ms step_avg:57.04ms
step:529/2330 train_time:30171ms step_avg:57.03ms
step:530/2330 train_time:30229ms step_avg:57.04ms
step:531/2330 train_time:30285ms step_avg:57.03ms
step:532/2330 train_time:30343ms step_avg:57.04ms
step:533/2330 train_time:30400ms step_avg:57.04ms
step:534/2330 train_time:30459ms step_avg:57.04ms
step:535/2330 train_time:30515ms step_avg:57.04ms
step:536/2330 train_time:30574ms step_avg:57.04ms
step:537/2330 train_time:30630ms step_avg:57.04ms
step:538/2330 train_time:30691ms step_avg:57.05ms
step:539/2330 train_time:30747ms step_avg:57.04ms
step:540/2330 train_time:30808ms step_avg:57.05ms
step:541/2330 train_time:30864ms step_avg:57.05ms
step:542/2330 train_time:30924ms step_avg:57.06ms
step:543/2330 train_time:30980ms step_avg:57.05ms
step:544/2330 train_time:31039ms step_avg:57.06ms
step:545/2330 train_time:31095ms step_avg:57.05ms
step:546/2330 train_time:31154ms step_avg:57.06ms
step:547/2330 train_time:31210ms step_avg:57.06ms
step:548/2330 train_time:31269ms step_avg:57.06ms
step:549/2330 train_time:31324ms step_avg:57.06ms
step:550/2330 train_time:31383ms step_avg:57.06ms
step:551/2330 train_time:31440ms step_avg:57.06ms
step:552/2330 train_time:31498ms step_avg:57.06ms
step:553/2330 train_time:31555ms step_avg:57.06ms
step:554/2330 train_time:31613ms step_avg:57.06ms
step:555/2330 train_time:31670ms step_avg:57.06ms
step:556/2330 train_time:31730ms step_avg:57.07ms
step:557/2330 train_time:31786ms step_avg:57.07ms
step:558/2330 train_time:31846ms step_avg:57.07ms
step:559/2330 train_time:31903ms step_avg:57.07ms
step:560/2330 train_time:31961ms step_avg:57.07ms
step:561/2330 train_time:32018ms step_avg:57.07ms
step:562/2330 train_time:32077ms step_avg:57.08ms
step:563/2330 train_time:32133ms step_avg:57.08ms
step:564/2330 train_time:32192ms step_avg:57.08ms
step:565/2330 train_time:32248ms step_avg:57.08ms
step:566/2330 train_time:32307ms step_avg:57.08ms
step:567/2330 train_time:32363ms step_avg:57.08ms
step:568/2330 train_time:32422ms step_avg:57.08ms
step:569/2330 train_time:32479ms step_avg:57.08ms
step:570/2330 train_time:32538ms step_avg:57.08ms
step:571/2330 train_time:32594ms step_avg:57.08ms
step:572/2330 train_time:32653ms step_avg:57.09ms
step:573/2330 train_time:32709ms step_avg:57.08ms
step:574/2330 train_time:32770ms step_avg:57.09ms
step:575/2330 train_time:32825ms step_avg:57.09ms
step:576/2330 train_time:32885ms step_avg:57.09ms
step:577/2330 train_time:32941ms step_avg:57.09ms
step:578/2330 train_time:33001ms step_avg:57.09ms
step:579/2330 train_time:33057ms step_avg:57.09ms
step:580/2330 train_time:33116ms step_avg:57.10ms
step:581/2330 train_time:33172ms step_avg:57.09ms
step:582/2330 train_time:33231ms step_avg:57.10ms
step:583/2330 train_time:33287ms step_avg:57.10ms
step:584/2330 train_time:33346ms step_avg:57.10ms
step:585/2330 train_time:33402ms step_avg:57.10ms
step:586/2330 train_time:33461ms step_avg:57.10ms
step:587/2330 train_time:33518ms step_avg:57.10ms
step:588/2330 train_time:33577ms step_avg:57.10ms
step:589/2330 train_time:33633ms step_avg:57.10ms
step:590/2330 train_time:33693ms step_avg:57.11ms
step:591/2330 train_time:33749ms step_avg:57.10ms
step:592/2330 train_time:33808ms step_avg:57.11ms
step:593/2330 train_time:33864ms step_avg:57.11ms
step:594/2330 train_time:33923ms step_avg:57.11ms
step:595/2330 train_time:33979ms step_avg:57.11ms
step:596/2330 train_time:34038ms step_avg:57.11ms
step:597/2330 train_time:34095ms step_avg:57.11ms
step:598/2330 train_time:34155ms step_avg:57.11ms
step:599/2330 train_time:34210ms step_avg:57.11ms
step:600/2330 train_time:34270ms step_avg:57.12ms
step:601/2330 train_time:34325ms step_avg:57.11ms
step:602/2330 train_time:34385ms step_avg:57.12ms
step:603/2330 train_time:34442ms step_avg:57.12ms
step:604/2330 train_time:34501ms step_avg:57.12ms
step:605/2330 train_time:34558ms step_avg:57.12ms
step:606/2330 train_time:34617ms step_avg:57.12ms
step:607/2330 train_time:34673ms step_avg:57.12ms
step:608/2330 train_time:34733ms step_avg:57.13ms
step:609/2330 train_time:34789ms step_avg:57.12ms
step:610/2330 train_time:34849ms step_avg:57.13ms
step:611/2330 train_time:34905ms step_avg:57.13ms
step:612/2330 train_time:34965ms step_avg:57.13ms
step:613/2330 train_time:35022ms step_avg:57.13ms
step:614/2330 train_time:35081ms step_avg:57.14ms
step:615/2330 train_time:35138ms step_avg:57.13ms
step:616/2330 train_time:35197ms step_avg:57.14ms
step:617/2330 train_time:35253ms step_avg:57.14ms
step:618/2330 train_time:35313ms step_avg:57.14ms
step:619/2330 train_time:35368ms step_avg:57.14ms
step:620/2330 train_time:35428ms step_avg:57.14ms
step:621/2330 train_time:35484ms step_avg:57.14ms
step:622/2330 train_time:35544ms step_avg:57.14ms
step:623/2330 train_time:35601ms step_avg:57.14ms
step:624/2330 train_time:35660ms step_avg:57.15ms
step:625/2330 train_time:35717ms step_avg:57.15ms
step:626/2330 train_time:35776ms step_avg:57.15ms
step:627/2330 train_time:35832ms step_avg:57.15ms
step:628/2330 train_time:35891ms step_avg:57.15ms
step:629/2330 train_time:35947ms step_avg:57.15ms
step:630/2330 train_time:36007ms step_avg:57.15ms
step:631/2330 train_time:36063ms step_avg:57.15ms
step:632/2330 train_time:36122ms step_avg:57.16ms
step:633/2330 train_time:36179ms step_avg:57.15ms
step:634/2330 train_time:36238ms step_avg:57.16ms
step:635/2330 train_time:36294ms step_avg:57.16ms
step:636/2330 train_time:36354ms step_avg:57.16ms
step:637/2330 train_time:36410ms step_avg:57.16ms
step:638/2330 train_time:36470ms step_avg:57.16ms
step:639/2330 train_time:36526ms step_avg:57.16ms
step:640/2330 train_time:36587ms step_avg:57.17ms
step:641/2330 train_time:36643ms step_avg:57.17ms
step:642/2330 train_time:36702ms step_avg:57.17ms
step:643/2330 train_time:36758ms step_avg:57.17ms
step:644/2330 train_time:36817ms step_avg:57.17ms
step:645/2330 train_time:36873ms step_avg:57.17ms
step:646/2330 train_time:36933ms step_avg:57.17ms
step:647/2330 train_time:36989ms step_avg:57.17ms
step:648/2330 train_time:37048ms step_avg:57.17ms
step:649/2330 train_time:37104ms step_avg:57.17ms
step:650/2330 train_time:37163ms step_avg:57.17ms
step:651/2330 train_time:37220ms step_avg:57.17ms
step:652/2330 train_time:37280ms step_avg:57.18ms
step:653/2330 train_time:37336ms step_avg:57.18ms
step:654/2330 train_time:37395ms step_avg:57.18ms
step:655/2330 train_time:37450ms step_avg:57.18ms
step:656/2330 train_time:37511ms step_avg:57.18ms
step:657/2330 train_time:37567ms step_avg:57.18ms
step:658/2330 train_time:37627ms step_avg:57.18ms
step:659/2330 train_time:37683ms step_avg:57.18ms
step:660/2330 train_time:37742ms step_avg:57.19ms
step:661/2330 train_time:37799ms step_avg:57.18ms
step:662/2330 train_time:37858ms step_avg:57.19ms
step:663/2330 train_time:37914ms step_avg:57.19ms
step:664/2330 train_time:37974ms step_avg:57.19ms
step:665/2330 train_time:38030ms step_avg:57.19ms
step:666/2330 train_time:38089ms step_avg:57.19ms
step:667/2330 train_time:38145ms step_avg:57.19ms
step:668/2330 train_time:38204ms step_avg:57.19ms
step:669/2330 train_time:38261ms step_avg:57.19ms
step:670/2330 train_time:38321ms step_avg:57.19ms
step:671/2330 train_time:38378ms step_avg:57.19ms
step:672/2330 train_time:38437ms step_avg:57.20ms
step:673/2330 train_time:38493ms step_avg:57.20ms
step:674/2330 train_time:38553ms step_avg:57.20ms
step:675/2330 train_time:38609ms step_avg:57.20ms
step:676/2330 train_time:38669ms step_avg:57.20ms
step:677/2330 train_time:38724ms step_avg:57.20ms
step:678/2330 train_time:38786ms step_avg:57.21ms
step:679/2330 train_time:38842ms step_avg:57.20ms
step:680/2330 train_time:38901ms step_avg:57.21ms
step:681/2330 train_time:38957ms step_avg:57.21ms
step:682/2330 train_time:39016ms step_avg:57.21ms
step:683/2330 train_time:39072ms step_avg:57.21ms
step:684/2330 train_time:39132ms step_avg:57.21ms
step:685/2330 train_time:39188ms step_avg:57.21ms
step:686/2330 train_time:39248ms step_avg:57.21ms
step:687/2330 train_time:39304ms step_avg:57.21ms
step:688/2330 train_time:39363ms step_avg:57.21ms
step:689/2330 train_time:39419ms step_avg:57.21ms
step:690/2330 train_time:39479ms step_avg:57.22ms
step:691/2330 train_time:39535ms step_avg:57.21ms
step:692/2330 train_time:39594ms step_avg:57.22ms
step:693/2330 train_time:39650ms step_avg:57.21ms
step:694/2330 train_time:39710ms step_avg:57.22ms
step:695/2330 train_time:39766ms step_avg:57.22ms
step:696/2330 train_time:39825ms step_avg:57.22ms
step:697/2330 train_time:39882ms step_avg:57.22ms
step:698/2330 train_time:39941ms step_avg:57.22ms
step:699/2330 train_time:39997ms step_avg:57.22ms
step:700/2330 train_time:40056ms step_avg:57.22ms
step:701/2330 train_time:40112ms step_avg:57.22ms
step:702/2330 train_time:40172ms step_avg:57.22ms
step:703/2330 train_time:40228ms step_avg:57.22ms
step:704/2330 train_time:40288ms step_avg:57.23ms
step:705/2330 train_time:40344ms step_avg:57.23ms
step:706/2330 train_time:40405ms step_avg:57.23ms
step:707/2330 train_time:40461ms step_avg:57.23ms
step:708/2330 train_time:40520ms step_avg:57.23ms
step:709/2330 train_time:40576ms step_avg:57.23ms
step:710/2330 train_time:40635ms step_avg:57.23ms
step:711/2330 train_time:40692ms step_avg:57.23ms
step:712/2330 train_time:40751ms step_avg:57.23ms
step:713/2330 train_time:40806ms step_avg:57.23ms
step:714/2330 train_time:40867ms step_avg:57.24ms
step:715/2330 train_time:40923ms step_avg:57.24ms
step:716/2330 train_time:40983ms step_avg:57.24ms
step:717/2330 train_time:41039ms step_avg:57.24ms
step:718/2330 train_time:41098ms step_avg:57.24ms
step:719/2330 train_time:41154ms step_avg:57.24ms
step:720/2330 train_time:41213ms step_avg:57.24ms
step:721/2330 train_time:41269ms step_avg:57.24ms
step:722/2330 train_time:41329ms step_avg:57.24ms
step:723/2330 train_time:41384ms step_avg:57.24ms
step:724/2330 train_time:41445ms step_avg:57.24ms
step:725/2330 train_time:41501ms step_avg:57.24ms
step:726/2330 train_time:41560ms step_avg:57.25ms
step:727/2330 train_time:41617ms step_avg:57.24ms
step:728/2330 train_time:41676ms step_avg:57.25ms
step:729/2330 train_time:41732ms step_avg:57.25ms
step:730/2330 train_time:41791ms step_avg:57.25ms
step:731/2330 train_time:41847ms step_avg:57.25ms
step:732/2330 train_time:41908ms step_avg:57.25ms
step:733/2330 train_time:41963ms step_avg:57.25ms
step:734/2330 train_time:42023ms step_avg:57.25ms
step:735/2330 train_time:42079ms step_avg:57.25ms
step:736/2330 train_time:42138ms step_avg:57.25ms
step:737/2330 train_time:42195ms step_avg:57.25ms
step:738/2330 train_time:42253ms step_avg:57.25ms
step:739/2330 train_time:42310ms step_avg:57.25ms
step:740/2330 train_time:42369ms step_avg:57.26ms
step:741/2330 train_time:42425ms step_avg:57.25ms
step:742/2330 train_time:42485ms step_avg:57.26ms
step:743/2330 train_time:42541ms step_avg:57.26ms
step:744/2330 train_time:42600ms step_avg:57.26ms
step:745/2330 train_time:42656ms step_avg:57.26ms
step:746/2330 train_time:42716ms step_avg:57.26ms
step:747/2330 train_time:42772ms step_avg:57.26ms
step:748/2330 train_time:42831ms step_avg:57.26ms
step:749/2330 train_time:42887ms step_avg:57.26ms
step:750/2330 train_time:42947ms step_avg:57.26ms
step:750/2330 val_loss:5.7710 train_time:43026ms step_avg:57.37ms
step:751/2330 train_time:43045ms step_avg:57.32ms
step:752/2330 train_time:43065ms step_avg:57.27ms
step:753/2330 train_time:43125ms step_avg:57.27ms
step:754/2330 train_time:43187ms step_avg:57.28ms
step:755/2330 train_time:43244ms step_avg:57.28ms
step:756/2330 train_time:43304ms step_avg:57.28ms
step:757/2330 train_time:43360ms step_avg:57.28ms
step:758/2330 train_time:43419ms step_avg:57.28ms
step:759/2330 train_time:43475ms step_avg:57.28ms
step:760/2330 train_time:43533ms step_avg:57.28ms
step:761/2330 train_time:43589ms step_avg:57.28ms
step:762/2330 train_time:43648ms step_avg:57.28ms
step:763/2330 train_time:43703ms step_avg:57.28ms
step:764/2330 train_time:43762ms step_avg:57.28ms
step:765/2330 train_time:43819ms step_avg:57.28ms
step:766/2330 train_time:43877ms step_avg:57.28ms
step:767/2330 train_time:43933ms step_avg:57.28ms
step:768/2330 train_time:43992ms step_avg:57.28ms
step:769/2330 train_time:44049ms step_avg:57.28ms
step:770/2330 train_time:44112ms step_avg:57.29ms
step:771/2330 train_time:44168ms step_avg:57.29ms
step:772/2330 train_time:44232ms step_avg:57.29ms
step:773/2330 train_time:44288ms step_avg:57.29ms
step:774/2330 train_time:44350ms step_avg:57.30ms
step:775/2330 train_time:44406ms step_avg:57.30ms
step:776/2330 train_time:44467ms step_avg:57.30ms
step:777/2330 train_time:44523ms step_avg:57.30ms
step:778/2330 train_time:44583ms step_avg:57.30ms
step:779/2330 train_time:44640ms step_avg:57.30ms
step:780/2330 train_time:44699ms step_avg:57.31ms
step:781/2330 train_time:44755ms step_avg:57.30ms
step:782/2330 train_time:44814ms step_avg:57.31ms
step:783/2330 train_time:44871ms step_avg:57.31ms
step:784/2330 train_time:44931ms step_avg:57.31ms
step:785/2330 train_time:44987ms step_avg:57.31ms
step:786/2330 train_time:45048ms step_avg:57.31ms
step:787/2330 train_time:45105ms step_avg:57.31ms
step:788/2330 train_time:45167ms step_avg:57.32ms
step:789/2330 train_time:45224ms step_avg:57.32ms
step:790/2330 train_time:45283ms step_avg:57.32ms
step:791/2330 train_time:45340ms step_avg:57.32ms
step:792/2330 train_time:45401ms step_avg:57.32ms
step:793/2330 train_time:45457ms step_avg:57.32ms
step:794/2330 train_time:45517ms step_avg:57.33ms
step:795/2330 train_time:45574ms step_avg:57.33ms
step:796/2330 train_time:45634ms step_avg:57.33ms
step:797/2330 train_time:45690ms step_avg:57.33ms
step:798/2330 train_time:45750ms step_avg:57.33ms
step:799/2330 train_time:45807ms step_avg:57.33ms
step:800/2330 train_time:45867ms step_avg:57.33ms
step:801/2330 train_time:45923ms step_avg:57.33ms
step:802/2330 train_time:45983ms step_avg:57.33ms
step:803/2330 train_time:46040ms step_avg:57.33ms
step:804/2330 train_time:46100ms step_avg:57.34ms
step:805/2330 train_time:46157ms step_avg:57.34ms
step:806/2330 train_time:46218ms step_avg:57.34ms
step:807/2330 train_time:46275ms step_avg:57.34ms
step:808/2330 train_time:46335ms step_avg:57.35ms
step:809/2330 train_time:46392ms step_avg:57.34ms
step:810/2330 train_time:46452ms step_avg:57.35ms
step:811/2330 train_time:46508ms step_avg:57.35ms
step:812/2330 train_time:46568ms step_avg:57.35ms
step:813/2330 train_time:46625ms step_avg:57.35ms
step:814/2330 train_time:46686ms step_avg:57.35ms
step:815/2330 train_time:46742ms step_avg:57.35ms
step:816/2330 train_time:46801ms step_avg:57.35ms
step:817/2330 train_time:46858ms step_avg:57.35ms
step:818/2330 train_time:46917ms step_avg:57.36ms
step:819/2330 train_time:46974ms step_avg:57.36ms
step:820/2330 train_time:47035ms step_avg:57.36ms
step:821/2330 train_time:47091ms step_avg:57.36ms
step:822/2330 train_time:47152ms step_avg:57.36ms
step:823/2330 train_time:47208ms step_avg:57.36ms
step:824/2330 train_time:47270ms step_avg:57.37ms
step:825/2330 train_time:47326ms step_avg:57.36ms
step:826/2330 train_time:47387ms step_avg:57.37ms
step:827/2330 train_time:47443ms step_avg:57.37ms
step:828/2330 train_time:47504ms step_avg:57.37ms
step:829/2330 train_time:47562ms step_avg:57.37ms
step:830/2330 train_time:47621ms step_avg:57.37ms
step:831/2330 train_time:47678ms step_avg:57.37ms
step:832/2330 train_time:47738ms step_avg:57.38ms
step:833/2330 train_time:47794ms step_avg:57.38ms
step:834/2330 train_time:47854ms step_avg:57.38ms
step:835/2330 train_time:47911ms step_avg:57.38ms
step:836/2330 train_time:47971ms step_avg:57.38ms
step:837/2330 train_time:48028ms step_avg:57.38ms
step:838/2330 train_time:48088ms step_avg:57.38ms
step:839/2330 train_time:48145ms step_avg:57.38ms
step:840/2330 train_time:48205ms step_avg:57.39ms
step:841/2330 train_time:48263ms step_avg:57.39ms
step:842/2330 train_time:48322ms step_avg:57.39ms
step:843/2330 train_time:48379ms step_avg:57.39ms
step:844/2330 train_time:48439ms step_avg:57.39ms
step:845/2330 train_time:48496ms step_avg:57.39ms
step:846/2330 train_time:48556ms step_avg:57.39ms
step:847/2330 train_time:48613ms step_avg:57.39ms
step:848/2330 train_time:48673ms step_avg:57.40ms
step:849/2330 train_time:48730ms step_avg:57.40ms
step:850/2330 train_time:48790ms step_avg:57.40ms
step:851/2330 train_time:48846ms step_avg:57.40ms
step:852/2330 train_time:48908ms step_avg:57.40ms
step:853/2330 train_time:48963ms step_avg:57.40ms
step:854/2330 train_time:49024ms step_avg:57.41ms
step:855/2330 train_time:49081ms step_avg:57.40ms
step:856/2330 train_time:49140ms step_avg:57.41ms
step:857/2330 train_time:49197ms step_avg:57.41ms
step:858/2330 train_time:49257ms step_avg:57.41ms
step:859/2330 train_time:49314ms step_avg:57.41ms
step:860/2330 train_time:49374ms step_avg:57.41ms
step:861/2330 train_time:49431ms step_avg:57.41ms
step:862/2330 train_time:49492ms step_avg:57.41ms
step:863/2330 train_time:49548ms step_avg:57.41ms
step:864/2330 train_time:49608ms step_avg:57.42ms
step:865/2330 train_time:49665ms step_avg:57.42ms
step:866/2330 train_time:49725ms step_avg:57.42ms
step:867/2330 train_time:49781ms step_avg:57.42ms
step:868/2330 train_time:49841ms step_avg:57.42ms
step:869/2330 train_time:49898ms step_avg:57.42ms
step:870/2330 train_time:49958ms step_avg:57.42ms
step:871/2330 train_time:50015ms step_avg:57.42ms
step:872/2330 train_time:50074ms step_avg:57.42ms
step:873/2330 train_time:50131ms step_avg:57.42ms
step:874/2330 train_time:50191ms step_avg:57.43ms
step:875/2330 train_time:50248ms step_avg:57.43ms
step:876/2330 train_time:50309ms step_avg:57.43ms
step:877/2330 train_time:50365ms step_avg:57.43ms
step:878/2330 train_time:50427ms step_avg:57.43ms
step:879/2330 train_time:50483ms step_avg:57.43ms
step:880/2330 train_time:50544ms step_avg:57.44ms
step:881/2330 train_time:50601ms step_avg:57.44ms
step:882/2330 train_time:50661ms step_avg:57.44ms
step:883/2330 train_time:50718ms step_avg:57.44ms
step:884/2330 train_time:50777ms step_avg:57.44ms
step:885/2330 train_time:50834ms step_avg:57.44ms
step:886/2330 train_time:50894ms step_avg:57.44ms
step:887/2330 train_time:50951ms step_avg:57.44ms
step:888/2330 train_time:51011ms step_avg:57.44ms
step:889/2330 train_time:51067ms step_avg:57.44ms
step:890/2330 train_time:51127ms step_avg:57.45ms
step:891/2330 train_time:51183ms step_avg:57.44ms
step:892/2330 train_time:51244ms step_avg:57.45ms
step:893/2330 train_time:51302ms step_avg:57.45ms
step:894/2330 train_time:51362ms step_avg:57.45ms
step:895/2330 train_time:51419ms step_avg:57.45ms
step:896/2330 train_time:51479ms step_avg:57.45ms
step:897/2330 train_time:51535ms step_avg:57.45ms
step:898/2330 train_time:51595ms step_avg:57.46ms
step:899/2330 train_time:51652ms step_avg:57.46ms
step:900/2330 train_time:51711ms step_avg:57.46ms
step:901/2330 train_time:51767ms step_avg:57.46ms
step:902/2330 train_time:51827ms step_avg:57.46ms
step:903/2330 train_time:51883ms step_avg:57.46ms
step:904/2330 train_time:51944ms step_avg:57.46ms
step:905/2330 train_time:52001ms step_avg:57.46ms
step:906/2330 train_time:52060ms step_avg:57.46ms
step:907/2330 train_time:52117ms step_avg:57.46ms
step:908/2330 train_time:52177ms step_avg:57.46ms
step:909/2330 train_time:52234ms step_avg:57.46ms
step:910/2330 train_time:52294ms step_avg:57.47ms
step:911/2330 train_time:52351ms step_avg:57.47ms
step:912/2330 train_time:52411ms step_avg:57.47ms
step:913/2330 train_time:52468ms step_avg:57.47ms
step:914/2330 train_time:52528ms step_avg:57.47ms
step:915/2330 train_time:52585ms step_avg:57.47ms
step:916/2330 train_time:52646ms step_avg:57.47ms
step:917/2330 train_time:52702ms step_avg:57.47ms
step:918/2330 train_time:52762ms step_avg:57.48ms
step:919/2330 train_time:52819ms step_avg:57.47ms
step:920/2330 train_time:52878ms step_avg:57.48ms
step:921/2330 train_time:52935ms step_avg:57.48ms
step:922/2330 train_time:52995ms step_avg:57.48ms
step:923/2330 train_time:53051ms step_avg:57.48ms
step:924/2330 train_time:53113ms step_avg:57.48ms
step:925/2330 train_time:53169ms step_avg:57.48ms
step:926/2330 train_time:53231ms step_avg:57.48ms
step:927/2330 train_time:53287ms step_avg:57.48ms
step:928/2330 train_time:53347ms step_avg:57.49ms
step:929/2330 train_time:53404ms step_avg:57.49ms
step:930/2330 train_time:53464ms step_avg:57.49ms
step:931/2330 train_time:53521ms step_avg:57.49ms
step:932/2330 train_time:53581ms step_avg:57.49ms
step:933/2330 train_time:53637ms step_avg:57.49ms
step:934/2330 train_time:53697ms step_avg:57.49ms
step:935/2330 train_time:53754ms step_avg:57.49ms
step:936/2330 train_time:53814ms step_avg:57.49ms
step:937/2330 train_time:53871ms step_avg:57.49ms
step:938/2330 train_time:53932ms step_avg:57.50ms
step:939/2330 train_time:53988ms step_avg:57.49ms
step:940/2330 train_time:54048ms step_avg:57.50ms
step:941/2330 train_time:54105ms step_avg:57.50ms
step:942/2330 train_time:54165ms step_avg:57.50ms
step:943/2330 train_time:54222ms step_avg:57.50ms
step:944/2330 train_time:54282ms step_avg:57.50ms
step:945/2330 train_time:54339ms step_avg:57.50ms
step:946/2330 train_time:54399ms step_avg:57.50ms
step:947/2330 train_time:54456ms step_avg:57.50ms
step:948/2330 train_time:54515ms step_avg:57.51ms
step:949/2330 train_time:54572ms step_avg:57.51ms
step:950/2330 train_time:54632ms step_avg:57.51ms
step:951/2330 train_time:54688ms step_avg:57.51ms
step:952/2330 train_time:54748ms step_avg:57.51ms
step:953/2330 train_time:54805ms step_avg:57.51ms
step:954/2330 train_time:54865ms step_avg:57.51ms
step:955/2330 train_time:54922ms step_avg:57.51ms
step:956/2330 train_time:54981ms step_avg:57.51ms
step:957/2330 train_time:55038ms step_avg:57.51ms
step:958/2330 train_time:55098ms step_avg:57.51ms
step:959/2330 train_time:55154ms step_avg:57.51ms
step:960/2330 train_time:55215ms step_avg:57.52ms
step:961/2330 train_time:55272ms step_avg:57.52ms
step:962/2330 train_time:55333ms step_avg:57.52ms
step:963/2330 train_time:55388ms step_avg:57.52ms
step:964/2330 train_time:55450ms step_avg:57.52ms
step:965/2330 train_time:55506ms step_avg:57.52ms
step:966/2330 train_time:55566ms step_avg:57.52ms
step:967/2330 train_time:55623ms step_avg:57.52ms
step:968/2330 train_time:55683ms step_avg:57.52ms
step:969/2330 train_time:55740ms step_avg:57.52ms
step:970/2330 train_time:55800ms step_avg:57.53ms
step:971/2330 train_time:55857ms step_avg:57.53ms
step:972/2330 train_time:55917ms step_avg:57.53ms
step:973/2330 train_time:55974ms step_avg:57.53ms
step:974/2330 train_time:56033ms step_avg:57.53ms
step:975/2330 train_time:56089ms step_avg:57.53ms
step:976/2330 train_time:56150ms step_avg:57.53ms
step:977/2330 train_time:56207ms step_avg:57.53ms
step:978/2330 train_time:56267ms step_avg:57.53ms
step:979/2330 train_time:56323ms step_avg:57.53ms
step:980/2330 train_time:56383ms step_avg:57.53ms
step:981/2330 train_time:56440ms step_avg:57.53ms
step:982/2330 train_time:56499ms step_avg:57.54ms
step:983/2330 train_time:56556ms step_avg:57.53ms
step:984/2330 train_time:56616ms step_avg:57.54ms
step:985/2330 train_time:56672ms step_avg:57.54ms
step:986/2330 train_time:56733ms step_avg:57.54ms
step:987/2330 train_time:56789ms step_avg:57.54ms
step:988/2330 train_time:56850ms step_avg:57.54ms
step:989/2330 train_time:56906ms step_avg:57.54ms
step:990/2330 train_time:56967ms step_avg:57.54ms
step:991/2330 train_time:57024ms step_avg:57.54ms
step:992/2330 train_time:57085ms step_avg:57.55ms
step:993/2330 train_time:57142ms step_avg:57.55ms
step:994/2330 train_time:57202ms step_avg:57.55ms
step:995/2330 train_time:57259ms step_avg:57.55ms
step:996/2330 train_time:57318ms step_avg:57.55ms
step:997/2330 train_time:57375ms step_avg:57.55ms
step:998/2330 train_time:57435ms step_avg:57.55ms
step:999/2330 train_time:57491ms step_avg:57.55ms
step:1000/2330 train_time:57552ms step_avg:57.55ms
step:1000/2330 val_loss:5.5211 train_time:57633ms step_avg:57.63ms
step:1001/2330 train_time:57651ms step_avg:57.59ms
step:1002/2330 train_time:57672ms step_avg:57.56ms
step:1003/2330 train_time:57730ms step_avg:57.56ms
step:1004/2330 train_time:57797ms step_avg:57.57ms
step:1005/2330 train_time:57854ms step_avg:57.57ms
step:1006/2330 train_time:57917ms step_avg:57.57ms
step:1007/2330 train_time:57974ms step_avg:57.57ms
step:1008/2330 train_time:58033ms step_avg:57.57ms
step:1009/2330 train_time:58089ms step_avg:57.57ms
step:1010/2330 train_time:58149ms step_avg:57.57ms
step:1011/2330 train_time:58205ms step_avg:57.57ms
step:1012/2330 train_time:58265ms step_avg:57.57ms
step:1013/2330 train_time:58321ms step_avg:57.57ms
step:1014/2330 train_time:58380ms step_avg:57.57ms
step:1015/2330 train_time:58436ms step_avg:57.57ms
step:1016/2330 train_time:58495ms step_avg:57.57ms
step:1017/2330 train_time:58551ms step_avg:57.57ms
step:1018/2330 train_time:58613ms step_avg:57.58ms
step:1019/2330 train_time:58671ms step_avg:57.58ms
step:1020/2330 train_time:58733ms step_avg:57.58ms
step:1021/2330 train_time:58790ms step_avg:57.58ms
step:1022/2330 train_time:58852ms step_avg:57.59ms
step:1023/2330 train_time:58908ms step_avg:57.58ms
step:1024/2330 train_time:58970ms step_avg:57.59ms
step:1025/2330 train_time:59027ms step_avg:57.59ms
step:1026/2330 train_time:59087ms step_avg:57.59ms
step:1027/2330 train_time:59143ms step_avg:57.59ms
step:1028/2330 train_time:59202ms step_avg:57.59ms
step:1029/2330 train_time:59259ms step_avg:57.59ms
step:1030/2330 train_time:59318ms step_avg:57.59ms
step:1031/2330 train_time:59375ms step_avg:57.59ms
step:1032/2330 train_time:59434ms step_avg:57.59ms
step:1033/2330 train_time:59491ms step_avg:57.59ms
step:1034/2330 train_time:59551ms step_avg:57.59ms
step:1035/2330 train_time:59608ms step_avg:57.59ms
step:1036/2330 train_time:59669ms step_avg:57.60ms
step:1037/2330 train_time:59726ms step_avg:57.60ms
step:1038/2330 train_time:59787ms step_avg:57.60ms
step:1039/2330 train_time:59844ms step_avg:57.60ms
step:1040/2330 train_time:59906ms step_avg:57.60ms
step:1041/2330 train_time:59962ms step_avg:57.60ms
step:1042/2330 train_time:60023ms step_avg:57.60ms
step:1043/2330 train_time:60080ms step_avg:57.60ms
step:1044/2330 train_time:60139ms step_avg:57.60ms
step:1045/2330 train_time:60196ms step_avg:57.60ms
step:1046/2330 train_time:60255ms step_avg:57.61ms
step:1047/2330 train_time:60312ms step_avg:57.60ms
step:1048/2330 train_time:60371ms step_avg:57.61ms
step:1049/2330 train_time:60427ms step_avg:57.60ms
step:1050/2330 train_time:60487ms step_avg:57.61ms
step:1051/2330 train_time:60544ms step_avg:57.61ms
step:1052/2330 train_time:60605ms step_avg:57.61ms
step:1053/2330 train_time:60662ms step_avg:57.61ms
step:1054/2330 train_time:60721ms step_avg:57.61ms
step:1055/2330 train_time:60778ms step_avg:57.61ms
step:1056/2330 train_time:60838ms step_avg:57.61ms
step:1057/2330 train_time:60895ms step_avg:57.61ms
step:1058/2330 train_time:60956ms step_avg:57.61ms
step:1059/2330 train_time:61014ms step_avg:57.61ms
step:1060/2330 train_time:61074ms step_avg:57.62ms
step:1061/2330 train_time:61130ms step_avg:57.62ms
step:1062/2330 train_time:61190ms step_avg:57.62ms
step:1063/2330 train_time:61247ms step_avg:57.62ms
step:1064/2330 train_time:61306ms step_avg:57.62ms
step:1065/2330 train_time:61363ms step_avg:57.62ms
step:1066/2330 train_time:61423ms step_avg:57.62ms
step:1067/2330 train_time:61481ms step_avg:57.62ms
step:1068/2330 train_time:61540ms step_avg:57.62ms
step:1069/2330 train_time:61597ms step_avg:57.62ms
step:1070/2330 train_time:61656ms step_avg:57.62ms
step:1071/2330 train_time:61714ms step_avg:57.62ms
step:1072/2330 train_time:61773ms step_avg:57.62ms
step:1073/2330 train_time:61830ms step_avg:57.62ms
step:1074/2330 train_time:61891ms step_avg:57.63ms
step:1075/2330 train_time:61948ms step_avg:57.63ms
step:1076/2330 train_time:62008ms step_avg:57.63ms
step:1077/2330 train_time:62065ms step_avg:57.63ms
step:1078/2330 train_time:62126ms step_avg:57.63ms
step:1079/2330 train_time:62182ms step_avg:57.63ms
step:1080/2330 train_time:62242ms step_avg:57.63ms
step:1081/2330 train_time:62299ms step_avg:57.63ms
step:1082/2330 train_time:62358ms step_avg:57.63ms
step:1083/2330 train_time:62415ms step_avg:57.63ms
step:1084/2330 train_time:62475ms step_avg:57.63ms
step:1085/2330 train_time:62533ms step_avg:57.63ms
step:1086/2330 train_time:62592ms step_avg:57.64ms
step:1087/2330 train_time:62649ms step_avg:57.63ms
step:1088/2330 train_time:62709ms step_avg:57.64ms
step:1089/2330 train_time:62766ms step_avg:57.64ms
step:1090/2330 train_time:62826ms step_avg:57.64ms
step:1091/2330 train_time:62883ms step_avg:57.64ms
step:1092/2330 train_time:62943ms step_avg:57.64ms
step:1093/2330 train_time:63000ms step_avg:57.64ms
step:1094/2330 train_time:63060ms step_avg:57.64ms
step:1095/2330 train_time:63117ms step_avg:57.64ms
step:1096/2330 train_time:63177ms step_avg:57.64ms
step:1097/2330 train_time:63234ms step_avg:57.64ms
step:1098/2330 train_time:63294ms step_avg:57.65ms
step:1099/2330 train_time:63351ms step_avg:57.64ms
step:1100/2330 train_time:63410ms step_avg:57.65ms
step:1101/2330 train_time:63468ms step_avg:57.65ms
step:1102/2330 train_time:63527ms step_avg:57.65ms
step:1103/2330 train_time:63585ms step_avg:57.65ms
step:1104/2330 train_time:63644ms step_avg:57.65ms
step:1105/2330 train_time:63702ms step_avg:57.65ms
step:1106/2330 train_time:63762ms step_avg:57.65ms
step:1107/2330 train_time:63819ms step_avg:57.65ms
step:1108/2330 train_time:63880ms step_avg:57.65ms
step:1109/2330 train_time:63936ms step_avg:57.65ms
step:1110/2330 train_time:63996ms step_avg:57.65ms
step:1111/2330 train_time:64054ms step_avg:57.65ms
step:1112/2330 train_time:64113ms step_avg:57.66ms
step:1113/2330 train_time:64170ms step_avg:57.65ms
step:1114/2330 train_time:64230ms step_avg:57.66ms
step:1115/2330 train_time:64287ms step_avg:57.66ms
step:1116/2330 train_time:64347ms step_avg:57.66ms
step:1117/2330 train_time:64403ms step_avg:57.66ms
step:1118/2330 train_time:64464ms step_avg:57.66ms
step:1119/2330 train_time:64521ms step_avg:57.66ms
step:1120/2330 train_time:64580ms step_avg:57.66ms
step:1121/2330 train_time:64637ms step_avg:57.66ms
step:1122/2330 train_time:64697ms step_avg:57.66ms
step:1123/2330 train_time:64754ms step_avg:57.66ms
step:1124/2330 train_time:64814ms step_avg:57.66ms
step:1125/2330 train_time:64871ms step_avg:57.66ms
step:1126/2330 train_time:64930ms step_avg:57.66ms
step:1127/2330 train_time:64987ms step_avg:57.66ms
step:1128/2330 train_time:65047ms step_avg:57.67ms
step:1129/2330 train_time:65104ms step_avg:57.67ms
step:1130/2330 train_time:65164ms step_avg:57.67ms
step:1131/2330 train_time:65221ms step_avg:57.67ms
step:1132/2330 train_time:65281ms step_avg:57.67ms
step:1133/2330 train_time:65338ms step_avg:57.67ms
step:1134/2330 train_time:65397ms step_avg:57.67ms
step:1135/2330 train_time:65454ms step_avg:57.67ms
step:1136/2330 train_time:65514ms step_avg:57.67ms
step:1137/2330 train_time:65570ms step_avg:57.67ms
step:1138/2330 train_time:65630ms step_avg:57.67ms
step:1139/2330 train_time:65686ms step_avg:57.67ms
step:1140/2330 train_time:65747ms step_avg:57.67ms
step:1141/2330 train_time:65803ms step_avg:57.67ms
step:1142/2330 train_time:65864ms step_avg:57.67ms
step:1143/2330 train_time:65921ms step_avg:57.67ms
step:1144/2330 train_time:65981ms step_avg:57.68ms
step:1145/2330 train_time:66038ms step_avg:57.68ms
step:1146/2330 train_time:66098ms step_avg:57.68ms
step:1147/2330 train_time:66156ms step_avg:57.68ms
step:1148/2330 train_time:66215ms step_avg:57.68ms
step:1149/2330 train_time:66273ms step_avg:57.68ms
step:1150/2330 train_time:66333ms step_avg:57.68ms
step:1151/2330 train_time:66389ms step_avg:57.68ms
step:1152/2330 train_time:66450ms step_avg:57.68ms
step:1153/2330 train_time:66506ms step_avg:57.68ms
step:1154/2330 train_time:66566ms step_avg:57.68ms
step:1155/2330 train_time:66623ms step_avg:57.68ms
step:1156/2330 train_time:66684ms step_avg:57.68ms
step:1157/2330 train_time:66741ms step_avg:57.68ms
step:1158/2330 train_time:66801ms step_avg:57.69ms
step:1159/2330 train_time:66858ms step_avg:57.69ms
step:1160/2330 train_time:66918ms step_avg:57.69ms
step:1161/2330 train_time:66975ms step_avg:57.69ms
step:1162/2330 train_time:67034ms step_avg:57.69ms
step:1163/2330 train_time:67090ms step_avg:57.69ms
step:1164/2330 train_time:67151ms step_avg:57.69ms
step:1165/2330 train_time:67208ms step_avg:57.69ms
step:1166/2330 train_time:67267ms step_avg:57.69ms
step:1167/2330 train_time:67324ms step_avg:57.69ms
step:1168/2330 train_time:67384ms step_avg:57.69ms
step:1169/2330 train_time:67441ms step_avg:57.69ms
step:1170/2330 train_time:67501ms step_avg:57.69ms
step:1171/2330 train_time:67557ms step_avg:57.69ms
step:1172/2330 train_time:67617ms step_avg:57.69ms
step:1173/2330 train_time:67675ms step_avg:57.69ms
step:1174/2330 train_time:67735ms step_avg:57.70ms
step:1175/2330 train_time:67791ms step_avg:57.69ms
step:1176/2330 train_time:67850ms step_avg:57.70ms
step:1177/2330 train_time:67907ms step_avg:57.70ms
step:1178/2330 train_time:67967ms step_avg:57.70ms
step:1179/2330 train_time:68024ms step_avg:57.70ms
step:1180/2330 train_time:68085ms step_avg:57.70ms
step:1181/2330 train_time:68142ms step_avg:57.70ms
step:1182/2330 train_time:68202ms step_avg:57.70ms
step:1183/2330 train_time:68258ms step_avg:57.70ms
step:1184/2330 train_time:68318ms step_avg:57.70ms
step:1185/2330 train_time:68375ms step_avg:57.70ms
step:1186/2330 train_time:68435ms step_avg:57.70ms
step:1187/2330 train_time:68491ms step_avg:57.70ms
step:1188/2330 train_time:68551ms step_avg:57.70ms
step:1189/2330 train_time:68608ms step_avg:57.70ms
step:1190/2330 train_time:68669ms step_avg:57.71ms
step:1191/2330 train_time:68726ms step_avg:57.70ms
step:1192/2330 train_time:68786ms step_avg:57.71ms
step:1193/2330 train_time:68842ms step_avg:57.71ms
step:1194/2330 train_time:68903ms step_avg:57.71ms
step:1195/2330 train_time:68959ms step_avg:57.71ms
step:1196/2330 train_time:69019ms step_avg:57.71ms
step:1197/2330 train_time:69076ms step_avg:57.71ms
step:1198/2330 train_time:69136ms step_avg:57.71ms
step:1199/2330 train_time:69193ms step_avg:57.71ms
step:1200/2330 train_time:69252ms step_avg:57.71ms
step:1201/2330 train_time:69308ms step_avg:57.71ms
step:1202/2330 train_time:69370ms step_avg:57.71ms
step:1203/2330 train_time:69426ms step_avg:57.71ms
step:1204/2330 train_time:69486ms step_avg:57.71ms
step:1205/2330 train_time:69542ms step_avg:57.71ms
step:1206/2330 train_time:69603ms step_avg:57.71ms
step:1207/2330 train_time:69660ms step_avg:57.71ms
step:1208/2330 train_time:69719ms step_avg:57.71ms
step:1209/2330 train_time:69777ms step_avg:57.71ms
step:1210/2330 train_time:69836ms step_avg:57.72ms
step:1211/2330 train_time:69893ms step_avg:57.72ms
step:1212/2330 train_time:69953ms step_avg:57.72ms
step:1213/2330 train_time:70010ms step_avg:57.72ms
step:1214/2330 train_time:70069ms step_avg:57.72ms
step:1215/2330 train_time:70126ms step_avg:57.72ms
step:1216/2330 train_time:70186ms step_avg:57.72ms
step:1217/2330 train_time:70242ms step_avg:57.72ms
step:1218/2330 train_time:70303ms step_avg:57.72ms
step:1219/2330 train_time:70360ms step_avg:57.72ms
step:1220/2330 train_time:70419ms step_avg:57.72ms
step:1221/2330 train_time:70477ms step_avg:57.72ms
step:1222/2330 train_time:70537ms step_avg:57.72ms
step:1223/2330 train_time:70594ms step_avg:57.72ms
step:1224/2330 train_time:70653ms step_avg:57.72ms
step:1225/2330 train_time:70711ms step_avg:57.72ms
step:1226/2330 train_time:70770ms step_avg:57.72ms
step:1227/2330 train_time:70827ms step_avg:57.72ms
step:1228/2330 train_time:70887ms step_avg:57.73ms
step:1229/2330 train_time:70943ms step_avg:57.72ms
step:1230/2330 train_time:71004ms step_avg:57.73ms
step:1231/2330 train_time:71061ms step_avg:57.73ms
step:1232/2330 train_time:71121ms step_avg:57.73ms
step:1233/2330 train_time:71178ms step_avg:57.73ms
step:1234/2330 train_time:71237ms step_avg:57.73ms
step:1235/2330 train_time:71295ms step_avg:57.73ms
step:1236/2330 train_time:71354ms step_avg:57.73ms
step:1237/2330 train_time:71411ms step_avg:57.73ms
step:1238/2330 train_time:71471ms step_avg:57.73ms
step:1239/2330 train_time:71528ms step_avg:57.73ms
step:1240/2330 train_time:71588ms step_avg:57.73ms
step:1241/2330 train_time:71644ms step_avg:57.73ms
step:1242/2330 train_time:71705ms step_avg:57.73ms
step:1243/2330 train_time:71761ms step_avg:57.73ms
step:1244/2330 train_time:71822ms step_avg:57.73ms
step:1245/2330 train_time:71879ms step_avg:57.73ms
step:1246/2330 train_time:71939ms step_avg:57.74ms
step:1247/2330 train_time:71997ms step_avg:57.74ms
step:1248/2330 train_time:72057ms step_avg:57.74ms
step:1249/2330 train_time:72114ms step_avg:57.74ms
step:1250/2330 train_time:72174ms step_avg:57.74ms
step:1250/2330 val_loss:5.3729 train_time:72254ms step_avg:57.80ms
step:1251/2330 train_time:72272ms step_avg:57.77ms
step:1252/2330 train_time:72293ms step_avg:57.74ms
step:1253/2330 train_time:72351ms step_avg:57.74ms
step:1254/2330 train_time:72418ms step_avg:57.75ms
step:1255/2330 train_time:72474ms step_avg:57.75ms
step:1256/2330 train_time:72534ms step_avg:57.75ms
step:1257/2330 train_time:72590ms step_avg:57.75ms
step:1258/2330 train_time:72651ms step_avg:57.75ms
step:1259/2330 train_time:72707ms step_avg:57.75ms
step:1260/2330 train_time:72766ms step_avg:57.75ms
step:1261/2330 train_time:72822ms step_avg:57.75ms
step:1262/2330 train_time:72882ms step_avg:57.75ms
step:1263/2330 train_time:72939ms step_avg:57.75ms
step:1264/2330 train_time:72999ms step_avg:57.75ms
step:1265/2330 train_time:73055ms step_avg:57.75ms
step:1266/2330 train_time:73114ms step_avg:57.75ms
step:1267/2330 train_time:73170ms step_avg:57.75ms
step:1268/2330 train_time:73230ms step_avg:57.75ms
step:1269/2330 train_time:73288ms step_avg:57.75ms
step:1270/2330 train_time:73351ms step_avg:57.76ms
step:1271/2330 train_time:73408ms step_avg:57.76ms
step:1272/2330 train_time:73469ms step_avg:57.76ms
step:1273/2330 train_time:73527ms step_avg:57.76ms
step:1274/2330 train_time:73586ms step_avg:57.76ms
step:1275/2330 train_time:73643ms step_avg:57.76ms
step:1276/2330 train_time:73703ms step_avg:57.76ms
step:1277/2330 train_time:73759ms step_avg:57.76ms
step:1278/2330 train_time:73819ms step_avg:57.76ms
step:1279/2330 train_time:73875ms step_avg:57.76ms
step:1280/2330 train_time:73935ms step_avg:57.76ms
step:1281/2330 train_time:73991ms step_avg:57.76ms
step:1282/2330 train_time:74051ms step_avg:57.76ms
step:1283/2330 train_time:74108ms step_avg:57.76ms
step:1284/2330 train_time:74166ms step_avg:57.76ms
step:1285/2330 train_time:74223ms step_avg:57.76ms
step:1286/2330 train_time:74284ms step_avg:57.76ms
step:1287/2330 train_time:74341ms step_avg:57.76ms
step:1288/2330 train_time:74402ms step_avg:57.77ms
step:1289/2330 train_time:74459ms step_avg:57.76ms
step:1290/2330 train_time:74520ms step_avg:57.77ms
step:1291/2330 train_time:74576ms step_avg:57.77ms
step:1292/2330 train_time:74637ms step_avg:57.77ms
step:1293/2330 train_time:74694ms step_avg:57.77ms
step:1294/2330 train_time:74754ms step_avg:57.77ms
step:1295/2330 train_time:74810ms step_avg:57.77ms
step:1296/2330 train_time:74870ms step_avg:57.77ms
step:1297/2330 train_time:74926ms step_avg:57.77ms
step:1298/2330 train_time:74986ms step_avg:57.77ms
step:1299/2330 train_time:75042ms step_avg:57.77ms
step:1300/2330 train_time:75102ms step_avg:57.77ms
step:1301/2330 train_time:75159ms step_avg:57.77ms
step:1302/2330 train_time:75218ms step_avg:57.77ms
step:1303/2330 train_time:75275ms step_avg:57.77ms
step:1304/2330 train_time:75337ms step_avg:57.77ms
step:1305/2330 train_time:75393ms step_avg:57.77ms
step:1306/2330 train_time:75455ms step_avg:57.78ms
step:1307/2330 train_time:75512ms step_avg:57.77ms
step:1308/2330 train_time:75572ms step_avg:57.78ms
step:1309/2330 train_time:75629ms step_avg:57.78ms
step:1310/2330 train_time:75689ms step_avg:57.78ms
step:1311/2330 train_time:75745ms step_avg:57.78ms
step:1312/2330 train_time:75805ms step_avg:57.78ms
step:1313/2330 train_time:75861ms step_avg:57.78ms
step:1314/2330 train_time:75921ms step_avg:57.78ms
step:1315/2330 train_time:75978ms step_avg:57.78ms
step:1316/2330 train_time:76037ms step_avg:57.78ms
step:1317/2330 train_time:76093ms step_avg:57.78ms
step:1318/2330 train_time:76154ms step_avg:57.78ms
step:1319/2330 train_time:76211ms step_avg:57.78ms
step:1320/2330 train_time:76271ms step_avg:57.78ms
step:1321/2330 train_time:76328ms step_avg:57.78ms
step:1322/2330 train_time:76388ms step_avg:57.78ms
step:1323/2330 train_time:76446ms step_avg:57.78ms
step:1324/2330 train_time:76505ms step_avg:57.78ms
step:1325/2330 train_time:76562ms step_avg:57.78ms
step:1326/2330 train_time:76623ms step_avg:57.78ms
step:1327/2330 train_time:76679ms step_avg:57.78ms
step:1328/2330 train_time:76740ms step_avg:57.79ms
step:1329/2330 train_time:76797ms step_avg:57.79ms
step:1330/2330 train_time:76857ms step_avg:57.79ms
step:1331/2330 train_time:76913ms step_avg:57.79ms
step:1332/2330 train_time:76973ms step_avg:57.79ms
step:1333/2330 train_time:77030ms step_avg:57.79ms
step:1334/2330 train_time:77089ms step_avg:57.79ms
step:1335/2330 train_time:77146ms step_avg:57.79ms
step:1336/2330 train_time:77206ms step_avg:57.79ms
step:1337/2330 train_time:77263ms step_avg:57.79ms
step:1338/2330 train_time:77322ms step_avg:57.79ms
step:1339/2330 train_time:77379ms step_avg:57.79ms
step:1340/2330 train_time:77439ms step_avg:57.79ms
step:1341/2330 train_time:77496ms step_avg:57.79ms
step:1342/2330 train_time:77556ms step_avg:57.79ms
step:1343/2330 train_time:77612ms step_avg:57.79ms
step:1344/2330 train_time:77674ms step_avg:57.79ms
step:1345/2330 train_time:77730ms step_avg:57.79ms
step:1346/2330 train_time:77790ms step_avg:57.79ms
step:1347/2330 train_time:77846ms step_avg:57.79ms
step:1348/2330 train_time:77906ms step_avg:57.79ms
step:1349/2330 train_time:77963ms step_avg:57.79ms
step:1350/2330 train_time:78023ms step_avg:57.80ms
step:1351/2330 train_time:78079ms step_avg:57.79ms
step:1352/2330 train_time:78140ms step_avg:57.80ms
step:1353/2330 train_time:78196ms step_avg:57.79ms
step:1354/2330 train_time:78258ms step_avg:57.80ms
step:1355/2330 train_time:78314ms step_avg:57.80ms
step:1356/2330 train_time:78374ms step_avg:57.80ms
step:1357/2330 train_time:78431ms step_avg:57.80ms
step:1358/2330 train_time:78491ms step_avg:57.80ms
step:1359/2330 train_time:78548ms step_avg:57.80ms
step:1360/2330 train_time:78608ms step_avg:57.80ms
step:1361/2330 train_time:78665ms step_avg:57.80ms
step:1362/2330 train_time:78724ms step_avg:57.80ms
step:1363/2330 train_time:78781ms step_avg:57.80ms
step:1364/2330 train_time:78841ms step_avg:57.80ms
step:1365/2330 train_time:78898ms step_avg:57.80ms
step:1366/2330 train_time:78958ms step_avg:57.80ms
step:1367/2330 train_time:79015ms step_avg:57.80ms
step:1368/2330 train_time:79074ms step_avg:57.80ms
step:1369/2330 train_time:79131ms step_avg:57.80ms
step:1370/2330 train_time:79191ms step_avg:57.80ms
step:1371/2330 train_time:79248ms step_avg:57.80ms
step:1372/2330 train_time:79308ms step_avg:57.80ms
step:1373/2330 train_time:79366ms step_avg:57.80ms
step:1374/2330 train_time:79425ms step_avg:57.81ms
step:1375/2330 train_time:79482ms step_avg:57.81ms
step:1376/2330 train_time:79542ms step_avg:57.81ms
step:1377/2330 train_time:79599ms step_avg:57.81ms
step:1378/2330 train_time:79660ms step_avg:57.81ms
step:1379/2330 train_time:79716ms step_avg:57.81ms
step:1380/2330 train_time:79776ms step_avg:57.81ms
step:1381/2330 train_time:79833ms step_avg:57.81ms
step:1382/2330 train_time:79894ms step_avg:57.81ms
step:1383/2330 train_time:79951ms step_avg:57.81ms
step:1384/2330 train_time:80010ms step_avg:57.81ms
step:1385/2330 train_time:80067ms step_avg:57.81ms
step:1386/2330 train_time:80127ms step_avg:57.81ms
step:1387/2330 train_time:80184ms step_avg:57.81ms
step:1388/2330 train_time:80243ms step_avg:57.81ms
step:1389/2330 train_time:80300ms step_avg:57.81ms
step:1390/2330 train_time:80360ms step_avg:57.81ms
step:1391/2330 train_time:80417ms step_avg:57.81ms
step:1392/2330 train_time:80477ms step_avg:57.81ms
step:1393/2330 train_time:80533ms step_avg:57.81ms
step:1394/2330 train_time:80594ms step_avg:57.82ms
step:1395/2330 train_time:80651ms step_avg:57.81ms
step:1396/2330 train_time:80711ms step_avg:57.82ms
step:1397/2330 train_time:80768ms step_avg:57.82ms
step:1398/2330 train_time:80829ms step_avg:57.82ms
step:1399/2330 train_time:80885ms step_avg:57.82ms
step:1400/2330 train_time:80945ms step_avg:57.82ms
step:1401/2330 train_time:81002ms step_avg:57.82ms
step:1402/2330 train_time:81062ms step_avg:57.82ms
step:1403/2330 train_time:81119ms step_avg:57.82ms
step:1404/2330 train_time:81178ms step_avg:57.82ms
step:1405/2330 train_time:81235ms step_avg:57.82ms
step:1406/2330 train_time:81295ms step_avg:57.82ms
step:1407/2330 train_time:81352ms step_avg:57.82ms
step:1408/2330 train_time:81411ms step_avg:57.82ms
step:1409/2330 train_time:81468ms step_avg:57.82ms
step:1410/2330 train_time:81528ms step_avg:57.82ms
step:1411/2330 train_time:81585ms step_avg:57.82ms
step:1412/2330 train_time:81644ms step_avg:57.82ms
step:1413/2330 train_time:81701ms step_avg:57.82ms
step:1414/2330 train_time:81762ms step_avg:57.82ms
step:1415/2330 train_time:81818ms step_avg:57.82ms
step:1416/2330 train_time:81879ms step_avg:57.82ms
step:1417/2330 train_time:81935ms step_avg:57.82ms
step:1418/2330 train_time:81996ms step_avg:57.82ms
step:1419/2330 train_time:82053ms step_avg:57.82ms
step:1420/2330 train_time:82112ms step_avg:57.83ms
step:1421/2330 train_time:82170ms step_avg:57.83ms
step:1422/2330 train_time:82230ms step_avg:57.83ms
step:1423/2330 train_time:82287ms step_avg:57.83ms
step:1424/2330 train_time:82346ms step_avg:57.83ms
step:1425/2330 train_time:82403ms step_avg:57.83ms
step:1426/2330 train_time:82462ms step_avg:57.83ms
step:1427/2330 train_time:82520ms step_avg:57.83ms
step:1428/2330 train_time:82579ms step_avg:57.83ms
step:1429/2330 train_time:82635ms step_avg:57.83ms
step:1430/2330 train_time:82696ms step_avg:57.83ms
step:1431/2330 train_time:82752ms step_avg:57.83ms
step:1432/2330 train_time:82813ms step_avg:57.83ms
step:1433/2330 train_time:82869ms step_avg:57.83ms
step:1434/2330 train_time:82930ms step_avg:57.83ms
step:1435/2330 train_time:82986ms step_avg:57.83ms
step:1436/2330 train_time:83047ms step_avg:57.83ms
step:1437/2330 train_time:83104ms step_avg:57.83ms
step:1438/2330 train_time:83164ms step_avg:57.83ms
step:1439/2330 train_time:83221ms step_avg:57.83ms
step:1440/2330 train_time:83281ms step_avg:57.83ms
step:1441/2330 train_time:83337ms step_avg:57.83ms
step:1442/2330 train_time:83398ms step_avg:57.83ms
step:1443/2330 train_time:83454ms step_avg:57.83ms
step:1444/2330 train_time:83516ms step_avg:57.84ms
step:1445/2330 train_time:83572ms step_avg:57.84ms
step:1446/2330 train_time:83632ms step_avg:57.84ms
step:1447/2330 train_time:83688ms step_avg:57.84ms
step:1448/2330 train_time:83748ms step_avg:57.84ms
step:1449/2330 train_time:83805ms step_avg:57.84ms
step:1450/2330 train_time:83865ms step_avg:57.84ms
step:1451/2330 train_time:83921ms step_avg:57.84ms
step:1452/2330 train_time:83982ms step_avg:57.84ms
step:1453/2330 train_time:84037ms step_avg:57.84ms
step:1454/2330 train_time:84099ms step_avg:57.84ms
step:1455/2330 train_time:84155ms step_avg:57.84ms
step:1456/2330 train_time:84216ms step_avg:57.84ms
step:1457/2330 train_time:84273ms step_avg:57.84ms
step:1458/2330 train_time:84333ms step_avg:57.84ms
step:1459/2330 train_time:84389ms step_avg:57.84ms
step:1460/2330 train_time:84450ms step_avg:57.84ms
step:1461/2330 train_time:84508ms step_avg:57.84ms
step:1462/2330 train_time:84567ms step_avg:57.84ms
step:1463/2330 train_time:84625ms step_avg:57.84ms
step:1464/2330 train_time:84685ms step_avg:57.84ms
step:1465/2330 train_time:84742ms step_avg:57.84ms
step:1466/2330 train_time:84801ms step_avg:57.85ms
step:1467/2330 train_time:84858ms step_avg:57.84ms
step:1468/2330 train_time:84919ms step_avg:57.85ms
step:1469/2330 train_time:84975ms step_avg:57.85ms
step:1470/2330 train_time:85036ms step_avg:57.85ms
step:1471/2330 train_time:85092ms step_avg:57.85ms
step:1472/2330 train_time:85152ms step_avg:57.85ms
step:1473/2330 train_time:85209ms step_avg:57.85ms
step:1474/2330 train_time:85268ms step_avg:57.85ms
step:1475/2330 train_time:85324ms step_avg:57.85ms
step:1476/2330 train_time:85385ms step_avg:57.85ms
step:1477/2330 train_time:85441ms step_avg:57.85ms
step:1478/2330 train_time:85501ms step_avg:57.85ms
step:1479/2330 train_time:85557ms step_avg:57.85ms
step:1480/2330 train_time:85618ms step_avg:57.85ms
step:1481/2330 train_time:85674ms step_avg:57.85ms
step:1482/2330 train_time:85736ms step_avg:57.85ms
step:1483/2330 train_time:85792ms step_avg:57.85ms
step:1484/2330 train_time:85853ms step_avg:57.85ms
step:1485/2330 train_time:85910ms step_avg:57.85ms
step:1486/2330 train_time:85970ms step_avg:57.85ms
step:1487/2330 train_time:86027ms step_avg:57.85ms
step:1488/2330 train_time:86086ms step_avg:57.85ms
step:1489/2330 train_time:86144ms step_avg:57.85ms
step:1490/2330 train_time:86203ms step_avg:57.85ms
step:1491/2330 train_time:86260ms step_avg:57.85ms
step:1492/2330 train_time:86320ms step_avg:57.86ms
step:1493/2330 train_time:86377ms step_avg:57.85ms
step:1494/2330 train_time:86438ms step_avg:57.86ms
step:1495/2330 train_time:86494ms step_avg:57.86ms
step:1496/2330 train_time:86555ms step_avg:57.86ms
step:1497/2330 train_time:86612ms step_avg:57.86ms
step:1498/2330 train_time:86671ms step_avg:57.86ms
step:1499/2330 train_time:86728ms step_avg:57.86ms
step:1500/2330 train_time:86787ms step_avg:57.86ms
step:1500/2330 val_loss:5.2273 train_time:86868ms step_avg:57.91ms
step:1501/2330 train_time:86886ms step_avg:57.89ms
step:1502/2330 train_time:86907ms step_avg:57.86ms
step:1503/2330 train_time:86965ms step_avg:57.86ms
step:1504/2330 train_time:87030ms step_avg:57.87ms
step:1505/2330 train_time:87088ms step_avg:57.87ms
step:1506/2330 train_time:87150ms step_avg:57.87ms
step:1507/2330 train_time:87207ms step_avg:57.87ms
step:1508/2330 train_time:87267ms step_avg:57.87ms
step:1509/2330 train_time:87323ms step_avg:57.87ms
step:1510/2330 train_time:87383ms step_avg:57.87ms
step:1511/2330 train_time:87439ms step_avg:57.87ms
step:1512/2330 train_time:87498ms step_avg:57.87ms
step:1513/2330 train_time:87555ms step_avg:57.87ms
step:1514/2330 train_time:87613ms step_avg:57.87ms
step:1515/2330 train_time:87669ms step_avg:57.87ms
step:1516/2330 train_time:87729ms step_avg:57.87ms
step:1517/2330 train_time:87786ms step_avg:57.87ms
step:1518/2330 train_time:87845ms step_avg:57.87ms
step:1519/2330 train_time:87903ms step_avg:57.87ms
step:1520/2330 train_time:87964ms step_avg:57.87ms
step:1521/2330 train_time:88022ms step_avg:57.87ms
step:1522/2330 train_time:88084ms step_avg:57.87ms
step:1523/2330 train_time:88141ms step_avg:57.87ms
step:1524/2330 train_time:88202ms step_avg:57.88ms
step:1525/2330 train_time:88259ms step_avg:57.87ms
step:1526/2330 train_time:88319ms step_avg:57.88ms
step:1527/2330 train_time:88375ms step_avg:57.88ms
step:1528/2330 train_time:88436ms step_avg:57.88ms
step:1529/2330 train_time:88493ms step_avg:57.88ms
step:1530/2330 train_time:88552ms step_avg:57.88ms
step:1531/2330 train_time:88609ms step_avg:57.88ms
step:1532/2330 train_time:88669ms step_avg:57.88ms
step:1533/2330 train_time:88725ms step_avg:57.88ms
step:1534/2330 train_time:88785ms step_avg:57.88ms
step:1535/2330 train_time:88842ms step_avg:57.88ms
step:1536/2330 train_time:88902ms step_avg:57.88ms
step:1537/2330 train_time:88960ms step_avg:57.88ms
step:1538/2330 train_time:89022ms step_avg:57.88ms
step:1539/2330 train_time:89079ms step_avg:57.88ms
step:1540/2330 train_time:89141ms step_avg:57.88ms
step:1541/2330 train_time:89198ms step_avg:57.88ms
step:1542/2330 train_time:89259ms step_avg:57.89ms
step:1543/2330 train_time:89316ms step_avg:57.88ms
step:1544/2330 train_time:89377ms step_avg:57.89ms
step:1545/2330 train_time:89434ms step_avg:57.89ms
step:1546/2330 train_time:89495ms step_avg:57.89ms
step:1547/2330 train_time:89552ms step_avg:57.89ms
step:1548/2330 train_time:89612ms step_avg:57.89ms
step:1549/2330 train_time:89668ms step_avg:57.89ms
step:1550/2330 train_time:89729ms step_avg:57.89ms
step:1551/2330 train_time:89786ms step_avg:57.89ms
step:1552/2330 train_time:89845ms step_avg:57.89ms
step:1553/2330 train_time:89903ms step_avg:57.89ms
step:1554/2330 train_time:89963ms step_avg:57.89ms
step:1555/2330 train_time:90020ms step_avg:57.89ms
step:1556/2330 train_time:90081ms step_avg:57.89ms
step:1557/2330 train_time:90139ms step_avg:57.89ms
step:1558/2330 train_time:90200ms step_avg:57.89ms
step:1559/2330 train_time:90257ms step_avg:57.89ms
step:1560/2330 train_time:90318ms step_avg:57.90ms
step:1561/2330 train_time:90376ms step_avg:57.90ms
step:1562/2330 train_time:90436ms step_avg:57.90ms
step:1563/2330 train_time:90493ms step_avg:57.90ms
step:1564/2330 train_time:90554ms step_avg:57.90ms
step:1565/2330 train_time:90610ms step_avg:57.90ms
step:1566/2330 train_time:90672ms step_avg:57.90ms
step:1567/2330 train_time:90728ms step_avg:57.90ms
step:1568/2330 train_time:90790ms step_avg:57.90ms
step:1569/2330 train_time:90847ms step_avg:57.90ms
step:1570/2330 train_time:90908ms step_avg:57.90ms
step:1571/2330 train_time:90965ms step_avg:57.90ms
step:1572/2330 train_time:91026ms step_avg:57.90ms
step:1573/2330 train_time:91084ms step_avg:57.90ms
step:1574/2330 train_time:91144ms step_avg:57.91ms
step:1575/2330 train_time:91202ms step_avg:57.91ms
step:1576/2330 train_time:91263ms step_avg:57.91ms
step:1577/2330 train_time:91321ms step_avg:57.91ms
step:1578/2330 train_time:91382ms step_avg:57.91ms
step:1579/2330 train_time:91440ms step_avg:57.91ms
step:1580/2330 train_time:91500ms step_avg:57.91ms
step:1581/2330 train_time:91557ms step_avg:57.91ms
step:1582/2330 train_time:91618ms step_avg:57.91ms
step:1583/2330 train_time:91674ms step_avg:57.91ms
step:1584/2330 train_time:91737ms step_avg:57.91ms
step:1585/2330 train_time:91793ms step_avg:57.91ms
step:1586/2330 train_time:91854ms step_avg:57.92ms
step:1587/2330 train_time:91910ms step_avg:57.91ms
step:1588/2330 train_time:91973ms step_avg:57.92ms
step:1589/2330 train_time:92029ms step_avg:57.92ms
step:1590/2330 train_time:92092ms step_avg:57.92ms
step:1591/2330 train_time:92148ms step_avg:57.92ms
step:1592/2330 train_time:92210ms step_avg:57.92ms
step:1593/2330 train_time:92266ms step_avg:57.92ms
step:1594/2330 train_time:92329ms step_avg:57.92ms
step:1595/2330 train_time:92387ms step_avg:57.92ms
step:1596/2330 train_time:92447ms step_avg:57.92ms
step:1597/2330 train_time:92505ms step_avg:57.92ms
step:1598/2330 train_time:92565ms step_avg:57.93ms
step:1599/2330 train_time:92623ms step_avg:57.93ms
step:1600/2330 train_time:92684ms step_avg:57.93ms
step:1601/2330 train_time:92741ms step_avg:57.93ms
step:1602/2330 train_time:92802ms step_avg:57.93ms
step:1603/2330 train_time:92860ms step_avg:57.93ms
step:1604/2330 train_time:92919ms step_avg:57.93ms
step:1605/2330 train_time:92976ms step_avg:57.93ms
step:1606/2330 train_time:93037ms step_avg:57.93ms
step:1607/2330 train_time:93094ms step_avg:57.93ms
step:1608/2330 train_time:93155ms step_avg:57.93ms
step:1609/2330 train_time:93211ms step_avg:57.93ms
step:1610/2330 train_time:93273ms step_avg:57.93ms
step:1611/2330 train_time:93330ms step_avg:57.93ms
step:1612/2330 train_time:93392ms step_avg:57.94ms
step:1613/2330 train_time:93448ms step_avg:57.93ms
step:1614/2330 train_time:93511ms step_avg:57.94ms
step:1615/2330 train_time:93567ms step_avg:57.94ms
step:1616/2330 train_time:93628ms step_avg:57.94ms
step:1617/2330 train_time:93685ms step_avg:57.94ms
step:1618/2330 train_time:93746ms step_avg:57.94ms
step:1619/2330 train_time:93804ms step_avg:57.94ms
step:1620/2330 train_time:93864ms step_avg:57.94ms
step:1621/2330 train_time:93921ms step_avg:57.94ms
step:1622/2330 train_time:93982ms step_avg:57.94ms
step:1623/2330 train_time:94039ms step_avg:57.94ms
step:1624/2330 train_time:94099ms step_avg:57.94ms
step:1625/2330 train_time:94157ms step_avg:57.94ms
step:1626/2330 train_time:94218ms step_avg:57.94ms
step:1627/2330 train_time:94275ms step_avg:57.94ms
step:1628/2330 train_time:94336ms step_avg:57.95ms
step:1629/2330 train_time:94393ms step_avg:57.95ms
step:1630/2330 train_time:94455ms step_avg:57.95ms
step:1631/2330 train_time:94511ms step_avg:57.95ms
step:1632/2330 train_time:94573ms step_avg:57.95ms
step:1633/2330 train_time:94629ms step_avg:57.95ms
step:1634/2330 train_time:94691ms step_avg:57.95ms
step:1635/2330 train_time:94748ms step_avg:57.95ms
step:1636/2330 train_time:94810ms step_avg:57.95ms
step:1637/2330 train_time:94867ms step_avg:57.95ms
step:1638/2330 train_time:94927ms step_avg:57.95ms
step:1639/2330 train_time:94984ms step_avg:57.95ms
step:1640/2330 train_time:95045ms step_avg:57.95ms
step:1641/2330 train_time:95103ms step_avg:57.95ms
step:1642/2330 train_time:95163ms step_avg:57.96ms
step:1643/2330 train_time:95221ms step_avg:57.96ms
step:1644/2330 train_time:95281ms step_avg:57.96ms
step:1645/2330 train_time:95338ms step_avg:57.96ms
step:1646/2330 train_time:95399ms step_avg:57.96ms
step:1647/2330 train_time:95456ms step_avg:57.96ms
step:1648/2330 train_time:95518ms step_avg:57.96ms
step:1649/2330 train_time:95575ms step_avg:57.96ms
step:1650/2330 train_time:95637ms step_avg:57.96ms
step:1651/2330 train_time:95693ms step_avg:57.96ms
step:1652/2330 train_time:95755ms step_avg:57.96ms
step:1653/2330 train_time:95812ms step_avg:57.96ms
step:1654/2330 train_time:95873ms step_avg:57.96ms
step:1655/2330 train_time:95930ms step_avg:57.96ms
step:1656/2330 train_time:95990ms step_avg:57.96ms
step:1657/2330 train_time:96047ms step_avg:57.96ms
step:1658/2330 train_time:96109ms step_avg:57.97ms
step:1659/2330 train_time:96166ms step_avg:57.97ms
step:1660/2330 train_time:96227ms step_avg:57.97ms
step:1661/2330 train_time:96284ms step_avg:57.97ms
step:1662/2330 train_time:96345ms step_avg:57.97ms
step:1663/2330 train_time:96403ms step_avg:57.97ms
step:1664/2330 train_time:96463ms step_avg:57.97ms
step:1665/2330 train_time:96521ms step_avg:57.97ms
step:1666/2330 train_time:96581ms step_avg:57.97ms
step:1667/2330 train_time:96639ms step_avg:57.97ms
step:1668/2330 train_time:96700ms step_avg:57.97ms
step:1669/2330 train_time:96757ms step_avg:57.97ms
step:1670/2330 train_time:96818ms step_avg:57.97ms
step:1671/2330 train_time:96874ms step_avg:57.97ms
step:1672/2330 train_time:96937ms step_avg:57.98ms
step:1673/2330 train_time:96994ms step_avg:57.98ms
step:1674/2330 train_time:97054ms step_avg:57.98ms
step:1675/2330 train_time:97110ms step_avg:57.98ms
step:1676/2330 train_time:97171ms step_avg:57.98ms
step:1677/2330 train_time:97227ms step_avg:57.98ms
step:1678/2330 train_time:97290ms step_avg:57.98ms
step:1679/2330 train_time:97346ms step_avg:57.98ms
step:1680/2330 train_time:97408ms step_avg:57.98ms
step:1681/2330 train_time:97465ms step_avg:57.98ms
step:1682/2330 train_time:97525ms step_avg:57.98ms
step:1683/2330 train_time:97583ms step_avg:57.98ms
step:1684/2330 train_time:97643ms step_avg:57.98ms
step:1685/2330 train_time:97701ms step_avg:57.98ms
step:1686/2330 train_time:97762ms step_avg:57.98ms
step:1687/2330 train_time:97819ms step_avg:57.98ms
step:1688/2330 train_time:97879ms step_avg:57.99ms
step:1689/2330 train_time:97937ms step_avg:57.99ms
step:1690/2330 train_time:97997ms step_avg:57.99ms
step:1691/2330 train_time:98054ms step_avg:57.99ms
step:1692/2330 train_time:98115ms step_avg:57.99ms
step:1693/2330 train_time:98172ms step_avg:57.99ms
step:1694/2330 train_time:98233ms step_avg:57.99ms
step:1695/2330 train_time:98290ms step_avg:57.99ms
step:1696/2330 train_time:98352ms step_avg:57.99ms
step:1697/2330 train_time:98408ms step_avg:57.99ms
step:1698/2330 train_time:98470ms step_avg:57.99ms
step:1699/2330 train_time:98527ms step_avg:57.99ms
step:1700/2330 train_time:98588ms step_avg:57.99ms
step:1701/2330 train_time:98645ms step_avg:57.99ms
step:1702/2330 train_time:98706ms step_avg:57.99ms
step:1703/2330 train_time:98763ms step_avg:57.99ms
step:1704/2330 train_time:98823ms step_avg:57.99ms
step:1705/2330 train_time:98880ms step_avg:57.99ms
step:1706/2330 train_time:98941ms step_avg:58.00ms
step:1707/2330 train_time:99000ms step_avg:58.00ms
step:1708/2330 train_time:99060ms step_avg:58.00ms
step:1709/2330 train_time:99117ms step_avg:58.00ms
step:1710/2330 train_time:99178ms step_avg:58.00ms
step:1711/2330 train_time:99235ms step_avg:58.00ms
step:1712/2330 train_time:99296ms step_avg:58.00ms
step:1713/2330 train_time:99353ms step_avg:58.00ms
step:1714/2330 train_time:99416ms step_avg:58.00ms
step:1715/2330 train_time:99472ms step_avg:58.00ms
step:1716/2330 train_time:99535ms step_avg:58.00ms
step:1717/2330 train_time:99592ms step_avg:58.00ms
step:1718/2330 train_time:99652ms step_avg:58.00ms
step:1719/2330 train_time:99708ms step_avg:58.00ms
step:1720/2330 train_time:99771ms step_avg:58.01ms
step:1721/2330 train_time:99827ms step_avg:58.01ms
step:1722/2330 train_time:99889ms step_avg:58.01ms
step:1723/2330 train_time:99946ms step_avg:58.01ms
step:1724/2330 train_time:100006ms step_avg:58.01ms
step:1725/2330 train_time:100064ms step_avg:58.01ms
step:1726/2330 train_time:100125ms step_avg:58.01ms
step:1727/2330 train_time:100182ms step_avg:58.01ms
step:1728/2330 train_time:100243ms step_avg:58.01ms
step:1729/2330 train_time:100301ms step_avg:58.01ms
step:1730/2330 train_time:100362ms step_avg:58.01ms
step:1731/2330 train_time:100420ms step_avg:58.01ms
step:1732/2330 train_time:100480ms step_avg:58.01ms
step:1733/2330 train_time:100537ms step_avg:58.01ms
step:1734/2330 train_time:100598ms step_avg:58.02ms
step:1735/2330 train_time:100655ms step_avg:58.01ms
step:1736/2330 train_time:100718ms step_avg:58.02ms
step:1737/2330 train_time:100774ms step_avg:58.02ms
step:1738/2330 train_time:100835ms step_avg:58.02ms
step:1739/2330 train_time:100891ms step_avg:58.02ms
step:1740/2330 train_time:100953ms step_avg:58.02ms
step:1741/2330 train_time:101009ms step_avg:58.02ms
step:1742/2330 train_time:101070ms step_avg:58.02ms
step:1743/2330 train_time:101127ms step_avg:58.02ms
step:1744/2330 train_time:101189ms step_avg:58.02ms
step:1745/2330 train_time:101246ms step_avg:58.02ms
step:1746/2330 train_time:101306ms step_avg:58.02ms
step:1747/2330 train_time:101364ms step_avg:58.02ms
step:1748/2330 train_time:101424ms step_avg:58.02ms
step:1749/2330 train_time:101483ms step_avg:58.02ms
step:1750/2330 train_time:101543ms step_avg:58.02ms
step:1750/2330 val_loss:5.1055 train_time:101624ms step_avg:58.07ms
step:1751/2330 train_time:101643ms step_avg:58.05ms
step:1752/2330 train_time:101663ms step_avg:58.03ms
step:1753/2330 train_time:101718ms step_avg:58.03ms
step:1754/2330 train_time:101782ms step_avg:58.03ms
step:1755/2330 train_time:101839ms step_avg:58.03ms
step:1756/2330 train_time:101900ms step_avg:58.03ms
step:1757/2330 train_time:101956ms step_avg:58.03ms
step:1758/2330 train_time:102017ms step_avg:58.03ms
step:1759/2330 train_time:102073ms step_avg:58.03ms
step:1760/2330 train_time:102133ms step_avg:58.03ms
step:1761/2330 train_time:102190ms step_avg:58.03ms
step:1762/2330 train_time:102249ms step_avg:58.03ms
step:1763/2330 train_time:102306ms step_avg:58.03ms
step:1764/2330 train_time:102366ms step_avg:58.03ms
step:1765/2330 train_time:102422ms step_avg:58.03ms
step:1766/2330 train_time:102482ms step_avg:58.03ms
step:1767/2330 train_time:102541ms step_avg:58.03ms
step:1768/2330 train_time:102606ms step_avg:58.03ms
step:1769/2330 train_time:102663ms step_avg:58.03ms
step:1770/2330 train_time:102727ms step_avg:58.04ms
step:1771/2330 train_time:102784ms step_avg:58.04ms
step:1772/2330 train_time:102845ms step_avg:58.04ms
step:1773/2330 train_time:102901ms step_avg:58.04ms
step:1774/2330 train_time:102963ms step_avg:58.04ms
step:1775/2330 train_time:103018ms step_avg:58.04ms
step:1776/2330 train_time:103081ms step_avg:58.04ms
step:1777/2330 train_time:103137ms step_avg:58.04ms
step:1778/2330 train_time:103198ms step_avg:58.04ms
step:1779/2330 train_time:103255ms step_avg:58.04ms
step:1780/2330 train_time:103315ms step_avg:58.04ms
step:1781/2330 train_time:103371ms step_avg:58.04ms
step:1782/2330 train_time:103431ms step_avg:58.04ms
step:1783/2330 train_time:103489ms step_avg:58.04ms
step:1784/2330 train_time:103550ms step_avg:58.04ms
step:1785/2330 train_time:103609ms step_avg:58.04ms
step:1786/2330 train_time:103670ms step_avg:58.05ms
step:1787/2330 train_time:103729ms step_avg:58.05ms
step:1788/2330 train_time:103789ms step_avg:58.05ms
step:1789/2330 train_time:103846ms step_avg:58.05ms
step:1790/2330 train_time:103908ms step_avg:58.05ms
step:1791/2330 train_time:103966ms step_avg:58.05ms
step:1792/2330 train_time:104027ms step_avg:58.05ms
step:1793/2330 train_time:104083ms step_avg:58.05ms
step:1794/2330 train_time:104144ms step_avg:58.05ms
step:1795/2330 train_time:104201ms step_avg:58.05ms
step:1796/2330 train_time:104262ms step_avg:58.05ms
step:1797/2330 train_time:104319ms step_avg:58.05ms
step:1798/2330 train_time:104379ms step_avg:58.05ms
step:1799/2330 train_time:104435ms step_avg:58.05ms
step:1800/2330 train_time:104497ms step_avg:58.05ms
step:1801/2330 train_time:104555ms step_avg:58.05ms
step:1802/2330 train_time:104617ms step_avg:58.06ms
step:1803/2330 train_time:104675ms step_avg:58.06ms
step:1804/2330 train_time:104736ms step_avg:58.06ms
step:1805/2330 train_time:104794ms step_avg:58.06ms
step:1806/2330 train_time:104854ms step_avg:58.06ms
step:1807/2330 train_time:104913ms step_avg:58.06ms
step:1808/2330 train_time:104973ms step_avg:58.06ms
step:1809/2330 train_time:105030ms step_avg:58.06ms
step:1810/2330 train_time:105090ms step_avg:58.06ms
step:1811/2330 train_time:105148ms step_avg:58.06ms
step:1812/2330 train_time:105208ms step_avg:58.06ms
step:1813/2330 train_time:105265ms step_avg:58.06ms
step:1814/2330 train_time:105326ms step_avg:58.06ms
step:1815/2330 train_time:105383ms step_avg:58.06ms
step:1816/2330 train_time:105443ms step_avg:58.06ms
step:1817/2330 train_time:105500ms step_avg:58.06ms
step:1818/2330 train_time:105561ms step_avg:58.06ms
step:1819/2330 train_time:105619ms step_avg:58.06ms
step:1820/2330 train_time:105680ms step_avg:58.07ms
step:1821/2330 train_time:105736ms step_avg:58.07ms
step:1822/2330 train_time:105797ms step_avg:58.07ms
step:1823/2330 train_time:105854ms step_avg:58.07ms
step:1824/2330 train_time:105916ms step_avg:58.07ms
step:1825/2330 train_time:105974ms step_avg:58.07ms
step:1826/2330 train_time:106035ms step_avg:58.07ms
step:1827/2330 train_time:106092ms step_avg:58.07ms
step:1828/2330 train_time:106152ms step_avg:58.07ms
step:1829/2330 train_time:106209ms step_avg:58.07ms
step:1830/2330 train_time:106270ms step_avg:58.07ms
step:1831/2330 train_time:106328ms step_avg:58.07ms
step:1832/2330 train_time:106388ms step_avg:58.07ms
step:1833/2330 train_time:106446ms step_avg:58.07ms
step:1834/2330 train_time:106505ms step_avg:58.07ms
step:1835/2330 train_time:106562ms step_avg:58.07ms
step:1836/2330 train_time:106624ms step_avg:58.07ms
step:1837/2330 train_time:106681ms step_avg:58.07ms
step:1838/2330 train_time:106742ms step_avg:58.08ms
step:1839/2330 train_time:106799ms step_avg:58.07ms
step:1840/2330 train_time:106862ms step_avg:58.08ms
step:1841/2330 train_time:106919ms step_avg:58.08ms
step:1842/2330 train_time:106980ms step_avg:58.08ms
step:1843/2330 train_time:107037ms step_avg:58.08ms
step:1844/2330 train_time:107098ms step_avg:58.08ms
step:1845/2330 train_time:107155ms step_avg:58.08ms
step:1846/2330 train_time:107216ms step_avg:58.08ms
step:1847/2330 train_time:107273ms step_avg:58.08ms
step:1848/2330 train_time:107334ms step_avg:58.08ms
step:1849/2330 train_time:107391ms step_avg:58.08ms
step:1850/2330 train_time:107451ms step_avg:58.08ms
step:1851/2330 train_time:107509ms step_avg:58.08ms
step:1852/2330 train_time:107569ms step_avg:58.08ms
step:1853/2330 train_time:107627ms step_avg:58.08ms
step:1854/2330 train_time:107688ms step_avg:58.08ms
step:1855/2330 train_time:107745ms step_avg:58.08ms
step:1856/2330 train_time:107806ms step_avg:58.09ms
step:1857/2330 train_time:107864ms step_avg:58.09ms
step:1858/2330 train_time:107925ms step_avg:58.09ms
step:1859/2330 train_time:107981ms step_avg:58.09ms
step:1860/2330 train_time:108043ms step_avg:58.09ms
step:1861/2330 train_time:108100ms step_avg:58.09ms
step:1862/2330 train_time:108162ms step_avg:58.09ms
step:1863/2330 train_time:108219ms step_avg:58.09ms
step:1864/2330 train_time:108280ms step_avg:58.09ms
step:1865/2330 train_time:108336ms step_avg:58.09ms
step:1866/2330 train_time:108398ms step_avg:58.09ms
step:1867/2330 train_time:108455ms step_avg:58.09ms
step:1868/2330 train_time:108516ms step_avg:58.09ms
step:1869/2330 train_time:108574ms step_avg:58.09ms
step:1870/2330 train_time:108634ms step_avg:58.09ms
step:1871/2330 train_time:108691ms step_avg:58.09ms
step:1872/2330 train_time:108751ms step_avg:58.09ms
step:1873/2330 train_time:108810ms step_avg:58.09ms
step:1874/2330 train_time:108871ms step_avg:58.10ms
step:1875/2330 train_time:108928ms step_avg:58.09ms
step:1876/2330 train_time:108988ms step_avg:58.10ms
step:1877/2330 train_time:109045ms step_avg:58.10ms
step:1878/2330 train_time:109106ms step_avg:58.10ms
step:1879/2330 train_time:109163ms step_avg:58.10ms
step:1880/2330 train_time:109224ms step_avg:58.10ms
step:1881/2330 train_time:109281ms step_avg:58.10ms
step:1882/2330 train_time:109343ms step_avg:58.10ms
step:1883/2330 train_time:109400ms step_avg:58.10ms
step:1884/2330 train_time:109461ms step_avg:58.10ms
step:1885/2330 train_time:109518ms step_avg:58.10ms
step:1886/2330 train_time:109580ms step_avg:58.10ms
step:1887/2330 train_time:109637ms step_avg:58.10ms
step:1888/2330 train_time:109698ms step_avg:58.10ms
step:1889/2330 train_time:109755ms step_avg:58.10ms
step:1890/2330 train_time:109817ms step_avg:58.10ms
step:1891/2330 train_time:109875ms step_avg:58.10ms
step:1892/2330 train_time:109935ms step_avg:58.11ms
step:1893/2330 train_time:109992ms step_avg:58.10ms
step:1894/2330 train_time:110053ms step_avg:58.11ms
step:1895/2330 train_time:110111ms step_avg:58.11ms
step:1896/2330 train_time:110171ms step_avg:58.11ms
step:1897/2330 train_time:110229ms step_avg:58.11ms
step:1898/2330 train_time:110290ms step_avg:58.11ms
step:1899/2330 train_time:110347ms step_avg:58.11ms
step:1900/2330 train_time:110408ms step_avg:58.11ms
step:1901/2330 train_time:110465ms step_avg:58.11ms
step:1902/2330 train_time:110527ms step_avg:58.11ms
step:1903/2330 train_time:110583ms step_avg:58.11ms
step:1904/2330 train_time:110645ms step_avg:58.11ms
step:1905/2330 train_time:110702ms step_avg:58.11ms
step:1906/2330 train_time:110764ms step_avg:58.11ms
step:1907/2330 train_time:110821ms step_avg:58.11ms
step:1908/2330 train_time:110882ms step_avg:58.11ms
step:1909/2330 train_time:110939ms step_avg:58.11ms
step:1910/2330 train_time:111000ms step_avg:58.12ms
step:1911/2330 train_time:111056ms step_avg:58.11ms
step:1912/2330 train_time:111118ms step_avg:58.12ms
step:1913/2330 train_time:111175ms step_avg:58.12ms
step:1914/2330 train_time:111235ms step_avg:58.12ms
step:1915/2330 train_time:111293ms step_avg:58.12ms
step:1916/2330 train_time:111353ms step_avg:58.12ms
step:1917/2330 train_time:111411ms step_avg:58.12ms
step:1918/2330 train_time:111472ms step_avg:58.12ms
step:1919/2330 train_time:111530ms step_avg:58.12ms
step:1920/2330 train_time:111590ms step_avg:58.12ms
step:1921/2330 train_time:111648ms step_avg:58.12ms
step:1922/2330 train_time:111708ms step_avg:58.12ms
step:1923/2330 train_time:111765ms step_avg:58.12ms
step:1924/2330 train_time:111826ms step_avg:58.12ms
step:1925/2330 train_time:111883ms step_avg:58.12ms
step:1926/2330 train_time:111945ms step_avg:58.12ms
step:1927/2330 train_time:112001ms step_avg:58.12ms
step:1928/2330 train_time:112064ms step_avg:58.12ms
step:1929/2330 train_time:112120ms step_avg:58.12ms
step:1930/2330 train_time:112183ms step_avg:58.13ms
step:1931/2330 train_time:112239ms step_avg:58.13ms
step:1932/2330 train_time:112301ms step_avg:58.13ms
step:1933/2330 train_time:112357ms step_avg:58.13ms
step:1934/2330 train_time:112420ms step_avg:58.13ms
step:1935/2330 train_time:112476ms step_avg:58.13ms
step:1936/2330 train_time:112538ms step_avg:58.13ms
step:1937/2330 train_time:112595ms step_avg:58.13ms
step:1938/2330 train_time:112655ms step_avg:58.13ms
step:1939/2330 train_time:112713ms step_avg:58.13ms
step:1940/2330 train_time:112774ms step_avg:58.13ms
step:1941/2330 train_time:112833ms step_avg:58.13ms
step:1942/2330 train_time:112893ms step_avg:58.13ms
step:1943/2330 train_time:112950ms step_avg:58.13ms
step:1944/2330 train_time:113011ms step_avg:58.13ms
step:1945/2330 train_time:113069ms step_avg:58.13ms
step:1946/2330 train_time:113129ms step_avg:58.13ms
step:1947/2330 train_time:113186ms step_avg:58.13ms
step:1948/2330 train_time:113247ms step_avg:58.14ms
step:1949/2330 train_time:113305ms step_avg:58.13ms
step:1950/2330 train_time:113366ms step_avg:58.14ms
step:1951/2330 train_time:113422ms step_avg:58.14ms
step:1952/2330 train_time:113484ms step_avg:58.14ms
step:1953/2330 train_time:113540ms step_avg:58.14ms
step:1954/2330 train_time:113602ms step_avg:58.14ms
step:1955/2330 train_time:113658ms step_avg:58.14ms
step:1956/2330 train_time:113720ms step_avg:58.14ms
step:1957/2330 train_time:113777ms step_avg:58.14ms
step:1958/2330 train_time:113837ms step_avg:58.14ms
step:1959/2330 train_time:113894ms step_avg:58.14ms
step:1960/2330 train_time:113955ms step_avg:58.14ms
step:1961/2330 train_time:114012ms step_avg:58.14ms
step:1962/2330 train_time:114072ms step_avg:58.14ms
step:1963/2330 train_time:114131ms step_avg:58.14ms
step:1964/2330 train_time:114191ms step_avg:58.14ms
step:1965/2330 train_time:114249ms step_avg:58.14ms
step:1966/2330 train_time:114310ms step_avg:58.14ms
step:1967/2330 train_time:114368ms step_avg:58.14ms
step:1968/2330 train_time:114428ms step_avg:58.14ms
step:1969/2330 train_time:114485ms step_avg:58.14ms
step:1970/2330 train_time:114545ms step_avg:58.14ms
step:1971/2330 train_time:114603ms step_avg:58.14ms
step:1972/2330 train_time:114663ms step_avg:58.15ms
step:1973/2330 train_time:114720ms step_avg:58.14ms
step:1974/2330 train_time:114782ms step_avg:58.15ms
step:1975/2330 train_time:114838ms step_avg:58.15ms
step:1976/2330 train_time:114899ms step_avg:58.15ms
step:1977/2330 train_time:114956ms step_avg:58.15ms
step:1978/2330 train_time:115018ms step_avg:58.15ms
step:1979/2330 train_time:115075ms step_avg:58.15ms
step:1980/2330 train_time:115138ms step_avg:58.15ms
step:1981/2330 train_time:115195ms step_avg:58.15ms
step:1982/2330 train_time:115257ms step_avg:58.15ms
step:1983/2330 train_time:115315ms step_avg:58.15ms
step:1984/2330 train_time:115376ms step_avg:58.15ms
step:1985/2330 train_time:115435ms step_avg:58.15ms
step:1986/2330 train_time:115495ms step_avg:58.15ms
step:1987/2330 train_time:115552ms step_avg:58.15ms
step:1988/2330 train_time:115612ms step_avg:58.15ms
step:1989/2330 train_time:115669ms step_avg:58.15ms
step:1990/2330 train_time:115729ms step_avg:58.16ms
step:1991/2330 train_time:115786ms step_avg:58.15ms
step:1992/2330 train_time:115847ms step_avg:58.16ms
step:1993/2330 train_time:115905ms step_avg:58.16ms
step:1994/2330 train_time:115966ms step_avg:58.16ms
step:1995/2330 train_time:116023ms step_avg:58.16ms
step:1996/2330 train_time:116086ms step_avg:58.16ms
step:1997/2330 train_time:116142ms step_avg:58.16ms
step:1998/2330 train_time:116205ms step_avg:58.16ms
step:1999/2330 train_time:116261ms step_avg:58.16ms
step:2000/2330 train_time:116323ms step_avg:58.16ms
step:2000/2330 val_loss:5.0161 train_time:116405ms step_avg:58.20ms
step:2001/2330 train_time:116424ms step_avg:58.18ms
step:2002/2330 train_time:116445ms step_avg:58.16ms
step:2003/2330 train_time:116505ms step_avg:58.17ms
step:2004/2330 train_time:116569ms step_avg:58.17ms
step:2005/2330 train_time:116627ms step_avg:58.17ms
step:2006/2330 train_time:116689ms step_avg:58.17ms
step:2007/2330 train_time:116745ms step_avg:58.17ms
step:2008/2330 train_time:116806ms step_avg:58.17ms
step:2009/2330 train_time:116862ms step_avg:58.17ms
step:2010/2330 train_time:116922ms step_avg:58.17ms
step:2011/2330 train_time:116979ms step_avg:58.17ms
step:2012/2330 train_time:117039ms step_avg:58.17ms
step:2013/2330 train_time:117095ms step_avg:58.17ms
step:2014/2330 train_time:117155ms step_avg:58.17ms
step:2015/2330 train_time:117212ms step_avg:58.17ms
step:2016/2330 train_time:117272ms step_avg:58.17ms
step:2017/2330 train_time:117328ms step_avg:58.17ms
step:2018/2330 train_time:117389ms step_avg:58.17ms
step:2019/2330 train_time:117447ms step_avg:58.17ms
step:2020/2330 train_time:117509ms step_avg:58.17ms
step:2021/2330 train_time:117567ms step_avg:58.17ms
step:2022/2330 train_time:117629ms step_avg:58.17ms
step:2023/2330 train_time:117686ms step_avg:58.17ms
step:2024/2330 train_time:117749ms step_avg:58.18ms
step:2025/2330 train_time:117806ms step_avg:58.18ms
step:2026/2330 train_time:117866ms step_avg:58.18ms
step:2027/2330 train_time:117923ms step_avg:58.18ms
step:2028/2330 train_time:117983ms step_avg:58.18ms
step:2029/2330 train_time:118040ms step_avg:58.18ms
step:2030/2330 train_time:118099ms step_avg:58.18ms
step:2031/2330 train_time:118156ms step_avg:58.18ms
step:2032/2330 train_time:118216ms step_avg:58.18ms
step:2033/2330 train_time:118273ms step_avg:58.18ms
step:2034/2330 train_time:118332ms step_avg:58.18ms
step:2035/2330 train_time:118390ms step_avg:58.18ms
step:2036/2330 train_time:118450ms step_avg:58.18ms
step:2037/2330 train_time:118508ms step_avg:58.18ms
step:2038/2330 train_time:118570ms step_avg:58.18ms
step:2039/2330 train_time:118628ms step_avg:58.18ms
step:2040/2330 train_time:118691ms step_avg:58.18ms
step:2041/2330 train_time:118747ms step_avg:58.18ms
step:2042/2330 train_time:118810ms step_avg:58.18ms
step:2043/2330 train_time:118866ms step_avg:58.18ms
step:2044/2330 train_time:118928ms step_avg:58.18ms
step:2045/2330 train_time:118985ms step_avg:58.18ms
step:2046/2330 train_time:119046ms step_avg:58.18ms
step:2047/2330 train_time:119102ms step_avg:58.18ms
step:2048/2330 train_time:119164ms step_avg:58.19ms
step:2049/2330 train_time:119221ms step_avg:58.18ms
step:2050/2330 train_time:119280ms step_avg:58.19ms
step:2051/2330 train_time:119338ms step_avg:58.19ms
step:2052/2330 train_time:119398ms step_avg:58.19ms
step:2053/2330 train_time:119456ms step_avg:58.19ms
step:2054/2330 train_time:119517ms step_avg:58.19ms
step:2055/2330 train_time:119576ms step_avg:58.19ms
step:2056/2330 train_time:119637ms step_avg:58.19ms
step:2057/2330 train_time:119695ms step_avg:58.19ms
step:2058/2330 train_time:119756ms step_avg:58.19ms
step:2059/2330 train_time:119813ms step_avg:58.19ms
step:2060/2330 train_time:119874ms step_avg:58.19ms
step:2061/2330 train_time:119931ms step_avg:58.19ms
step:2062/2330 train_time:119993ms step_avg:58.19ms
step:2063/2330 train_time:120051ms step_avg:58.19ms
step:2064/2330 train_time:120111ms step_avg:58.19ms
step:2065/2330 train_time:120167ms step_avg:58.19ms
step:2066/2330 train_time:120229ms step_avg:58.19ms
step:2067/2330 train_time:120286ms step_avg:58.19ms
step:2068/2330 train_time:120347ms step_avg:58.20ms
step:2069/2330 train_time:120404ms step_avg:58.19ms
step:2070/2330 train_time:120465ms step_avg:58.20ms
step:2071/2330 train_time:120522ms step_avg:58.20ms
step:2072/2330 train_time:120583ms step_avg:58.20ms
step:2073/2330 train_time:120641ms step_avg:58.20ms
step:2074/2330 train_time:120700ms step_avg:58.20ms
step:2075/2330 train_time:120758ms step_avg:58.20ms
step:2076/2330 train_time:120819ms step_avg:58.20ms
step:2077/2330 train_time:120877ms step_avg:58.20ms
step:2078/2330 train_time:120937ms step_avg:58.20ms
step:2079/2330 train_time:120994ms step_avg:58.20ms
step:2080/2330 train_time:121054ms step_avg:58.20ms
step:2081/2330 train_time:121111ms step_avg:58.20ms
step:2082/2330 train_time:121172ms step_avg:58.20ms
step:2083/2330 train_time:121230ms step_avg:58.20ms
step:2084/2330 train_time:121290ms step_avg:58.20ms
step:2085/2330 train_time:121347ms step_avg:58.20ms
step:2086/2330 train_time:121409ms step_avg:58.20ms
step:2087/2330 train_time:121465ms step_avg:58.20ms
step:2088/2330 train_time:121526ms step_avg:58.20ms
step:2089/2330 train_time:121583ms step_avg:58.20ms
step:2090/2330 train_time:121644ms step_avg:58.20ms
step:2091/2330 train_time:121701ms step_avg:58.20ms
step:2092/2330 train_time:121762ms step_avg:58.20ms
step:2093/2330 train_time:121820ms step_avg:58.20ms
step:2094/2330 train_time:121880ms step_avg:58.20ms
step:2095/2330 train_time:121938ms step_avg:58.20ms
step:2096/2330 train_time:121997ms step_avg:58.20ms
step:2097/2330 train_time:122056ms step_avg:58.21ms
step:2098/2330 train_time:122116ms step_avg:58.21ms
step:2099/2330 train_time:122174ms step_avg:58.21ms
step:2100/2330 train_time:122234ms step_avg:58.21ms
step:2101/2330 train_time:122291ms step_avg:58.21ms
step:2102/2330 train_time:122353ms step_avg:58.21ms
step:2103/2330 train_time:122410ms step_avg:58.21ms
step:2104/2330 train_time:122471ms step_avg:58.21ms
step:2105/2330 train_time:122528ms step_avg:58.21ms
step:2106/2330 train_time:122589ms step_avg:58.21ms
step:2107/2330 train_time:122645ms step_avg:58.21ms
step:2108/2330 train_time:122707ms step_avg:58.21ms
step:2109/2330 train_time:122764ms step_avg:58.21ms
step:2110/2330 train_time:122825ms step_avg:58.21ms
step:2111/2330 train_time:122882ms step_avg:58.21ms
step:2112/2330 train_time:122943ms step_avg:58.21ms
step:2113/2330 train_time:123000ms step_avg:58.21ms
step:2114/2330 train_time:123062ms step_avg:58.21ms
step:2115/2330 train_time:123119ms step_avg:58.21ms
step:2116/2330 train_time:123180ms step_avg:58.21ms
step:2117/2330 train_time:123237ms step_avg:58.21ms
step:2118/2330 train_time:123297ms step_avg:58.21ms
step:2119/2330 train_time:123355ms step_avg:58.21ms
step:2120/2330 train_time:123415ms step_avg:58.21ms
step:2121/2330 train_time:123472ms step_avg:58.21ms
step:2122/2330 train_time:123534ms step_avg:58.22ms
step:2123/2330 train_time:123591ms step_avg:58.22ms
step:2124/2330 train_time:123653ms step_avg:58.22ms
step:2125/2330 train_time:123709ms step_avg:58.22ms
step:2126/2330 train_time:123770ms step_avg:58.22ms
step:2127/2330 train_time:123827ms step_avg:58.22ms
step:2128/2330 train_time:123890ms step_avg:58.22ms
step:2129/2330 train_time:123947ms step_avg:58.22ms
step:2130/2330 train_time:124009ms step_avg:58.22ms
step:2131/2330 train_time:124065ms step_avg:58.22ms
step:2132/2330 train_time:124127ms step_avg:58.22ms
step:2133/2330 train_time:124183ms step_avg:58.22ms
step:2134/2330 train_time:124245ms step_avg:58.22ms
step:2135/2330 train_time:124302ms step_avg:58.22ms
step:2136/2330 train_time:124363ms step_avg:58.22ms
step:2137/2330 train_time:124420ms step_avg:58.22ms
step:2138/2330 train_time:124480ms step_avg:58.22ms
step:2139/2330 train_time:124539ms step_avg:58.22ms
step:2140/2330 train_time:124599ms step_avg:58.22ms
step:2141/2330 train_time:124658ms step_avg:58.22ms
step:2142/2330 train_time:124718ms step_avg:58.23ms
step:2143/2330 train_time:124776ms step_avg:58.22ms
step:2144/2330 train_time:124836ms step_avg:58.23ms
step:2145/2330 train_time:124893ms step_avg:58.23ms
step:2146/2330 train_time:124954ms step_avg:58.23ms
step:2147/2330 train_time:125012ms step_avg:58.23ms
step:2148/2330 train_time:125072ms step_avg:58.23ms
step:2149/2330 train_time:125129ms step_avg:58.23ms
step:2150/2330 train_time:125191ms step_avg:58.23ms
step:2151/2330 train_time:125248ms step_avg:58.23ms
step:2152/2330 train_time:125310ms step_avg:58.23ms
step:2153/2330 train_time:125366ms step_avg:58.23ms
step:2154/2330 train_time:125428ms step_avg:58.23ms
step:2155/2330 train_time:125484ms step_avg:58.23ms
step:2156/2330 train_time:125546ms step_avg:58.23ms
step:2157/2330 train_time:125603ms step_avg:58.23ms
step:2158/2330 train_time:125664ms step_avg:58.23ms
step:2159/2330 train_time:125722ms step_avg:58.23ms
step:2160/2330 train_time:125783ms step_avg:58.23ms
step:2161/2330 train_time:125840ms step_avg:58.23ms
step:2162/2330 train_time:125900ms step_avg:58.23ms
step:2163/2330 train_time:125958ms step_avg:58.23ms
step:2164/2330 train_time:126018ms step_avg:58.23ms
step:2165/2330 train_time:126076ms step_avg:58.23ms
step:2166/2330 train_time:126137ms step_avg:58.24ms
step:2167/2330 train_time:126196ms step_avg:58.24ms
step:2168/2330 train_time:126256ms step_avg:58.24ms
step:2169/2330 train_time:126313ms step_avg:58.24ms
step:2170/2330 train_time:126374ms step_avg:58.24ms
step:2171/2330 train_time:126431ms step_avg:58.24ms
step:2172/2330 train_time:126493ms step_avg:58.24ms
step:2173/2330 train_time:126549ms step_avg:58.24ms
step:2174/2330 train_time:126611ms step_avg:58.24ms
step:2175/2330 train_time:126668ms step_avg:58.24ms
step:2176/2330 train_time:126730ms step_avg:58.24ms
step:2177/2330 train_time:126787ms step_avg:58.24ms
step:2178/2330 train_time:126848ms step_avg:58.24ms
step:2179/2330 train_time:126904ms step_avg:58.24ms
step:2180/2330 train_time:126966ms step_avg:58.24ms
step:2181/2330 train_time:127023ms step_avg:58.24ms
step:2182/2330 train_time:127084ms step_avg:58.24ms
step:2183/2330 train_time:127142ms step_avg:58.24ms
step:2184/2330 train_time:127203ms step_avg:58.24ms
step:2185/2330 train_time:127260ms step_avg:58.24ms
step:2186/2330 train_time:127320ms step_avg:58.24ms
step:2187/2330 train_time:127378ms step_avg:58.24ms
step:2188/2330 train_time:127439ms step_avg:58.24ms
step:2189/2330 train_time:127497ms step_avg:58.24ms
step:2190/2330 train_time:127557ms step_avg:58.25ms
step:2191/2330 train_time:127615ms step_avg:58.25ms
step:2192/2330 train_time:127675ms step_avg:58.25ms
step:2193/2330 train_time:127732ms step_avg:58.25ms
step:2194/2330 train_time:127793ms step_avg:58.25ms
step:2195/2330 train_time:127850ms step_avg:58.25ms
step:2196/2330 train_time:127911ms step_avg:58.25ms
step:2197/2330 train_time:127967ms step_avg:58.25ms
step:2198/2330 train_time:128030ms step_avg:58.25ms
step:2199/2330 train_time:128086ms step_avg:58.25ms
step:2200/2330 train_time:128149ms step_avg:58.25ms
step:2201/2330 train_time:128204ms step_avg:58.25ms
step:2202/2330 train_time:128267ms step_avg:58.25ms
step:2203/2330 train_time:128324ms step_avg:58.25ms
step:2204/2330 train_time:128385ms step_avg:58.25ms
step:2205/2330 train_time:128442ms step_avg:58.25ms
step:2206/2330 train_time:128502ms step_avg:58.25ms
step:2207/2330 train_time:128560ms step_avg:58.25ms
step:2208/2330 train_time:128620ms step_avg:58.25ms
step:2209/2330 train_time:128678ms step_avg:58.25ms
step:2210/2330 train_time:128739ms step_avg:58.25ms
step:2211/2330 train_time:128797ms step_avg:58.25ms
step:2212/2330 train_time:128857ms step_avg:58.25ms
step:2213/2330 train_time:128914ms step_avg:58.25ms
step:2214/2330 train_time:128975ms step_avg:58.25ms
step:2215/2330 train_time:129031ms step_avg:58.25ms
step:2216/2330 train_time:129092ms step_avg:58.25ms
step:2217/2330 train_time:129150ms step_avg:58.25ms
step:2218/2330 train_time:129211ms step_avg:58.26ms
step:2219/2330 train_time:129267ms step_avg:58.25ms
step:2220/2330 train_time:129329ms step_avg:58.26ms
step:2221/2330 train_time:129385ms step_avg:58.26ms
step:2222/2330 train_time:129447ms step_avg:58.26ms
step:2223/2330 train_time:129504ms step_avg:58.26ms
step:2224/2330 train_time:129565ms step_avg:58.26ms
step:2225/2330 train_time:129621ms step_avg:58.26ms
step:2226/2330 train_time:129683ms step_avg:58.26ms
step:2227/2330 train_time:129740ms step_avg:58.26ms
step:2228/2330 train_time:129801ms step_avg:58.26ms
step:2229/2330 train_time:129859ms step_avg:58.26ms
step:2230/2330 train_time:129919ms step_avg:58.26ms
step:2231/2330 train_time:129977ms step_avg:58.26ms
step:2232/2330 train_time:130037ms step_avg:58.26ms
step:2233/2330 train_time:130094ms step_avg:58.26ms
step:2234/2330 train_time:130154ms step_avg:58.26ms
step:2235/2330 train_time:130211ms step_avg:58.26ms
step:2236/2330 train_time:130272ms step_avg:58.26ms
step:2237/2330 train_time:130328ms step_avg:58.26ms
step:2238/2330 train_time:130390ms step_avg:58.26ms
step:2239/2330 train_time:130446ms step_avg:58.26ms
step:2240/2330 train_time:130509ms step_avg:58.26ms
step:2241/2330 train_time:130565ms step_avg:58.26ms
step:2242/2330 train_time:130627ms step_avg:58.26ms
step:2243/2330 train_time:130684ms step_avg:58.26ms
step:2244/2330 train_time:130745ms step_avg:58.26ms
step:2245/2330 train_time:130802ms step_avg:58.26ms
step:2246/2330 train_time:130864ms step_avg:58.27ms
step:2247/2330 train_time:130921ms step_avg:58.26ms
step:2248/2330 train_time:130982ms step_avg:58.27ms
step:2249/2330 train_time:131040ms step_avg:58.27ms
step:2250/2330 train_time:131100ms step_avg:58.27ms
step:2250/2330 val_loss:4.9465 train_time:131181ms step_avg:58.30ms
step:2251/2330 train_time:131200ms step_avg:58.29ms
step:2252/2330 train_time:131222ms step_avg:58.27ms
step:2253/2330 train_time:131280ms step_avg:58.27ms
step:2254/2330 train_time:131341ms step_avg:58.27ms
step:2255/2330 train_time:131398ms step_avg:58.27ms
step:2256/2330 train_time:131459ms step_avg:58.27ms
step:2257/2330 train_time:131516ms step_avg:58.27ms
step:2258/2330 train_time:131576ms step_avg:58.27ms
step:2259/2330 train_time:131633ms step_avg:58.27ms
step:2260/2330 train_time:131693ms step_avg:58.27ms
step:2261/2330 train_time:131750ms step_avg:58.27ms
step:2262/2330 train_time:131809ms step_avg:58.27ms
step:2263/2330 train_time:131866ms step_avg:58.27ms
step:2264/2330 train_time:131926ms step_avg:58.27ms
step:2265/2330 train_time:131982ms step_avg:58.27ms
step:2266/2330 train_time:132042ms step_avg:58.27ms
step:2267/2330 train_time:132099ms step_avg:58.27ms
step:2268/2330 train_time:132161ms step_avg:58.27ms
step:2269/2330 train_time:132219ms step_avg:58.27ms
step:2270/2330 train_time:132281ms step_avg:58.27ms
step:2271/2330 train_time:132338ms step_avg:58.27ms
step:2272/2330 train_time:132401ms step_avg:58.27ms
step:2273/2330 train_time:132458ms step_avg:58.27ms
step:2274/2330 train_time:132519ms step_avg:58.28ms
step:2275/2330 train_time:132576ms step_avg:58.28ms
step:2276/2330 train_time:132637ms step_avg:58.28ms
step:2277/2330 train_time:132694ms step_avg:58.28ms
step:2278/2330 train_time:132756ms step_avg:58.28ms
step:2279/2330 train_time:132813ms step_avg:58.28ms
step:2280/2330 train_time:132874ms step_avg:58.28ms
step:2281/2330 train_time:132931ms step_avg:58.28ms
step:2282/2330 train_time:132991ms step_avg:58.28ms
step:2283/2330 train_time:133049ms step_avg:58.28ms
step:2284/2330 train_time:133109ms step_avg:58.28ms
step:2285/2330 train_time:133167ms step_avg:58.28ms
step:2286/2330 train_time:133228ms step_avg:58.28ms
step:2287/2330 train_time:133285ms step_avg:58.28ms
step:2288/2330 train_time:133348ms step_avg:58.28ms
step:2289/2330 train_time:133406ms step_avg:58.28ms
step:2290/2330 train_time:133469ms step_avg:58.28ms
step:2291/2330 train_time:133525ms step_avg:58.28ms
step:2292/2330 train_time:133587ms step_avg:58.28ms
step:2293/2330 train_time:133644ms step_avg:58.28ms
step:2294/2330 train_time:133706ms step_avg:58.29ms
step:2295/2330 train_time:133762ms step_avg:58.28ms
step:2296/2330 train_time:133824ms step_avg:58.29ms
step:2297/2330 train_time:133880ms step_avg:58.28ms
step:2298/2330 train_time:133941ms step_avg:58.29ms
step:2299/2330 train_time:133998ms step_avg:58.29ms
step:2300/2330 train_time:134058ms step_avg:58.29ms
step:2301/2330 train_time:134116ms step_avg:58.29ms
step:2302/2330 train_time:134176ms step_avg:58.29ms
step:2303/2330 train_time:134234ms step_avg:58.29ms
step:2304/2330 train_time:134294ms step_avg:58.29ms
step:2305/2330 train_time:134352ms step_avg:58.29ms
step:2306/2330 train_time:134412ms step_avg:58.29ms
step:2307/2330 train_time:134470ms step_avg:58.29ms
step:2308/2330 train_time:134531ms step_avg:58.29ms
step:2309/2330 train_time:134589ms step_avg:58.29ms
step:2310/2330 train_time:134649ms step_avg:58.29ms
step:2311/2330 train_time:134707ms step_avg:58.29ms
step:2312/2330 train_time:134767ms step_avg:58.29ms
step:2313/2330 train_time:134824ms step_avg:58.29ms
step:2314/2330 train_time:134885ms step_avg:58.29ms
step:2315/2330 train_time:134941ms step_avg:58.29ms
step:2316/2330 train_time:135003ms step_avg:58.29ms
step:2317/2330 train_time:135060ms step_avg:58.29ms
step:2318/2330 train_time:135120ms step_avg:58.29ms
step:2319/2330 train_time:135176ms step_avg:58.29ms
step:2320/2330 train_time:135238ms step_avg:58.29ms
step:2321/2330 train_time:135295ms step_avg:58.29ms
step:2322/2330 train_time:135356ms step_avg:58.29ms
step:2323/2330 train_time:135414ms step_avg:58.29ms
step:2324/2330 train_time:135473ms step_avg:58.29ms
step:2325/2330 train_time:135532ms step_avg:58.29ms
step:2326/2330 train_time:135592ms step_avg:58.29ms
step:2327/2330 train_time:135649ms step_avg:58.29ms
step:2328/2330 train_time:135710ms step_avg:58.29ms
step:2329/2330 train_time:135768ms step_avg:58.29ms
step:2330/2330 train_time:135828ms step_avg:58.30ms
step:2330/2330 val_loss:4.9296 train_time:135910ms step_avg:58.33ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
