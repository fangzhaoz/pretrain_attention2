import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--adamw_lr', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_lr_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_lr_tuning/{args2.adamw_lr}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=float(args2.adamw_lr),  betas=(0.85, 0.85), eps=1e-10, weight_decay=0.0, fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_lr_tuning/{args2.adamw_lr}", exist_ok=True)
            torch.save(log, f"logs_adamw_small_lr_tuning/{args2.adamw_lr}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 01:05:13 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   26C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   26C    P0             116W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   25C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   27C    P0             114W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   25C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   26C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:82ms step_avg:81.89ms
step:2/2330 train_time:175ms step_avg:87.26ms
step:3/2330 train_time:194ms step_avg:64.82ms
step:4/2330 train_time:215ms step_avg:53.65ms
step:5/2330 train_time:266ms step_avg:53.11ms
step:6/2330 train_time:324ms step_avg:53.96ms
step:7/2330 train_time:379ms step_avg:54.09ms
step:8/2330 train_time:437ms step_avg:54.67ms
step:9/2330 train_time:493ms step_avg:54.80ms
step:10/2330 train_time:552ms step_avg:55.19ms
step:11/2330 train_time:607ms step_avg:55.22ms
step:12/2330 train_time:667ms step_avg:55.56ms
step:13/2330 train_time:723ms step_avg:55.60ms
step:14/2330 train_time:781ms step_avg:55.79ms
step:15/2330 train_time:837ms step_avg:55.79ms
step:16/2330 train_time:896ms step_avg:55.97ms
step:17/2330 train_time:952ms step_avg:55.97ms
step:18/2330 train_time:1011ms step_avg:56.18ms
step:19/2330 train_time:1070ms step_avg:56.31ms
step:20/2330 train_time:1133ms step_avg:56.63ms
step:21/2330 train_time:1190ms step_avg:56.68ms
step:22/2330 train_time:1252ms step_avg:56.92ms
step:23/2330 train_time:1309ms step_avg:56.91ms
step:24/2330 train_time:1368ms step_avg:57.01ms
step:25/2330 train_time:1424ms step_avg:56.97ms
step:26/2330 train_time:1484ms step_avg:57.06ms
step:27/2330 train_time:1540ms step_avg:57.03ms
step:28/2330 train_time:1599ms step_avg:57.09ms
step:29/2330 train_time:1655ms step_avg:57.06ms
step:30/2330 train_time:1713ms step_avg:57.10ms
step:31/2330 train_time:1769ms step_avg:57.06ms
step:32/2330 train_time:1828ms step_avg:57.12ms
step:33/2330 train_time:1884ms step_avg:57.09ms
step:34/2330 train_time:1943ms step_avg:57.14ms
step:35/2330 train_time:2000ms step_avg:57.14ms
step:36/2330 train_time:2060ms step_avg:57.22ms
step:37/2330 train_time:2118ms step_avg:57.24ms
step:38/2330 train_time:2177ms step_avg:57.30ms
step:39/2330 train_time:2235ms step_avg:57.31ms
step:40/2330 train_time:2295ms step_avg:57.37ms
step:41/2330 train_time:2352ms step_avg:57.37ms
step:42/2330 train_time:2411ms step_avg:57.40ms
step:43/2330 train_time:2467ms step_avg:57.38ms
step:44/2330 train_time:2527ms step_avg:57.43ms
step:45/2330 train_time:2583ms step_avg:57.41ms
step:46/2330 train_time:2642ms step_avg:57.45ms
step:47/2330 train_time:2698ms step_avg:57.41ms
step:48/2330 train_time:2756ms step_avg:57.43ms
step:49/2330 train_time:2813ms step_avg:57.41ms
step:50/2330 train_time:2872ms step_avg:57.43ms
step:51/2330 train_time:2927ms step_avg:57.40ms
step:52/2330 train_time:2987ms step_avg:57.45ms
step:53/2330 train_time:3044ms step_avg:57.43ms
step:54/2330 train_time:3105ms step_avg:57.50ms
step:55/2330 train_time:3161ms step_avg:57.47ms
step:56/2330 train_time:3223ms step_avg:57.55ms
step:57/2330 train_time:3279ms step_avg:57.52ms
step:58/2330 train_time:3340ms step_avg:57.58ms
step:59/2330 train_time:3396ms step_avg:57.57ms
step:60/2330 train_time:3455ms step_avg:57.59ms
step:61/2330 train_time:3512ms step_avg:57.57ms
step:62/2330 train_time:3570ms step_avg:57.58ms
step:63/2330 train_time:3627ms step_avg:57.57ms
step:64/2330 train_time:3685ms step_avg:57.59ms
step:65/2330 train_time:3742ms step_avg:57.57ms
step:66/2330 train_time:3800ms step_avg:57.58ms
step:67/2330 train_time:3858ms step_avg:57.58ms
step:68/2330 train_time:3916ms step_avg:57.59ms
step:69/2330 train_time:3973ms step_avg:57.59ms
step:70/2330 train_time:4033ms step_avg:57.61ms
step:71/2330 train_time:4089ms step_avg:57.60ms
step:72/2330 train_time:4150ms step_avg:57.63ms
step:73/2330 train_time:4206ms step_avg:57.62ms
step:74/2330 train_time:4267ms step_avg:57.66ms
step:75/2330 train_time:4323ms step_avg:57.64ms
step:76/2330 train_time:4385ms step_avg:57.69ms
step:77/2330 train_time:4442ms step_avg:57.68ms
step:78/2330 train_time:4500ms step_avg:57.70ms
step:79/2330 train_time:4557ms step_avg:57.69ms
step:80/2330 train_time:4616ms step_avg:57.70ms
step:81/2330 train_time:4672ms step_avg:57.68ms
step:82/2330 train_time:4731ms step_avg:57.69ms
step:83/2330 train_time:4787ms step_avg:57.68ms
step:84/2330 train_time:4846ms step_avg:57.70ms
step:85/2330 train_time:4903ms step_avg:57.68ms
step:86/2330 train_time:4961ms step_avg:57.69ms
step:87/2330 train_time:5017ms step_avg:57.67ms
step:88/2330 train_time:5077ms step_avg:57.69ms
step:89/2330 train_time:5134ms step_avg:57.69ms
step:90/2330 train_time:5193ms step_avg:57.70ms
step:91/2330 train_time:5250ms step_avg:57.70ms
step:92/2330 train_time:5310ms step_avg:57.72ms
step:93/2330 train_time:5366ms step_avg:57.70ms
step:94/2330 train_time:5426ms step_avg:57.73ms
step:95/2330 train_time:5483ms step_avg:57.71ms
step:96/2330 train_time:5544ms step_avg:57.74ms
step:97/2330 train_time:5600ms step_avg:57.73ms
step:98/2330 train_time:5658ms step_avg:57.74ms
step:99/2330 train_time:5715ms step_avg:57.73ms
step:100/2330 train_time:5774ms step_avg:57.74ms
step:101/2330 train_time:5830ms step_avg:57.73ms
step:102/2330 train_time:5889ms step_avg:57.74ms
step:103/2330 train_time:5946ms step_avg:57.73ms
step:104/2330 train_time:6005ms step_avg:57.74ms
step:105/2330 train_time:6061ms step_avg:57.73ms
step:106/2330 train_time:6121ms step_avg:57.75ms
step:107/2330 train_time:6178ms step_avg:57.74ms
step:108/2330 train_time:6239ms step_avg:57.76ms
step:109/2330 train_time:6296ms step_avg:57.76ms
step:110/2330 train_time:6355ms step_avg:57.78ms
step:111/2330 train_time:6413ms step_avg:57.77ms
step:112/2330 train_time:6471ms step_avg:57.78ms
step:113/2330 train_time:6528ms step_avg:57.77ms
step:114/2330 train_time:6587ms step_avg:57.78ms
step:115/2330 train_time:6643ms step_avg:57.77ms
step:116/2330 train_time:6704ms step_avg:57.79ms
step:117/2330 train_time:6759ms step_avg:57.77ms
step:118/2330 train_time:6819ms step_avg:57.79ms
step:119/2330 train_time:6876ms step_avg:57.78ms
step:120/2330 train_time:6934ms step_avg:57.79ms
step:121/2330 train_time:6990ms step_avg:57.77ms
step:122/2330 train_time:7050ms step_avg:57.78ms
step:123/2330 train_time:7106ms step_avg:57.77ms
step:124/2330 train_time:7166ms step_avg:57.79ms
step:125/2330 train_time:7222ms step_avg:57.78ms
step:126/2330 train_time:7283ms step_avg:57.80ms
step:127/2330 train_time:7339ms step_avg:57.79ms
step:128/2330 train_time:7399ms step_avg:57.80ms
step:129/2330 train_time:7455ms step_avg:57.79ms
step:130/2330 train_time:7515ms step_avg:57.81ms
step:131/2330 train_time:7571ms step_avg:57.79ms
step:132/2330 train_time:7630ms step_avg:57.80ms
step:133/2330 train_time:7686ms step_avg:57.79ms
step:134/2330 train_time:7746ms step_avg:57.81ms
step:135/2330 train_time:7803ms step_avg:57.80ms
step:136/2330 train_time:7862ms step_avg:57.81ms
step:137/2330 train_time:7919ms step_avg:57.80ms
step:138/2330 train_time:7979ms step_avg:57.82ms
step:139/2330 train_time:8036ms step_avg:57.81ms
step:140/2330 train_time:8095ms step_avg:57.82ms
step:141/2330 train_time:8151ms step_avg:57.81ms
step:142/2330 train_time:8210ms step_avg:57.82ms
step:143/2330 train_time:8266ms step_avg:57.81ms
step:144/2330 train_time:8326ms step_avg:57.82ms
step:145/2330 train_time:8383ms step_avg:57.81ms
step:146/2330 train_time:8443ms step_avg:57.83ms
step:147/2330 train_time:8500ms step_avg:57.82ms
step:148/2330 train_time:8559ms step_avg:57.83ms
step:149/2330 train_time:8616ms step_avg:57.83ms
step:150/2330 train_time:8676ms step_avg:57.84ms
step:151/2330 train_time:8733ms step_avg:57.83ms
step:152/2330 train_time:8791ms step_avg:57.84ms
step:153/2330 train_time:8848ms step_avg:57.83ms
step:154/2330 train_time:8907ms step_avg:57.84ms
step:155/2330 train_time:8963ms step_avg:57.83ms
step:156/2330 train_time:9022ms step_avg:57.83ms
step:157/2330 train_time:9079ms step_avg:57.83ms
step:158/2330 train_time:9138ms step_avg:57.83ms
step:159/2330 train_time:9194ms step_avg:57.83ms
step:160/2330 train_time:9253ms step_avg:57.83ms
step:161/2330 train_time:9310ms step_avg:57.82ms
step:162/2330 train_time:9369ms step_avg:57.83ms
step:163/2330 train_time:9426ms step_avg:57.83ms
step:164/2330 train_time:9486ms step_avg:57.84ms
step:165/2330 train_time:9542ms step_avg:57.83ms
step:166/2330 train_time:9602ms step_avg:57.85ms
step:167/2330 train_time:9659ms step_avg:57.84ms
step:168/2330 train_time:9718ms step_avg:57.84ms
step:169/2330 train_time:9776ms step_avg:57.84ms
step:170/2330 train_time:9835ms step_avg:57.85ms
step:171/2330 train_time:9892ms step_avg:57.85ms
step:172/2330 train_time:9950ms step_avg:57.85ms
step:173/2330 train_time:10007ms step_avg:57.84ms
step:174/2330 train_time:10066ms step_avg:57.85ms
step:175/2330 train_time:10123ms step_avg:57.84ms
step:176/2330 train_time:10182ms step_avg:57.85ms
step:177/2330 train_time:10238ms step_avg:57.84ms
step:178/2330 train_time:10297ms step_avg:57.85ms
step:179/2330 train_time:10354ms step_avg:57.85ms
step:180/2330 train_time:10414ms step_avg:57.85ms
step:181/2330 train_time:10470ms step_avg:57.85ms
step:182/2330 train_time:10529ms step_avg:57.85ms
step:183/2330 train_time:10586ms step_avg:57.85ms
step:184/2330 train_time:10646ms step_avg:57.86ms
step:185/2330 train_time:10703ms step_avg:57.85ms
step:186/2330 train_time:10762ms step_avg:57.86ms
step:187/2330 train_time:10818ms step_avg:57.85ms
step:188/2330 train_time:10878ms step_avg:57.86ms
step:189/2330 train_time:10935ms step_avg:57.86ms
step:190/2330 train_time:10994ms step_avg:57.86ms
step:191/2330 train_time:11051ms step_avg:57.86ms
step:192/2330 train_time:11109ms step_avg:57.86ms
step:193/2330 train_time:11165ms step_avg:57.85ms
step:194/2330 train_time:11225ms step_avg:57.86ms
step:195/2330 train_time:11281ms step_avg:57.85ms
step:196/2330 train_time:11341ms step_avg:57.86ms
step:197/2330 train_time:11397ms step_avg:57.85ms
step:198/2330 train_time:11456ms step_avg:57.86ms
step:199/2330 train_time:11514ms step_avg:57.86ms
step:200/2330 train_time:11573ms step_avg:57.87ms
step:201/2330 train_time:11631ms step_avg:57.87ms
step:202/2330 train_time:11690ms step_avg:57.87ms
step:203/2330 train_time:11746ms step_avg:57.86ms
step:204/2330 train_time:11806ms step_avg:57.87ms
step:205/2330 train_time:11861ms step_avg:57.86ms
step:206/2330 train_time:11922ms step_avg:57.87ms
step:207/2330 train_time:11979ms step_avg:57.87ms
step:208/2330 train_time:12039ms step_avg:57.88ms
step:209/2330 train_time:12095ms step_avg:57.87ms
step:210/2330 train_time:12154ms step_avg:57.88ms
step:211/2330 train_time:12211ms step_avg:57.87ms
step:212/2330 train_time:12269ms step_avg:57.87ms
step:213/2330 train_time:12326ms step_avg:57.87ms
step:214/2330 train_time:12386ms step_avg:57.88ms
step:215/2330 train_time:12442ms step_avg:57.87ms
step:216/2330 train_time:12502ms step_avg:57.88ms
step:217/2330 train_time:12559ms step_avg:57.87ms
step:218/2330 train_time:12618ms step_avg:57.88ms
step:219/2330 train_time:12675ms step_avg:57.88ms
step:220/2330 train_time:12735ms step_avg:57.89ms
step:221/2330 train_time:12791ms step_avg:57.88ms
step:222/2330 train_time:12850ms step_avg:57.88ms
step:223/2330 train_time:12907ms step_avg:57.88ms
step:224/2330 train_time:12966ms step_avg:57.88ms
step:225/2330 train_time:13023ms step_avg:57.88ms
step:226/2330 train_time:13082ms step_avg:57.89ms
step:227/2330 train_time:13139ms step_avg:57.88ms
step:228/2330 train_time:13198ms step_avg:57.89ms
step:229/2330 train_time:13255ms step_avg:57.88ms
step:230/2330 train_time:13314ms step_avg:57.89ms
step:231/2330 train_time:13370ms step_avg:57.88ms
step:232/2330 train_time:13429ms step_avg:57.89ms
step:233/2330 train_time:13487ms step_avg:57.88ms
step:234/2330 train_time:13546ms step_avg:57.89ms
step:235/2330 train_time:13603ms step_avg:57.88ms
step:236/2330 train_time:13663ms step_avg:57.89ms
step:237/2330 train_time:13719ms step_avg:57.89ms
step:238/2330 train_time:13778ms step_avg:57.89ms
step:239/2330 train_time:13835ms step_avg:57.89ms
step:240/2330 train_time:13894ms step_avg:57.89ms
step:241/2330 train_time:13951ms step_avg:57.89ms
step:242/2330 train_time:14010ms step_avg:57.89ms
step:243/2330 train_time:14067ms step_avg:57.89ms
step:244/2330 train_time:14126ms step_avg:57.89ms
step:245/2330 train_time:14182ms step_avg:57.89ms
step:246/2330 train_time:14242ms step_avg:57.90ms
step:247/2330 train_time:14299ms step_avg:57.89ms
step:248/2330 train_time:14358ms step_avg:57.90ms
step:249/2330 train_time:14415ms step_avg:57.89ms
step:250/2330 train_time:14474ms step_avg:57.90ms
step:250/2330 val_loss:5.0197 train_time:14554ms step_avg:58.21ms
step:251/2330 train_time:14573ms step_avg:58.06ms
step:252/2330 train_time:14593ms step_avg:57.91ms
step:253/2330 train_time:14648ms step_avg:57.90ms
step:254/2330 train_time:14713ms step_avg:57.93ms
step:255/2330 train_time:14769ms step_avg:57.92ms
step:256/2330 train_time:14835ms step_avg:57.95ms
step:257/2330 train_time:14890ms step_avg:57.94ms
step:258/2330 train_time:14951ms step_avg:57.95ms
step:259/2330 train_time:15007ms step_avg:57.94ms
step:260/2330 train_time:15066ms step_avg:57.95ms
step:261/2330 train_time:15123ms step_avg:57.94ms
step:262/2330 train_time:15181ms step_avg:57.94ms
step:263/2330 train_time:15237ms step_avg:57.94ms
step:264/2330 train_time:15296ms step_avg:57.94ms
step:265/2330 train_time:15351ms step_avg:57.93ms
step:266/2330 train_time:15410ms step_avg:57.93ms
step:267/2330 train_time:15467ms step_avg:57.93ms
step:268/2330 train_time:15527ms step_avg:57.94ms
step:269/2330 train_time:15584ms step_avg:57.93ms
step:270/2330 train_time:15644ms step_avg:57.94ms
step:271/2330 train_time:15701ms step_avg:57.94ms
step:272/2330 train_time:15764ms step_avg:57.95ms
step:273/2330 train_time:15821ms step_avg:57.95ms
step:274/2330 train_time:15882ms step_avg:57.96ms
step:275/2330 train_time:15938ms step_avg:57.96ms
step:276/2330 train_time:15998ms step_avg:57.96ms
step:277/2330 train_time:16054ms step_avg:57.96ms
step:278/2330 train_time:16113ms step_avg:57.96ms
step:279/2330 train_time:16169ms step_avg:57.95ms
step:280/2330 train_time:16228ms step_avg:57.96ms
step:281/2330 train_time:16284ms step_avg:57.95ms
step:282/2330 train_time:16342ms step_avg:57.95ms
step:283/2330 train_time:16398ms step_avg:57.95ms
step:284/2330 train_time:16457ms step_avg:57.95ms
step:285/2330 train_time:16515ms step_avg:57.95ms
step:286/2330 train_time:16574ms step_avg:57.95ms
step:287/2330 train_time:16630ms step_avg:57.94ms
step:288/2330 train_time:16692ms step_avg:57.96ms
step:289/2330 train_time:16748ms step_avg:57.95ms
step:290/2330 train_time:16810ms step_avg:57.96ms
step:291/2330 train_time:16867ms step_avg:57.96ms
step:292/2330 train_time:16927ms step_avg:57.97ms
step:293/2330 train_time:16983ms step_avg:57.96ms
step:294/2330 train_time:17042ms step_avg:57.97ms
step:295/2330 train_time:17099ms step_avg:57.96ms
step:296/2330 train_time:17158ms step_avg:57.97ms
step:297/2330 train_time:17214ms step_avg:57.96ms
step:298/2330 train_time:17272ms step_avg:57.96ms
step:299/2330 train_time:17328ms step_avg:57.95ms
step:300/2330 train_time:17388ms step_avg:57.96ms
step:301/2330 train_time:17444ms step_avg:57.96ms
step:302/2330 train_time:17504ms step_avg:57.96ms
step:303/2330 train_time:17562ms step_avg:57.96ms
step:304/2330 train_time:17622ms step_avg:57.97ms
step:305/2330 train_time:17679ms step_avg:57.96ms
step:306/2330 train_time:17738ms step_avg:57.97ms
step:307/2330 train_time:17795ms step_avg:57.96ms
step:308/2330 train_time:17856ms step_avg:57.97ms
step:309/2330 train_time:17913ms step_avg:57.97ms
step:310/2330 train_time:17973ms step_avg:57.98ms
step:311/2330 train_time:18029ms step_avg:57.97ms
step:312/2330 train_time:18089ms step_avg:57.98ms
step:313/2330 train_time:18145ms step_avg:57.97ms
step:314/2330 train_time:18205ms step_avg:57.98ms
step:315/2330 train_time:18261ms step_avg:57.97ms
step:316/2330 train_time:18319ms step_avg:57.97ms
step:317/2330 train_time:18376ms step_avg:57.97ms
step:318/2330 train_time:18435ms step_avg:57.97ms
step:319/2330 train_time:18491ms step_avg:57.97ms
step:320/2330 train_time:18551ms step_avg:57.97ms
step:321/2330 train_time:18608ms step_avg:57.97ms
step:322/2330 train_time:18668ms step_avg:57.97ms
step:323/2330 train_time:18724ms step_avg:57.97ms
step:324/2330 train_time:18783ms step_avg:57.97ms
step:325/2330 train_time:18841ms step_avg:57.97ms
step:326/2330 train_time:18901ms step_avg:57.98ms
step:327/2330 train_time:18958ms step_avg:57.98ms
step:328/2330 train_time:19018ms step_avg:57.98ms
step:329/2330 train_time:19075ms step_avg:57.98ms
step:330/2330 train_time:19134ms step_avg:57.98ms
step:331/2330 train_time:19191ms step_avg:57.98ms
step:332/2330 train_time:19249ms step_avg:57.98ms
step:333/2330 train_time:19306ms step_avg:57.98ms
step:334/2330 train_time:19365ms step_avg:57.98ms
step:335/2330 train_time:19421ms step_avg:57.97ms
step:336/2330 train_time:19481ms step_avg:57.98ms
step:337/2330 train_time:19537ms step_avg:57.97ms
step:338/2330 train_time:19597ms step_avg:57.98ms
step:339/2330 train_time:19652ms step_avg:57.97ms
step:340/2330 train_time:19713ms step_avg:57.98ms
step:341/2330 train_time:19769ms step_avg:57.97ms
step:342/2330 train_time:19830ms step_avg:57.98ms
step:343/2330 train_time:19886ms step_avg:57.98ms
step:344/2330 train_time:19947ms step_avg:57.98ms
step:345/2330 train_time:20003ms step_avg:57.98ms
step:346/2330 train_time:20063ms step_avg:57.99ms
step:347/2330 train_time:20119ms step_avg:57.98ms
step:348/2330 train_time:20179ms step_avg:57.99ms
step:349/2330 train_time:20236ms step_avg:57.98ms
step:350/2330 train_time:20295ms step_avg:57.99ms
step:351/2330 train_time:20351ms step_avg:57.98ms
step:352/2330 train_time:20411ms step_avg:57.98ms
step:353/2330 train_time:20466ms step_avg:57.98ms
step:354/2330 train_time:20525ms step_avg:57.98ms
step:355/2330 train_time:20582ms step_avg:57.98ms
step:356/2330 train_time:20642ms step_avg:57.98ms
step:357/2330 train_time:20698ms step_avg:57.98ms
step:358/2330 train_time:20758ms step_avg:57.98ms
step:359/2330 train_time:20815ms step_avg:57.98ms
step:360/2330 train_time:20874ms step_avg:57.98ms
step:361/2330 train_time:20930ms step_avg:57.98ms
step:362/2330 train_time:20990ms step_avg:57.98ms
step:363/2330 train_time:21047ms step_avg:57.98ms
step:364/2330 train_time:21107ms step_avg:57.99ms
step:365/2330 train_time:21163ms step_avg:57.98ms
step:366/2330 train_time:21222ms step_avg:57.98ms
step:367/2330 train_time:21280ms step_avg:57.98ms
step:368/2330 train_time:21339ms step_avg:57.99ms
step:369/2330 train_time:21396ms step_avg:57.98ms
step:370/2330 train_time:21455ms step_avg:57.99ms
step:371/2330 train_time:21512ms step_avg:57.98ms
step:372/2330 train_time:21571ms step_avg:57.99ms
step:373/2330 train_time:21627ms step_avg:57.98ms
step:374/2330 train_time:21686ms step_avg:57.99ms
step:375/2330 train_time:21743ms step_avg:57.98ms
step:376/2330 train_time:21803ms step_avg:57.99ms
step:377/2330 train_time:21860ms step_avg:57.98ms
step:378/2330 train_time:21919ms step_avg:57.99ms
step:379/2330 train_time:21977ms step_avg:57.99ms
step:380/2330 train_time:22036ms step_avg:57.99ms
step:381/2330 train_time:22092ms step_avg:57.98ms
step:382/2330 train_time:22152ms step_avg:57.99ms
step:383/2330 train_time:22209ms step_avg:57.99ms
step:384/2330 train_time:22268ms step_avg:57.99ms
step:385/2330 train_time:22325ms step_avg:57.99ms
step:386/2330 train_time:22384ms step_avg:57.99ms
step:387/2330 train_time:22441ms step_avg:57.99ms
step:388/2330 train_time:22501ms step_avg:57.99ms
step:389/2330 train_time:22559ms step_avg:57.99ms
step:390/2330 train_time:22618ms step_avg:57.99ms
step:391/2330 train_time:22675ms step_avg:57.99ms
step:392/2330 train_time:22734ms step_avg:57.99ms
step:393/2330 train_time:22791ms step_avg:57.99ms
step:394/2330 train_time:22850ms step_avg:57.99ms
step:395/2330 train_time:22906ms step_avg:57.99ms
step:396/2330 train_time:22966ms step_avg:57.99ms
step:397/2330 train_time:23022ms step_avg:57.99ms
step:398/2330 train_time:23083ms step_avg:58.00ms
step:399/2330 train_time:23139ms step_avg:57.99ms
step:400/2330 train_time:23199ms step_avg:58.00ms
step:401/2330 train_time:23256ms step_avg:57.99ms
step:402/2330 train_time:23315ms step_avg:58.00ms
step:403/2330 train_time:23370ms step_avg:57.99ms
step:404/2330 train_time:23430ms step_avg:58.00ms
step:405/2330 train_time:23486ms step_avg:57.99ms
step:406/2330 train_time:23546ms step_avg:58.00ms
step:407/2330 train_time:23604ms step_avg:57.99ms
step:408/2330 train_time:23664ms step_avg:58.00ms
step:409/2330 train_time:23722ms step_avg:58.00ms
step:410/2330 train_time:23781ms step_avg:58.00ms
step:411/2330 train_time:23838ms step_avg:58.00ms
step:412/2330 train_time:23897ms step_avg:58.00ms
step:413/2330 train_time:23953ms step_avg:58.00ms
step:414/2330 train_time:24013ms step_avg:58.00ms
step:415/2330 train_time:24069ms step_avg:58.00ms
step:416/2330 train_time:24129ms step_avg:58.00ms
step:417/2330 train_time:24186ms step_avg:58.00ms
step:418/2330 train_time:24245ms step_avg:58.00ms
step:419/2330 train_time:24302ms step_avg:58.00ms
step:420/2330 train_time:24362ms step_avg:58.00ms
step:421/2330 train_time:24419ms step_avg:58.00ms
step:422/2330 train_time:24478ms step_avg:58.00ms
step:423/2330 train_time:24534ms step_avg:58.00ms
step:424/2330 train_time:24593ms step_avg:58.00ms
step:425/2330 train_time:24650ms step_avg:58.00ms
step:426/2330 train_time:24710ms step_avg:58.00ms
step:427/2330 train_time:24766ms step_avg:58.00ms
step:428/2330 train_time:24826ms step_avg:58.01ms
step:429/2330 train_time:24883ms step_avg:58.00ms
step:430/2330 train_time:24943ms step_avg:58.01ms
step:431/2330 train_time:24999ms step_avg:58.00ms
step:432/2330 train_time:25059ms step_avg:58.01ms
step:433/2330 train_time:25116ms step_avg:58.01ms
step:434/2330 train_time:25175ms step_avg:58.01ms
step:435/2330 train_time:25231ms step_avg:58.00ms
step:436/2330 train_time:25290ms step_avg:58.00ms
step:437/2330 train_time:25347ms step_avg:58.00ms
step:438/2330 train_time:25407ms step_avg:58.01ms
step:439/2330 train_time:25464ms step_avg:58.00ms
step:440/2330 train_time:25523ms step_avg:58.01ms
step:441/2330 train_time:25580ms step_avg:58.00ms
step:442/2330 train_time:25638ms step_avg:58.01ms
step:443/2330 train_time:25695ms step_avg:58.00ms
step:444/2330 train_time:25754ms step_avg:58.00ms
step:445/2330 train_time:25811ms step_avg:58.00ms
step:446/2330 train_time:25870ms step_avg:58.01ms
step:447/2330 train_time:25926ms step_avg:58.00ms
step:448/2330 train_time:25987ms step_avg:58.01ms
step:449/2330 train_time:26043ms step_avg:58.00ms
step:450/2330 train_time:26103ms step_avg:58.01ms
step:451/2330 train_time:26161ms step_avg:58.01ms
step:452/2330 train_time:26220ms step_avg:58.01ms
step:453/2330 train_time:26275ms step_avg:58.00ms
step:454/2330 train_time:26334ms step_avg:58.00ms
step:455/2330 train_time:26391ms step_avg:58.00ms
step:456/2330 train_time:26451ms step_avg:58.01ms
step:457/2330 train_time:26507ms step_avg:58.00ms
step:458/2330 train_time:26567ms step_avg:58.01ms
step:459/2330 train_time:26623ms step_avg:58.00ms
step:460/2330 train_time:26683ms step_avg:58.01ms
step:461/2330 train_time:26740ms step_avg:58.00ms
step:462/2330 train_time:26800ms step_avg:58.01ms
step:463/2330 train_time:26856ms step_avg:58.01ms
step:464/2330 train_time:26915ms step_avg:58.01ms
step:465/2330 train_time:26972ms step_avg:58.00ms
step:466/2330 train_time:27032ms step_avg:58.01ms
step:467/2330 train_time:27088ms step_avg:58.01ms
step:468/2330 train_time:27148ms step_avg:58.01ms
step:469/2330 train_time:27205ms step_avg:58.01ms
step:470/2330 train_time:27264ms step_avg:58.01ms
step:471/2330 train_time:27322ms step_avg:58.01ms
step:472/2330 train_time:27381ms step_avg:58.01ms
step:473/2330 train_time:27438ms step_avg:58.01ms
step:474/2330 train_time:27497ms step_avg:58.01ms
step:475/2330 train_time:27554ms step_avg:58.01ms
step:476/2330 train_time:27612ms step_avg:58.01ms
step:477/2330 train_time:27669ms step_avg:58.01ms
step:478/2330 train_time:27728ms step_avg:58.01ms
step:479/2330 train_time:27785ms step_avg:58.01ms
step:480/2330 train_time:27844ms step_avg:58.01ms
step:481/2330 train_time:27903ms step_avg:58.01ms
step:482/2330 train_time:27962ms step_avg:58.01ms
step:483/2330 train_time:28019ms step_avg:58.01ms
step:484/2330 train_time:28078ms step_avg:58.01ms
step:485/2330 train_time:28135ms step_avg:58.01ms
step:486/2330 train_time:28195ms step_avg:58.01ms
step:487/2330 train_time:28251ms step_avg:58.01ms
step:488/2330 train_time:28312ms step_avg:58.02ms
step:489/2330 train_time:28368ms step_avg:58.01ms
step:490/2330 train_time:28429ms step_avg:58.02ms
step:491/2330 train_time:28485ms step_avg:58.01ms
step:492/2330 train_time:28545ms step_avg:58.02ms
step:493/2330 train_time:28601ms step_avg:58.01ms
step:494/2330 train_time:28662ms step_avg:58.02ms
step:495/2330 train_time:28718ms step_avg:58.02ms
step:496/2330 train_time:28777ms step_avg:58.02ms
step:497/2330 train_time:28834ms step_avg:58.02ms
step:498/2330 train_time:28892ms step_avg:58.02ms
step:499/2330 train_time:28949ms step_avg:58.01ms
step:500/2330 train_time:29008ms step_avg:58.02ms
step:500/2330 val_loss:4.5602 train_time:29089ms step_avg:58.18ms
step:501/2330 train_time:29108ms step_avg:58.10ms
step:502/2330 train_time:29128ms step_avg:58.02ms
step:503/2330 train_time:29183ms step_avg:58.02ms
step:504/2330 train_time:29249ms step_avg:58.03ms
step:505/2330 train_time:29305ms step_avg:58.03ms
step:506/2330 train_time:29364ms step_avg:58.03ms
step:507/2330 train_time:29421ms step_avg:58.03ms
step:508/2330 train_time:29480ms step_avg:58.03ms
step:509/2330 train_time:29537ms step_avg:58.03ms
step:510/2330 train_time:29595ms step_avg:58.03ms
step:511/2330 train_time:29651ms step_avg:58.03ms
step:512/2330 train_time:29710ms step_avg:58.03ms
step:513/2330 train_time:29766ms step_avg:58.02ms
step:514/2330 train_time:29825ms step_avg:58.03ms
step:515/2330 train_time:29881ms step_avg:58.02ms
step:516/2330 train_time:29941ms step_avg:58.02ms
step:517/2330 train_time:29997ms step_avg:58.02ms
step:518/2330 train_time:30057ms step_avg:58.02ms
step:519/2330 train_time:30114ms step_avg:58.02ms
step:520/2330 train_time:30174ms step_avg:58.03ms
step:521/2330 train_time:30231ms step_avg:58.03ms
step:522/2330 train_time:30293ms step_avg:58.03ms
step:523/2330 train_time:30349ms step_avg:58.03ms
step:524/2330 train_time:30410ms step_avg:58.03ms
step:525/2330 train_time:30466ms step_avg:58.03ms
step:526/2330 train_time:30525ms step_avg:58.03ms
step:527/2330 train_time:30582ms step_avg:58.03ms
step:528/2330 train_time:30641ms step_avg:58.03ms
step:529/2330 train_time:30697ms step_avg:58.03ms
step:530/2330 train_time:30756ms step_avg:58.03ms
step:531/2330 train_time:30812ms step_avg:58.03ms
step:532/2330 train_time:30871ms step_avg:58.03ms
step:533/2330 train_time:30927ms step_avg:58.02ms
step:534/2330 train_time:30986ms step_avg:58.03ms
step:535/2330 train_time:31044ms step_avg:58.03ms
step:536/2330 train_time:31103ms step_avg:58.03ms
step:537/2330 train_time:31161ms step_avg:58.03ms
step:538/2330 train_time:31221ms step_avg:58.03ms
step:539/2330 train_time:31278ms step_avg:58.03ms
step:540/2330 train_time:31339ms step_avg:58.04ms
step:541/2330 train_time:31396ms step_avg:58.03ms
step:542/2330 train_time:31457ms step_avg:58.04ms
step:543/2330 train_time:31513ms step_avg:58.03ms
step:544/2330 train_time:31573ms step_avg:58.04ms
step:545/2330 train_time:31629ms step_avg:58.04ms
step:546/2330 train_time:31688ms step_avg:58.04ms
step:547/2330 train_time:31745ms step_avg:58.03ms
step:548/2330 train_time:31804ms step_avg:58.04ms
step:549/2330 train_time:31861ms step_avg:58.03ms
step:550/2330 train_time:31919ms step_avg:58.04ms
step:551/2330 train_time:31975ms step_avg:58.03ms
step:552/2330 train_time:32036ms step_avg:58.04ms
step:553/2330 train_time:32092ms step_avg:58.03ms
step:554/2330 train_time:32152ms step_avg:58.04ms
step:555/2330 train_time:32209ms step_avg:58.03ms
step:556/2330 train_time:32270ms step_avg:58.04ms
step:557/2330 train_time:32327ms step_avg:58.04ms
step:558/2330 train_time:32387ms step_avg:58.04ms
step:559/2330 train_time:32443ms step_avg:58.04ms
step:560/2330 train_time:32503ms step_avg:58.04ms
step:561/2330 train_time:32560ms step_avg:58.04ms
step:562/2330 train_time:32619ms step_avg:58.04ms
step:563/2330 train_time:32675ms step_avg:58.04ms
step:564/2330 train_time:32734ms step_avg:58.04ms
step:565/2330 train_time:32790ms step_avg:58.04ms
step:566/2330 train_time:32850ms step_avg:58.04ms
step:567/2330 train_time:32906ms step_avg:58.04ms
step:568/2330 train_time:32965ms step_avg:58.04ms
step:569/2330 train_time:33022ms step_avg:58.04ms
step:570/2330 train_time:33082ms step_avg:58.04ms
step:571/2330 train_time:33139ms step_avg:58.04ms
step:572/2330 train_time:33197ms step_avg:58.04ms
step:573/2330 train_time:33255ms step_avg:58.04ms
step:574/2330 train_time:33315ms step_avg:58.04ms
step:575/2330 train_time:33371ms step_avg:58.04ms
step:576/2330 train_time:33432ms step_avg:58.04ms
step:577/2330 train_time:33489ms step_avg:58.04ms
step:578/2330 train_time:33549ms step_avg:58.04ms
step:579/2330 train_time:33606ms step_avg:58.04ms
step:580/2330 train_time:33664ms step_avg:58.04ms
step:581/2330 train_time:33721ms step_avg:58.04ms
step:582/2330 train_time:33780ms step_avg:58.04ms
step:583/2330 train_time:33836ms step_avg:58.04ms
step:584/2330 train_time:33895ms step_avg:58.04ms
step:585/2330 train_time:33951ms step_avg:58.04ms
step:586/2330 train_time:34011ms step_avg:58.04ms
step:587/2330 train_time:34067ms step_avg:58.04ms
step:588/2330 train_time:34127ms step_avg:58.04ms
step:589/2330 train_time:34185ms step_avg:58.04ms
step:590/2330 train_time:34245ms step_avg:58.04ms
step:591/2330 train_time:34302ms step_avg:58.04ms
step:592/2330 train_time:34362ms step_avg:58.04ms
step:593/2330 train_time:34418ms step_avg:58.04ms
step:594/2330 train_time:34477ms step_avg:58.04ms
step:595/2330 train_time:34534ms step_avg:58.04ms
step:596/2330 train_time:34594ms step_avg:58.04ms
step:597/2330 train_time:34650ms step_avg:58.04ms
step:598/2330 train_time:34710ms step_avg:58.04ms
step:599/2330 train_time:34767ms step_avg:58.04ms
step:600/2330 train_time:34827ms step_avg:58.05ms
step:601/2330 train_time:34884ms step_avg:58.04ms
step:602/2330 train_time:34943ms step_avg:58.05ms
step:603/2330 train_time:35000ms step_avg:58.04ms
step:604/2330 train_time:35059ms step_avg:58.05ms
step:605/2330 train_time:35116ms step_avg:58.04ms
step:606/2330 train_time:35175ms step_avg:58.04ms
step:607/2330 train_time:35232ms step_avg:58.04ms
step:608/2330 train_time:35291ms step_avg:58.04ms
step:609/2330 train_time:35347ms step_avg:58.04ms
step:610/2330 train_time:35407ms step_avg:58.04ms
step:611/2330 train_time:35464ms step_avg:58.04ms
step:612/2330 train_time:35524ms step_avg:58.05ms
step:613/2330 train_time:35580ms step_avg:58.04ms
step:614/2330 train_time:35640ms step_avg:58.05ms
step:615/2330 train_time:35696ms step_avg:58.04ms
step:616/2330 train_time:35755ms step_avg:58.04ms
step:617/2330 train_time:35811ms step_avg:58.04ms
step:618/2330 train_time:35872ms step_avg:58.05ms
step:619/2330 train_time:35928ms step_avg:58.04ms
step:620/2330 train_time:35987ms step_avg:58.04ms
step:621/2330 train_time:36044ms step_avg:58.04ms
step:622/2330 train_time:36104ms step_avg:58.04ms
step:623/2330 train_time:36161ms step_avg:58.04ms
step:624/2330 train_time:36221ms step_avg:58.05ms
step:625/2330 train_time:36277ms step_avg:58.04ms
step:626/2330 train_time:36336ms step_avg:58.05ms
step:627/2330 train_time:36392ms step_avg:58.04ms
step:628/2330 train_time:36453ms step_avg:58.05ms
step:629/2330 train_time:36509ms step_avg:58.04ms
step:630/2330 train_time:36568ms step_avg:58.05ms
step:631/2330 train_time:36625ms step_avg:58.04ms
step:632/2330 train_time:36684ms step_avg:58.04ms
step:633/2330 train_time:36741ms step_avg:58.04ms
step:634/2330 train_time:36801ms step_avg:58.05ms
step:635/2330 train_time:36858ms step_avg:58.04ms
step:636/2330 train_time:36917ms step_avg:58.05ms
step:637/2330 train_time:36973ms step_avg:58.04ms
step:638/2330 train_time:37033ms step_avg:58.05ms
step:639/2330 train_time:37089ms step_avg:58.04ms
step:640/2330 train_time:37149ms step_avg:58.05ms
step:641/2330 train_time:37206ms step_avg:58.04ms
step:642/2330 train_time:37267ms step_avg:58.05ms
step:643/2330 train_time:37324ms step_avg:58.05ms
step:644/2330 train_time:37384ms step_avg:58.05ms
step:645/2330 train_time:37441ms step_avg:58.05ms
step:646/2330 train_time:37500ms step_avg:58.05ms
step:647/2330 train_time:37556ms step_avg:58.05ms
step:648/2330 train_time:37615ms step_avg:58.05ms
step:649/2330 train_time:37671ms step_avg:58.04ms
step:650/2330 train_time:37732ms step_avg:58.05ms
step:651/2330 train_time:37788ms step_avg:58.05ms
step:652/2330 train_time:37849ms step_avg:58.05ms
step:653/2330 train_time:37905ms step_avg:58.05ms
step:654/2330 train_time:37964ms step_avg:58.05ms
step:655/2330 train_time:38020ms step_avg:58.05ms
step:656/2330 train_time:38079ms step_avg:58.05ms
step:657/2330 train_time:38136ms step_avg:58.05ms
step:658/2330 train_time:38195ms step_avg:58.05ms
step:659/2330 train_time:38251ms step_avg:58.04ms
step:660/2330 train_time:38311ms step_avg:58.05ms
step:661/2330 train_time:38367ms step_avg:58.04ms
step:662/2330 train_time:38428ms step_avg:58.05ms
step:663/2330 train_time:38484ms step_avg:58.05ms
step:664/2330 train_time:38544ms step_avg:58.05ms
step:665/2330 train_time:38601ms step_avg:58.05ms
step:666/2330 train_time:38661ms step_avg:58.05ms
step:667/2330 train_time:38717ms step_avg:58.05ms
step:668/2330 train_time:38776ms step_avg:58.05ms
step:669/2330 train_time:38832ms step_avg:58.05ms
step:670/2330 train_time:38892ms step_avg:58.05ms
step:671/2330 train_time:38948ms step_avg:58.05ms
step:672/2330 train_time:39008ms step_avg:58.05ms
step:673/2330 train_time:39065ms step_avg:58.05ms
step:674/2330 train_time:39124ms step_avg:58.05ms
step:675/2330 train_time:39181ms step_avg:58.05ms
step:676/2330 train_time:39240ms step_avg:58.05ms
step:677/2330 train_time:39296ms step_avg:58.04ms
step:678/2330 train_time:39356ms step_avg:58.05ms
step:679/2330 train_time:39412ms step_avg:58.04ms
step:680/2330 train_time:39471ms step_avg:58.05ms
step:681/2330 train_time:39527ms step_avg:58.04ms
step:682/2330 train_time:39587ms step_avg:58.05ms
step:683/2330 train_time:39644ms step_avg:58.04ms
step:684/2330 train_time:39704ms step_avg:58.05ms
step:685/2330 train_time:39761ms step_avg:58.05ms
step:686/2330 train_time:39821ms step_avg:58.05ms
step:687/2330 train_time:39877ms step_avg:58.05ms
step:688/2330 train_time:39936ms step_avg:58.05ms
step:689/2330 train_time:39992ms step_avg:58.04ms
step:690/2330 train_time:40053ms step_avg:58.05ms
step:691/2330 train_time:40108ms step_avg:58.04ms
step:692/2330 train_time:40167ms step_avg:58.04ms
step:693/2330 train_time:40224ms step_avg:58.04ms
step:694/2330 train_time:40284ms step_avg:58.05ms
step:695/2330 train_time:40342ms step_avg:58.05ms
step:696/2330 train_time:40402ms step_avg:58.05ms
step:697/2330 train_time:40458ms step_avg:58.05ms
step:698/2330 train_time:40516ms step_avg:58.05ms
step:699/2330 train_time:40572ms step_avg:58.04ms
step:700/2330 train_time:40631ms step_avg:58.04ms
step:701/2330 train_time:40688ms step_avg:58.04ms
step:702/2330 train_time:40750ms step_avg:58.05ms
step:703/2330 train_time:40806ms step_avg:58.05ms
step:704/2330 train_time:40865ms step_avg:58.05ms
step:705/2330 train_time:40922ms step_avg:58.05ms
step:706/2330 train_time:40981ms step_avg:58.05ms
step:707/2330 train_time:41038ms step_avg:58.05ms
step:708/2330 train_time:41096ms step_avg:58.05ms
step:709/2330 train_time:41153ms step_avg:58.04ms
step:710/2330 train_time:41214ms step_avg:58.05ms
step:711/2330 train_time:41270ms step_avg:58.04ms
step:712/2330 train_time:41330ms step_avg:58.05ms
step:713/2330 train_time:41386ms step_avg:58.04ms
step:714/2330 train_time:41445ms step_avg:58.05ms
step:715/2330 train_time:41502ms step_avg:58.04ms
step:716/2330 train_time:41562ms step_avg:58.05ms
step:717/2330 train_time:41618ms step_avg:58.04ms
step:718/2330 train_time:41677ms step_avg:58.05ms
step:719/2330 train_time:41734ms step_avg:58.04ms
step:720/2330 train_time:41794ms step_avg:58.05ms
step:721/2330 train_time:41851ms step_avg:58.05ms
step:722/2330 train_time:41910ms step_avg:58.05ms
step:723/2330 train_time:41966ms step_avg:58.04ms
step:724/2330 train_time:42026ms step_avg:58.05ms
step:725/2330 train_time:42084ms step_avg:58.05ms
step:726/2330 train_time:42143ms step_avg:58.05ms
step:727/2330 train_time:42201ms step_avg:58.05ms
step:728/2330 train_time:42260ms step_avg:58.05ms
step:729/2330 train_time:42316ms step_avg:58.05ms
step:730/2330 train_time:42376ms step_avg:58.05ms
step:731/2330 train_time:42432ms step_avg:58.05ms
step:732/2330 train_time:42491ms step_avg:58.05ms
step:733/2330 train_time:42548ms step_avg:58.05ms
step:734/2330 train_time:42608ms step_avg:58.05ms
step:735/2330 train_time:42664ms step_avg:58.05ms
step:736/2330 train_time:42725ms step_avg:58.05ms
step:737/2330 train_time:42783ms step_avg:58.05ms
step:738/2330 train_time:42842ms step_avg:58.05ms
step:739/2330 train_time:42899ms step_avg:58.05ms
step:740/2330 train_time:42958ms step_avg:58.05ms
step:741/2330 train_time:43014ms step_avg:58.05ms
step:742/2330 train_time:43073ms step_avg:58.05ms
step:743/2330 train_time:43130ms step_avg:58.05ms
step:744/2330 train_time:43189ms step_avg:58.05ms
step:745/2330 train_time:43246ms step_avg:58.05ms
step:746/2330 train_time:43306ms step_avg:58.05ms
step:747/2330 train_time:43363ms step_avg:58.05ms
step:748/2330 train_time:43423ms step_avg:58.05ms
step:749/2330 train_time:43480ms step_avg:58.05ms
step:750/2330 train_time:43539ms step_avg:58.05ms
step:750/2330 val_loss:4.3022 train_time:43619ms step_avg:58.16ms
step:751/2330 train_time:43639ms step_avg:58.11ms
step:752/2330 train_time:43659ms step_avg:58.06ms
step:753/2330 train_time:43714ms step_avg:58.05ms
step:754/2330 train_time:43780ms step_avg:58.06ms
step:755/2330 train_time:43837ms step_avg:58.06ms
step:756/2330 train_time:43898ms step_avg:58.07ms
step:757/2330 train_time:43954ms step_avg:58.06ms
step:758/2330 train_time:44013ms step_avg:58.06ms
step:759/2330 train_time:44069ms step_avg:58.06ms
step:760/2330 train_time:44127ms step_avg:58.06ms
step:761/2330 train_time:44183ms step_avg:58.06ms
step:762/2330 train_time:44242ms step_avg:58.06ms
step:763/2330 train_time:44297ms step_avg:58.06ms
step:764/2330 train_time:44356ms step_avg:58.06ms
step:765/2330 train_time:44413ms step_avg:58.06ms
step:766/2330 train_time:44471ms step_avg:58.06ms
step:767/2330 train_time:44528ms step_avg:58.05ms
step:768/2330 train_time:44588ms step_avg:58.06ms
step:769/2330 train_time:44647ms step_avg:58.06ms
step:770/2330 train_time:44708ms step_avg:58.06ms
step:771/2330 train_time:44768ms step_avg:58.06ms
step:772/2330 train_time:44829ms step_avg:58.07ms
step:773/2330 train_time:44888ms step_avg:58.07ms
step:774/2330 train_time:44948ms step_avg:58.07ms
step:775/2330 train_time:45006ms step_avg:58.07ms
step:776/2330 train_time:45066ms step_avg:58.07ms
step:777/2330 train_time:45123ms step_avg:58.07ms
step:778/2330 train_time:45182ms step_avg:58.07ms
step:779/2330 train_time:45239ms step_avg:58.07ms
step:780/2330 train_time:45298ms step_avg:58.07ms
step:781/2330 train_time:45355ms step_avg:58.07ms
step:782/2330 train_time:45414ms step_avg:58.07ms
step:783/2330 train_time:45471ms step_avg:58.07ms
step:784/2330 train_time:45531ms step_avg:58.08ms
step:785/2330 train_time:45588ms step_avg:58.07ms
step:786/2330 train_time:45649ms step_avg:58.08ms
step:787/2330 train_time:45708ms step_avg:58.08ms
step:788/2330 train_time:45769ms step_avg:58.08ms
step:789/2330 train_time:45827ms step_avg:58.08ms
step:790/2330 train_time:45887ms step_avg:58.08ms
step:791/2330 train_time:45944ms step_avg:58.08ms
step:792/2330 train_time:46005ms step_avg:58.09ms
step:793/2330 train_time:46063ms step_avg:58.09ms
step:794/2330 train_time:46121ms step_avg:58.09ms
step:795/2330 train_time:46179ms step_avg:58.09ms
step:796/2330 train_time:46238ms step_avg:58.09ms
step:797/2330 train_time:46296ms step_avg:58.09ms
step:798/2330 train_time:46355ms step_avg:58.09ms
step:799/2330 train_time:46411ms step_avg:58.09ms
step:800/2330 train_time:46471ms step_avg:58.09ms
step:801/2330 train_time:46527ms step_avg:58.09ms
step:802/2330 train_time:46588ms step_avg:58.09ms
step:803/2330 train_time:46645ms step_avg:58.09ms
step:804/2330 train_time:46705ms step_avg:58.09ms
step:805/2330 train_time:46763ms step_avg:58.09ms
step:806/2330 train_time:46823ms step_avg:58.09ms
step:807/2330 train_time:46881ms step_avg:58.09ms
step:808/2330 train_time:46941ms step_avg:58.10ms
step:809/2330 train_time:46999ms step_avg:58.09ms
step:810/2330 train_time:47059ms step_avg:58.10ms
step:811/2330 train_time:47115ms step_avg:58.10ms
step:812/2330 train_time:47176ms step_avg:58.10ms
step:813/2330 train_time:47233ms step_avg:58.10ms
step:814/2330 train_time:47294ms step_avg:58.10ms
step:815/2330 train_time:47351ms step_avg:58.10ms
step:816/2330 train_time:47410ms step_avg:58.10ms
step:817/2330 train_time:47467ms step_avg:58.10ms
step:818/2330 train_time:47527ms step_avg:58.10ms
step:819/2330 train_time:47584ms step_avg:58.10ms
step:820/2330 train_time:47644ms step_avg:58.10ms
step:821/2330 train_time:47702ms step_avg:58.10ms
step:822/2330 train_time:47762ms step_avg:58.11ms
step:823/2330 train_time:47819ms step_avg:58.10ms
step:824/2330 train_time:47880ms step_avg:58.11ms
step:825/2330 train_time:47937ms step_avg:58.11ms
step:826/2330 train_time:47998ms step_avg:58.11ms
step:827/2330 train_time:48056ms step_avg:58.11ms
step:828/2330 train_time:48116ms step_avg:58.11ms
step:829/2330 train_time:48173ms step_avg:58.11ms
step:830/2330 train_time:48234ms step_avg:58.11ms
step:831/2330 train_time:48291ms step_avg:58.11ms
step:832/2330 train_time:48350ms step_avg:58.11ms
step:833/2330 train_time:48406ms step_avg:58.11ms
step:834/2330 train_time:48467ms step_avg:58.11ms
step:835/2330 train_time:48524ms step_avg:58.11ms
step:836/2330 train_time:48585ms step_avg:58.12ms
step:837/2330 train_time:48642ms step_avg:58.11ms
step:838/2330 train_time:48702ms step_avg:58.12ms
step:839/2330 train_time:48759ms step_avg:58.12ms
step:840/2330 train_time:48819ms step_avg:58.12ms
step:841/2330 train_time:48877ms step_avg:58.12ms
step:842/2330 train_time:48937ms step_avg:58.12ms
step:843/2330 train_time:48994ms step_avg:58.12ms
step:844/2330 train_time:49055ms step_avg:58.12ms
step:845/2330 train_time:49112ms step_avg:58.12ms
step:846/2330 train_time:49173ms step_avg:58.12ms
step:847/2330 train_time:49230ms step_avg:58.12ms
step:848/2330 train_time:49290ms step_avg:58.12ms
step:849/2330 train_time:49347ms step_avg:58.12ms
step:850/2330 train_time:49406ms step_avg:58.12ms
step:851/2330 train_time:49462ms step_avg:58.12ms
step:852/2330 train_time:49524ms step_avg:58.13ms
step:853/2330 train_time:49580ms step_avg:58.12ms
step:854/2330 train_time:49640ms step_avg:58.13ms
step:855/2330 train_time:49698ms step_avg:58.13ms
step:856/2330 train_time:49758ms step_avg:58.13ms
step:857/2330 train_time:49815ms step_avg:58.13ms
step:858/2330 train_time:49876ms step_avg:58.13ms
step:859/2330 train_time:49933ms step_avg:58.13ms
step:860/2330 train_time:49994ms step_avg:58.13ms
step:861/2330 train_time:50051ms step_avg:58.13ms
step:862/2330 train_time:50111ms step_avg:58.13ms
step:863/2330 train_time:50169ms step_avg:58.13ms
step:864/2330 train_time:50229ms step_avg:58.13ms
step:865/2330 train_time:50286ms step_avg:58.13ms
step:866/2330 train_time:50346ms step_avg:58.14ms
step:867/2330 train_time:50403ms step_avg:58.14ms
step:868/2330 train_time:50462ms step_avg:58.14ms
step:869/2330 train_time:50519ms step_avg:58.13ms
step:870/2330 train_time:50579ms step_avg:58.14ms
step:871/2330 train_time:50637ms step_avg:58.14ms
step:872/2330 train_time:50697ms step_avg:58.14ms
step:873/2330 train_time:50754ms step_avg:58.14ms
step:874/2330 train_time:50814ms step_avg:58.14ms
step:875/2330 train_time:50872ms step_avg:58.14ms
step:876/2330 train_time:50932ms step_avg:58.14ms
step:877/2330 train_time:50990ms step_avg:58.14ms
step:878/2330 train_time:51050ms step_avg:58.14ms
step:879/2330 train_time:51108ms step_avg:58.14ms
step:880/2330 train_time:51168ms step_avg:58.14ms
step:881/2330 train_time:51225ms step_avg:58.14ms
step:882/2330 train_time:51285ms step_avg:58.15ms
step:883/2330 train_time:51342ms step_avg:58.14ms
step:884/2330 train_time:51402ms step_avg:58.15ms
step:885/2330 train_time:51459ms step_avg:58.15ms
step:886/2330 train_time:51519ms step_avg:58.15ms
step:887/2330 train_time:51577ms step_avg:58.15ms
step:888/2330 train_time:51636ms step_avg:58.15ms
step:889/2330 train_time:51693ms step_avg:58.15ms
step:890/2330 train_time:51754ms step_avg:58.15ms
step:891/2330 train_time:51811ms step_avg:58.15ms
step:892/2330 train_time:51871ms step_avg:58.15ms
step:893/2330 train_time:51928ms step_avg:58.15ms
step:894/2330 train_time:51989ms step_avg:58.15ms
step:895/2330 train_time:52046ms step_avg:58.15ms
step:896/2330 train_time:52106ms step_avg:58.15ms
step:897/2330 train_time:52163ms step_avg:58.15ms
step:898/2330 train_time:52223ms step_avg:58.15ms
step:899/2330 train_time:52280ms step_avg:58.15ms
step:900/2330 train_time:52340ms step_avg:58.16ms
step:901/2330 train_time:52398ms step_avg:58.16ms
step:902/2330 train_time:52458ms step_avg:58.16ms
step:903/2330 train_time:52514ms step_avg:58.16ms
step:904/2330 train_time:52575ms step_avg:58.16ms
step:905/2330 train_time:52632ms step_avg:58.16ms
step:906/2330 train_time:52693ms step_avg:58.16ms
step:907/2330 train_time:52750ms step_avg:58.16ms
step:908/2330 train_time:52810ms step_avg:58.16ms
step:909/2330 train_time:52867ms step_avg:58.16ms
step:910/2330 train_time:52926ms step_avg:58.16ms
step:911/2330 train_time:52984ms step_avg:58.16ms
step:912/2330 train_time:53043ms step_avg:58.16ms
step:913/2330 train_time:53100ms step_avg:58.16ms
step:914/2330 train_time:53162ms step_avg:58.16ms
step:915/2330 train_time:53218ms step_avg:58.16ms
step:916/2330 train_time:53279ms step_avg:58.16ms
step:917/2330 train_time:53336ms step_avg:58.16ms
step:918/2330 train_time:53396ms step_avg:58.17ms
step:919/2330 train_time:53453ms step_avg:58.16ms
step:920/2330 train_time:53514ms step_avg:58.17ms
step:921/2330 train_time:53571ms step_avg:58.17ms
step:922/2330 train_time:53631ms step_avg:58.17ms
step:923/2330 train_time:53688ms step_avg:58.17ms
step:924/2330 train_time:53747ms step_avg:58.17ms
step:925/2330 train_time:53804ms step_avg:58.17ms
step:926/2330 train_time:53864ms step_avg:58.17ms
step:927/2330 train_time:53921ms step_avg:58.17ms
step:928/2330 train_time:53982ms step_avg:58.17ms
step:929/2330 train_time:54039ms step_avg:58.17ms
step:930/2330 train_time:54099ms step_avg:58.17ms
step:931/2330 train_time:54156ms step_avg:58.17ms
step:932/2330 train_time:54216ms step_avg:58.17ms
step:933/2330 train_time:54273ms step_avg:58.17ms
step:934/2330 train_time:54334ms step_avg:58.17ms
step:935/2330 train_time:54391ms step_avg:58.17ms
step:936/2330 train_time:54451ms step_avg:58.17ms
step:937/2330 train_time:54508ms step_avg:58.17ms
step:938/2330 train_time:54567ms step_avg:58.17ms
step:939/2330 train_time:54624ms step_avg:58.17ms
step:940/2330 train_time:54686ms step_avg:58.18ms
step:941/2330 train_time:54743ms step_avg:58.18ms
step:942/2330 train_time:54804ms step_avg:58.18ms
step:943/2330 train_time:54860ms step_avg:58.18ms
step:944/2330 train_time:54921ms step_avg:58.18ms
step:945/2330 train_time:54979ms step_avg:58.18ms
step:946/2330 train_time:55039ms step_avg:58.18ms
step:947/2330 train_time:55096ms step_avg:58.18ms
step:948/2330 train_time:55156ms step_avg:58.18ms
step:949/2330 train_time:55213ms step_avg:58.18ms
step:950/2330 train_time:55273ms step_avg:58.18ms
step:951/2330 train_time:55330ms step_avg:58.18ms
step:952/2330 train_time:55390ms step_avg:58.18ms
step:953/2330 train_time:55448ms step_avg:58.18ms
step:954/2330 train_time:55507ms step_avg:58.18ms
step:955/2330 train_time:55565ms step_avg:58.18ms
step:956/2330 train_time:55624ms step_avg:58.18ms
step:957/2330 train_time:55682ms step_avg:58.18ms
step:958/2330 train_time:55742ms step_avg:58.19ms
step:959/2330 train_time:55800ms step_avg:58.19ms
step:960/2330 train_time:55859ms step_avg:58.19ms
step:961/2330 train_time:55917ms step_avg:58.19ms
step:962/2330 train_time:55976ms step_avg:58.19ms
step:963/2330 train_time:56033ms step_avg:58.19ms
step:964/2330 train_time:56093ms step_avg:58.19ms
step:965/2330 train_time:56150ms step_avg:58.19ms
step:966/2330 train_time:56210ms step_avg:58.19ms
step:967/2330 train_time:56267ms step_avg:58.19ms
step:968/2330 train_time:56329ms step_avg:58.19ms
step:969/2330 train_time:56386ms step_avg:58.19ms
step:970/2330 train_time:56446ms step_avg:58.19ms
step:971/2330 train_time:56503ms step_avg:58.19ms
step:972/2330 train_time:56563ms step_avg:58.19ms
step:973/2330 train_time:56620ms step_avg:58.19ms
step:974/2330 train_time:56680ms step_avg:58.19ms
step:975/2330 train_time:56737ms step_avg:58.19ms
step:976/2330 train_time:56797ms step_avg:58.19ms
step:977/2330 train_time:56854ms step_avg:58.19ms
step:978/2330 train_time:56914ms step_avg:58.19ms
step:979/2330 train_time:56972ms step_avg:58.19ms
step:980/2330 train_time:57031ms step_avg:58.20ms
step:981/2330 train_time:57088ms step_avg:58.19ms
step:982/2330 train_time:57149ms step_avg:58.20ms
step:983/2330 train_time:57205ms step_avg:58.19ms
step:984/2330 train_time:57266ms step_avg:58.20ms
step:985/2330 train_time:57322ms step_avg:58.20ms
step:986/2330 train_time:57383ms step_avg:58.20ms
step:987/2330 train_time:57439ms step_avg:58.20ms
step:988/2330 train_time:57500ms step_avg:58.20ms
step:989/2330 train_time:57557ms step_avg:58.20ms
step:990/2330 train_time:57618ms step_avg:58.20ms
step:991/2330 train_time:57675ms step_avg:58.20ms
step:992/2330 train_time:57734ms step_avg:58.20ms
step:993/2330 train_time:57792ms step_avg:58.20ms
step:994/2330 train_time:57852ms step_avg:58.20ms
step:995/2330 train_time:57909ms step_avg:58.20ms
step:996/2330 train_time:57969ms step_avg:58.20ms
step:997/2330 train_time:58026ms step_avg:58.20ms
step:998/2330 train_time:58087ms step_avg:58.20ms
step:999/2330 train_time:58144ms step_avg:58.20ms
step:1000/2330 train_time:58204ms step_avg:58.20ms
step:1000/2330 val_loss:4.1382 train_time:58285ms step_avg:58.28ms
step:1001/2330 train_time:58304ms step_avg:58.25ms
step:1002/2330 train_time:58325ms step_avg:58.21ms
step:1003/2330 train_time:58378ms step_avg:58.20ms
step:1004/2330 train_time:58445ms step_avg:58.21ms
step:1005/2330 train_time:58502ms step_avg:58.21ms
step:1006/2330 train_time:58569ms step_avg:58.22ms
step:1007/2330 train_time:58626ms step_avg:58.22ms
step:1008/2330 train_time:58685ms step_avg:58.22ms
step:1009/2330 train_time:58742ms step_avg:58.22ms
step:1010/2330 train_time:58801ms step_avg:58.22ms
step:1011/2330 train_time:58857ms step_avg:58.22ms
step:1012/2330 train_time:58917ms step_avg:58.22ms
step:1013/2330 train_time:58973ms step_avg:58.22ms
step:1014/2330 train_time:59032ms step_avg:58.22ms
step:1015/2330 train_time:59089ms step_avg:58.22ms
step:1016/2330 train_time:59147ms step_avg:58.22ms
step:1017/2330 train_time:59205ms step_avg:58.22ms
step:1018/2330 train_time:59268ms step_avg:58.22ms
step:1019/2330 train_time:59326ms step_avg:58.22ms
step:1020/2330 train_time:59389ms step_avg:58.22ms
step:1021/2330 train_time:59448ms step_avg:58.22ms
step:1022/2330 train_time:59508ms step_avg:58.23ms
step:1023/2330 train_time:59565ms step_avg:58.23ms
step:1024/2330 train_time:59626ms step_avg:58.23ms
step:1025/2330 train_time:59683ms step_avg:58.23ms
step:1026/2330 train_time:59743ms step_avg:58.23ms
step:1027/2330 train_time:59799ms step_avg:58.23ms
step:1028/2330 train_time:59859ms step_avg:58.23ms
step:1029/2330 train_time:59916ms step_avg:58.23ms
step:1030/2330 train_time:59975ms step_avg:58.23ms
step:1031/2330 train_time:60032ms step_avg:58.23ms
step:1032/2330 train_time:60091ms step_avg:58.23ms
step:1033/2330 train_time:60148ms step_avg:58.23ms
step:1034/2330 train_time:60209ms step_avg:58.23ms
step:1035/2330 train_time:60267ms step_avg:58.23ms
step:1036/2330 train_time:60327ms step_avg:58.23ms
step:1037/2330 train_time:60386ms step_avg:58.23ms
step:1038/2330 train_time:60447ms step_avg:58.23ms
step:1039/2330 train_time:60505ms step_avg:58.23ms
step:1040/2330 train_time:60565ms step_avg:58.24ms
step:1041/2330 train_time:60622ms step_avg:58.23ms
step:1042/2330 train_time:60682ms step_avg:58.24ms
step:1043/2330 train_time:60739ms step_avg:58.24ms
step:1044/2330 train_time:60800ms step_avg:58.24ms
step:1045/2330 train_time:60857ms step_avg:58.24ms
step:1046/2330 train_time:60916ms step_avg:58.24ms
step:1047/2330 train_time:60974ms step_avg:58.24ms
step:1048/2330 train_time:61034ms step_avg:58.24ms
step:1049/2330 train_time:61092ms step_avg:58.24ms
step:1050/2330 train_time:61152ms step_avg:58.24ms
step:1051/2330 train_time:61210ms step_avg:58.24ms
step:1052/2330 train_time:61269ms step_avg:58.24ms
step:1053/2330 train_time:61327ms step_avg:58.24ms
step:1054/2330 train_time:61387ms step_avg:58.24ms
step:1055/2330 train_time:61445ms step_avg:58.24ms
step:1056/2330 train_time:61505ms step_avg:58.24ms
step:1057/2330 train_time:61562ms step_avg:58.24ms
step:1058/2330 train_time:61623ms step_avg:58.25ms
step:1059/2330 train_time:61680ms step_avg:58.24ms
step:1060/2330 train_time:61740ms step_avg:58.25ms
step:1061/2330 train_time:61797ms step_avg:58.24ms
step:1062/2330 train_time:61858ms step_avg:58.25ms
step:1063/2330 train_time:61914ms step_avg:58.25ms
step:1064/2330 train_time:61975ms step_avg:58.25ms
step:1065/2330 train_time:62032ms step_avg:58.25ms
step:1066/2330 train_time:62091ms step_avg:58.25ms
step:1067/2330 train_time:62148ms step_avg:58.25ms
step:1068/2330 train_time:62209ms step_avg:58.25ms
step:1069/2330 train_time:62266ms step_avg:58.25ms
step:1070/2330 train_time:62327ms step_avg:58.25ms
step:1071/2330 train_time:62385ms step_avg:58.25ms
step:1072/2330 train_time:62445ms step_avg:58.25ms
step:1073/2330 train_time:62502ms step_avg:58.25ms
step:1074/2330 train_time:62562ms step_avg:58.25ms
step:1075/2330 train_time:62620ms step_avg:58.25ms
step:1076/2330 train_time:62681ms step_avg:58.25ms
step:1077/2330 train_time:62738ms step_avg:58.25ms
step:1078/2330 train_time:62799ms step_avg:58.25ms
step:1079/2330 train_time:62856ms step_avg:58.25ms
step:1080/2330 train_time:62916ms step_avg:58.26ms
step:1081/2330 train_time:62972ms step_avg:58.25ms
step:1082/2330 train_time:63033ms step_avg:58.26ms
step:1083/2330 train_time:63090ms step_avg:58.26ms
step:1084/2330 train_time:63151ms step_avg:58.26ms
step:1085/2330 train_time:63208ms step_avg:58.26ms
step:1086/2330 train_time:63267ms step_avg:58.26ms
step:1087/2330 train_time:63325ms step_avg:58.26ms
step:1088/2330 train_time:63385ms step_avg:58.26ms
step:1089/2330 train_time:63442ms step_avg:58.26ms
step:1090/2330 train_time:63503ms step_avg:58.26ms
step:1091/2330 train_time:63560ms step_avg:58.26ms
step:1092/2330 train_time:63621ms step_avg:58.26ms
step:1093/2330 train_time:63678ms step_avg:58.26ms
step:1094/2330 train_time:63739ms step_avg:58.26ms
step:1095/2330 train_time:63796ms step_avg:58.26ms
step:1096/2330 train_time:63856ms step_avg:58.26ms
step:1097/2330 train_time:63913ms step_avg:58.26ms
step:1098/2330 train_time:63975ms step_avg:58.27ms
step:1099/2330 train_time:64033ms step_avg:58.26ms
step:1100/2330 train_time:64093ms step_avg:58.27ms
step:1101/2330 train_time:64150ms step_avg:58.26ms
step:1102/2330 train_time:64209ms step_avg:58.27ms
step:1103/2330 train_time:64267ms step_avg:58.27ms
step:1104/2330 train_time:64327ms step_avg:58.27ms
step:1105/2330 train_time:64384ms step_avg:58.27ms
step:1106/2330 train_time:64445ms step_avg:58.27ms
step:1107/2330 train_time:64502ms step_avg:58.27ms
step:1108/2330 train_time:64563ms step_avg:58.27ms
step:1109/2330 train_time:64620ms step_avg:58.27ms
step:1110/2330 train_time:64681ms step_avg:58.27ms
step:1111/2330 train_time:64738ms step_avg:58.27ms
step:1112/2330 train_time:64798ms step_avg:58.27ms
step:1113/2330 train_time:64855ms step_avg:58.27ms
step:1114/2330 train_time:64916ms step_avg:58.27ms
step:1115/2330 train_time:64973ms step_avg:58.27ms
step:1116/2330 train_time:65034ms step_avg:58.27ms
step:1117/2330 train_time:65091ms step_avg:58.27ms
step:1118/2330 train_time:65151ms step_avg:58.27ms
step:1119/2330 train_time:65209ms step_avg:58.27ms
step:1120/2330 train_time:65269ms step_avg:58.28ms
step:1121/2330 train_time:65326ms step_avg:58.27ms
step:1122/2330 train_time:65386ms step_avg:58.28ms
step:1123/2330 train_time:65444ms step_avg:58.28ms
step:1124/2330 train_time:65504ms step_avg:58.28ms
step:1125/2330 train_time:65561ms step_avg:58.28ms
step:1126/2330 train_time:65623ms step_avg:58.28ms
step:1127/2330 train_time:65680ms step_avg:58.28ms
step:1128/2330 train_time:65742ms step_avg:58.28ms
step:1129/2330 train_time:65798ms step_avg:58.28ms
step:1130/2330 train_time:65858ms step_avg:58.28ms
step:1131/2330 train_time:65915ms step_avg:58.28ms
step:1132/2330 train_time:65976ms step_avg:58.28ms
step:1133/2330 train_time:66034ms step_avg:58.28ms
step:1134/2330 train_time:66094ms step_avg:58.28ms
step:1135/2330 train_time:66153ms step_avg:58.28ms
step:1136/2330 train_time:66213ms step_avg:58.29ms
step:1137/2330 train_time:66271ms step_avg:58.29ms
step:1138/2330 train_time:66331ms step_avg:58.29ms
step:1139/2330 train_time:66389ms step_avg:58.29ms
step:1140/2330 train_time:66449ms step_avg:58.29ms
step:1141/2330 train_time:66507ms step_avg:58.29ms
step:1142/2330 train_time:66566ms step_avg:58.29ms
step:1143/2330 train_time:66624ms step_avg:58.29ms
step:1144/2330 train_time:66686ms step_avg:58.29ms
step:1145/2330 train_time:66742ms step_avg:58.29ms
step:1146/2330 train_time:66804ms step_avg:58.29ms
step:1147/2330 train_time:66860ms step_avg:58.29ms
step:1148/2330 train_time:66922ms step_avg:58.29ms
step:1149/2330 train_time:66979ms step_avg:58.29ms
step:1150/2330 train_time:67040ms step_avg:58.30ms
step:1151/2330 train_time:67097ms step_avg:58.29ms
step:1152/2330 train_time:67158ms step_avg:58.30ms
step:1153/2330 train_time:67216ms step_avg:58.30ms
step:1154/2330 train_time:67277ms step_avg:58.30ms
step:1155/2330 train_time:67335ms step_avg:58.30ms
step:1156/2330 train_time:67396ms step_avg:58.30ms
step:1157/2330 train_time:67455ms step_avg:58.30ms
step:1158/2330 train_time:67515ms step_avg:58.30ms
step:1159/2330 train_time:67573ms step_avg:58.30ms
step:1160/2330 train_time:67634ms step_avg:58.30ms
step:1161/2330 train_time:67691ms step_avg:58.30ms
step:1162/2330 train_time:67751ms step_avg:58.31ms
step:1163/2330 train_time:67807ms step_avg:58.30ms
step:1164/2330 train_time:67869ms step_avg:58.31ms
step:1165/2330 train_time:67926ms step_avg:58.31ms
step:1166/2330 train_time:67988ms step_avg:58.31ms
step:1167/2330 train_time:68045ms step_avg:58.31ms
step:1168/2330 train_time:68106ms step_avg:58.31ms
step:1169/2330 train_time:68162ms step_avg:58.31ms
step:1170/2330 train_time:68224ms step_avg:58.31ms
step:1171/2330 train_time:68280ms step_avg:58.31ms
step:1172/2330 train_time:68343ms step_avg:58.31ms
step:1173/2330 train_time:68400ms step_avg:58.31ms
step:1174/2330 train_time:68461ms step_avg:58.31ms
step:1175/2330 train_time:68518ms step_avg:58.31ms
step:1176/2330 train_time:68578ms step_avg:58.31ms
step:1177/2330 train_time:68637ms step_avg:58.31ms
step:1178/2330 train_time:68696ms step_avg:58.32ms
step:1179/2330 train_time:68754ms step_avg:58.32ms
step:1180/2330 train_time:68814ms step_avg:58.32ms
step:1181/2330 train_time:68871ms step_avg:58.32ms
step:1182/2330 train_time:68931ms step_avg:58.32ms
step:1183/2330 train_time:68988ms step_avg:58.32ms
step:1184/2330 train_time:69048ms step_avg:58.32ms
step:1185/2330 train_time:69106ms step_avg:58.32ms
step:1186/2330 train_time:69166ms step_avg:58.32ms
step:1187/2330 train_time:69223ms step_avg:58.32ms
step:1188/2330 train_time:69284ms step_avg:58.32ms
step:1189/2330 train_time:69341ms step_avg:58.32ms
step:1190/2330 train_time:69402ms step_avg:58.32ms
step:1191/2330 train_time:69458ms step_avg:58.32ms
step:1192/2330 train_time:69520ms step_avg:58.32ms
step:1193/2330 train_time:69578ms step_avg:58.32ms
step:1194/2330 train_time:69639ms step_avg:58.32ms
step:1195/2330 train_time:69697ms step_avg:58.32ms
step:1196/2330 train_time:69757ms step_avg:58.33ms
step:1197/2330 train_time:69816ms step_avg:58.33ms
step:1198/2330 train_time:69875ms step_avg:58.33ms
step:1199/2330 train_time:69934ms step_avg:58.33ms
step:1200/2330 train_time:69994ms step_avg:58.33ms
step:1201/2330 train_time:70053ms step_avg:58.33ms
step:1202/2330 train_time:70113ms step_avg:58.33ms
step:1203/2330 train_time:70171ms step_avg:58.33ms
step:1204/2330 train_time:70231ms step_avg:58.33ms
step:1205/2330 train_time:70288ms step_avg:58.33ms
step:1206/2330 train_time:70348ms step_avg:58.33ms
step:1207/2330 train_time:70405ms step_avg:58.33ms
step:1208/2330 train_time:70467ms step_avg:58.33ms
step:1209/2330 train_time:70523ms step_avg:58.33ms
step:1210/2330 train_time:70585ms step_avg:58.33ms
step:1211/2330 train_time:70642ms step_avg:58.33ms
step:1212/2330 train_time:70704ms step_avg:58.34ms
step:1213/2330 train_time:70759ms step_avg:58.33ms
step:1214/2330 train_time:70821ms step_avg:58.34ms
step:1215/2330 train_time:70878ms step_avg:58.34ms
step:1216/2330 train_time:70941ms step_avg:58.34ms
step:1217/2330 train_time:70998ms step_avg:58.34ms
step:1218/2330 train_time:71059ms step_avg:58.34ms
step:1219/2330 train_time:71116ms step_avg:58.34ms
step:1220/2330 train_time:71177ms step_avg:58.34ms
step:1221/2330 train_time:71234ms step_avg:58.34ms
step:1222/2330 train_time:71296ms step_avg:58.34ms
step:1223/2330 train_time:71354ms step_avg:58.34ms
step:1224/2330 train_time:71414ms step_avg:58.34ms
step:1225/2330 train_time:71471ms step_avg:58.34ms
step:1226/2330 train_time:71531ms step_avg:58.34ms
step:1227/2330 train_time:71588ms step_avg:58.34ms
step:1228/2330 train_time:71648ms step_avg:58.35ms
step:1229/2330 train_time:71705ms step_avg:58.34ms
step:1230/2330 train_time:71766ms step_avg:58.35ms
step:1231/2330 train_time:71823ms step_avg:58.35ms
step:1232/2330 train_time:71884ms step_avg:58.35ms
step:1233/2330 train_time:71941ms step_avg:58.35ms
step:1234/2330 train_time:72002ms step_avg:58.35ms
step:1235/2330 train_time:72059ms step_avg:58.35ms
step:1236/2330 train_time:72120ms step_avg:58.35ms
step:1237/2330 train_time:72177ms step_avg:58.35ms
step:1238/2330 train_time:72239ms step_avg:58.35ms
step:1239/2330 train_time:72296ms step_avg:58.35ms
step:1240/2330 train_time:72356ms step_avg:58.35ms
step:1241/2330 train_time:72413ms step_avg:58.35ms
step:1242/2330 train_time:72474ms step_avg:58.35ms
step:1243/2330 train_time:72533ms step_avg:58.35ms
step:1244/2330 train_time:72593ms step_avg:58.35ms
step:1245/2330 train_time:72650ms step_avg:58.35ms
step:1246/2330 train_time:72709ms step_avg:58.35ms
step:1247/2330 train_time:72767ms step_avg:58.35ms
step:1248/2330 train_time:72827ms step_avg:58.36ms
step:1249/2330 train_time:72884ms step_avg:58.35ms
step:1250/2330 train_time:72946ms step_avg:58.36ms
step:1250/2330 val_loss:4.0388 train_time:73028ms step_avg:58.42ms
step:1251/2330 train_time:73047ms step_avg:58.39ms
step:1252/2330 train_time:73068ms step_avg:58.36ms
step:1253/2330 train_time:73123ms step_avg:58.36ms
step:1254/2330 train_time:73192ms step_avg:58.37ms
step:1255/2330 train_time:73251ms step_avg:58.37ms
step:1256/2330 train_time:73312ms step_avg:58.37ms
step:1257/2330 train_time:73370ms step_avg:58.37ms
step:1258/2330 train_time:73431ms step_avg:58.37ms
step:1259/2330 train_time:73488ms step_avg:58.37ms
step:1260/2330 train_time:73547ms step_avg:58.37ms
step:1261/2330 train_time:73604ms step_avg:58.37ms
step:1262/2330 train_time:73663ms step_avg:58.37ms
step:1263/2330 train_time:73720ms step_avg:58.37ms
step:1264/2330 train_time:73779ms step_avg:58.37ms
step:1265/2330 train_time:73836ms step_avg:58.37ms
step:1266/2330 train_time:73896ms step_avg:58.37ms
step:1267/2330 train_time:73952ms step_avg:58.37ms
step:1268/2330 train_time:74014ms step_avg:58.37ms
step:1269/2330 train_time:74071ms step_avg:58.37ms
step:1270/2330 train_time:74134ms step_avg:58.37ms
step:1271/2330 train_time:74192ms step_avg:58.37ms
step:1272/2330 train_time:74254ms step_avg:58.38ms
step:1273/2330 train_time:74311ms step_avg:58.37ms
step:1274/2330 train_time:74372ms step_avg:58.38ms
step:1275/2330 train_time:74429ms step_avg:58.38ms
step:1276/2330 train_time:74490ms step_avg:58.38ms
step:1277/2330 train_time:74547ms step_avg:58.38ms
step:1278/2330 train_time:74607ms step_avg:58.38ms
step:1279/2330 train_time:74664ms step_avg:58.38ms
step:1280/2330 train_time:74723ms step_avg:58.38ms
step:1281/2330 train_time:74781ms step_avg:58.38ms
step:1282/2330 train_time:74840ms step_avg:58.38ms
step:1283/2330 train_time:74897ms step_avg:58.38ms
step:1284/2330 train_time:74957ms step_avg:58.38ms
step:1285/2330 train_time:75014ms step_avg:58.38ms
step:1286/2330 train_time:75076ms step_avg:58.38ms
step:1287/2330 train_time:75133ms step_avg:58.38ms
step:1288/2330 train_time:75195ms step_avg:58.38ms
step:1289/2330 train_time:75253ms step_avg:58.38ms
step:1290/2330 train_time:75314ms step_avg:58.38ms
step:1291/2330 train_time:75372ms step_avg:58.38ms
step:1292/2330 train_time:75432ms step_avg:58.38ms
step:1293/2330 train_time:75489ms step_avg:58.38ms
step:1294/2330 train_time:75549ms step_avg:58.38ms
step:1295/2330 train_time:75606ms step_avg:58.38ms
step:1296/2330 train_time:75666ms step_avg:58.38ms
step:1297/2330 train_time:75723ms step_avg:58.38ms
step:1298/2330 train_time:75782ms step_avg:58.38ms
step:1299/2330 train_time:75839ms step_avg:58.38ms
step:1300/2330 train_time:75899ms step_avg:58.38ms
step:1301/2330 train_time:75956ms step_avg:58.38ms
step:1302/2330 train_time:76017ms step_avg:58.38ms
step:1303/2330 train_time:76074ms step_avg:58.38ms
step:1304/2330 train_time:76136ms step_avg:58.39ms
step:1305/2330 train_time:76193ms step_avg:58.39ms
step:1306/2330 train_time:76254ms step_avg:58.39ms
step:1307/2330 train_time:76312ms step_avg:58.39ms
step:1308/2330 train_time:76372ms step_avg:58.39ms
step:1309/2330 train_time:76430ms step_avg:58.39ms
step:1310/2330 train_time:76490ms step_avg:58.39ms
step:1311/2330 train_time:76547ms step_avg:58.39ms
step:1312/2330 train_time:76607ms step_avg:58.39ms
step:1313/2330 train_time:76665ms step_avg:58.39ms
step:1314/2330 train_time:76725ms step_avg:58.39ms
step:1315/2330 train_time:76784ms step_avg:58.39ms
step:1316/2330 train_time:76844ms step_avg:58.39ms
step:1317/2330 train_time:76901ms step_avg:58.39ms
step:1318/2330 train_time:76961ms step_avg:58.39ms
step:1319/2330 train_time:77019ms step_avg:58.39ms
step:1320/2330 train_time:77080ms step_avg:58.39ms
step:1321/2330 train_time:77137ms step_avg:58.39ms
step:1322/2330 train_time:77199ms step_avg:58.40ms
step:1323/2330 train_time:77255ms step_avg:58.39ms
step:1324/2330 train_time:77319ms step_avg:58.40ms
step:1325/2330 train_time:77375ms step_avg:58.40ms
step:1326/2330 train_time:77437ms step_avg:58.40ms
step:1327/2330 train_time:77493ms step_avg:58.40ms
step:1328/2330 train_time:77556ms step_avg:58.40ms
step:1329/2330 train_time:77613ms step_avg:58.40ms
step:1330/2330 train_time:77673ms step_avg:58.40ms
step:1331/2330 train_time:77731ms step_avg:58.40ms
step:1332/2330 train_time:77792ms step_avg:58.40ms
step:1333/2330 train_time:77849ms step_avg:58.40ms
step:1334/2330 train_time:77909ms step_avg:58.40ms
step:1335/2330 train_time:77967ms step_avg:58.40ms
step:1336/2330 train_time:78028ms step_avg:58.40ms
step:1337/2330 train_time:78086ms step_avg:58.40ms
step:1338/2330 train_time:78146ms step_avg:58.41ms
step:1339/2330 train_time:78204ms step_avg:58.40ms
step:1340/2330 train_time:78263ms step_avg:58.41ms
step:1341/2330 train_time:78321ms step_avg:58.40ms
step:1342/2330 train_time:78382ms step_avg:58.41ms
step:1343/2330 train_time:78439ms step_avg:58.41ms
step:1344/2330 train_time:78500ms step_avg:58.41ms
step:1345/2330 train_time:78556ms step_avg:58.41ms
step:1346/2330 train_time:78618ms step_avg:58.41ms
step:1347/2330 train_time:78675ms step_avg:58.41ms
step:1348/2330 train_time:78736ms step_avg:58.41ms
step:1349/2330 train_time:78792ms step_avg:58.41ms
step:1350/2330 train_time:78854ms step_avg:58.41ms
step:1351/2330 train_time:78911ms step_avg:58.41ms
step:1352/2330 train_time:78972ms step_avg:58.41ms
step:1353/2330 train_time:79029ms step_avg:58.41ms
step:1354/2330 train_time:79090ms step_avg:58.41ms
step:1355/2330 train_time:79148ms step_avg:58.41ms
step:1356/2330 train_time:79209ms step_avg:58.41ms
step:1357/2330 train_time:79266ms step_avg:58.41ms
step:1358/2330 train_time:79326ms step_avg:58.41ms
step:1359/2330 train_time:79384ms step_avg:58.41ms
step:1360/2330 train_time:79444ms step_avg:58.41ms
step:1361/2330 train_time:79501ms step_avg:58.41ms
step:1362/2330 train_time:79562ms step_avg:58.42ms
step:1363/2330 train_time:79619ms step_avg:58.41ms
step:1364/2330 train_time:79681ms step_avg:58.42ms
step:1365/2330 train_time:79738ms step_avg:58.42ms
step:1366/2330 train_time:79798ms step_avg:58.42ms
step:1367/2330 train_time:79854ms step_avg:58.42ms
step:1368/2330 train_time:79917ms step_avg:58.42ms
step:1369/2330 train_time:79973ms step_avg:58.42ms
step:1370/2330 train_time:80035ms step_avg:58.42ms
step:1371/2330 train_time:80091ms step_avg:58.42ms
step:1372/2330 train_time:80152ms step_avg:58.42ms
step:1373/2330 train_time:80210ms step_avg:58.42ms
step:1374/2330 train_time:80270ms step_avg:58.42ms
step:1375/2330 train_time:80329ms step_avg:58.42ms
step:1376/2330 train_time:80389ms step_avg:58.42ms
step:1377/2330 train_time:80446ms step_avg:58.42ms
step:1378/2330 train_time:80506ms step_avg:58.42ms
step:1379/2330 train_time:80564ms step_avg:58.42ms
step:1380/2330 train_time:80623ms step_avg:58.42ms
step:1381/2330 train_time:80681ms step_avg:58.42ms
step:1382/2330 train_time:80741ms step_avg:58.42ms
step:1383/2330 train_time:80798ms step_avg:58.42ms
step:1384/2330 train_time:80860ms step_avg:58.42ms
step:1385/2330 train_time:80917ms step_avg:58.42ms
step:1386/2330 train_time:80978ms step_avg:58.43ms
step:1387/2330 train_time:81034ms step_avg:58.42ms
step:1388/2330 train_time:81097ms step_avg:58.43ms
step:1389/2330 train_time:81154ms step_avg:58.43ms
step:1390/2330 train_time:81216ms step_avg:58.43ms
step:1391/2330 train_time:81273ms step_avg:58.43ms
step:1392/2330 train_time:81334ms step_avg:58.43ms
step:1393/2330 train_time:81391ms step_avg:58.43ms
step:1394/2330 train_time:81452ms step_avg:58.43ms
step:1395/2330 train_time:81510ms step_avg:58.43ms
step:1396/2330 train_time:81571ms step_avg:58.43ms
step:1397/2330 train_time:81629ms step_avg:58.43ms
step:1398/2330 train_time:81689ms step_avg:58.43ms
step:1399/2330 train_time:81747ms step_avg:58.43ms
step:1400/2330 train_time:81807ms step_avg:58.43ms
step:1401/2330 train_time:81865ms step_avg:58.43ms
step:1402/2330 train_time:81924ms step_avg:58.43ms
step:1403/2330 train_time:81982ms step_avg:58.43ms
step:1404/2330 train_time:82042ms step_avg:58.43ms
step:1405/2330 train_time:82099ms step_avg:58.43ms
step:1406/2330 train_time:82160ms step_avg:58.44ms
step:1407/2330 train_time:82217ms step_avg:58.43ms
step:1408/2330 train_time:82278ms step_avg:58.44ms
step:1409/2330 train_time:82335ms step_avg:58.44ms
step:1410/2330 train_time:82396ms step_avg:58.44ms
step:1411/2330 train_time:82453ms step_avg:58.44ms
step:1412/2330 train_time:82514ms step_avg:58.44ms
step:1413/2330 train_time:82571ms step_avg:58.44ms
step:1414/2330 train_time:82632ms step_avg:58.44ms
step:1415/2330 train_time:82689ms step_avg:58.44ms
step:1416/2330 train_time:82750ms step_avg:58.44ms
step:1417/2330 train_time:82808ms step_avg:58.44ms
step:1418/2330 train_time:82868ms step_avg:58.44ms
step:1419/2330 train_time:82927ms step_avg:58.44ms
step:1420/2330 train_time:82987ms step_avg:58.44ms
step:1421/2330 train_time:83046ms step_avg:58.44ms
step:1422/2330 train_time:83105ms step_avg:58.44ms
step:1423/2330 train_time:83164ms step_avg:58.44ms
step:1424/2330 train_time:83223ms step_avg:58.44ms
step:1425/2330 train_time:83280ms step_avg:58.44ms
step:1426/2330 train_time:83341ms step_avg:58.44ms
step:1427/2330 train_time:83397ms step_avg:58.44ms
step:1428/2330 train_time:83458ms step_avg:58.44ms
step:1429/2330 train_time:83515ms step_avg:58.44ms
step:1430/2330 train_time:83577ms step_avg:58.45ms
step:1431/2330 train_time:83634ms step_avg:58.44ms
step:1432/2330 train_time:83694ms step_avg:58.45ms
step:1433/2330 train_time:83751ms step_avg:58.44ms
step:1434/2330 train_time:83811ms step_avg:58.45ms
step:1435/2330 train_time:83869ms step_avg:58.44ms
step:1436/2330 train_time:83930ms step_avg:58.45ms
step:1437/2330 train_time:83987ms step_avg:58.45ms
step:1438/2330 train_time:84049ms step_avg:58.45ms
step:1439/2330 train_time:84107ms step_avg:58.45ms
step:1440/2330 train_time:84167ms step_avg:58.45ms
step:1441/2330 train_time:84226ms step_avg:58.45ms
step:1442/2330 train_time:84286ms step_avg:58.45ms
step:1443/2330 train_time:84344ms step_avg:58.45ms
step:1444/2330 train_time:84403ms step_avg:58.45ms
step:1445/2330 train_time:84461ms step_avg:58.45ms
step:1446/2330 train_time:84521ms step_avg:58.45ms
step:1447/2330 train_time:84578ms step_avg:58.45ms
step:1448/2330 train_time:84640ms step_avg:58.45ms
step:1449/2330 train_time:84696ms step_avg:58.45ms
step:1450/2330 train_time:84757ms step_avg:58.45ms
step:1451/2330 train_time:84814ms step_avg:58.45ms
step:1452/2330 train_time:84876ms step_avg:58.45ms
step:1453/2330 train_time:84932ms step_avg:58.45ms
step:1454/2330 train_time:84993ms step_avg:58.45ms
step:1455/2330 train_time:85050ms step_avg:58.45ms
step:1456/2330 train_time:85111ms step_avg:58.46ms
step:1457/2330 train_time:85169ms step_avg:58.45ms
step:1458/2330 train_time:85230ms step_avg:58.46ms
step:1459/2330 train_time:85288ms step_avg:58.46ms
step:1460/2330 train_time:85347ms step_avg:58.46ms
step:1461/2330 train_time:85405ms step_avg:58.46ms
step:1462/2330 train_time:85465ms step_avg:58.46ms
step:1463/2330 train_time:85524ms step_avg:58.46ms
step:1464/2330 train_time:85584ms step_avg:58.46ms
step:1465/2330 train_time:85641ms step_avg:58.46ms
step:1466/2330 train_time:85701ms step_avg:58.46ms
step:1467/2330 train_time:85759ms step_avg:58.46ms
step:1468/2330 train_time:85820ms step_avg:58.46ms
step:1469/2330 train_time:85876ms step_avg:58.46ms
step:1470/2330 train_time:85939ms step_avg:58.46ms
step:1471/2330 train_time:85995ms step_avg:58.46ms
step:1472/2330 train_time:86058ms step_avg:58.46ms
step:1473/2330 train_time:86114ms step_avg:58.46ms
step:1474/2330 train_time:86176ms step_avg:58.46ms
step:1475/2330 train_time:86232ms step_avg:58.46ms
step:1476/2330 train_time:86293ms step_avg:58.46ms
step:1477/2330 train_time:86351ms step_avg:58.46ms
step:1478/2330 train_time:86411ms step_avg:58.46ms
step:1479/2330 train_time:86469ms step_avg:58.46ms
step:1480/2330 train_time:86530ms step_avg:58.47ms
step:1481/2330 train_time:86588ms step_avg:58.47ms
step:1482/2330 train_time:86648ms step_avg:58.47ms
step:1483/2330 train_time:86706ms step_avg:58.47ms
step:1484/2330 train_time:86766ms step_avg:58.47ms
step:1485/2330 train_time:86824ms step_avg:58.47ms
step:1486/2330 train_time:86884ms step_avg:58.47ms
step:1487/2330 train_time:86941ms step_avg:58.47ms
step:1488/2330 train_time:87002ms step_avg:58.47ms
step:1489/2330 train_time:87059ms step_avg:58.47ms
step:1490/2330 train_time:87120ms step_avg:58.47ms
step:1491/2330 train_time:87176ms step_avg:58.47ms
step:1492/2330 train_time:87239ms step_avg:58.47ms
step:1493/2330 train_time:87295ms step_avg:58.47ms
step:1494/2330 train_time:87357ms step_avg:58.47ms
step:1495/2330 train_time:87414ms step_avg:58.47ms
step:1496/2330 train_time:87475ms step_avg:58.47ms
step:1497/2330 train_time:87532ms step_avg:58.47ms
step:1498/2330 train_time:87593ms step_avg:58.47ms
step:1499/2330 train_time:87651ms step_avg:58.47ms
step:1500/2330 train_time:87711ms step_avg:58.47ms
step:1500/2330 val_loss:3.9501 train_time:87792ms step_avg:58.53ms
step:1501/2330 train_time:87811ms step_avg:58.50ms
step:1502/2330 train_time:87831ms step_avg:58.48ms
step:1503/2330 train_time:87889ms step_avg:58.48ms
step:1504/2330 train_time:87956ms step_avg:58.48ms
step:1505/2330 train_time:88014ms step_avg:58.48ms
step:1506/2330 train_time:88075ms step_avg:58.48ms
step:1507/2330 train_time:88131ms step_avg:58.48ms
step:1508/2330 train_time:88191ms step_avg:58.48ms
step:1509/2330 train_time:88247ms step_avg:58.48ms
step:1510/2330 train_time:88308ms step_avg:58.48ms
step:1511/2330 train_time:88365ms step_avg:58.48ms
step:1512/2330 train_time:88424ms step_avg:58.48ms
step:1513/2330 train_time:88481ms step_avg:58.48ms
step:1514/2330 train_time:88540ms step_avg:58.48ms
step:1515/2330 train_time:88597ms step_avg:58.48ms
step:1516/2330 train_time:88657ms step_avg:58.48ms
step:1517/2330 train_time:88714ms step_avg:58.48ms
step:1518/2330 train_time:88774ms step_avg:58.48ms
step:1519/2330 train_time:88832ms step_avg:58.48ms
step:1520/2330 train_time:88896ms step_avg:58.48ms
step:1521/2330 train_time:88954ms step_avg:58.48ms
step:1522/2330 train_time:89015ms step_avg:58.49ms
step:1523/2330 train_time:89072ms step_avg:58.48ms
step:1524/2330 train_time:89132ms step_avg:58.49ms
step:1525/2330 train_time:89190ms step_avg:58.49ms
step:1526/2330 train_time:89249ms step_avg:58.49ms
step:1527/2330 train_time:89306ms step_avg:58.48ms
step:1528/2330 train_time:89366ms step_avg:58.49ms
step:1529/2330 train_time:89424ms step_avg:58.49ms
step:1530/2330 train_time:89482ms step_avg:58.48ms
step:1531/2330 train_time:89539ms step_avg:58.48ms
step:1532/2330 train_time:89600ms step_avg:58.49ms
step:1533/2330 train_time:89659ms step_avg:58.49ms
step:1534/2330 train_time:89719ms step_avg:58.49ms
step:1535/2330 train_time:89779ms step_avg:58.49ms
step:1536/2330 train_time:89840ms step_avg:58.49ms
step:1537/2330 train_time:89900ms step_avg:58.49ms
step:1538/2330 train_time:89961ms step_avg:58.49ms
step:1539/2330 train_time:90020ms step_avg:58.49ms
step:1540/2330 train_time:90081ms step_avg:58.49ms
step:1541/2330 train_time:90140ms step_avg:58.49ms
step:1542/2330 train_time:90200ms step_avg:58.50ms
step:1543/2330 train_time:90259ms step_avg:58.50ms
step:1544/2330 train_time:90319ms step_avg:58.50ms
step:1545/2330 train_time:90377ms step_avg:58.50ms
step:1546/2330 train_time:90437ms step_avg:58.50ms
step:1547/2330 train_time:90494ms step_avg:58.50ms
step:1548/2330 train_time:90554ms step_avg:58.50ms
step:1549/2330 train_time:90611ms step_avg:58.50ms
step:1550/2330 train_time:90672ms step_avg:58.50ms
step:1551/2330 train_time:90729ms step_avg:58.50ms
step:1552/2330 train_time:90791ms step_avg:58.50ms
step:1553/2330 train_time:90848ms step_avg:58.50ms
step:1554/2330 train_time:90911ms step_avg:58.50ms
step:1555/2330 train_time:90969ms step_avg:58.50ms
step:1556/2330 train_time:91031ms step_avg:58.50ms
step:1557/2330 train_time:91088ms step_avg:58.50ms
step:1558/2330 train_time:91150ms step_avg:58.50ms
step:1559/2330 train_time:91207ms step_avg:58.50ms
step:1560/2330 train_time:91269ms step_avg:58.51ms
step:1561/2330 train_time:91326ms step_avg:58.51ms
step:1562/2330 train_time:91388ms step_avg:58.51ms
step:1563/2330 train_time:91445ms step_avg:58.51ms
step:1564/2330 train_time:91507ms step_avg:58.51ms
step:1565/2330 train_time:91564ms step_avg:58.51ms
step:1566/2330 train_time:91624ms step_avg:58.51ms
step:1567/2330 train_time:91681ms step_avg:58.51ms
step:1568/2330 train_time:91742ms step_avg:58.51ms
step:1569/2330 train_time:91800ms step_avg:58.51ms
step:1570/2330 train_time:91860ms step_avg:58.51ms
step:1571/2330 train_time:91920ms step_avg:58.51ms
step:1572/2330 train_time:91981ms step_avg:58.51ms
step:1573/2330 train_time:92040ms step_avg:58.51ms
step:1574/2330 train_time:92100ms step_avg:58.51ms
step:1575/2330 train_time:92159ms step_avg:58.51ms
step:1576/2330 train_time:92220ms step_avg:58.51ms
step:1577/2330 train_time:92277ms step_avg:58.51ms
step:1578/2330 train_time:92338ms step_avg:58.52ms
step:1579/2330 train_time:92396ms step_avg:58.52ms
step:1580/2330 train_time:92457ms step_avg:58.52ms
step:1581/2330 train_time:92515ms step_avg:58.52ms
step:1582/2330 train_time:92575ms step_avg:58.52ms
step:1583/2330 train_time:92633ms step_avg:58.52ms
step:1584/2330 train_time:92694ms step_avg:58.52ms
step:1585/2330 train_time:92751ms step_avg:58.52ms
step:1586/2330 train_time:92812ms step_avg:58.52ms
step:1587/2330 train_time:92869ms step_avg:58.52ms
step:1588/2330 train_time:92930ms step_avg:58.52ms
step:1589/2330 train_time:92988ms step_avg:58.52ms
step:1590/2330 train_time:93050ms step_avg:58.52ms
step:1591/2330 train_time:93108ms step_avg:58.52ms
step:1592/2330 train_time:93170ms step_avg:58.52ms
step:1593/2330 train_time:93227ms step_avg:58.52ms
step:1594/2330 train_time:93289ms step_avg:58.53ms
step:1595/2330 train_time:93347ms step_avg:58.52ms
step:1596/2330 train_time:93409ms step_avg:58.53ms
step:1597/2330 train_time:93466ms step_avg:58.53ms
step:1598/2330 train_time:93527ms step_avg:58.53ms
step:1599/2330 train_time:93584ms step_avg:58.53ms
step:1600/2330 train_time:93645ms step_avg:58.53ms
step:1601/2330 train_time:93704ms step_avg:58.53ms
step:1602/2330 train_time:93764ms step_avg:58.53ms
step:1603/2330 train_time:93823ms step_avg:58.53ms
step:1604/2330 train_time:93883ms step_avg:58.53ms
step:1605/2330 train_time:93940ms step_avg:58.53ms
step:1606/2330 train_time:94001ms step_avg:58.53ms
step:1607/2330 train_time:94060ms step_avg:58.53ms
step:1608/2330 train_time:94120ms step_avg:58.53ms
step:1609/2330 train_time:94178ms step_avg:58.53ms
step:1610/2330 train_time:94239ms step_avg:58.53ms
step:1611/2330 train_time:94297ms step_avg:58.53ms
step:1612/2330 train_time:94357ms step_avg:58.53ms
step:1613/2330 train_time:94416ms step_avg:58.53ms
step:1614/2330 train_time:94476ms step_avg:58.54ms
step:1615/2330 train_time:94534ms step_avg:58.54ms
step:1616/2330 train_time:94594ms step_avg:58.54ms
step:1617/2330 train_time:94652ms step_avg:58.54ms
step:1618/2330 train_time:94714ms step_avg:58.54ms
step:1619/2330 train_time:94771ms step_avg:58.54ms
step:1620/2330 train_time:94832ms step_avg:58.54ms
step:1621/2330 train_time:94889ms step_avg:58.54ms
step:1622/2330 train_time:94951ms step_avg:58.54ms
step:1623/2330 train_time:95008ms step_avg:58.54ms
step:1624/2330 train_time:95069ms step_avg:58.54ms
step:1625/2330 train_time:95126ms step_avg:58.54ms
step:1626/2330 train_time:95187ms step_avg:58.54ms
step:1627/2330 train_time:95245ms step_avg:58.54ms
step:1628/2330 train_time:95307ms step_avg:58.54ms
step:1629/2330 train_time:95365ms step_avg:58.54ms
step:1630/2330 train_time:95426ms step_avg:58.54ms
step:1631/2330 train_time:95484ms step_avg:58.54ms
step:1632/2330 train_time:95545ms step_avg:58.54ms
step:1633/2330 train_time:95603ms step_avg:58.54ms
step:1634/2330 train_time:95663ms step_avg:58.55ms
step:1635/2330 train_time:95721ms step_avg:58.55ms
step:1636/2330 train_time:95782ms step_avg:58.55ms
step:1637/2330 train_time:95840ms step_avg:58.55ms
step:1638/2330 train_time:95900ms step_avg:58.55ms
step:1639/2330 train_time:95959ms step_avg:58.55ms
step:1640/2330 train_time:96019ms step_avg:58.55ms
step:1641/2330 train_time:96078ms step_avg:58.55ms
step:1642/2330 train_time:96138ms step_avg:58.55ms
step:1643/2330 train_time:96197ms step_avg:58.55ms
step:1644/2330 train_time:96257ms step_avg:58.55ms
step:1645/2330 train_time:96314ms step_avg:58.55ms
step:1646/2330 train_time:96376ms step_avg:58.55ms
step:1647/2330 train_time:96433ms step_avg:58.55ms
step:1648/2330 train_time:96495ms step_avg:58.55ms
step:1649/2330 train_time:96552ms step_avg:58.55ms
step:1650/2330 train_time:96614ms step_avg:58.55ms
step:1651/2330 train_time:96671ms step_avg:58.55ms
step:1652/2330 train_time:96732ms step_avg:58.55ms
step:1653/2330 train_time:96790ms step_avg:58.55ms
step:1654/2330 train_time:96852ms step_avg:58.56ms
step:1655/2330 train_time:96909ms step_avg:58.56ms
step:1656/2330 train_time:96970ms step_avg:58.56ms
step:1657/2330 train_time:97027ms step_avg:58.56ms
step:1658/2330 train_time:97089ms step_avg:58.56ms
step:1659/2330 train_time:97146ms step_avg:58.56ms
step:1660/2330 train_time:97208ms step_avg:58.56ms
step:1661/2330 train_time:97266ms step_avg:58.56ms
step:1662/2330 train_time:97326ms step_avg:58.56ms
step:1663/2330 train_time:97384ms step_avg:58.56ms
step:1664/2330 train_time:97445ms step_avg:58.56ms
step:1665/2330 train_time:97502ms step_avg:58.56ms
step:1666/2330 train_time:97564ms step_avg:58.56ms
step:1667/2330 train_time:97622ms step_avg:58.56ms
step:1668/2330 train_time:97683ms step_avg:58.56ms
step:1669/2330 train_time:97741ms step_avg:58.56ms
step:1670/2330 train_time:97801ms step_avg:58.56ms
step:1671/2330 train_time:97858ms step_avg:58.56ms
step:1672/2330 train_time:97919ms step_avg:58.56ms
step:1673/2330 train_time:97977ms step_avg:58.56ms
step:1674/2330 train_time:98037ms step_avg:58.56ms
step:1675/2330 train_time:98096ms step_avg:58.56ms
step:1676/2330 train_time:98156ms step_avg:58.57ms
step:1677/2330 train_time:98213ms step_avg:58.56ms
step:1678/2330 train_time:98275ms step_avg:58.57ms
step:1679/2330 train_time:98332ms step_avg:58.57ms
step:1680/2330 train_time:98394ms step_avg:58.57ms
step:1681/2330 train_time:98451ms step_avg:58.57ms
step:1682/2330 train_time:98513ms step_avg:58.57ms
step:1683/2330 train_time:98569ms step_avg:58.57ms
step:1684/2330 train_time:98632ms step_avg:58.57ms
step:1685/2330 train_time:98690ms step_avg:58.57ms
step:1686/2330 train_time:98750ms step_avg:58.57ms
step:1687/2330 train_time:98807ms step_avg:58.57ms
step:1688/2330 train_time:98868ms step_avg:58.57ms
step:1689/2330 train_time:98926ms step_avg:58.57ms
step:1690/2330 train_time:98987ms step_avg:58.57ms
step:1691/2330 train_time:99045ms step_avg:58.57ms
step:1692/2330 train_time:99108ms step_avg:58.57ms
step:1693/2330 train_time:99166ms step_avg:58.57ms
step:1694/2330 train_time:99226ms step_avg:58.58ms
step:1695/2330 train_time:99284ms step_avg:58.57ms
step:1696/2330 train_time:99345ms step_avg:58.58ms
step:1697/2330 train_time:99403ms step_avg:58.58ms
step:1698/2330 train_time:99464ms step_avg:58.58ms
step:1699/2330 train_time:99522ms step_avg:58.58ms
step:1700/2330 train_time:99583ms step_avg:58.58ms
step:1701/2330 train_time:99640ms step_avg:58.58ms
step:1702/2330 train_time:99702ms step_avg:58.58ms
step:1703/2330 train_time:99759ms step_avg:58.58ms
step:1704/2330 train_time:99820ms step_avg:58.58ms
step:1705/2330 train_time:99877ms step_avg:58.58ms
step:1706/2330 train_time:99938ms step_avg:58.58ms
step:1707/2330 train_time:99996ms step_avg:58.58ms
step:1708/2330 train_time:100058ms step_avg:58.58ms
step:1709/2330 train_time:100115ms step_avg:58.58ms
step:1710/2330 train_time:100177ms step_avg:58.58ms
step:1711/2330 train_time:100234ms step_avg:58.58ms
step:1712/2330 train_time:100296ms step_avg:58.58ms
step:1713/2330 train_time:100353ms step_avg:58.58ms
step:1714/2330 train_time:100415ms step_avg:58.58ms
step:1715/2330 train_time:100472ms step_avg:58.58ms
step:1716/2330 train_time:100533ms step_avg:58.59ms
step:1717/2330 train_time:100590ms step_avg:58.58ms
step:1718/2330 train_time:100652ms step_avg:58.59ms
step:1719/2330 train_time:100710ms step_avg:58.59ms
step:1720/2330 train_time:100771ms step_avg:58.59ms
step:1721/2330 train_time:100827ms step_avg:58.59ms
step:1722/2330 train_time:100889ms step_avg:58.59ms
step:1723/2330 train_time:100946ms step_avg:58.59ms
step:1724/2330 train_time:101009ms step_avg:58.59ms
step:1725/2330 train_time:101066ms step_avg:58.59ms
step:1726/2330 train_time:101127ms step_avg:58.59ms
step:1727/2330 train_time:101185ms step_avg:58.59ms
step:1728/2330 train_time:101245ms step_avg:58.59ms
step:1729/2330 train_time:101303ms step_avg:58.59ms
step:1730/2330 train_time:101365ms step_avg:58.59ms
step:1731/2330 train_time:101422ms step_avg:58.59ms
step:1732/2330 train_time:101483ms step_avg:58.59ms
step:1733/2330 train_time:101542ms step_avg:58.59ms
step:1734/2330 train_time:101602ms step_avg:58.59ms
step:1735/2330 train_time:101660ms step_avg:58.59ms
step:1736/2330 train_time:101721ms step_avg:58.59ms
step:1737/2330 train_time:101778ms step_avg:58.59ms
step:1738/2330 train_time:101839ms step_avg:58.60ms
step:1739/2330 train_time:101896ms step_avg:58.59ms
step:1740/2330 train_time:101958ms step_avg:58.60ms
step:1741/2330 train_time:102016ms step_avg:58.60ms
step:1742/2330 train_time:102077ms step_avg:58.60ms
step:1743/2330 train_time:102135ms step_avg:58.60ms
step:1744/2330 train_time:102197ms step_avg:58.60ms
step:1745/2330 train_time:102254ms step_avg:58.60ms
step:1746/2330 train_time:102316ms step_avg:58.60ms
step:1747/2330 train_time:102374ms step_avg:58.60ms
step:1748/2330 train_time:102435ms step_avg:58.60ms
step:1749/2330 train_time:102492ms step_avg:58.60ms
step:1750/2330 train_time:102554ms step_avg:58.60ms
step:1750/2330 val_loss:3.8654 train_time:102636ms step_avg:58.65ms
step:1751/2330 train_time:102656ms step_avg:58.63ms
step:1752/2330 train_time:102677ms step_avg:58.61ms
step:1753/2330 train_time:102731ms step_avg:58.60ms
step:1754/2330 train_time:102797ms step_avg:58.61ms
step:1755/2330 train_time:102854ms step_avg:58.61ms
step:1756/2330 train_time:102918ms step_avg:58.61ms
step:1757/2330 train_time:102974ms step_avg:58.61ms
step:1758/2330 train_time:103035ms step_avg:58.61ms
step:1759/2330 train_time:103092ms step_avg:58.61ms
step:1760/2330 train_time:103151ms step_avg:58.61ms
step:1761/2330 train_time:103208ms step_avg:58.61ms
step:1762/2330 train_time:103268ms step_avg:58.61ms
step:1763/2330 train_time:103325ms step_avg:58.61ms
step:1764/2330 train_time:103385ms step_avg:58.61ms
step:1765/2330 train_time:103441ms step_avg:58.61ms
step:1766/2330 train_time:103501ms step_avg:58.61ms
step:1767/2330 train_time:103561ms step_avg:58.61ms
step:1768/2330 train_time:103627ms step_avg:58.61ms
step:1769/2330 train_time:103686ms step_avg:58.61ms
step:1770/2330 train_time:103748ms step_avg:58.61ms
step:1771/2330 train_time:103806ms step_avg:58.61ms
step:1772/2330 train_time:103866ms step_avg:58.62ms
step:1773/2330 train_time:103923ms step_avg:58.61ms
step:1774/2330 train_time:103984ms step_avg:58.62ms
step:1775/2330 train_time:104041ms step_avg:58.61ms
step:1776/2330 train_time:104102ms step_avg:58.62ms
step:1777/2330 train_time:104159ms step_avg:58.61ms
step:1778/2330 train_time:104220ms step_avg:58.62ms
step:1779/2330 train_time:104278ms step_avg:58.62ms
step:1780/2330 train_time:104338ms step_avg:58.62ms
step:1781/2330 train_time:104395ms step_avg:58.62ms
step:1782/2330 train_time:104455ms step_avg:58.62ms
step:1783/2330 train_time:104515ms step_avg:58.62ms
step:1784/2330 train_time:104576ms step_avg:58.62ms
step:1785/2330 train_time:104635ms step_avg:58.62ms
step:1786/2330 train_time:104696ms step_avg:58.62ms
step:1787/2330 train_time:104755ms step_avg:58.62ms
step:1788/2330 train_time:104817ms step_avg:58.62ms
step:1789/2330 train_time:104875ms step_avg:58.62ms
step:1790/2330 train_time:104935ms step_avg:58.62ms
step:1791/2330 train_time:104992ms step_avg:58.62ms
step:1792/2330 train_time:105052ms step_avg:58.62ms
step:1793/2330 train_time:105110ms step_avg:58.62ms
step:1794/2330 train_time:105169ms step_avg:58.62ms
step:1795/2330 train_time:105226ms step_avg:58.62ms
step:1796/2330 train_time:105287ms step_avg:58.62ms
step:1797/2330 train_time:105343ms step_avg:58.62ms
step:1798/2330 train_time:105406ms step_avg:58.62ms
step:1799/2330 train_time:105463ms step_avg:58.62ms
step:1800/2330 train_time:105525ms step_avg:58.62ms
step:1801/2330 train_time:105583ms step_avg:58.62ms
step:1802/2330 train_time:105644ms step_avg:58.63ms
step:1803/2330 train_time:105702ms step_avg:58.63ms
step:1804/2330 train_time:105764ms step_avg:58.63ms
step:1805/2330 train_time:105822ms step_avg:58.63ms
step:1806/2330 train_time:105884ms step_avg:58.63ms
step:1807/2330 train_time:105941ms step_avg:58.63ms
step:1808/2330 train_time:106002ms step_avg:58.63ms
step:1809/2330 train_time:106059ms step_avg:58.63ms
step:1810/2330 train_time:106122ms step_avg:58.63ms
step:1811/2330 train_time:106179ms step_avg:58.63ms
step:1812/2330 train_time:106239ms step_avg:58.63ms
step:1813/2330 train_time:106298ms step_avg:58.63ms
step:1814/2330 train_time:106358ms step_avg:58.63ms
step:1815/2330 train_time:106416ms step_avg:58.63ms
step:1816/2330 train_time:106476ms step_avg:58.63ms
step:1817/2330 train_time:106535ms step_avg:58.63ms
step:1818/2330 train_time:106595ms step_avg:58.63ms
step:1819/2330 train_time:106654ms step_avg:58.63ms
step:1820/2330 train_time:106715ms step_avg:58.63ms
step:1821/2330 train_time:106775ms step_avg:58.64ms
step:1822/2330 train_time:106836ms step_avg:58.64ms
step:1823/2330 train_time:106893ms step_avg:58.64ms
step:1824/2330 train_time:106954ms step_avg:58.64ms
step:1825/2330 train_time:107012ms step_avg:58.64ms
step:1826/2330 train_time:107072ms step_avg:58.64ms
step:1827/2330 train_time:107130ms step_avg:58.64ms
step:1828/2330 train_time:107191ms step_avg:58.64ms
step:1829/2330 train_time:107248ms step_avg:58.64ms
step:1830/2330 train_time:107309ms step_avg:58.64ms
step:1831/2330 train_time:107366ms step_avg:58.64ms
step:1832/2330 train_time:107427ms step_avg:58.64ms
step:1833/2330 train_time:107485ms step_avg:58.64ms
step:1834/2330 train_time:107545ms step_avg:58.64ms
step:1835/2330 train_time:107603ms step_avg:58.64ms
step:1836/2330 train_time:107665ms step_avg:58.64ms
step:1837/2330 train_time:107723ms step_avg:58.64ms
step:1838/2330 train_time:107784ms step_avg:58.64ms
step:1839/2330 train_time:107841ms step_avg:58.64ms
step:1840/2330 train_time:107903ms step_avg:58.64ms
step:1841/2330 train_time:107960ms step_avg:58.64ms
step:1842/2330 train_time:108022ms step_avg:58.64ms
step:1843/2330 train_time:108080ms step_avg:58.64ms
step:1844/2330 train_time:108140ms step_avg:58.64ms
step:1845/2330 train_time:108199ms step_avg:58.64ms
step:1846/2330 train_time:108259ms step_avg:58.65ms
step:1847/2330 train_time:108318ms step_avg:58.65ms
step:1848/2330 train_time:108378ms step_avg:58.65ms
step:1849/2330 train_time:108435ms step_avg:58.65ms
step:1850/2330 train_time:108496ms step_avg:58.65ms
step:1851/2330 train_time:108554ms step_avg:58.65ms
step:1852/2330 train_time:108615ms step_avg:58.65ms
step:1853/2330 train_time:108674ms step_avg:58.65ms
step:1854/2330 train_time:108734ms step_avg:58.65ms
step:1855/2330 train_time:108792ms step_avg:58.65ms
step:1856/2330 train_time:108852ms step_avg:58.65ms
step:1857/2330 train_time:108910ms step_avg:58.65ms
step:1858/2330 train_time:108972ms step_avg:58.65ms
step:1859/2330 train_time:109029ms step_avg:58.65ms
step:1860/2330 train_time:109091ms step_avg:58.65ms
step:1861/2330 train_time:109148ms step_avg:58.65ms
step:1862/2330 train_time:109210ms step_avg:58.65ms
step:1863/2330 train_time:109268ms step_avg:58.65ms
step:1864/2330 train_time:109328ms step_avg:58.65ms
step:1865/2330 train_time:109385ms step_avg:58.65ms
step:1866/2330 train_time:109446ms step_avg:58.65ms
step:1867/2330 train_time:109502ms step_avg:58.65ms
step:1868/2330 train_time:109564ms step_avg:58.65ms
step:1869/2330 train_time:109621ms step_avg:58.65ms
step:1870/2330 train_time:109683ms step_avg:58.65ms
step:1871/2330 train_time:109740ms step_avg:58.65ms
step:1872/2330 train_time:109802ms step_avg:58.65ms
step:1873/2330 train_time:109860ms step_avg:58.65ms
step:1874/2330 train_time:109921ms step_avg:58.66ms
step:1875/2330 train_time:109979ms step_avg:58.66ms
step:1876/2330 train_time:110039ms step_avg:58.66ms
step:1877/2330 train_time:110098ms step_avg:58.66ms
step:1878/2330 train_time:110158ms step_avg:58.66ms
step:1879/2330 train_time:110218ms step_avg:58.66ms
step:1880/2330 train_time:110278ms step_avg:58.66ms
step:1881/2330 train_time:110336ms step_avg:58.66ms
step:1882/2330 train_time:110396ms step_avg:58.66ms
step:1883/2330 train_time:110454ms step_avg:58.66ms
step:1884/2330 train_time:110515ms step_avg:58.66ms
step:1885/2330 train_time:110574ms step_avg:58.66ms
step:1886/2330 train_time:110634ms step_avg:58.66ms
step:1887/2330 train_time:110692ms step_avg:58.66ms
step:1888/2330 train_time:110753ms step_avg:58.66ms
step:1889/2330 train_time:110811ms step_avg:58.66ms
step:1890/2330 train_time:110872ms step_avg:58.66ms
step:1891/2330 train_time:110930ms step_avg:58.66ms
step:1892/2330 train_time:110991ms step_avg:58.66ms
step:1893/2330 train_time:111049ms step_avg:58.66ms
step:1894/2330 train_time:111110ms step_avg:58.66ms
step:1895/2330 train_time:111168ms step_avg:58.66ms
step:1896/2330 train_time:111229ms step_avg:58.66ms
step:1897/2330 train_time:111286ms step_avg:58.66ms
step:1898/2330 train_time:111347ms step_avg:58.67ms
step:1899/2330 train_time:111404ms step_avg:58.66ms
step:1900/2330 train_time:111465ms step_avg:58.67ms
step:1901/2330 train_time:111522ms step_avg:58.67ms
step:1902/2330 train_time:111584ms step_avg:58.67ms
step:1903/2330 train_time:111641ms step_avg:58.67ms
step:1904/2330 train_time:111702ms step_avg:58.67ms
step:1905/2330 train_time:111759ms step_avg:58.67ms
step:1906/2330 train_time:111821ms step_avg:58.67ms
step:1907/2330 train_time:111879ms step_avg:58.67ms
step:1908/2330 train_time:111940ms step_avg:58.67ms
step:1909/2330 train_time:111998ms step_avg:58.67ms
step:1910/2330 train_time:112058ms step_avg:58.67ms
step:1911/2330 train_time:112116ms step_avg:58.67ms
step:1912/2330 train_time:112177ms step_avg:58.67ms
step:1913/2330 train_time:112235ms step_avg:58.67ms
step:1914/2330 train_time:112296ms step_avg:58.67ms
step:1915/2330 train_time:112354ms step_avg:58.67ms
step:1916/2330 train_time:112415ms step_avg:58.67ms
step:1917/2330 train_time:112473ms step_avg:58.67ms
step:1918/2330 train_time:112533ms step_avg:58.67ms
step:1919/2330 train_time:112591ms step_avg:58.67ms
step:1920/2330 train_time:112651ms step_avg:58.67ms
step:1921/2330 train_time:112709ms step_avg:58.67ms
step:1922/2330 train_time:112771ms step_avg:58.67ms
step:1923/2330 train_time:112829ms step_avg:58.67ms
step:1924/2330 train_time:112890ms step_avg:58.67ms
step:1925/2330 train_time:112947ms step_avg:58.67ms
step:1926/2330 train_time:113009ms step_avg:58.68ms
step:1927/2330 train_time:113066ms step_avg:58.67ms
step:1928/2330 train_time:113128ms step_avg:58.68ms
step:1929/2330 train_time:113186ms step_avg:58.68ms
step:1930/2330 train_time:113247ms step_avg:58.68ms
step:1931/2330 train_time:113304ms step_avg:58.68ms
step:1932/2330 train_time:113366ms step_avg:58.68ms
step:1933/2330 train_time:113423ms step_avg:58.68ms
step:1934/2330 train_time:113485ms step_avg:58.68ms
step:1935/2330 train_time:113541ms step_avg:58.68ms
step:1936/2330 train_time:113604ms step_avg:58.68ms
step:1937/2330 train_time:113661ms step_avg:58.68ms
step:1938/2330 train_time:113723ms step_avg:58.68ms
step:1939/2330 train_time:113781ms step_avg:58.68ms
step:1940/2330 train_time:113842ms step_avg:58.68ms
step:1941/2330 train_time:113900ms step_avg:58.68ms
step:1942/2330 train_time:113960ms step_avg:58.68ms
step:1943/2330 train_time:114018ms step_avg:58.68ms
step:1944/2330 train_time:114078ms step_avg:58.68ms
step:1945/2330 train_time:114136ms step_avg:58.68ms
step:1946/2330 train_time:114197ms step_avg:58.68ms
step:1947/2330 train_time:114255ms step_avg:58.68ms
step:1948/2330 train_time:114316ms step_avg:58.68ms
step:1949/2330 train_time:114375ms step_avg:58.68ms
step:1950/2330 train_time:114435ms step_avg:58.68ms
step:1951/2330 train_time:114493ms step_avg:58.68ms
step:1952/2330 train_time:114554ms step_avg:58.69ms
step:1953/2330 train_time:114610ms step_avg:58.68ms
step:1954/2330 train_time:114672ms step_avg:58.69ms
step:1955/2330 train_time:114729ms step_avg:58.68ms
step:1956/2330 train_time:114790ms step_avg:58.69ms
step:1957/2330 train_time:114848ms step_avg:58.69ms
step:1958/2330 train_time:114909ms step_avg:58.69ms
step:1959/2330 train_time:114966ms step_avg:58.69ms
step:1960/2330 train_time:115028ms step_avg:58.69ms
step:1961/2330 train_time:115085ms step_avg:58.69ms
step:1962/2330 train_time:115148ms step_avg:58.69ms
step:1963/2330 train_time:115205ms step_avg:58.69ms
step:1964/2330 train_time:115267ms step_avg:58.69ms
step:1965/2330 train_time:115325ms step_avg:58.69ms
step:1966/2330 train_time:115386ms step_avg:58.69ms
step:1967/2330 train_time:115442ms step_avg:58.69ms
step:1968/2330 train_time:115504ms step_avg:58.69ms
step:1969/2330 train_time:115561ms step_avg:58.69ms
step:1970/2330 train_time:115623ms step_avg:58.69ms
step:1971/2330 train_time:115680ms step_avg:58.69ms
step:1972/2330 train_time:115741ms step_avg:58.69ms
step:1973/2330 train_time:115799ms step_avg:58.69ms
step:1974/2330 train_time:115860ms step_avg:58.69ms
step:1975/2330 train_time:115918ms step_avg:58.69ms
step:1976/2330 train_time:115978ms step_avg:58.69ms
step:1977/2330 train_time:116036ms step_avg:58.69ms
step:1978/2330 train_time:116096ms step_avg:58.69ms
step:1979/2330 train_time:116155ms step_avg:58.69ms
step:1980/2330 train_time:116216ms step_avg:58.69ms
step:1981/2330 train_time:116275ms step_avg:58.70ms
step:1982/2330 train_time:116336ms step_avg:58.70ms
step:1983/2330 train_time:116395ms step_avg:58.70ms
step:1984/2330 train_time:116455ms step_avg:58.70ms
step:1985/2330 train_time:116512ms step_avg:58.70ms
step:1986/2330 train_time:116573ms step_avg:58.70ms
step:1987/2330 train_time:116630ms step_avg:58.70ms
step:1988/2330 train_time:116692ms step_avg:58.70ms
step:1989/2330 train_time:116748ms step_avg:58.70ms
step:1990/2330 train_time:116810ms step_avg:58.70ms
step:1991/2330 train_time:116867ms step_avg:58.70ms
step:1992/2330 train_time:116930ms step_avg:58.70ms
step:1993/2330 train_time:116987ms step_avg:58.70ms
step:1994/2330 train_time:117048ms step_avg:58.70ms
step:1995/2330 train_time:117106ms step_avg:58.70ms
step:1996/2330 train_time:117167ms step_avg:58.70ms
step:1997/2330 train_time:117224ms step_avg:58.70ms
step:1998/2330 train_time:117286ms step_avg:58.70ms
step:1999/2330 train_time:117343ms step_avg:58.70ms
step:2000/2330 train_time:117405ms step_avg:58.70ms
step:2000/2330 val_loss:3.8037 train_time:117487ms step_avg:58.74ms
step:2001/2330 train_time:117506ms step_avg:58.72ms
step:2002/2330 train_time:117526ms step_avg:58.70ms
step:2003/2330 train_time:117586ms step_avg:58.70ms
step:2004/2330 train_time:117653ms step_avg:58.71ms
step:2005/2330 train_time:117711ms step_avg:58.71ms
step:2006/2330 train_time:117771ms step_avg:58.71ms
step:2007/2330 train_time:117829ms step_avg:58.71ms
step:2008/2330 train_time:117889ms step_avg:58.71ms
step:2009/2330 train_time:117945ms step_avg:58.71ms
step:2010/2330 train_time:118006ms step_avg:58.71ms
step:2011/2330 train_time:118063ms step_avg:58.71ms
step:2012/2330 train_time:118123ms step_avg:58.71ms
step:2013/2330 train_time:118180ms step_avg:58.71ms
step:2014/2330 train_time:118239ms step_avg:58.71ms
step:2015/2330 train_time:118297ms step_avg:58.71ms
step:2016/2330 train_time:118356ms step_avg:58.71ms
step:2017/2330 train_time:118413ms step_avg:58.71ms
step:2018/2330 train_time:118474ms step_avg:58.71ms
step:2019/2330 train_time:118533ms step_avg:58.71ms
step:2020/2330 train_time:118596ms step_avg:58.71ms
step:2021/2330 train_time:118654ms step_avg:58.71ms
step:2022/2330 train_time:118716ms step_avg:58.71ms
step:2023/2330 train_time:118774ms step_avg:58.71ms
step:2024/2330 train_time:118836ms step_avg:58.71ms
step:2025/2330 train_time:118892ms step_avg:58.71ms
step:2026/2330 train_time:118954ms step_avg:58.71ms
step:2027/2330 train_time:119011ms step_avg:58.71ms
step:2028/2330 train_time:119072ms step_avg:58.71ms
step:2029/2330 train_time:119129ms step_avg:58.71ms
step:2030/2330 train_time:119189ms step_avg:58.71ms
step:2031/2330 train_time:119245ms step_avg:58.71ms
step:2032/2330 train_time:119306ms step_avg:58.71ms
step:2033/2330 train_time:119363ms step_avg:58.71ms
step:2034/2330 train_time:119424ms step_avg:58.71ms
step:2035/2330 train_time:119481ms step_avg:58.71ms
step:2036/2330 train_time:119544ms step_avg:58.71ms
step:2037/2330 train_time:119603ms step_avg:58.72ms
step:2038/2330 train_time:119665ms step_avg:58.72ms
step:2039/2330 train_time:119723ms step_avg:58.72ms
step:2040/2330 train_time:119783ms step_avg:58.72ms
step:2041/2330 train_time:119842ms step_avg:58.72ms
step:2042/2330 train_time:119903ms step_avg:58.72ms
step:2043/2330 train_time:119962ms step_avg:58.72ms
step:2044/2330 train_time:120021ms step_avg:58.72ms
step:2045/2330 train_time:120079ms step_avg:58.72ms
step:2046/2330 train_time:120140ms step_avg:58.72ms
step:2047/2330 train_time:120198ms step_avg:58.72ms
step:2048/2330 train_time:120258ms step_avg:58.72ms
step:2049/2330 train_time:120315ms step_avg:58.72ms
step:2050/2330 train_time:120375ms step_avg:58.72ms
step:2051/2330 train_time:120432ms step_avg:58.72ms
step:2052/2330 train_time:120494ms step_avg:58.72ms
step:2053/2330 train_time:120551ms step_avg:58.72ms
step:2054/2330 train_time:120613ms step_avg:58.72ms
step:2055/2330 train_time:120670ms step_avg:58.72ms
step:2056/2330 train_time:120734ms step_avg:58.72ms
step:2057/2330 train_time:120791ms step_avg:58.72ms
step:2058/2330 train_time:120854ms step_avg:58.72ms
step:2059/2330 train_time:120910ms step_avg:58.72ms
step:2060/2330 train_time:120972ms step_avg:58.72ms
step:2061/2330 train_time:121029ms step_avg:58.72ms
step:2062/2330 train_time:121090ms step_avg:58.72ms
step:2063/2330 train_time:121147ms step_avg:58.72ms
step:2064/2330 train_time:121209ms step_avg:58.73ms
step:2065/2330 train_time:121265ms step_avg:58.72ms
step:2066/2330 train_time:121326ms step_avg:58.72ms
step:2067/2330 train_time:121384ms step_avg:58.72ms
step:2068/2330 train_time:121445ms step_avg:58.73ms
step:2069/2330 train_time:121504ms step_avg:58.73ms
step:2070/2330 train_time:121565ms step_avg:58.73ms
step:2071/2330 train_time:121623ms step_avg:58.73ms
step:2072/2330 train_time:121684ms step_avg:58.73ms
step:2073/2330 train_time:121744ms step_avg:58.73ms
step:2074/2330 train_time:121805ms step_avg:58.73ms
step:2075/2330 train_time:121863ms step_avg:58.73ms
step:2076/2330 train_time:121924ms step_avg:58.73ms
step:2077/2330 train_time:121982ms step_avg:58.73ms
step:2078/2330 train_time:122042ms step_avg:58.73ms
step:2079/2330 train_time:122100ms step_avg:58.73ms
step:2080/2330 train_time:122160ms step_avg:58.73ms
step:2081/2330 train_time:122218ms step_avg:58.73ms
step:2082/2330 train_time:122278ms step_avg:58.73ms
step:2083/2330 train_time:122335ms step_avg:58.73ms
step:2084/2330 train_time:122397ms step_avg:58.73ms
step:2085/2330 train_time:122454ms step_avg:58.73ms
step:2086/2330 train_time:122514ms step_avg:58.73ms
step:2087/2330 train_time:122572ms step_avg:58.73ms
step:2088/2330 train_time:122634ms step_avg:58.73ms
step:2089/2330 train_time:122692ms step_avg:58.73ms
step:2090/2330 train_time:122753ms step_avg:58.73ms
step:2091/2330 train_time:122810ms step_avg:58.73ms
step:2092/2330 train_time:122872ms step_avg:58.73ms
step:2093/2330 train_time:122930ms step_avg:58.73ms
step:2094/2330 train_time:122991ms step_avg:58.73ms
step:2095/2330 train_time:123049ms step_avg:58.73ms
step:2096/2330 train_time:123110ms step_avg:58.74ms
step:2097/2330 train_time:123166ms step_avg:58.73ms
step:2098/2330 train_time:123227ms step_avg:58.74ms
step:2099/2330 train_time:123284ms step_avg:58.73ms
step:2100/2330 train_time:123346ms step_avg:58.74ms
step:2101/2330 train_time:123404ms step_avg:58.74ms
step:2102/2330 train_time:123465ms step_avg:58.74ms
step:2103/2330 train_time:123522ms step_avg:58.74ms
step:2104/2330 train_time:123583ms step_avg:58.74ms
step:2105/2330 train_time:123643ms step_avg:58.74ms
step:2106/2330 train_time:123704ms step_avg:58.74ms
step:2107/2330 train_time:123761ms step_avg:58.74ms
step:2108/2330 train_time:123822ms step_avg:58.74ms
step:2109/2330 train_time:123880ms step_avg:58.74ms
step:2110/2330 train_time:123940ms step_avg:58.74ms
step:2111/2330 train_time:123998ms step_avg:58.74ms
step:2112/2330 train_time:124059ms step_avg:58.74ms
step:2113/2330 train_time:124118ms step_avg:58.74ms
step:2114/2330 train_time:124178ms step_avg:58.74ms
step:2115/2330 train_time:124235ms step_avg:58.74ms
step:2116/2330 train_time:124297ms step_avg:58.74ms
step:2117/2330 train_time:124353ms step_avg:58.74ms
step:2118/2330 train_time:124415ms step_avg:58.74ms
step:2119/2330 train_time:124472ms step_avg:58.74ms
step:2120/2330 train_time:124534ms step_avg:58.74ms
step:2121/2330 train_time:124591ms step_avg:58.74ms
step:2122/2330 train_time:124653ms step_avg:58.74ms
step:2123/2330 train_time:124711ms step_avg:58.74ms
step:2124/2330 train_time:124772ms step_avg:58.74ms
step:2125/2330 train_time:124830ms step_avg:58.74ms
step:2126/2330 train_time:124892ms step_avg:58.75ms
step:2127/2330 train_time:124948ms step_avg:58.74ms
step:2128/2330 train_time:125012ms step_avg:58.75ms
step:2129/2330 train_time:125068ms step_avg:58.74ms
step:2130/2330 train_time:125130ms step_avg:58.75ms
step:2131/2330 train_time:125188ms step_avg:58.75ms
step:2132/2330 train_time:125250ms step_avg:58.75ms
step:2133/2330 train_time:125307ms step_avg:58.75ms
step:2134/2330 train_time:125369ms step_avg:58.75ms
step:2135/2330 train_time:125426ms step_avg:58.75ms
step:2136/2330 train_time:125487ms step_avg:58.75ms
step:2137/2330 train_time:125544ms step_avg:58.75ms
step:2138/2330 train_time:125605ms step_avg:58.75ms
step:2139/2330 train_time:125662ms step_avg:58.75ms
step:2140/2330 train_time:125724ms step_avg:58.75ms
step:2141/2330 train_time:125783ms step_avg:58.75ms
step:2142/2330 train_time:125843ms step_avg:58.75ms
step:2143/2330 train_time:125902ms step_avg:58.75ms
step:2144/2330 train_time:125963ms step_avg:58.75ms
step:2145/2330 train_time:126021ms step_avg:58.75ms
step:2146/2330 train_time:126081ms step_avg:58.75ms
step:2147/2330 train_time:126138ms step_avg:58.75ms
step:2148/2330 train_time:126199ms step_avg:58.75ms
step:2149/2330 train_time:126256ms step_avg:58.75ms
step:2150/2330 train_time:126317ms step_avg:58.75ms
step:2151/2330 train_time:126375ms step_avg:58.75ms
step:2152/2330 train_time:126436ms step_avg:58.75ms
step:2153/2330 train_time:126493ms step_avg:58.75ms
step:2154/2330 train_time:126555ms step_avg:58.75ms
step:2155/2330 train_time:126612ms step_avg:58.75ms
step:2156/2330 train_time:126674ms step_avg:58.75ms
step:2157/2330 train_time:126731ms step_avg:58.75ms
step:2158/2330 train_time:126793ms step_avg:58.75ms
step:2159/2330 train_time:126850ms step_avg:58.75ms
step:2160/2330 train_time:126913ms step_avg:58.76ms
step:2161/2330 train_time:126970ms step_avg:58.76ms
step:2162/2330 train_time:127031ms step_avg:58.76ms
step:2163/2330 train_time:127088ms step_avg:58.76ms
step:2164/2330 train_time:127151ms step_avg:58.76ms
step:2165/2330 train_time:127208ms step_avg:58.76ms
step:2166/2330 train_time:127269ms step_avg:58.76ms
step:2167/2330 train_time:127326ms step_avg:58.76ms
step:2168/2330 train_time:127387ms step_avg:58.76ms
step:2169/2330 train_time:127445ms step_avg:58.76ms
step:2170/2330 train_time:127507ms step_avg:58.76ms
step:2171/2330 train_time:127565ms step_avg:58.76ms
step:2172/2330 train_time:127626ms step_avg:58.76ms
step:2173/2330 train_time:127684ms step_avg:58.76ms
step:2174/2330 train_time:127745ms step_avg:58.76ms
step:2175/2330 train_time:127804ms step_avg:58.76ms
step:2176/2330 train_time:127864ms step_avg:58.76ms
step:2177/2330 train_time:127922ms step_avg:58.76ms
step:2178/2330 train_time:127982ms step_avg:58.76ms
step:2179/2330 train_time:128040ms step_avg:58.76ms
step:2180/2330 train_time:128101ms step_avg:58.76ms
step:2181/2330 train_time:128160ms step_avg:58.76ms
step:2182/2330 train_time:128220ms step_avg:58.76ms
step:2183/2330 train_time:128278ms step_avg:58.76ms
step:2184/2330 train_time:128339ms step_avg:58.76ms
step:2185/2330 train_time:128397ms step_avg:58.76ms
step:2186/2330 train_time:128457ms step_avg:58.76ms
step:2187/2330 train_time:128514ms step_avg:58.76ms
step:2188/2330 train_time:128576ms step_avg:58.76ms
step:2189/2330 train_time:128633ms step_avg:58.76ms
step:2190/2330 train_time:128695ms step_avg:58.77ms
step:2191/2330 train_time:128753ms step_avg:58.76ms
step:2192/2330 train_time:128815ms step_avg:58.77ms
step:2193/2330 train_time:128871ms step_avg:58.76ms
step:2194/2330 train_time:128934ms step_avg:58.77ms
step:2195/2330 train_time:128991ms step_avg:58.77ms
step:2196/2330 train_time:129051ms step_avg:58.77ms
step:2197/2330 train_time:129108ms step_avg:58.77ms
step:2198/2330 train_time:129170ms step_avg:58.77ms
step:2199/2330 train_time:129228ms step_avg:58.77ms
step:2200/2330 train_time:129289ms step_avg:58.77ms
step:2201/2330 train_time:129346ms step_avg:58.77ms
step:2202/2330 train_time:129408ms step_avg:58.77ms
step:2203/2330 train_time:129465ms step_avg:58.77ms
step:2204/2330 train_time:129527ms step_avg:58.77ms
step:2205/2330 train_time:129586ms step_avg:58.77ms
step:2206/2330 train_time:129647ms step_avg:58.77ms
step:2207/2330 train_time:129705ms step_avg:58.77ms
step:2208/2330 train_time:129765ms step_avg:58.77ms
step:2209/2330 train_time:129823ms step_avg:58.77ms
step:2210/2330 train_time:129883ms step_avg:58.77ms
step:2211/2330 train_time:129942ms step_avg:58.77ms
step:2212/2330 train_time:130003ms step_avg:58.77ms
step:2213/2330 train_time:130060ms step_avg:58.77ms
step:2214/2330 train_time:130120ms step_avg:58.77ms
step:2215/2330 train_time:130177ms step_avg:58.77ms
step:2216/2330 train_time:130238ms step_avg:58.77ms
step:2217/2330 train_time:130295ms step_avg:58.77ms
step:2218/2330 train_time:130357ms step_avg:58.77ms
step:2219/2330 train_time:130413ms step_avg:58.77ms
step:2220/2330 train_time:130475ms step_avg:58.77ms
step:2221/2330 train_time:130532ms step_avg:58.77ms
step:2222/2330 train_time:130594ms step_avg:58.77ms
step:2223/2330 train_time:130651ms step_avg:58.77ms
step:2224/2330 train_time:130713ms step_avg:58.77ms
step:2225/2330 train_time:130770ms step_avg:58.77ms
step:2226/2330 train_time:130831ms step_avg:58.77ms
step:2227/2330 train_time:130888ms step_avg:58.77ms
step:2228/2330 train_time:130951ms step_avg:58.78ms
step:2229/2330 train_time:131008ms step_avg:58.77ms
step:2230/2330 train_time:131069ms step_avg:58.78ms
step:2231/2330 train_time:131126ms step_avg:58.77ms
step:2232/2330 train_time:131188ms step_avg:58.78ms
step:2233/2330 train_time:131245ms step_avg:58.78ms
step:2234/2330 train_time:131306ms step_avg:58.78ms
step:2235/2330 train_time:131363ms step_avg:58.78ms
step:2236/2330 train_time:131424ms step_avg:58.78ms
step:2237/2330 train_time:131482ms step_avg:58.78ms
step:2238/2330 train_time:131542ms step_avg:58.78ms
step:2239/2330 train_time:131600ms step_avg:58.78ms
step:2240/2330 train_time:131662ms step_avg:58.78ms
step:2241/2330 train_time:131720ms step_avg:58.78ms
step:2242/2330 train_time:131781ms step_avg:58.78ms
step:2243/2330 train_time:131839ms step_avg:58.78ms
step:2244/2330 train_time:131900ms step_avg:58.78ms
step:2245/2330 train_time:131957ms step_avg:58.78ms
step:2246/2330 train_time:132018ms step_avg:58.78ms
step:2247/2330 train_time:132074ms step_avg:58.78ms
step:2248/2330 train_time:132136ms step_avg:58.78ms
step:2249/2330 train_time:132194ms step_avg:58.78ms
step:2250/2330 train_time:132255ms step_avg:58.78ms
step:2250/2330 val_loss:3.7578 train_time:132337ms step_avg:58.82ms
step:2251/2330 train_time:132357ms step_avg:58.80ms
step:2252/2330 train_time:132378ms step_avg:58.78ms
step:2253/2330 train_time:132436ms step_avg:58.78ms
step:2254/2330 train_time:132500ms step_avg:58.78ms
step:2255/2330 train_time:132559ms step_avg:58.78ms
step:2256/2330 train_time:132622ms step_avg:58.79ms
step:2257/2330 train_time:132679ms step_avg:58.79ms
step:2258/2330 train_time:132740ms step_avg:58.79ms
step:2259/2330 train_time:132796ms step_avg:58.79ms
step:2260/2330 train_time:132857ms step_avg:58.79ms
step:2261/2330 train_time:132914ms step_avg:58.79ms
step:2262/2330 train_time:132974ms step_avg:58.79ms
step:2263/2330 train_time:133031ms step_avg:58.79ms
step:2264/2330 train_time:133091ms step_avg:58.79ms
step:2265/2330 train_time:133147ms step_avg:58.78ms
step:2266/2330 train_time:133207ms step_avg:58.79ms
step:2267/2330 train_time:133266ms step_avg:58.79ms
step:2268/2330 train_time:133327ms step_avg:58.79ms
step:2269/2330 train_time:133387ms step_avg:58.79ms
step:2270/2330 train_time:133449ms step_avg:58.79ms
step:2271/2330 train_time:133508ms step_avg:58.79ms
step:2272/2330 train_time:133570ms step_avg:58.79ms
step:2273/2330 train_time:133628ms step_avg:58.79ms
step:2274/2330 train_time:133689ms step_avg:58.79ms
step:2275/2330 train_time:133746ms step_avg:58.79ms
step:2276/2330 train_time:133807ms step_avg:58.79ms
step:2277/2330 train_time:133864ms step_avg:58.79ms
step:2278/2330 train_time:133925ms step_avg:58.79ms
step:2279/2330 train_time:133982ms step_avg:58.79ms
step:2280/2330 train_time:134043ms step_avg:58.79ms
step:2281/2330 train_time:134099ms step_avg:58.79ms
step:2282/2330 train_time:134159ms step_avg:58.79ms
step:2283/2330 train_time:134217ms step_avg:58.79ms
step:2284/2330 train_time:134278ms step_avg:58.79ms
step:2285/2330 train_time:134336ms step_avg:58.79ms
step:2286/2330 train_time:134397ms step_avg:58.79ms
step:2287/2330 train_time:134454ms step_avg:58.79ms
step:2288/2330 train_time:134519ms step_avg:58.79ms
step:2289/2330 train_time:134576ms step_avg:58.79ms
step:2290/2330 train_time:134640ms step_avg:58.79ms
step:2291/2330 train_time:134696ms step_avg:58.79ms
step:2292/2330 train_time:134759ms step_avg:58.80ms
step:2293/2330 train_time:134815ms step_avg:58.79ms
step:2294/2330 train_time:134877ms step_avg:58.80ms
step:2295/2330 train_time:134934ms step_avg:58.79ms
step:2296/2330 train_time:134994ms step_avg:58.80ms
step:2297/2330 train_time:135051ms step_avg:58.79ms
step:2298/2330 train_time:135112ms step_avg:58.80ms
step:2299/2330 train_time:135169ms step_avg:58.79ms
step:2300/2330 train_time:135231ms step_avg:58.80ms
step:2301/2330 train_time:135289ms step_avg:58.80ms
step:2302/2330 train_time:135350ms step_avg:58.80ms
step:2303/2330 train_time:135409ms step_avg:58.80ms
step:2304/2330 train_time:135470ms step_avg:58.80ms
step:2305/2330 train_time:135529ms step_avg:58.80ms
step:2306/2330 train_time:135589ms step_avg:58.80ms
step:2307/2330 train_time:135648ms step_avg:58.80ms
step:2308/2330 train_time:135709ms step_avg:58.80ms
step:2309/2330 train_time:135768ms step_avg:58.80ms
step:2310/2330 train_time:135829ms step_avg:58.80ms
step:2311/2330 train_time:135886ms step_avg:58.80ms
step:2312/2330 train_time:135946ms step_avg:58.80ms
step:2313/2330 train_time:136003ms step_avg:58.80ms
step:2314/2330 train_time:136063ms step_avg:58.80ms
step:2315/2330 train_time:136120ms step_avg:58.80ms
step:2316/2330 train_time:136181ms step_avg:58.80ms
step:2317/2330 train_time:136238ms step_avg:58.80ms
step:2318/2330 train_time:136298ms step_avg:58.80ms
step:2319/2330 train_time:136355ms step_avg:58.80ms
step:2320/2330 train_time:136417ms step_avg:58.80ms
step:2321/2330 train_time:136475ms step_avg:58.80ms
step:2322/2330 train_time:136536ms step_avg:58.80ms
step:2323/2330 train_time:136594ms step_avg:58.80ms
step:2324/2330 train_time:136657ms step_avg:58.80ms
step:2325/2330 train_time:136714ms step_avg:58.80ms
step:2326/2330 train_time:136777ms step_avg:58.80ms
step:2327/2330 train_time:136833ms step_avg:58.80ms
step:2328/2330 train_time:136895ms step_avg:58.80ms
step:2329/2330 train_time:136952ms step_avg:58.80ms
step:2330/2330 train_time:137013ms step_avg:58.80ms
step:2330/2330 val_loss:3.7434 train_time:137094ms step_avg:58.84ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
