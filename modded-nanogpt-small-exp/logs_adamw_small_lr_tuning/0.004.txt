import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--adamw_lr', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_lr_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_lr_tuning/{args2.adamw_lr}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=float(args2.adamw_lr),  betas=(0.85, 0.85), eps=1e-10, weight_decay=0.0, fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_lr_tuning/{args2.adamw_lr}", exist_ok=True)
            torch.save(log, f"logs_adamw_small_lr_tuning/{args2.adamw_lr}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 01:51:42 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   35C    P0             118W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   35C    P0             117W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             118W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             115W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   34C    P0             118W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             114W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:93ms step_avg:93.44ms
step:2/2330 train_time:218ms step_avg:109.03ms
step:3/2330 train_time:236ms step_avg:78.63ms
step:4/2330 train_time:255ms step_avg:63.73ms
step:5/2330 train_time:309ms step_avg:61.79ms
step:6/2330 train_time:367ms step_avg:61.12ms
step:7/2330 train_time:422ms step_avg:60.27ms
step:8/2330 train_time:479ms step_avg:59.90ms
step:9/2330 train_time:534ms step_avg:59.31ms
step:10/2330 train_time:591ms step_avg:59.12ms
step:11/2330 train_time:645ms step_avg:58.64ms
step:12/2330 train_time:703ms step_avg:58.57ms
step:13/2330 train_time:757ms step_avg:58.25ms
step:14/2330 train_time:814ms step_avg:58.15ms
step:15/2330 train_time:869ms step_avg:57.92ms
step:16/2330 train_time:926ms step_avg:57.87ms
step:17/2330 train_time:980ms step_avg:57.67ms
step:18/2330 train_time:1037ms step_avg:57.63ms
step:19/2330 train_time:1092ms step_avg:57.47ms
step:20/2330 train_time:1153ms step_avg:57.67ms
step:21/2330 train_time:1210ms step_avg:57.60ms
step:22/2330 train_time:1269ms step_avg:57.69ms
step:23/2330 train_time:1324ms step_avg:57.57ms
step:24/2330 train_time:1383ms step_avg:57.64ms
step:25/2330 train_time:1439ms step_avg:57.56ms
step:26/2330 train_time:1497ms step_avg:57.57ms
step:27/2330 train_time:1552ms step_avg:57.48ms
step:28/2330 train_time:1609ms step_avg:57.48ms
step:29/2330 train_time:1664ms step_avg:57.38ms
step:30/2330 train_time:1722ms step_avg:57.40ms
step:31/2330 train_time:1777ms step_avg:57.33ms
step:32/2330 train_time:1835ms step_avg:57.33ms
step:33/2330 train_time:1889ms step_avg:57.24ms
step:34/2330 train_time:1947ms step_avg:57.26ms
step:35/2330 train_time:2002ms step_avg:57.19ms
step:36/2330 train_time:2060ms step_avg:57.22ms
step:37/2330 train_time:2115ms step_avg:57.16ms
step:38/2330 train_time:2174ms step_avg:57.21ms
step:39/2330 train_time:2229ms step_avg:57.16ms
step:40/2330 train_time:2289ms step_avg:57.23ms
step:41/2330 train_time:2344ms step_avg:57.18ms
step:42/2330 train_time:2404ms step_avg:57.24ms
step:43/2330 train_time:2460ms step_avg:57.20ms
step:44/2330 train_time:2518ms step_avg:57.22ms
step:45/2330 train_time:2573ms step_avg:57.17ms
step:46/2330 train_time:2631ms step_avg:57.20ms
step:47/2330 train_time:2686ms step_avg:57.15ms
step:48/2330 train_time:2746ms step_avg:57.20ms
step:49/2330 train_time:2801ms step_avg:57.16ms
step:50/2330 train_time:2859ms step_avg:57.17ms
step:51/2330 train_time:2913ms step_avg:57.12ms
step:52/2330 train_time:2971ms step_avg:57.13ms
step:53/2330 train_time:3025ms step_avg:57.08ms
step:54/2330 train_time:3084ms step_avg:57.12ms
step:55/2330 train_time:3140ms step_avg:57.09ms
step:56/2330 train_time:3198ms step_avg:57.12ms
step:57/2330 train_time:3254ms step_avg:57.09ms
step:58/2330 train_time:3313ms step_avg:57.12ms
step:59/2330 train_time:3369ms step_avg:57.10ms
step:60/2330 train_time:3427ms step_avg:57.12ms
step:61/2330 train_time:3483ms step_avg:57.10ms
step:62/2330 train_time:3541ms step_avg:57.11ms
step:63/2330 train_time:3596ms step_avg:57.08ms
step:64/2330 train_time:3654ms step_avg:57.10ms
step:65/2330 train_time:3710ms step_avg:57.07ms
step:66/2330 train_time:3768ms step_avg:57.09ms
step:67/2330 train_time:3823ms step_avg:57.06ms
step:68/2330 train_time:3881ms step_avg:57.08ms
step:69/2330 train_time:3937ms step_avg:57.06ms
step:70/2330 train_time:3995ms step_avg:57.07ms
step:71/2330 train_time:4050ms step_avg:57.04ms
step:72/2330 train_time:4108ms step_avg:57.05ms
step:73/2330 train_time:4163ms step_avg:57.03ms
step:74/2330 train_time:4223ms step_avg:57.07ms
step:75/2330 train_time:4279ms step_avg:57.05ms
step:76/2330 train_time:4337ms step_avg:57.07ms
step:77/2330 train_time:4393ms step_avg:57.05ms
step:78/2330 train_time:4451ms step_avg:57.06ms
step:79/2330 train_time:4506ms step_avg:57.04ms
step:80/2330 train_time:4565ms step_avg:57.06ms
step:81/2330 train_time:4621ms step_avg:57.05ms
step:82/2330 train_time:4679ms step_avg:57.06ms
step:83/2330 train_time:4734ms step_avg:57.03ms
step:84/2330 train_time:4792ms step_avg:57.05ms
step:85/2330 train_time:4847ms step_avg:57.02ms
step:86/2330 train_time:4906ms step_avg:57.05ms
step:87/2330 train_time:4962ms step_avg:57.03ms
step:88/2330 train_time:5020ms step_avg:57.04ms
step:89/2330 train_time:5075ms step_avg:57.03ms
step:90/2330 train_time:5133ms step_avg:57.03ms
step:91/2330 train_time:5189ms step_avg:57.02ms
step:92/2330 train_time:5247ms step_avg:57.04ms
step:93/2330 train_time:5303ms step_avg:57.02ms
step:94/2330 train_time:5361ms step_avg:57.03ms
step:95/2330 train_time:5417ms step_avg:57.02ms
step:96/2330 train_time:5475ms step_avg:57.04ms
step:97/2330 train_time:5531ms step_avg:57.02ms
step:98/2330 train_time:5589ms step_avg:57.03ms
step:99/2330 train_time:5644ms step_avg:57.01ms
step:100/2330 train_time:5704ms step_avg:57.04ms
step:101/2330 train_time:5760ms step_avg:57.03ms
step:102/2330 train_time:5818ms step_avg:57.04ms
step:103/2330 train_time:5873ms step_avg:57.02ms
step:104/2330 train_time:5931ms step_avg:57.03ms
step:105/2330 train_time:5986ms step_avg:57.01ms
step:106/2330 train_time:6045ms step_avg:57.03ms
step:107/2330 train_time:6100ms step_avg:57.01ms
step:108/2330 train_time:6159ms step_avg:57.02ms
step:109/2330 train_time:6214ms step_avg:57.01ms
step:110/2330 train_time:6273ms step_avg:57.03ms
step:111/2330 train_time:6328ms step_avg:57.01ms
step:112/2330 train_time:6388ms step_avg:57.03ms
step:113/2330 train_time:6443ms step_avg:57.02ms
step:114/2330 train_time:6502ms step_avg:57.04ms
step:115/2330 train_time:6558ms step_avg:57.03ms
step:116/2330 train_time:6616ms step_avg:57.03ms
step:117/2330 train_time:6671ms step_avg:57.02ms
step:118/2330 train_time:6729ms step_avg:57.03ms
step:119/2330 train_time:6785ms step_avg:57.01ms
step:120/2330 train_time:6845ms step_avg:57.04ms
step:121/2330 train_time:6900ms step_avg:57.03ms
step:122/2330 train_time:6959ms step_avg:57.04ms
step:123/2330 train_time:7014ms step_avg:57.03ms
step:124/2330 train_time:7072ms step_avg:57.03ms
step:125/2330 train_time:7127ms step_avg:57.02ms
step:126/2330 train_time:7186ms step_avg:57.03ms
step:127/2330 train_time:7242ms step_avg:57.02ms
step:128/2330 train_time:7300ms step_avg:57.03ms
step:129/2330 train_time:7356ms step_avg:57.02ms
step:130/2330 train_time:7414ms step_avg:57.03ms
step:131/2330 train_time:7469ms step_avg:57.01ms
step:132/2330 train_time:7528ms step_avg:57.03ms
step:133/2330 train_time:7583ms step_avg:57.02ms
step:134/2330 train_time:7642ms step_avg:57.03ms
step:135/2330 train_time:7697ms step_avg:57.02ms
step:136/2330 train_time:7756ms step_avg:57.03ms
step:137/2330 train_time:7811ms step_avg:57.02ms
step:138/2330 train_time:7869ms step_avg:57.02ms
step:139/2330 train_time:7925ms step_avg:57.01ms
step:140/2330 train_time:7984ms step_avg:57.03ms
step:141/2330 train_time:8040ms step_avg:57.02ms
step:142/2330 train_time:8099ms step_avg:57.04ms
step:143/2330 train_time:8155ms step_avg:57.03ms
step:144/2330 train_time:8213ms step_avg:57.03ms
step:145/2330 train_time:8268ms step_avg:57.02ms
step:146/2330 train_time:8327ms step_avg:57.03ms
step:147/2330 train_time:8382ms step_avg:57.02ms
step:148/2330 train_time:8440ms step_avg:57.03ms
step:149/2330 train_time:8496ms step_avg:57.02ms
step:150/2330 train_time:8554ms step_avg:57.03ms
step:151/2330 train_time:8609ms step_avg:57.02ms
step:152/2330 train_time:8669ms step_avg:57.03ms
step:153/2330 train_time:8723ms step_avg:57.02ms
step:154/2330 train_time:8784ms step_avg:57.04ms
step:155/2330 train_time:8840ms step_avg:57.03ms
step:156/2330 train_time:8898ms step_avg:57.04ms
step:157/2330 train_time:8954ms step_avg:57.03ms
step:158/2330 train_time:9012ms step_avg:57.04ms
step:159/2330 train_time:9066ms step_avg:57.02ms
step:160/2330 train_time:9125ms step_avg:57.03ms
step:161/2330 train_time:9181ms step_avg:57.02ms
step:162/2330 train_time:9239ms step_avg:57.03ms
step:163/2330 train_time:9295ms step_avg:57.02ms
step:164/2330 train_time:9354ms step_avg:57.03ms
step:165/2330 train_time:9409ms step_avg:57.02ms
step:166/2330 train_time:9467ms step_avg:57.03ms
step:167/2330 train_time:9522ms step_avg:57.02ms
step:168/2330 train_time:9580ms step_avg:57.02ms
step:169/2330 train_time:9635ms step_avg:57.01ms
step:170/2330 train_time:9694ms step_avg:57.02ms
step:171/2330 train_time:9749ms step_avg:57.01ms
step:172/2330 train_time:9808ms step_avg:57.02ms
step:173/2330 train_time:9862ms step_avg:57.01ms
step:174/2330 train_time:9922ms step_avg:57.02ms
step:175/2330 train_time:9977ms step_avg:57.01ms
step:176/2330 train_time:10035ms step_avg:57.02ms
step:177/2330 train_time:10091ms step_avg:57.01ms
step:178/2330 train_time:10149ms step_avg:57.02ms
step:179/2330 train_time:10204ms step_avg:57.01ms
step:180/2330 train_time:10263ms step_avg:57.02ms
step:181/2330 train_time:10319ms step_avg:57.01ms
step:182/2330 train_time:10378ms step_avg:57.02ms
step:183/2330 train_time:10433ms step_avg:57.01ms
step:184/2330 train_time:10491ms step_avg:57.02ms
step:185/2330 train_time:10547ms step_avg:57.01ms
step:186/2330 train_time:10605ms step_avg:57.02ms
step:187/2330 train_time:10661ms step_avg:57.01ms
step:188/2330 train_time:10719ms step_avg:57.02ms
step:189/2330 train_time:10774ms step_avg:57.01ms
step:190/2330 train_time:10832ms step_avg:57.01ms
step:191/2330 train_time:10887ms step_avg:57.00ms
step:192/2330 train_time:10947ms step_avg:57.01ms
step:193/2330 train_time:11002ms step_avg:57.01ms
step:194/2330 train_time:11059ms step_avg:57.01ms
step:195/2330 train_time:11115ms step_avg:57.00ms
step:196/2330 train_time:11173ms step_avg:57.00ms
step:197/2330 train_time:11228ms step_avg:56.99ms
step:198/2330 train_time:11287ms step_avg:57.00ms
step:199/2330 train_time:11342ms step_avg:57.00ms
step:200/2330 train_time:11401ms step_avg:57.00ms
step:201/2330 train_time:11457ms step_avg:57.00ms
step:202/2330 train_time:11515ms step_avg:57.00ms
step:203/2330 train_time:11570ms step_avg:56.99ms
step:204/2330 train_time:11629ms step_avg:57.01ms
step:205/2330 train_time:11684ms step_avg:57.00ms
step:206/2330 train_time:11744ms step_avg:57.01ms
step:207/2330 train_time:11800ms step_avg:57.00ms
step:208/2330 train_time:11858ms step_avg:57.01ms
step:209/2330 train_time:11914ms step_avg:57.00ms
step:210/2330 train_time:11972ms step_avg:57.01ms
step:211/2330 train_time:12027ms step_avg:57.00ms
step:212/2330 train_time:12087ms step_avg:57.01ms
step:213/2330 train_time:12142ms step_avg:57.01ms
step:214/2330 train_time:12201ms step_avg:57.01ms
step:215/2330 train_time:12256ms step_avg:57.01ms
step:216/2330 train_time:12314ms step_avg:57.01ms
step:217/2330 train_time:12368ms step_avg:57.00ms
step:218/2330 train_time:12428ms step_avg:57.01ms
step:219/2330 train_time:12483ms step_avg:57.00ms
step:220/2330 train_time:12541ms step_avg:57.00ms
step:221/2330 train_time:12596ms step_avg:57.00ms
step:222/2330 train_time:12655ms step_avg:57.00ms
step:223/2330 train_time:12710ms step_avg:56.99ms
step:224/2330 train_time:12768ms step_avg:57.00ms
step:225/2330 train_time:12824ms step_avg:56.99ms
step:226/2330 train_time:12882ms step_avg:57.00ms
step:227/2330 train_time:12938ms step_avg:57.00ms
step:228/2330 train_time:12996ms step_avg:57.00ms
step:229/2330 train_time:13052ms step_avg:57.00ms
step:230/2330 train_time:13110ms step_avg:57.00ms
step:231/2330 train_time:13165ms step_avg:56.99ms
step:232/2330 train_time:13224ms step_avg:57.00ms
step:233/2330 train_time:13280ms step_avg:57.00ms
step:234/2330 train_time:13338ms step_avg:57.00ms
step:235/2330 train_time:13393ms step_avg:56.99ms
step:236/2330 train_time:13452ms step_avg:57.00ms
step:237/2330 train_time:13507ms step_avg:56.99ms
step:238/2330 train_time:13567ms step_avg:57.00ms
step:239/2330 train_time:13623ms step_avg:57.00ms
step:240/2330 train_time:13681ms step_avg:57.00ms
step:241/2330 train_time:13736ms step_avg:57.00ms
step:242/2330 train_time:13795ms step_avg:57.00ms
step:243/2330 train_time:13850ms step_avg:57.00ms
step:244/2330 train_time:13908ms step_avg:57.00ms
step:245/2330 train_time:13963ms step_avg:56.99ms
step:246/2330 train_time:14023ms step_avg:57.00ms
step:247/2330 train_time:14078ms step_avg:57.00ms
step:248/2330 train_time:14136ms step_avg:57.00ms
step:249/2330 train_time:14192ms step_avg:57.00ms
step:250/2330 train_time:14251ms step_avg:57.00ms
step:250/2330 val_loss:5.1754 train_time:14329ms step_avg:57.32ms
step:251/2330 train_time:14346ms step_avg:57.16ms
step:252/2330 train_time:14367ms step_avg:57.01ms
step:253/2330 train_time:14421ms step_avg:57.00ms
step:254/2330 train_time:14485ms step_avg:57.03ms
step:255/2330 train_time:14540ms step_avg:57.02ms
step:256/2330 train_time:14603ms step_avg:57.04ms
step:257/2330 train_time:14657ms step_avg:57.03ms
step:258/2330 train_time:14719ms step_avg:57.05ms
step:259/2330 train_time:14774ms step_avg:57.04ms
step:260/2330 train_time:14833ms step_avg:57.05ms
step:261/2330 train_time:14888ms step_avg:57.04ms
step:262/2330 train_time:14946ms step_avg:57.04ms
step:263/2330 train_time:15000ms step_avg:57.04ms
step:264/2330 train_time:15058ms step_avg:57.04ms
step:265/2330 train_time:15113ms step_avg:57.03ms
step:266/2330 train_time:15171ms step_avg:57.04ms
step:267/2330 train_time:15226ms step_avg:57.03ms
step:268/2330 train_time:15285ms step_avg:57.03ms
step:269/2330 train_time:15341ms step_avg:57.03ms
step:270/2330 train_time:15401ms step_avg:57.04ms
step:271/2330 train_time:15457ms step_avg:57.04ms
step:272/2330 train_time:15516ms step_avg:57.04ms
step:273/2330 train_time:15572ms step_avg:57.04ms
step:274/2330 train_time:15630ms step_avg:57.04ms
step:275/2330 train_time:15684ms step_avg:57.03ms
step:276/2330 train_time:15743ms step_avg:57.04ms
step:277/2330 train_time:15799ms step_avg:57.03ms
step:278/2330 train_time:15858ms step_avg:57.04ms
step:279/2330 train_time:15913ms step_avg:57.04ms
step:280/2330 train_time:15971ms step_avg:57.04ms
step:281/2330 train_time:16026ms step_avg:57.03ms
step:282/2330 train_time:16084ms step_avg:57.04ms
step:283/2330 train_time:16139ms step_avg:57.03ms
step:284/2330 train_time:16197ms step_avg:57.03ms
step:285/2330 train_time:16253ms step_avg:57.03ms
step:286/2330 train_time:16312ms step_avg:57.03ms
step:287/2330 train_time:16367ms step_avg:57.03ms
step:288/2330 train_time:16425ms step_avg:57.03ms
step:289/2330 train_time:16481ms step_avg:57.03ms
step:290/2330 train_time:16540ms step_avg:57.03ms
step:291/2330 train_time:16596ms step_avg:57.03ms
step:292/2330 train_time:16655ms step_avg:57.04ms
step:293/2330 train_time:16710ms step_avg:57.03ms
step:294/2330 train_time:16768ms step_avg:57.03ms
step:295/2330 train_time:16823ms step_avg:57.03ms
step:296/2330 train_time:16882ms step_avg:57.03ms
step:297/2330 train_time:16937ms step_avg:57.03ms
step:298/2330 train_time:16996ms step_avg:57.03ms
step:299/2330 train_time:17051ms step_avg:57.03ms
step:300/2330 train_time:17110ms step_avg:57.03ms
step:301/2330 train_time:17165ms step_avg:57.03ms
step:302/2330 train_time:17223ms step_avg:57.03ms
step:303/2330 train_time:17279ms step_avg:57.02ms
step:304/2330 train_time:17338ms step_avg:57.03ms
step:305/2330 train_time:17394ms step_avg:57.03ms
step:306/2330 train_time:17453ms step_avg:57.04ms
step:307/2330 train_time:17508ms step_avg:57.03ms
step:308/2330 train_time:17567ms step_avg:57.03ms
step:309/2330 train_time:17622ms step_avg:57.03ms
step:310/2330 train_time:17682ms step_avg:57.04ms
step:311/2330 train_time:17737ms step_avg:57.03ms
step:312/2330 train_time:17796ms step_avg:57.04ms
step:313/2330 train_time:17852ms step_avg:57.03ms
step:314/2330 train_time:17911ms step_avg:57.04ms
step:315/2330 train_time:17966ms step_avg:57.04ms
step:316/2330 train_time:18024ms step_avg:57.04ms
step:317/2330 train_time:18079ms step_avg:57.03ms
step:318/2330 train_time:18137ms step_avg:57.04ms
step:319/2330 train_time:18193ms step_avg:57.03ms
step:320/2330 train_time:18251ms step_avg:57.04ms
step:321/2330 train_time:18307ms step_avg:57.03ms
step:322/2330 train_time:18365ms step_avg:57.03ms
step:323/2330 train_time:18420ms step_avg:57.03ms
step:324/2330 train_time:18479ms step_avg:57.03ms
step:325/2330 train_time:18535ms step_avg:57.03ms
step:326/2330 train_time:18593ms step_avg:57.03ms
step:327/2330 train_time:18649ms step_avg:57.03ms
step:328/2330 train_time:18708ms step_avg:57.04ms
step:329/2330 train_time:18763ms step_avg:57.03ms
step:330/2330 train_time:18821ms step_avg:57.03ms
step:331/2330 train_time:18877ms step_avg:57.03ms
step:332/2330 train_time:18936ms step_avg:57.04ms
step:333/2330 train_time:18991ms step_avg:57.03ms
step:334/2330 train_time:19049ms step_avg:57.03ms
step:335/2330 train_time:19104ms step_avg:57.03ms
step:336/2330 train_time:19162ms step_avg:57.03ms
step:337/2330 train_time:19218ms step_avg:57.03ms
step:338/2330 train_time:19277ms step_avg:57.03ms
step:339/2330 train_time:19332ms step_avg:57.03ms
step:340/2330 train_time:19391ms step_avg:57.03ms
step:341/2330 train_time:19447ms step_avg:57.03ms
step:342/2330 train_time:19505ms step_avg:57.03ms
step:343/2330 train_time:19559ms step_avg:57.02ms
step:344/2330 train_time:19619ms step_avg:57.03ms
step:345/2330 train_time:19674ms step_avg:57.03ms
step:346/2330 train_time:19733ms step_avg:57.03ms
step:347/2330 train_time:19788ms step_avg:57.03ms
step:348/2330 train_time:19848ms step_avg:57.03ms
step:349/2330 train_time:19903ms step_avg:57.03ms
step:350/2330 train_time:19961ms step_avg:57.03ms
step:351/2330 train_time:20017ms step_avg:57.03ms
step:352/2330 train_time:20076ms step_avg:57.03ms
step:353/2330 train_time:20131ms step_avg:57.03ms
step:354/2330 train_time:20189ms step_avg:57.03ms
step:355/2330 train_time:20244ms step_avg:57.03ms
step:356/2330 train_time:20303ms step_avg:57.03ms
step:357/2330 train_time:20359ms step_avg:57.03ms
step:358/2330 train_time:20418ms step_avg:57.03ms
step:359/2330 train_time:20473ms step_avg:57.03ms
step:360/2330 train_time:20533ms step_avg:57.04ms
step:361/2330 train_time:20588ms step_avg:57.03ms
step:362/2330 train_time:20646ms step_avg:57.03ms
step:363/2330 train_time:20701ms step_avg:57.03ms
step:364/2330 train_time:20761ms step_avg:57.03ms
step:365/2330 train_time:20816ms step_avg:57.03ms
step:366/2330 train_time:20875ms step_avg:57.03ms
step:367/2330 train_time:20930ms step_avg:57.03ms
step:368/2330 train_time:20988ms step_avg:57.03ms
step:369/2330 train_time:21044ms step_avg:57.03ms
step:370/2330 train_time:21103ms step_avg:57.03ms
step:371/2330 train_time:21159ms step_avg:57.03ms
step:372/2330 train_time:21217ms step_avg:57.04ms
step:373/2330 train_time:21273ms step_avg:57.03ms
step:374/2330 train_time:21331ms step_avg:57.03ms
step:375/2330 train_time:21386ms step_avg:57.03ms
step:376/2330 train_time:21444ms step_avg:57.03ms
step:377/2330 train_time:21499ms step_avg:57.03ms
step:378/2330 train_time:21559ms step_avg:57.03ms
step:379/2330 train_time:21614ms step_avg:57.03ms
step:380/2330 train_time:21673ms step_avg:57.03ms
step:381/2330 train_time:21729ms step_avg:57.03ms
step:382/2330 train_time:21787ms step_avg:57.03ms
step:383/2330 train_time:21842ms step_avg:57.03ms
step:384/2330 train_time:21900ms step_avg:57.03ms
step:385/2330 train_time:21957ms step_avg:57.03ms
step:386/2330 train_time:22015ms step_avg:57.03ms
step:387/2330 train_time:22070ms step_avg:57.03ms
step:388/2330 train_time:22128ms step_avg:57.03ms
step:389/2330 train_time:22184ms step_avg:57.03ms
step:390/2330 train_time:22242ms step_avg:57.03ms
step:391/2330 train_time:22299ms step_avg:57.03ms
step:392/2330 train_time:22357ms step_avg:57.03ms
step:393/2330 train_time:22413ms step_avg:57.03ms
step:394/2330 train_time:22471ms step_avg:57.03ms
step:395/2330 train_time:22527ms step_avg:57.03ms
step:396/2330 train_time:22586ms step_avg:57.03ms
step:397/2330 train_time:22641ms step_avg:57.03ms
step:398/2330 train_time:22700ms step_avg:57.03ms
step:399/2330 train_time:22755ms step_avg:57.03ms
step:400/2330 train_time:22813ms step_avg:57.03ms
step:401/2330 train_time:22868ms step_avg:57.03ms
step:402/2330 train_time:22927ms step_avg:57.03ms
step:403/2330 train_time:22982ms step_avg:57.03ms
step:404/2330 train_time:23041ms step_avg:57.03ms
step:405/2330 train_time:23096ms step_avg:57.03ms
step:406/2330 train_time:23154ms step_avg:57.03ms
step:407/2330 train_time:23210ms step_avg:57.03ms
step:408/2330 train_time:23268ms step_avg:57.03ms
step:409/2330 train_time:23324ms step_avg:57.03ms
step:410/2330 train_time:23383ms step_avg:57.03ms
step:411/2330 train_time:23438ms step_avg:57.03ms
step:412/2330 train_time:23497ms step_avg:57.03ms
step:413/2330 train_time:23553ms step_avg:57.03ms
step:414/2330 train_time:23611ms step_avg:57.03ms
step:415/2330 train_time:23667ms step_avg:57.03ms
step:416/2330 train_time:23725ms step_avg:57.03ms
step:417/2330 train_time:23780ms step_avg:57.03ms
step:418/2330 train_time:23840ms step_avg:57.03ms
step:419/2330 train_time:23895ms step_avg:57.03ms
step:420/2330 train_time:23954ms step_avg:57.03ms
step:421/2330 train_time:24010ms step_avg:57.03ms
step:422/2330 train_time:24068ms step_avg:57.03ms
step:423/2330 train_time:24123ms step_avg:57.03ms
step:424/2330 train_time:24181ms step_avg:57.03ms
step:425/2330 train_time:24237ms step_avg:57.03ms
step:426/2330 train_time:24295ms step_avg:57.03ms
step:427/2330 train_time:24351ms step_avg:57.03ms
step:428/2330 train_time:24409ms step_avg:57.03ms
step:429/2330 train_time:24464ms step_avg:57.03ms
step:430/2330 train_time:24522ms step_avg:57.03ms
step:431/2330 train_time:24578ms step_avg:57.03ms
step:432/2330 train_time:24637ms step_avg:57.03ms
step:433/2330 train_time:24693ms step_avg:57.03ms
step:434/2330 train_time:24753ms step_avg:57.03ms
step:435/2330 train_time:24808ms step_avg:57.03ms
step:436/2330 train_time:24867ms step_avg:57.03ms
step:437/2330 train_time:24922ms step_avg:57.03ms
step:438/2330 train_time:24980ms step_avg:57.03ms
step:439/2330 train_time:25035ms step_avg:57.03ms
step:440/2330 train_time:25094ms step_avg:57.03ms
step:441/2330 train_time:25149ms step_avg:57.03ms
step:442/2330 train_time:25207ms step_avg:57.03ms
step:443/2330 train_time:25263ms step_avg:57.03ms
step:444/2330 train_time:25322ms step_avg:57.03ms
step:445/2330 train_time:25377ms step_avg:57.03ms
step:446/2330 train_time:25435ms step_avg:57.03ms
step:447/2330 train_time:25491ms step_avg:57.03ms
step:448/2330 train_time:25550ms step_avg:57.03ms
step:449/2330 train_time:25605ms step_avg:57.03ms
step:450/2330 train_time:25664ms step_avg:57.03ms
step:451/2330 train_time:25719ms step_avg:57.03ms
step:452/2330 train_time:25780ms step_avg:57.03ms
step:453/2330 train_time:25835ms step_avg:57.03ms
step:454/2330 train_time:25893ms step_avg:57.03ms
step:455/2330 train_time:25948ms step_avg:57.03ms
step:456/2330 train_time:26008ms step_avg:57.03ms
step:457/2330 train_time:26062ms step_avg:57.03ms
step:458/2330 train_time:26122ms step_avg:57.04ms
step:459/2330 train_time:26178ms step_avg:57.03ms
step:460/2330 train_time:26236ms step_avg:57.04ms
step:461/2330 train_time:26292ms step_avg:57.03ms
step:462/2330 train_time:26350ms step_avg:57.03ms
step:463/2330 train_time:26405ms step_avg:57.03ms
step:464/2330 train_time:26464ms step_avg:57.04ms
step:465/2330 train_time:26520ms step_avg:57.03ms
step:466/2330 train_time:26579ms step_avg:57.04ms
step:467/2330 train_time:26634ms step_avg:57.03ms
step:468/2330 train_time:26693ms step_avg:57.04ms
step:469/2330 train_time:26749ms step_avg:57.03ms
step:470/2330 train_time:26808ms step_avg:57.04ms
step:471/2330 train_time:26863ms step_avg:57.03ms
step:472/2330 train_time:26921ms step_avg:57.04ms
step:473/2330 train_time:26977ms step_avg:57.03ms
step:474/2330 train_time:27036ms step_avg:57.04ms
step:475/2330 train_time:27091ms step_avg:57.03ms
step:476/2330 train_time:27150ms step_avg:57.04ms
step:477/2330 train_time:27205ms step_avg:57.03ms
step:478/2330 train_time:27264ms step_avg:57.04ms
step:479/2330 train_time:27319ms step_avg:57.03ms
step:480/2330 train_time:27378ms step_avg:57.04ms
step:481/2330 train_time:27434ms step_avg:57.03ms
step:482/2330 train_time:27493ms step_avg:57.04ms
step:483/2330 train_time:27548ms step_avg:57.04ms
step:484/2330 train_time:27607ms step_avg:57.04ms
step:485/2330 train_time:27663ms step_avg:57.04ms
step:486/2330 train_time:27722ms step_avg:57.04ms
step:487/2330 train_time:27778ms step_avg:57.04ms
step:488/2330 train_time:27836ms step_avg:57.04ms
step:489/2330 train_time:27892ms step_avg:57.04ms
step:490/2330 train_time:27950ms step_avg:57.04ms
step:491/2330 train_time:28006ms step_avg:57.04ms
step:492/2330 train_time:28064ms step_avg:57.04ms
step:493/2330 train_time:28120ms step_avg:57.04ms
step:494/2330 train_time:28179ms step_avg:57.04ms
step:495/2330 train_time:28234ms step_avg:57.04ms
step:496/2330 train_time:28293ms step_avg:57.04ms
step:497/2330 train_time:28348ms step_avg:57.04ms
step:498/2330 train_time:28407ms step_avg:57.04ms
step:499/2330 train_time:28462ms step_avg:57.04ms
step:500/2330 train_time:28522ms step_avg:57.04ms
step:500/2330 val_loss:4.6109 train_time:28601ms step_avg:57.20ms
step:501/2330 train_time:28619ms step_avg:57.12ms
step:502/2330 train_time:28639ms step_avg:57.05ms
step:503/2330 train_time:28694ms step_avg:57.05ms
step:504/2330 train_time:28760ms step_avg:57.06ms
step:505/2330 train_time:28815ms step_avg:57.06ms
step:506/2330 train_time:28876ms step_avg:57.07ms
step:507/2330 train_time:28931ms step_avg:57.06ms
step:508/2330 train_time:28991ms step_avg:57.07ms
step:509/2330 train_time:29047ms step_avg:57.07ms
step:510/2330 train_time:29105ms step_avg:57.07ms
step:511/2330 train_time:29160ms step_avg:57.06ms
step:512/2330 train_time:29218ms step_avg:57.07ms
step:513/2330 train_time:29273ms step_avg:57.06ms
step:514/2330 train_time:29331ms step_avg:57.06ms
step:515/2330 train_time:29387ms step_avg:57.06ms
step:516/2330 train_time:29445ms step_avg:57.06ms
step:517/2330 train_time:29500ms step_avg:57.06ms
step:518/2330 train_time:29558ms step_avg:57.06ms
step:519/2330 train_time:29614ms step_avg:57.06ms
step:520/2330 train_time:29674ms step_avg:57.07ms
step:521/2330 train_time:29730ms step_avg:57.06ms
step:522/2330 train_time:29790ms step_avg:57.07ms
step:523/2330 train_time:29846ms step_avg:57.07ms
step:524/2330 train_time:29904ms step_avg:57.07ms
step:525/2330 train_time:29959ms step_avg:57.07ms
step:526/2330 train_time:30019ms step_avg:57.07ms
step:527/2330 train_time:30074ms step_avg:57.07ms
step:528/2330 train_time:30134ms step_avg:57.07ms
step:529/2330 train_time:30188ms step_avg:57.07ms
step:530/2330 train_time:30248ms step_avg:57.07ms
step:531/2330 train_time:30303ms step_avg:57.07ms
step:532/2330 train_time:30361ms step_avg:57.07ms
step:533/2330 train_time:30416ms step_avg:57.07ms
step:534/2330 train_time:30475ms step_avg:57.07ms
step:535/2330 train_time:30529ms step_avg:57.06ms
step:536/2330 train_time:30589ms step_avg:57.07ms
step:537/2330 train_time:30645ms step_avg:57.07ms
step:538/2330 train_time:30704ms step_avg:57.07ms
step:539/2330 train_time:30760ms step_avg:57.07ms
step:540/2330 train_time:30818ms step_avg:57.07ms
step:541/2330 train_time:30873ms step_avg:57.07ms
step:542/2330 train_time:30933ms step_avg:57.07ms
step:543/2330 train_time:30988ms step_avg:57.07ms
step:544/2330 train_time:31047ms step_avg:57.07ms
step:545/2330 train_time:31102ms step_avg:57.07ms
step:546/2330 train_time:31160ms step_avg:57.07ms
step:547/2330 train_time:31216ms step_avg:57.07ms
step:548/2330 train_time:31274ms step_avg:57.07ms
step:549/2330 train_time:31329ms step_avg:57.07ms
step:550/2330 train_time:31389ms step_avg:57.07ms
step:551/2330 train_time:31444ms step_avg:57.07ms
step:552/2330 train_time:31502ms step_avg:57.07ms
step:553/2330 train_time:31558ms step_avg:57.07ms
step:554/2330 train_time:31616ms step_avg:57.07ms
step:555/2330 train_time:31672ms step_avg:57.07ms
step:556/2330 train_time:31732ms step_avg:57.07ms
step:557/2330 train_time:31788ms step_avg:57.07ms
step:558/2330 train_time:31847ms step_avg:57.07ms
step:559/2330 train_time:31902ms step_avg:57.07ms
step:560/2330 train_time:31961ms step_avg:57.07ms
step:561/2330 train_time:32017ms step_avg:57.07ms
step:562/2330 train_time:32076ms step_avg:57.07ms
step:563/2330 train_time:32131ms step_avg:57.07ms
step:564/2330 train_time:32191ms step_avg:57.08ms
step:565/2330 train_time:32246ms step_avg:57.07ms
step:566/2330 train_time:32304ms step_avg:57.08ms
step:567/2330 train_time:32360ms step_avg:57.07ms
step:568/2330 train_time:32419ms step_avg:57.08ms
step:569/2330 train_time:32474ms step_avg:57.07ms
step:570/2330 train_time:32533ms step_avg:57.08ms
step:571/2330 train_time:32588ms step_avg:57.07ms
step:572/2330 train_time:32647ms step_avg:57.08ms
step:573/2330 train_time:32703ms step_avg:57.07ms
step:574/2330 train_time:32763ms step_avg:57.08ms
step:575/2330 train_time:32818ms step_avg:57.07ms
step:576/2330 train_time:32877ms step_avg:57.08ms
step:577/2330 train_time:32932ms step_avg:57.07ms
step:578/2330 train_time:32992ms step_avg:57.08ms
step:579/2330 train_time:33048ms step_avg:57.08ms
step:580/2330 train_time:33107ms step_avg:57.08ms
step:581/2330 train_time:33163ms step_avg:57.08ms
step:582/2330 train_time:33222ms step_avg:57.08ms
step:583/2330 train_time:33278ms step_avg:57.08ms
step:584/2330 train_time:33336ms step_avg:57.08ms
step:585/2330 train_time:33391ms step_avg:57.08ms
step:586/2330 train_time:33451ms step_avg:57.08ms
step:587/2330 train_time:33506ms step_avg:57.08ms
step:588/2330 train_time:33565ms step_avg:57.08ms
step:589/2330 train_time:33620ms step_avg:57.08ms
step:590/2330 train_time:33679ms step_avg:57.08ms
step:591/2330 train_time:33735ms step_avg:57.08ms
step:592/2330 train_time:33794ms step_avg:57.08ms
step:593/2330 train_time:33850ms step_avg:57.08ms
step:594/2330 train_time:33909ms step_avg:57.09ms
step:595/2330 train_time:33965ms step_avg:57.08ms
step:596/2330 train_time:34024ms step_avg:57.09ms
step:597/2330 train_time:34079ms step_avg:57.08ms
step:598/2330 train_time:34139ms step_avg:57.09ms
step:599/2330 train_time:34194ms step_avg:57.08ms
step:600/2330 train_time:34253ms step_avg:57.09ms
step:601/2330 train_time:34309ms step_avg:57.09ms
step:602/2330 train_time:34367ms step_avg:57.09ms
step:603/2330 train_time:34423ms step_avg:57.09ms
step:604/2330 train_time:34482ms step_avg:57.09ms
step:605/2330 train_time:34537ms step_avg:57.09ms
step:606/2330 train_time:34596ms step_avg:57.09ms
step:607/2330 train_time:34651ms step_avg:57.08ms
step:608/2330 train_time:34711ms step_avg:57.09ms
step:609/2330 train_time:34767ms step_avg:57.09ms
step:610/2330 train_time:34825ms step_avg:57.09ms
step:611/2330 train_time:34881ms step_avg:57.09ms
step:612/2330 train_time:34939ms step_avg:57.09ms
step:613/2330 train_time:34995ms step_avg:57.09ms
step:614/2330 train_time:35054ms step_avg:57.09ms
step:615/2330 train_time:35110ms step_avg:57.09ms
step:616/2330 train_time:35170ms step_avg:57.09ms
step:617/2330 train_time:35226ms step_avg:57.09ms
step:618/2330 train_time:35284ms step_avg:57.09ms
step:619/2330 train_time:35340ms step_avg:57.09ms
step:620/2330 train_time:35399ms step_avg:57.10ms
step:621/2330 train_time:35454ms step_avg:57.09ms
step:622/2330 train_time:35513ms step_avg:57.10ms
step:623/2330 train_time:35569ms step_avg:57.09ms
step:624/2330 train_time:35628ms step_avg:57.10ms
step:625/2330 train_time:35683ms step_avg:57.09ms
step:626/2330 train_time:35742ms step_avg:57.10ms
step:627/2330 train_time:35797ms step_avg:57.09ms
step:628/2330 train_time:35857ms step_avg:57.10ms
step:629/2330 train_time:35912ms step_avg:57.09ms
step:630/2330 train_time:35972ms step_avg:57.10ms
step:631/2330 train_time:36028ms step_avg:57.10ms
step:632/2330 train_time:36086ms step_avg:57.10ms
step:633/2330 train_time:36141ms step_avg:57.09ms
step:634/2330 train_time:36200ms step_avg:57.10ms
step:635/2330 train_time:36255ms step_avg:57.09ms
step:636/2330 train_time:36315ms step_avg:57.10ms
step:637/2330 train_time:36370ms step_avg:57.10ms
step:638/2330 train_time:36430ms step_avg:57.10ms
step:639/2330 train_time:36486ms step_avg:57.10ms
step:640/2330 train_time:36545ms step_avg:57.10ms
step:641/2330 train_time:36600ms step_avg:57.10ms
step:642/2330 train_time:36659ms step_avg:57.10ms
step:643/2330 train_time:36714ms step_avg:57.10ms
step:644/2330 train_time:36774ms step_avg:57.10ms
step:645/2330 train_time:36829ms step_avg:57.10ms
step:646/2330 train_time:36890ms step_avg:57.10ms
step:647/2330 train_time:36945ms step_avg:57.10ms
step:648/2330 train_time:37004ms step_avg:57.10ms
step:649/2330 train_time:37059ms step_avg:57.10ms
step:650/2330 train_time:37117ms step_avg:57.10ms
step:651/2330 train_time:37173ms step_avg:57.10ms
step:652/2330 train_time:37232ms step_avg:57.10ms
step:653/2330 train_time:37288ms step_avg:57.10ms
step:654/2330 train_time:37346ms step_avg:57.10ms
step:655/2330 train_time:37402ms step_avg:57.10ms
step:656/2330 train_time:37460ms step_avg:57.10ms
step:657/2330 train_time:37515ms step_avg:57.10ms
step:658/2330 train_time:37574ms step_avg:57.10ms
step:659/2330 train_time:37629ms step_avg:57.10ms
step:660/2330 train_time:37689ms step_avg:57.10ms
step:661/2330 train_time:37745ms step_avg:57.10ms
step:662/2330 train_time:37803ms step_avg:57.10ms
step:663/2330 train_time:37859ms step_avg:57.10ms
step:664/2330 train_time:37918ms step_avg:57.11ms
step:665/2330 train_time:37974ms step_avg:57.10ms
step:666/2330 train_time:38032ms step_avg:57.11ms
step:667/2330 train_time:38088ms step_avg:57.10ms
step:668/2330 train_time:38147ms step_avg:57.11ms
step:669/2330 train_time:38202ms step_avg:57.10ms
step:670/2330 train_time:38261ms step_avg:57.11ms
step:671/2330 train_time:38316ms step_avg:57.10ms
step:672/2330 train_time:38376ms step_avg:57.11ms
step:673/2330 train_time:38431ms step_avg:57.10ms
step:674/2330 train_time:38491ms step_avg:57.11ms
step:675/2330 train_time:38547ms step_avg:57.11ms
step:676/2330 train_time:38605ms step_avg:57.11ms
step:677/2330 train_time:38661ms step_avg:57.11ms
step:678/2330 train_time:38719ms step_avg:57.11ms
step:679/2330 train_time:38774ms step_avg:57.10ms
step:680/2330 train_time:38834ms step_avg:57.11ms
step:681/2330 train_time:38889ms step_avg:57.11ms
step:682/2330 train_time:38949ms step_avg:57.11ms
step:683/2330 train_time:39004ms step_avg:57.11ms
step:684/2330 train_time:39063ms step_avg:57.11ms
step:685/2330 train_time:39118ms step_avg:57.11ms
step:686/2330 train_time:39177ms step_avg:57.11ms
step:687/2330 train_time:39232ms step_avg:57.11ms
step:688/2330 train_time:39292ms step_avg:57.11ms
step:689/2330 train_time:39348ms step_avg:57.11ms
step:690/2330 train_time:39407ms step_avg:57.11ms
step:691/2330 train_time:39462ms step_avg:57.11ms
step:692/2330 train_time:39522ms step_avg:57.11ms
step:693/2330 train_time:39577ms step_avg:57.11ms
step:694/2330 train_time:39637ms step_avg:57.11ms
step:695/2330 train_time:39692ms step_avg:57.11ms
step:696/2330 train_time:39752ms step_avg:57.12ms
step:697/2330 train_time:39808ms step_avg:57.11ms
step:698/2330 train_time:39867ms step_avg:57.12ms
step:699/2330 train_time:39922ms step_avg:57.11ms
step:700/2330 train_time:39982ms step_avg:57.12ms
step:701/2330 train_time:40037ms step_avg:57.11ms
step:702/2330 train_time:40096ms step_avg:57.12ms
step:703/2330 train_time:40151ms step_avg:57.11ms
step:704/2330 train_time:40210ms step_avg:57.12ms
step:705/2330 train_time:40266ms step_avg:57.11ms
step:706/2330 train_time:40325ms step_avg:57.12ms
step:707/2330 train_time:40381ms step_avg:57.12ms
step:708/2330 train_time:40440ms step_avg:57.12ms
step:709/2330 train_time:40496ms step_avg:57.12ms
step:710/2330 train_time:40554ms step_avg:57.12ms
step:711/2330 train_time:40610ms step_avg:57.12ms
step:712/2330 train_time:40669ms step_avg:57.12ms
step:713/2330 train_time:40725ms step_avg:57.12ms
step:714/2330 train_time:40784ms step_avg:57.12ms
step:715/2330 train_time:40840ms step_avg:57.12ms
step:716/2330 train_time:40898ms step_avg:57.12ms
step:717/2330 train_time:40954ms step_avg:57.12ms
step:718/2330 train_time:41014ms step_avg:57.12ms
step:719/2330 train_time:41069ms step_avg:57.12ms
step:720/2330 train_time:41129ms step_avg:57.12ms
step:721/2330 train_time:41184ms step_avg:57.12ms
step:722/2330 train_time:41243ms step_avg:57.12ms
step:723/2330 train_time:41298ms step_avg:57.12ms
step:724/2330 train_time:41357ms step_avg:57.12ms
step:725/2330 train_time:41412ms step_avg:57.12ms
step:726/2330 train_time:41473ms step_avg:57.12ms
step:727/2330 train_time:41528ms step_avg:57.12ms
step:728/2330 train_time:41588ms step_avg:57.13ms
step:729/2330 train_time:41643ms step_avg:57.12ms
step:730/2330 train_time:41703ms step_avg:57.13ms
step:731/2330 train_time:41758ms step_avg:57.12ms
step:732/2330 train_time:41817ms step_avg:57.13ms
step:733/2330 train_time:41872ms step_avg:57.12ms
step:734/2330 train_time:41932ms step_avg:57.13ms
step:735/2330 train_time:41988ms step_avg:57.13ms
step:736/2330 train_time:42047ms step_avg:57.13ms
step:737/2330 train_time:42102ms step_avg:57.13ms
step:738/2330 train_time:42161ms step_avg:57.13ms
step:739/2330 train_time:42217ms step_avg:57.13ms
step:740/2330 train_time:42276ms step_avg:57.13ms
step:741/2330 train_time:42331ms step_avg:57.13ms
step:742/2330 train_time:42390ms step_avg:57.13ms
step:743/2330 train_time:42446ms step_avg:57.13ms
step:744/2330 train_time:42505ms step_avg:57.13ms
step:745/2330 train_time:42560ms step_avg:57.13ms
step:746/2330 train_time:42620ms step_avg:57.13ms
step:747/2330 train_time:42675ms step_avg:57.13ms
step:748/2330 train_time:42735ms step_avg:57.13ms
step:749/2330 train_time:42790ms step_avg:57.13ms
step:750/2330 train_time:42849ms step_avg:57.13ms
step:750/2330 val_loss:4.3474 train_time:42928ms step_avg:57.24ms
step:751/2330 train_time:42946ms step_avg:57.19ms
step:752/2330 train_time:42967ms step_avg:57.14ms
step:753/2330 train_time:43024ms step_avg:57.14ms
step:754/2330 train_time:43087ms step_avg:57.14ms
step:755/2330 train_time:43144ms step_avg:57.14ms
step:756/2330 train_time:43204ms step_avg:57.15ms
step:757/2330 train_time:43259ms step_avg:57.15ms
step:758/2330 train_time:43319ms step_avg:57.15ms
step:759/2330 train_time:43374ms step_avg:57.15ms
step:760/2330 train_time:43433ms step_avg:57.15ms
step:761/2330 train_time:43488ms step_avg:57.15ms
step:762/2330 train_time:43547ms step_avg:57.15ms
step:763/2330 train_time:43602ms step_avg:57.15ms
step:764/2330 train_time:43660ms step_avg:57.15ms
step:765/2330 train_time:43717ms step_avg:57.15ms
step:766/2330 train_time:43775ms step_avg:57.15ms
step:767/2330 train_time:43831ms step_avg:57.15ms
step:768/2330 train_time:43890ms step_avg:57.15ms
step:769/2330 train_time:43947ms step_avg:57.15ms
step:770/2330 train_time:44007ms step_avg:57.15ms
step:771/2330 train_time:44064ms step_avg:57.15ms
step:772/2330 train_time:44123ms step_avg:57.15ms
step:773/2330 train_time:44180ms step_avg:57.15ms
step:774/2330 train_time:44241ms step_avg:57.16ms
step:775/2330 train_time:44297ms step_avg:57.16ms
step:776/2330 train_time:44357ms step_avg:57.16ms
step:777/2330 train_time:44413ms step_avg:57.16ms
step:778/2330 train_time:44473ms step_avg:57.16ms
step:779/2330 train_time:44529ms step_avg:57.16ms
step:780/2330 train_time:44589ms step_avg:57.17ms
step:781/2330 train_time:44645ms step_avg:57.16ms
step:782/2330 train_time:44704ms step_avg:57.17ms
step:783/2330 train_time:44760ms step_avg:57.17ms
step:784/2330 train_time:44818ms step_avg:57.17ms
step:785/2330 train_time:44874ms step_avg:57.16ms
step:786/2330 train_time:44935ms step_avg:57.17ms
step:787/2330 train_time:44992ms step_avg:57.17ms
step:788/2330 train_time:45052ms step_avg:57.17ms
step:789/2330 train_time:45109ms step_avg:57.17ms
step:790/2330 train_time:45169ms step_avg:57.18ms
step:791/2330 train_time:45225ms step_avg:57.17ms
step:792/2330 train_time:45285ms step_avg:57.18ms
step:793/2330 train_time:45341ms step_avg:57.18ms
step:794/2330 train_time:45400ms step_avg:57.18ms
step:795/2330 train_time:45457ms step_avg:57.18ms
step:796/2330 train_time:45516ms step_avg:57.18ms
step:797/2330 train_time:45572ms step_avg:57.18ms
step:798/2330 train_time:45632ms step_avg:57.18ms
step:799/2330 train_time:45689ms step_avg:57.18ms
step:800/2330 train_time:45748ms step_avg:57.18ms
step:801/2330 train_time:45804ms step_avg:57.18ms
step:802/2330 train_time:45862ms step_avg:57.19ms
step:803/2330 train_time:45919ms step_avg:57.18ms
step:804/2330 train_time:45979ms step_avg:57.19ms
step:805/2330 train_time:46035ms step_avg:57.19ms
step:806/2330 train_time:46095ms step_avg:57.19ms
step:807/2330 train_time:46152ms step_avg:57.19ms
step:808/2330 train_time:46212ms step_avg:57.19ms
step:809/2330 train_time:46269ms step_avg:57.19ms
step:810/2330 train_time:46329ms step_avg:57.20ms
step:811/2330 train_time:46385ms step_avg:57.20ms
step:812/2330 train_time:46445ms step_avg:57.20ms
step:813/2330 train_time:46501ms step_avg:57.20ms
step:814/2330 train_time:46560ms step_avg:57.20ms
step:815/2330 train_time:46616ms step_avg:57.20ms
step:816/2330 train_time:46676ms step_avg:57.20ms
step:817/2330 train_time:46732ms step_avg:57.20ms
step:818/2330 train_time:46792ms step_avg:57.20ms
step:819/2330 train_time:46848ms step_avg:57.20ms
step:820/2330 train_time:46908ms step_avg:57.20ms
step:821/2330 train_time:46965ms step_avg:57.20ms
step:822/2330 train_time:47024ms step_avg:57.21ms
step:823/2330 train_time:47081ms step_avg:57.21ms
step:824/2330 train_time:47140ms step_avg:57.21ms
step:825/2330 train_time:47197ms step_avg:57.21ms
step:826/2330 train_time:47257ms step_avg:57.21ms
step:827/2330 train_time:47314ms step_avg:57.21ms
step:828/2330 train_time:47373ms step_avg:57.21ms
step:829/2330 train_time:47430ms step_avg:57.21ms
step:830/2330 train_time:47490ms step_avg:57.22ms
step:831/2330 train_time:47547ms step_avg:57.22ms
step:832/2330 train_time:47606ms step_avg:57.22ms
step:833/2330 train_time:47663ms step_avg:57.22ms
step:834/2330 train_time:47721ms step_avg:57.22ms
step:835/2330 train_time:47778ms step_avg:57.22ms
step:836/2330 train_time:47837ms step_avg:57.22ms
step:837/2330 train_time:47894ms step_avg:57.22ms
step:838/2330 train_time:47953ms step_avg:57.22ms
step:839/2330 train_time:48010ms step_avg:57.22ms
step:840/2330 train_time:48070ms step_avg:57.23ms
step:841/2330 train_time:48128ms step_avg:57.23ms
step:842/2330 train_time:48187ms step_avg:57.23ms
step:843/2330 train_time:48245ms step_avg:57.23ms
step:844/2330 train_time:48304ms step_avg:57.23ms
step:845/2330 train_time:48360ms step_avg:57.23ms
step:846/2330 train_time:48418ms step_avg:57.23ms
step:847/2330 train_time:48474ms step_avg:57.23ms
step:848/2330 train_time:48535ms step_avg:57.23ms
step:849/2330 train_time:48591ms step_avg:57.23ms
step:850/2330 train_time:48651ms step_avg:57.24ms
step:851/2330 train_time:48708ms step_avg:57.24ms
step:852/2330 train_time:48767ms step_avg:57.24ms
step:853/2330 train_time:48823ms step_avg:57.24ms
step:854/2330 train_time:48882ms step_avg:57.24ms
step:855/2330 train_time:48938ms step_avg:57.24ms
step:856/2330 train_time:48998ms step_avg:57.24ms
step:857/2330 train_time:49054ms step_avg:57.24ms
step:858/2330 train_time:49114ms step_avg:57.24ms
step:859/2330 train_time:49172ms step_avg:57.24ms
step:860/2330 train_time:49231ms step_avg:57.25ms
step:861/2330 train_time:49288ms step_avg:57.25ms
step:862/2330 train_time:49348ms step_avg:57.25ms
step:863/2330 train_time:49405ms step_avg:57.25ms
step:864/2330 train_time:49463ms step_avg:57.25ms
step:865/2330 train_time:49520ms step_avg:57.25ms
step:866/2330 train_time:49579ms step_avg:57.25ms
step:867/2330 train_time:49635ms step_avg:57.25ms
step:868/2330 train_time:49696ms step_avg:57.25ms
step:869/2330 train_time:49752ms step_avg:57.25ms
step:870/2330 train_time:49811ms step_avg:57.25ms
step:871/2330 train_time:49868ms step_avg:57.25ms
step:872/2330 train_time:49927ms step_avg:57.26ms
step:873/2330 train_time:49984ms step_avg:57.26ms
step:874/2330 train_time:50044ms step_avg:57.26ms
step:875/2330 train_time:50100ms step_avg:57.26ms
step:876/2330 train_time:50160ms step_avg:57.26ms
step:877/2330 train_time:50216ms step_avg:57.26ms
step:878/2330 train_time:50277ms step_avg:57.26ms
step:879/2330 train_time:50333ms step_avg:57.26ms
step:880/2330 train_time:50393ms step_avg:57.26ms
step:881/2330 train_time:50450ms step_avg:57.26ms
step:882/2330 train_time:50509ms step_avg:57.27ms
step:883/2330 train_time:50566ms step_avg:57.27ms
step:884/2330 train_time:50625ms step_avg:57.27ms
step:885/2330 train_time:50682ms step_avg:57.27ms
step:886/2330 train_time:50741ms step_avg:57.27ms
step:887/2330 train_time:50797ms step_avg:57.27ms
step:888/2330 train_time:50857ms step_avg:57.27ms
step:889/2330 train_time:50913ms step_avg:57.27ms
step:890/2330 train_time:50973ms step_avg:57.27ms
step:891/2330 train_time:51029ms step_avg:57.27ms
step:892/2330 train_time:51090ms step_avg:57.28ms
step:893/2330 train_time:51147ms step_avg:57.28ms
step:894/2330 train_time:51206ms step_avg:57.28ms
step:895/2330 train_time:51263ms step_avg:57.28ms
step:896/2330 train_time:51322ms step_avg:57.28ms
step:897/2330 train_time:51379ms step_avg:57.28ms
step:898/2330 train_time:51439ms step_avg:57.28ms
step:899/2330 train_time:51495ms step_avg:57.28ms
step:900/2330 train_time:51555ms step_avg:57.28ms
step:901/2330 train_time:51612ms step_avg:57.28ms
step:902/2330 train_time:51671ms step_avg:57.29ms
step:903/2330 train_time:51728ms step_avg:57.28ms
step:904/2330 train_time:51787ms step_avg:57.29ms
step:905/2330 train_time:51843ms step_avg:57.29ms
step:906/2330 train_time:51902ms step_avg:57.29ms
step:907/2330 train_time:51959ms step_avg:57.29ms
step:908/2330 train_time:52017ms step_avg:57.29ms
step:909/2330 train_time:52074ms step_avg:57.29ms
step:910/2330 train_time:52134ms step_avg:57.29ms
step:911/2330 train_time:52191ms step_avg:57.29ms
step:912/2330 train_time:52251ms step_avg:57.29ms
step:913/2330 train_time:52308ms step_avg:57.29ms
step:914/2330 train_time:52367ms step_avg:57.29ms
step:915/2330 train_time:52424ms step_avg:57.29ms
step:916/2330 train_time:52483ms step_avg:57.30ms
step:917/2330 train_time:52540ms step_avg:57.30ms
step:918/2330 train_time:52599ms step_avg:57.30ms
step:919/2330 train_time:52655ms step_avg:57.30ms
step:920/2330 train_time:52714ms step_avg:57.30ms
step:921/2330 train_time:52770ms step_avg:57.30ms
step:922/2330 train_time:52830ms step_avg:57.30ms
step:923/2330 train_time:52887ms step_avg:57.30ms
step:924/2330 train_time:52946ms step_avg:57.30ms
step:925/2330 train_time:53002ms step_avg:57.30ms
step:926/2330 train_time:53061ms step_avg:57.30ms
step:927/2330 train_time:53117ms step_avg:57.30ms
step:928/2330 train_time:53178ms step_avg:57.30ms
step:929/2330 train_time:53234ms step_avg:57.30ms
step:930/2330 train_time:53294ms step_avg:57.30ms
step:931/2330 train_time:53351ms step_avg:57.30ms
step:932/2330 train_time:53410ms step_avg:57.31ms
step:933/2330 train_time:53467ms step_avg:57.31ms
step:934/2330 train_time:53526ms step_avg:57.31ms
step:935/2330 train_time:53583ms step_avg:57.31ms
step:936/2330 train_time:53642ms step_avg:57.31ms
step:937/2330 train_time:53698ms step_avg:57.31ms
step:938/2330 train_time:53757ms step_avg:57.31ms
step:939/2330 train_time:53814ms step_avg:57.31ms
step:940/2330 train_time:53874ms step_avg:57.31ms
step:941/2330 train_time:53930ms step_avg:57.31ms
step:942/2330 train_time:53989ms step_avg:57.31ms
step:943/2330 train_time:54046ms step_avg:57.31ms
step:944/2330 train_time:54105ms step_avg:57.31ms
step:945/2330 train_time:54162ms step_avg:57.31ms
step:946/2330 train_time:54221ms step_avg:57.32ms
step:947/2330 train_time:54279ms step_avg:57.32ms
step:948/2330 train_time:54338ms step_avg:57.32ms
step:949/2330 train_time:54394ms step_avg:57.32ms
step:950/2330 train_time:54453ms step_avg:57.32ms
step:951/2330 train_time:54510ms step_avg:57.32ms
step:952/2330 train_time:54570ms step_avg:57.32ms
step:953/2330 train_time:54627ms step_avg:57.32ms
step:954/2330 train_time:54686ms step_avg:57.32ms
step:955/2330 train_time:54742ms step_avg:57.32ms
step:956/2330 train_time:54801ms step_avg:57.32ms
step:957/2330 train_time:54858ms step_avg:57.32ms
step:958/2330 train_time:54918ms step_avg:57.33ms
step:959/2330 train_time:54974ms step_avg:57.32ms
step:960/2330 train_time:55034ms step_avg:57.33ms
step:961/2330 train_time:55091ms step_avg:57.33ms
step:962/2330 train_time:55151ms step_avg:57.33ms
step:963/2330 train_time:55207ms step_avg:57.33ms
step:964/2330 train_time:55267ms step_avg:57.33ms
step:965/2330 train_time:55323ms step_avg:57.33ms
step:966/2330 train_time:55383ms step_avg:57.33ms
step:967/2330 train_time:55439ms step_avg:57.33ms
step:968/2330 train_time:55498ms step_avg:57.33ms
step:969/2330 train_time:55554ms step_avg:57.33ms
step:970/2330 train_time:55615ms step_avg:57.33ms
step:971/2330 train_time:55671ms step_avg:57.33ms
step:972/2330 train_time:55731ms step_avg:57.34ms
step:973/2330 train_time:55788ms step_avg:57.34ms
step:974/2330 train_time:55847ms step_avg:57.34ms
step:975/2330 train_time:55904ms step_avg:57.34ms
step:976/2330 train_time:55963ms step_avg:57.34ms
step:977/2330 train_time:56019ms step_avg:57.34ms
step:978/2330 train_time:56079ms step_avg:57.34ms
step:979/2330 train_time:56135ms step_avg:57.34ms
step:980/2330 train_time:56195ms step_avg:57.34ms
step:981/2330 train_time:56251ms step_avg:57.34ms
step:982/2330 train_time:56310ms step_avg:57.34ms
step:983/2330 train_time:56366ms step_avg:57.34ms
step:984/2330 train_time:56426ms step_avg:57.34ms
step:985/2330 train_time:56483ms step_avg:57.34ms
step:986/2330 train_time:56542ms step_avg:57.34ms
step:987/2330 train_time:56598ms step_avg:57.34ms
step:988/2330 train_time:56658ms step_avg:57.35ms
step:989/2330 train_time:56715ms step_avg:57.35ms
step:990/2330 train_time:56775ms step_avg:57.35ms
step:991/2330 train_time:56832ms step_avg:57.35ms
step:992/2330 train_time:56891ms step_avg:57.35ms
step:993/2330 train_time:56948ms step_avg:57.35ms
step:994/2330 train_time:57008ms step_avg:57.35ms
step:995/2330 train_time:57065ms step_avg:57.35ms
step:996/2330 train_time:57124ms step_avg:57.35ms
step:997/2330 train_time:57181ms step_avg:57.35ms
step:998/2330 train_time:57240ms step_avg:57.35ms
step:999/2330 train_time:57296ms step_avg:57.35ms
step:1000/2330 train_time:57357ms step_avg:57.36ms
step:1000/2330 val_loss:4.1902 train_time:57436ms step_avg:57.44ms
step:1001/2330 train_time:57455ms step_avg:57.40ms
step:1002/2330 train_time:57474ms step_avg:57.36ms
step:1003/2330 train_time:57528ms step_avg:57.36ms
step:1004/2330 train_time:57597ms step_avg:57.37ms
step:1005/2330 train_time:57652ms step_avg:57.37ms
step:1006/2330 train_time:57714ms step_avg:57.37ms
step:1007/2330 train_time:57770ms step_avg:57.37ms
step:1008/2330 train_time:57829ms step_avg:57.37ms
step:1009/2330 train_time:57885ms step_avg:57.37ms
step:1010/2330 train_time:57943ms step_avg:57.37ms
step:1011/2330 train_time:57999ms step_avg:57.37ms
step:1012/2330 train_time:58058ms step_avg:57.37ms
step:1013/2330 train_time:58114ms step_avg:57.37ms
step:1014/2330 train_time:58173ms step_avg:57.37ms
step:1015/2330 train_time:58229ms step_avg:57.37ms
step:1016/2330 train_time:58287ms step_avg:57.37ms
step:1017/2330 train_time:58344ms step_avg:57.37ms
step:1018/2330 train_time:58405ms step_avg:57.37ms
step:1019/2330 train_time:58463ms step_avg:57.37ms
step:1020/2330 train_time:58523ms step_avg:57.38ms
step:1021/2330 train_time:58579ms step_avg:57.37ms
step:1022/2330 train_time:58641ms step_avg:57.38ms
step:1023/2330 train_time:58697ms step_avg:57.38ms
step:1024/2330 train_time:58757ms step_avg:57.38ms
step:1025/2330 train_time:58814ms step_avg:57.38ms
step:1026/2330 train_time:58873ms step_avg:57.38ms
step:1027/2330 train_time:58930ms step_avg:57.38ms
step:1028/2330 train_time:58989ms step_avg:57.38ms
step:1029/2330 train_time:59044ms step_avg:57.38ms
step:1030/2330 train_time:59104ms step_avg:57.38ms
step:1031/2330 train_time:59160ms step_avg:57.38ms
step:1032/2330 train_time:59220ms step_avg:57.38ms
step:1033/2330 train_time:59276ms step_avg:57.38ms
step:1034/2330 train_time:59336ms step_avg:57.38ms
step:1035/2330 train_time:59393ms step_avg:57.38ms
step:1036/2330 train_time:59453ms step_avg:57.39ms
step:1037/2330 train_time:59511ms step_avg:57.39ms
step:1038/2330 train_time:59571ms step_avg:57.39ms
step:1039/2330 train_time:59628ms step_avg:57.39ms
step:1040/2330 train_time:59687ms step_avg:57.39ms
step:1041/2330 train_time:59743ms step_avg:57.39ms
step:1042/2330 train_time:59804ms step_avg:57.39ms
step:1043/2330 train_time:59860ms step_avg:57.39ms
step:1044/2330 train_time:59920ms step_avg:57.39ms
step:1045/2330 train_time:59976ms step_avg:57.39ms
step:1046/2330 train_time:60036ms step_avg:57.40ms
step:1047/2330 train_time:60092ms step_avg:57.39ms
step:1048/2330 train_time:60151ms step_avg:57.40ms
step:1049/2330 train_time:60208ms step_avg:57.40ms
step:1050/2330 train_time:60267ms step_avg:57.40ms
step:1051/2330 train_time:60323ms step_avg:57.40ms
step:1052/2330 train_time:60382ms step_avg:57.40ms
step:1053/2330 train_time:60439ms step_avg:57.40ms
step:1054/2330 train_time:60499ms step_avg:57.40ms
step:1055/2330 train_time:60556ms step_avg:57.40ms
step:1056/2330 train_time:60616ms step_avg:57.40ms
step:1057/2330 train_time:60672ms step_avg:57.40ms
step:1058/2330 train_time:60734ms step_avg:57.40ms
step:1059/2330 train_time:60790ms step_avg:57.40ms
step:1060/2330 train_time:60851ms step_avg:57.41ms
step:1061/2330 train_time:60907ms step_avg:57.41ms
step:1062/2330 train_time:60968ms step_avg:57.41ms
step:1063/2330 train_time:61024ms step_avg:57.41ms
step:1064/2330 train_time:61082ms step_avg:57.41ms
step:1065/2330 train_time:61139ms step_avg:57.41ms
step:1066/2330 train_time:61198ms step_avg:57.41ms
step:1067/2330 train_time:61255ms step_avg:57.41ms
step:1068/2330 train_time:61314ms step_avg:57.41ms
step:1069/2330 train_time:61371ms step_avg:57.41ms
step:1070/2330 train_time:61430ms step_avg:57.41ms
step:1071/2330 train_time:61487ms step_avg:57.41ms
step:1072/2330 train_time:61547ms step_avg:57.41ms
step:1073/2330 train_time:61603ms step_avg:57.41ms
step:1074/2330 train_time:61664ms step_avg:57.42ms
step:1075/2330 train_time:61721ms step_avg:57.41ms
step:1076/2330 train_time:61780ms step_avg:57.42ms
step:1077/2330 train_time:61837ms step_avg:57.42ms
step:1078/2330 train_time:61896ms step_avg:57.42ms
step:1079/2330 train_time:61952ms step_avg:57.42ms
step:1080/2330 train_time:62011ms step_avg:57.42ms
step:1081/2330 train_time:62068ms step_avg:57.42ms
step:1082/2330 train_time:62127ms step_avg:57.42ms
step:1083/2330 train_time:62183ms step_avg:57.42ms
step:1084/2330 train_time:62242ms step_avg:57.42ms
step:1085/2330 train_time:62298ms step_avg:57.42ms
step:1086/2330 train_time:62358ms step_avg:57.42ms
step:1087/2330 train_time:62415ms step_avg:57.42ms
step:1088/2330 train_time:62474ms step_avg:57.42ms
step:1089/2330 train_time:62531ms step_avg:57.42ms
step:1090/2330 train_time:62592ms step_avg:57.42ms
step:1091/2330 train_time:62648ms step_avg:57.42ms
step:1092/2330 train_time:62708ms step_avg:57.43ms
step:1093/2330 train_time:62765ms step_avg:57.42ms
step:1094/2330 train_time:62824ms step_avg:57.43ms
step:1095/2330 train_time:62881ms step_avg:57.43ms
step:1096/2330 train_time:62941ms step_avg:57.43ms
step:1097/2330 train_time:62997ms step_avg:57.43ms
step:1098/2330 train_time:63056ms step_avg:57.43ms
step:1099/2330 train_time:63113ms step_avg:57.43ms
step:1100/2330 train_time:63172ms step_avg:57.43ms
step:1101/2330 train_time:63228ms step_avg:57.43ms
step:1102/2330 train_time:63287ms step_avg:57.43ms
step:1103/2330 train_time:63343ms step_avg:57.43ms
step:1104/2330 train_time:63404ms step_avg:57.43ms
step:1105/2330 train_time:63460ms step_avg:57.43ms
step:1106/2330 train_time:63519ms step_avg:57.43ms
step:1107/2330 train_time:63575ms step_avg:57.43ms
step:1108/2330 train_time:63636ms step_avg:57.43ms
step:1109/2330 train_time:63693ms step_avg:57.43ms
step:1110/2330 train_time:63753ms step_avg:57.44ms
step:1111/2330 train_time:63811ms step_avg:57.44ms
step:1112/2330 train_time:63871ms step_avg:57.44ms
step:1113/2330 train_time:63927ms step_avg:57.44ms
step:1114/2330 train_time:63986ms step_avg:57.44ms
step:1115/2330 train_time:64043ms step_avg:57.44ms
step:1116/2330 train_time:64102ms step_avg:57.44ms
step:1117/2330 train_time:64159ms step_avg:57.44ms
step:1118/2330 train_time:64218ms step_avg:57.44ms
step:1119/2330 train_time:64275ms step_avg:57.44ms
step:1120/2330 train_time:64334ms step_avg:57.44ms
step:1121/2330 train_time:64390ms step_avg:57.44ms
step:1122/2330 train_time:64450ms step_avg:57.44ms
step:1123/2330 train_time:64506ms step_avg:57.44ms
step:1124/2330 train_time:64565ms step_avg:57.44ms
step:1125/2330 train_time:64622ms step_avg:57.44ms
step:1126/2330 train_time:64682ms step_avg:57.44ms
step:1127/2330 train_time:64738ms step_avg:57.44ms
step:1128/2330 train_time:64798ms step_avg:57.45ms
step:1129/2330 train_time:64854ms step_avg:57.44ms
step:1130/2330 train_time:64915ms step_avg:57.45ms
step:1131/2330 train_time:64972ms step_avg:57.45ms
step:1132/2330 train_time:65033ms step_avg:57.45ms
step:1133/2330 train_time:65089ms step_avg:57.45ms
step:1134/2330 train_time:65149ms step_avg:57.45ms
step:1135/2330 train_time:65206ms step_avg:57.45ms
step:1136/2330 train_time:65265ms step_avg:57.45ms
step:1137/2330 train_time:65321ms step_avg:57.45ms
step:1138/2330 train_time:65381ms step_avg:57.45ms
step:1139/2330 train_time:65438ms step_avg:57.45ms
step:1140/2330 train_time:65497ms step_avg:57.45ms
step:1141/2330 train_time:65553ms step_avg:57.45ms
step:1142/2330 train_time:65615ms step_avg:57.46ms
step:1143/2330 train_time:65671ms step_avg:57.45ms
step:1144/2330 train_time:65730ms step_avg:57.46ms
step:1145/2330 train_time:65786ms step_avg:57.46ms
step:1146/2330 train_time:65846ms step_avg:57.46ms
step:1147/2330 train_time:65902ms step_avg:57.46ms
step:1148/2330 train_time:65962ms step_avg:57.46ms
step:1149/2330 train_time:66018ms step_avg:57.46ms
step:1150/2330 train_time:66078ms step_avg:57.46ms
step:1151/2330 train_time:66135ms step_avg:57.46ms
step:1152/2330 train_time:66194ms step_avg:57.46ms
step:1153/2330 train_time:66250ms step_avg:57.46ms
step:1154/2330 train_time:66310ms step_avg:57.46ms
step:1155/2330 train_time:66366ms step_avg:57.46ms
step:1156/2330 train_time:66425ms step_avg:57.46ms
step:1157/2330 train_time:66482ms step_avg:57.46ms
step:1158/2330 train_time:66541ms step_avg:57.46ms
step:1159/2330 train_time:66597ms step_avg:57.46ms
step:1160/2330 train_time:66658ms step_avg:57.46ms
step:1161/2330 train_time:66714ms step_avg:57.46ms
step:1162/2330 train_time:66774ms step_avg:57.46ms
step:1163/2330 train_time:66831ms step_avg:57.46ms
step:1164/2330 train_time:66891ms step_avg:57.47ms
step:1165/2330 train_time:66949ms step_avg:57.47ms
step:1166/2330 train_time:67008ms step_avg:57.47ms
step:1167/2330 train_time:67064ms step_avg:57.47ms
step:1168/2330 train_time:67124ms step_avg:57.47ms
step:1169/2330 train_time:67180ms step_avg:57.47ms
step:1170/2330 train_time:67240ms step_avg:57.47ms
step:1171/2330 train_time:67297ms step_avg:57.47ms
step:1172/2330 train_time:67356ms step_avg:57.47ms
step:1173/2330 train_time:67413ms step_avg:57.47ms
step:1174/2330 train_time:67472ms step_avg:57.47ms
step:1175/2330 train_time:67529ms step_avg:57.47ms
step:1176/2330 train_time:67588ms step_avg:57.47ms
step:1177/2330 train_time:67644ms step_avg:57.47ms
step:1178/2330 train_time:67705ms step_avg:57.47ms
step:1179/2330 train_time:67762ms step_avg:57.47ms
step:1180/2330 train_time:67821ms step_avg:57.48ms
step:1181/2330 train_time:67877ms step_avg:57.47ms
step:1182/2330 train_time:67938ms step_avg:57.48ms
step:1183/2330 train_time:67994ms step_avg:57.48ms
step:1184/2330 train_time:68054ms step_avg:57.48ms
step:1185/2330 train_time:68111ms step_avg:57.48ms
step:1186/2330 train_time:68170ms step_avg:57.48ms
step:1187/2330 train_time:68227ms step_avg:57.48ms
step:1188/2330 train_time:68286ms step_avg:57.48ms
step:1189/2330 train_time:68341ms step_avg:57.48ms
step:1190/2330 train_time:68400ms step_avg:57.48ms
step:1191/2330 train_time:68457ms step_avg:57.48ms
step:1192/2330 train_time:68517ms step_avg:57.48ms
step:1193/2330 train_time:68574ms step_avg:57.48ms
step:1194/2330 train_time:68634ms step_avg:57.48ms
step:1195/2330 train_time:68691ms step_avg:57.48ms
step:1196/2330 train_time:68751ms step_avg:57.48ms
step:1197/2330 train_time:68807ms step_avg:57.48ms
step:1198/2330 train_time:68867ms step_avg:57.48ms
step:1199/2330 train_time:68923ms step_avg:57.48ms
step:1200/2330 train_time:68982ms step_avg:57.49ms
step:1201/2330 train_time:69038ms step_avg:57.48ms
step:1202/2330 train_time:69100ms step_avg:57.49ms
step:1203/2330 train_time:69157ms step_avg:57.49ms
step:1204/2330 train_time:69217ms step_avg:57.49ms
step:1205/2330 train_time:69273ms step_avg:57.49ms
step:1206/2330 train_time:69332ms step_avg:57.49ms
step:1207/2330 train_time:69389ms step_avg:57.49ms
step:1208/2330 train_time:69449ms step_avg:57.49ms
step:1209/2330 train_time:69505ms step_avg:57.49ms
step:1210/2330 train_time:69564ms step_avg:57.49ms
step:1211/2330 train_time:69621ms step_avg:57.49ms
step:1212/2330 train_time:69681ms step_avg:57.49ms
step:1213/2330 train_time:69736ms step_avg:57.49ms
step:1214/2330 train_time:69797ms step_avg:57.49ms
step:1215/2330 train_time:69854ms step_avg:57.49ms
step:1216/2330 train_time:69915ms step_avg:57.50ms
step:1217/2330 train_time:69972ms step_avg:57.50ms
step:1218/2330 train_time:70032ms step_avg:57.50ms
step:1219/2330 train_time:70089ms step_avg:57.50ms
step:1220/2330 train_time:70148ms step_avg:57.50ms
step:1221/2330 train_time:70204ms step_avg:57.50ms
step:1222/2330 train_time:70263ms step_avg:57.50ms
step:1223/2330 train_time:70319ms step_avg:57.50ms
step:1224/2330 train_time:70378ms step_avg:57.50ms
step:1225/2330 train_time:70435ms step_avg:57.50ms
step:1226/2330 train_time:70495ms step_avg:57.50ms
step:1227/2330 train_time:70552ms step_avg:57.50ms
step:1228/2330 train_time:70611ms step_avg:57.50ms
step:1229/2330 train_time:70668ms step_avg:57.50ms
step:1230/2330 train_time:70727ms step_avg:57.50ms
step:1231/2330 train_time:70784ms step_avg:57.50ms
step:1232/2330 train_time:70843ms step_avg:57.50ms
step:1233/2330 train_time:70899ms step_avg:57.50ms
step:1234/2330 train_time:70959ms step_avg:57.50ms
step:1235/2330 train_time:71016ms step_avg:57.50ms
step:1236/2330 train_time:71075ms step_avg:57.50ms
step:1237/2330 train_time:71132ms step_avg:57.50ms
step:1238/2330 train_time:71191ms step_avg:57.51ms
step:1239/2330 train_time:71247ms step_avg:57.50ms
step:1240/2330 train_time:71307ms step_avg:57.51ms
step:1241/2330 train_time:71364ms step_avg:57.51ms
step:1242/2330 train_time:71423ms step_avg:57.51ms
step:1243/2330 train_time:71478ms step_avg:57.50ms
step:1244/2330 train_time:71538ms step_avg:57.51ms
step:1245/2330 train_time:71594ms step_avg:57.51ms
step:1246/2330 train_time:71655ms step_avg:57.51ms
step:1247/2330 train_time:71712ms step_avg:57.51ms
step:1248/2330 train_time:71772ms step_avg:57.51ms
step:1249/2330 train_time:71828ms step_avg:57.51ms
step:1250/2330 train_time:71889ms step_avg:57.51ms
step:1250/2330 val_loss:4.1112 train_time:71968ms step_avg:57.57ms
step:1251/2330 train_time:71986ms step_avg:57.54ms
step:1252/2330 train_time:72007ms step_avg:57.51ms
step:1253/2330 train_time:72066ms step_avg:57.51ms
step:1254/2330 train_time:72128ms step_avg:57.52ms
step:1255/2330 train_time:72186ms step_avg:57.52ms
step:1256/2330 train_time:72245ms step_avg:57.52ms
step:1257/2330 train_time:72301ms step_avg:57.52ms
step:1258/2330 train_time:72360ms step_avg:57.52ms
step:1259/2330 train_time:72417ms step_avg:57.52ms
step:1260/2330 train_time:72475ms step_avg:57.52ms
step:1261/2330 train_time:72531ms step_avg:57.52ms
step:1262/2330 train_time:72590ms step_avg:57.52ms
step:1263/2330 train_time:72646ms step_avg:57.52ms
step:1264/2330 train_time:72705ms step_avg:57.52ms
step:1265/2330 train_time:72761ms step_avg:57.52ms
step:1266/2330 train_time:72820ms step_avg:57.52ms
step:1267/2330 train_time:72876ms step_avg:57.52ms
step:1268/2330 train_time:72935ms step_avg:57.52ms
step:1269/2330 train_time:72993ms step_avg:57.52ms
step:1270/2330 train_time:73053ms step_avg:57.52ms
step:1271/2330 train_time:73111ms step_avg:57.52ms
step:1272/2330 train_time:73171ms step_avg:57.52ms
step:1273/2330 train_time:73228ms step_avg:57.52ms
step:1274/2330 train_time:73287ms step_avg:57.53ms
step:1275/2330 train_time:73344ms step_avg:57.52ms
step:1276/2330 train_time:73403ms step_avg:57.53ms
step:1277/2330 train_time:73459ms step_avg:57.52ms
step:1278/2330 train_time:73518ms step_avg:57.53ms
step:1279/2330 train_time:73575ms step_avg:57.53ms
step:1280/2330 train_time:73634ms step_avg:57.53ms
step:1281/2330 train_time:73690ms step_avg:57.53ms
step:1282/2330 train_time:73749ms step_avg:57.53ms
step:1283/2330 train_time:73804ms step_avg:57.52ms
step:1284/2330 train_time:73864ms step_avg:57.53ms
step:1285/2330 train_time:73920ms step_avg:57.53ms
step:1286/2330 train_time:73982ms step_avg:57.53ms
step:1287/2330 train_time:74039ms step_avg:57.53ms
step:1288/2330 train_time:74099ms step_avg:57.53ms
step:1289/2330 train_time:74156ms step_avg:57.53ms
step:1290/2330 train_time:74216ms step_avg:57.53ms
step:1291/2330 train_time:74272ms step_avg:57.53ms
step:1292/2330 train_time:74332ms step_avg:57.53ms
step:1293/2330 train_time:74388ms step_avg:57.53ms
step:1294/2330 train_time:74447ms step_avg:57.53ms
step:1295/2330 train_time:74503ms step_avg:57.53ms
step:1296/2330 train_time:74562ms step_avg:57.53ms
step:1297/2330 train_time:74619ms step_avg:57.53ms
step:1298/2330 train_time:74678ms step_avg:57.53ms
step:1299/2330 train_time:74735ms step_avg:57.53ms
step:1300/2330 train_time:74794ms step_avg:57.53ms
step:1301/2330 train_time:74851ms step_avg:57.53ms
step:1302/2330 train_time:74909ms step_avg:57.53ms
step:1303/2330 train_time:74966ms step_avg:57.53ms
step:1304/2330 train_time:75026ms step_avg:57.54ms
step:1305/2330 train_time:75083ms step_avg:57.53ms
step:1306/2330 train_time:75143ms step_avg:57.54ms
step:1307/2330 train_time:75199ms step_avg:57.54ms
step:1308/2330 train_time:75260ms step_avg:57.54ms
step:1309/2330 train_time:75316ms step_avg:57.54ms
step:1310/2330 train_time:75376ms step_avg:57.54ms
step:1311/2330 train_time:75433ms step_avg:57.54ms
step:1312/2330 train_time:75492ms step_avg:57.54ms
step:1313/2330 train_time:75548ms step_avg:57.54ms
step:1314/2330 train_time:75607ms step_avg:57.54ms
step:1315/2330 train_time:75663ms step_avg:57.54ms
step:1316/2330 train_time:75723ms step_avg:57.54ms
step:1317/2330 train_time:75780ms step_avg:57.54ms
step:1318/2330 train_time:75839ms step_avg:57.54ms
step:1319/2330 train_time:75895ms step_avg:57.54ms
step:1320/2330 train_time:75955ms step_avg:57.54ms
step:1321/2330 train_time:76012ms step_avg:57.54ms
step:1322/2330 train_time:76071ms step_avg:57.54ms
step:1323/2330 train_time:76128ms step_avg:57.54ms
step:1324/2330 train_time:76187ms step_avg:57.54ms
step:1325/2330 train_time:76243ms step_avg:57.54ms
step:1326/2330 train_time:76304ms step_avg:57.54ms
step:1327/2330 train_time:76360ms step_avg:57.54ms
step:1328/2330 train_time:76421ms step_avg:57.55ms
step:1329/2330 train_time:76478ms step_avg:57.55ms
step:1330/2330 train_time:76537ms step_avg:57.55ms
step:1331/2330 train_time:76594ms step_avg:57.55ms
step:1332/2330 train_time:76653ms step_avg:57.55ms
step:1333/2330 train_time:76710ms step_avg:57.55ms
step:1334/2330 train_time:76768ms step_avg:57.55ms
step:1335/2330 train_time:76825ms step_avg:57.55ms
step:1336/2330 train_time:76885ms step_avg:57.55ms
step:1337/2330 train_time:76941ms step_avg:57.55ms
step:1338/2330 train_time:77001ms step_avg:57.55ms
step:1339/2330 train_time:77058ms step_avg:57.55ms
step:1340/2330 train_time:77117ms step_avg:57.55ms
step:1341/2330 train_time:77173ms step_avg:57.55ms
step:1342/2330 train_time:77233ms step_avg:57.55ms
step:1343/2330 train_time:77290ms step_avg:57.55ms
step:1344/2330 train_time:77349ms step_avg:57.55ms
step:1345/2330 train_time:77405ms step_avg:57.55ms
step:1346/2330 train_time:77465ms step_avg:57.55ms
step:1347/2330 train_time:77521ms step_avg:57.55ms
step:1348/2330 train_time:77581ms step_avg:57.55ms
step:1349/2330 train_time:77638ms step_avg:57.55ms
step:1350/2330 train_time:77698ms step_avg:57.55ms
step:1351/2330 train_time:77755ms step_avg:57.55ms
step:1352/2330 train_time:77815ms step_avg:57.56ms
step:1353/2330 train_time:77871ms step_avg:57.55ms
step:1354/2330 train_time:77931ms step_avg:57.56ms
step:1355/2330 train_time:77988ms step_avg:57.56ms
step:1356/2330 train_time:78047ms step_avg:57.56ms
step:1357/2330 train_time:78103ms step_avg:57.56ms
step:1358/2330 train_time:78163ms step_avg:57.56ms
step:1359/2330 train_time:78220ms step_avg:57.56ms
step:1360/2330 train_time:78280ms step_avg:57.56ms
step:1361/2330 train_time:78337ms step_avg:57.56ms
step:1362/2330 train_time:78397ms step_avg:57.56ms
step:1363/2330 train_time:78454ms step_avg:57.56ms
step:1364/2330 train_time:78513ms step_avg:57.56ms
step:1365/2330 train_time:78569ms step_avg:57.56ms
step:1366/2330 train_time:78629ms step_avg:57.56ms
step:1367/2330 train_time:78685ms step_avg:57.56ms
step:1368/2330 train_time:78745ms step_avg:57.56ms
step:1369/2330 train_time:78801ms step_avg:57.56ms
step:1370/2330 train_time:78861ms step_avg:57.56ms
step:1371/2330 train_time:78918ms step_avg:57.56ms
step:1372/2330 train_time:78978ms step_avg:57.56ms
step:1373/2330 train_time:79035ms step_avg:57.56ms
step:1374/2330 train_time:79094ms step_avg:57.56ms
step:1375/2330 train_time:79151ms step_avg:57.56ms
step:1376/2330 train_time:79209ms step_avg:57.57ms
step:1377/2330 train_time:79266ms step_avg:57.56ms
step:1378/2330 train_time:79325ms step_avg:57.57ms
step:1379/2330 train_time:79382ms step_avg:57.56ms
step:1380/2330 train_time:79442ms step_avg:57.57ms
step:1381/2330 train_time:79498ms step_avg:57.57ms
step:1382/2330 train_time:79558ms step_avg:57.57ms
step:1383/2330 train_time:79615ms step_avg:57.57ms
step:1384/2330 train_time:79675ms step_avg:57.57ms
step:1385/2330 train_time:79732ms step_avg:57.57ms
step:1386/2330 train_time:79792ms step_avg:57.57ms
step:1387/2330 train_time:79848ms step_avg:57.57ms
step:1388/2330 train_time:79907ms step_avg:57.57ms
step:1389/2330 train_time:79963ms step_avg:57.57ms
step:1390/2330 train_time:80024ms step_avg:57.57ms
step:1391/2330 train_time:80080ms step_avg:57.57ms
step:1392/2330 train_time:80140ms step_avg:57.57ms
step:1393/2330 train_time:80196ms step_avg:57.57ms
step:1394/2330 train_time:80256ms step_avg:57.57ms
step:1395/2330 train_time:80313ms step_avg:57.57ms
step:1396/2330 train_time:80372ms step_avg:57.57ms
step:1397/2330 train_time:80429ms step_avg:57.57ms
step:1398/2330 train_time:80488ms step_avg:57.57ms
step:1399/2330 train_time:80545ms step_avg:57.57ms
step:1400/2330 train_time:80604ms step_avg:57.57ms
step:1401/2330 train_time:80661ms step_avg:57.57ms
step:1402/2330 train_time:80721ms step_avg:57.58ms
step:1403/2330 train_time:80779ms step_avg:57.58ms
step:1404/2330 train_time:80838ms step_avg:57.58ms
step:1405/2330 train_time:80896ms step_avg:57.58ms
step:1406/2330 train_time:80955ms step_avg:57.58ms
step:1407/2330 train_time:81012ms step_avg:57.58ms
step:1408/2330 train_time:81071ms step_avg:57.58ms
step:1409/2330 train_time:81127ms step_avg:57.58ms
step:1410/2330 train_time:81186ms step_avg:57.58ms
step:1411/2330 train_time:81243ms step_avg:57.58ms
step:1412/2330 train_time:81302ms step_avg:57.58ms
step:1413/2330 train_time:81359ms step_avg:57.58ms
step:1414/2330 train_time:81418ms step_avg:57.58ms
step:1415/2330 train_time:81475ms step_avg:57.58ms
step:1416/2330 train_time:81535ms step_avg:57.58ms
step:1417/2330 train_time:81591ms step_avg:57.58ms
step:1418/2330 train_time:81650ms step_avg:57.58ms
step:1419/2330 train_time:81707ms step_avg:57.58ms
step:1420/2330 train_time:81767ms step_avg:57.58ms
step:1421/2330 train_time:81823ms step_avg:57.58ms
step:1422/2330 train_time:81883ms step_avg:57.58ms
step:1423/2330 train_time:81940ms step_avg:57.58ms
step:1424/2330 train_time:81999ms step_avg:57.58ms
step:1425/2330 train_time:82056ms step_avg:57.58ms
step:1426/2330 train_time:82116ms step_avg:57.58ms
step:1427/2330 train_time:82173ms step_avg:57.58ms
step:1428/2330 train_time:82232ms step_avg:57.59ms
step:1429/2330 train_time:82289ms step_avg:57.59ms
step:1430/2330 train_time:82349ms step_avg:57.59ms
step:1431/2330 train_time:82405ms step_avg:57.59ms
step:1432/2330 train_time:82465ms step_avg:57.59ms
step:1433/2330 train_time:82521ms step_avg:57.59ms
step:1434/2330 train_time:82582ms step_avg:57.59ms
step:1435/2330 train_time:82638ms step_avg:57.59ms
step:1436/2330 train_time:82698ms step_avg:57.59ms
step:1437/2330 train_time:82756ms step_avg:57.59ms
step:1438/2330 train_time:82815ms step_avg:57.59ms
step:1439/2330 train_time:82872ms step_avg:57.59ms
step:1440/2330 train_time:82930ms step_avg:57.59ms
step:1441/2330 train_time:82987ms step_avg:57.59ms
step:1442/2330 train_time:83046ms step_avg:57.59ms
step:1443/2330 train_time:83102ms step_avg:57.59ms
step:1444/2330 train_time:83162ms step_avg:57.59ms
step:1445/2330 train_time:83219ms step_avg:57.59ms
step:1446/2330 train_time:83278ms step_avg:57.59ms
step:1447/2330 train_time:83335ms step_avg:57.59ms
step:1448/2330 train_time:83394ms step_avg:57.59ms
step:1449/2330 train_time:83451ms step_avg:57.59ms
step:1450/2330 train_time:83510ms step_avg:57.59ms
step:1451/2330 train_time:83565ms step_avg:57.59ms
step:1452/2330 train_time:83626ms step_avg:57.59ms
step:1453/2330 train_time:83682ms step_avg:57.59ms
step:1454/2330 train_time:83743ms step_avg:57.59ms
step:1455/2330 train_time:83799ms step_avg:57.59ms
step:1456/2330 train_time:83860ms step_avg:57.60ms
step:1457/2330 train_time:83917ms step_avg:57.60ms
step:1458/2330 train_time:83977ms step_avg:57.60ms
step:1459/2330 train_time:84034ms step_avg:57.60ms
step:1460/2330 train_time:84093ms step_avg:57.60ms
step:1461/2330 train_time:84150ms step_avg:57.60ms
step:1462/2330 train_time:84209ms step_avg:57.60ms
step:1463/2330 train_time:84266ms step_avg:57.60ms
step:1464/2330 train_time:84325ms step_avg:57.60ms
step:1465/2330 train_time:84381ms step_avg:57.60ms
step:1466/2330 train_time:84441ms step_avg:57.60ms
step:1467/2330 train_time:84498ms step_avg:57.60ms
step:1468/2330 train_time:84558ms step_avg:57.60ms
step:1469/2330 train_time:84615ms step_avg:57.60ms
step:1470/2330 train_time:84675ms step_avg:57.60ms
step:1471/2330 train_time:84731ms step_avg:57.60ms
step:1472/2330 train_time:84790ms step_avg:57.60ms
step:1473/2330 train_time:84846ms step_avg:57.60ms
step:1474/2330 train_time:84906ms step_avg:57.60ms
step:1475/2330 train_time:84962ms step_avg:57.60ms
step:1476/2330 train_time:85022ms step_avg:57.60ms
step:1477/2330 train_time:85079ms step_avg:57.60ms
step:1478/2330 train_time:85138ms step_avg:57.60ms
step:1479/2330 train_time:85195ms step_avg:57.60ms
step:1480/2330 train_time:85255ms step_avg:57.60ms
step:1481/2330 train_time:85311ms step_avg:57.60ms
step:1482/2330 train_time:85371ms step_avg:57.60ms
step:1483/2330 train_time:85427ms step_avg:57.60ms
step:1484/2330 train_time:85486ms step_avg:57.61ms
step:1485/2330 train_time:85542ms step_avg:57.60ms
step:1486/2330 train_time:85603ms step_avg:57.61ms
step:1487/2330 train_time:85659ms step_avg:57.61ms
step:1488/2330 train_time:85719ms step_avg:57.61ms
step:1489/2330 train_time:85776ms step_avg:57.61ms
step:1490/2330 train_time:85835ms step_avg:57.61ms
step:1491/2330 train_time:85892ms step_avg:57.61ms
step:1492/2330 train_time:85952ms step_avg:57.61ms
step:1493/2330 train_time:86008ms step_avg:57.61ms
step:1494/2330 train_time:86068ms step_avg:57.61ms
step:1495/2330 train_time:86124ms step_avg:57.61ms
step:1496/2330 train_time:86184ms step_avg:57.61ms
step:1497/2330 train_time:86241ms step_avg:57.61ms
step:1498/2330 train_time:86300ms step_avg:57.61ms
step:1499/2330 train_time:86357ms step_avg:57.61ms
step:1500/2330 train_time:86417ms step_avg:57.61ms
step:1500/2330 val_loss:4.0267 train_time:86496ms step_avg:57.66ms
step:1501/2330 train_time:86513ms step_avg:57.64ms
step:1502/2330 train_time:86535ms step_avg:57.61ms
step:1503/2330 train_time:86593ms step_avg:57.61ms
step:1504/2330 train_time:86659ms step_avg:57.62ms
step:1505/2330 train_time:86717ms step_avg:57.62ms
step:1506/2330 train_time:86779ms step_avg:57.62ms
step:1507/2330 train_time:86835ms step_avg:57.62ms
step:1508/2330 train_time:86894ms step_avg:57.62ms
step:1509/2330 train_time:86950ms step_avg:57.62ms
step:1510/2330 train_time:87009ms step_avg:57.62ms
step:1511/2330 train_time:87065ms step_avg:57.62ms
step:1512/2330 train_time:87124ms step_avg:57.62ms
step:1513/2330 train_time:87180ms step_avg:57.62ms
step:1514/2330 train_time:87238ms step_avg:57.62ms
step:1515/2330 train_time:87294ms step_avg:57.62ms
step:1516/2330 train_time:87353ms step_avg:57.62ms
step:1517/2330 train_time:87409ms step_avg:57.62ms
step:1518/2330 train_time:87469ms step_avg:57.62ms
step:1519/2330 train_time:87527ms step_avg:57.62ms
step:1520/2330 train_time:87587ms step_avg:57.62ms
step:1521/2330 train_time:87644ms step_avg:57.62ms
step:1522/2330 train_time:87705ms step_avg:57.63ms
step:1523/2330 train_time:87762ms step_avg:57.62ms
step:1524/2330 train_time:87823ms step_avg:57.63ms
step:1525/2330 train_time:87879ms step_avg:57.63ms
step:1526/2330 train_time:87938ms step_avg:57.63ms
step:1527/2330 train_time:87995ms step_avg:57.63ms
step:1528/2330 train_time:88055ms step_avg:57.63ms
step:1529/2330 train_time:88112ms step_avg:57.63ms
step:1530/2330 train_time:88170ms step_avg:57.63ms
step:1531/2330 train_time:88227ms step_avg:57.63ms
step:1532/2330 train_time:88286ms step_avg:57.63ms
step:1533/2330 train_time:88342ms step_avg:57.63ms
step:1534/2330 train_time:88402ms step_avg:57.63ms
step:1535/2330 train_time:88459ms step_avg:57.63ms
step:1536/2330 train_time:88520ms step_avg:57.63ms
step:1537/2330 train_time:88577ms step_avg:57.63ms
step:1538/2330 train_time:88638ms step_avg:57.63ms
step:1539/2330 train_time:88696ms step_avg:57.63ms
step:1540/2330 train_time:88757ms step_avg:57.63ms
step:1541/2330 train_time:88814ms step_avg:57.63ms
step:1542/2330 train_time:88874ms step_avg:57.64ms
step:1543/2330 train_time:88932ms step_avg:57.64ms
step:1544/2330 train_time:88992ms step_avg:57.64ms
step:1545/2330 train_time:89049ms step_avg:57.64ms
step:1546/2330 train_time:89109ms step_avg:57.64ms
step:1547/2330 train_time:89166ms step_avg:57.64ms
step:1548/2330 train_time:89225ms step_avg:57.64ms
step:1549/2330 train_time:89282ms step_avg:57.64ms
step:1550/2330 train_time:89341ms step_avg:57.64ms
step:1551/2330 train_time:89398ms step_avg:57.64ms
step:1552/2330 train_time:89458ms step_avg:57.64ms
step:1553/2330 train_time:89515ms step_avg:57.64ms
step:1554/2330 train_time:89575ms step_avg:57.64ms
step:1555/2330 train_time:89632ms step_avg:57.64ms
step:1556/2330 train_time:89693ms step_avg:57.64ms
step:1557/2330 train_time:89751ms step_avg:57.64ms
step:1558/2330 train_time:89811ms step_avg:57.64ms
step:1559/2330 train_time:89868ms step_avg:57.64ms
step:1560/2330 train_time:89928ms step_avg:57.65ms
step:1561/2330 train_time:89985ms step_avg:57.65ms
step:1562/2330 train_time:90044ms step_avg:57.65ms
step:1563/2330 train_time:90101ms step_avg:57.65ms
step:1564/2330 train_time:90161ms step_avg:57.65ms
step:1565/2330 train_time:90218ms step_avg:57.65ms
step:1566/2330 train_time:90278ms step_avg:57.65ms
step:1567/2330 train_time:90335ms step_avg:57.65ms
step:1568/2330 train_time:90394ms step_avg:57.65ms
step:1569/2330 train_time:90451ms step_avg:57.65ms
step:1570/2330 train_time:90511ms step_avg:57.65ms
step:1571/2330 train_time:90568ms step_avg:57.65ms
step:1572/2330 train_time:90628ms step_avg:57.65ms
step:1573/2330 train_time:90686ms step_avg:57.65ms
step:1574/2330 train_time:90746ms step_avg:57.65ms
step:1575/2330 train_time:90803ms step_avg:57.65ms
step:1576/2330 train_time:90864ms step_avg:57.65ms
step:1577/2330 train_time:90921ms step_avg:57.65ms
step:1578/2330 train_time:90982ms step_avg:57.66ms
step:1579/2330 train_time:91038ms step_avg:57.66ms
step:1580/2330 train_time:91098ms step_avg:57.66ms
step:1581/2330 train_time:91155ms step_avg:57.66ms
step:1582/2330 train_time:91215ms step_avg:57.66ms
step:1583/2330 train_time:91273ms step_avg:57.66ms
step:1584/2330 train_time:91334ms step_avg:57.66ms
step:1585/2330 train_time:91390ms step_avg:57.66ms
step:1586/2330 train_time:91449ms step_avg:57.66ms
step:1587/2330 train_time:91506ms step_avg:57.66ms
step:1588/2330 train_time:91566ms step_avg:57.66ms
step:1589/2330 train_time:91623ms step_avg:57.66ms
step:1590/2330 train_time:91683ms step_avg:57.66ms
step:1591/2330 train_time:91740ms step_avg:57.66ms
step:1592/2330 train_time:91800ms step_avg:57.66ms
step:1593/2330 train_time:91857ms step_avg:57.66ms
step:1594/2330 train_time:91919ms step_avg:57.67ms
step:1595/2330 train_time:91976ms step_avg:57.67ms
step:1596/2330 train_time:92036ms step_avg:57.67ms
step:1597/2330 train_time:92093ms step_avg:57.67ms
step:1598/2330 train_time:92153ms step_avg:57.67ms
step:1599/2330 train_time:92210ms step_avg:57.67ms
step:1600/2330 train_time:92271ms step_avg:57.67ms
step:1601/2330 train_time:92328ms step_avg:57.67ms
step:1602/2330 train_time:92388ms step_avg:57.67ms
step:1603/2330 train_time:92444ms step_avg:57.67ms
step:1604/2330 train_time:92503ms step_avg:57.67ms
step:1605/2330 train_time:92560ms step_avg:57.67ms
step:1606/2330 train_time:92620ms step_avg:57.67ms
step:1607/2330 train_time:92677ms step_avg:57.67ms
step:1608/2330 train_time:92737ms step_avg:57.67ms
step:1609/2330 train_time:92794ms step_avg:57.67ms
step:1610/2330 train_time:92854ms step_avg:57.67ms
step:1611/2330 train_time:92912ms step_avg:57.67ms
step:1612/2330 train_time:92972ms step_avg:57.67ms
step:1613/2330 train_time:93030ms step_avg:57.68ms
step:1614/2330 train_time:93089ms step_avg:57.68ms
step:1615/2330 train_time:93146ms step_avg:57.68ms
step:1616/2330 train_time:93206ms step_avg:57.68ms
step:1617/2330 train_time:93263ms step_avg:57.68ms
step:1618/2330 train_time:93323ms step_avg:57.68ms
step:1619/2330 train_time:93380ms step_avg:57.68ms
step:1620/2330 train_time:93439ms step_avg:57.68ms
step:1621/2330 train_time:93496ms step_avg:57.68ms
step:1622/2330 train_time:93557ms step_avg:57.68ms
step:1623/2330 train_time:93614ms step_avg:57.68ms
step:1624/2330 train_time:93674ms step_avg:57.68ms
step:1625/2330 train_time:93731ms step_avg:57.68ms
step:1626/2330 train_time:93790ms step_avg:57.68ms
step:1627/2330 train_time:93848ms step_avg:57.68ms
step:1628/2330 train_time:93908ms step_avg:57.68ms
step:1629/2330 train_time:93965ms step_avg:57.68ms
step:1630/2330 train_time:94025ms step_avg:57.68ms
step:1631/2330 train_time:94082ms step_avg:57.68ms
step:1632/2330 train_time:94142ms step_avg:57.69ms
step:1633/2330 train_time:94199ms step_avg:57.68ms
step:1634/2330 train_time:94259ms step_avg:57.69ms
step:1635/2330 train_time:94316ms step_avg:57.69ms
step:1636/2330 train_time:94378ms step_avg:57.69ms
step:1637/2330 train_time:94435ms step_avg:57.69ms
step:1638/2330 train_time:94494ms step_avg:57.69ms
step:1639/2330 train_time:94551ms step_avg:57.69ms
step:1640/2330 train_time:94611ms step_avg:57.69ms
step:1641/2330 train_time:94669ms step_avg:57.69ms
step:1642/2330 train_time:94729ms step_avg:57.69ms
step:1643/2330 train_time:94786ms step_avg:57.69ms
step:1644/2330 train_time:94845ms step_avg:57.69ms
step:1645/2330 train_time:94902ms step_avg:57.69ms
step:1646/2330 train_time:94962ms step_avg:57.69ms
step:1647/2330 train_time:95019ms step_avg:57.69ms
step:1648/2330 train_time:95080ms step_avg:57.69ms
step:1649/2330 train_time:95137ms step_avg:57.69ms
step:1650/2330 train_time:95197ms step_avg:57.69ms
step:1651/2330 train_time:95253ms step_avg:57.69ms
step:1652/2330 train_time:95314ms step_avg:57.70ms
step:1653/2330 train_time:95372ms step_avg:57.70ms
step:1654/2330 train_time:95432ms step_avg:57.70ms
step:1655/2330 train_time:95488ms step_avg:57.70ms
step:1656/2330 train_time:95548ms step_avg:57.70ms
step:1657/2330 train_time:95604ms step_avg:57.70ms
step:1658/2330 train_time:95664ms step_avg:57.70ms
step:1659/2330 train_time:95722ms step_avg:57.70ms
step:1660/2330 train_time:95781ms step_avg:57.70ms
step:1661/2330 train_time:95838ms step_avg:57.70ms
step:1662/2330 train_time:95898ms step_avg:57.70ms
step:1663/2330 train_time:95954ms step_avg:57.70ms
step:1664/2330 train_time:96015ms step_avg:57.70ms
step:1665/2330 train_time:96073ms step_avg:57.70ms
step:1666/2330 train_time:96134ms step_avg:57.70ms
step:1667/2330 train_time:96191ms step_avg:57.70ms
step:1668/2330 train_time:96250ms step_avg:57.70ms
step:1669/2330 train_time:96306ms step_avg:57.70ms
step:1670/2330 train_time:96367ms step_avg:57.70ms
step:1671/2330 train_time:96424ms step_avg:57.70ms
step:1672/2330 train_time:96483ms step_avg:57.71ms
step:1673/2330 train_time:96540ms step_avg:57.70ms
step:1674/2330 train_time:96600ms step_avg:57.71ms
step:1675/2330 train_time:96657ms step_avg:57.71ms
step:1676/2330 train_time:96718ms step_avg:57.71ms
step:1677/2330 train_time:96775ms step_avg:57.71ms
step:1678/2330 train_time:96835ms step_avg:57.71ms
step:1679/2330 train_time:96892ms step_avg:57.71ms
step:1680/2330 train_time:96952ms step_avg:57.71ms
step:1681/2330 train_time:97010ms step_avg:57.71ms
step:1682/2330 train_time:97070ms step_avg:57.71ms
step:1683/2330 train_time:97127ms step_avg:57.71ms
step:1684/2330 train_time:97187ms step_avg:57.71ms
step:1685/2330 train_time:97244ms step_avg:57.71ms
step:1686/2330 train_time:97304ms step_avg:57.71ms
step:1687/2330 train_time:97361ms step_avg:57.71ms
step:1688/2330 train_time:97421ms step_avg:57.71ms
step:1689/2330 train_time:97478ms step_avg:57.71ms
step:1690/2330 train_time:97539ms step_avg:57.72ms
step:1691/2330 train_time:97595ms step_avg:57.71ms
step:1692/2330 train_time:97656ms step_avg:57.72ms
step:1693/2330 train_time:97714ms step_avg:57.72ms
step:1694/2330 train_time:97774ms step_avg:57.72ms
step:1695/2330 train_time:97831ms step_avg:57.72ms
step:1696/2330 train_time:97891ms step_avg:57.72ms
step:1697/2330 train_time:97948ms step_avg:57.72ms
step:1698/2330 train_time:98008ms step_avg:57.72ms
step:1699/2330 train_time:98065ms step_avg:57.72ms
step:1700/2330 train_time:98125ms step_avg:57.72ms
step:1701/2330 train_time:98181ms step_avg:57.72ms
step:1702/2330 train_time:98242ms step_avg:57.72ms
step:1703/2330 train_time:98299ms step_avg:57.72ms
step:1704/2330 train_time:98359ms step_avg:57.72ms
step:1705/2330 train_time:98416ms step_avg:57.72ms
step:1706/2330 train_time:98476ms step_avg:57.72ms
step:1707/2330 train_time:98533ms step_avg:57.72ms
step:1708/2330 train_time:98593ms step_avg:57.72ms
step:1709/2330 train_time:98650ms step_avg:57.72ms
step:1710/2330 train_time:98710ms step_avg:57.73ms
step:1711/2330 train_time:98767ms step_avg:57.72ms
step:1712/2330 train_time:98827ms step_avg:57.73ms
step:1713/2330 train_time:98883ms step_avg:57.73ms
step:1714/2330 train_time:98943ms step_avg:57.73ms
step:1715/2330 train_time:99000ms step_avg:57.73ms
step:1716/2330 train_time:99061ms step_avg:57.73ms
step:1717/2330 train_time:99118ms step_avg:57.73ms
step:1718/2330 train_time:99179ms step_avg:57.73ms
step:1719/2330 train_time:99236ms step_avg:57.73ms
step:1720/2330 train_time:99296ms step_avg:57.73ms
step:1721/2330 train_time:99352ms step_avg:57.73ms
step:1722/2330 train_time:99413ms step_avg:57.73ms
step:1723/2330 train_time:99470ms step_avg:57.73ms
step:1724/2330 train_time:99530ms step_avg:57.73ms
step:1725/2330 train_time:99586ms step_avg:57.73ms
step:1726/2330 train_time:99646ms step_avg:57.73ms
step:1727/2330 train_time:99704ms step_avg:57.73ms
step:1728/2330 train_time:99763ms step_avg:57.73ms
step:1729/2330 train_time:99821ms step_avg:57.73ms
step:1730/2330 train_time:99881ms step_avg:57.73ms
step:1731/2330 train_time:99938ms step_avg:57.73ms
step:1732/2330 train_time:99998ms step_avg:57.74ms
step:1733/2330 train_time:100054ms step_avg:57.73ms
step:1734/2330 train_time:100116ms step_avg:57.74ms
step:1735/2330 train_time:100173ms step_avg:57.74ms
step:1736/2330 train_time:100233ms step_avg:57.74ms
step:1737/2330 train_time:100290ms step_avg:57.74ms
step:1738/2330 train_time:100351ms step_avg:57.74ms
step:1739/2330 train_time:100407ms step_avg:57.74ms
step:1740/2330 train_time:100467ms step_avg:57.74ms
step:1741/2330 train_time:100523ms step_avg:57.74ms
step:1742/2330 train_time:100583ms step_avg:57.74ms
step:1743/2330 train_time:100639ms step_avg:57.74ms
step:1744/2330 train_time:100701ms step_avg:57.74ms
step:1745/2330 train_time:100757ms step_avg:57.74ms
step:1746/2330 train_time:100818ms step_avg:57.74ms
step:1747/2330 train_time:100875ms step_avg:57.74ms
step:1748/2330 train_time:100935ms step_avg:57.74ms
step:1749/2330 train_time:100993ms step_avg:57.74ms
step:1750/2330 train_time:101053ms step_avg:57.74ms
step:1750/2330 val_loss:3.9465 train_time:101134ms step_avg:57.79ms
step:1751/2330 train_time:101151ms step_avg:57.77ms
step:1752/2330 train_time:101171ms step_avg:57.75ms
step:1753/2330 train_time:101227ms step_avg:57.74ms
step:1754/2330 train_time:101291ms step_avg:57.75ms
step:1755/2330 train_time:101347ms step_avg:57.75ms
step:1756/2330 train_time:101411ms step_avg:57.75ms
step:1757/2330 train_time:101467ms step_avg:57.75ms
step:1758/2330 train_time:101527ms step_avg:57.75ms
step:1759/2330 train_time:101583ms step_avg:57.75ms
step:1760/2330 train_time:101642ms step_avg:57.75ms
step:1761/2330 train_time:101699ms step_avg:57.75ms
step:1762/2330 train_time:101758ms step_avg:57.75ms
step:1763/2330 train_time:101814ms step_avg:57.75ms
step:1764/2330 train_time:101874ms step_avg:57.75ms
step:1765/2330 train_time:101930ms step_avg:57.75ms
step:1766/2330 train_time:101989ms step_avg:57.75ms
step:1767/2330 train_time:102047ms step_avg:57.75ms
step:1768/2330 train_time:102112ms step_avg:57.76ms
step:1769/2330 train_time:102169ms step_avg:57.76ms
step:1770/2330 train_time:102230ms step_avg:57.76ms
step:1771/2330 train_time:102288ms step_avg:57.76ms
step:1772/2330 train_time:102348ms step_avg:57.76ms
step:1773/2330 train_time:102405ms step_avg:57.76ms
step:1774/2330 train_time:102464ms step_avg:57.76ms
step:1775/2330 train_time:102521ms step_avg:57.76ms
step:1776/2330 train_time:102580ms step_avg:57.76ms
step:1777/2330 train_time:102637ms step_avg:57.76ms
step:1778/2330 train_time:102696ms step_avg:57.76ms
step:1779/2330 train_time:102753ms step_avg:57.76ms
step:1780/2330 train_time:102812ms step_avg:57.76ms
step:1781/2330 train_time:102869ms step_avg:57.76ms
step:1782/2330 train_time:102928ms step_avg:57.76ms
step:1783/2330 train_time:102986ms step_avg:57.76ms
step:1784/2330 train_time:103046ms step_avg:57.76ms
step:1785/2330 train_time:103103ms step_avg:57.76ms
step:1786/2330 train_time:103165ms step_avg:57.76ms
step:1787/2330 train_time:103222ms step_avg:57.76ms
step:1788/2330 train_time:103282ms step_avg:57.76ms
step:1789/2330 train_time:103339ms step_avg:57.76ms
step:1790/2330 train_time:103399ms step_avg:57.76ms
step:1791/2330 train_time:103455ms step_avg:57.76ms
step:1792/2330 train_time:103515ms step_avg:57.76ms
step:1793/2330 train_time:103571ms step_avg:57.76ms
step:1794/2330 train_time:103631ms step_avg:57.77ms
step:1795/2330 train_time:103687ms step_avg:57.76ms
step:1796/2330 train_time:103747ms step_avg:57.77ms
step:1797/2330 train_time:103803ms step_avg:57.76ms
step:1798/2330 train_time:103864ms step_avg:57.77ms
step:1799/2330 train_time:103921ms step_avg:57.77ms
step:1800/2330 train_time:103981ms step_avg:57.77ms
step:1801/2330 train_time:104038ms step_avg:57.77ms
step:1802/2330 train_time:104100ms step_avg:57.77ms
step:1803/2330 train_time:104157ms step_avg:57.77ms
step:1804/2330 train_time:104219ms step_avg:57.77ms
step:1805/2330 train_time:104275ms step_avg:57.77ms
step:1806/2330 train_time:104338ms step_avg:57.77ms
step:1807/2330 train_time:104394ms step_avg:57.77ms
step:1808/2330 train_time:104454ms step_avg:57.77ms
step:1809/2330 train_time:104510ms step_avg:57.77ms
step:1810/2330 train_time:104570ms step_avg:57.77ms
step:1811/2330 train_time:104627ms step_avg:57.77ms
step:1812/2330 train_time:104686ms step_avg:57.77ms
step:1813/2330 train_time:104743ms step_avg:57.77ms
step:1814/2330 train_time:104803ms step_avg:57.77ms
step:1815/2330 train_time:104860ms step_avg:57.77ms
step:1816/2330 train_time:104919ms step_avg:57.77ms
step:1817/2330 train_time:104976ms step_avg:57.77ms
step:1818/2330 train_time:105036ms step_avg:57.78ms
step:1819/2330 train_time:105093ms step_avg:57.78ms
step:1820/2330 train_time:105153ms step_avg:57.78ms
step:1821/2330 train_time:105210ms step_avg:57.78ms
step:1822/2330 train_time:105270ms step_avg:57.78ms
step:1823/2330 train_time:105327ms step_avg:57.78ms
step:1824/2330 train_time:105388ms step_avg:57.78ms
step:1825/2330 train_time:105444ms step_avg:57.78ms
step:1826/2330 train_time:105504ms step_avg:57.78ms
step:1827/2330 train_time:105561ms step_avg:57.78ms
step:1828/2330 train_time:105621ms step_avg:57.78ms
step:1829/2330 train_time:105678ms step_avg:57.78ms
step:1830/2330 train_time:105738ms step_avg:57.78ms
step:1831/2330 train_time:105794ms step_avg:57.78ms
step:1832/2330 train_time:105854ms step_avg:57.78ms
step:1833/2330 train_time:105910ms step_avg:57.78ms
step:1834/2330 train_time:105970ms step_avg:57.78ms
step:1835/2330 train_time:106027ms step_avg:57.78ms
step:1836/2330 train_time:106087ms step_avg:57.78ms
step:1837/2330 train_time:106144ms step_avg:57.78ms
step:1838/2330 train_time:106205ms step_avg:57.78ms
step:1839/2330 train_time:106262ms step_avg:57.78ms
step:1840/2330 train_time:106324ms step_avg:57.78ms
step:1841/2330 train_time:106382ms step_avg:57.78ms
step:1842/2330 train_time:106442ms step_avg:57.79ms
step:1843/2330 train_time:106499ms step_avg:57.79ms
step:1844/2330 train_time:106558ms step_avg:57.79ms
step:1845/2330 train_time:106616ms step_avg:57.79ms
step:1846/2330 train_time:106675ms step_avg:57.79ms
step:1847/2330 train_time:106733ms step_avg:57.79ms
step:1848/2330 train_time:106792ms step_avg:57.79ms
step:1849/2330 train_time:106848ms step_avg:57.79ms
step:1850/2330 train_time:106908ms step_avg:57.79ms
step:1851/2330 train_time:106965ms step_avg:57.79ms
step:1852/2330 train_time:107025ms step_avg:57.79ms
step:1853/2330 train_time:107082ms step_avg:57.79ms
step:1854/2330 train_time:107143ms step_avg:57.79ms
step:1855/2330 train_time:107200ms step_avg:57.79ms
step:1856/2330 train_time:107261ms step_avg:57.79ms
step:1857/2330 train_time:107317ms step_avg:57.79ms
step:1858/2330 train_time:107379ms step_avg:57.79ms
step:1859/2330 train_time:107435ms step_avg:57.79ms
step:1860/2330 train_time:107496ms step_avg:57.79ms
step:1861/2330 train_time:107552ms step_avg:57.79ms
step:1862/2330 train_time:107612ms step_avg:57.79ms
step:1863/2330 train_time:107669ms step_avg:57.79ms
step:1864/2330 train_time:107729ms step_avg:57.79ms
step:1865/2330 train_time:107786ms step_avg:57.79ms
step:1866/2330 train_time:107846ms step_avg:57.80ms
step:1867/2330 train_time:107903ms step_avg:57.79ms
step:1868/2330 train_time:107962ms step_avg:57.80ms
step:1869/2330 train_time:108020ms step_avg:57.80ms
step:1870/2330 train_time:108079ms step_avg:57.80ms
step:1871/2330 train_time:108136ms step_avg:57.80ms
step:1872/2330 train_time:108196ms step_avg:57.80ms
step:1873/2330 train_time:108252ms step_avg:57.80ms
step:1874/2330 train_time:108313ms step_avg:57.80ms
step:1875/2330 train_time:108369ms step_avg:57.80ms
step:1876/2330 train_time:108430ms step_avg:57.80ms
step:1877/2330 train_time:108487ms step_avg:57.80ms
step:1878/2330 train_time:108547ms step_avg:57.80ms
step:1879/2330 train_time:108604ms step_avg:57.80ms
step:1880/2330 train_time:108663ms step_avg:57.80ms
step:1881/2330 train_time:108720ms step_avg:57.80ms
step:1882/2330 train_time:108780ms step_avg:57.80ms
step:1883/2330 train_time:108837ms step_avg:57.80ms
step:1884/2330 train_time:108896ms step_avg:57.80ms
step:1885/2330 train_time:108953ms step_avg:57.80ms
step:1886/2330 train_time:109013ms step_avg:57.80ms
step:1887/2330 train_time:109070ms step_avg:57.80ms
step:1888/2330 train_time:109131ms step_avg:57.80ms
step:1889/2330 train_time:109187ms step_avg:57.80ms
step:1890/2330 train_time:109247ms step_avg:57.80ms
step:1891/2330 train_time:109304ms step_avg:57.80ms
step:1892/2330 train_time:109365ms step_avg:57.80ms
step:1893/2330 train_time:109422ms step_avg:57.80ms
step:1894/2330 train_time:109483ms step_avg:57.81ms
step:1895/2330 train_time:109540ms step_avg:57.80ms
step:1896/2330 train_time:109600ms step_avg:57.81ms
step:1897/2330 train_time:109657ms step_avg:57.81ms
step:1898/2330 train_time:109716ms step_avg:57.81ms
step:1899/2330 train_time:109772ms step_avg:57.81ms
step:1900/2330 train_time:109832ms step_avg:57.81ms
step:1901/2330 train_time:109889ms step_avg:57.81ms
step:1902/2330 train_time:109949ms step_avg:57.81ms
step:1903/2330 train_time:110005ms step_avg:57.81ms
step:1904/2330 train_time:110066ms step_avg:57.81ms
step:1905/2330 train_time:110122ms step_avg:57.81ms
step:1906/2330 train_time:110184ms step_avg:57.81ms
step:1907/2330 train_time:110240ms step_avg:57.81ms
step:1908/2330 train_time:110301ms step_avg:57.81ms
step:1909/2330 train_time:110357ms step_avg:57.81ms
step:1910/2330 train_time:110418ms step_avg:57.81ms
step:1911/2330 train_time:110475ms step_avg:57.81ms
step:1912/2330 train_time:110536ms step_avg:57.81ms
step:1913/2330 train_time:110592ms step_avg:57.81ms
step:1914/2330 train_time:110652ms step_avg:57.81ms
step:1915/2330 train_time:110708ms step_avg:57.81ms
step:1916/2330 train_time:110769ms step_avg:57.81ms
step:1917/2330 train_time:110826ms step_avg:57.81ms
step:1918/2330 train_time:110886ms step_avg:57.81ms
step:1919/2330 train_time:110943ms step_avg:57.81ms
step:1920/2330 train_time:111003ms step_avg:57.81ms
step:1921/2330 train_time:111060ms step_avg:57.81ms
step:1922/2330 train_time:111120ms step_avg:57.81ms
step:1923/2330 train_time:111177ms step_avg:57.81ms
step:1924/2330 train_time:111237ms step_avg:57.82ms
step:1925/2330 train_time:111293ms step_avg:57.81ms
step:1926/2330 train_time:111354ms step_avg:57.82ms
step:1927/2330 train_time:111411ms step_avg:57.82ms
step:1928/2330 train_time:111471ms step_avg:57.82ms
step:1929/2330 train_time:111527ms step_avg:57.82ms
step:1930/2330 train_time:111588ms step_avg:57.82ms
step:1931/2330 train_time:111645ms step_avg:57.82ms
step:1932/2330 train_time:111705ms step_avg:57.82ms
step:1933/2330 train_time:111762ms step_avg:57.82ms
step:1934/2330 train_time:111822ms step_avg:57.82ms
step:1935/2330 train_time:111878ms step_avg:57.82ms
step:1936/2330 train_time:111938ms step_avg:57.82ms
step:1937/2330 train_time:111995ms step_avg:57.82ms
step:1938/2330 train_time:112054ms step_avg:57.82ms
step:1939/2330 train_time:112111ms step_avg:57.82ms
step:1940/2330 train_time:112172ms step_avg:57.82ms
step:1941/2330 train_time:112229ms step_avg:57.82ms
step:1942/2330 train_time:112289ms step_avg:57.82ms
step:1943/2330 train_time:112345ms step_avg:57.82ms
step:1944/2330 train_time:112406ms step_avg:57.82ms
step:1945/2330 train_time:112462ms step_avg:57.82ms
step:1946/2330 train_time:112524ms step_avg:57.82ms
step:1947/2330 train_time:112580ms step_avg:57.82ms
step:1948/2330 train_time:112641ms step_avg:57.82ms
step:1949/2330 train_time:112698ms step_avg:57.82ms
step:1950/2330 train_time:112757ms step_avg:57.82ms
step:1951/2330 train_time:112813ms step_avg:57.82ms
step:1952/2330 train_time:112874ms step_avg:57.82ms
step:1953/2330 train_time:112931ms step_avg:57.82ms
step:1954/2330 train_time:112990ms step_avg:57.82ms
step:1955/2330 train_time:113047ms step_avg:57.82ms
step:1956/2330 train_time:113107ms step_avg:57.83ms
step:1957/2330 train_time:113164ms step_avg:57.83ms
step:1958/2330 train_time:113224ms step_avg:57.83ms
step:1959/2330 train_time:113281ms step_avg:57.83ms
step:1960/2330 train_time:113342ms step_avg:57.83ms
step:1961/2330 train_time:113398ms step_avg:57.83ms
step:1962/2330 train_time:113458ms step_avg:57.83ms
step:1963/2330 train_time:113515ms step_avg:57.83ms
step:1964/2330 train_time:113575ms step_avg:57.83ms
step:1965/2330 train_time:113632ms step_avg:57.83ms
step:1966/2330 train_time:113692ms step_avg:57.83ms
step:1967/2330 train_time:113749ms step_avg:57.83ms
step:1968/2330 train_time:113810ms step_avg:57.83ms
step:1969/2330 train_time:113866ms step_avg:57.83ms
step:1970/2330 train_time:113927ms step_avg:57.83ms
step:1971/2330 train_time:113984ms step_avg:57.83ms
step:1972/2330 train_time:114043ms step_avg:57.83ms
step:1973/2330 train_time:114099ms step_avg:57.83ms
step:1974/2330 train_time:114161ms step_avg:57.83ms
step:1975/2330 train_time:114217ms step_avg:57.83ms
step:1976/2330 train_time:114278ms step_avg:57.83ms
step:1977/2330 train_time:114334ms step_avg:57.83ms
step:1978/2330 train_time:114394ms step_avg:57.83ms
step:1979/2330 train_time:114451ms step_avg:57.83ms
step:1980/2330 train_time:114511ms step_avg:57.83ms
step:1981/2330 train_time:114568ms step_avg:57.83ms
step:1982/2330 train_time:114629ms step_avg:57.83ms
step:1983/2330 train_time:114685ms step_avg:57.83ms
step:1984/2330 train_time:114745ms step_avg:57.84ms
step:1985/2330 train_time:114802ms step_avg:57.83ms
step:1986/2330 train_time:114862ms step_avg:57.84ms
step:1987/2330 train_time:114918ms step_avg:57.84ms
step:1988/2330 train_time:114979ms step_avg:57.84ms
step:1989/2330 train_time:115035ms step_avg:57.84ms
step:1990/2330 train_time:115095ms step_avg:57.84ms
step:1991/2330 train_time:115152ms step_avg:57.84ms
step:1992/2330 train_time:115212ms step_avg:57.84ms
step:1993/2330 train_time:115269ms step_avg:57.84ms
step:1994/2330 train_time:115329ms step_avg:57.84ms
step:1995/2330 train_time:115386ms step_avg:57.84ms
step:1996/2330 train_time:115447ms step_avg:57.84ms
step:1997/2330 train_time:115504ms step_avg:57.84ms
step:1998/2330 train_time:115563ms step_avg:57.84ms
step:1999/2330 train_time:115620ms step_avg:57.84ms
step:2000/2330 train_time:115680ms step_avg:57.84ms
step:2000/2330 val_loss:3.8874 train_time:115761ms step_avg:57.88ms
step:2001/2330 train_time:115779ms step_avg:57.86ms
step:2002/2330 train_time:115800ms step_avg:57.84ms
step:2003/2330 train_time:115858ms step_avg:57.84ms
step:2004/2330 train_time:115922ms step_avg:57.85ms
step:2005/2330 train_time:115978ms step_avg:57.84ms
step:2006/2330 train_time:116040ms step_avg:57.85ms
step:2007/2330 train_time:116097ms step_avg:57.85ms
step:2008/2330 train_time:116158ms step_avg:57.85ms
step:2009/2330 train_time:116214ms step_avg:57.85ms
step:2010/2330 train_time:116274ms step_avg:57.85ms
step:2011/2330 train_time:116331ms step_avg:57.85ms
step:2012/2330 train_time:116391ms step_avg:57.85ms
step:2013/2330 train_time:116447ms step_avg:57.85ms
step:2014/2330 train_time:116507ms step_avg:57.85ms
step:2015/2330 train_time:116563ms step_avg:57.85ms
step:2016/2330 train_time:116622ms step_avg:57.85ms
step:2017/2330 train_time:116678ms step_avg:57.85ms
step:2018/2330 train_time:116738ms step_avg:57.85ms
step:2019/2330 train_time:116795ms step_avg:57.85ms
step:2020/2330 train_time:116857ms step_avg:57.85ms
step:2021/2330 train_time:116915ms step_avg:57.85ms
step:2022/2330 train_time:116976ms step_avg:57.85ms
step:2023/2330 train_time:117032ms step_avg:57.85ms
step:2024/2330 train_time:117094ms step_avg:57.85ms
step:2025/2330 train_time:117151ms step_avg:57.85ms
step:2026/2330 train_time:117211ms step_avg:57.85ms
step:2027/2330 train_time:117268ms step_avg:57.85ms
step:2028/2330 train_time:117327ms step_avg:57.85ms
step:2029/2330 train_time:117384ms step_avg:57.85ms
step:2030/2330 train_time:117443ms step_avg:57.85ms
step:2031/2330 train_time:117500ms step_avg:57.85ms
step:2032/2330 train_time:117560ms step_avg:57.85ms
step:2033/2330 train_time:117617ms step_avg:57.85ms
step:2034/2330 train_time:117676ms step_avg:57.85ms
step:2035/2330 train_time:117734ms step_avg:57.85ms
step:2036/2330 train_time:117794ms step_avg:57.86ms
step:2037/2330 train_time:117852ms step_avg:57.86ms
step:2038/2330 train_time:117911ms step_avg:57.86ms
step:2039/2330 train_time:117968ms step_avg:57.86ms
step:2040/2330 train_time:118029ms step_avg:57.86ms
step:2041/2330 train_time:118086ms step_avg:57.86ms
step:2042/2330 train_time:118146ms step_avg:57.86ms
step:2043/2330 train_time:118203ms step_avg:57.86ms
step:2044/2330 train_time:118263ms step_avg:57.86ms
step:2045/2330 train_time:118320ms step_avg:57.86ms
step:2046/2330 train_time:118380ms step_avg:57.86ms
step:2047/2330 train_time:118437ms step_avg:57.86ms
step:2048/2330 train_time:118496ms step_avg:57.86ms
step:2049/2330 train_time:118553ms step_avg:57.86ms
step:2050/2330 train_time:118614ms step_avg:57.86ms
step:2051/2330 train_time:118670ms step_avg:57.86ms
step:2052/2330 train_time:118731ms step_avg:57.86ms
step:2053/2330 train_time:118788ms step_avg:57.86ms
step:2054/2330 train_time:118848ms step_avg:57.86ms
step:2055/2330 train_time:118904ms step_avg:57.86ms
step:2056/2330 train_time:118966ms step_avg:57.86ms
step:2057/2330 train_time:119022ms step_avg:57.86ms
step:2058/2330 train_time:119083ms step_avg:57.86ms
step:2059/2330 train_time:119140ms step_avg:57.86ms
step:2060/2330 train_time:119201ms step_avg:57.86ms
step:2061/2330 train_time:119257ms step_avg:57.86ms
step:2062/2330 train_time:119318ms step_avg:57.87ms
step:2063/2330 train_time:119374ms step_avg:57.86ms
step:2064/2330 train_time:119435ms step_avg:57.87ms
step:2065/2330 train_time:119492ms step_avg:57.87ms
step:2066/2330 train_time:119552ms step_avg:57.87ms
step:2067/2330 train_time:119608ms step_avg:57.87ms
step:2068/2330 train_time:119669ms step_avg:57.87ms
step:2069/2330 train_time:119726ms step_avg:57.87ms
step:2070/2330 train_time:119785ms step_avg:57.87ms
step:2071/2330 train_time:119842ms step_avg:57.87ms
step:2072/2330 train_time:119902ms step_avg:57.87ms
step:2073/2330 train_time:119959ms step_avg:57.87ms
step:2074/2330 train_time:120020ms step_avg:57.87ms
step:2075/2330 train_time:120077ms step_avg:57.87ms
step:2076/2330 train_time:120137ms step_avg:57.87ms
step:2077/2330 train_time:120194ms step_avg:57.87ms
step:2078/2330 train_time:120254ms step_avg:57.87ms
step:2079/2330 train_time:120311ms step_avg:57.87ms
step:2080/2330 train_time:120370ms step_avg:57.87ms
step:2081/2330 train_time:120427ms step_avg:57.87ms
step:2082/2330 train_time:120487ms step_avg:57.87ms
step:2083/2330 train_time:120543ms step_avg:57.87ms
step:2084/2330 train_time:120604ms step_avg:57.87ms
step:2085/2330 train_time:120660ms step_avg:57.87ms
step:2086/2330 train_time:120720ms step_avg:57.87ms
step:2087/2330 train_time:120776ms step_avg:57.87ms
step:2088/2330 train_time:120836ms step_avg:57.87ms
step:2089/2330 train_time:120893ms step_avg:57.87ms
step:2090/2330 train_time:120955ms step_avg:57.87ms
step:2091/2330 train_time:121012ms step_avg:57.87ms
step:2092/2330 train_time:121073ms step_avg:57.87ms
step:2093/2330 train_time:121129ms step_avg:57.87ms
step:2094/2330 train_time:121190ms step_avg:57.88ms
step:2095/2330 train_time:121247ms step_avg:57.87ms
step:2096/2330 train_time:121307ms step_avg:57.88ms
step:2097/2330 train_time:121364ms step_avg:57.88ms
step:2098/2330 train_time:121423ms step_avg:57.88ms
step:2099/2330 train_time:121479ms step_avg:57.87ms
step:2100/2330 train_time:121540ms step_avg:57.88ms
step:2101/2330 train_time:121597ms step_avg:57.88ms
step:2102/2330 train_time:121657ms step_avg:57.88ms
step:2103/2330 train_time:121715ms step_avg:57.88ms
step:2104/2330 train_time:121774ms step_avg:57.88ms
step:2105/2330 train_time:121832ms step_avg:57.88ms
step:2106/2330 train_time:121892ms step_avg:57.88ms
step:2107/2330 train_time:121949ms step_avg:57.88ms
step:2108/2330 train_time:122009ms step_avg:57.88ms
step:2109/2330 train_time:122067ms step_avg:57.88ms
step:2110/2330 train_time:122126ms step_avg:57.88ms
step:2111/2330 train_time:122183ms step_avg:57.88ms
step:2112/2330 train_time:122243ms step_avg:57.88ms
step:2113/2330 train_time:122299ms step_avg:57.88ms
step:2114/2330 train_time:122359ms step_avg:57.88ms
step:2115/2330 train_time:122416ms step_avg:57.88ms
step:2116/2330 train_time:122476ms step_avg:57.88ms
step:2117/2330 train_time:122533ms step_avg:57.88ms
step:2118/2330 train_time:122593ms step_avg:57.88ms
step:2119/2330 train_time:122649ms step_avg:57.88ms
step:2120/2330 train_time:122709ms step_avg:57.88ms
step:2121/2330 train_time:122766ms step_avg:57.88ms
step:2122/2330 train_time:122826ms step_avg:57.88ms
step:2123/2330 train_time:122882ms step_avg:57.88ms
step:2124/2330 train_time:122942ms step_avg:57.88ms
step:2125/2330 train_time:122999ms step_avg:57.88ms
step:2126/2330 train_time:123060ms step_avg:57.88ms
step:2127/2330 train_time:123116ms step_avg:57.88ms
step:2128/2330 train_time:123177ms step_avg:57.88ms
step:2129/2330 train_time:123234ms step_avg:57.88ms
step:2130/2330 train_time:123295ms step_avg:57.88ms
step:2131/2330 train_time:123352ms step_avg:57.88ms
step:2132/2330 train_time:123412ms step_avg:57.89ms
step:2133/2330 train_time:123469ms step_avg:57.89ms
step:2134/2330 train_time:123530ms step_avg:57.89ms
step:2135/2330 train_time:123587ms step_avg:57.89ms
step:2136/2330 train_time:123646ms step_avg:57.89ms
step:2137/2330 train_time:123703ms step_avg:57.89ms
step:2138/2330 train_time:123763ms step_avg:57.89ms
step:2139/2330 train_time:123820ms step_avg:57.89ms
step:2140/2330 train_time:123881ms step_avg:57.89ms
step:2141/2330 train_time:123937ms step_avg:57.89ms
step:2142/2330 train_time:123999ms step_avg:57.89ms
step:2143/2330 train_time:124056ms step_avg:57.89ms
step:2144/2330 train_time:124116ms step_avg:57.89ms
step:2145/2330 train_time:124173ms step_avg:57.89ms
step:2146/2330 train_time:124233ms step_avg:57.89ms
step:2147/2330 train_time:124289ms step_avg:57.89ms
step:2148/2330 train_time:124349ms step_avg:57.89ms
step:2149/2330 train_time:124406ms step_avg:57.89ms
step:2150/2330 train_time:124466ms step_avg:57.89ms
step:2151/2330 train_time:124522ms step_avg:57.89ms
step:2152/2330 train_time:124583ms step_avg:57.89ms
step:2153/2330 train_time:124639ms step_avg:57.89ms
step:2154/2330 train_time:124699ms step_avg:57.89ms
step:2155/2330 train_time:124756ms step_avg:57.89ms
step:2156/2330 train_time:124817ms step_avg:57.89ms
step:2157/2330 train_time:124873ms step_avg:57.89ms
step:2158/2330 train_time:124934ms step_avg:57.89ms
step:2159/2330 train_time:124991ms step_avg:57.89ms
step:2160/2330 train_time:125053ms step_avg:57.90ms
step:2161/2330 train_time:125110ms step_avg:57.89ms
step:2162/2330 train_time:125171ms step_avg:57.90ms
step:2163/2330 train_time:125228ms step_avg:57.90ms
step:2164/2330 train_time:125287ms step_avg:57.90ms
step:2165/2330 train_time:125344ms step_avg:57.90ms
step:2166/2330 train_time:125404ms step_avg:57.90ms
step:2167/2330 train_time:125461ms step_avg:57.90ms
step:2168/2330 train_time:125521ms step_avg:57.90ms
step:2169/2330 train_time:125577ms step_avg:57.90ms
step:2170/2330 train_time:125638ms step_avg:57.90ms
step:2171/2330 train_time:125694ms step_avg:57.90ms
step:2172/2330 train_time:125755ms step_avg:57.90ms
step:2173/2330 train_time:125812ms step_avg:57.90ms
step:2174/2330 train_time:125872ms step_avg:57.90ms
step:2175/2330 train_time:125929ms step_avg:57.90ms
step:2176/2330 train_time:125990ms step_avg:57.90ms
step:2177/2330 train_time:126047ms step_avg:57.90ms
step:2178/2330 train_time:126107ms step_avg:57.90ms
step:2179/2330 train_time:126163ms step_avg:57.90ms
step:2180/2330 train_time:126224ms step_avg:57.90ms
step:2181/2330 train_time:126281ms step_avg:57.90ms
step:2182/2330 train_time:126341ms step_avg:57.90ms
step:2183/2330 train_time:126398ms step_avg:57.90ms
step:2184/2330 train_time:126459ms step_avg:57.90ms
step:2185/2330 train_time:126516ms step_avg:57.90ms
step:2186/2330 train_time:126575ms step_avg:57.90ms
step:2187/2330 train_time:126632ms step_avg:57.90ms
step:2188/2330 train_time:126692ms step_avg:57.90ms
step:2189/2330 train_time:126749ms step_avg:57.90ms
step:2190/2330 train_time:126808ms step_avg:57.90ms
step:2191/2330 train_time:126865ms step_avg:57.90ms
step:2192/2330 train_time:126925ms step_avg:57.90ms
step:2193/2330 train_time:126981ms step_avg:57.90ms
step:2194/2330 train_time:127044ms step_avg:57.91ms
step:2195/2330 train_time:127100ms step_avg:57.90ms
step:2196/2330 train_time:127161ms step_avg:57.91ms
step:2197/2330 train_time:127217ms step_avg:57.90ms
step:2198/2330 train_time:127277ms step_avg:57.91ms
step:2199/2330 train_time:127335ms step_avg:57.91ms
step:2200/2330 train_time:127395ms step_avg:57.91ms
step:2201/2330 train_time:127452ms step_avg:57.91ms
step:2202/2330 train_time:127512ms step_avg:57.91ms
step:2203/2330 train_time:127569ms step_avg:57.91ms
step:2204/2330 train_time:127630ms step_avg:57.91ms
step:2205/2330 train_time:127686ms step_avg:57.91ms
step:2206/2330 train_time:127747ms step_avg:57.91ms
step:2207/2330 train_time:127803ms step_avg:57.91ms
step:2208/2330 train_time:127864ms step_avg:57.91ms
step:2209/2330 train_time:127920ms step_avg:57.91ms
step:2210/2330 train_time:127981ms step_avg:57.91ms
step:2211/2330 train_time:128037ms step_avg:57.91ms
step:2212/2330 train_time:128099ms step_avg:57.91ms
step:2213/2330 train_time:128155ms step_avg:57.91ms
step:2214/2330 train_time:128217ms step_avg:57.91ms
step:2215/2330 train_time:128273ms step_avg:57.91ms
step:2216/2330 train_time:128335ms step_avg:57.91ms
step:2217/2330 train_time:128392ms step_avg:57.91ms
step:2218/2330 train_time:128452ms step_avg:57.91ms
step:2219/2330 train_time:128508ms step_avg:57.91ms
step:2220/2330 train_time:128569ms step_avg:57.91ms
step:2221/2330 train_time:128626ms step_avg:57.91ms
step:2222/2330 train_time:128685ms step_avg:57.91ms
step:2223/2330 train_time:128742ms step_avg:57.91ms
step:2224/2330 train_time:128801ms step_avg:57.91ms
step:2225/2330 train_time:128858ms step_avg:57.91ms
step:2226/2330 train_time:128918ms step_avg:57.91ms
step:2227/2330 train_time:128975ms step_avg:57.91ms
step:2228/2330 train_time:129035ms step_avg:57.92ms
step:2229/2330 train_time:129092ms step_avg:57.91ms
step:2230/2330 train_time:129152ms step_avg:57.92ms
step:2231/2330 train_time:129209ms step_avg:57.92ms
step:2232/2330 train_time:129270ms step_avg:57.92ms
step:2233/2330 train_time:129327ms step_avg:57.92ms
step:2234/2330 train_time:129387ms step_avg:57.92ms
step:2235/2330 train_time:129443ms step_avg:57.92ms
step:2236/2330 train_time:129504ms step_avg:57.92ms
step:2237/2330 train_time:129560ms step_avg:57.92ms
step:2238/2330 train_time:129621ms step_avg:57.92ms
step:2239/2330 train_time:129677ms step_avg:57.92ms
step:2240/2330 train_time:129738ms step_avg:57.92ms
step:2241/2330 train_time:129795ms step_avg:57.92ms
step:2242/2330 train_time:129856ms step_avg:57.92ms
step:2243/2330 train_time:129913ms step_avg:57.92ms
step:2244/2330 train_time:129972ms step_avg:57.92ms
step:2245/2330 train_time:130029ms step_avg:57.92ms
step:2246/2330 train_time:130089ms step_avg:57.92ms
step:2247/2330 train_time:130145ms step_avg:57.92ms
step:2248/2330 train_time:130206ms step_avg:57.92ms
step:2249/2330 train_time:130262ms step_avg:57.92ms
step:2250/2330 train_time:130323ms step_avg:57.92ms
step:2250/2330 val_loss:3.8425 train_time:130403ms step_avg:57.96ms
step:2251/2330 train_time:130421ms step_avg:57.94ms
step:2252/2330 train_time:130442ms step_avg:57.92ms
step:2253/2330 train_time:130501ms step_avg:57.92ms
step:2254/2330 train_time:130566ms step_avg:57.93ms
step:2255/2330 train_time:130624ms step_avg:57.93ms
step:2256/2330 train_time:130684ms step_avg:57.93ms
step:2257/2330 train_time:130741ms step_avg:57.93ms
step:2258/2330 train_time:130800ms step_avg:57.93ms
step:2259/2330 train_time:130857ms step_avg:57.93ms
step:2260/2330 train_time:130916ms step_avg:57.93ms
step:2261/2330 train_time:130973ms step_avg:57.93ms
step:2262/2330 train_time:131032ms step_avg:57.93ms
step:2263/2330 train_time:131088ms step_avg:57.93ms
step:2264/2330 train_time:131148ms step_avg:57.93ms
step:2265/2330 train_time:131203ms step_avg:57.93ms
step:2266/2330 train_time:131263ms step_avg:57.93ms
step:2267/2330 train_time:131320ms step_avg:57.93ms
step:2268/2330 train_time:131381ms step_avg:57.93ms
step:2269/2330 train_time:131439ms step_avg:57.93ms
step:2270/2330 train_time:131501ms step_avg:57.93ms
step:2271/2330 train_time:131559ms step_avg:57.93ms
step:2272/2330 train_time:131621ms step_avg:57.93ms
step:2273/2330 train_time:131679ms step_avg:57.93ms
step:2274/2330 train_time:131739ms step_avg:57.93ms
step:2275/2330 train_time:131796ms step_avg:57.93ms
step:2276/2330 train_time:131855ms step_avg:57.93ms
step:2277/2330 train_time:131912ms step_avg:57.93ms
step:2278/2330 train_time:131971ms step_avg:57.93ms
step:2279/2330 train_time:132028ms step_avg:57.93ms
step:2280/2330 train_time:132087ms step_avg:57.93ms
step:2281/2330 train_time:132144ms step_avg:57.93ms
step:2282/2330 train_time:132203ms step_avg:57.93ms
step:2283/2330 train_time:132260ms step_avg:57.93ms
step:2284/2330 train_time:132319ms step_avg:57.93ms
step:2285/2330 train_time:132376ms step_avg:57.93ms
step:2286/2330 train_time:132436ms step_avg:57.93ms
step:2287/2330 train_time:132493ms step_avg:57.93ms
step:2288/2330 train_time:132555ms step_avg:57.93ms
step:2289/2330 train_time:132612ms step_avg:57.93ms
step:2290/2330 train_time:132674ms step_avg:57.94ms
step:2291/2330 train_time:132730ms step_avg:57.94ms
step:2292/2330 train_time:132790ms step_avg:57.94ms
step:2293/2330 train_time:132847ms step_avg:57.94ms
step:2294/2330 train_time:132907ms step_avg:57.94ms
step:2295/2330 train_time:132964ms step_avg:57.94ms
step:2296/2330 train_time:133024ms step_avg:57.94ms
step:2297/2330 train_time:133080ms step_avg:57.94ms
step:2298/2330 train_time:133139ms step_avg:57.94ms
step:2299/2330 train_time:133196ms step_avg:57.94ms
step:2300/2330 train_time:133255ms step_avg:57.94ms
step:2301/2330 train_time:133313ms step_avg:57.94ms
step:2302/2330 train_time:133372ms step_avg:57.94ms
step:2303/2330 train_time:133429ms step_avg:57.94ms
step:2304/2330 train_time:133490ms step_avg:57.94ms
step:2305/2330 train_time:133546ms step_avg:57.94ms
step:2306/2330 train_time:133607ms step_avg:57.94ms
step:2307/2330 train_time:133665ms step_avg:57.94ms
step:2308/2330 train_time:133725ms step_avg:57.94ms
step:2309/2330 train_time:133782ms step_avg:57.94ms
step:2310/2330 train_time:133842ms step_avg:57.94ms
step:2311/2330 train_time:133899ms step_avg:57.94ms
step:2312/2330 train_time:133959ms step_avg:57.94ms
step:2313/2330 train_time:134015ms step_avg:57.94ms
step:2314/2330 train_time:134075ms step_avg:57.94ms
step:2315/2330 train_time:134131ms step_avg:57.94ms
step:2316/2330 train_time:134191ms step_avg:57.94ms
step:2317/2330 train_time:134248ms step_avg:57.94ms
step:2318/2330 train_time:134307ms step_avg:57.94ms
step:2319/2330 train_time:134363ms step_avg:57.94ms
step:2320/2330 train_time:134425ms step_avg:57.94ms
step:2321/2330 train_time:134482ms step_avg:57.94ms
step:2322/2330 train_time:134543ms step_avg:57.94ms
step:2323/2330 train_time:134600ms step_avg:57.94ms
step:2324/2330 train_time:134660ms step_avg:57.94ms
step:2325/2330 train_time:134717ms step_avg:57.94ms
step:2326/2330 train_time:134778ms step_avg:57.94ms
step:2327/2330 train_time:134836ms step_avg:57.94ms
step:2328/2330 train_time:134896ms step_avg:57.95ms
step:2329/2330 train_time:134953ms step_avg:57.94ms
step:2330/2330 train_time:135012ms step_avg:57.94ms
step:2330/2330 val_loss:3.8284 train_time:135093ms step_avg:57.98ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
