import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--adamw_lr', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_lr_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_lr_tuning/{args2.adamw_lr}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=float(args2.adamw_lr),  betas=(0.85, 0.85), eps=1e-10, weight_decay=0.0, fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_lr_tuning/{args2.adamw_lr}", exist_ok=True)
            torch.save(log, f"logs_adamw_small_lr_tuning/{args2.adamw_lr}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 03:07:13 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   27C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   26C    P0             116W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   24C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   27C    P0             114W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   25C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   26C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:87ms step_avg:87.39ms
step:2/2330 train_time:182ms step_avg:91.11ms
step:3/2330 train_time:202ms step_avg:67.32ms
step:4/2330 train_time:222ms step_avg:55.48ms
step:5/2330 train_time:273ms step_avg:54.61ms
step:6/2330 train_time:330ms step_avg:55.04ms
step:7/2330 train_time:384ms step_avg:54.86ms
step:8/2330 train_time:441ms step_avg:55.11ms
step:9/2330 train_time:495ms step_avg:54.95ms
step:10/2330 train_time:552ms step_avg:55.18ms
step:11/2330 train_time:606ms step_avg:55.05ms
step:12/2330 train_time:663ms step_avg:55.22ms
step:13/2330 train_time:716ms step_avg:55.10ms
step:14/2330 train_time:773ms step_avg:55.24ms
step:15/2330 train_time:827ms step_avg:55.15ms
step:16/2330 train_time:885ms step_avg:55.29ms
step:17/2330 train_time:938ms step_avg:55.20ms
step:18/2330 train_time:996ms step_avg:55.32ms
step:19/2330 train_time:1049ms step_avg:55.24ms
step:20/2330 train_time:1108ms step_avg:55.39ms
step:21/2330 train_time:1162ms step_avg:55.34ms
step:22/2330 train_time:1220ms step_avg:55.45ms
step:23/2330 train_time:1274ms step_avg:55.41ms
step:24/2330 train_time:1332ms step_avg:55.50ms
step:25/2330 train_time:1387ms step_avg:55.47ms
step:26/2330 train_time:1444ms step_avg:55.54ms
step:27/2330 train_time:1499ms step_avg:55.51ms
step:28/2330 train_time:1556ms step_avg:55.56ms
step:29/2330 train_time:1610ms step_avg:55.52ms
step:30/2330 train_time:1667ms step_avg:55.56ms
step:31/2330 train_time:1721ms step_avg:55.52ms
step:32/2330 train_time:1778ms step_avg:55.57ms
step:33/2330 train_time:1832ms step_avg:55.52ms
step:34/2330 train_time:1890ms step_avg:55.58ms
step:35/2330 train_time:1944ms step_avg:55.53ms
step:36/2330 train_time:2001ms step_avg:55.59ms
step:37/2330 train_time:2055ms step_avg:55.55ms
step:38/2330 train_time:2113ms step_avg:55.59ms
step:39/2330 train_time:2167ms step_avg:55.56ms
step:40/2330 train_time:2225ms step_avg:55.62ms
step:41/2330 train_time:2279ms step_avg:55.58ms
step:42/2330 train_time:2338ms step_avg:55.66ms
step:43/2330 train_time:2392ms step_avg:55.64ms
step:44/2330 train_time:2450ms step_avg:55.69ms
step:45/2330 train_time:2505ms step_avg:55.67ms
step:46/2330 train_time:2563ms step_avg:55.71ms
step:47/2330 train_time:2618ms step_avg:55.69ms
step:48/2330 train_time:2675ms step_avg:55.73ms
step:49/2330 train_time:2730ms step_avg:55.71ms
step:50/2330 train_time:2787ms step_avg:55.74ms
step:51/2330 train_time:2842ms step_avg:55.72ms
step:52/2330 train_time:2899ms step_avg:55.74ms
step:53/2330 train_time:2953ms step_avg:55.72ms
step:54/2330 train_time:3011ms step_avg:55.76ms
step:55/2330 train_time:3065ms step_avg:55.73ms
step:56/2330 train_time:3124ms step_avg:55.78ms
step:57/2330 train_time:3178ms step_avg:55.76ms
step:58/2330 train_time:3236ms step_avg:55.79ms
step:59/2330 train_time:3291ms step_avg:55.78ms
step:60/2330 train_time:3349ms step_avg:55.81ms
step:61/2330 train_time:3404ms step_avg:55.80ms
step:62/2330 train_time:3461ms step_avg:55.83ms
step:63/2330 train_time:3516ms step_avg:55.81ms
step:64/2330 train_time:3574ms step_avg:55.85ms
step:65/2330 train_time:3629ms step_avg:55.83ms
step:66/2330 train_time:3687ms step_avg:55.86ms
step:67/2330 train_time:3742ms step_avg:55.85ms
step:68/2330 train_time:3799ms step_avg:55.87ms
step:69/2330 train_time:3854ms step_avg:55.85ms
step:70/2330 train_time:3911ms step_avg:55.88ms
step:71/2330 train_time:3966ms step_avg:55.86ms
step:72/2330 train_time:4023ms step_avg:55.88ms
step:73/2330 train_time:4078ms step_avg:55.86ms
step:74/2330 train_time:4136ms step_avg:55.90ms
step:75/2330 train_time:4191ms step_avg:55.88ms
step:76/2330 train_time:4249ms step_avg:55.90ms
step:77/2330 train_time:4304ms step_avg:55.90ms
step:78/2330 train_time:4362ms step_avg:55.92ms
step:79/2330 train_time:4417ms step_avg:55.91ms
step:80/2330 train_time:4475ms step_avg:55.94ms
step:81/2330 train_time:4530ms step_avg:55.93ms
step:82/2330 train_time:4588ms step_avg:55.95ms
step:83/2330 train_time:4644ms step_avg:55.95ms
step:84/2330 train_time:4702ms step_avg:55.97ms
step:85/2330 train_time:4756ms step_avg:55.96ms
step:86/2330 train_time:4814ms step_avg:55.98ms
step:87/2330 train_time:4869ms step_avg:55.97ms
step:88/2330 train_time:4927ms step_avg:55.99ms
step:89/2330 train_time:4982ms step_avg:55.98ms
step:90/2330 train_time:5039ms step_avg:55.99ms
step:91/2330 train_time:5095ms step_avg:55.99ms
step:92/2330 train_time:5153ms step_avg:56.01ms
step:93/2330 train_time:5208ms step_avg:56.00ms
step:94/2330 train_time:5266ms step_avg:56.02ms
step:95/2330 train_time:5322ms step_avg:56.02ms
step:96/2330 train_time:5379ms step_avg:56.03ms
step:97/2330 train_time:5434ms step_avg:56.02ms
step:98/2330 train_time:5492ms step_avg:56.04ms
step:99/2330 train_time:5547ms step_avg:56.03ms
step:100/2330 train_time:5605ms step_avg:56.05ms
step:101/2330 train_time:5660ms step_avg:56.04ms
step:102/2330 train_time:5719ms step_avg:56.07ms
step:103/2330 train_time:5774ms step_avg:56.06ms
step:104/2330 train_time:5831ms step_avg:56.07ms
step:105/2330 train_time:5886ms step_avg:56.06ms
step:106/2330 train_time:5944ms step_avg:56.08ms
step:107/2330 train_time:5999ms step_avg:56.06ms
step:108/2330 train_time:6058ms step_avg:56.09ms
step:109/2330 train_time:6112ms step_avg:56.08ms
step:110/2330 train_time:6170ms step_avg:56.09ms
step:111/2330 train_time:6226ms step_avg:56.09ms
step:112/2330 train_time:6283ms step_avg:56.10ms
step:113/2330 train_time:6338ms step_avg:56.08ms
step:114/2330 train_time:6396ms step_avg:56.11ms
step:115/2330 train_time:6451ms step_avg:56.09ms
step:116/2330 train_time:6510ms step_avg:56.12ms
step:117/2330 train_time:6564ms step_avg:56.10ms
step:118/2330 train_time:6623ms step_avg:56.13ms
step:119/2330 train_time:6678ms step_avg:56.12ms
step:120/2330 train_time:6735ms step_avg:56.13ms
step:121/2330 train_time:6790ms step_avg:56.12ms
step:122/2330 train_time:6848ms step_avg:56.13ms
step:123/2330 train_time:6904ms step_avg:56.13ms
step:124/2330 train_time:6962ms step_avg:56.14ms
step:125/2330 train_time:7017ms step_avg:56.14ms
step:126/2330 train_time:7075ms step_avg:56.15ms
step:127/2330 train_time:7130ms step_avg:56.14ms
step:128/2330 train_time:7188ms step_avg:56.16ms
step:129/2330 train_time:7243ms step_avg:56.15ms
step:130/2330 train_time:7301ms step_avg:56.16ms
step:131/2330 train_time:7355ms step_avg:56.15ms
step:132/2330 train_time:7414ms step_avg:56.16ms
step:133/2330 train_time:7469ms step_avg:56.16ms
step:134/2330 train_time:7527ms step_avg:56.17ms
step:135/2330 train_time:7582ms step_avg:56.16ms
step:136/2330 train_time:7640ms step_avg:56.18ms
step:137/2330 train_time:7695ms step_avg:56.17ms
step:138/2330 train_time:7754ms step_avg:56.19ms
step:139/2330 train_time:7809ms step_avg:56.18ms
step:140/2330 train_time:7868ms step_avg:56.20ms
step:141/2330 train_time:7923ms step_avg:56.19ms
step:142/2330 train_time:7982ms step_avg:56.21ms
step:143/2330 train_time:8036ms step_avg:56.20ms
step:144/2330 train_time:8095ms step_avg:56.21ms
step:145/2330 train_time:8150ms step_avg:56.21ms
step:146/2330 train_time:8208ms step_avg:56.22ms
step:147/2330 train_time:8263ms step_avg:56.21ms
step:148/2330 train_time:8322ms step_avg:56.23ms
step:149/2330 train_time:8376ms step_avg:56.22ms
step:150/2330 train_time:8435ms step_avg:56.23ms
step:151/2330 train_time:8490ms step_avg:56.22ms
step:152/2330 train_time:8549ms step_avg:56.24ms
step:153/2330 train_time:8605ms step_avg:56.24ms
step:154/2330 train_time:8663ms step_avg:56.25ms
step:155/2330 train_time:8718ms step_avg:56.24ms
step:156/2330 train_time:8776ms step_avg:56.26ms
step:157/2330 train_time:8830ms step_avg:56.24ms
step:158/2330 train_time:8890ms step_avg:56.27ms
step:159/2330 train_time:8945ms step_avg:56.26ms
step:160/2330 train_time:9004ms step_avg:56.27ms
step:161/2330 train_time:9058ms step_avg:56.26ms
step:162/2330 train_time:9116ms step_avg:56.27ms
step:163/2330 train_time:9171ms step_avg:56.26ms
step:164/2330 train_time:9230ms step_avg:56.28ms
step:165/2330 train_time:9285ms step_avg:56.27ms
step:166/2330 train_time:9343ms step_avg:56.28ms
step:167/2330 train_time:9398ms step_avg:56.27ms
step:168/2330 train_time:9456ms step_avg:56.28ms
step:169/2330 train_time:9511ms step_avg:56.28ms
step:170/2330 train_time:9570ms step_avg:56.29ms
step:171/2330 train_time:9625ms step_avg:56.29ms
step:172/2330 train_time:9684ms step_avg:56.30ms
step:173/2330 train_time:9739ms step_avg:56.29ms
step:174/2330 train_time:9797ms step_avg:56.31ms
step:175/2330 train_time:9853ms step_avg:56.30ms
step:176/2330 train_time:9911ms step_avg:56.32ms
step:177/2330 train_time:9967ms step_avg:56.31ms
step:178/2330 train_time:10026ms step_avg:56.33ms
step:179/2330 train_time:10082ms step_avg:56.32ms
step:180/2330 train_time:10140ms step_avg:56.33ms
step:181/2330 train_time:10196ms step_avg:56.33ms
step:182/2330 train_time:10255ms step_avg:56.34ms
step:183/2330 train_time:10311ms step_avg:56.34ms
step:184/2330 train_time:10369ms step_avg:56.35ms
step:185/2330 train_time:10425ms step_avg:56.35ms
step:186/2330 train_time:10483ms step_avg:56.36ms
step:187/2330 train_time:10538ms step_avg:56.36ms
step:188/2330 train_time:10597ms step_avg:56.37ms
step:189/2330 train_time:10652ms step_avg:56.36ms
step:190/2330 train_time:10710ms step_avg:56.37ms
step:191/2330 train_time:10766ms step_avg:56.37ms
step:192/2330 train_time:10825ms step_avg:56.38ms
step:193/2330 train_time:10880ms step_avg:56.38ms
step:194/2330 train_time:10939ms step_avg:56.39ms
step:195/2330 train_time:10994ms step_avg:56.38ms
step:196/2330 train_time:11053ms step_avg:56.39ms
step:197/2330 train_time:11109ms step_avg:56.39ms
step:198/2330 train_time:11168ms step_avg:56.40ms
step:199/2330 train_time:11224ms step_avg:56.40ms
step:200/2330 train_time:11282ms step_avg:56.41ms
step:201/2330 train_time:11338ms step_avg:56.41ms
step:202/2330 train_time:11397ms step_avg:56.42ms
step:203/2330 train_time:11452ms step_avg:56.41ms
step:204/2330 train_time:11511ms step_avg:56.43ms
step:205/2330 train_time:11567ms step_avg:56.42ms
step:206/2330 train_time:11625ms step_avg:56.43ms
step:207/2330 train_time:11681ms step_avg:56.43ms
step:208/2330 train_time:11741ms step_avg:56.45ms
step:209/2330 train_time:11796ms step_avg:56.44ms
step:210/2330 train_time:11854ms step_avg:56.45ms
step:211/2330 train_time:11911ms step_avg:56.45ms
step:212/2330 train_time:11969ms step_avg:56.46ms
step:213/2330 train_time:12025ms step_avg:56.45ms
step:214/2330 train_time:12083ms step_avg:56.46ms
step:215/2330 train_time:12139ms step_avg:56.46ms
step:216/2330 train_time:12198ms step_avg:56.47ms
step:217/2330 train_time:12254ms step_avg:56.47ms
step:218/2330 train_time:12313ms step_avg:56.48ms
step:219/2330 train_time:12369ms step_avg:56.48ms
step:220/2330 train_time:12428ms step_avg:56.49ms
step:221/2330 train_time:12483ms step_avg:56.49ms
step:222/2330 train_time:12542ms step_avg:56.49ms
step:223/2330 train_time:12597ms step_avg:56.49ms
step:224/2330 train_time:12656ms step_avg:56.50ms
step:225/2330 train_time:12712ms step_avg:56.50ms
step:226/2330 train_time:12771ms step_avg:56.51ms
step:227/2330 train_time:12827ms step_avg:56.51ms
step:228/2330 train_time:12886ms step_avg:56.52ms
step:229/2330 train_time:12942ms step_avg:56.51ms
step:230/2330 train_time:13000ms step_avg:56.52ms
step:231/2330 train_time:13055ms step_avg:56.52ms
step:232/2330 train_time:13115ms step_avg:56.53ms
step:233/2330 train_time:13170ms step_avg:56.53ms
step:234/2330 train_time:13230ms step_avg:56.54ms
step:235/2330 train_time:13286ms step_avg:56.54ms
step:236/2330 train_time:13345ms step_avg:56.55ms
step:237/2330 train_time:13401ms step_avg:56.54ms
step:238/2330 train_time:13459ms step_avg:56.55ms
step:239/2330 train_time:13515ms step_avg:56.55ms
step:240/2330 train_time:13574ms step_avg:56.56ms
step:241/2330 train_time:13630ms step_avg:56.55ms
step:242/2330 train_time:13689ms step_avg:56.57ms
step:243/2330 train_time:13744ms step_avg:56.56ms
step:244/2330 train_time:13805ms step_avg:56.58ms
step:245/2330 train_time:13860ms step_avg:56.57ms
step:246/2330 train_time:13919ms step_avg:56.58ms
step:247/2330 train_time:13975ms step_avg:56.58ms
step:248/2330 train_time:14034ms step_avg:56.59ms
step:249/2330 train_time:14089ms step_avg:56.58ms
step:250/2330 train_time:14148ms step_avg:56.59ms
step:250/2330 val_loss:6.5634 train_time:14227ms step_avg:56.91ms
step:251/2330 train_time:14247ms step_avg:56.76ms
step:252/2330 train_time:14266ms step_avg:56.61ms
step:253/2330 train_time:14320ms step_avg:56.60ms
step:254/2330 train_time:14384ms step_avg:56.63ms
step:255/2330 train_time:14439ms step_avg:56.62ms
step:256/2330 train_time:14502ms step_avg:56.65ms
step:257/2330 train_time:14557ms step_avg:56.64ms
step:258/2330 train_time:14618ms step_avg:56.66ms
step:259/2330 train_time:14672ms step_avg:56.65ms
step:260/2330 train_time:14732ms step_avg:56.66ms
step:261/2330 train_time:14787ms step_avg:56.65ms
step:262/2330 train_time:14845ms step_avg:56.66ms
step:263/2330 train_time:14900ms step_avg:56.66ms
step:264/2330 train_time:14959ms step_avg:56.66ms
step:265/2330 train_time:15014ms step_avg:56.66ms
step:266/2330 train_time:15073ms step_avg:56.67ms
step:267/2330 train_time:15129ms step_avg:56.66ms
step:268/2330 train_time:15188ms step_avg:56.67ms
step:269/2330 train_time:15245ms step_avg:56.67ms
step:270/2330 train_time:15305ms step_avg:56.68ms
step:271/2330 train_time:15362ms step_avg:56.69ms
step:272/2330 train_time:15421ms step_avg:56.69ms
step:273/2330 train_time:15477ms step_avg:56.69ms
step:274/2330 train_time:15537ms step_avg:56.70ms
step:275/2330 train_time:15593ms step_avg:56.70ms
step:276/2330 train_time:15653ms step_avg:56.71ms
step:277/2330 train_time:15708ms step_avg:56.71ms
step:278/2330 train_time:15767ms step_avg:56.72ms
step:279/2330 train_time:15822ms step_avg:56.71ms
step:280/2330 train_time:15882ms step_avg:56.72ms
step:281/2330 train_time:15937ms step_avg:56.72ms
step:282/2330 train_time:15996ms step_avg:56.72ms
step:283/2330 train_time:16051ms step_avg:56.72ms
step:284/2330 train_time:16111ms step_avg:56.73ms
step:285/2330 train_time:16167ms step_avg:56.73ms
step:286/2330 train_time:16226ms step_avg:56.73ms
step:287/2330 train_time:16282ms step_avg:56.73ms
step:288/2330 train_time:16342ms step_avg:56.74ms
step:289/2330 train_time:16398ms step_avg:56.74ms
step:290/2330 train_time:16457ms step_avg:56.75ms
step:291/2330 train_time:16513ms step_avg:56.75ms
step:292/2330 train_time:16574ms step_avg:56.76ms
step:293/2330 train_time:16629ms step_avg:56.76ms
step:294/2330 train_time:16689ms step_avg:56.77ms
step:295/2330 train_time:16744ms step_avg:56.76ms
step:296/2330 train_time:16804ms step_avg:56.77ms
step:297/2330 train_time:16859ms step_avg:56.77ms
step:298/2330 train_time:16918ms step_avg:56.77ms
step:299/2330 train_time:16973ms step_avg:56.77ms
step:300/2330 train_time:17033ms step_avg:56.78ms
step:301/2330 train_time:17088ms step_avg:56.77ms
step:302/2330 train_time:17147ms step_avg:56.78ms
step:303/2330 train_time:17203ms step_avg:56.78ms
step:304/2330 train_time:17262ms step_avg:56.78ms
step:305/2330 train_time:17319ms step_avg:56.78ms
step:306/2330 train_time:17378ms step_avg:56.79ms
step:307/2330 train_time:17434ms step_avg:56.79ms
step:308/2330 train_time:17494ms step_avg:56.80ms
step:309/2330 train_time:17550ms step_avg:56.80ms
step:310/2330 train_time:17609ms step_avg:56.80ms
step:311/2330 train_time:17665ms step_avg:56.80ms
step:312/2330 train_time:17724ms step_avg:56.81ms
step:313/2330 train_time:17780ms step_avg:56.81ms
step:314/2330 train_time:17839ms step_avg:56.81ms
step:315/2330 train_time:17894ms step_avg:56.81ms
step:316/2330 train_time:17953ms step_avg:56.81ms
step:317/2330 train_time:18009ms step_avg:56.81ms
step:318/2330 train_time:18068ms step_avg:56.82ms
step:319/2330 train_time:18123ms step_avg:56.81ms
step:320/2330 train_time:18182ms step_avg:56.82ms
step:321/2330 train_time:18238ms step_avg:56.81ms
step:322/2330 train_time:18297ms step_avg:56.82ms
step:323/2330 train_time:18353ms step_avg:56.82ms
step:324/2330 train_time:18414ms step_avg:56.83ms
step:325/2330 train_time:18470ms step_avg:56.83ms
step:326/2330 train_time:18530ms step_avg:56.84ms
step:327/2330 train_time:18585ms step_avg:56.83ms
step:328/2330 train_time:18644ms step_avg:56.84ms
step:329/2330 train_time:18702ms step_avg:56.84ms
step:330/2330 train_time:18760ms step_avg:56.85ms
step:331/2330 train_time:18816ms step_avg:56.85ms
step:332/2330 train_time:18876ms step_avg:56.86ms
step:333/2330 train_time:18932ms step_avg:56.85ms
step:334/2330 train_time:18992ms step_avg:56.86ms
step:335/2330 train_time:19047ms step_avg:56.86ms
step:336/2330 train_time:19107ms step_avg:56.87ms
step:337/2330 train_time:19162ms step_avg:56.86ms
step:338/2330 train_time:19221ms step_avg:56.87ms
step:339/2330 train_time:19277ms step_avg:56.87ms
step:340/2330 train_time:19336ms step_avg:56.87ms
step:341/2330 train_time:19392ms step_avg:56.87ms
step:342/2330 train_time:19453ms step_avg:56.88ms
step:343/2330 train_time:19509ms step_avg:56.88ms
step:344/2330 train_time:19568ms step_avg:56.88ms
step:345/2330 train_time:19623ms step_avg:56.88ms
step:346/2330 train_time:19683ms step_avg:56.89ms
step:347/2330 train_time:19739ms step_avg:56.88ms
step:348/2330 train_time:19798ms step_avg:56.89ms
step:349/2330 train_time:19854ms step_avg:56.89ms
step:350/2330 train_time:19913ms step_avg:56.89ms
step:351/2330 train_time:19969ms step_avg:56.89ms
step:352/2330 train_time:20027ms step_avg:56.89ms
step:353/2330 train_time:20083ms step_avg:56.89ms
step:354/2330 train_time:20142ms step_avg:56.90ms
step:355/2330 train_time:20198ms step_avg:56.90ms
step:356/2330 train_time:20257ms step_avg:56.90ms
step:357/2330 train_time:20313ms step_avg:56.90ms
step:358/2330 train_time:20372ms step_avg:56.91ms
step:359/2330 train_time:20428ms step_avg:56.90ms
step:360/2330 train_time:20488ms step_avg:56.91ms
step:361/2330 train_time:20544ms step_avg:56.91ms
step:362/2330 train_time:20603ms step_avg:56.91ms
step:363/2330 train_time:20658ms step_avg:56.91ms
step:364/2330 train_time:20718ms step_avg:56.92ms
step:365/2330 train_time:20773ms step_avg:56.91ms
step:366/2330 train_time:20833ms step_avg:56.92ms
step:367/2330 train_time:20889ms step_avg:56.92ms
step:368/2330 train_time:20948ms step_avg:56.92ms
step:369/2330 train_time:21003ms step_avg:56.92ms
step:370/2330 train_time:21063ms step_avg:56.93ms
step:371/2330 train_time:21118ms step_avg:56.92ms
step:372/2330 train_time:21178ms step_avg:56.93ms
step:373/2330 train_time:21234ms step_avg:56.93ms
step:374/2330 train_time:21294ms step_avg:56.93ms
step:375/2330 train_time:21349ms step_avg:56.93ms
step:376/2330 train_time:21409ms step_avg:56.94ms
step:377/2330 train_time:21464ms step_avg:56.93ms
step:378/2330 train_time:21524ms step_avg:56.94ms
step:379/2330 train_time:21580ms step_avg:56.94ms
step:380/2330 train_time:21639ms step_avg:56.94ms
step:381/2330 train_time:21695ms step_avg:56.94ms
step:382/2330 train_time:21754ms step_avg:56.95ms
step:383/2330 train_time:21810ms step_avg:56.95ms
step:384/2330 train_time:21870ms step_avg:56.95ms
step:385/2330 train_time:21925ms step_avg:56.95ms
step:386/2330 train_time:21985ms step_avg:56.96ms
step:387/2330 train_time:22041ms step_avg:56.95ms
step:388/2330 train_time:22100ms step_avg:56.96ms
step:389/2330 train_time:22156ms step_avg:56.96ms
step:390/2330 train_time:22216ms step_avg:56.96ms
step:391/2330 train_time:22271ms step_avg:56.96ms
step:392/2330 train_time:22331ms step_avg:56.97ms
step:393/2330 train_time:22387ms step_avg:56.96ms
step:394/2330 train_time:22446ms step_avg:56.97ms
step:395/2330 train_time:22501ms step_avg:56.97ms
step:396/2330 train_time:22561ms step_avg:56.97ms
step:397/2330 train_time:22617ms step_avg:56.97ms
step:398/2330 train_time:22676ms step_avg:56.97ms
step:399/2330 train_time:22732ms step_avg:56.97ms
step:400/2330 train_time:22792ms step_avg:56.98ms
step:401/2330 train_time:22848ms step_avg:56.98ms
step:402/2330 train_time:22908ms step_avg:56.98ms
step:403/2330 train_time:22963ms step_avg:56.98ms
step:404/2330 train_time:23022ms step_avg:56.98ms
step:405/2330 train_time:23077ms step_avg:56.98ms
step:406/2330 train_time:23137ms step_avg:56.99ms
step:407/2330 train_time:23192ms step_avg:56.98ms
step:408/2330 train_time:23253ms step_avg:56.99ms
step:409/2330 train_time:23309ms step_avg:56.99ms
step:410/2330 train_time:23368ms step_avg:56.99ms
step:411/2330 train_time:23423ms step_avg:56.99ms
step:412/2330 train_time:23482ms step_avg:57.00ms
step:413/2330 train_time:23538ms step_avg:56.99ms
step:414/2330 train_time:23598ms step_avg:57.00ms
step:415/2330 train_time:23654ms step_avg:57.00ms
step:416/2330 train_time:23714ms step_avg:57.00ms
step:417/2330 train_time:23769ms step_avg:57.00ms
step:418/2330 train_time:23828ms step_avg:57.01ms
step:419/2330 train_time:23884ms step_avg:57.00ms
step:420/2330 train_time:23943ms step_avg:57.01ms
step:421/2330 train_time:23999ms step_avg:57.00ms
step:422/2330 train_time:24058ms step_avg:57.01ms
step:423/2330 train_time:24113ms step_avg:57.01ms
step:424/2330 train_time:24173ms step_avg:57.01ms
step:425/2330 train_time:24228ms step_avg:57.01ms
step:426/2330 train_time:24288ms step_avg:57.01ms
step:427/2330 train_time:24344ms step_avg:57.01ms
step:428/2330 train_time:24403ms step_avg:57.02ms
step:429/2330 train_time:24460ms step_avg:57.02ms
step:430/2330 train_time:24518ms step_avg:57.02ms
step:431/2330 train_time:24574ms step_avg:57.02ms
step:432/2330 train_time:24634ms step_avg:57.02ms
step:433/2330 train_time:24689ms step_avg:57.02ms
step:434/2330 train_time:24749ms step_avg:57.02ms
step:435/2330 train_time:24804ms step_avg:57.02ms
step:436/2330 train_time:24864ms step_avg:57.03ms
step:437/2330 train_time:24920ms step_avg:57.03ms
step:438/2330 train_time:24979ms step_avg:57.03ms
step:439/2330 train_time:25035ms step_avg:57.03ms
step:440/2330 train_time:25095ms step_avg:57.03ms
step:441/2330 train_time:25151ms step_avg:57.03ms
step:442/2330 train_time:25210ms step_avg:57.04ms
step:443/2330 train_time:25265ms step_avg:57.03ms
step:444/2330 train_time:25325ms step_avg:57.04ms
step:445/2330 train_time:25381ms step_avg:57.04ms
step:446/2330 train_time:25441ms step_avg:57.04ms
step:447/2330 train_time:25497ms step_avg:57.04ms
step:448/2330 train_time:25556ms step_avg:57.04ms
step:449/2330 train_time:25612ms step_avg:57.04ms
step:450/2330 train_time:25671ms step_avg:57.05ms
step:451/2330 train_time:25727ms step_avg:57.04ms
step:452/2330 train_time:25786ms step_avg:57.05ms
step:453/2330 train_time:25841ms step_avg:57.05ms
step:454/2330 train_time:25901ms step_avg:57.05ms
step:455/2330 train_time:25957ms step_avg:57.05ms
step:456/2330 train_time:26016ms step_avg:57.05ms
step:457/2330 train_time:26072ms step_avg:57.05ms
step:458/2330 train_time:26132ms step_avg:57.06ms
step:459/2330 train_time:26188ms step_avg:57.05ms
step:460/2330 train_time:26247ms step_avg:57.06ms
step:461/2330 train_time:26303ms step_avg:57.06ms
step:462/2330 train_time:26362ms step_avg:57.06ms
step:463/2330 train_time:26418ms step_avg:57.06ms
step:464/2330 train_time:26477ms step_avg:57.06ms
step:465/2330 train_time:26533ms step_avg:57.06ms
step:466/2330 train_time:26593ms step_avg:57.07ms
step:467/2330 train_time:26649ms step_avg:57.06ms
step:468/2330 train_time:26708ms step_avg:57.07ms
step:469/2330 train_time:26764ms step_avg:57.07ms
step:470/2330 train_time:26823ms step_avg:57.07ms
step:471/2330 train_time:26880ms step_avg:57.07ms
step:472/2330 train_time:26939ms step_avg:57.07ms
step:473/2330 train_time:26995ms step_avg:57.07ms
step:474/2330 train_time:27055ms step_avg:57.08ms
step:475/2330 train_time:27111ms step_avg:57.08ms
step:476/2330 train_time:27170ms step_avg:57.08ms
step:477/2330 train_time:27226ms step_avg:57.08ms
step:478/2330 train_time:27286ms step_avg:57.08ms
step:479/2330 train_time:27342ms step_avg:57.08ms
step:480/2330 train_time:27400ms step_avg:57.08ms
step:481/2330 train_time:27456ms step_avg:57.08ms
step:482/2330 train_time:27517ms step_avg:57.09ms
step:483/2330 train_time:27573ms step_avg:57.09ms
step:484/2330 train_time:27632ms step_avg:57.09ms
step:485/2330 train_time:27688ms step_avg:57.09ms
step:486/2330 train_time:27747ms step_avg:57.09ms
step:487/2330 train_time:27803ms step_avg:57.09ms
step:488/2330 train_time:27862ms step_avg:57.09ms
step:489/2330 train_time:27918ms step_avg:57.09ms
step:490/2330 train_time:27977ms step_avg:57.10ms
step:491/2330 train_time:28034ms step_avg:57.10ms
step:492/2330 train_time:28094ms step_avg:57.10ms
step:493/2330 train_time:28150ms step_avg:57.10ms
step:494/2330 train_time:28209ms step_avg:57.10ms
step:495/2330 train_time:28264ms step_avg:57.10ms
step:496/2330 train_time:28324ms step_avg:57.10ms
step:497/2330 train_time:28380ms step_avg:57.10ms
step:498/2330 train_time:28439ms step_avg:57.11ms
step:499/2330 train_time:28495ms step_avg:57.10ms
step:500/2330 train_time:28554ms step_avg:57.11ms
step:500/2330 val_loss:6.0572 train_time:28633ms step_avg:57.27ms
step:501/2330 train_time:28654ms step_avg:57.19ms
step:502/2330 train_time:28673ms step_avg:57.12ms
step:503/2330 train_time:28730ms step_avg:57.12ms
step:504/2330 train_time:28791ms step_avg:57.12ms
step:505/2330 train_time:28847ms step_avg:57.12ms
step:506/2330 train_time:28908ms step_avg:57.13ms
step:507/2330 train_time:28963ms step_avg:57.13ms
step:508/2330 train_time:29022ms step_avg:57.13ms
step:509/2330 train_time:29078ms step_avg:57.13ms
step:510/2330 train_time:29138ms step_avg:57.13ms
step:511/2330 train_time:29194ms step_avg:57.13ms
step:512/2330 train_time:29252ms step_avg:57.13ms
step:513/2330 train_time:29308ms step_avg:57.13ms
step:514/2330 train_time:29367ms step_avg:57.13ms
step:515/2330 train_time:29422ms step_avg:57.13ms
step:516/2330 train_time:29481ms step_avg:57.13ms
step:517/2330 train_time:29536ms step_avg:57.13ms
step:518/2330 train_time:29595ms step_avg:57.13ms
step:519/2330 train_time:29652ms step_avg:57.13ms
step:520/2330 train_time:29712ms step_avg:57.14ms
step:521/2330 train_time:29769ms step_avg:57.14ms
step:522/2330 train_time:29829ms step_avg:57.14ms
step:523/2330 train_time:29886ms step_avg:57.14ms
step:524/2330 train_time:29946ms step_avg:57.15ms
step:525/2330 train_time:30002ms step_avg:57.15ms
step:526/2330 train_time:30061ms step_avg:57.15ms
step:527/2330 train_time:30117ms step_avg:57.15ms
step:528/2330 train_time:30175ms step_avg:57.15ms
step:529/2330 train_time:30231ms step_avg:57.15ms
step:530/2330 train_time:30290ms step_avg:57.15ms
step:531/2330 train_time:30346ms step_avg:57.15ms
step:532/2330 train_time:30405ms step_avg:57.15ms
step:533/2330 train_time:30460ms step_avg:57.15ms
step:534/2330 train_time:30519ms step_avg:57.15ms
step:535/2330 train_time:30575ms step_avg:57.15ms
step:536/2330 train_time:30634ms step_avg:57.15ms
step:537/2330 train_time:30691ms step_avg:57.15ms
step:538/2330 train_time:30751ms step_avg:57.16ms
step:539/2330 train_time:30807ms step_avg:57.16ms
step:540/2330 train_time:30867ms step_avg:57.16ms
step:541/2330 train_time:30924ms step_avg:57.16ms
step:542/2330 train_time:30984ms step_avg:57.17ms
step:543/2330 train_time:31040ms step_avg:57.16ms
step:544/2330 train_time:31099ms step_avg:57.17ms
step:545/2330 train_time:31155ms step_avg:57.17ms
step:546/2330 train_time:31214ms step_avg:57.17ms
step:547/2330 train_time:31271ms step_avg:57.17ms
step:548/2330 train_time:31330ms step_avg:57.17ms
step:549/2330 train_time:31387ms step_avg:57.17ms
step:550/2330 train_time:31446ms step_avg:57.17ms
step:551/2330 train_time:31502ms step_avg:57.17ms
step:552/2330 train_time:31560ms step_avg:57.17ms
step:553/2330 train_time:31616ms step_avg:57.17ms
step:554/2330 train_time:31675ms step_avg:57.18ms
step:555/2330 train_time:31731ms step_avg:57.17ms
step:556/2330 train_time:31791ms step_avg:57.18ms
step:557/2330 train_time:31848ms step_avg:57.18ms
step:558/2330 train_time:31908ms step_avg:57.18ms
step:559/2330 train_time:31964ms step_avg:57.18ms
step:560/2330 train_time:32024ms step_avg:57.19ms
step:561/2330 train_time:32080ms step_avg:57.18ms
step:562/2330 train_time:32140ms step_avg:57.19ms
step:563/2330 train_time:32196ms step_avg:57.19ms
step:564/2330 train_time:32255ms step_avg:57.19ms
step:565/2330 train_time:32311ms step_avg:57.19ms
step:566/2330 train_time:32370ms step_avg:57.19ms
step:567/2330 train_time:32426ms step_avg:57.19ms
step:568/2330 train_time:32485ms step_avg:57.19ms
step:569/2330 train_time:32541ms step_avg:57.19ms
step:570/2330 train_time:32600ms step_avg:57.19ms
step:571/2330 train_time:32656ms step_avg:57.19ms
step:572/2330 train_time:32714ms step_avg:57.19ms
step:573/2330 train_time:32771ms step_avg:57.19ms
step:574/2330 train_time:32831ms step_avg:57.20ms
step:575/2330 train_time:32887ms step_avg:57.20ms
step:576/2330 train_time:32948ms step_avg:57.20ms
step:577/2330 train_time:33004ms step_avg:57.20ms
step:578/2330 train_time:33064ms step_avg:57.20ms
step:579/2330 train_time:33119ms step_avg:57.20ms
step:580/2330 train_time:33178ms step_avg:57.20ms
step:581/2330 train_time:33234ms step_avg:57.20ms
step:582/2330 train_time:33293ms step_avg:57.20ms
step:583/2330 train_time:33349ms step_avg:57.20ms
step:584/2330 train_time:33408ms step_avg:57.21ms
step:585/2330 train_time:33464ms step_avg:57.20ms
step:586/2330 train_time:33524ms step_avg:57.21ms
step:587/2330 train_time:33580ms step_avg:57.21ms
step:588/2330 train_time:33639ms step_avg:57.21ms
step:589/2330 train_time:33695ms step_avg:57.21ms
step:590/2330 train_time:33754ms step_avg:57.21ms
step:591/2330 train_time:33810ms step_avg:57.21ms
step:592/2330 train_time:33869ms step_avg:57.21ms
step:593/2330 train_time:33925ms step_avg:57.21ms
step:594/2330 train_time:33986ms step_avg:57.21ms
step:595/2330 train_time:34041ms step_avg:57.21ms
step:596/2330 train_time:34101ms step_avg:57.22ms
step:597/2330 train_time:34157ms step_avg:57.21ms
step:598/2330 train_time:34217ms step_avg:57.22ms
step:599/2330 train_time:34272ms step_avg:57.22ms
step:600/2330 train_time:34332ms step_avg:57.22ms
step:601/2330 train_time:34387ms step_avg:57.22ms
step:602/2330 train_time:34447ms step_avg:57.22ms
step:603/2330 train_time:34503ms step_avg:57.22ms
step:604/2330 train_time:34563ms step_avg:57.22ms
step:605/2330 train_time:34618ms step_avg:57.22ms
step:606/2330 train_time:34677ms step_avg:57.22ms
step:607/2330 train_time:34734ms step_avg:57.22ms
step:608/2330 train_time:34793ms step_avg:57.23ms
step:609/2330 train_time:34849ms step_avg:57.22ms
step:610/2330 train_time:34908ms step_avg:57.23ms
step:611/2330 train_time:34964ms step_avg:57.22ms
step:612/2330 train_time:35025ms step_avg:57.23ms
step:613/2330 train_time:35081ms step_avg:57.23ms
step:614/2330 train_time:35140ms step_avg:57.23ms
step:615/2330 train_time:35196ms step_avg:57.23ms
step:616/2330 train_time:35255ms step_avg:57.23ms
step:617/2330 train_time:35312ms step_avg:57.23ms
step:618/2330 train_time:35372ms step_avg:57.24ms
step:619/2330 train_time:35428ms step_avg:57.23ms
step:620/2330 train_time:35487ms step_avg:57.24ms
step:621/2330 train_time:35543ms step_avg:57.23ms
step:622/2330 train_time:35602ms step_avg:57.24ms
step:623/2330 train_time:35658ms step_avg:57.24ms
step:624/2330 train_time:35717ms step_avg:57.24ms
step:625/2330 train_time:35773ms step_avg:57.24ms
step:626/2330 train_time:35833ms step_avg:57.24ms
step:627/2330 train_time:35889ms step_avg:57.24ms
step:628/2330 train_time:35949ms step_avg:57.24ms
step:629/2330 train_time:36006ms step_avg:57.24ms
step:630/2330 train_time:36066ms step_avg:57.25ms
step:631/2330 train_time:36122ms step_avg:57.25ms
step:632/2330 train_time:36181ms step_avg:57.25ms
step:633/2330 train_time:36237ms step_avg:57.25ms
step:634/2330 train_time:36297ms step_avg:57.25ms
step:635/2330 train_time:36353ms step_avg:57.25ms
step:636/2330 train_time:36412ms step_avg:57.25ms
step:637/2330 train_time:36469ms step_avg:57.25ms
step:638/2330 train_time:36527ms step_avg:57.25ms
step:639/2330 train_time:36583ms step_avg:57.25ms
step:640/2330 train_time:36642ms step_avg:57.25ms
step:641/2330 train_time:36698ms step_avg:57.25ms
step:642/2330 train_time:36758ms step_avg:57.26ms
step:643/2330 train_time:36814ms step_avg:57.25ms
step:644/2330 train_time:36873ms step_avg:57.26ms
step:645/2330 train_time:36930ms step_avg:57.26ms
step:646/2330 train_time:36989ms step_avg:57.26ms
step:647/2330 train_time:37046ms step_avg:57.26ms
step:648/2330 train_time:37105ms step_avg:57.26ms
step:649/2330 train_time:37161ms step_avg:57.26ms
step:650/2330 train_time:37220ms step_avg:57.26ms
step:651/2330 train_time:37276ms step_avg:57.26ms
step:652/2330 train_time:37335ms step_avg:57.26ms
step:653/2330 train_time:37392ms step_avg:57.26ms
step:654/2330 train_time:37451ms step_avg:57.26ms
step:655/2330 train_time:37507ms step_avg:57.26ms
step:656/2330 train_time:37567ms step_avg:57.27ms
step:657/2330 train_time:37622ms step_avg:57.26ms
step:658/2330 train_time:37682ms step_avg:57.27ms
step:659/2330 train_time:37737ms step_avg:57.26ms
step:660/2330 train_time:37797ms step_avg:57.27ms
step:661/2330 train_time:37853ms step_avg:57.27ms
step:662/2330 train_time:37912ms step_avg:57.27ms
step:663/2330 train_time:37968ms step_avg:57.27ms
step:664/2330 train_time:38028ms step_avg:57.27ms
step:665/2330 train_time:38084ms step_avg:57.27ms
step:666/2330 train_time:38144ms step_avg:57.27ms
step:667/2330 train_time:38201ms step_avg:57.27ms
step:668/2330 train_time:38259ms step_avg:57.27ms
step:669/2330 train_time:38315ms step_avg:57.27ms
step:670/2330 train_time:38374ms step_avg:57.28ms
step:671/2330 train_time:38431ms step_avg:57.27ms
step:672/2330 train_time:38491ms step_avg:57.28ms
step:673/2330 train_time:38549ms step_avg:57.28ms
step:674/2330 train_time:38608ms step_avg:57.28ms
step:675/2330 train_time:38664ms step_avg:57.28ms
step:676/2330 train_time:38723ms step_avg:57.28ms
step:677/2330 train_time:38779ms step_avg:57.28ms
step:678/2330 train_time:38838ms step_avg:57.28ms
step:679/2330 train_time:38894ms step_avg:57.28ms
step:680/2330 train_time:38952ms step_avg:57.28ms
step:681/2330 train_time:39008ms step_avg:57.28ms
step:682/2330 train_time:39069ms step_avg:57.29ms
step:683/2330 train_time:39125ms step_avg:57.28ms
step:684/2330 train_time:39185ms step_avg:57.29ms
step:685/2330 train_time:39241ms step_avg:57.29ms
step:686/2330 train_time:39300ms step_avg:57.29ms
step:687/2330 train_time:39356ms step_avg:57.29ms
step:688/2330 train_time:39415ms step_avg:57.29ms
step:689/2330 train_time:39471ms step_avg:57.29ms
step:690/2330 train_time:39531ms step_avg:57.29ms
step:691/2330 train_time:39587ms step_avg:57.29ms
step:692/2330 train_time:39647ms step_avg:57.29ms
step:693/2330 train_time:39703ms step_avg:57.29ms
step:694/2330 train_time:39762ms step_avg:57.29ms
step:695/2330 train_time:39818ms step_avg:57.29ms
step:696/2330 train_time:39878ms step_avg:57.30ms
step:697/2330 train_time:39934ms step_avg:57.29ms
step:698/2330 train_time:39993ms step_avg:57.30ms
step:699/2330 train_time:40050ms step_avg:57.30ms
step:700/2330 train_time:40111ms step_avg:57.30ms
step:701/2330 train_time:40167ms step_avg:57.30ms
step:702/2330 train_time:40226ms step_avg:57.30ms
step:703/2330 train_time:40282ms step_avg:57.30ms
step:704/2330 train_time:40342ms step_avg:57.30ms
step:705/2330 train_time:40397ms step_avg:57.30ms
step:706/2330 train_time:40456ms step_avg:57.30ms
step:707/2330 train_time:40512ms step_avg:57.30ms
step:708/2330 train_time:40572ms step_avg:57.30ms
step:709/2330 train_time:40628ms step_avg:57.30ms
step:710/2330 train_time:40689ms step_avg:57.31ms
step:711/2330 train_time:40746ms step_avg:57.31ms
step:712/2330 train_time:40805ms step_avg:57.31ms
step:713/2330 train_time:40861ms step_avg:57.31ms
step:714/2330 train_time:40920ms step_avg:57.31ms
step:715/2330 train_time:40976ms step_avg:57.31ms
step:716/2330 train_time:41035ms step_avg:57.31ms
step:717/2330 train_time:41092ms step_avg:57.31ms
step:718/2330 train_time:41152ms step_avg:57.31ms
step:719/2330 train_time:41208ms step_avg:57.31ms
step:720/2330 train_time:41268ms step_avg:57.32ms
step:721/2330 train_time:41324ms step_avg:57.31ms
step:722/2330 train_time:41383ms step_avg:57.32ms
step:723/2330 train_time:41439ms step_avg:57.32ms
step:724/2330 train_time:41499ms step_avg:57.32ms
step:725/2330 train_time:41555ms step_avg:57.32ms
step:726/2330 train_time:41614ms step_avg:57.32ms
step:727/2330 train_time:41670ms step_avg:57.32ms
step:728/2330 train_time:41730ms step_avg:57.32ms
step:729/2330 train_time:41786ms step_avg:57.32ms
step:730/2330 train_time:41845ms step_avg:57.32ms
step:731/2330 train_time:41901ms step_avg:57.32ms
step:732/2330 train_time:41960ms step_avg:57.32ms
step:733/2330 train_time:42016ms step_avg:57.32ms
step:734/2330 train_time:42075ms step_avg:57.32ms
step:735/2330 train_time:42132ms step_avg:57.32ms
step:736/2330 train_time:42192ms step_avg:57.33ms
step:737/2330 train_time:42249ms step_avg:57.33ms
step:738/2330 train_time:42308ms step_avg:57.33ms
step:739/2330 train_time:42364ms step_avg:57.33ms
step:740/2330 train_time:42424ms step_avg:57.33ms
step:741/2330 train_time:42481ms step_avg:57.33ms
step:742/2330 train_time:42540ms step_avg:57.33ms
step:743/2330 train_time:42596ms step_avg:57.33ms
step:744/2330 train_time:42655ms step_avg:57.33ms
step:745/2330 train_time:42712ms step_avg:57.33ms
step:746/2330 train_time:42772ms step_avg:57.33ms
step:747/2330 train_time:42828ms step_avg:57.33ms
step:748/2330 train_time:42888ms step_avg:57.34ms
step:749/2330 train_time:42944ms step_avg:57.34ms
step:750/2330 train_time:43003ms step_avg:57.34ms
step:750/2330 val_loss:5.9598 train_time:43082ms step_avg:57.44ms
step:751/2330 train_time:43102ms step_avg:57.39ms
step:752/2330 train_time:43122ms step_avg:57.34ms
step:753/2330 train_time:43177ms step_avg:57.34ms
step:754/2330 train_time:43241ms step_avg:57.35ms
step:755/2330 train_time:43297ms step_avg:57.35ms
step:756/2330 train_time:43362ms step_avg:57.36ms
step:757/2330 train_time:43417ms step_avg:57.35ms
step:758/2330 train_time:43478ms step_avg:57.36ms
step:759/2330 train_time:43533ms step_avg:57.36ms
step:760/2330 train_time:43594ms step_avg:57.36ms
step:761/2330 train_time:43649ms step_avg:57.36ms
step:762/2330 train_time:43708ms step_avg:57.36ms
step:763/2330 train_time:43764ms step_avg:57.36ms
step:764/2330 train_time:43822ms step_avg:57.36ms
step:765/2330 train_time:43879ms step_avg:57.36ms
step:766/2330 train_time:43937ms step_avg:57.36ms
step:767/2330 train_time:43994ms step_avg:57.36ms
step:768/2330 train_time:44055ms step_avg:57.36ms
step:769/2330 train_time:44112ms step_avg:57.36ms
step:770/2330 train_time:44174ms step_avg:57.37ms
step:771/2330 train_time:44231ms step_avg:57.37ms
step:772/2330 train_time:44293ms step_avg:57.37ms
step:773/2330 train_time:44350ms step_avg:57.37ms
step:774/2330 train_time:44411ms step_avg:57.38ms
step:775/2330 train_time:44468ms step_avg:57.38ms
step:776/2330 train_time:44528ms step_avg:57.38ms
step:777/2330 train_time:44584ms step_avg:57.38ms
step:778/2330 train_time:44644ms step_avg:57.38ms
step:779/2330 train_time:44701ms step_avg:57.38ms
step:780/2330 train_time:44760ms step_avg:57.38ms
step:781/2330 train_time:44817ms step_avg:57.38ms
step:782/2330 train_time:44877ms step_avg:57.39ms
step:783/2330 train_time:44932ms step_avg:57.38ms
step:784/2330 train_time:44993ms step_avg:57.39ms
step:785/2330 train_time:45050ms step_avg:57.39ms
step:786/2330 train_time:45111ms step_avg:57.39ms
step:787/2330 train_time:45168ms step_avg:57.39ms
step:788/2330 train_time:45228ms step_avg:57.40ms
step:789/2330 train_time:45285ms step_avg:57.40ms
step:790/2330 train_time:45346ms step_avg:57.40ms
step:791/2330 train_time:45403ms step_avg:57.40ms
step:792/2330 train_time:45464ms step_avg:57.40ms
step:793/2330 train_time:45521ms step_avg:57.40ms
step:794/2330 train_time:45581ms step_avg:57.41ms
step:795/2330 train_time:45638ms step_avg:57.41ms
step:796/2330 train_time:45699ms step_avg:57.41ms
step:797/2330 train_time:45755ms step_avg:57.41ms
step:798/2330 train_time:45816ms step_avg:57.41ms
step:799/2330 train_time:45872ms step_avg:57.41ms
step:800/2330 train_time:45933ms step_avg:57.42ms
step:801/2330 train_time:45989ms step_avg:57.41ms
step:802/2330 train_time:46050ms step_avg:57.42ms
step:803/2330 train_time:46106ms step_avg:57.42ms
step:804/2330 train_time:46167ms step_avg:57.42ms
step:805/2330 train_time:46225ms step_avg:57.42ms
step:806/2330 train_time:46285ms step_avg:57.43ms
step:807/2330 train_time:46342ms step_avg:57.43ms
step:808/2330 train_time:46404ms step_avg:57.43ms
step:809/2330 train_time:46461ms step_avg:57.43ms
step:810/2330 train_time:46521ms step_avg:57.43ms
step:811/2330 train_time:46579ms step_avg:57.43ms
step:812/2330 train_time:46639ms step_avg:57.44ms
step:813/2330 train_time:46696ms step_avg:57.44ms
step:814/2330 train_time:46756ms step_avg:57.44ms
step:815/2330 train_time:46812ms step_avg:57.44ms
step:816/2330 train_time:46872ms step_avg:57.44ms
step:817/2330 train_time:46929ms step_avg:57.44ms
step:818/2330 train_time:46989ms step_avg:57.44ms
step:819/2330 train_time:47045ms step_avg:57.44ms
step:820/2330 train_time:47106ms step_avg:57.45ms
step:821/2330 train_time:47163ms step_avg:57.45ms
step:822/2330 train_time:47224ms step_avg:57.45ms
step:823/2330 train_time:47281ms step_avg:57.45ms
step:824/2330 train_time:47342ms step_avg:57.45ms
step:825/2330 train_time:47399ms step_avg:57.45ms
step:826/2330 train_time:47461ms step_avg:57.46ms
step:827/2330 train_time:47518ms step_avg:57.46ms
step:828/2330 train_time:47579ms step_avg:57.46ms
step:829/2330 train_time:47635ms step_avg:57.46ms
step:830/2330 train_time:47696ms step_avg:57.46ms
step:831/2330 train_time:47752ms step_avg:57.46ms
step:832/2330 train_time:47813ms step_avg:57.47ms
step:833/2330 train_time:47869ms step_avg:57.47ms
step:834/2330 train_time:47930ms step_avg:57.47ms
step:835/2330 train_time:47986ms step_avg:57.47ms
step:836/2330 train_time:48046ms step_avg:57.47ms
step:837/2330 train_time:48103ms step_avg:57.47ms
step:838/2330 train_time:48163ms step_avg:57.47ms
step:839/2330 train_time:48221ms step_avg:57.47ms
step:840/2330 train_time:48281ms step_avg:57.48ms
step:841/2330 train_time:48337ms step_avg:57.48ms
step:842/2330 train_time:48398ms step_avg:57.48ms
step:843/2330 train_time:48455ms step_avg:57.48ms
step:844/2330 train_time:48517ms step_avg:57.48ms
step:845/2330 train_time:48573ms step_avg:57.48ms
step:846/2330 train_time:48634ms step_avg:57.49ms
step:847/2330 train_time:48690ms step_avg:57.49ms
step:848/2330 train_time:48752ms step_avg:57.49ms
step:849/2330 train_time:48809ms step_avg:57.49ms
step:850/2330 train_time:48869ms step_avg:57.49ms
step:851/2330 train_time:48925ms step_avg:57.49ms
step:852/2330 train_time:48985ms step_avg:57.49ms
step:853/2330 train_time:49041ms step_avg:57.49ms
step:854/2330 train_time:49103ms step_avg:57.50ms
step:855/2330 train_time:49159ms step_avg:57.50ms
step:856/2330 train_time:49220ms step_avg:57.50ms
step:857/2330 train_time:49277ms step_avg:57.50ms
step:858/2330 train_time:49338ms step_avg:57.50ms
step:859/2330 train_time:49394ms step_avg:57.50ms
step:860/2330 train_time:49456ms step_avg:57.51ms
step:861/2330 train_time:49513ms step_avg:57.51ms
step:862/2330 train_time:49573ms step_avg:57.51ms
step:863/2330 train_time:49630ms step_avg:57.51ms
step:864/2330 train_time:49691ms step_avg:57.51ms
step:865/2330 train_time:49747ms step_avg:57.51ms
step:866/2330 train_time:49807ms step_avg:57.51ms
step:867/2330 train_time:49864ms step_avg:57.51ms
step:868/2330 train_time:49924ms step_avg:57.52ms
step:869/2330 train_time:49981ms step_avg:57.52ms
step:870/2330 train_time:50041ms step_avg:57.52ms
step:871/2330 train_time:50098ms step_avg:57.52ms
step:872/2330 train_time:50159ms step_avg:57.52ms
step:873/2330 train_time:50216ms step_avg:57.52ms
step:874/2330 train_time:50276ms step_avg:57.52ms
step:875/2330 train_time:50333ms step_avg:57.52ms
step:876/2330 train_time:50394ms step_avg:57.53ms
step:877/2330 train_time:50451ms step_avg:57.53ms
step:878/2330 train_time:50512ms step_avg:57.53ms
step:879/2330 train_time:50568ms step_avg:57.53ms
step:880/2330 train_time:50630ms step_avg:57.53ms
step:881/2330 train_time:50686ms step_avg:57.53ms
step:882/2330 train_time:50746ms step_avg:57.54ms
step:883/2330 train_time:50803ms step_avg:57.53ms
step:884/2330 train_time:50863ms step_avg:57.54ms
step:885/2330 train_time:50920ms step_avg:57.54ms
step:886/2330 train_time:50981ms step_avg:57.54ms
step:887/2330 train_time:51038ms step_avg:57.54ms
step:888/2330 train_time:51098ms step_avg:57.54ms
step:889/2330 train_time:51155ms step_avg:57.54ms
step:890/2330 train_time:51214ms step_avg:57.54ms
step:891/2330 train_time:51272ms step_avg:57.54ms
step:892/2330 train_time:51331ms step_avg:57.55ms
step:893/2330 train_time:51387ms step_avg:57.54ms
step:894/2330 train_time:51448ms step_avg:57.55ms
step:895/2330 train_time:51505ms step_avg:57.55ms
step:896/2330 train_time:51566ms step_avg:57.55ms
step:897/2330 train_time:51623ms step_avg:57.55ms
step:898/2330 train_time:51683ms step_avg:57.55ms
step:899/2330 train_time:51740ms step_avg:57.55ms
step:900/2330 train_time:51801ms step_avg:57.56ms
step:901/2330 train_time:51857ms step_avg:57.56ms
step:902/2330 train_time:51919ms step_avg:57.56ms
step:903/2330 train_time:51975ms step_avg:57.56ms
step:904/2330 train_time:52036ms step_avg:57.56ms
step:905/2330 train_time:52092ms step_avg:57.56ms
step:906/2330 train_time:52154ms step_avg:57.57ms
step:907/2330 train_time:52210ms step_avg:57.56ms
step:908/2330 train_time:52270ms step_avg:57.57ms
step:909/2330 train_time:52327ms step_avg:57.57ms
step:910/2330 train_time:52387ms step_avg:57.57ms
step:911/2330 train_time:52444ms step_avg:57.57ms
step:912/2330 train_time:52505ms step_avg:57.57ms
step:913/2330 train_time:52562ms step_avg:57.57ms
step:914/2330 train_time:52623ms step_avg:57.57ms
step:915/2330 train_time:52680ms step_avg:57.57ms
step:916/2330 train_time:52740ms step_avg:57.58ms
step:917/2330 train_time:52796ms step_avg:57.57ms
step:918/2330 train_time:52858ms step_avg:57.58ms
step:919/2330 train_time:52914ms step_avg:57.58ms
step:920/2330 train_time:52975ms step_avg:57.58ms
step:921/2330 train_time:53031ms step_avg:57.58ms
step:922/2330 train_time:53092ms step_avg:57.58ms
step:923/2330 train_time:53149ms step_avg:57.58ms
step:924/2330 train_time:53209ms step_avg:57.59ms
step:925/2330 train_time:53266ms step_avg:57.59ms
step:926/2330 train_time:53326ms step_avg:57.59ms
step:927/2330 train_time:53383ms step_avg:57.59ms
step:928/2330 train_time:53443ms step_avg:57.59ms
step:929/2330 train_time:53500ms step_avg:57.59ms
step:930/2330 train_time:53561ms step_avg:57.59ms
step:931/2330 train_time:53618ms step_avg:57.59ms
step:932/2330 train_time:53679ms step_avg:57.59ms
step:933/2330 train_time:53735ms step_avg:57.59ms
step:934/2330 train_time:53796ms step_avg:57.60ms
step:935/2330 train_time:53853ms step_avg:57.60ms
step:936/2330 train_time:53913ms step_avg:57.60ms
step:937/2330 train_time:53969ms step_avg:57.60ms
step:938/2330 train_time:54029ms step_avg:57.60ms
step:939/2330 train_time:54086ms step_avg:57.60ms
step:940/2330 train_time:54146ms step_avg:57.60ms
step:941/2330 train_time:54203ms step_avg:57.60ms
step:942/2330 train_time:54264ms step_avg:57.60ms
step:943/2330 train_time:54320ms step_avg:57.60ms
step:944/2330 train_time:54381ms step_avg:57.61ms
step:945/2330 train_time:54438ms step_avg:57.61ms
step:946/2330 train_time:54499ms step_avg:57.61ms
step:947/2330 train_time:54556ms step_avg:57.61ms
step:948/2330 train_time:54616ms step_avg:57.61ms
step:949/2330 train_time:54672ms step_avg:57.61ms
step:950/2330 train_time:54733ms step_avg:57.61ms
step:951/2330 train_time:54789ms step_avg:57.61ms
step:952/2330 train_time:54849ms step_avg:57.61ms
step:953/2330 train_time:54906ms step_avg:57.61ms
step:954/2330 train_time:54967ms step_avg:57.62ms
step:955/2330 train_time:55024ms step_avg:57.62ms
step:956/2330 train_time:55083ms step_avg:57.62ms
step:957/2330 train_time:55140ms step_avg:57.62ms
step:958/2330 train_time:55201ms step_avg:57.62ms
step:959/2330 train_time:55258ms step_avg:57.62ms
step:960/2330 train_time:55319ms step_avg:57.62ms
step:961/2330 train_time:55375ms step_avg:57.62ms
step:962/2330 train_time:55437ms step_avg:57.63ms
step:963/2330 train_time:55493ms step_avg:57.63ms
step:964/2330 train_time:55554ms step_avg:57.63ms
step:965/2330 train_time:55611ms step_avg:57.63ms
step:966/2330 train_time:55671ms step_avg:57.63ms
step:967/2330 train_time:55727ms step_avg:57.63ms
step:968/2330 train_time:55788ms step_avg:57.63ms
step:969/2330 train_time:55844ms step_avg:57.63ms
step:970/2330 train_time:55905ms step_avg:57.63ms
step:971/2330 train_time:55961ms step_avg:57.63ms
step:972/2330 train_time:56022ms step_avg:57.64ms
step:973/2330 train_time:56079ms step_avg:57.64ms
step:974/2330 train_time:56140ms step_avg:57.64ms
step:975/2330 train_time:56196ms step_avg:57.64ms
step:976/2330 train_time:56258ms step_avg:57.64ms
step:977/2330 train_time:56315ms step_avg:57.64ms
step:978/2330 train_time:56375ms step_avg:57.64ms
step:979/2330 train_time:56431ms step_avg:57.64ms
step:980/2330 train_time:56492ms step_avg:57.64ms
step:981/2330 train_time:56548ms step_avg:57.64ms
step:982/2330 train_time:56609ms step_avg:57.65ms
step:983/2330 train_time:56666ms step_avg:57.65ms
step:984/2330 train_time:56726ms step_avg:57.65ms
step:985/2330 train_time:56783ms step_avg:57.65ms
step:986/2330 train_time:56843ms step_avg:57.65ms
step:987/2330 train_time:56899ms step_avg:57.65ms
step:988/2330 train_time:56961ms step_avg:57.65ms
step:989/2330 train_time:57018ms step_avg:57.65ms
step:990/2330 train_time:57079ms step_avg:57.66ms
step:991/2330 train_time:57136ms step_avg:57.65ms
step:992/2330 train_time:57196ms step_avg:57.66ms
step:993/2330 train_time:57252ms step_avg:57.66ms
step:994/2330 train_time:57312ms step_avg:57.66ms
step:995/2330 train_time:57370ms step_avg:57.66ms
step:996/2330 train_time:57430ms step_avg:57.66ms
step:997/2330 train_time:57487ms step_avg:57.66ms
step:998/2330 train_time:57547ms step_avg:57.66ms
step:999/2330 train_time:57603ms step_avg:57.66ms
step:1000/2330 train_time:57665ms step_avg:57.67ms
step:1000/2330 val_loss:6.4517 train_time:57746ms step_avg:57.75ms
step:1001/2330 train_time:57767ms step_avg:57.71ms
step:1002/2330 train_time:57787ms step_avg:57.67ms
step:1003/2330 train_time:57839ms step_avg:57.67ms
step:1004/2330 train_time:57908ms step_avg:57.68ms
step:1005/2330 train_time:57964ms step_avg:57.68ms
step:1006/2330 train_time:58028ms step_avg:57.68ms
step:1007/2330 train_time:58084ms step_avg:57.68ms
step:1008/2330 train_time:58145ms step_avg:57.68ms
step:1009/2330 train_time:58201ms step_avg:57.68ms
step:1010/2330 train_time:58261ms step_avg:57.68ms
step:1011/2330 train_time:58318ms step_avg:57.68ms
step:1012/2330 train_time:58377ms step_avg:57.68ms
step:1013/2330 train_time:58432ms step_avg:57.68ms
step:1014/2330 train_time:58492ms step_avg:57.68ms
step:1015/2330 train_time:58549ms step_avg:57.68ms
step:1016/2330 train_time:58608ms step_avg:57.69ms
step:1017/2330 train_time:58666ms step_avg:57.69ms
step:1018/2330 train_time:58729ms step_avg:57.69ms
step:1019/2330 train_time:58788ms step_avg:57.69ms
step:1020/2330 train_time:58850ms step_avg:57.70ms
step:1021/2330 train_time:58908ms step_avg:57.70ms
step:1022/2330 train_time:58968ms step_avg:57.70ms
step:1023/2330 train_time:59025ms step_avg:57.70ms
step:1024/2330 train_time:59086ms step_avg:57.70ms
step:1025/2330 train_time:59142ms step_avg:57.70ms
step:1026/2330 train_time:59202ms step_avg:57.70ms
step:1027/2330 train_time:59259ms step_avg:57.70ms
step:1028/2330 train_time:59318ms step_avg:57.70ms
step:1029/2330 train_time:59375ms step_avg:57.70ms
step:1030/2330 train_time:59435ms step_avg:57.70ms
step:1031/2330 train_time:59491ms step_avg:57.70ms
step:1032/2330 train_time:59550ms step_avg:57.70ms
step:1033/2330 train_time:59606ms step_avg:57.70ms
step:1034/2330 train_time:59667ms step_avg:57.70ms
step:1035/2330 train_time:59725ms step_avg:57.70ms
step:1036/2330 train_time:59786ms step_avg:57.71ms
step:1037/2330 train_time:59845ms step_avg:57.71ms
step:1038/2330 train_time:59905ms step_avg:57.71ms
step:1039/2330 train_time:59962ms step_avg:57.71ms
step:1040/2330 train_time:60023ms step_avg:57.71ms
step:1041/2330 train_time:60081ms step_avg:57.71ms
step:1042/2330 train_time:60141ms step_avg:57.72ms
step:1043/2330 train_time:60198ms step_avg:57.72ms
step:1044/2330 train_time:60257ms step_avg:57.72ms
step:1045/2330 train_time:60314ms step_avg:57.72ms
step:1046/2330 train_time:60374ms step_avg:57.72ms
step:1047/2330 train_time:60431ms step_avg:57.72ms
step:1048/2330 train_time:60490ms step_avg:57.72ms
step:1049/2330 train_time:60547ms step_avg:57.72ms
step:1050/2330 train_time:60607ms step_avg:57.72ms
step:1051/2330 train_time:60663ms step_avg:57.72ms
step:1052/2330 train_time:60725ms step_avg:57.72ms
step:1053/2330 train_time:60782ms step_avg:57.72ms
step:1054/2330 train_time:60843ms step_avg:57.73ms
step:1055/2330 train_time:60900ms step_avg:57.73ms
step:1056/2330 train_time:60961ms step_avg:57.73ms
step:1057/2330 train_time:61017ms step_avg:57.73ms
step:1058/2330 train_time:61079ms step_avg:57.73ms
step:1059/2330 train_time:61135ms step_avg:57.73ms
step:1060/2330 train_time:61196ms step_avg:57.73ms
step:1061/2330 train_time:61252ms step_avg:57.73ms
step:1062/2330 train_time:61312ms step_avg:57.73ms
step:1063/2330 train_time:61369ms step_avg:57.73ms
step:1064/2330 train_time:61429ms step_avg:57.73ms
step:1065/2330 train_time:61486ms step_avg:57.73ms
step:1066/2330 train_time:61545ms step_avg:57.73ms
step:1067/2330 train_time:61602ms step_avg:57.73ms
step:1068/2330 train_time:61662ms step_avg:57.74ms
step:1069/2330 train_time:61718ms step_avg:57.73ms
step:1070/2330 train_time:61780ms step_avg:57.74ms
step:1071/2330 train_time:61837ms step_avg:57.74ms
step:1072/2330 train_time:61897ms step_avg:57.74ms
step:1073/2330 train_time:61953ms step_avg:57.74ms
step:1074/2330 train_time:62014ms step_avg:57.74ms
step:1075/2330 train_time:62072ms step_avg:57.74ms
step:1076/2330 train_time:62131ms step_avg:57.74ms
step:1077/2330 train_time:62188ms step_avg:57.74ms
step:1078/2330 train_time:62249ms step_avg:57.74ms
step:1079/2330 train_time:62306ms step_avg:57.74ms
step:1080/2330 train_time:62365ms step_avg:57.75ms
step:1081/2330 train_time:62423ms step_avg:57.75ms
step:1082/2330 train_time:62482ms step_avg:57.75ms
step:1083/2330 train_time:62539ms step_avg:57.75ms
step:1084/2330 train_time:62599ms step_avg:57.75ms
step:1085/2330 train_time:62655ms step_avg:57.75ms
step:1086/2330 train_time:62716ms step_avg:57.75ms
step:1087/2330 train_time:62773ms step_avg:57.75ms
step:1088/2330 train_time:62833ms step_avg:57.75ms
step:1089/2330 train_time:62890ms step_avg:57.75ms
step:1090/2330 train_time:62950ms step_avg:57.75ms
step:1091/2330 train_time:63008ms step_avg:57.75ms
step:1092/2330 train_time:63068ms step_avg:57.75ms
step:1093/2330 train_time:63125ms step_avg:57.75ms
step:1094/2330 train_time:63185ms step_avg:57.76ms
step:1095/2330 train_time:63242ms step_avg:57.75ms
step:1096/2330 train_time:63303ms step_avg:57.76ms
step:1097/2330 train_time:63360ms step_avg:57.76ms
step:1098/2330 train_time:63420ms step_avg:57.76ms
step:1099/2330 train_time:63477ms step_avg:57.76ms
step:1100/2330 train_time:63538ms step_avg:57.76ms
step:1101/2330 train_time:63594ms step_avg:57.76ms
step:1102/2330 train_time:63655ms step_avg:57.76ms
step:1103/2330 train_time:63711ms step_avg:57.76ms
step:1104/2330 train_time:63771ms step_avg:57.76ms
step:1105/2330 train_time:63828ms step_avg:57.76ms
step:1106/2330 train_time:63888ms step_avg:57.77ms
step:1107/2330 train_time:63946ms step_avg:57.76ms
step:1108/2330 train_time:64006ms step_avg:57.77ms
step:1109/2330 train_time:64063ms step_avg:57.77ms
step:1110/2330 train_time:64123ms step_avg:57.77ms
step:1111/2330 train_time:64180ms step_avg:57.77ms
step:1112/2330 train_time:64240ms step_avg:57.77ms
step:1113/2330 train_time:64297ms step_avg:57.77ms
step:1114/2330 train_time:64357ms step_avg:57.77ms
step:1115/2330 train_time:64414ms step_avg:57.77ms
step:1116/2330 train_time:64474ms step_avg:57.77ms
step:1117/2330 train_time:64531ms step_avg:57.77ms
step:1118/2330 train_time:64590ms step_avg:57.77ms
step:1119/2330 train_time:64647ms step_avg:57.77ms
step:1120/2330 train_time:64707ms step_avg:57.77ms
step:1121/2330 train_time:64764ms step_avg:57.77ms
step:1122/2330 train_time:64825ms step_avg:57.78ms
step:1123/2330 train_time:64882ms step_avg:57.78ms
step:1124/2330 train_time:64943ms step_avg:57.78ms
step:1125/2330 train_time:64999ms step_avg:57.78ms
step:1126/2330 train_time:65060ms step_avg:57.78ms
step:1127/2330 train_time:65117ms step_avg:57.78ms
step:1128/2330 train_time:65177ms step_avg:57.78ms
step:1129/2330 train_time:65234ms step_avg:57.78ms
step:1130/2330 train_time:65294ms step_avg:57.78ms
step:1131/2330 train_time:65351ms step_avg:57.78ms
step:1132/2330 train_time:65411ms step_avg:57.78ms
step:1133/2330 train_time:65468ms step_avg:57.78ms
step:1134/2330 train_time:65528ms step_avg:57.78ms
step:1135/2330 train_time:65585ms step_avg:57.78ms
step:1136/2330 train_time:65645ms step_avg:57.79ms
step:1137/2330 train_time:65702ms step_avg:57.79ms
step:1138/2330 train_time:65762ms step_avg:57.79ms
step:1139/2330 train_time:65819ms step_avg:57.79ms
step:1140/2330 train_time:65880ms step_avg:57.79ms
step:1141/2330 train_time:65937ms step_avg:57.79ms
step:1142/2330 train_time:65997ms step_avg:57.79ms
step:1143/2330 train_time:66054ms step_avg:57.79ms
step:1144/2330 train_time:66114ms step_avg:57.79ms
step:1145/2330 train_time:66172ms step_avg:57.79ms
step:1146/2330 train_time:66231ms step_avg:57.79ms
step:1147/2330 train_time:66288ms step_avg:57.79ms
step:1148/2330 train_time:66348ms step_avg:57.79ms
step:1149/2330 train_time:66406ms step_avg:57.79ms
step:1150/2330 train_time:66465ms step_avg:57.80ms
step:1151/2330 train_time:66522ms step_avg:57.80ms
step:1152/2330 train_time:66582ms step_avg:57.80ms
step:1153/2330 train_time:66639ms step_avg:57.80ms
step:1154/2330 train_time:66699ms step_avg:57.80ms
step:1155/2330 train_time:66755ms step_avg:57.80ms
step:1156/2330 train_time:66816ms step_avg:57.80ms
step:1157/2330 train_time:66873ms step_avg:57.80ms
step:1158/2330 train_time:66934ms step_avg:57.80ms
step:1159/2330 train_time:66991ms step_avg:57.80ms
step:1160/2330 train_time:67050ms step_avg:57.80ms
step:1161/2330 train_time:67107ms step_avg:57.80ms
step:1162/2330 train_time:67167ms step_avg:57.80ms
step:1163/2330 train_time:67224ms step_avg:57.80ms
step:1164/2330 train_time:67284ms step_avg:57.80ms
step:1165/2330 train_time:67341ms step_avg:57.80ms
step:1166/2330 train_time:67402ms step_avg:57.81ms
step:1167/2330 train_time:67458ms step_avg:57.80ms
step:1168/2330 train_time:67519ms step_avg:57.81ms
step:1169/2330 train_time:67576ms step_avg:57.81ms
step:1170/2330 train_time:67637ms step_avg:57.81ms
step:1171/2330 train_time:67694ms step_avg:57.81ms
step:1172/2330 train_time:67753ms step_avg:57.81ms
step:1173/2330 train_time:67810ms step_avg:57.81ms
step:1174/2330 train_time:67870ms step_avg:57.81ms
step:1175/2330 train_time:67928ms step_avg:57.81ms
step:1176/2330 train_time:67987ms step_avg:57.81ms
step:1177/2330 train_time:68044ms step_avg:57.81ms
step:1178/2330 train_time:68105ms step_avg:57.81ms
step:1179/2330 train_time:68161ms step_avg:57.81ms
step:1180/2330 train_time:68223ms step_avg:57.82ms
step:1181/2330 train_time:68279ms step_avg:57.81ms
step:1182/2330 train_time:68339ms step_avg:57.82ms
step:1183/2330 train_time:68396ms step_avg:57.82ms
step:1184/2330 train_time:68456ms step_avg:57.82ms
step:1185/2330 train_time:68513ms step_avg:57.82ms
step:1186/2330 train_time:68573ms step_avg:57.82ms
step:1187/2330 train_time:68630ms step_avg:57.82ms
step:1188/2330 train_time:68691ms step_avg:57.82ms
step:1189/2330 train_time:68748ms step_avg:57.82ms
step:1190/2330 train_time:68808ms step_avg:57.82ms
step:1191/2330 train_time:68865ms step_avg:57.82ms
step:1192/2330 train_time:68925ms step_avg:57.82ms
step:1193/2330 train_time:68982ms step_avg:57.82ms
step:1194/2330 train_time:69042ms step_avg:57.82ms
step:1195/2330 train_time:69099ms step_avg:57.82ms
step:1196/2330 train_time:69159ms step_avg:57.83ms
step:1197/2330 train_time:69216ms step_avg:57.82ms
step:1198/2330 train_time:69276ms step_avg:57.83ms
step:1199/2330 train_time:69333ms step_avg:57.83ms
step:1200/2330 train_time:69393ms step_avg:57.83ms
step:1201/2330 train_time:69450ms step_avg:57.83ms
step:1202/2330 train_time:69510ms step_avg:57.83ms
step:1203/2330 train_time:69567ms step_avg:57.83ms
step:1204/2330 train_time:69627ms step_avg:57.83ms
step:1205/2330 train_time:69685ms step_avg:57.83ms
step:1206/2330 train_time:69745ms step_avg:57.83ms
step:1207/2330 train_time:69802ms step_avg:57.83ms
step:1208/2330 train_time:69862ms step_avg:57.83ms
step:1209/2330 train_time:69920ms step_avg:57.83ms
step:1210/2330 train_time:69981ms step_avg:57.84ms
step:1211/2330 train_time:70037ms step_avg:57.83ms
step:1212/2330 train_time:70098ms step_avg:57.84ms
step:1213/2330 train_time:70154ms step_avg:57.84ms
step:1214/2330 train_time:70214ms step_avg:57.84ms
step:1215/2330 train_time:70271ms step_avg:57.84ms
step:1216/2330 train_time:70331ms step_avg:57.84ms
step:1217/2330 train_time:70388ms step_avg:57.84ms
step:1218/2330 train_time:70448ms step_avg:57.84ms
step:1219/2330 train_time:70505ms step_avg:57.84ms
step:1220/2330 train_time:70565ms step_avg:57.84ms
step:1221/2330 train_time:70622ms step_avg:57.84ms
step:1222/2330 train_time:70682ms step_avg:57.84ms
step:1223/2330 train_time:70740ms step_avg:57.84ms
step:1224/2330 train_time:70799ms step_avg:57.84ms
step:1225/2330 train_time:70855ms step_avg:57.84ms
step:1226/2330 train_time:70916ms step_avg:57.84ms
step:1227/2330 train_time:70973ms step_avg:57.84ms
step:1228/2330 train_time:71033ms step_avg:57.84ms
step:1229/2330 train_time:71090ms step_avg:57.84ms
step:1230/2330 train_time:71150ms step_avg:57.85ms
step:1231/2330 train_time:71207ms step_avg:57.84ms
step:1232/2330 train_time:71267ms step_avg:57.85ms
step:1233/2330 train_time:71324ms step_avg:57.85ms
step:1234/2330 train_time:71384ms step_avg:57.85ms
step:1235/2330 train_time:71441ms step_avg:57.85ms
step:1236/2330 train_time:71502ms step_avg:57.85ms
step:1237/2330 train_time:71559ms step_avg:57.85ms
step:1238/2330 train_time:71620ms step_avg:57.85ms
step:1239/2330 train_time:71676ms step_avg:57.85ms
step:1240/2330 train_time:71737ms step_avg:57.85ms
step:1241/2330 train_time:71794ms step_avg:57.85ms
step:1242/2330 train_time:71853ms step_avg:57.85ms
step:1243/2330 train_time:71910ms step_avg:57.85ms
step:1244/2330 train_time:71970ms step_avg:57.85ms
step:1245/2330 train_time:72028ms step_avg:57.85ms
step:1246/2330 train_time:72088ms step_avg:57.86ms
step:1247/2330 train_time:72145ms step_avg:57.85ms
step:1248/2330 train_time:72205ms step_avg:57.86ms
step:1249/2330 train_time:72262ms step_avg:57.86ms
step:1250/2330 train_time:72321ms step_avg:57.86ms
step:1250/2330 val_loss:5.4163 train_time:72403ms step_avg:57.92ms
step:1251/2330 train_time:72423ms step_avg:57.89ms
step:1252/2330 train_time:72444ms step_avg:57.86ms
step:1253/2330 train_time:72503ms step_avg:57.86ms
step:1254/2330 train_time:72567ms step_avg:57.87ms
step:1255/2330 train_time:72624ms step_avg:57.87ms
step:1256/2330 train_time:72685ms step_avg:57.87ms
step:1257/2330 train_time:72742ms step_avg:57.87ms
step:1258/2330 train_time:72802ms step_avg:57.87ms
step:1259/2330 train_time:72858ms step_avg:57.87ms
step:1260/2330 train_time:72917ms step_avg:57.87ms
step:1261/2330 train_time:72974ms step_avg:57.87ms
step:1262/2330 train_time:73033ms step_avg:57.87ms
step:1263/2330 train_time:73089ms step_avg:57.87ms
step:1264/2330 train_time:73149ms step_avg:57.87ms
step:1265/2330 train_time:73206ms step_avg:57.87ms
step:1266/2330 train_time:73264ms step_avg:57.87ms
step:1267/2330 train_time:73321ms step_avg:57.87ms
step:1268/2330 train_time:73381ms step_avg:57.87ms
step:1269/2330 train_time:73440ms step_avg:57.87ms
step:1270/2330 train_time:73500ms step_avg:57.87ms
step:1271/2330 train_time:73559ms step_avg:57.87ms
step:1272/2330 train_time:73620ms step_avg:57.88ms
step:1273/2330 train_time:73678ms step_avg:57.88ms
step:1274/2330 train_time:73738ms step_avg:57.88ms
step:1275/2330 train_time:73795ms step_avg:57.88ms
step:1276/2330 train_time:73855ms step_avg:57.88ms
step:1277/2330 train_time:73912ms step_avg:57.88ms
step:1278/2330 train_time:73972ms step_avg:57.88ms
step:1279/2330 train_time:74028ms step_avg:57.88ms
step:1280/2330 train_time:74088ms step_avg:57.88ms
step:1281/2330 train_time:74144ms step_avg:57.88ms
step:1282/2330 train_time:74204ms step_avg:57.88ms
step:1283/2330 train_time:74261ms step_avg:57.88ms
step:1284/2330 train_time:74320ms step_avg:57.88ms
step:1285/2330 train_time:74377ms step_avg:57.88ms
step:1286/2330 train_time:74438ms step_avg:57.88ms
step:1287/2330 train_time:74494ms step_avg:57.88ms
step:1288/2330 train_time:74556ms step_avg:57.88ms
step:1289/2330 train_time:74613ms step_avg:57.88ms
step:1290/2330 train_time:74675ms step_avg:57.89ms
step:1291/2330 train_time:74732ms step_avg:57.89ms
step:1292/2330 train_time:74794ms step_avg:57.89ms
step:1293/2330 train_time:74851ms step_avg:57.89ms
step:1294/2330 train_time:74911ms step_avg:57.89ms
step:1295/2330 train_time:74968ms step_avg:57.89ms
step:1296/2330 train_time:75028ms step_avg:57.89ms
step:1297/2330 train_time:75084ms step_avg:57.89ms
step:1298/2330 train_time:75144ms step_avg:57.89ms
step:1299/2330 train_time:75200ms step_avg:57.89ms
step:1300/2330 train_time:75260ms step_avg:57.89ms
step:1301/2330 train_time:75317ms step_avg:57.89ms
step:1302/2330 train_time:75377ms step_avg:57.89ms
step:1303/2330 train_time:75434ms step_avg:57.89ms
step:1304/2330 train_time:75494ms step_avg:57.89ms
step:1305/2330 train_time:75552ms step_avg:57.89ms
step:1306/2330 train_time:75613ms step_avg:57.90ms
step:1307/2330 train_time:75670ms step_avg:57.90ms
step:1308/2330 train_time:75730ms step_avg:57.90ms
step:1309/2330 train_time:75787ms step_avg:57.90ms
step:1310/2330 train_time:75849ms step_avg:57.90ms
step:1311/2330 train_time:75907ms step_avg:57.90ms
step:1312/2330 train_time:75967ms step_avg:57.90ms
step:1313/2330 train_time:76023ms step_avg:57.90ms
step:1314/2330 train_time:76083ms step_avg:57.90ms
step:1315/2330 train_time:76139ms step_avg:57.90ms
step:1316/2330 train_time:76200ms step_avg:57.90ms
step:1317/2330 train_time:76257ms step_avg:57.90ms
step:1318/2330 train_time:76316ms step_avg:57.90ms
step:1319/2330 train_time:76373ms step_avg:57.90ms
step:1320/2330 train_time:76434ms step_avg:57.90ms
step:1321/2330 train_time:76491ms step_avg:57.90ms
step:1322/2330 train_time:76552ms step_avg:57.91ms
step:1323/2330 train_time:76609ms step_avg:57.91ms
step:1324/2330 train_time:76669ms step_avg:57.91ms
step:1325/2330 train_time:76727ms step_avg:57.91ms
step:1326/2330 train_time:76787ms step_avg:57.91ms
step:1327/2330 train_time:76844ms step_avg:57.91ms
step:1328/2330 train_time:76905ms step_avg:57.91ms
step:1329/2330 train_time:76962ms step_avg:57.91ms
step:1330/2330 train_time:77022ms step_avg:57.91ms
step:1331/2330 train_time:77078ms step_avg:57.91ms
step:1332/2330 train_time:77138ms step_avg:57.91ms
step:1333/2330 train_time:77195ms step_avg:57.91ms
step:1334/2330 train_time:77255ms step_avg:57.91ms
step:1335/2330 train_time:77312ms step_avg:57.91ms
step:1336/2330 train_time:77371ms step_avg:57.91ms
step:1337/2330 train_time:77429ms step_avg:57.91ms
step:1338/2330 train_time:77489ms step_avg:57.91ms
step:1339/2330 train_time:77546ms step_avg:57.91ms
step:1340/2330 train_time:77607ms step_avg:57.92ms
step:1341/2330 train_time:77664ms step_avg:57.91ms
step:1342/2330 train_time:77724ms step_avg:57.92ms
step:1343/2330 train_time:77781ms step_avg:57.92ms
step:1344/2330 train_time:77841ms step_avg:57.92ms
step:1345/2330 train_time:77898ms step_avg:57.92ms
step:1346/2330 train_time:77958ms step_avg:57.92ms
step:1347/2330 train_time:78015ms step_avg:57.92ms
step:1348/2330 train_time:78074ms step_avg:57.92ms
step:1349/2330 train_time:78131ms step_avg:57.92ms
step:1350/2330 train_time:78191ms step_avg:57.92ms
step:1351/2330 train_time:78248ms step_avg:57.92ms
step:1352/2330 train_time:78309ms step_avg:57.92ms
step:1353/2330 train_time:78366ms step_avg:57.92ms
step:1354/2330 train_time:78426ms step_avg:57.92ms
step:1355/2330 train_time:78483ms step_avg:57.92ms
step:1356/2330 train_time:78544ms step_avg:57.92ms
step:1357/2330 train_time:78602ms step_avg:57.92ms
step:1358/2330 train_time:78661ms step_avg:57.92ms
step:1359/2330 train_time:78718ms step_avg:57.92ms
step:1360/2330 train_time:78778ms step_avg:57.93ms
step:1361/2330 train_time:78836ms step_avg:57.92ms
step:1362/2330 train_time:78896ms step_avg:57.93ms
step:1363/2330 train_time:78953ms step_avg:57.93ms
step:1364/2330 train_time:79013ms step_avg:57.93ms
step:1365/2330 train_time:79070ms step_avg:57.93ms
step:1366/2330 train_time:79131ms step_avg:57.93ms
step:1367/2330 train_time:79187ms step_avg:57.93ms
step:1368/2330 train_time:79248ms step_avg:57.93ms
step:1369/2330 train_time:79305ms step_avg:57.93ms
step:1370/2330 train_time:79366ms step_avg:57.93ms
step:1371/2330 train_time:79423ms step_avg:57.93ms
step:1372/2330 train_time:79483ms step_avg:57.93ms
step:1373/2330 train_time:79540ms step_avg:57.93ms
step:1374/2330 train_time:79601ms step_avg:57.93ms
step:1375/2330 train_time:79658ms step_avg:57.93ms
step:1376/2330 train_time:79717ms step_avg:57.93ms
step:1377/2330 train_time:79775ms step_avg:57.93ms
step:1378/2330 train_time:79835ms step_avg:57.94ms
step:1379/2330 train_time:79892ms step_avg:57.94ms
step:1380/2330 train_time:79953ms step_avg:57.94ms
step:1381/2330 train_time:80010ms step_avg:57.94ms
step:1382/2330 train_time:80070ms step_avg:57.94ms
step:1383/2330 train_time:80128ms step_avg:57.94ms
step:1384/2330 train_time:80188ms step_avg:57.94ms
step:1385/2330 train_time:80245ms step_avg:57.94ms
step:1386/2330 train_time:80305ms step_avg:57.94ms
step:1387/2330 train_time:80362ms step_avg:57.94ms
step:1388/2330 train_time:80423ms step_avg:57.94ms
step:1389/2330 train_time:80479ms step_avg:57.94ms
step:1390/2330 train_time:80539ms step_avg:57.94ms
step:1391/2330 train_time:80595ms step_avg:57.94ms
step:1392/2330 train_time:80656ms step_avg:57.94ms
step:1393/2330 train_time:80713ms step_avg:57.94ms
step:1394/2330 train_time:80772ms step_avg:57.94ms
step:1395/2330 train_time:80829ms step_avg:57.94ms
step:1396/2330 train_time:80890ms step_avg:57.94ms
step:1397/2330 train_time:80947ms step_avg:57.94ms
step:1398/2330 train_time:81007ms step_avg:57.94ms
step:1399/2330 train_time:81063ms step_avg:57.94ms
step:1400/2330 train_time:81124ms step_avg:57.95ms
step:1401/2330 train_time:81181ms step_avg:57.94ms
step:1402/2330 train_time:81241ms step_avg:57.95ms
step:1403/2330 train_time:81298ms step_avg:57.95ms
step:1404/2330 train_time:81358ms step_avg:57.95ms
step:1405/2330 train_time:81416ms step_avg:57.95ms
step:1406/2330 train_time:81475ms step_avg:57.95ms
step:1407/2330 train_time:81532ms step_avg:57.95ms
step:1408/2330 train_time:81592ms step_avg:57.95ms
step:1409/2330 train_time:81649ms step_avg:57.95ms
step:1410/2330 train_time:81709ms step_avg:57.95ms
step:1411/2330 train_time:81767ms step_avg:57.95ms
step:1412/2330 train_time:81826ms step_avg:57.95ms
step:1413/2330 train_time:81883ms step_avg:57.95ms
step:1414/2330 train_time:81944ms step_avg:57.95ms
step:1415/2330 train_time:82001ms step_avg:57.95ms
step:1416/2330 train_time:82061ms step_avg:57.95ms
step:1417/2330 train_time:82119ms step_avg:57.95ms
step:1418/2330 train_time:82178ms step_avg:57.95ms
step:1419/2330 train_time:82235ms step_avg:57.95ms
step:1420/2330 train_time:82296ms step_avg:57.95ms
step:1421/2330 train_time:82353ms step_avg:57.95ms
step:1422/2330 train_time:82414ms step_avg:57.96ms
step:1423/2330 train_time:82472ms step_avg:57.96ms
step:1424/2330 train_time:82531ms step_avg:57.96ms
step:1425/2330 train_time:82588ms step_avg:57.96ms
step:1426/2330 train_time:82648ms step_avg:57.96ms
step:1427/2330 train_time:82705ms step_avg:57.96ms
step:1428/2330 train_time:82766ms step_avg:57.96ms
step:1429/2330 train_time:82822ms step_avg:57.96ms
step:1430/2330 train_time:82882ms step_avg:57.96ms
step:1431/2330 train_time:82939ms step_avg:57.96ms
step:1432/2330 train_time:83000ms step_avg:57.96ms
step:1433/2330 train_time:83057ms step_avg:57.96ms
step:1434/2330 train_time:83116ms step_avg:57.96ms
step:1435/2330 train_time:83173ms step_avg:57.96ms
step:1436/2330 train_time:83234ms step_avg:57.96ms
step:1437/2330 train_time:83291ms step_avg:57.96ms
step:1438/2330 train_time:83351ms step_avg:57.96ms
step:1439/2330 train_time:83409ms step_avg:57.96ms
step:1440/2330 train_time:83469ms step_avg:57.96ms
step:1441/2330 train_time:83527ms step_avg:57.96ms
step:1442/2330 train_time:83586ms step_avg:57.97ms
step:1443/2330 train_time:83643ms step_avg:57.96ms
step:1444/2330 train_time:83703ms step_avg:57.97ms
step:1445/2330 train_time:83760ms step_avg:57.97ms
step:1446/2330 train_time:83820ms step_avg:57.97ms
step:1447/2330 train_time:83877ms step_avg:57.97ms
step:1448/2330 train_time:83937ms step_avg:57.97ms
step:1449/2330 train_time:83994ms step_avg:57.97ms
step:1450/2330 train_time:84054ms step_avg:57.97ms
step:1451/2330 train_time:84111ms step_avg:57.97ms
step:1452/2330 train_time:84171ms step_avg:57.97ms
step:1453/2330 train_time:84228ms step_avg:57.97ms
step:1454/2330 train_time:84289ms step_avg:57.97ms
step:1455/2330 train_time:84346ms step_avg:57.97ms
step:1456/2330 train_time:84407ms step_avg:57.97ms
step:1457/2330 train_time:84464ms step_avg:57.97ms
step:1458/2330 train_time:84523ms step_avg:57.97ms
step:1459/2330 train_time:84580ms step_avg:57.97ms
step:1460/2330 train_time:84640ms step_avg:57.97ms
step:1461/2330 train_time:84697ms step_avg:57.97ms
step:1462/2330 train_time:84757ms step_avg:57.97ms
step:1463/2330 train_time:84814ms step_avg:57.97ms
step:1464/2330 train_time:84874ms step_avg:57.97ms
step:1465/2330 train_time:84931ms step_avg:57.97ms
step:1466/2330 train_time:84992ms step_avg:57.98ms
step:1467/2330 train_time:85049ms step_avg:57.97ms
step:1468/2330 train_time:85110ms step_avg:57.98ms
step:1469/2330 train_time:85166ms step_avg:57.98ms
step:1470/2330 train_time:85228ms step_avg:57.98ms
step:1471/2330 train_time:85284ms step_avg:57.98ms
step:1472/2330 train_time:85344ms step_avg:57.98ms
step:1473/2330 train_time:85401ms step_avg:57.98ms
step:1474/2330 train_time:85462ms step_avg:57.98ms
step:1475/2330 train_time:85519ms step_avg:57.98ms
step:1476/2330 train_time:85578ms step_avg:57.98ms
step:1477/2330 train_time:85635ms step_avg:57.98ms
step:1478/2330 train_time:85695ms step_avg:57.98ms
step:1479/2330 train_time:85752ms step_avg:57.98ms
step:1480/2330 train_time:85812ms step_avg:57.98ms
step:1481/2330 train_time:85869ms step_avg:57.98ms
step:1482/2330 train_time:85930ms step_avg:57.98ms
step:1483/2330 train_time:85987ms step_avg:57.98ms
step:1484/2330 train_time:86047ms step_avg:57.98ms
step:1485/2330 train_time:86105ms step_avg:57.98ms
step:1486/2330 train_time:86165ms step_avg:57.98ms
step:1487/2330 train_time:86222ms step_avg:57.98ms
step:1488/2330 train_time:86282ms step_avg:57.99ms
step:1489/2330 train_time:86339ms step_avg:57.98ms
step:1490/2330 train_time:86398ms step_avg:57.99ms
step:1491/2330 train_time:86456ms step_avg:57.99ms
step:1492/2330 train_time:86516ms step_avg:57.99ms
step:1493/2330 train_time:86573ms step_avg:57.99ms
step:1494/2330 train_time:86633ms step_avg:57.99ms
step:1495/2330 train_time:86690ms step_avg:57.99ms
step:1496/2330 train_time:86750ms step_avg:57.99ms
step:1497/2330 train_time:86807ms step_avg:57.99ms
step:1498/2330 train_time:86867ms step_avg:57.99ms
step:1499/2330 train_time:86924ms step_avg:57.99ms
step:1500/2330 train_time:86983ms step_avg:57.99ms
step:1500/2330 val_loss:5.2648 train_time:87064ms step_avg:58.04ms
step:1501/2330 train_time:87084ms step_avg:58.02ms
step:1502/2330 train_time:87105ms step_avg:57.99ms
step:1503/2330 train_time:87161ms step_avg:57.99ms
step:1504/2330 train_time:87226ms step_avg:58.00ms
step:1505/2330 train_time:87284ms step_avg:58.00ms
step:1506/2330 train_time:87345ms step_avg:58.00ms
step:1507/2330 train_time:87401ms step_avg:58.00ms
step:1508/2330 train_time:87462ms step_avg:58.00ms
step:1509/2330 train_time:87518ms step_avg:58.00ms
step:1510/2330 train_time:87577ms step_avg:58.00ms
step:1511/2330 train_time:87634ms step_avg:58.00ms
step:1512/2330 train_time:87693ms step_avg:58.00ms
step:1513/2330 train_time:87750ms step_avg:58.00ms
step:1514/2330 train_time:87809ms step_avg:58.00ms
step:1515/2330 train_time:87866ms step_avg:58.00ms
step:1516/2330 train_time:87926ms step_avg:58.00ms
step:1517/2330 train_time:87983ms step_avg:58.00ms
step:1518/2330 train_time:88043ms step_avg:58.00ms
step:1519/2330 train_time:88101ms step_avg:58.00ms
step:1520/2330 train_time:88162ms step_avg:58.00ms
step:1521/2330 train_time:88221ms step_avg:58.00ms
step:1522/2330 train_time:88281ms step_avg:58.00ms
step:1523/2330 train_time:88338ms step_avg:58.00ms
step:1524/2330 train_time:88399ms step_avg:58.00ms
step:1525/2330 train_time:88455ms step_avg:58.00ms
step:1526/2330 train_time:88515ms step_avg:58.00ms
step:1527/2330 train_time:88572ms step_avg:58.00ms
step:1528/2330 train_time:88632ms step_avg:58.01ms
step:1529/2330 train_time:88690ms step_avg:58.01ms
step:1530/2330 train_time:88748ms step_avg:58.01ms
step:1531/2330 train_time:88805ms step_avg:58.00ms
step:1532/2330 train_time:88865ms step_avg:58.01ms
step:1533/2330 train_time:88922ms step_avg:58.00ms
step:1534/2330 train_time:88983ms step_avg:58.01ms
step:1535/2330 train_time:89040ms step_avg:58.01ms
step:1536/2330 train_time:89100ms step_avg:58.01ms
step:1537/2330 train_time:89158ms step_avg:58.01ms
step:1538/2330 train_time:89219ms step_avg:58.01ms
step:1539/2330 train_time:89277ms step_avg:58.01ms
step:1540/2330 train_time:89338ms step_avg:58.01ms
step:1541/2330 train_time:89396ms step_avg:58.01ms
step:1542/2330 train_time:89456ms step_avg:58.01ms
step:1543/2330 train_time:89514ms step_avg:58.01ms
step:1544/2330 train_time:89575ms step_avg:58.01ms
step:1545/2330 train_time:89632ms step_avg:58.01ms
step:1546/2330 train_time:89693ms step_avg:58.02ms
step:1547/2330 train_time:89750ms step_avg:58.02ms
step:1548/2330 train_time:89810ms step_avg:58.02ms
step:1549/2330 train_time:89867ms step_avg:58.02ms
step:1550/2330 train_time:89928ms step_avg:58.02ms
step:1551/2330 train_time:89986ms step_avg:58.02ms
step:1552/2330 train_time:90046ms step_avg:58.02ms
step:1553/2330 train_time:90102ms step_avg:58.02ms
step:1554/2330 train_time:90164ms step_avg:58.02ms
step:1555/2330 train_time:90221ms step_avg:58.02ms
step:1556/2330 train_time:90283ms step_avg:58.02ms
step:1557/2330 train_time:90340ms step_avg:58.02ms
step:1558/2330 train_time:90400ms step_avg:58.02ms
step:1559/2330 train_time:90457ms step_avg:58.02ms
step:1560/2330 train_time:90518ms step_avg:58.02ms
step:1561/2330 train_time:90576ms step_avg:58.02ms
step:1562/2330 train_time:90636ms step_avg:58.03ms
step:1563/2330 train_time:90694ms step_avg:58.03ms
step:1564/2330 train_time:90754ms step_avg:58.03ms
step:1565/2330 train_time:90812ms step_avg:58.03ms
step:1566/2330 train_time:90872ms step_avg:58.03ms
step:1567/2330 train_time:90930ms step_avg:58.03ms
step:1568/2330 train_time:90991ms step_avg:58.03ms
step:1569/2330 train_time:91049ms step_avg:58.03ms
step:1570/2330 train_time:91110ms step_avg:58.03ms
step:1571/2330 train_time:91167ms step_avg:58.03ms
step:1572/2330 train_time:91230ms step_avg:58.03ms
step:1573/2330 train_time:91287ms step_avg:58.03ms
step:1574/2330 train_time:91349ms step_avg:58.04ms
step:1575/2330 train_time:91405ms step_avg:58.04ms
step:1576/2330 train_time:91468ms step_avg:58.04ms
step:1577/2330 train_time:91524ms step_avg:58.04ms
step:1578/2330 train_time:91587ms step_avg:58.04ms
step:1579/2330 train_time:91644ms step_avg:58.04ms
step:1580/2330 train_time:91704ms step_avg:58.04ms
step:1581/2330 train_time:91761ms step_avg:58.04ms
step:1582/2330 train_time:91821ms step_avg:58.04ms
step:1583/2330 train_time:91878ms step_avg:58.04ms
step:1584/2330 train_time:91940ms step_avg:58.04ms
step:1585/2330 train_time:91997ms step_avg:58.04ms
step:1586/2330 train_time:92057ms step_avg:58.04ms
step:1587/2330 train_time:92115ms step_avg:58.04ms
step:1588/2330 train_time:92176ms step_avg:58.05ms
step:1589/2330 train_time:92235ms step_avg:58.05ms
step:1590/2330 train_time:92296ms step_avg:58.05ms
step:1591/2330 train_time:92355ms step_avg:58.05ms
step:1592/2330 train_time:92415ms step_avg:58.05ms
step:1593/2330 train_time:92472ms step_avg:58.05ms
step:1594/2330 train_time:92534ms step_avg:58.05ms
step:1595/2330 train_time:92591ms step_avg:58.05ms
step:1596/2330 train_time:92652ms step_avg:58.05ms
step:1597/2330 train_time:92709ms step_avg:58.05ms
step:1598/2330 train_time:92770ms step_avg:58.05ms
step:1599/2330 train_time:92827ms step_avg:58.05ms
step:1600/2330 train_time:92889ms step_avg:58.06ms
step:1601/2330 train_time:92946ms step_avg:58.05ms
step:1602/2330 train_time:93006ms step_avg:58.06ms
step:1603/2330 train_time:93063ms step_avg:58.06ms
step:1604/2330 train_time:93124ms step_avg:58.06ms
step:1605/2330 train_time:93182ms step_avg:58.06ms
step:1606/2330 train_time:93242ms step_avg:58.06ms
step:1607/2330 train_time:93301ms step_avg:58.06ms
step:1608/2330 train_time:93361ms step_avg:58.06ms
step:1609/2330 train_time:93419ms step_avg:58.06ms
step:1610/2330 train_time:93480ms step_avg:58.06ms
step:1611/2330 train_time:93539ms step_avg:58.06ms
step:1612/2330 train_time:93599ms step_avg:58.06ms
step:1613/2330 train_time:93656ms step_avg:58.06ms
step:1614/2330 train_time:93717ms step_avg:58.06ms
step:1615/2330 train_time:93776ms step_avg:58.07ms
step:1616/2330 train_time:93836ms step_avg:58.07ms
step:1617/2330 train_time:93894ms step_avg:58.07ms
step:1618/2330 train_time:93954ms step_avg:58.07ms
step:1619/2330 train_time:94011ms step_avg:58.07ms
step:1620/2330 train_time:94074ms step_avg:58.07ms
step:1621/2330 train_time:94130ms step_avg:58.07ms
step:1622/2330 train_time:94192ms step_avg:58.07ms
step:1623/2330 train_time:94249ms step_avg:58.07ms
step:1624/2330 train_time:94311ms step_avg:58.07ms
step:1625/2330 train_time:94368ms step_avg:58.07ms
step:1626/2330 train_time:94430ms step_avg:58.08ms
step:1627/2330 train_time:94487ms step_avg:58.07ms
step:1628/2330 train_time:94549ms step_avg:58.08ms
step:1629/2330 train_time:94605ms step_avg:58.08ms
step:1630/2330 train_time:94668ms step_avg:58.08ms
step:1631/2330 train_time:94725ms step_avg:58.08ms
step:1632/2330 train_time:94787ms step_avg:58.08ms
step:1633/2330 train_time:94843ms step_avg:58.08ms
step:1634/2330 train_time:94904ms step_avg:58.08ms
step:1635/2330 train_time:94960ms step_avg:58.08ms
step:1636/2330 train_time:95022ms step_avg:58.08ms
step:1637/2330 train_time:95079ms step_avg:58.08ms
step:1638/2330 train_time:95140ms step_avg:58.08ms
step:1639/2330 train_time:95197ms step_avg:58.08ms
step:1640/2330 train_time:95257ms step_avg:58.08ms
step:1641/2330 train_time:95315ms step_avg:58.08ms
step:1642/2330 train_time:95376ms step_avg:58.09ms
step:1643/2330 train_time:95435ms step_avg:58.09ms
step:1644/2330 train_time:95495ms step_avg:58.09ms
step:1645/2330 train_time:95554ms step_avg:58.09ms
step:1646/2330 train_time:95614ms step_avg:58.09ms
step:1647/2330 train_time:95672ms step_avg:58.09ms
step:1648/2330 train_time:95732ms step_avg:58.09ms
step:1649/2330 train_time:95789ms step_avg:58.09ms
step:1650/2330 train_time:95851ms step_avg:58.09ms
step:1651/2330 train_time:95908ms step_avg:58.09ms
step:1652/2330 train_time:95970ms step_avg:58.09ms
step:1653/2330 train_time:96027ms step_avg:58.09ms
step:1654/2330 train_time:96089ms step_avg:58.09ms
step:1655/2330 train_time:96146ms step_avg:58.09ms
step:1656/2330 train_time:96207ms step_avg:58.10ms
step:1657/2330 train_time:96264ms step_avg:58.10ms
step:1658/2330 train_time:96327ms step_avg:58.10ms
step:1659/2330 train_time:96384ms step_avg:58.10ms
step:1660/2330 train_time:96445ms step_avg:58.10ms
step:1661/2330 train_time:96501ms step_avg:58.10ms
step:1662/2330 train_time:96563ms step_avg:58.10ms
step:1663/2330 train_time:96620ms step_avg:58.10ms
step:1664/2330 train_time:96680ms step_avg:58.10ms
step:1665/2330 train_time:96738ms step_avg:58.10ms
step:1666/2330 train_time:96798ms step_avg:58.10ms
step:1667/2330 train_time:96857ms step_avg:58.10ms
step:1668/2330 train_time:96917ms step_avg:58.10ms
step:1669/2330 train_time:96975ms step_avg:58.10ms
step:1670/2330 train_time:97036ms step_avg:58.11ms
step:1671/2330 train_time:97094ms step_avg:58.11ms
step:1672/2330 train_time:97155ms step_avg:58.11ms
step:1673/2330 train_time:97214ms step_avg:58.11ms
step:1674/2330 train_time:97274ms step_avg:58.11ms
step:1675/2330 train_time:97332ms step_avg:58.11ms
step:1676/2330 train_time:97392ms step_avg:58.11ms
step:1677/2330 train_time:97449ms step_avg:58.11ms
step:1678/2330 train_time:97510ms step_avg:58.11ms
step:1679/2330 train_time:97568ms step_avg:58.11ms
step:1680/2330 train_time:97630ms step_avg:58.11ms
step:1681/2330 train_time:97686ms step_avg:58.11ms
step:1682/2330 train_time:97749ms step_avg:58.11ms
step:1683/2330 train_time:97805ms step_avg:58.11ms
step:1684/2330 train_time:97868ms step_avg:58.12ms
step:1685/2330 train_time:97925ms step_avg:58.12ms
step:1686/2330 train_time:97986ms step_avg:58.12ms
step:1687/2330 train_time:98043ms step_avg:58.12ms
step:1688/2330 train_time:98103ms step_avg:58.12ms
step:1689/2330 train_time:98161ms step_avg:58.12ms
step:1690/2330 train_time:98221ms step_avg:58.12ms
step:1691/2330 train_time:98280ms step_avg:58.12ms
step:1692/2330 train_time:98340ms step_avg:58.12ms
step:1693/2330 train_time:98398ms step_avg:58.12ms
step:1694/2330 train_time:98458ms step_avg:58.12ms
step:1695/2330 train_time:98516ms step_avg:58.12ms
step:1696/2330 train_time:98576ms step_avg:58.12ms
step:1697/2330 train_time:98635ms step_avg:58.12ms
step:1698/2330 train_time:98696ms step_avg:58.12ms
step:1699/2330 train_time:98754ms step_avg:58.12ms
step:1700/2330 train_time:98814ms step_avg:58.13ms
step:1701/2330 train_time:98872ms step_avg:58.13ms
step:1702/2330 train_time:98933ms step_avg:58.13ms
step:1703/2330 train_time:98991ms step_avg:58.13ms
step:1704/2330 train_time:99051ms step_avg:58.13ms
step:1705/2330 train_time:99109ms step_avg:58.13ms
step:1706/2330 train_time:99170ms step_avg:58.13ms
step:1707/2330 train_time:99227ms step_avg:58.13ms
step:1708/2330 train_time:99289ms step_avg:58.13ms
step:1709/2330 train_time:99346ms step_avg:58.13ms
step:1710/2330 train_time:99407ms step_avg:58.13ms
step:1711/2330 train_time:99464ms step_avg:58.13ms
step:1712/2330 train_time:99525ms step_avg:58.13ms
step:1713/2330 train_time:99582ms step_avg:58.13ms
step:1714/2330 train_time:99643ms step_avg:58.13ms
step:1715/2330 train_time:99700ms step_avg:58.13ms
step:1716/2330 train_time:99761ms step_avg:58.14ms
step:1717/2330 train_time:99819ms step_avg:58.14ms
step:1718/2330 train_time:99879ms step_avg:58.14ms
step:1719/2330 train_time:99937ms step_avg:58.14ms
step:1720/2330 train_time:99997ms step_avg:58.14ms
step:1721/2330 train_time:100055ms step_avg:58.14ms
step:1722/2330 train_time:100115ms step_avg:58.14ms
step:1723/2330 train_time:100172ms step_avg:58.14ms
step:1724/2330 train_time:100233ms step_avg:58.14ms
step:1725/2330 train_time:100291ms step_avg:58.14ms
step:1726/2330 train_time:100351ms step_avg:58.14ms
step:1727/2330 train_time:100409ms step_avg:58.14ms
step:1728/2330 train_time:100470ms step_avg:58.14ms
step:1729/2330 train_time:100527ms step_avg:58.14ms
step:1730/2330 train_time:100590ms step_avg:58.14ms
step:1731/2330 train_time:100646ms step_avg:58.14ms
step:1732/2330 train_time:100707ms step_avg:58.14ms
step:1733/2330 train_time:100764ms step_avg:58.14ms
step:1734/2330 train_time:100826ms step_avg:58.15ms
step:1735/2330 train_time:100883ms step_avg:58.15ms
step:1736/2330 train_time:100944ms step_avg:58.15ms
step:1737/2330 train_time:101000ms step_avg:58.15ms
step:1738/2330 train_time:101061ms step_avg:58.15ms
step:1739/2330 train_time:101118ms step_avg:58.15ms
step:1740/2330 train_time:101179ms step_avg:58.15ms
step:1741/2330 train_time:101236ms step_avg:58.15ms
step:1742/2330 train_time:101296ms step_avg:58.15ms
step:1743/2330 train_time:101354ms step_avg:58.15ms
step:1744/2330 train_time:101415ms step_avg:58.15ms
step:1745/2330 train_time:101473ms step_avg:58.15ms
step:1746/2330 train_time:101534ms step_avg:58.15ms
step:1747/2330 train_time:101591ms step_avg:58.15ms
step:1748/2330 train_time:101653ms step_avg:58.15ms
step:1749/2330 train_time:101710ms step_avg:58.15ms
step:1750/2330 train_time:101771ms step_avg:58.15ms
step:1750/2330 val_loss:5.1446 train_time:101853ms step_avg:58.20ms
step:1751/2330 train_time:101872ms step_avg:58.18ms
step:1752/2330 train_time:101893ms step_avg:58.16ms
step:1753/2330 train_time:101947ms step_avg:58.16ms
step:1754/2330 train_time:102016ms step_avg:58.16ms
step:1755/2330 train_time:102073ms step_avg:58.16ms
step:1756/2330 train_time:102138ms step_avg:58.17ms
step:1757/2330 train_time:102195ms step_avg:58.16ms
step:1758/2330 train_time:102255ms step_avg:58.17ms
step:1759/2330 train_time:102312ms step_avg:58.16ms
step:1760/2330 train_time:102373ms step_avg:58.17ms
step:1761/2330 train_time:102430ms step_avg:58.17ms
step:1762/2330 train_time:102490ms step_avg:58.17ms
step:1763/2330 train_time:102547ms step_avg:58.17ms
step:1764/2330 train_time:102607ms step_avg:58.17ms
step:1765/2330 train_time:102663ms step_avg:58.17ms
step:1766/2330 train_time:102723ms step_avg:58.17ms
step:1767/2330 train_time:102782ms step_avg:58.17ms
step:1768/2330 train_time:102843ms step_avg:58.17ms
step:1769/2330 train_time:102902ms step_avg:58.17ms
step:1770/2330 train_time:102964ms step_avg:58.17ms
step:1771/2330 train_time:103022ms step_avg:58.17ms
step:1772/2330 train_time:103082ms step_avg:58.17ms
step:1773/2330 train_time:103139ms step_avg:58.17ms
step:1774/2330 train_time:103200ms step_avg:58.17ms
step:1775/2330 train_time:103257ms step_avg:58.17ms
step:1776/2330 train_time:103317ms step_avg:58.17ms
step:1777/2330 train_time:103375ms step_avg:58.17ms
step:1778/2330 train_time:103436ms step_avg:58.18ms
step:1779/2330 train_time:103493ms step_avg:58.17ms
step:1780/2330 train_time:103553ms step_avg:58.18ms
step:1781/2330 train_time:103610ms step_avg:58.18ms
step:1782/2330 train_time:103670ms step_avg:58.18ms
step:1783/2330 train_time:103727ms step_avg:58.18ms
step:1784/2330 train_time:103788ms step_avg:58.18ms
step:1785/2330 train_time:103845ms step_avg:58.18ms
step:1786/2330 train_time:103907ms step_avg:58.18ms
step:1787/2330 train_time:103965ms step_avg:58.18ms
step:1788/2330 train_time:104027ms step_avg:58.18ms
step:1789/2330 train_time:104083ms step_avg:58.18ms
step:1790/2330 train_time:104144ms step_avg:58.18ms
step:1791/2330 train_time:104201ms step_avg:58.18ms
step:1792/2330 train_time:104262ms step_avg:58.18ms
step:1793/2330 train_time:104320ms step_avg:58.18ms
step:1794/2330 train_time:104380ms step_avg:58.18ms
step:1795/2330 train_time:104437ms step_avg:58.18ms
step:1796/2330 train_time:104497ms step_avg:58.18ms
step:1797/2330 train_time:104554ms step_avg:58.18ms
step:1798/2330 train_time:104614ms step_avg:58.18ms
step:1799/2330 train_time:104671ms step_avg:58.18ms
step:1800/2330 train_time:104733ms step_avg:58.18ms
step:1801/2330 train_time:104790ms step_avg:58.18ms
step:1802/2330 train_time:104852ms step_avg:58.19ms
step:1803/2330 train_time:104910ms step_avg:58.19ms
step:1804/2330 train_time:104971ms step_avg:58.19ms
step:1805/2330 train_time:105028ms step_avg:58.19ms
step:1806/2330 train_time:105090ms step_avg:58.19ms
step:1807/2330 train_time:105147ms step_avg:58.19ms
step:1808/2330 train_time:105209ms step_avg:58.19ms
step:1809/2330 train_time:105265ms step_avg:58.19ms
step:1810/2330 train_time:105327ms step_avg:58.19ms
step:1811/2330 train_time:105384ms step_avg:58.19ms
step:1812/2330 train_time:105445ms step_avg:58.19ms
step:1813/2330 train_time:105502ms step_avg:58.19ms
step:1814/2330 train_time:105562ms step_avg:58.19ms
step:1815/2330 train_time:105620ms step_avg:58.19ms
step:1816/2330 train_time:105680ms step_avg:58.19ms
step:1817/2330 train_time:105737ms step_avg:58.19ms
step:1818/2330 train_time:105798ms step_avg:58.19ms
step:1819/2330 train_time:105856ms step_avg:58.19ms
step:1820/2330 train_time:105917ms step_avg:58.20ms
step:1821/2330 train_time:105975ms step_avg:58.20ms
step:1822/2330 train_time:106036ms step_avg:58.20ms
step:1823/2330 train_time:106094ms step_avg:58.20ms
step:1824/2330 train_time:106155ms step_avg:58.20ms
step:1825/2330 train_time:106213ms step_avg:58.20ms
step:1826/2330 train_time:106274ms step_avg:58.20ms
step:1827/2330 train_time:106330ms step_avg:58.20ms
step:1828/2330 train_time:106393ms step_avg:58.20ms
step:1829/2330 train_time:106449ms step_avg:58.20ms
step:1830/2330 train_time:106512ms step_avg:58.20ms
step:1831/2330 train_time:106568ms step_avg:58.20ms
step:1832/2330 train_time:106629ms step_avg:58.20ms
step:1833/2330 train_time:106686ms step_avg:58.20ms
step:1834/2330 train_time:106746ms step_avg:58.20ms
step:1835/2330 train_time:106803ms step_avg:58.20ms
step:1836/2330 train_time:106864ms step_avg:58.20ms
step:1837/2330 train_time:106922ms step_avg:58.20ms
step:1838/2330 train_time:106982ms step_avg:58.21ms
step:1839/2330 train_time:107039ms step_avg:58.21ms
step:1840/2330 train_time:107100ms step_avg:58.21ms
step:1841/2330 train_time:107158ms step_avg:58.21ms
step:1842/2330 train_time:107219ms step_avg:58.21ms
step:1843/2330 train_time:107277ms step_avg:58.21ms
step:1844/2330 train_time:107338ms step_avg:58.21ms
step:1845/2330 train_time:107396ms step_avg:58.21ms
step:1846/2330 train_time:107457ms step_avg:58.21ms
step:1847/2330 train_time:107516ms step_avg:58.21ms
step:1848/2330 train_time:107576ms step_avg:58.21ms
step:1849/2330 train_time:107634ms step_avg:58.21ms
step:1850/2330 train_time:107694ms step_avg:58.21ms
step:1851/2330 train_time:107752ms step_avg:58.21ms
step:1852/2330 train_time:107813ms step_avg:58.21ms
step:1853/2330 train_time:107870ms step_avg:58.21ms
step:1854/2330 train_time:107931ms step_avg:58.22ms
step:1855/2330 train_time:107987ms step_avg:58.21ms
step:1856/2330 train_time:108050ms step_avg:58.22ms
step:1857/2330 train_time:108107ms step_avg:58.22ms
step:1858/2330 train_time:108168ms step_avg:58.22ms
step:1859/2330 train_time:108224ms step_avg:58.22ms
step:1860/2330 train_time:108286ms step_avg:58.22ms
step:1861/2330 train_time:108343ms step_avg:58.22ms
step:1862/2330 train_time:108405ms step_avg:58.22ms
step:1863/2330 train_time:108462ms step_avg:58.22ms
step:1864/2330 train_time:108522ms step_avg:58.22ms
step:1865/2330 train_time:108579ms step_avg:58.22ms
step:1866/2330 train_time:108640ms step_avg:58.22ms
step:1867/2330 train_time:108697ms step_avg:58.22ms
step:1868/2330 train_time:108757ms step_avg:58.22ms
step:1869/2330 train_time:108815ms step_avg:58.22ms
step:1870/2330 train_time:108876ms step_avg:58.22ms
step:1871/2330 train_time:108933ms step_avg:58.22ms
step:1872/2330 train_time:108993ms step_avg:58.22ms
step:1873/2330 train_time:109051ms step_avg:58.22ms
step:1874/2330 train_time:109112ms step_avg:58.22ms
step:1875/2330 train_time:109169ms step_avg:58.22ms
step:1876/2330 train_time:109231ms step_avg:58.23ms
step:1877/2330 train_time:109287ms step_avg:58.22ms
step:1878/2330 train_time:109351ms step_avg:58.23ms
step:1879/2330 train_time:109407ms step_avg:58.23ms
step:1880/2330 train_time:109471ms step_avg:58.23ms
step:1881/2330 train_time:109527ms step_avg:58.23ms
step:1882/2330 train_time:109588ms step_avg:58.23ms
step:1883/2330 train_time:109645ms step_avg:58.23ms
step:1884/2330 train_time:109707ms step_avg:58.23ms
step:1885/2330 train_time:109763ms step_avg:58.23ms
step:1886/2330 train_time:109824ms step_avg:58.23ms
step:1887/2330 train_time:109881ms step_avg:58.23ms
step:1888/2330 train_time:109941ms step_avg:58.23ms
step:1889/2330 train_time:109998ms step_avg:58.23ms
step:1890/2330 train_time:110059ms step_avg:58.23ms
step:1891/2330 train_time:110117ms step_avg:58.23ms
step:1892/2330 train_time:110178ms step_avg:58.23ms
step:1893/2330 train_time:110237ms step_avg:58.23ms
step:1894/2330 train_time:110297ms step_avg:58.24ms
step:1895/2330 train_time:110356ms step_avg:58.24ms
step:1896/2330 train_time:110417ms step_avg:58.24ms
step:1897/2330 train_time:110475ms step_avg:58.24ms
step:1898/2330 train_time:110535ms step_avg:58.24ms
step:1899/2330 train_time:110593ms step_avg:58.24ms
step:1900/2330 train_time:110653ms step_avg:58.24ms
step:1901/2330 train_time:110711ms step_avg:58.24ms
step:1902/2330 train_time:110773ms step_avg:58.24ms
step:1903/2330 train_time:110830ms step_avg:58.24ms
step:1904/2330 train_time:110890ms step_avg:58.24ms
step:1905/2330 train_time:110947ms step_avg:58.24ms
step:1906/2330 train_time:111008ms step_avg:58.24ms
step:1907/2330 train_time:111065ms step_avg:58.24ms
step:1908/2330 train_time:111125ms step_avg:58.24ms
step:1909/2330 train_time:111181ms step_avg:58.24ms
step:1910/2330 train_time:111242ms step_avg:58.24ms
step:1911/2330 train_time:111299ms step_avg:58.24ms
step:1912/2330 train_time:111360ms step_avg:58.24ms
step:1913/2330 train_time:111417ms step_avg:58.24ms
step:1914/2330 train_time:111478ms step_avg:58.24ms
step:1915/2330 train_time:111535ms step_avg:58.24ms
step:1916/2330 train_time:111597ms step_avg:58.24ms
step:1917/2330 train_time:111654ms step_avg:58.24ms
step:1918/2330 train_time:111715ms step_avg:58.25ms
step:1919/2330 train_time:111773ms step_avg:58.25ms
step:1920/2330 train_time:111834ms step_avg:58.25ms
step:1921/2330 train_time:111891ms step_avg:58.25ms
step:1922/2330 train_time:111952ms step_avg:58.25ms
step:1923/2330 train_time:112009ms step_avg:58.25ms
step:1924/2330 train_time:112072ms step_avg:58.25ms
step:1925/2330 train_time:112128ms step_avg:58.25ms
step:1926/2330 train_time:112190ms step_avg:58.25ms
step:1927/2330 train_time:112246ms step_avg:58.25ms
step:1928/2330 train_time:112309ms step_avg:58.25ms
step:1929/2330 train_time:112366ms step_avg:58.25ms
step:1930/2330 train_time:112427ms step_avg:58.25ms
step:1931/2330 train_time:112483ms step_avg:58.25ms
step:1932/2330 train_time:112544ms step_avg:58.25ms
step:1933/2330 train_time:112601ms step_avg:58.25ms
step:1934/2330 train_time:112662ms step_avg:58.25ms
step:1935/2330 train_time:112720ms step_avg:58.25ms
step:1936/2330 train_time:112781ms step_avg:58.25ms
step:1937/2330 train_time:112839ms step_avg:58.25ms
step:1938/2330 train_time:112899ms step_avg:58.26ms
step:1939/2330 train_time:112957ms step_avg:58.26ms
step:1940/2330 train_time:113017ms step_avg:58.26ms
step:1941/2330 train_time:113075ms step_avg:58.26ms
step:1942/2330 train_time:113135ms step_avg:58.26ms
step:1943/2330 train_time:113193ms step_avg:58.26ms
step:1944/2330 train_time:113255ms step_avg:58.26ms
step:1945/2330 train_time:113312ms step_avg:58.26ms
step:1946/2330 train_time:113374ms step_avg:58.26ms
step:1947/2330 train_time:113431ms step_avg:58.26ms
step:1948/2330 train_time:113493ms step_avg:58.26ms
step:1949/2330 train_time:113550ms step_avg:58.26ms
step:1950/2330 train_time:113611ms step_avg:58.26ms
step:1951/2330 train_time:113667ms step_avg:58.26ms
step:1952/2330 train_time:113730ms step_avg:58.26ms
step:1953/2330 train_time:113786ms step_avg:58.26ms
step:1954/2330 train_time:113847ms step_avg:58.26ms
step:1955/2330 train_time:113903ms step_avg:58.26ms
step:1956/2330 train_time:113965ms step_avg:58.26ms
step:1957/2330 train_time:114023ms step_avg:58.26ms
step:1958/2330 train_time:114082ms step_avg:58.26ms
step:1959/2330 train_time:114140ms step_avg:58.26ms
step:1960/2330 train_time:114200ms step_avg:58.27ms
step:1961/2330 train_time:114257ms step_avg:58.26ms
step:1962/2330 train_time:114317ms step_avg:58.27ms
step:1963/2330 train_time:114375ms step_avg:58.27ms
step:1964/2330 train_time:114436ms step_avg:58.27ms
step:1965/2330 train_time:114494ms step_avg:58.27ms
step:1966/2330 train_time:114555ms step_avg:58.27ms
step:1967/2330 train_time:114612ms step_avg:58.27ms
step:1968/2330 train_time:114673ms step_avg:58.27ms
step:1969/2330 train_time:114731ms step_avg:58.27ms
step:1970/2330 train_time:114792ms step_avg:58.27ms
step:1971/2330 train_time:114849ms step_avg:58.27ms
step:1972/2330 train_time:114911ms step_avg:58.27ms
step:1973/2330 train_time:114967ms step_avg:58.27ms
step:1974/2330 train_time:115029ms step_avg:58.27ms
step:1975/2330 train_time:115086ms step_avg:58.27ms
step:1976/2330 train_time:115148ms step_avg:58.27ms
step:1977/2330 train_time:115205ms step_avg:58.27ms
step:1978/2330 train_time:115265ms step_avg:58.27ms
step:1979/2330 train_time:115322ms step_avg:58.27ms
step:1980/2330 train_time:115382ms step_avg:58.27ms
step:1981/2330 train_time:115439ms step_avg:58.27ms
step:1982/2330 train_time:115499ms step_avg:58.27ms
step:1983/2330 train_time:115557ms step_avg:58.27ms
step:1984/2330 train_time:115617ms step_avg:58.27ms
step:1985/2330 train_time:115675ms step_avg:58.27ms
step:1986/2330 train_time:115737ms step_avg:58.28ms
step:1987/2330 train_time:115795ms step_avg:58.28ms
step:1988/2330 train_time:115855ms step_avg:58.28ms
step:1989/2330 train_time:115914ms step_avg:58.28ms
step:1990/2330 train_time:115975ms step_avg:58.28ms
step:1991/2330 train_time:116032ms step_avg:58.28ms
step:1992/2330 train_time:116093ms step_avg:58.28ms
step:1993/2330 train_time:116151ms step_avg:58.28ms
step:1994/2330 train_time:116212ms step_avg:58.28ms
step:1995/2330 train_time:116269ms step_avg:58.28ms
step:1996/2330 train_time:116331ms step_avg:58.28ms
step:1997/2330 train_time:116387ms step_avg:58.28ms
step:1998/2330 train_time:116448ms step_avg:58.28ms
step:1999/2330 train_time:116505ms step_avg:58.28ms
step:2000/2330 train_time:116567ms step_avg:58.28ms
step:2000/2330 val_loss:5.0571 train_time:116648ms step_avg:58.32ms
step:2001/2330 train_time:116668ms step_avg:58.30ms
step:2002/2330 train_time:116688ms step_avg:58.29ms
step:2003/2330 train_time:116746ms step_avg:58.29ms
step:2004/2330 train_time:116810ms step_avg:58.29ms
step:2005/2330 train_time:116867ms step_avg:58.29ms
step:2006/2330 train_time:116928ms step_avg:58.29ms
step:2007/2330 train_time:116985ms step_avg:58.29ms
step:2008/2330 train_time:117045ms step_avg:58.29ms
step:2009/2330 train_time:117102ms step_avg:58.29ms
step:2010/2330 train_time:117163ms step_avg:58.29ms
step:2011/2330 train_time:117220ms step_avg:58.29ms
step:2012/2330 train_time:117279ms step_avg:58.29ms
step:2013/2330 train_time:117336ms step_avg:58.29ms
step:2014/2330 train_time:117396ms step_avg:58.29ms
step:2015/2330 train_time:117452ms step_avg:58.29ms
step:2016/2330 train_time:117513ms step_avg:58.29ms
step:2017/2330 train_time:117569ms step_avg:58.29ms
step:2018/2330 train_time:117631ms step_avg:58.29ms
step:2019/2330 train_time:117689ms step_avg:58.29ms
step:2020/2330 train_time:117754ms step_avg:58.29ms
step:2021/2330 train_time:117811ms step_avg:58.29ms
step:2022/2330 train_time:117875ms step_avg:58.30ms
step:2023/2330 train_time:117932ms step_avg:58.30ms
step:2024/2330 train_time:117996ms step_avg:58.30ms
step:2025/2330 train_time:118052ms step_avg:58.30ms
step:2026/2330 train_time:118114ms step_avg:58.30ms
step:2027/2330 train_time:118171ms step_avg:58.30ms
step:2028/2330 train_time:118232ms step_avg:58.30ms
step:2029/2330 train_time:118288ms step_avg:58.30ms
step:2030/2330 train_time:118349ms step_avg:58.30ms
step:2031/2330 train_time:118405ms step_avg:58.30ms
step:2032/2330 train_time:118465ms step_avg:58.30ms
step:2033/2330 train_time:118522ms step_avg:58.30ms
step:2034/2330 train_time:118582ms step_avg:58.30ms
step:2035/2330 train_time:118640ms step_avg:58.30ms
step:2036/2330 train_time:118701ms step_avg:58.30ms
step:2037/2330 train_time:118759ms step_avg:58.30ms
step:2038/2330 train_time:118821ms step_avg:58.30ms
step:2039/2330 train_time:118879ms step_avg:58.30ms
step:2040/2330 train_time:118940ms step_avg:58.30ms
step:2041/2330 train_time:118998ms step_avg:58.30ms
step:2042/2330 train_time:119059ms step_avg:58.31ms
step:2043/2330 train_time:119116ms step_avg:58.30ms
step:2044/2330 train_time:119177ms step_avg:58.31ms
step:2045/2330 train_time:119234ms step_avg:58.31ms
step:2046/2330 train_time:119295ms step_avg:58.31ms
step:2047/2330 train_time:119353ms step_avg:58.31ms
step:2048/2330 train_time:119413ms step_avg:58.31ms
step:2049/2330 train_time:119470ms step_avg:58.31ms
step:2050/2330 train_time:119531ms step_avg:58.31ms
step:2051/2330 train_time:119587ms step_avg:58.31ms
step:2052/2330 train_time:119649ms step_avg:58.31ms
step:2053/2330 train_time:119706ms step_avg:58.31ms
step:2054/2330 train_time:119768ms step_avg:58.31ms
step:2055/2330 train_time:119826ms step_avg:58.31ms
step:2056/2330 train_time:119888ms step_avg:58.31ms
step:2057/2330 train_time:119946ms step_avg:58.31ms
step:2058/2330 train_time:120008ms step_avg:58.31ms
step:2059/2330 train_time:120064ms step_avg:58.31ms
step:2060/2330 train_time:120126ms step_avg:58.31ms
step:2061/2330 train_time:120182ms step_avg:58.31ms
step:2062/2330 train_time:120243ms step_avg:58.31ms
step:2063/2330 train_time:120301ms step_avg:58.31ms
step:2064/2330 train_time:120361ms step_avg:58.31ms
step:2065/2330 train_time:120418ms step_avg:58.31ms
step:2066/2330 train_time:120479ms step_avg:58.32ms
step:2067/2330 train_time:120537ms step_avg:58.32ms
step:2068/2330 train_time:120597ms step_avg:58.32ms
step:2069/2330 train_time:120655ms step_avg:58.32ms
step:2070/2330 train_time:120716ms step_avg:58.32ms
step:2071/2330 train_time:120774ms step_avg:58.32ms
step:2072/2330 train_time:120836ms step_avg:58.32ms
step:2073/2330 train_time:120895ms step_avg:58.32ms
step:2074/2330 train_time:120955ms step_avg:58.32ms
step:2075/2330 train_time:121013ms step_avg:58.32ms
step:2076/2330 train_time:121075ms step_avg:58.32ms
step:2077/2330 train_time:121132ms step_avg:58.32ms
step:2078/2330 train_time:121193ms step_avg:58.32ms
step:2079/2330 train_time:121250ms step_avg:58.32ms
step:2080/2330 train_time:121312ms step_avg:58.32ms
step:2081/2330 train_time:121368ms step_avg:58.32ms
step:2082/2330 train_time:121430ms step_avg:58.32ms
step:2083/2330 train_time:121487ms step_avg:58.32ms
step:2084/2330 train_time:121547ms step_avg:58.32ms
step:2085/2330 train_time:121604ms step_avg:58.32ms
step:2086/2330 train_time:121664ms step_avg:58.32ms
step:2087/2330 train_time:121722ms step_avg:58.32ms
step:2088/2330 train_time:121782ms step_avg:58.32ms
step:2089/2330 train_time:121840ms step_avg:58.32ms
step:2090/2330 train_time:121900ms step_avg:58.33ms
step:2091/2330 train_time:121957ms step_avg:58.32ms
step:2092/2330 train_time:122019ms step_avg:58.33ms
step:2093/2330 train_time:122078ms step_avg:58.33ms
step:2094/2330 train_time:122138ms step_avg:58.33ms
step:2095/2330 train_time:122197ms step_avg:58.33ms
step:2096/2330 train_time:122257ms step_avg:58.33ms
step:2097/2330 train_time:122315ms step_avg:58.33ms
step:2098/2330 train_time:122375ms step_avg:58.33ms
step:2099/2330 train_time:122432ms step_avg:58.33ms
step:2100/2330 train_time:122494ms step_avg:58.33ms
step:2101/2330 train_time:122551ms step_avg:58.33ms
step:2102/2330 train_time:122612ms step_avg:58.33ms
step:2103/2330 train_time:122669ms step_avg:58.33ms
step:2104/2330 train_time:122731ms step_avg:58.33ms
step:2105/2330 train_time:122788ms step_avg:58.33ms
step:2106/2330 train_time:122849ms step_avg:58.33ms
step:2107/2330 train_time:122906ms step_avg:58.33ms
step:2108/2330 train_time:122968ms step_avg:58.33ms
step:2109/2330 train_time:123026ms step_avg:58.33ms
step:2110/2330 train_time:123086ms step_avg:58.33ms
step:2111/2330 train_time:123143ms step_avg:58.33ms
step:2112/2330 train_time:123203ms step_avg:58.33ms
step:2113/2330 train_time:123261ms step_avg:58.33ms
step:2114/2330 train_time:123321ms step_avg:58.34ms
step:2115/2330 train_time:123379ms step_avg:58.34ms
step:2116/2330 train_time:123439ms step_avg:58.34ms
step:2117/2330 train_time:123497ms step_avg:58.34ms
step:2118/2330 train_time:123557ms step_avg:58.34ms
step:2119/2330 train_time:123615ms step_avg:58.34ms
step:2120/2330 train_time:123676ms step_avg:58.34ms
step:2121/2330 train_time:123734ms step_avg:58.34ms
step:2122/2330 train_time:123794ms step_avg:58.34ms
step:2123/2330 train_time:123852ms step_avg:58.34ms
step:2124/2330 train_time:123913ms step_avg:58.34ms
step:2125/2330 train_time:123970ms step_avg:58.34ms
step:2126/2330 train_time:124032ms step_avg:58.34ms
step:2127/2330 train_time:124089ms step_avg:58.34ms
step:2128/2330 train_time:124152ms step_avg:58.34ms
step:2129/2330 train_time:124208ms step_avg:58.34ms
step:2130/2330 train_time:124272ms step_avg:58.34ms
step:2131/2330 train_time:124329ms step_avg:58.34ms
step:2132/2330 train_time:124391ms step_avg:58.34ms
step:2133/2330 train_time:124448ms step_avg:58.34ms
step:2134/2330 train_time:124509ms step_avg:58.35ms
step:2135/2330 train_time:124565ms step_avg:58.34ms
step:2136/2330 train_time:124627ms step_avg:58.35ms
step:2137/2330 train_time:124683ms step_avg:58.35ms
step:2138/2330 train_time:124744ms step_avg:58.35ms
step:2139/2330 train_time:124802ms step_avg:58.35ms
step:2140/2330 train_time:124863ms step_avg:58.35ms
step:2141/2330 train_time:124921ms step_avg:58.35ms
step:2142/2330 train_time:124980ms step_avg:58.35ms
step:2143/2330 train_time:125038ms step_avg:58.35ms
step:2144/2330 train_time:125099ms step_avg:58.35ms
step:2145/2330 train_time:125158ms step_avg:58.35ms
step:2146/2330 train_time:125218ms step_avg:58.35ms
step:2147/2330 train_time:125276ms step_avg:58.35ms
step:2148/2330 train_time:125337ms step_avg:58.35ms
step:2149/2330 train_time:125395ms step_avg:58.35ms
step:2150/2330 train_time:125456ms step_avg:58.35ms
step:2151/2330 train_time:125513ms step_avg:58.35ms
step:2152/2330 train_time:125574ms step_avg:58.35ms
step:2153/2330 train_time:125631ms step_avg:58.35ms
step:2154/2330 train_time:125693ms step_avg:58.35ms
step:2155/2330 train_time:125750ms step_avg:58.35ms
step:2156/2330 train_time:125811ms step_avg:58.35ms
step:2157/2330 train_time:125867ms step_avg:58.35ms
step:2158/2330 train_time:125930ms step_avg:58.35ms
step:2159/2330 train_time:125987ms step_avg:58.35ms
step:2160/2330 train_time:126048ms step_avg:58.36ms
step:2161/2330 train_time:126105ms step_avg:58.35ms
step:2162/2330 train_time:126166ms step_avg:58.36ms
step:2163/2330 train_time:126224ms step_avg:58.36ms
step:2164/2330 train_time:126285ms step_avg:58.36ms
step:2165/2330 train_time:126342ms step_avg:58.36ms
step:2166/2330 train_time:126402ms step_avg:58.36ms
step:2167/2330 train_time:126459ms step_avg:58.36ms
step:2168/2330 train_time:126520ms step_avg:58.36ms
step:2169/2330 train_time:126577ms step_avg:58.36ms
step:2170/2330 train_time:126638ms step_avg:58.36ms
step:2171/2330 train_time:126695ms step_avg:58.36ms
step:2172/2330 train_time:126757ms step_avg:58.36ms
step:2173/2330 train_time:126815ms step_avg:58.36ms
step:2174/2330 train_time:126876ms step_avg:58.36ms
step:2175/2330 train_time:126934ms step_avg:58.36ms
step:2176/2330 train_time:126994ms step_avg:58.36ms
step:2177/2330 train_time:127052ms step_avg:58.36ms
step:2178/2330 train_time:127113ms step_avg:58.36ms
step:2179/2330 train_time:127169ms step_avg:58.36ms
step:2180/2330 train_time:127234ms step_avg:58.36ms
step:2181/2330 train_time:127290ms step_avg:58.36ms
step:2182/2330 train_time:127352ms step_avg:58.36ms
step:2183/2330 train_time:127409ms step_avg:58.36ms
step:2184/2330 train_time:127470ms step_avg:58.37ms
step:2185/2330 train_time:127527ms step_avg:58.36ms
step:2186/2330 train_time:127588ms step_avg:58.37ms
step:2187/2330 train_time:127645ms step_avg:58.37ms
step:2188/2330 train_time:127705ms step_avg:58.37ms
step:2189/2330 train_time:127763ms step_avg:58.37ms
step:2190/2330 train_time:127823ms step_avg:58.37ms
step:2191/2330 train_time:127880ms step_avg:58.37ms
step:2192/2330 train_time:127941ms step_avg:58.37ms
step:2193/2330 train_time:127999ms step_avg:58.37ms
step:2194/2330 train_time:128060ms step_avg:58.37ms
step:2195/2330 train_time:128117ms step_avg:58.37ms
step:2196/2330 train_time:128178ms step_avg:58.37ms
step:2197/2330 train_time:128235ms step_avg:58.37ms
step:2198/2330 train_time:128296ms step_avg:58.37ms
step:2199/2330 train_time:128354ms step_avg:58.37ms
step:2200/2330 train_time:128415ms step_avg:58.37ms
step:2201/2330 train_time:128472ms step_avg:58.37ms
step:2202/2330 train_time:128533ms step_avg:58.37ms
step:2203/2330 train_time:128590ms step_avg:58.37ms
step:2204/2330 train_time:128652ms step_avg:58.37ms
step:2205/2330 train_time:128708ms step_avg:58.37ms
step:2206/2330 train_time:128771ms step_avg:58.37ms
step:2207/2330 train_time:128827ms step_avg:58.37ms
step:2208/2330 train_time:128889ms step_avg:58.37ms
step:2209/2330 train_time:128946ms step_avg:58.37ms
step:2210/2330 train_time:129008ms step_avg:58.37ms
step:2211/2330 train_time:129065ms step_avg:58.37ms
step:2212/2330 train_time:129126ms step_avg:58.38ms
step:2213/2330 train_time:129182ms step_avg:58.37ms
step:2214/2330 train_time:129243ms step_avg:58.38ms
step:2215/2330 train_time:129301ms step_avg:58.37ms
step:2216/2330 train_time:129361ms step_avg:58.38ms
step:2217/2330 train_time:129419ms step_avg:58.38ms
step:2218/2330 train_time:129480ms step_avg:58.38ms
step:2219/2330 train_time:129538ms step_avg:58.38ms
step:2220/2330 train_time:129598ms step_avg:58.38ms
step:2221/2330 train_time:129656ms step_avg:58.38ms
step:2222/2330 train_time:129717ms step_avg:58.38ms
step:2223/2330 train_time:129775ms step_avg:58.38ms
step:2224/2330 train_time:129835ms step_avg:58.38ms
step:2225/2330 train_time:129892ms step_avg:58.38ms
step:2226/2330 train_time:129954ms step_avg:58.38ms
step:2227/2330 train_time:130011ms step_avg:58.38ms
step:2228/2330 train_time:130072ms step_avg:58.38ms
step:2229/2330 train_time:130129ms step_avg:58.38ms
step:2230/2330 train_time:130191ms step_avg:58.38ms
step:2231/2330 train_time:130247ms step_avg:58.38ms
step:2232/2330 train_time:130308ms step_avg:58.38ms
step:2233/2330 train_time:130365ms step_avg:58.38ms
step:2234/2330 train_time:130426ms step_avg:58.38ms
step:2235/2330 train_time:130483ms step_avg:58.38ms
step:2236/2330 train_time:130543ms step_avg:58.38ms
step:2237/2330 train_time:130600ms step_avg:58.38ms
step:2238/2330 train_time:130661ms step_avg:58.38ms
step:2239/2330 train_time:130719ms step_avg:58.38ms
step:2240/2330 train_time:130780ms step_avg:58.38ms
step:2241/2330 train_time:130838ms step_avg:58.38ms
step:2242/2330 train_time:130899ms step_avg:58.39ms
step:2243/2330 train_time:130957ms step_avg:58.38ms
step:2244/2330 train_time:131017ms step_avg:58.39ms
step:2245/2330 train_time:131074ms step_avg:58.38ms
step:2246/2330 train_time:131136ms step_avg:58.39ms
step:2247/2330 train_time:131194ms step_avg:58.39ms
step:2248/2330 train_time:131255ms step_avg:58.39ms
step:2249/2330 train_time:131312ms step_avg:58.39ms
step:2250/2330 train_time:131374ms step_avg:58.39ms
step:2250/2330 val_loss:4.9844 train_time:131456ms step_avg:58.42ms
step:2251/2330 train_time:131476ms step_avg:58.41ms
step:2252/2330 train_time:131497ms step_avg:58.39ms
step:2253/2330 train_time:131556ms step_avg:58.39ms
step:2254/2330 train_time:131621ms step_avg:58.39ms
step:2255/2330 train_time:131679ms step_avg:58.39ms
step:2256/2330 train_time:131739ms step_avg:58.40ms
step:2257/2330 train_time:131797ms step_avg:58.39ms
step:2258/2330 train_time:131857ms step_avg:58.40ms
step:2259/2330 train_time:131915ms step_avg:58.40ms
step:2260/2330 train_time:131974ms step_avg:58.40ms
step:2261/2330 train_time:132031ms step_avg:58.40ms
step:2262/2330 train_time:132091ms step_avg:58.40ms
step:2263/2330 train_time:132147ms step_avg:58.39ms
step:2264/2330 train_time:132208ms step_avg:58.40ms
step:2265/2330 train_time:132264ms step_avg:58.39ms
step:2266/2330 train_time:132324ms step_avg:58.40ms
step:2267/2330 train_time:132380ms step_avg:58.39ms
step:2268/2330 train_time:132442ms step_avg:58.40ms
step:2269/2330 train_time:132499ms step_avg:58.40ms
step:2270/2330 train_time:132562ms step_avg:58.40ms
step:2271/2330 train_time:132619ms step_avg:58.40ms
step:2272/2330 train_time:132682ms step_avg:58.40ms
step:2273/2330 train_time:132738ms step_avg:58.40ms
step:2274/2330 train_time:132799ms step_avg:58.40ms
step:2275/2330 train_time:132856ms step_avg:58.40ms
step:2276/2330 train_time:132915ms step_avg:58.40ms
step:2277/2330 train_time:132973ms step_avg:58.40ms
step:2278/2330 train_time:133033ms step_avg:58.40ms
step:2279/2330 train_time:133090ms step_avg:58.40ms
step:2280/2330 train_time:133150ms step_avg:58.40ms
step:2281/2330 train_time:133207ms step_avg:58.40ms
step:2282/2330 train_time:133267ms step_avg:58.40ms
step:2283/2330 train_time:133323ms step_avg:58.40ms
step:2284/2330 train_time:133384ms step_avg:58.40ms
step:2285/2330 train_time:133441ms step_avg:58.40ms
step:2286/2330 train_time:133504ms step_avg:58.40ms
step:2287/2330 train_time:133561ms step_avg:58.40ms
step:2288/2330 train_time:133623ms step_avg:58.40ms
step:2289/2330 train_time:133680ms step_avg:58.40ms
step:2290/2330 train_time:133741ms step_avg:58.40ms
step:2291/2330 train_time:133798ms step_avg:58.40ms
step:2292/2330 train_time:133858ms step_avg:58.40ms
step:2293/2330 train_time:133916ms step_avg:58.40ms
step:2294/2330 train_time:133977ms step_avg:58.40ms
step:2295/2330 train_time:134035ms step_avg:58.40ms
step:2296/2330 train_time:134096ms step_avg:58.40ms
step:2297/2330 train_time:134154ms step_avg:58.40ms
step:2298/2330 train_time:134215ms step_avg:58.40ms
step:2299/2330 train_time:134273ms step_avg:58.40ms
step:2300/2330 train_time:134333ms step_avg:58.41ms
step:2301/2330 train_time:134390ms step_avg:58.41ms
step:2302/2330 train_time:134452ms step_avg:58.41ms
step:2303/2330 train_time:134510ms step_avg:58.41ms
step:2304/2330 train_time:134571ms step_avg:58.41ms
step:2305/2330 train_time:134628ms step_avg:58.41ms
step:2306/2330 train_time:134691ms step_avg:58.41ms
step:2307/2330 train_time:134748ms step_avg:58.41ms
step:2308/2330 train_time:134810ms step_avg:58.41ms
step:2309/2330 train_time:134867ms step_avg:58.41ms
step:2310/2330 train_time:134928ms step_avg:58.41ms
step:2311/2330 train_time:134985ms step_avg:58.41ms
step:2312/2330 train_time:135045ms step_avg:58.41ms
step:2313/2330 train_time:135102ms step_avg:58.41ms
step:2314/2330 train_time:135163ms step_avg:58.41ms
step:2315/2330 train_time:135219ms step_avg:58.41ms
step:2316/2330 train_time:135281ms step_avg:58.41ms
step:2317/2330 train_time:135338ms step_avg:58.41ms
step:2318/2330 train_time:135398ms step_avg:58.41ms
step:2319/2330 train_time:135456ms step_avg:58.41ms
step:2320/2330 train_time:135516ms step_avg:58.41ms
step:2321/2330 train_time:135573ms step_avg:58.41ms
step:2322/2330 train_time:135635ms step_avg:58.41ms
step:2323/2330 train_time:135692ms step_avg:58.41ms
step:2324/2330 train_time:135754ms step_avg:58.41ms
step:2325/2330 train_time:135812ms step_avg:58.41ms
step:2326/2330 train_time:135873ms step_avg:58.41ms
step:2327/2330 train_time:135931ms step_avg:58.41ms
step:2328/2330 train_time:135991ms step_avg:58.42ms
step:2329/2330 train_time:136048ms step_avg:58.41ms
step:2330/2330 train_time:136108ms step_avg:58.42ms
step:2330/2330 val_loss:4.9683 train_time:136190ms step_avg:58.45ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
