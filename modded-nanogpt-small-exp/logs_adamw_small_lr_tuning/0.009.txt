import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--adamw_lr', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_lr_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_lr_tuning/{args2.adamw_lr}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=float(args2.adamw_lr),  betas=(0.85, 0.85), eps=1e-10, weight_decay=0.0, fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_lr_tuning/{args2.adamw_lr}", exist_ok=True)
            torch.save(log, f"logs_adamw_small_lr_tuning/{args2.adamw_lr}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 02:09:08 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   36C    P0             118W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   35C    P0             117W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             118W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             115W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   34C    P0             118W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             114W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:84ms step_avg:83.75ms
step:2/2330 train_time:183ms step_avg:91.57ms
step:3/2330 train_time:202ms step_avg:67.33ms
step:4/2330 train_time:221ms step_avg:55.34ms
step:5/2330 train_time:274ms step_avg:54.80ms
step:6/2330 train_time:332ms step_avg:55.28ms
step:7/2330 train_time:386ms step_avg:55.19ms
step:8/2330 train_time:443ms step_avg:55.40ms
step:9/2330 train_time:497ms step_avg:55.27ms
step:10/2330 train_time:555ms step_avg:55.47ms
step:11/2330 train_time:609ms step_avg:55.37ms
step:12/2330 train_time:666ms step_avg:55.53ms
step:13/2330 train_time:721ms step_avg:55.43ms
step:14/2330 train_time:778ms step_avg:55.58ms
step:15/2330 train_time:832ms step_avg:55.49ms
step:16/2330 train_time:891ms step_avg:55.67ms
step:17/2330 train_time:945ms step_avg:55.57ms
step:18/2330 train_time:1003ms step_avg:55.71ms
step:19/2330 train_time:1057ms step_avg:55.63ms
step:20/2330 train_time:1117ms step_avg:55.86ms
step:21/2330 train_time:1173ms step_avg:55.84ms
step:22/2330 train_time:1232ms step_avg:55.99ms
step:23/2330 train_time:1286ms step_avg:55.91ms
step:24/2330 train_time:1346ms step_avg:56.08ms
step:25/2330 train_time:1400ms step_avg:56.00ms
step:26/2330 train_time:1460ms step_avg:56.15ms
step:27/2330 train_time:1514ms step_avg:56.07ms
step:28/2330 train_time:1572ms step_avg:56.15ms
step:29/2330 train_time:1626ms step_avg:56.08ms
step:30/2330 train_time:1685ms step_avg:56.16ms
step:31/2330 train_time:1740ms step_avg:56.12ms
step:32/2330 train_time:1797ms step_avg:56.17ms
step:33/2330 train_time:1852ms step_avg:56.11ms
step:34/2330 train_time:1909ms step_avg:56.14ms
step:35/2330 train_time:1964ms step_avg:56.10ms
step:36/2330 train_time:2022ms step_avg:56.16ms
step:37/2330 train_time:2076ms step_avg:56.12ms
step:38/2330 train_time:2136ms step_avg:56.20ms
step:39/2330 train_time:2191ms step_avg:56.18ms
step:40/2330 train_time:2248ms step_avg:56.21ms
step:41/2330 train_time:2303ms step_avg:56.17ms
step:42/2330 train_time:2363ms step_avg:56.26ms
step:43/2330 train_time:2418ms step_avg:56.24ms
step:44/2330 train_time:2477ms step_avg:56.29ms
step:45/2330 train_time:2532ms step_avg:56.26ms
step:46/2330 train_time:2589ms step_avg:56.28ms
step:47/2330 train_time:2644ms step_avg:56.25ms
step:48/2330 train_time:2702ms step_avg:56.29ms
step:49/2330 train_time:2756ms step_avg:56.25ms
step:50/2330 train_time:2815ms step_avg:56.31ms
step:51/2330 train_time:2870ms step_avg:56.27ms
step:52/2330 train_time:2928ms step_avg:56.30ms
step:53/2330 train_time:2982ms step_avg:56.27ms
step:54/2330 train_time:3042ms step_avg:56.33ms
step:55/2330 train_time:3097ms step_avg:56.30ms
step:56/2330 train_time:3155ms step_avg:56.34ms
step:57/2330 train_time:3210ms step_avg:56.32ms
step:58/2330 train_time:3268ms step_avg:56.34ms
step:59/2330 train_time:3323ms step_avg:56.32ms
step:60/2330 train_time:3382ms step_avg:56.37ms
step:61/2330 train_time:3438ms step_avg:56.35ms
step:62/2330 train_time:3496ms step_avg:56.38ms
step:63/2330 train_time:3551ms step_avg:56.37ms
step:64/2330 train_time:3609ms step_avg:56.39ms
step:65/2330 train_time:3663ms step_avg:56.36ms
step:66/2330 train_time:3722ms step_avg:56.40ms
step:67/2330 train_time:3778ms step_avg:56.39ms
step:68/2330 train_time:3836ms step_avg:56.41ms
step:69/2330 train_time:3891ms step_avg:56.39ms
step:70/2330 train_time:3948ms step_avg:56.40ms
step:71/2330 train_time:4003ms step_avg:56.38ms
step:72/2330 train_time:4061ms step_avg:56.41ms
step:73/2330 train_time:4117ms step_avg:56.39ms
step:74/2330 train_time:4175ms step_avg:56.41ms
step:75/2330 train_time:4230ms step_avg:56.40ms
step:76/2330 train_time:4288ms step_avg:56.42ms
step:77/2330 train_time:4344ms step_avg:56.41ms
step:78/2330 train_time:4402ms step_avg:56.44ms
step:79/2330 train_time:4458ms step_avg:56.43ms
step:80/2330 train_time:4517ms step_avg:56.46ms
step:81/2330 train_time:4573ms step_avg:56.45ms
step:82/2330 train_time:4630ms step_avg:56.47ms
step:83/2330 train_time:4685ms step_avg:56.44ms
step:84/2330 train_time:4743ms step_avg:56.47ms
step:85/2330 train_time:4800ms step_avg:56.47ms
step:86/2330 train_time:4858ms step_avg:56.49ms
step:87/2330 train_time:4913ms step_avg:56.47ms
step:88/2330 train_time:4971ms step_avg:56.49ms
step:89/2330 train_time:5025ms step_avg:56.46ms
step:90/2330 train_time:5085ms step_avg:56.50ms
step:91/2330 train_time:5140ms step_avg:56.48ms
step:92/2330 train_time:5198ms step_avg:56.50ms
step:93/2330 train_time:5253ms step_avg:56.49ms
step:94/2330 train_time:5312ms step_avg:56.51ms
step:95/2330 train_time:5367ms step_avg:56.49ms
step:96/2330 train_time:5426ms step_avg:56.52ms
step:97/2330 train_time:5481ms step_avg:56.51ms
step:98/2330 train_time:5541ms step_avg:56.54ms
step:99/2330 train_time:5596ms step_avg:56.52ms
step:100/2330 train_time:5654ms step_avg:56.54ms
step:101/2330 train_time:5709ms step_avg:56.53ms
step:102/2330 train_time:5767ms step_avg:56.54ms
step:103/2330 train_time:5823ms step_avg:56.53ms
step:104/2330 train_time:5883ms step_avg:56.56ms
step:105/2330 train_time:5937ms step_avg:56.55ms
step:106/2330 train_time:5996ms step_avg:56.57ms
step:107/2330 train_time:6051ms step_avg:56.55ms
step:108/2330 train_time:6111ms step_avg:56.58ms
step:109/2330 train_time:6166ms step_avg:56.57ms
step:110/2330 train_time:6225ms step_avg:56.59ms
step:111/2330 train_time:6280ms step_avg:56.58ms
step:112/2330 train_time:6341ms step_avg:56.61ms
step:113/2330 train_time:6395ms step_avg:56.60ms
step:114/2330 train_time:6454ms step_avg:56.61ms
step:115/2330 train_time:6509ms step_avg:56.60ms
step:116/2330 train_time:6568ms step_avg:56.62ms
step:117/2330 train_time:6623ms step_avg:56.61ms
step:118/2330 train_time:6682ms step_avg:56.62ms
step:119/2330 train_time:6737ms step_avg:56.61ms
step:120/2330 train_time:6796ms step_avg:56.63ms
step:121/2330 train_time:6850ms step_avg:56.62ms
step:122/2330 train_time:6909ms step_avg:56.63ms
step:123/2330 train_time:6964ms step_avg:56.62ms
step:124/2330 train_time:7024ms step_avg:56.64ms
step:125/2330 train_time:7079ms step_avg:56.63ms
step:126/2330 train_time:7137ms step_avg:56.65ms
step:127/2330 train_time:7192ms step_avg:56.63ms
step:128/2330 train_time:7251ms step_avg:56.65ms
step:129/2330 train_time:7306ms step_avg:56.64ms
step:130/2330 train_time:7366ms step_avg:56.66ms
step:131/2330 train_time:7421ms step_avg:56.65ms
step:132/2330 train_time:7480ms step_avg:56.67ms
step:133/2330 train_time:7535ms step_avg:56.65ms
step:134/2330 train_time:7593ms step_avg:56.67ms
step:135/2330 train_time:7648ms step_avg:56.65ms
step:136/2330 train_time:7706ms step_avg:56.66ms
step:137/2330 train_time:7762ms step_avg:56.65ms
step:138/2330 train_time:7821ms step_avg:56.68ms
step:139/2330 train_time:7877ms step_avg:56.67ms
step:140/2330 train_time:7935ms step_avg:56.68ms
step:141/2330 train_time:7990ms step_avg:56.66ms
step:142/2330 train_time:8048ms step_avg:56.68ms
step:143/2330 train_time:8103ms step_avg:56.67ms
step:144/2330 train_time:8162ms step_avg:56.68ms
step:145/2330 train_time:8218ms step_avg:56.67ms
step:146/2330 train_time:8275ms step_avg:56.68ms
step:147/2330 train_time:8331ms step_avg:56.67ms
step:148/2330 train_time:8389ms step_avg:56.68ms
step:149/2330 train_time:8444ms step_avg:56.67ms
step:150/2330 train_time:8502ms step_avg:56.68ms
step:151/2330 train_time:8557ms step_avg:56.67ms
step:152/2330 train_time:8616ms step_avg:56.68ms
step:153/2330 train_time:8670ms step_avg:56.67ms
step:154/2330 train_time:8729ms step_avg:56.68ms
step:155/2330 train_time:8784ms step_avg:56.67ms
step:156/2330 train_time:8844ms step_avg:56.69ms
step:157/2330 train_time:8899ms step_avg:56.68ms
step:158/2330 train_time:8957ms step_avg:56.69ms
step:159/2330 train_time:9012ms step_avg:56.68ms
step:160/2330 train_time:9071ms step_avg:56.69ms
step:161/2330 train_time:9126ms step_avg:56.68ms
step:162/2330 train_time:9185ms step_avg:56.69ms
step:163/2330 train_time:9240ms step_avg:56.69ms
step:164/2330 train_time:9298ms step_avg:56.70ms
step:165/2330 train_time:9353ms step_avg:56.69ms
step:166/2330 train_time:9412ms step_avg:56.70ms
step:167/2330 train_time:9466ms step_avg:56.68ms
step:168/2330 train_time:9526ms step_avg:56.70ms
step:169/2330 train_time:9581ms step_avg:56.69ms
step:170/2330 train_time:9640ms step_avg:56.71ms
step:171/2330 train_time:9695ms step_avg:56.69ms
step:172/2330 train_time:9753ms step_avg:56.70ms
step:173/2330 train_time:9808ms step_avg:56.69ms
step:174/2330 train_time:9866ms step_avg:56.70ms
step:175/2330 train_time:9922ms step_avg:56.70ms
step:176/2330 train_time:9980ms step_avg:56.71ms
step:177/2330 train_time:10036ms step_avg:56.70ms
step:178/2330 train_time:10094ms step_avg:56.71ms
step:179/2330 train_time:10149ms step_avg:56.70ms
step:180/2330 train_time:10207ms step_avg:56.71ms
step:181/2330 train_time:10262ms step_avg:56.70ms
step:182/2330 train_time:10322ms step_avg:56.72ms
step:183/2330 train_time:10378ms step_avg:56.71ms
step:184/2330 train_time:10436ms step_avg:56.72ms
step:185/2330 train_time:10491ms step_avg:56.71ms
step:186/2330 train_time:10549ms step_avg:56.72ms
step:187/2330 train_time:10604ms step_avg:56.71ms
step:188/2330 train_time:10663ms step_avg:56.72ms
step:189/2330 train_time:10718ms step_avg:56.71ms
step:190/2330 train_time:10777ms step_avg:56.72ms
step:191/2330 train_time:10832ms step_avg:56.71ms
step:192/2330 train_time:10890ms step_avg:56.72ms
step:193/2330 train_time:10944ms step_avg:56.71ms
step:194/2330 train_time:11004ms step_avg:56.72ms
step:195/2330 train_time:11059ms step_avg:56.71ms
step:196/2330 train_time:11119ms step_avg:56.73ms
step:197/2330 train_time:11174ms step_avg:56.72ms
step:198/2330 train_time:11232ms step_avg:56.73ms
step:199/2330 train_time:11287ms step_avg:56.72ms
step:200/2330 train_time:11346ms step_avg:56.73ms
step:201/2330 train_time:11402ms step_avg:56.73ms
step:202/2330 train_time:11461ms step_avg:56.74ms
step:203/2330 train_time:11516ms step_avg:56.73ms
step:204/2330 train_time:11574ms step_avg:56.74ms
step:205/2330 train_time:11630ms step_avg:56.73ms
step:206/2330 train_time:11689ms step_avg:56.74ms
step:207/2330 train_time:11744ms step_avg:56.73ms
step:208/2330 train_time:11803ms step_avg:56.74ms
step:209/2330 train_time:11858ms step_avg:56.74ms
step:210/2330 train_time:11916ms step_avg:56.75ms
step:211/2330 train_time:11972ms step_avg:56.74ms
step:212/2330 train_time:12029ms step_avg:56.74ms
step:213/2330 train_time:12084ms step_avg:56.73ms
step:214/2330 train_time:12143ms step_avg:56.74ms
step:215/2330 train_time:12198ms step_avg:56.74ms
step:216/2330 train_time:12257ms step_avg:56.74ms
step:217/2330 train_time:12311ms step_avg:56.73ms
step:218/2330 train_time:12370ms step_avg:56.74ms
step:219/2330 train_time:12425ms step_avg:56.74ms
step:220/2330 train_time:12484ms step_avg:56.75ms
step:221/2330 train_time:12540ms step_avg:56.74ms
step:222/2330 train_time:12598ms step_avg:56.75ms
step:223/2330 train_time:12654ms step_avg:56.74ms
step:224/2330 train_time:12712ms step_avg:56.75ms
step:225/2330 train_time:12768ms step_avg:56.74ms
step:226/2330 train_time:12826ms step_avg:56.75ms
step:227/2330 train_time:12881ms step_avg:56.75ms
step:228/2330 train_time:12940ms step_avg:56.75ms
step:229/2330 train_time:12995ms step_avg:56.75ms
step:230/2330 train_time:13053ms step_avg:56.75ms
step:231/2330 train_time:13108ms step_avg:56.75ms
step:232/2330 train_time:13167ms step_avg:56.75ms
step:233/2330 train_time:13223ms step_avg:56.75ms
step:234/2330 train_time:13281ms step_avg:56.76ms
step:235/2330 train_time:13337ms step_avg:56.75ms
step:236/2330 train_time:13395ms step_avg:56.76ms
step:237/2330 train_time:13450ms step_avg:56.75ms
step:238/2330 train_time:13508ms step_avg:56.76ms
step:239/2330 train_time:13563ms step_avg:56.75ms
step:240/2330 train_time:13623ms step_avg:56.76ms
step:241/2330 train_time:13678ms step_avg:56.76ms
step:242/2330 train_time:13736ms step_avg:56.76ms
step:243/2330 train_time:13792ms step_avg:56.76ms
step:244/2330 train_time:13849ms step_avg:56.76ms
step:245/2330 train_time:13904ms step_avg:56.75ms
step:246/2330 train_time:13963ms step_avg:56.76ms
step:247/2330 train_time:14019ms step_avg:56.76ms
step:248/2330 train_time:14077ms step_avg:56.76ms
step:249/2330 train_time:14133ms step_avg:56.76ms
step:250/2330 train_time:14190ms step_avg:56.76ms
step:250/2330 val_loss:5.7193 train_time:14269ms step_avg:57.08ms
step:251/2330 train_time:14287ms step_avg:56.92ms
step:252/2330 train_time:14307ms step_avg:56.77ms
step:253/2330 train_time:14361ms step_avg:56.76ms
step:254/2330 train_time:14425ms step_avg:56.79ms
step:255/2330 train_time:14480ms step_avg:56.78ms
step:256/2330 train_time:14542ms step_avg:56.81ms
step:257/2330 train_time:14597ms step_avg:56.80ms
step:258/2330 train_time:14657ms step_avg:56.81ms
step:259/2330 train_time:14712ms step_avg:56.80ms
step:260/2330 train_time:14770ms step_avg:56.81ms
step:261/2330 train_time:14825ms step_avg:56.80ms
step:262/2330 train_time:14882ms step_avg:56.80ms
step:263/2330 train_time:14937ms step_avg:56.79ms
step:264/2330 train_time:14996ms step_avg:56.80ms
step:265/2330 train_time:15051ms step_avg:56.80ms
step:266/2330 train_time:15109ms step_avg:56.80ms
step:267/2330 train_time:15164ms step_avg:56.79ms
step:268/2330 train_time:15222ms step_avg:56.80ms
step:269/2330 train_time:15277ms step_avg:56.79ms
step:270/2330 train_time:15338ms step_avg:56.81ms
step:271/2330 train_time:15394ms step_avg:56.81ms
step:272/2330 train_time:15453ms step_avg:56.81ms
step:273/2330 train_time:15509ms step_avg:56.81ms
step:274/2330 train_time:15568ms step_avg:56.82ms
step:275/2330 train_time:15623ms step_avg:56.81ms
step:276/2330 train_time:15683ms step_avg:56.82ms
step:277/2330 train_time:15737ms step_avg:56.81ms
step:278/2330 train_time:15796ms step_avg:56.82ms
step:279/2330 train_time:15851ms step_avg:56.81ms
step:280/2330 train_time:15909ms step_avg:56.82ms
step:281/2330 train_time:15964ms step_avg:56.81ms
step:282/2330 train_time:16022ms step_avg:56.81ms
step:283/2330 train_time:16076ms step_avg:56.81ms
step:284/2330 train_time:16136ms step_avg:56.82ms
step:285/2330 train_time:16192ms step_avg:56.81ms
step:286/2330 train_time:16250ms step_avg:56.82ms
step:287/2330 train_time:16306ms step_avg:56.82ms
step:288/2330 train_time:16365ms step_avg:56.82ms
step:289/2330 train_time:16421ms step_avg:56.82ms
step:290/2330 train_time:16479ms step_avg:56.82ms
step:291/2330 train_time:16535ms step_avg:56.82ms
step:292/2330 train_time:16594ms step_avg:56.83ms
step:293/2330 train_time:16650ms step_avg:56.83ms
step:294/2330 train_time:16708ms step_avg:56.83ms
step:295/2330 train_time:16764ms step_avg:56.83ms
step:296/2330 train_time:16821ms step_avg:56.83ms
step:297/2330 train_time:16876ms step_avg:56.82ms
step:298/2330 train_time:16935ms step_avg:56.83ms
step:299/2330 train_time:16990ms step_avg:56.82ms
step:300/2330 train_time:17048ms step_avg:56.83ms
step:301/2330 train_time:17103ms step_avg:56.82ms
step:302/2330 train_time:17161ms step_avg:56.82ms
step:303/2330 train_time:17216ms step_avg:56.82ms
step:304/2330 train_time:17275ms step_avg:56.83ms
step:305/2330 train_time:17330ms step_avg:56.82ms
step:306/2330 train_time:17390ms step_avg:56.83ms
step:307/2330 train_time:17445ms step_avg:56.82ms
step:308/2330 train_time:17504ms step_avg:56.83ms
step:309/2330 train_time:17559ms step_avg:56.83ms
step:310/2330 train_time:17618ms step_avg:56.83ms
step:311/2330 train_time:17674ms step_avg:56.83ms
step:312/2330 train_time:17732ms step_avg:56.83ms
step:313/2330 train_time:17788ms step_avg:56.83ms
step:314/2330 train_time:17847ms step_avg:56.84ms
step:315/2330 train_time:17902ms step_avg:56.83ms
step:316/2330 train_time:17960ms step_avg:56.83ms
step:317/2330 train_time:18015ms step_avg:56.83ms
step:318/2330 train_time:18074ms step_avg:56.84ms
step:319/2330 train_time:18129ms step_avg:56.83ms
step:320/2330 train_time:18188ms step_avg:56.84ms
step:321/2330 train_time:18243ms step_avg:56.83ms
step:322/2330 train_time:18302ms step_avg:56.84ms
step:323/2330 train_time:18357ms step_avg:56.83ms
step:324/2330 train_time:18415ms step_avg:56.84ms
step:325/2330 train_time:18471ms step_avg:56.83ms
step:326/2330 train_time:18530ms step_avg:56.84ms
step:327/2330 train_time:18585ms step_avg:56.83ms
step:328/2330 train_time:18643ms step_avg:56.84ms
step:329/2330 train_time:18699ms step_avg:56.84ms
step:330/2330 train_time:18758ms step_avg:56.84ms
step:331/2330 train_time:18814ms step_avg:56.84ms
step:332/2330 train_time:18873ms step_avg:56.85ms
step:333/2330 train_time:18928ms step_avg:56.84ms
step:334/2330 train_time:18986ms step_avg:56.84ms
step:335/2330 train_time:19041ms step_avg:56.84ms
step:336/2330 train_time:19100ms step_avg:56.84ms
step:337/2330 train_time:19155ms step_avg:56.84ms
step:338/2330 train_time:19214ms step_avg:56.85ms
step:339/2330 train_time:19270ms step_avg:56.84ms
step:340/2330 train_time:19329ms step_avg:56.85ms
step:341/2330 train_time:19384ms step_avg:56.84ms
step:342/2330 train_time:19442ms step_avg:56.85ms
step:343/2330 train_time:19497ms step_avg:56.84ms
step:344/2330 train_time:19557ms step_avg:56.85ms
step:345/2330 train_time:19613ms step_avg:56.85ms
step:346/2330 train_time:19672ms step_avg:56.85ms
step:347/2330 train_time:19727ms step_avg:56.85ms
step:348/2330 train_time:19786ms step_avg:56.86ms
step:349/2330 train_time:19841ms step_avg:56.85ms
step:350/2330 train_time:19899ms step_avg:56.85ms
step:351/2330 train_time:19955ms step_avg:56.85ms
step:352/2330 train_time:20013ms step_avg:56.86ms
step:353/2330 train_time:20069ms step_avg:56.85ms
step:354/2330 train_time:20127ms step_avg:56.86ms
step:355/2330 train_time:20182ms step_avg:56.85ms
step:356/2330 train_time:20240ms step_avg:56.85ms
step:357/2330 train_time:20295ms step_avg:56.85ms
step:358/2330 train_time:20354ms step_avg:56.86ms
step:359/2330 train_time:20410ms step_avg:56.85ms
step:360/2330 train_time:20469ms step_avg:56.86ms
step:361/2330 train_time:20524ms step_avg:56.85ms
step:362/2330 train_time:20582ms step_avg:56.86ms
step:363/2330 train_time:20638ms step_avg:56.85ms
step:364/2330 train_time:20698ms step_avg:56.86ms
step:365/2330 train_time:20753ms step_avg:56.86ms
step:366/2330 train_time:20811ms step_avg:56.86ms
step:367/2330 train_time:20866ms step_avg:56.86ms
step:368/2330 train_time:20925ms step_avg:56.86ms
step:369/2330 train_time:20980ms step_avg:56.86ms
step:370/2330 train_time:21039ms step_avg:56.86ms
step:371/2330 train_time:21095ms step_avg:56.86ms
step:372/2330 train_time:21153ms step_avg:56.86ms
step:373/2330 train_time:21209ms step_avg:56.86ms
step:374/2330 train_time:21267ms step_avg:56.86ms
step:375/2330 train_time:21322ms step_avg:56.86ms
step:376/2330 train_time:21380ms step_avg:56.86ms
step:377/2330 train_time:21435ms step_avg:56.86ms
step:378/2330 train_time:21495ms step_avg:56.87ms
step:379/2330 train_time:21551ms step_avg:56.86ms
step:380/2330 train_time:21609ms step_avg:56.87ms
step:381/2330 train_time:21664ms step_avg:56.86ms
step:382/2330 train_time:21723ms step_avg:56.87ms
step:383/2330 train_time:21778ms step_avg:56.86ms
step:384/2330 train_time:21838ms step_avg:56.87ms
step:385/2330 train_time:21894ms step_avg:56.87ms
step:386/2330 train_time:21952ms step_avg:56.87ms
step:387/2330 train_time:22007ms step_avg:56.87ms
step:388/2330 train_time:22065ms step_avg:56.87ms
step:389/2330 train_time:22120ms step_avg:56.86ms
step:390/2330 train_time:22179ms step_avg:56.87ms
step:391/2330 train_time:22234ms step_avg:56.86ms
step:392/2330 train_time:22293ms step_avg:56.87ms
step:393/2330 train_time:22348ms step_avg:56.87ms
step:394/2330 train_time:22407ms step_avg:56.87ms
step:395/2330 train_time:22462ms step_avg:56.87ms
step:396/2330 train_time:22520ms step_avg:56.87ms
step:397/2330 train_time:22575ms step_avg:56.86ms
step:398/2330 train_time:22634ms step_avg:56.87ms
step:399/2330 train_time:22689ms step_avg:56.87ms
step:400/2330 train_time:22748ms step_avg:56.87ms
step:401/2330 train_time:22803ms step_avg:56.87ms
step:402/2330 train_time:22863ms step_avg:56.87ms
step:403/2330 train_time:22918ms step_avg:56.87ms
step:404/2330 train_time:22978ms step_avg:56.88ms
step:405/2330 train_time:23033ms step_avg:56.87ms
step:406/2330 train_time:23093ms step_avg:56.88ms
step:407/2330 train_time:23148ms step_avg:56.87ms
step:408/2330 train_time:23206ms step_avg:56.88ms
step:409/2330 train_time:23261ms step_avg:56.87ms
step:410/2330 train_time:23321ms step_avg:56.88ms
step:411/2330 train_time:23376ms step_avg:56.87ms
step:412/2330 train_time:23435ms step_avg:56.88ms
step:413/2330 train_time:23491ms step_avg:56.88ms
step:414/2330 train_time:23549ms step_avg:56.88ms
step:415/2330 train_time:23604ms step_avg:56.88ms
step:416/2330 train_time:23663ms step_avg:56.88ms
step:417/2330 train_time:23719ms step_avg:56.88ms
step:418/2330 train_time:23777ms step_avg:56.88ms
step:419/2330 train_time:23833ms step_avg:56.88ms
step:420/2330 train_time:23892ms step_avg:56.89ms
step:421/2330 train_time:23947ms step_avg:56.88ms
step:422/2330 train_time:24006ms step_avg:56.89ms
step:423/2330 train_time:24061ms step_avg:56.88ms
step:424/2330 train_time:24119ms step_avg:56.88ms
step:425/2330 train_time:24174ms step_avg:56.88ms
step:426/2330 train_time:24234ms step_avg:56.89ms
step:427/2330 train_time:24289ms step_avg:56.88ms
step:428/2330 train_time:24348ms step_avg:56.89ms
step:429/2330 train_time:24403ms step_avg:56.88ms
step:430/2330 train_time:24461ms step_avg:56.89ms
step:431/2330 train_time:24516ms step_avg:56.88ms
step:432/2330 train_time:24575ms step_avg:56.89ms
step:433/2330 train_time:24630ms step_avg:56.88ms
step:434/2330 train_time:24689ms step_avg:56.89ms
step:435/2330 train_time:24745ms step_avg:56.88ms
step:436/2330 train_time:24802ms step_avg:56.89ms
step:437/2330 train_time:24858ms step_avg:56.88ms
step:438/2330 train_time:24917ms step_avg:56.89ms
step:439/2330 train_time:24973ms step_avg:56.89ms
step:440/2330 train_time:25032ms step_avg:56.89ms
step:441/2330 train_time:25087ms step_avg:56.89ms
step:442/2330 train_time:25145ms step_avg:56.89ms
step:443/2330 train_time:25201ms step_avg:56.89ms
step:444/2330 train_time:25260ms step_avg:56.89ms
step:445/2330 train_time:25315ms step_avg:56.89ms
step:446/2330 train_time:25373ms step_avg:56.89ms
step:447/2330 train_time:25429ms step_avg:56.89ms
step:448/2330 train_time:25487ms step_avg:56.89ms
step:449/2330 train_time:25543ms step_avg:56.89ms
step:450/2330 train_time:25601ms step_avg:56.89ms
step:451/2330 train_time:25656ms step_avg:56.89ms
step:452/2330 train_time:25715ms step_avg:56.89ms
step:453/2330 train_time:25771ms step_avg:56.89ms
step:454/2330 train_time:25829ms step_avg:56.89ms
step:455/2330 train_time:25885ms step_avg:56.89ms
step:456/2330 train_time:25944ms step_avg:56.89ms
step:457/2330 train_time:25998ms step_avg:56.89ms
step:458/2330 train_time:26058ms step_avg:56.90ms
step:459/2330 train_time:26114ms step_avg:56.89ms
step:460/2330 train_time:26173ms step_avg:56.90ms
step:461/2330 train_time:26228ms step_avg:56.89ms
step:462/2330 train_time:26287ms step_avg:56.90ms
step:463/2330 train_time:26343ms step_avg:56.90ms
step:464/2330 train_time:26401ms step_avg:56.90ms
step:465/2330 train_time:26456ms step_avg:56.90ms
step:466/2330 train_time:26516ms step_avg:56.90ms
step:467/2330 train_time:26571ms step_avg:56.90ms
step:468/2330 train_time:26630ms step_avg:56.90ms
step:469/2330 train_time:26685ms step_avg:56.90ms
step:470/2330 train_time:26744ms step_avg:56.90ms
step:471/2330 train_time:26799ms step_avg:56.90ms
step:472/2330 train_time:26859ms step_avg:56.90ms
step:473/2330 train_time:26914ms step_avg:56.90ms
step:474/2330 train_time:26973ms step_avg:56.90ms
step:475/2330 train_time:27028ms step_avg:56.90ms
step:476/2330 train_time:27087ms step_avg:56.90ms
step:477/2330 train_time:27142ms step_avg:56.90ms
step:478/2330 train_time:27200ms step_avg:56.90ms
step:479/2330 train_time:27256ms step_avg:56.90ms
step:480/2330 train_time:27314ms step_avg:56.90ms
step:481/2330 train_time:27370ms step_avg:56.90ms
step:482/2330 train_time:27428ms step_avg:56.91ms
step:483/2330 train_time:27484ms step_avg:56.90ms
step:484/2330 train_time:27542ms step_avg:56.91ms
step:485/2330 train_time:27597ms step_avg:56.90ms
step:486/2330 train_time:27656ms step_avg:56.91ms
step:487/2330 train_time:27712ms step_avg:56.90ms
step:488/2330 train_time:27771ms step_avg:56.91ms
step:489/2330 train_time:27825ms step_avg:56.90ms
step:490/2330 train_time:27884ms step_avg:56.91ms
step:491/2330 train_time:27939ms step_avg:56.90ms
step:492/2330 train_time:28000ms step_avg:56.91ms
step:493/2330 train_time:28055ms step_avg:56.91ms
step:494/2330 train_time:28115ms step_avg:56.91ms
step:495/2330 train_time:28171ms step_avg:56.91ms
step:496/2330 train_time:28228ms step_avg:56.91ms
step:497/2330 train_time:28284ms step_avg:56.91ms
step:498/2330 train_time:28343ms step_avg:56.91ms
step:499/2330 train_time:28397ms step_avg:56.91ms
step:500/2330 train_time:28457ms step_avg:56.91ms
step:500/2330 val_loss:5.0098 train_time:28536ms step_avg:57.07ms
step:501/2330 train_time:28555ms step_avg:57.00ms
step:502/2330 train_time:28577ms step_avg:56.93ms
step:503/2330 train_time:28632ms step_avg:56.92ms
step:504/2330 train_time:28694ms step_avg:56.93ms
step:505/2330 train_time:28749ms step_avg:56.93ms
step:506/2330 train_time:28812ms step_avg:56.94ms
step:507/2330 train_time:28866ms step_avg:56.94ms
step:508/2330 train_time:28926ms step_avg:56.94ms
step:509/2330 train_time:28981ms step_avg:56.94ms
step:510/2330 train_time:29039ms step_avg:56.94ms
step:511/2330 train_time:29094ms step_avg:56.94ms
step:512/2330 train_time:29152ms step_avg:56.94ms
step:513/2330 train_time:29207ms step_avg:56.93ms
step:514/2330 train_time:29265ms step_avg:56.94ms
step:515/2330 train_time:29320ms step_avg:56.93ms
step:516/2330 train_time:29378ms step_avg:56.93ms
step:517/2330 train_time:29433ms step_avg:56.93ms
step:518/2330 train_time:29491ms step_avg:56.93ms
step:519/2330 train_time:29546ms step_avg:56.93ms
step:520/2330 train_time:29606ms step_avg:56.94ms
step:521/2330 train_time:29662ms step_avg:56.93ms
step:522/2330 train_time:29723ms step_avg:56.94ms
step:523/2330 train_time:29779ms step_avg:56.94ms
step:524/2330 train_time:29839ms step_avg:56.94ms
step:525/2330 train_time:29894ms step_avg:56.94ms
step:526/2330 train_time:29953ms step_avg:56.94ms
step:527/2330 train_time:30008ms step_avg:56.94ms
step:528/2330 train_time:30066ms step_avg:56.94ms
step:529/2330 train_time:30122ms step_avg:56.94ms
step:530/2330 train_time:30180ms step_avg:56.94ms
step:531/2330 train_time:30235ms step_avg:56.94ms
step:532/2330 train_time:30293ms step_avg:56.94ms
step:533/2330 train_time:30348ms step_avg:56.94ms
step:534/2330 train_time:30407ms step_avg:56.94ms
step:535/2330 train_time:30463ms step_avg:56.94ms
step:536/2330 train_time:30520ms step_avg:56.94ms
step:537/2330 train_time:30575ms step_avg:56.94ms
step:538/2330 train_time:30635ms step_avg:56.94ms
step:539/2330 train_time:30691ms step_avg:56.94ms
step:540/2330 train_time:30751ms step_avg:56.95ms
step:541/2330 train_time:30806ms step_avg:56.94ms
step:542/2330 train_time:30865ms step_avg:56.95ms
step:543/2330 train_time:30921ms step_avg:56.94ms
step:544/2330 train_time:30979ms step_avg:56.95ms
step:545/2330 train_time:31035ms step_avg:56.94ms
step:546/2330 train_time:31093ms step_avg:56.95ms
step:547/2330 train_time:31147ms step_avg:56.94ms
step:548/2330 train_time:31207ms step_avg:56.95ms
step:549/2330 train_time:31262ms step_avg:56.94ms
step:550/2330 train_time:31320ms step_avg:56.95ms
step:551/2330 train_time:31375ms step_avg:56.94ms
step:552/2330 train_time:31434ms step_avg:56.95ms
step:553/2330 train_time:31489ms step_avg:56.94ms
step:554/2330 train_time:31548ms step_avg:56.95ms
step:555/2330 train_time:31604ms step_avg:56.94ms
step:556/2330 train_time:31663ms step_avg:56.95ms
step:557/2330 train_time:31718ms step_avg:56.94ms
step:558/2330 train_time:31777ms step_avg:56.95ms
step:559/2330 train_time:31832ms step_avg:56.94ms
step:560/2330 train_time:31891ms step_avg:56.95ms
step:561/2330 train_time:31946ms step_avg:56.95ms
step:562/2330 train_time:32006ms step_avg:56.95ms
step:563/2330 train_time:32061ms step_avg:56.95ms
step:564/2330 train_time:32120ms step_avg:56.95ms
step:565/2330 train_time:32175ms step_avg:56.95ms
step:566/2330 train_time:32235ms step_avg:56.95ms
step:567/2330 train_time:32290ms step_avg:56.95ms
step:568/2330 train_time:32348ms step_avg:56.95ms
step:569/2330 train_time:32403ms step_avg:56.95ms
step:570/2330 train_time:32461ms step_avg:56.95ms
step:571/2330 train_time:32517ms step_avg:56.95ms
step:572/2330 train_time:32575ms step_avg:56.95ms
step:573/2330 train_time:32631ms step_avg:56.95ms
step:574/2330 train_time:32689ms step_avg:56.95ms
step:575/2330 train_time:32745ms step_avg:56.95ms
step:576/2330 train_time:32804ms step_avg:56.95ms
step:577/2330 train_time:32860ms step_avg:56.95ms
step:578/2330 train_time:32919ms step_avg:56.95ms
step:579/2330 train_time:32974ms step_avg:56.95ms
step:580/2330 train_time:33032ms step_avg:56.95ms
step:581/2330 train_time:33086ms step_avg:56.95ms
step:582/2330 train_time:33147ms step_avg:56.95ms
step:583/2330 train_time:33202ms step_avg:56.95ms
step:584/2330 train_time:33261ms step_avg:56.95ms
step:585/2330 train_time:33316ms step_avg:56.95ms
step:586/2330 train_time:33375ms step_avg:56.95ms
step:587/2330 train_time:33429ms step_avg:56.95ms
step:588/2330 train_time:33489ms step_avg:56.95ms
step:589/2330 train_time:33544ms step_avg:56.95ms
step:590/2330 train_time:33603ms step_avg:56.95ms
step:591/2330 train_time:33658ms step_avg:56.95ms
step:592/2330 train_time:33716ms step_avg:56.95ms
step:593/2330 train_time:33771ms step_avg:56.95ms
step:594/2330 train_time:33830ms step_avg:56.95ms
step:595/2330 train_time:33885ms step_avg:56.95ms
step:596/2330 train_time:33945ms step_avg:56.96ms
step:597/2330 train_time:34001ms step_avg:56.95ms
step:598/2330 train_time:34060ms step_avg:56.96ms
step:599/2330 train_time:34115ms step_avg:56.95ms
step:600/2330 train_time:34173ms step_avg:56.96ms
step:601/2330 train_time:34228ms step_avg:56.95ms
step:602/2330 train_time:34287ms step_avg:56.96ms
step:603/2330 train_time:34342ms step_avg:56.95ms
step:604/2330 train_time:34402ms step_avg:56.96ms
step:605/2330 train_time:34457ms step_avg:56.95ms
step:606/2330 train_time:34515ms step_avg:56.96ms
step:607/2330 train_time:34570ms step_avg:56.95ms
step:608/2330 train_time:34629ms step_avg:56.96ms
step:609/2330 train_time:34684ms step_avg:56.95ms
step:610/2330 train_time:34743ms step_avg:56.96ms
step:611/2330 train_time:34799ms step_avg:56.95ms
step:612/2330 train_time:34857ms step_avg:56.96ms
step:613/2330 train_time:34912ms step_avg:56.95ms
step:614/2330 train_time:34970ms step_avg:56.95ms
step:615/2330 train_time:35025ms step_avg:56.95ms
step:616/2330 train_time:35085ms step_avg:56.96ms
step:617/2330 train_time:35140ms step_avg:56.95ms
step:618/2330 train_time:35199ms step_avg:56.96ms
step:619/2330 train_time:35255ms step_avg:56.95ms
step:620/2330 train_time:35313ms step_avg:56.96ms
step:621/2330 train_time:35367ms step_avg:56.95ms
step:622/2330 train_time:35427ms step_avg:56.96ms
step:623/2330 train_time:35483ms step_avg:56.95ms
step:624/2330 train_time:35541ms step_avg:56.96ms
step:625/2330 train_time:35596ms step_avg:56.95ms
step:626/2330 train_time:35655ms step_avg:56.96ms
step:627/2330 train_time:35710ms step_avg:56.95ms
step:628/2330 train_time:35769ms step_avg:56.96ms
step:629/2330 train_time:35825ms step_avg:56.95ms
step:630/2330 train_time:35883ms step_avg:56.96ms
step:631/2330 train_time:35939ms step_avg:56.95ms
step:632/2330 train_time:35998ms step_avg:56.96ms
step:633/2330 train_time:36053ms step_avg:56.96ms
step:634/2330 train_time:36111ms step_avg:56.96ms
step:635/2330 train_time:36166ms step_avg:56.95ms
step:636/2330 train_time:36225ms step_avg:56.96ms
step:637/2330 train_time:36280ms step_avg:56.95ms
step:638/2330 train_time:36339ms step_avg:56.96ms
step:639/2330 train_time:36393ms step_avg:56.95ms
step:640/2330 train_time:36453ms step_avg:56.96ms
step:641/2330 train_time:36507ms step_avg:56.95ms
step:642/2330 train_time:36567ms step_avg:56.96ms
step:643/2330 train_time:36622ms step_avg:56.95ms
step:644/2330 train_time:36681ms step_avg:56.96ms
step:645/2330 train_time:36736ms step_avg:56.95ms
step:646/2330 train_time:36795ms step_avg:56.96ms
step:647/2330 train_time:36851ms step_avg:56.96ms
step:648/2330 train_time:36909ms step_avg:56.96ms
step:649/2330 train_time:36965ms step_avg:56.96ms
step:650/2330 train_time:37023ms step_avg:56.96ms
step:651/2330 train_time:37079ms step_avg:56.96ms
step:652/2330 train_time:37138ms step_avg:56.96ms
step:653/2330 train_time:37193ms step_avg:56.96ms
step:654/2330 train_time:37251ms step_avg:56.96ms
step:655/2330 train_time:37306ms step_avg:56.96ms
step:656/2330 train_time:37365ms step_avg:56.96ms
step:657/2330 train_time:37420ms step_avg:56.96ms
step:658/2330 train_time:37479ms step_avg:56.96ms
step:659/2330 train_time:37534ms step_avg:56.96ms
step:660/2330 train_time:37592ms step_avg:56.96ms
step:661/2330 train_time:37647ms step_avg:56.95ms
step:662/2330 train_time:37707ms step_avg:56.96ms
step:663/2330 train_time:37762ms step_avg:56.96ms
step:664/2330 train_time:37821ms step_avg:56.96ms
step:665/2330 train_time:37877ms step_avg:56.96ms
step:666/2330 train_time:37934ms step_avg:56.96ms
step:667/2330 train_time:37990ms step_avg:56.96ms
step:668/2330 train_time:38049ms step_avg:56.96ms
step:669/2330 train_time:38105ms step_avg:56.96ms
step:670/2330 train_time:38163ms step_avg:56.96ms
step:671/2330 train_time:38218ms step_avg:56.96ms
step:672/2330 train_time:38277ms step_avg:56.96ms
step:673/2330 train_time:38331ms step_avg:56.96ms
step:674/2330 train_time:38391ms step_avg:56.96ms
step:675/2330 train_time:38446ms step_avg:56.96ms
step:676/2330 train_time:38505ms step_avg:56.96ms
step:677/2330 train_time:38561ms step_avg:56.96ms
step:678/2330 train_time:38619ms step_avg:56.96ms
step:679/2330 train_time:38674ms step_avg:56.96ms
step:680/2330 train_time:38733ms step_avg:56.96ms
step:681/2330 train_time:38788ms step_avg:56.96ms
step:682/2330 train_time:38847ms step_avg:56.96ms
step:683/2330 train_time:38902ms step_avg:56.96ms
step:684/2330 train_time:38961ms step_avg:56.96ms
step:685/2330 train_time:39016ms step_avg:56.96ms
step:686/2330 train_time:39074ms step_avg:56.96ms
step:687/2330 train_time:39130ms step_avg:56.96ms
step:688/2330 train_time:39189ms step_avg:56.96ms
step:689/2330 train_time:39244ms step_avg:56.96ms
step:690/2330 train_time:39304ms step_avg:56.96ms
step:691/2330 train_time:39359ms step_avg:56.96ms
step:692/2330 train_time:39418ms step_avg:56.96ms
step:693/2330 train_time:39473ms step_avg:56.96ms
step:694/2330 train_time:39533ms step_avg:56.96ms
step:695/2330 train_time:39588ms step_avg:56.96ms
step:696/2330 train_time:39647ms step_avg:56.96ms
step:697/2330 train_time:39702ms step_avg:56.96ms
step:698/2330 train_time:39762ms step_avg:56.96ms
step:699/2330 train_time:39816ms step_avg:56.96ms
step:700/2330 train_time:39875ms step_avg:56.96ms
step:701/2330 train_time:39930ms step_avg:56.96ms
step:702/2330 train_time:39989ms step_avg:56.96ms
step:703/2330 train_time:40044ms step_avg:56.96ms
step:704/2330 train_time:40103ms step_avg:56.96ms
step:705/2330 train_time:40158ms step_avg:56.96ms
step:706/2330 train_time:40216ms step_avg:56.96ms
step:707/2330 train_time:40272ms step_avg:56.96ms
step:708/2330 train_time:40330ms step_avg:56.96ms
step:709/2330 train_time:40386ms step_avg:56.96ms
step:710/2330 train_time:40445ms step_avg:56.96ms
step:711/2330 train_time:40501ms step_avg:56.96ms
step:712/2330 train_time:40560ms step_avg:56.97ms
step:713/2330 train_time:40615ms step_avg:56.96ms
step:714/2330 train_time:40673ms step_avg:56.97ms
step:715/2330 train_time:40728ms step_avg:56.96ms
step:716/2330 train_time:40788ms step_avg:56.97ms
step:717/2330 train_time:40844ms step_avg:56.96ms
step:718/2330 train_time:40902ms step_avg:56.97ms
step:719/2330 train_time:40958ms step_avg:56.96ms
step:720/2330 train_time:41016ms step_avg:56.97ms
step:721/2330 train_time:41071ms step_avg:56.96ms
step:722/2330 train_time:41130ms step_avg:56.97ms
step:723/2330 train_time:41185ms step_avg:56.96ms
step:724/2330 train_time:41245ms step_avg:56.97ms
step:725/2330 train_time:41300ms step_avg:56.97ms
step:726/2330 train_time:41359ms step_avg:56.97ms
step:727/2330 train_time:41414ms step_avg:56.97ms
step:728/2330 train_time:41473ms step_avg:56.97ms
step:729/2330 train_time:41529ms step_avg:56.97ms
step:730/2330 train_time:41589ms step_avg:56.97ms
step:731/2330 train_time:41645ms step_avg:56.97ms
step:732/2330 train_time:41703ms step_avg:56.97ms
step:733/2330 train_time:41759ms step_avg:56.97ms
step:734/2330 train_time:41817ms step_avg:56.97ms
step:735/2330 train_time:41872ms step_avg:56.97ms
step:736/2330 train_time:41932ms step_avg:56.97ms
step:737/2330 train_time:41986ms step_avg:56.97ms
step:738/2330 train_time:42047ms step_avg:56.97ms
step:739/2330 train_time:42102ms step_avg:56.97ms
step:740/2330 train_time:42160ms step_avg:56.97ms
step:741/2330 train_time:42215ms step_avg:56.97ms
step:742/2330 train_time:42274ms step_avg:56.97ms
step:743/2330 train_time:42328ms step_avg:56.97ms
step:744/2330 train_time:42389ms step_avg:56.97ms
step:745/2330 train_time:42444ms step_avg:56.97ms
step:746/2330 train_time:42504ms step_avg:56.98ms
step:747/2330 train_time:42559ms step_avg:56.97ms
step:748/2330 train_time:42618ms step_avg:56.98ms
step:749/2330 train_time:42674ms step_avg:56.97ms
step:750/2330 train_time:42732ms step_avg:56.98ms
step:750/2330 val_loss:4.6331 train_time:42810ms step_avg:57.08ms
step:751/2330 train_time:42829ms step_avg:57.03ms
step:752/2330 train_time:42848ms step_avg:56.98ms
step:753/2330 train_time:42904ms step_avg:56.98ms
step:754/2330 train_time:42968ms step_avg:56.99ms
step:755/2330 train_time:43025ms step_avg:56.99ms
step:756/2330 train_time:43086ms step_avg:56.99ms
step:757/2330 train_time:43142ms step_avg:56.99ms
step:758/2330 train_time:43200ms step_avg:56.99ms
step:759/2330 train_time:43255ms step_avg:56.99ms
step:760/2330 train_time:43313ms step_avg:56.99ms
step:761/2330 train_time:43368ms step_avg:56.99ms
step:762/2330 train_time:43427ms step_avg:56.99ms
step:763/2330 train_time:43482ms step_avg:56.99ms
step:764/2330 train_time:43541ms step_avg:56.99ms
step:765/2330 train_time:43598ms step_avg:56.99ms
step:766/2330 train_time:43655ms step_avg:56.99ms
step:767/2330 train_time:43711ms step_avg:56.99ms
step:768/2330 train_time:43770ms step_avg:56.99ms
step:769/2330 train_time:43827ms step_avg:56.99ms
step:770/2330 train_time:43888ms step_avg:57.00ms
step:771/2330 train_time:43946ms step_avg:57.00ms
step:772/2330 train_time:44006ms step_avg:57.00ms
step:773/2330 train_time:44063ms step_avg:57.00ms
step:774/2330 train_time:44123ms step_avg:57.01ms
step:775/2330 train_time:44180ms step_avg:57.01ms
step:776/2330 train_time:44239ms step_avg:57.01ms
step:777/2330 train_time:44295ms step_avg:57.01ms
step:778/2330 train_time:44353ms step_avg:57.01ms
step:779/2330 train_time:44409ms step_avg:57.01ms
step:780/2330 train_time:44469ms step_avg:57.01ms
step:781/2330 train_time:44524ms step_avg:57.01ms
step:782/2330 train_time:44584ms step_avg:57.01ms
step:783/2330 train_time:44641ms step_avg:57.01ms
step:784/2330 train_time:44700ms step_avg:57.01ms
step:785/2330 train_time:44755ms step_avg:57.01ms
step:786/2330 train_time:44815ms step_avg:57.02ms
step:787/2330 train_time:44871ms step_avg:57.02ms
step:788/2330 train_time:44930ms step_avg:57.02ms
step:789/2330 train_time:44986ms step_avg:57.02ms
step:790/2330 train_time:45047ms step_avg:57.02ms
step:791/2330 train_time:45104ms step_avg:57.02ms
step:792/2330 train_time:45164ms step_avg:57.02ms
step:793/2330 train_time:45220ms step_avg:57.02ms
step:794/2330 train_time:45281ms step_avg:57.03ms
step:795/2330 train_time:45337ms step_avg:57.03ms
step:796/2330 train_time:45396ms step_avg:57.03ms
step:797/2330 train_time:45451ms step_avg:57.03ms
step:798/2330 train_time:45510ms step_avg:57.03ms
step:799/2330 train_time:45566ms step_avg:57.03ms
step:800/2330 train_time:45626ms step_avg:57.03ms
step:801/2330 train_time:45682ms step_avg:57.03ms
step:802/2330 train_time:45741ms step_avg:57.03ms
step:803/2330 train_time:45798ms step_avg:57.03ms
step:804/2330 train_time:45857ms step_avg:57.04ms
step:805/2330 train_time:45914ms step_avg:57.04ms
step:806/2330 train_time:45973ms step_avg:57.04ms
step:807/2330 train_time:46029ms step_avg:57.04ms
step:808/2330 train_time:46090ms step_avg:57.04ms
step:809/2330 train_time:46146ms step_avg:57.04ms
step:810/2330 train_time:46206ms step_avg:57.04ms
step:811/2330 train_time:46263ms step_avg:57.04ms
step:812/2330 train_time:46323ms step_avg:57.05ms
step:813/2330 train_time:46379ms step_avg:57.05ms
step:814/2330 train_time:46439ms step_avg:57.05ms
step:815/2330 train_time:46496ms step_avg:57.05ms
step:816/2330 train_time:46554ms step_avg:57.05ms
step:817/2330 train_time:46610ms step_avg:57.05ms
step:818/2330 train_time:46670ms step_avg:57.05ms
step:819/2330 train_time:46725ms step_avg:57.05ms
step:820/2330 train_time:46786ms step_avg:57.06ms
step:821/2330 train_time:46842ms step_avg:57.05ms
step:822/2330 train_time:46901ms step_avg:57.06ms
step:823/2330 train_time:46957ms step_avg:57.06ms
step:824/2330 train_time:47017ms step_avg:57.06ms
step:825/2330 train_time:47074ms step_avg:57.06ms
step:826/2330 train_time:47133ms step_avg:57.06ms
step:827/2330 train_time:47189ms step_avg:57.06ms
step:828/2330 train_time:47249ms step_avg:57.06ms
step:829/2330 train_time:47306ms step_avg:57.06ms
step:830/2330 train_time:47367ms step_avg:57.07ms
step:831/2330 train_time:47423ms step_avg:57.07ms
step:832/2330 train_time:47482ms step_avg:57.07ms
step:833/2330 train_time:47539ms step_avg:57.07ms
step:834/2330 train_time:47599ms step_avg:57.07ms
step:835/2330 train_time:47655ms step_avg:57.07ms
step:836/2330 train_time:47713ms step_avg:57.07ms
step:837/2330 train_time:47769ms step_avg:57.07ms
step:838/2330 train_time:47829ms step_avg:57.08ms
step:839/2330 train_time:47885ms step_avg:57.07ms
step:840/2330 train_time:47945ms step_avg:57.08ms
step:841/2330 train_time:48002ms step_avg:57.08ms
step:842/2330 train_time:48061ms step_avg:57.08ms
step:843/2330 train_time:48117ms step_avg:57.08ms
step:844/2330 train_time:48177ms step_avg:57.08ms
step:845/2330 train_time:48234ms step_avg:57.08ms
step:846/2330 train_time:48292ms step_avg:57.08ms
step:847/2330 train_time:48349ms step_avg:57.08ms
step:848/2330 train_time:48408ms step_avg:57.09ms
step:849/2330 train_time:48465ms step_avg:57.08ms
step:850/2330 train_time:48525ms step_avg:57.09ms
step:851/2330 train_time:48582ms step_avg:57.09ms
step:852/2330 train_time:48641ms step_avg:57.09ms
step:853/2330 train_time:48698ms step_avg:57.09ms
step:854/2330 train_time:48757ms step_avg:57.09ms
step:855/2330 train_time:48813ms step_avg:57.09ms
step:856/2330 train_time:48873ms step_avg:57.09ms
step:857/2330 train_time:48928ms step_avg:57.09ms
step:858/2330 train_time:48988ms step_avg:57.10ms
step:859/2330 train_time:49044ms step_avg:57.09ms
step:860/2330 train_time:49104ms step_avg:57.10ms
step:861/2330 train_time:49161ms step_avg:57.10ms
step:862/2330 train_time:49220ms step_avg:57.10ms
step:863/2330 train_time:49277ms step_avg:57.10ms
step:864/2330 train_time:49336ms step_avg:57.10ms
step:865/2330 train_time:49392ms step_avg:57.10ms
step:866/2330 train_time:49451ms step_avg:57.10ms
step:867/2330 train_time:49507ms step_avg:57.10ms
step:868/2330 train_time:49567ms step_avg:57.11ms
step:869/2330 train_time:49623ms step_avg:57.10ms
step:870/2330 train_time:49684ms step_avg:57.11ms
step:871/2330 train_time:49740ms step_avg:57.11ms
step:872/2330 train_time:49800ms step_avg:57.11ms
step:873/2330 train_time:49856ms step_avg:57.11ms
step:874/2330 train_time:49915ms step_avg:57.11ms
step:875/2330 train_time:49971ms step_avg:57.11ms
step:876/2330 train_time:50030ms step_avg:57.11ms
step:877/2330 train_time:50086ms step_avg:57.11ms
step:878/2330 train_time:50146ms step_avg:57.11ms
step:879/2330 train_time:50203ms step_avg:57.11ms
step:880/2330 train_time:50262ms step_avg:57.12ms
step:881/2330 train_time:50318ms step_avg:57.12ms
step:882/2330 train_time:50378ms step_avg:57.12ms
step:883/2330 train_time:50434ms step_avg:57.12ms
step:884/2330 train_time:50493ms step_avg:57.12ms
step:885/2330 train_time:50550ms step_avg:57.12ms
step:886/2330 train_time:50609ms step_avg:57.12ms
step:887/2330 train_time:50665ms step_avg:57.12ms
step:888/2330 train_time:50725ms step_avg:57.12ms
step:889/2330 train_time:50782ms step_avg:57.12ms
step:890/2330 train_time:50841ms step_avg:57.12ms
step:891/2330 train_time:50898ms step_avg:57.12ms
step:892/2330 train_time:50957ms step_avg:57.13ms
step:893/2330 train_time:51012ms step_avg:57.12ms
step:894/2330 train_time:51073ms step_avg:57.13ms
step:895/2330 train_time:51129ms step_avg:57.13ms
step:896/2330 train_time:51189ms step_avg:57.13ms
step:897/2330 train_time:51245ms step_avg:57.13ms
step:898/2330 train_time:51304ms step_avg:57.13ms
step:899/2330 train_time:51361ms step_avg:57.13ms
step:900/2330 train_time:51421ms step_avg:57.13ms
step:901/2330 train_time:51477ms step_avg:57.13ms
step:902/2330 train_time:51536ms step_avg:57.14ms
step:903/2330 train_time:51592ms step_avg:57.13ms
step:904/2330 train_time:51651ms step_avg:57.14ms
step:905/2330 train_time:51707ms step_avg:57.13ms
step:906/2330 train_time:51767ms step_avg:57.14ms
step:907/2330 train_time:51823ms step_avg:57.14ms
step:908/2330 train_time:51883ms step_avg:57.14ms
step:909/2330 train_time:51941ms step_avg:57.14ms
step:910/2330 train_time:52000ms step_avg:57.14ms
step:911/2330 train_time:52056ms step_avg:57.14ms
step:912/2330 train_time:52115ms step_avg:57.14ms
step:913/2330 train_time:52171ms step_avg:57.14ms
step:914/2330 train_time:52230ms step_avg:57.14ms
step:915/2330 train_time:52286ms step_avg:57.14ms
step:916/2330 train_time:52347ms step_avg:57.15ms
step:917/2330 train_time:52403ms step_avg:57.15ms
step:918/2330 train_time:52464ms step_avg:57.15ms
step:919/2330 train_time:52520ms step_avg:57.15ms
step:920/2330 train_time:52581ms step_avg:57.15ms
step:921/2330 train_time:52637ms step_avg:57.15ms
step:922/2330 train_time:52696ms step_avg:57.15ms
step:923/2330 train_time:52752ms step_avg:57.15ms
step:924/2330 train_time:52811ms step_avg:57.16ms
step:925/2330 train_time:52868ms step_avg:57.15ms
step:926/2330 train_time:52927ms step_avg:57.16ms
step:927/2330 train_time:52984ms step_avg:57.16ms
step:928/2330 train_time:53043ms step_avg:57.16ms
step:929/2330 train_time:53100ms step_avg:57.16ms
step:930/2330 train_time:53158ms step_avg:57.16ms
step:931/2330 train_time:53215ms step_avg:57.16ms
step:932/2330 train_time:53274ms step_avg:57.16ms
step:933/2330 train_time:53330ms step_avg:57.16ms
step:934/2330 train_time:53389ms step_avg:57.16ms
step:935/2330 train_time:53444ms step_avg:57.16ms
step:936/2330 train_time:53505ms step_avg:57.16ms
step:937/2330 train_time:53561ms step_avg:57.16ms
step:938/2330 train_time:53621ms step_avg:57.17ms
step:939/2330 train_time:53678ms step_avg:57.16ms
step:940/2330 train_time:53738ms step_avg:57.17ms
step:941/2330 train_time:53794ms step_avg:57.17ms
step:942/2330 train_time:53852ms step_avg:57.17ms
step:943/2330 train_time:53908ms step_avg:57.17ms
step:944/2330 train_time:53969ms step_avg:57.17ms
step:945/2330 train_time:54025ms step_avg:57.17ms
step:946/2330 train_time:54085ms step_avg:57.17ms
step:947/2330 train_time:54142ms step_avg:57.17ms
step:948/2330 train_time:54201ms step_avg:57.17ms
step:949/2330 train_time:54258ms step_avg:57.17ms
step:950/2330 train_time:54316ms step_avg:57.18ms
step:951/2330 train_time:54372ms step_avg:57.17ms
step:952/2330 train_time:54432ms step_avg:57.18ms
step:953/2330 train_time:54488ms step_avg:57.18ms
step:954/2330 train_time:54548ms step_avg:57.18ms
step:955/2330 train_time:54603ms step_avg:57.18ms
step:956/2330 train_time:54664ms step_avg:57.18ms
step:957/2330 train_time:54720ms step_avg:57.18ms
step:958/2330 train_time:54780ms step_avg:57.18ms
step:959/2330 train_time:54837ms step_avg:57.18ms
step:960/2330 train_time:54897ms step_avg:57.18ms
step:961/2330 train_time:54953ms step_avg:57.18ms
step:962/2330 train_time:55011ms step_avg:57.18ms
step:963/2330 train_time:55068ms step_avg:57.18ms
step:964/2330 train_time:55127ms step_avg:57.19ms
step:965/2330 train_time:55183ms step_avg:57.18ms
step:966/2330 train_time:55243ms step_avg:57.19ms
step:967/2330 train_time:55300ms step_avg:57.19ms
step:968/2330 train_time:55360ms step_avg:57.19ms
step:969/2330 train_time:55416ms step_avg:57.19ms
step:970/2330 train_time:55475ms step_avg:57.19ms
step:971/2330 train_time:55530ms step_avg:57.19ms
step:972/2330 train_time:55590ms step_avg:57.19ms
step:973/2330 train_time:55646ms step_avg:57.19ms
step:974/2330 train_time:55706ms step_avg:57.19ms
step:975/2330 train_time:55763ms step_avg:57.19ms
step:976/2330 train_time:55823ms step_avg:57.20ms
step:977/2330 train_time:55879ms step_avg:57.19ms
step:978/2330 train_time:55939ms step_avg:57.20ms
step:979/2330 train_time:55996ms step_avg:57.20ms
step:980/2330 train_time:56054ms step_avg:57.20ms
step:981/2330 train_time:56110ms step_avg:57.20ms
step:982/2330 train_time:56169ms step_avg:57.20ms
step:983/2330 train_time:56225ms step_avg:57.20ms
step:984/2330 train_time:56285ms step_avg:57.20ms
step:985/2330 train_time:56342ms step_avg:57.20ms
step:986/2330 train_time:56401ms step_avg:57.20ms
step:987/2330 train_time:56458ms step_avg:57.20ms
step:988/2330 train_time:56517ms step_avg:57.20ms
step:989/2330 train_time:56574ms step_avg:57.20ms
step:990/2330 train_time:56632ms step_avg:57.20ms
step:991/2330 train_time:56688ms step_avg:57.20ms
step:992/2330 train_time:56748ms step_avg:57.21ms
step:993/2330 train_time:56804ms step_avg:57.20ms
step:994/2330 train_time:56864ms step_avg:57.21ms
step:995/2330 train_time:56920ms step_avg:57.21ms
step:996/2330 train_time:56980ms step_avg:57.21ms
step:997/2330 train_time:57037ms step_avg:57.21ms
step:998/2330 train_time:57096ms step_avg:57.21ms
step:999/2330 train_time:57153ms step_avg:57.21ms
step:1000/2330 train_time:57211ms step_avg:57.21ms
step:1000/2330 val_loss:4.4338 train_time:57291ms step_avg:57.29ms
step:1001/2330 train_time:57310ms step_avg:57.25ms
step:1002/2330 train_time:57328ms step_avg:57.21ms
step:1003/2330 train_time:57383ms step_avg:57.21ms
step:1004/2330 train_time:57447ms step_avg:57.22ms
step:1005/2330 train_time:57502ms step_avg:57.22ms
step:1006/2330 train_time:57565ms step_avg:57.22ms
step:1007/2330 train_time:57621ms step_avg:57.22ms
step:1008/2330 train_time:57680ms step_avg:57.22ms
step:1009/2330 train_time:57736ms step_avg:57.22ms
step:1010/2330 train_time:57795ms step_avg:57.22ms
step:1011/2330 train_time:57851ms step_avg:57.22ms
step:1012/2330 train_time:57910ms step_avg:57.22ms
step:1013/2330 train_time:57965ms step_avg:57.22ms
step:1014/2330 train_time:58024ms step_avg:57.22ms
step:1015/2330 train_time:58079ms step_avg:57.22ms
step:1016/2330 train_time:58138ms step_avg:57.22ms
step:1017/2330 train_time:58194ms step_avg:57.22ms
step:1018/2330 train_time:58255ms step_avg:57.23ms
step:1019/2330 train_time:58313ms step_avg:57.23ms
step:1020/2330 train_time:58375ms step_avg:57.23ms
step:1021/2330 train_time:58432ms step_avg:57.23ms
step:1022/2330 train_time:58493ms step_avg:57.23ms
step:1023/2330 train_time:58550ms step_avg:57.23ms
step:1024/2330 train_time:58609ms step_avg:57.24ms
step:1025/2330 train_time:58665ms step_avg:57.23ms
step:1026/2330 train_time:58723ms step_avg:57.24ms
step:1027/2330 train_time:58779ms step_avg:57.23ms
step:1028/2330 train_time:58838ms step_avg:57.24ms
step:1029/2330 train_time:58894ms step_avg:57.23ms
step:1030/2330 train_time:58953ms step_avg:57.24ms
step:1031/2330 train_time:59009ms step_avg:57.24ms
step:1032/2330 train_time:59068ms step_avg:57.24ms
step:1033/2330 train_time:59125ms step_avg:57.24ms
step:1034/2330 train_time:59183ms step_avg:57.24ms
step:1035/2330 train_time:59239ms step_avg:57.24ms
step:1036/2330 train_time:59300ms step_avg:57.24ms
step:1037/2330 train_time:59356ms step_avg:57.24ms
step:1038/2330 train_time:59417ms step_avg:57.24ms
step:1039/2330 train_time:59474ms step_avg:57.24ms
step:1040/2330 train_time:59535ms step_avg:57.24ms
step:1041/2330 train_time:59592ms step_avg:57.24ms
step:1042/2330 train_time:59651ms step_avg:57.25ms
step:1043/2330 train_time:59707ms step_avg:57.25ms
step:1044/2330 train_time:59766ms step_avg:57.25ms
step:1045/2330 train_time:59822ms step_avg:57.25ms
step:1046/2330 train_time:59881ms step_avg:57.25ms
step:1047/2330 train_time:59937ms step_avg:57.25ms
step:1048/2330 train_time:59996ms step_avg:57.25ms
step:1049/2330 train_time:60052ms step_avg:57.25ms
step:1050/2330 train_time:60111ms step_avg:57.25ms
step:1051/2330 train_time:60168ms step_avg:57.25ms
step:1052/2330 train_time:60227ms step_avg:57.25ms
step:1053/2330 train_time:60283ms step_avg:57.25ms
step:1054/2330 train_time:60342ms step_avg:57.25ms
step:1055/2330 train_time:60398ms step_avg:57.25ms
step:1056/2330 train_time:60459ms step_avg:57.25ms
step:1057/2330 train_time:60516ms step_avg:57.25ms
step:1058/2330 train_time:60576ms step_avg:57.26ms
step:1059/2330 train_time:60633ms step_avg:57.25ms
step:1060/2330 train_time:60693ms step_avg:57.26ms
step:1061/2330 train_time:60749ms step_avg:57.26ms
step:1062/2330 train_time:60808ms step_avg:57.26ms
step:1063/2330 train_time:60864ms step_avg:57.26ms
step:1064/2330 train_time:60922ms step_avg:57.26ms
step:1065/2330 train_time:60978ms step_avg:57.26ms
step:1066/2330 train_time:61037ms step_avg:57.26ms
step:1067/2330 train_time:61094ms step_avg:57.26ms
step:1068/2330 train_time:61153ms step_avg:57.26ms
step:1069/2330 train_time:61210ms step_avg:57.26ms
step:1070/2330 train_time:61270ms step_avg:57.26ms
step:1071/2330 train_time:61326ms step_avg:57.26ms
step:1072/2330 train_time:61385ms step_avg:57.26ms
step:1073/2330 train_time:61440ms step_avg:57.26ms
step:1074/2330 train_time:61501ms step_avg:57.26ms
step:1075/2330 train_time:61557ms step_avg:57.26ms
step:1076/2330 train_time:61618ms step_avg:57.27ms
step:1077/2330 train_time:61675ms step_avg:57.27ms
step:1078/2330 train_time:61735ms step_avg:57.27ms
step:1079/2330 train_time:61791ms step_avg:57.27ms
step:1080/2330 train_time:61850ms step_avg:57.27ms
step:1081/2330 train_time:61906ms step_avg:57.27ms
step:1082/2330 train_time:61966ms step_avg:57.27ms
step:1083/2330 train_time:62022ms step_avg:57.27ms
step:1084/2330 train_time:62081ms step_avg:57.27ms
step:1085/2330 train_time:62137ms step_avg:57.27ms
step:1086/2330 train_time:62197ms step_avg:57.27ms
step:1087/2330 train_time:62253ms step_avg:57.27ms
step:1088/2330 train_time:62312ms step_avg:57.27ms
step:1089/2330 train_time:62369ms step_avg:57.27ms
step:1090/2330 train_time:62428ms step_avg:57.27ms
step:1091/2330 train_time:62484ms step_avg:57.27ms
step:1092/2330 train_time:62544ms step_avg:57.27ms
step:1093/2330 train_time:62600ms step_avg:57.27ms
step:1094/2330 train_time:62661ms step_avg:57.28ms
step:1095/2330 train_time:62717ms step_avg:57.28ms
step:1096/2330 train_time:62777ms step_avg:57.28ms
step:1097/2330 train_time:62834ms step_avg:57.28ms
step:1098/2330 train_time:62893ms step_avg:57.28ms
step:1099/2330 train_time:62949ms step_avg:57.28ms
step:1100/2330 train_time:63009ms step_avg:57.28ms
step:1101/2330 train_time:63065ms step_avg:57.28ms
step:1102/2330 train_time:63123ms step_avg:57.28ms
step:1103/2330 train_time:63179ms step_avg:57.28ms
step:1104/2330 train_time:63238ms step_avg:57.28ms
step:1105/2330 train_time:63295ms step_avg:57.28ms
step:1106/2330 train_time:63355ms step_avg:57.28ms
step:1107/2330 train_time:63411ms step_avg:57.28ms
step:1108/2330 train_time:63471ms step_avg:57.28ms
step:1109/2330 train_time:63528ms step_avg:57.28ms
step:1110/2330 train_time:63587ms step_avg:57.29ms
step:1111/2330 train_time:63643ms step_avg:57.28ms
step:1112/2330 train_time:63702ms step_avg:57.29ms
step:1113/2330 train_time:63758ms step_avg:57.28ms
step:1114/2330 train_time:63818ms step_avg:57.29ms
step:1115/2330 train_time:63875ms step_avg:57.29ms
step:1116/2330 train_time:63935ms step_avg:57.29ms
step:1117/2330 train_time:63991ms step_avg:57.29ms
step:1118/2330 train_time:64051ms step_avg:57.29ms
step:1119/2330 train_time:64107ms step_avg:57.29ms
step:1120/2330 train_time:64166ms step_avg:57.29ms
step:1121/2330 train_time:64222ms step_avg:57.29ms
step:1122/2330 train_time:64281ms step_avg:57.29ms
step:1123/2330 train_time:64337ms step_avg:57.29ms
step:1124/2330 train_time:64398ms step_avg:57.29ms
step:1125/2330 train_time:64454ms step_avg:57.29ms
step:1126/2330 train_time:64514ms step_avg:57.29ms
step:1127/2330 train_time:64570ms step_avg:57.29ms
step:1128/2330 train_time:64630ms step_avg:57.30ms
step:1129/2330 train_time:64686ms step_avg:57.29ms
step:1130/2330 train_time:64744ms step_avg:57.30ms
step:1131/2330 train_time:64800ms step_avg:57.29ms
step:1132/2330 train_time:64860ms step_avg:57.30ms
step:1133/2330 train_time:64916ms step_avg:57.30ms
step:1134/2330 train_time:64976ms step_avg:57.30ms
step:1135/2330 train_time:65033ms step_avg:57.30ms
step:1136/2330 train_time:65093ms step_avg:57.30ms
step:1137/2330 train_time:65149ms step_avg:57.30ms
step:1138/2330 train_time:65208ms step_avg:57.30ms
step:1139/2330 train_time:65263ms step_avg:57.30ms
step:1140/2330 train_time:65323ms step_avg:57.30ms
step:1141/2330 train_time:65379ms step_avg:57.30ms
step:1142/2330 train_time:65439ms step_avg:57.30ms
step:1143/2330 train_time:65496ms step_avg:57.30ms
step:1144/2330 train_time:65555ms step_avg:57.30ms
step:1145/2330 train_time:65612ms step_avg:57.30ms
step:1146/2330 train_time:65671ms step_avg:57.30ms
step:1147/2330 train_time:65727ms step_avg:57.30ms
step:1148/2330 train_time:65787ms step_avg:57.31ms
step:1149/2330 train_time:65843ms step_avg:57.30ms
step:1150/2330 train_time:65902ms step_avg:57.31ms
step:1151/2330 train_time:65958ms step_avg:57.31ms
step:1152/2330 train_time:66019ms step_avg:57.31ms
step:1153/2330 train_time:66076ms step_avg:57.31ms
step:1154/2330 train_time:66135ms step_avg:57.31ms
step:1155/2330 train_time:66191ms step_avg:57.31ms
step:1156/2330 train_time:66251ms step_avg:57.31ms
step:1157/2330 train_time:66306ms step_avg:57.31ms
step:1158/2330 train_time:66366ms step_avg:57.31ms
step:1159/2330 train_time:66422ms step_avg:57.31ms
step:1160/2330 train_time:66481ms step_avg:57.31ms
step:1161/2330 train_time:66538ms step_avg:57.31ms
step:1162/2330 train_time:66598ms step_avg:57.31ms
step:1163/2330 train_time:66654ms step_avg:57.31ms
step:1164/2330 train_time:66714ms step_avg:57.31ms
step:1165/2330 train_time:66771ms step_avg:57.31ms
step:1166/2330 train_time:66831ms step_avg:57.32ms
step:1167/2330 train_time:66888ms step_avg:57.32ms
step:1168/2330 train_time:66946ms step_avg:57.32ms
step:1169/2330 train_time:67002ms step_avg:57.32ms
step:1170/2330 train_time:67061ms step_avg:57.32ms
step:1171/2330 train_time:67118ms step_avg:57.32ms
step:1172/2330 train_time:67178ms step_avg:57.32ms
step:1173/2330 train_time:67234ms step_avg:57.32ms
step:1174/2330 train_time:67293ms step_avg:57.32ms
step:1175/2330 train_time:67349ms step_avg:57.32ms
step:1176/2330 train_time:67408ms step_avg:57.32ms
step:1177/2330 train_time:67464ms step_avg:57.32ms
step:1178/2330 train_time:67523ms step_avg:57.32ms
step:1179/2330 train_time:67579ms step_avg:57.32ms
step:1180/2330 train_time:67639ms step_avg:57.32ms
step:1181/2330 train_time:67695ms step_avg:57.32ms
step:1182/2330 train_time:67755ms step_avg:57.32ms
step:1183/2330 train_time:67812ms step_avg:57.32ms
step:1184/2330 train_time:67871ms step_avg:57.32ms
step:1185/2330 train_time:67928ms step_avg:57.32ms
step:1186/2330 train_time:67986ms step_avg:57.32ms
step:1187/2330 train_time:68042ms step_avg:57.32ms
step:1188/2330 train_time:68101ms step_avg:57.32ms
step:1189/2330 train_time:68158ms step_avg:57.32ms
step:1190/2330 train_time:68217ms step_avg:57.33ms
step:1191/2330 train_time:68274ms step_avg:57.32ms
step:1192/2330 train_time:68333ms step_avg:57.33ms
step:1193/2330 train_time:68390ms step_avg:57.33ms
step:1194/2330 train_time:68449ms step_avg:57.33ms
step:1195/2330 train_time:68505ms step_avg:57.33ms
step:1196/2330 train_time:68564ms step_avg:57.33ms
step:1197/2330 train_time:68620ms step_avg:57.33ms
step:1198/2330 train_time:68679ms step_avg:57.33ms
step:1199/2330 train_time:68736ms step_avg:57.33ms
step:1200/2330 train_time:68796ms step_avg:57.33ms
step:1201/2330 train_time:68852ms step_avg:57.33ms
step:1202/2330 train_time:68912ms step_avg:57.33ms
step:1203/2330 train_time:68968ms step_avg:57.33ms
step:1204/2330 train_time:69027ms step_avg:57.33ms
step:1205/2330 train_time:69083ms step_avg:57.33ms
step:1206/2330 train_time:69142ms step_avg:57.33ms
step:1207/2330 train_time:69198ms step_avg:57.33ms
step:1208/2330 train_time:69258ms step_avg:57.33ms
step:1209/2330 train_time:69314ms step_avg:57.33ms
step:1210/2330 train_time:69374ms step_avg:57.33ms
step:1211/2330 train_time:69431ms step_avg:57.33ms
step:1212/2330 train_time:69491ms step_avg:57.34ms
step:1213/2330 train_time:69546ms step_avg:57.33ms
step:1214/2330 train_time:69606ms step_avg:57.34ms
step:1215/2330 train_time:69661ms step_avg:57.33ms
step:1216/2330 train_time:69721ms step_avg:57.34ms
step:1217/2330 train_time:69777ms step_avg:57.34ms
step:1218/2330 train_time:69837ms step_avg:57.34ms
step:1219/2330 train_time:69893ms step_avg:57.34ms
step:1220/2330 train_time:69953ms step_avg:57.34ms
step:1221/2330 train_time:70010ms step_avg:57.34ms
step:1222/2330 train_time:70069ms step_avg:57.34ms
step:1223/2330 train_time:70125ms step_avg:57.34ms
step:1224/2330 train_time:70184ms step_avg:57.34ms
step:1225/2330 train_time:70239ms step_avg:57.34ms
step:1226/2330 train_time:70299ms step_avg:57.34ms
step:1227/2330 train_time:70356ms step_avg:57.34ms
step:1228/2330 train_time:70415ms step_avg:57.34ms
step:1229/2330 train_time:70472ms step_avg:57.34ms
step:1230/2330 train_time:70532ms step_avg:57.34ms
step:1231/2330 train_time:70588ms step_avg:57.34ms
step:1232/2330 train_time:70647ms step_avg:57.34ms
step:1233/2330 train_time:70704ms step_avg:57.34ms
step:1234/2330 train_time:70763ms step_avg:57.34ms
step:1235/2330 train_time:70819ms step_avg:57.34ms
step:1236/2330 train_time:70878ms step_avg:57.35ms
step:1237/2330 train_time:70934ms step_avg:57.34ms
step:1238/2330 train_time:70995ms step_avg:57.35ms
step:1239/2330 train_time:71052ms step_avg:57.35ms
step:1240/2330 train_time:71111ms step_avg:57.35ms
step:1241/2330 train_time:71167ms step_avg:57.35ms
step:1242/2330 train_time:71226ms step_avg:57.35ms
step:1243/2330 train_time:71282ms step_avg:57.35ms
step:1244/2330 train_time:71342ms step_avg:57.35ms
step:1245/2330 train_time:71398ms step_avg:57.35ms
step:1246/2330 train_time:71458ms step_avg:57.35ms
step:1247/2330 train_time:71515ms step_avg:57.35ms
step:1248/2330 train_time:71574ms step_avg:57.35ms
step:1249/2330 train_time:71631ms step_avg:57.35ms
step:1250/2330 train_time:71690ms step_avg:57.35ms
step:1250/2330 val_loss:4.3240 train_time:71769ms step_avg:57.42ms
step:1251/2330 train_time:71788ms step_avg:57.38ms
step:1252/2330 train_time:71808ms step_avg:57.35ms
step:1253/2330 train_time:71865ms step_avg:57.35ms
step:1254/2330 train_time:71931ms step_avg:57.36ms
step:1255/2330 train_time:71987ms step_avg:57.36ms
step:1256/2330 train_time:72046ms step_avg:57.36ms
step:1257/2330 train_time:72102ms step_avg:57.36ms
step:1258/2330 train_time:72162ms step_avg:57.36ms
step:1259/2330 train_time:72217ms step_avg:57.36ms
step:1260/2330 train_time:72276ms step_avg:57.36ms
step:1261/2330 train_time:72332ms step_avg:57.36ms
step:1262/2330 train_time:72391ms step_avg:57.36ms
step:1263/2330 train_time:72446ms step_avg:57.36ms
step:1264/2330 train_time:72505ms step_avg:57.36ms
step:1265/2330 train_time:72561ms step_avg:57.36ms
step:1266/2330 train_time:72619ms step_avg:57.36ms
step:1267/2330 train_time:72675ms step_avg:57.36ms
step:1268/2330 train_time:72734ms step_avg:57.36ms
step:1269/2330 train_time:72792ms step_avg:57.36ms
step:1270/2330 train_time:72853ms step_avg:57.36ms
step:1271/2330 train_time:72911ms step_avg:57.37ms
step:1272/2330 train_time:72971ms step_avg:57.37ms
step:1273/2330 train_time:73028ms step_avg:57.37ms
step:1274/2330 train_time:73087ms step_avg:57.37ms
step:1275/2330 train_time:73143ms step_avg:57.37ms
step:1276/2330 train_time:73202ms step_avg:57.37ms
step:1277/2330 train_time:73258ms step_avg:57.37ms
step:1278/2330 train_time:73318ms step_avg:57.37ms
step:1279/2330 train_time:73374ms step_avg:57.37ms
step:1280/2330 train_time:73433ms step_avg:57.37ms
step:1281/2330 train_time:73489ms step_avg:57.37ms
step:1282/2330 train_time:73548ms step_avg:57.37ms
step:1283/2330 train_time:73604ms step_avg:57.37ms
step:1284/2330 train_time:73663ms step_avg:57.37ms
step:1285/2330 train_time:73719ms step_avg:57.37ms
step:1286/2330 train_time:73778ms step_avg:57.37ms
step:1287/2330 train_time:73835ms step_avg:57.37ms
step:1288/2330 train_time:73896ms step_avg:57.37ms
step:1289/2330 train_time:73952ms step_avg:57.37ms
step:1290/2330 train_time:74013ms step_avg:57.37ms
step:1291/2330 train_time:74070ms step_avg:57.37ms
step:1292/2330 train_time:74130ms step_avg:57.38ms
step:1293/2330 train_time:74186ms step_avg:57.38ms
step:1294/2330 train_time:74245ms step_avg:57.38ms
step:1295/2330 train_time:74301ms step_avg:57.38ms
step:1296/2330 train_time:74359ms step_avg:57.38ms
step:1297/2330 train_time:74415ms step_avg:57.37ms
step:1298/2330 train_time:74474ms step_avg:57.38ms
step:1299/2330 train_time:74530ms step_avg:57.37ms
step:1300/2330 train_time:74589ms step_avg:57.38ms
step:1301/2330 train_time:74645ms step_avg:57.37ms
step:1302/2330 train_time:74703ms step_avg:57.38ms
step:1303/2330 train_time:74759ms step_avg:57.37ms
step:1304/2330 train_time:74819ms step_avg:57.38ms
step:1305/2330 train_time:74875ms step_avg:57.38ms
step:1306/2330 train_time:75310ms step_avg:57.66ms
step:1307/2330 train_time:75366ms step_avg:57.66ms
step:1308/2330 train_time:75424ms step_avg:57.66ms
step:1309/2330 train_time:75479ms step_avg:57.66ms
step:1310/2330 train_time:75538ms step_avg:57.66ms
step:1311/2330 train_time:75593ms step_avg:57.66ms
step:1312/2330 train_time:75652ms step_avg:57.66ms
step:1313/2330 train_time:75708ms step_avg:57.66ms
step:1314/2330 train_time:75766ms step_avg:57.66ms
step:1315/2330 train_time:75822ms step_avg:57.66ms
step:1316/2330 train_time:75880ms step_avg:57.66ms
step:1317/2330 train_time:75936ms step_avg:57.66ms
step:1318/2330 train_time:75994ms step_avg:57.66ms
step:1319/2330 train_time:76050ms step_avg:57.66ms
step:1320/2330 train_time:76109ms step_avg:57.66ms
step:1321/2330 train_time:76166ms step_avg:57.66ms
step:1322/2330 train_time:76228ms step_avg:57.66ms
step:1323/2330 train_time:76285ms step_avg:57.66ms
step:1324/2330 train_time:76346ms step_avg:57.66ms
step:1325/2330 train_time:76402ms step_avg:57.66ms
step:1326/2330 train_time:76460ms step_avg:57.66ms
step:1327/2330 train_time:76516ms step_avg:57.66ms
step:1328/2330 train_time:76576ms step_avg:57.66ms
step:1329/2330 train_time:76632ms step_avg:57.66ms
step:1330/2330 train_time:76691ms step_avg:57.66ms
step:1331/2330 train_time:76747ms step_avg:57.66ms
step:1332/2330 train_time:76806ms step_avg:57.66ms
step:1333/2330 train_time:76862ms step_avg:57.66ms
step:1334/2330 train_time:76920ms step_avg:57.66ms
step:1335/2330 train_time:76976ms step_avg:57.66ms
step:1336/2330 train_time:77036ms step_avg:57.66ms
step:1337/2330 train_time:77092ms step_avg:57.66ms
step:1338/2330 train_time:77152ms step_avg:57.66ms
step:1339/2330 train_time:77210ms step_avg:57.66ms
step:1340/2330 train_time:77270ms step_avg:57.66ms
step:1341/2330 train_time:77327ms step_avg:57.66ms
step:1342/2330 train_time:77386ms step_avg:57.66ms
step:1343/2330 train_time:77443ms step_avg:57.66ms
step:1344/2330 train_time:77501ms step_avg:57.66ms
step:1345/2330 train_time:77557ms step_avg:57.66ms
step:1346/2330 train_time:77617ms step_avg:57.67ms
step:1347/2330 train_time:77673ms step_avg:57.66ms
step:1348/2330 train_time:77733ms step_avg:57.67ms
step:1349/2330 train_time:77789ms step_avg:57.66ms
step:1350/2330 train_time:77849ms step_avg:57.67ms
step:1351/2330 train_time:77904ms step_avg:57.66ms
step:1352/2330 train_time:77963ms step_avg:57.66ms
step:1353/2330 train_time:78018ms step_avg:57.66ms
step:1354/2330 train_time:78078ms step_avg:57.66ms
step:1355/2330 train_time:78135ms step_avg:57.66ms
step:1356/2330 train_time:78195ms step_avg:57.67ms
step:1357/2330 train_time:78253ms step_avg:57.67ms
step:1358/2330 train_time:78312ms step_avg:57.67ms
step:1359/2330 train_time:78369ms step_avg:57.67ms
step:1360/2330 train_time:78428ms step_avg:57.67ms
step:1361/2330 train_time:78484ms step_avg:57.67ms
step:1362/2330 train_time:78543ms step_avg:57.67ms
step:1363/2330 train_time:78599ms step_avg:57.67ms
step:1364/2330 train_time:78659ms step_avg:57.67ms
step:1365/2330 train_time:78715ms step_avg:57.67ms
step:1366/2330 train_time:78775ms step_avg:57.67ms
step:1367/2330 train_time:78831ms step_avg:57.67ms
step:1368/2330 train_time:78890ms step_avg:57.67ms
step:1369/2330 train_time:78946ms step_avg:57.67ms
step:1370/2330 train_time:79005ms step_avg:57.67ms
step:1371/2330 train_time:79062ms step_avg:57.67ms
step:1372/2330 train_time:79120ms step_avg:57.67ms
step:1373/2330 train_time:79177ms step_avg:57.67ms
step:1374/2330 train_time:79236ms step_avg:57.67ms
step:1375/2330 train_time:79292ms step_avg:57.67ms
step:1376/2330 train_time:79353ms step_avg:57.67ms
step:1377/2330 train_time:79411ms step_avg:57.67ms
step:1378/2330 train_time:79470ms step_avg:57.67ms
step:1379/2330 train_time:79527ms step_avg:57.67ms
step:1380/2330 train_time:79585ms step_avg:57.67ms
step:1381/2330 train_time:79640ms step_avg:57.67ms
step:1382/2330 train_time:79700ms step_avg:57.67ms
step:1383/2330 train_time:79756ms step_avg:57.67ms
step:1384/2330 train_time:79815ms step_avg:57.67ms
step:1385/2330 train_time:79871ms step_avg:57.67ms
step:1386/2330 train_time:79930ms step_avg:57.67ms
step:1387/2330 train_time:79986ms step_avg:57.67ms
step:1388/2330 train_time:80046ms step_avg:57.67ms
step:1389/2330 train_time:80102ms step_avg:57.67ms
step:1390/2330 train_time:80160ms step_avg:57.67ms
step:1391/2330 train_time:80216ms step_avg:57.67ms
step:1392/2330 train_time:80276ms step_avg:57.67ms
step:1393/2330 train_time:80332ms step_avg:57.67ms
step:1394/2330 train_time:80392ms step_avg:57.67ms
step:1395/2330 train_time:80449ms step_avg:57.67ms
step:1396/2330 train_time:80510ms step_avg:57.67ms
step:1397/2330 train_time:80566ms step_avg:57.67ms
step:1398/2330 train_time:80624ms step_avg:57.67ms
step:1399/2330 train_time:80680ms step_avg:57.67ms
step:1400/2330 train_time:80739ms step_avg:57.67ms
step:1401/2330 train_time:80795ms step_avg:57.67ms
step:1402/2330 train_time:80855ms step_avg:57.67ms
step:1403/2330 train_time:80911ms step_avg:57.67ms
step:1404/2330 train_time:80971ms step_avg:57.67ms
step:1405/2330 train_time:81027ms step_avg:57.67ms
step:1406/2330 train_time:81086ms step_avg:57.67ms
step:1407/2330 train_time:81142ms step_avg:57.67ms
step:1408/2330 train_time:81201ms step_avg:57.67ms
step:1409/2330 train_time:81257ms step_avg:57.67ms
step:1410/2330 train_time:81316ms step_avg:57.67ms
step:1411/2330 train_time:81373ms step_avg:57.67ms
step:1412/2330 train_time:81433ms step_avg:57.67ms
step:1413/2330 train_time:81490ms step_avg:57.67ms
step:1414/2330 train_time:81550ms step_avg:57.67ms
step:1415/2330 train_time:81606ms step_avg:57.67ms
step:1416/2330 train_time:81665ms step_avg:57.67ms
step:1417/2330 train_time:81721ms step_avg:57.67ms
step:1418/2330 train_time:81780ms step_avg:57.67ms
step:1419/2330 train_time:81836ms step_avg:57.67ms
step:1420/2330 train_time:81896ms step_avg:57.67ms
step:1421/2330 train_time:81953ms step_avg:57.67ms
step:1422/2330 train_time:82013ms step_avg:57.67ms
step:1423/2330 train_time:82070ms step_avg:57.67ms
step:1424/2330 train_time:82128ms step_avg:57.67ms
step:1425/2330 train_time:82184ms step_avg:57.67ms
step:1426/2330 train_time:82243ms step_avg:57.67ms
step:1427/2330 train_time:82299ms step_avg:57.67ms
step:1428/2330 train_time:82359ms step_avg:57.67ms
step:1429/2330 train_time:82415ms step_avg:57.67ms
step:1430/2330 train_time:82474ms step_avg:57.67ms
step:1431/2330 train_time:82531ms step_avg:57.67ms
step:1432/2330 train_time:82590ms step_avg:57.67ms
step:1433/2330 train_time:82647ms step_avg:57.67ms
step:1434/2330 train_time:82706ms step_avg:57.67ms
step:1435/2330 train_time:82762ms step_avg:57.67ms
step:1436/2330 train_time:82821ms step_avg:57.67ms
step:1437/2330 train_time:82876ms step_avg:57.67ms
step:1438/2330 train_time:82937ms step_avg:57.68ms
step:1439/2330 train_time:82994ms step_avg:57.67ms
step:1440/2330 train_time:83053ms step_avg:57.68ms
step:1441/2330 train_time:83110ms step_avg:57.68ms
step:1442/2330 train_time:83169ms step_avg:57.68ms
step:1443/2330 train_time:83226ms step_avg:57.68ms
step:1444/2330 train_time:83284ms step_avg:57.68ms
step:1445/2330 train_time:83340ms step_avg:57.67ms
step:1446/2330 train_time:83399ms step_avg:57.68ms
step:1447/2330 train_time:83456ms step_avg:57.67ms
step:1448/2330 train_time:83515ms step_avg:57.68ms
step:1449/2330 train_time:83571ms step_avg:57.68ms
step:1450/2330 train_time:83631ms step_avg:57.68ms
step:1451/2330 train_time:83688ms step_avg:57.68ms
step:1452/2330 train_time:83747ms step_avg:57.68ms
step:1453/2330 train_time:83803ms step_avg:57.68ms
step:1454/2330 train_time:83862ms step_avg:57.68ms
step:1455/2330 train_time:83918ms step_avg:57.68ms
step:1456/2330 train_time:83977ms step_avg:57.68ms
step:1457/2330 train_time:84034ms step_avg:57.68ms
step:1458/2330 train_time:84094ms step_avg:57.68ms
step:1459/2330 train_time:84150ms step_avg:57.68ms
step:1460/2330 train_time:84210ms step_avg:57.68ms
step:1461/2330 train_time:84268ms step_avg:57.68ms
step:1462/2330 train_time:84327ms step_avg:57.68ms
step:1463/2330 train_time:84383ms step_avg:57.68ms
step:1464/2330 train_time:84441ms step_avg:57.68ms
step:1465/2330 train_time:84497ms step_avg:57.68ms
step:1466/2330 train_time:84558ms step_avg:57.68ms
step:1467/2330 train_time:84614ms step_avg:57.68ms
step:1468/2330 train_time:84673ms step_avg:57.68ms
step:1469/2330 train_time:84729ms step_avg:57.68ms
step:1470/2330 train_time:84789ms step_avg:57.68ms
step:1471/2330 train_time:84846ms step_avg:57.68ms
step:1472/2330 train_time:84904ms step_avg:57.68ms
step:1473/2330 train_time:84960ms step_avg:57.68ms
step:1474/2330 train_time:85019ms step_avg:57.68ms
step:1475/2330 train_time:85076ms step_avg:57.68ms
step:1476/2330 train_time:85135ms step_avg:57.68ms
step:1477/2330 train_time:85192ms step_avg:57.68ms
step:1478/2330 train_time:85252ms step_avg:57.68ms
step:1479/2330 train_time:85308ms step_avg:57.68ms
step:1480/2330 train_time:85367ms step_avg:57.68ms
step:1481/2330 train_time:85423ms step_avg:57.68ms
step:1482/2330 train_time:85481ms step_avg:57.68ms
step:1483/2330 train_time:85537ms step_avg:57.68ms
step:1484/2330 train_time:85598ms step_avg:57.68ms
step:1485/2330 train_time:85655ms step_avg:57.68ms
step:1486/2330 train_time:85715ms step_avg:57.68ms
step:1487/2330 train_time:85772ms step_avg:57.68ms
step:1488/2330 train_time:85831ms step_avg:57.68ms
step:1489/2330 train_time:85888ms step_avg:57.68ms
step:1490/2330 train_time:85946ms step_avg:57.68ms
step:1491/2330 train_time:86002ms step_avg:57.68ms
step:1492/2330 train_time:86062ms step_avg:57.68ms
step:1493/2330 train_time:86118ms step_avg:57.68ms
step:1494/2330 train_time:86178ms step_avg:57.68ms
step:1495/2330 train_time:86234ms step_avg:57.68ms
step:1496/2330 train_time:86295ms step_avg:57.68ms
step:1497/2330 train_time:86352ms step_avg:57.68ms
step:1498/2330 train_time:86411ms step_avg:57.68ms
step:1499/2330 train_time:86468ms step_avg:57.68ms
step:1500/2330 train_time:86526ms step_avg:57.68ms
step:1500/2330 val_loss:4.2258 train_time:86606ms step_avg:57.74ms
step:1501/2330 train_time:86625ms step_avg:57.71ms
step:1502/2330 train_time:86645ms step_avg:57.69ms
step:1503/2330 train_time:86701ms step_avg:57.69ms
step:1504/2330 train_time:86763ms step_avg:57.69ms
step:1505/2330 train_time:86819ms step_avg:57.69ms
step:1506/2330 train_time:86880ms step_avg:57.69ms
step:1507/2330 train_time:86936ms step_avg:57.69ms
step:1508/2330 train_time:86995ms step_avg:57.69ms
step:1509/2330 train_time:87050ms step_avg:57.69ms
step:1510/2330 train_time:87110ms step_avg:57.69ms
step:1511/2330 train_time:87166ms step_avg:57.69ms
step:1512/2330 train_time:87226ms step_avg:57.69ms
step:1513/2330 train_time:87282ms step_avg:57.69ms
step:1514/2330 train_time:87340ms step_avg:57.69ms
step:1515/2330 train_time:87396ms step_avg:57.69ms
step:1516/2330 train_time:87455ms step_avg:57.69ms
step:1517/2330 train_time:87510ms step_avg:57.69ms
step:1518/2330 train_time:87572ms step_avg:57.69ms
step:1519/2330 train_time:87628ms step_avg:57.69ms
step:1520/2330 train_time:87689ms step_avg:57.69ms
step:1521/2330 train_time:87746ms step_avg:57.69ms
step:1522/2330 train_time:87806ms step_avg:57.69ms
step:1523/2330 train_time:87864ms step_avg:57.69ms
step:1524/2330 train_time:87923ms step_avg:57.69ms
step:1525/2330 train_time:87980ms step_avg:57.69ms
step:1526/2330 train_time:88039ms step_avg:57.69ms
step:1527/2330 train_time:88094ms step_avg:57.69ms
step:1528/2330 train_time:88153ms step_avg:57.69ms
step:1529/2330 train_time:88210ms step_avg:57.69ms
step:1530/2330 train_time:88268ms step_avg:57.69ms
step:1531/2330 train_time:88324ms step_avg:57.69ms
step:1532/2330 train_time:88384ms step_avg:57.69ms
step:1533/2330 train_time:88440ms step_avg:57.69ms
step:1534/2330 train_time:88499ms step_avg:57.69ms
step:1535/2330 train_time:88556ms step_avg:57.69ms
step:1536/2330 train_time:88615ms step_avg:57.69ms
step:1537/2330 train_time:88671ms step_avg:57.69ms
step:1538/2330 train_time:88733ms step_avg:57.69ms
step:1539/2330 train_time:88790ms step_avg:57.69ms
step:1540/2330 train_time:88851ms step_avg:57.70ms
step:1541/2330 train_time:88908ms step_avg:57.70ms
step:1542/2330 train_time:88969ms step_avg:57.70ms
step:1543/2330 train_time:89026ms step_avg:57.70ms
step:1544/2330 train_time:89086ms step_avg:57.70ms
step:1545/2330 train_time:89143ms step_avg:57.70ms
step:1546/2330 train_time:89202ms step_avg:57.70ms
step:1547/2330 train_time:89259ms step_avg:57.70ms
step:1548/2330 train_time:89318ms step_avg:57.70ms
step:1549/2330 train_time:89374ms step_avg:57.70ms
step:1550/2330 train_time:89434ms step_avg:57.70ms
step:1551/2330 train_time:89490ms step_avg:57.70ms
step:1552/2330 train_time:89550ms step_avg:57.70ms
step:1553/2330 train_time:89607ms step_avg:57.70ms
step:1554/2330 train_time:89667ms step_avg:57.70ms
step:1555/2330 train_time:89724ms step_avg:57.70ms
step:1556/2330 train_time:89784ms step_avg:57.70ms
step:1557/2330 train_time:89842ms step_avg:57.70ms
step:1558/2330 train_time:89902ms step_avg:57.70ms
step:1559/2330 train_time:89960ms step_avg:57.70ms
step:1560/2330 train_time:90019ms step_avg:57.70ms
step:1561/2330 train_time:90075ms step_avg:57.70ms
step:1562/2330 train_time:90135ms step_avg:57.70ms
step:1563/2330 train_time:90192ms step_avg:57.70ms
step:1564/2330 train_time:90251ms step_avg:57.71ms
step:1565/2330 train_time:90308ms step_avg:57.70ms
step:1566/2330 train_time:90368ms step_avg:57.71ms
step:1567/2330 train_time:90425ms step_avg:57.71ms
step:1568/2330 train_time:90484ms step_avg:57.71ms
step:1569/2330 train_time:90541ms step_avg:57.71ms
step:1570/2330 train_time:90600ms step_avg:57.71ms
step:1571/2330 train_time:90657ms step_avg:57.71ms
step:1572/2330 train_time:90717ms step_avg:57.71ms
step:1573/2330 train_time:90774ms step_avg:57.71ms
step:1574/2330 train_time:90833ms step_avg:57.71ms
step:1575/2330 train_time:90890ms step_avg:57.71ms
step:1576/2330 train_time:90951ms step_avg:57.71ms
step:1577/2330 train_time:91008ms step_avg:57.71ms
step:1578/2330 train_time:91068ms step_avg:57.71ms
step:1579/2330 train_time:91125ms step_avg:57.71ms
step:1580/2330 train_time:91185ms step_avg:57.71ms
step:1581/2330 train_time:91242ms step_avg:57.71ms
step:1582/2330 train_time:91302ms step_avg:57.71ms
step:1583/2330 train_time:91358ms step_avg:57.71ms
step:1584/2330 train_time:91418ms step_avg:57.71ms
step:1585/2330 train_time:91474ms step_avg:57.71ms
step:1586/2330 train_time:91533ms step_avg:57.71ms
step:1587/2330 train_time:91589ms step_avg:57.71ms
step:1588/2330 train_time:91650ms step_avg:57.71ms
step:1589/2330 train_time:91706ms step_avg:57.71ms
step:1590/2330 train_time:91767ms step_avg:57.72ms
step:1591/2330 train_time:91824ms step_avg:57.71ms
step:1592/2330 train_time:91883ms step_avg:57.72ms
step:1593/2330 train_time:91940ms step_avg:57.71ms
step:1594/2330 train_time:92001ms step_avg:57.72ms
step:1595/2330 train_time:92058ms step_avg:57.72ms
step:1596/2330 train_time:92117ms step_avg:57.72ms
step:1597/2330 train_time:92174ms step_avg:57.72ms
step:1598/2330 train_time:92233ms step_avg:57.72ms
step:1599/2330 train_time:92290ms step_avg:57.72ms
step:1600/2330 train_time:92351ms step_avg:57.72ms
step:1601/2330 train_time:92408ms step_avg:57.72ms
step:1602/2330 train_time:92467ms step_avg:57.72ms
step:1603/2330 train_time:92523ms step_avg:57.72ms
step:1604/2330 train_time:92583ms step_avg:57.72ms
step:1605/2330 train_time:92641ms step_avg:57.72ms
step:1606/2330 train_time:92700ms step_avg:57.72ms
step:1607/2330 train_time:92757ms step_avg:57.72ms
step:1608/2330 train_time:92816ms step_avg:57.72ms
step:1609/2330 train_time:92873ms step_avg:57.72ms
step:1610/2330 train_time:92933ms step_avg:57.72ms
step:1611/2330 train_time:92989ms step_avg:57.72ms
step:1612/2330 train_time:93049ms step_avg:57.72ms
step:1613/2330 train_time:93105ms step_avg:57.72ms
step:1614/2330 train_time:93166ms step_avg:57.72ms
step:1615/2330 train_time:93223ms step_avg:57.72ms
step:1616/2330 train_time:93283ms step_avg:57.72ms
step:1617/2330 train_time:93340ms step_avg:57.72ms
step:1618/2330 train_time:93399ms step_avg:57.73ms
step:1619/2330 train_time:93456ms step_avg:57.72ms
step:1620/2330 train_time:93515ms step_avg:57.73ms
step:1621/2330 train_time:93572ms step_avg:57.72ms
step:1622/2330 train_time:93632ms step_avg:57.73ms
step:1623/2330 train_time:93689ms step_avg:57.73ms
step:1624/2330 train_time:93749ms step_avg:57.73ms
step:1625/2330 train_time:93806ms step_avg:57.73ms
step:1626/2330 train_time:93867ms step_avg:57.73ms
step:1627/2330 train_time:93924ms step_avg:57.73ms
step:1628/2330 train_time:93984ms step_avg:57.73ms
step:1629/2330 train_time:94040ms step_avg:57.73ms
step:1630/2330 train_time:94101ms step_avg:57.73ms
step:1631/2330 train_time:94157ms step_avg:57.73ms
step:1632/2330 train_time:94216ms step_avg:57.73ms
step:1633/2330 train_time:94273ms step_avg:57.73ms
step:1634/2330 train_time:94333ms step_avg:57.73ms
step:1635/2330 train_time:94389ms step_avg:57.73ms
step:1636/2330 train_time:94450ms step_avg:57.73ms
step:1637/2330 train_time:94507ms step_avg:57.73ms
step:1638/2330 train_time:94567ms step_avg:57.73ms
step:1639/2330 train_time:94623ms step_avg:57.73ms
step:1640/2330 train_time:94683ms step_avg:57.73ms
step:1641/2330 train_time:94739ms step_avg:57.73ms
step:1642/2330 train_time:94800ms step_avg:57.73ms
step:1643/2330 train_time:94857ms step_avg:57.73ms
step:1644/2330 train_time:94916ms step_avg:57.73ms
step:1645/2330 train_time:94973ms step_avg:57.73ms
step:1646/2330 train_time:95033ms step_avg:57.74ms
step:1647/2330 train_time:95089ms step_avg:57.73ms
step:1648/2330 train_time:95149ms step_avg:57.74ms
step:1649/2330 train_time:95206ms step_avg:57.74ms
step:1650/2330 train_time:95267ms step_avg:57.74ms
step:1651/2330 train_time:95323ms step_avg:57.74ms
step:1652/2330 train_time:95384ms step_avg:57.74ms
step:1653/2330 train_time:95441ms step_avg:57.74ms
step:1654/2330 train_time:95500ms step_avg:57.74ms
step:1655/2330 train_time:95556ms step_avg:57.74ms
step:1656/2330 train_time:95616ms step_avg:57.74ms
step:1657/2330 train_time:95672ms step_avg:57.74ms
step:1658/2330 train_time:95733ms step_avg:57.74ms
step:1659/2330 train_time:95790ms step_avg:57.74ms
step:1660/2330 train_time:95849ms step_avg:57.74ms
step:1661/2330 train_time:95906ms step_avg:57.74ms
step:1662/2330 train_time:95966ms step_avg:57.74ms
step:1663/2330 train_time:96023ms step_avg:57.74ms
step:1664/2330 train_time:96083ms step_avg:57.74ms
step:1665/2330 train_time:96140ms step_avg:57.74ms
step:1666/2330 train_time:96199ms step_avg:57.74ms
step:1667/2330 train_time:96255ms step_avg:57.74ms
step:1668/2330 train_time:96314ms step_avg:57.74ms
step:1669/2330 train_time:96371ms step_avg:57.74ms
step:1670/2330 train_time:96431ms step_avg:57.74ms
step:1671/2330 train_time:96488ms step_avg:57.74ms
step:1672/2330 train_time:96548ms step_avg:57.74ms
step:1673/2330 train_time:96605ms step_avg:57.74ms
step:1674/2330 train_time:96664ms step_avg:57.74ms
step:1675/2330 train_time:96721ms step_avg:57.74ms
step:1676/2330 train_time:96781ms step_avg:57.75ms
step:1677/2330 train_time:96837ms step_avg:57.74ms
step:1678/2330 train_time:96897ms step_avg:57.75ms
step:1679/2330 train_time:96953ms step_avg:57.74ms
step:1680/2330 train_time:97013ms step_avg:57.75ms
step:1681/2330 train_time:97070ms step_avg:57.75ms
step:1682/2330 train_time:97131ms step_avg:57.75ms
step:1683/2330 train_time:97188ms step_avg:57.75ms
step:1684/2330 train_time:97248ms step_avg:57.75ms
step:1685/2330 train_time:97305ms step_avg:57.75ms
step:1686/2330 train_time:97364ms step_avg:57.75ms
step:1687/2330 train_time:97421ms step_avg:57.75ms
step:1688/2330 train_time:97481ms step_avg:57.75ms
step:1689/2330 train_time:97539ms step_avg:57.75ms
step:1690/2330 train_time:97598ms step_avg:57.75ms
step:1691/2330 train_time:97655ms step_avg:57.75ms
step:1692/2330 train_time:97714ms step_avg:57.75ms
step:1693/2330 train_time:97771ms step_avg:57.75ms
step:1694/2330 train_time:97830ms step_avg:57.75ms
step:1695/2330 train_time:97887ms step_avg:57.75ms
step:1696/2330 train_time:97947ms step_avg:57.75ms
step:1697/2330 train_time:98004ms step_avg:57.75ms
step:1698/2330 train_time:98066ms step_avg:57.75ms
step:1699/2330 train_time:98123ms step_avg:57.75ms
step:1700/2330 train_time:98183ms step_avg:57.75ms
step:1701/2330 train_time:98239ms step_avg:57.75ms
step:1702/2330 train_time:98299ms step_avg:57.76ms
step:1703/2330 train_time:98355ms step_avg:57.75ms
step:1704/2330 train_time:98415ms step_avg:57.76ms
step:1705/2330 train_time:98471ms step_avg:57.75ms
step:1706/2330 train_time:98531ms step_avg:57.76ms
step:1707/2330 train_time:98588ms step_avg:57.76ms
step:1708/2330 train_time:98648ms step_avg:57.76ms
step:1709/2330 train_time:98706ms step_avg:57.76ms
step:1710/2330 train_time:98765ms step_avg:57.76ms
step:1711/2330 train_time:98822ms step_avg:57.76ms
step:1712/2330 train_time:98882ms step_avg:57.76ms
step:1713/2330 train_time:98938ms step_avg:57.76ms
step:1714/2330 train_time:98997ms step_avg:57.76ms
step:1715/2330 train_time:99053ms step_avg:57.76ms
step:1716/2330 train_time:99114ms step_avg:57.76ms
step:1717/2330 train_time:99170ms step_avg:57.76ms
step:1718/2330 train_time:99231ms step_avg:57.76ms
step:1719/2330 train_time:99287ms step_avg:57.76ms
step:1720/2330 train_time:99348ms step_avg:57.76ms
step:1721/2330 train_time:99404ms step_avg:57.76ms
step:1722/2330 train_time:99465ms step_avg:57.76ms
step:1723/2330 train_time:99522ms step_avg:57.76ms
step:1724/2330 train_time:99582ms step_avg:57.76ms
step:1725/2330 train_time:99639ms step_avg:57.76ms
step:1726/2330 train_time:99698ms step_avg:57.76ms
step:1727/2330 train_time:99754ms step_avg:57.76ms
step:1728/2330 train_time:99814ms step_avg:57.76ms
step:1729/2330 train_time:99870ms step_avg:57.76ms
step:1730/2330 train_time:99931ms step_avg:57.76ms
step:1731/2330 train_time:99988ms step_avg:57.76ms
step:1732/2330 train_time:100047ms step_avg:57.76ms
step:1733/2330 train_time:100104ms step_avg:57.76ms
step:1734/2330 train_time:100164ms step_avg:57.76ms
step:1735/2330 train_time:100222ms step_avg:57.76ms
step:1736/2330 train_time:100281ms step_avg:57.77ms
step:1737/2330 train_time:100338ms step_avg:57.77ms
step:1738/2330 train_time:100397ms step_avg:57.77ms
step:1739/2330 train_time:100454ms step_avg:57.77ms
step:1740/2330 train_time:100514ms step_avg:57.77ms
step:1741/2330 train_time:100570ms step_avg:57.77ms
step:1742/2330 train_time:100631ms step_avg:57.77ms
step:1743/2330 train_time:100688ms step_avg:57.77ms
step:1744/2330 train_time:100748ms step_avg:57.77ms
step:1745/2330 train_time:100805ms step_avg:57.77ms
step:1746/2330 train_time:100864ms step_avg:57.77ms
step:1747/2330 train_time:100921ms step_avg:57.77ms
step:1748/2330 train_time:100981ms step_avg:57.77ms
step:1749/2330 train_time:101038ms step_avg:57.77ms
step:1750/2330 train_time:101097ms step_avg:57.77ms
step:1750/2330 val_loss:4.1371 train_time:101177ms step_avg:57.82ms
step:1751/2330 train_time:101196ms step_avg:57.79ms
step:1752/2330 train_time:101215ms step_avg:57.77ms
step:1753/2330 train_time:101270ms step_avg:57.77ms
step:1754/2330 train_time:101335ms step_avg:57.77ms
step:1755/2330 train_time:101391ms step_avg:57.77ms
step:1756/2330 train_time:101453ms step_avg:57.78ms
step:1757/2330 train_time:101510ms step_avg:57.77ms
step:1758/2330 train_time:101570ms step_avg:57.78ms
step:1759/2330 train_time:101625ms step_avg:57.77ms
step:1760/2330 train_time:101686ms step_avg:57.78ms
step:1761/2330 train_time:101742ms step_avg:57.78ms
step:1762/2330 train_time:101802ms step_avg:57.78ms
step:1763/2330 train_time:101858ms step_avg:57.78ms
step:1764/2330 train_time:101917ms step_avg:57.78ms
step:1765/2330 train_time:101974ms step_avg:57.78ms
step:1766/2330 train_time:102033ms step_avg:57.78ms
step:1767/2330 train_time:102091ms step_avg:57.78ms
step:1768/2330 train_time:102153ms step_avg:57.78ms
step:1769/2330 train_time:102210ms step_avg:57.78ms
step:1770/2330 train_time:102272ms step_avg:57.78ms
step:1771/2330 train_time:102328ms step_avg:57.78ms
step:1772/2330 train_time:102390ms step_avg:57.78ms
step:1773/2330 train_time:102447ms step_avg:57.78ms
step:1774/2330 train_time:102507ms step_avg:57.78ms
step:1775/2330 train_time:102564ms step_avg:57.78ms
step:1776/2330 train_time:102623ms step_avg:57.78ms
step:1777/2330 train_time:102680ms step_avg:57.78ms
step:1778/2330 train_time:102740ms step_avg:57.78ms
step:1779/2330 train_time:102797ms step_avg:57.78ms
step:1780/2330 train_time:102856ms step_avg:57.78ms
step:1781/2330 train_time:102912ms step_avg:57.78ms
step:1782/2330 train_time:102972ms step_avg:57.78ms
step:1783/2330 train_time:103029ms step_avg:57.78ms
step:1784/2330 train_time:103090ms step_avg:57.79ms
step:1785/2330 train_time:103148ms step_avg:57.79ms
step:1786/2330 train_time:103207ms step_avg:57.79ms
step:1787/2330 train_time:103265ms step_avg:57.79ms
step:1788/2330 train_time:103325ms step_avg:57.79ms
step:1789/2330 train_time:103382ms step_avg:57.79ms
step:1790/2330 train_time:103441ms step_avg:57.79ms
step:1791/2330 train_time:103498ms step_avg:57.79ms
step:1792/2330 train_time:103558ms step_avg:57.79ms
step:1793/2330 train_time:103614ms step_avg:57.79ms
step:1794/2330 train_time:103674ms step_avg:57.79ms
step:1795/2330 train_time:103730ms step_avg:57.79ms
step:1796/2330 train_time:103790ms step_avg:57.79ms
step:1797/2330 train_time:103846ms step_avg:57.79ms
step:1798/2330 train_time:103907ms step_avg:57.79ms
step:1799/2330 train_time:103963ms step_avg:57.79ms
step:1800/2330 train_time:104023ms step_avg:57.79ms
step:1801/2330 train_time:104081ms step_avg:57.79ms
step:1802/2330 train_time:104140ms step_avg:57.79ms
step:1803/2330 train_time:104197ms step_avg:57.79ms
step:1804/2330 train_time:104256ms step_avg:57.79ms
step:1805/2330 train_time:104312ms step_avg:57.79ms
step:1806/2330 train_time:104373ms step_avg:57.79ms
step:1807/2330 train_time:104430ms step_avg:57.79ms
step:1808/2330 train_time:104491ms step_avg:57.79ms
step:1809/2330 train_time:104547ms step_avg:57.79ms
step:1810/2330 train_time:104608ms step_avg:57.79ms
step:1811/2330 train_time:104665ms step_avg:57.79ms
step:1812/2330 train_time:104724ms step_avg:57.79ms
step:1813/2330 train_time:104780ms step_avg:57.79ms
step:1814/2330 train_time:104840ms step_avg:57.79ms
step:1815/2330 train_time:104896ms step_avg:57.79ms
step:1816/2330 train_time:104955ms step_avg:57.79ms
step:1817/2330 train_time:105012ms step_avg:57.79ms
step:1818/2330 train_time:105073ms step_avg:57.80ms
step:1819/2330 train_time:105130ms step_avg:57.80ms
step:1820/2330 train_time:105190ms step_avg:57.80ms
step:1821/2330 train_time:105247ms step_avg:57.80ms
step:1822/2330 train_time:105308ms step_avg:57.80ms
step:1823/2330 train_time:105365ms step_avg:57.80ms
step:1824/2330 train_time:105426ms step_avg:57.80ms
step:1825/2330 train_time:105482ms step_avg:57.80ms
step:1826/2330 train_time:105542ms step_avg:57.80ms
step:1827/2330 train_time:105598ms step_avg:57.80ms
step:1828/2330 train_time:105657ms step_avg:57.80ms
step:1829/2330 train_time:105714ms step_avg:57.80ms
step:1830/2330 train_time:105774ms step_avg:57.80ms
step:1831/2330 train_time:105830ms step_avg:57.80ms
step:1832/2330 train_time:105891ms step_avg:57.80ms
step:1833/2330 train_time:105948ms step_avg:57.80ms
step:1834/2330 train_time:106007ms step_avg:57.80ms
step:1835/2330 train_time:106064ms step_avg:57.80ms
step:1836/2330 train_time:106124ms step_avg:57.80ms
step:1837/2330 train_time:106181ms step_avg:57.80ms
step:1838/2330 train_time:106240ms step_avg:57.80ms
step:1839/2330 train_time:106296ms step_avg:57.80ms
step:1840/2330 train_time:106356ms step_avg:57.80ms
step:1841/2330 train_time:106412ms step_avg:57.80ms
step:1842/2330 train_time:106473ms step_avg:57.80ms
step:1843/2330 train_time:106530ms step_avg:57.80ms
step:1844/2330 train_time:106591ms step_avg:57.80ms
step:1845/2330 train_time:106649ms step_avg:57.80ms
step:1846/2330 train_time:106708ms step_avg:57.81ms
step:1847/2330 train_time:106765ms step_avg:57.80ms
step:1848/2330 train_time:106824ms step_avg:57.81ms
step:1849/2330 train_time:106881ms step_avg:57.80ms
step:1850/2330 train_time:106941ms step_avg:57.81ms
step:1851/2330 train_time:106997ms step_avg:57.81ms
step:1852/2330 train_time:107056ms step_avg:57.81ms
step:1853/2330 train_time:107112ms step_avg:57.80ms
step:1854/2330 train_time:107174ms step_avg:57.81ms
step:1855/2330 train_time:107230ms step_avg:57.81ms
step:1856/2330 train_time:107290ms step_avg:57.81ms
step:1857/2330 train_time:107347ms step_avg:57.81ms
step:1858/2330 train_time:107407ms step_avg:57.81ms
step:1859/2330 train_time:107464ms step_avg:57.81ms
step:1860/2330 train_time:107524ms step_avg:57.81ms
step:1861/2330 train_time:107581ms step_avg:57.81ms
step:1862/2330 train_time:107640ms step_avg:57.81ms
step:1863/2330 train_time:107697ms step_avg:57.81ms
step:1864/2330 train_time:107757ms step_avg:57.81ms
step:1865/2330 train_time:107813ms step_avg:57.81ms
step:1866/2330 train_time:107874ms step_avg:57.81ms
step:1867/2330 train_time:107930ms step_avg:57.81ms
step:1868/2330 train_time:107991ms step_avg:57.81ms
step:1869/2330 train_time:108048ms step_avg:57.81ms
step:1870/2330 train_time:108108ms step_avg:57.81ms
step:1871/2330 train_time:108165ms step_avg:57.81ms
step:1872/2330 train_time:108224ms step_avg:57.81ms
step:1873/2330 train_time:108281ms step_avg:57.81ms
step:1874/2330 train_time:108341ms step_avg:57.81ms
step:1875/2330 train_time:108396ms step_avg:57.81ms
step:1876/2330 train_time:108457ms step_avg:57.81ms
step:1877/2330 train_time:108513ms step_avg:57.81ms
step:1878/2330 train_time:108574ms step_avg:57.81ms
step:1879/2330 train_time:108631ms step_avg:57.81ms
step:1880/2330 train_time:108690ms step_avg:57.81ms
step:1881/2330 train_time:108747ms step_avg:57.81ms
step:1882/2330 train_time:108808ms step_avg:57.81ms
step:1883/2330 train_time:108865ms step_avg:57.81ms
step:1884/2330 train_time:108924ms step_avg:57.82ms
step:1885/2330 train_time:108981ms step_avg:57.81ms
step:1886/2330 train_time:109040ms step_avg:57.82ms
step:1887/2330 train_time:109097ms step_avg:57.81ms
step:1888/2330 train_time:109156ms step_avg:57.82ms
step:1889/2330 train_time:109213ms step_avg:57.82ms
step:1890/2330 train_time:109273ms step_avg:57.82ms
step:1891/2330 train_time:109330ms step_avg:57.82ms
step:1892/2330 train_time:109391ms step_avg:57.82ms
step:1893/2330 train_time:109448ms step_avg:57.82ms
step:1894/2330 train_time:109507ms step_avg:57.82ms
step:1895/2330 train_time:109564ms step_avg:57.82ms
step:1896/2330 train_time:109624ms step_avg:57.82ms
step:1897/2330 train_time:109681ms step_avg:57.82ms
step:1898/2330 train_time:109740ms step_avg:57.82ms
step:1899/2330 train_time:109797ms step_avg:57.82ms
step:1900/2330 train_time:109857ms step_avg:57.82ms
step:1901/2330 train_time:109913ms step_avg:57.82ms
step:1902/2330 train_time:109973ms step_avg:57.82ms
step:1903/2330 train_time:110029ms step_avg:57.82ms
step:1904/2330 train_time:110091ms step_avg:57.82ms
step:1905/2330 train_time:110148ms step_avg:57.82ms
step:1906/2330 train_time:110207ms step_avg:57.82ms
step:1907/2330 train_time:110265ms step_avg:57.82ms
step:1908/2330 train_time:110324ms step_avg:57.82ms
step:1909/2330 train_time:110381ms step_avg:57.82ms
step:1910/2330 train_time:110440ms step_avg:57.82ms
step:1911/2330 train_time:110496ms step_avg:57.82ms
step:1912/2330 train_time:110556ms step_avg:57.82ms
step:1913/2330 train_time:110612ms step_avg:57.82ms
step:1914/2330 train_time:110674ms step_avg:57.82ms
step:1915/2330 train_time:110730ms step_avg:57.82ms
step:1916/2330 train_time:110791ms step_avg:57.82ms
step:1917/2330 train_time:110848ms step_avg:57.82ms
step:1918/2330 train_time:110908ms step_avg:57.83ms
step:1919/2330 train_time:110965ms step_avg:57.82ms
step:1920/2330 train_time:111025ms step_avg:57.83ms
step:1921/2330 train_time:111081ms step_avg:57.82ms
step:1922/2330 train_time:111140ms step_avg:57.83ms
step:1923/2330 train_time:111196ms step_avg:57.82ms
step:1924/2330 train_time:111256ms step_avg:57.83ms
step:1925/2330 train_time:111313ms step_avg:57.82ms
step:1926/2330 train_time:111375ms step_avg:57.83ms
step:1927/2330 train_time:111431ms step_avg:57.83ms
step:1928/2330 train_time:111491ms step_avg:57.83ms
step:1929/2330 train_time:111548ms step_avg:57.83ms
step:1930/2330 train_time:111608ms step_avg:57.83ms
step:1931/2330 train_time:111665ms step_avg:57.83ms
step:1932/2330 train_time:111724ms step_avg:57.83ms
step:1933/2330 train_time:111781ms step_avg:57.83ms
step:1934/2330 train_time:111841ms step_avg:57.83ms
step:1935/2330 train_time:111897ms step_avg:57.83ms
step:1936/2330 train_time:111957ms step_avg:57.83ms
step:1937/2330 train_time:112014ms step_avg:57.83ms
step:1938/2330 train_time:112074ms step_avg:57.83ms
step:1939/2330 train_time:112130ms step_avg:57.83ms
step:1940/2330 train_time:112190ms step_avg:57.83ms
step:1941/2330 train_time:112247ms step_avg:57.83ms
step:1942/2330 train_time:112307ms step_avg:57.83ms
step:1943/2330 train_time:112364ms step_avg:57.83ms
step:1944/2330 train_time:112424ms step_avg:57.83ms
step:1945/2330 train_time:112480ms step_avg:57.83ms
step:1946/2330 train_time:112540ms step_avg:57.83ms
step:1947/2330 train_time:112597ms step_avg:57.83ms
step:1948/2330 train_time:112656ms step_avg:57.83ms
step:1949/2330 train_time:112713ms step_avg:57.83ms
step:1950/2330 train_time:112774ms step_avg:57.83ms
step:1951/2330 train_time:112829ms step_avg:57.83ms
step:1952/2330 train_time:112891ms step_avg:57.83ms
step:1953/2330 train_time:112947ms step_avg:57.83ms
step:1954/2330 train_time:113007ms step_avg:57.83ms
step:1955/2330 train_time:113064ms step_avg:57.83ms
step:1956/2330 train_time:113124ms step_avg:57.83ms
step:1957/2330 train_time:113181ms step_avg:57.83ms
step:1958/2330 train_time:113240ms step_avg:57.83ms
step:1959/2330 train_time:113296ms step_avg:57.83ms
step:1960/2330 train_time:113356ms step_avg:57.83ms
step:1961/2330 train_time:113412ms step_avg:57.83ms
step:1962/2330 train_time:113473ms step_avg:57.84ms
step:1963/2330 train_time:113529ms step_avg:57.83ms
step:1964/2330 train_time:113591ms step_avg:57.84ms
step:1965/2330 train_time:113648ms step_avg:57.84ms
step:1966/2330 train_time:113708ms step_avg:57.84ms
step:1967/2330 train_time:113765ms step_avg:57.84ms
step:1968/2330 train_time:113825ms step_avg:57.84ms
step:1969/2330 train_time:113881ms step_avg:57.84ms
step:1970/2330 train_time:113941ms step_avg:57.84ms
step:1971/2330 train_time:113997ms step_avg:57.84ms
step:1972/2330 train_time:114058ms step_avg:57.84ms
step:1973/2330 train_time:114114ms step_avg:57.84ms
step:1974/2330 train_time:114175ms step_avg:57.84ms
step:1975/2330 train_time:114231ms step_avg:57.84ms
step:1976/2330 train_time:114291ms step_avg:57.84ms
step:1977/2330 train_time:114348ms step_avg:57.84ms
step:1978/2330 train_time:114407ms step_avg:57.84ms
step:1979/2330 train_time:114465ms step_avg:57.84ms
step:1980/2330 train_time:114524ms step_avg:57.84ms
step:1981/2330 train_time:114581ms step_avg:57.84ms
step:1982/2330 train_time:114640ms step_avg:57.84ms
step:1983/2330 train_time:114697ms step_avg:57.84ms
step:1984/2330 train_time:114756ms step_avg:57.84ms
step:1985/2330 train_time:114813ms step_avg:57.84ms
step:1986/2330 train_time:114873ms step_avg:57.84ms
step:1987/2330 train_time:114930ms step_avg:57.84ms
step:1988/2330 train_time:114991ms step_avg:57.84ms
step:1989/2330 train_time:115048ms step_avg:57.84ms
step:1990/2330 train_time:115107ms step_avg:57.84ms
step:1991/2330 train_time:115163ms step_avg:57.84ms
step:1992/2330 train_time:115224ms step_avg:57.84ms
step:1993/2330 train_time:115280ms step_avg:57.84ms
step:1994/2330 train_time:115340ms step_avg:57.84ms
step:1995/2330 train_time:115397ms step_avg:57.84ms
step:1996/2330 train_time:115457ms step_avg:57.84ms
step:1997/2330 train_time:115513ms step_avg:57.84ms
step:1998/2330 train_time:115574ms step_avg:57.84ms
step:1999/2330 train_time:115630ms step_avg:57.84ms
step:2000/2330 train_time:115691ms step_avg:57.85ms
step:2000/2330 val_loss:4.0751 train_time:115771ms step_avg:57.89ms
step:2001/2330 train_time:115791ms step_avg:57.87ms
step:2002/2330 train_time:115812ms step_avg:57.85ms
step:2003/2330 train_time:115868ms step_avg:57.85ms
step:2004/2330 train_time:115932ms step_avg:57.85ms
step:2005/2330 train_time:115989ms step_avg:57.85ms
step:2006/2330 train_time:116050ms step_avg:57.85ms
step:2007/2330 train_time:116106ms step_avg:57.85ms
step:2008/2330 train_time:116167ms step_avg:57.85ms
step:2009/2330 train_time:116223ms step_avg:57.85ms
step:2010/2330 train_time:116283ms step_avg:57.85ms
step:2011/2330 train_time:116339ms step_avg:57.85ms
step:2012/2330 train_time:116398ms step_avg:57.85ms
step:2013/2330 train_time:116453ms step_avg:57.85ms
step:2014/2330 train_time:116513ms step_avg:57.85ms
step:2015/2330 train_time:116569ms step_avg:57.85ms
step:2016/2330 train_time:116628ms step_avg:57.85ms
step:2017/2330 train_time:116684ms step_avg:57.85ms
step:2018/2330 train_time:116744ms step_avg:57.85ms
step:2019/2330 train_time:116801ms step_avg:57.85ms
step:2020/2330 train_time:116863ms step_avg:57.85ms
step:2021/2330 train_time:116921ms step_avg:57.85ms
step:2022/2330 train_time:116982ms step_avg:57.85ms
step:2023/2330 train_time:117039ms step_avg:57.85ms
step:2024/2330 train_time:117100ms step_avg:57.86ms
step:2025/2330 train_time:117157ms step_avg:57.86ms
step:2026/2330 train_time:117217ms step_avg:57.86ms
step:2027/2330 train_time:117274ms step_avg:57.86ms
step:2028/2330 train_time:117333ms step_avg:57.86ms
step:2029/2330 train_time:117389ms step_avg:57.86ms
step:2030/2330 train_time:117448ms step_avg:57.86ms
step:2031/2330 train_time:117504ms step_avg:57.86ms
step:2032/2330 train_time:117564ms step_avg:57.86ms
step:2033/2330 train_time:117621ms step_avg:57.86ms
step:2034/2330 train_time:117681ms step_avg:57.86ms
step:2035/2330 train_time:117737ms step_avg:57.86ms
step:2036/2330 train_time:117797ms step_avg:57.86ms
step:2037/2330 train_time:117854ms step_avg:57.86ms
step:2038/2330 train_time:117914ms step_avg:57.86ms
step:2039/2330 train_time:117970ms step_avg:57.86ms
step:2040/2330 train_time:118031ms step_avg:57.86ms
step:2041/2330 train_time:118088ms step_avg:57.86ms
step:2042/2330 train_time:118148ms step_avg:57.86ms
step:2043/2330 train_time:118204ms step_avg:57.86ms
step:2044/2330 train_time:118265ms step_avg:57.86ms
step:2045/2330 train_time:118322ms step_avg:57.86ms
step:2046/2330 train_time:118383ms step_avg:57.86ms
step:2047/2330 train_time:118439ms step_avg:57.86ms
step:2048/2330 train_time:118499ms step_avg:57.86ms
step:2049/2330 train_time:118555ms step_avg:57.86ms
step:2050/2330 train_time:118614ms step_avg:57.86ms
step:2051/2330 train_time:118671ms step_avg:57.86ms
step:2052/2330 train_time:118730ms step_avg:57.86ms
step:2053/2330 train_time:118786ms step_avg:57.86ms
step:2054/2330 train_time:118846ms step_avg:57.86ms
step:2055/2330 train_time:118903ms step_avg:57.86ms
step:2056/2330 train_time:118964ms step_avg:57.86ms
step:2057/2330 train_time:119020ms step_avg:57.86ms
step:2058/2330 train_time:119081ms step_avg:57.86ms
step:2059/2330 train_time:119138ms step_avg:57.86ms
step:2060/2330 train_time:119198ms step_avg:57.86ms
step:2061/2330 train_time:119255ms step_avg:57.86ms
step:2062/2330 train_time:119314ms step_avg:57.86ms
step:2063/2330 train_time:119371ms step_avg:57.86ms
step:2064/2330 train_time:119430ms step_avg:57.86ms
step:2065/2330 train_time:119486ms step_avg:57.86ms
step:2066/2330 train_time:119546ms step_avg:57.86ms
step:2067/2330 train_time:119603ms step_avg:57.86ms
step:2068/2330 train_time:119663ms step_avg:57.86ms
step:2069/2330 train_time:119720ms step_avg:57.86ms
step:2070/2330 train_time:119780ms step_avg:57.86ms
step:2071/2330 train_time:119836ms step_avg:57.86ms
step:2072/2330 train_time:119896ms step_avg:57.86ms
step:2073/2330 train_time:119953ms step_avg:57.86ms
step:2074/2330 train_time:120012ms step_avg:57.87ms
step:2075/2330 train_time:120069ms step_avg:57.86ms
step:2076/2330 train_time:120129ms step_avg:57.87ms
step:2077/2330 train_time:120186ms step_avg:57.87ms
step:2078/2330 train_time:120246ms step_avg:57.87ms
step:2079/2330 train_time:120302ms step_avg:57.87ms
step:2080/2330 train_time:120363ms step_avg:57.87ms
step:2081/2330 train_time:120420ms step_avg:57.87ms
step:2082/2330 train_time:120481ms step_avg:57.87ms
step:2083/2330 train_time:120537ms step_avg:57.87ms
step:2084/2330 train_time:120597ms step_avg:57.87ms
step:2085/2330 train_time:120653ms step_avg:57.87ms
step:2086/2330 train_time:120713ms step_avg:57.87ms
step:2087/2330 train_time:120770ms step_avg:57.87ms
step:2088/2330 train_time:120829ms step_avg:57.87ms
step:2089/2330 train_time:120885ms step_avg:57.87ms
step:2090/2330 train_time:120945ms step_avg:57.87ms
step:2091/2330 train_time:121002ms step_avg:57.87ms
step:2092/2330 train_time:121062ms step_avg:57.87ms
step:2093/2330 train_time:121120ms step_avg:57.87ms
step:2094/2330 train_time:121180ms step_avg:57.87ms
step:2095/2330 train_time:121237ms step_avg:57.87ms
step:2096/2330 train_time:121297ms step_avg:57.87ms
step:2097/2330 train_time:121354ms step_avg:57.87ms
step:2098/2330 train_time:121413ms step_avg:57.87ms
step:2099/2330 train_time:121469ms step_avg:57.87ms
step:2100/2330 train_time:121529ms step_avg:57.87ms
step:2101/2330 train_time:121585ms step_avg:57.87ms
step:2102/2330 train_time:121646ms step_avg:57.87ms
step:2103/2330 train_time:121702ms step_avg:57.87ms
step:2104/2330 train_time:121763ms step_avg:57.87ms
step:2105/2330 train_time:121820ms step_avg:57.87ms
step:2106/2330 train_time:121880ms step_avg:57.87ms
step:2107/2330 train_time:121937ms step_avg:57.87ms
step:2108/2330 train_time:121997ms step_avg:57.87ms
step:2109/2330 train_time:122055ms step_avg:57.87ms
step:2110/2330 train_time:122114ms step_avg:57.87ms
step:2111/2330 train_time:122171ms step_avg:57.87ms
step:2112/2330 train_time:122230ms step_avg:57.87ms
step:2113/2330 train_time:122287ms step_avg:57.87ms
step:2114/2330 train_time:122348ms step_avg:57.88ms
step:2115/2330 train_time:122404ms step_avg:57.87ms
step:2116/2330 train_time:122464ms step_avg:57.88ms
step:2117/2330 train_time:122521ms step_avg:57.87ms
step:2118/2330 train_time:122580ms step_avg:57.88ms
step:2119/2330 train_time:122637ms step_avg:57.87ms
step:2120/2330 train_time:122696ms step_avg:57.88ms
step:2121/2330 train_time:122752ms step_avg:57.87ms
step:2122/2330 train_time:122813ms step_avg:57.88ms
step:2123/2330 train_time:122869ms step_avg:57.88ms
step:2124/2330 train_time:122929ms step_avg:57.88ms
step:2125/2330 train_time:122986ms step_avg:57.88ms
step:2126/2330 train_time:123046ms step_avg:57.88ms
step:2127/2330 train_time:123103ms step_avg:57.88ms
step:2128/2330 train_time:123163ms step_avg:57.88ms
step:2129/2330 train_time:123220ms step_avg:57.88ms
step:2130/2330 train_time:123280ms step_avg:57.88ms
step:2131/2330 train_time:123337ms step_avg:57.88ms
step:2132/2330 train_time:123398ms step_avg:57.88ms
step:2133/2330 train_time:123454ms step_avg:57.88ms
step:2134/2330 train_time:123515ms step_avg:57.88ms
step:2135/2330 train_time:123571ms step_avg:57.88ms
step:2136/2330 train_time:123630ms step_avg:57.88ms
step:2137/2330 train_time:123687ms step_avg:57.88ms
step:2138/2330 train_time:123747ms step_avg:57.88ms
step:2139/2330 train_time:123803ms step_avg:57.88ms
step:2140/2330 train_time:123863ms step_avg:57.88ms
step:2141/2330 train_time:123920ms step_avg:57.88ms
step:2142/2330 train_time:123980ms step_avg:57.88ms
step:2143/2330 train_time:124038ms step_avg:57.88ms
step:2144/2330 train_time:124097ms step_avg:57.88ms
step:2145/2330 train_time:124154ms step_avg:57.88ms
step:2146/2330 train_time:124213ms step_avg:57.88ms
step:2147/2330 train_time:124270ms step_avg:57.88ms
step:2148/2330 train_time:124329ms step_avg:57.88ms
step:2149/2330 train_time:124386ms step_avg:57.88ms
step:2150/2330 train_time:124446ms step_avg:57.88ms
step:2151/2330 train_time:124503ms step_avg:57.88ms
step:2152/2330 train_time:124563ms step_avg:57.88ms
step:2153/2330 train_time:124620ms step_avg:57.88ms
step:2154/2330 train_time:124680ms step_avg:57.88ms
step:2155/2330 train_time:124736ms step_avg:57.88ms
step:2156/2330 train_time:124796ms step_avg:57.88ms
step:2157/2330 train_time:124852ms step_avg:57.88ms
step:2158/2330 train_time:124912ms step_avg:57.88ms
step:2159/2330 train_time:124968ms step_avg:57.88ms
step:2160/2330 train_time:125028ms step_avg:57.88ms
step:2161/2330 train_time:125084ms step_avg:57.88ms
step:2162/2330 train_time:125146ms step_avg:57.88ms
step:2163/2330 train_time:125203ms step_avg:57.88ms
step:2164/2330 train_time:125262ms step_avg:57.88ms
step:2165/2330 train_time:125319ms step_avg:57.88ms
step:2166/2330 train_time:125380ms step_avg:57.89ms
step:2167/2330 train_time:125437ms step_avg:57.89ms
step:2168/2330 train_time:125496ms step_avg:57.89ms
step:2169/2330 train_time:125552ms step_avg:57.88ms
step:2170/2330 train_time:125613ms step_avg:57.89ms
step:2171/2330 train_time:125669ms step_avg:57.89ms
step:2172/2330 train_time:125729ms step_avg:57.89ms
step:2173/2330 train_time:125785ms step_avg:57.89ms
step:2174/2330 train_time:125845ms step_avg:57.89ms
step:2175/2330 train_time:125902ms step_avg:57.89ms
step:2176/2330 train_time:125962ms step_avg:57.89ms
step:2177/2330 train_time:126019ms step_avg:57.89ms
step:2178/2330 train_time:126079ms step_avg:57.89ms
step:2179/2330 train_time:126136ms step_avg:57.89ms
step:2180/2330 train_time:126196ms step_avg:57.89ms
step:2181/2330 train_time:126252ms step_avg:57.89ms
step:2182/2330 train_time:126313ms step_avg:57.89ms
step:2183/2330 train_time:126369ms step_avg:57.89ms
step:2184/2330 train_time:126430ms step_avg:57.89ms
step:2185/2330 train_time:126486ms step_avg:57.89ms
step:2186/2330 train_time:126547ms step_avg:57.89ms
step:2187/2330 train_time:126603ms step_avg:57.89ms
step:2188/2330 train_time:126664ms step_avg:57.89ms
step:2189/2330 train_time:126720ms step_avg:57.89ms
step:2190/2330 train_time:126780ms step_avg:57.89ms
step:2191/2330 train_time:126837ms step_avg:57.89ms
step:2192/2330 train_time:126897ms step_avg:57.89ms
step:2193/2330 train_time:126954ms step_avg:57.89ms
step:2194/2330 train_time:127013ms step_avg:57.89ms
step:2195/2330 train_time:127069ms step_avg:57.89ms
step:2196/2330 train_time:127129ms step_avg:57.89ms
step:2197/2330 train_time:127185ms step_avg:57.89ms
step:2198/2330 train_time:127245ms step_avg:57.89ms
step:2199/2330 train_time:127303ms step_avg:57.89ms
step:2200/2330 train_time:127362ms step_avg:57.89ms
step:2201/2330 train_time:127419ms step_avg:57.89ms
step:2202/2330 train_time:127479ms step_avg:57.89ms
step:2203/2330 train_time:127536ms step_avg:57.89ms
step:2204/2330 train_time:127595ms step_avg:57.89ms
step:2205/2330 train_time:127652ms step_avg:57.89ms
step:2206/2330 train_time:127712ms step_avg:57.89ms
step:2207/2330 train_time:127768ms step_avg:57.89ms
step:2208/2330 train_time:127828ms step_avg:57.89ms
step:2209/2330 train_time:127885ms step_avg:57.89ms
step:2210/2330 train_time:127945ms step_avg:57.89ms
step:2211/2330 train_time:128002ms step_avg:57.89ms
step:2212/2330 train_time:128063ms step_avg:57.89ms
step:2213/2330 train_time:128120ms step_avg:57.89ms
step:2214/2330 train_time:128180ms step_avg:57.90ms
step:2215/2330 train_time:128238ms step_avg:57.90ms
step:2216/2330 train_time:128298ms step_avg:57.90ms
step:2217/2330 train_time:128354ms step_avg:57.90ms
step:2218/2330 train_time:128414ms step_avg:57.90ms
step:2219/2330 train_time:128471ms step_avg:57.90ms
step:2220/2330 train_time:128530ms step_avg:57.90ms
step:2221/2330 train_time:128586ms step_avg:57.90ms
step:2222/2330 train_time:128646ms step_avg:57.90ms
step:2223/2330 train_time:128703ms step_avg:57.90ms
step:2224/2330 train_time:128762ms step_avg:57.90ms
step:2225/2330 train_time:128819ms step_avg:57.90ms
step:2226/2330 train_time:128879ms step_avg:57.90ms
step:2227/2330 train_time:128936ms step_avg:57.90ms
step:2228/2330 train_time:128996ms step_avg:57.90ms
step:2229/2330 train_time:129052ms step_avg:57.90ms
step:2230/2330 train_time:129111ms step_avg:57.90ms
step:2231/2330 train_time:129167ms step_avg:57.90ms
step:2232/2330 train_time:129228ms step_avg:57.90ms
step:2233/2330 train_time:129285ms step_avg:57.90ms
step:2234/2330 train_time:129345ms step_avg:57.90ms
step:2235/2330 train_time:129401ms step_avg:57.90ms
step:2236/2330 train_time:129462ms step_avg:57.90ms
step:2237/2330 train_time:129519ms step_avg:57.90ms
step:2238/2330 train_time:129578ms step_avg:57.90ms
step:2239/2330 train_time:129635ms step_avg:57.90ms
step:2240/2330 train_time:129694ms step_avg:57.90ms
step:2241/2330 train_time:129750ms step_avg:57.90ms
step:2242/2330 train_time:129810ms step_avg:57.90ms
step:2243/2330 train_time:129866ms step_avg:57.90ms
step:2244/2330 train_time:129927ms step_avg:57.90ms
step:2245/2330 train_time:129983ms step_avg:57.90ms
step:2246/2330 train_time:130043ms step_avg:57.90ms
step:2247/2330 train_time:130099ms step_avg:57.90ms
step:2248/2330 train_time:130160ms step_avg:57.90ms
step:2249/2330 train_time:130217ms step_avg:57.90ms
step:2250/2330 train_time:130277ms step_avg:57.90ms
step:2250/2330 val_loss:4.0283 train_time:130359ms step_avg:57.94ms
step:2251/2330 train_time:130376ms step_avg:57.92ms
step:2252/2330 train_time:130398ms step_avg:57.90ms
step:2253/2330 train_time:130457ms step_avg:57.90ms
step:2254/2330 train_time:130523ms step_avg:57.91ms
step:2255/2330 train_time:130579ms step_avg:57.91ms
step:2256/2330 train_time:130641ms step_avg:57.91ms
step:2257/2330 train_time:130696ms step_avg:57.91ms
step:2258/2330 train_time:130757ms step_avg:57.91ms
step:2259/2330 train_time:130813ms step_avg:57.91ms
step:2260/2330 train_time:130873ms step_avg:57.91ms
step:2261/2330 train_time:130929ms step_avg:57.91ms
step:2262/2330 train_time:130989ms step_avg:57.91ms
step:2263/2330 train_time:131045ms step_avg:57.91ms
step:2264/2330 train_time:131105ms step_avg:57.91ms
step:2265/2330 train_time:131160ms step_avg:57.91ms
step:2266/2330 train_time:131220ms step_avg:57.91ms
step:2267/2330 train_time:131275ms step_avg:57.91ms
step:2268/2330 train_time:131337ms step_avg:57.91ms
step:2269/2330 train_time:131394ms step_avg:57.91ms
step:2270/2330 train_time:131458ms step_avg:57.91ms
step:2271/2330 train_time:131515ms step_avg:57.91ms
step:2272/2330 train_time:131577ms step_avg:57.91ms
step:2273/2330 train_time:131634ms step_avg:57.91ms
step:2274/2330 train_time:131695ms step_avg:57.91ms
step:2275/2330 train_time:131751ms step_avg:57.91ms
step:2276/2330 train_time:131811ms step_avg:57.91ms
step:2277/2330 train_time:131868ms step_avg:57.91ms
step:2278/2330 train_time:131927ms step_avg:57.91ms
step:2279/2330 train_time:131983ms step_avg:57.91ms
step:2280/2330 train_time:132043ms step_avg:57.91ms
step:2281/2330 train_time:132099ms step_avg:57.91ms
step:2282/2330 train_time:132159ms step_avg:57.91ms
step:2283/2330 train_time:132215ms step_avg:57.91ms
step:2284/2330 train_time:132275ms step_avg:57.91ms
step:2285/2330 train_time:132331ms step_avg:57.91ms
step:2286/2330 train_time:132392ms step_avg:57.91ms
step:2287/2330 train_time:132449ms step_avg:57.91ms
step:2288/2330 train_time:132511ms step_avg:57.92ms
step:2289/2330 train_time:132568ms step_avg:57.92ms
step:2290/2330 train_time:132629ms step_avg:57.92ms
step:2291/2330 train_time:132685ms step_avg:57.92ms
step:2292/2330 train_time:132745ms step_avg:57.92ms
step:2293/2330 train_time:132802ms step_avg:57.92ms
step:2294/2330 train_time:132862ms step_avg:57.92ms
step:2295/2330 train_time:132918ms step_avg:57.92ms
step:2296/2330 train_time:132978ms step_avg:57.92ms
step:2297/2330 train_time:133035ms step_avg:57.92ms
step:2298/2330 train_time:133095ms step_avg:57.92ms
step:2299/2330 train_time:133151ms step_avg:57.92ms
step:2300/2330 train_time:133211ms step_avg:57.92ms
step:2301/2330 train_time:133269ms step_avg:57.92ms
step:2302/2330 train_time:133328ms step_avg:57.92ms
step:2303/2330 train_time:133385ms step_avg:57.92ms
step:2304/2330 train_time:133444ms step_avg:57.92ms
step:2305/2330 train_time:133501ms step_avg:57.92ms
step:2306/2330 train_time:133561ms step_avg:57.92ms
step:2307/2330 train_time:133618ms step_avg:57.92ms
step:2308/2330 train_time:133678ms step_avg:57.92ms
step:2309/2330 train_time:133735ms step_avg:57.92ms
step:2310/2330 train_time:133795ms step_avg:57.92ms
step:2311/2330 train_time:133853ms step_avg:57.92ms
step:2312/2330 train_time:133912ms step_avg:57.92ms
step:2313/2330 train_time:133969ms step_avg:57.92ms
step:2314/2330 train_time:134028ms step_avg:57.92ms
step:2315/2330 train_time:134085ms step_avg:57.92ms
step:2316/2330 train_time:134144ms step_avg:57.92ms
step:2317/2330 train_time:134201ms step_avg:57.92ms
step:2318/2330 train_time:134261ms step_avg:57.92ms
step:2319/2330 train_time:134317ms step_avg:57.92ms
step:2320/2330 train_time:134377ms step_avg:57.92ms
step:2321/2330 train_time:134434ms step_avg:57.92ms
step:2322/2330 train_time:134494ms step_avg:57.92ms
step:2323/2330 train_time:134551ms step_avg:57.92ms
step:2324/2330 train_time:134611ms step_avg:57.92ms
step:2325/2330 train_time:134668ms step_avg:57.92ms
step:2326/2330 train_time:134727ms step_avg:57.92ms
step:2327/2330 train_time:134784ms step_avg:57.92ms
step:2328/2330 train_time:134844ms step_avg:57.92ms
step:2329/2330 train_time:134901ms step_avg:57.92ms
step:2330/2330 train_time:134960ms step_avg:57.92ms
step:2330/2330 val_loss:4.0150 train_time:135041ms step_avg:57.96ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
