import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--adamw_lr', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_lr_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_lr_tuning/{args2.adamw_lr}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=float(args2.adamw_lr),  betas=(0.85, 0.85), eps=1e-10, weight_decay=0.0, fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_lr_tuning/{args2.adamw_lr}", exist_ok=True)
            torch.save(log, f"logs_adamw_small_lr_tuning/{args2.adamw_lr}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 01:08:44 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   35C    P0             118W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   35C    P0             117W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             118W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   28C    P0             115W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   34C    P0             118W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             114W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:91ms step_avg:90.95ms
step:2/2330 train_time:181ms step_avg:90.26ms
step:3/2330 train_time:200ms step_avg:66.69ms
step:4/2330 train_time:220ms step_avg:54.94ms
step:5/2330 train_time:272ms step_avg:54.31ms
step:6/2330 train_time:330ms step_avg:54.92ms
step:7/2330 train_time:385ms step_avg:55.01ms
step:8/2330 train_time:443ms step_avg:55.39ms
step:9/2330 train_time:498ms step_avg:55.39ms
step:10/2330 train_time:558ms step_avg:55.77ms
step:11/2330 train_time:614ms step_avg:55.79ms
step:12/2330 train_time:672ms step_avg:56.02ms
step:13/2330 train_time:728ms step_avg:55.99ms
step:14/2330 train_time:786ms step_avg:56.17ms
step:15/2330 train_time:842ms step_avg:56.16ms
step:16/2330 train_time:902ms step_avg:56.35ms
step:17/2330 train_time:957ms step_avg:56.30ms
step:18/2330 train_time:1018ms step_avg:56.55ms
step:19/2330 train_time:1077ms step_avg:56.68ms
step:20/2330 train_time:1139ms step_avg:56.95ms
step:21/2330 train_time:1197ms step_avg:57.01ms
step:22/2330 train_time:1257ms step_avg:57.13ms
step:23/2330 train_time:1314ms step_avg:57.12ms
step:24/2330 train_time:1372ms step_avg:57.19ms
step:25/2330 train_time:1429ms step_avg:57.17ms
step:26/2330 train_time:1488ms step_avg:57.23ms
step:27/2330 train_time:1543ms step_avg:57.17ms
step:28/2330 train_time:1603ms step_avg:57.23ms
step:29/2330 train_time:1658ms step_avg:57.18ms
step:30/2330 train_time:1718ms step_avg:57.26ms
step:31/2330 train_time:1773ms step_avg:57.20ms
step:32/2330 train_time:1833ms step_avg:57.27ms
step:33/2330 train_time:1889ms step_avg:57.23ms
step:34/2330 train_time:1947ms step_avg:57.26ms
step:35/2330 train_time:2004ms step_avg:57.25ms
step:36/2330 train_time:2064ms step_avg:57.33ms
step:37/2330 train_time:2121ms step_avg:57.33ms
step:38/2330 train_time:2182ms step_avg:57.42ms
step:39/2330 train_time:2239ms step_avg:57.41ms
step:40/2330 train_time:2300ms step_avg:57.50ms
step:41/2330 train_time:2356ms step_avg:57.46ms
step:42/2330 train_time:2416ms step_avg:57.52ms
step:43/2330 train_time:2472ms step_avg:57.49ms
step:44/2330 train_time:2531ms step_avg:57.53ms
step:45/2330 train_time:2588ms step_avg:57.50ms
step:46/2330 train_time:2646ms step_avg:57.51ms
step:47/2330 train_time:2701ms step_avg:57.46ms
step:48/2330 train_time:2760ms step_avg:57.50ms
step:49/2330 train_time:2816ms step_avg:57.47ms
step:50/2330 train_time:2875ms step_avg:57.50ms
step:51/2330 train_time:2932ms step_avg:57.48ms
step:52/2330 train_time:2991ms step_avg:57.51ms
step:53/2330 train_time:3048ms step_avg:57.51ms
step:54/2330 train_time:3107ms step_avg:57.54ms
step:55/2330 train_time:3163ms step_avg:57.52ms
step:56/2330 train_time:3224ms step_avg:57.58ms
step:57/2330 train_time:3280ms step_avg:57.55ms
step:58/2330 train_time:3341ms step_avg:57.61ms
step:59/2330 train_time:3397ms step_avg:57.57ms
step:60/2330 train_time:3458ms step_avg:57.63ms
step:61/2330 train_time:3514ms step_avg:57.60ms
step:62/2330 train_time:3573ms step_avg:57.63ms
step:63/2330 train_time:3629ms step_avg:57.60ms
step:64/2330 train_time:3688ms step_avg:57.63ms
step:65/2330 train_time:3744ms step_avg:57.61ms
step:66/2330 train_time:3804ms step_avg:57.63ms
step:67/2330 train_time:3859ms step_avg:57.60ms
step:68/2330 train_time:3919ms step_avg:57.63ms
step:69/2330 train_time:3975ms step_avg:57.61ms
step:70/2330 train_time:4035ms step_avg:57.64ms
step:71/2330 train_time:4092ms step_avg:57.63ms
step:72/2330 train_time:4151ms step_avg:57.65ms
step:73/2330 train_time:4208ms step_avg:57.64ms
step:74/2330 train_time:4267ms step_avg:57.66ms
step:75/2330 train_time:4324ms step_avg:57.65ms
step:76/2330 train_time:4383ms step_avg:57.68ms
step:77/2330 train_time:4439ms step_avg:57.65ms
step:78/2330 train_time:4500ms step_avg:57.69ms
step:79/2330 train_time:4555ms step_avg:57.66ms
step:80/2330 train_time:4615ms step_avg:57.68ms
step:81/2330 train_time:4670ms step_avg:57.66ms
step:82/2330 train_time:4729ms step_avg:57.68ms
step:83/2330 train_time:4786ms step_avg:57.66ms
step:84/2330 train_time:4845ms step_avg:57.67ms
step:85/2330 train_time:4901ms step_avg:57.65ms
step:86/2330 train_time:4960ms step_avg:57.68ms
step:87/2330 train_time:5017ms step_avg:57.66ms
step:88/2330 train_time:5077ms step_avg:57.69ms
step:89/2330 train_time:5133ms step_avg:57.68ms
step:90/2330 train_time:5194ms step_avg:57.71ms
step:91/2330 train_time:5250ms step_avg:57.69ms
step:92/2330 train_time:5310ms step_avg:57.71ms
step:93/2330 train_time:5366ms step_avg:57.70ms
step:94/2330 train_time:5425ms step_avg:57.71ms
step:95/2330 train_time:5481ms step_avg:57.69ms
step:96/2330 train_time:5541ms step_avg:57.72ms
step:97/2330 train_time:5597ms step_avg:57.70ms
step:98/2330 train_time:5656ms step_avg:57.72ms
step:99/2330 train_time:5713ms step_avg:57.71ms
step:100/2330 train_time:5772ms step_avg:57.72ms
step:101/2330 train_time:5828ms step_avg:57.70ms
step:102/2330 train_time:5886ms step_avg:57.71ms
step:103/2330 train_time:5942ms step_avg:57.69ms
step:104/2330 train_time:6001ms step_avg:57.71ms
step:105/2330 train_time:6057ms step_avg:57.69ms
step:106/2330 train_time:6117ms step_avg:57.71ms
step:107/2330 train_time:6174ms step_avg:57.70ms
step:108/2330 train_time:6233ms step_avg:57.71ms
step:109/2330 train_time:6290ms step_avg:57.71ms
step:110/2330 train_time:6349ms step_avg:57.72ms
step:111/2330 train_time:6407ms step_avg:57.72ms
step:112/2330 train_time:6466ms step_avg:57.73ms
step:113/2330 train_time:6522ms step_avg:57.71ms
step:114/2330 train_time:6582ms step_avg:57.74ms
step:115/2330 train_time:6638ms step_avg:57.72ms
step:116/2330 train_time:6698ms step_avg:57.74ms
step:117/2330 train_time:6754ms step_avg:57.73ms
step:118/2330 train_time:6813ms step_avg:57.73ms
step:119/2330 train_time:6870ms step_avg:57.73ms
step:120/2330 train_time:6929ms step_avg:57.74ms
step:121/2330 train_time:6985ms step_avg:57.73ms
step:122/2330 train_time:7043ms step_avg:57.73ms
step:123/2330 train_time:7100ms step_avg:57.72ms
step:124/2330 train_time:7159ms step_avg:57.74ms
step:125/2330 train_time:7216ms step_avg:57.73ms
step:126/2330 train_time:7275ms step_avg:57.74ms
step:127/2330 train_time:7332ms step_avg:57.73ms
step:128/2330 train_time:7391ms step_avg:57.74ms
step:129/2330 train_time:7448ms step_avg:57.74ms
step:130/2330 train_time:7507ms step_avg:57.74ms
step:131/2330 train_time:7563ms step_avg:57.73ms
step:132/2330 train_time:7622ms step_avg:57.74ms
step:133/2330 train_time:7678ms step_avg:57.73ms
step:134/2330 train_time:7738ms step_avg:57.75ms
step:135/2330 train_time:7794ms step_avg:57.74ms
step:136/2330 train_time:7853ms step_avg:57.74ms
step:137/2330 train_time:7910ms step_avg:57.73ms
step:138/2330 train_time:7968ms step_avg:57.74ms
step:139/2330 train_time:8024ms step_avg:57.73ms
step:140/2330 train_time:8084ms step_avg:57.74ms
step:141/2330 train_time:8140ms step_avg:57.73ms
step:142/2330 train_time:8199ms step_avg:57.74ms
step:143/2330 train_time:8255ms step_avg:57.73ms
step:144/2330 train_time:8315ms step_avg:57.74ms
step:145/2330 train_time:8372ms step_avg:57.74ms
step:146/2330 train_time:8432ms step_avg:57.75ms
step:147/2330 train_time:8489ms step_avg:57.75ms
step:148/2330 train_time:8548ms step_avg:57.75ms
step:149/2330 train_time:8604ms step_avg:57.75ms
step:150/2330 train_time:8663ms step_avg:57.75ms
step:151/2330 train_time:8719ms step_avg:57.74ms
step:152/2330 train_time:8779ms step_avg:57.76ms
step:153/2330 train_time:8835ms step_avg:57.75ms
step:154/2330 train_time:8895ms step_avg:57.76ms
step:155/2330 train_time:8951ms step_avg:57.75ms
step:156/2330 train_time:9011ms step_avg:57.76ms
step:157/2330 train_time:9067ms step_avg:57.75ms
step:158/2330 train_time:9126ms step_avg:57.76ms
step:159/2330 train_time:9182ms step_avg:57.75ms
step:160/2330 train_time:9243ms step_avg:57.77ms
step:161/2330 train_time:9299ms step_avg:57.76ms
step:162/2330 train_time:9359ms step_avg:57.77ms
step:163/2330 train_time:9415ms step_avg:57.76ms
step:164/2330 train_time:9475ms step_avg:57.77ms
step:165/2330 train_time:9533ms step_avg:57.77ms
step:166/2330 train_time:9592ms step_avg:57.78ms
step:167/2330 train_time:9649ms step_avg:57.78ms
step:168/2330 train_time:9707ms step_avg:57.78ms
step:169/2330 train_time:9763ms step_avg:57.77ms
step:170/2330 train_time:9824ms step_avg:57.79ms
step:171/2330 train_time:9880ms step_avg:57.78ms
step:172/2330 train_time:9940ms step_avg:57.79ms
step:173/2330 train_time:9996ms step_avg:57.78ms
step:174/2330 train_time:10055ms step_avg:57.79ms
step:175/2330 train_time:10111ms step_avg:57.78ms
step:176/2330 train_time:10171ms step_avg:57.79ms
step:177/2330 train_time:10228ms step_avg:57.79ms
step:178/2330 train_time:10287ms step_avg:57.79ms
step:179/2330 train_time:10343ms step_avg:57.78ms
step:180/2330 train_time:10403ms step_avg:57.79ms
step:181/2330 train_time:10459ms step_avg:57.78ms
step:182/2330 train_time:10520ms step_avg:57.80ms
step:183/2330 train_time:10576ms step_avg:57.79ms
step:184/2330 train_time:10636ms step_avg:57.80ms
step:185/2330 train_time:10693ms step_avg:57.80ms
step:186/2330 train_time:10752ms step_avg:57.81ms
step:187/2330 train_time:10809ms step_avg:57.80ms
step:188/2330 train_time:10867ms step_avg:57.80ms
step:189/2330 train_time:10924ms step_avg:57.80ms
step:190/2330 train_time:10982ms step_avg:57.80ms
step:191/2330 train_time:11039ms step_avg:57.80ms
step:192/2330 train_time:11098ms step_avg:57.80ms
step:193/2330 train_time:11155ms step_avg:57.80ms
step:194/2330 train_time:11214ms step_avg:57.81ms
step:195/2330 train_time:11272ms step_avg:57.80ms
step:196/2330 train_time:11331ms step_avg:57.81ms
step:197/2330 train_time:11387ms step_avg:57.80ms
step:198/2330 train_time:11446ms step_avg:57.81ms
step:199/2330 train_time:11503ms step_avg:57.80ms
step:200/2330 train_time:11562ms step_avg:57.81ms
step:201/2330 train_time:11618ms step_avg:57.80ms
step:202/2330 train_time:11678ms step_avg:57.81ms
step:203/2330 train_time:11734ms step_avg:57.81ms
step:204/2330 train_time:11794ms step_avg:57.81ms
step:205/2330 train_time:11850ms step_avg:57.81ms
step:206/2330 train_time:11910ms step_avg:57.82ms
step:207/2330 train_time:11967ms step_avg:57.81ms
step:208/2330 train_time:12025ms step_avg:57.81ms
step:209/2330 train_time:12082ms step_avg:57.81ms
step:210/2330 train_time:12141ms step_avg:57.82ms
step:211/2330 train_time:12198ms step_avg:57.81ms
step:212/2330 train_time:12257ms step_avg:57.82ms
step:213/2330 train_time:12314ms step_avg:57.81ms
step:214/2330 train_time:12373ms step_avg:57.82ms
step:215/2330 train_time:12430ms step_avg:57.81ms
step:216/2330 train_time:12489ms step_avg:57.82ms
step:217/2330 train_time:12545ms step_avg:57.81ms
step:218/2330 train_time:12605ms step_avg:57.82ms
step:219/2330 train_time:12661ms step_avg:57.81ms
step:220/2330 train_time:12721ms step_avg:57.82ms
step:221/2330 train_time:12776ms step_avg:57.81ms
step:222/2330 train_time:12837ms step_avg:57.82ms
step:223/2330 train_time:12894ms step_avg:57.82ms
step:224/2330 train_time:12953ms step_avg:57.82ms
step:225/2330 train_time:13009ms step_avg:57.82ms
step:226/2330 train_time:13068ms step_avg:57.82ms
step:227/2330 train_time:13124ms step_avg:57.82ms
step:228/2330 train_time:13185ms step_avg:57.83ms
step:229/2330 train_time:13241ms step_avg:57.82ms
step:230/2330 train_time:13301ms step_avg:57.83ms
step:231/2330 train_time:13356ms step_avg:57.82ms
step:232/2330 train_time:13417ms step_avg:57.83ms
step:233/2330 train_time:13473ms step_avg:57.82ms
step:234/2330 train_time:13533ms step_avg:57.83ms
step:235/2330 train_time:13589ms step_avg:57.83ms
step:236/2330 train_time:13648ms step_avg:57.83ms
step:237/2330 train_time:13704ms step_avg:57.82ms
step:238/2330 train_time:13764ms step_avg:57.83ms
step:239/2330 train_time:13820ms step_avg:57.83ms
step:240/2330 train_time:13880ms step_avg:57.83ms
step:241/2330 train_time:13937ms step_avg:57.83ms
step:242/2330 train_time:13996ms step_avg:57.83ms
step:243/2330 train_time:14053ms step_avg:57.83ms
step:244/2330 train_time:14111ms step_avg:57.83ms
step:245/2330 train_time:14167ms step_avg:57.83ms
step:246/2330 train_time:14226ms step_avg:57.83ms
step:247/2330 train_time:14282ms step_avg:57.82ms
step:248/2330 train_time:14343ms step_avg:57.83ms
step:249/2330 train_time:14399ms step_avg:57.83ms
step:250/2330 train_time:14459ms step_avg:57.83ms
step:250/2330 val_loss:4.9701 train_time:14538ms step_avg:58.15ms
step:251/2330 train_time:14557ms step_avg:58.00ms
step:252/2330 train_time:14577ms step_avg:57.84ms
step:253/2330 train_time:14633ms step_avg:57.84ms
step:254/2330 train_time:14695ms step_avg:57.86ms
step:255/2330 train_time:14752ms step_avg:57.85ms
step:256/2330 train_time:14816ms step_avg:57.88ms
step:257/2330 train_time:14872ms step_avg:57.87ms
step:258/2330 train_time:14932ms step_avg:57.87ms
step:259/2330 train_time:14987ms step_avg:57.87ms
step:260/2330 train_time:15047ms step_avg:57.87ms
step:261/2330 train_time:15103ms step_avg:57.87ms
step:262/2330 train_time:15162ms step_avg:57.87ms
step:263/2330 train_time:15218ms step_avg:57.86ms
step:264/2330 train_time:15277ms step_avg:57.87ms
step:265/2330 train_time:15332ms step_avg:57.86ms
step:266/2330 train_time:15390ms step_avg:57.86ms
step:267/2330 train_time:15446ms step_avg:57.85ms
step:268/2330 train_time:15507ms step_avg:57.86ms
step:269/2330 train_time:15564ms step_avg:57.86ms
step:270/2330 train_time:15624ms step_avg:57.87ms
step:271/2330 train_time:15681ms step_avg:57.86ms
step:272/2330 train_time:15742ms step_avg:57.88ms
step:273/2330 train_time:15800ms step_avg:57.88ms
step:274/2330 train_time:15860ms step_avg:57.88ms
step:275/2330 train_time:15916ms step_avg:57.88ms
step:276/2330 train_time:15976ms step_avg:57.88ms
step:277/2330 train_time:16032ms step_avg:57.88ms
step:278/2330 train_time:16090ms step_avg:57.88ms
step:279/2330 train_time:16146ms step_avg:57.87ms
step:280/2330 train_time:16205ms step_avg:57.88ms
step:281/2330 train_time:16261ms step_avg:57.87ms
step:282/2330 train_time:16320ms step_avg:57.87ms
step:283/2330 train_time:16376ms step_avg:57.87ms
step:284/2330 train_time:16435ms step_avg:57.87ms
step:285/2330 train_time:16492ms step_avg:57.87ms
step:286/2330 train_time:16550ms step_avg:57.87ms
step:287/2330 train_time:16607ms step_avg:57.87ms
step:288/2330 train_time:16667ms step_avg:57.87ms
step:289/2330 train_time:16723ms step_avg:57.86ms
step:290/2330 train_time:16786ms step_avg:57.88ms
step:291/2330 train_time:16842ms step_avg:57.88ms
step:292/2330 train_time:16903ms step_avg:57.89ms
step:293/2330 train_time:16959ms step_avg:57.88ms
step:294/2330 train_time:17018ms step_avg:57.89ms
step:295/2330 train_time:17075ms step_avg:57.88ms
step:296/2330 train_time:17134ms step_avg:57.88ms
step:297/2330 train_time:17190ms step_avg:57.88ms
step:298/2330 train_time:17249ms step_avg:57.88ms
step:299/2330 train_time:17305ms step_avg:57.88ms
step:300/2330 train_time:17365ms step_avg:57.88ms
step:301/2330 train_time:17421ms step_avg:57.88ms
step:302/2330 train_time:17480ms step_avg:57.88ms
step:303/2330 train_time:17537ms step_avg:57.88ms
step:304/2330 train_time:17596ms step_avg:57.88ms
step:305/2330 train_time:17652ms step_avg:57.88ms
step:306/2330 train_time:17711ms step_avg:57.88ms
step:307/2330 train_time:17768ms step_avg:57.88ms
step:308/2330 train_time:17829ms step_avg:57.89ms
step:309/2330 train_time:17885ms step_avg:57.88ms
step:310/2330 train_time:17947ms step_avg:57.89ms
step:311/2330 train_time:18003ms step_avg:57.89ms
step:312/2330 train_time:18063ms step_avg:57.89ms
step:313/2330 train_time:18119ms step_avg:57.89ms
step:314/2330 train_time:18179ms step_avg:57.89ms
step:315/2330 train_time:18235ms step_avg:57.89ms
step:316/2330 train_time:18293ms step_avg:57.89ms
step:317/2330 train_time:18350ms step_avg:57.89ms
step:318/2330 train_time:18408ms step_avg:57.89ms
step:319/2330 train_time:18464ms step_avg:57.88ms
step:320/2330 train_time:18525ms step_avg:57.89ms
step:321/2330 train_time:18581ms step_avg:57.88ms
step:322/2330 train_time:18641ms step_avg:57.89ms
step:323/2330 train_time:18698ms step_avg:57.89ms
step:324/2330 train_time:18758ms step_avg:57.89ms
step:325/2330 train_time:18815ms step_avg:57.89ms
step:326/2330 train_time:18874ms step_avg:57.90ms
step:327/2330 train_time:18931ms step_avg:57.89ms
step:328/2330 train_time:18991ms step_avg:57.90ms
step:329/2330 train_time:19048ms step_avg:57.90ms
step:330/2330 train_time:19107ms step_avg:57.90ms
step:331/2330 train_time:19163ms step_avg:57.89ms
step:332/2330 train_time:19223ms step_avg:57.90ms
step:333/2330 train_time:19280ms step_avg:57.90ms
step:334/2330 train_time:19338ms step_avg:57.90ms
step:335/2330 train_time:19395ms step_avg:57.89ms
step:336/2330 train_time:19453ms step_avg:57.90ms
step:337/2330 train_time:19510ms step_avg:57.89ms
step:338/2330 train_time:19570ms step_avg:57.90ms
step:339/2330 train_time:19626ms step_avg:57.89ms
step:340/2330 train_time:19686ms step_avg:57.90ms
step:341/2330 train_time:19742ms step_avg:57.90ms
step:342/2330 train_time:19802ms step_avg:57.90ms
step:343/2330 train_time:19859ms step_avg:57.90ms
step:344/2330 train_time:19919ms step_avg:57.90ms
step:345/2330 train_time:19977ms step_avg:57.90ms
step:346/2330 train_time:20036ms step_avg:57.91ms
step:347/2330 train_time:20093ms step_avg:57.90ms
step:348/2330 train_time:20151ms step_avg:57.91ms
step:349/2330 train_time:20208ms step_avg:57.90ms
step:350/2330 train_time:20267ms step_avg:57.91ms
step:351/2330 train_time:20323ms step_avg:57.90ms
step:352/2330 train_time:20382ms step_avg:57.90ms
step:353/2330 train_time:20439ms step_avg:57.90ms
step:354/2330 train_time:20499ms step_avg:57.91ms
step:355/2330 train_time:20556ms step_avg:57.90ms
step:356/2330 train_time:20616ms step_avg:57.91ms
step:357/2330 train_time:20673ms step_avg:57.91ms
step:358/2330 train_time:20731ms step_avg:57.91ms
step:359/2330 train_time:20788ms step_avg:57.90ms
step:360/2330 train_time:20846ms step_avg:57.91ms
step:361/2330 train_time:20902ms step_avg:57.90ms
step:362/2330 train_time:20963ms step_avg:57.91ms
step:363/2330 train_time:21020ms step_avg:57.91ms
step:364/2330 train_time:21079ms step_avg:57.91ms
step:365/2330 train_time:21135ms step_avg:57.90ms
step:366/2330 train_time:21194ms step_avg:57.91ms
step:367/2330 train_time:21251ms step_avg:57.90ms
step:368/2330 train_time:21309ms step_avg:57.91ms
step:369/2330 train_time:21366ms step_avg:57.90ms
step:370/2330 train_time:21424ms step_avg:57.90ms
step:371/2330 train_time:21481ms step_avg:57.90ms
step:372/2330 train_time:21539ms step_avg:57.90ms
step:373/2330 train_time:21596ms step_avg:57.90ms
step:374/2330 train_time:21655ms step_avg:57.90ms
step:375/2330 train_time:21713ms step_avg:57.90ms
step:376/2330 train_time:21772ms step_avg:57.90ms
step:377/2330 train_time:21828ms step_avg:57.90ms
step:378/2330 train_time:21889ms step_avg:57.91ms
step:379/2330 train_time:21946ms step_avg:57.90ms
step:380/2330 train_time:22005ms step_avg:57.91ms
step:381/2330 train_time:22061ms step_avg:57.90ms
step:382/2330 train_time:22122ms step_avg:57.91ms
step:383/2330 train_time:22179ms step_avg:57.91ms
step:384/2330 train_time:22239ms step_avg:57.91ms
step:385/2330 train_time:22296ms step_avg:57.91ms
step:386/2330 train_time:22355ms step_avg:57.91ms
step:387/2330 train_time:22411ms step_avg:57.91ms
step:388/2330 train_time:22470ms step_avg:57.91ms
step:389/2330 train_time:22525ms step_avg:57.91ms
step:390/2330 train_time:22585ms step_avg:57.91ms
step:391/2330 train_time:22642ms step_avg:57.91ms
step:392/2330 train_time:22701ms step_avg:57.91ms
step:393/2330 train_time:22758ms step_avg:57.91ms
step:394/2330 train_time:22818ms step_avg:57.91ms
step:395/2330 train_time:22875ms step_avg:57.91ms
step:396/2330 train_time:22934ms step_avg:57.91ms
step:397/2330 train_time:22990ms step_avg:57.91ms
step:398/2330 train_time:23049ms step_avg:57.91ms
step:399/2330 train_time:23106ms step_avg:57.91ms
step:400/2330 train_time:23165ms step_avg:57.91ms
step:401/2330 train_time:23221ms step_avg:57.91ms
step:402/2330 train_time:23282ms step_avg:57.91ms
step:403/2330 train_time:23338ms step_avg:57.91ms
step:404/2330 train_time:23397ms step_avg:57.91ms
step:405/2330 train_time:23454ms step_avg:57.91ms
step:406/2330 train_time:23513ms step_avg:57.91ms
step:407/2330 train_time:23570ms step_avg:57.91ms
step:408/2330 train_time:23629ms step_avg:57.91ms
step:409/2330 train_time:23686ms step_avg:57.91ms
step:410/2330 train_time:23745ms step_avg:57.91ms
step:411/2330 train_time:23801ms step_avg:57.91ms
step:412/2330 train_time:23861ms step_avg:57.91ms
step:413/2330 train_time:23917ms step_avg:57.91ms
step:414/2330 train_time:23977ms step_avg:57.91ms
step:415/2330 train_time:24033ms step_avg:57.91ms
step:416/2330 train_time:24092ms step_avg:57.91ms
step:417/2330 train_time:24148ms step_avg:57.91ms
step:418/2330 train_time:24207ms step_avg:57.91ms
step:419/2330 train_time:24264ms step_avg:57.91ms
step:420/2330 train_time:24324ms step_avg:57.91ms
step:421/2330 train_time:24380ms step_avg:57.91ms
step:422/2330 train_time:24440ms step_avg:57.91ms
step:423/2330 train_time:24497ms step_avg:57.91ms
step:424/2330 train_time:24556ms step_avg:57.91ms
step:425/2330 train_time:24612ms step_avg:57.91ms
step:426/2330 train_time:24671ms step_avg:57.91ms
step:427/2330 train_time:24727ms step_avg:57.91ms
step:428/2330 train_time:24787ms step_avg:57.91ms
step:429/2330 train_time:24843ms step_avg:57.91ms
step:430/2330 train_time:24903ms step_avg:57.91ms
step:431/2330 train_time:24960ms step_avg:57.91ms
step:432/2330 train_time:25019ms step_avg:57.91ms
step:433/2330 train_time:25076ms step_avg:57.91ms
step:434/2330 train_time:25135ms step_avg:57.92ms
step:435/2330 train_time:25193ms step_avg:57.91ms
step:436/2330 train_time:25251ms step_avg:57.92ms
step:437/2330 train_time:25307ms step_avg:57.91ms
step:438/2330 train_time:25368ms step_avg:57.92ms
step:439/2330 train_time:25423ms step_avg:57.91ms
step:440/2330 train_time:25484ms step_avg:57.92ms
step:441/2330 train_time:25539ms step_avg:57.91ms
step:442/2330 train_time:25600ms step_avg:57.92ms
step:443/2330 train_time:25657ms step_avg:57.92ms
step:444/2330 train_time:25716ms step_avg:57.92ms
step:445/2330 train_time:25772ms step_avg:57.92ms
step:446/2330 train_time:25831ms step_avg:57.92ms
step:447/2330 train_time:25887ms step_avg:57.91ms
step:448/2330 train_time:25947ms step_avg:57.92ms
step:449/2330 train_time:26003ms step_avg:57.91ms
step:450/2330 train_time:26064ms step_avg:57.92ms
step:451/2330 train_time:26120ms step_avg:57.92ms
step:452/2330 train_time:26179ms step_avg:57.92ms
step:453/2330 train_time:26237ms step_avg:57.92ms
step:454/2330 train_time:26296ms step_avg:57.92ms
step:455/2330 train_time:26352ms step_avg:57.92ms
step:456/2330 train_time:26411ms step_avg:57.92ms
step:457/2330 train_time:26467ms step_avg:57.91ms
step:458/2330 train_time:26526ms step_avg:57.92ms
step:459/2330 train_time:26582ms step_avg:57.91ms
step:460/2330 train_time:26643ms step_avg:57.92ms
step:461/2330 train_time:26699ms step_avg:57.92ms
step:462/2330 train_time:26760ms step_avg:57.92ms
step:463/2330 train_time:26817ms step_avg:57.92ms
step:464/2330 train_time:26877ms step_avg:57.92ms
step:465/2330 train_time:26933ms step_avg:57.92ms
step:466/2330 train_time:26991ms step_avg:57.92ms
step:467/2330 train_time:27048ms step_avg:57.92ms
step:468/2330 train_time:27107ms step_avg:57.92ms
step:469/2330 train_time:27164ms step_avg:57.92ms
step:470/2330 train_time:27223ms step_avg:57.92ms
step:471/2330 train_time:27280ms step_avg:57.92ms
step:472/2330 train_time:27339ms step_avg:57.92ms
step:473/2330 train_time:27397ms step_avg:57.92ms
step:474/2330 train_time:27455ms step_avg:57.92ms
step:475/2330 train_time:27512ms step_avg:57.92ms
step:476/2330 train_time:27571ms step_avg:57.92ms
step:477/2330 train_time:27627ms step_avg:57.92ms
step:478/2330 train_time:27687ms step_avg:57.92ms
step:479/2330 train_time:27743ms step_avg:57.92ms
step:480/2330 train_time:27803ms step_avg:57.92ms
step:481/2330 train_time:27859ms step_avg:57.92ms
step:482/2330 train_time:27920ms step_avg:57.92ms
step:483/2330 train_time:27977ms step_avg:57.92ms
step:484/2330 train_time:28036ms step_avg:57.93ms
step:485/2330 train_time:28092ms step_avg:57.92ms
step:486/2330 train_time:28151ms step_avg:57.92ms
step:487/2330 train_time:28207ms step_avg:57.92ms
step:488/2330 train_time:28267ms step_avg:57.92ms
step:489/2330 train_time:28323ms step_avg:57.92ms
step:490/2330 train_time:28384ms step_avg:57.93ms
step:491/2330 train_time:28440ms step_avg:57.92ms
step:492/2330 train_time:28500ms step_avg:57.93ms
step:493/2330 train_time:28557ms step_avg:57.92ms
step:494/2330 train_time:28615ms step_avg:57.93ms
step:495/2330 train_time:28672ms step_avg:57.92ms
step:496/2330 train_time:28731ms step_avg:57.92ms
step:497/2330 train_time:28787ms step_avg:57.92ms
step:498/2330 train_time:28847ms step_avg:57.93ms
step:499/2330 train_time:28903ms step_avg:57.92ms
step:500/2330 train_time:28963ms step_avg:57.93ms
step:500/2330 val_loss:4.5201 train_time:29043ms step_avg:58.09ms
step:501/2330 train_time:29061ms step_avg:58.01ms
step:502/2330 train_time:29082ms step_avg:57.93ms
step:503/2330 train_time:29138ms step_avg:57.93ms
step:504/2330 train_time:29201ms step_avg:57.94ms
step:505/2330 train_time:29258ms step_avg:57.94ms
step:506/2330 train_time:29318ms step_avg:57.94ms
step:507/2330 train_time:29375ms step_avg:57.94ms
step:508/2330 train_time:29434ms step_avg:57.94ms
step:509/2330 train_time:29490ms step_avg:57.94ms
step:510/2330 train_time:29548ms step_avg:57.94ms
step:511/2330 train_time:29604ms step_avg:57.93ms
step:512/2330 train_time:29663ms step_avg:57.94ms
step:513/2330 train_time:29719ms step_avg:57.93ms
step:514/2330 train_time:29778ms step_avg:57.93ms
step:515/2330 train_time:29833ms step_avg:57.93ms
step:516/2330 train_time:29892ms step_avg:57.93ms
step:517/2330 train_time:29948ms step_avg:57.93ms
step:518/2330 train_time:30007ms step_avg:57.93ms
step:519/2330 train_time:30063ms step_avg:57.93ms
step:520/2330 train_time:30124ms step_avg:57.93ms
step:521/2330 train_time:30181ms step_avg:57.93ms
step:522/2330 train_time:30242ms step_avg:57.93ms
step:523/2330 train_time:30299ms step_avg:57.93ms
step:524/2330 train_time:30359ms step_avg:57.94ms
step:525/2330 train_time:30415ms step_avg:57.93ms
step:526/2330 train_time:30474ms step_avg:57.94ms
step:527/2330 train_time:30530ms step_avg:57.93ms
step:528/2330 train_time:30589ms step_avg:57.93ms
step:529/2330 train_time:30645ms step_avg:57.93ms
step:530/2330 train_time:30704ms step_avg:57.93ms
step:531/2330 train_time:30760ms step_avg:57.93ms
step:532/2330 train_time:30819ms step_avg:57.93ms
step:533/2330 train_time:30875ms step_avg:57.93ms
step:534/2330 train_time:30934ms step_avg:57.93ms
step:535/2330 train_time:30990ms step_avg:57.93ms
step:536/2330 train_time:31050ms step_avg:57.93ms
step:537/2330 train_time:31106ms step_avg:57.93ms
step:538/2330 train_time:31167ms step_avg:57.93ms
step:539/2330 train_time:31223ms step_avg:57.93ms
step:540/2330 train_time:31285ms step_avg:57.93ms
step:541/2330 train_time:31342ms step_avg:57.93ms
step:542/2330 train_time:31401ms step_avg:57.94ms
step:543/2330 train_time:31457ms step_avg:57.93ms
step:544/2330 train_time:31517ms step_avg:57.94ms
step:545/2330 train_time:31573ms step_avg:57.93ms
step:546/2330 train_time:31632ms step_avg:57.93ms
step:547/2330 train_time:31688ms step_avg:57.93ms
step:548/2330 train_time:31748ms step_avg:57.93ms
step:549/2330 train_time:31804ms step_avg:57.93ms
step:550/2330 train_time:31863ms step_avg:57.93ms
step:551/2330 train_time:31920ms step_avg:57.93ms
step:552/2330 train_time:31979ms step_avg:57.93ms
step:553/2330 train_time:32035ms step_avg:57.93ms
step:554/2330 train_time:32095ms step_avg:57.93ms
step:555/2330 train_time:32152ms step_avg:57.93ms
step:556/2330 train_time:32213ms step_avg:57.94ms
step:557/2330 train_time:32269ms step_avg:57.93ms
step:558/2330 train_time:32330ms step_avg:57.94ms
step:559/2330 train_time:32386ms step_avg:57.94ms
step:560/2330 train_time:32446ms step_avg:57.94ms
step:561/2330 train_time:32502ms step_avg:57.94ms
step:562/2330 train_time:32562ms step_avg:57.94ms
step:563/2330 train_time:32618ms step_avg:57.94ms
step:564/2330 train_time:32677ms step_avg:57.94ms
step:565/2330 train_time:32734ms step_avg:57.94ms
step:566/2330 train_time:32792ms step_avg:57.94ms
step:567/2330 train_time:32849ms step_avg:57.93ms
step:568/2330 train_time:32908ms step_avg:57.94ms
step:569/2330 train_time:32964ms step_avg:57.93ms
step:570/2330 train_time:33024ms step_avg:57.94ms
step:571/2330 train_time:33081ms step_avg:57.93ms
step:572/2330 train_time:33140ms step_avg:57.94ms
step:573/2330 train_time:33197ms step_avg:57.94ms
step:574/2330 train_time:33257ms step_avg:57.94ms
step:575/2330 train_time:33313ms step_avg:57.94ms
step:576/2330 train_time:33372ms step_avg:57.94ms
step:577/2330 train_time:33429ms step_avg:57.94ms
step:578/2330 train_time:33489ms step_avg:57.94ms
step:579/2330 train_time:33544ms step_avg:57.93ms
step:580/2330 train_time:33605ms step_avg:57.94ms
step:581/2330 train_time:33661ms step_avg:57.94ms
step:582/2330 train_time:33721ms step_avg:57.94ms
step:583/2330 train_time:33777ms step_avg:57.94ms
step:584/2330 train_time:33836ms step_avg:57.94ms
step:585/2330 train_time:33893ms step_avg:57.94ms
step:586/2330 train_time:33951ms step_avg:57.94ms
step:587/2330 train_time:34007ms step_avg:57.93ms
step:588/2330 train_time:34067ms step_avg:57.94ms
step:589/2330 train_time:34123ms step_avg:57.93ms
step:590/2330 train_time:34184ms step_avg:57.94ms
step:591/2330 train_time:34241ms step_avg:57.94ms
step:592/2330 train_time:34300ms step_avg:57.94ms
step:593/2330 train_time:34357ms step_avg:57.94ms
step:594/2330 train_time:34415ms step_avg:57.94ms
step:595/2330 train_time:34472ms step_avg:57.94ms
step:596/2330 train_time:34532ms step_avg:57.94ms
step:597/2330 train_time:34588ms step_avg:57.94ms
step:598/2330 train_time:34647ms step_avg:57.94ms
step:599/2330 train_time:34703ms step_avg:57.94ms
step:600/2330 train_time:34763ms step_avg:57.94ms
step:601/2330 train_time:34820ms step_avg:57.94ms
step:602/2330 train_time:34879ms step_avg:57.94ms
step:603/2330 train_time:34936ms step_avg:57.94ms
step:604/2330 train_time:34995ms step_avg:57.94ms
step:605/2330 train_time:35051ms step_avg:57.94ms
step:606/2330 train_time:35111ms step_avg:57.94ms
step:607/2330 train_time:35167ms step_avg:57.94ms
step:608/2330 train_time:35228ms step_avg:57.94ms
step:609/2330 train_time:35284ms step_avg:57.94ms
step:610/2330 train_time:35344ms step_avg:57.94ms
step:611/2330 train_time:35401ms step_avg:57.94ms
step:612/2330 train_time:35460ms step_avg:57.94ms
step:613/2330 train_time:35517ms step_avg:57.94ms
step:614/2330 train_time:35576ms step_avg:57.94ms
step:615/2330 train_time:35632ms step_avg:57.94ms
step:616/2330 train_time:35691ms step_avg:57.94ms
step:617/2330 train_time:35748ms step_avg:57.94ms
step:618/2330 train_time:35807ms step_avg:57.94ms
step:619/2330 train_time:35863ms step_avg:57.94ms
step:620/2330 train_time:35923ms step_avg:57.94ms
step:621/2330 train_time:35979ms step_avg:57.94ms
step:622/2330 train_time:36039ms step_avg:57.94ms
step:623/2330 train_time:36095ms step_avg:57.94ms
step:624/2330 train_time:36154ms step_avg:57.94ms
step:625/2330 train_time:36210ms step_avg:57.94ms
step:626/2330 train_time:36270ms step_avg:57.94ms
step:627/2330 train_time:36327ms step_avg:57.94ms
step:628/2330 train_time:36387ms step_avg:57.94ms
step:629/2330 train_time:36444ms step_avg:57.94ms
step:630/2330 train_time:36503ms step_avg:57.94ms
step:631/2330 train_time:36559ms step_avg:57.94ms
step:632/2330 train_time:36619ms step_avg:57.94ms
step:633/2330 train_time:36675ms step_avg:57.94ms
step:634/2330 train_time:36734ms step_avg:57.94ms
step:635/2330 train_time:36790ms step_avg:57.94ms
step:636/2330 train_time:36849ms step_avg:57.94ms
step:637/2330 train_time:36905ms step_avg:57.94ms
step:638/2330 train_time:36965ms step_avg:57.94ms
step:639/2330 train_time:37022ms step_avg:57.94ms
step:640/2330 train_time:37081ms step_avg:57.94ms
step:641/2330 train_time:37137ms step_avg:57.94ms
step:642/2330 train_time:37197ms step_avg:57.94ms
step:643/2330 train_time:37253ms step_avg:57.94ms
step:644/2330 train_time:37314ms step_avg:57.94ms
step:645/2330 train_time:37369ms step_avg:57.94ms
step:646/2330 train_time:37431ms step_avg:57.94ms
step:647/2330 train_time:37487ms step_avg:57.94ms
step:648/2330 train_time:37547ms step_avg:57.94ms
step:649/2330 train_time:37604ms step_avg:57.94ms
step:650/2330 train_time:37662ms step_avg:57.94ms
step:651/2330 train_time:37719ms step_avg:57.94ms
step:652/2330 train_time:37778ms step_avg:57.94ms
step:653/2330 train_time:37835ms step_avg:57.94ms
step:654/2330 train_time:37894ms step_avg:57.94ms
step:655/2330 train_time:37949ms step_avg:57.94ms
step:656/2330 train_time:38009ms step_avg:57.94ms
step:657/2330 train_time:38065ms step_avg:57.94ms
step:658/2330 train_time:38126ms step_avg:57.94ms
step:659/2330 train_time:38183ms step_avg:57.94ms
step:660/2330 train_time:38243ms step_avg:57.94ms
step:661/2330 train_time:38301ms step_avg:57.94ms
step:662/2330 train_time:38359ms step_avg:57.94ms
step:663/2330 train_time:38416ms step_avg:57.94ms
step:664/2330 train_time:38475ms step_avg:57.94ms
step:665/2330 train_time:38532ms step_avg:57.94ms
step:666/2330 train_time:38591ms step_avg:57.94ms
step:667/2330 train_time:38647ms step_avg:57.94ms
step:668/2330 train_time:38707ms step_avg:57.94ms
step:669/2330 train_time:38764ms step_avg:57.94ms
step:670/2330 train_time:38822ms step_avg:57.94ms
step:671/2330 train_time:38879ms step_avg:57.94ms
step:672/2330 train_time:38938ms step_avg:57.94ms
step:673/2330 train_time:38995ms step_avg:57.94ms
step:674/2330 train_time:39054ms step_avg:57.94ms
step:675/2330 train_time:39110ms step_avg:57.94ms
step:676/2330 train_time:39169ms step_avg:57.94ms
step:677/2330 train_time:39227ms step_avg:57.94ms
step:678/2330 train_time:39286ms step_avg:57.94ms
step:679/2330 train_time:39342ms step_avg:57.94ms
step:680/2330 train_time:39401ms step_avg:57.94ms
step:681/2330 train_time:39458ms step_avg:57.94ms
step:682/2330 train_time:39517ms step_avg:57.94ms
step:683/2330 train_time:39573ms step_avg:57.94ms
step:684/2330 train_time:39632ms step_avg:57.94ms
step:685/2330 train_time:39688ms step_avg:57.94ms
step:686/2330 train_time:39749ms step_avg:57.94ms
step:687/2330 train_time:39805ms step_avg:57.94ms
step:688/2330 train_time:39864ms step_avg:57.94ms
step:689/2330 train_time:39921ms step_avg:57.94ms
step:690/2330 train_time:39980ms step_avg:57.94ms
step:691/2330 train_time:40037ms step_avg:57.94ms
step:692/2330 train_time:40096ms step_avg:57.94ms
step:693/2330 train_time:40152ms step_avg:57.94ms
step:694/2330 train_time:40212ms step_avg:57.94ms
step:695/2330 train_time:40268ms step_avg:57.94ms
step:696/2330 train_time:40328ms step_avg:57.94ms
step:697/2330 train_time:40384ms step_avg:57.94ms
step:698/2330 train_time:40443ms step_avg:57.94ms
step:699/2330 train_time:40500ms step_avg:57.94ms
step:700/2330 train_time:40560ms step_avg:57.94ms
step:701/2330 train_time:40617ms step_avg:57.94ms
step:702/2330 train_time:40675ms step_avg:57.94ms
step:703/2330 train_time:40732ms step_avg:57.94ms
step:704/2330 train_time:40792ms step_avg:57.94ms
step:705/2330 train_time:40848ms step_avg:57.94ms
step:706/2330 train_time:40908ms step_avg:57.94ms
step:707/2330 train_time:40964ms step_avg:57.94ms
step:708/2330 train_time:41025ms step_avg:57.95ms
step:709/2330 train_time:41082ms step_avg:57.94ms
step:710/2330 train_time:41142ms step_avg:57.95ms
step:711/2330 train_time:41200ms step_avg:57.95ms
step:712/2330 train_time:41259ms step_avg:57.95ms
step:713/2330 train_time:41316ms step_avg:57.95ms
step:714/2330 train_time:41374ms step_avg:57.95ms
step:715/2330 train_time:41431ms step_avg:57.95ms
step:716/2330 train_time:41490ms step_avg:57.95ms
step:717/2330 train_time:41546ms step_avg:57.94ms
step:718/2330 train_time:41605ms step_avg:57.95ms
step:719/2330 train_time:41662ms step_avg:57.94ms
step:720/2330 train_time:41722ms step_avg:57.95ms
step:721/2330 train_time:41778ms step_avg:57.95ms
step:722/2330 train_time:41838ms step_avg:57.95ms
step:723/2330 train_time:41894ms step_avg:57.94ms
step:724/2330 train_time:41953ms step_avg:57.95ms
step:725/2330 train_time:42009ms step_avg:57.94ms
step:726/2330 train_time:42070ms step_avg:57.95ms
step:727/2330 train_time:42126ms step_avg:57.94ms
step:728/2330 train_time:42186ms step_avg:57.95ms
step:729/2330 train_time:42243ms step_avg:57.95ms
step:730/2330 train_time:42302ms step_avg:57.95ms
step:731/2330 train_time:42359ms step_avg:57.95ms
step:732/2330 train_time:42419ms step_avg:57.95ms
step:733/2330 train_time:42475ms step_avg:57.95ms
step:734/2330 train_time:42534ms step_avg:57.95ms
step:735/2330 train_time:42591ms step_avg:57.95ms
step:736/2330 train_time:42649ms step_avg:57.95ms
step:737/2330 train_time:42706ms step_avg:57.95ms
step:738/2330 train_time:42765ms step_avg:57.95ms
step:739/2330 train_time:42821ms step_avg:57.94ms
step:740/2330 train_time:42881ms step_avg:57.95ms
step:741/2330 train_time:42938ms step_avg:57.95ms
step:742/2330 train_time:42997ms step_avg:57.95ms
step:743/2330 train_time:43054ms step_avg:57.95ms
step:744/2330 train_time:43112ms step_avg:57.95ms
step:745/2330 train_time:43168ms step_avg:57.94ms
step:746/2330 train_time:43228ms step_avg:57.95ms
step:747/2330 train_time:43285ms step_avg:57.94ms
step:748/2330 train_time:43345ms step_avg:57.95ms
step:749/2330 train_time:43402ms step_avg:57.95ms
step:750/2330 train_time:43462ms step_avg:57.95ms
step:750/2330 val_loss:4.2549 train_time:43541ms step_avg:58.05ms
step:751/2330 train_time:43561ms step_avg:58.00ms
step:752/2330 train_time:43581ms step_avg:57.95ms
step:753/2330 train_time:43635ms step_avg:57.95ms
step:754/2330 train_time:43699ms step_avg:57.96ms
step:755/2330 train_time:43755ms step_avg:57.95ms
step:756/2330 train_time:43817ms step_avg:57.96ms
step:757/2330 train_time:43872ms step_avg:57.96ms
step:758/2330 train_time:43933ms step_avg:57.96ms
step:759/2330 train_time:43988ms step_avg:57.96ms
step:760/2330 train_time:44048ms step_avg:57.96ms
step:761/2330 train_time:44104ms step_avg:57.96ms
step:762/2330 train_time:44162ms step_avg:57.96ms
step:763/2330 train_time:44218ms step_avg:57.95ms
step:764/2330 train_time:44276ms step_avg:57.95ms
step:765/2330 train_time:44333ms step_avg:57.95ms
step:766/2330 train_time:44392ms step_avg:57.95ms
step:767/2330 train_time:44448ms step_avg:57.95ms
step:768/2330 train_time:44509ms step_avg:57.95ms
step:769/2330 train_time:44567ms step_avg:57.96ms
step:770/2330 train_time:44628ms step_avg:57.96ms
step:771/2330 train_time:44685ms step_avg:57.96ms
step:772/2330 train_time:44748ms step_avg:57.96ms
step:773/2330 train_time:44805ms step_avg:57.96ms
step:774/2330 train_time:44867ms step_avg:57.97ms
step:775/2330 train_time:44924ms step_avg:57.97ms
step:776/2330 train_time:44983ms step_avg:57.97ms
step:777/2330 train_time:45041ms step_avg:57.97ms
step:778/2330 train_time:45101ms step_avg:57.97ms
step:779/2330 train_time:45158ms step_avg:57.97ms
step:780/2330 train_time:45216ms step_avg:57.97ms
step:781/2330 train_time:45273ms step_avg:57.97ms
step:782/2330 train_time:45333ms step_avg:57.97ms
step:783/2330 train_time:45390ms step_avg:57.97ms
step:784/2330 train_time:45450ms step_avg:57.97ms
step:785/2330 train_time:45507ms step_avg:57.97ms
step:786/2330 train_time:45568ms step_avg:57.97ms
step:787/2330 train_time:45626ms step_avg:57.97ms
step:788/2330 train_time:45687ms step_avg:57.98ms
step:789/2330 train_time:45745ms step_avg:57.98ms
step:790/2330 train_time:45806ms step_avg:57.98ms
step:791/2330 train_time:45864ms step_avg:57.98ms
step:792/2330 train_time:45923ms step_avg:57.98ms
step:793/2330 train_time:45981ms step_avg:57.98ms
step:794/2330 train_time:46040ms step_avg:57.99ms
step:795/2330 train_time:46098ms step_avg:57.98ms
step:796/2330 train_time:46157ms step_avg:57.99ms
step:797/2330 train_time:46214ms step_avg:57.99ms
step:798/2330 train_time:46273ms step_avg:57.99ms
step:799/2330 train_time:46330ms step_avg:57.98ms
step:800/2330 train_time:46390ms step_avg:57.99ms
step:801/2330 train_time:46446ms step_avg:57.99ms
step:802/2330 train_time:46507ms step_avg:57.99ms
step:803/2330 train_time:46565ms step_avg:57.99ms
step:804/2330 train_time:46625ms step_avg:57.99ms
step:805/2330 train_time:46682ms step_avg:57.99ms
step:806/2330 train_time:46743ms step_avg:57.99ms
step:807/2330 train_time:46801ms step_avg:57.99ms
step:808/2330 train_time:46861ms step_avg:58.00ms
step:809/2330 train_time:46918ms step_avg:58.00ms
step:810/2330 train_time:46978ms step_avg:58.00ms
step:811/2330 train_time:47034ms step_avg:58.00ms
step:812/2330 train_time:47094ms step_avg:58.00ms
step:813/2330 train_time:47152ms step_avg:58.00ms
step:814/2330 train_time:47211ms step_avg:58.00ms
step:815/2330 train_time:47268ms step_avg:58.00ms
step:816/2330 train_time:47328ms step_avg:58.00ms
step:817/2330 train_time:47385ms step_avg:58.00ms
step:818/2330 train_time:47445ms step_avg:58.00ms
step:819/2330 train_time:47502ms step_avg:58.00ms
step:820/2330 train_time:47563ms step_avg:58.00ms
step:821/2330 train_time:47620ms step_avg:58.00ms
step:822/2330 train_time:47681ms step_avg:58.01ms
step:823/2330 train_time:47738ms step_avg:58.00ms
step:824/2330 train_time:47798ms step_avg:58.01ms
step:825/2330 train_time:47856ms step_avg:58.01ms
step:826/2330 train_time:47915ms step_avg:58.01ms
step:827/2330 train_time:47972ms step_avg:58.01ms
step:828/2330 train_time:48033ms step_avg:58.01ms
step:829/2330 train_time:48090ms step_avg:58.01ms
step:830/2330 train_time:48150ms step_avg:58.01ms
step:831/2330 train_time:48208ms step_avg:58.01ms
step:832/2330 train_time:48267ms step_avg:58.01ms
step:833/2330 train_time:48324ms step_avg:58.01ms
step:834/2330 train_time:48384ms step_avg:58.01ms
step:835/2330 train_time:48442ms step_avg:58.01ms
step:836/2330 train_time:48501ms step_avg:58.02ms
step:837/2330 train_time:48558ms step_avg:58.01ms
step:838/2330 train_time:48618ms step_avg:58.02ms
step:839/2330 train_time:48675ms step_avg:58.02ms
step:840/2330 train_time:48736ms step_avg:58.02ms
step:841/2330 train_time:48793ms step_avg:58.02ms
step:842/2330 train_time:48854ms step_avg:58.02ms
step:843/2330 train_time:48911ms step_avg:58.02ms
step:844/2330 train_time:48972ms step_avg:58.02ms
step:845/2330 train_time:49029ms step_avg:58.02ms
step:846/2330 train_time:49090ms step_avg:58.03ms
step:847/2330 train_time:49147ms step_avg:58.03ms
step:848/2330 train_time:49207ms step_avg:58.03ms
step:849/2330 train_time:49265ms step_avg:58.03ms
step:850/2330 train_time:49324ms step_avg:58.03ms
step:851/2330 train_time:49381ms step_avg:58.03ms
step:852/2330 train_time:49441ms step_avg:58.03ms
step:853/2330 train_time:49498ms step_avg:58.03ms
step:854/2330 train_time:49558ms step_avg:58.03ms
step:855/2330 train_time:49616ms step_avg:58.03ms
step:856/2330 train_time:49676ms step_avg:58.03ms
step:857/2330 train_time:49733ms step_avg:58.03ms
step:858/2330 train_time:49793ms step_avg:58.03ms
step:859/2330 train_time:49851ms step_avg:58.03ms
step:860/2330 train_time:49911ms step_avg:58.04ms
step:861/2330 train_time:49968ms step_avg:58.03ms
step:862/2330 train_time:50029ms step_avg:58.04ms
step:863/2330 train_time:50087ms step_avg:58.04ms
step:864/2330 train_time:50147ms step_avg:58.04ms
step:865/2330 train_time:50204ms step_avg:58.04ms
step:866/2330 train_time:50264ms step_avg:58.04ms
step:867/2330 train_time:50320ms step_avg:58.04ms
step:868/2330 train_time:50380ms step_avg:58.04ms
step:869/2330 train_time:50438ms step_avg:58.04ms
step:870/2330 train_time:50497ms step_avg:58.04ms
step:871/2330 train_time:50555ms step_avg:58.04ms
step:872/2330 train_time:50614ms step_avg:58.04ms
step:873/2330 train_time:50671ms step_avg:58.04ms
step:874/2330 train_time:50732ms step_avg:58.05ms
step:875/2330 train_time:50789ms step_avg:58.04ms
step:876/2330 train_time:50850ms step_avg:58.05ms
step:877/2330 train_time:50907ms step_avg:58.05ms
step:878/2330 train_time:50968ms step_avg:58.05ms
step:879/2330 train_time:51025ms step_avg:58.05ms
step:880/2330 train_time:51085ms step_avg:58.05ms
step:881/2330 train_time:51143ms step_avg:58.05ms
step:882/2330 train_time:51203ms step_avg:58.05ms
step:883/2330 train_time:51260ms step_avg:58.05ms
step:884/2330 train_time:51319ms step_avg:58.05ms
step:885/2330 train_time:51376ms step_avg:58.05ms
step:886/2330 train_time:51436ms step_avg:58.05ms
step:887/2330 train_time:51494ms step_avg:58.05ms
step:888/2330 train_time:51553ms step_avg:58.06ms
step:889/2330 train_time:51611ms step_avg:58.05ms
step:890/2330 train_time:51671ms step_avg:58.06ms
step:891/2330 train_time:51728ms step_avg:58.06ms
step:892/2330 train_time:51789ms step_avg:58.06ms
step:893/2330 train_time:51846ms step_avg:58.06ms
step:894/2330 train_time:51906ms step_avg:58.06ms
step:895/2330 train_time:51962ms step_avg:58.06ms
step:896/2330 train_time:52023ms step_avg:58.06ms
step:897/2330 train_time:52080ms step_avg:58.06ms
step:898/2330 train_time:52140ms step_avg:58.06ms
step:899/2330 train_time:52198ms step_avg:58.06ms
step:900/2330 train_time:52257ms step_avg:58.06ms
step:901/2330 train_time:52314ms step_avg:58.06ms
step:902/2330 train_time:52374ms step_avg:58.06ms
step:903/2330 train_time:52432ms step_avg:58.06ms
step:904/2330 train_time:52491ms step_avg:58.07ms
step:905/2330 train_time:52549ms step_avg:58.06ms
step:906/2330 train_time:52609ms step_avg:58.07ms
step:907/2330 train_time:52666ms step_avg:58.07ms
step:908/2330 train_time:52725ms step_avg:58.07ms
step:909/2330 train_time:52784ms step_avg:58.07ms
step:910/2330 train_time:52844ms step_avg:58.07ms
step:911/2330 train_time:52902ms step_avg:58.07ms
step:912/2330 train_time:52963ms step_avg:58.07ms
step:913/2330 train_time:53020ms step_avg:58.07ms
step:914/2330 train_time:53080ms step_avg:58.07ms
step:915/2330 train_time:53136ms step_avg:58.07ms
step:916/2330 train_time:53197ms step_avg:58.08ms
step:917/2330 train_time:53254ms step_avg:58.07ms
step:918/2330 train_time:53315ms step_avg:58.08ms
step:919/2330 train_time:53373ms step_avg:58.08ms
step:920/2330 train_time:53432ms step_avg:58.08ms
step:921/2330 train_time:53489ms step_avg:58.08ms
step:922/2330 train_time:53549ms step_avg:58.08ms
step:923/2330 train_time:53606ms step_avg:58.08ms
step:924/2330 train_time:53666ms step_avg:58.08ms
step:925/2330 train_time:53723ms step_avg:58.08ms
step:926/2330 train_time:53784ms step_avg:58.08ms
step:927/2330 train_time:53841ms step_avg:58.08ms
step:928/2330 train_time:53901ms step_avg:58.08ms
step:929/2330 train_time:53958ms step_avg:58.08ms
step:930/2330 train_time:54018ms step_avg:58.08ms
step:931/2330 train_time:54075ms step_avg:58.08ms
step:932/2330 train_time:54135ms step_avg:58.08ms
step:933/2330 train_time:54192ms step_avg:58.08ms
step:934/2330 train_time:54253ms step_avg:58.09ms
step:935/2330 train_time:54310ms step_avg:58.09ms
step:936/2330 train_time:54370ms step_avg:58.09ms
step:937/2330 train_time:54428ms step_avg:58.09ms
step:938/2330 train_time:54488ms step_avg:58.09ms
step:939/2330 train_time:54546ms step_avg:58.09ms
step:940/2330 train_time:54606ms step_avg:58.09ms
step:941/2330 train_time:54663ms step_avg:58.09ms
step:942/2330 train_time:54723ms step_avg:58.09ms
step:943/2330 train_time:54781ms step_avg:58.09ms
step:944/2330 train_time:54841ms step_avg:58.09ms
step:945/2330 train_time:54899ms step_avg:58.09ms
step:946/2330 train_time:54958ms step_avg:58.10ms
step:947/2330 train_time:55016ms step_avg:58.09ms
step:948/2330 train_time:55075ms step_avg:58.10ms
step:949/2330 train_time:55133ms step_avg:58.10ms
step:950/2330 train_time:55192ms step_avg:58.10ms
step:951/2330 train_time:55249ms step_avg:58.10ms
step:952/2330 train_time:55309ms step_avg:58.10ms
step:953/2330 train_time:55366ms step_avg:58.10ms
step:954/2330 train_time:55426ms step_avg:58.10ms
step:955/2330 train_time:55483ms step_avg:58.10ms
step:956/2330 train_time:55544ms step_avg:58.10ms
step:957/2330 train_time:55600ms step_avg:58.10ms
step:958/2330 train_time:55662ms step_avg:58.10ms
step:959/2330 train_time:55719ms step_avg:58.10ms
step:960/2330 train_time:55779ms step_avg:58.10ms
step:961/2330 train_time:55836ms step_avg:58.10ms
step:962/2330 train_time:55896ms step_avg:58.10ms
step:963/2330 train_time:55952ms step_avg:58.10ms
step:964/2330 train_time:56013ms step_avg:58.10ms
step:965/2330 train_time:56070ms step_avg:58.10ms
step:966/2330 train_time:56130ms step_avg:58.11ms
step:967/2330 train_time:56188ms step_avg:58.11ms
step:968/2330 train_time:56248ms step_avg:58.11ms
step:969/2330 train_time:56305ms step_avg:58.11ms
step:970/2330 train_time:56365ms step_avg:58.11ms
step:971/2330 train_time:56422ms step_avg:58.11ms
step:972/2330 train_time:56482ms step_avg:58.11ms
step:973/2330 train_time:56539ms step_avg:58.11ms
step:974/2330 train_time:56600ms step_avg:58.11ms
step:975/2330 train_time:56657ms step_avg:58.11ms
step:976/2330 train_time:56717ms step_avg:58.11ms
step:977/2330 train_time:56775ms step_avg:58.11ms
step:978/2330 train_time:56835ms step_avg:58.11ms
step:979/2330 train_time:56892ms step_avg:58.11ms
step:980/2330 train_time:56952ms step_avg:58.11ms
step:981/2330 train_time:57008ms step_avg:58.11ms
step:982/2330 train_time:57070ms step_avg:58.12ms
step:983/2330 train_time:57127ms step_avg:58.11ms
step:984/2330 train_time:57188ms step_avg:58.12ms
step:985/2330 train_time:57245ms step_avg:58.12ms
step:986/2330 train_time:57304ms step_avg:58.12ms
step:987/2330 train_time:57362ms step_avg:58.12ms
step:988/2330 train_time:57422ms step_avg:58.12ms
step:989/2330 train_time:57479ms step_avg:58.12ms
step:990/2330 train_time:57540ms step_avg:58.12ms
step:991/2330 train_time:57597ms step_avg:58.12ms
step:992/2330 train_time:57657ms step_avg:58.12ms
step:993/2330 train_time:57714ms step_avg:58.12ms
step:994/2330 train_time:57774ms step_avg:58.12ms
step:995/2330 train_time:57832ms step_avg:58.12ms
step:996/2330 train_time:57892ms step_avg:58.12ms
step:997/2330 train_time:57949ms step_avg:58.12ms
step:998/2330 train_time:58009ms step_avg:58.12ms
step:999/2330 train_time:58066ms step_avg:58.12ms
step:1000/2330 train_time:58126ms step_avg:58.13ms
step:1000/2330 val_loss:4.0953 train_time:58206ms step_avg:58.21ms
step:1001/2330 train_time:58226ms step_avg:58.17ms
step:1002/2330 train_time:58246ms step_avg:58.13ms
step:1003/2330 train_time:58301ms step_avg:58.13ms
step:1004/2330 train_time:58367ms step_avg:58.13ms
step:1005/2330 train_time:58423ms step_avg:58.13ms
step:1006/2330 train_time:58485ms step_avg:58.14ms
step:1007/2330 train_time:58542ms step_avg:58.13ms
step:1008/2330 train_time:58602ms step_avg:58.14ms
step:1009/2330 train_time:58659ms step_avg:58.14ms
step:1010/2330 train_time:58718ms step_avg:58.14ms
step:1011/2330 train_time:58774ms step_avg:58.13ms
step:1012/2330 train_time:58834ms step_avg:58.14ms
step:1013/2330 train_time:58890ms step_avg:58.13ms
step:1014/2330 train_time:58949ms step_avg:58.14ms
step:1015/2330 train_time:59005ms step_avg:58.13ms
step:1016/2330 train_time:59065ms step_avg:58.13ms
step:1017/2330 train_time:59124ms step_avg:58.14ms
step:1018/2330 train_time:59184ms step_avg:58.14ms
step:1019/2330 train_time:59242ms step_avg:58.14ms
step:1020/2330 train_time:59303ms step_avg:58.14ms
step:1021/2330 train_time:59361ms step_avg:58.14ms
step:1022/2330 train_time:59422ms step_avg:58.14ms
step:1023/2330 train_time:59480ms step_avg:58.14ms
step:1024/2330 train_time:59541ms step_avg:58.15ms
step:1025/2330 train_time:59597ms step_avg:58.14ms
step:1026/2330 train_time:59658ms step_avg:58.15ms
step:1027/2330 train_time:59715ms step_avg:58.14ms
step:1028/2330 train_time:59776ms step_avg:58.15ms
step:1029/2330 train_time:59832ms step_avg:58.15ms
step:1030/2330 train_time:59891ms step_avg:58.15ms
step:1031/2330 train_time:59948ms step_avg:58.15ms
step:1032/2330 train_time:60007ms step_avg:58.15ms
step:1033/2330 train_time:60065ms step_avg:58.15ms
step:1034/2330 train_time:60124ms step_avg:58.15ms
step:1035/2330 train_time:60182ms step_avg:58.15ms
step:1036/2330 train_time:60243ms step_avg:58.15ms
step:1037/2330 train_time:60301ms step_avg:58.15ms
step:1038/2330 train_time:60361ms step_avg:58.15ms
step:1039/2330 train_time:60419ms step_avg:58.15ms
step:1040/2330 train_time:60479ms step_avg:58.15ms
step:1041/2330 train_time:60537ms step_avg:58.15ms
step:1042/2330 train_time:60598ms step_avg:58.16ms
step:1043/2330 train_time:60655ms step_avg:58.15ms
step:1044/2330 train_time:60715ms step_avg:58.16ms
step:1045/2330 train_time:60771ms step_avg:58.15ms
step:1046/2330 train_time:60831ms step_avg:58.16ms
step:1047/2330 train_time:60887ms step_avg:58.15ms
step:1048/2330 train_time:60948ms step_avg:58.16ms
step:1049/2330 train_time:61005ms step_avg:58.16ms
step:1050/2330 train_time:61065ms step_avg:58.16ms
step:1051/2330 train_time:61122ms step_avg:58.16ms
step:1052/2330 train_time:61181ms step_avg:58.16ms
step:1053/2330 train_time:61238ms step_avg:58.16ms
step:1054/2330 train_time:61300ms step_avg:58.16ms
step:1055/2330 train_time:61357ms step_avg:58.16ms
step:1056/2330 train_time:61417ms step_avg:58.16ms
step:1057/2330 train_time:61474ms step_avg:58.16ms
step:1058/2330 train_time:61535ms step_avg:58.16ms
step:1059/2330 train_time:61592ms step_avg:58.16ms
step:1060/2330 train_time:61652ms step_avg:58.16ms
step:1061/2330 train_time:61710ms step_avg:58.16ms
step:1062/2330 train_time:61769ms step_avg:58.16ms
step:1063/2330 train_time:61826ms step_avg:58.16ms
step:1064/2330 train_time:61886ms step_avg:58.16ms
step:1065/2330 train_time:61942ms step_avg:58.16ms
step:1066/2330 train_time:62002ms step_avg:58.16ms
step:1067/2330 train_time:62059ms step_avg:58.16ms
step:1068/2330 train_time:62120ms step_avg:58.16ms
step:1069/2330 train_time:62176ms step_avg:58.16ms
step:1070/2330 train_time:62236ms step_avg:58.16ms
step:1071/2330 train_time:62293ms step_avg:58.16ms
step:1072/2330 train_time:62354ms step_avg:58.17ms
step:1073/2330 train_time:62411ms step_avg:58.17ms
step:1074/2330 train_time:62471ms step_avg:58.17ms
step:1075/2330 train_time:62528ms step_avg:58.17ms
step:1076/2330 train_time:62588ms step_avg:58.17ms
step:1077/2330 train_time:62645ms step_avg:58.17ms
step:1078/2330 train_time:62705ms step_avg:58.17ms
step:1079/2330 train_time:62762ms step_avg:58.17ms
step:1080/2330 train_time:62822ms step_avg:58.17ms
step:1081/2330 train_time:62879ms step_avg:58.17ms
step:1082/2330 train_time:62939ms step_avg:58.17ms
step:1083/2330 train_time:62996ms step_avg:58.17ms
step:1084/2330 train_time:63057ms step_avg:58.17ms
step:1085/2330 train_time:63114ms step_avg:58.17ms
step:1086/2330 train_time:63173ms step_avg:58.17ms
step:1087/2330 train_time:63232ms step_avg:58.17ms
step:1088/2330 train_time:63292ms step_avg:58.17ms
step:1089/2330 train_time:63350ms step_avg:58.17ms
step:1090/2330 train_time:63410ms step_avg:58.17ms
step:1091/2330 train_time:63467ms step_avg:58.17ms
step:1092/2330 train_time:63527ms step_avg:58.18ms
step:1093/2330 train_time:63584ms step_avg:58.17ms
step:1094/2330 train_time:63645ms step_avg:58.18ms
step:1095/2330 train_time:63702ms step_avg:58.18ms
step:1096/2330 train_time:63762ms step_avg:58.18ms
step:1097/2330 train_time:63819ms step_avg:58.18ms
step:1098/2330 train_time:63879ms step_avg:58.18ms
step:1099/2330 train_time:63936ms step_avg:58.18ms
step:1100/2330 train_time:63997ms step_avg:58.18ms
step:1101/2330 train_time:64054ms step_avg:58.18ms
step:1102/2330 train_time:64114ms step_avg:58.18ms
step:1103/2330 train_time:64171ms step_avg:58.18ms
step:1104/2330 train_time:64232ms step_avg:58.18ms
step:1105/2330 train_time:64289ms step_avg:58.18ms
step:1106/2330 train_time:64350ms step_avg:58.18ms
step:1107/2330 train_time:64408ms step_avg:58.18ms
step:1108/2330 train_time:64468ms step_avg:58.18ms
step:1109/2330 train_time:64525ms step_avg:58.18ms
step:1110/2330 train_time:64585ms step_avg:58.18ms
step:1111/2330 train_time:64642ms step_avg:58.18ms
step:1112/2330 train_time:64703ms step_avg:58.19ms
step:1113/2330 train_time:64760ms step_avg:58.19ms
step:1114/2330 train_time:64820ms step_avg:58.19ms
step:1115/2330 train_time:64876ms step_avg:58.19ms
step:1116/2330 train_time:64937ms step_avg:58.19ms
step:1117/2330 train_time:64993ms step_avg:58.19ms
step:1118/2330 train_time:65055ms step_avg:58.19ms
step:1119/2330 train_time:65112ms step_avg:58.19ms
step:1120/2330 train_time:65172ms step_avg:58.19ms
step:1121/2330 train_time:65229ms step_avg:58.19ms
step:1122/2330 train_time:65289ms step_avg:58.19ms
step:1123/2330 train_time:65346ms step_avg:58.19ms
step:1124/2330 train_time:65407ms step_avg:58.19ms
step:1125/2330 train_time:65465ms step_avg:58.19ms
step:1126/2330 train_time:65524ms step_avg:58.19ms
step:1127/2330 train_time:65582ms step_avg:58.19ms
step:1128/2330 train_time:65642ms step_avg:58.19ms
step:1129/2330 train_time:65700ms step_avg:58.19ms
step:1130/2330 train_time:65760ms step_avg:58.19ms
step:1131/2330 train_time:65816ms step_avg:58.19ms
step:1132/2330 train_time:65877ms step_avg:58.20ms
step:1133/2330 train_time:65934ms step_avg:58.19ms
step:1134/2330 train_time:65995ms step_avg:58.20ms
step:1135/2330 train_time:66052ms step_avg:58.20ms
step:1136/2330 train_time:66111ms step_avg:58.20ms
step:1137/2330 train_time:66168ms step_avg:58.20ms
step:1138/2330 train_time:66228ms step_avg:58.20ms
step:1139/2330 train_time:66285ms step_avg:58.20ms
step:1140/2330 train_time:66346ms step_avg:58.20ms
step:1141/2330 train_time:66403ms step_avg:58.20ms
step:1142/2330 train_time:66463ms step_avg:58.20ms
step:1143/2330 train_time:66520ms step_avg:58.20ms
step:1144/2330 train_time:66581ms step_avg:58.20ms
step:1145/2330 train_time:66638ms step_avg:58.20ms
step:1146/2330 train_time:66698ms step_avg:58.20ms
step:1147/2330 train_time:66755ms step_avg:58.20ms
step:1148/2330 train_time:66815ms step_avg:58.20ms
step:1149/2330 train_time:66873ms step_avg:58.20ms
step:1150/2330 train_time:66933ms step_avg:58.20ms
step:1151/2330 train_time:66990ms step_avg:58.20ms
step:1152/2330 train_time:67050ms step_avg:58.20ms
step:1153/2330 train_time:67107ms step_avg:58.20ms
step:1154/2330 train_time:67167ms step_avg:58.20ms
step:1155/2330 train_time:67225ms step_avg:58.20ms
step:1156/2330 train_time:67285ms step_avg:58.20ms
step:1157/2330 train_time:67342ms step_avg:58.20ms
step:1158/2330 train_time:67402ms step_avg:58.21ms
step:1159/2330 train_time:67459ms step_avg:58.20ms
step:1160/2330 train_time:67519ms step_avg:58.21ms
step:1161/2330 train_time:67575ms step_avg:58.20ms
step:1162/2330 train_time:67637ms step_avg:58.21ms
step:1163/2330 train_time:67693ms step_avg:58.21ms
step:1164/2330 train_time:67755ms step_avg:58.21ms
step:1165/2330 train_time:67812ms step_avg:58.21ms
step:1166/2330 train_time:67872ms step_avg:58.21ms
step:1167/2330 train_time:67929ms step_avg:58.21ms
step:1168/2330 train_time:67989ms step_avg:58.21ms
step:1169/2330 train_time:68047ms step_avg:58.21ms
step:1170/2330 train_time:68106ms step_avg:58.21ms
step:1171/2330 train_time:68165ms step_avg:58.21ms
step:1172/2330 train_time:68224ms step_avg:58.21ms
step:1173/2330 train_time:68281ms step_avg:58.21ms
step:1174/2330 train_time:68342ms step_avg:58.21ms
step:1175/2330 train_time:68400ms step_avg:58.21ms
step:1176/2330 train_time:68460ms step_avg:58.21ms
step:1177/2330 train_time:68517ms step_avg:58.21ms
step:1178/2330 train_time:68577ms step_avg:58.21ms
step:1179/2330 train_time:68634ms step_avg:58.21ms
step:1180/2330 train_time:68694ms step_avg:58.22ms
step:1181/2330 train_time:68752ms step_avg:58.21ms
step:1182/2330 train_time:68811ms step_avg:58.22ms
step:1183/2330 train_time:68868ms step_avg:58.21ms
step:1184/2330 train_time:68928ms step_avg:58.22ms
step:1185/2330 train_time:68985ms step_avg:58.22ms
step:1186/2330 train_time:69045ms step_avg:58.22ms
step:1187/2330 train_time:69102ms step_avg:58.22ms
step:1188/2330 train_time:69162ms step_avg:58.22ms
step:1189/2330 train_time:69220ms step_avg:58.22ms
step:1190/2330 train_time:69279ms step_avg:58.22ms
step:1191/2330 train_time:69336ms step_avg:58.22ms
step:1192/2330 train_time:69397ms step_avg:58.22ms
step:1193/2330 train_time:69454ms step_avg:58.22ms
step:1194/2330 train_time:69515ms step_avg:58.22ms
step:1195/2330 train_time:69571ms step_avg:58.22ms
step:1196/2330 train_time:69632ms step_avg:58.22ms
step:1197/2330 train_time:69689ms step_avg:58.22ms
step:1198/2330 train_time:69749ms step_avg:58.22ms
step:1199/2330 train_time:69806ms step_avg:58.22ms
step:1200/2330 train_time:69866ms step_avg:58.22ms
step:1201/2330 train_time:69923ms step_avg:58.22ms
step:1202/2330 train_time:69984ms step_avg:58.22ms
step:1203/2330 train_time:70041ms step_avg:58.22ms
step:1204/2330 train_time:70101ms step_avg:58.22ms
step:1205/2330 train_time:70158ms step_avg:58.22ms
step:1206/2330 train_time:70218ms step_avg:58.22ms
step:1207/2330 train_time:70275ms step_avg:58.22ms
step:1208/2330 train_time:70335ms step_avg:58.22ms
step:1209/2330 train_time:70393ms step_avg:58.22ms
step:1210/2330 train_time:70452ms step_avg:58.22ms
step:1211/2330 train_time:70510ms step_avg:58.22ms
step:1212/2330 train_time:70569ms step_avg:58.23ms
step:1213/2330 train_time:70626ms step_avg:58.22ms
step:1214/2330 train_time:70686ms step_avg:58.23ms
step:1215/2330 train_time:70743ms step_avg:58.22ms
step:1216/2330 train_time:70804ms step_avg:58.23ms
step:1217/2330 train_time:70861ms step_avg:58.23ms
step:1218/2330 train_time:70921ms step_avg:58.23ms
step:1219/2330 train_time:70978ms step_avg:58.23ms
step:1220/2330 train_time:71039ms step_avg:58.23ms
step:1221/2330 train_time:71097ms step_avg:58.23ms
step:1222/2330 train_time:71158ms step_avg:58.23ms
step:1223/2330 train_time:71214ms step_avg:58.23ms
step:1224/2330 train_time:71274ms step_avg:58.23ms
step:1225/2330 train_time:71332ms step_avg:58.23ms
step:1226/2330 train_time:71391ms step_avg:58.23ms
step:1227/2330 train_time:71449ms step_avg:58.23ms
step:1228/2330 train_time:71509ms step_avg:58.23ms
step:1229/2330 train_time:71567ms step_avg:58.23ms
step:1230/2330 train_time:71627ms step_avg:58.23ms
step:1231/2330 train_time:71684ms step_avg:58.23ms
step:1232/2330 train_time:71745ms step_avg:58.23ms
step:1233/2330 train_time:71802ms step_avg:58.23ms
step:1234/2330 train_time:71861ms step_avg:58.23ms
step:1235/2330 train_time:71918ms step_avg:58.23ms
step:1236/2330 train_time:71978ms step_avg:58.23ms
step:1237/2330 train_time:72035ms step_avg:58.23ms
step:1238/2330 train_time:72096ms step_avg:58.24ms
step:1239/2330 train_time:72152ms step_avg:58.23ms
step:1240/2330 train_time:72212ms step_avg:58.24ms
step:1241/2330 train_time:72269ms step_avg:58.23ms
step:1242/2330 train_time:72329ms step_avg:58.24ms
step:1243/2330 train_time:72386ms step_avg:58.24ms
step:1244/2330 train_time:72446ms step_avg:58.24ms
step:1245/2330 train_time:72503ms step_avg:58.24ms
step:1246/2330 train_time:72563ms step_avg:58.24ms
step:1247/2330 train_time:72620ms step_avg:58.24ms
step:1248/2330 train_time:72680ms step_avg:58.24ms
step:1249/2330 train_time:72737ms step_avg:58.24ms
step:1250/2330 train_time:72798ms step_avg:58.24ms
step:1250/2330 val_loss:4.0105 train_time:72879ms step_avg:58.30ms
step:1251/2330 train_time:72898ms step_avg:58.27ms
step:1252/2330 train_time:72919ms step_avg:58.24ms
step:1253/2330 train_time:72976ms step_avg:58.24ms
step:1254/2330 train_time:73043ms step_avg:58.25ms
step:1255/2330 train_time:73100ms step_avg:58.25ms
step:1256/2330 train_time:73162ms step_avg:58.25ms
step:1257/2330 train_time:73218ms step_avg:58.25ms
step:1258/2330 train_time:73278ms step_avg:58.25ms
step:1259/2330 train_time:73335ms step_avg:58.25ms
step:1260/2330 train_time:73395ms step_avg:58.25ms
step:1261/2330 train_time:73452ms step_avg:58.25ms
step:1262/2330 train_time:73511ms step_avg:58.25ms
step:1263/2330 train_time:73568ms step_avg:58.25ms
step:1264/2330 train_time:73627ms step_avg:58.25ms
step:1265/2330 train_time:73683ms step_avg:58.25ms
step:1266/2330 train_time:73742ms step_avg:58.25ms
step:1267/2330 train_time:73799ms step_avg:58.25ms
step:1268/2330 train_time:73860ms step_avg:58.25ms
step:1269/2330 train_time:73918ms step_avg:58.25ms
step:1270/2330 train_time:73980ms step_avg:58.25ms
step:1271/2330 train_time:74038ms step_avg:58.25ms
step:1272/2330 train_time:74101ms step_avg:58.26ms
step:1273/2330 train_time:74158ms step_avg:58.25ms
step:1274/2330 train_time:74219ms step_avg:58.26ms
step:1275/2330 train_time:74275ms step_avg:58.25ms
step:1276/2330 train_time:74336ms step_avg:58.26ms
step:1277/2330 train_time:74393ms step_avg:58.26ms
step:1278/2330 train_time:74452ms step_avg:58.26ms
step:1279/2330 train_time:74509ms step_avg:58.26ms
step:1280/2330 train_time:74569ms step_avg:58.26ms
step:1281/2330 train_time:74627ms step_avg:58.26ms
step:1282/2330 train_time:74685ms step_avg:58.26ms
step:1283/2330 train_time:74742ms step_avg:58.26ms
step:1284/2330 train_time:74801ms step_avg:58.26ms
step:1285/2330 train_time:74858ms step_avg:58.26ms
step:1286/2330 train_time:74919ms step_avg:58.26ms
step:1287/2330 train_time:74977ms step_avg:58.26ms
step:1288/2330 train_time:75038ms step_avg:58.26ms
step:1289/2330 train_time:75094ms step_avg:58.26ms
step:1290/2330 train_time:75158ms step_avg:58.26ms
step:1291/2330 train_time:75214ms step_avg:58.26ms
step:1292/2330 train_time:75276ms step_avg:58.26ms
step:1293/2330 train_time:75333ms step_avg:58.26ms
step:1294/2330 train_time:75394ms step_avg:58.26ms
step:1295/2330 train_time:75450ms step_avg:58.26ms
step:1296/2330 train_time:75511ms step_avg:58.26ms
step:1297/2330 train_time:75567ms step_avg:58.26ms
step:1298/2330 train_time:75626ms step_avg:58.26ms
step:1299/2330 train_time:75683ms step_avg:58.26ms
step:1300/2330 train_time:75743ms step_avg:58.26ms
step:1301/2330 train_time:75800ms step_avg:58.26ms
step:1302/2330 train_time:75860ms step_avg:58.26ms
step:1303/2330 train_time:75917ms step_avg:58.26ms
step:1304/2330 train_time:75978ms step_avg:58.27ms
step:1305/2330 train_time:76035ms step_avg:58.26ms
step:1306/2330 train_time:76095ms step_avg:58.27ms
step:1307/2330 train_time:76152ms step_avg:58.26ms
step:1308/2330 train_time:76214ms step_avg:58.27ms
step:1309/2330 train_time:76271ms step_avg:58.27ms
step:1310/2330 train_time:76331ms step_avg:58.27ms
step:1311/2330 train_time:76388ms step_avg:58.27ms
step:1312/2330 train_time:76448ms step_avg:58.27ms
step:1313/2330 train_time:76504ms step_avg:58.27ms
step:1314/2330 train_time:76565ms step_avg:58.27ms
step:1315/2330 train_time:76622ms step_avg:58.27ms
step:1316/2330 train_time:76682ms step_avg:58.27ms
step:1317/2330 train_time:76739ms step_avg:58.27ms
step:1318/2330 train_time:76799ms step_avg:58.27ms
step:1319/2330 train_time:76855ms step_avg:58.27ms
step:1320/2330 train_time:76916ms step_avg:58.27ms
step:1321/2330 train_time:76974ms step_avg:58.27ms
step:1322/2330 train_time:77034ms step_avg:58.27ms
step:1323/2330 train_time:77091ms step_avg:58.27ms
step:1324/2330 train_time:77153ms step_avg:58.27ms
step:1325/2330 train_time:77209ms step_avg:58.27ms
step:1326/2330 train_time:77271ms step_avg:58.27ms
step:1327/2330 train_time:77329ms step_avg:58.27ms
step:1328/2330 train_time:77390ms step_avg:58.28ms
step:1329/2330 train_time:77448ms step_avg:58.28ms
step:1330/2330 train_time:77508ms step_avg:58.28ms
step:1331/2330 train_time:77565ms step_avg:58.28ms
step:1332/2330 train_time:77624ms step_avg:58.28ms
step:1333/2330 train_time:77682ms step_avg:58.28ms
step:1334/2330 train_time:77741ms step_avg:58.28ms
step:1335/2330 train_time:77798ms step_avg:58.28ms
step:1336/2330 train_time:77858ms step_avg:58.28ms
step:1337/2330 train_time:77916ms step_avg:58.28ms
step:1338/2330 train_time:77975ms step_avg:58.28ms
step:1339/2330 train_time:78032ms step_avg:58.28ms
step:1340/2330 train_time:78092ms step_avg:58.28ms
step:1341/2330 train_time:78149ms step_avg:58.28ms
step:1342/2330 train_time:78210ms step_avg:58.28ms
step:1343/2330 train_time:78268ms step_avg:58.28ms
step:1344/2330 train_time:78329ms step_avg:58.28ms
step:1345/2330 train_time:78386ms step_avg:58.28ms
step:1346/2330 train_time:78446ms step_avg:58.28ms
step:1347/2330 train_time:78503ms step_avg:58.28ms
step:1348/2330 train_time:78563ms step_avg:58.28ms
step:1349/2330 train_time:78621ms step_avg:58.28ms
step:1350/2330 train_time:78681ms step_avg:58.28ms
step:1351/2330 train_time:78738ms step_avg:58.28ms
step:1352/2330 train_time:78798ms step_avg:58.28ms
step:1353/2330 train_time:78855ms step_avg:58.28ms
step:1354/2330 train_time:78915ms step_avg:58.28ms
step:1355/2330 train_time:78973ms step_avg:58.28ms
step:1356/2330 train_time:79033ms step_avg:58.28ms
step:1357/2330 train_time:79091ms step_avg:58.28ms
step:1358/2330 train_time:79151ms step_avg:58.28ms
step:1359/2330 train_time:79208ms step_avg:58.28ms
step:1360/2330 train_time:79268ms step_avg:58.29ms
step:1361/2330 train_time:79325ms step_avg:58.28ms
step:1362/2330 train_time:79387ms step_avg:58.29ms
step:1363/2330 train_time:79443ms step_avg:58.29ms
step:1364/2330 train_time:79503ms step_avg:58.29ms
step:1365/2330 train_time:79560ms step_avg:58.29ms
step:1366/2330 train_time:79620ms step_avg:58.29ms
step:1367/2330 train_time:79678ms step_avg:58.29ms
step:1368/2330 train_time:79738ms step_avg:58.29ms
step:1369/2330 train_time:79794ms step_avg:58.29ms
step:1370/2330 train_time:79856ms step_avg:58.29ms
step:1371/2330 train_time:79913ms step_avg:58.29ms
step:1372/2330 train_time:79975ms step_avg:58.29ms
step:1373/2330 train_time:80031ms step_avg:58.29ms
step:1374/2330 train_time:80091ms step_avg:58.29ms
step:1375/2330 train_time:80148ms step_avg:58.29ms
step:1376/2330 train_time:80209ms step_avg:58.29ms
step:1377/2330 train_time:80267ms step_avg:58.29ms
step:1378/2330 train_time:80327ms step_avg:58.29ms
step:1379/2330 train_time:80385ms step_avg:58.29ms
step:1380/2330 train_time:80444ms step_avg:58.29ms
step:1381/2330 train_time:80501ms step_avg:58.29ms
step:1382/2330 train_time:80561ms step_avg:58.29ms
step:1383/2330 train_time:80619ms step_avg:58.29ms
step:1384/2330 train_time:80679ms step_avg:58.29ms
step:1385/2330 train_time:80736ms step_avg:58.29ms
step:1386/2330 train_time:80796ms step_avg:58.29ms
step:1387/2330 train_time:80852ms step_avg:58.29ms
step:1388/2330 train_time:80914ms step_avg:58.30ms
step:1389/2330 train_time:80970ms step_avg:58.29ms
step:1390/2330 train_time:81031ms step_avg:58.30ms
step:1391/2330 train_time:81088ms step_avg:58.29ms
step:1392/2330 train_time:81149ms step_avg:58.30ms
step:1393/2330 train_time:81206ms step_avg:58.30ms
step:1394/2330 train_time:81267ms step_avg:58.30ms
step:1395/2330 train_time:81324ms step_avg:58.30ms
step:1396/2330 train_time:81384ms step_avg:58.30ms
step:1397/2330 train_time:81442ms step_avg:58.30ms
step:1398/2330 train_time:81500ms step_avg:58.30ms
step:1399/2330 train_time:81558ms step_avg:58.30ms
step:1400/2330 train_time:81618ms step_avg:58.30ms
step:1401/2330 train_time:81674ms step_avg:58.30ms
step:1402/2330 train_time:81736ms step_avg:58.30ms
step:1403/2330 train_time:81793ms step_avg:58.30ms
step:1404/2330 train_time:81853ms step_avg:58.30ms
step:1405/2330 train_time:81910ms step_avg:58.30ms
step:1406/2330 train_time:81970ms step_avg:58.30ms
step:1407/2330 train_time:82027ms step_avg:58.30ms
step:1408/2330 train_time:82087ms step_avg:58.30ms
step:1409/2330 train_time:82144ms step_avg:58.30ms
step:1410/2330 train_time:82204ms step_avg:58.30ms
step:1411/2330 train_time:82261ms step_avg:58.30ms
step:1412/2330 train_time:82321ms step_avg:58.30ms
step:1413/2330 train_time:82379ms step_avg:58.30ms
step:1414/2330 train_time:82439ms step_avg:58.30ms
step:1415/2330 train_time:82496ms step_avg:58.30ms
step:1416/2330 train_time:82556ms step_avg:58.30ms
step:1417/2330 train_time:82613ms step_avg:58.30ms
step:1418/2330 train_time:82674ms step_avg:58.30ms
step:1419/2330 train_time:82731ms step_avg:58.30ms
step:1420/2330 train_time:82791ms step_avg:58.30ms
step:1421/2330 train_time:82848ms step_avg:58.30ms
step:1422/2330 train_time:82908ms step_avg:58.30ms
step:1423/2330 train_time:82965ms step_avg:58.30ms
step:1424/2330 train_time:83025ms step_avg:58.30ms
step:1425/2330 train_time:83083ms step_avg:58.30ms
step:1426/2330 train_time:83142ms step_avg:58.30ms
step:1427/2330 train_time:83199ms step_avg:58.30ms
step:1428/2330 train_time:83259ms step_avg:58.30ms
step:1429/2330 train_time:83316ms step_avg:58.30ms
step:1430/2330 train_time:83377ms step_avg:58.31ms
step:1431/2330 train_time:83434ms step_avg:58.30ms
step:1432/2330 train_time:83495ms step_avg:58.31ms
step:1433/2330 train_time:83551ms step_avg:58.30ms
step:1434/2330 train_time:83611ms step_avg:58.31ms
step:1435/2330 train_time:83669ms step_avg:58.31ms
step:1436/2330 train_time:83729ms step_avg:58.31ms
step:1437/2330 train_time:83787ms step_avg:58.31ms
step:1438/2330 train_time:83846ms step_avg:58.31ms
step:1439/2330 train_time:83903ms step_avg:58.31ms
step:1440/2330 train_time:83963ms step_avg:58.31ms
step:1441/2330 train_time:84020ms step_avg:58.31ms
step:1442/2330 train_time:84080ms step_avg:58.31ms
step:1443/2330 train_time:84138ms step_avg:58.31ms
step:1444/2330 train_time:84198ms step_avg:58.31ms
step:1445/2330 train_time:84255ms step_avg:58.31ms
step:1446/2330 train_time:84316ms step_avg:58.31ms
step:1447/2330 train_time:84372ms step_avg:58.31ms
step:1448/2330 train_time:84434ms step_avg:58.31ms
step:1449/2330 train_time:84491ms step_avg:58.31ms
step:1450/2330 train_time:84551ms step_avg:58.31ms
step:1451/2330 train_time:84608ms step_avg:58.31ms
step:1452/2330 train_time:84668ms step_avg:58.31ms
step:1453/2330 train_time:84725ms step_avg:58.31ms
step:1454/2330 train_time:84785ms step_avg:58.31ms
step:1455/2330 train_time:84841ms step_avg:58.31ms
step:1456/2330 train_time:84902ms step_avg:58.31ms
step:1457/2330 train_time:84959ms step_avg:58.31ms
step:1458/2330 train_time:85020ms step_avg:58.31ms
step:1459/2330 train_time:85077ms step_avg:58.31ms
step:1460/2330 train_time:85138ms step_avg:58.31ms
step:1461/2330 train_time:85195ms step_avg:58.31ms
step:1462/2330 train_time:85256ms step_avg:58.31ms
step:1463/2330 train_time:85312ms step_avg:58.31ms
step:1464/2330 train_time:85374ms step_avg:58.32ms
step:1465/2330 train_time:85431ms step_avg:58.31ms
step:1466/2330 train_time:85491ms step_avg:58.32ms
step:1467/2330 train_time:85548ms step_avg:58.31ms
step:1468/2330 train_time:85607ms step_avg:58.32ms
step:1469/2330 train_time:85664ms step_avg:58.31ms
step:1470/2330 train_time:85724ms step_avg:58.32ms
step:1471/2330 train_time:85780ms step_avg:58.31ms
step:1472/2330 train_time:85841ms step_avg:58.32ms
step:1473/2330 train_time:85898ms step_avg:58.31ms
step:1474/2330 train_time:85959ms step_avg:58.32ms
step:1475/2330 train_time:86016ms step_avg:58.32ms
step:1476/2330 train_time:86077ms step_avg:58.32ms
step:1477/2330 train_time:86133ms step_avg:58.32ms
step:1478/2330 train_time:86193ms step_avg:58.32ms
step:1479/2330 train_time:86251ms step_avg:58.32ms
step:1480/2330 train_time:86310ms step_avg:58.32ms
step:1481/2330 train_time:86368ms step_avg:58.32ms
step:1482/2330 train_time:86428ms step_avg:58.32ms
step:1483/2330 train_time:86484ms step_avg:58.32ms
step:1484/2330 train_time:86545ms step_avg:58.32ms
step:1485/2330 train_time:86601ms step_avg:58.32ms
step:1486/2330 train_time:86663ms step_avg:58.32ms
step:1487/2330 train_time:86720ms step_avg:58.32ms
step:1488/2330 train_time:86781ms step_avg:58.32ms
step:1489/2330 train_time:86838ms step_avg:58.32ms
step:1490/2330 train_time:86898ms step_avg:58.32ms
step:1491/2330 train_time:86955ms step_avg:58.32ms
step:1492/2330 train_time:87016ms step_avg:58.32ms
step:1493/2330 train_time:87072ms step_avg:58.32ms
step:1494/2330 train_time:87133ms step_avg:58.32ms
step:1495/2330 train_time:87190ms step_avg:58.32ms
step:1496/2330 train_time:87251ms step_avg:58.32ms
step:1497/2330 train_time:87308ms step_avg:58.32ms
step:1498/2330 train_time:87368ms step_avg:58.32ms
step:1499/2330 train_time:87425ms step_avg:58.32ms
step:1500/2330 train_time:87486ms step_avg:58.32ms
step:1500/2330 val_loss:3.9247 train_time:87567ms step_avg:58.38ms
step:1501/2330 train_time:87585ms step_avg:58.35ms
step:1502/2330 train_time:87605ms step_avg:58.33ms
step:1503/2330 train_time:87662ms step_avg:58.32ms
step:1504/2330 train_time:87731ms step_avg:58.33ms
step:1505/2330 train_time:87792ms step_avg:58.33ms
step:1506/2330 train_time:87854ms step_avg:58.34ms
step:1507/2330 train_time:87912ms step_avg:58.34ms
step:1508/2330 train_time:87970ms step_avg:58.34ms
step:1509/2330 train_time:88027ms step_avg:58.33ms
step:1510/2330 train_time:88087ms step_avg:58.34ms
step:1511/2330 train_time:88143ms step_avg:58.33ms
step:1512/2330 train_time:88203ms step_avg:58.34ms
step:1513/2330 train_time:88259ms step_avg:58.33ms
step:1514/2330 train_time:88318ms step_avg:58.33ms
step:1515/2330 train_time:88375ms step_avg:58.33ms
step:1516/2330 train_time:88434ms step_avg:58.33ms
step:1517/2330 train_time:88491ms step_avg:58.33ms
step:1518/2330 train_time:88551ms step_avg:58.33ms
step:1519/2330 train_time:88608ms step_avg:58.33ms
step:1520/2330 train_time:88671ms step_avg:58.34ms
step:1521/2330 train_time:88729ms step_avg:58.34ms
step:1522/2330 train_time:88792ms step_avg:58.34ms
step:1523/2330 train_time:88850ms step_avg:58.34ms
step:1524/2330 train_time:88910ms step_avg:58.34ms
step:1525/2330 train_time:88967ms step_avg:58.34ms
step:1526/2330 train_time:89026ms step_avg:58.34ms
step:1527/2330 train_time:89084ms step_avg:58.34ms
step:1528/2330 train_time:89143ms step_avg:58.34ms
step:1529/2330 train_time:89202ms step_avg:58.34ms
step:1530/2330 train_time:89260ms step_avg:58.34ms
step:1531/2330 train_time:89317ms step_avg:58.34ms
step:1532/2330 train_time:89377ms step_avg:58.34ms
step:1533/2330 train_time:89433ms step_avg:58.34ms
step:1534/2330 train_time:89494ms step_avg:58.34ms
step:1535/2330 train_time:89552ms step_avg:58.34ms
step:1536/2330 train_time:89612ms step_avg:58.34ms
step:1537/2330 train_time:89671ms step_avg:58.34ms
step:1538/2330 train_time:89732ms step_avg:58.34ms
step:1539/2330 train_time:89791ms step_avg:58.34ms
step:1540/2330 train_time:89852ms step_avg:58.35ms
step:1541/2330 train_time:89911ms step_avg:58.35ms
step:1542/2330 train_time:89971ms step_avg:58.35ms
step:1543/2330 train_time:90029ms step_avg:58.35ms
step:1544/2330 train_time:90089ms step_avg:58.35ms
step:1545/2330 train_time:90145ms step_avg:58.35ms
step:1546/2330 train_time:90207ms step_avg:58.35ms
step:1547/2330 train_time:90263ms step_avg:58.35ms
step:1548/2330 train_time:90325ms step_avg:58.35ms
step:1549/2330 train_time:90382ms step_avg:58.35ms
step:1550/2330 train_time:90444ms step_avg:58.35ms
step:1551/2330 train_time:90501ms step_avg:58.35ms
step:1552/2330 train_time:90563ms step_avg:58.35ms
step:1553/2330 train_time:90619ms step_avg:58.35ms
step:1554/2330 train_time:90682ms step_avg:58.35ms
step:1555/2330 train_time:90740ms step_avg:58.35ms
step:1556/2330 train_time:90803ms step_avg:58.36ms
step:1557/2330 train_time:90860ms step_avg:58.36ms
step:1558/2330 train_time:90922ms step_avg:58.36ms
step:1559/2330 train_time:90979ms step_avg:58.36ms
step:1560/2330 train_time:91041ms step_avg:58.36ms
step:1561/2330 train_time:91099ms step_avg:58.36ms
step:1562/2330 train_time:91161ms step_avg:58.36ms
step:1563/2330 train_time:91218ms step_avg:58.36ms
step:1564/2330 train_time:91277ms step_avg:58.36ms
step:1565/2330 train_time:91335ms step_avg:58.36ms
step:1566/2330 train_time:91395ms step_avg:58.36ms
step:1567/2330 train_time:91453ms step_avg:58.36ms
step:1568/2330 train_time:91514ms step_avg:58.36ms
step:1569/2330 train_time:91572ms step_avg:58.36ms
step:1570/2330 train_time:91632ms step_avg:58.36ms
step:1571/2330 train_time:91690ms step_avg:58.36ms
step:1572/2330 train_time:91751ms step_avg:58.37ms
step:1573/2330 train_time:91810ms step_avg:58.37ms
step:1574/2330 train_time:91871ms step_avg:58.37ms
step:1575/2330 train_time:91928ms step_avg:58.37ms
step:1576/2330 train_time:91990ms step_avg:58.37ms
step:1577/2330 train_time:92048ms step_avg:58.37ms
step:1578/2330 train_time:92108ms step_avg:58.37ms
step:1579/2330 train_time:92166ms step_avg:58.37ms
step:1580/2330 train_time:92226ms step_avg:58.37ms
step:1581/2330 train_time:92283ms step_avg:58.37ms
step:1582/2330 train_time:92344ms step_avg:58.37ms
step:1583/2330 train_time:92401ms step_avg:58.37ms
step:1584/2330 train_time:92464ms step_avg:58.37ms
step:1585/2330 train_time:92521ms step_avg:58.37ms
step:1586/2330 train_time:92583ms step_avg:58.38ms
step:1587/2330 train_time:92639ms step_avg:58.37ms
step:1588/2330 train_time:92704ms step_avg:58.38ms
step:1589/2330 train_time:92760ms step_avg:58.38ms
step:1590/2330 train_time:92822ms step_avg:58.38ms
step:1591/2330 train_time:92880ms step_avg:58.38ms
step:1592/2330 train_time:92943ms step_avg:58.38ms
step:1593/2330 train_time:93000ms step_avg:58.38ms
step:1594/2330 train_time:93062ms step_avg:58.38ms
step:1595/2330 train_time:93120ms step_avg:58.38ms
step:1596/2330 train_time:93180ms step_avg:58.38ms
step:1597/2330 train_time:93237ms step_avg:58.38ms
step:1598/2330 train_time:93298ms step_avg:58.38ms
step:1599/2330 train_time:93356ms step_avg:58.38ms
step:1600/2330 train_time:93416ms step_avg:58.39ms
step:1601/2330 train_time:93474ms step_avg:58.38ms
step:1602/2330 train_time:93534ms step_avg:58.39ms
step:1603/2330 train_time:93592ms step_avg:58.39ms
step:1604/2330 train_time:93653ms step_avg:58.39ms
step:1605/2330 train_time:93712ms step_avg:58.39ms
step:1606/2330 train_time:93772ms step_avg:58.39ms
step:1607/2330 train_time:93830ms step_avg:58.39ms
step:1608/2330 train_time:93890ms step_avg:58.39ms
step:1609/2330 train_time:93949ms step_avg:58.39ms
step:1610/2330 train_time:94009ms step_avg:58.39ms
step:1611/2330 train_time:94066ms step_avg:58.39ms
step:1612/2330 train_time:94127ms step_avg:58.39ms
step:1613/2330 train_time:94185ms step_avg:58.39ms
step:1614/2330 train_time:94247ms step_avg:58.39ms
step:1615/2330 train_time:94304ms step_avg:58.39ms
step:1616/2330 train_time:94366ms step_avg:58.39ms
step:1617/2330 train_time:94422ms step_avg:58.39ms
step:1618/2330 train_time:94484ms step_avg:58.40ms
step:1619/2330 train_time:94542ms step_avg:58.40ms
step:1620/2330 train_time:94604ms step_avg:58.40ms
step:1621/2330 train_time:94660ms step_avg:58.40ms
step:1622/2330 train_time:94723ms step_avg:58.40ms
step:1623/2330 train_time:94780ms step_avg:58.40ms
step:1624/2330 train_time:94841ms step_avg:58.40ms
step:1625/2330 train_time:94898ms step_avg:58.40ms
step:1626/2330 train_time:94961ms step_avg:58.40ms
step:1627/2330 train_time:95018ms step_avg:58.40ms
step:1628/2330 train_time:95081ms step_avg:58.40ms
step:1629/2330 train_time:95138ms step_avg:58.40ms
step:1630/2330 train_time:95199ms step_avg:58.40ms
step:1631/2330 train_time:95257ms step_avg:58.40ms
step:1632/2330 train_time:95317ms step_avg:58.40ms
step:1633/2330 train_time:95374ms step_avg:58.40ms
step:1634/2330 train_time:95435ms step_avg:58.41ms
step:1635/2330 train_time:95493ms step_avg:58.41ms
step:1636/2330 train_time:95554ms step_avg:58.41ms
step:1637/2330 train_time:95611ms step_avg:58.41ms
step:1638/2330 train_time:95672ms step_avg:58.41ms
step:1639/2330 train_time:95730ms step_avg:58.41ms
step:1640/2330 train_time:95791ms step_avg:58.41ms
step:1641/2330 train_time:95849ms step_avg:58.41ms
step:1642/2330 train_time:95910ms step_avg:58.41ms
step:1643/2330 train_time:95967ms step_avg:58.41ms
step:1644/2330 train_time:96028ms step_avg:58.41ms
step:1645/2330 train_time:96085ms step_avg:58.41ms
step:1646/2330 train_time:96147ms step_avg:58.41ms
step:1647/2330 train_time:96205ms step_avg:58.41ms
step:1648/2330 train_time:96266ms step_avg:58.41ms
step:1649/2330 train_time:96322ms step_avg:58.41ms
step:1650/2330 train_time:96384ms step_avg:58.41ms
step:1651/2330 train_time:96441ms step_avg:58.41ms
step:1652/2330 train_time:96503ms step_avg:58.42ms
step:1653/2330 train_time:96560ms step_avg:58.41ms
step:1654/2330 train_time:96622ms step_avg:58.42ms
step:1655/2330 train_time:96679ms step_avg:58.42ms
step:1656/2330 train_time:96740ms step_avg:58.42ms
step:1657/2330 train_time:96797ms step_avg:58.42ms
step:1658/2330 train_time:96860ms step_avg:58.42ms
step:1659/2330 train_time:96917ms step_avg:58.42ms
step:1660/2330 train_time:96979ms step_avg:58.42ms
step:1661/2330 train_time:97036ms step_avg:58.42ms
step:1662/2330 train_time:97098ms step_avg:58.42ms
step:1663/2330 train_time:97156ms step_avg:58.42ms
step:1664/2330 train_time:97217ms step_avg:58.42ms
step:1665/2330 train_time:97274ms step_avg:58.42ms
step:1666/2330 train_time:97334ms step_avg:58.42ms
step:1667/2330 train_time:97392ms step_avg:58.42ms
step:1668/2330 train_time:97452ms step_avg:58.42ms
step:1669/2330 train_time:97509ms step_avg:58.42ms
step:1670/2330 train_time:97570ms step_avg:58.43ms
step:1671/2330 train_time:97627ms step_avg:58.42ms
step:1672/2330 train_time:97689ms step_avg:58.43ms
step:1673/2330 train_time:97747ms step_avg:58.43ms
step:1674/2330 train_time:97808ms step_avg:58.43ms
step:1675/2330 train_time:97864ms step_avg:58.43ms
step:1676/2330 train_time:97927ms step_avg:58.43ms
step:1677/2330 train_time:97983ms step_avg:58.43ms
step:1678/2330 train_time:98047ms step_avg:58.43ms
step:1679/2330 train_time:98104ms step_avg:58.43ms
step:1680/2330 train_time:98166ms step_avg:58.43ms
step:1681/2330 train_time:98223ms step_avg:58.43ms
step:1682/2330 train_time:98285ms step_avg:58.43ms
step:1683/2330 train_time:98342ms step_avg:58.43ms
step:1684/2330 train_time:98404ms step_avg:58.43ms
step:1685/2330 train_time:98461ms step_avg:58.43ms
step:1686/2330 train_time:98522ms step_avg:58.44ms
step:1687/2330 train_time:98578ms step_avg:58.43ms
step:1688/2330 train_time:98640ms step_avg:58.44ms
step:1689/2330 train_time:98697ms step_avg:58.44ms
step:1690/2330 train_time:98760ms step_avg:58.44ms
step:1691/2330 train_time:98817ms step_avg:58.44ms
step:1692/2330 train_time:98879ms step_avg:58.44ms
step:1693/2330 train_time:98936ms step_avg:58.44ms
step:1694/2330 train_time:98999ms step_avg:58.44ms
step:1695/2330 train_time:99057ms step_avg:58.44ms
step:1696/2330 train_time:99118ms step_avg:58.44ms
step:1697/2330 train_time:99176ms step_avg:58.44ms
step:1698/2330 train_time:99237ms step_avg:58.44ms
step:1699/2330 train_time:99295ms step_avg:58.44ms
step:1700/2330 train_time:99355ms step_avg:58.44ms
step:1701/2330 train_time:99413ms step_avg:58.44ms
step:1702/2330 train_time:99474ms step_avg:58.45ms
step:1703/2330 train_time:99532ms step_avg:58.45ms
step:1704/2330 train_time:99592ms step_avg:58.45ms
step:1705/2330 train_time:99649ms step_avg:58.45ms
step:1706/2330 train_time:99711ms step_avg:58.45ms
step:1707/2330 train_time:99769ms step_avg:58.45ms
step:1708/2330 train_time:99829ms step_avg:58.45ms
step:1709/2330 train_time:99886ms step_avg:58.45ms
step:1710/2330 train_time:99948ms step_avg:58.45ms
step:1711/2330 train_time:100006ms step_avg:58.45ms
step:1712/2330 train_time:100068ms step_avg:58.45ms
step:1713/2330 train_time:100124ms step_avg:58.45ms
step:1714/2330 train_time:100186ms step_avg:58.45ms
step:1715/2330 train_time:100243ms step_avg:58.45ms
step:1716/2330 train_time:100305ms step_avg:58.45ms
step:1717/2330 train_time:100362ms step_avg:58.45ms
step:1718/2330 train_time:100422ms step_avg:58.45ms
step:1719/2330 train_time:100479ms step_avg:58.45ms
step:1720/2330 train_time:100541ms step_avg:58.45ms
step:1721/2330 train_time:100598ms step_avg:58.45ms
step:1722/2330 train_time:100661ms step_avg:58.46ms
step:1723/2330 train_time:100718ms step_avg:58.45ms
step:1724/2330 train_time:100779ms step_avg:58.46ms
step:1725/2330 train_time:100837ms step_avg:58.46ms
step:1726/2330 train_time:100899ms step_avg:58.46ms
step:1727/2330 train_time:100958ms step_avg:58.46ms
step:1728/2330 train_time:101018ms step_avg:58.46ms
step:1729/2330 train_time:101075ms step_avg:58.46ms
step:1730/2330 train_time:101136ms step_avg:58.46ms
step:1731/2330 train_time:101194ms step_avg:58.46ms
step:1732/2330 train_time:101255ms step_avg:58.46ms
step:1733/2330 train_time:101313ms step_avg:58.46ms
step:1734/2330 train_time:101373ms step_avg:58.46ms
step:1735/2330 train_time:101431ms step_avg:58.46ms
step:1736/2330 train_time:101491ms step_avg:58.46ms
step:1737/2330 train_time:101548ms step_avg:58.46ms
step:1738/2330 train_time:101609ms step_avg:58.46ms
step:1739/2330 train_time:101665ms step_avg:58.46ms
step:1740/2330 train_time:101727ms step_avg:58.46ms
step:1741/2330 train_time:101784ms step_avg:58.46ms
step:1742/2330 train_time:101847ms step_avg:58.47ms
step:1743/2330 train_time:101903ms step_avg:58.46ms
step:1744/2330 train_time:101966ms step_avg:58.47ms
step:1745/2330 train_time:102022ms step_avg:58.47ms
step:1746/2330 train_time:102084ms step_avg:58.47ms
step:1747/2330 train_time:102141ms step_avg:58.47ms
step:1748/2330 train_time:102204ms step_avg:58.47ms
step:1749/2330 train_time:102261ms step_avg:58.47ms
step:1750/2330 train_time:102322ms step_avg:58.47ms
step:1750/2330 val_loss:3.8391 train_time:102404ms step_avg:58.52ms
step:1751/2330 train_time:102424ms step_avg:58.49ms
step:1752/2330 train_time:102443ms step_avg:58.47ms
step:1753/2330 train_time:102498ms step_avg:58.47ms
step:1754/2330 train_time:102569ms step_avg:58.48ms
step:1755/2330 train_time:102625ms step_avg:58.48ms
step:1756/2330 train_time:102689ms step_avg:58.48ms
step:1757/2330 train_time:102746ms step_avg:58.48ms
step:1758/2330 train_time:102807ms step_avg:58.48ms
step:1759/2330 train_time:102864ms step_avg:58.48ms
step:1760/2330 train_time:102924ms step_avg:58.48ms
step:1761/2330 train_time:102981ms step_avg:58.48ms
step:1762/2330 train_time:103040ms step_avg:58.48ms
step:1763/2330 train_time:103097ms step_avg:58.48ms
step:1764/2330 train_time:103157ms step_avg:58.48ms
step:1765/2330 train_time:103214ms step_avg:58.48ms
step:1766/2330 train_time:103274ms step_avg:58.48ms
step:1767/2330 train_time:103332ms step_avg:58.48ms
step:1768/2330 train_time:103395ms step_avg:58.48ms
step:1769/2330 train_time:103455ms step_avg:58.48ms
step:1770/2330 train_time:103517ms step_avg:58.48ms
step:1771/2330 train_time:103576ms step_avg:58.48ms
step:1772/2330 train_time:103637ms step_avg:58.49ms
step:1773/2330 train_time:103695ms step_avg:58.49ms
step:1774/2330 train_time:103755ms step_avg:58.49ms
step:1775/2330 train_time:103812ms step_avg:58.49ms
step:1776/2330 train_time:103873ms step_avg:58.49ms
step:1777/2330 train_time:103929ms step_avg:58.49ms
step:1778/2330 train_time:103991ms step_avg:58.49ms
step:1779/2330 train_time:104048ms step_avg:58.49ms
step:1780/2330 train_time:104109ms step_avg:58.49ms
step:1781/2330 train_time:104165ms step_avg:58.49ms
step:1782/2330 train_time:104225ms step_avg:58.49ms
step:1783/2330 train_time:104284ms step_avg:58.49ms
step:1784/2330 train_time:104345ms step_avg:58.49ms
step:1785/2330 train_time:104403ms step_avg:58.49ms
step:1786/2330 train_time:104464ms step_avg:58.49ms
step:1787/2330 train_time:104522ms step_avg:58.49ms
step:1788/2330 train_time:104585ms step_avg:58.49ms
step:1789/2330 train_time:104643ms step_avg:58.49ms
step:1790/2330 train_time:104706ms step_avg:58.49ms
step:1791/2330 train_time:104764ms step_avg:58.49ms
step:1792/2330 train_time:104825ms step_avg:58.50ms
step:1793/2330 train_time:104883ms step_avg:58.50ms
step:1794/2330 train_time:104943ms step_avg:58.50ms
step:1795/2330 train_time:105000ms step_avg:58.50ms
step:1796/2330 train_time:105060ms step_avg:58.50ms
step:1797/2330 train_time:105118ms step_avg:58.50ms
step:1798/2330 train_time:105178ms step_avg:58.50ms
step:1799/2330 train_time:105235ms step_avg:58.50ms
step:1800/2330 train_time:105297ms step_avg:58.50ms
step:1801/2330 train_time:105355ms step_avg:58.50ms
step:1802/2330 train_time:105416ms step_avg:58.50ms
step:1803/2330 train_time:105475ms step_avg:58.50ms
step:1804/2330 train_time:105535ms step_avg:58.50ms
step:1805/2330 train_time:105592ms step_avg:58.50ms
step:1806/2330 train_time:105656ms step_avg:58.50ms
step:1807/2330 train_time:105713ms step_avg:58.50ms
step:1808/2330 train_time:105774ms step_avg:58.50ms
step:1809/2330 train_time:105831ms step_avg:58.50ms
step:1810/2330 train_time:105893ms step_avg:58.50ms
step:1811/2330 train_time:105950ms step_avg:58.50ms
step:1812/2330 train_time:106011ms step_avg:58.51ms
step:1813/2330 train_time:106068ms step_avg:58.50ms
step:1814/2330 train_time:106129ms step_avg:58.51ms
step:1815/2330 train_time:106186ms step_avg:58.50ms
step:1816/2330 train_time:106247ms step_avg:58.51ms
step:1817/2330 train_time:106304ms step_avg:58.51ms
step:1818/2330 train_time:106366ms step_avg:58.51ms
step:1819/2330 train_time:106424ms step_avg:58.51ms
step:1820/2330 train_time:106485ms step_avg:58.51ms
step:1821/2330 train_time:106544ms step_avg:58.51ms
step:1822/2330 train_time:106606ms step_avg:58.51ms
step:1823/2330 train_time:106665ms step_avg:58.51ms
step:1824/2330 train_time:106726ms step_avg:58.51ms
step:1825/2330 train_time:106784ms step_avg:58.51ms
step:1826/2330 train_time:106844ms step_avg:58.51ms
step:1827/2330 train_time:106902ms step_avg:58.51ms
step:1828/2330 train_time:106961ms step_avg:58.51ms
step:1829/2330 train_time:107019ms step_avg:58.51ms
step:1830/2330 train_time:107080ms step_avg:58.51ms
step:1831/2330 train_time:107137ms step_avg:58.51ms
step:1832/2330 train_time:107197ms step_avg:58.51ms
step:1833/2330 train_time:107255ms step_avg:58.51ms
step:1834/2330 train_time:107316ms step_avg:58.51ms
step:1835/2330 train_time:107373ms step_avg:58.51ms
step:1836/2330 train_time:107434ms step_avg:58.52ms
step:1837/2330 train_time:107492ms step_avg:58.52ms
step:1838/2330 train_time:107554ms step_avg:58.52ms
step:1839/2330 train_time:107611ms step_avg:58.52ms
step:1840/2330 train_time:107673ms step_avg:58.52ms
step:1841/2330 train_time:107730ms step_avg:58.52ms
step:1842/2330 train_time:107791ms step_avg:58.52ms
step:1843/2330 train_time:107849ms step_avg:58.52ms
step:1844/2330 train_time:107910ms step_avg:58.52ms
step:1845/2330 train_time:107967ms step_avg:58.52ms
step:1846/2330 train_time:108028ms step_avg:58.52ms
step:1847/2330 train_time:108085ms step_avg:58.52ms
step:1848/2330 train_time:108146ms step_avg:58.52ms
step:1849/2330 train_time:108203ms step_avg:58.52ms
step:1850/2330 train_time:108265ms step_avg:58.52ms
step:1851/2330 train_time:108323ms step_avg:58.52ms
step:1852/2330 train_time:108384ms step_avg:58.52ms
step:1853/2330 train_time:108442ms step_avg:58.52ms
step:1854/2330 train_time:108505ms step_avg:58.52ms
step:1855/2330 train_time:108563ms step_avg:58.52ms
step:1856/2330 train_time:108624ms step_avg:58.53ms
step:1857/2330 train_time:108683ms step_avg:58.53ms
step:1858/2330 train_time:108742ms step_avg:58.53ms
step:1859/2330 train_time:108801ms step_avg:58.53ms
step:1860/2330 train_time:108862ms step_avg:58.53ms
step:1861/2330 train_time:108920ms step_avg:58.53ms
step:1862/2330 train_time:108980ms step_avg:58.53ms
step:1863/2330 train_time:109038ms step_avg:58.53ms
step:1864/2330 train_time:109098ms step_avg:58.53ms
step:1865/2330 train_time:109156ms step_avg:58.53ms
step:1866/2330 train_time:109216ms step_avg:58.53ms
step:1867/2330 train_time:109273ms step_avg:58.53ms
step:1868/2330 train_time:109334ms step_avg:58.53ms
step:1869/2330 train_time:109391ms step_avg:58.53ms
step:1870/2330 train_time:109453ms step_avg:58.53ms
step:1871/2330 train_time:109511ms step_avg:58.53ms
step:1872/2330 train_time:109572ms step_avg:58.53ms
step:1873/2330 train_time:109630ms step_avg:58.53ms
step:1874/2330 train_time:109691ms step_avg:58.53ms
step:1875/2330 train_time:109748ms step_avg:58.53ms
step:1876/2330 train_time:109809ms step_avg:58.53ms
step:1877/2330 train_time:109866ms step_avg:58.53ms
step:1878/2330 train_time:109928ms step_avg:58.53ms
step:1879/2330 train_time:109985ms step_avg:58.53ms
step:1880/2330 train_time:110045ms step_avg:58.53ms
step:1881/2330 train_time:110104ms step_avg:58.53ms
step:1882/2330 train_time:110164ms step_avg:58.54ms
step:1883/2330 train_time:110222ms step_avg:58.54ms
step:1884/2330 train_time:110283ms step_avg:58.54ms
step:1885/2330 train_time:110340ms step_avg:58.54ms
step:1886/2330 train_time:110401ms step_avg:58.54ms
step:1887/2330 train_time:110460ms step_avg:58.54ms
step:1888/2330 train_time:110520ms step_avg:58.54ms
step:1889/2330 train_time:110578ms step_avg:58.54ms
step:1890/2330 train_time:110639ms step_avg:58.54ms
step:1891/2330 train_time:110698ms step_avg:58.54ms
step:1892/2330 train_time:110758ms step_avg:58.54ms
step:1893/2330 train_time:110816ms step_avg:58.54ms
step:1894/2330 train_time:110876ms step_avg:58.54ms
step:1895/2330 train_time:110933ms step_avg:58.54ms
step:1896/2330 train_time:110994ms step_avg:58.54ms
step:1897/2330 train_time:111051ms step_avg:58.54ms
step:1898/2330 train_time:111113ms step_avg:58.54ms
step:1899/2330 train_time:111170ms step_avg:58.54ms
step:1900/2330 train_time:111231ms step_avg:58.54ms
step:1901/2330 train_time:111289ms step_avg:58.54ms
step:1902/2330 train_time:111351ms step_avg:58.54ms
step:1903/2330 train_time:111407ms step_avg:58.54ms
step:1904/2330 train_time:111470ms step_avg:58.55ms
step:1905/2330 train_time:111527ms step_avg:58.54ms
step:1906/2330 train_time:111589ms step_avg:58.55ms
step:1907/2330 train_time:111646ms step_avg:58.55ms
step:1908/2330 train_time:111708ms step_avg:58.55ms
step:1909/2330 train_time:111765ms step_avg:58.55ms
step:1910/2330 train_time:111827ms step_avg:58.55ms
step:1911/2330 train_time:111885ms step_avg:58.55ms
step:1912/2330 train_time:111947ms step_avg:58.55ms
step:1913/2330 train_time:112005ms step_avg:58.55ms
step:1914/2330 train_time:112066ms step_avg:58.55ms
step:1915/2330 train_time:112124ms step_avg:58.55ms
step:1916/2330 train_time:112184ms step_avg:58.55ms
step:1917/2330 train_time:112242ms step_avg:58.55ms
step:1918/2330 train_time:112303ms step_avg:58.55ms
step:1919/2330 train_time:112362ms step_avg:58.55ms
step:1920/2330 train_time:112421ms step_avg:58.55ms
step:1921/2330 train_time:112479ms step_avg:58.55ms
step:1922/2330 train_time:112540ms step_avg:58.55ms
step:1923/2330 train_time:112598ms step_avg:58.55ms
step:1924/2330 train_time:112659ms step_avg:58.55ms
step:1925/2330 train_time:112717ms step_avg:58.55ms
step:1926/2330 train_time:112777ms step_avg:58.55ms
step:1927/2330 train_time:112834ms step_avg:58.55ms
step:1928/2330 train_time:112895ms step_avg:58.56ms
step:1929/2330 train_time:112953ms step_avg:58.56ms
step:1930/2330 train_time:113014ms step_avg:58.56ms
step:1931/2330 train_time:113072ms step_avg:58.56ms
step:1932/2330 train_time:113132ms step_avg:58.56ms
step:1933/2330 train_time:113189ms step_avg:58.56ms
step:1934/2330 train_time:113251ms step_avg:58.56ms
step:1935/2330 train_time:113309ms step_avg:58.56ms
step:1936/2330 train_time:113370ms step_avg:58.56ms
step:1937/2330 train_time:113427ms step_avg:58.56ms
step:1938/2330 train_time:113488ms step_avg:58.56ms
step:1939/2330 train_time:113545ms step_avg:58.56ms
step:1940/2330 train_time:113609ms step_avg:58.56ms
step:1941/2330 train_time:113666ms step_avg:58.56ms
step:1942/2330 train_time:113727ms step_avg:58.56ms
step:1943/2330 train_time:113785ms step_avg:58.56ms
step:1944/2330 train_time:113847ms step_avg:58.56ms
step:1945/2330 train_time:113904ms step_avg:58.56ms
step:1946/2330 train_time:113966ms step_avg:58.56ms
step:1947/2330 train_time:114024ms step_avg:58.56ms
step:1948/2330 train_time:114085ms step_avg:58.57ms
step:1949/2330 train_time:114142ms step_avg:58.56ms
step:1950/2330 train_time:114203ms step_avg:58.57ms
step:1951/2330 train_time:114261ms step_avg:58.57ms
step:1952/2330 train_time:114321ms step_avg:58.57ms
step:1953/2330 train_time:114380ms step_avg:58.57ms
step:1954/2330 train_time:114440ms step_avg:58.57ms
step:1955/2330 train_time:114498ms step_avg:58.57ms
step:1956/2330 train_time:114558ms step_avg:58.57ms
step:1957/2330 train_time:114616ms step_avg:58.57ms
step:1958/2330 train_time:114677ms step_avg:58.57ms
step:1959/2330 train_time:114734ms step_avg:58.57ms
step:1960/2330 train_time:114796ms step_avg:58.57ms
step:1961/2330 train_time:114853ms step_avg:58.57ms
step:1962/2330 train_time:114914ms step_avg:58.57ms
step:1963/2330 train_time:114971ms step_avg:58.57ms
step:1964/2330 train_time:115033ms step_avg:58.57ms
step:1965/2330 train_time:115090ms step_avg:58.57ms
step:1966/2330 train_time:115153ms step_avg:58.57ms
step:1967/2330 train_time:115210ms step_avg:58.57ms
step:1968/2330 train_time:115271ms step_avg:58.57ms
step:1969/2330 train_time:115328ms step_avg:58.57ms
step:1970/2330 train_time:115390ms step_avg:58.57ms
step:1971/2330 train_time:115446ms step_avg:58.57ms
step:1972/2330 train_time:115509ms step_avg:58.57ms
step:1973/2330 train_time:115566ms step_avg:58.57ms
step:1974/2330 train_time:115627ms step_avg:58.57ms
step:1975/2330 train_time:115684ms step_avg:58.57ms
step:1976/2330 train_time:115745ms step_avg:58.58ms
step:1977/2330 train_time:115803ms step_avg:58.58ms
step:1978/2330 train_time:115863ms step_avg:58.58ms
step:1979/2330 train_time:115922ms step_avg:58.58ms
step:1980/2330 train_time:115982ms step_avg:58.58ms
step:1981/2330 train_time:116039ms step_avg:58.58ms
step:1982/2330 train_time:116102ms step_avg:58.58ms
step:1983/2330 train_time:116160ms step_avg:58.58ms
step:1984/2330 train_time:116221ms step_avg:58.58ms
step:1985/2330 train_time:116279ms step_avg:58.58ms
step:1986/2330 train_time:116340ms step_avg:58.58ms
step:1987/2330 train_time:116398ms step_avg:58.58ms
step:1988/2330 train_time:116458ms step_avg:58.58ms
step:1989/2330 train_time:116515ms step_avg:58.58ms
step:1990/2330 train_time:116576ms step_avg:58.58ms
step:1991/2330 train_time:116633ms step_avg:58.58ms
step:1992/2330 train_time:116695ms step_avg:58.58ms
step:1993/2330 train_time:116753ms step_avg:58.58ms
step:1994/2330 train_time:116815ms step_avg:58.58ms
step:1995/2330 train_time:116872ms step_avg:58.58ms
step:1996/2330 train_time:116933ms step_avg:58.58ms
step:1997/2330 train_time:116990ms step_avg:58.58ms
step:1998/2330 train_time:117052ms step_avg:58.58ms
step:1999/2330 train_time:117109ms step_avg:58.58ms
step:2000/2330 train_time:117171ms step_avg:58.59ms
step:2000/2330 val_loss:3.7778 train_time:117254ms step_avg:58.63ms
step:2001/2330 train_time:117272ms step_avg:58.61ms
step:2002/2330 train_time:117293ms step_avg:58.59ms
step:2003/2330 train_time:117350ms step_avg:58.59ms
step:2004/2330 train_time:117422ms step_avg:58.59ms
step:2005/2330 train_time:117478ms step_avg:58.59ms
step:2006/2330 train_time:117539ms step_avg:58.59ms
step:2007/2330 train_time:117596ms step_avg:58.59ms
step:2008/2330 train_time:117656ms step_avg:58.59ms
step:2009/2330 train_time:117713ms step_avg:58.59ms
step:2010/2330 train_time:117774ms step_avg:58.59ms
step:2011/2330 train_time:117831ms step_avg:58.59ms
step:2012/2330 train_time:117891ms step_avg:58.59ms
step:2013/2330 train_time:117947ms step_avg:58.59ms
step:2014/2330 train_time:118007ms step_avg:58.59ms
step:2015/2330 train_time:118063ms step_avg:58.59ms
step:2016/2330 train_time:118125ms step_avg:58.59ms
step:2017/2330 train_time:118182ms step_avg:58.59ms
step:2018/2330 train_time:118244ms step_avg:58.59ms
step:2019/2330 train_time:118302ms step_avg:58.59ms
step:2020/2330 train_time:118369ms step_avg:58.60ms
step:2021/2330 train_time:118428ms step_avg:58.60ms
step:2022/2330 train_time:118490ms step_avg:58.60ms
step:2023/2330 train_time:118548ms step_avg:58.60ms
step:2024/2330 train_time:118608ms step_avg:58.60ms
step:2025/2330 train_time:118665ms step_avg:58.60ms
step:2026/2330 train_time:118727ms step_avg:58.60ms
step:2027/2330 train_time:118784ms step_avg:58.60ms
step:2028/2330 train_time:118844ms step_avg:58.60ms
step:2029/2330 train_time:118902ms step_avg:58.60ms
step:2030/2330 train_time:118962ms step_avg:58.60ms
step:2031/2330 train_time:119018ms step_avg:58.60ms
step:2032/2330 train_time:119078ms step_avg:58.60ms
step:2033/2330 train_time:119135ms step_avg:58.60ms
step:2034/2330 train_time:119196ms step_avg:58.60ms
step:2035/2330 train_time:119253ms step_avg:58.60ms
step:2036/2330 train_time:119316ms step_avg:58.60ms
step:2037/2330 train_time:119373ms step_avg:58.60ms
step:2038/2330 train_time:119437ms step_avg:58.60ms
step:2039/2330 train_time:119493ms step_avg:58.60ms
step:2040/2330 train_time:119556ms step_avg:58.61ms
step:2041/2330 train_time:119613ms step_avg:58.60ms
step:2042/2330 train_time:119675ms step_avg:58.61ms
step:2043/2330 train_time:119732ms step_avg:58.61ms
step:2044/2330 train_time:119793ms step_avg:58.61ms
step:2045/2330 train_time:119849ms step_avg:58.61ms
step:2046/2330 train_time:119911ms step_avg:58.61ms
step:2047/2330 train_time:119967ms step_avg:58.61ms
step:2048/2330 train_time:120028ms step_avg:58.61ms
step:2049/2330 train_time:120085ms step_avg:58.61ms
step:2050/2330 train_time:120146ms step_avg:58.61ms
step:2051/2330 train_time:120204ms step_avg:58.61ms
step:2052/2330 train_time:120266ms step_avg:58.61ms
step:2053/2330 train_time:120324ms step_avg:58.61ms
step:2054/2330 train_time:120387ms step_avg:58.61ms
step:2055/2330 train_time:120444ms step_avg:58.61ms
step:2056/2330 train_time:120507ms step_avg:58.61ms
step:2057/2330 train_time:120565ms step_avg:58.61ms
step:2058/2330 train_time:120627ms step_avg:58.61ms
step:2059/2330 train_time:120685ms step_avg:58.61ms
step:2060/2330 train_time:120745ms step_avg:58.61ms
step:2061/2330 train_time:120803ms step_avg:58.61ms
step:2062/2330 train_time:120864ms step_avg:58.61ms
step:2063/2330 train_time:120921ms step_avg:58.61ms
step:2064/2330 train_time:120982ms step_avg:58.62ms
step:2065/2330 train_time:121039ms step_avg:58.61ms
step:2066/2330 train_time:121099ms step_avg:58.62ms
step:2067/2330 train_time:121157ms step_avg:58.61ms
step:2068/2330 train_time:121217ms step_avg:58.62ms
step:2069/2330 train_time:121275ms step_avg:58.62ms
step:2070/2330 train_time:121337ms step_avg:58.62ms
step:2071/2330 train_time:121393ms step_avg:58.62ms
step:2072/2330 train_time:121455ms step_avg:58.62ms
step:2073/2330 train_time:121512ms step_avg:58.62ms
step:2074/2330 train_time:121574ms step_avg:58.62ms
step:2075/2330 train_time:121632ms step_avg:58.62ms
step:2076/2330 train_time:121694ms step_avg:58.62ms
step:2077/2330 train_time:121750ms step_avg:58.62ms
step:2078/2330 train_time:121812ms step_avg:58.62ms
step:2079/2330 train_time:121868ms step_avg:58.62ms
step:2080/2330 train_time:121931ms step_avg:58.62ms
step:2081/2330 train_time:121988ms step_avg:58.62ms
step:2082/2330 train_time:122048ms step_avg:58.62ms
step:2083/2330 train_time:122105ms step_avg:58.62ms
step:2084/2330 train_time:122167ms step_avg:58.62ms
step:2085/2330 train_time:122225ms step_avg:58.62ms
step:2086/2330 train_time:122287ms step_avg:58.62ms
step:2087/2330 train_time:122345ms step_avg:58.62ms
step:2088/2330 train_time:122406ms step_avg:58.62ms
step:2089/2330 train_time:122463ms step_avg:58.62ms
step:2090/2330 train_time:122525ms step_avg:58.62ms
step:2091/2330 train_time:122584ms step_avg:58.62ms
step:2092/2330 train_time:122644ms step_avg:58.63ms
step:2093/2330 train_time:122702ms step_avg:58.63ms
step:2094/2330 train_time:122763ms step_avg:58.63ms
step:2095/2330 train_time:122821ms step_avg:58.63ms
step:2096/2330 train_time:122880ms step_avg:58.63ms
step:2097/2330 train_time:122938ms step_avg:58.63ms
step:2098/2330 train_time:122998ms step_avg:58.63ms
step:2099/2330 train_time:123054ms step_avg:58.63ms
step:2100/2330 train_time:123116ms step_avg:58.63ms
step:2101/2330 train_time:123173ms step_avg:58.63ms
step:2102/2330 train_time:123235ms step_avg:58.63ms
step:2103/2330 train_time:123292ms step_avg:58.63ms
step:2104/2330 train_time:123354ms step_avg:58.63ms
step:2105/2330 train_time:123411ms step_avg:58.63ms
step:2106/2330 train_time:123473ms step_avg:58.63ms
step:2107/2330 train_time:123531ms step_avg:58.63ms
step:2108/2330 train_time:123592ms step_avg:58.63ms
step:2109/2330 train_time:123649ms step_avg:58.63ms
step:2110/2330 train_time:123711ms step_avg:58.63ms
step:2111/2330 train_time:123769ms step_avg:58.63ms
step:2112/2330 train_time:123830ms step_avg:58.63ms
step:2113/2330 train_time:123887ms step_avg:58.63ms
step:2114/2330 train_time:123947ms step_avg:58.63ms
step:2115/2330 train_time:124005ms step_avg:58.63ms
step:2116/2330 train_time:124065ms step_avg:58.63ms
step:2117/2330 train_time:124123ms step_avg:58.63ms
step:2118/2330 train_time:124184ms step_avg:58.63ms
step:2119/2330 train_time:124242ms step_avg:58.63ms
step:2120/2330 train_time:124303ms step_avg:58.63ms
step:2121/2330 train_time:124360ms step_avg:58.63ms
step:2122/2330 train_time:124420ms step_avg:58.63ms
step:2123/2330 train_time:124478ms step_avg:58.63ms
step:2124/2330 train_time:124538ms step_avg:58.63ms
step:2125/2330 train_time:124596ms step_avg:58.63ms
step:2126/2330 train_time:124657ms step_avg:58.63ms
step:2127/2330 train_time:124714ms step_avg:58.63ms
step:2128/2330 train_time:124776ms step_avg:58.64ms
step:2129/2330 train_time:124832ms step_avg:58.63ms
step:2130/2330 train_time:124895ms step_avg:58.64ms
step:2131/2330 train_time:124951ms step_avg:58.63ms
step:2132/2330 train_time:125014ms step_avg:58.64ms
step:2133/2330 train_time:125071ms step_avg:58.64ms
step:2134/2330 train_time:125133ms step_avg:58.64ms
step:2135/2330 train_time:125189ms step_avg:58.64ms
step:2136/2330 train_time:125251ms step_avg:58.64ms
step:2137/2330 train_time:125308ms step_avg:58.64ms
step:2138/2330 train_time:125370ms step_avg:58.64ms
step:2139/2330 train_time:125428ms step_avg:58.64ms
step:2140/2330 train_time:125489ms step_avg:58.64ms
step:2141/2330 train_time:125547ms step_avg:58.64ms
step:2142/2330 train_time:125608ms step_avg:58.64ms
step:2143/2330 train_time:125667ms step_avg:58.64ms
step:2144/2330 train_time:125727ms step_avg:58.64ms
step:2145/2330 train_time:125785ms step_avg:58.64ms
step:2146/2330 train_time:125846ms step_avg:58.64ms
step:2147/2330 train_time:125904ms step_avg:58.64ms
step:2148/2330 train_time:125964ms step_avg:58.64ms
step:2149/2330 train_time:126022ms step_avg:58.64ms
step:2150/2330 train_time:126082ms step_avg:58.64ms
step:2151/2330 train_time:126139ms step_avg:58.64ms
step:2152/2330 train_time:126201ms step_avg:58.64ms
step:2153/2330 train_time:126258ms step_avg:58.64ms
step:2154/2330 train_time:126320ms step_avg:58.64ms
step:2155/2330 train_time:126376ms step_avg:58.64ms
step:2156/2330 train_time:126439ms step_avg:58.64ms
step:2157/2330 train_time:126495ms step_avg:58.64ms
step:2158/2330 train_time:126557ms step_avg:58.65ms
step:2159/2330 train_time:126614ms step_avg:58.64ms
step:2160/2330 train_time:126676ms step_avg:58.65ms
step:2161/2330 train_time:126733ms step_avg:58.65ms
step:2162/2330 train_time:126795ms step_avg:58.65ms
step:2163/2330 train_time:126852ms step_avg:58.65ms
step:2164/2330 train_time:126914ms step_avg:58.65ms
step:2165/2330 train_time:126971ms step_avg:58.65ms
step:2166/2330 train_time:127032ms step_avg:58.65ms
step:2167/2330 train_time:127089ms step_avg:58.65ms
step:2168/2330 train_time:127150ms step_avg:58.65ms
step:2169/2330 train_time:127208ms step_avg:58.65ms
step:2170/2330 train_time:127270ms step_avg:58.65ms
step:2171/2330 train_time:127327ms step_avg:58.65ms
step:2172/2330 train_time:127389ms step_avg:58.65ms
step:2173/2330 train_time:127447ms step_avg:58.65ms
step:2174/2330 train_time:127509ms step_avg:58.65ms
step:2175/2330 train_time:127566ms step_avg:58.65ms
step:2176/2330 train_time:127628ms step_avg:58.65ms
step:2177/2330 train_time:127686ms step_avg:58.65ms
step:2178/2330 train_time:127746ms step_avg:58.65ms
step:2179/2330 train_time:127803ms step_avg:58.65ms
step:2180/2330 train_time:127864ms step_avg:58.65ms
step:2181/2330 train_time:127922ms step_avg:58.65ms
step:2182/2330 train_time:127983ms step_avg:58.65ms
step:2183/2330 train_time:128041ms step_avg:58.65ms
step:2184/2330 train_time:128100ms step_avg:58.65ms
step:2185/2330 train_time:128158ms step_avg:58.65ms
step:2186/2330 train_time:128218ms step_avg:58.65ms
step:2187/2330 train_time:128276ms step_avg:58.65ms
step:2188/2330 train_time:128338ms step_avg:58.66ms
step:2189/2330 train_time:128394ms step_avg:58.65ms
step:2190/2330 train_time:128457ms step_avg:58.66ms
step:2191/2330 train_time:128514ms step_avg:58.66ms
step:2192/2330 train_time:128576ms step_avg:58.66ms
step:2193/2330 train_time:128632ms step_avg:58.66ms
step:2194/2330 train_time:128694ms step_avg:58.66ms
step:2195/2330 train_time:128750ms step_avg:58.66ms
step:2196/2330 train_time:128812ms step_avg:58.66ms
step:2197/2330 train_time:128868ms step_avg:58.66ms
step:2198/2330 train_time:128931ms step_avg:58.66ms
step:2199/2330 train_time:128988ms step_avg:58.66ms
step:2200/2330 train_time:129049ms step_avg:58.66ms
step:2201/2330 train_time:129106ms step_avg:58.66ms
step:2202/2330 train_time:129168ms step_avg:58.66ms
step:2203/2330 train_time:129225ms step_avg:58.66ms
step:2204/2330 train_time:129287ms step_avg:58.66ms
step:2205/2330 train_time:129346ms step_avg:58.66ms
step:2206/2330 train_time:129406ms step_avg:58.66ms
step:2207/2330 train_time:129464ms step_avg:58.66ms
step:2208/2330 train_time:129526ms step_avg:58.66ms
step:2209/2330 train_time:129584ms step_avg:58.66ms
step:2210/2330 train_time:129645ms step_avg:58.66ms
step:2211/2330 train_time:129703ms step_avg:58.66ms
step:2212/2330 train_time:129763ms step_avg:58.66ms
step:2213/2330 train_time:129820ms step_avg:58.66ms
step:2214/2330 train_time:129881ms step_avg:58.66ms
step:2215/2330 train_time:129938ms step_avg:58.66ms
step:2216/2330 train_time:129999ms step_avg:58.66ms
step:2217/2330 train_time:130056ms step_avg:58.66ms
step:2218/2330 train_time:130117ms step_avg:58.66ms
step:2219/2330 train_time:130173ms step_avg:58.66ms
step:2220/2330 train_time:130235ms step_avg:58.66ms
step:2221/2330 train_time:130292ms step_avg:58.66ms
step:2222/2330 train_time:130354ms step_avg:58.67ms
step:2223/2330 train_time:130410ms step_avg:58.66ms
step:2224/2330 train_time:130472ms step_avg:58.67ms
step:2225/2330 train_time:130530ms step_avg:58.67ms
step:2226/2330 train_time:130591ms step_avg:58.67ms
step:2227/2330 train_time:130648ms step_avg:58.67ms
step:2228/2330 train_time:130710ms step_avg:58.67ms
step:2229/2330 train_time:130767ms step_avg:58.67ms
step:2230/2330 train_time:130829ms step_avg:58.67ms
step:2231/2330 train_time:130887ms step_avg:58.67ms
step:2232/2330 train_time:130949ms step_avg:58.67ms
step:2233/2330 train_time:131007ms step_avg:58.67ms
step:2234/2330 train_time:131067ms step_avg:58.67ms
step:2235/2330 train_time:131125ms step_avg:58.67ms
step:2236/2330 train_time:131185ms step_avg:58.67ms
step:2237/2330 train_time:131243ms step_avg:58.67ms
step:2238/2330 train_time:131305ms step_avg:58.67ms
step:2239/2330 train_time:131363ms step_avg:58.67ms
step:2240/2330 train_time:131423ms step_avg:58.67ms
step:2241/2330 train_time:131480ms step_avg:58.67ms
step:2242/2330 train_time:131541ms step_avg:58.67ms
step:2243/2330 train_time:131599ms step_avg:58.67ms
step:2244/2330 train_time:131659ms step_avg:58.67ms
step:2245/2330 train_time:131716ms step_avg:58.67ms
step:2246/2330 train_time:131779ms step_avg:58.67ms
step:2247/2330 train_time:131835ms step_avg:58.67ms
step:2248/2330 train_time:131898ms step_avg:58.67ms
step:2249/2330 train_time:131954ms step_avg:58.67ms
step:2250/2330 train_time:132016ms step_avg:58.67ms
step:2250/2330 val_loss:3.7299 train_time:132097ms step_avg:58.71ms
step:2251/2330 train_time:132115ms step_avg:58.69ms
step:2252/2330 train_time:132136ms step_avg:58.68ms
step:2253/2330 train_time:132195ms step_avg:58.68ms
step:2254/2330 train_time:132264ms step_avg:58.68ms
step:2255/2330 train_time:132321ms step_avg:58.68ms
step:2256/2330 train_time:132383ms step_avg:58.68ms
step:2257/2330 train_time:132439ms step_avg:58.68ms
step:2258/2330 train_time:132501ms step_avg:58.68ms
step:2259/2330 train_time:132557ms step_avg:58.68ms
step:2260/2330 train_time:132619ms step_avg:58.68ms
step:2261/2330 train_time:132676ms step_avg:58.68ms
step:2262/2330 train_time:132737ms step_avg:58.68ms
step:2263/2330 train_time:132793ms step_avg:58.68ms
step:2264/2330 train_time:132854ms step_avg:58.68ms
step:2265/2330 train_time:132911ms step_avg:58.68ms
step:2266/2330 train_time:132972ms step_avg:58.68ms
step:2267/2330 train_time:133028ms step_avg:58.68ms
step:2268/2330 train_time:133090ms step_avg:58.68ms
step:2269/2330 train_time:133150ms step_avg:58.68ms
step:2270/2330 train_time:133215ms step_avg:58.69ms
step:2271/2330 train_time:133273ms step_avg:58.68ms
step:2272/2330 train_time:133337ms step_avg:58.69ms
step:2273/2330 train_time:133394ms step_avg:58.69ms
step:2274/2330 train_time:133456ms step_avg:58.69ms
step:2275/2330 train_time:133514ms step_avg:58.69ms
step:2276/2330 train_time:133574ms step_avg:58.69ms
step:2277/2330 train_time:133631ms step_avg:58.69ms
step:2278/2330 train_time:133691ms step_avg:58.69ms
step:2279/2330 train_time:133748ms step_avg:58.69ms
step:2280/2330 train_time:133808ms step_avg:58.69ms
step:2281/2330 train_time:133864ms step_avg:58.69ms
step:2282/2330 train_time:133925ms step_avg:58.69ms
step:2283/2330 train_time:133981ms step_avg:58.69ms
step:2284/2330 train_time:134043ms step_avg:58.69ms
step:2285/2330 train_time:134101ms step_avg:58.69ms
step:2286/2330 train_time:134164ms step_avg:58.69ms
step:2287/2330 train_time:134221ms step_avg:58.69ms
step:2288/2330 train_time:134285ms step_avg:58.69ms
step:2289/2330 train_time:134341ms step_avg:58.69ms
step:2290/2330 train_time:134405ms step_avg:58.69ms
step:2291/2330 train_time:134462ms step_avg:58.69ms
step:2292/2330 train_time:134525ms step_avg:58.69ms
step:2293/2330 train_time:134581ms step_avg:58.69ms
step:2294/2330 train_time:134643ms step_avg:58.69ms
step:2295/2330 train_time:134700ms step_avg:58.69ms
step:2296/2330 train_time:134760ms step_avg:58.69ms
step:2297/2330 train_time:134816ms step_avg:58.69ms
step:2298/2330 train_time:134877ms step_avg:58.69ms
step:2299/2330 train_time:134934ms step_avg:58.69ms
step:2300/2330 train_time:134994ms step_avg:58.69ms
step:2301/2330 train_time:135053ms step_avg:58.69ms
step:2302/2330 train_time:135114ms step_avg:58.69ms
step:2303/2330 train_time:135173ms step_avg:58.69ms
step:2304/2330 train_time:135234ms step_avg:58.70ms
step:2305/2330 train_time:135292ms step_avg:58.69ms
step:2306/2330 train_time:135355ms step_avg:58.70ms
step:2307/2330 train_time:135412ms step_avg:58.70ms
step:2308/2330 train_time:135475ms step_avg:58.70ms
step:2309/2330 train_time:135533ms step_avg:58.70ms
step:2310/2330 train_time:135593ms step_avg:58.70ms
step:2311/2330 train_time:135651ms step_avg:58.70ms
step:2312/2330 train_time:135712ms step_avg:58.70ms
step:2313/2330 train_time:135769ms step_avg:58.70ms
step:2314/2330 train_time:135829ms step_avg:58.70ms
step:2315/2330 train_time:135886ms step_avg:58.70ms
step:2316/2330 train_time:135945ms step_avg:58.70ms
step:2317/2330 train_time:136002ms step_avg:58.70ms
step:2318/2330 train_time:136063ms step_avg:58.70ms
step:2319/2330 train_time:136120ms step_avg:58.70ms
step:2320/2330 train_time:136182ms step_avg:58.70ms
step:2321/2330 train_time:136240ms step_avg:58.70ms
step:2322/2330 train_time:136302ms step_avg:58.70ms
step:2323/2330 train_time:136360ms step_avg:58.70ms
step:2324/2330 train_time:136423ms step_avg:58.70ms
step:2325/2330 train_time:136479ms step_avg:58.70ms
step:2326/2330 train_time:136542ms step_avg:58.70ms
step:2327/2330 train_time:136598ms step_avg:58.70ms
step:2328/2330 train_time:136660ms step_avg:58.70ms
step:2329/2330 train_time:136717ms step_avg:58.70ms
step:2330/2330 train_time:136778ms step_avg:58.70ms
step:2330/2330 val_loss:3.7147 train_time:136860ms step_avg:58.74ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
