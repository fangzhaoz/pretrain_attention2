import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--adamw_lr', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_lr_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_lr_tuning/{args2.adamw_lr}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=float(args2.adamw_lr),  betas=(0.85, 0.85), eps=1e-10, weight_decay=0.0, fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_lr_tuning/{args2.adamw_lr}", exist_ok=True)
            torch.save(log, f"logs_adamw_small_lr_tuning/{args2.adamw_lr}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 01:58:41 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   36C    P0             118W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   35C    P0             117W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             118W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             115W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   35C    P0             118W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             114W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:80ms step_avg:80.04ms
step:2/2330 train_time:200ms step_avg:100.04ms
step:3/2330 train_time:220ms step_avg:73.21ms
step:4/2330 train_time:240ms step_avg:59.95ms
step:5/2330 train_time:291ms step_avg:58.19ms
step:6/2330 train_time:348ms step_avg:58.05ms
step:7/2330 train_time:402ms step_avg:57.48ms
step:8/2330 train_time:460ms step_avg:57.54ms
step:9/2330 train_time:515ms step_avg:57.19ms
step:10/2330 train_time:572ms step_avg:57.19ms
step:11/2330 train_time:626ms step_avg:56.95ms
step:12/2330 train_time:684ms step_avg:57.00ms
step:13/2330 train_time:739ms step_avg:56.81ms
step:14/2330 train_time:796ms step_avg:56.86ms
step:15/2330 train_time:851ms step_avg:56.71ms
step:16/2330 train_time:908ms step_avg:56.78ms
step:17/2330 train_time:963ms step_avg:56.63ms
step:18/2330 train_time:1021ms step_avg:56.72ms
step:19/2330 train_time:1076ms step_avg:56.61ms
step:20/2330 train_time:1136ms step_avg:56.81ms
step:21/2330 train_time:1193ms step_avg:56.81ms
step:22/2330 train_time:1253ms step_avg:56.96ms
step:23/2330 train_time:1308ms step_avg:56.87ms
step:24/2330 train_time:1368ms step_avg:56.99ms
step:25/2330 train_time:1423ms step_avg:56.91ms
step:26/2330 train_time:1481ms step_avg:56.95ms
step:27/2330 train_time:1535ms step_avg:56.85ms
step:28/2330 train_time:1593ms step_avg:56.88ms
step:29/2330 train_time:1647ms step_avg:56.79ms
step:30/2330 train_time:1705ms step_avg:56.82ms
step:31/2330 train_time:1760ms step_avg:56.76ms
step:32/2330 train_time:1817ms step_avg:56.80ms
step:33/2330 train_time:1872ms step_avg:56.72ms
step:34/2330 train_time:1929ms step_avg:56.74ms
step:35/2330 train_time:1983ms step_avg:56.66ms
step:36/2330 train_time:2041ms step_avg:56.71ms
step:37/2330 train_time:2096ms step_avg:56.66ms
step:38/2330 train_time:2157ms step_avg:56.76ms
step:39/2330 train_time:2212ms step_avg:56.72ms
step:40/2330 train_time:2271ms step_avg:56.78ms
step:41/2330 train_time:2326ms step_avg:56.74ms
step:42/2330 train_time:2386ms step_avg:56.81ms
step:43/2330 train_time:2441ms step_avg:56.77ms
step:44/2330 train_time:2499ms step_avg:56.80ms
step:45/2330 train_time:2554ms step_avg:56.75ms
step:46/2330 train_time:2612ms step_avg:56.78ms
step:47/2330 train_time:2666ms step_avg:56.72ms
step:48/2330 train_time:2724ms step_avg:56.75ms
step:49/2330 train_time:2779ms step_avg:56.71ms
step:50/2330 train_time:2837ms step_avg:56.75ms
step:51/2330 train_time:2892ms step_avg:56.70ms
step:52/2330 train_time:2949ms step_avg:56.72ms
step:53/2330 train_time:3003ms step_avg:56.67ms
step:54/2330 train_time:3063ms step_avg:56.72ms
step:55/2330 train_time:3119ms step_avg:56.70ms
step:56/2330 train_time:3178ms step_avg:56.75ms
step:57/2330 train_time:3233ms step_avg:56.72ms
step:58/2330 train_time:3292ms step_avg:56.75ms
step:59/2330 train_time:3346ms step_avg:56.71ms
step:60/2330 train_time:3407ms step_avg:56.79ms
step:61/2330 train_time:3462ms step_avg:56.75ms
step:62/2330 train_time:3520ms step_avg:56.78ms
step:63/2330 train_time:3575ms step_avg:56.75ms
step:64/2330 train_time:3633ms step_avg:56.77ms
step:65/2330 train_time:3688ms step_avg:56.74ms
step:66/2330 train_time:3747ms step_avg:56.77ms
step:67/2330 train_time:3802ms step_avg:56.75ms
step:68/2330 train_time:3861ms step_avg:56.77ms
step:69/2330 train_time:3916ms step_avg:56.75ms
step:70/2330 train_time:3974ms step_avg:56.76ms
step:71/2330 train_time:4028ms step_avg:56.73ms
step:72/2330 train_time:4087ms step_avg:56.77ms
step:73/2330 train_time:4142ms step_avg:56.74ms
step:74/2330 train_time:4203ms step_avg:56.79ms
step:75/2330 train_time:4258ms step_avg:56.78ms
step:76/2330 train_time:4317ms step_avg:56.80ms
step:77/2330 train_time:4372ms step_avg:56.78ms
step:78/2330 train_time:4431ms step_avg:56.81ms
step:79/2330 train_time:4487ms step_avg:56.79ms
step:80/2330 train_time:4547ms step_avg:56.84ms
step:81/2330 train_time:4602ms step_avg:56.82ms
step:82/2330 train_time:4661ms step_avg:56.84ms
step:83/2330 train_time:4716ms step_avg:56.82ms
step:84/2330 train_time:4774ms step_avg:56.83ms
step:85/2330 train_time:4829ms step_avg:56.81ms
step:86/2330 train_time:4887ms step_avg:56.83ms
step:87/2330 train_time:4943ms step_avg:56.81ms
step:88/2330 train_time:5001ms step_avg:56.83ms
step:89/2330 train_time:5057ms step_avg:56.82ms
step:90/2330 train_time:5115ms step_avg:56.83ms
step:91/2330 train_time:5170ms step_avg:56.81ms
step:92/2330 train_time:5229ms step_avg:56.84ms
step:93/2330 train_time:5285ms step_avg:56.82ms
step:94/2330 train_time:5343ms step_avg:56.85ms
step:95/2330 train_time:5399ms step_avg:56.83ms
step:96/2330 train_time:5458ms step_avg:56.86ms
step:97/2330 train_time:5514ms step_avg:56.85ms
step:98/2330 train_time:5572ms step_avg:56.86ms
step:99/2330 train_time:5627ms step_avg:56.84ms
step:100/2330 train_time:5687ms step_avg:56.87ms
step:101/2330 train_time:5742ms step_avg:56.85ms
step:102/2330 train_time:5800ms step_avg:56.86ms
step:103/2330 train_time:5856ms step_avg:56.85ms
step:104/2330 train_time:5914ms step_avg:56.86ms
step:105/2330 train_time:5969ms step_avg:56.85ms
step:106/2330 train_time:6028ms step_avg:56.87ms
step:107/2330 train_time:6083ms step_avg:56.85ms
step:108/2330 train_time:6142ms step_avg:56.87ms
step:109/2330 train_time:6198ms step_avg:56.87ms
step:110/2330 train_time:6257ms step_avg:56.88ms
step:111/2330 train_time:6312ms step_avg:56.87ms
step:112/2330 train_time:6370ms step_avg:56.88ms
step:113/2330 train_time:6425ms step_avg:56.86ms
step:114/2330 train_time:6486ms step_avg:56.90ms
step:115/2330 train_time:6541ms step_avg:56.88ms
step:116/2330 train_time:6600ms step_avg:56.90ms
step:117/2330 train_time:6656ms step_avg:56.88ms
step:118/2330 train_time:6713ms step_avg:56.89ms
step:119/2330 train_time:6769ms step_avg:56.88ms
step:120/2330 train_time:6828ms step_avg:56.90ms
step:121/2330 train_time:6883ms step_avg:56.88ms
step:122/2330 train_time:6944ms step_avg:56.91ms
step:123/2330 train_time:6999ms step_avg:56.91ms
step:124/2330 train_time:7058ms step_avg:56.92ms
step:125/2330 train_time:7114ms step_avg:56.91ms
step:126/2330 train_time:7172ms step_avg:56.92ms
step:127/2330 train_time:7228ms step_avg:56.91ms
step:128/2330 train_time:7287ms step_avg:56.93ms
step:129/2330 train_time:7343ms step_avg:56.92ms
step:130/2330 train_time:7402ms step_avg:56.94ms
step:131/2330 train_time:7457ms step_avg:56.93ms
step:132/2330 train_time:7517ms step_avg:56.94ms
step:133/2330 train_time:7572ms step_avg:56.93ms
step:134/2330 train_time:7630ms step_avg:56.94ms
step:135/2330 train_time:7685ms step_avg:56.93ms
step:136/2330 train_time:7744ms step_avg:56.94ms
step:137/2330 train_time:7800ms step_avg:56.93ms
step:138/2330 train_time:7858ms step_avg:56.94ms
step:139/2330 train_time:7914ms step_avg:56.93ms
step:140/2330 train_time:7972ms step_avg:56.94ms
step:141/2330 train_time:8027ms step_avg:56.93ms
step:142/2330 train_time:8086ms step_avg:56.94ms
step:143/2330 train_time:8141ms step_avg:56.93ms
step:144/2330 train_time:8200ms step_avg:56.95ms
step:145/2330 train_time:8256ms step_avg:56.94ms
step:146/2330 train_time:8315ms step_avg:56.95ms
step:147/2330 train_time:8370ms step_avg:56.94ms
step:148/2330 train_time:8430ms step_avg:56.96ms
step:149/2330 train_time:8485ms step_avg:56.95ms
step:150/2330 train_time:8544ms step_avg:56.96ms
step:151/2330 train_time:8599ms step_avg:56.95ms
step:152/2330 train_time:8658ms step_avg:56.96ms
step:153/2330 train_time:8714ms step_avg:56.95ms
step:154/2330 train_time:8772ms step_avg:56.96ms
step:155/2330 train_time:8828ms step_avg:56.95ms
step:156/2330 train_time:8887ms step_avg:56.96ms
step:157/2330 train_time:8942ms step_avg:56.96ms
step:158/2330 train_time:9000ms step_avg:56.96ms
step:159/2330 train_time:9056ms step_avg:56.96ms
step:160/2330 train_time:9115ms step_avg:56.97ms
step:161/2330 train_time:9171ms step_avg:56.96ms
step:162/2330 train_time:9229ms step_avg:56.97ms
step:163/2330 train_time:9284ms step_avg:56.96ms
step:164/2330 train_time:9343ms step_avg:56.97ms
step:165/2330 train_time:9400ms step_avg:56.97ms
step:166/2330 train_time:9459ms step_avg:56.98ms
step:167/2330 train_time:9514ms step_avg:56.97ms
step:168/2330 train_time:9574ms step_avg:56.99ms
step:169/2330 train_time:9629ms step_avg:56.97ms
step:170/2330 train_time:9688ms step_avg:56.99ms
step:171/2330 train_time:9744ms step_avg:56.98ms
step:172/2330 train_time:9802ms step_avg:56.99ms
step:173/2330 train_time:9857ms step_avg:56.98ms
step:174/2330 train_time:9917ms step_avg:56.99ms
step:175/2330 train_time:9972ms step_avg:56.99ms
step:176/2330 train_time:10030ms step_avg:56.99ms
step:177/2330 train_time:10086ms step_avg:56.98ms
step:178/2330 train_time:10146ms step_avg:57.00ms
step:179/2330 train_time:10203ms step_avg:57.00ms
step:180/2330 train_time:10262ms step_avg:57.01ms
step:181/2330 train_time:10318ms step_avg:57.00ms
step:182/2330 train_time:10376ms step_avg:57.01ms
step:183/2330 train_time:10432ms step_avg:57.00ms
step:184/2330 train_time:10490ms step_avg:57.01ms
step:185/2330 train_time:10546ms step_avg:57.00ms
step:186/2330 train_time:10606ms step_avg:57.02ms
step:187/2330 train_time:10661ms step_avg:57.01ms
step:188/2330 train_time:10720ms step_avg:57.02ms
step:189/2330 train_time:10776ms step_avg:57.01ms
step:190/2330 train_time:10834ms step_avg:57.02ms
step:191/2330 train_time:10890ms step_avg:57.01ms
step:192/2330 train_time:10948ms step_avg:57.02ms
step:193/2330 train_time:11004ms step_avg:57.01ms
step:194/2330 train_time:11063ms step_avg:57.02ms
step:195/2330 train_time:11119ms step_avg:57.02ms
step:196/2330 train_time:11177ms step_avg:57.03ms
step:197/2330 train_time:11233ms step_avg:57.02ms
step:198/2330 train_time:11291ms step_avg:57.03ms
step:199/2330 train_time:11347ms step_avg:57.02ms
step:200/2330 train_time:11406ms step_avg:57.03ms
step:201/2330 train_time:11462ms step_avg:57.03ms
step:202/2330 train_time:11521ms step_avg:57.03ms
step:203/2330 train_time:11576ms step_avg:57.02ms
step:204/2330 train_time:11635ms step_avg:57.03ms
step:205/2330 train_time:11691ms step_avg:57.03ms
step:206/2330 train_time:11749ms step_avg:57.04ms
step:207/2330 train_time:11805ms step_avg:57.03ms
step:208/2330 train_time:11864ms step_avg:57.04ms
step:209/2330 train_time:11920ms step_avg:57.03ms
step:210/2330 train_time:11978ms step_avg:57.04ms
step:211/2330 train_time:12034ms step_avg:57.03ms
step:212/2330 train_time:12093ms step_avg:57.04ms
step:213/2330 train_time:12149ms step_avg:57.04ms
step:214/2330 train_time:12208ms step_avg:57.05ms
step:215/2330 train_time:12263ms step_avg:57.04ms
step:216/2330 train_time:12323ms step_avg:57.05ms
step:217/2330 train_time:12380ms step_avg:57.05ms
step:218/2330 train_time:12438ms step_avg:57.06ms
step:219/2330 train_time:12494ms step_avg:57.05ms
step:220/2330 train_time:12552ms step_avg:57.06ms
step:221/2330 train_time:12609ms step_avg:57.05ms
step:222/2330 train_time:12667ms step_avg:57.06ms
step:223/2330 train_time:12723ms step_avg:57.05ms
step:224/2330 train_time:12781ms step_avg:57.06ms
step:225/2330 train_time:12837ms step_avg:57.05ms
step:226/2330 train_time:12896ms step_avg:57.06ms
step:227/2330 train_time:12951ms step_avg:57.05ms
step:228/2330 train_time:13010ms step_avg:57.06ms
step:229/2330 train_time:13066ms step_avg:57.05ms
step:230/2330 train_time:13124ms step_avg:57.06ms
step:231/2330 train_time:13180ms step_avg:57.06ms
step:232/2330 train_time:13238ms step_avg:57.06ms
step:233/2330 train_time:13294ms step_avg:57.05ms
step:234/2330 train_time:13352ms step_avg:57.06ms
step:235/2330 train_time:13407ms step_avg:57.05ms
step:236/2330 train_time:13467ms step_avg:57.06ms
step:237/2330 train_time:13522ms step_avg:57.06ms
step:238/2330 train_time:13582ms step_avg:57.07ms
step:239/2330 train_time:13638ms step_avg:57.06ms
step:240/2330 train_time:13697ms step_avg:57.07ms
step:241/2330 train_time:13753ms step_avg:57.07ms
step:242/2330 train_time:13811ms step_avg:57.07ms
step:243/2330 train_time:13866ms step_avg:57.06ms
step:244/2330 train_time:13926ms step_avg:57.07ms
step:245/2330 train_time:13981ms step_avg:57.07ms
step:246/2330 train_time:14040ms step_avg:57.07ms
step:247/2330 train_time:14095ms step_avg:57.07ms
step:248/2330 train_time:14155ms step_avg:57.07ms
step:249/2330 train_time:14210ms step_avg:57.07ms
step:250/2330 train_time:14270ms step_avg:57.08ms
step:250/2330 val_loss:5.8853 train_time:14348ms step_avg:57.39ms
step:251/2330 train_time:14367ms step_avg:57.24ms
step:252/2330 train_time:14387ms step_avg:57.09ms
step:253/2330 train_time:14440ms step_avg:57.08ms
step:254/2330 train_time:14504ms step_avg:57.10ms
step:255/2330 train_time:14559ms step_avg:57.09ms
step:256/2330 train_time:14621ms step_avg:57.11ms
step:257/2330 train_time:14676ms step_avg:57.10ms
step:258/2330 train_time:14734ms step_avg:57.11ms
step:259/2330 train_time:14790ms step_avg:57.10ms
step:260/2330 train_time:14848ms step_avg:57.11ms
step:261/2330 train_time:14904ms step_avg:57.10ms
step:262/2330 train_time:14962ms step_avg:57.11ms
step:263/2330 train_time:15017ms step_avg:57.10ms
step:264/2330 train_time:15076ms step_avg:57.10ms
step:265/2330 train_time:15131ms step_avg:57.10ms
step:266/2330 train_time:15190ms step_avg:57.10ms
step:267/2330 train_time:15246ms step_avg:57.10ms
step:268/2330 train_time:15305ms step_avg:57.11ms
step:269/2330 train_time:15363ms step_avg:57.11ms
step:270/2330 train_time:15422ms step_avg:57.12ms
step:271/2330 train_time:15478ms step_avg:57.11ms
step:272/2330 train_time:15537ms step_avg:57.12ms
step:273/2330 train_time:15592ms step_avg:57.12ms
step:274/2330 train_time:15652ms step_avg:57.13ms
step:275/2330 train_time:15708ms step_avg:57.12ms
step:276/2330 train_time:15767ms step_avg:57.13ms
step:277/2330 train_time:15823ms step_avg:57.12ms
step:278/2330 train_time:15881ms step_avg:57.13ms
step:279/2330 train_time:15937ms step_avg:57.12ms
step:280/2330 train_time:15995ms step_avg:57.12ms
step:281/2330 train_time:16050ms step_avg:57.12ms
step:282/2330 train_time:16108ms step_avg:57.12ms
step:283/2330 train_time:16164ms step_avg:57.12ms
step:284/2330 train_time:16222ms step_avg:57.12ms
step:285/2330 train_time:16278ms step_avg:57.12ms
step:286/2330 train_time:16336ms step_avg:57.12ms
step:287/2330 train_time:16392ms step_avg:57.11ms
step:288/2330 train_time:16452ms step_avg:57.13ms
step:289/2330 train_time:16509ms step_avg:57.12ms
step:290/2330 train_time:16568ms step_avg:57.13ms
step:291/2330 train_time:16625ms step_avg:57.13ms
step:292/2330 train_time:16683ms step_avg:57.13ms
step:293/2330 train_time:16739ms step_avg:57.13ms
step:294/2330 train_time:16797ms step_avg:57.13ms
step:295/2330 train_time:16852ms step_avg:57.13ms
step:296/2330 train_time:16911ms step_avg:57.13ms
step:297/2330 train_time:16967ms step_avg:57.13ms
step:298/2330 train_time:17025ms step_avg:57.13ms
step:299/2330 train_time:17080ms step_avg:57.12ms
step:300/2330 train_time:17139ms step_avg:57.13ms
step:301/2330 train_time:17194ms step_avg:57.12ms
step:302/2330 train_time:17255ms step_avg:57.13ms
step:303/2330 train_time:17311ms step_avg:57.13ms
step:304/2330 train_time:17369ms step_avg:57.14ms
step:305/2330 train_time:17425ms step_avg:57.13ms
step:306/2330 train_time:17484ms step_avg:57.14ms
step:307/2330 train_time:17540ms step_avg:57.13ms
step:308/2330 train_time:17599ms step_avg:57.14ms
step:309/2330 train_time:17655ms step_avg:57.14ms
step:310/2330 train_time:17714ms step_avg:57.14ms
step:311/2330 train_time:17770ms step_avg:57.14ms
step:312/2330 train_time:17829ms step_avg:57.15ms
step:313/2330 train_time:17885ms step_avg:57.14ms
step:314/2330 train_time:17944ms step_avg:57.15ms
step:315/2330 train_time:18000ms step_avg:57.14ms
step:316/2330 train_time:18058ms step_avg:57.15ms
step:317/2330 train_time:18114ms step_avg:57.14ms
step:318/2330 train_time:18172ms step_avg:57.15ms
step:319/2330 train_time:18227ms step_avg:57.14ms
step:320/2330 train_time:18286ms step_avg:57.15ms
step:321/2330 train_time:18341ms step_avg:57.14ms
step:322/2330 train_time:18400ms step_avg:57.14ms
step:323/2330 train_time:18456ms step_avg:57.14ms
step:324/2330 train_time:18515ms step_avg:57.15ms
step:325/2330 train_time:18571ms step_avg:57.14ms
step:326/2330 train_time:18631ms step_avg:57.15ms
step:327/2330 train_time:18687ms step_avg:57.15ms
step:328/2330 train_time:18746ms step_avg:57.15ms
step:329/2330 train_time:18802ms step_avg:57.15ms
step:330/2330 train_time:18861ms step_avg:57.15ms
step:331/2330 train_time:18916ms step_avg:57.15ms
step:332/2330 train_time:18975ms step_avg:57.15ms
step:333/2330 train_time:19030ms step_avg:57.15ms
step:334/2330 train_time:19090ms step_avg:57.15ms
step:335/2330 train_time:19145ms step_avg:57.15ms
step:336/2330 train_time:19204ms step_avg:57.16ms
step:337/2330 train_time:19260ms step_avg:57.15ms
step:338/2330 train_time:19320ms step_avg:57.16ms
step:339/2330 train_time:19376ms step_avg:57.16ms
step:340/2330 train_time:19434ms step_avg:57.16ms
step:341/2330 train_time:19490ms step_avg:57.15ms
step:342/2330 train_time:19549ms step_avg:57.16ms
step:343/2330 train_time:19605ms step_avg:57.16ms
step:344/2330 train_time:19664ms step_avg:57.16ms
step:345/2330 train_time:19720ms step_avg:57.16ms
step:346/2330 train_time:19779ms step_avg:57.16ms
step:347/2330 train_time:19835ms step_avg:57.16ms
step:348/2330 train_time:19894ms step_avg:57.17ms
step:349/2330 train_time:19949ms step_avg:57.16ms
step:350/2330 train_time:20008ms step_avg:57.17ms
step:351/2330 train_time:20065ms step_avg:57.16ms
step:352/2330 train_time:20123ms step_avg:57.17ms
step:353/2330 train_time:20178ms step_avg:57.16ms
step:354/2330 train_time:20236ms step_avg:57.16ms
step:355/2330 train_time:20292ms step_avg:57.16ms
step:356/2330 train_time:20351ms step_avg:57.17ms
step:357/2330 train_time:20408ms step_avg:57.16ms
step:358/2330 train_time:20466ms step_avg:57.17ms
step:359/2330 train_time:20521ms step_avg:57.16ms
step:360/2330 train_time:20582ms step_avg:57.17ms
step:361/2330 train_time:20637ms step_avg:57.17ms
step:362/2330 train_time:20696ms step_avg:57.17ms
step:363/2330 train_time:20752ms step_avg:57.17ms
step:364/2330 train_time:20811ms step_avg:57.17ms
step:365/2330 train_time:20867ms step_avg:57.17ms
step:366/2330 train_time:20926ms step_avg:57.18ms
step:367/2330 train_time:20982ms step_avg:57.17ms
step:368/2330 train_time:21040ms step_avg:57.17ms
step:369/2330 train_time:21096ms step_avg:57.17ms
step:370/2330 train_time:21155ms step_avg:57.18ms
step:371/2330 train_time:21211ms step_avg:57.17ms
step:372/2330 train_time:21269ms step_avg:57.18ms
step:373/2330 train_time:21326ms step_avg:57.17ms
step:374/2330 train_time:21384ms step_avg:57.18ms
step:375/2330 train_time:21440ms step_avg:57.17ms
step:376/2330 train_time:21498ms step_avg:57.18ms
step:377/2330 train_time:21554ms step_avg:57.17ms
step:378/2330 train_time:21613ms step_avg:57.18ms
step:379/2330 train_time:21669ms step_avg:57.17ms
step:380/2330 train_time:21728ms step_avg:57.18ms
step:381/2330 train_time:21783ms step_avg:57.17ms
step:382/2330 train_time:21842ms step_avg:57.18ms
step:383/2330 train_time:21898ms step_avg:57.17ms
step:384/2330 train_time:21957ms step_avg:57.18ms
step:385/2330 train_time:22012ms step_avg:57.18ms
step:386/2330 train_time:22072ms step_avg:57.18ms
step:387/2330 train_time:22128ms step_avg:57.18ms
step:388/2330 train_time:22186ms step_avg:57.18ms
step:389/2330 train_time:22242ms step_avg:57.18ms
step:390/2330 train_time:22301ms step_avg:57.18ms
step:391/2330 train_time:22357ms step_avg:57.18ms
step:392/2330 train_time:22416ms step_avg:57.18ms
step:393/2330 train_time:22471ms step_avg:57.18ms
step:394/2330 train_time:22530ms step_avg:57.18ms
step:395/2330 train_time:22587ms step_avg:57.18ms
step:396/2330 train_time:22645ms step_avg:57.19ms
step:397/2330 train_time:22702ms step_avg:57.18ms
step:398/2330 train_time:22760ms step_avg:57.19ms
step:399/2330 train_time:22816ms step_avg:57.18ms
step:400/2330 train_time:22874ms step_avg:57.19ms
step:401/2330 train_time:22930ms step_avg:57.18ms
step:402/2330 train_time:22988ms step_avg:57.19ms
step:403/2330 train_time:23044ms step_avg:57.18ms
step:404/2330 train_time:23104ms step_avg:57.19ms
step:405/2330 train_time:23159ms step_avg:57.18ms
step:406/2330 train_time:23217ms step_avg:57.19ms
step:407/2330 train_time:23273ms step_avg:57.18ms
step:408/2330 train_time:23332ms step_avg:57.19ms
step:409/2330 train_time:23388ms step_avg:57.18ms
step:410/2330 train_time:23447ms step_avg:57.19ms
step:411/2330 train_time:23503ms step_avg:57.19ms
step:412/2330 train_time:23562ms step_avg:57.19ms
step:413/2330 train_time:23617ms step_avg:57.18ms
step:414/2330 train_time:23676ms step_avg:57.19ms
step:415/2330 train_time:23732ms step_avg:57.19ms
step:416/2330 train_time:23791ms step_avg:57.19ms
step:417/2330 train_time:23847ms step_avg:57.19ms
step:418/2330 train_time:23905ms step_avg:57.19ms
step:419/2330 train_time:23961ms step_avg:57.19ms
step:420/2330 train_time:24020ms step_avg:57.19ms
step:421/2330 train_time:24075ms step_avg:57.18ms
step:422/2330 train_time:24134ms step_avg:57.19ms
step:423/2330 train_time:24189ms step_avg:57.19ms
step:424/2330 train_time:24249ms step_avg:57.19ms
step:425/2330 train_time:24305ms step_avg:57.19ms
step:426/2330 train_time:24365ms step_avg:57.19ms
step:427/2330 train_time:24421ms step_avg:57.19ms
step:428/2330 train_time:24479ms step_avg:57.19ms
step:429/2330 train_time:24534ms step_avg:57.19ms
step:430/2330 train_time:24592ms step_avg:57.19ms
step:431/2330 train_time:24648ms step_avg:57.19ms
step:432/2330 train_time:24708ms step_avg:57.19ms
step:433/2330 train_time:24763ms step_avg:57.19ms
step:434/2330 train_time:24824ms step_avg:57.20ms
step:435/2330 train_time:24879ms step_avg:57.19ms
step:436/2330 train_time:24938ms step_avg:57.20ms
step:437/2330 train_time:24993ms step_avg:57.19ms
step:438/2330 train_time:25053ms step_avg:57.20ms
step:439/2330 train_time:25109ms step_avg:57.20ms
step:440/2330 train_time:25168ms step_avg:57.20ms
step:441/2330 train_time:25224ms step_avg:57.20ms
step:442/2330 train_time:25283ms step_avg:57.20ms
step:443/2330 train_time:25339ms step_avg:57.20ms
step:444/2330 train_time:25398ms step_avg:57.20ms
step:445/2330 train_time:25453ms step_avg:57.20ms
step:446/2330 train_time:25512ms step_avg:57.20ms
step:447/2330 train_time:25568ms step_avg:57.20ms
step:448/2330 train_time:25626ms step_avg:57.20ms
step:449/2330 train_time:25682ms step_avg:57.20ms
step:450/2330 train_time:25741ms step_avg:57.20ms
step:451/2330 train_time:25797ms step_avg:57.20ms
step:452/2330 train_time:25856ms step_avg:57.20ms
step:453/2330 train_time:25911ms step_avg:57.20ms
step:454/2330 train_time:25971ms step_avg:57.20ms
step:455/2330 train_time:26027ms step_avg:57.20ms
step:456/2330 train_time:26086ms step_avg:57.21ms
step:457/2330 train_time:26141ms step_avg:57.20ms
step:458/2330 train_time:26200ms step_avg:57.21ms
step:459/2330 train_time:26256ms step_avg:57.20ms
step:460/2330 train_time:26315ms step_avg:57.21ms
step:461/2330 train_time:26371ms step_avg:57.20ms
step:462/2330 train_time:26430ms step_avg:57.21ms
step:463/2330 train_time:26486ms step_avg:57.21ms
step:464/2330 train_time:26545ms step_avg:57.21ms
step:465/2330 train_time:26600ms step_avg:57.21ms
step:466/2330 train_time:26659ms step_avg:57.21ms
step:467/2330 train_time:26714ms step_avg:57.20ms
step:468/2330 train_time:26774ms step_avg:57.21ms
step:469/2330 train_time:26830ms step_avg:57.21ms
step:470/2330 train_time:26888ms step_avg:57.21ms
step:471/2330 train_time:26945ms step_avg:57.21ms
step:472/2330 train_time:27004ms step_avg:57.21ms
step:473/2330 train_time:27059ms step_avg:57.21ms
step:474/2330 train_time:27118ms step_avg:57.21ms
step:475/2330 train_time:27173ms step_avg:57.21ms
step:476/2330 train_time:27233ms step_avg:57.21ms
step:477/2330 train_time:27288ms step_avg:57.21ms
step:478/2330 train_time:27348ms step_avg:57.21ms
step:479/2330 train_time:27404ms step_avg:57.21ms
step:480/2330 train_time:27462ms step_avg:57.21ms
step:481/2330 train_time:27518ms step_avg:57.21ms
step:482/2330 train_time:27577ms step_avg:57.21ms
step:483/2330 train_time:27633ms step_avg:57.21ms
step:484/2330 train_time:27692ms step_avg:57.21ms
step:485/2330 train_time:27748ms step_avg:57.21ms
step:486/2330 train_time:27807ms step_avg:57.22ms
step:487/2330 train_time:27863ms step_avg:57.21ms
step:488/2330 train_time:27922ms step_avg:57.22ms
step:489/2330 train_time:27978ms step_avg:57.21ms
step:490/2330 train_time:28037ms step_avg:57.22ms
step:491/2330 train_time:28092ms step_avg:57.21ms
step:492/2330 train_time:28152ms step_avg:57.22ms
step:493/2330 train_time:28207ms step_avg:57.22ms
step:494/2330 train_time:28266ms step_avg:57.22ms
step:495/2330 train_time:28322ms step_avg:57.22ms
step:496/2330 train_time:28381ms step_avg:57.22ms
step:497/2330 train_time:28437ms step_avg:57.22ms
step:498/2330 train_time:28495ms step_avg:57.22ms
step:499/2330 train_time:28551ms step_avg:57.22ms
step:500/2330 train_time:28610ms step_avg:57.22ms
step:500/2330 val_loss:5.1688 train_time:28689ms step_avg:57.38ms
step:501/2330 train_time:28708ms step_avg:57.30ms
step:502/2330 train_time:28728ms step_avg:57.23ms
step:503/2330 train_time:28785ms step_avg:57.23ms
step:504/2330 train_time:28847ms step_avg:57.24ms
step:505/2330 train_time:28903ms step_avg:57.23ms
step:506/2330 train_time:28965ms step_avg:57.24ms
step:507/2330 train_time:29021ms step_avg:57.24ms
step:508/2330 train_time:29081ms step_avg:57.25ms
step:509/2330 train_time:29136ms step_avg:57.24ms
step:510/2330 train_time:29194ms step_avg:57.24ms
step:511/2330 train_time:29250ms step_avg:57.24ms
step:512/2330 train_time:29308ms step_avg:57.24ms
step:513/2330 train_time:29363ms step_avg:57.24ms
step:514/2330 train_time:29421ms step_avg:57.24ms
step:515/2330 train_time:29476ms step_avg:57.24ms
step:516/2330 train_time:29534ms step_avg:57.24ms
step:517/2330 train_time:29589ms step_avg:57.23ms
step:518/2330 train_time:29648ms step_avg:57.23ms
step:519/2330 train_time:29704ms step_avg:57.23ms
step:520/2330 train_time:29763ms step_avg:57.24ms
step:521/2330 train_time:29818ms step_avg:57.23ms
step:522/2330 train_time:29879ms step_avg:57.24ms
step:523/2330 train_time:29936ms step_avg:57.24ms
step:524/2330 train_time:29996ms step_avg:57.25ms
step:525/2330 train_time:30052ms step_avg:57.24ms
step:526/2330 train_time:30111ms step_avg:57.25ms
step:527/2330 train_time:30167ms step_avg:57.24ms
step:528/2330 train_time:30225ms step_avg:57.24ms
step:529/2330 train_time:30281ms step_avg:57.24ms
step:530/2330 train_time:30339ms step_avg:57.24ms
step:531/2330 train_time:30395ms step_avg:57.24ms
step:532/2330 train_time:30453ms step_avg:57.24ms
step:533/2330 train_time:30508ms step_avg:57.24ms
step:534/2330 train_time:30567ms step_avg:57.24ms
step:535/2330 train_time:30622ms step_avg:57.24ms
step:536/2330 train_time:30680ms step_avg:57.24ms
step:537/2330 train_time:30736ms step_avg:57.24ms
step:538/2330 train_time:30796ms step_avg:57.24ms
step:539/2330 train_time:30852ms step_avg:57.24ms
step:540/2330 train_time:30912ms step_avg:57.24ms
step:541/2330 train_time:30968ms step_avg:57.24ms
step:542/2330 train_time:31027ms step_avg:57.25ms
step:543/2330 train_time:31083ms step_avg:57.24ms
step:544/2330 train_time:31141ms step_avg:57.24ms
step:545/2330 train_time:31197ms step_avg:57.24ms
step:546/2330 train_time:31256ms step_avg:57.25ms
step:547/2330 train_time:31312ms step_avg:57.24ms
step:548/2330 train_time:31371ms step_avg:57.25ms
step:549/2330 train_time:31427ms step_avg:57.24ms
step:550/2330 train_time:31485ms step_avg:57.25ms
step:551/2330 train_time:31541ms step_avg:57.24ms
step:552/2330 train_time:31599ms step_avg:57.24ms
step:553/2330 train_time:31654ms step_avg:57.24ms
step:554/2330 train_time:31713ms step_avg:57.24ms
step:555/2330 train_time:31768ms step_avg:57.24ms
step:556/2330 train_time:31827ms step_avg:57.24ms
step:557/2330 train_time:31883ms step_avg:57.24ms
step:558/2330 train_time:31942ms step_avg:57.24ms
step:559/2330 train_time:31999ms step_avg:57.24ms
step:560/2330 train_time:32058ms step_avg:57.25ms
step:561/2330 train_time:32115ms step_avg:57.25ms
step:562/2330 train_time:32173ms step_avg:57.25ms
step:563/2330 train_time:32229ms step_avg:57.24ms
step:564/2330 train_time:32287ms step_avg:57.25ms
step:565/2330 train_time:32342ms step_avg:57.24ms
step:566/2330 train_time:32401ms step_avg:57.25ms
step:567/2330 train_time:32457ms step_avg:57.24ms
step:568/2330 train_time:32515ms step_avg:57.25ms
step:569/2330 train_time:32571ms step_avg:57.24ms
step:570/2330 train_time:32629ms step_avg:57.24ms
step:571/2330 train_time:32685ms step_avg:57.24ms
step:572/2330 train_time:32743ms step_avg:57.24ms
step:573/2330 train_time:32799ms step_avg:57.24ms
step:574/2330 train_time:32859ms step_avg:57.25ms
step:575/2330 train_time:32915ms step_avg:57.24ms
step:576/2330 train_time:32974ms step_avg:57.25ms
step:577/2330 train_time:33030ms step_avg:57.24ms
step:578/2330 train_time:33089ms step_avg:57.25ms
step:579/2330 train_time:33145ms step_avg:57.25ms
step:580/2330 train_time:33204ms step_avg:57.25ms
step:581/2330 train_time:33260ms step_avg:57.25ms
step:582/2330 train_time:33319ms step_avg:57.25ms
step:583/2330 train_time:33375ms step_avg:57.25ms
step:584/2330 train_time:33433ms step_avg:57.25ms
step:585/2330 train_time:33489ms step_avg:57.25ms
step:586/2330 train_time:33547ms step_avg:57.25ms
step:587/2330 train_time:33603ms step_avg:57.25ms
step:588/2330 train_time:33661ms step_avg:57.25ms
step:589/2330 train_time:33716ms step_avg:57.24ms
step:590/2330 train_time:33776ms step_avg:57.25ms
step:591/2330 train_time:33832ms step_avg:57.24ms
step:592/2330 train_time:33891ms step_avg:57.25ms
step:593/2330 train_time:33946ms step_avg:57.25ms
step:594/2330 train_time:34006ms step_avg:57.25ms
step:595/2330 train_time:34062ms step_avg:57.25ms
step:596/2330 train_time:34120ms step_avg:57.25ms
step:597/2330 train_time:34177ms step_avg:57.25ms
step:598/2330 train_time:34236ms step_avg:57.25ms
step:599/2330 train_time:34292ms step_avg:57.25ms
step:600/2330 train_time:34351ms step_avg:57.25ms
step:601/2330 train_time:34407ms step_avg:57.25ms
step:602/2330 train_time:34466ms step_avg:57.25ms
step:603/2330 train_time:34522ms step_avg:57.25ms
step:604/2330 train_time:34580ms step_avg:57.25ms
step:605/2330 train_time:34636ms step_avg:57.25ms
step:606/2330 train_time:34694ms step_avg:57.25ms
step:607/2330 train_time:34750ms step_avg:57.25ms
step:608/2330 train_time:34810ms step_avg:57.25ms
step:609/2330 train_time:34865ms step_avg:57.25ms
step:610/2330 train_time:34924ms step_avg:57.25ms
step:611/2330 train_time:34979ms step_avg:57.25ms
step:612/2330 train_time:35039ms step_avg:57.25ms
step:613/2330 train_time:35095ms step_avg:57.25ms
step:614/2330 train_time:35154ms step_avg:57.25ms
step:615/2330 train_time:35210ms step_avg:57.25ms
step:616/2330 train_time:35269ms step_avg:57.25ms
step:617/2330 train_time:35325ms step_avg:57.25ms
step:618/2330 train_time:35384ms step_avg:57.25ms
step:619/2330 train_time:35439ms step_avg:57.25ms
step:620/2330 train_time:35499ms step_avg:57.26ms
step:621/2330 train_time:35555ms step_avg:57.25ms
step:622/2330 train_time:35613ms step_avg:57.26ms
step:623/2330 train_time:35669ms step_avg:57.25ms
step:624/2330 train_time:35727ms step_avg:57.26ms
step:625/2330 train_time:35783ms step_avg:57.25ms
step:626/2330 train_time:35842ms step_avg:57.26ms
step:627/2330 train_time:35898ms step_avg:57.25ms
step:628/2330 train_time:35957ms step_avg:57.26ms
step:629/2330 train_time:36013ms step_avg:57.25ms
step:630/2330 train_time:36071ms step_avg:57.26ms
step:631/2330 train_time:36127ms step_avg:57.25ms
step:632/2330 train_time:36187ms step_avg:57.26ms
step:633/2330 train_time:36242ms step_avg:57.25ms
step:634/2330 train_time:36301ms step_avg:57.26ms
step:635/2330 train_time:36357ms step_avg:57.26ms
step:636/2330 train_time:36416ms step_avg:57.26ms
step:637/2330 train_time:36473ms step_avg:57.26ms
step:638/2330 train_time:36532ms step_avg:57.26ms
step:639/2330 train_time:36588ms step_avg:57.26ms
step:640/2330 train_time:36646ms step_avg:57.26ms
step:641/2330 train_time:36702ms step_avg:57.26ms
step:642/2330 train_time:36760ms step_avg:57.26ms
step:643/2330 train_time:36817ms step_avg:57.26ms
step:644/2330 train_time:36875ms step_avg:57.26ms
step:645/2330 train_time:36930ms step_avg:57.26ms
step:646/2330 train_time:36990ms step_avg:57.26ms
step:647/2330 train_time:37046ms step_avg:57.26ms
step:648/2330 train_time:37105ms step_avg:57.26ms
step:649/2330 train_time:37161ms step_avg:57.26ms
step:650/2330 train_time:37219ms step_avg:57.26ms
step:651/2330 train_time:37276ms step_avg:57.26ms
step:652/2330 train_time:37334ms step_avg:57.26ms
step:653/2330 train_time:37390ms step_avg:57.26ms
step:654/2330 train_time:37450ms step_avg:57.26ms
step:655/2330 train_time:37506ms step_avg:57.26ms
step:656/2330 train_time:37564ms step_avg:57.26ms
step:657/2330 train_time:37619ms step_avg:57.26ms
step:658/2330 train_time:37678ms step_avg:57.26ms
step:659/2330 train_time:37734ms step_avg:57.26ms
step:660/2330 train_time:37793ms step_avg:57.26ms
step:661/2330 train_time:37848ms step_avg:57.26ms
step:662/2330 train_time:37907ms step_avg:57.26ms
step:663/2330 train_time:37963ms step_avg:57.26ms
step:664/2330 train_time:38021ms step_avg:57.26ms
step:665/2330 train_time:38077ms step_avg:57.26ms
step:666/2330 train_time:38136ms step_avg:57.26ms
step:667/2330 train_time:38193ms step_avg:57.26ms
step:668/2330 train_time:38252ms step_avg:57.26ms
step:669/2330 train_time:38308ms step_avg:57.26ms
step:670/2330 train_time:38367ms step_avg:57.26ms
step:671/2330 train_time:38423ms step_avg:57.26ms
step:672/2330 train_time:38482ms step_avg:57.26ms
step:673/2330 train_time:38537ms step_avg:57.26ms
step:674/2330 train_time:38597ms step_avg:57.27ms
step:675/2330 train_time:38654ms step_avg:57.26ms
step:676/2330 train_time:38712ms step_avg:57.27ms
step:677/2330 train_time:38768ms step_avg:57.26ms
step:678/2330 train_time:38826ms step_avg:57.27ms
step:679/2330 train_time:38882ms step_avg:57.26ms
step:680/2330 train_time:38940ms step_avg:57.27ms
step:681/2330 train_time:38996ms step_avg:57.26ms
step:682/2330 train_time:39055ms step_avg:57.27ms
step:683/2330 train_time:39111ms step_avg:57.26ms
step:684/2330 train_time:39170ms step_avg:57.27ms
step:685/2330 train_time:39226ms step_avg:57.26ms
step:686/2330 train_time:39285ms step_avg:57.27ms
step:687/2330 train_time:39340ms step_avg:57.26ms
step:688/2330 train_time:39400ms step_avg:57.27ms
step:689/2330 train_time:39456ms step_avg:57.27ms
step:690/2330 train_time:39515ms step_avg:57.27ms
step:691/2330 train_time:39571ms step_avg:57.27ms
step:692/2330 train_time:39630ms step_avg:57.27ms
step:693/2330 train_time:39685ms step_avg:57.27ms
step:694/2330 train_time:39744ms step_avg:57.27ms
step:695/2330 train_time:39800ms step_avg:57.27ms
step:696/2330 train_time:39858ms step_avg:57.27ms
step:697/2330 train_time:39914ms step_avg:57.27ms
step:698/2330 train_time:39973ms step_avg:57.27ms
step:699/2330 train_time:40029ms step_avg:57.27ms
step:700/2330 train_time:40088ms step_avg:57.27ms
step:701/2330 train_time:40143ms step_avg:57.27ms
step:702/2330 train_time:40202ms step_avg:57.27ms
step:703/2330 train_time:40257ms step_avg:57.27ms
step:704/2330 train_time:40317ms step_avg:57.27ms
step:705/2330 train_time:40373ms step_avg:57.27ms
step:706/2330 train_time:40431ms step_avg:57.27ms
step:707/2330 train_time:40487ms step_avg:57.27ms
step:708/2330 train_time:40546ms step_avg:57.27ms
step:709/2330 train_time:40602ms step_avg:57.27ms
step:710/2330 train_time:40660ms step_avg:57.27ms
step:711/2330 train_time:40717ms step_avg:57.27ms
step:712/2330 train_time:40776ms step_avg:57.27ms
step:713/2330 train_time:40831ms step_avg:57.27ms
step:714/2330 train_time:40890ms step_avg:57.27ms
step:715/2330 train_time:40946ms step_avg:57.27ms
step:716/2330 train_time:41004ms step_avg:57.27ms
step:717/2330 train_time:41060ms step_avg:57.27ms
step:718/2330 train_time:41120ms step_avg:57.27ms
step:719/2330 train_time:41176ms step_avg:57.27ms
step:720/2330 train_time:41235ms step_avg:57.27ms
step:721/2330 train_time:41291ms step_avg:57.27ms
step:722/2330 train_time:41350ms step_avg:57.27ms
step:723/2330 train_time:41406ms step_avg:57.27ms
step:724/2330 train_time:41464ms step_avg:57.27ms
step:725/2330 train_time:41520ms step_avg:57.27ms
step:726/2330 train_time:41578ms step_avg:57.27ms
step:727/2330 train_time:41634ms step_avg:57.27ms
step:728/2330 train_time:41694ms step_avg:57.27ms
step:729/2330 train_time:41750ms step_avg:57.27ms
step:730/2330 train_time:41808ms step_avg:57.27ms
step:731/2330 train_time:41864ms step_avg:57.27ms
step:732/2330 train_time:41922ms step_avg:57.27ms
step:733/2330 train_time:41978ms step_avg:57.27ms
step:734/2330 train_time:42038ms step_avg:57.27ms
step:735/2330 train_time:42094ms step_avg:57.27ms
step:736/2330 train_time:42152ms step_avg:57.27ms
step:737/2330 train_time:42208ms step_avg:57.27ms
step:738/2330 train_time:42267ms step_avg:57.27ms
step:739/2330 train_time:42322ms step_avg:57.27ms
step:740/2330 train_time:42381ms step_avg:57.27ms
step:741/2330 train_time:42436ms step_avg:57.27ms
step:742/2330 train_time:42495ms step_avg:57.27ms
step:743/2330 train_time:42552ms step_avg:57.27ms
step:744/2330 train_time:42610ms step_avg:57.27ms
step:745/2330 train_time:42666ms step_avg:57.27ms
step:746/2330 train_time:42725ms step_avg:57.27ms
step:747/2330 train_time:42780ms step_avg:57.27ms
step:748/2330 train_time:42840ms step_avg:57.27ms
step:749/2330 train_time:42896ms step_avg:57.27ms
step:750/2330 train_time:42954ms step_avg:57.27ms
step:750/2330 val_loss:4.7337 train_time:43033ms step_avg:57.38ms
step:751/2330 train_time:43053ms step_avg:57.33ms
step:752/2330 train_time:43072ms step_avg:57.28ms
step:753/2330 train_time:43127ms step_avg:57.27ms
step:754/2330 train_time:43190ms step_avg:57.28ms
step:755/2330 train_time:43246ms step_avg:57.28ms
step:756/2330 train_time:43308ms step_avg:57.29ms
step:757/2330 train_time:43363ms step_avg:57.28ms
step:758/2330 train_time:43424ms step_avg:57.29ms
step:759/2330 train_time:43479ms step_avg:57.28ms
step:760/2330 train_time:43539ms step_avg:57.29ms
step:761/2330 train_time:43595ms step_avg:57.29ms
step:762/2330 train_time:43653ms step_avg:57.29ms
step:763/2330 train_time:43708ms step_avg:57.28ms
step:764/2330 train_time:43766ms step_avg:57.29ms
step:765/2330 train_time:43822ms step_avg:57.28ms
step:766/2330 train_time:43880ms step_avg:57.28ms
step:767/2330 train_time:43936ms step_avg:57.28ms
step:768/2330 train_time:43996ms step_avg:57.29ms
step:769/2330 train_time:44053ms step_avg:57.29ms
step:770/2330 train_time:44113ms step_avg:57.29ms
step:771/2330 train_time:44170ms step_avg:57.29ms
step:772/2330 train_time:44231ms step_avg:57.29ms
step:773/2330 train_time:44288ms step_avg:57.29ms
step:774/2330 train_time:44348ms step_avg:57.30ms
step:775/2330 train_time:44405ms step_avg:57.30ms
step:776/2330 train_time:44465ms step_avg:57.30ms
step:777/2330 train_time:44521ms step_avg:57.30ms
step:778/2330 train_time:44581ms step_avg:57.30ms
step:779/2330 train_time:44637ms step_avg:57.30ms
step:780/2330 train_time:44697ms step_avg:57.30ms
step:781/2330 train_time:44753ms step_avg:57.30ms
step:782/2330 train_time:44813ms step_avg:57.31ms
step:783/2330 train_time:44869ms step_avg:57.30ms
step:784/2330 train_time:44928ms step_avg:57.31ms
step:785/2330 train_time:44984ms step_avg:57.30ms
step:786/2330 train_time:45044ms step_avg:57.31ms
step:787/2330 train_time:45102ms step_avg:57.31ms
step:788/2330 train_time:45161ms step_avg:57.31ms
step:789/2330 train_time:45219ms step_avg:57.31ms
step:790/2330 train_time:45279ms step_avg:57.32ms
step:791/2330 train_time:45337ms step_avg:57.32ms
step:792/2330 train_time:45396ms step_avg:57.32ms
step:793/2330 train_time:45453ms step_avg:57.32ms
step:794/2330 train_time:45512ms step_avg:57.32ms
step:795/2330 train_time:45567ms step_avg:57.32ms
step:796/2330 train_time:45627ms step_avg:57.32ms
step:797/2330 train_time:45683ms step_avg:57.32ms
step:798/2330 train_time:45744ms step_avg:57.32ms
step:799/2330 train_time:45800ms step_avg:57.32ms
step:800/2330 train_time:45861ms step_avg:57.33ms
step:801/2330 train_time:45917ms step_avg:57.33ms
step:802/2330 train_time:45977ms step_avg:57.33ms
step:803/2330 train_time:46034ms step_avg:57.33ms
step:804/2330 train_time:46094ms step_avg:57.33ms
step:805/2330 train_time:46150ms step_avg:57.33ms
step:806/2330 train_time:46210ms step_avg:57.33ms
step:807/2330 train_time:46266ms step_avg:57.33ms
step:808/2330 train_time:46328ms step_avg:57.34ms
step:809/2330 train_time:46384ms step_avg:57.33ms
step:810/2330 train_time:46445ms step_avg:57.34ms
step:811/2330 train_time:46501ms step_avg:57.34ms
step:812/2330 train_time:46562ms step_avg:57.34ms
step:813/2330 train_time:46618ms step_avg:57.34ms
step:814/2330 train_time:46678ms step_avg:57.34ms
step:815/2330 train_time:46735ms step_avg:57.34ms
step:816/2330 train_time:46794ms step_avg:57.35ms
step:817/2330 train_time:46850ms step_avg:57.34ms
step:818/2330 train_time:46910ms step_avg:57.35ms
step:819/2330 train_time:46966ms step_avg:57.35ms
step:820/2330 train_time:47026ms step_avg:57.35ms
step:821/2330 train_time:47082ms step_avg:57.35ms
step:822/2330 train_time:47143ms step_avg:57.35ms
step:823/2330 train_time:47200ms step_avg:57.35ms
step:824/2330 train_time:47260ms step_avg:57.35ms
step:825/2330 train_time:47317ms step_avg:57.35ms
step:826/2330 train_time:47376ms step_avg:57.36ms
step:827/2330 train_time:47433ms step_avg:57.36ms
step:828/2330 train_time:47493ms step_avg:57.36ms
step:829/2330 train_time:47549ms step_avg:57.36ms
step:830/2330 train_time:47610ms step_avg:57.36ms
step:831/2330 train_time:47666ms step_avg:57.36ms
step:832/2330 train_time:47725ms step_avg:57.36ms
step:833/2330 train_time:47781ms step_avg:57.36ms
step:834/2330 train_time:47842ms step_avg:57.36ms
step:835/2330 train_time:47898ms step_avg:57.36ms
step:836/2330 train_time:47958ms step_avg:57.37ms
step:837/2330 train_time:48015ms step_avg:57.37ms
step:838/2330 train_time:48075ms step_avg:57.37ms
step:839/2330 train_time:48132ms step_avg:57.37ms
step:840/2330 train_time:48191ms step_avg:57.37ms
step:841/2330 train_time:48246ms step_avg:57.37ms
step:842/2330 train_time:48307ms step_avg:57.37ms
step:843/2330 train_time:48363ms step_avg:57.37ms
step:844/2330 train_time:48424ms step_avg:57.37ms
step:845/2330 train_time:48480ms step_avg:57.37ms
step:846/2330 train_time:48541ms step_avg:57.38ms
step:847/2330 train_time:48598ms step_avg:57.38ms
step:848/2330 train_time:48657ms step_avg:57.38ms
step:849/2330 train_time:48714ms step_avg:57.38ms
step:850/2330 train_time:48773ms step_avg:57.38ms
step:851/2330 train_time:48830ms step_avg:57.38ms
step:852/2330 train_time:48889ms step_avg:57.38ms
step:853/2330 train_time:48946ms step_avg:57.38ms
step:854/2330 train_time:49005ms step_avg:57.38ms
step:855/2330 train_time:49061ms step_avg:57.38ms
step:856/2330 train_time:49122ms step_avg:57.39ms
step:857/2330 train_time:49179ms step_avg:57.38ms
step:858/2330 train_time:49239ms step_avg:57.39ms
step:859/2330 train_time:49296ms step_avg:57.39ms
step:860/2330 train_time:49356ms step_avg:57.39ms
step:861/2330 train_time:49413ms step_avg:57.39ms
step:862/2330 train_time:49472ms step_avg:57.39ms
step:863/2330 train_time:49528ms step_avg:57.39ms
step:864/2330 train_time:49587ms step_avg:57.39ms
step:865/2330 train_time:49644ms step_avg:57.39ms
step:866/2330 train_time:49704ms step_avg:57.39ms
step:867/2330 train_time:49760ms step_avg:57.39ms
step:868/2330 train_time:49820ms step_avg:57.40ms
step:869/2330 train_time:49876ms step_avg:57.39ms
step:870/2330 train_time:49937ms step_avg:57.40ms
step:871/2330 train_time:49994ms step_avg:57.40ms
step:872/2330 train_time:50053ms step_avg:57.40ms
step:873/2330 train_time:50110ms step_avg:57.40ms
step:874/2330 train_time:50169ms step_avg:57.40ms
step:875/2330 train_time:50226ms step_avg:57.40ms
step:876/2330 train_time:50286ms step_avg:57.40ms
step:877/2330 train_time:50342ms step_avg:57.40ms
step:878/2330 train_time:50403ms step_avg:57.41ms
step:879/2330 train_time:50459ms step_avg:57.41ms
step:880/2330 train_time:50519ms step_avg:57.41ms
step:881/2330 train_time:50576ms step_avg:57.41ms
step:882/2330 train_time:50636ms step_avg:57.41ms
step:883/2330 train_time:50692ms step_avg:57.41ms
step:884/2330 train_time:50752ms step_avg:57.41ms
step:885/2330 train_time:50808ms step_avg:57.41ms
step:886/2330 train_time:50867ms step_avg:57.41ms
step:887/2330 train_time:50923ms step_avg:57.41ms
step:888/2330 train_time:50983ms step_avg:57.41ms
step:889/2330 train_time:51039ms step_avg:57.41ms
step:890/2330 train_time:51099ms step_avg:57.41ms
step:891/2330 train_time:51156ms step_avg:57.41ms
step:892/2330 train_time:51215ms step_avg:57.42ms
step:893/2330 train_time:51272ms step_avg:57.42ms
step:894/2330 train_time:51331ms step_avg:57.42ms
step:895/2330 train_time:51388ms step_avg:57.42ms
step:896/2330 train_time:51448ms step_avg:57.42ms
step:897/2330 train_time:51504ms step_avg:57.42ms
step:898/2330 train_time:51565ms step_avg:57.42ms
step:899/2330 train_time:51621ms step_avg:57.42ms
step:900/2330 train_time:51682ms step_avg:57.42ms
step:901/2330 train_time:51739ms step_avg:57.42ms
step:902/2330 train_time:51798ms step_avg:57.43ms
step:903/2330 train_time:51855ms step_avg:57.43ms
step:904/2330 train_time:51914ms step_avg:57.43ms
step:905/2330 train_time:51970ms step_avg:57.43ms
step:906/2330 train_time:52029ms step_avg:57.43ms
step:907/2330 train_time:52086ms step_avg:57.43ms
step:908/2330 train_time:52146ms step_avg:57.43ms
step:909/2330 train_time:52202ms step_avg:57.43ms
step:910/2330 train_time:52262ms step_avg:57.43ms
step:911/2330 train_time:52319ms step_avg:57.43ms
step:912/2330 train_time:52379ms step_avg:57.43ms
step:913/2330 train_time:52436ms step_avg:57.43ms
step:914/2330 train_time:52496ms step_avg:57.44ms
step:915/2330 train_time:52553ms step_avg:57.43ms
step:916/2330 train_time:52612ms step_avg:57.44ms
step:917/2330 train_time:52669ms step_avg:57.44ms
step:918/2330 train_time:52728ms step_avg:57.44ms
step:919/2330 train_time:52785ms step_avg:57.44ms
step:920/2330 train_time:52845ms step_avg:57.44ms
step:921/2330 train_time:52901ms step_avg:57.44ms
step:922/2330 train_time:52960ms step_avg:57.44ms
step:923/2330 train_time:53018ms step_avg:57.44ms
step:924/2330 train_time:53077ms step_avg:57.44ms
step:925/2330 train_time:53134ms step_avg:57.44ms
step:926/2330 train_time:53194ms step_avg:57.44ms
step:927/2330 train_time:53250ms step_avg:57.44ms
step:928/2330 train_time:53309ms step_avg:57.44ms
step:929/2330 train_time:53365ms step_avg:57.44ms
step:930/2330 train_time:53426ms step_avg:57.45ms
step:931/2330 train_time:53482ms step_avg:57.45ms
step:932/2330 train_time:53542ms step_avg:57.45ms
step:933/2330 train_time:53599ms step_avg:57.45ms
step:934/2330 train_time:53659ms step_avg:57.45ms
step:935/2330 train_time:53716ms step_avg:57.45ms
step:936/2330 train_time:53775ms step_avg:57.45ms
step:937/2330 train_time:53832ms step_avg:57.45ms
step:938/2330 train_time:53891ms step_avg:57.45ms
step:939/2330 train_time:53947ms step_avg:57.45ms
step:940/2330 train_time:54007ms step_avg:57.45ms
step:941/2330 train_time:54063ms step_avg:57.45ms
step:942/2330 train_time:54124ms step_avg:57.46ms
step:943/2330 train_time:54180ms step_avg:57.46ms
step:944/2330 train_time:54241ms step_avg:57.46ms
step:945/2330 train_time:54297ms step_avg:57.46ms
step:946/2330 train_time:54357ms step_avg:57.46ms
step:947/2330 train_time:54414ms step_avg:57.46ms
step:948/2330 train_time:54474ms step_avg:57.46ms
step:949/2330 train_time:54530ms step_avg:57.46ms
step:950/2330 train_time:54590ms step_avg:57.46ms
step:951/2330 train_time:54646ms step_avg:57.46ms
step:952/2330 train_time:54706ms step_avg:57.46ms
step:953/2330 train_time:54762ms step_avg:57.46ms
step:954/2330 train_time:54824ms step_avg:57.47ms
step:955/2330 train_time:54880ms step_avg:57.47ms
step:956/2330 train_time:54941ms step_avg:57.47ms
step:957/2330 train_time:54997ms step_avg:57.47ms
step:958/2330 train_time:55057ms step_avg:57.47ms
step:959/2330 train_time:55114ms step_avg:57.47ms
step:960/2330 train_time:55174ms step_avg:57.47ms
step:961/2330 train_time:55230ms step_avg:57.47ms
step:962/2330 train_time:55290ms step_avg:57.47ms
step:963/2330 train_time:55346ms step_avg:57.47ms
step:964/2330 train_time:55406ms step_avg:57.47ms
step:965/2330 train_time:55461ms step_avg:57.47ms
step:966/2330 train_time:55523ms step_avg:57.48ms
step:967/2330 train_time:55579ms step_avg:57.48ms
step:968/2330 train_time:55638ms step_avg:57.48ms
step:969/2330 train_time:55696ms step_avg:57.48ms
step:970/2330 train_time:55755ms step_avg:57.48ms
step:971/2330 train_time:55811ms step_avg:57.48ms
step:972/2330 train_time:55870ms step_avg:57.48ms
step:973/2330 train_time:55926ms step_avg:57.48ms
step:974/2330 train_time:55986ms step_avg:57.48ms
step:975/2330 train_time:56043ms step_avg:57.48ms
step:976/2330 train_time:56103ms step_avg:57.48ms
step:977/2330 train_time:56160ms step_avg:57.48ms
step:978/2330 train_time:56220ms step_avg:57.49ms
step:979/2330 train_time:56277ms step_avg:57.48ms
step:980/2330 train_time:56338ms step_avg:57.49ms
step:981/2330 train_time:56395ms step_avg:57.49ms
step:982/2330 train_time:56454ms step_avg:57.49ms
step:983/2330 train_time:56511ms step_avg:57.49ms
step:984/2330 train_time:56570ms step_avg:57.49ms
step:985/2330 train_time:56626ms step_avg:57.49ms
step:986/2330 train_time:56686ms step_avg:57.49ms
step:987/2330 train_time:56742ms step_avg:57.49ms
step:988/2330 train_time:56802ms step_avg:57.49ms
step:989/2330 train_time:56858ms step_avg:57.49ms
step:990/2330 train_time:56918ms step_avg:57.49ms
step:991/2330 train_time:56974ms step_avg:57.49ms
step:992/2330 train_time:57034ms step_avg:57.49ms
step:993/2330 train_time:57091ms step_avg:57.49ms
step:994/2330 train_time:57150ms step_avg:57.50ms
step:995/2330 train_time:57206ms step_avg:57.49ms
step:996/2330 train_time:57266ms step_avg:57.50ms
step:997/2330 train_time:57322ms step_avg:57.49ms
step:998/2330 train_time:57383ms step_avg:57.50ms
step:999/2330 train_time:57439ms step_avg:57.50ms
step:1000/2330 train_time:57500ms step_avg:57.50ms
step:1000/2330 val_loss:4.4550 train_time:57579ms step_avg:57.58ms
step:1001/2330 train_time:57599ms step_avg:57.54ms
step:1002/2330 train_time:57620ms step_avg:57.51ms
step:1003/2330 train_time:57676ms step_avg:57.50ms
step:1004/2330 train_time:57737ms step_avg:57.51ms
step:1005/2330 train_time:57793ms step_avg:57.51ms
step:1006/2330 train_time:57857ms step_avg:57.51ms
step:1007/2330 train_time:57913ms step_avg:57.51ms
step:1008/2330 train_time:57973ms step_avg:57.51ms
step:1009/2330 train_time:58029ms step_avg:57.51ms
step:1010/2330 train_time:58089ms step_avg:57.51ms
step:1011/2330 train_time:58145ms step_avg:57.51ms
step:1012/2330 train_time:58204ms step_avg:57.51ms
step:1013/2330 train_time:58260ms step_avg:57.51ms
step:1014/2330 train_time:58319ms step_avg:57.51ms
step:1015/2330 train_time:58374ms step_avg:57.51ms
step:1016/2330 train_time:58433ms step_avg:57.51ms
step:1017/2330 train_time:58489ms step_avg:57.51ms
step:1018/2330 train_time:58553ms step_avg:57.52ms
step:1019/2330 train_time:58610ms step_avg:57.52ms
step:1020/2330 train_time:58672ms step_avg:57.52ms
step:1021/2330 train_time:58729ms step_avg:57.52ms
step:1022/2330 train_time:58790ms step_avg:57.52ms
step:1023/2330 train_time:58847ms step_avg:57.52ms
step:1024/2330 train_time:58909ms step_avg:57.53ms
step:1025/2330 train_time:58965ms step_avg:57.53ms
step:1026/2330 train_time:59025ms step_avg:57.53ms
step:1027/2330 train_time:59082ms step_avg:57.53ms
step:1028/2330 train_time:59140ms step_avg:57.53ms
step:1029/2330 train_time:59197ms step_avg:57.53ms
step:1030/2330 train_time:59256ms step_avg:57.53ms
step:1031/2330 train_time:59311ms step_avg:57.53ms
step:1032/2330 train_time:59371ms step_avg:57.53ms
step:1033/2330 train_time:59426ms step_avg:57.53ms
step:1034/2330 train_time:59486ms step_avg:57.53ms
step:1035/2330 train_time:59543ms step_avg:57.53ms
step:1036/2330 train_time:59604ms step_avg:57.53ms
step:1037/2330 train_time:59662ms step_avg:57.53ms
step:1038/2330 train_time:59722ms step_avg:57.54ms
step:1039/2330 train_time:59779ms step_avg:57.54ms
step:1040/2330 train_time:59840ms step_avg:57.54ms
step:1041/2330 train_time:59897ms step_avg:57.54ms
step:1042/2330 train_time:59956ms step_avg:57.54ms
step:1043/2330 train_time:60012ms step_avg:57.54ms
step:1044/2330 train_time:60072ms step_avg:57.54ms
step:1045/2330 train_time:60128ms step_avg:57.54ms
step:1046/2330 train_time:60188ms step_avg:57.54ms
step:1047/2330 train_time:60245ms step_avg:57.54ms
step:1048/2330 train_time:60305ms step_avg:57.54ms
step:1049/2330 train_time:60362ms step_avg:57.54ms
step:1050/2330 train_time:60421ms step_avg:57.54ms
step:1051/2330 train_time:60477ms step_avg:57.54ms
step:1052/2330 train_time:60537ms step_avg:57.54ms
step:1053/2330 train_time:60594ms step_avg:57.54ms
step:1054/2330 train_time:60654ms step_avg:57.55ms
step:1055/2330 train_time:60710ms step_avg:57.55ms
step:1056/2330 train_time:60771ms step_avg:57.55ms
step:1057/2330 train_time:60828ms step_avg:57.55ms
step:1058/2330 train_time:60888ms step_avg:57.55ms
step:1059/2330 train_time:60944ms step_avg:57.55ms
step:1060/2330 train_time:61005ms step_avg:57.55ms
step:1061/2330 train_time:61061ms step_avg:57.55ms
step:1062/2330 train_time:61121ms step_avg:57.55ms
step:1063/2330 train_time:61177ms step_avg:57.55ms
step:1064/2330 train_time:61237ms step_avg:57.55ms
step:1065/2330 train_time:61293ms step_avg:57.55ms
step:1066/2330 train_time:61352ms step_avg:57.55ms
step:1067/2330 train_time:61409ms step_avg:57.55ms
step:1068/2330 train_time:61469ms step_avg:57.56ms
step:1069/2330 train_time:61526ms step_avg:57.55ms
step:1070/2330 train_time:61586ms step_avg:57.56ms
step:1071/2330 train_time:61643ms step_avg:57.56ms
step:1072/2330 train_time:61702ms step_avg:57.56ms
step:1073/2330 train_time:61759ms step_avg:57.56ms
step:1074/2330 train_time:61819ms step_avg:57.56ms
step:1075/2330 train_time:61876ms step_avg:57.56ms
step:1076/2330 train_time:61934ms step_avg:57.56ms
step:1077/2330 train_time:61991ms step_avg:57.56ms
step:1078/2330 train_time:62052ms step_avg:57.56ms
step:1079/2330 train_time:62108ms step_avg:57.56ms
step:1080/2330 train_time:62168ms step_avg:57.56ms
step:1081/2330 train_time:62224ms step_avg:57.56ms
step:1082/2330 train_time:62285ms step_avg:57.56ms
step:1083/2330 train_time:62342ms step_avg:57.56ms
step:1084/2330 train_time:62401ms step_avg:57.57ms
step:1085/2330 train_time:62458ms step_avg:57.57ms
step:1086/2330 train_time:62518ms step_avg:57.57ms
step:1087/2330 train_time:62574ms step_avg:57.57ms
step:1088/2330 train_time:62633ms step_avg:57.57ms
step:1089/2330 train_time:62690ms step_avg:57.57ms
step:1090/2330 train_time:62750ms step_avg:57.57ms
step:1091/2330 train_time:62808ms step_avg:57.57ms
step:1092/2330 train_time:62868ms step_avg:57.57ms
step:1093/2330 train_time:62924ms step_avg:57.57ms
step:1094/2330 train_time:62984ms step_avg:57.57ms
step:1095/2330 train_time:63040ms step_avg:57.57ms
step:1096/2330 train_time:63101ms step_avg:57.57ms
step:1097/2330 train_time:63158ms step_avg:57.57ms
step:1098/2330 train_time:63217ms step_avg:57.57ms
step:1099/2330 train_time:63273ms step_avg:57.57ms
step:1100/2330 train_time:63333ms step_avg:57.58ms
step:1101/2330 train_time:63390ms step_avg:57.57ms
step:1102/2330 train_time:63450ms step_avg:57.58ms
step:1103/2330 train_time:63507ms step_avg:57.58ms
step:1104/2330 train_time:63566ms step_avg:57.58ms
step:1105/2330 train_time:63623ms step_avg:57.58ms
step:1106/2330 train_time:63682ms step_avg:57.58ms
step:1107/2330 train_time:63739ms step_avg:57.58ms
step:1108/2330 train_time:63799ms step_avg:57.58ms
step:1109/2330 train_time:63855ms step_avg:57.58ms
step:1110/2330 train_time:63915ms step_avg:57.58ms
step:1111/2330 train_time:63971ms step_avg:57.58ms
step:1112/2330 train_time:64032ms step_avg:57.58ms
step:1113/2330 train_time:64089ms step_avg:57.58ms
step:1114/2330 train_time:64148ms step_avg:57.58ms
step:1115/2330 train_time:64205ms step_avg:57.58ms
step:1116/2330 train_time:64266ms step_avg:57.59ms
step:1117/2330 train_time:64324ms step_avg:57.59ms
step:1118/2330 train_time:64383ms step_avg:57.59ms
step:1119/2330 train_time:64440ms step_avg:57.59ms
step:1120/2330 train_time:64498ms step_avg:57.59ms
step:1121/2330 train_time:64554ms step_avg:57.59ms
step:1122/2330 train_time:64614ms step_avg:57.59ms
step:1123/2330 train_time:64671ms step_avg:57.59ms
step:1124/2330 train_time:64731ms step_avg:57.59ms
step:1125/2330 train_time:64788ms step_avg:57.59ms
step:1126/2330 train_time:64847ms step_avg:57.59ms
step:1127/2330 train_time:64904ms step_avg:57.59ms
step:1128/2330 train_time:64964ms step_avg:57.59ms
step:1129/2330 train_time:65021ms step_avg:57.59ms
step:1130/2330 train_time:65080ms step_avg:57.59ms
step:1131/2330 train_time:65136ms step_avg:57.59ms
step:1132/2330 train_time:65196ms step_avg:57.59ms
step:1133/2330 train_time:65252ms step_avg:57.59ms
step:1134/2330 train_time:65313ms step_avg:57.60ms
step:1135/2330 train_time:65370ms step_avg:57.59ms
step:1136/2330 train_time:65429ms step_avg:57.60ms
step:1137/2330 train_time:65486ms step_avg:57.60ms
step:1138/2330 train_time:65545ms step_avg:57.60ms
step:1139/2330 train_time:65602ms step_avg:57.60ms
step:1140/2330 train_time:65663ms step_avg:57.60ms
step:1141/2330 train_time:65719ms step_avg:57.60ms
step:1142/2330 train_time:65778ms step_avg:57.60ms
step:1143/2330 train_time:65834ms step_avg:57.60ms
step:1144/2330 train_time:65894ms step_avg:57.60ms
step:1145/2330 train_time:65951ms step_avg:57.60ms
step:1146/2330 train_time:66011ms step_avg:57.60ms
step:1147/2330 train_time:66068ms step_avg:57.60ms
step:1148/2330 train_time:66128ms step_avg:57.60ms
step:1149/2330 train_time:66185ms step_avg:57.60ms
step:1150/2330 train_time:66245ms step_avg:57.60ms
step:1151/2330 train_time:66302ms step_avg:57.60ms
step:1152/2330 train_time:66362ms step_avg:57.61ms
step:1153/2330 train_time:66419ms step_avg:57.61ms
step:1154/2330 train_time:66478ms step_avg:57.61ms
step:1155/2330 train_time:66534ms step_avg:57.61ms
step:1156/2330 train_time:66594ms step_avg:57.61ms
step:1157/2330 train_time:66650ms step_avg:57.61ms
step:1158/2330 train_time:66711ms step_avg:57.61ms
step:1159/2330 train_time:66767ms step_avg:57.61ms
step:1160/2330 train_time:66827ms step_avg:57.61ms
step:1161/2330 train_time:66884ms step_avg:57.61ms
step:1162/2330 train_time:66943ms step_avg:57.61ms
step:1163/2330 train_time:67000ms step_avg:57.61ms
step:1164/2330 train_time:67060ms step_avg:57.61ms
step:1165/2330 train_time:67117ms step_avg:57.61ms
step:1166/2330 train_time:67176ms step_avg:57.61ms
step:1167/2330 train_time:67232ms step_avg:57.61ms
step:1168/2330 train_time:67291ms step_avg:57.61ms
step:1169/2330 train_time:67348ms step_avg:57.61ms
step:1170/2330 train_time:67408ms step_avg:57.61ms
step:1171/2330 train_time:67465ms step_avg:57.61ms
step:1172/2330 train_time:67525ms step_avg:57.61ms
step:1173/2330 train_time:67582ms step_avg:57.61ms
step:1174/2330 train_time:67642ms step_avg:57.62ms
step:1175/2330 train_time:67699ms step_avg:57.62ms
step:1176/2330 train_time:67758ms step_avg:57.62ms
step:1177/2330 train_time:67815ms step_avg:57.62ms
step:1178/2330 train_time:67874ms step_avg:57.62ms
step:1179/2330 train_time:67930ms step_avg:57.62ms
step:1180/2330 train_time:67990ms step_avg:57.62ms
step:1181/2330 train_time:68046ms step_avg:57.62ms
step:1182/2330 train_time:68106ms step_avg:57.62ms
step:1183/2330 train_time:68164ms step_avg:57.62ms
step:1184/2330 train_time:68223ms step_avg:57.62ms
step:1185/2330 train_time:68280ms step_avg:57.62ms
step:1186/2330 train_time:68339ms step_avg:57.62ms
step:1187/2330 train_time:68395ms step_avg:57.62ms
step:1188/2330 train_time:68455ms step_avg:57.62ms
step:1189/2330 train_time:68512ms step_avg:57.62ms
step:1190/2330 train_time:68572ms step_avg:57.62ms
step:1191/2330 train_time:68629ms step_avg:57.62ms
step:1192/2330 train_time:68689ms step_avg:57.62ms
step:1193/2330 train_time:68745ms step_avg:57.62ms
step:1194/2330 train_time:68805ms step_avg:57.63ms
step:1195/2330 train_time:68861ms step_avg:57.62ms
step:1196/2330 train_time:68921ms step_avg:57.63ms
step:1197/2330 train_time:68978ms step_avg:57.63ms
step:1198/2330 train_time:69037ms step_avg:57.63ms
step:1199/2330 train_time:69093ms step_avg:57.63ms
step:1200/2330 train_time:69152ms step_avg:57.63ms
step:1201/2330 train_time:69209ms step_avg:57.63ms
step:1202/2330 train_time:69269ms step_avg:57.63ms
step:1203/2330 train_time:69326ms step_avg:57.63ms
step:1204/2330 train_time:69385ms step_avg:57.63ms
step:1205/2330 train_time:69442ms step_avg:57.63ms
step:1206/2330 train_time:69502ms step_avg:57.63ms
step:1207/2330 train_time:69559ms step_avg:57.63ms
step:1208/2330 train_time:69618ms step_avg:57.63ms
step:1209/2330 train_time:69675ms step_avg:57.63ms
step:1210/2330 train_time:69734ms step_avg:57.63ms
step:1211/2330 train_time:69791ms step_avg:57.63ms
step:1212/2330 train_time:69851ms step_avg:57.63ms
step:1213/2330 train_time:69908ms step_avg:57.63ms
step:1214/2330 train_time:69968ms step_avg:57.63ms
step:1215/2330 train_time:70025ms step_avg:57.63ms
step:1216/2330 train_time:70084ms step_avg:57.64ms
step:1217/2330 train_time:70141ms step_avg:57.63ms
step:1218/2330 train_time:70200ms step_avg:57.64ms
step:1219/2330 train_time:70257ms step_avg:57.64ms
step:1220/2330 train_time:70317ms step_avg:57.64ms
step:1221/2330 train_time:70373ms step_avg:57.64ms
step:1222/2330 train_time:70433ms step_avg:57.64ms
step:1223/2330 train_time:70488ms step_avg:57.64ms
step:1224/2330 train_time:70550ms step_avg:57.64ms
step:1225/2330 train_time:70608ms step_avg:57.64ms
step:1226/2330 train_time:70667ms step_avg:57.64ms
step:1227/2330 train_time:70724ms step_avg:57.64ms
step:1228/2330 train_time:70783ms step_avg:57.64ms
step:1229/2330 train_time:70840ms step_avg:57.64ms
step:1230/2330 train_time:70899ms step_avg:57.64ms
step:1231/2330 train_time:70956ms step_avg:57.64ms
step:1232/2330 train_time:71015ms step_avg:57.64ms
step:1233/2330 train_time:71072ms step_avg:57.64ms
step:1234/2330 train_time:71132ms step_avg:57.64ms
step:1235/2330 train_time:71188ms step_avg:57.64ms
step:1236/2330 train_time:71249ms step_avg:57.64ms
step:1237/2330 train_time:71305ms step_avg:57.64ms
step:1238/2330 train_time:71365ms step_avg:57.65ms
step:1239/2330 train_time:71422ms step_avg:57.64ms
step:1240/2330 train_time:71481ms step_avg:57.65ms
step:1241/2330 train_time:71537ms step_avg:57.64ms
step:1242/2330 train_time:71597ms step_avg:57.65ms
step:1243/2330 train_time:71654ms step_avg:57.65ms
step:1244/2330 train_time:71714ms step_avg:57.65ms
step:1245/2330 train_time:71770ms step_avg:57.65ms
step:1246/2330 train_time:71830ms step_avg:57.65ms
step:1247/2330 train_time:71887ms step_avg:57.65ms
step:1248/2330 train_time:71946ms step_avg:57.65ms
step:1249/2330 train_time:72003ms step_avg:57.65ms
step:1250/2330 train_time:72063ms step_avg:57.65ms
step:1250/2330 val_loss:4.3095 train_time:72143ms step_avg:57.71ms
step:1251/2330 train_time:72162ms step_avg:57.68ms
step:1252/2330 train_time:72184ms step_avg:57.66ms
step:1253/2330 train_time:72242ms step_avg:57.66ms
step:1254/2330 train_time:72304ms step_avg:57.66ms
step:1255/2330 train_time:72362ms step_avg:57.66ms
step:1256/2330 train_time:72422ms step_avg:57.66ms
step:1257/2330 train_time:72479ms step_avg:57.66ms
step:1258/2330 train_time:72538ms step_avg:57.66ms
step:1259/2330 train_time:72594ms step_avg:57.66ms
step:1260/2330 train_time:72653ms step_avg:57.66ms
step:1261/2330 train_time:72709ms step_avg:57.66ms
step:1262/2330 train_time:72768ms step_avg:57.66ms
step:1263/2330 train_time:72824ms step_avg:57.66ms
step:1264/2330 train_time:72883ms step_avg:57.66ms
step:1265/2330 train_time:72939ms step_avg:57.66ms
step:1266/2330 train_time:72998ms step_avg:57.66ms
step:1267/2330 train_time:73055ms step_avg:57.66ms
step:1268/2330 train_time:73114ms step_avg:57.66ms
step:1269/2330 train_time:73171ms step_avg:57.66ms
step:1270/2330 train_time:73232ms step_avg:57.66ms
step:1271/2330 train_time:73289ms step_avg:57.66ms
step:1272/2330 train_time:73350ms step_avg:57.67ms
step:1273/2330 train_time:73407ms step_avg:57.66ms
step:1274/2330 train_time:73467ms step_avg:57.67ms
step:1275/2330 train_time:73524ms step_avg:57.67ms
step:1276/2330 train_time:73584ms step_avg:57.67ms
step:1277/2330 train_time:73640ms step_avg:57.67ms
step:1278/2330 train_time:73700ms step_avg:57.67ms
step:1279/2330 train_time:73757ms step_avg:57.67ms
step:1280/2330 train_time:73816ms step_avg:57.67ms
step:1281/2330 train_time:73872ms step_avg:57.67ms
step:1282/2330 train_time:73931ms step_avg:57.67ms
step:1283/2330 train_time:73988ms step_avg:57.67ms
step:1284/2330 train_time:74047ms step_avg:57.67ms
step:1285/2330 train_time:74104ms step_avg:57.67ms
step:1286/2330 train_time:74164ms step_avg:57.67ms
step:1287/2330 train_time:74221ms step_avg:57.67ms
step:1288/2330 train_time:74282ms step_avg:57.67ms
step:1289/2330 train_time:74339ms step_avg:57.67ms
step:1290/2330 train_time:74399ms step_avg:57.67ms
step:1291/2330 train_time:74457ms step_avg:57.67ms
step:1292/2330 train_time:74516ms step_avg:57.67ms
step:1293/2330 train_time:74572ms step_avg:57.67ms
step:1294/2330 train_time:74631ms step_avg:57.67ms
step:1295/2330 train_time:74688ms step_avg:57.67ms
step:1296/2330 train_time:74749ms step_avg:57.68ms
step:1297/2330 train_time:74804ms step_avg:57.67ms
step:1298/2330 train_time:74865ms step_avg:57.68ms
step:1299/2330 train_time:74921ms step_avg:57.68ms
step:1300/2330 train_time:74981ms step_avg:57.68ms
step:1301/2330 train_time:75038ms step_avg:57.68ms
step:1302/2330 train_time:75097ms step_avg:57.68ms
step:1303/2330 train_time:75153ms step_avg:57.68ms
step:1304/2330 train_time:75212ms step_avg:57.68ms
step:1305/2330 train_time:75268ms step_avg:57.68ms
step:1306/2330 train_time:75330ms step_avg:57.68ms
step:1307/2330 train_time:75386ms step_avg:57.68ms
step:1308/2330 train_time:75447ms step_avg:57.68ms
step:1309/2330 train_time:75504ms step_avg:57.68ms
step:1310/2330 train_time:75564ms step_avg:57.68ms
step:1311/2330 train_time:75621ms step_avg:57.68ms
step:1312/2330 train_time:75680ms step_avg:57.68ms
step:1313/2330 train_time:75736ms step_avg:57.68ms
step:1314/2330 train_time:75795ms step_avg:57.68ms
step:1315/2330 train_time:75852ms step_avg:57.68ms
step:1316/2330 train_time:75911ms step_avg:57.68ms
step:1317/2330 train_time:75967ms step_avg:57.68ms
step:1318/2330 train_time:76028ms step_avg:57.68ms
step:1319/2330 train_time:76084ms step_avg:57.68ms
step:1320/2330 train_time:76144ms step_avg:57.68ms
step:1321/2330 train_time:76201ms step_avg:57.68ms
step:1322/2330 train_time:76261ms step_avg:57.69ms
step:1323/2330 train_time:76318ms step_avg:57.69ms
step:1324/2330 train_time:76377ms step_avg:57.69ms
step:1325/2330 train_time:76434ms step_avg:57.69ms
step:1326/2330 train_time:76493ms step_avg:57.69ms
step:1327/2330 train_time:76549ms step_avg:57.69ms
step:1328/2330 train_time:76609ms step_avg:57.69ms
step:1329/2330 train_time:76666ms step_avg:57.69ms
step:1330/2330 train_time:76726ms step_avg:57.69ms
step:1331/2330 train_time:76782ms step_avg:57.69ms
step:1332/2330 train_time:76841ms step_avg:57.69ms
step:1333/2330 train_time:76898ms step_avg:57.69ms
step:1334/2330 train_time:76958ms step_avg:57.69ms
step:1335/2330 train_time:77014ms step_avg:57.69ms
step:1336/2330 train_time:77073ms step_avg:57.69ms
step:1337/2330 train_time:77130ms step_avg:57.69ms
step:1338/2330 train_time:77189ms step_avg:57.69ms
step:1339/2330 train_time:77245ms step_avg:57.69ms
step:1340/2330 train_time:77305ms step_avg:57.69ms
step:1341/2330 train_time:77362ms step_avg:57.69ms
step:1342/2330 train_time:77422ms step_avg:57.69ms
step:1343/2330 train_time:77479ms step_avg:57.69ms
step:1344/2330 train_time:77539ms step_avg:57.69ms
step:1345/2330 train_time:77595ms step_avg:57.69ms
step:1346/2330 train_time:77655ms step_avg:57.69ms
step:1347/2330 train_time:77711ms step_avg:57.69ms
step:1348/2330 train_time:77769ms step_avg:57.69ms
step:1349/2330 train_time:77826ms step_avg:57.69ms
step:1350/2330 train_time:77886ms step_avg:57.69ms
step:1351/2330 train_time:77942ms step_avg:57.69ms
step:1352/2330 train_time:78002ms step_avg:57.69ms
step:1353/2330 train_time:78060ms step_avg:57.69ms
step:1354/2330 train_time:78119ms step_avg:57.70ms
step:1355/2330 train_time:78176ms step_avg:57.69ms
step:1356/2330 train_time:78235ms step_avg:57.70ms
step:1357/2330 train_time:78292ms step_avg:57.70ms
step:1358/2330 train_time:78351ms step_avg:57.70ms
step:1359/2330 train_time:78408ms step_avg:57.70ms
step:1360/2330 train_time:78467ms step_avg:57.70ms
step:1361/2330 train_time:78524ms step_avg:57.70ms
step:1362/2330 train_time:78584ms step_avg:57.70ms
step:1363/2330 train_time:78642ms step_avg:57.70ms
step:1364/2330 train_time:78701ms step_avg:57.70ms
step:1365/2330 train_time:78757ms step_avg:57.70ms
step:1366/2330 train_time:78816ms step_avg:57.70ms
step:1367/2330 train_time:78873ms step_avg:57.70ms
step:1368/2330 train_time:78932ms step_avg:57.70ms
step:1369/2330 train_time:78988ms step_avg:57.70ms
step:1370/2330 train_time:79048ms step_avg:57.70ms
step:1371/2330 train_time:79105ms step_avg:57.70ms
step:1372/2330 train_time:79165ms step_avg:57.70ms
step:1373/2330 train_time:79222ms step_avg:57.70ms
step:1374/2330 train_time:79281ms step_avg:57.70ms
step:1375/2330 train_time:79338ms step_avg:57.70ms
step:1376/2330 train_time:79398ms step_avg:57.70ms
step:1377/2330 train_time:79455ms step_avg:57.70ms
step:1378/2330 train_time:79513ms step_avg:57.70ms
step:1379/2330 train_time:79570ms step_avg:57.70ms
step:1380/2330 train_time:79629ms step_avg:57.70ms
step:1381/2330 train_time:79686ms step_avg:57.70ms
step:1382/2330 train_time:79746ms step_avg:57.70ms
step:1383/2330 train_time:79804ms step_avg:57.70ms
step:1384/2330 train_time:79863ms step_avg:57.70ms
step:1385/2330 train_time:79920ms step_avg:57.70ms
step:1386/2330 train_time:79980ms step_avg:57.71ms
step:1387/2330 train_time:80037ms step_avg:57.70ms
step:1388/2330 train_time:80096ms step_avg:57.71ms
step:1389/2330 train_time:80153ms step_avg:57.71ms
step:1390/2330 train_time:80212ms step_avg:57.71ms
step:1391/2330 train_time:80268ms step_avg:57.71ms
step:1392/2330 train_time:80329ms step_avg:57.71ms
step:1393/2330 train_time:80385ms step_avg:57.71ms
step:1394/2330 train_time:80445ms step_avg:57.71ms
step:1395/2330 train_time:80502ms step_avg:57.71ms
step:1396/2330 train_time:80561ms step_avg:57.71ms
step:1397/2330 train_time:80617ms step_avg:57.71ms
step:1398/2330 train_time:80676ms step_avg:57.71ms
step:1399/2330 train_time:80734ms step_avg:57.71ms
step:1400/2330 train_time:80793ms step_avg:57.71ms
step:1401/2330 train_time:80849ms step_avg:57.71ms
step:1402/2330 train_time:80909ms step_avg:57.71ms
step:1403/2330 train_time:80965ms step_avg:57.71ms
step:1404/2330 train_time:81026ms step_avg:57.71ms
step:1405/2330 train_time:81082ms step_avg:57.71ms
step:1406/2330 train_time:81143ms step_avg:57.71ms
step:1407/2330 train_time:81200ms step_avg:57.71ms
step:1408/2330 train_time:81259ms step_avg:57.71ms
step:1409/2330 train_time:81315ms step_avg:57.71ms
step:1410/2330 train_time:81375ms step_avg:57.71ms
step:1411/2330 train_time:81432ms step_avg:57.71ms
step:1412/2330 train_time:81491ms step_avg:57.71ms
step:1413/2330 train_time:81547ms step_avg:57.71ms
step:1414/2330 train_time:81606ms step_avg:57.71ms
step:1415/2330 train_time:81663ms step_avg:57.71ms
step:1416/2330 train_time:81722ms step_avg:57.71ms
step:1417/2330 train_time:81779ms step_avg:57.71ms
step:1418/2330 train_time:81838ms step_avg:57.71ms
step:1419/2330 train_time:81895ms step_avg:57.71ms
step:1420/2330 train_time:81954ms step_avg:57.71ms
step:1421/2330 train_time:82011ms step_avg:57.71ms
step:1422/2330 train_time:82070ms step_avg:57.71ms
step:1423/2330 train_time:82128ms step_avg:57.71ms
step:1424/2330 train_time:82187ms step_avg:57.72ms
step:1425/2330 train_time:82244ms step_avg:57.72ms
step:1426/2330 train_time:82303ms step_avg:57.72ms
step:1427/2330 train_time:82360ms step_avg:57.72ms
step:1428/2330 train_time:82420ms step_avg:57.72ms
step:1429/2330 train_time:82477ms step_avg:57.72ms
step:1430/2330 train_time:82536ms step_avg:57.72ms
step:1431/2330 train_time:82592ms step_avg:57.72ms
step:1432/2330 train_time:82651ms step_avg:57.72ms
step:1433/2330 train_time:82708ms step_avg:57.72ms
step:1434/2330 train_time:82767ms step_avg:57.72ms
step:1435/2330 train_time:82824ms step_avg:57.72ms
step:1436/2330 train_time:82884ms step_avg:57.72ms
step:1437/2330 train_time:82942ms step_avg:57.72ms
step:1438/2330 train_time:83001ms step_avg:57.72ms
step:1439/2330 train_time:83058ms step_avg:57.72ms
step:1440/2330 train_time:83117ms step_avg:57.72ms
step:1441/2330 train_time:83174ms step_avg:57.72ms
step:1442/2330 train_time:83234ms step_avg:57.72ms
step:1443/2330 train_time:83290ms step_avg:57.72ms
step:1444/2330 train_time:83349ms step_avg:57.72ms
step:1445/2330 train_time:83405ms step_avg:57.72ms
step:1446/2330 train_time:83467ms step_avg:57.72ms
step:1447/2330 train_time:83523ms step_avg:57.72ms
step:1448/2330 train_time:83584ms step_avg:57.72ms
step:1449/2330 train_time:83641ms step_avg:57.72ms
step:1450/2330 train_time:83700ms step_avg:57.72ms
step:1451/2330 train_time:83757ms step_avg:57.72ms
step:1452/2330 train_time:83816ms step_avg:57.72ms
step:1453/2330 train_time:83872ms step_avg:57.72ms
step:1454/2330 train_time:83932ms step_avg:57.73ms
step:1455/2330 train_time:83989ms step_avg:57.72ms
step:1456/2330 train_time:84048ms step_avg:57.73ms
step:1457/2330 train_time:84105ms step_avg:57.72ms
step:1458/2330 train_time:84165ms step_avg:57.73ms
step:1459/2330 train_time:84223ms step_avg:57.73ms
step:1460/2330 train_time:84283ms step_avg:57.73ms
step:1461/2330 train_time:84340ms step_avg:57.73ms
step:1462/2330 train_time:84399ms step_avg:57.73ms
step:1463/2330 train_time:84456ms step_avg:57.73ms
step:1464/2330 train_time:84515ms step_avg:57.73ms
step:1465/2330 train_time:84572ms step_avg:57.73ms
step:1466/2330 train_time:84631ms step_avg:57.73ms
step:1467/2330 train_time:84687ms step_avg:57.73ms
step:1468/2330 train_time:84747ms step_avg:57.73ms
step:1469/2330 train_time:84804ms step_avg:57.73ms
step:1470/2330 train_time:84863ms step_avg:57.73ms
step:1471/2330 train_time:84920ms step_avg:57.73ms
step:1472/2330 train_time:84979ms step_avg:57.73ms
step:1473/2330 train_time:85036ms step_avg:57.73ms
step:1474/2330 train_time:85095ms step_avg:57.73ms
step:1475/2330 train_time:85152ms step_avg:57.73ms
step:1476/2330 train_time:85210ms step_avg:57.73ms
step:1477/2330 train_time:85267ms step_avg:57.73ms
step:1478/2330 train_time:85327ms step_avg:57.73ms
step:1479/2330 train_time:85383ms step_avg:57.73ms
step:1480/2330 train_time:85444ms step_avg:57.73ms
step:1481/2330 train_time:85501ms step_avg:57.73ms
step:1482/2330 train_time:85561ms step_avg:57.73ms
step:1483/2330 train_time:85618ms step_avg:57.73ms
step:1484/2330 train_time:85677ms step_avg:57.73ms
step:1485/2330 train_time:85734ms step_avg:57.73ms
step:1486/2330 train_time:85794ms step_avg:57.73ms
step:1487/2330 train_time:85851ms step_avg:57.73ms
step:1488/2330 train_time:85909ms step_avg:57.73ms
step:1489/2330 train_time:85966ms step_avg:57.73ms
step:1490/2330 train_time:86026ms step_avg:57.74ms
step:1491/2330 train_time:86083ms step_avg:57.74ms
step:1492/2330 train_time:86142ms step_avg:57.74ms
step:1493/2330 train_time:86199ms step_avg:57.74ms
step:1494/2330 train_time:86258ms step_avg:57.74ms
step:1495/2330 train_time:86315ms step_avg:57.74ms
step:1496/2330 train_time:86374ms step_avg:57.74ms
step:1497/2330 train_time:86431ms step_avg:57.74ms
step:1498/2330 train_time:86490ms step_avg:57.74ms
step:1499/2330 train_time:86546ms step_avg:57.74ms
step:1500/2330 train_time:86607ms step_avg:57.74ms
step:1500/2330 val_loss:4.1948 train_time:86687ms step_avg:57.79ms
step:1501/2330 train_time:86707ms step_avg:57.77ms
step:1502/2330 train_time:86727ms step_avg:57.74ms
step:1503/2330 train_time:86784ms step_avg:57.74ms
step:1504/2330 train_time:86849ms step_avg:57.75ms
step:1505/2330 train_time:86905ms step_avg:57.74ms
step:1506/2330 train_time:86965ms step_avg:57.75ms
step:1507/2330 train_time:87022ms step_avg:57.75ms
step:1508/2330 train_time:87082ms step_avg:57.75ms
step:1509/2330 train_time:87139ms step_avg:57.75ms
step:1510/2330 train_time:87197ms step_avg:57.75ms
step:1511/2330 train_time:87253ms step_avg:57.75ms
step:1512/2330 train_time:87312ms step_avg:57.75ms
step:1513/2330 train_time:87368ms step_avg:57.74ms
step:1514/2330 train_time:87427ms step_avg:57.75ms
step:1515/2330 train_time:87483ms step_avg:57.74ms
step:1516/2330 train_time:87542ms step_avg:57.75ms
step:1517/2330 train_time:87598ms step_avg:57.74ms
step:1518/2330 train_time:87658ms step_avg:57.75ms
step:1519/2330 train_time:87716ms step_avg:57.75ms
step:1520/2330 train_time:87777ms step_avg:57.75ms
step:1521/2330 train_time:87835ms step_avg:57.75ms
step:1522/2330 train_time:87896ms step_avg:57.75ms
step:1523/2330 train_time:87952ms step_avg:57.75ms
step:1524/2330 train_time:88011ms step_avg:57.75ms
step:1525/2330 train_time:88068ms step_avg:57.75ms
step:1526/2330 train_time:88128ms step_avg:57.75ms
step:1527/2330 train_time:88184ms step_avg:57.75ms
step:1528/2330 train_time:88245ms step_avg:57.75ms
step:1529/2330 train_time:88303ms step_avg:57.75ms
step:1530/2330 train_time:88361ms step_avg:57.75ms
step:1531/2330 train_time:88418ms step_avg:57.75ms
step:1532/2330 train_time:88478ms step_avg:57.75ms
step:1533/2330 train_time:88534ms step_avg:57.75ms
step:1534/2330 train_time:88594ms step_avg:57.75ms
step:1535/2330 train_time:88652ms step_avg:57.75ms
step:1536/2330 train_time:88711ms step_avg:57.75ms
step:1537/2330 train_time:88769ms step_avg:57.75ms
step:1538/2330 train_time:88830ms step_avg:57.76ms
step:1539/2330 train_time:88887ms step_avg:57.76ms
step:1540/2330 train_time:88948ms step_avg:57.76ms
step:1541/2330 train_time:89005ms step_avg:57.76ms
step:1542/2330 train_time:89066ms step_avg:57.76ms
step:1543/2330 train_time:89124ms step_avg:57.76ms
step:1544/2330 train_time:89184ms step_avg:57.76ms
step:1545/2330 train_time:89241ms step_avg:57.76ms
step:1546/2330 train_time:89302ms step_avg:57.76ms
step:1547/2330 train_time:89358ms step_avg:57.76ms
step:1548/2330 train_time:89419ms step_avg:57.76ms
step:1549/2330 train_time:89476ms step_avg:57.76ms
step:1550/2330 train_time:89536ms step_avg:57.77ms
step:1551/2330 train_time:89593ms step_avg:57.76ms
step:1552/2330 train_time:89653ms step_avg:57.77ms
step:1553/2330 train_time:89710ms step_avg:57.77ms
step:1554/2330 train_time:89770ms step_avg:57.77ms
step:1555/2330 train_time:89828ms step_avg:57.77ms
step:1556/2330 train_time:89888ms step_avg:57.77ms
step:1557/2330 train_time:89944ms step_avg:57.77ms
step:1558/2330 train_time:90005ms step_avg:57.77ms
step:1559/2330 train_time:90062ms step_avg:57.77ms
step:1560/2330 train_time:90123ms step_avg:57.77ms
step:1561/2330 train_time:90180ms step_avg:57.77ms
step:1562/2330 train_time:90240ms step_avg:57.77ms
step:1563/2330 train_time:90297ms step_avg:57.77ms
step:1564/2330 train_time:90357ms step_avg:57.77ms
step:1565/2330 train_time:90414ms step_avg:57.77ms
step:1566/2330 train_time:90474ms step_avg:57.77ms
step:1567/2330 train_time:90531ms step_avg:57.77ms
step:1568/2330 train_time:90591ms step_avg:57.77ms
step:1569/2330 train_time:90648ms step_avg:57.77ms
step:1570/2330 train_time:90708ms step_avg:57.78ms
step:1571/2330 train_time:90765ms step_avg:57.78ms
step:1572/2330 train_time:90826ms step_avg:57.78ms
step:1573/2330 train_time:90884ms step_avg:57.78ms
step:1574/2330 train_time:90944ms step_avg:57.78ms
step:1575/2330 train_time:91001ms step_avg:57.78ms
step:1576/2330 train_time:91062ms step_avg:57.78ms
step:1577/2330 train_time:91119ms step_avg:57.78ms
step:1578/2330 train_time:91178ms step_avg:57.78ms
step:1579/2330 train_time:91236ms step_avg:57.78ms
step:1580/2330 train_time:91296ms step_avg:57.78ms
step:1581/2330 train_time:91353ms step_avg:57.78ms
step:1582/2330 train_time:91412ms step_avg:57.78ms
step:1583/2330 train_time:91469ms step_avg:57.78ms
step:1584/2330 train_time:91530ms step_avg:57.78ms
step:1585/2330 train_time:91586ms step_avg:57.78ms
step:1586/2330 train_time:91646ms step_avg:57.78ms
step:1587/2330 train_time:91702ms step_avg:57.78ms
step:1588/2330 train_time:91763ms step_avg:57.79ms
step:1589/2330 train_time:91821ms step_avg:57.79ms
step:1590/2330 train_time:91881ms step_avg:57.79ms
step:1591/2330 train_time:91938ms step_avg:57.79ms
step:1592/2330 train_time:91998ms step_avg:57.79ms
step:1593/2330 train_time:92056ms step_avg:57.79ms
step:1594/2330 train_time:92116ms step_avg:57.79ms
step:1595/2330 train_time:92174ms step_avg:57.79ms
step:1596/2330 train_time:92234ms step_avg:57.79ms
step:1597/2330 train_time:92291ms step_avg:57.79ms
step:1598/2330 train_time:92350ms step_avg:57.79ms
step:1599/2330 train_time:92406ms step_avg:57.79ms
step:1600/2330 train_time:92467ms step_avg:57.79ms
step:1601/2330 train_time:92524ms step_avg:57.79ms
step:1602/2330 train_time:92584ms step_avg:57.79ms
step:1603/2330 train_time:92641ms step_avg:57.79ms
step:1604/2330 train_time:92701ms step_avg:57.79ms
step:1605/2330 train_time:92758ms step_avg:57.79ms
step:1606/2330 train_time:92817ms step_avg:57.79ms
step:1607/2330 train_time:92874ms step_avg:57.79ms
step:1608/2330 train_time:92934ms step_avg:57.79ms
step:1609/2330 train_time:92990ms step_avg:57.79ms
step:1610/2330 train_time:93051ms step_avg:57.80ms
step:1611/2330 train_time:93108ms step_avg:57.80ms
step:1612/2330 train_time:93168ms step_avg:57.80ms
step:1613/2330 train_time:93225ms step_avg:57.80ms
step:1614/2330 train_time:93287ms step_avg:57.80ms
step:1615/2330 train_time:93344ms step_avg:57.80ms
step:1616/2330 train_time:93404ms step_avg:57.80ms
step:1617/2330 train_time:93460ms step_avg:57.80ms
step:1618/2330 train_time:93521ms step_avg:57.80ms
step:1619/2330 train_time:93578ms step_avg:57.80ms
step:1620/2330 train_time:93637ms step_avg:57.80ms
step:1621/2330 train_time:93694ms step_avg:57.80ms
step:1622/2330 train_time:93754ms step_avg:57.80ms
step:1623/2330 train_time:93811ms step_avg:57.80ms
step:1624/2330 train_time:93870ms step_avg:57.80ms
step:1625/2330 train_time:93927ms step_avg:57.80ms
step:1626/2330 train_time:93987ms step_avg:57.80ms
step:1627/2330 train_time:94044ms step_avg:57.80ms
step:1628/2330 train_time:94104ms step_avg:57.80ms
step:1629/2330 train_time:94162ms step_avg:57.80ms
step:1630/2330 train_time:94222ms step_avg:57.81ms
step:1631/2330 train_time:94280ms step_avg:57.80ms
step:1632/2330 train_time:94339ms step_avg:57.81ms
step:1633/2330 train_time:94396ms step_avg:57.81ms
step:1634/2330 train_time:94456ms step_avg:57.81ms
step:1635/2330 train_time:94512ms step_avg:57.81ms
step:1636/2330 train_time:94572ms step_avg:57.81ms
step:1637/2330 train_time:94630ms step_avg:57.81ms
step:1638/2330 train_time:94689ms step_avg:57.81ms
step:1639/2330 train_time:94746ms step_avg:57.81ms
step:1640/2330 train_time:94806ms step_avg:57.81ms
step:1641/2330 train_time:94863ms step_avg:57.81ms
step:1642/2330 train_time:94923ms step_avg:57.81ms
step:1643/2330 train_time:94980ms step_avg:57.81ms
step:1644/2330 train_time:95041ms step_avg:57.81ms
step:1645/2330 train_time:95098ms step_avg:57.81ms
step:1646/2330 train_time:95158ms step_avg:57.81ms
step:1647/2330 train_time:95216ms step_avg:57.81ms
step:1648/2330 train_time:95275ms step_avg:57.81ms
step:1649/2330 train_time:95333ms step_avg:57.81ms
step:1650/2330 train_time:95393ms step_avg:57.81ms
step:1651/2330 train_time:95450ms step_avg:57.81ms
step:1652/2330 train_time:95509ms step_avg:57.81ms
step:1653/2330 train_time:95566ms step_avg:57.81ms
step:1654/2330 train_time:95627ms step_avg:57.82ms
step:1655/2330 train_time:95683ms step_avg:57.81ms
step:1656/2330 train_time:95744ms step_avg:57.82ms
step:1657/2330 train_time:95800ms step_avg:57.82ms
step:1658/2330 train_time:95862ms step_avg:57.82ms
step:1659/2330 train_time:95918ms step_avg:57.82ms
step:1660/2330 train_time:95978ms step_avg:57.82ms
step:1661/2330 train_time:96035ms step_avg:57.82ms
step:1662/2330 train_time:96095ms step_avg:57.82ms
step:1663/2330 train_time:96152ms step_avg:57.82ms
step:1664/2330 train_time:96211ms step_avg:57.82ms
step:1665/2330 train_time:96268ms step_avg:57.82ms
step:1666/2330 train_time:96328ms step_avg:57.82ms
step:1667/2330 train_time:96386ms step_avg:57.82ms
step:1668/2330 train_time:96446ms step_avg:57.82ms
step:1669/2330 train_time:96503ms step_avg:57.82ms
step:1670/2330 train_time:96563ms step_avg:57.82ms
step:1671/2330 train_time:96621ms step_avg:57.82ms
step:1672/2330 train_time:96680ms step_avg:57.82ms
step:1673/2330 train_time:96738ms step_avg:57.82ms
step:1674/2330 train_time:96798ms step_avg:57.82ms
step:1675/2330 train_time:96855ms step_avg:57.82ms
step:1676/2330 train_time:96915ms step_avg:57.82ms
step:1677/2330 train_time:96971ms step_avg:57.82ms
step:1678/2330 train_time:97031ms step_avg:57.83ms
step:1679/2330 train_time:97087ms step_avg:57.82ms
step:1680/2330 train_time:97148ms step_avg:57.83ms
step:1681/2330 train_time:97205ms step_avg:57.83ms
step:1682/2330 train_time:97265ms step_avg:57.83ms
step:1683/2330 train_time:97322ms step_avg:57.83ms
step:1684/2330 train_time:97383ms step_avg:57.83ms
step:1685/2330 train_time:97440ms step_avg:57.83ms
step:1686/2330 train_time:97500ms step_avg:57.83ms
step:1687/2330 train_time:97558ms step_avg:57.83ms
step:1688/2330 train_time:97618ms step_avg:57.83ms
step:1689/2330 train_time:97676ms step_avg:57.83ms
step:1690/2330 train_time:97736ms step_avg:57.83ms
step:1691/2330 train_time:97794ms step_avg:57.83ms
step:1692/2330 train_time:97853ms step_avg:57.83ms
step:1693/2330 train_time:97910ms step_avg:57.83ms
step:1694/2330 train_time:97970ms step_avg:57.83ms
step:1695/2330 train_time:98027ms step_avg:57.83ms
step:1696/2330 train_time:98087ms step_avg:57.83ms
step:1697/2330 train_time:98144ms step_avg:57.83ms
step:1698/2330 train_time:98205ms step_avg:57.84ms
step:1699/2330 train_time:98262ms step_avg:57.84ms
step:1700/2330 train_time:98321ms step_avg:57.84ms
step:1701/2330 train_time:98378ms step_avg:57.84ms
step:1702/2330 train_time:98439ms step_avg:57.84ms
step:1703/2330 train_time:98496ms step_avg:57.84ms
step:1704/2330 train_time:98555ms step_avg:57.84ms
step:1705/2330 train_time:98612ms step_avg:57.84ms
step:1706/2330 train_time:98672ms step_avg:57.84ms
step:1707/2330 train_time:98729ms step_avg:57.84ms
step:1708/2330 train_time:98790ms step_avg:57.84ms
step:1709/2330 train_time:98847ms step_avg:57.84ms
step:1710/2330 train_time:98907ms step_avg:57.84ms
step:1711/2330 train_time:98964ms step_avg:57.84ms
step:1712/2330 train_time:99025ms step_avg:57.84ms
step:1713/2330 train_time:99082ms step_avg:57.84ms
step:1714/2330 train_time:99142ms step_avg:57.84ms
step:1715/2330 train_time:99200ms step_avg:57.84ms
step:1716/2330 train_time:99260ms step_avg:57.84ms
step:1717/2330 train_time:99317ms step_avg:57.84ms
step:1718/2330 train_time:99377ms step_avg:57.84ms
step:1719/2330 train_time:99435ms step_avg:57.84ms
step:1720/2330 train_time:99494ms step_avg:57.85ms
step:1721/2330 train_time:99551ms step_avg:57.84ms
step:1722/2330 train_time:99611ms step_avg:57.85ms
step:1723/2330 train_time:99668ms step_avg:57.85ms
step:1724/2330 train_time:99729ms step_avg:57.85ms
step:1725/2330 train_time:99785ms step_avg:57.85ms
step:1726/2330 train_time:99846ms step_avg:57.85ms
step:1727/2330 train_time:99903ms step_avg:57.85ms
step:1728/2330 train_time:99964ms step_avg:57.85ms
step:1729/2330 train_time:100021ms step_avg:57.85ms
step:1730/2330 train_time:100080ms step_avg:57.85ms
step:1731/2330 train_time:100138ms step_avg:57.85ms
step:1732/2330 train_time:100198ms step_avg:57.85ms
step:1733/2330 train_time:100254ms step_avg:57.85ms
step:1734/2330 train_time:100314ms step_avg:57.85ms
step:1735/2330 train_time:100370ms step_avg:57.85ms
step:1736/2330 train_time:100431ms step_avg:57.85ms
step:1737/2330 train_time:100488ms step_avg:57.85ms
step:1738/2330 train_time:100548ms step_avg:57.85ms
step:1739/2330 train_time:100604ms step_avg:57.85ms
step:1740/2330 train_time:100666ms step_avg:57.85ms
step:1741/2330 train_time:100723ms step_avg:57.85ms
step:1742/2330 train_time:100784ms step_avg:57.86ms
step:1743/2330 train_time:100841ms step_avg:57.85ms
step:1744/2330 train_time:100900ms step_avg:57.86ms
step:1745/2330 train_time:100958ms step_avg:57.86ms
step:1746/2330 train_time:101018ms step_avg:57.86ms
step:1747/2330 train_time:101076ms step_avg:57.86ms
step:1748/2330 train_time:101136ms step_avg:57.86ms
step:1749/2330 train_time:101195ms step_avg:57.86ms
step:1750/2330 train_time:101255ms step_avg:57.86ms
step:1750/2330 val_loss:4.0962 train_time:101336ms step_avg:57.91ms
step:1751/2330 train_time:101355ms step_avg:57.88ms
step:1752/2330 train_time:101375ms step_avg:57.86ms
step:1753/2330 train_time:101429ms step_avg:57.86ms
step:1754/2330 train_time:101494ms step_avg:57.86ms
step:1755/2330 train_time:101551ms step_avg:57.86ms
step:1756/2330 train_time:101613ms step_avg:57.87ms
step:1757/2330 train_time:101669ms step_avg:57.86ms
step:1758/2330 train_time:101729ms step_avg:57.87ms
step:1759/2330 train_time:101785ms step_avg:57.87ms
step:1760/2330 train_time:101844ms step_avg:57.87ms
step:1761/2330 train_time:101900ms step_avg:57.87ms
step:1762/2330 train_time:101960ms step_avg:57.87ms
step:1763/2330 train_time:102016ms step_avg:57.86ms
step:1764/2330 train_time:102076ms step_avg:57.87ms
step:1765/2330 train_time:102132ms step_avg:57.87ms
step:1766/2330 train_time:102192ms step_avg:57.87ms
step:1767/2330 train_time:102251ms step_avg:57.87ms
step:1768/2330 train_time:102312ms step_avg:57.87ms
step:1769/2330 train_time:102371ms step_avg:57.87ms
step:1770/2330 train_time:102434ms step_avg:57.87ms
step:1771/2330 train_time:102492ms step_avg:57.87ms
step:1772/2330 train_time:102552ms step_avg:57.87ms
step:1773/2330 train_time:102609ms step_avg:57.87ms
step:1774/2330 train_time:102670ms step_avg:57.87ms
step:1775/2330 train_time:102726ms step_avg:57.87ms
step:1776/2330 train_time:102786ms step_avg:57.88ms
step:1777/2330 train_time:102842ms step_avg:57.87ms
step:1778/2330 train_time:102902ms step_avg:57.88ms
step:1779/2330 train_time:102959ms step_avg:57.87ms
step:1780/2330 train_time:103018ms step_avg:57.88ms
step:1781/2330 train_time:103074ms step_avg:57.87ms
step:1782/2330 train_time:103134ms step_avg:57.88ms
step:1783/2330 train_time:103191ms step_avg:57.87ms
step:1784/2330 train_time:103252ms step_avg:57.88ms
step:1785/2330 train_time:103309ms step_avg:57.88ms
step:1786/2330 train_time:103371ms step_avg:57.88ms
step:1787/2330 train_time:103428ms step_avg:57.88ms
step:1788/2330 train_time:103491ms step_avg:57.88ms
step:1789/2330 train_time:103548ms step_avg:57.88ms
step:1790/2330 train_time:103608ms step_avg:57.88ms
step:1791/2330 train_time:103664ms step_avg:57.88ms
step:1792/2330 train_time:103725ms step_avg:57.88ms
step:1793/2330 train_time:103782ms step_avg:57.88ms
step:1794/2330 train_time:103842ms step_avg:57.88ms
step:1795/2330 train_time:103898ms step_avg:57.88ms
step:1796/2330 train_time:103958ms step_avg:57.88ms
step:1797/2330 train_time:104014ms step_avg:57.88ms
step:1798/2330 train_time:104074ms step_avg:57.88ms
step:1799/2330 train_time:104131ms step_avg:57.88ms
step:1800/2330 train_time:104191ms step_avg:57.88ms
step:1801/2330 train_time:104248ms step_avg:57.88ms
step:1802/2330 train_time:104310ms step_avg:57.89ms
step:1803/2330 train_time:104366ms step_avg:57.88ms
step:1804/2330 train_time:104427ms step_avg:57.89ms
step:1805/2330 train_time:104484ms step_avg:57.89ms
step:1806/2330 train_time:104545ms step_avg:57.89ms
step:1807/2330 train_time:104602ms step_avg:57.89ms
step:1808/2330 train_time:104662ms step_avg:57.89ms
step:1809/2330 train_time:104718ms step_avg:57.89ms
step:1810/2330 train_time:104780ms step_avg:57.89ms
step:1811/2330 train_time:104837ms step_avg:57.89ms
step:1812/2330 train_time:104896ms step_avg:57.89ms
step:1813/2330 train_time:104952ms step_avg:57.89ms
step:1814/2330 train_time:105012ms step_avg:57.89ms
step:1815/2330 train_time:105069ms step_avg:57.89ms
step:1816/2330 train_time:105128ms step_avg:57.89ms
step:1817/2330 train_time:105185ms step_avg:57.89ms
step:1818/2330 train_time:105245ms step_avg:57.89ms
step:1819/2330 train_time:105303ms step_avg:57.89ms
step:1820/2330 train_time:105363ms step_avg:57.89ms
step:1821/2330 train_time:105419ms step_avg:57.89ms
step:1822/2330 train_time:105481ms step_avg:57.89ms
step:1823/2330 train_time:105538ms step_avg:57.89ms
step:1824/2330 train_time:105598ms step_avg:57.89ms
step:1825/2330 train_time:105656ms step_avg:57.89ms
step:1826/2330 train_time:105715ms step_avg:57.89ms
step:1827/2330 train_time:105772ms step_avg:57.89ms
step:1828/2330 train_time:105833ms step_avg:57.90ms
step:1829/2330 train_time:105890ms step_avg:57.89ms
step:1830/2330 train_time:105949ms step_avg:57.90ms
step:1831/2330 train_time:106006ms step_avg:57.89ms
step:1832/2330 train_time:106066ms step_avg:57.90ms
step:1833/2330 train_time:106122ms step_avg:57.90ms
step:1834/2330 train_time:106182ms step_avg:57.90ms
step:1835/2330 train_time:106239ms step_avg:57.90ms
step:1836/2330 train_time:106301ms step_avg:57.90ms
step:1837/2330 train_time:106358ms step_avg:57.90ms
step:1838/2330 train_time:106418ms step_avg:57.90ms
step:1839/2330 train_time:106476ms step_avg:57.90ms
step:1840/2330 train_time:106536ms step_avg:57.90ms
step:1841/2330 train_time:106594ms step_avg:57.90ms
step:1842/2330 train_time:106654ms step_avg:57.90ms
step:1843/2330 train_time:106712ms step_avg:57.90ms
step:1844/2330 train_time:106772ms step_avg:57.90ms
step:1845/2330 train_time:106829ms step_avg:57.90ms
step:1846/2330 train_time:106889ms step_avg:57.90ms
step:1847/2330 train_time:106946ms step_avg:57.90ms
step:1848/2330 train_time:107006ms step_avg:57.90ms
step:1849/2330 train_time:107062ms step_avg:57.90ms
step:1850/2330 train_time:107122ms step_avg:57.90ms
step:1851/2330 train_time:107179ms step_avg:57.90ms
step:1852/2330 train_time:107240ms step_avg:57.90ms
step:1853/2330 train_time:107297ms step_avg:57.90ms
step:1854/2330 train_time:107357ms step_avg:57.91ms
step:1855/2330 train_time:107414ms step_avg:57.91ms
step:1856/2330 train_time:107475ms step_avg:57.91ms
step:1857/2330 train_time:107532ms step_avg:57.91ms
step:1858/2330 train_time:107593ms step_avg:57.91ms
step:1859/2330 train_time:107650ms step_avg:57.91ms
step:1860/2330 train_time:107710ms step_avg:57.91ms
step:1861/2330 train_time:107767ms step_avg:57.91ms
step:1862/2330 train_time:107827ms step_avg:57.91ms
step:1863/2330 train_time:107885ms step_avg:57.91ms
step:1864/2330 train_time:107944ms step_avg:57.91ms
step:1865/2330 train_time:108001ms step_avg:57.91ms
step:1866/2330 train_time:108061ms step_avg:57.91ms
step:1867/2330 train_time:108117ms step_avg:57.91ms
step:1868/2330 train_time:108177ms step_avg:57.91ms
step:1869/2330 train_time:108234ms step_avg:57.91ms
step:1870/2330 train_time:108294ms step_avg:57.91ms
step:1871/2330 train_time:108351ms step_avg:57.91ms
step:1872/2330 train_time:108411ms step_avg:57.91ms
step:1873/2330 train_time:108467ms step_avg:57.91ms
step:1874/2330 train_time:108529ms step_avg:57.91ms
step:1875/2330 train_time:108585ms step_avg:57.91ms
step:1876/2330 train_time:108646ms step_avg:57.91ms
step:1877/2330 train_time:108702ms step_avg:57.91ms
step:1878/2330 train_time:108763ms step_avg:57.91ms
step:1879/2330 train_time:108820ms step_avg:57.91ms
step:1880/2330 train_time:108880ms step_avg:57.92ms
step:1881/2330 train_time:108937ms step_avg:57.91ms
step:1882/2330 train_time:108998ms step_avg:57.92ms
step:1883/2330 train_time:109054ms step_avg:57.92ms
step:1884/2330 train_time:109114ms step_avg:57.92ms
step:1885/2330 train_time:109171ms step_avg:57.92ms
step:1886/2330 train_time:109232ms step_avg:57.92ms
step:1887/2330 train_time:109289ms step_avg:57.92ms
step:1888/2330 train_time:109349ms step_avg:57.92ms
step:1889/2330 train_time:109406ms step_avg:57.92ms
step:1890/2330 train_time:109465ms step_avg:57.92ms
step:1891/2330 train_time:109523ms step_avg:57.92ms
step:1892/2330 train_time:109584ms step_avg:57.92ms
step:1893/2330 train_time:109641ms step_avg:57.92ms
step:1894/2330 train_time:109701ms step_avg:57.92ms
step:1895/2330 train_time:109758ms step_avg:57.92ms
step:1896/2330 train_time:109818ms step_avg:57.92ms
step:1897/2330 train_time:109876ms step_avg:57.92ms
step:1898/2330 train_time:109936ms step_avg:57.92ms
step:1899/2330 train_time:109993ms step_avg:57.92ms
step:1900/2330 train_time:110053ms step_avg:57.92ms
step:1901/2330 train_time:110110ms step_avg:57.92ms
step:1902/2330 train_time:110170ms step_avg:57.92ms
step:1903/2330 train_time:110227ms step_avg:57.92ms
step:1904/2330 train_time:110287ms step_avg:57.92ms
step:1905/2330 train_time:110345ms step_avg:57.92ms
step:1906/2330 train_time:110405ms step_avg:57.92ms
step:1907/2330 train_time:110461ms step_avg:57.92ms
step:1908/2330 train_time:110522ms step_avg:57.93ms
step:1909/2330 train_time:110579ms step_avg:57.93ms
step:1910/2330 train_time:110640ms step_avg:57.93ms
step:1911/2330 train_time:110697ms step_avg:57.93ms
step:1912/2330 train_time:110757ms step_avg:57.93ms
step:1913/2330 train_time:110814ms step_avg:57.93ms
step:1914/2330 train_time:110874ms step_avg:57.93ms
step:1915/2330 train_time:110931ms step_avg:57.93ms
step:1916/2330 train_time:110991ms step_avg:57.93ms
step:1917/2330 train_time:111049ms step_avg:57.93ms
step:1918/2330 train_time:111109ms step_avg:57.93ms
step:1919/2330 train_time:111166ms step_avg:57.93ms
step:1920/2330 train_time:111226ms step_avg:57.93ms
step:1921/2330 train_time:111282ms step_avg:57.93ms
step:1922/2330 train_time:111343ms step_avg:57.93ms
step:1923/2330 train_time:111399ms step_avg:57.93ms
step:1924/2330 train_time:111460ms step_avg:57.93ms
step:1925/2330 train_time:111517ms step_avg:57.93ms
step:1926/2330 train_time:111577ms step_avg:57.93ms
step:1927/2330 train_time:111634ms step_avg:57.93ms
step:1928/2330 train_time:111695ms step_avg:57.93ms
step:1929/2330 train_time:111752ms step_avg:57.93ms
step:1930/2330 train_time:111813ms step_avg:57.93ms
step:1931/2330 train_time:111871ms step_avg:57.93ms
step:1932/2330 train_time:111931ms step_avg:57.94ms
step:1933/2330 train_time:111988ms step_avg:57.93ms
step:1934/2330 train_time:112048ms step_avg:57.94ms
step:1935/2330 train_time:112105ms step_avg:57.94ms
step:1936/2330 train_time:112164ms step_avg:57.94ms
step:1937/2330 train_time:112222ms step_avg:57.94ms
step:1938/2330 train_time:112281ms step_avg:57.94ms
step:1939/2330 train_time:112338ms step_avg:57.94ms
step:1940/2330 train_time:112399ms step_avg:57.94ms
step:1941/2330 train_time:112456ms step_avg:57.94ms
step:1942/2330 train_time:112516ms step_avg:57.94ms
step:1943/2330 train_time:112573ms step_avg:57.94ms
step:1944/2330 train_time:112634ms step_avg:57.94ms
step:1945/2330 train_time:112690ms step_avg:57.94ms
step:1946/2330 train_time:112752ms step_avg:57.94ms
step:1947/2330 train_time:112809ms step_avg:57.94ms
step:1948/2330 train_time:112868ms step_avg:57.94ms
step:1949/2330 train_time:112925ms step_avg:57.94ms
step:1950/2330 train_time:112985ms step_avg:57.94ms
step:1951/2330 train_time:113041ms step_avg:57.94ms
step:1952/2330 train_time:113102ms step_avg:57.94ms
step:1953/2330 train_time:113158ms step_avg:57.94ms
step:1954/2330 train_time:113219ms step_avg:57.94ms
step:1955/2330 train_time:113276ms step_avg:57.94ms
step:1956/2330 train_time:113336ms step_avg:57.94ms
step:1957/2330 train_time:113394ms step_avg:57.94ms
step:1958/2330 train_time:113454ms step_avg:57.94ms
step:1959/2330 train_time:113511ms step_avg:57.94ms
step:1960/2330 train_time:113572ms step_avg:57.94ms
step:1961/2330 train_time:113629ms step_avg:57.94ms
step:1962/2330 train_time:113688ms step_avg:57.94ms
step:1963/2330 train_time:113745ms step_avg:57.94ms
step:1964/2330 train_time:113805ms step_avg:57.95ms
step:1965/2330 train_time:113862ms step_avg:57.94ms
step:1966/2330 train_time:113923ms step_avg:57.95ms
step:1967/2330 train_time:113980ms step_avg:57.95ms
step:1968/2330 train_time:114040ms step_avg:57.95ms
step:1969/2330 train_time:114097ms step_avg:57.95ms
step:1970/2330 train_time:114157ms step_avg:57.95ms
step:1971/2330 train_time:114214ms step_avg:57.95ms
step:1972/2330 train_time:114274ms step_avg:57.95ms
step:1973/2330 train_time:114331ms step_avg:57.95ms
step:1974/2330 train_time:114392ms step_avg:57.95ms
step:1975/2330 train_time:114450ms step_avg:57.95ms
step:1976/2330 train_time:114510ms step_avg:57.95ms
step:1977/2330 train_time:114567ms step_avg:57.95ms
step:1978/2330 train_time:114626ms step_avg:57.95ms
step:1979/2330 train_time:114683ms step_avg:57.95ms
step:1980/2330 train_time:114743ms step_avg:57.95ms
step:1981/2330 train_time:114800ms step_avg:57.95ms
step:1982/2330 train_time:114861ms step_avg:57.95ms
step:1983/2330 train_time:114918ms step_avg:57.95ms
step:1984/2330 train_time:114978ms step_avg:57.95ms
step:1985/2330 train_time:115036ms step_avg:57.95ms
step:1986/2330 train_time:115096ms step_avg:57.95ms
step:1987/2330 train_time:115153ms step_avg:57.95ms
step:1988/2330 train_time:115212ms step_avg:57.95ms
step:1989/2330 train_time:115269ms step_avg:57.95ms
step:1990/2330 train_time:115330ms step_avg:57.95ms
step:1991/2330 train_time:115387ms step_avg:57.95ms
step:1992/2330 train_time:115447ms step_avg:57.96ms
step:1993/2330 train_time:115504ms step_avg:57.96ms
step:1994/2330 train_time:115564ms step_avg:57.96ms
step:1995/2330 train_time:115622ms step_avg:57.96ms
step:1996/2330 train_time:115682ms step_avg:57.96ms
step:1997/2330 train_time:115738ms step_avg:57.96ms
step:1998/2330 train_time:115799ms step_avg:57.96ms
step:1999/2330 train_time:115855ms step_avg:57.96ms
step:2000/2330 train_time:115917ms step_avg:57.96ms
step:2000/2330 val_loss:4.0276 train_time:115998ms step_avg:58.00ms
step:2001/2330 train_time:116018ms step_avg:57.98ms
step:2002/2330 train_time:116038ms step_avg:57.96ms
step:2003/2330 train_time:116094ms step_avg:57.96ms
step:2004/2330 train_time:116158ms step_avg:57.96ms
step:2005/2330 train_time:116215ms step_avg:57.96ms
step:2006/2330 train_time:116277ms step_avg:57.96ms
step:2007/2330 train_time:116333ms step_avg:57.96ms
step:2008/2330 train_time:116393ms step_avg:57.96ms
step:2009/2330 train_time:116449ms step_avg:57.96ms
step:2010/2330 train_time:116509ms step_avg:57.96ms
step:2011/2330 train_time:116566ms step_avg:57.96ms
step:2012/2330 train_time:116625ms step_avg:57.96ms
step:2013/2330 train_time:116681ms step_avg:57.96ms
step:2014/2330 train_time:116741ms step_avg:57.96ms
step:2015/2330 train_time:116797ms step_avg:57.96ms
step:2016/2330 train_time:116856ms step_avg:57.96ms
step:2017/2330 train_time:116913ms step_avg:57.96ms
step:2018/2330 train_time:116974ms step_avg:57.97ms
step:2019/2330 train_time:117031ms step_avg:57.96ms
step:2020/2330 train_time:117093ms step_avg:57.97ms
step:2021/2330 train_time:117151ms step_avg:57.97ms
step:2022/2330 train_time:117213ms step_avg:57.97ms
step:2023/2330 train_time:117270ms step_avg:57.97ms
step:2024/2330 train_time:117331ms step_avg:57.97ms
step:2025/2330 train_time:117388ms step_avg:57.97ms
step:2026/2330 train_time:117448ms step_avg:57.97ms
step:2027/2330 train_time:117505ms step_avg:57.97ms
step:2028/2330 train_time:117564ms step_avg:57.97ms
step:2029/2330 train_time:117620ms step_avg:57.97ms
step:2030/2330 train_time:117681ms step_avg:57.97ms
step:2031/2330 train_time:117738ms step_avg:57.97ms
step:2032/2330 train_time:117797ms step_avg:57.97ms
step:2033/2330 train_time:117853ms step_avg:57.97ms
step:2034/2330 train_time:117913ms step_avg:57.97ms
step:2035/2330 train_time:117969ms step_avg:57.97ms
step:2036/2330 train_time:118031ms step_avg:57.97ms
step:2037/2330 train_time:118089ms step_avg:57.97ms
step:2038/2330 train_time:118149ms step_avg:57.97ms
step:2039/2330 train_time:118206ms step_avg:57.97ms
step:2040/2330 train_time:118267ms step_avg:57.97ms
step:2041/2330 train_time:118325ms step_avg:57.97ms
step:2042/2330 train_time:118385ms step_avg:57.98ms
step:2043/2330 train_time:118443ms step_avg:57.97ms
step:2044/2330 train_time:118502ms step_avg:57.98ms
step:2045/2330 train_time:118559ms step_avg:57.97ms
step:2046/2330 train_time:118618ms step_avg:57.98ms
step:2047/2330 train_time:118676ms step_avg:57.98ms
step:2048/2330 train_time:118735ms step_avg:57.98ms
step:2049/2330 train_time:118791ms step_avg:57.98ms
step:2050/2330 train_time:118851ms step_avg:57.98ms
step:2051/2330 train_time:118908ms step_avg:57.98ms
step:2052/2330 train_time:118969ms step_avg:57.98ms
step:2053/2330 train_time:119026ms step_avg:57.98ms
step:2054/2330 train_time:119087ms step_avg:57.98ms
step:2055/2330 train_time:119144ms step_avg:57.98ms
step:2056/2330 train_time:119206ms step_avg:57.98ms
step:2057/2330 train_time:119263ms step_avg:57.98ms
step:2058/2330 train_time:119323ms step_avg:57.98ms
step:2059/2330 train_time:119381ms step_avg:57.98ms
step:2060/2330 train_time:119441ms step_avg:57.98ms
step:2061/2330 train_time:119498ms step_avg:57.98ms
step:2062/2330 train_time:119558ms step_avg:57.98ms
step:2063/2330 train_time:119615ms step_avg:57.98ms
step:2064/2330 train_time:119675ms step_avg:57.98ms
step:2065/2330 train_time:119732ms step_avg:57.98ms
step:2066/2330 train_time:119791ms step_avg:57.98ms
step:2067/2330 train_time:119848ms step_avg:57.98ms
step:2068/2330 train_time:119908ms step_avg:57.98ms
step:2069/2330 train_time:119965ms step_avg:57.98ms
step:2070/2330 train_time:120027ms step_avg:57.98ms
step:2071/2330 train_time:120084ms step_avg:57.98ms
step:2072/2330 train_time:120145ms step_avg:57.98ms
step:2073/2330 train_time:120202ms step_avg:57.98ms
step:2074/2330 train_time:120263ms step_avg:57.99ms
step:2075/2330 train_time:120320ms step_avg:57.99ms
step:2076/2330 train_time:120380ms step_avg:57.99ms
step:2077/2330 train_time:120437ms step_avg:57.99ms
step:2078/2330 train_time:120496ms step_avg:57.99ms
step:2079/2330 train_time:120553ms step_avg:57.99ms
step:2080/2330 train_time:120614ms step_avg:57.99ms
step:2081/2330 train_time:120671ms step_avg:57.99ms
step:2082/2330 train_time:120731ms step_avg:57.99ms
step:2083/2330 train_time:120788ms step_avg:57.99ms
step:2084/2330 train_time:120847ms step_avg:57.99ms
step:2085/2330 train_time:120904ms step_avg:57.99ms
step:2086/2330 train_time:120966ms step_avg:57.99ms
step:2087/2330 train_time:121022ms step_avg:57.99ms
step:2088/2330 train_time:121083ms step_avg:57.99ms
step:2089/2330 train_time:121139ms step_avg:57.99ms
step:2090/2330 train_time:121200ms step_avg:57.99ms
step:2091/2330 train_time:121257ms step_avg:57.99ms
step:2092/2330 train_time:121317ms step_avg:57.99ms
step:2093/2330 train_time:121374ms step_avg:57.99ms
step:2094/2330 train_time:121434ms step_avg:57.99ms
step:2095/2330 train_time:121491ms step_avg:57.99ms
step:2096/2330 train_time:121551ms step_avg:57.99ms
step:2097/2330 train_time:121608ms step_avg:57.99ms
step:2098/2330 train_time:121668ms step_avg:57.99ms
step:2099/2330 train_time:121726ms step_avg:57.99ms
step:2100/2330 train_time:121786ms step_avg:57.99ms
step:2101/2330 train_time:121843ms step_avg:57.99ms
step:2102/2330 train_time:121903ms step_avg:57.99ms
step:2103/2330 train_time:121960ms step_avg:57.99ms
step:2104/2330 train_time:122020ms step_avg:57.99ms
step:2105/2330 train_time:122077ms step_avg:57.99ms
step:2106/2330 train_time:122136ms step_avg:57.99ms
step:2107/2330 train_time:122193ms step_avg:57.99ms
step:2108/2330 train_time:122254ms step_avg:58.00ms
step:2109/2330 train_time:122312ms step_avg:58.00ms
step:2110/2330 train_time:122372ms step_avg:58.00ms
step:2111/2330 train_time:122429ms step_avg:58.00ms
step:2112/2330 train_time:122489ms step_avg:58.00ms
step:2113/2330 train_time:122547ms step_avg:58.00ms
step:2114/2330 train_time:122607ms step_avg:58.00ms
step:2115/2330 train_time:122664ms step_avg:58.00ms
step:2116/2330 train_time:122724ms step_avg:58.00ms
step:2117/2330 train_time:122781ms step_avg:58.00ms
step:2118/2330 train_time:122841ms step_avg:58.00ms
step:2119/2330 train_time:122897ms step_avg:58.00ms
step:2120/2330 train_time:122957ms step_avg:58.00ms
step:2121/2330 train_time:123014ms step_avg:58.00ms
step:2122/2330 train_time:123074ms step_avg:58.00ms
step:2123/2330 train_time:123131ms step_avg:58.00ms
step:2124/2330 train_time:123191ms step_avg:58.00ms
step:2125/2330 train_time:123249ms step_avg:58.00ms
step:2126/2330 train_time:123309ms step_avg:58.00ms
step:2127/2330 train_time:123365ms step_avg:58.00ms
step:2128/2330 train_time:123427ms step_avg:58.00ms
step:2129/2330 train_time:123484ms step_avg:58.00ms
step:2130/2330 train_time:123545ms step_avg:58.00ms
step:2131/2330 train_time:123601ms step_avg:58.00ms
step:2132/2330 train_time:123663ms step_avg:58.00ms
step:2133/2330 train_time:123721ms step_avg:58.00ms
step:2134/2330 train_time:123781ms step_avg:58.00ms
step:2135/2330 train_time:123838ms step_avg:58.00ms
step:2136/2330 train_time:123897ms step_avg:58.00ms
step:2137/2330 train_time:123954ms step_avg:58.00ms
step:2138/2330 train_time:124013ms step_avg:58.00ms
step:2139/2330 train_time:124070ms step_avg:58.00ms
step:2140/2330 train_time:124130ms step_avg:58.00ms
step:2141/2330 train_time:124188ms step_avg:58.00ms
step:2142/2330 train_time:124249ms step_avg:58.01ms
step:2143/2330 train_time:124306ms step_avg:58.01ms
step:2144/2330 train_time:124365ms step_avg:58.01ms
step:2145/2330 train_time:124423ms step_avg:58.01ms
step:2146/2330 train_time:124484ms step_avg:58.01ms
step:2147/2330 train_time:124541ms step_avg:58.01ms
step:2148/2330 train_time:124600ms step_avg:58.01ms
step:2149/2330 train_time:124658ms step_avg:58.01ms
step:2150/2330 train_time:124717ms step_avg:58.01ms
step:2151/2330 train_time:124775ms step_avg:58.01ms
step:2152/2330 train_time:124834ms step_avg:58.01ms
step:2153/2330 train_time:124891ms step_avg:58.01ms
step:2154/2330 train_time:124951ms step_avg:58.01ms
step:2155/2330 train_time:125008ms step_avg:58.01ms
step:2156/2330 train_time:125069ms step_avg:58.01ms
step:2157/2330 train_time:125125ms step_avg:58.01ms
step:2158/2330 train_time:125185ms step_avg:58.01ms
step:2159/2330 train_time:125243ms step_avg:58.01ms
step:2160/2330 train_time:125303ms step_avg:58.01ms
step:2161/2330 train_time:125361ms step_avg:58.01ms
step:2162/2330 train_time:125421ms step_avg:58.01ms
step:2163/2330 train_time:125479ms step_avg:58.01ms
step:2164/2330 train_time:125538ms step_avg:58.01ms
step:2165/2330 train_time:125595ms step_avg:58.01ms
step:2166/2330 train_time:125655ms step_avg:58.01ms
step:2167/2330 train_time:125712ms step_avg:58.01ms
step:2168/2330 train_time:125773ms step_avg:58.01ms
step:2169/2330 train_time:125830ms step_avg:58.01ms
step:2170/2330 train_time:125890ms step_avg:58.01ms
step:2171/2330 train_time:125947ms step_avg:58.01ms
step:2172/2330 train_time:126007ms step_avg:58.01ms
step:2173/2330 train_time:126063ms step_avg:58.01ms
step:2174/2330 train_time:126125ms step_avg:58.02ms
step:2175/2330 train_time:126183ms step_avg:58.02ms
step:2176/2330 train_time:126243ms step_avg:58.02ms
step:2177/2330 train_time:126299ms step_avg:58.02ms
step:2178/2330 train_time:126359ms step_avg:58.02ms
step:2179/2330 train_time:126416ms step_avg:58.02ms
step:2180/2330 train_time:126477ms step_avg:58.02ms
step:2181/2330 train_time:126534ms step_avg:58.02ms
step:2182/2330 train_time:126594ms step_avg:58.02ms
step:2183/2330 train_time:126650ms step_avg:58.02ms
step:2184/2330 train_time:126711ms step_avg:58.02ms
step:2185/2330 train_time:126769ms step_avg:58.02ms
step:2186/2330 train_time:126830ms step_avg:58.02ms
step:2187/2330 train_time:126888ms step_avg:58.02ms
step:2188/2330 train_time:126948ms step_avg:58.02ms
step:2189/2330 train_time:127004ms step_avg:58.02ms
step:2190/2330 train_time:127065ms step_avg:58.02ms
step:2191/2330 train_time:127122ms step_avg:58.02ms
step:2192/2330 train_time:127183ms step_avg:58.02ms
step:2193/2330 train_time:127241ms step_avg:58.02ms
step:2194/2330 train_time:127301ms step_avg:58.02ms
step:2195/2330 train_time:127358ms step_avg:58.02ms
step:2196/2330 train_time:127418ms step_avg:58.02ms
step:2197/2330 train_time:127475ms step_avg:58.02ms
step:2198/2330 train_time:127534ms step_avg:58.02ms
step:2199/2330 train_time:127591ms step_avg:58.02ms
step:2200/2330 train_time:127652ms step_avg:58.02ms
step:2201/2330 train_time:127708ms step_avg:58.02ms
step:2202/2330 train_time:127769ms step_avg:58.02ms
step:2203/2330 train_time:127826ms step_avg:58.02ms
step:2204/2330 train_time:127886ms step_avg:58.02ms
step:2205/2330 train_time:127943ms step_avg:58.02ms
step:2206/2330 train_time:128003ms step_avg:58.03ms
step:2207/2330 train_time:128060ms step_avg:58.02ms
step:2208/2330 train_time:128121ms step_avg:58.03ms
step:2209/2330 train_time:128177ms step_avg:58.03ms
step:2210/2330 train_time:128238ms step_avg:58.03ms
step:2211/2330 train_time:128295ms step_avg:58.03ms
step:2212/2330 train_time:128355ms step_avg:58.03ms
step:2213/2330 train_time:128412ms step_avg:58.03ms
step:2214/2330 train_time:128473ms step_avg:58.03ms
step:2215/2330 train_time:128531ms step_avg:58.03ms
step:2216/2330 train_time:128591ms step_avg:58.03ms
step:2217/2330 train_time:128650ms step_avg:58.03ms
step:2218/2330 train_time:128708ms step_avg:58.03ms
step:2219/2330 train_time:128764ms step_avg:58.03ms
step:2220/2330 train_time:128825ms step_avg:58.03ms
step:2221/2330 train_time:128882ms step_avg:58.03ms
step:2222/2330 train_time:128942ms step_avg:58.03ms
step:2223/2330 train_time:128998ms step_avg:58.03ms
step:2224/2330 train_time:129058ms step_avg:58.03ms
step:2225/2330 train_time:129115ms step_avg:58.03ms
step:2226/2330 train_time:129175ms step_avg:58.03ms
step:2227/2330 train_time:129232ms step_avg:58.03ms
step:2228/2330 train_time:129292ms step_avg:58.03ms
step:2229/2330 train_time:129349ms step_avg:58.03ms
step:2230/2330 train_time:129409ms step_avg:58.03ms
step:2231/2330 train_time:129466ms step_avg:58.03ms
step:2232/2330 train_time:129527ms step_avg:58.03ms
step:2233/2330 train_time:129584ms step_avg:58.03ms
step:2234/2330 train_time:129645ms step_avg:58.03ms
step:2235/2330 train_time:129702ms step_avg:58.03ms
step:2236/2330 train_time:129761ms step_avg:58.03ms
step:2237/2330 train_time:129818ms step_avg:58.03ms
step:2238/2330 train_time:129878ms step_avg:58.03ms
step:2239/2330 train_time:129935ms step_avg:58.03ms
step:2240/2330 train_time:129994ms step_avg:58.03ms
step:2241/2330 train_time:130051ms step_avg:58.03ms
step:2242/2330 train_time:130111ms step_avg:58.03ms
step:2243/2330 train_time:130169ms step_avg:58.03ms
step:2244/2330 train_time:130229ms step_avg:58.03ms
step:2245/2330 train_time:130286ms step_avg:58.03ms
step:2246/2330 train_time:130347ms step_avg:58.04ms
step:2247/2330 train_time:130404ms step_avg:58.03ms
step:2248/2330 train_time:130464ms step_avg:58.04ms
step:2249/2330 train_time:130521ms step_avg:58.04ms
step:2250/2330 train_time:130582ms step_avg:58.04ms
step:2250/2330 val_loss:3.9779 train_time:130662ms step_avg:58.07ms
step:2251/2330 train_time:130682ms step_avg:58.05ms
step:2252/2330 train_time:130702ms step_avg:58.04ms
step:2253/2330 train_time:130763ms step_avg:58.04ms
step:2254/2330 train_time:130825ms step_avg:58.04ms
step:2255/2330 train_time:130883ms step_avg:58.04ms
step:2256/2330 train_time:130944ms step_avg:58.04ms
step:2257/2330 train_time:131000ms step_avg:58.04ms
step:2258/2330 train_time:131061ms step_avg:58.04ms
step:2259/2330 train_time:131117ms step_avg:58.04ms
step:2260/2330 train_time:131177ms step_avg:58.04ms
step:2261/2330 train_time:131234ms step_avg:58.04ms
step:2262/2330 train_time:131293ms step_avg:58.04ms
step:2263/2330 train_time:131350ms step_avg:58.04ms
step:2264/2330 train_time:131409ms step_avg:58.04ms
step:2265/2330 train_time:131466ms step_avg:58.04ms
step:2266/2330 train_time:131525ms step_avg:58.04ms
step:2267/2330 train_time:131582ms step_avg:58.04ms
step:2268/2330 train_time:131642ms step_avg:58.04ms
step:2269/2330 train_time:131701ms step_avg:58.04ms
step:2270/2330 train_time:131762ms step_avg:58.04ms
step:2271/2330 train_time:131820ms step_avg:58.04ms
step:2272/2330 train_time:131881ms step_avg:58.05ms
step:2273/2330 train_time:131938ms step_avg:58.05ms
step:2274/2330 train_time:132000ms step_avg:58.05ms
step:2275/2330 train_time:132056ms step_avg:58.05ms
step:2276/2330 train_time:132116ms step_avg:58.05ms
step:2277/2330 train_time:132172ms step_avg:58.05ms
step:2278/2330 train_time:132232ms step_avg:58.05ms
step:2279/2330 train_time:132289ms step_avg:58.05ms
step:2280/2330 train_time:132349ms step_avg:58.05ms
step:2281/2330 train_time:132405ms step_avg:58.05ms
step:2282/2330 train_time:132465ms step_avg:58.05ms
step:2283/2330 train_time:132521ms step_avg:58.05ms
step:2284/2330 train_time:132581ms step_avg:58.05ms
step:2285/2330 train_time:132638ms step_avg:58.05ms
step:2286/2330 train_time:132699ms step_avg:58.05ms
step:2287/2330 train_time:132757ms step_avg:58.05ms
step:2288/2330 train_time:132817ms step_avg:58.05ms
step:2289/2330 train_time:132876ms step_avg:58.05ms
step:2290/2330 train_time:132936ms step_avg:58.05ms
step:2291/2330 train_time:132994ms step_avg:58.05ms
step:2292/2330 train_time:133054ms step_avg:58.05ms
step:2293/2330 train_time:133111ms step_avg:58.05ms
step:2294/2330 train_time:133171ms step_avg:58.05ms
step:2295/2330 train_time:133228ms step_avg:58.05ms
step:2296/2330 train_time:133287ms step_avg:58.05ms
step:2297/2330 train_time:133345ms step_avg:58.05ms
step:2298/2330 train_time:133404ms step_avg:58.05ms
step:2299/2330 train_time:133461ms step_avg:58.05ms
step:2300/2330 train_time:133521ms step_avg:58.05ms
step:2301/2330 train_time:133578ms step_avg:58.05ms
step:2302/2330 train_time:133637ms step_avg:58.05ms
step:2303/2330 train_time:133695ms step_avg:58.05ms
step:2304/2330 train_time:133757ms step_avg:58.05ms
step:2305/2330 train_time:133814ms step_avg:58.05ms
step:2306/2330 train_time:133875ms step_avg:58.06ms
step:2307/2330 train_time:133933ms step_avg:58.05ms
step:2308/2330 train_time:133994ms step_avg:58.06ms
step:2309/2330 train_time:134051ms step_avg:58.06ms
step:2310/2330 train_time:134111ms step_avg:58.06ms
step:2311/2330 train_time:134169ms step_avg:58.06ms
step:2312/2330 train_time:134229ms step_avg:58.06ms
step:2313/2330 train_time:134286ms step_avg:58.06ms
step:2314/2330 train_time:134345ms step_avg:58.06ms
step:2315/2330 train_time:134401ms step_avg:58.06ms
step:2316/2330 train_time:134460ms step_avg:58.06ms
step:2317/2330 train_time:134517ms step_avg:58.06ms
step:2318/2330 train_time:134578ms step_avg:58.06ms
step:2319/2330 train_time:134635ms step_avg:58.06ms
step:2320/2330 train_time:134695ms step_avg:58.06ms
step:2321/2330 train_time:134753ms step_avg:58.06ms
step:2322/2330 train_time:134812ms step_avg:58.06ms
step:2323/2330 train_time:134870ms step_avg:58.06ms
step:2324/2330 train_time:134930ms step_avg:58.06ms
step:2325/2330 train_time:134987ms step_avg:58.06ms
step:2326/2330 train_time:135047ms step_avg:58.06ms
step:2327/2330 train_time:135104ms step_avg:58.06ms
step:2328/2330 train_time:135164ms step_avg:58.06ms
step:2329/2330 train_time:135221ms step_avg:58.06ms
step:2330/2330 train_time:135280ms step_avg:58.06ms
step:2330/2330 val_loss:3.9631 train_time:135361ms step_avg:58.09ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
