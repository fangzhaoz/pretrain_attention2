import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--adamw_lr', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_lr_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_lr_tuning/{args2.adamw_lr}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=float(args2.adamw_lr),  betas=(0.85, 0.85), eps=1e-10, weight_decay=0.0, fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_lr_tuning/{args2.adamw_lr}", exist_ok=True)
            torch.save(log, f"logs_adamw_small_lr_tuning/{args2.adamw_lr}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 02:39:14 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   27C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   25C    P0             116W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   24C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   27C    P0             113W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   25C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   26C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.06ms
step:1/2330 train_time:91ms step_avg:90.70ms
step:2/2330 train_time:194ms step_avg:96.75ms
step:3/2330 train_time:212ms step_avg:70.83ms
step:4/2330 train_time:232ms step_avg:57.97ms
step:5/2330 train_time:285ms step_avg:56.93ms
step:6/2330 train_time:342ms step_avg:56.99ms
step:7/2330 train_time:396ms step_avg:56.52ms
step:8/2330 train_time:453ms step_avg:56.63ms
step:9/2330 train_time:507ms step_avg:56.32ms
step:10/2330 train_time:564ms step_avg:56.41ms
step:11/2330 train_time:618ms step_avg:56.18ms
step:12/2330 train_time:675ms step_avg:56.25ms
step:13/2330 train_time:729ms step_avg:56.06ms
step:14/2330 train_time:786ms step_avg:56.13ms
step:15/2330 train_time:840ms step_avg:55.97ms
step:16/2330 train_time:897ms step_avg:56.06ms
step:17/2330 train_time:951ms step_avg:55.93ms
step:18/2330 train_time:1008ms step_avg:56.00ms
step:19/2330 train_time:1062ms step_avg:55.89ms
step:20/2330 train_time:1119ms step_avg:55.97ms
step:21/2330 train_time:1174ms step_avg:55.92ms
step:22/2330 train_time:1232ms step_avg:56.02ms
step:23/2330 train_time:1287ms step_avg:55.96ms
step:24/2330 train_time:1344ms step_avg:56.02ms
step:25/2330 train_time:1399ms step_avg:55.97ms
step:26/2330 train_time:1457ms step_avg:56.03ms
step:27/2330 train_time:1511ms step_avg:55.97ms
step:28/2330 train_time:1569ms step_avg:56.02ms
step:29/2330 train_time:1623ms step_avg:55.96ms
step:30/2330 train_time:1680ms step_avg:55.99ms
step:31/2330 train_time:1734ms step_avg:55.95ms
step:32/2330 train_time:1792ms step_avg:55.99ms
step:33/2330 train_time:1846ms step_avg:55.94ms
step:34/2330 train_time:1904ms step_avg:55.99ms
step:35/2330 train_time:1958ms step_avg:55.94ms
step:36/2330 train_time:2015ms step_avg:55.97ms
step:37/2330 train_time:2070ms step_avg:55.94ms
step:38/2330 train_time:2127ms step_avg:55.96ms
step:39/2330 train_time:2181ms step_avg:55.93ms
step:40/2330 train_time:2239ms step_avg:55.97ms
step:41/2330 train_time:2293ms step_avg:55.93ms
step:42/2330 train_time:2351ms step_avg:55.97ms
step:43/2330 train_time:2406ms step_avg:55.95ms
step:44/2330 train_time:2463ms step_avg:55.98ms
step:45/2330 train_time:2517ms step_avg:55.94ms
step:46/2330 train_time:2575ms step_avg:55.98ms
step:47/2330 train_time:2630ms step_avg:55.96ms
step:48/2330 train_time:2687ms step_avg:55.99ms
step:49/2330 train_time:2742ms step_avg:55.97ms
step:50/2330 train_time:2800ms step_avg:56.00ms
step:51/2330 train_time:2855ms step_avg:55.98ms
step:52/2330 train_time:2913ms step_avg:56.01ms
step:53/2330 train_time:2967ms step_avg:55.98ms
step:54/2330 train_time:3024ms step_avg:56.01ms
step:55/2330 train_time:3079ms step_avg:55.98ms
step:56/2330 train_time:3136ms step_avg:56.00ms
step:57/2330 train_time:3191ms step_avg:55.98ms
step:58/2330 train_time:3249ms step_avg:56.02ms
step:59/2330 train_time:3304ms step_avg:56.00ms
step:60/2330 train_time:3361ms step_avg:56.02ms
step:61/2330 train_time:3416ms step_avg:56.00ms
step:62/2330 train_time:3474ms step_avg:56.03ms
step:63/2330 train_time:3529ms step_avg:56.02ms
step:64/2330 train_time:3587ms step_avg:56.05ms
step:65/2330 train_time:3641ms step_avg:56.02ms
step:66/2330 train_time:3699ms step_avg:56.04ms
step:67/2330 train_time:3754ms step_avg:56.03ms
step:68/2330 train_time:3812ms step_avg:56.06ms
step:69/2330 train_time:3867ms step_avg:56.04ms
step:70/2330 train_time:3924ms step_avg:56.06ms
step:71/2330 train_time:3979ms step_avg:56.04ms
step:72/2330 train_time:4037ms step_avg:56.07ms
step:73/2330 train_time:4092ms step_avg:56.06ms
step:74/2330 train_time:4150ms step_avg:56.07ms
step:75/2330 train_time:4204ms step_avg:56.05ms
step:76/2330 train_time:4262ms step_avg:56.08ms
step:77/2330 train_time:4317ms step_avg:56.06ms
step:78/2330 train_time:4374ms step_avg:56.08ms
step:79/2330 train_time:4429ms step_avg:56.07ms
step:80/2330 train_time:4489ms step_avg:56.11ms
step:81/2330 train_time:4543ms step_avg:56.09ms
step:82/2330 train_time:4602ms step_avg:56.12ms
step:83/2330 train_time:4656ms step_avg:56.10ms
step:84/2330 train_time:4714ms step_avg:56.12ms
step:85/2330 train_time:4769ms step_avg:56.10ms
step:86/2330 train_time:4827ms step_avg:56.12ms
step:87/2330 train_time:4882ms step_avg:56.11ms
step:88/2330 train_time:4939ms step_avg:56.13ms
step:89/2330 train_time:4995ms step_avg:56.12ms
step:90/2330 train_time:5052ms step_avg:56.14ms
step:91/2330 train_time:5107ms step_avg:56.12ms
step:92/2330 train_time:5165ms step_avg:56.14ms
step:93/2330 train_time:5220ms step_avg:56.13ms
step:94/2330 train_time:5278ms step_avg:56.15ms
step:95/2330 train_time:5333ms step_avg:56.14ms
step:96/2330 train_time:5393ms step_avg:56.17ms
step:97/2330 train_time:5447ms step_avg:56.16ms
step:98/2330 train_time:5505ms step_avg:56.17ms
step:99/2330 train_time:5560ms step_avg:56.16ms
step:100/2330 train_time:5618ms step_avg:56.18ms
step:101/2330 train_time:5673ms step_avg:56.17ms
step:102/2330 train_time:5730ms step_avg:56.18ms
step:103/2330 train_time:5785ms step_avg:56.17ms
step:104/2330 train_time:5844ms step_avg:56.20ms
step:105/2330 train_time:5899ms step_avg:56.18ms
step:106/2330 train_time:5957ms step_avg:56.20ms
step:107/2330 train_time:6012ms step_avg:56.19ms
step:108/2330 train_time:6070ms step_avg:56.21ms
step:109/2330 train_time:6125ms step_avg:56.19ms
step:110/2330 train_time:6183ms step_avg:56.21ms
step:111/2330 train_time:6237ms step_avg:56.19ms
step:112/2330 train_time:6295ms step_avg:56.21ms
step:113/2330 train_time:6351ms step_avg:56.20ms
step:114/2330 train_time:6408ms step_avg:56.21ms
step:115/2330 train_time:6464ms step_avg:56.21ms
step:116/2330 train_time:6521ms step_avg:56.22ms
step:117/2330 train_time:6576ms step_avg:56.20ms
step:118/2330 train_time:6635ms step_avg:56.23ms
step:119/2330 train_time:6690ms step_avg:56.22ms
step:120/2330 train_time:6749ms step_avg:56.24ms
step:121/2330 train_time:6804ms step_avg:56.23ms
step:122/2330 train_time:6862ms step_avg:56.25ms
step:123/2330 train_time:6917ms step_avg:56.24ms
step:124/2330 train_time:6975ms step_avg:56.25ms
step:125/2330 train_time:7030ms step_avg:56.24ms
step:126/2330 train_time:7088ms step_avg:56.25ms
step:127/2330 train_time:7142ms step_avg:56.24ms
step:128/2330 train_time:7201ms step_avg:56.26ms
step:129/2330 train_time:7255ms step_avg:56.24ms
step:130/2330 train_time:7314ms step_avg:56.26ms
step:131/2330 train_time:7369ms step_avg:56.25ms
step:132/2330 train_time:7427ms step_avg:56.27ms
step:133/2330 train_time:7483ms step_avg:56.26ms
step:134/2330 train_time:7540ms step_avg:56.27ms
step:135/2330 train_time:7595ms step_avg:56.26ms
step:136/2330 train_time:7653ms step_avg:56.27ms
step:137/2330 train_time:7709ms step_avg:56.27ms
step:138/2330 train_time:7766ms step_avg:56.28ms
step:139/2330 train_time:7821ms step_avg:56.27ms
step:140/2330 train_time:7880ms step_avg:56.29ms
step:141/2330 train_time:7935ms step_avg:56.28ms
step:142/2330 train_time:7993ms step_avg:56.29ms
step:143/2330 train_time:8048ms step_avg:56.28ms
step:144/2330 train_time:8106ms step_avg:56.29ms
step:145/2330 train_time:8162ms step_avg:56.29ms
step:146/2330 train_time:8220ms step_avg:56.30ms
step:147/2330 train_time:8275ms step_avg:56.29ms
step:148/2330 train_time:8333ms step_avg:56.31ms
step:149/2330 train_time:8388ms step_avg:56.30ms
step:150/2330 train_time:8447ms step_avg:56.31ms
step:151/2330 train_time:8502ms step_avg:56.30ms
step:152/2330 train_time:8560ms step_avg:56.32ms
step:153/2330 train_time:8615ms step_avg:56.31ms
step:154/2330 train_time:8674ms step_avg:56.32ms
step:155/2330 train_time:8729ms step_avg:56.31ms
step:156/2330 train_time:8788ms step_avg:56.33ms
step:157/2330 train_time:8843ms step_avg:56.32ms
step:158/2330 train_time:8900ms step_avg:56.33ms
step:159/2330 train_time:8955ms step_avg:56.32ms
step:160/2330 train_time:9014ms step_avg:56.34ms
step:161/2330 train_time:9070ms step_avg:56.33ms
step:162/2330 train_time:9128ms step_avg:56.35ms
step:163/2330 train_time:9183ms step_avg:56.34ms
step:164/2330 train_time:9242ms step_avg:56.35ms
step:165/2330 train_time:9297ms step_avg:56.34ms
step:166/2330 train_time:9355ms step_avg:56.36ms
step:167/2330 train_time:9411ms step_avg:56.35ms
step:168/2330 train_time:9469ms step_avg:56.36ms
step:169/2330 train_time:9524ms step_avg:56.35ms
step:170/2330 train_time:9581ms step_avg:56.36ms
step:171/2330 train_time:9636ms step_avg:56.35ms
step:172/2330 train_time:9696ms step_avg:56.37ms
step:173/2330 train_time:9752ms step_avg:56.37ms
step:174/2330 train_time:9809ms step_avg:56.38ms
step:175/2330 train_time:9865ms step_avg:56.37ms
step:176/2330 train_time:9923ms step_avg:56.38ms
step:177/2330 train_time:9978ms step_avg:56.37ms
step:178/2330 train_time:10037ms step_avg:56.39ms
step:179/2330 train_time:10093ms step_avg:56.39ms
step:180/2330 train_time:10151ms step_avg:56.40ms
step:181/2330 train_time:10207ms step_avg:56.39ms
step:182/2330 train_time:10264ms step_avg:56.40ms
step:183/2330 train_time:10320ms step_avg:56.39ms
step:184/2330 train_time:10379ms step_avg:56.41ms
step:185/2330 train_time:10435ms step_avg:56.41ms
step:186/2330 train_time:10493ms step_avg:56.42ms
step:187/2330 train_time:10549ms step_avg:56.41ms
step:188/2330 train_time:10607ms step_avg:56.42ms
step:189/2330 train_time:10662ms step_avg:56.41ms
step:190/2330 train_time:10720ms step_avg:56.42ms
step:191/2330 train_time:10775ms step_avg:56.42ms
step:192/2330 train_time:10834ms step_avg:56.43ms
step:193/2330 train_time:10889ms step_avg:56.42ms
step:194/2330 train_time:10949ms step_avg:56.44ms
step:195/2330 train_time:11003ms step_avg:56.43ms
step:196/2330 train_time:11063ms step_avg:56.44ms
step:197/2330 train_time:11118ms step_avg:56.43ms
step:198/2330 train_time:11178ms step_avg:56.46ms
step:199/2330 train_time:11234ms step_avg:56.45ms
step:200/2330 train_time:11292ms step_avg:56.46ms
step:201/2330 train_time:11348ms step_avg:56.46ms
step:202/2330 train_time:11406ms step_avg:56.46ms
step:203/2330 train_time:11461ms step_avg:56.46ms
step:204/2330 train_time:11520ms step_avg:56.47ms
step:205/2330 train_time:11575ms step_avg:56.47ms
step:206/2330 train_time:11634ms step_avg:56.47ms
step:207/2330 train_time:11689ms step_avg:56.47ms
step:208/2330 train_time:11748ms step_avg:56.48ms
step:209/2330 train_time:11803ms step_avg:56.47ms
step:210/2330 train_time:11861ms step_avg:56.48ms
step:211/2330 train_time:11916ms step_avg:56.48ms
step:212/2330 train_time:11975ms step_avg:56.48ms
step:213/2330 train_time:12030ms step_avg:56.48ms
step:214/2330 train_time:12089ms step_avg:56.49ms
step:215/2330 train_time:12145ms step_avg:56.49ms
step:216/2330 train_time:12203ms step_avg:56.49ms
step:217/2330 train_time:12258ms step_avg:56.49ms
step:218/2330 train_time:12318ms step_avg:56.50ms
step:219/2330 train_time:12373ms step_avg:56.50ms
step:220/2330 train_time:12432ms step_avg:56.51ms
step:221/2330 train_time:12488ms step_avg:56.50ms
step:222/2330 train_time:12546ms step_avg:56.51ms
step:223/2330 train_time:12601ms step_avg:56.51ms
step:224/2330 train_time:12660ms step_avg:56.52ms
step:225/2330 train_time:12716ms step_avg:56.51ms
step:226/2330 train_time:12775ms step_avg:56.53ms
step:227/2330 train_time:12831ms step_avg:56.52ms
step:228/2330 train_time:12889ms step_avg:56.53ms
step:229/2330 train_time:12944ms step_avg:56.52ms
step:230/2330 train_time:13002ms step_avg:56.53ms
step:231/2330 train_time:13057ms step_avg:56.52ms
step:232/2330 train_time:13116ms step_avg:56.53ms
step:233/2330 train_time:13171ms step_avg:56.53ms
step:234/2330 train_time:13230ms step_avg:56.54ms
step:235/2330 train_time:13285ms step_avg:56.53ms
step:236/2330 train_time:13344ms step_avg:56.54ms
step:237/2330 train_time:13400ms step_avg:56.54ms
step:238/2330 train_time:13459ms step_avg:56.55ms
step:239/2330 train_time:13515ms step_avg:56.55ms
step:240/2330 train_time:13573ms step_avg:56.55ms
step:241/2330 train_time:13628ms step_avg:56.55ms
step:242/2330 train_time:13686ms step_avg:56.55ms
step:243/2330 train_time:13741ms step_avg:56.55ms
step:244/2330 train_time:13800ms step_avg:56.56ms
step:245/2330 train_time:13856ms step_avg:56.56ms
step:246/2330 train_time:13915ms step_avg:56.56ms
step:247/2330 train_time:13970ms step_avg:56.56ms
step:248/2330 train_time:14029ms step_avg:56.57ms
step:249/2330 train_time:14085ms step_avg:56.56ms
step:250/2330 train_time:14143ms step_avg:56.57ms
step:250/2330 val_loss:6.4660 train_time:14221ms step_avg:56.88ms
step:251/2330 train_time:14239ms step_avg:56.73ms
step:252/2330 train_time:14259ms step_avg:56.58ms
step:253/2330 train_time:14314ms step_avg:56.58ms
step:254/2330 train_time:14376ms step_avg:56.60ms
step:255/2330 train_time:14434ms step_avg:56.60ms
step:256/2330 train_time:14492ms step_avg:56.61ms
step:257/2330 train_time:14549ms step_avg:56.61ms
step:258/2330 train_time:14608ms step_avg:56.62ms
step:259/2330 train_time:14663ms step_avg:56.61ms
step:260/2330 train_time:14721ms step_avg:56.62ms
step:261/2330 train_time:14776ms step_avg:56.61ms
step:262/2330 train_time:14834ms step_avg:56.62ms
step:263/2330 train_time:14890ms step_avg:56.61ms
step:264/2330 train_time:14948ms step_avg:56.62ms
step:265/2330 train_time:15003ms step_avg:56.61ms
step:266/2330 train_time:15061ms step_avg:56.62ms
step:267/2330 train_time:15116ms step_avg:56.62ms
step:268/2330 train_time:15175ms step_avg:56.62ms
step:269/2330 train_time:15230ms step_avg:56.62ms
step:270/2330 train_time:15290ms step_avg:56.63ms
step:271/2330 train_time:15346ms step_avg:56.63ms
step:272/2330 train_time:15406ms step_avg:56.64ms
step:273/2330 train_time:15461ms step_avg:56.64ms
step:274/2330 train_time:15523ms step_avg:56.65ms
step:275/2330 train_time:15578ms step_avg:56.65ms
step:276/2330 train_time:15637ms step_avg:56.66ms
step:277/2330 train_time:15693ms step_avg:56.65ms
step:278/2330 train_time:15751ms step_avg:56.66ms
step:279/2330 train_time:15807ms step_avg:56.66ms
step:280/2330 train_time:15865ms step_avg:56.66ms
step:281/2330 train_time:15920ms step_avg:56.66ms
step:282/2330 train_time:15978ms step_avg:56.66ms
step:283/2330 train_time:16034ms step_avg:56.66ms
step:284/2330 train_time:16092ms step_avg:56.66ms
step:285/2330 train_time:16148ms step_avg:56.66ms
step:286/2330 train_time:16206ms step_avg:56.66ms
step:287/2330 train_time:16261ms step_avg:56.66ms
step:288/2330 train_time:16321ms step_avg:56.67ms
step:289/2330 train_time:16377ms step_avg:56.67ms
step:290/2330 train_time:16436ms step_avg:56.68ms
step:291/2330 train_time:16492ms step_avg:56.67ms
step:292/2330 train_time:16552ms step_avg:56.68ms
step:293/2330 train_time:16608ms step_avg:56.68ms
step:294/2330 train_time:16666ms step_avg:56.69ms
step:295/2330 train_time:16722ms step_avg:56.68ms
step:296/2330 train_time:16780ms step_avg:56.69ms
step:297/2330 train_time:16835ms step_avg:56.69ms
step:298/2330 train_time:16893ms step_avg:56.69ms
step:299/2330 train_time:16949ms step_avg:56.69ms
step:300/2330 train_time:17008ms step_avg:56.69ms
step:301/2330 train_time:17062ms step_avg:56.69ms
step:302/2330 train_time:17122ms step_avg:56.69ms
step:303/2330 train_time:17177ms step_avg:56.69ms
step:304/2330 train_time:17236ms step_avg:56.70ms
step:305/2330 train_time:17293ms step_avg:56.70ms
step:306/2330 train_time:17351ms step_avg:56.70ms
step:307/2330 train_time:17407ms step_avg:56.70ms
step:308/2330 train_time:17465ms step_avg:56.71ms
step:309/2330 train_time:17521ms step_avg:56.70ms
step:310/2330 train_time:17581ms step_avg:56.71ms
step:311/2330 train_time:17636ms step_avg:56.71ms
step:312/2330 train_time:17696ms step_avg:56.72ms
step:313/2330 train_time:17751ms step_avg:56.71ms
step:314/2330 train_time:17811ms step_avg:56.72ms
step:315/2330 train_time:17866ms step_avg:56.72ms
step:316/2330 train_time:17925ms step_avg:56.72ms
step:317/2330 train_time:17980ms step_avg:56.72ms
step:318/2330 train_time:18039ms step_avg:56.73ms
step:319/2330 train_time:18094ms step_avg:56.72ms
step:320/2330 train_time:18153ms step_avg:56.73ms
step:321/2330 train_time:18209ms step_avg:56.73ms
step:322/2330 train_time:18267ms step_avg:56.73ms
step:323/2330 train_time:18323ms step_avg:56.73ms
step:324/2330 train_time:18381ms step_avg:56.73ms
step:325/2330 train_time:18437ms step_avg:56.73ms
step:326/2330 train_time:18497ms step_avg:56.74ms
step:327/2330 train_time:18553ms step_avg:56.74ms
step:328/2330 train_time:18611ms step_avg:56.74ms
step:329/2330 train_time:18667ms step_avg:56.74ms
step:330/2330 train_time:18726ms step_avg:56.75ms
step:331/2330 train_time:18782ms step_avg:56.74ms
step:332/2330 train_time:18840ms step_avg:56.75ms
step:333/2330 train_time:18896ms step_avg:56.74ms
step:334/2330 train_time:18954ms step_avg:56.75ms
step:335/2330 train_time:19010ms step_avg:56.75ms
step:336/2330 train_time:19068ms step_avg:56.75ms
step:337/2330 train_time:19124ms step_avg:56.75ms
step:338/2330 train_time:19183ms step_avg:56.76ms
step:339/2330 train_time:19239ms step_avg:56.75ms
step:340/2330 train_time:19298ms step_avg:56.76ms
step:341/2330 train_time:19353ms step_avg:56.76ms
step:342/2330 train_time:19413ms step_avg:56.76ms
step:343/2330 train_time:19468ms step_avg:56.76ms
step:344/2330 train_time:19528ms step_avg:56.77ms
step:345/2330 train_time:19584ms step_avg:56.77ms
step:346/2330 train_time:19642ms step_avg:56.77ms
step:347/2330 train_time:19698ms step_avg:56.77ms
step:348/2330 train_time:19757ms step_avg:56.77ms
step:349/2330 train_time:19814ms step_avg:56.77ms
step:350/2330 train_time:19872ms step_avg:56.78ms
step:351/2330 train_time:19928ms step_avg:56.77ms
step:352/2330 train_time:19986ms step_avg:56.78ms
step:353/2330 train_time:20042ms step_avg:56.78ms
step:354/2330 train_time:20100ms step_avg:56.78ms
step:355/2330 train_time:20156ms step_avg:56.78ms
step:356/2330 train_time:20215ms step_avg:56.78ms
step:357/2330 train_time:20271ms step_avg:56.78ms
step:358/2330 train_time:20330ms step_avg:56.79ms
step:359/2330 train_time:20386ms step_avg:56.78ms
step:360/2330 train_time:20444ms step_avg:56.79ms
step:361/2330 train_time:20500ms step_avg:56.79ms
step:362/2330 train_time:20558ms step_avg:56.79ms
step:363/2330 train_time:20614ms step_avg:56.79ms
step:364/2330 train_time:20673ms step_avg:56.79ms
step:365/2330 train_time:20729ms step_avg:56.79ms
step:366/2330 train_time:20788ms step_avg:56.80ms
step:367/2330 train_time:20844ms step_avg:56.80ms
step:368/2330 train_time:20902ms step_avg:56.80ms
step:369/2330 train_time:20958ms step_avg:56.80ms
step:370/2330 train_time:21017ms step_avg:56.80ms
step:371/2330 train_time:21073ms step_avg:56.80ms
step:372/2330 train_time:21131ms step_avg:56.80ms
step:373/2330 train_time:21187ms step_avg:56.80ms
step:374/2330 train_time:21246ms step_avg:56.81ms
step:375/2330 train_time:21301ms step_avg:56.80ms
step:376/2330 train_time:21361ms step_avg:56.81ms
step:377/2330 train_time:21417ms step_avg:56.81ms
step:378/2330 train_time:21475ms step_avg:56.81ms
step:379/2330 train_time:21531ms step_avg:56.81ms
step:380/2330 train_time:21590ms step_avg:56.82ms
step:381/2330 train_time:21646ms step_avg:56.81ms
step:382/2330 train_time:21705ms step_avg:56.82ms
step:383/2330 train_time:21761ms step_avg:56.82ms
step:384/2330 train_time:21819ms step_avg:56.82ms
step:385/2330 train_time:21875ms step_avg:56.82ms
step:386/2330 train_time:21935ms step_avg:56.83ms
step:387/2330 train_time:21991ms step_avg:56.82ms
step:388/2330 train_time:22049ms step_avg:56.83ms
step:389/2330 train_time:22105ms step_avg:56.82ms
step:390/2330 train_time:22163ms step_avg:56.83ms
step:391/2330 train_time:22219ms step_avg:56.83ms
step:392/2330 train_time:22279ms step_avg:56.83ms
step:393/2330 train_time:22334ms step_avg:56.83ms
step:394/2330 train_time:22393ms step_avg:56.83ms
step:395/2330 train_time:22449ms step_avg:56.83ms
step:396/2330 train_time:22507ms step_avg:56.84ms
step:397/2330 train_time:22563ms step_avg:56.83ms
step:398/2330 train_time:22622ms step_avg:56.84ms
step:399/2330 train_time:22678ms step_avg:56.84ms
step:400/2330 train_time:22736ms step_avg:56.84ms
step:401/2330 train_time:22792ms step_avg:56.84ms
step:402/2330 train_time:22851ms step_avg:56.84ms
step:403/2330 train_time:22906ms step_avg:56.84ms
step:404/2330 train_time:22964ms step_avg:56.84ms
step:405/2330 train_time:23020ms step_avg:56.84ms
step:406/2330 train_time:23079ms step_avg:56.84ms
step:407/2330 train_time:23134ms step_avg:56.84ms
step:408/2330 train_time:23194ms step_avg:56.85ms
step:409/2330 train_time:23250ms step_avg:56.85ms
step:410/2330 train_time:23309ms step_avg:56.85ms
step:411/2330 train_time:23364ms step_avg:56.85ms
step:412/2330 train_time:23424ms step_avg:56.85ms
step:413/2330 train_time:23479ms step_avg:56.85ms
step:414/2330 train_time:23539ms step_avg:56.86ms
step:415/2330 train_time:23594ms step_avg:56.85ms
step:416/2330 train_time:23653ms step_avg:56.86ms
step:417/2330 train_time:23709ms step_avg:56.86ms
step:418/2330 train_time:23767ms step_avg:56.86ms
step:419/2330 train_time:23823ms step_avg:56.86ms
step:420/2330 train_time:23882ms step_avg:56.86ms
step:421/2330 train_time:23938ms step_avg:56.86ms
step:422/2330 train_time:23996ms step_avg:56.86ms
step:423/2330 train_time:24052ms step_avg:56.86ms
step:424/2330 train_time:24110ms step_avg:56.86ms
step:425/2330 train_time:24166ms step_avg:56.86ms
step:426/2330 train_time:24225ms step_avg:56.87ms
step:427/2330 train_time:24281ms step_avg:56.86ms
step:428/2330 train_time:24339ms step_avg:56.87ms
step:429/2330 train_time:24395ms step_avg:56.86ms
step:430/2330 train_time:24454ms step_avg:56.87ms
step:431/2330 train_time:24510ms step_avg:56.87ms
step:432/2330 train_time:24569ms step_avg:56.87ms
step:433/2330 train_time:24625ms step_avg:56.87ms
step:434/2330 train_time:24685ms step_avg:56.88ms
step:435/2330 train_time:24740ms step_avg:56.87ms
step:436/2330 train_time:24799ms step_avg:56.88ms
step:437/2330 train_time:24854ms step_avg:56.88ms
step:438/2330 train_time:24914ms step_avg:56.88ms
step:439/2330 train_time:24970ms step_avg:56.88ms
step:440/2330 train_time:25029ms step_avg:56.88ms
step:441/2330 train_time:25084ms step_avg:56.88ms
step:442/2330 train_time:25143ms step_avg:56.88ms
step:443/2330 train_time:25198ms step_avg:56.88ms
step:444/2330 train_time:25259ms step_avg:56.89ms
step:445/2330 train_time:25315ms step_avg:56.89ms
step:446/2330 train_time:25374ms step_avg:56.89ms
step:447/2330 train_time:25430ms step_avg:56.89ms
step:448/2330 train_time:25488ms step_avg:56.89ms
step:449/2330 train_time:25543ms step_avg:56.89ms
step:450/2330 train_time:25603ms step_avg:56.90ms
step:451/2330 train_time:25658ms step_avg:56.89ms
step:452/2330 train_time:25718ms step_avg:56.90ms
step:453/2330 train_time:25774ms step_avg:56.90ms
step:454/2330 train_time:25832ms step_avg:56.90ms
step:455/2330 train_time:25888ms step_avg:56.90ms
step:456/2330 train_time:25946ms step_avg:56.90ms
step:457/2330 train_time:26001ms step_avg:56.90ms
step:458/2330 train_time:26060ms step_avg:56.90ms
step:459/2330 train_time:26116ms step_avg:56.90ms
step:460/2330 train_time:26175ms step_avg:56.90ms
step:461/2330 train_time:26231ms step_avg:56.90ms
step:462/2330 train_time:26290ms step_avg:56.90ms
step:463/2330 train_time:26346ms step_avg:56.90ms
step:464/2330 train_time:26405ms step_avg:56.91ms
step:465/2330 train_time:26460ms step_avg:56.90ms
step:466/2330 train_time:26519ms step_avg:56.91ms
step:467/2330 train_time:26574ms step_avg:56.90ms
step:468/2330 train_time:26634ms step_avg:56.91ms
step:469/2330 train_time:26690ms step_avg:56.91ms
step:470/2330 train_time:26749ms step_avg:56.91ms
step:471/2330 train_time:26804ms step_avg:56.91ms
step:472/2330 train_time:26863ms step_avg:56.91ms
step:473/2330 train_time:26918ms step_avg:56.91ms
step:474/2330 train_time:26977ms step_avg:56.91ms
step:475/2330 train_time:27033ms step_avg:56.91ms
step:476/2330 train_time:27092ms step_avg:56.92ms
step:477/2330 train_time:27148ms step_avg:56.91ms
step:478/2330 train_time:27207ms step_avg:56.92ms
step:479/2330 train_time:27263ms step_avg:56.92ms
step:480/2330 train_time:27321ms step_avg:56.92ms
step:481/2330 train_time:27377ms step_avg:56.92ms
step:482/2330 train_time:27436ms step_avg:56.92ms
step:483/2330 train_time:27492ms step_avg:56.92ms
step:484/2330 train_time:27551ms step_avg:56.92ms
step:485/2330 train_time:27606ms step_avg:56.92ms
step:486/2330 train_time:27666ms step_avg:56.93ms
step:487/2330 train_time:27722ms step_avg:56.92ms
step:488/2330 train_time:27781ms step_avg:56.93ms
step:489/2330 train_time:27837ms step_avg:56.93ms
step:490/2330 train_time:27896ms step_avg:56.93ms
step:491/2330 train_time:27952ms step_avg:56.93ms
step:492/2330 train_time:28011ms step_avg:56.93ms
step:493/2330 train_time:28067ms step_avg:56.93ms
step:494/2330 train_time:28125ms step_avg:56.93ms
step:495/2330 train_time:28181ms step_avg:56.93ms
step:496/2330 train_time:28240ms step_avg:56.94ms
step:497/2330 train_time:28296ms step_avg:56.93ms
step:498/2330 train_time:28355ms step_avg:56.94ms
step:499/2330 train_time:28411ms step_avg:56.94ms
step:500/2330 train_time:28469ms step_avg:56.94ms
step:500/2330 val_loss:5.7690 train_time:28547ms step_avg:57.09ms
step:501/2330 train_time:28566ms step_avg:57.02ms
step:502/2330 train_time:28586ms step_avg:56.94ms
step:503/2330 train_time:28641ms step_avg:56.94ms
step:504/2330 train_time:28705ms step_avg:56.96ms
step:505/2330 train_time:28762ms step_avg:56.95ms
step:506/2330 train_time:28824ms step_avg:56.96ms
step:507/2330 train_time:28879ms step_avg:56.96ms
step:508/2330 train_time:28939ms step_avg:56.97ms
step:509/2330 train_time:28994ms step_avg:56.96ms
step:510/2330 train_time:29053ms step_avg:56.97ms
step:511/2330 train_time:29108ms step_avg:56.96ms
step:512/2330 train_time:29166ms step_avg:56.97ms
step:513/2330 train_time:29222ms step_avg:56.96ms
step:514/2330 train_time:29280ms step_avg:56.97ms
step:515/2330 train_time:29335ms step_avg:56.96ms
step:516/2330 train_time:29393ms step_avg:56.96ms
step:517/2330 train_time:29449ms step_avg:56.96ms
step:518/2330 train_time:29508ms step_avg:56.96ms
step:519/2330 train_time:29564ms step_avg:56.96ms
step:520/2330 train_time:29625ms step_avg:56.97ms
step:521/2330 train_time:29682ms step_avg:56.97ms
step:522/2330 train_time:29744ms step_avg:56.98ms
step:523/2330 train_time:29800ms step_avg:56.98ms
step:524/2330 train_time:29859ms step_avg:56.98ms
step:525/2330 train_time:29915ms step_avg:56.98ms
step:526/2330 train_time:29973ms step_avg:56.98ms
step:527/2330 train_time:30028ms step_avg:56.98ms
step:528/2330 train_time:30087ms step_avg:56.98ms
step:529/2330 train_time:30142ms step_avg:56.98ms
step:530/2330 train_time:30201ms step_avg:56.98ms
step:531/2330 train_time:30257ms step_avg:56.98ms
step:532/2330 train_time:30314ms step_avg:56.98ms
step:533/2330 train_time:30370ms step_avg:56.98ms
step:534/2330 train_time:30429ms step_avg:56.98ms
step:535/2330 train_time:30485ms step_avg:56.98ms
step:536/2330 train_time:30544ms step_avg:56.98ms
step:537/2330 train_time:30600ms step_avg:56.98ms
step:538/2330 train_time:30660ms step_avg:56.99ms
step:539/2330 train_time:30716ms step_avg:56.99ms
step:540/2330 train_time:30776ms step_avg:56.99ms
step:541/2330 train_time:30832ms step_avg:56.99ms
step:542/2330 train_time:30891ms step_avg:56.99ms
step:543/2330 train_time:30948ms step_avg:56.99ms
step:544/2330 train_time:31006ms step_avg:57.00ms
step:545/2330 train_time:31062ms step_avg:56.99ms
step:546/2330 train_time:31121ms step_avg:57.00ms
step:547/2330 train_time:31176ms step_avg:56.99ms
step:548/2330 train_time:31234ms step_avg:57.00ms
step:549/2330 train_time:31289ms step_avg:56.99ms
step:550/2330 train_time:31349ms step_avg:57.00ms
step:551/2330 train_time:31404ms step_avg:57.00ms
step:552/2330 train_time:31462ms step_avg:57.00ms
step:553/2330 train_time:31518ms step_avg:56.99ms
step:554/2330 train_time:31578ms step_avg:57.00ms
step:555/2330 train_time:31633ms step_avg:57.00ms
step:556/2330 train_time:31693ms step_avg:57.00ms
step:557/2330 train_time:31749ms step_avg:57.00ms
step:558/2330 train_time:31808ms step_avg:57.00ms
step:559/2330 train_time:31865ms step_avg:57.00ms
step:560/2330 train_time:31924ms step_avg:57.01ms
step:561/2330 train_time:31980ms step_avg:57.01ms
step:562/2330 train_time:32039ms step_avg:57.01ms
step:563/2330 train_time:32094ms step_avg:57.01ms
step:564/2330 train_time:32153ms step_avg:57.01ms
step:565/2330 train_time:32208ms step_avg:57.01ms
step:566/2330 train_time:32268ms step_avg:57.01ms
step:567/2330 train_time:32324ms step_avg:57.01ms
step:568/2330 train_time:32382ms step_avg:57.01ms
step:569/2330 train_time:32438ms step_avg:57.01ms
step:570/2330 train_time:32496ms step_avg:57.01ms
step:571/2330 train_time:32553ms step_avg:57.01ms
step:572/2330 train_time:32611ms step_avg:57.01ms
step:573/2330 train_time:32668ms step_avg:57.01ms
step:574/2330 train_time:32726ms step_avg:57.01ms
step:575/2330 train_time:32782ms step_avg:57.01ms
step:576/2330 train_time:32841ms step_avg:57.02ms
step:577/2330 train_time:32897ms step_avg:57.01ms
step:578/2330 train_time:32957ms step_avg:57.02ms
step:579/2330 train_time:33013ms step_avg:57.02ms
step:580/2330 train_time:33072ms step_avg:57.02ms
step:581/2330 train_time:33128ms step_avg:57.02ms
step:582/2330 train_time:33187ms step_avg:57.02ms
step:583/2330 train_time:33243ms step_avg:57.02ms
step:584/2330 train_time:33302ms step_avg:57.02ms
step:585/2330 train_time:33358ms step_avg:57.02ms
step:586/2330 train_time:33416ms step_avg:57.02ms
step:587/2330 train_time:33472ms step_avg:57.02ms
step:588/2330 train_time:33530ms step_avg:57.02ms
step:589/2330 train_time:33586ms step_avg:57.02ms
step:590/2330 train_time:33646ms step_avg:57.03ms
step:591/2330 train_time:33701ms step_avg:57.02ms
step:592/2330 train_time:33760ms step_avg:57.03ms
step:593/2330 train_time:33816ms step_avg:57.03ms
step:594/2330 train_time:33875ms step_avg:57.03ms
step:595/2330 train_time:33931ms step_avg:57.03ms
step:596/2330 train_time:33990ms step_avg:57.03ms
step:597/2330 train_time:34045ms step_avg:57.03ms
step:598/2330 train_time:34105ms step_avg:57.03ms
step:599/2330 train_time:34160ms step_avg:57.03ms
step:600/2330 train_time:34220ms step_avg:57.03ms
step:601/2330 train_time:34275ms step_avg:57.03ms
step:602/2330 train_time:34334ms step_avg:57.03ms
step:603/2330 train_time:34389ms step_avg:57.03ms
step:604/2330 train_time:34449ms step_avg:57.03ms
step:605/2330 train_time:34505ms step_avg:57.03ms
step:606/2330 train_time:34564ms step_avg:57.04ms
step:607/2330 train_time:34620ms step_avg:57.03ms
step:608/2330 train_time:34680ms step_avg:57.04ms
step:609/2330 train_time:34736ms step_avg:57.04ms
step:610/2330 train_time:34794ms step_avg:57.04ms
step:611/2330 train_time:34850ms step_avg:57.04ms
step:612/2330 train_time:34908ms step_avg:57.04ms
step:613/2330 train_time:34965ms step_avg:57.04ms
step:614/2330 train_time:35025ms step_avg:57.04ms
step:615/2330 train_time:35081ms step_avg:57.04ms
step:616/2330 train_time:35140ms step_avg:57.05ms
step:617/2330 train_time:35196ms step_avg:57.04ms
step:618/2330 train_time:35254ms step_avg:57.05ms
step:619/2330 train_time:35309ms step_avg:57.04ms
step:620/2330 train_time:35369ms step_avg:57.05ms
step:621/2330 train_time:35425ms step_avg:57.05ms
step:622/2330 train_time:35484ms step_avg:57.05ms
step:623/2330 train_time:35540ms step_avg:57.05ms
step:624/2330 train_time:35598ms step_avg:57.05ms
step:625/2330 train_time:35654ms step_avg:57.05ms
step:626/2330 train_time:35712ms step_avg:57.05ms
step:627/2330 train_time:35768ms step_avg:57.05ms
step:628/2330 train_time:35828ms step_avg:57.05ms
step:629/2330 train_time:35884ms step_avg:57.05ms
step:630/2330 train_time:35943ms step_avg:57.05ms
step:631/2330 train_time:35999ms step_avg:57.05ms
step:632/2330 train_time:36059ms step_avg:57.05ms
step:633/2330 train_time:36115ms step_avg:57.05ms
step:634/2330 train_time:36173ms step_avg:57.06ms
step:635/2330 train_time:36228ms step_avg:57.05ms
step:636/2330 train_time:36289ms step_avg:57.06ms
step:637/2330 train_time:36344ms step_avg:57.06ms
step:638/2330 train_time:36403ms step_avg:57.06ms
step:639/2330 train_time:36458ms step_avg:57.06ms
step:640/2330 train_time:36517ms step_avg:57.06ms
step:641/2330 train_time:36574ms step_avg:57.06ms
step:642/2330 train_time:36632ms step_avg:57.06ms
step:643/2330 train_time:36688ms step_avg:57.06ms
step:644/2330 train_time:36746ms step_avg:57.06ms
step:645/2330 train_time:36802ms step_avg:57.06ms
step:646/2330 train_time:36862ms step_avg:57.06ms
step:647/2330 train_time:36917ms step_avg:57.06ms
step:648/2330 train_time:36976ms step_avg:57.06ms
step:649/2330 train_time:37033ms step_avg:57.06ms
step:650/2330 train_time:37092ms step_avg:57.06ms
step:651/2330 train_time:37148ms step_avg:57.06ms
step:652/2330 train_time:37207ms step_avg:57.07ms
step:653/2330 train_time:37262ms step_avg:57.06ms
step:654/2330 train_time:37321ms step_avg:57.07ms
step:655/2330 train_time:37377ms step_avg:57.06ms
step:656/2330 train_time:37437ms step_avg:57.07ms
step:657/2330 train_time:37493ms step_avg:57.07ms
step:658/2330 train_time:37552ms step_avg:57.07ms
step:659/2330 train_time:37608ms step_avg:57.07ms
step:660/2330 train_time:37667ms step_avg:57.07ms
step:661/2330 train_time:37723ms step_avg:57.07ms
step:662/2330 train_time:37782ms step_avg:57.07ms
step:663/2330 train_time:37838ms step_avg:57.07ms
step:664/2330 train_time:37897ms step_avg:57.07ms
step:665/2330 train_time:37953ms step_avg:57.07ms
step:666/2330 train_time:38011ms step_avg:57.07ms
step:667/2330 train_time:38068ms step_avg:57.07ms
step:668/2330 train_time:38126ms step_avg:57.08ms
step:669/2330 train_time:38182ms step_avg:57.07ms
step:670/2330 train_time:38242ms step_avg:57.08ms
step:671/2330 train_time:38298ms step_avg:57.08ms
step:672/2330 train_time:38358ms step_avg:57.08ms
step:673/2330 train_time:38413ms step_avg:57.08ms
step:674/2330 train_time:38472ms step_avg:57.08ms
step:675/2330 train_time:38527ms step_avg:57.08ms
step:676/2330 train_time:38586ms step_avg:57.08ms
step:677/2330 train_time:38642ms step_avg:57.08ms
step:678/2330 train_time:38701ms step_avg:57.08ms
step:679/2330 train_time:38757ms step_avg:57.08ms
step:680/2330 train_time:38816ms step_avg:57.08ms
step:681/2330 train_time:38872ms step_avg:57.08ms
step:682/2330 train_time:38930ms step_avg:57.08ms
step:683/2330 train_time:38987ms step_avg:57.08ms
step:684/2330 train_time:39045ms step_avg:57.08ms
step:685/2330 train_time:39102ms step_avg:57.08ms
step:686/2330 train_time:39160ms step_avg:57.08ms
step:687/2330 train_time:39216ms step_avg:57.08ms
step:688/2330 train_time:39275ms step_avg:57.09ms
step:689/2330 train_time:39331ms step_avg:57.08ms
step:690/2330 train_time:39390ms step_avg:57.09ms
step:691/2330 train_time:39446ms step_avg:57.09ms
step:692/2330 train_time:39504ms step_avg:57.09ms
step:693/2330 train_time:39560ms step_avg:57.08ms
step:694/2330 train_time:39621ms step_avg:57.09ms
step:695/2330 train_time:39676ms step_avg:57.09ms
step:696/2330 train_time:39736ms step_avg:57.09ms
step:697/2330 train_time:39791ms step_avg:57.09ms
step:698/2330 train_time:39851ms step_avg:57.09ms
step:699/2330 train_time:39906ms step_avg:57.09ms
step:700/2330 train_time:39965ms step_avg:57.09ms
step:701/2330 train_time:40021ms step_avg:57.09ms
step:702/2330 train_time:40080ms step_avg:57.09ms
step:703/2330 train_time:40136ms step_avg:57.09ms
step:704/2330 train_time:40194ms step_avg:57.09ms
step:705/2330 train_time:40250ms step_avg:57.09ms
step:706/2330 train_time:40308ms step_avg:57.09ms
step:707/2330 train_time:40364ms step_avg:57.09ms
step:708/2330 train_time:40423ms step_avg:57.10ms
step:709/2330 train_time:40479ms step_avg:57.09ms
step:710/2330 train_time:40539ms step_avg:57.10ms
step:711/2330 train_time:40595ms step_avg:57.10ms
step:712/2330 train_time:40653ms step_avg:57.10ms
step:713/2330 train_time:40708ms step_avg:57.09ms
step:714/2330 train_time:40769ms step_avg:57.10ms
step:715/2330 train_time:40826ms step_avg:57.10ms
step:716/2330 train_time:40884ms step_avg:57.10ms
step:717/2330 train_time:40940ms step_avg:57.10ms
step:718/2330 train_time:40998ms step_avg:57.10ms
step:719/2330 train_time:41054ms step_avg:57.10ms
step:720/2330 train_time:41114ms step_avg:57.10ms
step:721/2330 train_time:41169ms step_avg:57.10ms
step:722/2330 train_time:41230ms step_avg:57.10ms
step:723/2330 train_time:41286ms step_avg:57.10ms
step:724/2330 train_time:41345ms step_avg:57.11ms
step:725/2330 train_time:41401ms step_avg:57.10ms
step:726/2330 train_time:41460ms step_avg:57.11ms
step:727/2330 train_time:41516ms step_avg:57.11ms
step:728/2330 train_time:41574ms step_avg:57.11ms
step:729/2330 train_time:41629ms step_avg:57.10ms
step:730/2330 train_time:41688ms step_avg:57.11ms
step:731/2330 train_time:41744ms step_avg:57.11ms
step:732/2330 train_time:41803ms step_avg:57.11ms
step:733/2330 train_time:41859ms step_avg:57.11ms
step:734/2330 train_time:41919ms step_avg:57.11ms
step:735/2330 train_time:41974ms step_avg:57.11ms
step:736/2330 train_time:42034ms step_avg:57.11ms
step:737/2330 train_time:42089ms step_avg:57.11ms
step:738/2330 train_time:42149ms step_avg:57.11ms
step:739/2330 train_time:42205ms step_avg:57.11ms
step:740/2330 train_time:42264ms step_avg:57.11ms
step:741/2330 train_time:42320ms step_avg:57.11ms
step:742/2330 train_time:42379ms step_avg:57.12ms
step:743/2330 train_time:42435ms step_avg:57.11ms
step:744/2330 train_time:42493ms step_avg:57.11ms
step:745/2330 train_time:42549ms step_avg:57.11ms
step:746/2330 train_time:42608ms step_avg:57.12ms
step:747/2330 train_time:42664ms step_avg:57.11ms
step:748/2330 train_time:42723ms step_avg:57.12ms
step:749/2330 train_time:42779ms step_avg:57.12ms
step:750/2330 train_time:42838ms step_avg:57.12ms
step:750/2330 val_loss:5.3439 train_time:42918ms step_avg:57.22ms
step:751/2330 train_time:42936ms step_avg:57.17ms
step:752/2330 train_time:42957ms step_avg:57.12ms
step:753/2330 train_time:43015ms step_avg:57.13ms
step:754/2330 train_time:43078ms step_avg:57.13ms
step:755/2330 train_time:43135ms step_avg:57.13ms
step:756/2330 train_time:43196ms step_avg:57.14ms
step:757/2330 train_time:43251ms step_avg:57.13ms
step:758/2330 train_time:43310ms step_avg:57.14ms
step:759/2330 train_time:43366ms step_avg:57.14ms
step:760/2330 train_time:43424ms step_avg:57.14ms
step:761/2330 train_time:43479ms step_avg:57.13ms
step:762/2330 train_time:43537ms step_avg:57.14ms
step:763/2330 train_time:43592ms step_avg:57.13ms
step:764/2330 train_time:43651ms step_avg:57.14ms
step:765/2330 train_time:43708ms step_avg:57.13ms
step:766/2330 train_time:43765ms step_avg:57.13ms
step:767/2330 train_time:43821ms step_avg:57.13ms
step:768/2330 train_time:43880ms step_avg:57.14ms
step:769/2330 train_time:43937ms step_avg:57.14ms
step:770/2330 train_time:43998ms step_avg:57.14ms
step:771/2330 train_time:44056ms step_avg:57.14ms
step:772/2330 train_time:44117ms step_avg:57.15ms
step:773/2330 train_time:44175ms step_avg:57.15ms
step:774/2330 train_time:44236ms step_avg:57.15ms
step:775/2330 train_time:44293ms step_avg:57.15ms
step:776/2330 train_time:44353ms step_avg:57.16ms
step:777/2330 train_time:44410ms step_avg:57.16ms
step:778/2330 train_time:44469ms step_avg:57.16ms
step:779/2330 train_time:44526ms step_avg:57.16ms
step:780/2330 train_time:44584ms step_avg:57.16ms
step:781/2330 train_time:44640ms step_avg:57.16ms
step:782/2330 train_time:44699ms step_avg:57.16ms
step:783/2330 train_time:44755ms step_avg:57.16ms
step:784/2330 train_time:44815ms step_avg:57.16ms
step:785/2330 train_time:44872ms step_avg:57.16ms
step:786/2330 train_time:44932ms step_avg:57.17ms
step:787/2330 train_time:44989ms step_avg:57.17ms
step:788/2330 train_time:45049ms step_avg:57.17ms
step:789/2330 train_time:45105ms step_avg:57.17ms
step:790/2330 train_time:45168ms step_avg:57.17ms
step:791/2330 train_time:45224ms step_avg:57.17ms
step:792/2330 train_time:45284ms step_avg:57.18ms
step:793/2330 train_time:45341ms step_avg:57.18ms
step:794/2330 train_time:45401ms step_avg:57.18ms
step:795/2330 train_time:45458ms step_avg:57.18ms
step:796/2330 train_time:45517ms step_avg:57.18ms
step:797/2330 train_time:45574ms step_avg:57.18ms
step:798/2330 train_time:45633ms step_avg:57.18ms
step:799/2330 train_time:45689ms step_avg:57.18ms
step:800/2330 train_time:45749ms step_avg:57.19ms
step:801/2330 train_time:45804ms step_avg:57.18ms
step:802/2330 train_time:45864ms step_avg:57.19ms
step:803/2330 train_time:45921ms step_avg:57.19ms
step:804/2330 train_time:45981ms step_avg:57.19ms
step:805/2330 train_time:46037ms step_avg:57.19ms
step:806/2330 train_time:46098ms step_avg:57.19ms
step:807/2330 train_time:46154ms step_avg:57.19ms
step:808/2330 train_time:46216ms step_avg:57.20ms
step:809/2330 train_time:46273ms step_avg:57.20ms
step:810/2330 train_time:46334ms step_avg:57.20ms
step:811/2330 train_time:46391ms step_avg:57.20ms
step:812/2330 train_time:46451ms step_avg:57.21ms
step:813/2330 train_time:46508ms step_avg:57.21ms
step:814/2330 train_time:46567ms step_avg:57.21ms
step:815/2330 train_time:46624ms step_avg:57.21ms
step:816/2330 train_time:46683ms step_avg:57.21ms
step:817/2330 train_time:46739ms step_avg:57.21ms
step:818/2330 train_time:46799ms step_avg:57.21ms
step:819/2330 train_time:46856ms step_avg:57.21ms
step:820/2330 train_time:46916ms step_avg:57.21ms
step:821/2330 train_time:46973ms step_avg:57.21ms
step:822/2330 train_time:47033ms step_avg:57.22ms
step:823/2330 train_time:47089ms step_avg:57.22ms
step:824/2330 train_time:47150ms step_avg:57.22ms
step:825/2330 train_time:47206ms step_avg:57.22ms
step:826/2330 train_time:47266ms step_avg:57.22ms
step:827/2330 train_time:47323ms step_avg:57.22ms
step:828/2330 train_time:47383ms step_avg:57.23ms
step:829/2330 train_time:47440ms step_avg:57.23ms
step:830/2330 train_time:47500ms step_avg:57.23ms
step:831/2330 train_time:47556ms step_avg:57.23ms
step:832/2330 train_time:47615ms step_avg:57.23ms
step:833/2330 train_time:47672ms step_avg:57.23ms
step:834/2330 train_time:47732ms step_avg:57.23ms
step:835/2330 train_time:47788ms step_avg:57.23ms
step:836/2330 train_time:47847ms step_avg:57.23ms
step:837/2330 train_time:47903ms step_avg:57.23ms
step:838/2330 train_time:47965ms step_avg:57.24ms
step:839/2330 train_time:48021ms step_avg:57.24ms
step:840/2330 train_time:48081ms step_avg:57.24ms
step:841/2330 train_time:48138ms step_avg:57.24ms
step:842/2330 train_time:48198ms step_avg:57.24ms
step:843/2330 train_time:48255ms step_avg:57.24ms
step:844/2330 train_time:48315ms step_avg:57.25ms
step:845/2330 train_time:48372ms step_avg:57.25ms
step:846/2330 train_time:48433ms step_avg:57.25ms
step:847/2330 train_time:48490ms step_avg:57.25ms
step:848/2330 train_time:48550ms step_avg:57.25ms
step:849/2330 train_time:48606ms step_avg:57.25ms
step:850/2330 train_time:48666ms step_avg:57.25ms
step:851/2330 train_time:48722ms step_avg:57.25ms
step:852/2330 train_time:48782ms step_avg:57.26ms
step:853/2330 train_time:48838ms step_avg:57.25ms
step:854/2330 train_time:48898ms step_avg:57.26ms
step:855/2330 train_time:48955ms step_avg:57.26ms
step:856/2330 train_time:49015ms step_avg:57.26ms
step:857/2330 train_time:49071ms step_avg:57.26ms
step:858/2330 train_time:49132ms step_avg:57.26ms
step:859/2330 train_time:49189ms step_avg:57.26ms
step:860/2330 train_time:49248ms step_avg:57.27ms
step:861/2330 train_time:49305ms step_avg:57.26ms
step:862/2330 train_time:49365ms step_avg:57.27ms
step:863/2330 train_time:49422ms step_avg:57.27ms
step:864/2330 train_time:49482ms step_avg:57.27ms
step:865/2330 train_time:49539ms step_avg:57.27ms
step:866/2330 train_time:49599ms step_avg:57.27ms
step:867/2330 train_time:49655ms step_avg:57.27ms
step:868/2330 train_time:49716ms step_avg:57.28ms
step:869/2330 train_time:49773ms step_avg:57.28ms
step:870/2330 train_time:49832ms step_avg:57.28ms
step:871/2330 train_time:49888ms step_avg:57.28ms
step:872/2330 train_time:49949ms step_avg:57.28ms
step:873/2330 train_time:50005ms step_avg:57.28ms
step:874/2330 train_time:50065ms step_avg:57.28ms
step:875/2330 train_time:50121ms step_avg:57.28ms
step:876/2330 train_time:50182ms step_avg:57.29ms
step:877/2330 train_time:50238ms step_avg:57.28ms
step:878/2330 train_time:50299ms step_avg:57.29ms
step:879/2330 train_time:50356ms step_avg:57.29ms
step:880/2330 train_time:50417ms step_avg:57.29ms
step:881/2330 train_time:50474ms step_avg:57.29ms
step:882/2330 train_time:50533ms step_avg:57.29ms
step:883/2330 train_time:50590ms step_avg:57.29ms
step:884/2330 train_time:50650ms step_avg:57.30ms
step:885/2330 train_time:50706ms step_avg:57.29ms
step:886/2330 train_time:50766ms step_avg:57.30ms
step:887/2330 train_time:50823ms step_avg:57.30ms
step:888/2330 train_time:50883ms step_avg:57.30ms
step:889/2330 train_time:50939ms step_avg:57.30ms
step:890/2330 train_time:50999ms step_avg:57.30ms
step:891/2330 train_time:51055ms step_avg:57.30ms
step:892/2330 train_time:51116ms step_avg:57.31ms
step:893/2330 train_time:51173ms step_avg:57.30ms
step:894/2330 train_time:51233ms step_avg:57.31ms
step:895/2330 train_time:51290ms step_avg:57.31ms
step:896/2330 train_time:51350ms step_avg:57.31ms
step:897/2330 train_time:51406ms step_avg:57.31ms
step:898/2330 train_time:51466ms step_avg:57.31ms
step:899/2330 train_time:51523ms step_avg:57.31ms
step:900/2330 train_time:51583ms step_avg:57.31ms
step:901/2330 train_time:51639ms step_avg:57.31ms
step:902/2330 train_time:51700ms step_avg:57.32ms
step:903/2330 train_time:51756ms step_avg:57.32ms
step:904/2330 train_time:51816ms step_avg:57.32ms
step:905/2330 train_time:51874ms step_avg:57.32ms
step:906/2330 train_time:51934ms step_avg:57.32ms
step:907/2330 train_time:51991ms step_avg:57.32ms
step:908/2330 train_time:52051ms step_avg:57.32ms
step:909/2330 train_time:52107ms step_avg:57.32ms
step:910/2330 train_time:52167ms step_avg:57.33ms
step:911/2330 train_time:52224ms step_avg:57.33ms
step:912/2330 train_time:52284ms step_avg:57.33ms
step:913/2330 train_time:52340ms step_avg:57.33ms
step:914/2330 train_time:52401ms step_avg:57.33ms
step:915/2330 train_time:52457ms step_avg:57.33ms
step:916/2330 train_time:52517ms step_avg:57.33ms
step:917/2330 train_time:52574ms step_avg:57.33ms
step:918/2330 train_time:52634ms step_avg:57.33ms
step:919/2330 train_time:52691ms step_avg:57.33ms
step:920/2330 train_time:52750ms step_avg:57.34ms
step:921/2330 train_time:52806ms step_avg:57.34ms
step:922/2330 train_time:52866ms step_avg:57.34ms
step:923/2330 train_time:52923ms step_avg:57.34ms
step:924/2330 train_time:52982ms step_avg:57.34ms
step:925/2330 train_time:53038ms step_avg:57.34ms
step:926/2330 train_time:53098ms step_avg:57.34ms
step:927/2330 train_time:53155ms step_avg:57.34ms
step:928/2330 train_time:53215ms step_avg:57.34ms
step:929/2330 train_time:53272ms step_avg:57.34ms
step:930/2330 train_time:53332ms step_avg:57.35ms
step:931/2330 train_time:53389ms step_avg:57.35ms
step:932/2330 train_time:53448ms step_avg:57.35ms
step:933/2330 train_time:53505ms step_avg:57.35ms
step:934/2330 train_time:53564ms step_avg:57.35ms
step:935/2330 train_time:53621ms step_avg:57.35ms
step:936/2330 train_time:53681ms step_avg:57.35ms
step:937/2330 train_time:53737ms step_avg:57.35ms
step:938/2330 train_time:53797ms step_avg:57.35ms
step:939/2330 train_time:53854ms step_avg:57.35ms
step:940/2330 train_time:53914ms step_avg:57.36ms
step:941/2330 train_time:53971ms step_avg:57.35ms
step:942/2330 train_time:54031ms step_avg:57.36ms
step:943/2330 train_time:54087ms step_avg:57.36ms
step:944/2330 train_time:54147ms step_avg:57.36ms
step:945/2330 train_time:54204ms step_avg:57.36ms
step:946/2330 train_time:54264ms step_avg:57.36ms
step:947/2330 train_time:54320ms step_avg:57.36ms
step:948/2330 train_time:54380ms step_avg:57.36ms
step:949/2330 train_time:54437ms step_avg:57.36ms
step:950/2330 train_time:54497ms step_avg:57.37ms
step:951/2330 train_time:54554ms step_avg:57.36ms
step:952/2330 train_time:54614ms step_avg:57.37ms
step:953/2330 train_time:54672ms step_avg:57.37ms
step:954/2330 train_time:54731ms step_avg:57.37ms
step:955/2330 train_time:54787ms step_avg:57.37ms
step:956/2330 train_time:54847ms step_avg:57.37ms
step:957/2330 train_time:54903ms step_avg:57.37ms
step:958/2330 train_time:54963ms step_avg:57.37ms
step:959/2330 train_time:55020ms step_avg:57.37ms
step:960/2330 train_time:55080ms step_avg:57.38ms
step:961/2330 train_time:55137ms step_avg:57.38ms
step:962/2330 train_time:55197ms step_avg:57.38ms
step:963/2330 train_time:55254ms step_avg:57.38ms
step:964/2330 train_time:55314ms step_avg:57.38ms
step:965/2330 train_time:55372ms step_avg:57.38ms
step:966/2330 train_time:55431ms step_avg:57.38ms
step:967/2330 train_time:55487ms step_avg:57.38ms
step:968/2330 train_time:55547ms step_avg:57.38ms
step:969/2330 train_time:55604ms step_avg:57.38ms
step:970/2330 train_time:55664ms step_avg:57.39ms
step:971/2330 train_time:55721ms step_avg:57.38ms
step:972/2330 train_time:55780ms step_avg:57.39ms
step:973/2330 train_time:55837ms step_avg:57.39ms
step:974/2330 train_time:55896ms step_avg:57.39ms
step:975/2330 train_time:55954ms step_avg:57.39ms
step:976/2330 train_time:56015ms step_avg:57.39ms
step:977/2330 train_time:56072ms step_avg:57.39ms
step:978/2330 train_time:56131ms step_avg:57.39ms
step:979/2330 train_time:56188ms step_avg:57.39ms
step:980/2330 train_time:56249ms step_avg:57.40ms
step:981/2330 train_time:56305ms step_avg:57.40ms
step:982/2330 train_time:56365ms step_avg:57.40ms
step:983/2330 train_time:56421ms step_avg:57.40ms
step:984/2330 train_time:56481ms step_avg:57.40ms
step:985/2330 train_time:56538ms step_avg:57.40ms
step:986/2330 train_time:56598ms step_avg:57.40ms
step:987/2330 train_time:56654ms step_avg:57.40ms
step:988/2330 train_time:56715ms step_avg:57.40ms
step:989/2330 train_time:56772ms step_avg:57.40ms
step:990/2330 train_time:56832ms step_avg:57.41ms
step:991/2330 train_time:56889ms step_avg:57.41ms
step:992/2330 train_time:56949ms step_avg:57.41ms
step:993/2330 train_time:57005ms step_avg:57.41ms
step:994/2330 train_time:57065ms step_avg:57.41ms
step:995/2330 train_time:57122ms step_avg:57.41ms
step:996/2330 train_time:57183ms step_avg:57.41ms
step:997/2330 train_time:57239ms step_avg:57.41ms
step:998/2330 train_time:57299ms step_avg:57.41ms
step:999/2330 train_time:57355ms step_avg:57.41ms
step:1000/2330 train_time:57416ms step_avg:57.42ms
step:1000/2330 val_loss:4.9986 train_time:57496ms step_avg:57.50ms
step:1001/2330 train_time:57514ms step_avg:57.46ms
step:1002/2330 train_time:57534ms step_avg:57.42ms
step:1003/2330 train_time:57588ms step_avg:57.42ms
step:1004/2330 train_time:57654ms step_avg:57.42ms
step:1005/2330 train_time:57710ms step_avg:57.42ms
step:1006/2330 train_time:57773ms step_avg:57.43ms
step:1007/2330 train_time:57829ms step_avg:57.43ms
step:1008/2330 train_time:57890ms step_avg:57.43ms
step:1009/2330 train_time:57946ms step_avg:57.43ms
step:1010/2330 train_time:58006ms step_avg:57.43ms
step:1011/2330 train_time:58062ms step_avg:57.43ms
step:1012/2330 train_time:58120ms step_avg:57.43ms
step:1013/2330 train_time:58176ms step_avg:57.43ms
step:1014/2330 train_time:58236ms step_avg:57.43ms
step:1015/2330 train_time:58292ms step_avg:57.43ms
step:1016/2330 train_time:58351ms step_avg:57.43ms
step:1017/2330 train_time:58408ms step_avg:57.43ms
step:1018/2330 train_time:58469ms step_avg:57.44ms
step:1019/2330 train_time:58527ms step_avg:57.44ms
step:1020/2330 train_time:58587ms step_avg:57.44ms
step:1021/2330 train_time:58644ms step_avg:57.44ms
step:1022/2330 train_time:58704ms step_avg:57.44ms
step:1023/2330 train_time:58760ms step_avg:57.44ms
step:1024/2330 train_time:58821ms step_avg:57.44ms
step:1025/2330 train_time:58877ms step_avg:57.44ms
step:1026/2330 train_time:58938ms step_avg:57.44ms
step:1027/2330 train_time:58995ms step_avg:57.44ms
step:1028/2330 train_time:59054ms step_avg:57.45ms
step:1029/2330 train_time:59111ms step_avg:57.44ms
step:1030/2330 train_time:59170ms step_avg:57.45ms
step:1031/2330 train_time:59226ms step_avg:57.44ms
step:1032/2330 train_time:59284ms step_avg:57.45ms
step:1033/2330 train_time:59341ms step_avg:57.45ms
step:1034/2330 train_time:59401ms step_avg:57.45ms
step:1035/2330 train_time:59458ms step_avg:57.45ms
step:1036/2330 train_time:59519ms step_avg:57.45ms
step:1037/2330 train_time:59576ms step_avg:57.45ms
step:1038/2330 train_time:59636ms step_avg:57.45ms
step:1039/2330 train_time:59693ms step_avg:57.45ms
step:1040/2330 train_time:59753ms step_avg:57.45ms
step:1041/2330 train_time:59810ms step_avg:57.45ms
step:1042/2330 train_time:59869ms step_avg:57.46ms
step:1043/2330 train_time:59926ms step_avg:57.46ms
step:1044/2330 train_time:59986ms step_avg:57.46ms
step:1045/2330 train_time:60042ms step_avg:57.46ms
step:1046/2330 train_time:60102ms step_avg:57.46ms
step:1047/2330 train_time:60158ms step_avg:57.46ms
step:1048/2330 train_time:60218ms step_avg:57.46ms
step:1049/2330 train_time:60275ms step_avg:57.46ms
step:1050/2330 train_time:60334ms step_avg:57.46ms
step:1051/2330 train_time:60391ms step_avg:57.46ms
step:1052/2330 train_time:60451ms step_avg:57.46ms
step:1053/2330 train_time:60508ms step_avg:57.46ms
step:1054/2330 train_time:60567ms step_avg:57.46ms
step:1055/2330 train_time:60625ms step_avg:57.46ms
step:1056/2330 train_time:60684ms step_avg:57.47ms
step:1057/2330 train_time:60741ms step_avg:57.47ms
step:1058/2330 train_time:60801ms step_avg:57.47ms
step:1059/2330 train_time:60857ms step_avg:57.47ms
step:1060/2330 train_time:60918ms step_avg:57.47ms
step:1061/2330 train_time:60974ms step_avg:57.47ms
step:1062/2330 train_time:61035ms step_avg:57.47ms
step:1063/2330 train_time:61091ms step_avg:57.47ms
step:1064/2330 train_time:61151ms step_avg:57.47ms
step:1065/2330 train_time:61207ms step_avg:57.47ms
step:1066/2330 train_time:61266ms step_avg:57.47ms
step:1067/2330 train_time:61322ms step_avg:57.47ms
step:1068/2330 train_time:61382ms step_avg:57.47ms
step:1069/2330 train_time:61438ms step_avg:57.47ms
step:1070/2330 train_time:61498ms step_avg:57.47ms
step:1071/2330 train_time:61555ms step_avg:57.47ms
step:1072/2330 train_time:61615ms step_avg:57.48ms
step:1073/2330 train_time:61672ms step_avg:57.48ms
step:1074/2330 train_time:61733ms step_avg:57.48ms
step:1075/2330 train_time:61790ms step_avg:57.48ms
step:1076/2330 train_time:61849ms step_avg:57.48ms
step:1077/2330 train_time:61906ms step_avg:57.48ms
step:1078/2330 train_time:61965ms step_avg:57.48ms
step:1079/2330 train_time:62021ms step_avg:57.48ms
step:1080/2330 train_time:62081ms step_avg:57.48ms
step:1081/2330 train_time:62137ms step_avg:57.48ms
step:1082/2330 train_time:62198ms step_avg:57.48ms
step:1083/2330 train_time:62254ms step_avg:57.48ms
step:1084/2330 train_time:62314ms step_avg:57.49ms
step:1085/2330 train_time:62371ms step_avg:57.48ms
step:1086/2330 train_time:62430ms step_avg:57.49ms
step:1087/2330 train_time:62487ms step_avg:57.49ms
step:1088/2330 train_time:62546ms step_avg:57.49ms
step:1089/2330 train_time:62603ms step_avg:57.49ms
step:1090/2330 train_time:62662ms step_avg:57.49ms
step:1091/2330 train_time:62719ms step_avg:57.49ms
step:1092/2330 train_time:62779ms step_avg:57.49ms
step:1093/2330 train_time:62836ms step_avg:57.49ms
step:1094/2330 train_time:62896ms step_avg:57.49ms
step:1095/2330 train_time:62953ms step_avg:57.49ms
step:1096/2330 train_time:63013ms step_avg:57.49ms
step:1097/2330 train_time:63070ms step_avg:57.49ms
step:1098/2330 train_time:63129ms step_avg:57.49ms
step:1099/2330 train_time:63186ms step_avg:57.49ms
step:1100/2330 train_time:63246ms step_avg:57.50ms
step:1101/2330 train_time:63302ms step_avg:57.50ms
step:1102/2330 train_time:63362ms step_avg:57.50ms
step:1103/2330 train_time:63418ms step_avg:57.50ms
step:1104/2330 train_time:63479ms step_avg:57.50ms
step:1105/2330 train_time:63535ms step_avg:57.50ms
step:1106/2330 train_time:63595ms step_avg:57.50ms
step:1107/2330 train_time:63652ms step_avg:57.50ms
step:1108/2330 train_time:63711ms step_avg:57.50ms
step:1109/2330 train_time:63768ms step_avg:57.50ms
step:1110/2330 train_time:63828ms step_avg:57.50ms
step:1111/2330 train_time:63885ms step_avg:57.50ms
step:1112/2330 train_time:63945ms step_avg:57.50ms
step:1113/2330 train_time:64002ms step_avg:57.50ms
step:1114/2330 train_time:64062ms step_avg:57.51ms
step:1115/2330 train_time:64118ms step_avg:57.51ms
step:1116/2330 train_time:64178ms step_avg:57.51ms
step:1117/2330 train_time:64235ms step_avg:57.51ms
step:1118/2330 train_time:64295ms step_avg:57.51ms
step:1119/2330 train_time:64351ms step_avg:57.51ms
step:1120/2330 train_time:64411ms step_avg:57.51ms
step:1121/2330 train_time:64467ms step_avg:57.51ms
step:1122/2330 train_time:64527ms step_avg:57.51ms
step:1123/2330 train_time:64584ms step_avg:57.51ms
step:1124/2330 train_time:64644ms step_avg:57.51ms
step:1125/2330 train_time:64700ms step_avg:57.51ms
step:1126/2330 train_time:64761ms step_avg:57.51ms
step:1127/2330 train_time:64817ms step_avg:57.51ms
step:1128/2330 train_time:64877ms step_avg:57.52ms
step:1129/2330 train_time:64935ms step_avg:57.52ms
step:1130/2330 train_time:64995ms step_avg:57.52ms
step:1131/2330 train_time:65052ms step_avg:57.52ms
step:1132/2330 train_time:65112ms step_avg:57.52ms
step:1133/2330 train_time:65168ms step_avg:57.52ms
step:1134/2330 train_time:65228ms step_avg:57.52ms
step:1135/2330 train_time:65285ms step_avg:57.52ms
step:1136/2330 train_time:65346ms step_avg:57.52ms
step:1137/2330 train_time:65402ms step_avg:57.52ms
step:1138/2330 train_time:65461ms step_avg:57.52ms
step:1139/2330 train_time:65517ms step_avg:57.52ms
step:1140/2330 train_time:65578ms step_avg:57.52ms
step:1141/2330 train_time:65634ms step_avg:57.52ms
step:1142/2330 train_time:65694ms step_avg:57.53ms
step:1143/2330 train_time:65752ms step_avg:57.53ms
step:1144/2330 train_time:65811ms step_avg:57.53ms
step:1145/2330 train_time:65868ms step_avg:57.53ms
step:1146/2330 train_time:65928ms step_avg:57.53ms
step:1147/2330 train_time:65984ms step_avg:57.53ms
step:1148/2330 train_time:66044ms step_avg:57.53ms
step:1149/2330 train_time:66101ms step_avg:57.53ms
step:1150/2330 train_time:66161ms step_avg:57.53ms
step:1151/2330 train_time:66217ms step_avg:57.53ms
step:1152/2330 train_time:66278ms step_avg:57.53ms
step:1153/2330 train_time:66334ms step_avg:57.53ms
step:1154/2330 train_time:66394ms step_avg:57.53ms
step:1155/2330 train_time:66450ms step_avg:57.53ms
step:1156/2330 train_time:66510ms step_avg:57.53ms
step:1157/2330 train_time:66566ms step_avg:57.53ms
step:1158/2330 train_time:66627ms step_avg:57.54ms
step:1159/2330 train_time:66683ms step_avg:57.54ms
step:1160/2330 train_time:66744ms step_avg:57.54ms
step:1161/2330 train_time:66800ms step_avg:57.54ms
step:1162/2330 train_time:66860ms step_avg:57.54ms
step:1163/2330 train_time:66917ms step_avg:57.54ms
step:1164/2330 train_time:66977ms step_avg:57.54ms
step:1165/2330 train_time:67034ms step_avg:57.54ms
step:1166/2330 train_time:67094ms step_avg:57.54ms
step:1167/2330 train_time:67150ms step_avg:57.54ms
step:1168/2330 train_time:67210ms step_avg:57.54ms
step:1169/2330 train_time:67267ms step_avg:57.54ms
step:1170/2330 train_time:67326ms step_avg:57.54ms
step:1171/2330 train_time:67383ms step_avg:57.54ms
step:1172/2330 train_time:67442ms step_avg:57.54ms
step:1173/2330 train_time:67499ms step_avg:57.54ms
step:1174/2330 train_time:67558ms step_avg:57.55ms
step:1175/2330 train_time:67615ms step_avg:57.54ms
step:1176/2330 train_time:67675ms step_avg:57.55ms
step:1177/2330 train_time:67731ms step_avg:57.55ms
step:1178/2330 train_time:67791ms step_avg:57.55ms
step:1179/2330 train_time:67848ms step_avg:57.55ms
step:1180/2330 train_time:67908ms step_avg:57.55ms
step:1181/2330 train_time:67965ms step_avg:57.55ms
step:1182/2330 train_time:68025ms step_avg:57.55ms
step:1183/2330 train_time:68081ms step_avg:57.55ms
step:1184/2330 train_time:68142ms step_avg:57.55ms
step:1185/2330 train_time:68198ms step_avg:57.55ms
step:1186/2330 train_time:68258ms step_avg:57.55ms
step:1187/2330 train_time:68314ms step_avg:57.55ms
step:1188/2330 train_time:68375ms step_avg:57.56ms
step:1189/2330 train_time:68433ms step_avg:57.55ms
step:1190/2330 train_time:68493ms step_avg:57.56ms
step:1191/2330 train_time:68549ms step_avg:57.56ms
step:1192/2330 train_time:68609ms step_avg:57.56ms
step:1193/2330 train_time:68666ms step_avg:57.56ms
step:1194/2330 train_time:68725ms step_avg:57.56ms
step:1195/2330 train_time:68782ms step_avg:57.56ms
step:1196/2330 train_time:68841ms step_avg:57.56ms
step:1197/2330 train_time:68898ms step_avg:57.56ms
step:1198/2330 train_time:68958ms step_avg:57.56ms
step:1199/2330 train_time:69015ms step_avg:57.56ms
step:1200/2330 train_time:69075ms step_avg:57.56ms
step:1201/2330 train_time:69131ms step_avg:57.56ms
step:1202/2330 train_time:69192ms step_avg:57.56ms
step:1203/2330 train_time:69248ms step_avg:57.56ms
step:1204/2330 train_time:69308ms step_avg:57.56ms
step:1205/2330 train_time:69364ms step_avg:57.56ms
step:1206/2330 train_time:69424ms step_avg:57.57ms
step:1207/2330 train_time:69480ms step_avg:57.56ms
step:1208/2330 train_time:69541ms step_avg:57.57ms
step:1209/2330 train_time:69598ms step_avg:57.57ms
step:1210/2330 train_time:69657ms step_avg:57.57ms
step:1211/2330 train_time:69714ms step_avg:57.57ms
step:1212/2330 train_time:69774ms step_avg:57.57ms
step:1213/2330 train_time:69831ms step_avg:57.57ms
step:1214/2330 train_time:69892ms step_avg:57.57ms
step:1215/2330 train_time:69948ms step_avg:57.57ms
step:1216/2330 train_time:70009ms step_avg:57.57ms
step:1217/2330 train_time:70065ms step_avg:57.57ms
step:1218/2330 train_time:70125ms step_avg:57.57ms
step:1219/2330 train_time:70181ms step_avg:57.57ms
step:1220/2330 train_time:70242ms step_avg:57.58ms
step:1221/2330 train_time:70299ms step_avg:57.57ms
step:1222/2330 train_time:70359ms step_avg:57.58ms
step:1223/2330 train_time:70415ms step_avg:57.58ms
step:1224/2330 train_time:70475ms step_avg:57.58ms
step:1225/2330 train_time:70532ms step_avg:57.58ms
step:1226/2330 train_time:70592ms step_avg:57.58ms
step:1227/2330 train_time:70649ms step_avg:57.58ms
step:1228/2330 train_time:70708ms step_avg:57.58ms
step:1229/2330 train_time:70764ms step_avg:57.58ms
step:1230/2330 train_time:70824ms step_avg:57.58ms
step:1231/2330 train_time:70880ms step_avg:57.58ms
step:1232/2330 train_time:70940ms step_avg:57.58ms
step:1233/2330 train_time:70997ms step_avg:57.58ms
step:1234/2330 train_time:71056ms step_avg:57.58ms
step:1235/2330 train_time:71113ms step_avg:57.58ms
step:1236/2330 train_time:71173ms step_avg:57.58ms
step:1237/2330 train_time:71230ms step_avg:57.58ms
step:1238/2330 train_time:71290ms step_avg:57.58ms
step:1239/2330 train_time:71347ms step_avg:57.58ms
step:1240/2330 train_time:71406ms step_avg:57.59ms
step:1241/2330 train_time:71462ms step_avg:57.58ms
step:1242/2330 train_time:71523ms step_avg:57.59ms
step:1243/2330 train_time:71579ms step_avg:57.59ms
step:1244/2330 train_time:71639ms step_avg:57.59ms
step:1245/2330 train_time:71695ms step_avg:57.59ms
step:1246/2330 train_time:71756ms step_avg:57.59ms
step:1247/2330 train_time:71813ms step_avg:57.59ms
step:1248/2330 train_time:71872ms step_avg:57.59ms
step:1249/2330 train_time:71929ms step_avg:57.59ms
step:1250/2330 train_time:71988ms step_avg:57.59ms
step:1250/2330 val_loss:4.7342 train_time:72069ms step_avg:57.66ms
step:1251/2330 train_time:72088ms step_avg:57.62ms
step:1252/2330 train_time:72107ms step_avg:57.59ms
step:1253/2330 train_time:72167ms step_avg:57.60ms
step:1254/2330 train_time:72230ms step_avg:57.60ms
step:1255/2330 train_time:72287ms step_avg:57.60ms
step:1256/2330 train_time:72347ms step_avg:57.60ms
step:1257/2330 train_time:72403ms step_avg:57.60ms
step:1258/2330 train_time:72463ms step_avg:57.60ms
step:1259/2330 train_time:72518ms step_avg:57.60ms
step:1260/2330 train_time:72580ms step_avg:57.60ms
step:1261/2330 train_time:72636ms step_avg:57.60ms
step:1262/2330 train_time:72695ms step_avg:57.60ms
step:1263/2330 train_time:72751ms step_avg:57.60ms
step:1264/2330 train_time:72811ms step_avg:57.60ms
step:1265/2330 train_time:72867ms step_avg:57.60ms
step:1266/2330 train_time:72926ms step_avg:57.60ms
step:1267/2330 train_time:72982ms step_avg:57.60ms
step:1268/2330 train_time:73043ms step_avg:57.60ms
step:1269/2330 train_time:73100ms step_avg:57.60ms
step:1270/2330 train_time:73162ms step_avg:57.61ms
step:1271/2330 train_time:73218ms step_avg:57.61ms
step:1272/2330 train_time:73280ms step_avg:57.61ms
step:1273/2330 train_time:73336ms step_avg:57.61ms
step:1274/2330 train_time:73399ms step_avg:57.61ms
step:1275/2330 train_time:73455ms step_avg:57.61ms
step:1276/2330 train_time:73516ms step_avg:57.61ms
step:1277/2330 train_time:73571ms step_avg:57.61ms
step:1278/2330 train_time:73632ms step_avg:57.61ms
step:1279/2330 train_time:73688ms step_avg:57.61ms
step:1280/2330 train_time:73747ms step_avg:57.62ms
step:1281/2330 train_time:73804ms step_avg:57.61ms
step:1282/2330 train_time:73864ms step_avg:57.62ms
step:1283/2330 train_time:73920ms step_avg:57.61ms
step:1284/2330 train_time:73979ms step_avg:57.62ms
step:1285/2330 train_time:74036ms step_avg:57.62ms
step:1286/2330 train_time:74095ms step_avg:57.62ms
step:1287/2330 train_time:74152ms step_avg:57.62ms
step:1288/2330 train_time:74213ms step_avg:57.62ms
step:1289/2330 train_time:74270ms step_avg:57.62ms
step:1290/2330 train_time:74330ms step_avg:57.62ms
step:1291/2330 train_time:74387ms step_avg:57.62ms
step:1292/2330 train_time:74448ms step_avg:57.62ms
step:1293/2330 train_time:74505ms step_avg:57.62ms
step:1294/2330 train_time:74565ms step_avg:57.62ms
step:1295/2330 train_time:74621ms step_avg:57.62ms
step:1296/2330 train_time:74681ms step_avg:57.62ms
step:1297/2330 train_time:74737ms step_avg:57.62ms
step:1298/2330 train_time:74796ms step_avg:57.62ms
step:1299/2330 train_time:74852ms step_avg:57.62ms
step:1300/2330 train_time:74912ms step_avg:57.62ms
step:1301/2330 train_time:74969ms step_avg:57.62ms
step:1302/2330 train_time:75028ms step_avg:57.63ms
step:1303/2330 train_time:75085ms step_avg:57.63ms
step:1304/2330 train_time:75146ms step_avg:57.63ms
step:1305/2330 train_time:75203ms step_avg:57.63ms
step:1306/2330 train_time:75264ms step_avg:57.63ms
step:1307/2330 train_time:75321ms step_avg:57.63ms
step:1308/2330 train_time:75381ms step_avg:57.63ms
step:1309/2330 train_time:75438ms step_avg:57.63ms
step:1310/2330 train_time:75497ms step_avg:57.63ms
step:1311/2330 train_time:75554ms step_avg:57.63ms
step:1312/2330 train_time:75615ms step_avg:57.63ms
step:1313/2330 train_time:75671ms step_avg:57.63ms
step:1314/2330 train_time:75730ms step_avg:57.63ms
step:1315/2330 train_time:75787ms step_avg:57.63ms
step:1316/2330 train_time:75846ms step_avg:57.63ms
step:1317/2330 train_time:75901ms step_avg:57.63ms
step:1318/2330 train_time:75963ms step_avg:57.63ms
step:1319/2330 train_time:76018ms step_avg:57.63ms
step:1320/2330 train_time:76079ms step_avg:57.64ms
step:1321/2330 train_time:76136ms step_avg:57.63ms
step:1322/2330 train_time:76196ms step_avg:57.64ms
step:1323/2330 train_time:76253ms step_avg:57.64ms
step:1324/2330 train_time:76313ms step_avg:57.64ms
step:1325/2330 train_time:76371ms step_avg:57.64ms
step:1326/2330 train_time:76431ms step_avg:57.64ms
step:1327/2330 train_time:76488ms step_avg:57.64ms
step:1328/2330 train_time:76548ms step_avg:57.64ms
step:1329/2330 train_time:76604ms step_avg:57.64ms
step:1330/2330 train_time:76664ms step_avg:57.64ms
step:1331/2330 train_time:76720ms step_avg:57.64ms
step:1332/2330 train_time:76780ms step_avg:57.64ms
step:1333/2330 train_time:76836ms step_avg:57.64ms
step:1334/2330 train_time:76896ms step_avg:57.64ms
step:1335/2330 train_time:76952ms step_avg:57.64ms
step:1336/2330 train_time:77012ms step_avg:57.64ms
step:1337/2330 train_time:77069ms step_avg:57.64ms
step:1338/2330 train_time:77129ms step_avg:57.64ms
step:1339/2330 train_time:77185ms step_avg:57.64ms
step:1340/2330 train_time:77246ms step_avg:57.65ms
step:1341/2330 train_time:77303ms step_avg:57.65ms
step:1342/2330 train_time:77363ms step_avg:57.65ms
step:1343/2330 train_time:77420ms step_avg:57.65ms
step:1344/2330 train_time:77481ms step_avg:57.65ms
step:1345/2330 train_time:77537ms step_avg:57.65ms
step:1346/2330 train_time:77597ms step_avg:57.65ms
step:1347/2330 train_time:77653ms step_avg:57.65ms
step:1348/2330 train_time:77713ms step_avg:57.65ms
step:1349/2330 train_time:77770ms step_avg:57.65ms
step:1350/2330 train_time:77829ms step_avg:57.65ms
step:1351/2330 train_time:77885ms step_avg:57.65ms
step:1352/2330 train_time:77946ms step_avg:57.65ms
step:1353/2330 train_time:78002ms step_avg:57.65ms
step:1354/2330 train_time:78062ms step_avg:57.65ms
step:1355/2330 train_time:78119ms step_avg:57.65ms
step:1356/2330 train_time:78178ms step_avg:57.65ms
step:1357/2330 train_time:78234ms step_avg:57.65ms
step:1358/2330 train_time:78295ms step_avg:57.65ms
step:1359/2330 train_time:78351ms step_avg:57.65ms
step:1360/2330 train_time:78412ms step_avg:57.66ms
step:1361/2330 train_time:78469ms step_avg:57.66ms
step:1362/2330 train_time:78529ms step_avg:57.66ms
step:1363/2330 train_time:78586ms step_avg:57.66ms
step:1364/2330 train_time:78645ms step_avg:57.66ms
step:1365/2330 train_time:78701ms step_avg:57.66ms
step:1366/2330 train_time:78762ms step_avg:57.66ms
step:1367/2330 train_time:78817ms step_avg:57.66ms
step:1368/2330 train_time:78877ms step_avg:57.66ms
step:1369/2330 train_time:78934ms step_avg:57.66ms
step:1370/2330 train_time:78994ms step_avg:57.66ms
step:1371/2330 train_time:79051ms step_avg:57.66ms
step:1372/2330 train_time:79110ms step_avg:57.66ms
step:1373/2330 train_time:79167ms step_avg:57.66ms
step:1374/2330 train_time:79227ms step_avg:57.66ms
step:1375/2330 train_time:79283ms step_avg:57.66ms
step:1376/2330 train_time:79343ms step_avg:57.66ms
step:1377/2330 train_time:79399ms step_avg:57.66ms
step:1378/2330 train_time:79461ms step_avg:57.66ms
step:1379/2330 train_time:79517ms step_avg:57.66ms
step:1380/2330 train_time:79579ms step_avg:57.67ms
step:1381/2330 train_time:79635ms step_avg:57.66ms
step:1382/2330 train_time:79695ms step_avg:57.67ms
step:1383/2330 train_time:79752ms step_avg:57.67ms
step:1384/2330 train_time:79811ms step_avg:57.67ms
step:1385/2330 train_time:79867ms step_avg:57.67ms
step:1386/2330 train_time:79928ms step_avg:57.67ms
step:1387/2330 train_time:79985ms step_avg:57.67ms
step:1388/2330 train_time:80044ms step_avg:57.67ms
step:1389/2330 train_time:80100ms step_avg:57.67ms
step:1390/2330 train_time:80160ms step_avg:57.67ms
step:1391/2330 train_time:80216ms step_avg:57.67ms
step:1392/2330 train_time:80276ms step_avg:57.67ms
step:1393/2330 train_time:80333ms step_avg:57.67ms
step:1394/2330 train_time:80393ms step_avg:57.67ms
step:1395/2330 train_time:80449ms step_avg:57.67ms
step:1396/2330 train_time:80509ms step_avg:57.67ms
step:1397/2330 train_time:80567ms step_avg:57.67ms
step:1398/2330 train_time:80627ms step_avg:57.67ms
step:1399/2330 train_time:80683ms step_avg:57.67ms
step:1400/2330 train_time:80744ms step_avg:57.67ms
step:1401/2330 train_time:80800ms step_avg:57.67ms
step:1402/2330 train_time:80861ms step_avg:57.68ms
step:1403/2330 train_time:80917ms step_avg:57.67ms
step:1404/2330 train_time:80978ms step_avg:57.68ms
step:1405/2330 train_time:81034ms step_avg:57.68ms
step:1406/2330 train_time:81094ms step_avg:57.68ms
step:1407/2330 train_time:81150ms step_avg:57.68ms
step:1408/2330 train_time:81210ms step_avg:57.68ms
step:1409/2330 train_time:81268ms step_avg:57.68ms
step:1410/2330 train_time:81328ms step_avg:57.68ms
step:1411/2330 train_time:81385ms step_avg:57.68ms
step:1412/2330 train_time:81444ms step_avg:57.68ms
step:1413/2330 train_time:81500ms step_avg:57.68ms
step:1414/2330 train_time:81560ms step_avg:57.68ms
step:1415/2330 train_time:81617ms step_avg:57.68ms
step:1416/2330 train_time:81678ms step_avg:57.68ms
step:1417/2330 train_time:81734ms step_avg:57.68ms
step:1418/2330 train_time:81794ms step_avg:57.68ms
step:1419/2330 train_time:81851ms step_avg:57.68ms
step:1420/2330 train_time:81911ms step_avg:57.68ms
step:1421/2330 train_time:81968ms step_avg:57.68ms
step:1422/2330 train_time:82028ms step_avg:57.68ms
step:1423/2330 train_time:82084ms step_avg:57.68ms
step:1424/2330 train_time:82145ms step_avg:57.69ms
step:1425/2330 train_time:82201ms step_avg:57.69ms
step:1426/2330 train_time:82261ms step_avg:57.69ms
step:1427/2330 train_time:82317ms step_avg:57.69ms
step:1428/2330 train_time:82377ms step_avg:57.69ms
step:1429/2330 train_time:82434ms step_avg:57.69ms
step:1430/2330 train_time:82493ms step_avg:57.69ms
step:1431/2330 train_time:82550ms step_avg:57.69ms
step:1432/2330 train_time:82611ms step_avg:57.69ms
step:1433/2330 train_time:82667ms step_avg:57.69ms
step:1434/2330 train_time:82728ms step_avg:57.69ms
step:1435/2330 train_time:82785ms step_avg:57.69ms
step:1436/2330 train_time:82844ms step_avg:57.69ms
step:1437/2330 train_time:82901ms step_avg:57.69ms
step:1438/2330 train_time:82961ms step_avg:57.69ms
step:1439/2330 train_time:83017ms step_avg:57.69ms
step:1440/2330 train_time:83078ms step_avg:57.69ms
step:1441/2330 train_time:83134ms step_avg:57.69ms
step:1442/2330 train_time:83195ms step_avg:57.69ms
step:1443/2330 train_time:83251ms step_avg:57.69ms
step:1444/2330 train_time:83311ms step_avg:57.69ms
step:1445/2330 train_time:83368ms step_avg:57.69ms
step:1446/2330 train_time:83427ms step_avg:57.70ms
step:1447/2330 train_time:83484ms step_avg:57.69ms
step:1448/2330 train_time:83545ms step_avg:57.70ms
step:1449/2330 train_time:83601ms step_avg:57.70ms
step:1450/2330 train_time:83662ms step_avg:57.70ms
step:1451/2330 train_time:83719ms step_avg:57.70ms
step:1452/2330 train_time:83778ms step_avg:57.70ms
step:1453/2330 train_time:83835ms step_avg:57.70ms
step:1454/2330 train_time:83894ms step_avg:57.70ms
step:1455/2330 train_time:83951ms step_avg:57.70ms
step:1456/2330 train_time:84011ms step_avg:57.70ms
step:1457/2330 train_time:84067ms step_avg:57.70ms
step:1458/2330 train_time:84127ms step_avg:57.70ms
step:1459/2330 train_time:84185ms step_avg:57.70ms
step:1460/2330 train_time:84244ms step_avg:57.70ms
step:1461/2330 train_time:84300ms step_avg:57.70ms
step:1462/2330 train_time:84360ms step_avg:57.70ms
step:1463/2330 train_time:84416ms step_avg:57.70ms
step:1464/2330 train_time:84478ms step_avg:57.70ms
step:1465/2330 train_time:84535ms step_avg:57.70ms
step:1466/2330 train_time:84595ms step_avg:57.70ms
step:1467/2330 train_time:84652ms step_avg:57.70ms
step:1468/2330 train_time:84711ms step_avg:57.71ms
step:1469/2330 train_time:84769ms step_avg:57.70ms
step:1470/2330 train_time:84828ms step_avg:57.71ms
step:1471/2330 train_time:84885ms step_avg:57.71ms
step:1472/2330 train_time:84945ms step_avg:57.71ms
step:1473/2330 train_time:85001ms step_avg:57.71ms
step:1474/2330 train_time:85062ms step_avg:57.71ms
step:1475/2330 train_time:85118ms step_avg:57.71ms
step:1476/2330 train_time:85179ms step_avg:57.71ms
step:1477/2330 train_time:85235ms step_avg:57.71ms
step:1478/2330 train_time:85296ms step_avg:57.71ms
step:1479/2330 train_time:85352ms step_avg:57.71ms
step:1480/2330 train_time:85412ms step_avg:57.71ms
step:1481/2330 train_time:85469ms step_avg:57.71ms
step:1482/2330 train_time:85529ms step_avg:57.71ms
step:1483/2330 train_time:85585ms step_avg:57.71ms
step:1484/2330 train_time:85645ms step_avg:57.71ms
step:1485/2330 train_time:85702ms step_avg:57.71ms
step:1486/2330 train_time:85763ms step_avg:57.71ms
step:1487/2330 train_time:85819ms step_avg:57.71ms
step:1488/2330 train_time:85879ms step_avg:57.71ms
step:1489/2330 train_time:85935ms step_avg:57.71ms
step:1490/2330 train_time:85995ms step_avg:57.71ms
step:1491/2330 train_time:86052ms step_avg:57.71ms
step:1492/2330 train_time:86111ms step_avg:57.71ms
step:1493/2330 train_time:86168ms step_avg:57.71ms
step:1494/2330 train_time:86228ms step_avg:57.72ms
step:1495/2330 train_time:86284ms step_avg:57.72ms
step:1496/2330 train_time:86346ms step_avg:57.72ms
step:1497/2330 train_time:86402ms step_avg:57.72ms
step:1498/2330 train_time:86462ms step_avg:57.72ms
step:1499/2330 train_time:86518ms step_avg:57.72ms
step:1500/2330 train_time:86578ms step_avg:57.72ms
step:1500/2330 val_loss:4.5427 train_time:86658ms step_avg:57.77ms
step:1501/2330 train_time:86678ms step_avg:57.75ms
step:1502/2330 train_time:86697ms step_avg:57.72ms
step:1503/2330 train_time:86756ms step_avg:57.72ms
step:1504/2330 train_time:86822ms step_avg:57.73ms
step:1505/2330 train_time:86878ms step_avg:57.73ms
step:1506/2330 train_time:86939ms step_avg:57.73ms
step:1507/2330 train_time:86995ms step_avg:57.73ms
step:1508/2330 train_time:87055ms step_avg:57.73ms
step:1509/2330 train_time:87111ms step_avg:57.73ms
step:1510/2330 train_time:87170ms step_avg:57.73ms
step:1511/2330 train_time:87226ms step_avg:57.73ms
step:1512/2330 train_time:87286ms step_avg:57.73ms
step:1513/2330 train_time:87342ms step_avg:57.73ms
step:1514/2330 train_time:87401ms step_avg:57.73ms
step:1515/2330 train_time:87457ms step_avg:57.73ms
step:1516/2330 train_time:87516ms step_avg:57.73ms
step:1517/2330 train_time:87573ms step_avg:57.73ms
step:1518/2330 train_time:87633ms step_avg:57.73ms
step:1519/2330 train_time:87690ms step_avg:57.73ms
step:1520/2330 train_time:87752ms step_avg:57.73ms
step:1521/2330 train_time:87810ms step_avg:57.73ms
step:1522/2330 train_time:87872ms step_avg:57.73ms
step:1523/2330 train_time:87929ms step_avg:57.73ms
step:1524/2330 train_time:87990ms step_avg:57.74ms
step:1525/2330 train_time:88046ms step_avg:57.74ms
step:1526/2330 train_time:88106ms step_avg:57.74ms
step:1527/2330 train_time:88162ms step_avg:57.74ms
step:1528/2330 train_time:88222ms step_avg:57.74ms
step:1529/2330 train_time:88279ms step_avg:57.74ms
step:1530/2330 train_time:88337ms step_avg:57.74ms
step:1531/2330 train_time:88394ms step_avg:57.74ms
step:1532/2330 train_time:88454ms step_avg:57.74ms
step:1533/2330 train_time:88511ms step_avg:57.74ms
step:1534/2330 train_time:88571ms step_avg:57.74ms
step:1535/2330 train_time:88628ms step_avg:57.74ms
step:1536/2330 train_time:88689ms step_avg:57.74ms
step:1537/2330 train_time:88747ms step_avg:57.74ms
step:1538/2330 train_time:88809ms step_avg:57.74ms
step:1539/2330 train_time:88867ms step_avg:57.74ms
step:1540/2330 train_time:88927ms step_avg:57.74ms
step:1541/2330 train_time:88985ms step_avg:57.74ms
step:1542/2330 train_time:89045ms step_avg:57.75ms
step:1543/2330 train_time:89101ms step_avg:57.75ms
step:1544/2330 train_time:89162ms step_avg:57.75ms
step:1545/2330 train_time:89218ms step_avg:57.75ms
step:1546/2330 train_time:89279ms step_avg:57.75ms
step:1547/2330 train_time:89335ms step_avg:57.75ms
step:1548/2330 train_time:89396ms step_avg:57.75ms
step:1549/2330 train_time:89453ms step_avg:57.75ms
step:1550/2330 train_time:89512ms step_avg:57.75ms
step:1551/2330 train_time:89569ms step_avg:57.75ms
step:1552/2330 train_time:89629ms step_avg:57.75ms
step:1553/2330 train_time:89686ms step_avg:57.75ms
step:1554/2330 train_time:89748ms step_avg:57.75ms
step:1555/2330 train_time:89807ms step_avg:57.75ms
step:1556/2330 train_time:89867ms step_avg:57.76ms
step:1557/2330 train_time:89925ms step_avg:57.76ms
step:1558/2330 train_time:89985ms step_avg:57.76ms
step:1559/2330 train_time:90043ms step_avg:57.76ms
step:1560/2330 train_time:90103ms step_avg:57.76ms
step:1561/2330 train_time:90161ms step_avg:57.76ms
step:1562/2330 train_time:90220ms step_avg:57.76ms
step:1563/2330 train_time:90276ms step_avg:57.76ms
step:1564/2330 train_time:90336ms step_avg:57.76ms
step:1565/2330 train_time:90393ms step_avg:57.76ms
step:1566/2330 train_time:90453ms step_avg:57.76ms
step:1567/2330 train_time:90510ms step_avg:57.76ms
step:1568/2330 train_time:90570ms step_avg:57.76ms
step:1569/2330 train_time:90627ms step_avg:57.76ms
step:1570/2330 train_time:90688ms step_avg:57.76ms
step:1571/2330 train_time:90746ms step_avg:57.76ms
step:1572/2330 train_time:90806ms step_avg:57.76ms
step:1573/2330 train_time:90864ms step_avg:57.76ms
step:1574/2330 train_time:90924ms step_avg:57.77ms
step:1575/2330 train_time:90981ms step_avg:57.77ms
step:1576/2330 train_time:91042ms step_avg:57.77ms
step:1577/2330 train_time:91099ms step_avg:57.77ms
step:1578/2330 train_time:91159ms step_avg:57.77ms
step:1579/2330 train_time:91215ms step_avg:57.77ms
step:1580/2330 train_time:91276ms step_avg:57.77ms
step:1581/2330 train_time:91333ms step_avg:57.77ms
step:1582/2330 train_time:91392ms step_avg:57.77ms
step:1583/2330 train_time:91449ms step_avg:57.77ms
step:1584/2330 train_time:91509ms step_avg:57.77ms
step:1585/2330 train_time:91565ms step_avg:57.77ms
step:1586/2330 train_time:91626ms step_avg:57.77ms
step:1587/2330 train_time:91682ms step_avg:57.77ms
step:1588/2330 train_time:91744ms step_avg:57.77ms
step:1589/2330 train_time:91801ms step_avg:57.77ms
step:1590/2330 train_time:91861ms step_avg:57.77ms
step:1591/2330 train_time:91919ms step_avg:57.77ms
step:1592/2330 train_time:91980ms step_avg:57.78ms
step:1593/2330 train_time:92037ms step_avg:57.78ms
step:1594/2330 train_time:92098ms step_avg:57.78ms
step:1595/2330 train_time:92155ms step_avg:57.78ms
step:1596/2330 train_time:92215ms step_avg:57.78ms
step:1597/2330 train_time:92272ms step_avg:57.78ms
step:1598/2330 train_time:92332ms step_avg:57.78ms
step:1599/2330 train_time:92388ms step_avg:57.78ms
step:1600/2330 train_time:92449ms step_avg:57.78ms
step:1601/2330 train_time:92506ms step_avg:57.78ms
step:1602/2330 train_time:92565ms step_avg:57.78ms
step:1603/2330 train_time:92622ms step_avg:57.78ms
step:1604/2330 train_time:92682ms step_avg:57.78ms
step:1605/2330 train_time:92740ms step_avg:57.78ms
step:1606/2330 train_time:92800ms step_avg:57.78ms
step:1607/2330 train_time:92857ms step_avg:57.78ms
step:1608/2330 train_time:92918ms step_avg:57.78ms
step:1609/2330 train_time:92975ms step_avg:57.78ms
step:1610/2330 train_time:93036ms step_avg:57.79ms
step:1611/2330 train_time:93093ms step_avg:57.79ms
step:1612/2330 train_time:93153ms step_avg:57.79ms
step:1613/2330 train_time:93210ms step_avg:57.79ms
step:1614/2330 train_time:93270ms step_avg:57.79ms
step:1615/2330 train_time:93326ms step_avg:57.79ms
step:1616/2330 train_time:93388ms step_avg:57.79ms
step:1617/2330 train_time:93445ms step_avg:57.79ms
step:1618/2330 train_time:93505ms step_avg:57.79ms
step:1619/2330 train_time:93562ms step_avg:57.79ms
step:1620/2330 train_time:93622ms step_avg:57.79ms
step:1621/2330 train_time:93679ms step_avg:57.79ms
step:1622/2330 train_time:93740ms step_avg:57.79ms
step:1623/2330 train_time:93796ms step_avg:57.79ms
step:1624/2330 train_time:93857ms step_avg:57.79ms
step:1625/2330 train_time:93914ms step_avg:57.79ms
step:1626/2330 train_time:93974ms step_avg:57.79ms
step:1627/2330 train_time:94031ms step_avg:57.79ms
step:1628/2330 train_time:94092ms step_avg:57.80ms
step:1629/2330 train_time:94149ms step_avg:57.80ms
step:1630/2330 train_time:94209ms step_avg:57.80ms
step:1631/2330 train_time:94266ms step_avg:57.80ms
step:1632/2330 train_time:94326ms step_avg:57.80ms
step:1633/2330 train_time:94383ms step_avg:57.80ms
step:1634/2330 train_time:94443ms step_avg:57.80ms
step:1635/2330 train_time:94500ms step_avg:57.80ms
step:1636/2330 train_time:94560ms step_avg:57.80ms
step:1637/2330 train_time:94617ms step_avg:57.80ms
step:1638/2330 train_time:94678ms step_avg:57.80ms
step:1639/2330 train_time:94735ms step_avg:57.80ms
step:1640/2330 train_time:94795ms step_avg:57.80ms
step:1641/2330 train_time:94853ms step_avg:57.80ms
step:1642/2330 train_time:94912ms step_avg:57.80ms
step:1643/2330 train_time:94969ms step_avg:57.80ms
step:1644/2330 train_time:95030ms step_avg:57.80ms
step:1645/2330 train_time:95087ms step_avg:57.80ms
step:1646/2330 train_time:95148ms step_avg:57.81ms
step:1647/2330 train_time:95205ms step_avg:57.81ms
step:1648/2330 train_time:95266ms step_avg:57.81ms
step:1649/2330 train_time:95323ms step_avg:57.81ms
step:1650/2330 train_time:95383ms step_avg:57.81ms
step:1651/2330 train_time:95440ms step_avg:57.81ms
step:1652/2330 train_time:95500ms step_avg:57.81ms
step:1653/2330 train_time:95557ms step_avg:57.81ms
step:1654/2330 train_time:95617ms step_avg:57.81ms
step:1655/2330 train_time:95674ms step_avg:57.81ms
step:1656/2330 train_time:95734ms step_avg:57.81ms
step:1657/2330 train_time:95792ms step_avg:57.81ms
step:1658/2330 train_time:95852ms step_avg:57.81ms
step:1659/2330 train_time:95909ms step_avg:57.81ms
step:1660/2330 train_time:95970ms step_avg:57.81ms
step:1661/2330 train_time:96026ms step_avg:57.81ms
step:1662/2330 train_time:96088ms step_avg:57.81ms
step:1663/2330 train_time:96145ms step_avg:57.81ms
step:1664/2330 train_time:96205ms step_avg:57.82ms
step:1665/2330 train_time:96263ms step_avg:57.82ms
step:1666/2330 train_time:96323ms step_avg:57.82ms
step:1667/2330 train_time:96380ms step_avg:57.82ms
step:1668/2330 train_time:96439ms step_avg:57.82ms
step:1669/2330 train_time:96496ms step_avg:57.82ms
step:1670/2330 train_time:96557ms step_avg:57.82ms
step:1671/2330 train_time:96614ms step_avg:57.82ms
step:1672/2330 train_time:96674ms step_avg:57.82ms
step:1673/2330 train_time:96731ms step_avg:57.82ms
step:1674/2330 train_time:96792ms step_avg:57.82ms
step:1675/2330 train_time:96850ms step_avg:57.82ms
step:1676/2330 train_time:96909ms step_avg:57.82ms
step:1677/2330 train_time:96966ms step_avg:57.82ms
step:1678/2330 train_time:97027ms step_avg:57.82ms
step:1679/2330 train_time:97084ms step_avg:57.82ms
step:1680/2330 train_time:97145ms step_avg:57.82ms
step:1681/2330 train_time:97203ms step_avg:57.82ms
step:1682/2330 train_time:97262ms step_avg:57.83ms
step:1683/2330 train_time:97319ms step_avg:57.82ms
step:1684/2330 train_time:97380ms step_avg:57.83ms
step:1685/2330 train_time:97436ms step_avg:57.83ms
step:1686/2330 train_time:97496ms step_avg:57.83ms
step:1687/2330 train_time:97554ms step_avg:57.83ms
step:1688/2330 train_time:97614ms step_avg:57.83ms
step:1689/2330 train_time:97672ms step_avg:57.83ms
step:1690/2330 train_time:97731ms step_avg:57.83ms
step:1691/2330 train_time:97789ms step_avg:57.83ms
step:1692/2330 train_time:97849ms step_avg:57.83ms
step:1693/2330 train_time:97907ms step_avg:57.83ms
step:1694/2330 train_time:97966ms step_avg:57.83ms
step:1695/2330 train_time:98024ms step_avg:57.83ms
step:1696/2330 train_time:98085ms step_avg:57.83ms
step:1697/2330 train_time:98142ms step_avg:57.83ms
step:1698/2330 train_time:98201ms step_avg:57.83ms
step:1699/2330 train_time:98259ms step_avg:57.83ms
step:1700/2330 train_time:98318ms step_avg:57.83ms
step:1701/2330 train_time:98376ms step_avg:57.83ms
step:1702/2330 train_time:98436ms step_avg:57.84ms
step:1703/2330 train_time:98493ms step_avg:57.84ms
step:1704/2330 train_time:98553ms step_avg:57.84ms
step:1705/2330 train_time:98611ms step_avg:57.84ms
step:1706/2330 train_time:98670ms step_avg:57.84ms
step:1707/2330 train_time:98727ms step_avg:57.84ms
step:1708/2330 train_time:98787ms step_avg:57.84ms
step:1709/2330 train_time:98844ms step_avg:57.84ms
step:1710/2330 train_time:98905ms step_avg:57.84ms
step:1711/2330 train_time:98962ms step_avg:57.84ms
step:1712/2330 train_time:99022ms step_avg:57.84ms
step:1713/2330 train_time:99079ms step_avg:57.84ms
step:1714/2330 train_time:99139ms step_avg:57.84ms
step:1715/2330 train_time:99196ms step_avg:57.84ms
step:1716/2330 train_time:99257ms step_avg:57.84ms
step:1717/2330 train_time:99314ms step_avg:57.84ms
step:1718/2330 train_time:99375ms step_avg:57.84ms
step:1719/2330 train_time:99432ms step_avg:57.84ms
step:1720/2330 train_time:99492ms step_avg:57.84ms
step:1721/2330 train_time:99550ms step_avg:57.84ms
step:1722/2330 train_time:99609ms step_avg:57.85ms
step:1723/2330 train_time:99666ms step_avg:57.84ms
step:1724/2330 train_time:99727ms step_avg:57.85ms
step:1725/2330 train_time:99784ms step_avg:57.85ms
step:1726/2330 train_time:99845ms step_avg:57.85ms
step:1727/2330 train_time:99902ms step_avg:57.85ms
step:1728/2330 train_time:99962ms step_avg:57.85ms
step:1729/2330 train_time:100019ms step_avg:57.85ms
step:1730/2330 train_time:100079ms step_avg:57.85ms
step:1731/2330 train_time:100136ms step_avg:57.85ms
step:1732/2330 train_time:100197ms step_avg:57.85ms
step:1733/2330 train_time:100254ms step_avg:57.85ms
step:1734/2330 train_time:100314ms step_avg:57.85ms
step:1735/2330 train_time:100372ms step_avg:57.85ms
step:1736/2330 train_time:100432ms step_avg:57.85ms
step:1737/2330 train_time:100489ms step_avg:57.85ms
step:1738/2330 train_time:100550ms step_avg:57.85ms
step:1739/2330 train_time:100608ms step_avg:57.85ms
step:1740/2330 train_time:100668ms step_avg:57.86ms
step:1741/2330 train_time:100727ms step_avg:57.86ms
step:1742/2330 train_time:100787ms step_avg:57.86ms
step:1743/2330 train_time:100844ms step_avg:57.86ms
step:1744/2330 train_time:100904ms step_avg:57.86ms
step:1745/2330 train_time:100962ms step_avg:57.86ms
step:1746/2330 train_time:101021ms step_avg:57.86ms
step:1747/2330 train_time:101078ms step_avg:57.86ms
step:1748/2330 train_time:101138ms step_avg:57.86ms
step:1749/2330 train_time:101195ms step_avg:57.86ms
step:1750/2330 train_time:101255ms step_avg:57.86ms
step:1750/2330 val_loss:4.4119 train_time:101336ms step_avg:57.91ms
step:1751/2330 train_time:101355ms step_avg:57.88ms
step:1752/2330 train_time:101375ms step_avg:57.86ms
step:1753/2330 train_time:101429ms step_avg:57.86ms
step:1754/2330 train_time:101494ms step_avg:57.86ms
step:1755/2330 train_time:101550ms step_avg:57.86ms
step:1756/2330 train_time:101612ms step_avg:57.87ms
step:1757/2330 train_time:101669ms step_avg:57.86ms
step:1758/2330 train_time:101727ms step_avg:57.87ms
step:1759/2330 train_time:101784ms step_avg:57.86ms
step:1760/2330 train_time:101844ms step_avg:57.87ms
step:1761/2330 train_time:101900ms step_avg:57.86ms
step:1762/2330 train_time:101959ms step_avg:57.87ms
step:1763/2330 train_time:102016ms step_avg:57.86ms
step:1764/2330 train_time:102075ms step_avg:57.87ms
step:1765/2330 train_time:102131ms step_avg:57.86ms
step:1766/2330 train_time:102190ms step_avg:57.87ms
step:1767/2330 train_time:102249ms step_avg:57.87ms
step:1768/2330 train_time:102313ms step_avg:57.87ms
step:1769/2330 train_time:102371ms step_avg:57.87ms
step:1770/2330 train_time:102432ms step_avg:57.87ms
step:1771/2330 train_time:102490ms step_avg:57.87ms
step:1772/2330 train_time:102549ms step_avg:57.87ms
step:1773/2330 train_time:102606ms step_avg:57.87ms
step:1774/2330 train_time:102667ms step_avg:57.87ms
step:1775/2330 train_time:102723ms step_avg:57.87ms
step:1776/2330 train_time:102784ms step_avg:57.87ms
step:1777/2330 train_time:102841ms step_avg:57.87ms
step:1778/2330 train_time:102901ms step_avg:57.87ms
step:1779/2330 train_time:102958ms step_avg:57.87ms
step:1780/2330 train_time:103018ms step_avg:57.88ms
step:1781/2330 train_time:103074ms step_avg:57.87ms
step:1782/2330 train_time:103134ms step_avg:57.88ms
step:1783/2330 train_time:103191ms step_avg:57.88ms
step:1784/2330 train_time:103253ms step_avg:57.88ms
step:1785/2330 train_time:103310ms step_avg:57.88ms
step:1786/2330 train_time:103370ms step_avg:57.88ms
step:1787/2330 train_time:103427ms step_avg:57.88ms
step:1788/2330 train_time:103489ms step_avg:57.88ms
step:1789/2330 train_time:103546ms step_avg:57.88ms
step:1790/2330 train_time:103606ms step_avg:57.88ms
step:1791/2330 train_time:103663ms step_avg:57.88ms
step:1792/2330 train_time:103724ms step_avg:57.88ms
step:1793/2330 train_time:103780ms step_avg:57.88ms
step:1794/2330 train_time:103841ms step_avg:57.88ms
step:1795/2330 train_time:103897ms step_avg:57.88ms
step:1796/2330 train_time:103957ms step_avg:57.88ms
step:1797/2330 train_time:104013ms step_avg:57.88ms
step:1798/2330 train_time:104074ms step_avg:57.88ms
step:1799/2330 train_time:104131ms step_avg:57.88ms
step:1800/2330 train_time:104191ms step_avg:57.88ms
step:1801/2330 train_time:104249ms step_avg:57.88ms
step:1802/2330 train_time:104309ms step_avg:57.89ms
step:1803/2330 train_time:104367ms step_avg:57.89ms
step:1804/2330 train_time:104427ms step_avg:57.89ms
step:1805/2330 train_time:104484ms step_avg:57.89ms
step:1806/2330 train_time:104545ms step_avg:57.89ms
step:1807/2330 train_time:104604ms step_avg:57.89ms
step:1808/2330 train_time:104663ms step_avg:57.89ms
step:1809/2330 train_time:104721ms step_avg:57.89ms
step:1810/2330 train_time:104781ms step_avg:57.89ms
step:1811/2330 train_time:104839ms step_avg:57.89ms
step:1812/2330 train_time:104898ms step_avg:57.89ms
step:1813/2330 train_time:104955ms step_avg:57.89ms
step:1814/2330 train_time:105015ms step_avg:57.89ms
step:1815/2330 train_time:105072ms step_avg:57.89ms
step:1816/2330 train_time:105131ms step_avg:57.89ms
step:1817/2330 train_time:105189ms step_avg:57.89ms
step:1818/2330 train_time:105249ms step_avg:57.89ms
step:1819/2330 train_time:105306ms step_avg:57.89ms
step:1820/2330 train_time:105366ms step_avg:57.89ms
step:1821/2330 train_time:105423ms step_avg:57.89ms
step:1822/2330 train_time:105484ms step_avg:57.89ms
step:1823/2330 train_time:105541ms step_avg:57.89ms
step:1824/2330 train_time:105601ms step_avg:57.90ms
step:1825/2330 train_time:105658ms step_avg:57.89ms
step:1826/2330 train_time:105718ms step_avg:57.90ms
step:1827/2330 train_time:105775ms step_avg:57.90ms
step:1828/2330 train_time:105836ms step_avg:57.90ms
step:1829/2330 train_time:105892ms step_avg:57.90ms
step:1830/2330 train_time:105952ms step_avg:57.90ms
step:1831/2330 train_time:106009ms step_avg:57.90ms
step:1832/2330 train_time:106069ms step_avg:57.90ms
step:1833/2330 train_time:106126ms step_avg:57.90ms
step:1834/2330 train_time:106186ms step_avg:57.90ms
step:1835/2330 train_time:106243ms step_avg:57.90ms
step:1836/2330 train_time:106303ms step_avg:57.90ms
step:1837/2330 train_time:106360ms step_avg:57.90ms
step:1838/2330 train_time:106419ms step_avg:57.90ms
step:1839/2330 train_time:106476ms step_avg:57.90ms
step:1840/2330 train_time:106537ms step_avg:57.90ms
step:1841/2330 train_time:106594ms step_avg:57.90ms
step:1842/2330 train_time:106655ms step_avg:57.90ms
step:1843/2330 train_time:106712ms step_avg:57.90ms
step:1844/2330 train_time:106773ms step_avg:57.90ms
step:1845/2330 train_time:106830ms step_avg:57.90ms
step:1846/2330 train_time:106890ms step_avg:57.90ms
step:1847/2330 train_time:106947ms step_avg:57.90ms
step:1848/2330 train_time:107007ms step_avg:57.90ms
step:1849/2330 train_time:107064ms step_avg:57.90ms
step:1850/2330 train_time:107124ms step_avg:57.91ms
step:1851/2330 train_time:107181ms step_avg:57.90ms
step:1852/2330 train_time:107242ms step_avg:57.91ms
step:1853/2330 train_time:107299ms step_avg:57.91ms
step:1854/2330 train_time:107360ms step_avg:57.91ms
step:1855/2330 train_time:107417ms step_avg:57.91ms
step:1856/2330 train_time:107478ms step_avg:57.91ms
step:1857/2330 train_time:107534ms step_avg:57.91ms
step:1858/2330 train_time:107595ms step_avg:57.91ms
step:1859/2330 train_time:107652ms step_avg:57.91ms
step:1860/2330 train_time:107712ms step_avg:57.91ms
step:1861/2330 train_time:107769ms step_avg:57.91ms
step:1862/2330 train_time:107830ms step_avg:57.91ms
step:1863/2330 train_time:107888ms step_avg:57.91ms
step:1864/2330 train_time:107947ms step_avg:57.91ms
step:1865/2330 train_time:108004ms step_avg:57.91ms
step:1866/2330 train_time:108065ms step_avg:57.91ms
step:1867/2330 train_time:108121ms step_avg:57.91ms
step:1868/2330 train_time:108184ms step_avg:57.91ms
step:1869/2330 train_time:108241ms step_avg:57.91ms
step:1870/2330 train_time:108301ms step_avg:57.92ms
step:1871/2330 train_time:108358ms step_avg:57.91ms
step:1872/2330 train_time:108418ms step_avg:57.92ms
step:1873/2330 train_time:108474ms step_avg:57.91ms
step:1874/2330 train_time:108534ms step_avg:57.92ms
step:1875/2330 train_time:108591ms step_avg:57.92ms
step:1876/2330 train_time:108651ms step_avg:57.92ms
step:1877/2330 train_time:108709ms step_avg:57.92ms
step:1878/2330 train_time:108769ms step_avg:57.92ms
step:1879/2330 train_time:108826ms step_avg:57.92ms
step:1880/2330 train_time:108887ms step_avg:57.92ms
step:1881/2330 train_time:108944ms step_avg:57.92ms
step:1882/2330 train_time:109004ms step_avg:57.92ms
step:1883/2330 train_time:109061ms step_avg:57.92ms
step:1884/2330 train_time:109121ms step_avg:57.92ms
step:1885/2330 train_time:109179ms step_avg:57.92ms
step:1886/2330 train_time:109239ms step_avg:57.92ms
step:1887/2330 train_time:109296ms step_avg:57.92ms
step:1888/2330 train_time:109356ms step_avg:57.92ms
step:1889/2330 train_time:109412ms step_avg:57.92ms
step:1890/2330 train_time:109473ms step_avg:57.92ms
step:1891/2330 train_time:109529ms step_avg:57.92ms
step:1892/2330 train_time:109590ms step_avg:57.92ms
step:1893/2330 train_time:109647ms step_avg:57.92ms
step:1894/2330 train_time:109707ms step_avg:57.92ms
step:1895/2330 train_time:109764ms step_avg:57.92ms
step:1896/2330 train_time:109825ms step_avg:57.92ms
step:1897/2330 train_time:109882ms step_avg:57.92ms
step:1898/2330 train_time:109942ms step_avg:57.92ms
step:1899/2330 train_time:109999ms step_avg:57.92ms
step:1900/2330 train_time:110059ms step_avg:57.93ms
step:1901/2330 train_time:110116ms step_avg:57.93ms
step:1902/2330 train_time:110176ms step_avg:57.93ms
step:1903/2330 train_time:110233ms step_avg:57.93ms
step:1904/2330 train_time:110293ms step_avg:57.93ms
step:1905/2330 train_time:110350ms step_avg:57.93ms
step:1906/2330 train_time:110411ms step_avg:57.93ms
step:1907/2330 train_time:110468ms step_avg:57.93ms
step:1908/2330 train_time:110528ms step_avg:57.93ms
step:1909/2330 train_time:110585ms step_avg:57.93ms
step:1910/2330 train_time:110645ms step_avg:57.93ms
step:1911/2330 train_time:110702ms step_avg:57.93ms
step:1912/2330 train_time:110762ms step_avg:57.93ms
step:1913/2330 train_time:110818ms step_avg:57.93ms
step:1914/2330 train_time:110879ms step_avg:57.93ms
step:1915/2330 train_time:110936ms step_avg:57.93ms
step:1916/2330 train_time:110996ms step_avg:57.93ms
step:1917/2330 train_time:111054ms step_avg:57.93ms
step:1918/2330 train_time:111114ms step_avg:57.93ms
step:1919/2330 train_time:111171ms step_avg:57.93ms
step:1920/2330 train_time:111231ms step_avg:57.93ms
step:1921/2330 train_time:111289ms step_avg:57.93ms
step:1922/2330 train_time:111349ms step_avg:57.93ms
step:1923/2330 train_time:111407ms step_avg:57.93ms
step:1924/2330 train_time:111467ms step_avg:57.93ms
step:1925/2330 train_time:111524ms step_avg:57.93ms
step:1926/2330 train_time:111585ms step_avg:57.94ms
step:1927/2330 train_time:111641ms step_avg:57.93ms
step:1928/2330 train_time:111703ms step_avg:57.94ms
step:1929/2330 train_time:111760ms step_avg:57.94ms
step:1930/2330 train_time:111820ms step_avg:57.94ms
step:1931/2330 train_time:111877ms step_avg:57.94ms
step:1932/2330 train_time:111936ms step_avg:57.94ms
step:1933/2330 train_time:111993ms step_avg:57.94ms
step:1934/2330 train_time:112054ms step_avg:57.94ms
step:1935/2330 train_time:112111ms step_avg:57.94ms
step:1936/2330 train_time:112171ms step_avg:57.94ms
step:1937/2330 train_time:112228ms step_avg:57.94ms
step:1938/2330 train_time:112288ms step_avg:57.94ms
step:1939/2330 train_time:112345ms step_avg:57.94ms
step:1940/2330 train_time:112405ms step_avg:57.94ms
step:1941/2330 train_time:112462ms step_avg:57.94ms
step:1942/2330 train_time:112522ms step_avg:57.94ms
step:1943/2330 train_time:112579ms step_avg:57.94ms
step:1944/2330 train_time:112639ms step_avg:57.94ms
step:1945/2330 train_time:112696ms step_avg:57.94ms
step:1946/2330 train_time:112756ms step_avg:57.94ms
step:1947/2330 train_time:112813ms step_avg:57.94ms
step:1948/2330 train_time:112874ms step_avg:57.94ms
step:1949/2330 train_time:112930ms step_avg:57.94ms
step:1950/2330 train_time:112991ms step_avg:57.94ms
step:1951/2330 train_time:113048ms step_avg:57.94ms
step:1952/2330 train_time:113108ms step_avg:57.94ms
step:1953/2330 train_time:113165ms step_avg:57.94ms
step:1954/2330 train_time:113225ms step_avg:57.95ms
step:1955/2330 train_time:113283ms step_avg:57.95ms
step:1956/2330 train_time:113342ms step_avg:57.95ms
step:1957/2330 train_time:113399ms step_avg:57.95ms
step:1958/2330 train_time:113459ms step_avg:57.95ms
step:1959/2330 train_time:113516ms step_avg:57.95ms
step:1960/2330 train_time:113576ms step_avg:57.95ms
step:1961/2330 train_time:113632ms step_avg:57.95ms
step:1962/2330 train_time:113693ms step_avg:57.95ms
step:1963/2330 train_time:113751ms step_avg:57.95ms
step:1964/2330 train_time:113811ms step_avg:57.95ms
step:1965/2330 train_time:113868ms step_avg:57.95ms
step:1966/2330 train_time:113929ms step_avg:57.95ms
step:1967/2330 train_time:113987ms step_avg:57.95ms
step:1968/2330 train_time:114047ms step_avg:57.95ms
step:1969/2330 train_time:114103ms step_avg:57.95ms
step:1970/2330 train_time:114163ms step_avg:57.95ms
step:1971/2330 train_time:114220ms step_avg:57.95ms
step:1972/2330 train_time:114280ms step_avg:57.95ms
step:1973/2330 train_time:114338ms step_avg:57.95ms
step:1974/2330 train_time:114398ms step_avg:57.95ms
step:1975/2330 train_time:114454ms step_avg:57.95ms
step:1976/2330 train_time:114515ms step_avg:57.95ms
step:1977/2330 train_time:114572ms step_avg:57.95ms
step:1978/2330 train_time:114632ms step_avg:57.95ms
step:1979/2330 train_time:114690ms step_avg:57.95ms
step:1980/2330 train_time:114749ms step_avg:57.95ms
step:1981/2330 train_time:114806ms step_avg:57.95ms
step:1982/2330 train_time:114867ms step_avg:57.96ms
step:1983/2330 train_time:114924ms step_avg:57.95ms
step:1984/2330 train_time:114985ms step_avg:57.96ms
step:1985/2330 train_time:115042ms step_avg:57.96ms
step:1986/2330 train_time:115103ms step_avg:57.96ms
step:1987/2330 train_time:115160ms step_avg:57.96ms
step:1988/2330 train_time:115220ms step_avg:57.96ms
step:1989/2330 train_time:115277ms step_avg:57.96ms
step:1990/2330 train_time:115337ms step_avg:57.96ms
step:1991/2330 train_time:115393ms step_avg:57.96ms
step:1992/2330 train_time:115454ms step_avg:57.96ms
step:1993/2330 train_time:115511ms step_avg:57.96ms
step:1994/2330 train_time:115572ms step_avg:57.96ms
step:1995/2330 train_time:115629ms step_avg:57.96ms
step:1996/2330 train_time:115690ms step_avg:57.96ms
step:1997/2330 train_time:115748ms step_avg:57.96ms
step:1998/2330 train_time:115807ms step_avg:57.96ms
step:1999/2330 train_time:115864ms step_avg:57.96ms
step:2000/2330 train_time:115924ms step_avg:57.96ms
step:2000/2330 val_loss:4.3226 train_time:116006ms step_avg:58.00ms
step:2001/2330 train_time:116025ms step_avg:57.98ms
step:2002/2330 train_time:116045ms step_avg:57.96ms
step:2003/2330 train_time:116108ms step_avg:57.97ms
step:2004/2330 train_time:116172ms step_avg:57.97ms
step:2005/2330 train_time:116231ms step_avg:57.97ms
step:2006/2330 train_time:116292ms step_avg:57.97ms
step:2007/2330 train_time:116348ms step_avg:57.97ms
step:2008/2330 train_time:116409ms step_avg:57.97ms
step:2009/2330 train_time:116465ms step_avg:57.97ms
step:2010/2330 train_time:116526ms step_avg:57.97ms
step:2011/2330 train_time:116583ms step_avg:57.97ms
step:2012/2330 train_time:116643ms step_avg:57.97ms
step:2013/2330 train_time:116699ms step_avg:57.97ms
step:2014/2330 train_time:116758ms step_avg:57.97ms
step:2015/2330 train_time:116815ms step_avg:57.97ms
step:2016/2330 train_time:116874ms step_avg:57.97ms
step:2017/2330 train_time:116930ms step_avg:57.97ms
step:2018/2330 train_time:116991ms step_avg:57.97ms
step:2019/2330 train_time:117050ms step_avg:57.97ms
step:2020/2330 train_time:117111ms step_avg:57.98ms
step:2021/2330 train_time:117170ms step_avg:57.98ms
step:2022/2330 train_time:117230ms step_avg:57.98ms
step:2023/2330 train_time:117288ms step_avg:57.98ms
step:2024/2330 train_time:117348ms step_avg:57.98ms
step:2025/2330 train_time:117405ms step_avg:57.98ms
step:2026/2330 train_time:117466ms step_avg:57.98ms
step:2027/2330 train_time:117523ms step_avg:57.98ms
step:2028/2330 train_time:117583ms step_avg:57.98ms
step:2029/2330 train_time:117639ms step_avg:57.98ms
step:2030/2330 train_time:117699ms step_avg:57.98ms
step:2031/2330 train_time:117756ms step_avg:57.98ms
step:2032/2330 train_time:117815ms step_avg:57.98ms
step:2033/2330 train_time:117871ms step_avg:57.98ms
step:2034/2330 train_time:117931ms step_avg:57.98ms
step:2035/2330 train_time:117988ms step_avg:57.98ms
step:2036/2330 train_time:118049ms step_avg:57.98ms
step:2037/2330 train_time:118107ms step_avg:57.98ms
step:2038/2330 train_time:118167ms step_avg:57.98ms
step:2039/2330 train_time:118225ms step_avg:57.98ms
step:2040/2330 train_time:118285ms step_avg:57.98ms
step:2041/2330 train_time:118342ms step_avg:57.98ms
step:2042/2330 train_time:118403ms step_avg:57.98ms
step:2043/2330 train_time:118460ms step_avg:57.98ms
step:2044/2330 train_time:118520ms step_avg:57.98ms
step:2045/2330 train_time:118577ms step_avg:57.98ms
step:2046/2330 train_time:118638ms step_avg:57.99ms
step:2047/2330 train_time:118694ms step_avg:57.98ms
step:2048/2330 train_time:118754ms step_avg:57.99ms
step:2049/2330 train_time:118811ms step_avg:57.98ms
step:2050/2330 train_time:118871ms step_avg:57.99ms
step:2051/2330 train_time:118927ms step_avg:57.98ms
step:2052/2330 train_time:118987ms step_avg:57.99ms
step:2053/2330 train_time:119045ms step_avg:57.99ms
step:2054/2330 train_time:119105ms step_avg:57.99ms
step:2055/2330 train_time:119162ms step_avg:57.99ms
step:2056/2330 train_time:119223ms step_avg:57.99ms
step:2057/2330 train_time:119281ms step_avg:57.99ms
step:2058/2330 train_time:119341ms step_avg:57.99ms
step:2059/2330 train_time:119398ms step_avg:57.99ms
step:2060/2330 train_time:119458ms step_avg:57.99ms
step:2061/2330 train_time:119515ms step_avg:57.99ms
step:2062/2330 train_time:119576ms step_avg:57.99ms
step:2063/2330 train_time:119633ms step_avg:57.99ms
step:2064/2330 train_time:119693ms step_avg:57.99ms
step:2065/2330 train_time:119749ms step_avg:57.99ms
step:2066/2330 train_time:119810ms step_avg:57.99ms
step:2067/2330 train_time:119867ms step_avg:57.99ms
step:2068/2330 train_time:119927ms step_avg:57.99ms
step:2069/2330 train_time:119984ms step_avg:57.99ms
step:2070/2330 train_time:120043ms step_avg:57.99ms
step:2071/2330 train_time:120100ms step_avg:57.99ms
step:2072/2330 train_time:120161ms step_avg:57.99ms
step:2073/2330 train_time:120219ms step_avg:57.99ms
step:2074/2330 train_time:120280ms step_avg:57.99ms
step:2075/2330 train_time:120337ms step_avg:57.99ms
step:2076/2330 train_time:120397ms step_avg:57.99ms
step:2077/2330 train_time:120454ms step_avg:57.99ms
step:2078/2330 train_time:120514ms step_avg:58.00ms
step:2079/2330 train_time:120571ms step_avg:57.99ms
step:2080/2330 train_time:120631ms step_avg:58.00ms
step:2081/2330 train_time:120688ms step_avg:58.00ms
step:2082/2330 train_time:120749ms step_avg:58.00ms
step:2083/2330 train_time:120806ms step_avg:58.00ms
step:2084/2330 train_time:120865ms step_avg:58.00ms
step:2085/2330 train_time:120922ms step_avg:58.00ms
step:2086/2330 train_time:120983ms step_avg:58.00ms
step:2087/2330 train_time:121040ms step_avg:58.00ms
step:2088/2330 train_time:121100ms step_avg:58.00ms
step:2089/2330 train_time:121157ms step_avg:58.00ms
step:2090/2330 train_time:121218ms step_avg:58.00ms
step:2091/2330 train_time:121274ms step_avg:58.00ms
step:2092/2330 train_time:121335ms step_avg:58.00ms
step:2093/2330 train_time:121392ms step_avg:58.00ms
step:2094/2330 train_time:121453ms step_avg:58.00ms
step:2095/2330 train_time:121510ms step_avg:58.00ms
step:2096/2330 train_time:121570ms step_avg:58.00ms
step:2097/2330 train_time:121627ms step_avg:58.00ms
step:2098/2330 train_time:121687ms step_avg:58.00ms
step:2099/2330 train_time:121744ms step_avg:58.00ms
step:2100/2330 train_time:121804ms step_avg:58.00ms
step:2101/2330 train_time:121861ms step_avg:58.00ms
step:2102/2330 train_time:121921ms step_avg:58.00ms
step:2103/2330 train_time:121978ms step_avg:58.00ms
step:2104/2330 train_time:122038ms step_avg:58.00ms
step:2105/2330 train_time:122096ms step_avg:58.00ms
step:2106/2330 train_time:122155ms step_avg:58.00ms
step:2107/2330 train_time:122212ms step_avg:58.00ms
step:2108/2330 train_time:122273ms step_avg:58.00ms
step:2109/2330 train_time:122331ms step_avg:58.00ms
step:2110/2330 train_time:122391ms step_avg:58.01ms
step:2111/2330 train_time:122448ms step_avg:58.00ms
step:2112/2330 train_time:122509ms step_avg:58.01ms
step:2113/2330 train_time:122567ms step_avg:58.01ms
step:2114/2330 train_time:122626ms step_avg:58.01ms
step:2115/2330 train_time:122684ms step_avg:58.01ms
step:2116/2330 train_time:122744ms step_avg:58.01ms
step:2117/2330 train_time:122801ms step_avg:58.01ms
step:2118/2330 train_time:122861ms step_avg:58.01ms
step:2119/2330 train_time:122917ms step_avg:58.01ms
step:2120/2330 train_time:122977ms step_avg:58.01ms
step:2121/2330 train_time:123033ms step_avg:58.01ms
step:2122/2330 train_time:123094ms step_avg:58.01ms
step:2123/2330 train_time:123151ms step_avg:58.01ms
step:2124/2330 train_time:123213ms step_avg:58.01ms
step:2125/2330 train_time:123270ms step_avg:58.01ms
step:2126/2330 train_time:123330ms step_avg:58.01ms
step:2127/2330 train_time:123387ms step_avg:58.01ms
step:2128/2330 train_time:123448ms step_avg:58.01ms
step:2129/2330 train_time:123505ms step_avg:58.01ms
step:2130/2330 train_time:123566ms step_avg:58.01ms
step:2131/2330 train_time:123624ms step_avg:58.01ms
step:2132/2330 train_time:123684ms step_avg:58.01ms
step:2133/2330 train_time:123741ms step_avg:58.01ms
step:2134/2330 train_time:123801ms step_avg:58.01ms
step:2135/2330 train_time:123858ms step_avg:58.01ms
step:2136/2330 train_time:123918ms step_avg:58.01ms
step:2137/2330 train_time:123974ms step_avg:58.01ms
step:2138/2330 train_time:124035ms step_avg:58.01ms
step:2139/2330 train_time:124092ms step_avg:58.01ms
step:2140/2330 train_time:124153ms step_avg:58.02ms
step:2141/2330 train_time:124210ms step_avg:58.01ms
step:2142/2330 train_time:124270ms step_avg:58.02ms
step:2143/2330 train_time:124328ms step_avg:58.02ms
step:2144/2330 train_time:124388ms step_avg:58.02ms
step:2145/2330 train_time:124445ms step_avg:58.02ms
step:2146/2330 train_time:124505ms step_avg:58.02ms
step:2147/2330 train_time:124562ms step_avg:58.02ms
step:2148/2330 train_time:124624ms step_avg:58.02ms
step:2149/2330 train_time:124681ms step_avg:58.02ms
step:2150/2330 train_time:124741ms step_avg:58.02ms
step:2151/2330 train_time:124798ms step_avg:58.02ms
step:2152/2330 train_time:124858ms step_avg:58.02ms
step:2153/2330 train_time:124915ms step_avg:58.02ms
step:2154/2330 train_time:124974ms step_avg:58.02ms
step:2155/2330 train_time:125031ms step_avg:58.02ms
step:2156/2330 train_time:125092ms step_avg:58.02ms
step:2157/2330 train_time:125148ms step_avg:58.02ms
step:2158/2330 train_time:125210ms step_avg:58.02ms
step:2159/2330 train_time:125267ms step_avg:58.02ms
step:2160/2330 train_time:125328ms step_avg:58.02ms
step:2161/2330 train_time:125385ms step_avg:58.02ms
step:2162/2330 train_time:125445ms step_avg:58.02ms
step:2163/2330 train_time:125502ms step_avg:58.02ms
step:2164/2330 train_time:125563ms step_avg:58.02ms
step:2165/2330 train_time:125621ms step_avg:58.02ms
step:2166/2330 train_time:125680ms step_avg:58.02ms
step:2167/2330 train_time:125737ms step_avg:58.02ms
step:2168/2330 train_time:125797ms step_avg:58.02ms
step:2169/2330 train_time:125853ms step_avg:58.02ms
step:2170/2330 train_time:125914ms step_avg:58.02ms
step:2171/2330 train_time:125971ms step_avg:58.02ms
step:2172/2330 train_time:126031ms step_avg:58.03ms
step:2173/2330 train_time:126088ms step_avg:58.02ms
step:2174/2330 train_time:126149ms step_avg:58.03ms
step:2175/2330 train_time:126205ms step_avg:58.03ms
step:2176/2330 train_time:126266ms step_avg:58.03ms
step:2177/2330 train_time:126323ms step_avg:58.03ms
step:2178/2330 train_time:126383ms step_avg:58.03ms
step:2179/2330 train_time:126440ms step_avg:58.03ms
step:2180/2330 train_time:126502ms step_avg:58.03ms
step:2181/2330 train_time:126559ms step_avg:58.03ms
step:2182/2330 train_time:126619ms step_avg:58.03ms
step:2183/2330 train_time:126677ms step_avg:58.03ms
step:2184/2330 train_time:126736ms step_avg:58.03ms
step:2185/2330 train_time:126794ms step_avg:58.03ms
step:2186/2330 train_time:126854ms step_avg:58.03ms
step:2187/2330 train_time:126911ms step_avg:58.03ms
step:2188/2330 train_time:126971ms step_avg:58.03ms
step:2189/2330 train_time:127028ms step_avg:58.03ms
step:2190/2330 train_time:127089ms step_avg:58.03ms
step:2191/2330 train_time:127146ms step_avg:58.03ms
step:2192/2330 train_time:127207ms step_avg:58.03ms
step:2193/2330 train_time:127264ms step_avg:58.03ms
step:2194/2330 train_time:127325ms step_avg:58.03ms
step:2195/2330 train_time:127382ms step_avg:58.03ms
step:2196/2330 train_time:127442ms step_avg:58.03ms
step:2197/2330 train_time:127499ms step_avg:58.03ms
step:2198/2330 train_time:127561ms step_avg:58.03ms
step:2199/2330 train_time:127618ms step_avg:58.03ms
step:2200/2330 train_time:127678ms step_avg:58.04ms
step:2201/2330 train_time:127735ms step_avg:58.03ms
step:2202/2330 train_time:127796ms step_avg:58.04ms
step:2203/2330 train_time:127852ms step_avg:58.04ms
step:2204/2330 train_time:127913ms step_avg:58.04ms
step:2205/2330 train_time:127970ms step_avg:58.04ms
step:2206/2330 train_time:128030ms step_avg:58.04ms
step:2207/2330 train_time:128087ms step_avg:58.04ms
step:2208/2330 train_time:128147ms step_avg:58.04ms
step:2209/2330 train_time:128205ms step_avg:58.04ms
step:2210/2330 train_time:128266ms step_avg:58.04ms
step:2211/2330 train_time:128324ms step_avg:58.04ms
step:2212/2330 train_time:128385ms step_avg:58.04ms
step:2213/2330 train_time:128442ms step_avg:58.04ms
step:2214/2330 train_time:128505ms step_avg:58.04ms
step:2215/2330 train_time:128562ms step_avg:58.04ms
step:2216/2330 train_time:128622ms step_avg:58.04ms
step:2217/2330 train_time:128679ms step_avg:58.04ms
step:2218/2330 train_time:128742ms step_avg:58.04ms
step:2219/2330 train_time:128798ms step_avg:58.04ms
step:2220/2330 train_time:128859ms step_avg:58.04ms
step:2221/2330 train_time:128915ms step_avg:58.04ms
step:2222/2330 train_time:128976ms step_avg:58.04ms
step:2223/2330 train_time:129032ms step_avg:58.04ms
step:2224/2330 train_time:129094ms step_avg:58.05ms
step:2225/2330 train_time:129150ms step_avg:58.05ms
step:2226/2330 train_time:129212ms step_avg:58.05ms
step:2227/2330 train_time:129269ms step_avg:58.05ms
step:2228/2330 train_time:129329ms step_avg:58.05ms
step:2229/2330 train_time:129386ms step_avg:58.05ms
step:2230/2330 train_time:129448ms step_avg:58.05ms
step:2231/2330 train_time:129506ms step_avg:58.05ms
step:2232/2330 train_time:129566ms step_avg:58.05ms
step:2233/2330 train_time:129624ms step_avg:58.05ms
step:2234/2330 train_time:129684ms step_avg:58.05ms
step:2235/2330 train_time:129741ms step_avg:58.05ms
step:2236/2330 train_time:129803ms step_avg:58.05ms
step:2237/2330 train_time:129859ms step_avg:58.05ms
step:2238/2330 train_time:129920ms step_avg:58.05ms
step:2239/2330 train_time:129977ms step_avg:58.05ms
step:2240/2330 train_time:130037ms step_avg:58.05ms
step:2241/2330 train_time:130093ms step_avg:58.05ms
step:2242/2330 train_time:130155ms step_avg:58.05ms
step:2243/2330 train_time:130212ms step_avg:58.05ms
step:2244/2330 train_time:130273ms step_avg:58.05ms
step:2245/2330 train_time:130330ms step_avg:58.05ms
step:2246/2330 train_time:130391ms step_avg:58.05ms
step:2247/2330 train_time:130448ms step_avg:58.05ms
step:2248/2330 train_time:130510ms step_avg:58.06ms
step:2249/2330 train_time:130568ms step_avg:58.06ms
step:2250/2330 train_time:130629ms step_avg:58.06ms
step:2250/2330 val_loss:4.5675 train_time:130712ms step_avg:58.09ms
step:2251/2330 train_time:130731ms step_avg:58.08ms
step:2252/2330 train_time:130752ms step_avg:58.06ms
step:2253/2330 train_time:130813ms step_avg:58.06ms
step:2254/2330 train_time:130878ms step_avg:58.06ms
step:2255/2330 train_time:130936ms step_avg:58.06ms
step:2256/2330 train_time:130997ms step_avg:58.07ms
step:2257/2330 train_time:131054ms step_avg:58.07ms
step:2258/2330 train_time:131114ms step_avg:58.07ms
step:2259/2330 train_time:131171ms step_avg:58.07ms
step:2260/2330 train_time:131231ms step_avg:58.07ms
step:2261/2330 train_time:131288ms step_avg:58.07ms
step:2262/2330 train_time:131348ms step_avg:58.07ms
step:2263/2330 train_time:131404ms step_avg:58.07ms
step:2264/2330 train_time:131464ms step_avg:58.07ms
step:2265/2330 train_time:131520ms step_avg:58.07ms
step:2266/2330 train_time:131580ms step_avg:58.07ms
step:2267/2330 train_time:131637ms step_avg:58.07ms
step:2268/2330 train_time:131698ms step_avg:58.07ms
step:2269/2330 train_time:131756ms step_avg:58.07ms
step:2270/2330 train_time:131819ms step_avg:58.07ms
step:2271/2330 train_time:131877ms step_avg:58.07ms
step:2272/2330 train_time:131939ms step_avg:58.07ms
step:2273/2330 train_time:131996ms step_avg:58.07ms
step:2274/2330 train_time:132057ms step_avg:58.07ms
step:2275/2330 train_time:132114ms step_avg:58.07ms
step:2276/2330 train_time:132174ms step_avg:58.07ms
step:2277/2330 train_time:132231ms step_avg:58.07ms
step:2278/2330 train_time:132291ms step_avg:58.07ms
step:2279/2330 train_time:132349ms step_avg:58.07ms
step:2280/2330 train_time:132408ms step_avg:58.07ms
step:2281/2330 train_time:132465ms step_avg:58.07ms
step:2282/2330 train_time:132524ms step_avg:58.07ms
step:2283/2330 train_time:132581ms step_avg:58.07ms
step:2284/2330 train_time:132642ms step_avg:58.07ms
step:2285/2330 train_time:132699ms step_avg:58.07ms
step:2286/2330 train_time:132761ms step_avg:58.08ms
step:2287/2330 train_time:132819ms step_avg:58.08ms
step:2288/2330 train_time:132881ms step_avg:58.08ms
step:2289/2330 train_time:132938ms step_avg:58.08ms
step:2290/2330 train_time:133000ms step_avg:58.08ms
step:2291/2330 train_time:133057ms step_avg:58.08ms
step:2292/2330 train_time:133119ms step_avg:58.08ms
step:2293/2330 train_time:133175ms step_avg:58.08ms
step:2294/2330 train_time:133237ms step_avg:58.08ms
step:2295/2330 train_time:133295ms step_avg:58.08ms
step:2296/2330 train_time:133355ms step_avg:58.08ms
step:2297/2330 train_time:133413ms step_avg:58.08ms
step:2298/2330 train_time:133473ms step_avg:58.08ms
step:2299/2330 train_time:133530ms step_avg:58.08ms
step:2300/2330 train_time:133590ms step_avg:58.08ms
step:2301/2330 train_time:133647ms step_avg:58.08ms
step:2302/2330 train_time:133708ms step_avg:58.08ms
step:2303/2330 train_time:133765ms step_avg:58.08ms
step:2304/2330 train_time:133828ms step_avg:58.08ms
step:2305/2330 train_time:133885ms step_avg:58.08ms
step:2306/2330 train_time:133946ms step_avg:58.09ms
step:2307/2330 train_time:134003ms step_avg:58.09ms
step:2308/2330 train_time:134063ms step_avg:58.09ms
step:2309/2330 train_time:134120ms step_avg:58.09ms
step:2310/2330 train_time:134181ms step_avg:58.09ms
step:2311/2330 train_time:134238ms step_avg:58.09ms
step:2312/2330 train_time:134300ms step_avg:58.09ms
step:2313/2330 train_time:134357ms step_avg:58.09ms
step:2314/2330 train_time:134418ms step_avg:58.09ms
step:2315/2330 train_time:134475ms step_avg:58.09ms
step:2316/2330 train_time:134535ms step_avg:58.09ms
step:2317/2330 train_time:134593ms step_avg:58.09ms
step:2318/2330 train_time:134653ms step_avg:58.09ms
step:2319/2330 train_time:134710ms step_avg:58.09ms
step:2320/2330 train_time:134772ms step_avg:58.09ms
step:2321/2330 train_time:134830ms step_avg:58.09ms
step:2322/2330 train_time:134891ms step_avg:58.09ms
step:2323/2330 train_time:134950ms step_avg:58.09ms
step:2324/2330 train_time:135010ms step_avg:58.09ms
step:2325/2330 train_time:135067ms step_avg:58.09ms
step:2326/2330 train_time:135128ms step_avg:58.09ms
step:2327/2330 train_time:135185ms step_avg:58.09ms
step:2328/2330 train_time:135247ms step_avg:58.10ms
step:2329/2330 train_time:135303ms step_avg:58.10ms
step:2330/2330 train_time:135365ms step_avg:58.10ms
step:2330/2330 val_loss:4.4167 train_time:135446ms step_avg:58.13ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
