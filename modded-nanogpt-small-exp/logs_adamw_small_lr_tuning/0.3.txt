import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--adamw_lr', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_lr_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_lr_tuning/{args2.adamw_lr}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=float(args2.adamw_lr),  betas=(0.85, 0.85), eps=1e-10, weight_decay=0.0, fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_lr_tuning/{args2.adamw_lr}", exist_ok=True)
            torch.save(log, f"logs_adamw_small_lr_tuning/{args2.adamw_lr}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 03:01:43 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   36C    P0             117W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   35C    P0             117W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             118W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   28C    P0             115W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   34C    P0             118W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             114W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:85ms step_avg:84.89ms
step:2/2330 train_time:183ms step_avg:91.28ms
step:3/2330 train_time:200ms step_avg:66.79ms
step:4/2330 train_time:220ms step_avg:54.88ms
step:5/2330 train_time:273ms step_avg:54.60ms
step:6/2330 train_time:330ms step_avg:55.06ms
step:7/2330 train_time:384ms step_avg:54.89ms
step:8/2330 train_time:441ms step_avg:55.13ms
step:9/2330 train_time:495ms step_avg:54.97ms
step:10/2330 train_time:551ms step_avg:55.14ms
step:11/2330 train_time:605ms step_avg:55.04ms
step:12/2330 train_time:662ms step_avg:55.18ms
step:13/2330 train_time:716ms step_avg:55.08ms
step:14/2330 train_time:773ms step_avg:55.18ms
step:15/2330 train_time:826ms step_avg:55.10ms
step:16/2330 train_time:884ms step_avg:55.22ms
step:17/2330 train_time:937ms step_avg:55.14ms
step:18/2330 train_time:994ms step_avg:55.23ms
step:19/2330 train_time:1048ms step_avg:55.14ms
step:20/2330 train_time:1105ms step_avg:55.25ms
step:21/2330 train_time:1160ms step_avg:55.23ms
step:22/2330 train_time:1218ms step_avg:55.38ms
step:23/2330 train_time:1273ms step_avg:55.33ms
step:24/2330 train_time:1331ms step_avg:55.47ms
step:25/2330 train_time:1385ms step_avg:55.41ms
step:26/2330 train_time:1443ms step_avg:55.50ms
step:27/2330 train_time:1497ms step_avg:55.45ms
step:28/2330 train_time:1554ms step_avg:55.50ms
step:29/2330 train_time:1608ms step_avg:55.44ms
step:30/2330 train_time:1665ms step_avg:55.50ms
step:31/2330 train_time:1719ms step_avg:55.46ms
step:32/2330 train_time:1777ms step_avg:55.53ms
step:33/2330 train_time:1831ms step_avg:55.48ms
step:34/2330 train_time:1888ms step_avg:55.54ms
step:35/2330 train_time:1943ms step_avg:55.51ms
step:36/2330 train_time:2000ms step_avg:55.55ms
step:37/2330 train_time:2054ms step_avg:55.52ms
step:38/2330 train_time:2111ms step_avg:55.56ms
step:39/2330 train_time:2166ms step_avg:55.54ms
step:40/2330 train_time:2224ms step_avg:55.59ms
step:41/2330 train_time:2278ms step_avg:55.57ms
step:42/2330 train_time:2336ms step_avg:55.63ms
step:43/2330 train_time:2391ms step_avg:55.59ms
step:44/2330 train_time:2449ms step_avg:55.65ms
step:45/2330 train_time:2503ms step_avg:55.63ms
step:46/2330 train_time:2560ms step_avg:55.66ms
step:47/2330 train_time:2614ms step_avg:55.62ms
step:48/2330 train_time:2673ms step_avg:55.68ms
step:49/2330 train_time:2727ms step_avg:55.66ms
step:50/2330 train_time:2785ms step_avg:55.69ms
step:51/2330 train_time:2839ms step_avg:55.67ms
step:52/2330 train_time:2896ms step_avg:55.70ms
step:53/2330 train_time:2951ms step_avg:55.68ms
step:54/2330 train_time:3009ms step_avg:55.72ms
step:55/2330 train_time:3064ms step_avg:55.71ms
step:56/2330 train_time:3121ms step_avg:55.73ms
step:57/2330 train_time:3175ms step_avg:55.70ms
step:58/2330 train_time:3235ms step_avg:55.78ms
step:59/2330 train_time:3290ms step_avg:55.76ms
step:60/2330 train_time:3348ms step_avg:55.79ms
step:61/2330 train_time:3402ms step_avg:55.77ms
step:62/2330 train_time:3461ms step_avg:55.82ms
step:63/2330 train_time:3515ms step_avg:55.79ms
step:64/2330 train_time:3573ms step_avg:55.83ms
step:65/2330 train_time:3627ms step_avg:55.80ms
step:66/2330 train_time:3686ms step_avg:55.85ms
step:67/2330 train_time:3740ms step_avg:55.82ms
step:68/2330 train_time:3798ms step_avg:55.86ms
step:69/2330 train_time:3853ms step_avg:55.84ms
step:70/2330 train_time:3911ms step_avg:55.87ms
step:71/2330 train_time:3965ms step_avg:55.85ms
step:72/2330 train_time:4024ms step_avg:55.88ms
step:73/2330 train_time:4078ms step_avg:55.87ms
step:74/2330 train_time:4137ms step_avg:55.91ms
step:75/2330 train_time:4192ms step_avg:55.90ms
step:76/2330 train_time:4250ms step_avg:55.92ms
step:77/2330 train_time:4305ms step_avg:55.91ms
step:78/2330 train_time:4363ms step_avg:55.93ms
step:79/2330 train_time:4418ms step_avg:55.92ms
step:80/2330 train_time:4475ms step_avg:55.94ms
step:81/2330 train_time:4530ms step_avg:55.93ms
step:82/2330 train_time:4589ms step_avg:55.96ms
step:83/2330 train_time:4644ms step_avg:55.95ms
step:84/2330 train_time:4702ms step_avg:55.97ms
step:85/2330 train_time:4757ms step_avg:55.97ms
step:86/2330 train_time:4814ms step_avg:55.98ms
step:87/2330 train_time:4869ms step_avg:55.96ms
step:88/2330 train_time:4927ms step_avg:55.99ms
step:89/2330 train_time:4982ms step_avg:55.98ms
step:90/2330 train_time:5041ms step_avg:56.01ms
step:91/2330 train_time:5096ms step_avg:56.00ms
step:92/2330 train_time:5155ms step_avg:56.03ms
step:93/2330 train_time:5209ms step_avg:56.02ms
step:94/2330 train_time:5267ms step_avg:56.03ms
step:95/2330 train_time:5322ms step_avg:56.02ms
step:96/2330 train_time:5381ms step_avg:56.05ms
step:97/2330 train_time:5436ms step_avg:56.04ms
step:98/2330 train_time:5494ms step_avg:56.06ms
step:99/2330 train_time:5549ms step_avg:56.05ms
step:100/2330 train_time:5607ms step_avg:56.07ms
step:101/2330 train_time:5662ms step_avg:56.06ms
step:102/2330 train_time:5721ms step_avg:56.09ms
step:103/2330 train_time:5776ms step_avg:56.08ms
step:104/2330 train_time:5834ms step_avg:56.10ms
step:105/2330 train_time:5890ms step_avg:56.09ms
step:106/2330 train_time:5947ms step_avg:56.11ms
step:107/2330 train_time:6003ms step_avg:56.10ms
step:108/2330 train_time:6061ms step_avg:56.12ms
step:109/2330 train_time:6116ms step_avg:56.11ms
step:110/2330 train_time:6174ms step_avg:56.13ms
step:111/2330 train_time:6229ms step_avg:56.12ms
step:112/2330 train_time:6288ms step_avg:56.14ms
step:113/2330 train_time:6343ms step_avg:56.13ms
step:114/2330 train_time:6401ms step_avg:56.15ms
step:115/2330 train_time:6456ms step_avg:56.14ms
step:116/2330 train_time:6514ms step_avg:56.16ms
step:117/2330 train_time:6569ms step_avg:56.15ms
step:118/2330 train_time:6628ms step_avg:56.17ms
step:119/2330 train_time:6683ms step_avg:56.16ms
step:120/2330 train_time:6742ms step_avg:56.18ms
step:121/2330 train_time:6797ms step_avg:56.17ms
step:122/2330 train_time:6855ms step_avg:56.18ms
step:123/2330 train_time:6909ms step_avg:56.17ms
step:124/2330 train_time:6967ms step_avg:56.19ms
step:125/2330 train_time:7022ms step_avg:56.18ms
step:126/2330 train_time:7082ms step_avg:56.20ms
step:127/2330 train_time:7136ms step_avg:56.19ms
step:128/2330 train_time:7194ms step_avg:56.20ms
step:129/2330 train_time:7249ms step_avg:56.20ms
step:130/2330 train_time:7307ms step_avg:56.21ms
step:131/2330 train_time:7363ms step_avg:56.21ms
step:132/2330 train_time:7421ms step_avg:56.22ms
step:133/2330 train_time:7476ms step_avg:56.21ms
step:134/2330 train_time:7535ms step_avg:56.23ms
step:135/2330 train_time:7590ms step_avg:56.22ms
step:136/2330 train_time:7649ms step_avg:56.24ms
step:137/2330 train_time:7704ms step_avg:56.23ms
step:138/2330 train_time:7762ms step_avg:56.25ms
step:139/2330 train_time:7817ms step_avg:56.24ms
step:140/2330 train_time:7876ms step_avg:56.26ms
step:141/2330 train_time:7931ms step_avg:56.25ms
step:142/2330 train_time:7991ms step_avg:56.27ms
step:143/2330 train_time:8046ms step_avg:56.27ms
step:144/2330 train_time:8104ms step_avg:56.28ms
step:145/2330 train_time:8159ms step_avg:56.27ms
step:146/2330 train_time:8217ms step_avg:56.28ms
step:147/2330 train_time:8272ms step_avg:56.27ms
step:148/2330 train_time:8331ms step_avg:56.29ms
step:149/2330 train_time:8386ms step_avg:56.28ms
step:150/2330 train_time:8445ms step_avg:56.30ms
step:151/2330 train_time:8500ms step_avg:56.29ms
step:152/2330 train_time:8558ms step_avg:56.30ms
step:153/2330 train_time:8613ms step_avg:56.29ms
step:154/2330 train_time:8671ms step_avg:56.30ms
step:155/2330 train_time:8726ms step_avg:56.30ms
step:156/2330 train_time:8785ms step_avg:56.31ms
step:157/2330 train_time:8841ms step_avg:56.31ms
step:158/2330 train_time:8899ms step_avg:56.32ms
step:159/2330 train_time:8953ms step_avg:56.31ms
step:160/2330 train_time:9013ms step_avg:56.33ms
step:161/2330 train_time:9067ms step_avg:56.32ms
step:162/2330 train_time:9127ms step_avg:56.34ms
step:163/2330 train_time:9182ms step_avg:56.33ms
step:164/2330 train_time:9241ms step_avg:56.35ms
step:165/2330 train_time:9296ms step_avg:56.34ms
step:166/2330 train_time:9355ms step_avg:56.35ms
step:167/2330 train_time:9409ms step_avg:56.34ms
step:168/2330 train_time:9469ms step_avg:56.37ms
step:169/2330 train_time:9524ms step_avg:56.36ms
step:170/2330 train_time:9584ms step_avg:56.37ms
step:171/2330 train_time:9638ms step_avg:56.36ms
step:172/2330 train_time:9698ms step_avg:56.38ms
step:173/2330 train_time:9752ms step_avg:56.37ms
step:174/2330 train_time:9812ms step_avg:56.39ms
step:175/2330 train_time:9867ms step_avg:56.38ms
step:176/2330 train_time:9925ms step_avg:56.39ms
step:177/2330 train_time:9980ms step_avg:56.38ms
step:178/2330 train_time:10040ms step_avg:56.40ms
step:179/2330 train_time:10094ms step_avg:56.39ms
step:180/2330 train_time:10153ms step_avg:56.41ms
step:181/2330 train_time:10208ms step_avg:56.40ms
step:182/2330 train_time:10267ms step_avg:56.41ms
step:183/2330 train_time:10322ms step_avg:56.41ms
step:184/2330 train_time:10381ms step_avg:56.42ms
step:185/2330 train_time:10437ms step_avg:56.41ms
step:186/2330 train_time:10496ms step_avg:56.43ms
step:187/2330 train_time:10551ms step_avg:56.42ms
step:188/2330 train_time:10610ms step_avg:56.44ms
step:189/2330 train_time:10665ms step_avg:56.43ms
step:190/2330 train_time:10724ms step_avg:56.44ms
step:191/2330 train_time:10778ms step_avg:56.43ms
step:192/2330 train_time:10838ms step_avg:56.45ms
step:193/2330 train_time:10893ms step_avg:56.44ms
step:194/2330 train_time:10953ms step_avg:56.46ms
step:195/2330 train_time:11007ms step_avg:56.45ms
step:196/2330 train_time:11068ms step_avg:56.47ms
step:197/2330 train_time:11123ms step_avg:56.46ms
step:198/2330 train_time:11182ms step_avg:56.48ms
step:199/2330 train_time:11237ms step_avg:56.47ms
step:200/2330 train_time:11297ms step_avg:56.49ms
step:201/2330 train_time:11352ms step_avg:56.48ms
step:202/2330 train_time:11412ms step_avg:56.49ms
step:203/2330 train_time:11467ms step_avg:56.49ms
step:204/2330 train_time:11527ms step_avg:56.50ms
step:205/2330 train_time:11582ms step_avg:56.50ms
step:206/2330 train_time:11641ms step_avg:56.51ms
step:207/2330 train_time:11697ms step_avg:56.51ms
step:208/2330 train_time:11755ms step_avg:56.52ms
step:209/2330 train_time:11811ms step_avg:56.51ms
step:210/2330 train_time:11870ms step_avg:56.53ms
step:211/2330 train_time:11926ms step_avg:56.52ms
step:212/2330 train_time:11985ms step_avg:56.53ms
step:213/2330 train_time:12040ms step_avg:56.53ms
step:214/2330 train_time:12100ms step_avg:56.54ms
step:215/2330 train_time:12155ms step_avg:56.53ms
step:216/2330 train_time:12214ms step_avg:56.54ms
step:217/2330 train_time:12269ms step_avg:56.54ms
step:218/2330 train_time:12329ms step_avg:56.55ms
step:219/2330 train_time:12384ms step_avg:56.55ms
step:220/2330 train_time:12444ms step_avg:56.56ms
step:221/2330 train_time:12499ms step_avg:56.56ms
step:222/2330 train_time:12560ms step_avg:56.58ms
step:223/2330 train_time:12615ms step_avg:56.57ms
step:224/2330 train_time:12675ms step_avg:56.59ms
step:225/2330 train_time:12731ms step_avg:56.58ms
step:226/2330 train_time:12789ms step_avg:56.59ms
step:227/2330 train_time:12845ms step_avg:56.59ms
step:228/2330 train_time:12903ms step_avg:56.59ms
step:229/2330 train_time:12959ms step_avg:56.59ms
step:230/2330 train_time:13018ms step_avg:56.60ms
step:231/2330 train_time:13073ms step_avg:56.59ms
step:232/2330 train_time:13132ms step_avg:56.60ms
step:233/2330 train_time:13187ms step_avg:56.60ms
step:234/2330 train_time:13246ms step_avg:56.61ms
step:235/2330 train_time:13302ms step_avg:56.60ms
step:236/2330 train_time:13362ms step_avg:56.62ms
step:237/2330 train_time:13417ms step_avg:56.61ms
step:238/2330 train_time:13477ms step_avg:56.63ms
step:239/2330 train_time:13533ms step_avg:56.62ms
step:240/2330 train_time:13592ms step_avg:56.63ms
step:241/2330 train_time:13648ms step_avg:56.63ms
step:242/2330 train_time:13707ms step_avg:56.64ms
step:243/2330 train_time:13763ms step_avg:56.64ms
step:244/2330 train_time:13822ms step_avg:56.65ms
step:245/2330 train_time:13878ms step_avg:56.64ms
step:246/2330 train_time:13937ms step_avg:56.65ms
step:247/2330 train_time:13992ms step_avg:56.65ms
step:248/2330 train_time:14051ms step_avg:56.66ms
step:249/2330 train_time:14106ms step_avg:56.65ms
step:250/2330 train_time:14166ms step_avg:56.66ms
step:250/2330 val_loss:6.5465 train_time:14244ms step_avg:56.98ms
step:251/2330 train_time:14263ms step_avg:56.82ms
step:252/2330 train_time:14283ms step_avg:56.68ms
step:253/2330 train_time:14338ms step_avg:56.67ms
step:254/2330 train_time:14402ms step_avg:56.70ms
step:255/2330 train_time:14458ms step_avg:56.70ms
step:256/2330 train_time:14517ms step_avg:56.71ms
step:257/2330 train_time:14573ms step_avg:56.70ms
step:258/2330 train_time:14632ms step_avg:56.71ms
step:259/2330 train_time:14687ms step_avg:56.71ms
step:260/2330 train_time:14748ms step_avg:56.72ms
step:261/2330 train_time:14803ms step_avg:56.72ms
step:262/2330 train_time:14862ms step_avg:56.73ms
step:263/2330 train_time:14917ms step_avg:56.72ms
step:264/2330 train_time:14976ms step_avg:56.73ms
step:265/2330 train_time:15032ms step_avg:56.72ms
step:266/2330 train_time:15091ms step_avg:56.73ms
step:267/2330 train_time:15145ms step_avg:56.72ms
step:268/2330 train_time:15206ms step_avg:56.74ms
step:269/2330 train_time:15262ms step_avg:56.74ms
step:270/2330 train_time:15322ms step_avg:56.75ms
step:271/2330 train_time:15377ms step_avg:56.74ms
step:272/2330 train_time:15439ms step_avg:56.76ms
step:273/2330 train_time:15495ms step_avg:56.76ms
step:274/2330 train_time:15554ms step_avg:56.77ms
step:275/2330 train_time:15611ms step_avg:56.77ms
step:276/2330 train_time:15670ms step_avg:56.78ms
step:277/2330 train_time:15726ms step_avg:56.77ms
step:278/2330 train_time:15785ms step_avg:56.78ms
step:279/2330 train_time:15840ms step_avg:56.78ms
step:280/2330 train_time:15899ms step_avg:56.78ms
step:281/2330 train_time:15955ms step_avg:56.78ms
step:282/2330 train_time:16013ms step_avg:56.78ms
step:283/2330 train_time:16069ms step_avg:56.78ms
step:284/2330 train_time:16128ms step_avg:56.79ms
step:285/2330 train_time:16183ms step_avg:56.78ms
step:286/2330 train_time:16242ms step_avg:56.79ms
step:287/2330 train_time:16298ms step_avg:56.79ms
step:288/2330 train_time:16358ms step_avg:56.80ms
step:289/2330 train_time:16414ms step_avg:56.80ms
step:290/2330 train_time:16473ms step_avg:56.80ms
step:291/2330 train_time:16529ms step_avg:56.80ms
step:292/2330 train_time:16589ms step_avg:56.81ms
step:293/2330 train_time:16645ms step_avg:56.81ms
step:294/2330 train_time:16705ms step_avg:56.82ms
step:295/2330 train_time:16760ms step_avg:56.81ms
step:296/2330 train_time:16819ms step_avg:56.82ms
step:297/2330 train_time:16875ms step_avg:56.82ms
step:298/2330 train_time:16934ms step_avg:56.83ms
step:299/2330 train_time:16989ms step_avg:56.82ms
step:300/2330 train_time:17048ms step_avg:56.83ms
step:301/2330 train_time:17104ms step_avg:56.82ms
step:302/2330 train_time:17163ms step_avg:56.83ms
step:303/2330 train_time:17219ms step_avg:56.83ms
step:304/2330 train_time:17278ms step_avg:56.84ms
step:305/2330 train_time:17335ms step_avg:56.84ms
step:306/2330 train_time:17394ms step_avg:56.84ms
step:307/2330 train_time:17451ms step_avg:56.84ms
step:308/2330 train_time:17510ms step_avg:56.85ms
step:309/2330 train_time:17566ms step_avg:56.85ms
step:310/2330 train_time:17626ms step_avg:56.86ms
step:311/2330 train_time:17681ms step_avg:56.85ms
step:312/2330 train_time:17741ms step_avg:56.86ms
step:313/2330 train_time:17797ms step_avg:56.86ms
step:314/2330 train_time:17857ms step_avg:56.87ms
step:315/2330 train_time:17912ms step_avg:56.86ms
step:316/2330 train_time:17971ms step_avg:56.87ms
step:317/2330 train_time:18027ms step_avg:56.87ms
step:318/2330 train_time:18086ms step_avg:56.87ms
step:319/2330 train_time:18141ms step_avg:56.87ms
step:320/2330 train_time:18201ms step_avg:56.88ms
step:321/2330 train_time:18256ms step_avg:56.87ms
step:322/2330 train_time:18316ms step_avg:56.88ms
step:323/2330 train_time:18371ms step_avg:56.88ms
step:324/2330 train_time:18432ms step_avg:56.89ms
step:325/2330 train_time:18487ms step_avg:56.88ms
step:326/2330 train_time:18548ms step_avg:56.90ms
step:327/2330 train_time:18603ms step_avg:56.89ms
step:328/2330 train_time:18664ms step_avg:56.90ms
step:329/2330 train_time:18720ms step_avg:56.90ms
step:330/2330 train_time:18779ms step_avg:56.91ms
step:331/2330 train_time:18835ms step_avg:56.90ms
step:332/2330 train_time:18894ms step_avg:56.91ms
step:333/2330 train_time:18949ms step_avg:56.90ms
step:334/2330 train_time:19009ms step_avg:56.91ms
step:335/2330 train_time:19065ms step_avg:56.91ms
step:336/2330 train_time:19124ms step_avg:56.92ms
step:337/2330 train_time:19180ms step_avg:56.91ms
step:338/2330 train_time:19239ms step_avg:56.92ms
step:339/2330 train_time:19295ms step_avg:56.92ms
step:340/2330 train_time:19353ms step_avg:56.92ms
step:341/2330 train_time:19410ms step_avg:56.92ms
step:342/2330 train_time:19469ms step_avg:56.93ms
step:343/2330 train_time:19526ms step_avg:56.93ms
step:344/2330 train_time:19584ms step_avg:56.93ms
step:345/2330 train_time:19640ms step_avg:56.93ms
step:346/2330 train_time:19700ms step_avg:56.94ms
step:347/2330 train_time:19756ms step_avg:56.93ms
step:348/2330 train_time:19815ms step_avg:56.94ms
step:349/2330 train_time:19870ms step_avg:56.94ms
step:350/2330 train_time:19931ms step_avg:56.94ms
step:351/2330 train_time:19986ms step_avg:56.94ms
step:352/2330 train_time:20046ms step_avg:56.95ms
step:353/2330 train_time:20101ms step_avg:56.94ms
step:354/2330 train_time:20161ms step_avg:56.95ms
step:355/2330 train_time:20217ms step_avg:56.95ms
step:356/2330 train_time:20276ms step_avg:56.95ms
step:357/2330 train_time:20332ms step_avg:56.95ms
step:358/2330 train_time:20391ms step_avg:56.96ms
step:359/2330 train_time:20447ms step_avg:56.96ms
step:360/2330 train_time:20506ms step_avg:56.96ms
step:361/2330 train_time:20562ms step_avg:56.96ms
step:362/2330 train_time:20621ms step_avg:56.97ms
step:363/2330 train_time:20677ms step_avg:56.96ms
step:364/2330 train_time:20736ms step_avg:56.97ms
step:365/2330 train_time:20792ms step_avg:56.96ms
step:366/2330 train_time:20851ms step_avg:56.97ms
step:367/2330 train_time:20907ms step_avg:56.97ms
step:368/2330 train_time:20967ms step_avg:56.97ms
step:369/2330 train_time:21023ms step_avg:56.97ms
step:370/2330 train_time:21081ms step_avg:56.98ms
step:371/2330 train_time:21137ms step_avg:56.97ms
step:372/2330 train_time:21196ms step_avg:56.98ms
step:373/2330 train_time:21252ms step_avg:56.98ms
step:374/2330 train_time:21311ms step_avg:56.98ms
step:375/2330 train_time:21367ms step_avg:56.98ms
step:376/2330 train_time:21427ms step_avg:56.99ms
step:377/2330 train_time:21483ms step_avg:56.98ms
step:378/2330 train_time:21543ms step_avg:56.99ms
step:379/2330 train_time:21598ms step_avg:56.99ms
step:380/2330 train_time:21658ms step_avg:57.00ms
step:381/2330 train_time:21714ms step_avg:56.99ms
step:382/2330 train_time:21773ms step_avg:57.00ms
step:383/2330 train_time:21828ms step_avg:56.99ms
step:384/2330 train_time:21889ms step_avg:57.00ms
step:385/2330 train_time:21944ms step_avg:57.00ms
step:386/2330 train_time:22005ms step_avg:57.01ms
step:387/2330 train_time:22060ms step_avg:57.00ms
step:388/2330 train_time:22120ms step_avg:57.01ms
step:389/2330 train_time:22175ms step_avg:57.01ms
step:390/2330 train_time:22234ms step_avg:57.01ms
step:391/2330 train_time:22290ms step_avg:57.01ms
step:392/2330 train_time:22350ms step_avg:57.01ms
step:393/2330 train_time:22406ms step_avg:57.01ms
step:394/2330 train_time:22466ms step_avg:57.02ms
step:395/2330 train_time:22521ms step_avg:57.02ms
step:396/2330 train_time:22581ms step_avg:57.02ms
step:397/2330 train_time:22636ms step_avg:57.02ms
step:398/2330 train_time:22695ms step_avg:57.02ms
step:399/2330 train_time:22751ms step_avg:57.02ms
step:400/2330 train_time:22811ms step_avg:57.03ms
step:401/2330 train_time:22867ms step_avg:57.03ms
step:402/2330 train_time:22926ms step_avg:57.03ms
step:403/2330 train_time:22981ms step_avg:57.03ms
step:404/2330 train_time:23041ms step_avg:57.03ms
step:405/2330 train_time:23097ms step_avg:57.03ms
step:406/2330 train_time:23156ms step_avg:57.04ms
step:407/2330 train_time:23212ms step_avg:57.03ms
step:408/2330 train_time:23271ms step_avg:57.04ms
step:409/2330 train_time:23326ms step_avg:57.03ms
step:410/2330 train_time:23386ms step_avg:57.04ms
step:411/2330 train_time:23441ms step_avg:57.03ms
step:412/2330 train_time:23501ms step_avg:57.04ms
step:413/2330 train_time:23557ms step_avg:57.04ms
step:414/2330 train_time:23617ms step_avg:57.05ms
step:415/2330 train_time:23672ms step_avg:57.04ms
step:416/2330 train_time:23732ms step_avg:57.05ms
step:417/2330 train_time:23787ms step_avg:57.04ms
step:418/2330 train_time:23847ms step_avg:57.05ms
step:419/2330 train_time:23903ms step_avg:57.05ms
step:420/2330 train_time:23963ms step_avg:57.05ms
step:421/2330 train_time:24018ms step_avg:57.05ms
step:422/2330 train_time:24077ms step_avg:57.06ms
step:423/2330 train_time:24133ms step_avg:57.05ms
step:424/2330 train_time:24193ms step_avg:57.06ms
step:425/2330 train_time:24248ms step_avg:57.05ms
step:426/2330 train_time:24308ms step_avg:57.06ms
step:427/2330 train_time:24364ms step_avg:57.06ms
step:428/2330 train_time:24423ms step_avg:57.06ms
step:429/2330 train_time:24478ms step_avg:57.06ms
step:430/2330 train_time:24537ms step_avg:57.06ms
step:431/2330 train_time:24593ms step_avg:57.06ms
step:432/2330 train_time:24653ms step_avg:57.07ms
step:433/2330 train_time:24709ms step_avg:57.07ms
step:434/2330 train_time:24769ms step_avg:57.07ms
step:435/2330 train_time:24825ms step_avg:57.07ms
step:436/2330 train_time:24884ms step_avg:57.07ms
step:437/2330 train_time:24940ms step_avg:57.07ms
step:438/2330 train_time:25000ms step_avg:57.08ms
step:439/2330 train_time:25055ms step_avg:57.07ms
step:440/2330 train_time:25115ms step_avg:57.08ms
step:441/2330 train_time:25170ms step_avg:57.08ms
step:442/2330 train_time:25230ms step_avg:57.08ms
step:443/2330 train_time:25286ms step_avg:57.08ms
step:444/2330 train_time:25345ms step_avg:57.08ms
step:445/2330 train_time:25401ms step_avg:57.08ms
step:446/2330 train_time:25460ms step_avg:57.09ms
step:447/2330 train_time:25516ms step_avg:57.08ms
step:448/2330 train_time:25575ms step_avg:57.09ms
step:449/2330 train_time:25632ms step_avg:57.09ms
step:450/2330 train_time:25690ms step_avg:57.09ms
step:451/2330 train_time:25746ms step_avg:57.09ms
step:452/2330 train_time:25806ms step_avg:57.09ms
step:453/2330 train_time:25862ms step_avg:57.09ms
step:454/2330 train_time:25921ms step_avg:57.09ms
step:455/2330 train_time:25976ms step_avg:57.09ms
step:456/2330 train_time:26036ms step_avg:57.10ms
step:457/2330 train_time:26092ms step_avg:57.09ms
step:458/2330 train_time:26152ms step_avg:57.10ms
step:459/2330 train_time:26208ms step_avg:57.10ms
step:460/2330 train_time:26268ms step_avg:57.10ms
step:461/2330 train_time:26324ms step_avg:57.10ms
step:462/2330 train_time:26384ms step_avg:57.11ms
step:463/2330 train_time:26440ms step_avg:57.11ms
step:464/2330 train_time:26499ms step_avg:57.11ms
step:465/2330 train_time:26554ms step_avg:57.11ms
step:466/2330 train_time:26614ms step_avg:57.11ms
step:467/2330 train_time:26671ms step_avg:57.11ms
step:468/2330 train_time:26731ms step_avg:57.12ms
step:469/2330 train_time:26786ms step_avg:57.11ms
step:470/2330 train_time:26847ms step_avg:57.12ms
step:471/2330 train_time:26902ms step_avg:57.12ms
step:472/2330 train_time:26962ms step_avg:57.12ms
step:473/2330 train_time:27017ms step_avg:57.12ms
step:474/2330 train_time:27077ms step_avg:57.12ms
step:475/2330 train_time:27133ms step_avg:57.12ms
step:476/2330 train_time:27192ms step_avg:57.13ms
step:477/2330 train_time:27248ms step_avg:57.12ms
step:478/2330 train_time:27306ms step_avg:57.13ms
step:479/2330 train_time:27362ms step_avg:57.12ms
step:480/2330 train_time:27422ms step_avg:57.13ms
step:481/2330 train_time:27477ms step_avg:57.12ms
step:482/2330 train_time:27537ms step_avg:57.13ms
step:483/2330 train_time:27592ms step_avg:57.13ms
step:484/2330 train_time:27652ms step_avg:57.13ms
step:485/2330 train_time:27707ms step_avg:57.13ms
step:486/2330 train_time:27767ms step_avg:57.13ms
step:487/2330 train_time:27823ms step_avg:57.13ms
step:488/2330 train_time:27882ms step_avg:57.14ms
step:489/2330 train_time:27937ms step_avg:57.13ms
step:490/2330 train_time:27997ms step_avg:57.14ms
step:491/2330 train_time:28053ms step_avg:57.13ms
step:492/2330 train_time:28112ms step_avg:57.14ms
step:493/2330 train_time:28168ms step_avg:57.14ms
step:494/2330 train_time:28228ms step_avg:57.14ms
step:495/2330 train_time:28284ms step_avg:57.14ms
step:496/2330 train_time:28344ms step_avg:57.15ms
step:497/2330 train_time:28400ms step_avg:57.14ms
step:498/2330 train_time:28459ms step_avg:57.15ms
step:499/2330 train_time:28515ms step_avg:57.14ms
step:500/2330 train_time:28573ms step_avg:57.15ms
step:500/2330 val_loss:5.8660 train_time:28653ms step_avg:57.31ms
step:501/2330 train_time:28672ms step_avg:57.23ms
step:502/2330 train_time:28692ms step_avg:57.16ms
step:503/2330 train_time:28747ms step_avg:57.15ms
step:504/2330 train_time:28815ms step_avg:57.17ms
step:505/2330 train_time:28872ms step_avg:57.17ms
step:506/2330 train_time:28931ms step_avg:57.18ms
step:507/2330 train_time:28987ms step_avg:57.17ms
step:508/2330 train_time:29046ms step_avg:57.18ms
step:509/2330 train_time:29102ms step_avg:57.17ms
step:510/2330 train_time:29160ms step_avg:57.18ms
step:511/2330 train_time:29215ms step_avg:57.17ms
step:512/2330 train_time:29274ms step_avg:57.18ms
step:513/2330 train_time:29329ms step_avg:57.17ms
step:514/2330 train_time:29387ms step_avg:57.17ms
step:515/2330 train_time:29443ms step_avg:57.17ms
step:516/2330 train_time:29501ms step_avg:57.17ms
step:517/2330 train_time:29557ms step_avg:57.17ms
step:518/2330 train_time:29617ms step_avg:57.18ms
step:519/2330 train_time:29673ms step_avg:57.17ms
step:520/2330 train_time:29732ms step_avg:57.18ms
step:521/2330 train_time:29789ms step_avg:57.18ms
step:522/2330 train_time:29850ms step_avg:57.18ms
step:523/2330 train_time:29906ms step_avg:57.18ms
step:524/2330 train_time:29966ms step_avg:57.19ms
step:525/2330 train_time:30021ms step_avg:57.18ms
step:526/2330 train_time:30082ms step_avg:57.19ms
step:527/2330 train_time:30138ms step_avg:57.19ms
step:528/2330 train_time:30196ms step_avg:57.19ms
step:529/2330 train_time:30252ms step_avg:57.19ms
step:530/2330 train_time:30312ms step_avg:57.19ms
step:531/2330 train_time:30367ms step_avg:57.19ms
step:532/2330 train_time:30426ms step_avg:57.19ms
step:533/2330 train_time:30482ms step_avg:57.19ms
step:534/2330 train_time:30541ms step_avg:57.19ms
step:535/2330 train_time:30597ms step_avg:57.19ms
step:536/2330 train_time:30656ms step_avg:57.19ms
step:537/2330 train_time:30712ms step_avg:57.19ms
step:538/2330 train_time:30772ms step_avg:57.20ms
step:539/2330 train_time:30828ms step_avg:57.20ms
step:540/2330 train_time:30888ms step_avg:57.20ms
step:541/2330 train_time:30945ms step_avg:57.20ms
step:542/2330 train_time:31004ms step_avg:57.20ms
step:543/2330 train_time:31061ms step_avg:57.20ms
step:544/2330 train_time:31119ms step_avg:57.20ms
step:545/2330 train_time:31175ms step_avg:57.20ms
step:546/2330 train_time:31234ms step_avg:57.21ms
step:547/2330 train_time:31290ms step_avg:57.20ms
step:548/2330 train_time:31349ms step_avg:57.21ms
step:549/2330 train_time:31405ms step_avg:57.20ms
step:550/2330 train_time:31463ms step_avg:57.21ms
step:551/2330 train_time:31518ms step_avg:57.20ms
step:552/2330 train_time:31577ms step_avg:57.21ms
step:553/2330 train_time:31633ms step_avg:57.20ms
step:554/2330 train_time:31692ms step_avg:57.21ms
step:555/2330 train_time:31749ms step_avg:57.20ms
step:556/2330 train_time:31808ms step_avg:57.21ms
step:557/2330 train_time:31864ms step_avg:57.21ms
step:558/2330 train_time:31924ms step_avg:57.21ms
step:559/2330 train_time:31981ms step_avg:57.21ms
step:560/2330 train_time:32039ms step_avg:57.21ms
step:561/2330 train_time:32095ms step_avg:57.21ms
step:562/2330 train_time:32155ms step_avg:57.22ms
step:563/2330 train_time:32211ms step_avg:57.21ms
step:564/2330 train_time:32270ms step_avg:57.22ms
step:565/2330 train_time:32325ms step_avg:57.21ms
step:566/2330 train_time:32384ms step_avg:57.22ms
step:567/2330 train_time:32440ms step_avg:57.21ms
step:568/2330 train_time:32499ms step_avg:57.22ms
step:569/2330 train_time:32554ms step_avg:57.21ms
step:570/2330 train_time:32613ms step_avg:57.22ms
step:571/2330 train_time:32669ms step_avg:57.21ms
step:572/2330 train_time:32728ms step_avg:57.22ms
step:573/2330 train_time:32784ms step_avg:57.21ms
step:574/2330 train_time:32843ms step_avg:57.22ms
step:575/2330 train_time:32899ms step_avg:57.22ms
step:576/2330 train_time:32959ms step_avg:57.22ms
step:577/2330 train_time:33015ms step_avg:57.22ms
step:578/2330 train_time:33074ms step_avg:57.22ms
step:579/2330 train_time:33131ms step_avg:57.22ms
step:580/2330 train_time:33190ms step_avg:57.22ms
step:581/2330 train_time:33246ms step_avg:57.22ms
step:582/2330 train_time:33305ms step_avg:57.23ms
step:583/2330 train_time:33361ms step_avg:57.22ms
step:584/2330 train_time:33420ms step_avg:57.23ms
step:585/2330 train_time:33475ms step_avg:57.22ms
step:586/2330 train_time:33535ms step_avg:57.23ms
step:587/2330 train_time:33591ms step_avg:57.22ms
step:588/2330 train_time:33649ms step_avg:57.23ms
step:589/2330 train_time:33705ms step_avg:57.22ms
step:590/2330 train_time:33764ms step_avg:57.23ms
step:591/2330 train_time:33820ms step_avg:57.23ms
step:592/2330 train_time:33880ms step_avg:57.23ms
step:593/2330 train_time:33936ms step_avg:57.23ms
step:594/2330 train_time:33996ms step_avg:57.23ms
step:595/2330 train_time:34052ms step_avg:57.23ms
step:596/2330 train_time:34112ms step_avg:57.23ms
step:597/2330 train_time:34169ms step_avg:57.23ms
step:598/2330 train_time:34228ms step_avg:57.24ms
step:599/2330 train_time:34284ms step_avg:57.24ms
step:600/2330 train_time:34344ms step_avg:57.24ms
step:601/2330 train_time:34399ms step_avg:57.24ms
step:602/2330 train_time:34459ms step_avg:57.24ms
step:603/2330 train_time:34515ms step_avg:57.24ms
step:604/2330 train_time:34574ms step_avg:57.24ms
step:605/2330 train_time:34629ms step_avg:57.24ms
step:606/2330 train_time:34689ms step_avg:57.24ms
step:607/2330 train_time:34744ms step_avg:57.24ms
step:608/2330 train_time:34803ms step_avg:57.24ms
step:609/2330 train_time:34859ms step_avg:57.24ms
step:610/2330 train_time:34918ms step_avg:57.24ms
step:611/2330 train_time:34973ms step_avg:57.24ms
step:612/2330 train_time:35033ms step_avg:57.24ms
step:613/2330 train_time:35089ms step_avg:57.24ms
step:614/2330 train_time:35149ms step_avg:57.25ms
step:615/2330 train_time:35206ms step_avg:57.24ms
step:616/2330 train_time:35266ms step_avg:57.25ms
step:617/2330 train_time:35322ms step_avg:57.25ms
step:618/2330 train_time:35381ms step_avg:57.25ms
step:619/2330 train_time:35437ms step_avg:57.25ms
step:620/2330 train_time:35496ms step_avg:57.25ms
step:621/2330 train_time:35552ms step_avg:57.25ms
step:622/2330 train_time:35611ms step_avg:57.25ms
step:623/2330 train_time:35667ms step_avg:57.25ms
step:624/2330 train_time:35726ms step_avg:57.25ms
step:625/2330 train_time:35782ms step_avg:57.25ms
step:626/2330 train_time:35841ms step_avg:57.25ms
step:627/2330 train_time:35897ms step_avg:57.25ms
step:628/2330 train_time:35956ms step_avg:57.25ms
step:629/2330 train_time:36011ms step_avg:57.25ms
step:630/2330 train_time:36071ms step_avg:57.26ms
step:631/2330 train_time:36127ms step_avg:57.25ms
step:632/2330 train_time:36188ms step_avg:57.26ms
step:633/2330 train_time:36244ms step_avg:57.26ms
step:634/2330 train_time:36304ms step_avg:57.26ms
step:635/2330 train_time:36359ms step_avg:57.26ms
step:636/2330 train_time:36418ms step_avg:57.26ms
step:637/2330 train_time:36474ms step_avg:57.26ms
step:638/2330 train_time:36533ms step_avg:57.26ms
step:639/2330 train_time:36589ms step_avg:57.26ms
step:640/2330 train_time:36648ms step_avg:57.26ms
step:641/2330 train_time:36704ms step_avg:57.26ms
step:642/2330 train_time:36763ms step_avg:57.26ms
step:643/2330 train_time:36818ms step_avg:57.26ms
step:644/2330 train_time:36878ms step_avg:57.26ms
step:645/2330 train_time:36934ms step_avg:57.26ms
step:646/2330 train_time:36994ms step_avg:57.27ms
step:647/2330 train_time:37049ms step_avg:57.26ms
step:648/2330 train_time:37109ms step_avg:57.27ms
step:649/2330 train_time:37165ms step_avg:57.26ms
step:650/2330 train_time:37226ms step_avg:57.27ms
step:651/2330 train_time:37281ms step_avg:57.27ms
step:652/2330 train_time:37342ms step_avg:57.27ms
step:653/2330 train_time:37398ms step_avg:57.27ms
step:654/2330 train_time:37457ms step_avg:57.27ms
step:655/2330 train_time:37512ms step_avg:57.27ms
step:656/2330 train_time:37572ms step_avg:57.27ms
step:657/2330 train_time:37629ms step_avg:57.27ms
step:658/2330 train_time:37688ms step_avg:57.28ms
step:659/2330 train_time:37744ms step_avg:57.27ms
step:660/2330 train_time:37804ms step_avg:57.28ms
step:661/2330 train_time:37859ms step_avg:57.28ms
step:662/2330 train_time:37919ms step_avg:57.28ms
step:663/2330 train_time:37974ms step_avg:57.28ms
step:664/2330 train_time:38034ms step_avg:57.28ms
step:665/2330 train_time:38090ms step_avg:57.28ms
step:666/2330 train_time:38149ms step_avg:57.28ms
step:667/2330 train_time:38205ms step_avg:57.28ms
step:668/2330 train_time:38265ms step_avg:57.28ms
step:669/2330 train_time:38321ms step_avg:57.28ms
step:670/2330 train_time:38380ms step_avg:57.28ms
step:671/2330 train_time:38435ms step_avg:57.28ms
step:672/2330 train_time:38495ms step_avg:57.28ms
step:673/2330 train_time:38551ms step_avg:57.28ms
step:674/2330 train_time:38610ms step_avg:57.29ms
step:675/2330 train_time:38667ms step_avg:57.28ms
step:676/2330 train_time:38726ms step_avg:57.29ms
step:677/2330 train_time:38782ms step_avg:57.29ms
step:678/2330 train_time:38841ms step_avg:57.29ms
step:679/2330 train_time:38897ms step_avg:57.29ms
step:680/2330 train_time:38957ms step_avg:57.29ms
step:681/2330 train_time:39012ms step_avg:57.29ms
step:682/2330 train_time:39072ms step_avg:57.29ms
step:683/2330 train_time:39128ms step_avg:57.29ms
step:684/2330 train_time:39187ms step_avg:57.29ms
step:685/2330 train_time:39244ms step_avg:57.29ms
step:686/2330 train_time:39304ms step_avg:57.29ms
step:687/2330 train_time:39359ms step_avg:57.29ms
step:688/2330 train_time:39419ms step_avg:57.29ms
step:689/2330 train_time:39474ms step_avg:57.29ms
step:690/2330 train_time:39534ms step_avg:57.30ms
step:691/2330 train_time:39591ms step_avg:57.29ms
step:692/2330 train_time:39650ms step_avg:57.30ms
step:693/2330 train_time:39707ms step_avg:57.30ms
step:694/2330 train_time:39766ms step_avg:57.30ms
step:695/2330 train_time:39821ms step_avg:57.30ms
step:696/2330 train_time:39882ms step_avg:57.30ms
step:697/2330 train_time:39937ms step_avg:57.30ms
step:698/2330 train_time:39997ms step_avg:57.30ms
step:699/2330 train_time:40052ms step_avg:57.30ms
step:700/2330 train_time:40112ms step_avg:57.30ms
step:701/2330 train_time:40168ms step_avg:57.30ms
step:702/2330 train_time:40227ms step_avg:57.30ms
step:703/2330 train_time:40284ms step_avg:57.30ms
step:704/2330 train_time:40343ms step_avg:57.31ms
step:705/2330 train_time:40400ms step_avg:57.30ms
step:706/2330 train_time:40458ms step_avg:57.31ms
step:707/2330 train_time:40513ms step_avg:57.30ms
step:708/2330 train_time:40574ms step_avg:57.31ms
step:709/2330 train_time:40629ms step_avg:57.31ms
step:710/2330 train_time:40689ms step_avg:57.31ms
step:711/2330 train_time:40745ms step_avg:57.31ms
step:712/2330 train_time:40804ms step_avg:57.31ms
step:713/2330 train_time:40861ms step_avg:57.31ms
step:714/2330 train_time:40920ms step_avg:57.31ms
step:715/2330 train_time:40975ms step_avg:57.31ms
step:716/2330 train_time:41035ms step_avg:57.31ms
step:717/2330 train_time:41091ms step_avg:57.31ms
step:718/2330 train_time:41151ms step_avg:57.31ms
step:719/2330 train_time:41208ms step_avg:57.31ms
step:720/2330 train_time:41267ms step_avg:57.32ms
step:721/2330 train_time:41324ms step_avg:57.31ms
step:722/2330 train_time:41383ms step_avg:57.32ms
step:723/2330 train_time:41438ms step_avg:57.31ms
step:724/2330 train_time:41498ms step_avg:57.32ms
step:725/2330 train_time:41554ms step_avg:57.32ms
step:726/2330 train_time:41613ms step_avg:57.32ms
step:727/2330 train_time:41669ms step_avg:57.32ms
step:728/2330 train_time:41729ms step_avg:57.32ms
step:729/2330 train_time:41785ms step_avg:57.32ms
step:730/2330 train_time:41845ms step_avg:57.32ms
step:731/2330 train_time:41900ms step_avg:57.32ms
step:732/2330 train_time:41960ms step_avg:57.32ms
step:733/2330 train_time:42016ms step_avg:57.32ms
step:734/2330 train_time:42075ms step_avg:57.32ms
step:735/2330 train_time:42131ms step_avg:57.32ms
step:736/2330 train_time:42190ms step_avg:57.32ms
step:737/2330 train_time:42247ms step_avg:57.32ms
step:738/2330 train_time:42306ms step_avg:57.33ms
step:739/2330 train_time:42363ms step_avg:57.32ms
step:740/2330 train_time:42423ms step_avg:57.33ms
step:741/2330 train_time:42478ms step_avg:57.33ms
step:742/2330 train_time:42539ms step_avg:57.33ms
step:743/2330 train_time:42594ms step_avg:57.33ms
step:744/2330 train_time:42654ms step_avg:57.33ms
step:745/2330 train_time:42711ms step_avg:57.33ms
step:746/2330 train_time:42770ms step_avg:57.33ms
step:747/2330 train_time:42826ms step_avg:57.33ms
step:748/2330 train_time:42886ms step_avg:57.33ms
step:749/2330 train_time:42942ms step_avg:57.33ms
step:750/2330 train_time:43001ms step_avg:57.33ms
step:750/2330 val_loss:5.5363 train_time:43079ms step_avg:57.44ms
step:751/2330 train_time:43098ms step_avg:57.39ms
step:752/2330 train_time:43118ms step_avg:57.34ms
step:753/2330 train_time:43174ms step_avg:57.34ms
step:754/2330 train_time:43239ms step_avg:57.35ms
step:755/2330 train_time:43296ms step_avg:57.35ms
step:756/2330 train_time:43357ms step_avg:57.35ms
step:757/2330 train_time:43412ms step_avg:57.35ms
step:758/2330 train_time:43472ms step_avg:57.35ms
step:759/2330 train_time:43528ms step_avg:57.35ms
step:760/2330 train_time:43587ms step_avg:57.35ms
step:761/2330 train_time:43643ms step_avg:57.35ms
step:762/2330 train_time:43701ms step_avg:57.35ms
step:763/2330 train_time:43757ms step_avg:57.35ms
step:764/2330 train_time:43816ms step_avg:57.35ms
step:765/2330 train_time:43873ms step_avg:57.35ms
step:766/2330 train_time:43931ms step_avg:57.35ms
step:767/2330 train_time:43987ms step_avg:57.35ms
step:768/2330 train_time:44049ms step_avg:57.36ms
step:769/2330 train_time:44107ms step_avg:57.36ms
step:770/2330 train_time:44168ms step_avg:57.36ms
step:771/2330 train_time:44225ms step_avg:57.36ms
step:772/2330 train_time:44287ms step_avg:57.37ms
step:773/2330 train_time:44343ms step_avg:57.36ms
step:774/2330 train_time:44406ms step_avg:57.37ms
step:775/2330 train_time:44461ms step_avg:57.37ms
step:776/2330 train_time:44522ms step_avg:57.37ms
step:777/2330 train_time:44577ms step_avg:57.37ms
step:778/2330 train_time:44637ms step_avg:57.37ms
step:779/2330 train_time:44694ms step_avg:57.37ms
step:780/2330 train_time:44754ms step_avg:57.38ms
step:781/2330 train_time:44810ms step_avg:57.37ms
step:782/2330 train_time:44870ms step_avg:57.38ms
step:783/2330 train_time:44926ms step_avg:57.38ms
step:784/2330 train_time:44986ms step_avg:57.38ms
step:785/2330 train_time:45043ms step_avg:57.38ms
step:786/2330 train_time:45103ms step_avg:57.38ms
step:787/2330 train_time:45160ms step_avg:57.38ms
step:788/2330 train_time:45221ms step_avg:57.39ms
step:789/2330 train_time:45278ms step_avg:57.39ms
step:790/2330 train_time:45340ms step_avg:57.39ms
step:791/2330 train_time:45396ms step_avg:57.39ms
step:792/2330 train_time:45457ms step_avg:57.40ms
step:793/2330 train_time:45514ms step_avg:57.39ms
step:794/2330 train_time:45574ms step_avg:57.40ms
step:795/2330 train_time:45631ms step_avg:57.40ms
step:796/2330 train_time:45691ms step_avg:57.40ms
step:797/2330 train_time:45747ms step_avg:57.40ms
step:798/2330 train_time:45808ms step_avg:57.40ms
step:799/2330 train_time:45864ms step_avg:57.40ms
step:800/2330 train_time:45926ms step_avg:57.41ms
step:801/2330 train_time:45982ms step_avg:57.41ms
step:802/2330 train_time:46042ms step_avg:57.41ms
step:803/2330 train_time:46099ms step_avg:57.41ms
step:804/2330 train_time:46159ms step_avg:57.41ms
step:805/2330 train_time:46216ms step_avg:57.41ms
step:806/2330 train_time:46276ms step_avg:57.41ms
step:807/2330 train_time:46333ms step_avg:57.41ms
step:808/2330 train_time:46394ms step_avg:57.42ms
step:809/2330 train_time:46451ms step_avg:57.42ms
step:810/2330 train_time:46511ms step_avg:57.42ms
step:811/2330 train_time:46567ms step_avg:57.42ms
step:812/2330 train_time:46629ms step_avg:57.42ms
step:813/2330 train_time:46685ms step_avg:57.42ms
step:814/2330 train_time:46746ms step_avg:57.43ms
step:815/2330 train_time:46802ms step_avg:57.43ms
step:816/2330 train_time:46862ms step_avg:57.43ms
step:817/2330 train_time:46919ms step_avg:57.43ms
step:818/2330 train_time:46978ms step_avg:57.43ms
step:819/2330 train_time:47035ms step_avg:57.43ms
step:820/2330 train_time:47095ms step_avg:57.43ms
step:821/2330 train_time:47152ms step_avg:57.43ms
step:822/2330 train_time:47211ms step_avg:57.43ms
step:823/2330 train_time:47268ms step_avg:57.43ms
step:824/2330 train_time:47329ms step_avg:57.44ms
step:825/2330 train_time:47386ms step_avg:57.44ms
step:826/2330 train_time:47447ms step_avg:57.44ms
step:827/2330 train_time:47504ms step_avg:57.44ms
step:828/2330 train_time:47564ms step_avg:57.44ms
step:829/2330 train_time:47620ms step_avg:57.44ms
step:830/2330 train_time:47681ms step_avg:57.45ms
step:831/2330 train_time:47738ms step_avg:57.45ms
step:832/2330 train_time:47798ms step_avg:57.45ms
step:833/2330 train_time:47854ms step_avg:57.45ms
step:834/2330 train_time:47915ms step_avg:57.45ms
step:835/2330 train_time:47971ms step_avg:57.45ms
step:836/2330 train_time:48031ms step_avg:57.45ms
step:837/2330 train_time:48088ms step_avg:57.45ms
step:838/2330 train_time:48149ms step_avg:57.46ms
step:839/2330 train_time:48206ms step_avg:57.46ms
step:840/2330 train_time:48267ms step_avg:57.46ms
step:841/2330 train_time:48324ms step_avg:57.46ms
step:842/2330 train_time:48385ms step_avg:57.46ms
step:843/2330 train_time:48441ms step_avg:57.46ms
step:844/2330 train_time:48502ms step_avg:57.47ms
step:845/2330 train_time:48558ms step_avg:57.47ms
step:846/2330 train_time:48618ms step_avg:57.47ms
step:847/2330 train_time:48675ms step_avg:57.47ms
step:848/2330 train_time:48735ms step_avg:57.47ms
step:849/2330 train_time:48793ms step_avg:57.47ms
step:850/2330 train_time:48852ms step_avg:57.47ms
step:851/2330 train_time:48908ms step_avg:57.47ms
step:852/2330 train_time:48970ms step_avg:57.48ms
step:853/2330 train_time:49026ms step_avg:57.47ms
step:854/2330 train_time:49086ms step_avg:57.48ms
step:855/2330 train_time:49142ms step_avg:57.48ms
step:856/2330 train_time:49203ms step_avg:57.48ms
step:857/2330 train_time:49259ms step_avg:57.48ms
step:858/2330 train_time:49320ms step_avg:57.48ms
step:859/2330 train_time:49377ms step_avg:57.48ms
step:860/2330 train_time:49437ms step_avg:57.48ms
step:861/2330 train_time:49494ms step_avg:57.48ms
step:862/2330 train_time:49555ms step_avg:57.49ms
step:863/2330 train_time:49612ms step_avg:57.49ms
step:864/2330 train_time:49672ms step_avg:57.49ms
step:865/2330 train_time:49728ms step_avg:57.49ms
step:866/2330 train_time:49789ms step_avg:57.49ms
step:867/2330 train_time:49846ms step_avg:57.49ms
step:868/2330 train_time:49906ms step_avg:57.50ms
step:869/2330 train_time:49962ms step_avg:57.49ms
step:870/2330 train_time:50023ms step_avg:57.50ms
step:871/2330 train_time:50079ms step_avg:57.50ms
step:872/2330 train_time:50140ms step_avg:57.50ms
step:873/2330 train_time:50196ms step_avg:57.50ms
step:874/2330 train_time:50256ms step_avg:57.50ms
step:875/2330 train_time:50313ms step_avg:57.50ms
step:876/2330 train_time:50374ms step_avg:57.50ms
step:877/2330 train_time:50431ms step_avg:57.50ms
step:878/2330 train_time:50491ms step_avg:57.51ms
step:879/2330 train_time:50548ms step_avg:57.51ms
step:880/2330 train_time:50609ms step_avg:57.51ms
step:881/2330 train_time:50665ms step_avg:57.51ms
step:882/2330 train_time:50727ms step_avg:57.51ms
step:883/2330 train_time:50783ms step_avg:57.51ms
step:884/2330 train_time:50843ms step_avg:57.51ms
step:885/2330 train_time:50900ms step_avg:57.51ms
step:886/2330 train_time:50960ms step_avg:57.52ms
step:887/2330 train_time:51016ms step_avg:57.52ms
step:888/2330 train_time:51076ms step_avg:57.52ms
step:889/2330 train_time:51132ms step_avg:57.52ms
step:890/2330 train_time:51192ms step_avg:57.52ms
step:891/2330 train_time:51248ms step_avg:57.52ms
step:892/2330 train_time:51310ms step_avg:57.52ms
step:893/2330 train_time:51366ms step_avg:57.52ms
step:894/2330 train_time:51428ms step_avg:57.53ms
step:895/2330 train_time:51484ms step_avg:57.52ms
step:896/2330 train_time:51545ms step_avg:57.53ms
step:897/2330 train_time:51601ms step_avg:57.53ms
step:898/2330 train_time:51661ms step_avg:57.53ms
step:899/2330 train_time:51718ms step_avg:57.53ms
step:900/2330 train_time:51779ms step_avg:57.53ms
step:901/2330 train_time:51836ms step_avg:57.53ms
step:902/2330 train_time:51896ms step_avg:57.53ms
step:903/2330 train_time:51953ms step_avg:57.53ms
step:904/2330 train_time:52012ms step_avg:57.54ms
step:905/2330 train_time:52068ms step_avg:57.53ms
step:906/2330 train_time:52130ms step_avg:57.54ms
step:907/2330 train_time:52187ms step_avg:57.54ms
step:908/2330 train_time:52247ms step_avg:57.54ms
step:909/2330 train_time:52303ms step_avg:57.54ms
step:910/2330 train_time:52362ms step_avg:57.54ms
step:911/2330 train_time:52419ms step_avg:57.54ms
step:912/2330 train_time:52480ms step_avg:57.54ms
step:913/2330 train_time:52537ms step_avg:57.54ms
step:914/2330 train_time:52597ms step_avg:57.55ms
step:915/2330 train_time:52653ms step_avg:57.54ms
step:916/2330 train_time:52715ms step_avg:57.55ms
step:917/2330 train_time:52772ms step_avg:57.55ms
step:918/2330 train_time:52832ms step_avg:57.55ms
step:919/2330 train_time:52890ms step_avg:57.55ms
step:920/2330 train_time:52950ms step_avg:57.55ms
step:921/2330 train_time:53007ms step_avg:57.55ms
step:922/2330 train_time:53067ms step_avg:57.56ms
step:923/2330 train_time:53124ms step_avg:57.56ms
step:924/2330 train_time:53184ms step_avg:57.56ms
step:925/2330 train_time:53241ms step_avg:57.56ms
step:926/2330 train_time:53300ms step_avg:57.56ms
step:927/2330 train_time:53357ms step_avg:57.56ms
step:928/2330 train_time:53417ms step_avg:57.56ms
step:929/2330 train_time:53473ms step_avg:57.56ms
step:930/2330 train_time:53534ms step_avg:57.56ms
step:931/2330 train_time:53591ms step_avg:57.56ms
step:932/2330 train_time:53652ms step_avg:57.57ms
step:933/2330 train_time:53708ms step_avg:57.56ms
step:934/2330 train_time:53770ms step_avg:57.57ms
step:935/2330 train_time:53826ms step_avg:57.57ms
step:936/2330 train_time:53887ms step_avg:57.57ms
step:937/2330 train_time:53944ms step_avg:57.57ms
step:938/2330 train_time:54003ms step_avg:57.57ms
step:939/2330 train_time:54059ms step_avg:57.57ms
step:940/2330 train_time:54120ms step_avg:57.57ms
step:941/2330 train_time:54177ms step_avg:57.57ms
step:942/2330 train_time:54237ms step_avg:57.58ms
step:943/2330 train_time:54293ms step_avg:57.57ms
step:944/2330 train_time:54354ms step_avg:57.58ms
step:945/2330 train_time:54411ms step_avg:57.58ms
step:946/2330 train_time:54472ms step_avg:57.58ms
step:947/2330 train_time:54529ms step_avg:57.58ms
step:948/2330 train_time:54589ms step_avg:57.58ms
step:949/2330 train_time:54645ms step_avg:57.58ms
step:950/2330 train_time:54706ms step_avg:57.59ms
step:951/2330 train_time:54763ms step_avg:57.58ms
step:952/2330 train_time:54825ms step_avg:57.59ms
step:953/2330 train_time:54881ms step_avg:57.59ms
step:954/2330 train_time:54941ms step_avg:57.59ms
step:955/2330 train_time:54997ms step_avg:57.59ms
step:956/2330 train_time:55057ms step_avg:57.59ms
step:957/2330 train_time:55113ms step_avg:57.59ms
step:958/2330 train_time:55175ms step_avg:57.59ms
step:959/2330 train_time:55231ms step_avg:57.59ms
step:960/2330 train_time:55292ms step_avg:57.60ms
step:961/2330 train_time:55349ms step_avg:57.60ms
step:962/2330 train_time:55410ms step_avg:57.60ms
step:963/2330 train_time:55466ms step_avg:57.60ms
step:964/2330 train_time:55527ms step_avg:57.60ms
step:965/2330 train_time:55583ms step_avg:57.60ms
step:966/2330 train_time:55644ms step_avg:57.60ms
step:967/2330 train_time:55700ms step_avg:57.60ms
step:968/2330 train_time:55760ms step_avg:57.60ms
step:969/2330 train_time:55817ms step_avg:57.60ms
step:970/2330 train_time:55878ms step_avg:57.61ms
step:971/2330 train_time:55935ms step_avg:57.61ms
step:972/2330 train_time:55995ms step_avg:57.61ms
step:973/2330 train_time:56052ms step_avg:57.61ms
step:974/2330 train_time:56111ms step_avg:57.61ms
step:975/2330 train_time:56168ms step_avg:57.61ms
step:976/2330 train_time:56228ms step_avg:57.61ms
step:977/2330 train_time:56285ms step_avg:57.61ms
step:978/2330 train_time:56345ms step_avg:57.61ms
step:979/2330 train_time:56401ms step_avg:57.61ms
step:980/2330 train_time:56461ms step_avg:57.61ms
step:981/2330 train_time:56518ms step_avg:57.61ms
step:982/2330 train_time:56579ms step_avg:57.62ms
step:983/2330 train_time:56636ms step_avg:57.62ms
step:984/2330 train_time:56696ms step_avg:57.62ms
step:985/2330 train_time:56752ms step_avg:57.62ms
step:986/2330 train_time:56814ms step_avg:57.62ms
step:987/2330 train_time:56870ms step_avg:57.62ms
step:988/2330 train_time:56931ms step_avg:57.62ms
step:989/2330 train_time:56988ms step_avg:57.62ms
step:990/2330 train_time:57049ms step_avg:57.62ms
step:991/2330 train_time:57105ms step_avg:57.62ms
step:992/2330 train_time:57167ms step_avg:57.63ms
step:993/2330 train_time:57223ms step_avg:57.63ms
step:994/2330 train_time:57285ms step_avg:57.63ms
step:995/2330 train_time:57341ms step_avg:57.63ms
step:996/2330 train_time:57401ms step_avg:57.63ms
step:997/2330 train_time:57457ms step_avg:57.63ms
step:998/2330 train_time:57518ms step_avg:57.63ms
step:999/2330 train_time:57574ms step_avg:57.63ms
step:1000/2330 train_time:57635ms step_avg:57.63ms
step:1000/2330 val_loss:5.3163 train_time:57715ms step_avg:57.72ms
step:1001/2330 train_time:57735ms step_avg:57.68ms
step:1002/2330 train_time:57756ms step_avg:57.64ms
step:1003/2330 train_time:57808ms step_avg:57.64ms
step:1004/2330 train_time:57873ms step_avg:57.64ms
step:1005/2330 train_time:57929ms step_avg:57.64ms
step:1006/2330 train_time:57991ms step_avg:57.65ms
step:1007/2330 train_time:58048ms step_avg:57.64ms
step:1008/2330 train_time:58106ms step_avg:57.65ms
step:1009/2330 train_time:58162ms step_avg:57.64ms
step:1010/2330 train_time:58222ms step_avg:57.65ms
step:1011/2330 train_time:58278ms step_avg:57.64ms
step:1012/2330 train_time:58337ms step_avg:57.65ms
step:1013/2330 train_time:58393ms step_avg:57.64ms
step:1014/2330 train_time:58452ms step_avg:57.64ms
step:1015/2330 train_time:58508ms step_avg:57.64ms
step:1016/2330 train_time:58567ms step_avg:57.64ms
step:1017/2330 train_time:58627ms step_avg:57.65ms
step:1018/2330 train_time:58689ms step_avg:57.65ms
step:1019/2330 train_time:58747ms step_avg:57.65ms
step:1020/2330 train_time:58810ms step_avg:57.66ms
step:1021/2330 train_time:58866ms step_avg:57.66ms
step:1022/2330 train_time:58927ms step_avg:57.66ms
step:1023/2330 train_time:58984ms step_avg:57.66ms
step:1024/2330 train_time:59045ms step_avg:57.66ms
step:1025/2330 train_time:59101ms step_avg:57.66ms
step:1026/2330 train_time:59161ms step_avg:57.66ms
step:1027/2330 train_time:59217ms step_avg:57.66ms
step:1028/2330 train_time:59276ms step_avg:57.66ms
step:1029/2330 train_time:59332ms step_avg:57.66ms
step:1030/2330 train_time:59391ms step_avg:57.66ms
step:1031/2330 train_time:59448ms step_avg:57.66ms
step:1032/2330 train_time:59507ms step_avg:57.66ms
step:1033/2330 train_time:59566ms step_avg:57.66ms
step:1034/2330 train_time:59626ms step_avg:57.67ms
step:1035/2330 train_time:59683ms step_avg:57.66ms
step:1036/2330 train_time:59745ms step_avg:57.67ms
step:1037/2330 train_time:59802ms step_avg:57.67ms
step:1038/2330 train_time:59864ms step_avg:57.67ms
step:1039/2330 train_time:59921ms step_avg:57.67ms
step:1040/2330 train_time:59982ms step_avg:57.68ms
step:1041/2330 train_time:60039ms step_avg:57.67ms
step:1042/2330 train_time:60099ms step_avg:57.68ms
step:1043/2330 train_time:60155ms step_avg:57.68ms
step:1044/2330 train_time:60215ms step_avg:57.68ms
step:1045/2330 train_time:60271ms step_avg:57.68ms
step:1046/2330 train_time:60331ms step_avg:57.68ms
step:1047/2330 train_time:60387ms step_avg:57.68ms
step:1048/2330 train_time:60447ms step_avg:57.68ms
step:1049/2330 train_time:60503ms step_avg:57.68ms
step:1050/2330 train_time:60563ms step_avg:57.68ms
step:1051/2330 train_time:60620ms step_avg:57.68ms
step:1052/2330 train_time:60680ms step_avg:57.68ms
step:1053/2330 train_time:60738ms step_avg:57.68ms
step:1054/2330 train_time:60800ms step_avg:57.68ms
step:1055/2330 train_time:60856ms step_avg:57.68ms
step:1056/2330 train_time:60918ms step_avg:57.69ms
step:1057/2330 train_time:60974ms step_avg:57.69ms
step:1058/2330 train_time:61035ms step_avg:57.69ms
step:1059/2330 train_time:61091ms step_avg:57.69ms
step:1060/2330 train_time:61151ms step_avg:57.69ms
step:1061/2330 train_time:61207ms step_avg:57.69ms
step:1062/2330 train_time:61267ms step_avg:57.69ms
step:1063/2330 train_time:61324ms step_avg:57.69ms
step:1064/2330 train_time:61384ms step_avg:57.69ms
step:1065/2330 train_time:61440ms step_avg:57.69ms
step:1066/2330 train_time:61501ms step_avg:57.69ms
step:1067/2330 train_time:61557ms step_avg:57.69ms
step:1068/2330 train_time:61617ms step_avg:57.69ms
step:1069/2330 train_time:61674ms step_avg:57.69ms
step:1070/2330 train_time:61735ms step_avg:57.70ms
step:1071/2330 train_time:61792ms step_avg:57.70ms
step:1072/2330 train_time:61852ms step_avg:57.70ms
step:1073/2330 train_time:61909ms step_avg:57.70ms
step:1074/2330 train_time:61969ms step_avg:57.70ms
step:1075/2330 train_time:62027ms step_avg:57.70ms
step:1076/2330 train_time:62087ms step_avg:57.70ms
step:1077/2330 train_time:62144ms step_avg:57.70ms
step:1078/2330 train_time:62204ms step_avg:57.70ms
step:1079/2330 train_time:62260ms step_avg:57.70ms
step:1080/2330 train_time:62320ms step_avg:57.70ms
step:1081/2330 train_time:62376ms step_avg:57.70ms
step:1082/2330 train_time:62436ms step_avg:57.70ms
step:1083/2330 train_time:62493ms step_avg:57.70ms
step:1084/2330 train_time:62552ms step_avg:57.70ms
step:1085/2330 train_time:62609ms step_avg:57.70ms
step:1086/2330 train_time:62671ms step_avg:57.71ms
step:1087/2330 train_time:62728ms step_avg:57.71ms
step:1088/2330 train_time:62788ms step_avg:57.71ms
step:1089/2330 train_time:62845ms step_avg:57.71ms
step:1090/2330 train_time:62905ms step_avg:57.71ms
step:1091/2330 train_time:62962ms step_avg:57.71ms
step:1092/2330 train_time:63023ms step_avg:57.71ms
step:1093/2330 train_time:63079ms step_avg:57.71ms
step:1094/2330 train_time:63142ms step_avg:57.72ms
step:1095/2330 train_time:63198ms step_avg:57.71ms
step:1096/2330 train_time:63258ms step_avg:57.72ms
step:1097/2330 train_time:63315ms step_avg:57.72ms
step:1098/2330 train_time:63375ms step_avg:57.72ms
step:1099/2330 train_time:63432ms step_avg:57.72ms
step:1100/2330 train_time:63492ms step_avg:57.72ms
step:1101/2330 train_time:63548ms step_avg:57.72ms
step:1102/2330 train_time:63609ms step_avg:57.72ms
step:1103/2330 train_time:63666ms step_avg:57.72ms
step:1104/2330 train_time:63726ms step_avg:57.72ms
step:1105/2330 train_time:63783ms step_avg:57.72ms
step:1106/2330 train_time:63844ms step_avg:57.73ms
step:1107/2330 train_time:63901ms step_avg:57.72ms
step:1108/2330 train_time:63963ms step_avg:57.73ms
step:1109/2330 train_time:64020ms step_avg:57.73ms
step:1110/2330 train_time:64080ms step_avg:57.73ms
step:1111/2330 train_time:64137ms step_avg:57.73ms
step:1112/2330 train_time:64199ms step_avg:57.73ms
step:1113/2330 train_time:64255ms step_avg:57.73ms
step:1114/2330 train_time:64315ms step_avg:57.73ms
step:1115/2330 train_time:64371ms step_avg:57.73ms
step:1116/2330 train_time:64432ms step_avg:57.73ms
step:1117/2330 train_time:64489ms step_avg:57.73ms
step:1118/2330 train_time:64548ms step_avg:57.74ms
step:1119/2330 train_time:64605ms step_avg:57.73ms
step:1120/2330 train_time:64665ms step_avg:57.74ms
step:1121/2330 train_time:64722ms step_avg:57.74ms
step:1122/2330 train_time:64782ms step_avg:57.74ms
step:1123/2330 train_time:64838ms step_avg:57.74ms
step:1124/2330 train_time:64900ms step_avg:57.74ms
step:1125/2330 train_time:64957ms step_avg:57.74ms
step:1126/2330 train_time:65017ms step_avg:57.74ms
step:1127/2330 train_time:65075ms step_avg:57.74ms
step:1128/2330 train_time:65135ms step_avg:57.74ms
step:1129/2330 train_time:65192ms step_avg:57.74ms
step:1130/2330 train_time:65252ms step_avg:57.75ms
step:1131/2330 train_time:65309ms step_avg:57.74ms
step:1132/2330 train_time:65369ms step_avg:57.75ms
step:1133/2330 train_time:65425ms step_avg:57.74ms
step:1134/2330 train_time:65485ms step_avg:57.75ms
step:1135/2330 train_time:65542ms step_avg:57.75ms
step:1136/2330 train_time:65602ms step_avg:57.75ms
step:1137/2330 train_time:65658ms step_avg:57.75ms
step:1138/2330 train_time:65719ms step_avg:57.75ms
step:1139/2330 train_time:65776ms step_avg:57.75ms
step:1140/2330 train_time:65836ms step_avg:57.75ms
step:1141/2330 train_time:65893ms step_avg:57.75ms
step:1142/2330 train_time:65953ms step_avg:57.75ms
step:1143/2330 train_time:66009ms step_avg:57.75ms
step:1144/2330 train_time:66070ms step_avg:57.75ms
step:1145/2330 train_time:66128ms step_avg:57.75ms
step:1146/2330 train_time:66188ms step_avg:57.76ms
step:1147/2330 train_time:66245ms step_avg:57.75ms
step:1148/2330 train_time:66305ms step_avg:57.76ms
step:1149/2330 train_time:66362ms step_avg:57.76ms
step:1150/2330 train_time:66423ms step_avg:57.76ms
step:1151/2330 train_time:66480ms step_avg:57.76ms
step:1152/2330 train_time:66540ms step_avg:57.76ms
step:1153/2330 train_time:66596ms step_avg:57.76ms
step:1154/2330 train_time:66657ms step_avg:57.76ms
step:1155/2330 train_time:66713ms step_avg:57.76ms
step:1156/2330 train_time:66774ms step_avg:57.76ms
step:1157/2330 train_time:66830ms step_avg:57.76ms
step:1158/2330 train_time:66891ms step_avg:57.76ms
step:1159/2330 train_time:66948ms step_avg:57.76ms
step:1160/2330 train_time:67009ms step_avg:57.77ms
step:1161/2330 train_time:67065ms step_avg:57.77ms
step:1162/2330 train_time:67125ms step_avg:57.77ms
step:1163/2330 train_time:67182ms step_avg:57.77ms
step:1164/2330 train_time:67243ms step_avg:57.77ms
step:1165/2330 train_time:67301ms step_avg:57.77ms
step:1166/2330 train_time:67360ms step_avg:57.77ms
step:1167/2330 train_time:67416ms step_avg:57.77ms
step:1168/2330 train_time:67477ms step_avg:57.77ms
step:1169/2330 train_time:67533ms step_avg:57.77ms
step:1170/2330 train_time:67593ms step_avg:57.77ms
step:1171/2330 train_time:67650ms step_avg:57.77ms
step:1172/2330 train_time:67710ms step_avg:57.77ms
step:1173/2330 train_time:67767ms step_avg:57.77ms
step:1174/2330 train_time:67828ms step_avg:57.78ms
step:1175/2330 train_time:67886ms step_avg:57.78ms
step:1176/2330 train_time:67946ms step_avg:57.78ms
step:1177/2330 train_time:68003ms step_avg:57.78ms
step:1178/2330 train_time:68063ms step_avg:57.78ms
step:1179/2330 train_time:68120ms step_avg:57.78ms
step:1180/2330 train_time:68180ms step_avg:57.78ms
step:1181/2330 train_time:68237ms step_avg:57.78ms
step:1182/2330 train_time:68297ms step_avg:57.78ms
step:1183/2330 train_time:68354ms step_avg:57.78ms
step:1184/2330 train_time:68414ms step_avg:57.78ms
step:1185/2330 train_time:68471ms step_avg:57.78ms
step:1186/2330 train_time:68531ms step_avg:57.78ms
step:1187/2330 train_time:68588ms step_avg:57.78ms
step:1188/2330 train_time:68648ms step_avg:57.78ms
step:1189/2330 train_time:68706ms step_avg:57.78ms
step:1190/2330 train_time:68765ms step_avg:57.79ms
step:1191/2330 train_time:68822ms step_avg:57.78ms
step:1192/2330 train_time:68882ms step_avg:57.79ms
step:1193/2330 train_time:68939ms step_avg:57.79ms
step:1194/2330 train_time:69000ms step_avg:57.79ms
step:1195/2330 train_time:69056ms step_avg:57.79ms
step:1196/2330 train_time:69118ms step_avg:57.79ms
step:1197/2330 train_time:69174ms step_avg:57.79ms
step:1198/2330 train_time:69235ms step_avg:57.79ms
step:1199/2330 train_time:69292ms step_avg:57.79ms
step:1200/2330 train_time:69351ms step_avg:57.79ms
step:1201/2330 train_time:69408ms step_avg:57.79ms
step:1202/2330 train_time:69468ms step_avg:57.79ms
step:1203/2330 train_time:69525ms step_avg:57.79ms
step:1204/2330 train_time:69585ms step_avg:57.79ms
step:1205/2330 train_time:69642ms step_avg:57.79ms
step:1206/2330 train_time:69702ms step_avg:57.80ms
step:1207/2330 train_time:69759ms step_avg:57.80ms
step:1208/2330 train_time:69819ms step_avg:57.80ms
step:1209/2330 train_time:69876ms step_avg:57.80ms
step:1210/2330 train_time:69936ms step_avg:57.80ms
step:1211/2330 train_time:69992ms step_avg:57.80ms
step:1212/2330 train_time:70052ms step_avg:57.80ms
step:1213/2330 train_time:70109ms step_avg:57.80ms
step:1214/2330 train_time:70171ms step_avg:57.80ms
step:1215/2330 train_time:70228ms step_avg:57.80ms
step:1216/2330 train_time:70288ms step_avg:57.80ms
step:1217/2330 train_time:70345ms step_avg:57.80ms
step:1218/2330 train_time:70406ms step_avg:57.80ms
step:1219/2330 train_time:70462ms step_avg:57.80ms
step:1220/2330 train_time:70525ms step_avg:57.81ms
step:1221/2330 train_time:70581ms step_avg:57.81ms
step:1222/2330 train_time:70643ms step_avg:57.81ms
step:1223/2330 train_time:70699ms step_avg:57.81ms
step:1224/2330 train_time:70760ms step_avg:57.81ms
step:1225/2330 train_time:70817ms step_avg:57.81ms
step:1226/2330 train_time:70877ms step_avg:57.81ms
step:1227/2330 train_time:70934ms step_avg:57.81ms
step:1228/2330 train_time:70994ms step_avg:57.81ms
step:1229/2330 train_time:71051ms step_avg:57.81ms
step:1230/2330 train_time:71111ms step_avg:57.81ms
step:1231/2330 train_time:71167ms step_avg:57.81ms
step:1232/2330 train_time:71229ms step_avg:57.82ms
step:1233/2330 train_time:71287ms step_avg:57.82ms
step:1234/2330 train_time:71347ms step_avg:57.82ms
step:1235/2330 train_time:71405ms step_avg:57.82ms
step:1236/2330 train_time:71464ms step_avg:57.82ms
step:1237/2330 train_time:71522ms step_avg:57.82ms
step:1238/2330 train_time:71582ms step_avg:57.82ms
step:1239/2330 train_time:71640ms step_avg:57.82ms
step:1240/2330 train_time:71701ms step_avg:57.82ms
step:1241/2330 train_time:71757ms step_avg:57.82ms
step:1242/2330 train_time:71817ms step_avg:57.82ms
step:1243/2330 train_time:71874ms step_avg:57.82ms
step:1244/2330 train_time:71934ms step_avg:57.82ms
step:1245/2330 train_time:71991ms step_avg:57.82ms
step:1246/2330 train_time:72051ms step_avg:57.83ms
step:1247/2330 train_time:72108ms step_avg:57.82ms
step:1248/2330 train_time:72168ms step_avg:57.83ms
step:1249/2330 train_time:72226ms step_avg:57.83ms
step:1250/2330 train_time:72285ms step_avg:57.83ms
step:1250/2330 val_loss:5.4233 train_time:72366ms step_avg:57.89ms
step:1251/2330 train_time:72385ms step_avg:57.86ms
step:1252/2330 train_time:72406ms step_avg:57.83ms
step:1253/2330 train_time:72465ms step_avg:57.83ms
step:1254/2330 train_time:72531ms step_avg:57.84ms
step:1255/2330 train_time:72587ms step_avg:57.84ms
step:1256/2330 train_time:72649ms step_avg:57.84ms
step:1257/2330 train_time:72705ms step_avg:57.84ms
step:1258/2330 train_time:72766ms step_avg:57.84ms
step:1259/2330 train_time:72822ms step_avg:57.84ms
step:1260/2330 train_time:72882ms step_avg:57.84ms
step:1261/2330 train_time:72938ms step_avg:57.84ms
step:1262/2330 train_time:72997ms step_avg:57.84ms
step:1263/2330 train_time:73053ms step_avg:57.84ms
step:1264/2330 train_time:73113ms step_avg:57.84ms
step:1265/2330 train_time:73169ms step_avg:57.84ms
step:1266/2330 train_time:73229ms step_avg:57.84ms
step:1267/2330 train_time:73285ms step_avg:57.84ms
step:1268/2330 train_time:73347ms step_avg:57.84ms
step:1269/2330 train_time:73405ms step_avg:57.84ms
step:1270/2330 train_time:73469ms step_avg:57.85ms
step:1271/2330 train_time:73525ms step_avg:57.85ms
step:1272/2330 train_time:73588ms step_avg:57.85ms
step:1273/2330 train_time:73644ms step_avg:57.85ms
step:1274/2330 train_time:73706ms step_avg:57.85ms
step:1275/2330 train_time:73762ms step_avg:57.85ms
step:1276/2330 train_time:73824ms step_avg:57.86ms
step:1277/2330 train_time:73879ms step_avg:57.85ms
step:1278/2330 train_time:73940ms step_avg:57.86ms
step:1279/2330 train_time:73996ms step_avg:57.85ms
step:1280/2330 train_time:74056ms step_avg:57.86ms
step:1281/2330 train_time:74113ms step_avg:57.86ms
step:1282/2330 train_time:74173ms step_avg:57.86ms
step:1283/2330 train_time:74230ms step_avg:57.86ms
step:1284/2330 train_time:74289ms step_avg:57.86ms
step:1285/2330 train_time:74347ms step_avg:57.86ms
step:1286/2330 train_time:74408ms step_avg:57.86ms
step:1287/2330 train_time:74466ms step_avg:57.86ms
step:1288/2330 train_time:74527ms step_avg:57.86ms
step:1289/2330 train_time:74584ms step_avg:57.86ms
step:1290/2330 train_time:74646ms step_avg:57.86ms
step:1291/2330 train_time:74702ms step_avg:57.86ms
step:1292/2330 train_time:74763ms step_avg:57.87ms
step:1293/2330 train_time:74820ms step_avg:57.87ms
step:1294/2330 train_time:74880ms step_avg:57.87ms
step:1295/2330 train_time:74936ms step_avg:57.87ms
step:1296/2330 train_time:74997ms step_avg:57.87ms
step:1297/2330 train_time:75054ms step_avg:57.87ms
step:1298/2330 train_time:75113ms step_avg:57.87ms
step:1299/2330 train_time:75170ms step_avg:57.87ms
step:1300/2330 train_time:75229ms step_avg:57.87ms
step:1301/2330 train_time:75287ms step_avg:57.87ms
step:1302/2330 train_time:75347ms step_avg:57.87ms
step:1303/2330 train_time:75404ms step_avg:57.87ms
step:1304/2330 train_time:75465ms step_avg:57.87ms
step:1305/2330 train_time:75522ms step_avg:57.87ms
step:1306/2330 train_time:75582ms step_avg:57.87ms
step:1307/2330 train_time:75640ms step_avg:57.87ms
step:1308/2330 train_time:75700ms step_avg:57.87ms
step:1309/2330 train_time:75758ms step_avg:57.87ms
step:1310/2330 train_time:75818ms step_avg:57.88ms
step:1311/2330 train_time:75874ms step_avg:57.88ms
step:1312/2330 train_time:75935ms step_avg:57.88ms
step:1313/2330 train_time:75991ms step_avg:57.88ms
step:1314/2330 train_time:76051ms step_avg:57.88ms
step:1315/2330 train_time:76108ms step_avg:57.88ms
step:1316/2330 train_time:76169ms step_avg:57.88ms
step:1317/2330 train_time:76225ms step_avg:57.88ms
step:1318/2330 train_time:76285ms step_avg:57.88ms
step:1319/2330 train_time:76343ms step_avg:57.88ms
step:1320/2330 train_time:76402ms step_avg:57.88ms
step:1321/2330 train_time:76460ms step_avg:57.88ms
step:1322/2330 train_time:76520ms step_avg:57.88ms
step:1323/2330 train_time:76577ms step_avg:57.88ms
step:1324/2330 train_time:76638ms step_avg:57.88ms
step:1325/2330 train_time:76695ms step_avg:57.88ms
step:1326/2330 train_time:76755ms step_avg:57.88ms
step:1327/2330 train_time:76812ms step_avg:57.88ms
step:1328/2330 train_time:76872ms step_avg:57.89ms
step:1329/2330 train_time:76929ms step_avg:57.88ms
step:1330/2330 train_time:76989ms step_avg:57.89ms
step:1331/2330 train_time:77045ms step_avg:57.89ms
step:1332/2330 train_time:77106ms step_avg:57.89ms
step:1333/2330 train_time:77163ms step_avg:57.89ms
step:1334/2330 train_time:77222ms step_avg:57.89ms
step:1335/2330 train_time:77279ms step_avg:57.89ms
step:1336/2330 train_time:77340ms step_avg:57.89ms
step:1337/2330 train_time:77397ms step_avg:57.89ms
step:1338/2330 train_time:77457ms step_avg:57.89ms
step:1339/2330 train_time:77515ms step_avg:57.89ms
step:1340/2330 train_time:77575ms step_avg:57.89ms
step:1341/2330 train_time:77633ms step_avg:57.89ms
step:1342/2330 train_time:77693ms step_avg:57.89ms
step:1343/2330 train_time:77751ms step_avg:57.89ms
step:1344/2330 train_time:77811ms step_avg:57.89ms
step:1345/2330 train_time:77868ms step_avg:57.89ms
step:1346/2330 train_time:77928ms step_avg:57.90ms
step:1347/2330 train_time:77985ms step_avg:57.90ms
step:1348/2330 train_time:78045ms step_avg:57.90ms
step:1349/2330 train_time:78101ms step_avg:57.90ms
step:1350/2330 train_time:78163ms step_avg:57.90ms
step:1351/2330 train_time:78219ms step_avg:57.90ms
step:1352/2330 train_time:78279ms step_avg:57.90ms
step:1353/2330 train_time:78336ms step_avg:57.90ms
step:1354/2330 train_time:78396ms step_avg:57.90ms
step:1355/2330 train_time:78453ms step_avg:57.90ms
step:1356/2330 train_time:78515ms step_avg:57.90ms
step:1357/2330 train_time:78573ms step_avg:57.90ms
step:1358/2330 train_time:78632ms step_avg:57.90ms
step:1359/2330 train_time:78690ms step_avg:57.90ms
step:1360/2330 train_time:78751ms step_avg:57.91ms
step:1361/2330 train_time:78808ms step_avg:57.90ms
step:1362/2330 train_time:78869ms step_avg:57.91ms
step:1363/2330 train_time:78926ms step_avg:57.91ms
step:1364/2330 train_time:78986ms step_avg:57.91ms
step:1365/2330 train_time:79042ms step_avg:57.91ms
step:1366/2330 train_time:79104ms step_avg:57.91ms
step:1367/2330 train_time:79160ms step_avg:57.91ms
step:1368/2330 train_time:79221ms step_avg:57.91ms
step:1369/2330 train_time:79277ms step_avg:57.91ms
step:1370/2330 train_time:79338ms step_avg:57.91ms
step:1371/2330 train_time:79394ms step_avg:57.91ms
step:1372/2330 train_time:79455ms step_avg:57.91ms
step:1373/2330 train_time:79512ms step_avg:57.91ms
step:1374/2330 train_time:79573ms step_avg:57.91ms
step:1375/2330 train_time:79630ms step_avg:57.91ms
step:1376/2330 train_time:79690ms step_avg:57.91ms
step:1377/2330 train_time:79747ms step_avg:57.91ms
step:1378/2330 train_time:79808ms step_avg:57.92ms
step:1379/2330 train_time:79866ms step_avg:57.92ms
step:1380/2330 train_time:79926ms step_avg:57.92ms
step:1381/2330 train_time:79983ms step_avg:57.92ms
step:1382/2330 train_time:80044ms step_avg:57.92ms
step:1383/2330 train_time:80101ms step_avg:57.92ms
step:1384/2330 train_time:80162ms step_avg:57.92ms
step:1385/2330 train_time:80218ms step_avg:57.92ms
step:1386/2330 train_time:80278ms step_avg:57.92ms
step:1387/2330 train_time:80334ms step_avg:57.92ms
step:1388/2330 train_time:80395ms step_avg:57.92ms
step:1389/2330 train_time:80452ms step_avg:57.92ms
step:1390/2330 train_time:80513ms step_avg:57.92ms
step:1391/2330 train_time:80569ms step_avg:57.92ms
step:1392/2330 train_time:80630ms step_avg:57.92ms
step:1393/2330 train_time:80687ms step_avg:57.92ms
step:1394/2330 train_time:80747ms step_avg:57.92ms
step:1395/2330 train_time:80804ms step_avg:57.92ms
step:1396/2330 train_time:80866ms step_avg:57.93ms
step:1397/2330 train_time:80923ms step_avg:57.93ms
step:1398/2330 train_time:80983ms step_avg:57.93ms
step:1399/2330 train_time:81040ms step_avg:57.93ms
step:1400/2330 train_time:81099ms step_avg:57.93ms
step:1401/2330 train_time:81156ms step_avg:57.93ms
step:1402/2330 train_time:81216ms step_avg:57.93ms
step:1403/2330 train_time:81273ms step_avg:57.93ms
step:1404/2330 train_time:81333ms step_avg:57.93ms
step:1405/2330 train_time:81390ms step_avg:57.93ms
step:1406/2330 train_time:81450ms step_avg:57.93ms
step:1407/2330 train_time:81507ms step_avg:57.93ms
step:1408/2330 train_time:81568ms step_avg:57.93ms
step:1409/2330 train_time:81625ms step_avg:57.93ms
step:1410/2330 train_time:81685ms step_avg:57.93ms
step:1411/2330 train_time:81742ms step_avg:57.93ms
step:1412/2330 train_time:81802ms step_avg:57.93ms
step:1413/2330 train_time:81859ms step_avg:57.93ms
step:1414/2330 train_time:81919ms step_avg:57.93ms
step:1415/2330 train_time:81977ms step_avg:57.93ms
step:1416/2330 train_time:82037ms step_avg:57.94ms
step:1417/2330 train_time:82093ms step_avg:57.93ms
step:1418/2330 train_time:82154ms step_avg:57.94ms
step:1419/2330 train_time:82211ms step_avg:57.94ms
step:1420/2330 train_time:82271ms step_avg:57.94ms
step:1421/2330 train_time:82327ms step_avg:57.94ms
step:1422/2330 train_time:82388ms step_avg:57.94ms
step:1423/2330 train_time:82446ms step_avg:57.94ms
step:1424/2330 train_time:82506ms step_avg:57.94ms
step:1425/2330 train_time:82563ms step_avg:57.94ms
step:1426/2330 train_time:82623ms step_avg:57.94ms
step:1427/2330 train_time:82679ms step_avg:57.94ms
step:1428/2330 train_time:82740ms step_avg:57.94ms
step:1429/2330 train_time:82797ms step_avg:57.94ms
step:1430/2330 train_time:82857ms step_avg:57.94ms
step:1431/2330 train_time:82914ms step_avg:57.94ms
step:1432/2330 train_time:82974ms step_avg:57.94ms
step:1433/2330 train_time:83031ms step_avg:57.94ms
step:1434/2330 train_time:83091ms step_avg:57.94ms
step:1435/2330 train_time:83148ms step_avg:57.94ms
step:1436/2330 train_time:83209ms step_avg:57.95ms
step:1437/2330 train_time:83266ms step_avg:57.94ms
step:1438/2330 train_time:83327ms step_avg:57.95ms
step:1439/2330 train_time:83383ms step_avg:57.94ms
step:1440/2330 train_time:83444ms step_avg:57.95ms
step:1441/2330 train_time:83501ms step_avg:57.95ms
step:1442/2330 train_time:83561ms step_avg:57.95ms
step:1443/2330 train_time:83617ms step_avg:57.95ms
step:1444/2330 train_time:83677ms step_avg:57.95ms
step:1445/2330 train_time:83734ms step_avg:57.95ms
step:1446/2330 train_time:83794ms step_avg:57.95ms
step:1447/2330 train_time:83851ms step_avg:57.95ms
step:1448/2330 train_time:83911ms step_avg:57.95ms
step:1449/2330 train_time:83968ms step_avg:57.95ms
step:1450/2330 train_time:84028ms step_avg:57.95ms
step:1451/2330 train_time:84085ms step_avg:57.95ms
step:1452/2330 train_time:84146ms step_avg:57.95ms
step:1453/2330 train_time:84203ms step_avg:57.95ms
step:1454/2330 train_time:84264ms step_avg:57.95ms
step:1455/2330 train_time:84320ms step_avg:57.95ms
step:1456/2330 train_time:84381ms step_avg:57.95ms
step:1457/2330 train_time:84438ms step_avg:57.95ms
step:1458/2330 train_time:84497ms step_avg:57.95ms
step:1459/2330 train_time:84554ms step_avg:57.95ms
step:1460/2330 train_time:84614ms step_avg:57.96ms
step:1461/2330 train_time:84670ms step_avg:57.95ms
step:1462/2330 train_time:84732ms step_avg:57.96ms
step:1463/2330 train_time:84788ms step_avg:57.96ms
step:1464/2330 train_time:84850ms step_avg:57.96ms
step:1465/2330 train_time:84906ms step_avg:57.96ms
step:1466/2330 train_time:84967ms step_avg:57.96ms
step:1467/2330 train_time:85024ms step_avg:57.96ms
step:1468/2330 train_time:85084ms step_avg:57.96ms
step:1469/2330 train_time:85140ms step_avg:57.96ms
step:1470/2330 train_time:85201ms step_avg:57.96ms
step:1471/2330 train_time:85259ms step_avg:57.96ms
step:1472/2330 train_time:85318ms step_avg:57.96ms
step:1473/2330 train_time:85375ms step_avg:57.96ms
step:1474/2330 train_time:85435ms step_avg:57.96ms
step:1475/2330 train_time:85492ms step_avg:57.96ms
step:1476/2330 train_time:85553ms step_avg:57.96ms
step:1477/2330 train_time:85610ms step_avg:57.96ms
step:1478/2330 train_time:85670ms step_avg:57.96ms
step:1479/2330 train_time:85727ms step_avg:57.96ms
step:1480/2330 train_time:85788ms step_avg:57.96ms
step:1481/2330 train_time:85843ms step_avg:57.96ms
step:1482/2330 train_time:85905ms step_avg:57.97ms
step:1483/2330 train_time:85961ms step_avg:57.96ms
step:1484/2330 train_time:86022ms step_avg:57.97ms
step:1485/2330 train_time:86078ms step_avg:57.96ms
step:1486/2330 train_time:86139ms step_avg:57.97ms
step:1487/2330 train_time:86196ms step_avg:57.97ms
step:1488/2330 train_time:86257ms step_avg:57.97ms
step:1489/2330 train_time:86314ms step_avg:57.97ms
step:1490/2330 train_time:86375ms step_avg:57.97ms
step:1491/2330 train_time:86432ms step_avg:57.97ms
step:1492/2330 train_time:86492ms step_avg:57.97ms
step:1493/2330 train_time:86550ms step_avg:57.97ms
step:1494/2330 train_time:86610ms step_avg:57.97ms
step:1495/2330 train_time:86667ms step_avg:57.97ms
step:1496/2330 train_time:86728ms step_avg:57.97ms
step:1497/2330 train_time:86784ms step_avg:57.97ms
step:1498/2330 train_time:86845ms step_avg:57.97ms
step:1499/2330 train_time:86902ms step_avg:57.97ms
step:1500/2330 train_time:86961ms step_avg:57.97ms
step:1500/2330 val_loss:5.1118 train_time:87041ms step_avg:58.03ms
step:1501/2330 train_time:87060ms step_avg:58.00ms
step:1502/2330 train_time:87083ms step_avg:57.98ms
step:1503/2330 train_time:87142ms step_avg:57.98ms
step:1504/2330 train_time:87204ms step_avg:57.98ms
step:1505/2330 train_time:87262ms step_avg:57.98ms
step:1506/2330 train_time:87322ms step_avg:57.98ms
step:1507/2330 train_time:87379ms step_avg:57.98ms
step:1508/2330 train_time:87439ms step_avg:57.98ms
step:1509/2330 train_time:87495ms step_avg:57.98ms
step:1510/2330 train_time:87556ms step_avg:57.98ms
step:1511/2330 train_time:87612ms step_avg:57.98ms
step:1512/2330 train_time:87672ms step_avg:57.98ms
step:1513/2330 train_time:87728ms step_avg:57.98ms
step:1514/2330 train_time:87788ms step_avg:57.98ms
step:1515/2330 train_time:87844ms step_avg:57.98ms
step:1516/2330 train_time:87903ms step_avg:57.98ms
step:1517/2330 train_time:87960ms step_avg:57.98ms
step:1518/2330 train_time:88020ms step_avg:57.98ms
step:1519/2330 train_time:88080ms step_avg:57.99ms
step:1520/2330 train_time:88142ms step_avg:57.99ms
step:1521/2330 train_time:88201ms step_avg:57.99ms
step:1522/2330 train_time:88261ms step_avg:57.99ms
step:1523/2330 train_time:88319ms step_avg:57.99ms
step:1524/2330 train_time:88380ms step_avg:57.99ms
step:1525/2330 train_time:88438ms step_avg:57.99ms
step:1526/2330 train_time:88497ms step_avg:57.99ms
step:1527/2330 train_time:88554ms step_avg:57.99ms
step:1528/2330 train_time:88615ms step_avg:57.99ms
step:1529/2330 train_time:88672ms step_avg:57.99ms
step:1530/2330 train_time:88732ms step_avg:57.99ms
step:1531/2330 train_time:88789ms step_avg:57.99ms
step:1532/2330 train_time:88849ms step_avg:58.00ms
step:1533/2330 train_time:88905ms step_avg:57.99ms
step:1534/2330 train_time:88966ms step_avg:58.00ms
step:1535/2330 train_time:89023ms step_avg:58.00ms
step:1536/2330 train_time:89085ms step_avg:58.00ms
step:1537/2330 train_time:89143ms step_avg:58.00ms
step:1538/2330 train_time:89204ms step_avg:58.00ms
step:1539/2330 train_time:89262ms step_avg:58.00ms
step:1540/2330 train_time:89323ms step_avg:58.00ms
step:1541/2330 train_time:89381ms step_avg:58.00ms
step:1542/2330 train_time:89442ms step_avg:58.00ms
step:1543/2330 train_time:89500ms step_avg:58.00ms
step:1544/2330 train_time:89560ms step_avg:58.01ms
step:1545/2330 train_time:89618ms step_avg:58.01ms
step:1546/2330 train_time:89679ms step_avg:58.01ms
step:1547/2330 train_time:89736ms step_avg:58.01ms
step:1548/2330 train_time:89796ms step_avg:58.01ms
step:1549/2330 train_time:89853ms step_avg:58.01ms
step:1550/2330 train_time:89914ms step_avg:58.01ms
step:1551/2330 train_time:89971ms step_avg:58.01ms
step:1552/2330 train_time:90032ms step_avg:58.01ms
step:1553/2330 train_time:90089ms step_avg:58.01ms
step:1554/2330 train_time:90152ms step_avg:58.01ms
step:1555/2330 train_time:90208ms step_avg:58.01ms
step:1556/2330 train_time:90272ms step_avg:58.02ms
step:1557/2330 train_time:90328ms step_avg:58.01ms
step:1558/2330 train_time:90391ms step_avg:58.02ms
step:1559/2330 train_time:90447ms step_avg:58.02ms
step:1560/2330 train_time:90508ms step_avg:58.02ms
step:1561/2330 train_time:90565ms step_avg:58.02ms
step:1562/2330 train_time:90627ms step_avg:58.02ms
step:1563/2330 train_time:90684ms step_avg:58.02ms
step:1564/2330 train_time:90744ms step_avg:58.02ms
step:1565/2330 train_time:90801ms step_avg:58.02ms
step:1566/2330 train_time:90862ms step_avg:58.02ms
step:1567/2330 train_time:90920ms step_avg:58.02ms
step:1568/2330 train_time:90981ms step_avg:58.02ms
step:1569/2330 train_time:91039ms step_avg:58.02ms
step:1570/2330 train_time:91100ms step_avg:58.03ms
step:1571/2330 train_time:91157ms step_avg:58.02ms
step:1572/2330 train_time:91219ms step_avg:58.03ms
step:1573/2330 train_time:91277ms step_avg:58.03ms
step:1574/2330 train_time:91339ms step_avg:58.03ms
step:1575/2330 train_time:91396ms step_avg:58.03ms
step:1576/2330 train_time:91458ms step_avg:58.03ms
step:1577/2330 train_time:91515ms step_avg:58.03ms
step:1578/2330 train_time:91578ms step_avg:58.03ms
step:1579/2330 train_time:91635ms step_avg:58.03ms
step:1580/2330 train_time:91696ms step_avg:58.04ms
step:1581/2330 train_time:91753ms step_avg:58.03ms
step:1582/2330 train_time:91814ms step_avg:58.04ms
step:1583/2330 train_time:91871ms step_avg:58.04ms
step:1584/2330 train_time:91934ms step_avg:58.04ms
step:1585/2330 train_time:91990ms step_avg:58.04ms
step:1586/2330 train_time:92053ms step_avg:58.04ms
step:1587/2330 train_time:92108ms step_avg:58.04ms
step:1588/2330 train_time:92173ms step_avg:58.04ms
step:1589/2330 train_time:92229ms step_avg:58.04ms
step:1590/2330 train_time:92292ms step_avg:58.05ms
step:1591/2330 train_time:92349ms step_avg:58.04ms
step:1592/2330 train_time:92412ms step_avg:58.05ms
step:1593/2330 train_time:92468ms step_avg:58.05ms
step:1594/2330 train_time:92531ms step_avg:58.05ms
step:1595/2330 train_time:92588ms step_avg:58.05ms
step:1596/2330 train_time:92648ms step_avg:58.05ms
step:1597/2330 train_time:92704ms step_avg:58.05ms
step:1598/2330 train_time:92765ms step_avg:58.05ms
step:1599/2330 train_time:92822ms step_avg:58.05ms
step:1600/2330 train_time:92884ms step_avg:58.05ms
step:1601/2330 train_time:92942ms step_avg:58.05ms
step:1602/2330 train_time:93002ms step_avg:58.05ms
step:1603/2330 train_time:93060ms step_avg:58.05ms
step:1604/2330 train_time:93121ms step_avg:58.06ms
step:1605/2330 train_time:93180ms step_avg:58.06ms
step:1606/2330 train_time:93240ms step_avg:58.06ms
step:1607/2330 train_time:93298ms step_avg:58.06ms
step:1608/2330 train_time:93359ms step_avg:58.06ms
step:1609/2330 train_time:93416ms step_avg:58.06ms
step:1610/2330 train_time:93477ms step_avg:58.06ms
step:1611/2330 train_time:93534ms step_avg:58.06ms
step:1612/2330 train_time:93596ms step_avg:58.06ms
step:1613/2330 train_time:93653ms step_avg:58.06ms
step:1614/2330 train_time:93714ms step_avg:58.06ms
step:1615/2330 train_time:93771ms step_avg:58.06ms
step:1616/2330 train_time:93834ms step_avg:58.07ms
step:1617/2330 train_time:93890ms step_avg:58.06ms
step:1618/2330 train_time:93954ms step_avg:58.07ms
step:1619/2330 train_time:94010ms step_avg:58.07ms
step:1620/2330 train_time:94072ms step_avg:58.07ms
step:1621/2330 train_time:94128ms step_avg:58.07ms
step:1622/2330 train_time:94192ms step_avg:58.07ms
step:1623/2330 train_time:94249ms step_avg:58.07ms
step:1624/2330 train_time:94309ms step_avg:58.07ms
step:1625/2330 train_time:94366ms step_avg:58.07ms
step:1626/2330 train_time:94427ms step_avg:58.07ms
step:1627/2330 train_time:94484ms step_avg:58.07ms
step:1628/2330 train_time:94545ms step_avg:58.07ms
step:1629/2330 train_time:94602ms step_avg:58.07ms
step:1630/2330 train_time:94663ms step_avg:58.08ms
step:1631/2330 train_time:94720ms step_avg:58.07ms
step:1632/2330 train_time:94781ms step_avg:58.08ms
step:1633/2330 train_time:94838ms step_avg:58.08ms
step:1634/2330 train_time:94899ms step_avg:58.08ms
step:1635/2330 train_time:94956ms step_avg:58.08ms
step:1636/2330 train_time:95018ms step_avg:58.08ms
step:1637/2330 train_time:95076ms step_avg:58.08ms
step:1638/2330 train_time:95137ms step_avg:58.08ms
step:1639/2330 train_time:95195ms step_avg:58.08ms
step:1640/2330 train_time:95257ms step_avg:58.08ms
step:1641/2330 train_time:95314ms step_avg:58.08ms
step:1642/2330 train_time:95376ms step_avg:58.09ms
step:1643/2330 train_time:95432ms step_avg:58.08ms
step:1644/2330 train_time:95494ms step_avg:58.09ms
step:1645/2330 train_time:95550ms step_avg:58.09ms
step:1646/2330 train_time:95612ms step_avg:58.09ms
step:1647/2330 train_time:95669ms step_avg:58.09ms
step:1648/2330 train_time:95730ms step_avg:58.09ms
step:1649/2330 train_time:95787ms step_avg:58.09ms
step:1650/2330 train_time:95848ms step_avg:58.09ms
step:1651/2330 train_time:95904ms step_avg:58.09ms
step:1652/2330 train_time:95965ms step_avg:58.09ms
step:1653/2330 train_time:96023ms step_avg:58.09ms
step:1654/2330 train_time:96084ms step_avg:58.09ms
step:1655/2330 train_time:96141ms step_avg:58.09ms
step:1656/2330 train_time:96201ms step_avg:58.09ms
step:1657/2330 train_time:96259ms step_avg:58.09ms
step:1658/2330 train_time:96320ms step_avg:58.09ms
step:1659/2330 train_time:96379ms step_avg:58.09ms
step:1660/2330 train_time:96439ms step_avg:58.10ms
step:1661/2330 train_time:96496ms step_avg:58.10ms
step:1662/2330 train_time:96557ms step_avg:58.10ms
step:1663/2330 train_time:96615ms step_avg:58.10ms
step:1664/2330 train_time:96675ms step_avg:58.10ms
step:1665/2330 train_time:96732ms step_avg:58.10ms
step:1666/2330 train_time:96795ms step_avg:58.10ms
step:1667/2330 train_time:96852ms step_avg:58.10ms
step:1668/2330 train_time:96913ms step_avg:58.10ms
step:1669/2330 train_time:96970ms step_avg:58.10ms
step:1670/2330 train_time:97031ms step_avg:58.10ms
step:1671/2330 train_time:97088ms step_avg:58.10ms
step:1672/2330 train_time:97148ms step_avg:58.10ms
step:1673/2330 train_time:97205ms step_avg:58.10ms
step:1674/2330 train_time:97267ms step_avg:58.10ms
step:1675/2330 train_time:97323ms step_avg:58.10ms
step:1676/2330 train_time:97385ms step_avg:58.11ms
step:1677/2330 train_time:97441ms step_avg:58.10ms
step:1678/2330 train_time:97502ms step_avg:58.11ms
step:1679/2330 train_time:97559ms step_avg:58.11ms
step:1680/2330 train_time:97620ms step_avg:58.11ms
step:1681/2330 train_time:97678ms step_avg:58.11ms
step:1682/2330 train_time:97739ms step_avg:58.11ms
step:1683/2330 train_time:97798ms step_avg:58.11ms
step:1684/2330 train_time:97859ms step_avg:58.11ms
step:1685/2330 train_time:97917ms step_avg:58.11ms
step:1686/2330 train_time:97978ms step_avg:58.11ms
step:1687/2330 train_time:98035ms step_avg:58.11ms
step:1688/2330 train_time:98097ms step_avg:58.11ms
step:1689/2330 train_time:98154ms step_avg:58.11ms
step:1690/2330 train_time:98215ms step_avg:58.12ms
step:1691/2330 train_time:98273ms step_avg:58.12ms
step:1692/2330 train_time:98334ms step_avg:58.12ms
step:1693/2330 train_time:98391ms step_avg:58.12ms
step:1694/2330 train_time:98453ms step_avg:58.12ms
step:1695/2330 train_time:98510ms step_avg:58.12ms
step:1696/2330 train_time:98572ms step_avg:58.12ms
step:1697/2330 train_time:98629ms step_avg:58.12ms
step:1698/2330 train_time:98690ms step_avg:58.12ms
step:1699/2330 train_time:98747ms step_avg:58.12ms
step:1700/2330 train_time:98807ms step_avg:58.12ms
step:1701/2330 train_time:98864ms step_avg:58.12ms
step:1702/2330 train_time:98926ms step_avg:58.12ms
step:1703/2330 train_time:98983ms step_avg:58.12ms
step:1704/2330 train_time:99043ms step_avg:58.12ms
step:1705/2330 train_time:99100ms step_avg:58.12ms
step:1706/2330 train_time:99161ms step_avg:58.13ms
step:1707/2330 train_time:99219ms step_avg:58.12ms
step:1708/2330 train_time:99280ms step_avg:58.13ms
step:1709/2330 train_time:99339ms step_avg:58.13ms
step:1710/2330 train_time:99399ms step_avg:58.13ms
step:1711/2330 train_time:99456ms step_avg:58.13ms
step:1712/2330 train_time:99517ms step_avg:58.13ms
step:1713/2330 train_time:99574ms step_avg:58.13ms
step:1714/2330 train_time:99637ms step_avg:58.13ms
step:1715/2330 train_time:99695ms step_avg:58.13ms
step:1716/2330 train_time:99756ms step_avg:58.13ms
step:1717/2330 train_time:99813ms step_avg:58.13ms
step:1718/2330 train_time:99875ms step_avg:58.13ms
step:1719/2330 train_time:99932ms step_avg:58.13ms
step:1720/2330 train_time:99995ms step_avg:58.14ms
step:1721/2330 train_time:100051ms step_avg:58.14ms
step:1722/2330 train_time:100114ms step_avg:58.14ms
step:1723/2330 train_time:100170ms step_avg:58.14ms
step:1724/2330 train_time:100233ms step_avg:58.14ms
step:1725/2330 train_time:100289ms step_avg:58.14ms
step:1726/2330 train_time:100351ms step_avg:58.14ms
step:1727/2330 train_time:100408ms step_avg:58.14ms
step:1728/2330 train_time:100470ms step_avg:58.14ms
step:1729/2330 train_time:100526ms step_avg:58.14ms
step:1730/2330 train_time:100588ms step_avg:58.14ms
step:1731/2330 train_time:100645ms step_avg:58.14ms
step:1732/2330 train_time:100705ms step_avg:58.14ms
step:1733/2330 train_time:100762ms step_avg:58.14ms
step:1734/2330 train_time:100823ms step_avg:58.14ms
step:1735/2330 train_time:100881ms step_avg:58.14ms
step:1736/2330 train_time:100942ms step_avg:58.15ms
step:1737/2330 train_time:100999ms step_avg:58.15ms
step:1738/2330 train_time:101060ms step_avg:58.15ms
step:1739/2330 train_time:101117ms step_avg:58.15ms
step:1740/2330 train_time:101179ms step_avg:58.15ms
step:1741/2330 train_time:101236ms step_avg:58.15ms
step:1742/2330 train_time:101297ms step_avg:58.15ms
step:1743/2330 train_time:101355ms step_avg:58.15ms
step:1744/2330 train_time:101416ms step_avg:58.15ms
step:1745/2330 train_time:101474ms step_avg:58.15ms
step:1746/2330 train_time:101536ms step_avg:58.15ms
step:1747/2330 train_time:101592ms step_avg:58.15ms
step:1748/2330 train_time:101655ms step_avg:58.15ms
step:1749/2330 train_time:101712ms step_avg:58.15ms
step:1750/2330 train_time:101773ms step_avg:58.16ms
step:1750/2330 val_loss:4.9612 train_time:101856ms step_avg:58.20ms
step:1751/2330 train_time:101876ms step_avg:58.18ms
step:1752/2330 train_time:101896ms step_avg:58.16ms
step:1753/2330 train_time:101949ms step_avg:58.16ms
step:1754/2330 train_time:102017ms step_avg:58.16ms
step:1755/2330 train_time:102073ms step_avg:58.16ms
step:1756/2330 train_time:102136ms step_avg:58.16ms
step:1757/2330 train_time:102193ms step_avg:58.16ms
step:1758/2330 train_time:102252ms step_avg:58.16ms
step:1759/2330 train_time:102309ms step_avg:58.16ms
step:1760/2330 train_time:102369ms step_avg:58.16ms
step:1761/2330 train_time:102425ms step_avg:58.16ms
step:1762/2330 train_time:102485ms step_avg:58.16ms
step:1763/2330 train_time:102542ms step_avg:58.16ms
step:1764/2330 train_time:102601ms step_avg:58.16ms
step:1765/2330 train_time:102658ms step_avg:58.16ms
step:1766/2330 train_time:102717ms step_avg:58.16ms
step:1767/2330 train_time:102776ms step_avg:58.16ms
step:1768/2330 train_time:102840ms step_avg:58.17ms
step:1769/2330 train_time:102900ms step_avg:58.17ms
step:1770/2330 train_time:102960ms step_avg:58.17ms
step:1771/2330 train_time:103019ms step_avg:58.17ms
step:1772/2330 train_time:103079ms step_avg:58.17ms
step:1773/2330 train_time:103136ms step_avg:58.17ms
step:1774/2330 train_time:103196ms step_avg:58.17ms
step:1775/2330 train_time:103253ms step_avg:58.17ms
step:1776/2330 train_time:103313ms step_avg:58.17ms
step:1777/2330 train_time:103370ms step_avg:58.17ms
step:1778/2330 train_time:103431ms step_avg:58.17ms
step:1779/2330 train_time:103488ms step_avg:58.17ms
step:1780/2330 train_time:103549ms step_avg:58.17ms
step:1781/2330 train_time:103605ms step_avg:58.17ms
step:1782/2330 train_time:103665ms step_avg:58.17ms
step:1783/2330 train_time:103722ms step_avg:58.17ms
step:1784/2330 train_time:103783ms step_avg:58.17ms
step:1785/2330 train_time:103841ms step_avg:58.17ms
step:1786/2330 train_time:103903ms step_avg:58.18ms
step:1787/2330 train_time:103960ms step_avg:58.18ms
step:1788/2330 train_time:104022ms step_avg:58.18ms
step:1789/2330 train_time:104079ms step_avg:58.18ms
step:1790/2330 train_time:104142ms step_avg:58.18ms
step:1791/2330 train_time:104199ms step_avg:58.18ms
step:1792/2330 train_time:104259ms step_avg:58.18ms
step:1793/2330 train_time:104317ms step_avg:58.18ms
step:1794/2330 train_time:104377ms step_avg:58.18ms
step:1795/2330 train_time:104434ms step_avg:58.18ms
step:1796/2330 train_time:104494ms step_avg:58.18ms
step:1797/2330 train_time:104551ms step_avg:58.18ms
step:1798/2330 train_time:104611ms step_avg:58.18ms
step:1799/2330 train_time:104668ms step_avg:58.18ms
step:1800/2330 train_time:104730ms step_avg:58.18ms
step:1801/2330 train_time:104788ms step_avg:58.18ms
step:1802/2330 train_time:104850ms step_avg:58.19ms
step:1803/2330 train_time:104907ms step_avg:58.18ms
step:1804/2330 train_time:104969ms step_avg:58.19ms
step:1805/2330 train_time:105026ms step_avg:58.19ms
step:1806/2330 train_time:105090ms step_avg:58.19ms
step:1807/2330 train_time:105146ms step_avg:58.19ms
step:1808/2330 train_time:105208ms step_avg:58.19ms
step:1809/2330 train_time:105264ms step_avg:58.19ms
step:1810/2330 train_time:105325ms step_avg:58.19ms
step:1811/2330 train_time:105382ms step_avg:58.19ms
step:1812/2330 train_time:105444ms step_avg:58.19ms
step:1813/2330 train_time:105500ms step_avg:58.19ms
step:1814/2330 train_time:105561ms step_avg:58.19ms
step:1815/2330 train_time:105618ms step_avg:58.19ms
step:1816/2330 train_time:105678ms step_avg:58.19ms
step:1817/2330 train_time:105736ms step_avg:58.19ms
step:1818/2330 train_time:105797ms step_avg:58.19ms
step:1819/2330 train_time:105856ms step_avg:58.19ms
step:1820/2330 train_time:105916ms step_avg:58.20ms
step:1821/2330 train_time:105974ms step_avg:58.20ms
step:1822/2330 train_time:106035ms step_avg:58.20ms
step:1823/2330 train_time:106094ms step_avg:58.20ms
step:1824/2330 train_time:106154ms step_avg:58.20ms
step:1825/2330 train_time:106212ms step_avg:58.20ms
step:1826/2330 train_time:106273ms step_avg:58.20ms
step:1827/2330 train_time:106330ms step_avg:58.20ms
step:1828/2330 train_time:106391ms step_avg:58.20ms
step:1829/2330 train_time:106448ms step_avg:58.20ms
step:1830/2330 train_time:106509ms step_avg:58.20ms
step:1831/2330 train_time:106566ms step_avg:58.20ms
step:1832/2330 train_time:106628ms step_avg:58.20ms
step:1833/2330 train_time:106684ms step_avg:58.20ms
step:1834/2330 train_time:106746ms step_avg:58.20ms
step:1835/2330 train_time:106802ms step_avg:58.20ms
step:1836/2330 train_time:106863ms step_avg:58.20ms
step:1837/2330 train_time:106921ms step_avg:58.20ms
step:1838/2330 train_time:106981ms step_avg:58.21ms
step:1839/2330 train_time:107038ms step_avg:58.20ms
step:1840/2330 train_time:107099ms step_avg:58.21ms
step:1841/2330 train_time:107157ms step_avg:58.21ms
step:1842/2330 train_time:107217ms step_avg:58.21ms
step:1843/2330 train_time:107274ms step_avg:58.21ms
step:1844/2330 train_time:107335ms step_avg:58.21ms
step:1845/2330 train_time:107394ms step_avg:58.21ms
step:1846/2330 train_time:107454ms step_avg:58.21ms
step:1847/2330 train_time:107513ms step_avg:58.21ms
step:1848/2330 train_time:107573ms step_avg:58.21ms
step:1849/2330 train_time:107630ms step_avg:58.21ms
step:1850/2330 train_time:107691ms step_avg:58.21ms
step:1851/2330 train_time:107748ms step_avg:58.21ms
step:1852/2330 train_time:107809ms step_avg:58.21ms
step:1853/2330 train_time:107865ms step_avg:58.21ms
step:1854/2330 train_time:107928ms step_avg:58.21ms
step:1855/2330 train_time:107984ms step_avg:58.21ms
step:1856/2330 train_time:108048ms step_avg:58.22ms
step:1857/2330 train_time:108105ms step_avg:58.21ms
step:1858/2330 train_time:108167ms step_avg:58.22ms
step:1859/2330 train_time:108224ms step_avg:58.22ms
step:1860/2330 train_time:108286ms step_avg:58.22ms
step:1861/2330 train_time:108343ms step_avg:58.22ms
step:1862/2330 train_time:108404ms step_avg:58.22ms
step:1863/2330 train_time:108460ms step_avg:58.22ms
step:1864/2330 train_time:108521ms step_avg:58.22ms
step:1865/2330 train_time:108577ms step_avg:58.22ms
step:1866/2330 train_time:108639ms step_avg:58.22ms
step:1867/2330 train_time:108696ms step_avg:58.22ms
step:1868/2330 train_time:108756ms step_avg:58.22ms
step:1869/2330 train_time:108814ms step_avg:58.22ms
step:1870/2330 train_time:108874ms step_avg:58.22ms
step:1871/2330 train_time:108932ms step_avg:58.22ms
step:1872/2330 train_time:108993ms step_avg:58.22ms
step:1873/2330 train_time:109052ms step_avg:58.22ms
step:1874/2330 train_time:109112ms step_avg:58.22ms
step:1875/2330 train_time:109171ms step_avg:58.22ms
step:1876/2330 train_time:109231ms step_avg:58.23ms
step:1877/2330 train_time:109289ms step_avg:58.23ms
step:1878/2330 train_time:109350ms step_avg:58.23ms
step:1879/2330 train_time:109407ms step_avg:58.23ms
step:1880/2330 train_time:109467ms step_avg:58.23ms
step:1881/2330 train_time:109524ms step_avg:58.23ms
step:1882/2330 train_time:109586ms step_avg:58.23ms
step:1883/2330 train_time:109642ms step_avg:58.23ms
step:1884/2330 train_time:109703ms step_avg:58.23ms
step:1885/2330 train_time:109760ms step_avg:58.23ms
step:1886/2330 train_time:109821ms step_avg:58.23ms
step:1887/2330 train_time:109878ms step_avg:58.23ms
step:1888/2330 train_time:109938ms step_avg:58.23ms
step:1889/2330 train_time:109996ms step_avg:58.23ms
step:1890/2330 train_time:110056ms step_avg:58.23ms
step:1891/2330 train_time:110115ms step_avg:58.23ms
step:1892/2330 train_time:110175ms step_avg:58.23ms
step:1893/2330 train_time:110234ms step_avg:58.23ms
step:1894/2330 train_time:110294ms step_avg:58.23ms
step:1895/2330 train_time:110351ms step_avg:58.23ms
step:1896/2330 train_time:110412ms step_avg:58.23ms
step:1897/2330 train_time:110469ms step_avg:58.23ms
step:1898/2330 train_time:110531ms step_avg:58.24ms
step:1899/2330 train_time:110589ms step_avg:58.24ms
step:1900/2330 train_time:110649ms step_avg:58.24ms
step:1901/2330 train_time:110706ms step_avg:58.24ms
step:1902/2330 train_time:110767ms step_avg:58.24ms
step:1903/2330 train_time:110823ms step_avg:58.24ms
step:1904/2330 train_time:110885ms step_avg:58.24ms
step:1905/2330 train_time:110941ms step_avg:58.24ms
step:1906/2330 train_time:111002ms step_avg:58.24ms
step:1907/2330 train_time:111059ms step_avg:58.24ms
step:1908/2330 train_time:111120ms step_avg:58.24ms
step:1909/2330 train_time:111177ms step_avg:58.24ms
step:1910/2330 train_time:111238ms step_avg:58.24ms
step:1911/2330 train_time:111297ms step_avg:58.24ms
step:1912/2330 train_time:111357ms step_avg:58.24ms
step:1913/2330 train_time:111415ms step_avg:58.24ms
step:1914/2330 train_time:111476ms step_avg:58.24ms
step:1915/2330 train_time:111534ms step_avg:58.24ms
step:1916/2330 train_time:111595ms step_avg:58.24ms
step:1917/2330 train_time:111653ms step_avg:58.24ms
step:1918/2330 train_time:111714ms step_avg:58.24ms
step:1919/2330 train_time:111771ms step_avg:58.24ms
step:1920/2330 train_time:111832ms step_avg:58.25ms
step:1921/2330 train_time:111889ms step_avg:58.24ms
step:1922/2330 train_time:111951ms step_avg:58.25ms
step:1923/2330 train_time:112007ms step_avg:58.25ms
step:1924/2330 train_time:112069ms step_avg:58.25ms
step:1925/2330 train_time:112125ms step_avg:58.25ms
step:1926/2330 train_time:112187ms step_avg:58.25ms
step:1927/2330 train_time:112243ms step_avg:58.25ms
step:1928/2330 train_time:112305ms step_avg:58.25ms
step:1929/2330 train_time:112361ms step_avg:58.25ms
step:1930/2330 train_time:112423ms step_avg:58.25ms
step:1931/2330 train_time:112480ms step_avg:58.25ms
step:1932/2330 train_time:112541ms step_avg:58.25ms
step:1933/2330 train_time:112599ms step_avg:58.25ms
step:1934/2330 train_time:112659ms step_avg:58.25ms
step:1935/2330 train_time:112717ms step_avg:58.25ms
step:1936/2330 train_time:112778ms step_avg:58.25ms
step:1937/2330 train_time:112836ms step_avg:58.25ms
step:1938/2330 train_time:112896ms step_avg:58.25ms
step:1939/2330 train_time:112954ms step_avg:58.25ms
step:1940/2330 train_time:113014ms step_avg:58.25ms
step:1941/2330 train_time:113071ms step_avg:58.25ms
step:1942/2330 train_time:113132ms step_avg:58.26ms
step:1943/2330 train_time:113189ms step_avg:58.25ms
step:1944/2330 train_time:113251ms step_avg:58.26ms
step:1945/2330 train_time:113308ms step_avg:58.26ms
step:1946/2330 train_time:113369ms step_avg:58.26ms
step:1947/2330 train_time:113426ms step_avg:58.26ms
step:1948/2330 train_time:113487ms step_avg:58.26ms
step:1949/2330 train_time:113543ms step_avg:58.26ms
step:1950/2330 train_time:113604ms step_avg:58.26ms
step:1951/2330 train_time:113660ms step_avg:58.26ms
step:1952/2330 train_time:113723ms step_avg:58.26ms
step:1953/2330 train_time:113779ms step_avg:58.26ms
step:1954/2330 train_time:113840ms step_avg:58.26ms
step:1955/2330 train_time:113897ms step_avg:58.26ms
step:1956/2330 train_time:113957ms step_avg:58.26ms
step:1957/2330 train_time:114015ms step_avg:58.26ms
step:1958/2330 train_time:114075ms step_avg:58.26ms
step:1959/2330 train_time:114133ms step_avg:58.26ms
step:1960/2330 train_time:114193ms step_avg:58.26ms
step:1961/2330 train_time:114251ms step_avg:58.26ms
step:1962/2330 train_time:114312ms step_avg:58.26ms
step:1963/2330 train_time:114369ms step_avg:58.26ms
step:1964/2330 train_time:114431ms step_avg:58.26ms
step:1965/2330 train_time:114488ms step_avg:58.26ms
step:1966/2330 train_time:114549ms step_avg:58.27ms
step:1967/2330 train_time:114606ms step_avg:58.26ms
step:1968/2330 train_time:114667ms step_avg:58.27ms
step:1969/2330 train_time:114724ms step_avg:58.27ms
step:1970/2330 train_time:114786ms step_avg:58.27ms
step:1971/2330 train_time:114842ms step_avg:58.27ms
step:1972/2330 train_time:114903ms step_avg:58.27ms
step:1973/2330 train_time:114959ms step_avg:58.27ms
step:1974/2330 train_time:115020ms step_avg:58.27ms
step:1975/2330 train_time:115078ms step_avg:58.27ms
step:1976/2330 train_time:115138ms step_avg:58.27ms
step:1977/2330 train_time:115195ms step_avg:58.27ms
step:1978/2330 train_time:115255ms step_avg:58.27ms
step:1979/2330 train_time:115313ms step_avg:58.27ms
step:1980/2330 train_time:115374ms step_avg:58.27ms
step:1981/2330 train_time:115433ms step_avg:58.27ms
step:1982/2330 train_time:115494ms step_avg:58.27ms
step:1983/2330 train_time:115552ms step_avg:58.27ms
step:1984/2330 train_time:115613ms step_avg:58.27ms
step:1985/2330 train_time:115671ms step_avg:58.27ms
step:1986/2330 train_time:115733ms step_avg:58.27ms
step:1987/2330 train_time:115790ms step_avg:58.27ms
step:1988/2330 train_time:115851ms step_avg:58.28ms
step:1989/2330 train_time:115908ms step_avg:58.27ms
step:1990/2330 train_time:115969ms step_avg:58.28ms
step:1991/2330 train_time:116025ms step_avg:58.27ms
step:1992/2330 train_time:116087ms step_avg:58.28ms
step:1993/2330 train_time:116144ms step_avg:58.28ms
step:1994/2330 train_time:116205ms step_avg:58.28ms
step:1995/2330 train_time:116261ms step_avg:58.28ms
step:1996/2330 train_time:116323ms step_avg:58.28ms
step:1997/2330 train_time:116380ms step_avg:58.28ms
step:1998/2330 train_time:116440ms step_avg:58.28ms
step:1999/2330 train_time:116497ms step_avg:58.28ms
step:2000/2330 train_time:116558ms step_avg:58.28ms
step:2000/2330 val_loss:4.8547 train_time:116639ms step_avg:58.32ms
step:2001/2330 train_time:116658ms step_avg:58.30ms
step:2002/2330 train_time:116680ms step_avg:58.28ms
step:2003/2330 train_time:116740ms step_avg:58.28ms
step:2004/2330 train_time:116805ms step_avg:58.29ms
step:2005/2330 train_time:116864ms step_avg:58.29ms
step:2006/2330 train_time:116924ms step_avg:58.29ms
step:2007/2330 train_time:116981ms step_avg:58.29ms
step:2008/2330 train_time:117040ms step_avg:58.29ms
step:2009/2330 train_time:117096ms step_avg:58.29ms
step:2010/2330 train_time:117157ms step_avg:58.29ms
step:2011/2330 train_time:117214ms step_avg:58.29ms
step:2012/2330 train_time:117273ms step_avg:58.29ms
step:2013/2330 train_time:117330ms step_avg:58.29ms
step:2014/2330 train_time:117390ms step_avg:58.29ms
step:2015/2330 train_time:117446ms step_avg:58.29ms
step:2016/2330 train_time:117506ms step_avg:58.29ms
step:2017/2330 train_time:117563ms step_avg:58.29ms
step:2018/2330 train_time:117624ms step_avg:58.29ms
step:2019/2330 train_time:117682ms step_avg:58.29ms
step:2020/2330 train_time:117744ms step_avg:58.29ms
step:2021/2330 train_time:117802ms step_avg:58.29ms
step:2022/2330 train_time:117864ms step_avg:58.29ms
step:2023/2330 train_time:117921ms step_avg:58.29ms
step:2024/2330 train_time:117982ms step_avg:58.29ms
step:2025/2330 train_time:118039ms step_avg:58.29ms
step:2026/2330 train_time:118099ms step_avg:58.29ms
step:2027/2330 train_time:118156ms step_avg:58.29ms
step:2028/2330 train_time:118216ms step_avg:58.29ms
step:2029/2330 train_time:118272ms step_avg:58.29ms
step:2030/2330 train_time:118332ms step_avg:58.29ms
step:2031/2330 train_time:118389ms step_avg:58.29ms
step:2032/2330 train_time:118448ms step_avg:58.29ms
step:2033/2330 train_time:118506ms step_avg:58.29ms
step:2034/2330 train_time:118566ms step_avg:58.29ms
step:2035/2330 train_time:118623ms step_avg:58.29ms
step:2036/2330 train_time:118685ms step_avg:58.29ms
step:2037/2330 train_time:118742ms step_avg:58.29ms
step:2038/2330 train_time:118804ms step_avg:58.29ms
step:2039/2330 train_time:118861ms step_avg:58.29ms
step:2040/2330 train_time:118923ms step_avg:58.30ms
step:2041/2330 train_time:118979ms step_avg:58.29ms
step:2042/2330 train_time:119040ms step_avg:58.30ms
step:2043/2330 train_time:119097ms step_avg:58.30ms
step:2044/2330 train_time:119157ms step_avg:58.30ms
step:2045/2330 train_time:119214ms step_avg:58.30ms
step:2046/2330 train_time:119274ms step_avg:58.30ms
step:2047/2330 train_time:119332ms step_avg:58.30ms
step:2048/2330 train_time:119392ms step_avg:58.30ms
step:2049/2330 train_time:119449ms step_avg:58.30ms
step:2050/2330 train_time:119510ms step_avg:58.30ms
step:2051/2330 train_time:119568ms step_avg:58.30ms
step:2052/2330 train_time:119628ms step_avg:58.30ms
step:2053/2330 train_time:119685ms step_avg:58.30ms
step:2054/2330 train_time:119749ms step_avg:58.30ms
step:2055/2330 train_time:119806ms step_avg:58.30ms
step:2056/2330 train_time:119870ms step_avg:58.30ms
step:2057/2330 train_time:119927ms step_avg:58.30ms
step:2058/2330 train_time:119988ms step_avg:58.30ms
step:2059/2330 train_time:120044ms step_avg:58.30ms
step:2060/2330 train_time:120107ms step_avg:58.30ms
step:2061/2330 train_time:120163ms step_avg:58.30ms
step:2062/2330 train_time:120224ms step_avg:58.30ms
step:2063/2330 train_time:120281ms step_avg:58.30ms
step:2064/2330 train_time:120341ms step_avg:58.30ms
step:2065/2330 train_time:120398ms step_avg:58.30ms
step:2066/2330 train_time:120458ms step_avg:58.31ms
step:2067/2330 train_time:120516ms step_avg:58.30ms
step:2068/2330 train_time:120575ms step_avg:58.31ms
step:2069/2330 train_time:120634ms step_avg:58.31ms
step:2070/2330 train_time:120695ms step_avg:58.31ms
step:2071/2330 train_time:120753ms step_avg:58.31ms
step:2072/2330 train_time:120814ms step_avg:58.31ms
step:2073/2330 train_time:120873ms step_avg:58.31ms
step:2074/2330 train_time:120933ms step_avg:58.31ms
step:2075/2330 train_time:120990ms step_avg:58.31ms
step:2076/2330 train_time:121051ms step_avg:58.31ms
step:2077/2330 train_time:121109ms step_avg:58.31ms
step:2078/2330 train_time:121170ms step_avg:58.31ms
step:2079/2330 train_time:121227ms step_avg:58.31ms
step:2080/2330 train_time:121288ms step_avg:58.31ms
step:2081/2330 train_time:121345ms step_avg:58.31ms
step:2082/2330 train_time:121406ms step_avg:58.31ms
step:2083/2330 train_time:121463ms step_avg:58.31ms
step:2084/2330 train_time:121523ms step_avg:58.31ms
step:2085/2330 train_time:121579ms step_avg:58.31ms
step:2086/2330 train_time:121640ms step_avg:58.31ms
step:2087/2330 train_time:121698ms step_avg:58.31ms
step:2088/2330 train_time:121759ms step_avg:58.31ms
step:2089/2330 train_time:121816ms step_avg:58.31ms
step:2090/2330 train_time:121877ms step_avg:58.31ms
step:2091/2330 train_time:121934ms step_avg:58.31ms
step:2092/2330 train_time:121995ms step_avg:58.32ms
step:2093/2330 train_time:122053ms step_avg:58.31ms
step:2094/2330 train_time:122113ms step_avg:58.32ms
step:2095/2330 train_time:122170ms step_avg:58.31ms
step:2096/2330 train_time:122231ms step_avg:58.32ms
step:2097/2330 train_time:122288ms step_avg:58.32ms
step:2098/2330 train_time:122348ms step_avg:58.32ms
step:2099/2330 train_time:122406ms step_avg:58.32ms
step:2100/2330 train_time:122466ms step_avg:58.32ms
step:2101/2330 train_time:122523ms step_avg:58.32ms
step:2102/2330 train_time:122584ms step_avg:58.32ms
step:2103/2330 train_time:122640ms step_avg:58.32ms
step:2104/2330 train_time:122701ms step_avg:58.32ms
step:2105/2330 train_time:122758ms step_avg:58.32ms
step:2106/2330 train_time:122819ms step_avg:58.32ms
step:2107/2330 train_time:122875ms step_avg:58.32ms
step:2108/2330 train_time:122937ms step_avg:58.32ms
step:2109/2330 train_time:122994ms step_avg:58.32ms
step:2110/2330 train_time:123054ms step_avg:58.32ms
step:2111/2330 train_time:123111ms step_avg:58.32ms
step:2112/2330 train_time:123172ms step_avg:58.32ms
step:2113/2330 train_time:123229ms step_avg:58.32ms
step:2114/2330 train_time:123290ms step_avg:58.32ms
step:2115/2330 train_time:123348ms step_avg:58.32ms
step:2116/2330 train_time:123408ms step_avg:58.32ms
step:2117/2330 train_time:123466ms step_avg:58.32ms
step:2118/2330 train_time:123526ms step_avg:58.32ms
step:2119/2330 train_time:123583ms step_avg:58.32ms
step:2120/2330 train_time:123644ms step_avg:58.32ms
step:2121/2330 train_time:123701ms step_avg:58.32ms
step:2122/2330 train_time:123762ms step_avg:58.32ms
step:2123/2330 train_time:123819ms step_avg:58.32ms
step:2124/2330 train_time:123882ms step_avg:58.32ms
step:2125/2330 train_time:123938ms step_avg:58.32ms
step:2126/2330 train_time:123999ms step_avg:58.33ms
step:2127/2330 train_time:124056ms step_avg:58.32ms
step:2128/2330 train_time:124117ms step_avg:58.33ms
step:2129/2330 train_time:124174ms step_avg:58.33ms
step:2130/2330 train_time:124235ms step_avg:58.33ms
step:2131/2330 train_time:124293ms step_avg:58.33ms
step:2132/2330 train_time:124353ms step_avg:58.33ms
step:2133/2330 train_time:124412ms step_avg:58.33ms
step:2134/2330 train_time:124473ms step_avg:58.33ms
step:2135/2330 train_time:124530ms step_avg:58.33ms
step:2136/2330 train_time:124591ms step_avg:58.33ms
step:2137/2330 train_time:124648ms step_avg:58.33ms
step:2138/2330 train_time:124711ms step_avg:58.33ms
step:2139/2330 train_time:124768ms step_avg:58.33ms
step:2140/2330 train_time:124829ms step_avg:58.33ms
step:2141/2330 train_time:124886ms step_avg:58.33ms
step:2142/2330 train_time:124947ms step_avg:58.33ms
step:2143/2330 train_time:125003ms step_avg:58.33ms
step:2144/2330 train_time:125065ms step_avg:58.33ms
step:2145/2330 train_time:125122ms step_avg:58.33ms
step:2146/2330 train_time:125182ms step_avg:58.33ms
step:2147/2330 train_time:125239ms step_avg:58.33ms
step:2148/2330 train_time:125299ms step_avg:58.33ms
step:2149/2330 train_time:125356ms step_avg:58.33ms
step:2150/2330 train_time:125417ms step_avg:58.33ms
step:2151/2330 train_time:125475ms step_avg:58.33ms
step:2152/2330 train_time:125535ms step_avg:58.33ms
step:2153/2330 train_time:125593ms step_avg:58.33ms
step:2154/2330 train_time:125654ms step_avg:58.34ms
step:2155/2330 train_time:125712ms step_avg:58.34ms
step:2156/2330 train_time:125774ms step_avg:58.34ms
step:2157/2330 train_time:125831ms step_avg:58.34ms
step:2158/2330 train_time:125892ms step_avg:58.34ms
step:2159/2330 train_time:125950ms step_avg:58.34ms
step:2160/2330 train_time:126010ms step_avg:58.34ms
step:2161/2330 train_time:126068ms step_avg:58.34ms
step:2162/2330 train_time:126130ms step_avg:58.34ms
step:2163/2330 train_time:126186ms step_avg:58.34ms
step:2164/2330 train_time:126248ms step_avg:58.34ms
step:2165/2330 train_time:126305ms step_avg:58.34ms
step:2166/2330 train_time:126366ms step_avg:58.34ms
step:2167/2330 train_time:126422ms step_avg:58.34ms
step:2168/2330 train_time:126483ms step_avg:58.34ms
step:2169/2330 train_time:126540ms step_avg:58.34ms
step:2170/2330 train_time:126601ms step_avg:58.34ms
step:2171/2330 train_time:126659ms step_avg:58.34ms
step:2172/2330 train_time:126719ms step_avg:58.34ms
step:2173/2330 train_time:126777ms step_avg:58.34ms
step:2174/2330 train_time:126837ms step_avg:58.34ms
step:2175/2330 train_time:126894ms step_avg:58.34ms
step:2176/2330 train_time:126956ms step_avg:58.34ms
step:2177/2330 train_time:127014ms step_avg:58.34ms
step:2178/2330 train_time:127074ms step_avg:58.34ms
step:2179/2330 train_time:127131ms step_avg:58.34ms
step:2180/2330 train_time:127193ms step_avg:58.35ms
step:2181/2330 train_time:127252ms step_avg:58.35ms
step:2182/2330 train_time:127312ms step_avg:58.35ms
step:2183/2330 train_time:127370ms step_avg:58.35ms
step:2184/2330 train_time:127430ms step_avg:58.35ms
step:2185/2330 train_time:127488ms step_avg:58.35ms
step:2186/2330 train_time:127550ms step_avg:58.35ms
step:2187/2330 train_time:127606ms step_avg:58.35ms
step:2188/2330 train_time:127669ms step_avg:58.35ms
step:2189/2330 train_time:127725ms step_avg:58.35ms
step:2190/2330 train_time:127787ms step_avg:58.35ms
step:2191/2330 train_time:127844ms step_avg:58.35ms
step:2192/2330 train_time:127906ms step_avg:58.35ms
step:2193/2330 train_time:127962ms step_avg:58.35ms
step:2194/2330 train_time:128024ms step_avg:58.35ms
step:2195/2330 train_time:128080ms step_avg:58.35ms
step:2196/2330 train_time:128141ms step_avg:58.35ms
step:2197/2330 train_time:128197ms step_avg:58.35ms
step:2198/2330 train_time:128258ms step_avg:58.35ms
step:2199/2330 train_time:128316ms step_avg:58.35ms
step:2200/2330 train_time:128377ms step_avg:58.35ms
step:2201/2330 train_time:128434ms step_avg:58.35ms
step:2202/2330 train_time:128495ms step_avg:58.35ms
step:2203/2330 train_time:128552ms step_avg:58.35ms
step:2204/2330 train_time:128613ms step_avg:58.35ms
step:2205/2330 train_time:128670ms step_avg:58.35ms
step:2206/2330 train_time:128731ms step_avg:58.35ms
step:2207/2330 train_time:128788ms step_avg:58.35ms
step:2208/2330 train_time:128850ms step_avg:58.36ms
step:2209/2330 train_time:128908ms step_avg:58.36ms
step:2210/2330 train_time:128969ms step_avg:58.36ms
step:2211/2330 train_time:129026ms step_avg:58.36ms
step:2212/2330 train_time:129087ms step_avg:58.36ms
step:2213/2330 train_time:129144ms step_avg:58.36ms
step:2214/2330 train_time:129206ms step_avg:58.36ms
step:2215/2330 train_time:129262ms step_avg:58.36ms
step:2216/2330 train_time:129324ms step_avg:58.36ms
step:2217/2330 train_time:129381ms step_avg:58.36ms
step:2218/2330 train_time:129442ms step_avg:58.36ms
step:2219/2330 train_time:129498ms step_avg:58.36ms
step:2220/2330 train_time:129559ms step_avg:58.36ms
step:2221/2330 train_time:129616ms step_avg:58.36ms
step:2222/2330 train_time:129676ms step_avg:58.36ms
step:2223/2330 train_time:129734ms step_avg:58.36ms
step:2224/2330 train_time:129794ms step_avg:58.36ms
step:2225/2330 train_time:129852ms step_avg:58.36ms
step:2226/2330 train_time:129913ms step_avg:58.36ms
step:2227/2330 train_time:129971ms step_avg:58.36ms
step:2228/2330 train_time:130032ms step_avg:58.36ms
step:2229/2330 train_time:130090ms step_avg:58.36ms
step:2230/2330 train_time:130151ms step_avg:58.36ms
step:2231/2330 train_time:130208ms step_avg:58.36ms
step:2232/2330 train_time:130269ms step_avg:58.36ms
step:2233/2330 train_time:130326ms step_avg:58.36ms
step:2234/2330 train_time:130389ms step_avg:58.37ms
step:2235/2330 train_time:130446ms step_avg:58.36ms
step:2236/2330 train_time:130508ms step_avg:58.37ms
step:2237/2330 train_time:130565ms step_avg:58.37ms
step:2238/2330 train_time:130626ms step_avg:58.37ms
step:2239/2330 train_time:130682ms step_avg:58.37ms
step:2240/2330 train_time:130745ms step_avg:58.37ms
step:2241/2330 train_time:130801ms step_avg:58.37ms
step:2242/2330 train_time:130862ms step_avg:58.37ms
step:2243/2330 train_time:130918ms step_avg:58.37ms
step:2244/2330 train_time:130979ms step_avg:58.37ms
step:2245/2330 train_time:131036ms step_avg:58.37ms
step:2246/2330 train_time:131097ms step_avg:58.37ms
step:2247/2330 train_time:131155ms step_avg:58.37ms
step:2248/2330 train_time:131215ms step_avg:58.37ms
step:2249/2330 train_time:131272ms step_avg:58.37ms
step:2250/2330 train_time:131334ms step_avg:58.37ms
step:2250/2330 val_loss:4.7750 train_time:131415ms step_avg:58.41ms
step:2251/2330 train_time:131434ms step_avg:58.39ms
step:2252/2330 train_time:131454ms step_avg:58.37ms
step:2253/2330 train_time:131513ms step_avg:58.37ms
step:2254/2330 train_time:131579ms step_avg:58.38ms
step:2255/2330 train_time:131636ms step_avg:58.38ms
step:2256/2330 train_time:131696ms step_avg:58.38ms
step:2257/2330 train_time:131753ms step_avg:58.38ms
step:2258/2330 train_time:131812ms step_avg:58.38ms
step:2259/2330 train_time:131869ms step_avg:58.38ms
step:2260/2330 train_time:131929ms step_avg:58.38ms
step:2261/2330 train_time:131986ms step_avg:58.38ms
step:2262/2330 train_time:132047ms step_avg:58.38ms
step:2263/2330 train_time:132103ms step_avg:58.38ms
step:2264/2330 train_time:132164ms step_avg:58.38ms
step:2265/2330 train_time:132220ms step_avg:58.38ms
step:2266/2330 train_time:132281ms step_avg:58.38ms
step:2267/2330 train_time:132338ms step_avg:58.38ms
step:2268/2330 train_time:132399ms step_avg:58.38ms
step:2269/2330 train_time:132457ms step_avg:58.38ms
step:2270/2330 train_time:132520ms step_avg:58.38ms
step:2271/2330 train_time:132578ms step_avg:58.38ms
step:2272/2330 train_time:132640ms step_avg:58.38ms
step:2273/2330 train_time:132696ms step_avg:58.38ms
step:2274/2330 train_time:132757ms step_avg:58.38ms
step:2275/2330 train_time:132814ms step_avg:58.38ms
step:2276/2330 train_time:132873ms step_avg:58.38ms
step:2277/2330 train_time:132930ms step_avg:58.38ms
step:2278/2330 train_time:132990ms step_avg:58.38ms
step:2279/2330 train_time:133047ms step_avg:58.38ms
step:2280/2330 train_time:133107ms step_avg:58.38ms
step:2281/2330 train_time:133164ms step_avg:58.38ms
step:2282/2330 train_time:133224ms step_avg:58.38ms
step:2283/2330 train_time:133281ms step_avg:58.38ms
step:2284/2330 train_time:133343ms step_avg:58.38ms
step:2285/2330 train_time:133400ms step_avg:58.38ms
step:2286/2330 train_time:133464ms step_avg:58.38ms
step:2287/2330 train_time:133522ms step_avg:58.38ms
step:2288/2330 train_time:133583ms step_avg:58.38ms
step:2289/2330 train_time:133640ms step_avg:58.38ms
step:2290/2330 train_time:133702ms step_avg:58.39ms
step:2291/2330 train_time:133759ms step_avg:58.38ms
step:2292/2330 train_time:133819ms step_avg:58.39ms
step:2293/2330 train_time:133875ms step_avg:58.38ms
step:2294/2330 train_time:133936ms step_avg:58.39ms
step:2295/2330 train_time:133993ms step_avg:58.38ms
step:2296/2330 train_time:134053ms step_avg:58.39ms
step:2297/2330 train_time:134110ms step_avg:58.38ms
step:2298/2330 train_time:134170ms step_avg:58.39ms
step:2299/2330 train_time:134228ms step_avg:58.39ms
step:2300/2330 train_time:134288ms step_avg:58.39ms
step:2301/2330 train_time:134346ms step_avg:58.39ms
step:2302/2330 train_time:134406ms step_avg:58.39ms
step:2303/2330 train_time:134464ms step_avg:58.39ms
step:2304/2330 train_time:134526ms step_avg:58.39ms
step:2305/2330 train_time:134583ms step_avg:58.39ms
step:2306/2330 train_time:134646ms step_avg:58.39ms
step:2307/2330 train_time:134703ms step_avg:58.39ms
step:2308/2330 train_time:134765ms step_avg:58.39ms
step:2309/2330 train_time:134821ms step_avg:58.39ms
step:2310/2330 train_time:134883ms step_avg:58.39ms
step:2311/2330 train_time:134940ms step_avg:58.39ms
step:2312/2330 train_time:134999ms step_avg:58.39ms
step:2313/2330 train_time:135056ms step_avg:58.39ms
step:2314/2330 train_time:135117ms step_avg:58.39ms
step:2315/2330 train_time:135173ms step_avg:58.39ms
step:2316/2330 train_time:135233ms step_avg:58.39ms
step:2317/2330 train_time:135290ms step_avg:58.39ms
step:2318/2330 train_time:135351ms step_avg:58.39ms
step:2319/2330 train_time:135409ms step_avg:58.39ms
step:2320/2330 train_time:135470ms step_avg:58.39ms
step:2321/2330 train_time:135529ms step_avg:58.39ms
step:2322/2330 train_time:135591ms step_avg:58.39ms
step:2323/2330 train_time:135649ms step_avg:58.39ms
step:2324/2330 train_time:135709ms step_avg:58.39ms
step:2325/2330 train_time:135767ms step_avg:58.39ms
step:2326/2330 train_time:135828ms step_avg:58.40ms
step:2327/2330 train_time:135885ms step_avg:58.40ms
step:2328/2330 train_time:135947ms step_avg:58.40ms
step:2329/2330 train_time:136003ms step_avg:58.40ms
step:2330/2330 train_time:136064ms step_avg:58.40ms
step:2330/2330 val_loss:4.7567 train_time:136145ms step_avg:58.43ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
