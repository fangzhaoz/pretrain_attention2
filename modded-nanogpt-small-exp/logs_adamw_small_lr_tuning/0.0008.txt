import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--adamw_lr', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_lr_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_lr_tuning/{args2.adamw_lr}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=float(args2.adamw_lr),  betas=(0.85, 0.85), eps=1e-10, weight_decay=0.0, fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_lr_tuning/{args2.adamw_lr}", exist_ok=True)
            torch.save(log, f"logs_adamw_small_lr_tuning/{args2.adamw_lr}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 01:28:13 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   27C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   25C    P0             116W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   24C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   26C    P0             113W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   25C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   25C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:83ms step_avg:82.98ms
step:2/2330 train_time:186ms step_avg:93.14ms
step:3/2330 train_time:205ms step_avg:68.47ms
step:4/2330 train_time:225ms step_avg:56.27ms
step:5/2330 train_time:277ms step_avg:55.43ms
step:6/2330 train_time:335ms step_avg:55.82ms
step:7/2330 train_time:389ms step_avg:55.63ms
step:8/2330 train_time:448ms step_avg:56.03ms
step:9/2330 train_time:504ms step_avg:55.96ms
step:10/2330 train_time:561ms step_avg:56.14ms
step:11/2330 train_time:616ms step_avg:56.04ms
step:12/2330 train_time:674ms step_avg:56.20ms
step:13/2330 train_time:730ms step_avg:56.13ms
step:14/2330 train_time:787ms step_avg:56.25ms
step:15/2330 train_time:843ms step_avg:56.19ms
step:16/2330 train_time:901ms step_avg:56.30ms
step:17/2330 train_time:956ms step_avg:56.21ms
step:18/2330 train_time:1013ms step_avg:56.30ms
step:19/2330 train_time:1070ms step_avg:56.29ms
step:20/2330 train_time:1132ms step_avg:56.58ms
step:21/2330 train_time:1190ms step_avg:56.66ms
step:22/2330 train_time:1250ms step_avg:56.84ms
step:23/2330 train_time:1307ms step_avg:56.81ms
step:24/2330 train_time:1365ms step_avg:56.88ms
step:25/2330 train_time:1421ms step_avg:56.83ms
step:26/2330 train_time:1479ms step_avg:56.89ms
step:27/2330 train_time:1534ms step_avg:56.83ms
step:28/2330 train_time:1593ms step_avg:56.89ms
step:29/2330 train_time:1648ms step_avg:56.84ms
step:30/2330 train_time:1706ms step_avg:56.86ms
step:31/2330 train_time:1761ms step_avg:56.82ms
step:32/2330 train_time:1819ms step_avg:56.86ms
step:33/2330 train_time:1874ms step_avg:56.80ms
step:34/2330 train_time:1933ms step_avg:56.86ms
step:35/2330 train_time:1988ms step_avg:56.81ms
step:36/2330 train_time:2048ms step_avg:56.89ms
step:37/2330 train_time:2105ms step_avg:56.88ms
step:38/2330 train_time:2165ms step_avg:56.97ms
step:39/2330 train_time:2221ms step_avg:56.96ms
step:40/2330 train_time:2281ms step_avg:57.03ms
step:41/2330 train_time:2338ms step_avg:57.02ms
step:42/2330 train_time:2396ms step_avg:57.05ms
step:43/2330 train_time:2452ms step_avg:57.02ms
step:44/2330 train_time:2510ms step_avg:57.04ms
step:45/2330 train_time:2565ms step_avg:57.01ms
step:46/2330 train_time:2624ms step_avg:57.05ms
step:47/2330 train_time:2680ms step_avg:57.01ms
step:48/2330 train_time:2739ms step_avg:57.05ms
step:49/2330 train_time:2795ms step_avg:57.03ms
step:50/2330 train_time:2853ms step_avg:57.06ms
step:51/2330 train_time:2908ms step_avg:57.03ms
step:52/2330 train_time:2967ms step_avg:57.06ms
step:53/2330 train_time:3024ms step_avg:57.05ms
step:54/2330 train_time:3082ms step_avg:57.08ms
step:55/2330 train_time:3139ms step_avg:57.07ms
step:56/2330 train_time:3197ms step_avg:57.09ms
step:57/2330 train_time:3253ms step_avg:57.07ms
step:58/2330 train_time:3312ms step_avg:57.11ms
step:59/2330 train_time:3369ms step_avg:57.10ms
step:60/2330 train_time:3428ms step_avg:57.13ms
step:61/2330 train_time:3483ms step_avg:57.10ms
step:62/2330 train_time:3542ms step_avg:57.13ms
step:63/2330 train_time:3598ms step_avg:57.10ms
step:64/2330 train_time:3656ms step_avg:57.12ms
step:65/2330 train_time:3711ms step_avg:57.09ms
step:66/2330 train_time:3771ms step_avg:57.13ms
step:67/2330 train_time:3826ms step_avg:57.11ms
step:68/2330 train_time:3885ms step_avg:57.13ms
step:69/2330 train_time:3941ms step_avg:57.12ms
step:70/2330 train_time:4000ms step_avg:57.14ms
step:71/2330 train_time:4055ms step_avg:57.12ms
step:72/2330 train_time:4115ms step_avg:57.15ms
step:73/2330 train_time:4171ms step_avg:57.13ms
step:74/2330 train_time:4230ms step_avg:57.16ms
step:75/2330 train_time:4286ms step_avg:57.15ms
step:76/2330 train_time:4344ms step_avg:57.16ms
step:77/2330 train_time:4401ms step_avg:57.16ms
step:78/2330 train_time:4459ms step_avg:57.17ms
step:79/2330 train_time:4515ms step_avg:57.15ms
step:80/2330 train_time:4574ms step_avg:57.17ms
step:81/2330 train_time:4629ms step_avg:57.14ms
step:82/2330 train_time:4688ms step_avg:57.17ms
step:83/2330 train_time:4744ms step_avg:57.15ms
step:84/2330 train_time:4802ms step_avg:57.17ms
step:85/2330 train_time:4859ms step_avg:57.16ms
step:86/2330 train_time:4917ms step_avg:57.17ms
step:87/2330 train_time:4973ms step_avg:57.16ms
step:88/2330 train_time:5032ms step_avg:57.18ms
step:89/2330 train_time:5088ms step_avg:57.16ms
step:90/2330 train_time:5146ms step_avg:57.18ms
step:91/2330 train_time:5203ms step_avg:57.17ms
step:92/2330 train_time:5261ms step_avg:57.19ms
step:93/2330 train_time:5317ms step_avg:57.18ms
step:94/2330 train_time:5376ms step_avg:57.19ms
step:95/2330 train_time:5432ms step_avg:57.18ms
step:96/2330 train_time:5491ms step_avg:57.20ms
step:97/2330 train_time:5547ms step_avg:57.18ms
step:98/2330 train_time:5605ms step_avg:57.19ms
step:99/2330 train_time:5661ms step_avg:57.18ms
step:100/2330 train_time:5719ms step_avg:57.19ms
step:101/2330 train_time:5774ms step_avg:57.17ms
step:102/2330 train_time:5834ms step_avg:57.19ms
step:103/2330 train_time:5889ms step_avg:57.18ms
step:104/2330 train_time:5949ms step_avg:57.20ms
step:105/2330 train_time:6004ms step_avg:57.18ms
step:106/2330 train_time:6063ms step_avg:57.20ms
step:107/2330 train_time:6119ms step_avg:57.19ms
step:108/2330 train_time:6179ms step_avg:57.21ms
step:109/2330 train_time:6234ms step_avg:57.19ms
step:110/2330 train_time:6294ms step_avg:57.22ms
step:111/2330 train_time:6350ms step_avg:57.21ms
step:112/2330 train_time:6408ms step_avg:57.22ms
step:113/2330 train_time:6465ms step_avg:57.21ms
step:114/2330 train_time:6523ms step_avg:57.22ms
step:115/2330 train_time:6579ms step_avg:57.21ms
step:116/2330 train_time:6638ms step_avg:57.22ms
step:117/2330 train_time:6693ms step_avg:57.21ms
step:118/2330 train_time:6752ms step_avg:57.22ms
step:119/2330 train_time:6807ms step_avg:57.21ms
step:120/2330 train_time:6867ms step_avg:57.22ms
step:121/2330 train_time:6923ms step_avg:57.22ms
step:122/2330 train_time:6982ms step_avg:57.23ms
step:123/2330 train_time:7038ms step_avg:57.22ms
step:124/2330 train_time:7097ms step_avg:57.23ms
step:125/2330 train_time:7152ms step_avg:57.22ms
step:126/2330 train_time:7211ms step_avg:57.23ms
step:127/2330 train_time:7267ms step_avg:57.22ms
step:128/2330 train_time:7326ms step_avg:57.23ms
step:129/2330 train_time:7381ms step_avg:57.22ms
step:130/2330 train_time:7442ms step_avg:57.24ms
step:131/2330 train_time:7498ms step_avg:57.24ms
step:132/2330 train_time:7557ms step_avg:57.25ms
step:133/2330 train_time:7612ms step_avg:57.24ms
step:134/2330 train_time:7670ms step_avg:57.24ms
step:135/2330 train_time:7726ms step_avg:57.23ms
step:136/2330 train_time:7785ms step_avg:57.24ms
step:137/2330 train_time:7841ms step_avg:57.23ms
step:138/2330 train_time:7899ms step_avg:57.24ms
step:139/2330 train_time:7955ms step_avg:57.23ms
step:140/2330 train_time:8014ms step_avg:57.24ms
step:141/2330 train_time:8069ms step_avg:57.23ms
step:142/2330 train_time:8129ms step_avg:57.25ms
step:143/2330 train_time:8185ms step_avg:57.24ms
step:144/2330 train_time:8244ms step_avg:57.25ms
step:145/2330 train_time:8300ms step_avg:57.24ms
step:146/2330 train_time:8359ms step_avg:57.26ms
step:147/2330 train_time:8415ms step_avg:57.24ms
step:148/2330 train_time:8474ms step_avg:57.26ms
step:149/2330 train_time:8530ms step_avg:57.25ms
step:150/2330 train_time:8589ms step_avg:57.26ms
step:151/2330 train_time:8645ms step_avg:57.25ms
step:152/2330 train_time:8704ms step_avg:57.26ms
step:153/2330 train_time:8760ms step_avg:57.25ms
step:154/2330 train_time:8818ms step_avg:57.26ms
step:155/2330 train_time:8874ms step_avg:57.25ms
step:156/2330 train_time:8933ms step_avg:57.26ms
step:157/2330 train_time:8989ms step_avg:57.25ms
step:158/2330 train_time:9048ms step_avg:57.26ms
step:159/2330 train_time:9103ms step_avg:57.25ms
step:160/2330 train_time:9162ms step_avg:57.26ms
step:161/2330 train_time:9217ms step_avg:57.25ms
step:162/2330 train_time:9277ms step_avg:57.26ms
step:163/2330 train_time:9332ms step_avg:57.25ms
step:164/2330 train_time:9392ms step_avg:57.27ms
step:165/2330 train_time:9448ms step_avg:57.26ms
step:166/2330 train_time:9508ms step_avg:57.28ms
step:167/2330 train_time:9564ms step_avg:57.27ms
step:168/2330 train_time:9624ms step_avg:57.28ms
step:169/2330 train_time:9679ms step_avg:57.27ms
step:170/2330 train_time:9738ms step_avg:57.28ms
step:171/2330 train_time:9794ms step_avg:57.28ms
step:172/2330 train_time:9853ms step_avg:57.28ms
step:173/2330 train_time:9909ms step_avg:57.28ms
step:174/2330 train_time:9968ms step_avg:57.28ms
step:175/2330 train_time:10024ms step_avg:57.28ms
step:176/2330 train_time:10082ms step_avg:57.29ms
step:177/2330 train_time:10138ms step_avg:57.28ms
step:178/2330 train_time:10196ms step_avg:57.28ms
step:179/2330 train_time:10252ms step_avg:57.27ms
step:180/2330 train_time:10311ms step_avg:57.28ms
step:181/2330 train_time:10367ms step_avg:57.27ms
step:182/2330 train_time:10426ms step_avg:57.29ms
step:183/2330 train_time:10482ms step_avg:57.28ms
step:184/2330 train_time:10541ms step_avg:57.29ms
step:185/2330 train_time:10597ms step_avg:57.28ms
step:186/2330 train_time:10655ms step_avg:57.28ms
step:187/2330 train_time:10711ms step_avg:57.28ms
step:188/2330 train_time:10769ms step_avg:57.28ms
step:189/2330 train_time:10825ms step_avg:57.28ms
step:190/2330 train_time:10885ms step_avg:57.29ms
step:191/2330 train_time:10941ms step_avg:57.28ms
step:192/2330 train_time:11000ms step_avg:57.29ms
step:193/2330 train_time:11056ms step_avg:57.29ms
step:194/2330 train_time:11115ms step_avg:57.29ms
step:195/2330 train_time:11171ms step_avg:57.29ms
step:196/2330 train_time:11230ms step_avg:57.30ms
step:197/2330 train_time:11286ms step_avg:57.29ms
step:198/2330 train_time:11345ms step_avg:57.30ms
step:199/2330 train_time:11401ms step_avg:57.29ms
step:200/2330 train_time:11459ms step_avg:57.30ms
step:201/2330 train_time:11515ms step_avg:57.29ms
step:202/2330 train_time:11574ms step_avg:57.30ms
step:203/2330 train_time:11629ms step_avg:57.29ms
step:204/2330 train_time:11688ms step_avg:57.30ms
step:205/2330 train_time:11745ms step_avg:57.29ms
step:206/2330 train_time:11803ms step_avg:57.29ms
step:207/2330 train_time:11859ms step_avg:57.29ms
step:208/2330 train_time:11917ms step_avg:57.29ms
step:209/2330 train_time:11973ms step_avg:57.29ms
step:210/2330 train_time:12033ms step_avg:57.30ms
step:211/2330 train_time:12088ms step_avg:57.29ms
step:212/2330 train_time:12149ms step_avg:57.31ms
step:213/2330 train_time:12205ms step_avg:57.30ms
step:214/2330 train_time:12263ms step_avg:57.31ms
step:215/2330 train_time:12320ms step_avg:57.30ms
step:216/2330 train_time:12378ms step_avg:57.31ms
step:217/2330 train_time:12434ms step_avg:57.30ms
step:218/2330 train_time:12493ms step_avg:57.31ms
step:219/2330 train_time:12549ms step_avg:57.30ms
step:220/2330 train_time:12607ms step_avg:57.30ms
step:221/2330 train_time:12663ms step_avg:57.30ms
step:222/2330 train_time:12721ms step_avg:57.30ms
step:223/2330 train_time:12777ms step_avg:57.30ms
step:224/2330 train_time:12836ms step_avg:57.30ms
step:225/2330 train_time:12892ms step_avg:57.30ms
step:226/2330 train_time:12951ms step_avg:57.31ms
step:227/2330 train_time:13007ms step_avg:57.30ms
step:228/2330 train_time:13066ms step_avg:57.31ms
step:229/2330 train_time:13122ms step_avg:57.30ms
step:230/2330 train_time:13181ms step_avg:57.31ms
step:231/2330 train_time:13236ms step_avg:57.30ms
step:232/2330 train_time:13295ms step_avg:57.31ms
step:233/2330 train_time:13351ms step_avg:57.30ms
step:234/2330 train_time:13411ms step_avg:57.31ms
step:235/2330 train_time:13467ms step_avg:57.30ms
step:236/2330 train_time:13525ms step_avg:57.31ms
step:237/2330 train_time:13581ms step_avg:57.31ms
step:238/2330 train_time:13640ms step_avg:57.31ms
step:239/2330 train_time:13696ms step_avg:57.31ms
step:240/2330 train_time:13754ms step_avg:57.31ms
step:241/2330 train_time:13810ms step_avg:57.30ms
step:242/2330 train_time:13869ms step_avg:57.31ms
step:243/2330 train_time:13925ms step_avg:57.30ms
step:244/2330 train_time:13984ms step_avg:57.31ms
step:245/2330 train_time:14040ms step_avg:57.31ms
step:246/2330 train_time:14099ms step_avg:57.31ms
step:247/2330 train_time:14155ms step_avg:57.31ms
step:248/2330 train_time:14214ms step_avg:57.31ms
step:249/2330 train_time:14270ms step_avg:57.31ms
step:250/2330 train_time:14329ms step_avg:57.31ms
step:250/2330 val_loss:4.8954 train_time:14408ms step_avg:57.63ms
step:251/2330 train_time:14427ms step_avg:57.48ms
step:252/2330 train_time:14448ms step_avg:57.33ms
step:253/2330 train_time:14501ms step_avg:57.32ms
step:254/2330 train_time:14568ms step_avg:57.35ms
step:255/2330 train_time:14623ms step_avg:57.34ms
step:256/2330 train_time:14690ms step_avg:57.38ms
step:257/2330 train_time:14746ms step_avg:57.38ms
step:258/2330 train_time:14805ms step_avg:57.38ms
step:259/2330 train_time:14860ms step_avg:57.37ms
step:260/2330 train_time:14918ms step_avg:57.38ms
step:261/2330 train_time:14974ms step_avg:57.37ms
step:262/2330 train_time:15031ms step_avg:57.37ms
step:263/2330 train_time:15087ms step_avg:57.36ms
step:264/2330 train_time:15145ms step_avg:57.37ms
step:265/2330 train_time:15200ms step_avg:57.36ms
step:266/2330 train_time:15258ms step_avg:57.36ms
step:267/2330 train_time:15313ms step_avg:57.35ms
step:268/2330 train_time:15372ms step_avg:57.36ms
step:269/2330 train_time:15429ms step_avg:57.36ms
step:270/2330 train_time:15488ms step_avg:57.36ms
step:271/2330 train_time:15545ms step_avg:57.36ms
step:272/2330 train_time:15607ms step_avg:57.38ms
step:273/2330 train_time:15663ms step_avg:57.37ms
step:274/2330 train_time:15724ms step_avg:57.39ms
step:275/2330 train_time:15780ms step_avg:57.38ms
step:276/2330 train_time:15839ms step_avg:57.39ms
step:277/2330 train_time:15896ms step_avg:57.39ms
step:278/2330 train_time:15954ms step_avg:57.39ms
step:279/2330 train_time:16010ms step_avg:57.38ms
step:280/2330 train_time:16068ms step_avg:57.38ms
step:281/2330 train_time:16123ms step_avg:57.38ms
step:282/2330 train_time:16181ms step_avg:57.38ms
step:283/2330 train_time:16237ms step_avg:57.38ms
step:284/2330 train_time:16296ms step_avg:57.38ms
step:285/2330 train_time:16352ms step_avg:57.38ms
step:286/2330 train_time:16410ms step_avg:57.38ms
step:287/2330 train_time:16467ms step_avg:57.38ms
step:288/2330 train_time:16528ms step_avg:57.39ms
step:289/2330 train_time:16584ms step_avg:57.39ms
step:290/2330 train_time:16644ms step_avg:57.39ms
step:291/2330 train_time:16699ms step_avg:57.39ms
step:292/2330 train_time:16760ms step_avg:57.40ms
step:293/2330 train_time:16815ms step_avg:57.39ms
step:294/2330 train_time:16875ms step_avg:57.40ms
step:295/2330 train_time:16931ms step_avg:57.39ms
step:296/2330 train_time:16989ms step_avg:57.40ms
step:297/2330 train_time:17045ms step_avg:57.39ms
step:298/2330 train_time:17103ms step_avg:57.39ms
step:299/2330 train_time:17158ms step_avg:57.39ms
step:300/2330 train_time:17217ms step_avg:57.39ms
step:301/2330 train_time:17273ms step_avg:57.38ms
step:302/2330 train_time:17331ms step_avg:57.39ms
step:303/2330 train_time:17387ms step_avg:57.38ms
step:304/2330 train_time:17446ms step_avg:57.39ms
step:305/2330 train_time:17502ms step_avg:57.38ms
step:306/2330 train_time:17562ms step_avg:57.39ms
step:307/2330 train_time:17617ms step_avg:57.39ms
step:308/2330 train_time:17677ms step_avg:57.39ms
step:309/2330 train_time:17733ms step_avg:57.39ms
step:310/2330 train_time:17792ms step_avg:57.39ms
step:311/2330 train_time:17847ms step_avg:57.39ms
step:312/2330 train_time:17906ms step_avg:57.39ms
step:313/2330 train_time:17962ms step_avg:57.39ms
step:314/2330 train_time:18022ms step_avg:57.39ms
step:315/2330 train_time:18078ms step_avg:57.39ms
step:316/2330 train_time:18136ms step_avg:57.39ms
step:317/2330 train_time:18192ms step_avg:57.39ms
step:318/2330 train_time:18251ms step_avg:57.39ms
step:319/2330 train_time:18306ms step_avg:57.39ms
step:320/2330 train_time:18365ms step_avg:57.39ms
step:321/2330 train_time:18420ms step_avg:57.38ms
step:322/2330 train_time:18480ms step_avg:57.39ms
step:323/2330 train_time:18536ms step_avg:57.39ms
step:324/2330 train_time:18595ms step_avg:57.39ms
step:325/2330 train_time:18651ms step_avg:57.39ms
step:326/2330 train_time:18710ms step_avg:57.39ms
step:327/2330 train_time:18766ms step_avg:57.39ms
step:328/2330 train_time:18825ms step_avg:57.39ms
step:329/2330 train_time:18881ms step_avg:57.39ms
step:330/2330 train_time:18941ms step_avg:57.40ms
step:331/2330 train_time:18997ms step_avg:57.39ms
step:332/2330 train_time:19055ms step_avg:57.39ms
step:333/2330 train_time:19111ms step_avg:57.39ms
step:334/2330 train_time:19169ms step_avg:57.39ms
step:335/2330 train_time:19225ms step_avg:57.39ms
step:336/2330 train_time:19284ms step_avg:57.39ms
step:337/2330 train_time:19339ms step_avg:57.39ms
step:338/2330 train_time:19399ms step_avg:57.39ms
step:339/2330 train_time:19455ms step_avg:57.39ms
step:340/2330 train_time:19515ms step_avg:57.40ms
step:341/2330 train_time:19570ms step_avg:57.39ms
step:342/2330 train_time:19630ms step_avg:57.40ms
step:343/2330 train_time:19686ms step_avg:57.39ms
step:344/2330 train_time:19744ms step_avg:57.40ms
step:345/2330 train_time:19800ms step_avg:57.39ms
step:346/2330 train_time:19860ms step_avg:57.40ms
step:347/2330 train_time:19916ms step_avg:57.40ms
step:348/2330 train_time:19975ms step_avg:57.40ms
step:349/2330 train_time:20031ms step_avg:57.40ms
step:350/2330 train_time:20090ms step_avg:57.40ms
step:351/2330 train_time:20146ms step_avg:57.40ms
step:352/2330 train_time:20204ms step_avg:57.40ms
step:353/2330 train_time:20260ms step_avg:57.39ms
step:354/2330 train_time:20319ms step_avg:57.40ms
step:355/2330 train_time:20375ms step_avg:57.39ms
step:356/2330 train_time:20434ms step_avg:57.40ms
step:357/2330 train_time:20490ms step_avg:57.39ms
step:358/2330 train_time:20549ms step_avg:57.40ms
step:359/2330 train_time:20605ms step_avg:57.40ms
step:360/2330 train_time:20665ms step_avg:57.40ms
step:361/2330 train_time:20720ms step_avg:57.40ms
step:362/2330 train_time:20780ms step_avg:57.40ms
step:363/2330 train_time:20837ms step_avg:57.40ms
step:364/2330 train_time:20896ms step_avg:57.41ms
step:365/2330 train_time:20951ms step_avg:57.40ms
step:366/2330 train_time:21010ms step_avg:57.40ms
step:367/2330 train_time:21066ms step_avg:57.40ms
step:368/2330 train_time:21124ms step_avg:57.40ms
step:369/2330 train_time:21180ms step_avg:57.40ms
step:370/2330 train_time:21240ms step_avg:57.41ms
step:371/2330 train_time:21296ms step_avg:57.40ms
step:372/2330 train_time:21355ms step_avg:57.40ms
step:373/2330 train_time:21410ms step_avg:57.40ms
step:374/2330 train_time:21469ms step_avg:57.40ms
step:375/2330 train_time:21525ms step_avg:57.40ms
step:376/2330 train_time:21584ms step_avg:57.40ms
step:377/2330 train_time:21639ms step_avg:57.40ms
step:378/2330 train_time:21700ms step_avg:57.41ms
step:379/2330 train_time:21756ms step_avg:57.40ms
step:380/2330 train_time:21816ms step_avg:57.41ms
step:381/2330 train_time:21871ms step_avg:57.40ms
step:382/2330 train_time:21931ms step_avg:57.41ms
step:383/2330 train_time:21987ms step_avg:57.41ms
step:384/2330 train_time:22046ms step_avg:57.41ms
step:385/2330 train_time:22101ms step_avg:57.41ms
step:386/2330 train_time:22160ms step_avg:57.41ms
step:387/2330 train_time:22217ms step_avg:57.41ms
step:388/2330 train_time:22276ms step_avg:57.41ms
step:389/2330 train_time:22332ms step_avg:57.41ms
step:390/2330 train_time:22391ms step_avg:57.41ms
step:391/2330 train_time:22447ms step_avg:57.41ms
step:392/2330 train_time:22505ms step_avg:57.41ms
step:393/2330 train_time:22560ms step_avg:57.41ms
step:394/2330 train_time:22619ms step_avg:57.41ms
step:395/2330 train_time:22676ms step_avg:57.41ms
step:396/2330 train_time:22736ms step_avg:57.41ms
step:397/2330 train_time:22792ms step_avg:57.41ms
step:398/2330 train_time:22851ms step_avg:57.41ms
step:399/2330 train_time:22907ms step_avg:57.41ms
step:400/2330 train_time:22965ms step_avg:57.41ms
step:401/2330 train_time:23021ms step_avg:57.41ms
step:402/2330 train_time:23080ms step_avg:57.41ms
step:403/2330 train_time:23136ms step_avg:57.41ms
step:404/2330 train_time:23194ms step_avg:57.41ms
step:405/2330 train_time:23250ms step_avg:57.41ms
step:406/2330 train_time:23309ms step_avg:57.41ms
step:407/2330 train_time:23365ms step_avg:57.41ms
step:408/2330 train_time:23423ms step_avg:57.41ms
step:409/2330 train_time:23479ms step_avg:57.41ms
step:410/2330 train_time:23538ms step_avg:57.41ms
step:411/2330 train_time:23594ms step_avg:57.41ms
step:412/2330 train_time:23653ms step_avg:57.41ms
step:413/2330 train_time:23709ms step_avg:57.41ms
step:414/2330 train_time:23768ms step_avg:57.41ms
step:415/2330 train_time:23824ms step_avg:57.41ms
step:416/2330 train_time:23884ms step_avg:57.41ms
step:417/2330 train_time:23940ms step_avg:57.41ms
step:418/2330 train_time:23999ms step_avg:57.41ms
step:419/2330 train_time:24056ms step_avg:57.41ms
step:420/2330 train_time:24115ms step_avg:57.42ms
step:421/2330 train_time:24170ms step_avg:57.41ms
step:422/2330 train_time:24229ms step_avg:57.42ms
step:423/2330 train_time:24285ms step_avg:57.41ms
step:424/2330 train_time:24345ms step_avg:57.42ms
step:425/2330 train_time:24400ms step_avg:57.41ms
step:426/2330 train_time:24460ms step_avg:57.42ms
step:427/2330 train_time:24516ms step_avg:57.42ms
step:428/2330 train_time:24575ms step_avg:57.42ms
step:429/2330 train_time:24631ms step_avg:57.42ms
step:430/2330 train_time:24690ms step_avg:57.42ms
step:431/2330 train_time:24747ms step_avg:57.42ms
step:432/2330 train_time:24805ms step_avg:57.42ms
step:433/2330 train_time:24861ms step_avg:57.42ms
step:434/2330 train_time:24920ms step_avg:57.42ms
step:435/2330 train_time:24976ms step_avg:57.42ms
step:436/2330 train_time:25036ms step_avg:57.42ms
step:437/2330 train_time:25092ms step_avg:57.42ms
step:438/2330 train_time:25150ms step_avg:57.42ms
step:439/2330 train_time:25206ms step_avg:57.42ms
step:440/2330 train_time:25265ms step_avg:57.42ms
step:441/2330 train_time:25321ms step_avg:57.42ms
step:442/2330 train_time:25381ms step_avg:57.42ms
step:443/2330 train_time:25437ms step_avg:57.42ms
step:444/2330 train_time:25497ms step_avg:57.42ms
step:445/2330 train_time:25552ms step_avg:57.42ms
step:446/2330 train_time:25611ms step_avg:57.42ms
step:447/2330 train_time:25666ms step_avg:57.42ms
step:448/2330 train_time:25726ms step_avg:57.42ms
step:449/2330 train_time:25781ms step_avg:57.42ms
step:450/2330 train_time:25841ms step_avg:57.42ms
step:451/2330 train_time:25897ms step_avg:57.42ms
step:452/2330 train_time:25956ms step_avg:57.43ms
step:453/2330 train_time:26013ms step_avg:57.42ms
step:454/2330 train_time:26072ms step_avg:57.43ms
step:455/2330 train_time:26127ms step_avg:57.42ms
step:456/2330 train_time:26186ms step_avg:57.42ms
step:457/2330 train_time:26241ms step_avg:57.42ms
step:458/2330 train_time:26301ms step_avg:57.43ms
step:459/2330 train_time:26357ms step_avg:57.42ms
step:460/2330 train_time:26417ms step_avg:57.43ms
step:461/2330 train_time:26472ms step_avg:57.42ms
step:462/2330 train_time:26532ms step_avg:57.43ms
step:463/2330 train_time:26588ms step_avg:57.42ms
step:464/2330 train_time:26646ms step_avg:57.43ms
step:465/2330 train_time:26702ms step_avg:57.42ms
step:466/2330 train_time:26761ms step_avg:57.43ms
step:467/2330 train_time:26817ms step_avg:57.42ms
step:468/2330 train_time:26876ms step_avg:57.43ms
step:469/2330 train_time:26932ms step_avg:57.42ms
step:470/2330 train_time:26990ms step_avg:57.43ms
step:471/2330 train_time:27046ms step_avg:57.42ms
step:472/2330 train_time:27105ms step_avg:57.43ms
step:473/2330 train_time:27161ms step_avg:57.42ms
step:474/2330 train_time:27220ms step_avg:57.43ms
step:475/2330 train_time:27276ms step_avg:57.42ms
step:476/2330 train_time:27335ms step_avg:57.43ms
step:477/2330 train_time:27391ms step_avg:57.42ms
step:478/2330 train_time:27450ms step_avg:57.43ms
step:479/2330 train_time:27506ms step_avg:57.42ms
step:480/2330 train_time:27566ms step_avg:57.43ms
step:481/2330 train_time:27623ms step_avg:57.43ms
step:482/2330 train_time:27681ms step_avg:57.43ms
step:483/2330 train_time:27737ms step_avg:57.43ms
step:484/2330 train_time:27796ms step_avg:57.43ms
step:485/2330 train_time:27852ms step_avg:57.43ms
step:486/2330 train_time:27910ms step_avg:57.43ms
step:487/2330 train_time:27966ms step_avg:57.43ms
step:488/2330 train_time:28026ms step_avg:57.43ms
step:489/2330 train_time:28081ms step_avg:57.43ms
step:490/2330 train_time:28141ms step_avg:57.43ms
step:491/2330 train_time:28197ms step_avg:57.43ms
step:492/2330 train_time:28255ms step_avg:57.43ms
step:493/2330 train_time:28311ms step_avg:57.43ms
step:494/2330 train_time:28371ms step_avg:57.43ms
step:495/2330 train_time:28426ms step_avg:57.43ms
step:496/2330 train_time:28486ms step_avg:57.43ms
step:497/2330 train_time:28542ms step_avg:57.43ms
step:498/2330 train_time:28600ms step_avg:57.43ms
step:499/2330 train_time:28656ms step_avg:57.43ms
step:500/2330 train_time:28715ms step_avg:57.43ms
step:500/2330 val_loss:4.3988 train_time:28794ms step_avg:57.59ms
step:501/2330 train_time:28813ms step_avg:57.51ms
step:502/2330 train_time:28833ms step_avg:57.44ms
step:503/2330 train_time:28892ms step_avg:57.44ms
step:504/2330 train_time:28954ms step_avg:57.45ms
step:505/2330 train_time:29010ms step_avg:57.45ms
step:506/2330 train_time:29069ms step_avg:57.45ms
step:507/2330 train_time:29125ms step_avg:57.45ms
step:508/2330 train_time:29184ms step_avg:57.45ms
step:509/2330 train_time:29240ms step_avg:57.45ms
step:510/2330 train_time:29298ms step_avg:57.45ms
step:511/2330 train_time:29353ms step_avg:57.44ms
step:512/2330 train_time:29411ms step_avg:57.44ms
step:513/2330 train_time:29467ms step_avg:57.44ms
step:514/2330 train_time:29525ms step_avg:57.44ms
step:515/2330 train_time:29580ms step_avg:57.44ms
step:516/2330 train_time:29639ms step_avg:57.44ms
step:517/2330 train_time:29695ms step_avg:57.44ms
step:518/2330 train_time:29755ms step_avg:57.44ms
step:519/2330 train_time:29811ms step_avg:57.44ms
step:520/2330 train_time:29872ms step_avg:57.45ms
step:521/2330 train_time:29929ms step_avg:57.44ms
step:522/2330 train_time:29989ms step_avg:57.45ms
step:523/2330 train_time:30046ms step_avg:57.45ms
step:524/2330 train_time:30104ms step_avg:57.45ms
step:525/2330 train_time:30162ms step_avg:57.45ms
step:526/2330 train_time:30221ms step_avg:57.45ms
step:527/2330 train_time:30276ms step_avg:57.45ms
step:528/2330 train_time:30334ms step_avg:57.45ms
step:529/2330 train_time:30390ms step_avg:57.45ms
step:530/2330 train_time:30449ms step_avg:57.45ms
step:531/2330 train_time:30505ms step_avg:57.45ms
step:532/2330 train_time:30563ms step_avg:57.45ms
step:533/2330 train_time:30619ms step_avg:57.45ms
step:534/2330 train_time:30677ms step_avg:57.45ms
step:535/2330 train_time:30733ms step_avg:57.44ms
step:536/2330 train_time:30793ms step_avg:57.45ms
step:537/2330 train_time:30849ms step_avg:57.45ms
step:538/2330 train_time:30909ms step_avg:57.45ms
step:539/2330 train_time:30966ms step_avg:57.45ms
step:540/2330 train_time:31025ms step_avg:57.45ms
step:541/2330 train_time:31081ms step_avg:57.45ms
step:542/2330 train_time:31142ms step_avg:57.46ms
step:543/2330 train_time:31198ms step_avg:57.45ms
step:544/2330 train_time:31256ms step_avg:57.46ms
step:545/2330 train_time:31312ms step_avg:57.45ms
step:546/2330 train_time:31371ms step_avg:57.46ms
step:547/2330 train_time:31426ms step_avg:57.45ms
step:548/2330 train_time:31485ms step_avg:57.46ms
step:549/2330 train_time:31541ms step_avg:57.45ms
step:550/2330 train_time:31600ms step_avg:57.45ms
step:551/2330 train_time:31655ms step_avg:57.45ms
step:552/2330 train_time:31715ms step_avg:57.45ms
step:553/2330 train_time:31771ms step_avg:57.45ms
step:554/2330 train_time:31830ms step_avg:57.45ms
step:555/2330 train_time:31886ms step_avg:57.45ms
step:556/2330 train_time:31947ms step_avg:57.46ms
step:557/2330 train_time:32003ms step_avg:57.46ms
step:558/2330 train_time:32062ms step_avg:57.46ms
step:559/2330 train_time:32119ms step_avg:57.46ms
step:560/2330 train_time:32178ms step_avg:57.46ms
step:561/2330 train_time:32234ms step_avg:57.46ms
step:562/2330 train_time:32293ms step_avg:57.46ms
step:563/2330 train_time:32349ms step_avg:57.46ms
step:564/2330 train_time:32408ms step_avg:57.46ms
step:565/2330 train_time:32464ms step_avg:57.46ms
step:566/2330 train_time:32522ms step_avg:57.46ms
step:567/2330 train_time:32578ms step_avg:57.46ms
step:568/2330 train_time:32637ms step_avg:57.46ms
step:569/2330 train_time:32693ms step_avg:57.46ms
step:570/2330 train_time:32752ms step_avg:57.46ms
step:571/2330 train_time:32808ms step_avg:57.46ms
step:572/2330 train_time:32868ms step_avg:57.46ms
step:573/2330 train_time:32924ms step_avg:57.46ms
step:574/2330 train_time:32984ms step_avg:57.46ms
step:575/2330 train_time:33041ms step_avg:57.46ms
step:576/2330 train_time:33101ms step_avg:57.47ms
step:577/2330 train_time:33157ms step_avg:57.46ms
step:578/2330 train_time:33216ms step_avg:57.47ms
step:579/2330 train_time:33272ms step_avg:57.46ms
step:580/2330 train_time:33331ms step_avg:57.47ms
step:581/2330 train_time:33386ms step_avg:57.46ms
step:582/2330 train_time:33446ms step_avg:57.47ms
step:583/2330 train_time:33502ms step_avg:57.46ms
step:584/2330 train_time:33561ms step_avg:57.47ms
step:585/2330 train_time:33617ms step_avg:57.46ms
step:586/2330 train_time:33676ms step_avg:57.47ms
step:587/2330 train_time:33731ms step_avg:57.46ms
step:588/2330 train_time:33791ms step_avg:57.47ms
step:589/2330 train_time:33847ms step_avg:57.46ms
step:590/2330 train_time:33906ms step_avg:57.47ms
step:591/2330 train_time:33962ms step_avg:57.47ms
step:592/2330 train_time:34022ms step_avg:57.47ms
step:593/2330 train_time:34078ms step_avg:57.47ms
step:594/2330 train_time:34137ms step_avg:57.47ms
step:595/2330 train_time:34193ms step_avg:57.47ms
step:596/2330 train_time:34252ms step_avg:57.47ms
step:597/2330 train_time:34307ms step_avg:57.47ms
step:598/2330 train_time:34367ms step_avg:57.47ms
step:599/2330 train_time:34423ms step_avg:57.47ms
step:600/2330 train_time:34483ms step_avg:57.47ms
step:601/2330 train_time:34538ms step_avg:57.47ms
step:602/2330 train_time:34598ms step_avg:57.47ms
step:603/2330 train_time:34653ms step_avg:57.47ms
step:604/2330 train_time:34711ms step_avg:57.47ms
step:605/2330 train_time:34767ms step_avg:57.47ms
step:606/2330 train_time:34826ms step_avg:57.47ms
step:607/2330 train_time:34883ms step_avg:57.47ms
step:608/2330 train_time:34941ms step_avg:57.47ms
step:609/2330 train_time:34997ms step_avg:57.47ms
step:610/2330 train_time:35056ms step_avg:57.47ms
step:611/2330 train_time:35112ms step_avg:57.47ms
step:612/2330 train_time:35172ms step_avg:57.47ms
step:613/2330 train_time:35228ms step_avg:57.47ms
step:614/2330 train_time:35288ms step_avg:57.47ms
step:615/2330 train_time:35344ms step_avg:57.47ms
step:616/2330 train_time:35403ms step_avg:57.47ms
step:617/2330 train_time:35459ms step_avg:57.47ms
step:618/2330 train_time:35517ms step_avg:57.47ms
step:619/2330 train_time:35573ms step_avg:57.47ms
step:620/2330 train_time:35632ms step_avg:57.47ms
step:621/2330 train_time:35688ms step_avg:57.47ms
step:622/2330 train_time:35747ms step_avg:57.47ms
step:623/2330 train_time:35803ms step_avg:57.47ms
step:624/2330 train_time:35862ms step_avg:57.47ms
step:625/2330 train_time:35918ms step_avg:57.47ms
step:626/2330 train_time:35977ms step_avg:57.47ms
step:627/2330 train_time:36033ms step_avg:57.47ms
step:628/2330 train_time:36092ms step_avg:57.47ms
step:629/2330 train_time:36148ms step_avg:57.47ms
step:630/2330 train_time:36207ms step_avg:57.47ms
step:631/2330 train_time:36264ms step_avg:57.47ms
step:632/2330 train_time:36323ms step_avg:57.47ms
step:633/2330 train_time:36380ms step_avg:57.47ms
step:634/2330 train_time:36439ms step_avg:57.47ms
step:635/2330 train_time:36496ms step_avg:57.47ms
step:636/2330 train_time:36555ms step_avg:57.48ms
step:637/2330 train_time:36610ms step_avg:57.47ms
step:638/2330 train_time:36669ms step_avg:57.47ms
step:639/2330 train_time:36724ms step_avg:57.47ms
step:640/2330 train_time:36785ms step_avg:57.48ms
step:641/2330 train_time:36842ms step_avg:57.48ms
step:642/2330 train_time:36901ms step_avg:57.48ms
step:643/2330 train_time:36957ms step_avg:57.48ms
step:644/2330 train_time:37016ms step_avg:57.48ms
step:645/2330 train_time:37071ms step_avg:57.48ms
step:646/2330 train_time:37131ms step_avg:57.48ms
step:647/2330 train_time:37187ms step_avg:57.48ms
step:648/2330 train_time:37247ms step_avg:57.48ms
step:649/2330 train_time:37303ms step_avg:57.48ms
step:650/2330 train_time:37362ms step_avg:57.48ms
step:651/2330 train_time:37418ms step_avg:57.48ms
step:652/2330 train_time:37477ms step_avg:57.48ms
step:653/2330 train_time:37533ms step_avg:57.48ms
step:654/2330 train_time:37592ms step_avg:57.48ms
step:655/2330 train_time:37647ms step_avg:57.48ms
step:656/2330 train_time:37707ms step_avg:57.48ms
step:657/2330 train_time:37765ms step_avg:57.48ms
step:658/2330 train_time:37824ms step_avg:57.48ms
step:659/2330 train_time:37881ms step_avg:57.48ms
step:660/2330 train_time:37940ms step_avg:57.49ms
step:661/2330 train_time:37997ms step_avg:57.48ms
step:662/2330 train_time:38055ms step_avg:57.49ms
step:663/2330 train_time:38111ms step_avg:57.48ms
step:664/2330 train_time:38171ms step_avg:57.49ms
step:665/2330 train_time:38226ms step_avg:57.48ms
step:666/2330 train_time:38287ms step_avg:57.49ms
step:667/2330 train_time:38342ms step_avg:57.48ms
step:668/2330 train_time:38403ms step_avg:57.49ms
step:669/2330 train_time:38460ms step_avg:57.49ms
step:670/2330 train_time:38519ms step_avg:57.49ms
step:671/2330 train_time:38575ms step_avg:57.49ms
step:672/2330 train_time:38633ms step_avg:57.49ms
step:673/2330 train_time:38689ms step_avg:57.49ms
step:674/2330 train_time:38748ms step_avg:57.49ms
step:675/2330 train_time:38805ms step_avg:57.49ms
step:676/2330 train_time:38864ms step_avg:57.49ms
step:677/2330 train_time:38920ms step_avg:57.49ms
step:678/2330 train_time:38980ms step_avg:57.49ms
step:679/2330 train_time:39035ms step_avg:57.49ms
step:680/2330 train_time:39094ms step_avg:57.49ms
step:681/2330 train_time:39151ms step_avg:57.49ms
step:682/2330 train_time:39210ms step_avg:57.49ms
step:683/2330 train_time:39266ms step_avg:57.49ms
step:684/2330 train_time:39325ms step_avg:57.49ms
step:685/2330 train_time:39381ms step_avg:57.49ms
step:686/2330 train_time:39441ms step_avg:57.49ms
step:687/2330 train_time:39497ms step_avg:57.49ms
step:688/2330 train_time:39556ms step_avg:57.49ms
step:689/2330 train_time:39612ms step_avg:57.49ms
step:690/2330 train_time:39670ms step_avg:57.49ms
step:691/2330 train_time:39726ms step_avg:57.49ms
step:692/2330 train_time:39786ms step_avg:57.49ms
step:693/2330 train_time:39843ms step_avg:57.49ms
step:694/2330 train_time:39902ms step_avg:57.50ms
step:695/2330 train_time:39958ms step_avg:57.49ms
step:696/2330 train_time:40017ms step_avg:57.50ms
step:697/2330 train_time:40073ms step_avg:57.49ms
step:698/2330 train_time:40132ms step_avg:57.50ms
step:699/2330 train_time:40188ms step_avg:57.49ms
step:700/2330 train_time:40247ms step_avg:57.50ms
step:701/2330 train_time:40303ms step_avg:57.49ms
step:702/2330 train_time:40363ms step_avg:57.50ms
step:703/2330 train_time:40419ms step_avg:57.49ms
step:704/2330 train_time:40478ms step_avg:57.50ms
step:705/2330 train_time:40534ms step_avg:57.49ms
step:706/2330 train_time:40593ms step_avg:57.50ms
step:707/2330 train_time:40649ms step_avg:57.49ms
step:708/2330 train_time:40709ms step_avg:57.50ms
step:709/2330 train_time:40765ms step_avg:57.50ms
step:710/2330 train_time:40824ms step_avg:57.50ms
step:711/2330 train_time:40880ms step_avg:57.50ms
step:712/2330 train_time:40939ms step_avg:57.50ms
step:713/2330 train_time:40996ms step_avg:57.50ms
step:714/2330 train_time:41055ms step_avg:57.50ms
step:715/2330 train_time:41110ms step_avg:57.50ms
step:716/2330 train_time:41170ms step_avg:57.50ms
step:717/2330 train_time:41226ms step_avg:57.50ms
step:718/2330 train_time:41286ms step_avg:57.50ms
step:719/2330 train_time:41342ms step_avg:57.50ms
step:720/2330 train_time:41402ms step_avg:57.50ms
step:721/2330 train_time:41458ms step_avg:57.50ms
step:722/2330 train_time:41517ms step_avg:57.50ms
step:723/2330 train_time:41573ms step_avg:57.50ms
step:724/2330 train_time:41632ms step_avg:57.50ms
step:725/2330 train_time:41688ms step_avg:57.50ms
step:726/2330 train_time:41748ms step_avg:57.50ms
step:727/2330 train_time:41804ms step_avg:57.50ms
step:728/2330 train_time:41863ms step_avg:57.50ms
step:729/2330 train_time:41919ms step_avg:57.50ms
step:730/2330 train_time:41978ms step_avg:57.50ms
step:731/2330 train_time:42034ms step_avg:57.50ms
step:732/2330 train_time:42093ms step_avg:57.50ms
step:733/2330 train_time:42148ms step_avg:57.50ms
step:734/2330 train_time:42209ms step_avg:57.50ms
step:735/2330 train_time:42265ms step_avg:57.50ms
step:736/2330 train_time:42324ms step_avg:57.51ms
step:737/2330 train_time:42380ms step_avg:57.50ms
step:738/2330 train_time:42440ms step_avg:57.51ms
step:739/2330 train_time:42496ms step_avg:57.51ms
step:740/2330 train_time:42555ms step_avg:57.51ms
step:741/2330 train_time:42611ms step_avg:57.50ms
step:742/2330 train_time:42670ms step_avg:57.51ms
step:743/2330 train_time:42726ms step_avg:57.50ms
step:744/2330 train_time:42785ms step_avg:57.51ms
step:745/2330 train_time:42841ms step_avg:57.51ms
step:746/2330 train_time:42900ms step_avg:57.51ms
step:747/2330 train_time:42956ms step_avg:57.51ms
step:748/2330 train_time:43015ms step_avg:57.51ms
step:749/2330 train_time:43071ms step_avg:57.50ms
step:750/2330 train_time:43132ms step_avg:57.51ms
step:750/2330 val_loss:4.2066 train_time:43212ms step_avg:57.62ms
step:751/2330 train_time:43231ms step_avg:57.56ms
step:752/2330 train_time:43253ms step_avg:57.52ms
step:753/2330 train_time:43311ms step_avg:57.52ms
step:754/2330 train_time:43372ms step_avg:57.52ms
step:755/2330 train_time:43429ms step_avg:57.52ms
step:756/2330 train_time:43491ms step_avg:57.53ms
step:757/2330 train_time:43547ms step_avg:57.53ms
step:758/2330 train_time:43606ms step_avg:57.53ms
step:759/2330 train_time:43662ms step_avg:57.53ms
step:760/2330 train_time:43721ms step_avg:57.53ms
step:761/2330 train_time:43776ms step_avg:57.52ms
step:762/2330 train_time:43835ms step_avg:57.53ms
step:763/2330 train_time:43890ms step_avg:57.52ms
step:764/2330 train_time:43948ms step_avg:57.52ms
step:765/2330 train_time:44005ms step_avg:57.52ms
step:766/2330 train_time:44063ms step_avg:57.52ms
step:767/2330 train_time:44120ms step_avg:57.52ms
step:768/2330 train_time:44180ms step_avg:57.53ms
step:769/2330 train_time:44238ms step_avg:57.53ms
step:770/2330 train_time:44299ms step_avg:57.53ms
step:771/2330 train_time:44357ms step_avg:57.53ms
step:772/2330 train_time:44419ms step_avg:57.54ms
step:773/2330 train_time:44476ms step_avg:57.54ms
step:774/2330 train_time:44537ms step_avg:57.54ms
step:775/2330 train_time:44594ms step_avg:57.54ms
step:776/2330 train_time:44653ms step_avg:57.54ms
step:777/2330 train_time:44710ms step_avg:57.54ms
step:778/2330 train_time:44770ms step_avg:57.55ms
step:779/2330 train_time:44827ms step_avg:57.54ms
step:780/2330 train_time:44886ms step_avg:57.55ms
step:781/2330 train_time:44942ms step_avg:57.54ms
step:782/2330 train_time:45002ms step_avg:57.55ms
step:783/2330 train_time:45058ms step_avg:57.55ms
step:784/2330 train_time:45118ms step_avg:57.55ms
step:785/2330 train_time:45175ms step_avg:57.55ms
step:786/2330 train_time:45236ms step_avg:57.55ms
step:787/2330 train_time:45293ms step_avg:57.55ms
step:788/2330 train_time:45353ms step_avg:57.55ms
step:789/2330 train_time:45410ms step_avg:57.55ms
step:790/2330 train_time:45471ms step_avg:57.56ms
step:791/2330 train_time:45528ms step_avg:57.56ms
step:792/2330 train_time:45589ms step_avg:57.56ms
step:793/2330 train_time:45646ms step_avg:57.56ms
step:794/2330 train_time:45705ms step_avg:57.56ms
step:795/2330 train_time:45762ms step_avg:57.56ms
step:796/2330 train_time:45821ms step_avg:57.56ms
step:797/2330 train_time:45877ms step_avg:57.56ms
step:798/2330 train_time:45937ms step_avg:57.56ms
step:799/2330 train_time:45992ms step_avg:57.56ms
step:800/2330 train_time:46052ms step_avg:57.57ms
step:801/2330 train_time:46109ms step_avg:57.56ms
step:802/2330 train_time:46170ms step_avg:57.57ms
step:803/2330 train_time:46227ms step_avg:57.57ms
step:804/2330 train_time:46288ms step_avg:57.57ms
step:805/2330 train_time:46346ms step_avg:57.57ms
step:806/2330 train_time:46406ms step_avg:57.58ms
step:807/2330 train_time:46464ms step_avg:57.58ms
step:808/2330 train_time:46525ms step_avg:57.58ms
step:809/2330 train_time:46583ms step_avg:57.58ms
step:810/2330 train_time:46643ms step_avg:57.58ms
step:811/2330 train_time:46701ms step_avg:57.58ms
step:812/2330 train_time:46761ms step_avg:57.59ms
step:813/2330 train_time:46818ms step_avg:57.59ms
step:814/2330 train_time:46877ms step_avg:57.59ms
step:815/2330 train_time:46933ms step_avg:57.59ms
step:816/2330 train_time:46993ms step_avg:57.59ms
step:817/2330 train_time:47048ms step_avg:57.59ms
step:818/2330 train_time:47109ms step_avg:57.59ms
step:819/2330 train_time:47165ms step_avg:57.59ms
step:820/2330 train_time:47227ms step_avg:57.59ms
step:821/2330 train_time:47284ms step_avg:57.59ms
step:822/2330 train_time:47344ms step_avg:57.60ms
step:823/2330 train_time:47402ms step_avg:57.60ms
step:824/2330 train_time:47462ms step_avg:57.60ms
step:825/2330 train_time:47520ms step_avg:57.60ms
step:826/2330 train_time:47579ms step_avg:57.60ms
step:827/2330 train_time:47637ms step_avg:57.60ms
step:828/2330 train_time:47697ms step_avg:57.60ms
step:829/2330 train_time:47753ms step_avg:57.60ms
step:830/2330 train_time:47814ms step_avg:57.61ms
step:831/2330 train_time:47871ms step_avg:57.61ms
step:832/2330 train_time:47930ms step_avg:57.61ms
step:833/2330 train_time:47987ms step_avg:57.61ms
step:834/2330 train_time:48048ms step_avg:57.61ms
step:835/2330 train_time:48104ms step_avg:57.61ms
step:836/2330 train_time:48165ms step_avg:57.61ms
step:837/2330 train_time:48221ms step_avg:57.61ms
step:838/2330 train_time:48282ms step_avg:57.62ms
step:839/2330 train_time:48339ms step_avg:57.61ms
step:840/2330 train_time:48399ms step_avg:57.62ms
step:841/2330 train_time:48456ms step_avg:57.62ms
step:842/2330 train_time:48516ms step_avg:57.62ms
step:843/2330 train_time:48573ms step_avg:57.62ms
step:844/2330 train_time:48633ms step_avg:57.62ms
step:845/2330 train_time:48690ms step_avg:57.62ms
step:846/2330 train_time:48750ms step_avg:57.62ms
step:847/2330 train_time:48807ms step_avg:57.62ms
step:848/2330 train_time:48867ms step_avg:57.63ms
step:849/2330 train_time:48924ms step_avg:57.63ms
step:850/2330 train_time:48984ms step_avg:57.63ms
step:851/2330 train_time:49041ms step_avg:57.63ms
step:852/2330 train_time:49101ms step_avg:57.63ms
step:853/2330 train_time:49157ms step_avg:57.63ms
step:854/2330 train_time:49216ms step_avg:57.63ms
step:855/2330 train_time:49273ms step_avg:57.63ms
step:856/2330 train_time:49333ms step_avg:57.63ms
step:857/2330 train_time:49389ms step_avg:57.63ms
step:858/2330 train_time:49450ms step_avg:57.63ms
step:859/2330 train_time:49507ms step_avg:57.63ms
step:860/2330 train_time:49568ms step_avg:57.64ms
step:861/2330 train_time:49625ms step_avg:57.64ms
step:862/2330 train_time:49686ms step_avg:57.64ms
step:863/2330 train_time:49743ms step_avg:57.64ms
step:864/2330 train_time:49803ms step_avg:57.64ms
step:865/2330 train_time:49860ms step_avg:57.64ms
step:866/2330 train_time:49919ms step_avg:57.64ms
step:867/2330 train_time:49975ms step_avg:57.64ms
step:868/2330 train_time:50037ms step_avg:57.65ms
step:869/2330 train_time:50093ms step_avg:57.64ms
step:870/2330 train_time:50153ms step_avg:57.65ms
step:871/2330 train_time:50209ms step_avg:57.65ms
step:872/2330 train_time:50270ms step_avg:57.65ms
step:873/2330 train_time:50327ms step_avg:57.65ms
step:874/2330 train_time:50389ms step_avg:57.65ms
step:875/2330 train_time:50445ms step_avg:57.65ms
step:876/2330 train_time:50506ms step_avg:57.65ms
step:877/2330 train_time:50562ms step_avg:57.65ms
step:878/2330 train_time:50623ms step_avg:57.66ms
step:879/2330 train_time:50681ms step_avg:57.66ms
step:880/2330 train_time:50741ms step_avg:57.66ms
step:881/2330 train_time:50799ms step_avg:57.66ms
step:882/2330 train_time:50858ms step_avg:57.66ms
step:883/2330 train_time:50916ms step_avg:57.66ms
step:884/2330 train_time:50975ms step_avg:57.66ms
step:885/2330 train_time:51032ms step_avg:57.66ms
step:886/2330 train_time:51092ms step_avg:57.67ms
step:887/2330 train_time:51148ms step_avg:57.66ms
step:888/2330 train_time:51209ms step_avg:57.67ms
step:889/2330 train_time:51265ms step_avg:57.67ms
step:890/2330 train_time:51326ms step_avg:57.67ms
step:891/2330 train_time:51384ms step_avg:57.67ms
step:892/2330 train_time:51444ms step_avg:57.67ms
step:893/2330 train_time:51501ms step_avg:57.67ms
step:894/2330 train_time:51561ms step_avg:57.67ms
step:895/2330 train_time:51617ms step_avg:57.67ms
step:896/2330 train_time:51677ms step_avg:57.67ms
step:897/2330 train_time:51734ms step_avg:57.67ms
step:898/2330 train_time:51793ms step_avg:57.68ms
step:899/2330 train_time:51849ms step_avg:57.67ms
step:900/2330 train_time:51911ms step_avg:57.68ms
step:901/2330 train_time:51967ms step_avg:57.68ms
step:902/2330 train_time:52029ms step_avg:57.68ms
step:903/2330 train_time:52086ms step_avg:57.68ms
step:904/2330 train_time:52145ms step_avg:57.68ms
step:905/2330 train_time:52203ms step_avg:57.68ms
step:906/2330 train_time:52263ms step_avg:57.68ms
step:907/2330 train_time:52319ms step_avg:57.68ms
step:908/2330 train_time:52379ms step_avg:57.69ms
step:909/2330 train_time:52436ms step_avg:57.69ms
step:910/2330 train_time:52496ms step_avg:57.69ms
step:911/2330 train_time:52553ms step_avg:57.69ms
step:912/2330 train_time:52613ms step_avg:57.69ms
step:913/2330 train_time:52670ms step_avg:57.69ms
step:914/2330 train_time:52730ms step_avg:57.69ms
step:915/2330 train_time:52786ms step_avg:57.69ms
step:916/2330 train_time:52846ms step_avg:57.69ms
step:917/2330 train_time:52904ms step_avg:57.69ms
step:918/2330 train_time:52964ms step_avg:57.69ms
step:919/2330 train_time:53021ms step_avg:57.69ms
step:920/2330 train_time:53081ms step_avg:57.70ms
step:921/2330 train_time:53139ms step_avg:57.70ms
step:922/2330 train_time:53198ms step_avg:57.70ms
step:923/2330 train_time:53255ms step_avg:57.70ms
step:924/2330 train_time:53316ms step_avg:57.70ms
step:925/2330 train_time:53373ms step_avg:57.70ms
step:926/2330 train_time:53432ms step_avg:57.70ms
step:927/2330 train_time:53489ms step_avg:57.70ms
step:928/2330 train_time:53549ms step_avg:57.70ms
step:929/2330 train_time:53606ms step_avg:57.70ms
step:930/2330 train_time:53667ms step_avg:57.71ms
step:931/2330 train_time:53723ms step_avg:57.71ms
step:932/2330 train_time:53783ms step_avg:57.71ms
step:933/2330 train_time:53840ms step_avg:57.71ms
step:934/2330 train_time:53900ms step_avg:57.71ms
step:935/2330 train_time:53956ms step_avg:57.71ms
step:936/2330 train_time:54017ms step_avg:57.71ms
step:937/2330 train_time:54074ms step_avg:57.71ms
step:938/2330 train_time:54134ms step_avg:57.71ms
step:939/2330 train_time:54190ms step_avg:57.71ms
step:940/2330 train_time:54251ms step_avg:57.71ms
step:941/2330 train_time:54308ms step_avg:57.71ms
step:942/2330 train_time:54368ms step_avg:57.72ms
step:943/2330 train_time:54425ms step_avg:57.71ms
step:944/2330 train_time:54485ms step_avg:57.72ms
step:945/2330 train_time:54543ms step_avg:57.72ms
step:946/2330 train_time:54603ms step_avg:57.72ms
step:947/2330 train_time:54660ms step_avg:57.72ms
step:948/2330 train_time:54720ms step_avg:57.72ms
step:949/2330 train_time:54776ms step_avg:57.72ms
step:950/2330 train_time:54836ms step_avg:57.72ms
step:951/2330 train_time:54893ms step_avg:57.72ms
step:952/2330 train_time:54953ms step_avg:57.72ms
step:953/2330 train_time:55010ms step_avg:57.72ms
step:954/2330 train_time:55070ms step_avg:57.73ms
step:955/2330 train_time:55126ms step_avg:57.72ms
step:956/2330 train_time:55187ms step_avg:57.73ms
step:957/2330 train_time:55243ms step_avg:57.73ms
step:958/2330 train_time:55304ms step_avg:57.73ms
step:959/2330 train_time:55361ms step_avg:57.73ms
step:960/2330 train_time:55422ms step_avg:57.73ms
step:961/2330 train_time:55478ms step_avg:57.73ms
step:962/2330 train_time:55539ms step_avg:57.73ms
step:963/2330 train_time:55596ms step_avg:57.73ms
step:964/2330 train_time:55656ms step_avg:57.73ms
step:965/2330 train_time:55713ms step_avg:57.73ms
step:966/2330 train_time:55772ms step_avg:57.74ms
step:967/2330 train_time:55829ms step_avg:57.73ms
step:968/2330 train_time:55889ms step_avg:57.74ms
step:969/2330 train_time:55947ms step_avg:57.74ms
step:970/2330 train_time:56006ms step_avg:57.74ms
step:971/2330 train_time:56063ms step_avg:57.74ms
step:972/2330 train_time:56124ms step_avg:57.74ms
step:973/2330 train_time:56181ms step_avg:57.74ms
step:974/2330 train_time:56241ms step_avg:57.74ms
step:975/2330 train_time:56298ms step_avg:57.74ms
step:976/2330 train_time:56357ms step_avg:57.74ms
step:977/2330 train_time:56413ms step_avg:57.74ms
step:978/2330 train_time:56473ms step_avg:57.74ms
step:979/2330 train_time:56530ms step_avg:57.74ms
step:980/2330 train_time:56590ms step_avg:57.74ms
step:981/2330 train_time:56647ms step_avg:57.74ms
step:982/2330 train_time:56707ms step_avg:57.75ms
step:983/2330 train_time:56764ms step_avg:57.75ms
step:984/2330 train_time:56825ms step_avg:57.75ms
step:985/2330 train_time:56882ms step_avg:57.75ms
step:986/2330 train_time:56942ms step_avg:57.75ms
step:987/2330 train_time:56999ms step_avg:57.75ms
step:988/2330 train_time:57058ms step_avg:57.75ms
step:989/2330 train_time:57115ms step_avg:57.75ms
step:990/2330 train_time:57174ms step_avg:57.75ms
step:991/2330 train_time:57231ms step_avg:57.75ms
step:992/2330 train_time:57291ms step_avg:57.75ms
step:993/2330 train_time:57347ms step_avg:57.75ms
step:994/2330 train_time:57408ms step_avg:57.75ms
step:995/2330 train_time:57465ms step_avg:57.75ms
step:996/2330 train_time:57525ms step_avg:57.76ms
step:997/2330 train_time:57582ms step_avg:57.76ms
step:998/2330 train_time:57643ms step_avg:57.76ms
step:999/2330 train_time:57700ms step_avg:57.76ms
step:1000/2330 train_time:57760ms step_avg:57.76ms
step:1000/2330 val_loss:4.0692 train_time:57841ms step_avg:57.84ms
step:1001/2330 train_time:57858ms step_avg:57.80ms
step:1002/2330 train_time:57878ms step_avg:57.76ms
step:1003/2330 train_time:57933ms step_avg:57.76ms
step:1004/2330 train_time:58001ms step_avg:57.77ms
step:1005/2330 train_time:58057ms step_avg:57.77ms
step:1006/2330 train_time:58120ms step_avg:57.77ms
step:1007/2330 train_time:58176ms step_avg:57.77ms
step:1008/2330 train_time:58237ms step_avg:57.77ms
step:1009/2330 train_time:58293ms step_avg:57.77ms
step:1010/2330 train_time:58352ms step_avg:57.77ms
step:1011/2330 train_time:58409ms step_avg:57.77ms
step:1012/2330 train_time:58468ms step_avg:57.77ms
step:1013/2330 train_time:58524ms step_avg:57.77ms
step:1014/2330 train_time:58583ms step_avg:57.77ms
step:1015/2330 train_time:58639ms step_avg:57.77ms
step:1016/2330 train_time:58699ms step_avg:57.77ms
step:1017/2330 train_time:58757ms step_avg:57.77ms
step:1018/2330 train_time:58820ms step_avg:57.78ms
step:1019/2330 train_time:58877ms step_avg:57.78ms
step:1020/2330 train_time:58939ms step_avg:57.78ms
step:1021/2330 train_time:58995ms step_avg:57.78ms
step:1022/2330 train_time:59057ms step_avg:57.79ms
step:1023/2330 train_time:59114ms step_avg:57.78ms
step:1024/2330 train_time:59174ms step_avg:57.79ms
step:1025/2330 train_time:59231ms step_avg:57.79ms
step:1026/2330 train_time:59291ms step_avg:57.79ms
step:1027/2330 train_time:59347ms step_avg:57.79ms
step:1028/2330 train_time:59407ms step_avg:57.79ms
step:1029/2330 train_time:59463ms step_avg:57.79ms
step:1030/2330 train_time:59523ms step_avg:57.79ms
step:1031/2330 train_time:59580ms step_avg:57.79ms
step:1032/2330 train_time:59639ms step_avg:57.79ms
step:1033/2330 train_time:59698ms step_avg:57.79ms
step:1034/2330 train_time:59757ms step_avg:57.79ms
step:1035/2330 train_time:59816ms step_avg:57.79ms
step:1036/2330 train_time:59876ms step_avg:57.79ms
step:1037/2330 train_time:59933ms step_avg:57.79ms
step:1038/2330 train_time:59995ms step_avg:57.80ms
step:1039/2330 train_time:60051ms step_avg:57.80ms
step:1040/2330 train_time:60114ms step_avg:57.80ms
step:1041/2330 train_time:60171ms step_avg:57.80ms
step:1042/2330 train_time:60230ms step_avg:57.80ms
step:1043/2330 train_time:60287ms step_avg:57.80ms
step:1044/2330 train_time:60348ms step_avg:57.80ms
step:1045/2330 train_time:60405ms step_avg:57.80ms
step:1046/2330 train_time:60465ms step_avg:57.81ms
step:1047/2330 train_time:60522ms step_avg:57.81ms
step:1048/2330 train_time:60581ms step_avg:57.81ms
step:1049/2330 train_time:60639ms step_avg:57.81ms
step:1050/2330 train_time:60698ms step_avg:57.81ms
step:1051/2330 train_time:60756ms step_avg:57.81ms
step:1052/2330 train_time:60815ms step_avg:57.81ms
step:1053/2330 train_time:60873ms step_avg:57.81ms
step:1054/2330 train_time:60933ms step_avg:57.81ms
step:1055/2330 train_time:60990ms step_avg:57.81ms
step:1056/2330 train_time:61053ms step_avg:57.81ms
step:1057/2330 train_time:61110ms step_avg:57.81ms
step:1058/2330 train_time:61170ms step_avg:57.82ms
step:1059/2330 train_time:61227ms step_avg:57.82ms
step:1060/2330 train_time:61287ms step_avg:57.82ms
step:1061/2330 train_time:61344ms step_avg:57.82ms
step:1062/2330 train_time:61405ms step_avg:57.82ms
step:1063/2330 train_time:61461ms step_avg:57.82ms
step:1064/2330 train_time:61523ms step_avg:57.82ms
step:1065/2330 train_time:61579ms step_avg:57.82ms
step:1066/2330 train_time:61638ms step_avg:57.82ms
step:1067/2330 train_time:61696ms step_avg:57.82ms
step:1068/2330 train_time:61755ms step_avg:57.82ms
step:1069/2330 train_time:61812ms step_avg:57.82ms
step:1070/2330 train_time:61872ms step_avg:57.82ms
step:1071/2330 train_time:61929ms step_avg:57.82ms
step:1072/2330 train_time:61989ms step_avg:57.83ms
step:1073/2330 train_time:62047ms step_avg:57.83ms
step:1074/2330 train_time:62108ms step_avg:57.83ms
step:1075/2330 train_time:62164ms step_avg:57.83ms
step:1076/2330 train_time:62225ms step_avg:57.83ms
step:1077/2330 train_time:62281ms step_avg:57.83ms
step:1078/2330 train_time:62341ms step_avg:57.83ms
step:1079/2330 train_time:62398ms step_avg:57.83ms
step:1080/2330 train_time:62458ms step_avg:57.83ms
step:1081/2330 train_time:62514ms step_avg:57.83ms
step:1082/2330 train_time:62574ms step_avg:57.83ms
step:1083/2330 train_time:62631ms step_avg:57.83ms
step:1084/2330 train_time:62692ms step_avg:57.83ms
step:1085/2330 train_time:62749ms step_avg:57.83ms
step:1086/2330 train_time:62810ms step_avg:57.84ms
step:1087/2330 train_time:62867ms step_avg:57.84ms
step:1088/2330 train_time:62926ms step_avg:57.84ms
step:1089/2330 train_time:62983ms step_avg:57.84ms
step:1090/2330 train_time:63043ms step_avg:57.84ms
step:1091/2330 train_time:63100ms step_avg:57.84ms
step:1092/2330 train_time:63161ms step_avg:57.84ms
step:1093/2330 train_time:63217ms step_avg:57.84ms
step:1094/2330 train_time:63278ms step_avg:57.84ms
step:1095/2330 train_time:63334ms step_avg:57.84ms
step:1096/2330 train_time:63395ms step_avg:57.84ms
step:1097/2330 train_time:63452ms step_avg:57.84ms
step:1098/2330 train_time:63513ms step_avg:57.84ms
step:1099/2330 train_time:63570ms step_avg:57.84ms
step:1100/2330 train_time:63630ms step_avg:57.85ms
step:1101/2330 train_time:63686ms step_avg:57.84ms
step:1102/2330 train_time:63748ms step_avg:57.85ms
step:1103/2330 train_time:63806ms step_avg:57.85ms
step:1104/2330 train_time:63865ms step_avg:57.85ms
step:1105/2330 train_time:63922ms step_avg:57.85ms
step:1106/2330 train_time:63982ms step_avg:57.85ms
step:1107/2330 train_time:64040ms step_avg:57.85ms
step:1108/2330 train_time:64099ms step_avg:57.85ms
step:1109/2330 train_time:64156ms step_avg:57.85ms
step:1110/2330 train_time:64217ms step_avg:57.85ms
step:1111/2330 train_time:64273ms step_avg:57.85ms
step:1112/2330 train_time:64334ms step_avg:57.85ms
step:1113/2330 train_time:64390ms step_avg:57.85ms
step:1114/2330 train_time:64451ms step_avg:57.86ms
step:1115/2330 train_time:64508ms step_avg:57.85ms
step:1116/2330 train_time:64568ms step_avg:57.86ms
step:1117/2330 train_time:64625ms step_avg:57.86ms
step:1118/2330 train_time:64685ms step_avg:57.86ms
step:1119/2330 train_time:64742ms step_avg:57.86ms
step:1120/2330 train_time:64802ms step_avg:57.86ms
step:1121/2330 train_time:64858ms step_avg:57.86ms
step:1122/2330 train_time:64918ms step_avg:57.86ms
step:1123/2330 train_time:64975ms step_avg:57.86ms
step:1124/2330 train_time:65036ms step_avg:57.86ms
step:1125/2330 train_time:65093ms step_avg:57.86ms
step:1126/2330 train_time:65153ms step_avg:57.86ms
step:1127/2330 train_time:65210ms step_avg:57.86ms
step:1128/2330 train_time:65271ms step_avg:57.86ms
step:1129/2330 train_time:65328ms step_avg:57.86ms
step:1130/2330 train_time:65387ms step_avg:57.86ms
step:1131/2330 train_time:65444ms step_avg:57.86ms
step:1132/2330 train_time:65504ms step_avg:57.87ms
step:1133/2330 train_time:65561ms step_avg:57.86ms
step:1134/2330 train_time:65621ms step_avg:57.87ms
step:1135/2330 train_time:65678ms step_avg:57.87ms
step:1136/2330 train_time:65738ms step_avg:57.87ms
step:1137/2330 train_time:65795ms step_avg:57.87ms
step:1138/2330 train_time:65855ms step_avg:57.87ms
step:1139/2330 train_time:65912ms step_avg:57.87ms
step:1140/2330 train_time:65972ms step_avg:57.87ms
step:1141/2330 train_time:66030ms step_avg:57.87ms
step:1142/2330 train_time:66091ms step_avg:57.87ms
step:1143/2330 train_time:66147ms step_avg:57.87ms
step:1144/2330 train_time:66207ms step_avg:57.87ms
step:1145/2330 train_time:66265ms step_avg:57.87ms
step:1146/2330 train_time:66325ms step_avg:57.87ms
step:1147/2330 train_time:66382ms step_avg:57.87ms
step:1148/2330 train_time:66441ms step_avg:57.88ms
step:1149/2330 train_time:66498ms step_avg:57.87ms
step:1150/2330 train_time:66558ms step_avg:57.88ms
step:1151/2330 train_time:66615ms step_avg:57.88ms
step:1152/2330 train_time:66675ms step_avg:57.88ms
step:1153/2330 train_time:66732ms step_avg:57.88ms
step:1154/2330 train_time:66794ms step_avg:57.88ms
step:1155/2330 train_time:66851ms step_avg:57.88ms
step:1156/2330 train_time:66911ms step_avg:57.88ms
step:1157/2330 train_time:66968ms step_avg:57.88ms
step:1158/2330 train_time:67029ms step_avg:57.88ms
step:1159/2330 train_time:67086ms step_avg:57.88ms
step:1160/2330 train_time:67148ms step_avg:57.89ms
step:1161/2330 train_time:67204ms step_avg:57.88ms
step:1162/2330 train_time:67265ms step_avg:57.89ms
step:1163/2330 train_time:67322ms step_avg:57.89ms
step:1164/2330 train_time:67382ms step_avg:57.89ms
step:1165/2330 train_time:67440ms step_avg:57.89ms
step:1166/2330 train_time:67499ms step_avg:57.89ms
step:1167/2330 train_time:67556ms step_avg:57.89ms
step:1168/2330 train_time:67616ms step_avg:57.89ms
step:1169/2330 train_time:67672ms step_avg:57.89ms
step:1170/2330 train_time:67733ms step_avg:57.89ms
step:1171/2330 train_time:67790ms step_avg:57.89ms
step:1172/2330 train_time:67851ms step_avg:57.89ms
step:1173/2330 train_time:67908ms step_avg:57.89ms
step:1174/2330 train_time:67968ms step_avg:57.89ms
step:1175/2330 train_time:68025ms step_avg:57.89ms
step:1176/2330 train_time:68085ms step_avg:57.90ms
step:1177/2330 train_time:68142ms step_avg:57.89ms
step:1178/2330 train_time:68202ms step_avg:57.90ms
step:1179/2330 train_time:68259ms step_avg:57.90ms
step:1180/2330 train_time:68318ms step_avg:57.90ms
step:1181/2330 train_time:68375ms step_avg:57.90ms
step:1182/2330 train_time:68436ms step_avg:57.90ms
step:1183/2330 train_time:68493ms step_avg:57.90ms
step:1184/2330 train_time:68553ms step_avg:57.90ms
step:1185/2330 train_time:68610ms step_avg:57.90ms
step:1186/2330 train_time:68670ms step_avg:57.90ms
step:1187/2330 train_time:68728ms step_avg:57.90ms
step:1188/2330 train_time:68788ms step_avg:57.90ms
step:1189/2330 train_time:68845ms step_avg:57.90ms
step:1190/2330 train_time:68905ms step_avg:57.90ms
step:1191/2330 train_time:68961ms step_avg:57.90ms
step:1192/2330 train_time:69021ms step_avg:57.90ms
step:1193/2330 train_time:69077ms step_avg:57.90ms
step:1194/2330 train_time:69138ms step_avg:57.90ms
step:1195/2330 train_time:69195ms step_avg:57.90ms
step:1196/2330 train_time:69254ms step_avg:57.91ms
step:1197/2330 train_time:69311ms step_avg:57.90ms
step:1198/2330 train_time:69372ms step_avg:57.91ms
step:1199/2330 train_time:69429ms step_avg:57.91ms
step:1200/2330 train_time:69489ms step_avg:57.91ms
step:1201/2330 train_time:69547ms step_avg:57.91ms
step:1202/2330 train_time:69608ms step_avg:57.91ms
step:1203/2330 train_time:69666ms step_avg:57.91ms
step:1204/2330 train_time:69725ms step_avg:57.91ms
step:1205/2330 train_time:69782ms step_avg:57.91ms
step:1206/2330 train_time:69842ms step_avg:57.91ms
step:1207/2330 train_time:69899ms step_avg:57.91ms
step:1208/2330 train_time:69958ms step_avg:57.91ms
step:1209/2330 train_time:70015ms step_avg:57.91ms
step:1210/2330 train_time:70075ms step_avg:57.91ms
step:1211/2330 train_time:70132ms step_avg:57.91ms
step:1212/2330 train_time:70192ms step_avg:57.91ms
step:1213/2330 train_time:70249ms step_avg:57.91ms
step:1214/2330 train_time:70310ms step_avg:57.92ms
step:1215/2330 train_time:70367ms step_avg:57.92ms
step:1216/2330 train_time:70427ms step_avg:57.92ms
step:1217/2330 train_time:70485ms step_avg:57.92ms
step:1218/2330 train_time:70545ms step_avg:57.92ms
step:1219/2330 train_time:70601ms step_avg:57.92ms
step:1220/2330 train_time:70663ms step_avg:57.92ms
step:1221/2330 train_time:70720ms step_avg:57.92ms
step:1222/2330 train_time:70779ms step_avg:57.92ms
step:1223/2330 train_time:70836ms step_avg:57.92ms
step:1224/2330 train_time:70897ms step_avg:57.92ms
step:1225/2330 train_time:70954ms step_avg:57.92ms
step:1226/2330 train_time:71013ms step_avg:57.92ms
step:1227/2330 train_time:71070ms step_avg:57.92ms
step:1228/2330 train_time:71131ms step_avg:57.92ms
step:1229/2330 train_time:71188ms step_avg:57.92ms
step:1230/2330 train_time:71249ms step_avg:57.93ms
step:1231/2330 train_time:71306ms step_avg:57.92ms
step:1232/2330 train_time:71367ms step_avg:57.93ms
step:1233/2330 train_time:71423ms step_avg:57.93ms
step:1234/2330 train_time:71483ms step_avg:57.93ms
step:1235/2330 train_time:71540ms step_avg:57.93ms
step:1236/2330 train_time:71601ms step_avg:57.93ms
step:1237/2330 train_time:71657ms step_avg:57.93ms
step:1238/2330 train_time:71717ms step_avg:57.93ms
step:1239/2330 train_time:71774ms step_avg:57.93ms
step:1240/2330 train_time:71835ms step_avg:57.93ms
step:1241/2330 train_time:71892ms step_avg:57.93ms
step:1242/2330 train_time:71952ms step_avg:57.93ms
step:1243/2330 train_time:72008ms step_avg:57.93ms
step:1244/2330 train_time:72068ms step_avg:57.93ms
step:1245/2330 train_time:72125ms step_avg:57.93ms
step:1246/2330 train_time:72185ms step_avg:57.93ms
step:1247/2330 train_time:72243ms step_avg:57.93ms
step:1248/2330 train_time:72302ms step_avg:57.93ms
step:1249/2330 train_time:72359ms step_avg:57.93ms
step:1250/2330 train_time:72419ms step_avg:57.94ms
step:1250/2330 val_loss:3.9871 train_time:72500ms step_avg:58.00ms
step:1251/2330 train_time:72517ms step_avg:57.97ms
step:1252/2330 train_time:72539ms step_avg:57.94ms
step:1253/2330 train_time:72596ms step_avg:57.94ms
step:1254/2330 train_time:72666ms step_avg:57.95ms
step:1255/2330 train_time:72723ms step_avg:57.95ms
step:1256/2330 train_time:72782ms step_avg:57.95ms
step:1257/2330 train_time:72838ms step_avg:57.95ms
step:1258/2330 train_time:72898ms step_avg:57.95ms
step:1259/2330 train_time:72954ms step_avg:57.95ms
step:1260/2330 train_time:73015ms step_avg:57.95ms
step:1261/2330 train_time:73071ms step_avg:57.95ms
step:1262/2330 train_time:73131ms step_avg:57.95ms
step:1263/2330 train_time:73187ms step_avg:57.95ms
step:1264/2330 train_time:73246ms step_avg:57.95ms
step:1265/2330 train_time:73303ms step_avg:57.95ms
step:1266/2330 train_time:73361ms step_avg:57.95ms
step:1267/2330 train_time:73417ms step_avg:57.95ms
step:1268/2330 train_time:73478ms step_avg:57.95ms
step:1269/2330 train_time:73536ms step_avg:57.95ms
step:1270/2330 train_time:73598ms step_avg:57.95ms
step:1271/2330 train_time:73656ms step_avg:57.95ms
step:1272/2330 train_time:73718ms step_avg:57.95ms
step:1273/2330 train_time:73775ms step_avg:57.95ms
step:1274/2330 train_time:73835ms step_avg:57.96ms
step:1275/2330 train_time:73892ms step_avg:57.95ms
step:1276/2330 train_time:73954ms step_avg:57.96ms
step:1277/2330 train_time:74010ms step_avg:57.96ms
step:1278/2330 train_time:74071ms step_avg:57.96ms
step:1279/2330 train_time:74127ms step_avg:57.96ms
step:1280/2330 train_time:74186ms step_avg:57.96ms
step:1281/2330 train_time:74243ms step_avg:57.96ms
step:1282/2330 train_time:74302ms step_avg:57.96ms
step:1283/2330 train_time:74359ms step_avg:57.96ms
step:1284/2330 train_time:74418ms step_avg:57.96ms
step:1285/2330 train_time:74476ms step_avg:57.96ms
step:1286/2330 train_time:74537ms step_avg:57.96ms
step:1287/2330 train_time:74594ms step_avg:57.96ms
step:1288/2330 train_time:74658ms step_avg:57.96ms
step:1289/2330 train_time:74715ms step_avg:57.96ms
step:1290/2330 train_time:74776ms step_avg:57.97ms
step:1291/2330 train_time:74832ms step_avg:57.96ms
step:1292/2330 train_time:74893ms step_avg:57.97ms
step:1293/2330 train_time:74950ms step_avg:57.97ms
step:1294/2330 train_time:75010ms step_avg:57.97ms
step:1295/2330 train_time:75067ms step_avg:57.97ms
step:1296/2330 train_time:75126ms step_avg:57.97ms
step:1297/2330 train_time:75184ms step_avg:57.97ms
step:1298/2330 train_time:75243ms step_avg:57.97ms
step:1299/2330 train_time:75299ms step_avg:57.97ms
step:1300/2330 train_time:75359ms step_avg:57.97ms
step:1301/2330 train_time:75416ms step_avg:57.97ms
step:1302/2330 train_time:75475ms step_avg:57.97ms
step:1303/2330 train_time:75532ms step_avg:57.97ms
step:1304/2330 train_time:75593ms step_avg:57.97ms
step:1305/2330 train_time:75651ms step_avg:57.97ms
step:1306/2330 train_time:75711ms step_avg:57.97ms
step:1307/2330 train_time:75769ms step_avg:57.97ms
step:1308/2330 train_time:75829ms step_avg:57.97ms
step:1309/2330 train_time:75886ms step_avg:57.97ms
step:1310/2330 train_time:75946ms step_avg:57.97ms
step:1311/2330 train_time:76002ms step_avg:57.97ms
step:1312/2330 train_time:76063ms step_avg:57.97ms
step:1313/2330 train_time:76119ms step_avg:57.97ms
step:1314/2330 train_time:76179ms step_avg:57.98ms
step:1315/2330 train_time:76236ms step_avg:57.97ms
step:1316/2330 train_time:76296ms step_avg:57.98ms
step:1317/2330 train_time:76353ms step_avg:57.98ms
step:1318/2330 train_time:76413ms step_avg:57.98ms
step:1319/2330 train_time:76470ms step_avg:57.98ms
step:1320/2330 train_time:76530ms step_avg:57.98ms
step:1321/2330 train_time:76588ms step_avg:57.98ms
step:1322/2330 train_time:76648ms step_avg:57.98ms
step:1323/2330 train_time:76706ms step_avg:57.98ms
step:1324/2330 train_time:76767ms step_avg:57.98ms
step:1325/2330 train_time:76825ms step_avg:57.98ms
step:1326/2330 train_time:76885ms step_avg:57.98ms
step:1327/2330 train_time:76942ms step_avg:57.98ms
step:1328/2330 train_time:77002ms step_avg:57.98ms
step:1329/2330 train_time:77058ms step_avg:57.98ms
step:1330/2330 train_time:77118ms step_avg:57.98ms
step:1331/2330 train_time:77174ms step_avg:57.98ms
step:1332/2330 train_time:77235ms step_avg:57.98ms
step:1333/2330 train_time:77291ms step_avg:57.98ms
step:1334/2330 train_time:77351ms step_avg:57.98ms
step:1335/2330 train_time:77409ms step_avg:57.98ms
step:1336/2330 train_time:77469ms step_avg:57.99ms
step:1337/2330 train_time:77527ms step_avg:57.99ms
step:1338/2330 train_time:77586ms step_avg:57.99ms
step:1339/2330 train_time:77643ms step_avg:57.99ms
step:1340/2330 train_time:77703ms step_avg:57.99ms
step:1341/2330 train_time:77761ms step_avg:57.99ms
step:1342/2330 train_time:77822ms step_avg:57.99ms
step:1343/2330 train_time:77879ms step_avg:57.99ms
step:1344/2330 train_time:77939ms step_avg:57.99ms
step:1345/2330 train_time:77995ms step_avg:57.99ms
step:1346/2330 train_time:78056ms step_avg:57.99ms
step:1347/2330 train_time:78112ms step_avg:57.99ms
step:1348/2330 train_time:78173ms step_avg:57.99ms
step:1349/2330 train_time:78229ms step_avg:57.99ms
step:1350/2330 train_time:78289ms step_avg:57.99ms
step:1351/2330 train_time:78346ms step_avg:57.99ms
step:1352/2330 train_time:78406ms step_avg:57.99ms
step:1353/2330 train_time:78463ms step_avg:57.99ms
step:1354/2330 train_time:78523ms step_avg:57.99ms
step:1355/2330 train_time:78580ms step_avg:57.99ms
step:1356/2330 train_time:78641ms step_avg:58.00ms
step:1357/2330 train_time:78698ms step_avg:57.99ms
step:1358/2330 train_time:78759ms step_avg:58.00ms
step:1359/2330 train_time:78816ms step_avg:58.00ms
step:1360/2330 train_time:78876ms step_avg:58.00ms
step:1361/2330 train_time:78933ms step_avg:58.00ms
step:1362/2330 train_time:78993ms step_avg:58.00ms
step:1363/2330 train_time:79050ms step_avg:58.00ms
step:1364/2330 train_time:79110ms step_avg:58.00ms
step:1365/2330 train_time:79167ms step_avg:58.00ms
step:1366/2330 train_time:79227ms step_avg:58.00ms
step:1367/2330 train_time:79284ms step_avg:58.00ms
step:1368/2330 train_time:79344ms step_avg:58.00ms
step:1369/2330 train_time:79400ms step_avg:58.00ms
step:1370/2330 train_time:79460ms step_avg:58.00ms
step:1371/2330 train_time:79517ms step_avg:58.00ms
step:1372/2330 train_time:79578ms step_avg:58.00ms
step:1373/2330 train_time:79635ms step_avg:58.00ms
step:1374/2330 train_time:79696ms step_avg:58.00ms
step:1375/2330 train_time:79753ms step_avg:58.00ms
step:1376/2330 train_time:79814ms step_avg:58.00ms
step:1377/2330 train_time:79872ms step_avg:58.00ms
step:1378/2330 train_time:79931ms step_avg:58.01ms
step:1379/2330 train_time:79989ms step_avg:58.01ms
step:1380/2330 train_time:80049ms step_avg:58.01ms
step:1381/2330 train_time:80105ms step_avg:58.00ms
step:1382/2330 train_time:80166ms step_avg:58.01ms
step:1383/2330 train_time:80223ms step_avg:58.01ms
step:1384/2330 train_time:80283ms step_avg:58.01ms
step:1385/2330 train_time:80339ms step_avg:58.01ms
step:1386/2330 train_time:80399ms step_avg:58.01ms
step:1387/2330 train_time:80456ms step_avg:58.01ms
step:1388/2330 train_time:80516ms step_avg:58.01ms
step:1389/2330 train_time:80573ms step_avg:58.01ms
step:1390/2330 train_time:80633ms step_avg:58.01ms
step:1391/2330 train_time:80690ms step_avg:58.01ms
step:1392/2330 train_time:80750ms step_avg:58.01ms
step:1393/2330 train_time:80806ms step_avg:58.01ms
step:1394/2330 train_time:80867ms step_avg:58.01ms
step:1395/2330 train_time:80924ms step_avg:58.01ms
step:1396/2330 train_time:80984ms step_avg:58.01ms
step:1397/2330 train_time:81041ms step_avg:58.01ms
step:1398/2330 train_time:81101ms step_avg:58.01ms
step:1399/2330 train_time:81159ms step_avg:58.01ms
step:1400/2330 train_time:81219ms step_avg:58.01ms
step:1401/2330 train_time:81276ms step_avg:58.01ms
step:1402/2330 train_time:81336ms step_avg:58.01ms
step:1403/2330 train_time:81392ms step_avg:58.01ms
step:1404/2330 train_time:81453ms step_avg:58.02ms
step:1405/2330 train_time:81510ms step_avg:58.01ms
step:1406/2330 train_time:81570ms step_avg:58.02ms
step:1407/2330 train_time:81628ms step_avg:58.02ms
step:1408/2330 train_time:81687ms step_avg:58.02ms
step:1409/2330 train_time:81743ms step_avg:58.02ms
step:1410/2330 train_time:81804ms step_avg:58.02ms
step:1411/2330 train_time:81861ms step_avg:58.02ms
step:1412/2330 train_time:81921ms step_avg:58.02ms
step:1413/2330 train_time:81978ms step_avg:58.02ms
step:1414/2330 train_time:82038ms step_avg:58.02ms
step:1415/2330 train_time:82095ms step_avg:58.02ms
step:1416/2330 train_time:82156ms step_avg:58.02ms
step:1417/2330 train_time:82213ms step_avg:58.02ms
step:1418/2330 train_time:82274ms step_avg:58.02ms
step:1419/2330 train_time:82330ms step_avg:58.02ms
step:1420/2330 train_time:82391ms step_avg:58.02ms
step:1421/2330 train_time:82448ms step_avg:58.02ms
step:1422/2330 train_time:82508ms step_avg:58.02ms
step:1423/2330 train_time:82565ms step_avg:58.02ms
step:1424/2330 train_time:82624ms step_avg:58.02ms
step:1425/2330 train_time:82681ms step_avg:58.02ms
step:1426/2330 train_time:82741ms step_avg:58.02ms
step:1427/2330 train_time:82798ms step_avg:58.02ms
step:1428/2330 train_time:82857ms step_avg:58.02ms
step:1429/2330 train_time:82914ms step_avg:58.02ms
step:1430/2330 train_time:82975ms step_avg:58.02ms
step:1431/2330 train_time:83032ms step_avg:58.02ms
step:1432/2330 train_time:83093ms step_avg:58.03ms
step:1433/2330 train_time:83150ms step_avg:58.03ms
step:1434/2330 train_time:83209ms step_avg:58.03ms
step:1435/2330 train_time:83266ms step_avg:58.03ms
step:1436/2330 train_time:83326ms step_avg:58.03ms
step:1437/2330 train_time:83383ms step_avg:58.03ms
step:1438/2330 train_time:83442ms step_avg:58.03ms
step:1439/2330 train_time:83499ms step_avg:58.03ms
step:1440/2330 train_time:83560ms step_avg:58.03ms
step:1441/2330 train_time:83616ms step_avg:58.03ms
step:1442/2330 train_time:83677ms step_avg:58.03ms
step:1443/2330 train_time:83733ms step_avg:58.03ms
step:1444/2330 train_time:83794ms step_avg:58.03ms
step:1445/2330 train_time:83851ms step_avg:58.03ms
step:1446/2330 train_time:83910ms step_avg:58.03ms
step:1447/2330 train_time:83967ms step_avg:58.03ms
step:1448/2330 train_time:84027ms step_avg:58.03ms
step:1449/2330 train_time:84084ms step_avg:58.03ms
step:1450/2330 train_time:84143ms step_avg:58.03ms
step:1451/2330 train_time:84200ms step_avg:58.03ms
step:1452/2330 train_time:84261ms step_avg:58.03ms
step:1453/2330 train_time:84318ms step_avg:58.03ms
step:1454/2330 train_time:84378ms step_avg:58.03ms
step:1455/2330 train_time:84435ms step_avg:58.03ms
step:1456/2330 train_time:84497ms step_avg:58.03ms
step:1457/2330 train_time:84553ms step_avg:58.03ms
step:1458/2330 train_time:84615ms step_avg:58.04ms
step:1459/2330 train_time:84672ms step_avg:58.03ms
step:1460/2330 train_time:84733ms step_avg:58.04ms
step:1461/2330 train_time:84790ms step_avg:58.04ms
step:1462/2330 train_time:84850ms step_avg:58.04ms
step:1463/2330 train_time:84907ms step_avg:58.04ms
step:1464/2330 train_time:84967ms step_avg:58.04ms
step:1465/2330 train_time:85025ms step_avg:58.04ms
step:1466/2330 train_time:85085ms step_avg:58.04ms
step:1467/2330 train_time:85143ms step_avg:58.04ms
step:1468/2330 train_time:85202ms step_avg:58.04ms
step:1469/2330 train_time:85259ms step_avg:58.04ms
step:1470/2330 train_time:85319ms step_avg:58.04ms
step:1471/2330 train_time:85376ms step_avg:58.04ms
step:1472/2330 train_time:85437ms step_avg:58.04ms
step:1473/2330 train_time:85493ms step_avg:58.04ms
step:1474/2330 train_time:85554ms step_avg:58.04ms
step:1475/2330 train_time:85611ms step_avg:58.04ms
step:1476/2330 train_time:85672ms step_avg:58.04ms
step:1477/2330 train_time:85729ms step_avg:58.04ms
step:1478/2330 train_time:85788ms step_avg:58.04ms
step:1479/2330 train_time:85845ms step_avg:58.04ms
step:1480/2330 train_time:85905ms step_avg:58.04ms
step:1481/2330 train_time:85961ms step_avg:58.04ms
step:1482/2330 train_time:86021ms step_avg:58.04ms
step:1483/2330 train_time:86078ms step_avg:58.04ms
step:1484/2330 train_time:86139ms step_avg:58.05ms
step:1485/2330 train_time:86196ms step_avg:58.04ms
step:1486/2330 train_time:86255ms step_avg:58.05ms
step:1487/2330 train_time:86312ms step_avg:58.04ms
step:1488/2330 train_time:86373ms step_avg:58.05ms
step:1489/2330 train_time:86430ms step_avg:58.05ms
step:1490/2330 train_time:86489ms step_avg:58.05ms
step:1491/2330 train_time:86546ms step_avg:58.05ms
step:1492/2330 train_time:86606ms step_avg:58.05ms
step:1493/2330 train_time:86663ms step_avg:58.05ms
step:1494/2330 train_time:86724ms step_avg:58.05ms
step:1495/2330 train_time:86781ms step_avg:58.05ms
step:1496/2330 train_time:86842ms step_avg:58.05ms
step:1497/2330 train_time:86898ms step_avg:58.05ms
step:1498/2330 train_time:86959ms step_avg:58.05ms
step:1499/2330 train_time:87016ms step_avg:58.05ms
step:1500/2330 train_time:87077ms step_avg:58.05ms
step:1500/2330 val_loss:3.9081 train_time:87157ms step_avg:58.10ms
step:1501/2330 train_time:87177ms step_avg:58.08ms
step:1502/2330 train_time:87196ms step_avg:58.05ms
step:1503/2330 train_time:87258ms step_avg:58.06ms
step:1504/2330 train_time:87321ms step_avg:58.06ms
step:1505/2330 train_time:87379ms step_avg:58.06ms
step:1506/2330 train_time:87439ms step_avg:58.06ms
step:1507/2330 train_time:87495ms step_avg:58.06ms
step:1508/2330 train_time:87555ms step_avg:58.06ms
step:1509/2330 train_time:87612ms step_avg:58.06ms
step:1510/2330 train_time:87672ms step_avg:58.06ms
step:1511/2330 train_time:87727ms step_avg:58.06ms
step:1512/2330 train_time:87787ms step_avg:58.06ms
step:1513/2330 train_time:87843ms step_avg:58.06ms
step:1514/2330 train_time:87903ms step_avg:58.06ms
step:1515/2330 train_time:87959ms step_avg:58.06ms
step:1516/2330 train_time:88018ms step_avg:58.06ms
step:1517/2330 train_time:88075ms step_avg:58.06ms
step:1518/2330 train_time:88136ms step_avg:58.06ms
step:1519/2330 train_time:88196ms step_avg:58.06ms
step:1520/2330 train_time:88259ms step_avg:58.07ms
step:1521/2330 train_time:88316ms step_avg:58.06ms
step:1522/2330 train_time:88378ms step_avg:58.07ms
step:1523/2330 train_time:88435ms step_avg:58.07ms
step:1524/2330 train_time:88497ms step_avg:58.07ms
step:1525/2330 train_time:88555ms step_avg:58.07ms
step:1526/2330 train_time:88614ms step_avg:58.07ms
step:1527/2330 train_time:88671ms step_avg:58.07ms
step:1528/2330 train_time:88730ms step_avg:58.07ms
step:1529/2330 train_time:88787ms step_avg:58.07ms
step:1530/2330 train_time:88846ms step_avg:58.07ms
step:1531/2330 train_time:88902ms step_avg:58.07ms
step:1532/2330 train_time:88963ms step_avg:58.07ms
step:1533/2330 train_time:89019ms step_avg:58.07ms
step:1534/2330 train_time:89080ms step_avg:58.07ms
step:1535/2330 train_time:89138ms step_avg:58.07ms
step:1536/2330 train_time:89200ms step_avg:58.07ms
step:1537/2330 train_time:89258ms step_avg:58.07ms
step:1538/2330 train_time:89320ms step_avg:58.08ms
step:1539/2330 train_time:89378ms step_avg:58.08ms
step:1540/2330 train_time:89438ms step_avg:58.08ms
step:1541/2330 train_time:89496ms step_avg:58.08ms
step:1542/2330 train_time:89557ms step_avg:58.08ms
step:1543/2330 train_time:89615ms step_avg:58.08ms
step:1544/2330 train_time:89675ms step_avg:58.08ms
step:1545/2330 train_time:89732ms step_avg:58.08ms
step:1546/2330 train_time:89792ms step_avg:58.08ms
step:1547/2330 train_time:89849ms step_avg:58.08ms
step:1548/2330 train_time:89909ms step_avg:58.08ms
step:1549/2330 train_time:89966ms step_avg:58.08ms
step:1550/2330 train_time:90027ms step_avg:58.08ms
step:1551/2330 train_time:90083ms step_avg:58.08ms
step:1552/2330 train_time:90146ms step_avg:58.08ms
step:1553/2330 train_time:90202ms step_avg:58.08ms
step:1554/2330 train_time:90266ms step_avg:58.09ms
step:1555/2330 train_time:90322ms step_avg:58.09ms
step:1556/2330 train_time:90385ms step_avg:58.09ms
step:1557/2330 train_time:90442ms step_avg:58.09ms
step:1558/2330 train_time:90505ms step_avg:58.09ms
step:1559/2330 train_time:90562ms step_avg:58.09ms
step:1560/2330 train_time:90624ms step_avg:58.09ms
step:1561/2330 train_time:90680ms step_avg:58.09ms
step:1562/2330 train_time:90743ms step_avg:58.09ms
step:1563/2330 train_time:90800ms step_avg:58.09ms
step:1564/2330 train_time:90861ms step_avg:58.10ms
step:1565/2330 train_time:90919ms step_avg:58.10ms
step:1566/2330 train_time:90978ms step_avg:58.10ms
step:1567/2330 train_time:91035ms step_avg:58.10ms
step:1568/2330 train_time:91097ms step_avg:58.10ms
step:1569/2330 train_time:91156ms step_avg:58.10ms
step:1570/2330 train_time:91215ms step_avg:58.10ms
step:1571/2330 train_time:91273ms step_avg:58.10ms
step:1572/2330 train_time:91334ms step_avg:58.10ms
step:1573/2330 train_time:91392ms step_avg:58.10ms
step:1574/2330 train_time:91452ms step_avg:58.10ms
step:1575/2330 train_time:91509ms step_avg:58.10ms
step:1576/2330 train_time:91571ms step_avg:58.10ms
step:1577/2330 train_time:91628ms step_avg:58.10ms
step:1578/2330 train_time:91690ms step_avg:58.11ms
step:1579/2330 train_time:91747ms step_avg:58.10ms
step:1580/2330 train_time:91809ms step_avg:58.11ms
step:1581/2330 train_time:91865ms step_avg:58.11ms
step:1582/2330 train_time:91926ms step_avg:58.11ms
step:1583/2330 train_time:91983ms step_avg:58.11ms
step:1584/2330 train_time:92045ms step_avg:58.11ms
step:1585/2330 train_time:92102ms step_avg:58.11ms
step:1586/2330 train_time:92163ms step_avg:58.11ms
step:1587/2330 train_time:92219ms step_avg:58.11ms
step:1588/2330 train_time:92282ms step_avg:58.11ms
step:1589/2330 train_time:92339ms step_avg:58.11ms
step:1590/2330 train_time:92403ms step_avg:58.12ms
step:1591/2330 train_time:92461ms step_avg:58.11ms
step:1592/2330 train_time:92521ms step_avg:58.12ms
step:1593/2330 train_time:92578ms step_avg:58.12ms
step:1594/2330 train_time:92641ms step_avg:58.12ms
step:1595/2330 train_time:92699ms step_avg:58.12ms
step:1596/2330 train_time:92759ms step_avg:58.12ms
step:1597/2330 train_time:92816ms step_avg:58.12ms
step:1598/2330 train_time:92876ms step_avg:58.12ms
step:1599/2330 train_time:92933ms step_avg:58.12ms
step:1600/2330 train_time:92994ms step_avg:58.12ms
step:1601/2330 train_time:93051ms step_avg:58.12ms
step:1602/2330 train_time:93110ms step_avg:58.12ms
step:1603/2330 train_time:93167ms step_avg:58.12ms
step:1604/2330 train_time:93228ms step_avg:58.12ms
step:1605/2330 train_time:93284ms step_avg:58.12ms
step:1606/2330 train_time:93346ms step_avg:58.12ms
step:1607/2330 train_time:93403ms step_avg:58.12ms
step:1608/2330 train_time:93465ms step_avg:58.12ms
step:1609/2330 train_time:93521ms step_avg:58.12ms
step:1610/2330 train_time:93586ms step_avg:58.13ms
step:1611/2330 train_time:93642ms step_avg:58.13ms
step:1612/2330 train_time:93705ms step_avg:58.13ms
step:1613/2330 train_time:93762ms step_avg:58.13ms
step:1614/2330 train_time:93823ms step_avg:58.13ms
step:1615/2330 train_time:93880ms step_avg:58.13ms
step:1616/2330 train_time:93941ms step_avg:58.13ms
step:1617/2330 train_time:93999ms step_avg:58.13ms
step:1618/2330 train_time:94059ms step_avg:58.13ms
step:1619/2330 train_time:94117ms step_avg:58.13ms
step:1620/2330 train_time:94178ms step_avg:58.13ms
step:1621/2330 train_time:94236ms step_avg:58.13ms
step:1622/2330 train_time:94297ms step_avg:58.14ms
step:1623/2330 train_time:94354ms step_avg:58.14ms
step:1624/2330 train_time:94415ms step_avg:58.14ms
step:1625/2330 train_time:94473ms step_avg:58.14ms
step:1626/2330 train_time:94534ms step_avg:58.14ms
step:1627/2330 train_time:94592ms step_avg:58.14ms
step:1628/2330 train_time:94652ms step_avg:58.14ms
step:1629/2330 train_time:94709ms step_avg:58.14ms
step:1630/2330 train_time:94770ms step_avg:58.14ms
step:1631/2330 train_time:94826ms step_avg:58.14ms
step:1632/2330 train_time:94889ms step_avg:58.14ms
step:1633/2330 train_time:94945ms step_avg:58.14ms
step:1634/2330 train_time:95007ms step_avg:58.14ms
step:1635/2330 train_time:95064ms step_avg:58.14ms
step:1636/2330 train_time:95125ms step_avg:58.15ms
step:1637/2330 train_time:95182ms step_avg:58.14ms
step:1638/2330 train_time:95244ms step_avg:58.15ms
step:1639/2330 train_time:95301ms step_avg:58.15ms
step:1640/2330 train_time:95363ms step_avg:58.15ms
step:1641/2330 train_time:95420ms step_avg:58.15ms
step:1642/2330 train_time:95484ms step_avg:58.15ms
step:1643/2330 train_time:95541ms step_avg:58.15ms
step:1644/2330 train_time:95604ms step_avg:58.15ms
step:1645/2330 train_time:95661ms step_avg:58.15ms
step:1646/2330 train_time:95722ms step_avg:58.15ms
step:1647/2330 train_time:95780ms step_avg:58.15ms
step:1648/2330 train_time:95841ms step_avg:58.16ms
step:1649/2330 train_time:95899ms step_avg:58.16ms
step:1650/2330 train_time:95959ms step_avg:58.16ms
step:1651/2330 train_time:96016ms step_avg:58.16ms
step:1652/2330 train_time:96078ms step_avg:58.16ms
step:1653/2330 train_time:96135ms step_avg:58.16ms
step:1654/2330 train_time:96196ms step_avg:58.16ms
step:1655/2330 train_time:96254ms step_avg:58.16ms
step:1656/2330 train_time:96314ms step_avg:58.16ms
step:1657/2330 train_time:96372ms step_avg:58.16ms
step:1658/2330 train_time:96433ms step_avg:58.16ms
step:1659/2330 train_time:96490ms step_avg:58.16ms
step:1660/2330 train_time:96551ms step_avg:58.16ms
step:1661/2330 train_time:96608ms step_avg:58.16ms
step:1662/2330 train_time:96669ms step_avg:58.16ms
step:1663/2330 train_time:96725ms step_avg:58.16ms
step:1664/2330 train_time:96788ms step_avg:58.17ms
step:1665/2330 train_time:96845ms step_avg:58.17ms
step:1666/2330 train_time:96907ms step_avg:58.17ms
step:1667/2330 train_time:96964ms step_avg:58.17ms
step:1668/2330 train_time:97025ms step_avg:58.17ms
step:1669/2330 train_time:97081ms step_avg:58.17ms
step:1670/2330 train_time:97143ms step_avg:58.17ms
step:1671/2330 train_time:97200ms step_avg:58.17ms
step:1672/2330 train_time:97262ms step_avg:58.17ms
step:1673/2330 train_time:97319ms step_avg:58.17ms
step:1674/2330 train_time:97381ms step_avg:58.17ms
step:1675/2330 train_time:97438ms step_avg:58.17ms
step:1676/2330 train_time:97499ms step_avg:58.17ms
step:1677/2330 train_time:97558ms step_avg:58.17ms
step:1678/2330 train_time:97618ms step_avg:58.17ms
step:1679/2330 train_time:97675ms step_avg:58.17ms
step:1680/2330 train_time:97737ms step_avg:58.18ms
step:1681/2330 train_time:97795ms step_avg:58.18ms
step:1682/2330 train_time:97855ms step_avg:58.18ms
step:1683/2330 train_time:97912ms step_avg:58.18ms
step:1684/2330 train_time:97971ms step_avg:58.18ms
step:1685/2330 train_time:98028ms step_avg:58.18ms
step:1686/2330 train_time:98089ms step_avg:58.18ms
step:1687/2330 train_time:98145ms step_avg:58.18ms
step:1688/2330 train_time:98207ms step_avg:58.18ms
step:1689/2330 train_time:98263ms step_avg:58.18ms
step:1690/2330 train_time:98326ms step_avg:58.18ms
step:1691/2330 train_time:98383ms step_avg:58.18ms
step:1692/2330 train_time:98445ms step_avg:58.18ms
step:1693/2330 train_time:98502ms step_avg:58.18ms
step:1694/2330 train_time:98565ms step_avg:58.18ms
step:1695/2330 train_time:98622ms step_avg:58.18ms
step:1696/2330 train_time:98684ms step_avg:58.19ms
step:1697/2330 train_time:98741ms step_avg:58.19ms
step:1698/2330 train_time:98803ms step_avg:58.19ms
step:1699/2330 train_time:98861ms step_avg:58.19ms
step:1700/2330 train_time:98922ms step_avg:58.19ms
step:1701/2330 train_time:98979ms step_avg:58.19ms
step:1702/2330 train_time:99040ms step_avg:58.19ms
step:1703/2330 train_time:99097ms step_avg:58.19ms
step:1704/2330 train_time:99158ms step_avg:58.19ms
step:1705/2330 train_time:99216ms step_avg:58.19ms
step:1706/2330 train_time:99277ms step_avg:58.19ms
step:1707/2330 train_time:99334ms step_avg:58.19ms
step:1708/2330 train_time:99396ms step_avg:58.19ms
step:1709/2330 train_time:99454ms step_avg:58.19ms
step:1710/2330 train_time:99515ms step_avg:58.20ms
step:1711/2330 train_time:99572ms step_avg:58.20ms
step:1712/2330 train_time:99632ms step_avg:58.20ms
step:1713/2330 train_time:99689ms step_avg:58.20ms
step:1714/2330 train_time:99751ms step_avg:58.20ms
step:1715/2330 train_time:99807ms step_avg:58.20ms
step:1716/2330 train_time:99870ms step_avg:58.20ms
step:1717/2330 train_time:99926ms step_avg:58.20ms
step:1718/2330 train_time:99988ms step_avg:58.20ms
step:1719/2330 train_time:100044ms step_avg:58.20ms
step:1720/2330 train_time:100106ms step_avg:58.20ms
step:1721/2330 train_time:100163ms step_avg:58.20ms
step:1722/2330 train_time:100224ms step_avg:58.20ms
step:1723/2330 train_time:100281ms step_avg:58.20ms
step:1724/2330 train_time:100343ms step_avg:58.20ms
step:1725/2330 train_time:100400ms step_avg:58.20ms
step:1726/2330 train_time:100462ms step_avg:58.21ms
step:1727/2330 train_time:100519ms step_avg:58.20ms
step:1728/2330 train_time:100581ms step_avg:58.21ms
step:1729/2330 train_time:100638ms step_avg:58.21ms
step:1730/2330 train_time:100702ms step_avg:58.21ms
step:1731/2330 train_time:100760ms step_avg:58.21ms
step:1732/2330 train_time:100821ms step_avg:58.21ms
step:1733/2330 train_time:100879ms step_avg:58.21ms
step:1734/2330 train_time:100940ms step_avg:58.21ms
step:1735/2330 train_time:100998ms step_avg:58.21ms
step:1736/2330 train_time:101058ms step_avg:58.21ms
step:1737/2330 train_time:101115ms step_avg:58.21ms
step:1738/2330 train_time:101176ms step_avg:58.21ms
step:1739/2330 train_time:101233ms step_avg:58.21ms
step:1740/2330 train_time:101293ms step_avg:58.21ms
step:1741/2330 train_time:101350ms step_avg:58.21ms
step:1742/2330 train_time:101411ms step_avg:58.22ms
step:1743/2330 train_time:101468ms step_avg:58.21ms
step:1744/2330 train_time:101529ms step_avg:58.22ms
step:1745/2330 train_time:101586ms step_avg:58.22ms
step:1746/2330 train_time:101648ms step_avg:58.22ms
step:1747/2330 train_time:101706ms step_avg:58.22ms
step:1748/2330 train_time:101767ms step_avg:58.22ms
step:1749/2330 train_time:101824ms step_avg:58.22ms
step:1750/2330 train_time:101886ms step_avg:58.22ms
step:1750/2330 val_loss:3.8226 train_time:101970ms step_avg:58.27ms
step:1751/2330 train_time:101989ms step_avg:58.25ms
step:1752/2330 train_time:102009ms step_avg:58.22ms
step:1753/2330 train_time:102063ms step_avg:58.22ms
step:1754/2330 train_time:102131ms step_avg:58.23ms
step:1755/2330 train_time:102188ms step_avg:58.23ms
step:1756/2330 train_time:102250ms step_avg:58.23ms
step:1757/2330 train_time:102307ms step_avg:58.23ms
step:1758/2330 train_time:102366ms step_avg:58.23ms
step:1759/2330 train_time:102422ms step_avg:58.23ms
step:1760/2330 train_time:102483ms step_avg:58.23ms
step:1761/2330 train_time:102539ms step_avg:58.23ms
step:1762/2330 train_time:102599ms step_avg:58.23ms
step:1763/2330 train_time:102655ms step_avg:58.23ms
step:1764/2330 train_time:102717ms step_avg:58.23ms
step:1765/2330 train_time:102774ms step_avg:58.23ms
step:1766/2330 train_time:102833ms step_avg:58.23ms
step:1767/2330 train_time:102894ms step_avg:58.23ms
step:1768/2330 train_time:102957ms step_avg:58.23ms
step:1769/2330 train_time:103015ms step_avg:58.23ms
step:1770/2330 train_time:103078ms step_avg:58.24ms
step:1771/2330 train_time:103135ms step_avg:58.24ms
step:1772/2330 train_time:103197ms step_avg:58.24ms
step:1773/2330 train_time:103253ms step_avg:58.24ms
step:1774/2330 train_time:103315ms step_avg:58.24ms
step:1775/2330 train_time:103372ms step_avg:58.24ms
step:1776/2330 train_time:103433ms step_avg:58.24ms
step:1777/2330 train_time:103490ms step_avg:58.24ms
step:1778/2330 train_time:103551ms step_avg:58.24ms
step:1779/2330 train_time:103609ms step_avg:58.24ms
step:1780/2330 train_time:103669ms step_avg:58.24ms
step:1781/2330 train_time:103726ms step_avg:58.24ms
step:1782/2330 train_time:103786ms step_avg:58.24ms
step:1783/2330 train_time:103843ms step_avg:58.24ms
step:1784/2330 train_time:103904ms step_avg:58.24ms
step:1785/2330 train_time:103961ms step_avg:58.24ms
step:1786/2330 train_time:104023ms step_avg:58.24ms
step:1787/2330 train_time:104081ms step_avg:58.24ms
step:1788/2330 train_time:104143ms step_avg:58.25ms
step:1789/2330 train_time:104200ms step_avg:58.24ms
step:1790/2330 train_time:104261ms step_avg:58.25ms
step:1791/2330 train_time:104318ms step_avg:58.25ms
step:1792/2330 train_time:104380ms step_avg:58.25ms
step:1793/2330 train_time:104437ms step_avg:58.25ms
step:1794/2330 train_time:104499ms step_avg:58.25ms
step:1795/2330 train_time:104555ms step_avg:58.25ms
step:1796/2330 train_time:104617ms step_avg:58.25ms
step:1797/2330 train_time:104673ms step_avg:58.25ms
step:1798/2330 train_time:104735ms step_avg:58.25ms
step:1799/2330 train_time:104792ms step_avg:58.25ms
step:1800/2330 train_time:104853ms step_avg:58.25ms
step:1801/2330 train_time:104912ms step_avg:58.25ms
step:1802/2330 train_time:104974ms step_avg:58.25ms
step:1803/2330 train_time:105033ms step_avg:58.25ms
step:1804/2330 train_time:105094ms step_avg:58.26ms
step:1805/2330 train_time:105153ms step_avg:58.26ms
step:1806/2330 train_time:105213ms step_avg:58.26ms
step:1807/2330 train_time:105270ms step_avg:58.26ms
step:1808/2330 train_time:105331ms step_avg:58.26ms
step:1809/2330 train_time:105389ms step_avg:58.26ms
step:1810/2330 train_time:105450ms step_avg:58.26ms
step:1811/2330 train_time:105507ms step_avg:58.26ms
step:1812/2330 train_time:105567ms step_avg:58.26ms
step:1813/2330 train_time:105624ms step_avg:58.26ms
step:1814/2330 train_time:105684ms step_avg:58.26ms
step:1815/2330 train_time:105740ms step_avg:58.26ms
step:1816/2330 train_time:105801ms step_avg:58.26ms
step:1817/2330 train_time:105858ms step_avg:58.26ms
step:1818/2330 train_time:105920ms step_avg:58.26ms
step:1819/2330 train_time:105978ms step_avg:58.26ms
step:1820/2330 train_time:106040ms step_avg:58.26ms
step:1821/2330 train_time:106097ms step_avg:58.26ms
step:1822/2330 train_time:106158ms step_avg:58.26ms
step:1823/2330 train_time:106216ms step_avg:58.26ms
step:1824/2330 train_time:106276ms step_avg:58.27ms
step:1825/2330 train_time:106333ms step_avg:58.26ms
step:1826/2330 train_time:106394ms step_avg:58.27ms
step:1827/2330 train_time:106451ms step_avg:58.27ms
step:1828/2330 train_time:106513ms step_avg:58.27ms
step:1829/2330 train_time:106570ms step_avg:58.27ms
step:1830/2330 train_time:106632ms step_avg:58.27ms
step:1831/2330 train_time:106690ms step_avg:58.27ms
step:1832/2330 train_time:106750ms step_avg:58.27ms
step:1833/2330 train_time:106808ms step_avg:58.27ms
step:1834/2330 train_time:106868ms step_avg:58.27ms
step:1835/2330 train_time:106926ms step_avg:58.27ms
step:1836/2330 train_time:106986ms step_avg:58.27ms
step:1837/2330 train_time:107044ms step_avg:58.27ms
step:1838/2330 train_time:107104ms step_avg:58.27ms
step:1839/2330 train_time:107161ms step_avg:58.27ms
step:1840/2330 train_time:107224ms step_avg:58.27ms
step:1841/2330 train_time:107281ms step_avg:58.27ms
step:1842/2330 train_time:107343ms step_avg:58.28ms
step:1843/2330 train_time:107400ms step_avg:58.27ms
step:1844/2330 train_time:107461ms step_avg:58.28ms
step:1845/2330 train_time:107518ms step_avg:58.28ms
step:1846/2330 train_time:107581ms step_avg:58.28ms
step:1847/2330 train_time:107638ms step_avg:58.28ms
step:1848/2330 train_time:107699ms step_avg:58.28ms
step:1849/2330 train_time:107756ms step_avg:58.28ms
step:1850/2330 train_time:107817ms step_avg:58.28ms
step:1851/2330 train_time:107874ms step_avg:58.28ms
step:1852/2330 train_time:107936ms step_avg:58.28ms
step:1853/2330 train_time:107994ms step_avg:58.28ms
step:1854/2330 train_time:108055ms step_avg:58.28ms
step:1855/2330 train_time:108113ms step_avg:58.28ms
step:1856/2330 train_time:108174ms step_avg:58.28ms
step:1857/2330 train_time:108232ms step_avg:58.28ms
step:1858/2330 train_time:108293ms step_avg:58.28ms
step:1859/2330 train_time:108351ms step_avg:58.28ms
step:1860/2330 train_time:108412ms step_avg:58.29ms
step:1861/2330 train_time:108470ms step_avg:58.29ms
step:1862/2330 train_time:108531ms step_avg:58.29ms
step:1863/2330 train_time:108588ms step_avg:58.29ms
step:1864/2330 train_time:108649ms step_avg:58.29ms
step:1865/2330 train_time:108706ms step_avg:58.29ms
step:1866/2330 train_time:108766ms step_avg:58.29ms
step:1867/2330 train_time:108823ms step_avg:58.29ms
step:1868/2330 train_time:108884ms step_avg:58.29ms
step:1869/2330 train_time:108941ms step_avg:58.29ms
step:1870/2330 train_time:109002ms step_avg:58.29ms
step:1871/2330 train_time:109059ms step_avg:58.29ms
step:1872/2330 train_time:109120ms step_avg:58.29ms
step:1873/2330 train_time:109177ms step_avg:58.29ms
step:1874/2330 train_time:109240ms step_avg:58.29ms
step:1875/2330 train_time:109297ms step_avg:58.29ms
step:1876/2330 train_time:109357ms step_avg:58.29ms
step:1877/2330 train_time:109414ms step_avg:58.29ms
step:1878/2330 train_time:109475ms step_avg:58.29ms
step:1879/2330 train_time:109533ms step_avg:58.29ms
step:1880/2330 train_time:109594ms step_avg:58.29ms
step:1881/2330 train_time:109651ms step_avg:58.29ms
step:1882/2330 train_time:109713ms step_avg:58.30ms
step:1883/2330 train_time:109770ms step_avg:58.30ms
step:1884/2330 train_time:109832ms step_avg:58.30ms
step:1885/2330 train_time:109891ms step_avg:58.30ms
step:1886/2330 train_time:109951ms step_avg:58.30ms
step:1887/2330 train_time:110010ms step_avg:58.30ms
step:1888/2330 train_time:110070ms step_avg:58.30ms
step:1889/2330 train_time:110127ms step_avg:58.30ms
step:1890/2330 train_time:110188ms step_avg:58.30ms
step:1891/2330 train_time:110246ms step_avg:58.30ms
step:1892/2330 train_time:110306ms step_avg:58.30ms
step:1893/2330 train_time:110364ms step_avg:58.30ms
step:1894/2330 train_time:110424ms step_avg:58.30ms
step:1895/2330 train_time:110481ms step_avg:58.30ms
step:1896/2330 train_time:110544ms step_avg:58.30ms
step:1897/2330 train_time:110601ms step_avg:58.30ms
step:1898/2330 train_time:110662ms step_avg:58.30ms
step:1899/2330 train_time:110719ms step_avg:58.30ms
step:1900/2330 train_time:110781ms step_avg:58.31ms
step:1901/2330 train_time:110838ms step_avg:58.31ms
step:1902/2330 train_time:110899ms step_avg:58.31ms
step:1903/2330 train_time:110955ms step_avg:58.31ms
step:1904/2330 train_time:111017ms step_avg:58.31ms
step:1905/2330 train_time:111074ms step_avg:58.31ms
step:1906/2330 train_time:111136ms step_avg:58.31ms
step:1907/2330 train_time:111194ms step_avg:58.31ms
step:1908/2330 train_time:111254ms step_avg:58.31ms
step:1909/2330 train_time:111313ms step_avg:58.31ms
step:1910/2330 train_time:111374ms step_avg:58.31ms
step:1911/2330 train_time:111432ms step_avg:58.31ms
step:1912/2330 train_time:111493ms step_avg:58.31ms
step:1913/2330 train_time:111550ms step_avg:58.31ms
step:1914/2330 train_time:111612ms step_avg:58.31ms
step:1915/2330 train_time:111669ms step_avg:58.31ms
step:1916/2330 train_time:111730ms step_avg:58.31ms
step:1917/2330 train_time:111787ms step_avg:58.31ms
step:1918/2330 train_time:111848ms step_avg:58.31ms
step:1919/2330 train_time:111905ms step_avg:58.31ms
step:1920/2330 train_time:111965ms step_avg:58.32ms
step:1921/2330 train_time:112022ms step_avg:58.31ms
step:1922/2330 train_time:112085ms step_avg:58.32ms
step:1923/2330 train_time:112142ms step_avg:58.32ms
step:1924/2330 train_time:112203ms step_avg:58.32ms
step:1925/2330 train_time:112260ms step_avg:58.32ms
step:1926/2330 train_time:112321ms step_avg:58.32ms
step:1927/2330 train_time:112379ms step_avg:58.32ms
step:1928/2330 train_time:112441ms step_avg:58.32ms
step:1929/2330 train_time:112497ms step_avg:58.32ms
step:1930/2330 train_time:112560ms step_avg:58.32ms
step:1931/2330 train_time:112617ms step_avg:58.32ms
step:1932/2330 train_time:112679ms step_avg:58.32ms
step:1933/2330 train_time:112736ms step_avg:58.32ms
step:1934/2330 train_time:112797ms step_avg:58.32ms
step:1935/2330 train_time:112855ms step_avg:58.32ms
step:1936/2330 train_time:112916ms step_avg:58.32ms
step:1937/2330 train_time:112973ms step_avg:58.32ms
step:1938/2330 train_time:113033ms step_avg:58.32ms
step:1939/2330 train_time:113091ms step_avg:58.32ms
step:1940/2330 train_time:113152ms step_avg:58.33ms
step:1941/2330 train_time:113210ms step_avg:58.33ms
step:1942/2330 train_time:113271ms step_avg:58.33ms
step:1943/2330 train_time:113329ms step_avg:58.33ms
step:1944/2330 train_time:113390ms step_avg:58.33ms
step:1945/2330 train_time:113448ms step_avg:58.33ms
step:1946/2330 train_time:113508ms step_avg:58.33ms
step:1947/2330 train_time:113567ms step_avg:58.33ms
step:1948/2330 train_time:113627ms step_avg:58.33ms
step:1949/2330 train_time:113684ms step_avg:58.33ms
step:1950/2330 train_time:113744ms step_avg:58.33ms
step:1951/2330 train_time:113801ms step_avg:58.33ms
step:1952/2330 train_time:113861ms step_avg:58.33ms
step:1953/2330 train_time:113918ms step_avg:58.33ms
step:1954/2330 train_time:113979ms step_avg:58.33ms
step:1955/2330 train_time:114035ms step_avg:58.33ms
step:1956/2330 train_time:114097ms step_avg:58.33ms
step:1957/2330 train_time:114154ms step_avg:58.33ms
step:1958/2330 train_time:114215ms step_avg:58.33ms
step:1959/2330 train_time:114273ms step_avg:58.33ms
step:1960/2330 train_time:114334ms step_avg:58.33ms
step:1961/2330 train_time:114391ms step_avg:58.33ms
step:1962/2330 train_time:114453ms step_avg:58.33ms
step:1963/2330 train_time:114511ms step_avg:58.33ms
step:1964/2330 train_time:114573ms step_avg:58.34ms
step:1965/2330 train_time:114630ms step_avg:58.34ms
step:1966/2330 train_time:114691ms step_avg:58.34ms
step:1967/2330 train_time:114749ms step_avg:58.34ms
step:1968/2330 train_time:114810ms step_avg:58.34ms
step:1969/2330 train_time:114868ms step_avg:58.34ms
step:1970/2330 train_time:114928ms step_avg:58.34ms
step:1971/2330 train_time:114985ms step_avg:58.34ms
step:1972/2330 train_time:115046ms step_avg:58.34ms
step:1973/2330 train_time:115103ms step_avg:58.34ms
step:1974/2330 train_time:115164ms step_avg:58.34ms
step:1975/2330 train_time:115221ms step_avg:58.34ms
step:1976/2330 train_time:115282ms step_avg:58.34ms
step:1977/2330 train_time:115339ms step_avg:58.34ms
step:1978/2330 train_time:115401ms step_avg:58.34ms
step:1979/2330 train_time:115457ms step_avg:58.34ms
step:1980/2330 train_time:115520ms step_avg:58.34ms
step:1981/2330 train_time:115577ms step_avg:58.34ms
step:1982/2330 train_time:115639ms step_avg:58.34ms
step:1983/2330 train_time:115695ms step_avg:58.34ms
step:1984/2330 train_time:115757ms step_avg:58.35ms
step:1985/2330 train_time:115815ms step_avg:58.34ms
step:1986/2330 train_time:115876ms step_avg:58.35ms
step:1987/2330 train_time:115933ms step_avg:58.35ms
step:1988/2330 train_time:115995ms step_avg:58.35ms
step:1989/2330 train_time:116053ms step_avg:58.35ms
step:1990/2330 train_time:116113ms step_avg:58.35ms
step:1991/2330 train_time:116170ms step_avg:58.35ms
step:1992/2330 train_time:116231ms step_avg:58.35ms
step:1993/2330 train_time:116290ms step_avg:58.35ms
step:1994/2330 train_time:116350ms step_avg:58.35ms
step:1995/2330 train_time:116408ms step_avg:58.35ms
step:1996/2330 train_time:116468ms step_avg:58.35ms
step:1997/2330 train_time:116526ms step_avg:58.35ms
step:1998/2330 train_time:116586ms step_avg:58.35ms
step:1999/2330 train_time:116644ms step_avg:58.35ms
step:2000/2330 train_time:116704ms step_avg:58.35ms
step:2000/2330 val_loss:3.7612 train_time:116786ms step_avg:58.39ms
step:2001/2330 train_time:116805ms step_avg:58.37ms
step:2002/2330 train_time:116825ms step_avg:58.35ms
step:2003/2330 train_time:116885ms step_avg:58.35ms
step:2004/2330 train_time:116951ms step_avg:58.36ms
step:2005/2330 train_time:117009ms step_avg:58.36ms
step:2006/2330 train_time:117072ms step_avg:58.36ms
step:2007/2330 train_time:117129ms step_avg:58.36ms
step:2008/2330 train_time:117189ms step_avg:58.36ms
step:2009/2330 train_time:117247ms step_avg:58.36ms
step:2010/2330 train_time:117306ms step_avg:58.36ms
step:2011/2330 train_time:117363ms step_avg:58.36ms
step:2012/2330 train_time:117422ms step_avg:58.36ms
step:2013/2330 train_time:117479ms step_avg:58.36ms
step:2014/2330 train_time:117539ms step_avg:58.36ms
step:2015/2330 train_time:117595ms step_avg:58.36ms
step:2016/2330 train_time:117656ms step_avg:58.36ms
step:2017/2330 train_time:117713ms step_avg:58.36ms
step:2018/2330 train_time:117773ms step_avg:58.36ms
step:2019/2330 train_time:117831ms step_avg:58.36ms
step:2020/2330 train_time:117895ms step_avg:58.36ms
step:2021/2330 train_time:117954ms step_avg:58.36ms
step:2022/2330 train_time:118017ms step_avg:58.37ms
step:2023/2330 train_time:118074ms step_avg:58.37ms
step:2024/2330 train_time:118136ms step_avg:58.37ms
step:2025/2330 train_time:118192ms step_avg:58.37ms
step:2026/2330 train_time:118254ms step_avg:58.37ms
step:2027/2330 train_time:118310ms step_avg:58.37ms
step:2028/2330 train_time:118371ms step_avg:58.37ms
step:2029/2330 train_time:118428ms step_avg:58.37ms
step:2030/2330 train_time:118489ms step_avg:58.37ms
step:2031/2330 train_time:118545ms step_avg:58.37ms
step:2032/2330 train_time:118607ms step_avg:58.37ms
step:2033/2330 train_time:118664ms step_avg:58.37ms
step:2034/2330 train_time:118725ms step_avg:58.37ms
step:2035/2330 train_time:118782ms step_avg:58.37ms
step:2036/2330 train_time:118846ms step_avg:58.37ms
step:2037/2330 train_time:118906ms step_avg:58.37ms
step:2038/2330 train_time:118966ms step_avg:58.37ms
step:2039/2330 train_time:119025ms step_avg:58.37ms
step:2040/2330 train_time:119086ms step_avg:58.38ms
step:2041/2330 train_time:119143ms step_avg:58.37ms
step:2042/2330 train_time:119204ms step_avg:58.38ms
step:2043/2330 train_time:119261ms step_avg:58.38ms
step:2044/2330 train_time:119321ms step_avg:58.38ms
step:2045/2330 train_time:119378ms step_avg:58.38ms
step:2046/2330 train_time:119439ms step_avg:58.38ms
step:2047/2330 train_time:119496ms step_avg:58.38ms
step:2048/2330 train_time:119557ms step_avg:58.38ms
step:2049/2330 train_time:119613ms step_avg:58.38ms
step:2050/2330 train_time:119674ms step_avg:58.38ms
step:2051/2330 train_time:119730ms step_avg:58.38ms
step:2052/2330 train_time:119793ms step_avg:58.38ms
step:2053/2330 train_time:119850ms step_avg:58.38ms
step:2054/2330 train_time:119914ms step_avg:58.38ms
step:2055/2330 train_time:119971ms step_avg:58.38ms
step:2056/2330 train_time:120033ms step_avg:58.38ms
step:2057/2330 train_time:120090ms step_avg:58.38ms
step:2058/2330 train_time:120153ms step_avg:58.38ms
step:2059/2330 train_time:120209ms step_avg:58.38ms
step:2060/2330 train_time:120271ms step_avg:58.38ms
step:2061/2330 train_time:120328ms step_avg:58.38ms
step:2062/2330 train_time:120390ms step_avg:58.38ms
step:2063/2330 train_time:120447ms step_avg:58.38ms
step:2064/2330 train_time:120508ms step_avg:58.39ms
step:2065/2330 train_time:120565ms step_avg:58.39ms
step:2066/2330 train_time:120625ms step_avg:58.39ms
step:2067/2330 train_time:120683ms step_avg:58.39ms
step:2068/2330 train_time:120744ms step_avg:58.39ms
step:2069/2330 train_time:120802ms step_avg:58.39ms
step:2070/2330 train_time:120864ms step_avg:58.39ms
step:2071/2330 train_time:120922ms step_avg:58.39ms
step:2072/2330 train_time:120983ms step_avg:58.39ms
step:2073/2330 train_time:121042ms step_avg:58.39ms
step:2074/2330 train_time:121102ms step_avg:58.39ms
step:2075/2330 train_time:121160ms step_avg:58.39ms
step:2076/2330 train_time:121219ms step_avg:58.39ms
step:2077/2330 train_time:121276ms step_avg:58.39ms
step:2078/2330 train_time:121338ms step_avg:58.39ms
step:2079/2330 train_time:121394ms step_avg:58.39ms
step:2080/2330 train_time:121458ms step_avg:58.39ms
step:2081/2330 train_time:121514ms step_avg:58.39ms
step:2082/2330 train_time:121577ms step_avg:58.39ms
step:2083/2330 train_time:121633ms step_avg:58.39ms
step:2084/2330 train_time:121694ms step_avg:58.39ms
step:2085/2330 train_time:121751ms step_avg:58.39ms
step:2086/2330 train_time:121812ms step_avg:58.40ms
step:2087/2330 train_time:121869ms step_avg:58.39ms
step:2088/2330 train_time:121931ms step_avg:58.40ms
step:2089/2330 train_time:121988ms step_avg:58.40ms
step:2090/2330 train_time:122051ms step_avg:58.40ms
step:2091/2330 train_time:122108ms step_avg:58.40ms
step:2092/2330 train_time:122169ms step_avg:58.40ms
step:2093/2330 train_time:122226ms step_avg:58.40ms
step:2094/2330 train_time:122289ms step_avg:58.40ms
step:2095/2330 train_time:122346ms step_avg:58.40ms
step:2096/2330 train_time:122408ms step_avg:58.40ms
step:2097/2330 train_time:122466ms step_avg:58.40ms
step:2098/2330 train_time:122526ms step_avg:58.40ms
step:2099/2330 train_time:122583ms step_avg:58.40ms
step:2100/2330 train_time:122644ms step_avg:58.40ms
step:2101/2330 train_time:122701ms step_avg:58.40ms
step:2102/2330 train_time:122762ms step_avg:58.40ms
step:2103/2330 train_time:122819ms step_avg:58.40ms
step:2104/2330 train_time:122879ms step_avg:58.40ms
step:2105/2330 train_time:122937ms step_avg:58.40ms
step:2106/2330 train_time:122997ms step_avg:58.40ms
step:2107/2330 train_time:123054ms step_avg:58.40ms
step:2108/2330 train_time:123115ms step_avg:58.40ms
step:2109/2330 train_time:123172ms step_avg:58.40ms
step:2110/2330 train_time:123235ms step_avg:58.41ms
step:2111/2330 train_time:123291ms step_avg:58.40ms
step:2112/2330 train_time:123354ms step_avg:58.41ms
step:2113/2330 train_time:123411ms step_avg:58.41ms
step:2114/2330 train_time:123472ms step_avg:58.41ms
step:2115/2330 train_time:123530ms step_avg:58.41ms
step:2116/2330 train_time:123590ms step_avg:58.41ms
step:2117/2330 train_time:123647ms step_avg:58.41ms
step:2118/2330 train_time:123708ms step_avg:58.41ms
step:2119/2330 train_time:123764ms step_avg:58.41ms
step:2120/2330 train_time:123826ms step_avg:58.41ms
step:2121/2330 train_time:123883ms step_avg:58.41ms
step:2122/2330 train_time:123945ms step_avg:58.41ms
step:2123/2330 train_time:124003ms step_avg:58.41ms
step:2124/2330 train_time:124064ms step_avg:58.41ms
step:2125/2330 train_time:124122ms step_avg:58.41ms
step:2126/2330 train_time:124182ms step_avg:58.41ms
step:2127/2330 train_time:124239ms step_avg:58.41ms
step:2128/2330 train_time:124301ms step_avg:58.41ms
step:2129/2330 train_time:124358ms step_avg:58.41ms
step:2130/2330 train_time:124419ms step_avg:58.41ms
step:2131/2330 train_time:124475ms step_avg:58.41ms
step:2132/2330 train_time:124538ms step_avg:58.41ms
step:2133/2330 train_time:124594ms step_avg:58.41ms
step:2134/2330 train_time:124656ms step_avg:58.41ms
step:2135/2330 train_time:124713ms step_avg:58.41ms
step:2136/2330 train_time:124774ms step_avg:58.41ms
step:2137/2330 train_time:124831ms step_avg:58.41ms
step:2138/2330 train_time:124894ms step_avg:58.42ms
step:2139/2330 train_time:124950ms step_avg:58.42ms
step:2140/2330 train_time:125012ms step_avg:58.42ms
step:2141/2330 train_time:125068ms step_avg:58.42ms
step:2142/2330 train_time:125130ms step_avg:58.42ms
step:2143/2330 train_time:125188ms step_avg:58.42ms
step:2144/2330 train_time:125251ms step_avg:58.42ms
step:2145/2330 train_time:125309ms step_avg:58.42ms
step:2146/2330 train_time:125369ms step_avg:58.42ms
step:2147/2330 train_time:125426ms step_avg:58.42ms
step:2148/2330 train_time:125487ms step_avg:58.42ms
step:2149/2330 train_time:125545ms step_avg:58.42ms
step:2150/2330 train_time:125606ms step_avg:58.42ms
step:2151/2330 train_time:125663ms step_avg:58.42ms
step:2152/2330 train_time:125724ms step_avg:58.42ms
step:2153/2330 train_time:125782ms step_avg:58.42ms
step:2154/2330 train_time:125843ms step_avg:58.42ms
step:2155/2330 train_time:125900ms step_avg:58.42ms
step:2156/2330 train_time:125960ms step_avg:58.42ms
step:2157/2330 train_time:126017ms step_avg:58.42ms
step:2158/2330 train_time:126077ms step_avg:58.42ms
step:2159/2330 train_time:126134ms step_avg:58.42ms
step:2160/2330 train_time:126197ms step_avg:58.42ms
step:2161/2330 train_time:126254ms step_avg:58.42ms
step:2162/2330 train_time:126315ms step_avg:58.42ms
step:2163/2330 train_time:126371ms step_avg:58.42ms
step:2164/2330 train_time:126433ms step_avg:58.43ms
step:2165/2330 train_time:126489ms step_avg:58.42ms
step:2166/2330 train_time:126551ms step_avg:58.43ms
step:2167/2330 train_time:126608ms step_avg:58.43ms
step:2168/2330 train_time:126669ms step_avg:58.43ms
step:2169/2330 train_time:126726ms step_avg:58.43ms
step:2170/2330 train_time:126788ms step_avg:58.43ms
step:2171/2330 train_time:126846ms step_avg:58.43ms
step:2172/2330 train_time:126907ms step_avg:58.43ms
step:2173/2330 train_time:126965ms step_avg:58.43ms
step:2174/2330 train_time:127027ms step_avg:58.43ms
step:2175/2330 train_time:127085ms step_avg:58.43ms
step:2176/2330 train_time:127147ms step_avg:58.43ms
step:2177/2330 train_time:127206ms step_avg:58.43ms
step:2178/2330 train_time:127265ms step_avg:58.43ms
step:2179/2330 train_time:127323ms step_avg:58.43ms
step:2180/2330 train_time:127385ms step_avg:58.43ms
step:2181/2330 train_time:127443ms step_avg:58.43ms
step:2182/2330 train_time:127504ms step_avg:58.43ms
step:2183/2330 train_time:127561ms step_avg:58.43ms
step:2184/2330 train_time:127621ms step_avg:58.43ms
step:2185/2330 train_time:127678ms step_avg:58.43ms
step:2186/2330 train_time:127739ms step_avg:58.43ms
step:2187/2330 train_time:127795ms step_avg:58.43ms
step:2188/2330 train_time:127857ms step_avg:58.44ms
step:2189/2330 train_time:127914ms step_avg:58.43ms
step:2190/2330 train_time:127975ms step_avg:58.44ms
step:2191/2330 train_time:128032ms step_avg:58.44ms
step:2192/2330 train_time:128094ms step_avg:58.44ms
step:2193/2330 train_time:128151ms step_avg:58.44ms
step:2194/2330 train_time:128212ms step_avg:58.44ms
step:2195/2330 train_time:128269ms step_avg:58.44ms
step:2196/2330 train_time:128331ms step_avg:58.44ms
step:2197/2330 train_time:128387ms step_avg:58.44ms
step:2198/2330 train_time:128450ms step_avg:58.44ms
step:2199/2330 train_time:128508ms step_avg:58.44ms
step:2200/2330 train_time:128570ms step_avg:58.44ms
step:2201/2330 train_time:128627ms step_avg:58.44ms
step:2202/2330 train_time:128689ms step_avg:58.44ms
step:2203/2330 train_time:128746ms step_avg:58.44ms
step:2204/2330 train_time:128807ms step_avg:58.44ms
step:2205/2330 train_time:128865ms step_avg:58.44ms
step:2206/2330 train_time:128926ms step_avg:58.44ms
step:2207/2330 train_time:128984ms step_avg:58.44ms
step:2208/2330 train_time:129046ms step_avg:58.44ms
step:2209/2330 train_time:129104ms step_avg:58.44ms
step:2210/2330 train_time:129165ms step_avg:58.45ms
step:2211/2330 train_time:129222ms step_avg:58.45ms
step:2212/2330 train_time:129283ms step_avg:58.45ms
step:2213/2330 train_time:129340ms step_avg:58.45ms
step:2214/2330 train_time:129401ms step_avg:58.45ms
step:2215/2330 train_time:129458ms step_avg:58.45ms
step:2216/2330 train_time:129520ms step_avg:58.45ms
step:2217/2330 train_time:129577ms step_avg:58.45ms
step:2218/2330 train_time:129637ms step_avg:58.45ms
step:2219/2330 train_time:129694ms step_avg:58.45ms
step:2220/2330 train_time:129756ms step_avg:58.45ms
step:2221/2330 train_time:129812ms step_avg:58.45ms
step:2222/2330 train_time:129873ms step_avg:58.45ms
step:2223/2330 train_time:129929ms step_avg:58.45ms
step:2224/2330 train_time:129992ms step_avg:58.45ms
step:2225/2330 train_time:130049ms step_avg:58.45ms
step:2226/2330 train_time:130111ms step_avg:58.45ms
step:2227/2330 train_time:130168ms step_avg:58.45ms
step:2228/2330 train_time:130230ms step_avg:58.45ms
step:2229/2330 train_time:130288ms step_avg:58.45ms
step:2230/2330 train_time:130348ms step_avg:58.45ms
step:2231/2330 train_time:130407ms step_avg:58.45ms
step:2232/2330 train_time:130468ms step_avg:58.45ms
step:2233/2330 train_time:130526ms step_avg:58.45ms
step:2234/2330 train_time:130586ms step_avg:58.45ms
step:2235/2330 train_time:130644ms step_avg:58.45ms
step:2236/2330 train_time:130705ms step_avg:58.45ms
step:2237/2330 train_time:130763ms step_avg:58.45ms
step:2238/2330 train_time:130823ms step_avg:58.46ms
step:2239/2330 train_time:130881ms step_avg:58.45ms
step:2240/2330 train_time:130942ms step_avg:58.46ms
step:2241/2330 train_time:131000ms step_avg:58.46ms
step:2242/2330 train_time:131060ms step_avg:58.46ms
step:2243/2330 train_time:131116ms step_avg:58.46ms
step:2244/2330 train_time:131177ms step_avg:58.46ms
step:2245/2330 train_time:131233ms step_avg:58.46ms
step:2246/2330 train_time:131297ms step_avg:58.46ms
step:2247/2330 train_time:131354ms step_avg:58.46ms
step:2248/2330 train_time:131417ms step_avg:58.46ms
step:2249/2330 train_time:131473ms step_avg:58.46ms
step:2250/2330 train_time:131535ms step_avg:58.46ms
step:2250/2330 val_loss:3.7136 train_time:131617ms step_avg:58.50ms
step:2251/2330 train_time:131638ms step_avg:58.48ms
step:2252/2330 train_time:131658ms step_avg:58.46ms
step:2253/2330 train_time:131718ms step_avg:58.46ms
step:2254/2330 train_time:131781ms step_avg:58.47ms
step:2255/2330 train_time:131839ms step_avg:58.47ms
step:2256/2330 train_time:131899ms step_avg:58.47ms
step:2257/2330 train_time:131956ms step_avg:58.47ms
step:2258/2330 train_time:132018ms step_avg:58.47ms
step:2259/2330 train_time:132074ms step_avg:58.47ms
step:2260/2330 train_time:132134ms step_avg:58.47ms
step:2261/2330 train_time:132191ms step_avg:58.47ms
step:2262/2330 train_time:132251ms step_avg:58.47ms
step:2263/2330 train_time:132307ms step_avg:58.47ms
step:2264/2330 train_time:132369ms step_avg:58.47ms
step:2265/2330 train_time:132425ms step_avg:58.47ms
step:2266/2330 train_time:132484ms step_avg:58.47ms
step:2267/2330 train_time:132541ms step_avg:58.47ms
step:2268/2330 train_time:132603ms step_avg:58.47ms
step:2269/2330 train_time:132663ms step_avg:58.47ms
step:2270/2330 train_time:132725ms step_avg:58.47ms
step:2271/2330 train_time:132785ms step_avg:58.47ms
step:2272/2330 train_time:132847ms step_avg:58.47ms
step:2273/2330 train_time:132905ms step_avg:58.47ms
step:2274/2330 train_time:132966ms step_avg:58.47ms
step:2275/2330 train_time:133024ms step_avg:58.47ms
step:2276/2330 train_time:133083ms step_avg:58.47ms
step:2277/2330 train_time:133140ms step_avg:58.47ms
step:2278/2330 train_time:133200ms step_avg:58.47ms
step:2279/2330 train_time:133257ms step_avg:58.47ms
step:2280/2330 train_time:133317ms step_avg:58.47ms
step:2281/2330 train_time:133373ms step_avg:58.47ms
step:2282/2330 train_time:133433ms step_avg:58.47ms
step:2283/2330 train_time:133489ms step_avg:58.47ms
step:2284/2330 train_time:133550ms step_avg:58.47ms
step:2285/2330 train_time:133608ms step_avg:58.47ms
step:2286/2330 train_time:133671ms step_avg:58.47ms
step:2287/2330 train_time:133728ms step_avg:58.47ms
step:2288/2330 train_time:133791ms step_avg:58.48ms
step:2289/2330 train_time:133848ms step_avg:58.47ms
step:2290/2330 train_time:133911ms step_avg:58.48ms
step:2291/2330 train_time:133969ms step_avg:58.48ms
step:2292/2330 train_time:134031ms step_avg:58.48ms
step:2293/2330 train_time:134087ms step_avg:58.48ms
step:2294/2330 train_time:134149ms step_avg:58.48ms
step:2295/2330 train_time:134206ms step_avg:58.48ms
step:2296/2330 train_time:134267ms step_avg:58.48ms
step:2297/2330 train_time:134325ms step_avg:58.48ms
step:2298/2330 train_time:134384ms step_avg:58.48ms
step:2299/2330 train_time:134442ms step_avg:58.48ms
step:2300/2330 train_time:134502ms step_avg:58.48ms
step:2301/2330 train_time:134559ms step_avg:58.48ms
step:2302/2330 train_time:134619ms step_avg:58.48ms
step:2303/2330 train_time:134677ms step_avg:58.48ms
step:2304/2330 train_time:134738ms step_avg:58.48ms
step:2305/2330 train_time:134795ms step_avg:58.48ms
step:2306/2330 train_time:134858ms step_avg:58.48ms
step:2307/2330 train_time:134915ms step_avg:58.48ms
step:2308/2330 train_time:134977ms step_avg:58.48ms
step:2309/2330 train_time:135034ms step_avg:58.48ms
step:2310/2330 train_time:135094ms step_avg:58.48ms
step:2311/2330 train_time:135151ms step_avg:58.48ms
step:2312/2330 train_time:135213ms step_avg:58.48ms
step:2313/2330 train_time:135269ms step_avg:58.48ms
step:2314/2330 train_time:135331ms step_avg:58.48ms
step:2315/2330 train_time:135387ms step_avg:58.48ms
step:2316/2330 train_time:135448ms step_avg:58.48ms
step:2317/2330 train_time:135505ms step_avg:58.48ms
step:2318/2330 train_time:135566ms step_avg:58.48ms
step:2319/2330 train_time:135625ms step_avg:58.48ms
step:2320/2330 train_time:135686ms step_avg:58.49ms
step:2321/2330 train_time:135745ms step_avg:58.49ms
step:2322/2330 train_time:135805ms step_avg:58.49ms
step:2323/2330 train_time:135865ms step_avg:58.49ms
step:2324/2330 train_time:135925ms step_avg:58.49ms
step:2325/2330 train_time:135982ms step_avg:58.49ms
step:2326/2330 train_time:136043ms step_avg:58.49ms
step:2327/2330 train_time:136100ms step_avg:58.49ms
step:2328/2330 train_time:136161ms step_avg:58.49ms
step:2329/2330 train_time:136219ms step_avg:58.49ms
step:2330/2330 train_time:136278ms step_avg:58.49ms
step:2330/2330 val_loss:3.6983 train_time:136359ms step_avg:58.52ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
