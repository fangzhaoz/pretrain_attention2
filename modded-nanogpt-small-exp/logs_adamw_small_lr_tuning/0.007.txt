import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--adamw_lr', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_lr_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_lr_tuning/{args2.adamw_lr}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=float(args2.adamw_lr),  betas=(0.85, 0.85), eps=1e-10, weight_decay=0.0, fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_lr_tuning/{args2.adamw_lr}", exist_ok=True)
            torch.save(log, f"logs_adamw_small_lr_tuning/{args2.adamw_lr}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 02:02:12 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   36C    P0             118W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   35C    P0             117W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             118W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             115W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   34C    P0             118W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             114W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:80ms step_avg:79.80ms
step:2/2330 train_time:194ms step_avg:97.02ms
step:3/2330 train_time:213ms step_avg:70.86ms
step:4/2330 train_time:232ms step_avg:58.05ms
step:5/2330 train_time:285ms step_avg:57.06ms
step:6/2330 train_time:343ms step_avg:57.14ms
step:7/2330 train_time:397ms step_avg:56.70ms
step:8/2330 train_time:454ms step_avg:56.79ms
step:9/2330 train_time:509ms step_avg:56.53ms
step:10/2330 train_time:566ms step_avg:56.59ms
step:11/2330 train_time:621ms step_avg:56.41ms
step:12/2330 train_time:678ms step_avg:56.49ms
step:13/2330 train_time:732ms step_avg:56.35ms
step:14/2330 train_time:790ms step_avg:56.41ms
step:15/2330 train_time:844ms step_avg:56.25ms
step:16/2330 train_time:902ms step_avg:56.39ms
step:17/2330 train_time:957ms step_avg:56.31ms
step:18/2330 train_time:1014ms step_avg:56.35ms
step:19/2330 train_time:1070ms step_avg:56.30ms
step:20/2330 train_time:1129ms step_avg:56.45ms
step:21/2330 train_time:1186ms step_avg:56.46ms
step:22/2330 train_time:1249ms step_avg:56.77ms
step:23/2330 train_time:1304ms step_avg:56.70ms
step:24/2330 train_time:1365ms step_avg:56.87ms
step:25/2330 train_time:1421ms step_avg:56.83ms
step:26/2330 train_time:1478ms step_avg:56.86ms
step:27/2330 train_time:1533ms step_avg:56.79ms
step:28/2330 train_time:1591ms step_avg:56.82ms
step:29/2330 train_time:1645ms step_avg:56.73ms
step:30/2330 train_time:1703ms step_avg:56.78ms
step:31/2330 train_time:1759ms step_avg:56.73ms
step:32/2330 train_time:1816ms step_avg:56.76ms
step:33/2330 train_time:1871ms step_avg:56.69ms
step:34/2330 train_time:1928ms step_avg:56.72ms
step:35/2330 train_time:1983ms step_avg:56.66ms
step:36/2330 train_time:2041ms step_avg:56.71ms
step:37/2330 train_time:2097ms step_avg:56.69ms
step:38/2330 train_time:2156ms step_avg:56.74ms
step:39/2330 train_time:2212ms step_avg:56.72ms
step:40/2330 train_time:2271ms step_avg:56.77ms
step:41/2330 train_time:2326ms step_avg:56.74ms
step:42/2330 train_time:2386ms step_avg:56.80ms
step:43/2330 train_time:2442ms step_avg:56.78ms
step:44/2330 train_time:2500ms step_avg:56.81ms
step:45/2330 train_time:2555ms step_avg:56.78ms
step:46/2330 train_time:2613ms step_avg:56.80ms
step:47/2330 train_time:2668ms step_avg:56.77ms
step:48/2330 train_time:2726ms step_avg:56.79ms
step:49/2330 train_time:2782ms step_avg:56.77ms
step:50/2330 train_time:2839ms step_avg:56.79ms
step:51/2330 train_time:2894ms step_avg:56.75ms
step:52/2330 train_time:2951ms step_avg:56.76ms
step:53/2330 train_time:3007ms step_avg:56.73ms
step:54/2330 train_time:3065ms step_avg:56.76ms
step:55/2330 train_time:3121ms step_avg:56.75ms
step:56/2330 train_time:3179ms step_avg:56.77ms
step:57/2330 train_time:3235ms step_avg:56.75ms
step:58/2330 train_time:3293ms step_avg:56.78ms
step:59/2330 train_time:3348ms step_avg:56.75ms
step:60/2330 train_time:3408ms step_avg:56.80ms
step:61/2330 train_time:3464ms step_avg:56.78ms
step:62/2330 train_time:3522ms step_avg:56.81ms
step:63/2330 train_time:3578ms step_avg:56.79ms
step:64/2330 train_time:3635ms step_avg:56.80ms
step:65/2330 train_time:3690ms step_avg:56.78ms
step:66/2330 train_time:3749ms step_avg:56.80ms
step:67/2330 train_time:3804ms step_avg:56.78ms
step:68/2330 train_time:3862ms step_avg:56.80ms
step:69/2330 train_time:3918ms step_avg:56.78ms
step:70/2330 train_time:3975ms step_avg:56.79ms
step:71/2330 train_time:4030ms step_avg:56.76ms
step:72/2330 train_time:4088ms step_avg:56.78ms
step:73/2330 train_time:4143ms step_avg:56.76ms
step:74/2330 train_time:4202ms step_avg:56.78ms
step:75/2330 train_time:4258ms step_avg:56.78ms
step:76/2330 train_time:4316ms step_avg:56.80ms
step:77/2330 train_time:4372ms step_avg:56.78ms
step:78/2330 train_time:4430ms step_avg:56.79ms
step:79/2330 train_time:4485ms step_avg:56.78ms
step:80/2330 train_time:4544ms step_avg:56.81ms
step:81/2330 train_time:4600ms step_avg:56.79ms
step:82/2330 train_time:4658ms step_avg:56.81ms
step:83/2330 train_time:4714ms step_avg:56.79ms
step:84/2330 train_time:4772ms step_avg:56.81ms
step:85/2330 train_time:4828ms step_avg:56.80ms
step:86/2330 train_time:4887ms step_avg:56.82ms
step:87/2330 train_time:4943ms step_avg:56.81ms
step:88/2330 train_time:5001ms step_avg:56.83ms
step:89/2330 train_time:5056ms step_avg:56.80ms
step:90/2330 train_time:5113ms step_avg:56.82ms
step:91/2330 train_time:5168ms step_avg:56.80ms
step:92/2330 train_time:5228ms step_avg:56.83ms
step:93/2330 train_time:5283ms step_avg:56.81ms
step:94/2330 train_time:5342ms step_avg:56.83ms
step:95/2330 train_time:5398ms step_avg:56.82ms
step:96/2330 train_time:5456ms step_avg:56.84ms
step:97/2330 train_time:5511ms step_avg:56.82ms
step:98/2330 train_time:5570ms step_avg:56.84ms
step:99/2330 train_time:5624ms step_avg:56.81ms
step:100/2330 train_time:5684ms step_avg:56.84ms
step:101/2330 train_time:5741ms step_avg:56.84ms
step:102/2330 train_time:5799ms step_avg:56.85ms
step:103/2330 train_time:5854ms step_avg:56.84ms
step:104/2330 train_time:5912ms step_avg:56.84ms
step:105/2330 train_time:5967ms step_avg:56.83ms
step:106/2330 train_time:6025ms step_avg:56.84ms
step:107/2330 train_time:6081ms step_avg:56.84ms
step:108/2330 train_time:6140ms step_avg:56.85ms
step:109/2330 train_time:6195ms step_avg:56.84ms
step:110/2330 train_time:6253ms step_avg:56.84ms
step:111/2330 train_time:6308ms step_avg:56.83ms
step:112/2330 train_time:6367ms step_avg:56.85ms
step:113/2330 train_time:6422ms step_avg:56.84ms
step:114/2330 train_time:6483ms step_avg:56.87ms
step:115/2330 train_time:6538ms step_avg:56.85ms
step:116/2330 train_time:6597ms step_avg:56.87ms
step:117/2330 train_time:6652ms step_avg:56.85ms
step:118/2330 train_time:6710ms step_avg:56.86ms
step:119/2330 train_time:6765ms step_avg:56.84ms
step:120/2330 train_time:6824ms step_avg:56.87ms
step:121/2330 train_time:6880ms step_avg:56.86ms
step:122/2330 train_time:6938ms step_avg:56.87ms
step:123/2330 train_time:6994ms step_avg:56.86ms
step:124/2330 train_time:7052ms step_avg:56.87ms
step:125/2330 train_time:7107ms step_avg:56.86ms
step:126/2330 train_time:7166ms step_avg:56.87ms
step:127/2330 train_time:7221ms step_avg:56.86ms
step:128/2330 train_time:7280ms step_avg:56.88ms
step:129/2330 train_time:7336ms step_avg:56.87ms
step:130/2330 train_time:7394ms step_avg:56.88ms
step:131/2330 train_time:7449ms step_avg:56.86ms
step:132/2330 train_time:7508ms step_avg:56.88ms
step:133/2330 train_time:7564ms step_avg:56.87ms
step:134/2330 train_time:7622ms step_avg:56.88ms
step:135/2330 train_time:7678ms step_avg:56.87ms
step:136/2330 train_time:7736ms step_avg:56.88ms
step:137/2330 train_time:7791ms step_avg:56.87ms
step:138/2330 train_time:7849ms step_avg:56.88ms
step:139/2330 train_time:7904ms step_avg:56.87ms
step:140/2330 train_time:7963ms step_avg:56.88ms
step:141/2330 train_time:8019ms step_avg:56.87ms
step:142/2330 train_time:8077ms step_avg:56.88ms
step:143/2330 train_time:8132ms step_avg:56.87ms
step:144/2330 train_time:8190ms step_avg:56.88ms
step:145/2330 train_time:8245ms step_avg:56.86ms
step:146/2330 train_time:8304ms step_avg:56.88ms
step:147/2330 train_time:8360ms step_avg:56.87ms
step:148/2330 train_time:8418ms step_avg:56.88ms
step:149/2330 train_time:8473ms step_avg:56.87ms
step:150/2330 train_time:8533ms step_avg:56.89ms
step:151/2330 train_time:8588ms step_avg:56.87ms
step:152/2330 train_time:8647ms step_avg:56.89ms
step:153/2330 train_time:8702ms step_avg:56.88ms
step:154/2330 train_time:8760ms step_avg:56.88ms
step:155/2330 train_time:8815ms step_avg:56.87ms
step:156/2330 train_time:8873ms step_avg:56.88ms
step:157/2330 train_time:8929ms step_avg:56.87ms
step:158/2330 train_time:8988ms step_avg:56.88ms
step:159/2330 train_time:9043ms step_avg:56.87ms
step:160/2330 train_time:9101ms step_avg:56.88ms
step:161/2330 train_time:9156ms step_avg:56.87ms
step:162/2330 train_time:9214ms step_avg:56.88ms
step:163/2330 train_time:9269ms step_avg:56.87ms
step:164/2330 train_time:9328ms step_avg:56.88ms
step:165/2330 train_time:9383ms step_avg:56.87ms
step:166/2330 train_time:9442ms step_avg:56.88ms
step:167/2330 train_time:9497ms step_avg:56.87ms
step:168/2330 train_time:9556ms step_avg:56.88ms
step:169/2330 train_time:9611ms step_avg:56.87ms
step:170/2330 train_time:9668ms step_avg:56.87ms
step:171/2330 train_time:9723ms step_avg:56.86ms
step:172/2330 train_time:9782ms step_avg:56.87ms
step:173/2330 train_time:9837ms step_avg:56.86ms
step:174/2330 train_time:9895ms step_avg:56.87ms
step:175/2330 train_time:9950ms step_avg:56.85ms
step:176/2330 train_time:10008ms step_avg:56.86ms
step:177/2330 train_time:10063ms step_avg:56.85ms
step:178/2330 train_time:10122ms step_avg:56.86ms
step:179/2330 train_time:10178ms step_avg:56.86ms
step:180/2330 train_time:10235ms step_avg:56.86ms
step:181/2330 train_time:10291ms step_avg:56.86ms
step:182/2330 train_time:10349ms step_avg:56.86ms
step:183/2330 train_time:10404ms step_avg:56.85ms
step:184/2330 train_time:10464ms step_avg:56.87ms
step:185/2330 train_time:10520ms step_avg:56.87ms
step:186/2330 train_time:10579ms step_avg:56.87ms
step:187/2330 train_time:10634ms step_avg:56.87ms
step:188/2330 train_time:10692ms step_avg:56.87ms
step:189/2330 train_time:10747ms step_avg:56.86ms
step:190/2330 train_time:10806ms step_avg:56.87ms
step:191/2330 train_time:10861ms step_avg:56.86ms
step:192/2330 train_time:10919ms step_avg:56.87ms
step:193/2330 train_time:10975ms step_avg:56.86ms
step:194/2330 train_time:11032ms step_avg:56.87ms
step:195/2330 train_time:11087ms step_avg:56.86ms
step:196/2330 train_time:11146ms step_avg:56.87ms
step:197/2330 train_time:11202ms step_avg:56.86ms
step:198/2330 train_time:11260ms step_avg:56.87ms
step:199/2330 train_time:11315ms step_avg:56.86ms
step:200/2330 train_time:11374ms step_avg:56.87ms
step:201/2330 train_time:11429ms step_avg:56.86ms
step:202/2330 train_time:11487ms step_avg:56.87ms
step:203/2330 train_time:11542ms step_avg:56.86ms
step:204/2330 train_time:11601ms step_avg:56.87ms
step:205/2330 train_time:11656ms step_avg:56.86ms
step:206/2330 train_time:11715ms step_avg:56.87ms
step:207/2330 train_time:11770ms step_avg:56.86ms
step:208/2330 train_time:11828ms step_avg:56.86ms
step:209/2330 train_time:11883ms step_avg:56.86ms
step:210/2330 train_time:11942ms step_avg:56.86ms
step:211/2330 train_time:11997ms step_avg:56.86ms
step:212/2330 train_time:12055ms step_avg:56.86ms
step:213/2330 train_time:12110ms step_avg:56.85ms
step:214/2330 train_time:12168ms step_avg:56.86ms
step:215/2330 train_time:12223ms step_avg:56.85ms
step:216/2330 train_time:12283ms step_avg:56.87ms
step:217/2330 train_time:12339ms step_avg:56.86ms
step:218/2330 train_time:12397ms step_avg:56.87ms
step:219/2330 train_time:12452ms step_avg:56.86ms
step:220/2330 train_time:12510ms step_avg:56.86ms
step:221/2330 train_time:12565ms step_avg:56.85ms
step:222/2330 train_time:12625ms step_avg:56.87ms
step:223/2330 train_time:12681ms step_avg:56.86ms
step:224/2330 train_time:12740ms step_avg:56.87ms
step:225/2330 train_time:12795ms step_avg:56.87ms
step:226/2330 train_time:12853ms step_avg:56.87ms
step:227/2330 train_time:12909ms step_avg:56.87ms
step:228/2330 train_time:12967ms step_avg:56.87ms
step:229/2330 train_time:13023ms step_avg:56.87ms
step:230/2330 train_time:13082ms step_avg:56.88ms
step:231/2330 train_time:13137ms step_avg:56.87ms
step:232/2330 train_time:13195ms step_avg:56.88ms
step:233/2330 train_time:13251ms step_avg:56.87ms
step:234/2330 train_time:13309ms step_avg:56.87ms
step:235/2330 train_time:13364ms step_avg:56.87ms
step:236/2330 train_time:13423ms step_avg:56.88ms
step:237/2330 train_time:13478ms step_avg:56.87ms
step:238/2330 train_time:13538ms step_avg:56.88ms
step:239/2330 train_time:13593ms step_avg:56.88ms
step:240/2330 train_time:13651ms step_avg:56.88ms
step:241/2330 train_time:13706ms step_avg:56.87ms
step:242/2330 train_time:13765ms step_avg:56.88ms
step:243/2330 train_time:13821ms step_avg:56.88ms
step:244/2330 train_time:13880ms step_avg:56.88ms
step:245/2330 train_time:13935ms step_avg:56.88ms
step:246/2330 train_time:13992ms step_avg:56.88ms
step:247/2330 train_time:14047ms step_avg:56.87ms
step:248/2330 train_time:14107ms step_avg:56.88ms
step:249/2330 train_time:14162ms step_avg:56.88ms
step:250/2330 train_time:14220ms step_avg:56.88ms
step:250/2330 val_loss:5.7267 train_time:14298ms step_avg:57.19ms
step:251/2330 train_time:14316ms step_avg:57.04ms
step:252/2330 train_time:14336ms step_avg:56.89ms
step:253/2330 train_time:14390ms step_avg:56.88ms
step:254/2330 train_time:14451ms step_avg:56.89ms
step:255/2330 train_time:14507ms step_avg:56.89ms
step:256/2330 train_time:14569ms step_avg:56.91ms
step:257/2330 train_time:14624ms step_avg:56.90ms
step:258/2330 train_time:14684ms step_avg:56.91ms
step:259/2330 train_time:14739ms step_avg:56.91ms
step:260/2330 train_time:14797ms step_avg:56.91ms
step:261/2330 train_time:14852ms step_avg:56.90ms
step:262/2330 train_time:14910ms step_avg:56.91ms
step:263/2330 train_time:14964ms step_avg:56.90ms
step:264/2330 train_time:15022ms step_avg:56.90ms
step:265/2330 train_time:15077ms step_avg:56.89ms
step:266/2330 train_time:15135ms step_avg:56.90ms
step:267/2330 train_time:15190ms step_avg:56.89ms
step:268/2330 train_time:15249ms step_avg:56.90ms
step:269/2330 train_time:15306ms step_avg:56.90ms
step:270/2330 train_time:15364ms step_avg:56.90ms
step:271/2330 train_time:15419ms step_avg:56.90ms
step:272/2330 train_time:15480ms step_avg:56.91ms
step:273/2330 train_time:15535ms step_avg:56.90ms
step:274/2330 train_time:15596ms step_avg:56.92ms
step:275/2330 train_time:15651ms step_avg:56.91ms
step:276/2330 train_time:15711ms step_avg:56.92ms
step:277/2330 train_time:15766ms step_avg:56.92ms
step:278/2330 train_time:15824ms step_avg:56.92ms
step:279/2330 train_time:15879ms step_avg:56.91ms
step:280/2330 train_time:15937ms step_avg:56.92ms
step:281/2330 train_time:15992ms step_avg:56.91ms
step:282/2330 train_time:16050ms step_avg:56.92ms
step:283/2330 train_time:16105ms step_avg:56.91ms
step:284/2330 train_time:16164ms step_avg:56.91ms
step:285/2330 train_time:16219ms step_avg:56.91ms
step:286/2330 train_time:16277ms step_avg:56.91ms
step:287/2330 train_time:16333ms step_avg:56.91ms
step:288/2330 train_time:16393ms step_avg:56.92ms
step:289/2330 train_time:16448ms step_avg:56.91ms
step:290/2330 train_time:16508ms step_avg:56.92ms
step:291/2330 train_time:16564ms step_avg:56.92ms
step:292/2330 train_time:16622ms step_avg:56.92ms
step:293/2330 train_time:16677ms step_avg:56.92ms
step:294/2330 train_time:16736ms step_avg:56.93ms
step:295/2330 train_time:16792ms step_avg:56.92ms
step:296/2330 train_time:16851ms step_avg:56.93ms
step:297/2330 train_time:16906ms step_avg:56.92ms
step:298/2330 train_time:16964ms step_avg:56.93ms
step:299/2330 train_time:17020ms step_avg:56.92ms
step:300/2330 train_time:17078ms step_avg:56.93ms
step:301/2330 train_time:17133ms step_avg:56.92ms
step:302/2330 train_time:17192ms step_avg:56.93ms
step:303/2330 train_time:17247ms step_avg:56.92ms
step:304/2330 train_time:17306ms step_avg:56.93ms
step:305/2330 train_time:17361ms step_avg:56.92ms
step:306/2330 train_time:17419ms step_avg:56.93ms
step:307/2330 train_time:17475ms step_avg:56.92ms
step:308/2330 train_time:17534ms step_avg:56.93ms
step:309/2330 train_time:17590ms step_avg:56.93ms
step:310/2330 train_time:17649ms step_avg:56.93ms
step:311/2330 train_time:17705ms step_avg:56.93ms
step:312/2330 train_time:17763ms step_avg:56.93ms
step:313/2330 train_time:17818ms step_avg:56.93ms
step:314/2330 train_time:17877ms step_avg:56.93ms
step:315/2330 train_time:17932ms step_avg:56.93ms
step:316/2330 train_time:17990ms step_avg:56.93ms
step:317/2330 train_time:18046ms step_avg:56.93ms
step:318/2330 train_time:18104ms step_avg:56.93ms
step:319/2330 train_time:18159ms step_avg:56.92ms
step:320/2330 train_time:18217ms step_avg:56.93ms
step:321/2330 train_time:18272ms step_avg:56.92ms
step:322/2330 train_time:18330ms step_avg:56.93ms
step:323/2330 train_time:18386ms step_avg:56.92ms
step:324/2330 train_time:18444ms step_avg:56.93ms
step:325/2330 train_time:18500ms step_avg:56.92ms
step:326/2330 train_time:18558ms step_avg:56.93ms
step:327/2330 train_time:18613ms step_avg:56.92ms
step:328/2330 train_time:18673ms step_avg:56.93ms
step:329/2330 train_time:18728ms step_avg:56.92ms
step:330/2330 train_time:18786ms step_avg:56.93ms
step:331/2330 train_time:18842ms step_avg:56.92ms
step:332/2330 train_time:18900ms step_avg:56.93ms
step:333/2330 train_time:18955ms step_avg:56.92ms
step:334/2330 train_time:19014ms step_avg:56.93ms
step:335/2330 train_time:19070ms step_avg:56.92ms
step:336/2330 train_time:19128ms step_avg:56.93ms
step:337/2330 train_time:19183ms step_avg:56.92ms
step:338/2330 train_time:19241ms step_avg:56.93ms
step:339/2330 train_time:19296ms step_avg:56.92ms
step:340/2330 train_time:19354ms step_avg:56.92ms
step:341/2330 train_time:19410ms step_avg:56.92ms
step:342/2330 train_time:19469ms step_avg:56.93ms
step:343/2330 train_time:19524ms step_avg:56.92ms
step:344/2330 train_time:19581ms step_avg:56.92ms
step:345/2330 train_time:19636ms step_avg:56.92ms
step:346/2330 train_time:19695ms step_avg:56.92ms
step:347/2330 train_time:19750ms step_avg:56.92ms
step:348/2330 train_time:19810ms step_avg:56.93ms
step:349/2330 train_time:19865ms step_avg:56.92ms
step:350/2330 train_time:19924ms step_avg:56.93ms
step:351/2330 train_time:19979ms step_avg:56.92ms
step:352/2330 train_time:20037ms step_avg:56.92ms
step:353/2330 train_time:20093ms step_avg:56.92ms
step:354/2330 train_time:20153ms step_avg:56.93ms
step:355/2330 train_time:20208ms step_avg:56.93ms
step:356/2330 train_time:20266ms step_avg:56.93ms
step:357/2330 train_time:20322ms step_avg:56.92ms
step:358/2330 train_time:20380ms step_avg:56.93ms
step:359/2330 train_time:20436ms step_avg:56.92ms
step:360/2330 train_time:20494ms step_avg:56.93ms
step:361/2330 train_time:20551ms step_avg:56.93ms
step:362/2330 train_time:20609ms step_avg:56.93ms
step:363/2330 train_time:20664ms step_avg:56.93ms
step:364/2330 train_time:20722ms step_avg:56.93ms
step:365/2330 train_time:20777ms step_avg:56.92ms
step:366/2330 train_time:20836ms step_avg:56.93ms
step:367/2330 train_time:20891ms step_avg:56.92ms
step:368/2330 train_time:20950ms step_avg:56.93ms
step:369/2330 train_time:21006ms step_avg:56.93ms
step:370/2330 train_time:21064ms step_avg:56.93ms
step:371/2330 train_time:21120ms step_avg:56.93ms
step:372/2330 train_time:21177ms step_avg:56.93ms
step:373/2330 train_time:21232ms step_avg:56.92ms
step:374/2330 train_time:21291ms step_avg:56.93ms
step:375/2330 train_time:21347ms step_avg:56.93ms
step:376/2330 train_time:21406ms step_avg:56.93ms
step:377/2330 train_time:21462ms step_avg:56.93ms
step:378/2330 train_time:21520ms step_avg:56.93ms
step:379/2330 train_time:21575ms step_avg:56.93ms
step:380/2330 train_time:21634ms step_avg:56.93ms
step:381/2330 train_time:21690ms step_avg:56.93ms
step:382/2330 train_time:21749ms step_avg:56.93ms
step:383/2330 train_time:21804ms step_avg:56.93ms
step:384/2330 train_time:21862ms step_avg:56.93ms
step:385/2330 train_time:21917ms step_avg:56.93ms
step:386/2330 train_time:21975ms step_avg:56.93ms
step:387/2330 train_time:22031ms step_avg:56.93ms
step:388/2330 train_time:22089ms step_avg:56.93ms
step:389/2330 train_time:22144ms step_avg:56.93ms
step:390/2330 train_time:22203ms step_avg:56.93ms
step:391/2330 train_time:22257ms step_avg:56.92ms
step:392/2330 train_time:22316ms step_avg:56.93ms
step:393/2330 train_time:22372ms step_avg:56.93ms
step:394/2330 train_time:22431ms step_avg:56.93ms
step:395/2330 train_time:22487ms step_avg:56.93ms
step:396/2330 train_time:22546ms step_avg:56.93ms
step:397/2330 train_time:22601ms step_avg:56.93ms
step:398/2330 train_time:22660ms step_avg:56.93ms
step:399/2330 train_time:22715ms step_avg:56.93ms
step:400/2330 train_time:22773ms step_avg:56.93ms
step:401/2330 train_time:22829ms step_avg:56.93ms
step:402/2330 train_time:22887ms step_avg:56.93ms
step:403/2330 train_time:22943ms step_avg:56.93ms
step:404/2330 train_time:23001ms step_avg:56.93ms
step:405/2330 train_time:23056ms step_avg:56.93ms
step:406/2330 train_time:23116ms step_avg:56.94ms
step:407/2330 train_time:23171ms step_avg:56.93ms
step:408/2330 train_time:23230ms step_avg:56.94ms
step:409/2330 train_time:23285ms step_avg:56.93ms
step:410/2330 train_time:23344ms step_avg:56.94ms
step:411/2330 train_time:23400ms step_avg:56.93ms
step:412/2330 train_time:23458ms step_avg:56.94ms
step:413/2330 train_time:23513ms step_avg:56.93ms
step:414/2330 train_time:23572ms step_avg:56.94ms
step:415/2330 train_time:23628ms step_avg:56.94ms
step:416/2330 train_time:23686ms step_avg:56.94ms
step:417/2330 train_time:23742ms step_avg:56.94ms
step:418/2330 train_time:23800ms step_avg:56.94ms
step:419/2330 train_time:23855ms step_avg:56.93ms
step:420/2330 train_time:23914ms step_avg:56.94ms
step:421/2330 train_time:23970ms step_avg:56.94ms
step:422/2330 train_time:24028ms step_avg:56.94ms
step:423/2330 train_time:24083ms step_avg:56.93ms
step:424/2330 train_time:24142ms step_avg:56.94ms
step:425/2330 train_time:24197ms step_avg:56.93ms
step:426/2330 train_time:24256ms step_avg:56.94ms
step:427/2330 train_time:24312ms step_avg:56.94ms
step:428/2330 train_time:24370ms step_avg:56.94ms
step:429/2330 train_time:24425ms step_avg:56.94ms
step:430/2330 train_time:24483ms step_avg:56.94ms
step:431/2330 train_time:24538ms step_avg:56.93ms
step:432/2330 train_time:24597ms step_avg:56.94ms
step:433/2330 train_time:24653ms step_avg:56.93ms
step:434/2330 train_time:24712ms step_avg:56.94ms
step:435/2330 train_time:24768ms step_avg:56.94ms
step:436/2330 train_time:24826ms step_avg:56.94ms
step:437/2330 train_time:24882ms step_avg:56.94ms
step:438/2330 train_time:24940ms step_avg:56.94ms
step:439/2330 train_time:24996ms step_avg:56.94ms
step:440/2330 train_time:25056ms step_avg:56.94ms
step:441/2330 train_time:25111ms step_avg:56.94ms
step:442/2330 train_time:25170ms step_avg:56.94ms
step:443/2330 train_time:25225ms step_avg:56.94ms
step:444/2330 train_time:25283ms step_avg:56.94ms
step:445/2330 train_time:25338ms step_avg:56.94ms
step:446/2330 train_time:25396ms step_avg:56.94ms
step:447/2330 train_time:25452ms step_avg:56.94ms
step:448/2330 train_time:25511ms step_avg:56.94ms
step:449/2330 train_time:25566ms step_avg:56.94ms
step:450/2330 train_time:25624ms step_avg:56.94ms
step:451/2330 train_time:25679ms step_avg:56.94ms
step:452/2330 train_time:25739ms step_avg:56.94ms
step:453/2330 train_time:25794ms step_avg:56.94ms
step:454/2330 train_time:25854ms step_avg:56.95ms
step:455/2330 train_time:25910ms step_avg:56.94ms
step:456/2330 train_time:25968ms step_avg:56.95ms
step:457/2330 train_time:26023ms step_avg:56.94ms
step:458/2330 train_time:26081ms step_avg:56.95ms
step:459/2330 train_time:26137ms step_avg:56.94ms
step:460/2330 train_time:26196ms step_avg:56.95ms
step:461/2330 train_time:26252ms step_avg:56.95ms
step:462/2330 train_time:26311ms step_avg:56.95ms
step:463/2330 train_time:26367ms step_avg:56.95ms
step:464/2330 train_time:26425ms step_avg:56.95ms
step:465/2330 train_time:26480ms step_avg:56.95ms
step:466/2330 train_time:26539ms step_avg:56.95ms
step:467/2330 train_time:26595ms step_avg:56.95ms
step:468/2330 train_time:26654ms step_avg:56.95ms
step:469/2330 train_time:26710ms step_avg:56.95ms
step:470/2330 train_time:26768ms step_avg:56.95ms
step:471/2330 train_time:26824ms step_avg:56.95ms
step:472/2330 train_time:26882ms step_avg:56.95ms
step:473/2330 train_time:26937ms step_avg:56.95ms
step:474/2330 train_time:26996ms step_avg:56.95ms
step:475/2330 train_time:27051ms step_avg:56.95ms
step:476/2330 train_time:27110ms step_avg:56.95ms
step:477/2330 train_time:27165ms step_avg:56.95ms
step:478/2330 train_time:27224ms step_avg:56.95ms
step:479/2330 train_time:27279ms step_avg:56.95ms
step:480/2330 train_time:27337ms step_avg:56.95ms
step:481/2330 train_time:27393ms step_avg:56.95ms
step:482/2330 train_time:27452ms step_avg:56.95ms
step:483/2330 train_time:27507ms step_avg:56.95ms
step:484/2330 train_time:27567ms step_avg:56.96ms
step:485/2330 train_time:27622ms step_avg:56.95ms
step:486/2330 train_time:27681ms step_avg:56.96ms
step:487/2330 train_time:27736ms step_avg:56.95ms
step:488/2330 train_time:27796ms step_avg:56.96ms
step:489/2330 train_time:27852ms step_avg:56.96ms
step:490/2330 train_time:27911ms step_avg:56.96ms
step:491/2330 train_time:27967ms step_avg:56.96ms
step:492/2330 train_time:28025ms step_avg:56.96ms
step:493/2330 train_time:28080ms step_avg:56.96ms
step:494/2330 train_time:28139ms step_avg:56.96ms
step:495/2330 train_time:28194ms step_avg:56.96ms
step:496/2330 train_time:28254ms step_avg:56.96ms
step:497/2330 train_time:28310ms step_avg:56.96ms
step:498/2330 train_time:28368ms step_avg:56.96ms
step:499/2330 train_time:28423ms step_avg:56.96ms
step:500/2330 train_time:28483ms step_avg:56.97ms
step:500/2330 val_loss:5.1528 train_time:28561ms step_avg:57.12ms
step:501/2330 train_time:28579ms step_avg:57.04ms
step:502/2330 train_time:28599ms step_avg:56.97ms
step:503/2330 train_time:28656ms step_avg:56.97ms
step:504/2330 train_time:28720ms step_avg:56.98ms
step:505/2330 train_time:28777ms step_avg:56.98ms
step:506/2330 train_time:28837ms step_avg:56.99ms
step:507/2330 train_time:28893ms step_avg:56.99ms
step:508/2330 train_time:28952ms step_avg:56.99ms
step:509/2330 train_time:29007ms step_avg:56.99ms
step:510/2330 train_time:29066ms step_avg:56.99ms
step:511/2330 train_time:29121ms step_avg:56.99ms
step:512/2330 train_time:29178ms step_avg:56.99ms
step:513/2330 train_time:29233ms step_avg:56.98ms
step:514/2330 train_time:29291ms step_avg:56.99ms
step:515/2330 train_time:29346ms step_avg:56.98ms
step:516/2330 train_time:29404ms step_avg:56.99ms
step:517/2330 train_time:29459ms step_avg:56.98ms
step:518/2330 train_time:29517ms step_avg:56.98ms
step:519/2330 train_time:29573ms step_avg:56.98ms
step:520/2330 train_time:29632ms step_avg:56.98ms
step:521/2330 train_time:29689ms step_avg:56.98ms
step:522/2330 train_time:29748ms step_avg:56.99ms
step:523/2330 train_time:29804ms step_avg:56.99ms
step:524/2330 train_time:29863ms step_avg:56.99ms
step:525/2330 train_time:29919ms step_avg:56.99ms
step:526/2330 train_time:29978ms step_avg:56.99ms
step:527/2330 train_time:30034ms step_avg:56.99ms
step:528/2330 train_time:30093ms step_avg:56.99ms
step:529/2330 train_time:30148ms step_avg:56.99ms
step:530/2330 train_time:30207ms step_avg:56.99ms
step:531/2330 train_time:30262ms step_avg:56.99ms
step:532/2330 train_time:30320ms step_avg:56.99ms
step:533/2330 train_time:30376ms step_avg:56.99ms
step:534/2330 train_time:30434ms step_avg:56.99ms
step:535/2330 train_time:30489ms step_avg:56.99ms
step:536/2330 train_time:30548ms step_avg:56.99ms
step:537/2330 train_time:30603ms step_avg:56.99ms
step:538/2330 train_time:30662ms step_avg:56.99ms
step:539/2330 train_time:30717ms step_avg:56.99ms
step:540/2330 train_time:30777ms step_avg:56.99ms
step:541/2330 train_time:30832ms step_avg:56.99ms
step:542/2330 train_time:30892ms step_avg:57.00ms
step:543/2330 train_time:30948ms step_avg:56.99ms
step:544/2330 train_time:31007ms step_avg:57.00ms
step:545/2330 train_time:31062ms step_avg:56.99ms
step:546/2330 train_time:31120ms step_avg:57.00ms
step:547/2330 train_time:31175ms step_avg:56.99ms
step:548/2330 train_time:31234ms step_avg:57.00ms
step:549/2330 train_time:31290ms step_avg:56.99ms
step:550/2330 train_time:31349ms step_avg:57.00ms
step:551/2330 train_time:31404ms step_avg:56.99ms
step:552/2330 train_time:31462ms step_avg:57.00ms
step:553/2330 train_time:31517ms step_avg:56.99ms
step:554/2330 train_time:31576ms step_avg:57.00ms
step:555/2330 train_time:31631ms step_avg:56.99ms
step:556/2330 train_time:31691ms step_avg:57.00ms
step:557/2330 train_time:31747ms step_avg:57.00ms
step:558/2330 train_time:31806ms step_avg:57.00ms
step:559/2330 train_time:31862ms step_avg:57.00ms
step:560/2330 train_time:31920ms step_avg:57.00ms
step:561/2330 train_time:31975ms step_avg:57.00ms
step:562/2330 train_time:32034ms step_avg:57.00ms
step:563/2330 train_time:32090ms step_avg:57.00ms
step:564/2330 train_time:32149ms step_avg:57.00ms
step:565/2330 train_time:32204ms step_avg:57.00ms
step:566/2330 train_time:32262ms step_avg:57.00ms
step:567/2330 train_time:32317ms step_avg:57.00ms
step:568/2330 train_time:32376ms step_avg:57.00ms
step:569/2330 train_time:32431ms step_avg:57.00ms
step:570/2330 train_time:32490ms step_avg:57.00ms
step:571/2330 train_time:32545ms step_avg:57.00ms
step:572/2330 train_time:32604ms step_avg:57.00ms
step:573/2330 train_time:32659ms step_avg:57.00ms
step:574/2330 train_time:32719ms step_avg:57.00ms
step:575/2330 train_time:32775ms step_avg:57.00ms
step:576/2330 train_time:32834ms step_avg:57.00ms
step:577/2330 train_time:32890ms step_avg:57.00ms
step:578/2330 train_time:32949ms step_avg:57.01ms
step:579/2330 train_time:33004ms step_avg:57.00ms
step:580/2330 train_time:33064ms step_avg:57.01ms
step:581/2330 train_time:33119ms step_avg:57.00ms
step:582/2330 train_time:33178ms step_avg:57.01ms
step:583/2330 train_time:33233ms step_avg:57.00ms
step:584/2330 train_time:33292ms step_avg:57.01ms
step:585/2330 train_time:33348ms step_avg:57.00ms
step:586/2330 train_time:33406ms step_avg:57.01ms
step:587/2330 train_time:33461ms step_avg:57.00ms
step:588/2330 train_time:33519ms step_avg:57.01ms
step:589/2330 train_time:33574ms step_avg:57.00ms
step:590/2330 train_time:33634ms step_avg:57.01ms
step:591/2330 train_time:33690ms step_avg:57.00ms
step:592/2330 train_time:33749ms step_avg:57.01ms
step:593/2330 train_time:33805ms step_avg:57.01ms
step:594/2330 train_time:33863ms step_avg:57.01ms
step:595/2330 train_time:33918ms step_avg:57.01ms
step:596/2330 train_time:33978ms step_avg:57.01ms
step:597/2330 train_time:34033ms step_avg:57.01ms
step:598/2330 train_time:34093ms step_avg:57.01ms
step:599/2330 train_time:34149ms step_avg:57.01ms
step:600/2330 train_time:34207ms step_avg:57.01ms
step:601/2330 train_time:34263ms step_avg:57.01ms
step:602/2330 train_time:34322ms step_avg:57.01ms
step:603/2330 train_time:34377ms step_avg:57.01ms
step:604/2330 train_time:34436ms step_avg:57.01ms
step:605/2330 train_time:34491ms step_avg:57.01ms
step:606/2330 train_time:34550ms step_avg:57.01ms
step:607/2330 train_time:34605ms step_avg:57.01ms
step:608/2330 train_time:34663ms step_avg:57.01ms
step:609/2330 train_time:34719ms step_avg:57.01ms
step:610/2330 train_time:34778ms step_avg:57.01ms
step:611/2330 train_time:34833ms step_avg:57.01ms
step:612/2330 train_time:34892ms step_avg:57.01ms
step:613/2330 train_time:34948ms step_avg:57.01ms
step:614/2330 train_time:35007ms step_avg:57.01ms
step:615/2330 train_time:35063ms step_avg:57.01ms
step:616/2330 train_time:35120ms step_avg:57.01ms
step:617/2330 train_time:35176ms step_avg:57.01ms
step:618/2330 train_time:35236ms step_avg:57.02ms
step:619/2330 train_time:35291ms step_avg:57.01ms
step:620/2330 train_time:35351ms step_avg:57.02ms
step:621/2330 train_time:35406ms step_avg:57.01ms
step:622/2330 train_time:35465ms step_avg:57.02ms
step:623/2330 train_time:35520ms step_avg:57.01ms
step:624/2330 train_time:35579ms step_avg:57.02ms
step:625/2330 train_time:35634ms step_avg:57.01ms
step:626/2330 train_time:35693ms step_avg:57.02ms
step:627/2330 train_time:35749ms step_avg:57.02ms
step:628/2330 train_time:35808ms step_avg:57.02ms
step:629/2330 train_time:35863ms step_avg:57.02ms
step:630/2330 train_time:35921ms step_avg:57.02ms
step:631/2330 train_time:35977ms step_avg:57.02ms
step:632/2330 train_time:36035ms step_avg:57.02ms
step:633/2330 train_time:36091ms step_avg:57.02ms
step:634/2330 train_time:36150ms step_avg:57.02ms
step:635/2330 train_time:36205ms step_avg:57.02ms
step:636/2330 train_time:36265ms step_avg:57.02ms
step:637/2330 train_time:36320ms step_avg:57.02ms
step:638/2330 train_time:36379ms step_avg:57.02ms
step:639/2330 train_time:36434ms step_avg:57.02ms
step:640/2330 train_time:36494ms step_avg:57.02ms
step:641/2330 train_time:36550ms step_avg:57.02ms
step:642/2330 train_time:36608ms step_avg:57.02ms
step:643/2330 train_time:36664ms step_avg:57.02ms
step:644/2330 train_time:36723ms step_avg:57.02ms
step:645/2330 train_time:36778ms step_avg:57.02ms
step:646/2330 train_time:36838ms step_avg:57.02ms
step:647/2330 train_time:36893ms step_avg:57.02ms
step:648/2330 train_time:36953ms step_avg:57.03ms
step:649/2330 train_time:37009ms step_avg:57.02ms
step:650/2330 train_time:37068ms step_avg:57.03ms
step:651/2330 train_time:37123ms step_avg:57.02ms
step:652/2330 train_time:37182ms step_avg:57.03ms
step:653/2330 train_time:37238ms step_avg:57.03ms
step:654/2330 train_time:37297ms step_avg:57.03ms
step:655/2330 train_time:37352ms step_avg:57.03ms
step:656/2330 train_time:37411ms step_avg:57.03ms
step:657/2330 train_time:37466ms step_avg:57.03ms
step:658/2330 train_time:37525ms step_avg:57.03ms
step:659/2330 train_time:37580ms step_avg:57.03ms
step:660/2330 train_time:37638ms step_avg:57.03ms
step:661/2330 train_time:37694ms step_avg:57.03ms
step:662/2330 train_time:37754ms step_avg:57.03ms
step:663/2330 train_time:37809ms step_avg:57.03ms
step:664/2330 train_time:37868ms step_avg:57.03ms
step:665/2330 train_time:37923ms step_avg:57.03ms
step:666/2330 train_time:37982ms step_avg:57.03ms
step:667/2330 train_time:38037ms step_avg:57.03ms
step:668/2330 train_time:38097ms step_avg:57.03ms
step:669/2330 train_time:38153ms step_avg:57.03ms
step:670/2330 train_time:38211ms step_avg:57.03ms
step:671/2330 train_time:38267ms step_avg:57.03ms
step:672/2330 train_time:38325ms step_avg:57.03ms
step:673/2330 train_time:38380ms step_avg:57.03ms
step:674/2330 train_time:38439ms step_avg:57.03ms
step:675/2330 train_time:38495ms step_avg:57.03ms
step:676/2330 train_time:38554ms step_avg:57.03ms
step:677/2330 train_time:38609ms step_avg:57.03ms
step:678/2330 train_time:38668ms step_avg:57.03ms
step:679/2330 train_time:38723ms step_avg:57.03ms
step:680/2330 train_time:38782ms step_avg:57.03ms
step:681/2330 train_time:38837ms step_avg:57.03ms
step:682/2330 train_time:38897ms step_avg:57.03ms
step:683/2330 train_time:38953ms step_avg:57.03ms
step:684/2330 train_time:39012ms step_avg:57.04ms
step:685/2330 train_time:39067ms step_avg:57.03ms
step:686/2330 train_time:39127ms step_avg:57.04ms
step:687/2330 train_time:39182ms step_avg:57.03ms
step:688/2330 train_time:39240ms step_avg:57.04ms
step:689/2330 train_time:39296ms step_avg:57.03ms
step:690/2330 train_time:39355ms step_avg:57.04ms
step:691/2330 train_time:39411ms step_avg:57.03ms
step:692/2330 train_time:39470ms step_avg:57.04ms
step:693/2330 train_time:39526ms step_avg:57.04ms
step:694/2330 train_time:39585ms step_avg:57.04ms
step:695/2330 train_time:39641ms step_avg:57.04ms
step:696/2330 train_time:39699ms step_avg:57.04ms
step:697/2330 train_time:39754ms step_avg:57.04ms
step:698/2330 train_time:39814ms step_avg:57.04ms
step:699/2330 train_time:39869ms step_avg:57.04ms
step:700/2330 train_time:39927ms step_avg:57.04ms
step:701/2330 train_time:39982ms step_avg:57.04ms
step:702/2330 train_time:40042ms step_avg:57.04ms
step:703/2330 train_time:40097ms step_avg:57.04ms
step:704/2330 train_time:40157ms step_avg:57.04ms
step:705/2330 train_time:40213ms step_avg:57.04ms
step:706/2330 train_time:40271ms step_avg:57.04ms
step:707/2330 train_time:40326ms step_avg:57.04ms
step:708/2330 train_time:40385ms step_avg:57.04ms
step:709/2330 train_time:40440ms step_avg:57.04ms
step:710/2330 train_time:40499ms step_avg:57.04ms
step:711/2330 train_time:40555ms step_avg:57.04ms
step:712/2330 train_time:40614ms step_avg:57.04ms
step:713/2330 train_time:40669ms step_avg:57.04ms
step:714/2330 train_time:40728ms step_avg:57.04ms
step:715/2330 train_time:40783ms step_avg:57.04ms
step:716/2330 train_time:40842ms step_avg:57.04ms
step:717/2330 train_time:40897ms step_avg:57.04ms
step:718/2330 train_time:40956ms step_avg:57.04ms
step:719/2330 train_time:41012ms step_avg:57.04ms
step:720/2330 train_time:41070ms step_avg:57.04ms
step:721/2330 train_time:41126ms step_avg:57.04ms
step:722/2330 train_time:41185ms step_avg:57.04ms
step:723/2330 train_time:41239ms step_avg:57.04ms
step:724/2330 train_time:41299ms step_avg:57.04ms
step:725/2330 train_time:41355ms step_avg:57.04ms
step:726/2330 train_time:41414ms step_avg:57.04ms
step:727/2330 train_time:41469ms step_avg:57.04ms
step:728/2330 train_time:41528ms step_avg:57.04ms
step:729/2330 train_time:41584ms step_avg:57.04ms
step:730/2330 train_time:41642ms step_avg:57.04ms
step:731/2330 train_time:41698ms step_avg:57.04ms
step:732/2330 train_time:41757ms step_avg:57.05ms
step:733/2330 train_time:41813ms step_avg:57.04ms
step:734/2330 train_time:41872ms step_avg:57.05ms
step:735/2330 train_time:41928ms step_avg:57.04ms
step:736/2330 train_time:41986ms step_avg:57.05ms
step:737/2330 train_time:42041ms step_avg:57.04ms
step:738/2330 train_time:42100ms step_avg:57.05ms
step:739/2330 train_time:42156ms step_avg:57.04ms
step:740/2330 train_time:42214ms step_avg:57.05ms
step:741/2330 train_time:42270ms step_avg:57.04ms
step:742/2330 train_time:42329ms step_avg:57.05ms
step:743/2330 train_time:42384ms step_avg:57.04ms
step:744/2330 train_time:42442ms step_avg:57.05ms
step:745/2330 train_time:42498ms step_avg:57.04ms
step:746/2330 train_time:42557ms step_avg:57.05ms
step:747/2330 train_time:42613ms step_avg:57.04ms
step:748/2330 train_time:42671ms step_avg:57.05ms
step:749/2330 train_time:42727ms step_avg:57.05ms
step:750/2330 train_time:42786ms step_avg:57.05ms
step:750/2330 val_loss:4.7408 train_time:42865ms step_avg:57.15ms
step:751/2330 train_time:42881ms step_avg:57.10ms
step:752/2330 train_time:42902ms step_avg:57.05ms
step:753/2330 train_time:42958ms step_avg:57.05ms
step:754/2330 train_time:43022ms step_avg:57.06ms
step:755/2330 train_time:43078ms step_avg:57.06ms
step:756/2330 train_time:43140ms step_avg:57.06ms
step:757/2330 train_time:43195ms step_avg:57.06ms
step:758/2330 train_time:43255ms step_avg:57.07ms
step:759/2330 train_time:43311ms step_avg:57.06ms
step:760/2330 train_time:43368ms step_avg:57.06ms
step:761/2330 train_time:43424ms step_avg:57.06ms
step:762/2330 train_time:43482ms step_avg:57.06ms
step:763/2330 train_time:43538ms step_avg:57.06ms
step:764/2330 train_time:43596ms step_avg:57.06ms
step:765/2330 train_time:43652ms step_avg:57.06ms
step:766/2330 train_time:43709ms step_avg:57.06ms
step:767/2330 train_time:43765ms step_avg:57.06ms
step:768/2330 train_time:43824ms step_avg:57.06ms
step:769/2330 train_time:43881ms step_avg:57.06ms
step:770/2330 train_time:43942ms step_avg:57.07ms
step:771/2330 train_time:43999ms step_avg:57.07ms
step:772/2330 train_time:44059ms step_avg:57.07ms
step:773/2330 train_time:44117ms step_avg:57.07ms
step:774/2330 train_time:44177ms step_avg:57.08ms
step:775/2330 train_time:44234ms step_avg:57.08ms
step:776/2330 train_time:44293ms step_avg:57.08ms
step:777/2330 train_time:44350ms step_avg:57.08ms
step:778/2330 train_time:44409ms step_avg:57.08ms
step:779/2330 train_time:44465ms step_avg:57.08ms
step:780/2330 train_time:44524ms step_avg:57.08ms
step:781/2330 train_time:44580ms step_avg:57.08ms
step:782/2330 train_time:44640ms step_avg:57.08ms
step:783/2330 train_time:44696ms step_avg:57.08ms
step:784/2330 train_time:44755ms step_avg:57.09ms
step:785/2330 train_time:44811ms step_avg:57.08ms
step:786/2330 train_time:44870ms step_avg:57.09ms
step:787/2330 train_time:44927ms step_avg:57.09ms
step:788/2330 train_time:44986ms step_avg:57.09ms
step:789/2330 train_time:45043ms step_avg:57.09ms
step:790/2330 train_time:45103ms step_avg:57.09ms
step:791/2330 train_time:45160ms step_avg:57.09ms
step:792/2330 train_time:45220ms step_avg:57.10ms
step:793/2330 train_time:45277ms step_avg:57.10ms
step:794/2330 train_time:45337ms step_avg:57.10ms
step:795/2330 train_time:45395ms step_avg:57.10ms
step:796/2330 train_time:45455ms step_avg:57.10ms
step:797/2330 train_time:45511ms step_avg:57.10ms
step:798/2330 train_time:45569ms step_avg:57.10ms
step:799/2330 train_time:45625ms step_avg:57.10ms
step:800/2330 train_time:45684ms step_avg:57.11ms
step:801/2330 train_time:45741ms step_avg:57.10ms
step:802/2330 train_time:45800ms step_avg:57.11ms
step:803/2330 train_time:45856ms step_avg:57.11ms
step:804/2330 train_time:45916ms step_avg:57.11ms
step:805/2330 train_time:45972ms step_avg:57.11ms
step:806/2330 train_time:46032ms step_avg:57.11ms
step:807/2330 train_time:46089ms step_avg:57.11ms
step:808/2330 train_time:46148ms step_avg:57.11ms
step:809/2330 train_time:46205ms step_avg:57.11ms
step:810/2330 train_time:46264ms step_avg:57.12ms
step:811/2330 train_time:46320ms step_avg:57.12ms
step:812/2330 train_time:46382ms step_avg:57.12ms
step:813/2330 train_time:46438ms step_avg:57.12ms
step:814/2330 train_time:46498ms step_avg:57.12ms
step:815/2330 train_time:46555ms step_avg:57.12ms
step:816/2330 train_time:46615ms step_avg:57.13ms
step:817/2330 train_time:46671ms step_avg:57.13ms
step:818/2330 train_time:46731ms step_avg:57.13ms
step:819/2330 train_time:46787ms step_avg:57.13ms
step:820/2330 train_time:46846ms step_avg:57.13ms
step:821/2330 train_time:46902ms step_avg:57.13ms
step:822/2330 train_time:46962ms step_avg:57.13ms
step:823/2330 train_time:47018ms step_avg:57.13ms
step:824/2330 train_time:47077ms step_avg:57.13ms
step:825/2330 train_time:47134ms step_avg:57.13ms
step:826/2330 train_time:47195ms step_avg:57.14ms
step:827/2330 train_time:47251ms step_avg:57.14ms
step:828/2330 train_time:47311ms step_avg:57.14ms
step:829/2330 train_time:47367ms step_avg:57.14ms
step:830/2330 train_time:47427ms step_avg:57.14ms
step:831/2330 train_time:47483ms step_avg:57.14ms
step:832/2330 train_time:47543ms step_avg:57.14ms
step:833/2330 train_time:47600ms step_avg:57.14ms
step:834/2330 train_time:47660ms step_avg:57.15ms
step:835/2330 train_time:47716ms step_avg:57.15ms
step:836/2330 train_time:47777ms step_avg:57.15ms
step:837/2330 train_time:47833ms step_avg:57.15ms
step:838/2330 train_time:47892ms step_avg:57.15ms
step:839/2330 train_time:47949ms step_avg:57.15ms
step:840/2330 train_time:48008ms step_avg:57.15ms
step:841/2330 train_time:48064ms step_avg:57.15ms
step:842/2330 train_time:48123ms step_avg:57.15ms
step:843/2330 train_time:48180ms step_avg:57.15ms
step:844/2330 train_time:48240ms step_avg:57.16ms
step:845/2330 train_time:48296ms step_avg:57.15ms
step:846/2330 train_time:48357ms step_avg:57.16ms
step:847/2330 train_time:48413ms step_avg:57.16ms
step:848/2330 train_time:48473ms step_avg:57.16ms
step:849/2330 train_time:48530ms step_avg:57.16ms
step:850/2330 train_time:48589ms step_avg:57.16ms
step:851/2330 train_time:48645ms step_avg:57.16ms
step:852/2330 train_time:48704ms step_avg:57.16ms
step:853/2330 train_time:48760ms step_avg:57.16ms
step:854/2330 train_time:48820ms step_avg:57.17ms
step:855/2330 train_time:48877ms step_avg:57.17ms
step:856/2330 train_time:48936ms step_avg:57.17ms
step:857/2330 train_time:48993ms step_avg:57.17ms
step:858/2330 train_time:49053ms step_avg:57.17ms
step:859/2330 train_time:49109ms step_avg:57.17ms
step:860/2330 train_time:49168ms step_avg:57.17ms
step:861/2330 train_time:49224ms step_avg:57.17ms
step:862/2330 train_time:49284ms step_avg:57.17ms
step:863/2330 train_time:49340ms step_avg:57.17ms
step:864/2330 train_time:49400ms step_avg:57.18ms
step:865/2330 train_time:49456ms step_avg:57.17ms
step:866/2330 train_time:49516ms step_avg:57.18ms
step:867/2330 train_time:49572ms step_avg:57.18ms
step:868/2330 train_time:49632ms step_avg:57.18ms
step:869/2330 train_time:49688ms step_avg:57.18ms
step:870/2330 train_time:49748ms step_avg:57.18ms
step:871/2330 train_time:49804ms step_avg:57.18ms
step:872/2330 train_time:49864ms step_avg:57.18ms
step:873/2330 train_time:49920ms step_avg:57.18ms
step:874/2330 train_time:49980ms step_avg:57.19ms
step:875/2330 train_time:50037ms step_avg:57.18ms
step:876/2330 train_time:50096ms step_avg:57.19ms
step:877/2330 train_time:50153ms step_avg:57.19ms
step:878/2330 train_time:50213ms step_avg:57.19ms
step:879/2330 train_time:50270ms step_avg:57.19ms
step:880/2330 train_time:50329ms step_avg:57.19ms
step:881/2330 train_time:50385ms step_avg:57.19ms
step:882/2330 train_time:50445ms step_avg:57.19ms
step:883/2330 train_time:50501ms step_avg:57.19ms
step:884/2330 train_time:50560ms step_avg:57.19ms
step:885/2330 train_time:50618ms step_avg:57.20ms
step:886/2330 train_time:50677ms step_avg:57.20ms
step:887/2330 train_time:50734ms step_avg:57.20ms
step:888/2330 train_time:50793ms step_avg:57.20ms
step:889/2330 train_time:50850ms step_avg:57.20ms
step:890/2330 train_time:50909ms step_avg:57.20ms
step:891/2330 train_time:50965ms step_avg:57.20ms
step:892/2330 train_time:51024ms step_avg:57.20ms
step:893/2330 train_time:51080ms step_avg:57.20ms
step:894/2330 train_time:51140ms step_avg:57.20ms
step:895/2330 train_time:51197ms step_avg:57.20ms
step:896/2330 train_time:51257ms step_avg:57.21ms
step:897/2330 train_time:51313ms step_avg:57.20ms
step:898/2330 train_time:51373ms step_avg:57.21ms
step:899/2330 train_time:51429ms step_avg:57.21ms
step:900/2330 train_time:51489ms step_avg:57.21ms
step:901/2330 train_time:51545ms step_avg:57.21ms
step:902/2330 train_time:51605ms step_avg:57.21ms
step:903/2330 train_time:51660ms step_avg:57.21ms
step:904/2330 train_time:51721ms step_avg:57.21ms
step:905/2330 train_time:51777ms step_avg:57.21ms
step:906/2330 train_time:51838ms step_avg:57.22ms
step:907/2330 train_time:51895ms step_avg:57.22ms
step:908/2330 train_time:51954ms step_avg:57.22ms
step:909/2330 train_time:52011ms step_avg:57.22ms
step:910/2330 train_time:52070ms step_avg:57.22ms
step:911/2330 train_time:52126ms step_avg:57.22ms
step:912/2330 train_time:52185ms step_avg:57.22ms
step:913/2330 train_time:52241ms step_avg:57.22ms
step:914/2330 train_time:52302ms step_avg:57.22ms
step:915/2330 train_time:52358ms step_avg:57.22ms
step:916/2330 train_time:52418ms step_avg:57.22ms
step:917/2330 train_time:52474ms step_avg:57.22ms
step:918/2330 train_time:52534ms step_avg:57.23ms
step:919/2330 train_time:52590ms step_avg:57.22ms
step:920/2330 train_time:52649ms step_avg:57.23ms
step:921/2330 train_time:52705ms step_avg:57.23ms
step:922/2330 train_time:52764ms step_avg:57.23ms
step:923/2330 train_time:52820ms step_avg:57.23ms
step:924/2330 train_time:52881ms step_avg:57.23ms
step:925/2330 train_time:52938ms step_avg:57.23ms
step:926/2330 train_time:52998ms step_avg:57.23ms
step:927/2330 train_time:53054ms step_avg:57.23ms
step:928/2330 train_time:53114ms step_avg:57.23ms
step:929/2330 train_time:53170ms step_avg:57.23ms
step:930/2330 train_time:53229ms step_avg:57.24ms
step:931/2330 train_time:53285ms step_avg:57.23ms
step:932/2330 train_time:53345ms step_avg:57.24ms
step:933/2330 train_time:53401ms step_avg:57.24ms
step:934/2330 train_time:53461ms step_avg:57.24ms
step:935/2330 train_time:53517ms step_avg:57.24ms
step:936/2330 train_time:53577ms step_avg:57.24ms
step:937/2330 train_time:53633ms step_avg:57.24ms
step:938/2330 train_time:53693ms step_avg:57.24ms
step:939/2330 train_time:53749ms step_avg:57.24ms
step:940/2330 train_time:53809ms step_avg:57.24ms
step:941/2330 train_time:53865ms step_avg:57.24ms
step:942/2330 train_time:53923ms step_avg:57.24ms
step:943/2330 train_time:53980ms step_avg:57.24ms
step:944/2330 train_time:54040ms step_avg:57.25ms
step:945/2330 train_time:54096ms step_avg:57.24ms
step:946/2330 train_time:54156ms step_avg:57.25ms
step:947/2330 train_time:54213ms step_avg:57.25ms
step:948/2330 train_time:54272ms step_avg:57.25ms
step:949/2330 train_time:54328ms step_avg:57.25ms
step:950/2330 train_time:54388ms step_avg:57.25ms
step:951/2330 train_time:54444ms step_avg:57.25ms
step:952/2330 train_time:54503ms step_avg:57.25ms
step:953/2330 train_time:54559ms step_avg:57.25ms
step:954/2330 train_time:54620ms step_avg:57.25ms
step:955/2330 train_time:54676ms step_avg:57.25ms
step:956/2330 train_time:54736ms step_avg:57.26ms
step:957/2330 train_time:54793ms step_avg:57.26ms
step:958/2330 train_time:54853ms step_avg:57.26ms
step:959/2330 train_time:54910ms step_avg:57.26ms
step:960/2330 train_time:54969ms step_avg:57.26ms
step:961/2330 train_time:55025ms step_avg:57.26ms
step:962/2330 train_time:55084ms step_avg:57.26ms
step:963/2330 train_time:55141ms step_avg:57.26ms
step:964/2330 train_time:55201ms step_avg:57.26ms
step:965/2330 train_time:55257ms step_avg:57.26ms
step:966/2330 train_time:55317ms step_avg:57.26ms
step:967/2330 train_time:55373ms step_avg:57.26ms
step:968/2330 train_time:55433ms step_avg:57.27ms
step:969/2330 train_time:55489ms step_avg:57.26ms
step:970/2330 train_time:55549ms step_avg:57.27ms
step:971/2330 train_time:55606ms step_avg:57.27ms
step:972/2330 train_time:55664ms step_avg:57.27ms
step:973/2330 train_time:55721ms step_avg:57.27ms
step:974/2330 train_time:55781ms step_avg:57.27ms
step:975/2330 train_time:55838ms step_avg:57.27ms
step:976/2330 train_time:55898ms step_avg:57.27ms
step:977/2330 train_time:55955ms step_avg:57.27ms
step:978/2330 train_time:56014ms step_avg:57.27ms
step:979/2330 train_time:56070ms step_avg:57.27ms
step:980/2330 train_time:56129ms step_avg:57.27ms
step:981/2330 train_time:56186ms step_avg:57.27ms
step:982/2330 train_time:56246ms step_avg:57.28ms
step:983/2330 train_time:56302ms step_avg:57.28ms
step:984/2330 train_time:56362ms step_avg:57.28ms
step:985/2330 train_time:56418ms step_avg:57.28ms
step:986/2330 train_time:56478ms step_avg:57.28ms
step:987/2330 train_time:56535ms step_avg:57.28ms
step:988/2330 train_time:56595ms step_avg:57.28ms
step:989/2330 train_time:56652ms step_avg:57.28ms
step:990/2330 train_time:56711ms step_avg:57.28ms
step:991/2330 train_time:56768ms step_avg:57.28ms
step:992/2330 train_time:56827ms step_avg:57.29ms
step:993/2330 train_time:56883ms step_avg:57.28ms
step:994/2330 train_time:56944ms step_avg:57.29ms
step:995/2330 train_time:57000ms step_avg:57.29ms
step:996/2330 train_time:57060ms step_avg:57.29ms
step:997/2330 train_time:57117ms step_avg:57.29ms
step:998/2330 train_time:57176ms step_avg:57.29ms
step:999/2330 train_time:57233ms step_avg:57.29ms
step:1000/2330 train_time:57292ms step_avg:57.29ms
step:1000/2330 val_loss:4.4734 train_time:57371ms step_avg:57.37ms
step:1001/2330 train_time:57390ms step_avg:57.33ms
step:1002/2330 train_time:57410ms step_avg:57.30ms
step:1003/2330 train_time:57467ms step_avg:57.30ms
step:1004/2330 train_time:57530ms step_avg:57.30ms
step:1005/2330 train_time:57586ms step_avg:57.30ms
step:1006/2330 train_time:57648ms step_avg:57.30ms
step:1007/2330 train_time:57704ms step_avg:57.30ms
step:1008/2330 train_time:57762ms step_avg:57.30ms
step:1009/2330 train_time:57818ms step_avg:57.30ms
step:1010/2330 train_time:57877ms step_avg:57.30ms
step:1011/2330 train_time:57933ms step_avg:57.30ms
step:1012/2330 train_time:57992ms step_avg:57.30ms
step:1013/2330 train_time:58048ms step_avg:57.30ms
step:1014/2330 train_time:58106ms step_avg:57.30ms
step:1015/2330 train_time:58162ms step_avg:57.30ms
step:1016/2330 train_time:58220ms step_avg:57.30ms
step:1017/2330 train_time:58276ms step_avg:57.30ms
step:1018/2330 train_time:58336ms step_avg:57.30ms
step:1019/2330 train_time:58393ms step_avg:57.30ms
step:1020/2330 train_time:58455ms step_avg:57.31ms
step:1021/2330 train_time:58512ms step_avg:57.31ms
step:1022/2330 train_time:58571ms step_avg:57.31ms
step:1023/2330 train_time:58629ms step_avg:57.31ms
step:1024/2330 train_time:58688ms step_avg:57.31ms
step:1025/2330 train_time:58744ms step_avg:57.31ms
step:1026/2330 train_time:58803ms step_avg:57.31ms
step:1027/2330 train_time:58859ms step_avg:57.31ms
step:1028/2330 train_time:58918ms step_avg:57.31ms
step:1029/2330 train_time:58974ms step_avg:57.31ms
step:1030/2330 train_time:59034ms step_avg:57.31ms
step:1031/2330 train_time:59091ms step_avg:57.31ms
step:1032/2330 train_time:59150ms step_avg:57.32ms
step:1033/2330 train_time:59206ms step_avg:57.31ms
step:1034/2330 train_time:59264ms step_avg:57.32ms
step:1035/2330 train_time:59320ms step_avg:57.31ms
step:1036/2330 train_time:59381ms step_avg:57.32ms
step:1037/2330 train_time:59437ms step_avg:57.32ms
step:1038/2330 train_time:59498ms step_avg:57.32ms
step:1039/2330 train_time:59555ms step_avg:57.32ms
step:1040/2330 train_time:59615ms step_avg:57.32ms
step:1041/2330 train_time:59672ms step_avg:57.32ms
step:1042/2330 train_time:59732ms step_avg:57.32ms
step:1043/2330 train_time:59789ms step_avg:57.32ms
step:1044/2330 train_time:59849ms step_avg:57.33ms
step:1045/2330 train_time:59905ms step_avg:57.33ms
step:1046/2330 train_time:59965ms step_avg:57.33ms
step:1047/2330 train_time:60020ms step_avg:57.33ms
step:1048/2330 train_time:60080ms step_avg:57.33ms
step:1049/2330 train_time:60136ms step_avg:57.33ms
step:1050/2330 train_time:60196ms step_avg:57.33ms
step:1051/2330 train_time:60251ms step_avg:57.33ms
step:1052/2330 train_time:60311ms step_avg:57.33ms
step:1053/2330 train_time:60367ms step_avg:57.33ms
step:1054/2330 train_time:60428ms step_avg:57.33ms
step:1055/2330 train_time:60484ms step_avg:57.33ms
step:1056/2330 train_time:60543ms step_avg:57.33ms
step:1057/2330 train_time:60599ms step_avg:57.33ms
step:1058/2330 train_time:60659ms step_avg:57.33ms
step:1059/2330 train_time:60715ms step_avg:57.33ms
step:1060/2330 train_time:60776ms step_avg:57.34ms
step:1061/2330 train_time:60832ms step_avg:57.33ms
step:1062/2330 train_time:60893ms step_avg:57.34ms
step:1063/2330 train_time:60950ms step_avg:57.34ms
step:1064/2330 train_time:61009ms step_avg:57.34ms
step:1065/2330 train_time:61065ms step_avg:57.34ms
step:1066/2330 train_time:61123ms step_avg:57.34ms
step:1067/2330 train_time:61179ms step_avg:57.34ms
step:1068/2330 train_time:61239ms step_avg:57.34ms
step:1069/2330 train_time:61295ms step_avg:57.34ms
step:1070/2330 train_time:61355ms step_avg:57.34ms
step:1071/2330 train_time:61412ms step_avg:57.34ms
step:1072/2330 train_time:61471ms step_avg:57.34ms
step:1073/2330 train_time:61527ms step_avg:57.34ms
step:1074/2330 train_time:61587ms step_avg:57.34ms
step:1075/2330 train_time:61644ms step_avg:57.34ms
step:1076/2330 train_time:61703ms step_avg:57.34ms
step:1077/2330 train_time:61759ms step_avg:57.34ms
step:1078/2330 train_time:61818ms step_avg:57.35ms
step:1079/2330 train_time:61874ms step_avg:57.34ms
step:1080/2330 train_time:61935ms step_avg:57.35ms
step:1081/2330 train_time:61991ms step_avg:57.35ms
step:1082/2330 train_time:62052ms step_avg:57.35ms
step:1083/2330 train_time:62108ms step_avg:57.35ms
step:1084/2330 train_time:62168ms step_avg:57.35ms
step:1085/2330 train_time:62223ms step_avg:57.35ms
step:1086/2330 train_time:62284ms step_avg:57.35ms
step:1087/2330 train_time:62340ms step_avg:57.35ms
step:1088/2330 train_time:62399ms step_avg:57.35ms
step:1089/2330 train_time:62455ms step_avg:57.35ms
step:1090/2330 train_time:62516ms step_avg:57.35ms
step:1091/2330 train_time:62572ms step_avg:57.35ms
step:1092/2330 train_time:62631ms step_avg:57.35ms
step:1093/2330 train_time:62688ms step_avg:57.35ms
step:1094/2330 train_time:62748ms step_avg:57.36ms
step:1095/2330 train_time:62803ms step_avg:57.35ms
step:1096/2330 train_time:62864ms step_avg:57.36ms
step:1097/2330 train_time:62920ms step_avg:57.36ms
step:1098/2330 train_time:62980ms step_avg:57.36ms
step:1099/2330 train_time:63036ms step_avg:57.36ms
step:1100/2330 train_time:63096ms step_avg:57.36ms
step:1101/2330 train_time:63153ms step_avg:57.36ms
step:1102/2330 train_time:63213ms step_avg:57.36ms
step:1103/2330 train_time:63269ms step_avg:57.36ms
step:1104/2330 train_time:63329ms step_avg:57.36ms
step:1105/2330 train_time:63385ms step_avg:57.36ms
step:1106/2330 train_time:63445ms step_avg:57.36ms
step:1107/2330 train_time:63501ms step_avg:57.36ms
step:1108/2330 train_time:63560ms step_avg:57.36ms
step:1109/2330 train_time:63616ms step_avg:57.36ms
step:1110/2330 train_time:63677ms step_avg:57.37ms
step:1111/2330 train_time:63733ms step_avg:57.37ms
step:1112/2330 train_time:63793ms step_avg:57.37ms
step:1113/2330 train_time:63850ms step_avg:57.37ms
step:1114/2330 train_time:63909ms step_avg:57.37ms
step:1115/2330 train_time:63966ms step_avg:57.37ms
step:1116/2330 train_time:64025ms step_avg:57.37ms
step:1117/2330 train_time:64081ms step_avg:57.37ms
step:1118/2330 train_time:64141ms step_avg:57.37ms
step:1119/2330 train_time:64196ms step_avg:57.37ms
step:1120/2330 train_time:64256ms step_avg:57.37ms
step:1121/2330 train_time:64312ms step_avg:57.37ms
step:1122/2330 train_time:64372ms step_avg:57.37ms
step:1123/2330 train_time:64428ms step_avg:57.37ms
step:1124/2330 train_time:64490ms step_avg:57.38ms
step:1125/2330 train_time:64546ms step_avg:57.37ms
step:1126/2330 train_time:64606ms step_avg:57.38ms
step:1127/2330 train_time:64662ms step_avg:57.38ms
step:1128/2330 train_time:64721ms step_avg:57.38ms
step:1129/2330 train_time:64777ms step_avg:57.38ms
step:1130/2330 train_time:64837ms step_avg:57.38ms
step:1131/2330 train_time:64893ms step_avg:57.38ms
step:1132/2330 train_time:64953ms step_avg:57.38ms
step:1133/2330 train_time:65010ms step_avg:57.38ms
step:1134/2330 train_time:65070ms step_avg:57.38ms
step:1135/2330 train_time:65126ms step_avg:57.38ms
step:1136/2330 train_time:65185ms step_avg:57.38ms
step:1137/2330 train_time:65241ms step_avg:57.38ms
step:1138/2330 train_time:65301ms step_avg:57.38ms
step:1139/2330 train_time:65357ms step_avg:57.38ms
step:1140/2330 train_time:65417ms step_avg:57.38ms
step:1141/2330 train_time:65473ms step_avg:57.38ms
step:1142/2330 train_time:65533ms step_avg:57.38ms
step:1143/2330 train_time:65589ms step_avg:57.38ms
step:1144/2330 train_time:65650ms step_avg:57.39ms
step:1145/2330 train_time:65707ms step_avg:57.39ms
step:1146/2330 train_time:65766ms step_avg:57.39ms
step:1147/2330 train_time:65821ms step_avg:57.39ms
step:1148/2330 train_time:65881ms step_avg:57.39ms
step:1149/2330 train_time:65937ms step_avg:57.39ms
step:1150/2330 train_time:65998ms step_avg:57.39ms
step:1151/2330 train_time:66054ms step_avg:57.39ms
step:1152/2330 train_time:66114ms step_avg:57.39ms
step:1153/2330 train_time:66170ms step_avg:57.39ms
step:1154/2330 train_time:66229ms step_avg:57.39ms
step:1155/2330 train_time:66286ms step_avg:57.39ms
step:1156/2330 train_time:66345ms step_avg:57.39ms
step:1157/2330 train_time:66401ms step_avg:57.39ms
step:1158/2330 train_time:66461ms step_avg:57.39ms
step:1159/2330 train_time:66516ms step_avg:57.39ms
step:1160/2330 train_time:66576ms step_avg:57.39ms
step:1161/2330 train_time:66632ms step_avg:57.39ms
step:1162/2330 train_time:66693ms step_avg:57.40ms
step:1163/2330 train_time:66750ms step_avg:57.39ms
step:1164/2330 train_time:66810ms step_avg:57.40ms
step:1165/2330 train_time:66866ms step_avg:57.40ms
step:1166/2330 train_time:66925ms step_avg:57.40ms
step:1167/2330 train_time:66981ms step_avg:57.40ms
step:1168/2330 train_time:67040ms step_avg:57.40ms
step:1169/2330 train_time:67097ms step_avg:57.40ms
step:1170/2330 train_time:67156ms step_avg:57.40ms
step:1171/2330 train_time:67213ms step_avg:57.40ms
step:1172/2330 train_time:67273ms step_avg:57.40ms
step:1173/2330 train_time:67329ms step_avg:57.40ms
step:1174/2330 train_time:67389ms step_avg:57.40ms
step:1175/2330 train_time:67446ms step_avg:57.40ms
step:1176/2330 train_time:67505ms step_avg:57.40ms
step:1177/2330 train_time:67560ms step_avg:57.40ms
step:1178/2330 train_time:67620ms step_avg:57.40ms
step:1179/2330 train_time:67676ms step_avg:57.40ms
step:1180/2330 train_time:67737ms step_avg:57.40ms
step:1181/2330 train_time:67793ms step_avg:57.40ms
step:1182/2330 train_time:67853ms step_avg:57.41ms
step:1183/2330 train_time:67909ms step_avg:57.40ms
step:1184/2330 train_time:67969ms step_avg:57.41ms
step:1185/2330 train_time:68025ms step_avg:57.41ms
step:1186/2330 train_time:68084ms step_avg:57.41ms
step:1187/2330 train_time:68140ms step_avg:57.40ms
step:1188/2330 train_time:68199ms step_avg:57.41ms
step:1189/2330 train_time:68255ms step_avg:57.41ms
step:1190/2330 train_time:68315ms step_avg:57.41ms
step:1191/2330 train_time:68371ms step_avg:57.41ms
step:1192/2330 train_time:68432ms step_avg:57.41ms
step:1193/2330 train_time:68488ms step_avg:57.41ms
step:1194/2330 train_time:68548ms step_avg:57.41ms
step:1195/2330 train_time:68604ms step_avg:57.41ms
step:1196/2330 train_time:68663ms step_avg:57.41ms
step:1197/2330 train_time:68719ms step_avg:57.41ms
step:1198/2330 train_time:68779ms step_avg:57.41ms
step:1199/2330 train_time:68835ms step_avg:57.41ms
step:1200/2330 train_time:68895ms step_avg:57.41ms
step:1201/2330 train_time:68952ms step_avg:57.41ms
step:1202/2330 train_time:69012ms step_avg:57.41ms
step:1203/2330 train_time:69068ms step_avg:57.41ms
step:1204/2330 train_time:69128ms step_avg:57.41ms
step:1205/2330 train_time:69183ms step_avg:57.41ms
step:1206/2330 train_time:69243ms step_avg:57.42ms
step:1207/2330 train_time:69299ms step_avg:57.41ms
step:1208/2330 train_time:69359ms step_avg:57.42ms
step:1209/2330 train_time:69415ms step_avg:57.42ms
step:1210/2330 train_time:69475ms step_avg:57.42ms
step:1211/2330 train_time:69531ms step_avg:57.42ms
step:1212/2330 train_time:69591ms step_avg:57.42ms
step:1213/2330 train_time:69647ms step_avg:57.42ms
step:1214/2330 train_time:69708ms step_avg:57.42ms
step:1215/2330 train_time:69764ms step_avg:57.42ms
step:1216/2330 train_time:69823ms step_avg:57.42ms
step:1217/2330 train_time:69878ms step_avg:57.42ms
step:1218/2330 train_time:69939ms step_avg:57.42ms
step:1219/2330 train_time:69995ms step_avg:57.42ms
step:1220/2330 train_time:70055ms step_avg:57.42ms
step:1221/2330 train_time:70112ms step_avg:57.42ms
step:1222/2330 train_time:70173ms step_avg:57.42ms
step:1223/2330 train_time:70229ms step_avg:57.42ms
step:1224/2330 train_time:70290ms step_avg:57.43ms
step:1225/2330 train_time:70345ms step_avg:57.42ms
step:1226/2330 train_time:70406ms step_avg:57.43ms
step:1227/2330 train_time:70463ms step_avg:57.43ms
step:1228/2330 train_time:70521ms step_avg:57.43ms
step:1229/2330 train_time:70577ms step_avg:57.43ms
step:1230/2330 train_time:70638ms step_avg:57.43ms
step:1231/2330 train_time:70695ms step_avg:57.43ms
step:1232/2330 train_time:70754ms step_avg:57.43ms
step:1233/2330 train_time:70811ms step_avg:57.43ms
step:1234/2330 train_time:70870ms step_avg:57.43ms
step:1235/2330 train_time:70926ms step_avg:57.43ms
step:1236/2330 train_time:70985ms step_avg:57.43ms
step:1237/2330 train_time:71041ms step_avg:57.43ms
step:1238/2330 train_time:71100ms step_avg:57.43ms
step:1239/2330 train_time:71156ms step_avg:57.43ms
step:1240/2330 train_time:71217ms step_avg:57.43ms
step:1241/2330 train_time:71273ms step_avg:57.43ms
step:1242/2330 train_time:71334ms step_avg:57.43ms
step:1243/2330 train_time:71390ms step_avg:57.43ms
step:1244/2330 train_time:71450ms step_avg:57.44ms
step:1245/2330 train_time:71506ms step_avg:57.43ms
step:1246/2330 train_time:71566ms step_avg:57.44ms
step:1247/2330 train_time:71622ms step_avg:57.44ms
step:1248/2330 train_time:71681ms step_avg:57.44ms
step:1249/2330 train_time:71736ms step_avg:57.44ms
step:1250/2330 train_time:71797ms step_avg:57.44ms
step:1250/2330 val_loss:4.3388 train_time:71877ms step_avg:57.50ms
step:1251/2330 train_time:71895ms step_avg:57.47ms
step:1252/2330 train_time:71916ms step_avg:57.44ms
step:1253/2330 train_time:71973ms step_avg:57.44ms
step:1254/2330 train_time:72036ms step_avg:57.45ms
step:1255/2330 train_time:72092ms step_avg:57.44ms
step:1256/2330 train_time:72157ms step_avg:57.45ms
step:1257/2330 train_time:72212ms step_avg:57.45ms
step:1258/2330 train_time:72273ms step_avg:57.45ms
step:1259/2330 train_time:72328ms step_avg:57.45ms
step:1260/2330 train_time:72389ms step_avg:57.45ms
step:1261/2330 train_time:72445ms step_avg:57.45ms
step:1262/2330 train_time:72503ms step_avg:57.45ms
step:1263/2330 train_time:72559ms step_avg:57.45ms
step:1264/2330 train_time:72618ms step_avg:57.45ms
step:1265/2330 train_time:72673ms step_avg:57.45ms
step:1266/2330 train_time:72733ms step_avg:57.45ms
step:1267/2330 train_time:72788ms step_avg:57.45ms
step:1268/2330 train_time:72848ms step_avg:57.45ms
step:1269/2330 train_time:72906ms step_avg:57.45ms
step:1270/2330 train_time:72966ms step_avg:57.45ms
step:1271/2330 train_time:73024ms step_avg:57.45ms
step:1272/2330 train_time:73084ms step_avg:57.46ms
step:1273/2330 train_time:73141ms step_avg:57.46ms
step:1274/2330 train_time:73200ms step_avg:57.46ms
step:1275/2330 train_time:73256ms step_avg:57.46ms
step:1276/2330 train_time:73317ms step_avg:57.46ms
step:1277/2330 train_time:73372ms step_avg:57.46ms
step:1278/2330 train_time:73432ms step_avg:57.46ms
step:1279/2330 train_time:73488ms step_avg:57.46ms
step:1280/2330 train_time:73547ms step_avg:57.46ms
step:1281/2330 train_time:73603ms step_avg:57.46ms
step:1282/2330 train_time:73661ms step_avg:57.46ms
step:1283/2330 train_time:73717ms step_avg:57.46ms
step:1284/2330 train_time:73776ms step_avg:57.46ms
step:1285/2330 train_time:73831ms step_avg:57.46ms
step:1286/2330 train_time:73892ms step_avg:57.46ms
step:1287/2330 train_time:73948ms step_avg:57.46ms
step:1288/2330 train_time:74009ms step_avg:57.46ms
step:1289/2330 train_time:74066ms step_avg:57.46ms
step:1290/2330 train_time:74127ms step_avg:57.46ms
step:1291/2330 train_time:74184ms step_avg:57.46ms
step:1292/2330 train_time:74244ms step_avg:57.46ms
step:1293/2330 train_time:74301ms step_avg:57.46ms
step:1294/2330 train_time:74360ms step_avg:57.46ms
step:1295/2330 train_time:74416ms step_avg:57.46ms
step:1296/2330 train_time:74475ms step_avg:57.47ms
step:1297/2330 train_time:74531ms step_avg:57.46ms
step:1298/2330 train_time:74590ms step_avg:57.47ms
step:1299/2330 train_time:74646ms step_avg:57.46ms
step:1300/2330 train_time:74707ms step_avg:57.47ms
step:1301/2330 train_time:74763ms step_avg:57.47ms
step:1302/2330 train_time:74822ms step_avg:57.47ms
step:1303/2330 train_time:74878ms step_avg:57.47ms
step:1304/2330 train_time:74938ms step_avg:57.47ms
step:1305/2330 train_time:74993ms step_avg:57.47ms
step:1306/2330 train_time:75055ms step_avg:57.47ms
step:1307/2330 train_time:75111ms step_avg:57.47ms
step:1308/2330 train_time:75171ms step_avg:57.47ms
step:1309/2330 train_time:75228ms step_avg:57.47ms
step:1310/2330 train_time:75288ms step_avg:57.47ms
step:1311/2330 train_time:75345ms step_avg:57.47ms
step:1312/2330 train_time:75404ms step_avg:57.47ms
step:1313/2330 train_time:75460ms step_avg:57.47ms
step:1314/2330 train_time:75519ms step_avg:57.47ms
step:1315/2330 train_time:75575ms step_avg:57.47ms
step:1316/2330 train_time:75634ms step_avg:57.47ms
step:1317/2330 train_time:75691ms step_avg:57.47ms
step:1318/2330 train_time:75750ms step_avg:57.47ms
step:1319/2330 train_time:75807ms step_avg:57.47ms
step:1320/2330 train_time:75867ms step_avg:57.48ms
step:1321/2330 train_time:75924ms step_avg:57.47ms
step:1322/2330 train_time:75985ms step_avg:57.48ms
step:1323/2330 train_time:76041ms step_avg:57.48ms
step:1324/2330 train_time:76100ms step_avg:57.48ms
step:1325/2330 train_time:76156ms step_avg:57.48ms
step:1326/2330 train_time:76216ms step_avg:57.48ms
step:1327/2330 train_time:76272ms step_avg:57.48ms
step:1328/2330 train_time:76332ms step_avg:57.48ms
step:1329/2330 train_time:76389ms step_avg:57.48ms
step:1330/2330 train_time:76448ms step_avg:57.48ms
step:1331/2330 train_time:76505ms step_avg:57.48ms
step:1332/2330 train_time:76564ms step_avg:57.48ms
step:1333/2330 train_time:76620ms step_avg:57.48ms
step:1334/2330 train_time:76679ms step_avg:57.48ms
step:1335/2330 train_time:76735ms step_avg:57.48ms
step:1336/2330 train_time:76795ms step_avg:57.48ms
step:1337/2330 train_time:76851ms step_avg:57.48ms
step:1338/2330 train_time:76911ms step_avg:57.48ms
step:1339/2330 train_time:76967ms step_avg:57.48ms
step:1340/2330 train_time:77028ms step_avg:57.48ms
step:1341/2330 train_time:77085ms step_avg:57.48ms
step:1342/2330 train_time:77145ms step_avg:57.48ms
step:1343/2330 train_time:77201ms step_avg:57.48ms
step:1344/2330 train_time:77261ms step_avg:57.49ms
step:1345/2330 train_time:77317ms step_avg:57.48ms
step:1346/2330 train_time:77376ms step_avg:57.49ms
step:1347/2330 train_time:77433ms step_avg:57.49ms
step:1348/2330 train_time:77492ms step_avg:57.49ms
step:1349/2330 train_time:77548ms step_avg:57.49ms
step:1350/2330 train_time:77608ms step_avg:57.49ms
step:1351/2330 train_time:77665ms step_avg:57.49ms
step:1352/2330 train_time:77724ms step_avg:57.49ms
step:1353/2330 train_time:77780ms step_avg:57.49ms
step:1354/2330 train_time:77840ms step_avg:57.49ms
step:1355/2330 train_time:77896ms step_avg:57.49ms
step:1356/2330 train_time:77956ms step_avg:57.49ms
step:1357/2330 train_time:78012ms step_avg:57.49ms
step:1358/2330 train_time:78072ms step_avg:57.49ms
step:1359/2330 train_time:78128ms step_avg:57.49ms
step:1360/2330 train_time:78189ms step_avg:57.49ms
step:1361/2330 train_time:78246ms step_avg:57.49ms
step:1362/2330 train_time:78306ms step_avg:57.49ms
step:1363/2330 train_time:78363ms step_avg:57.49ms
step:1364/2330 train_time:78421ms step_avg:57.49ms
step:1365/2330 train_time:78477ms step_avg:57.49ms
step:1366/2330 train_time:78536ms step_avg:57.49ms
step:1367/2330 train_time:78592ms step_avg:57.49ms
step:1368/2330 train_time:78652ms step_avg:57.49ms
step:1369/2330 train_time:78708ms step_avg:57.49ms
step:1370/2330 train_time:78768ms step_avg:57.50ms
step:1371/2330 train_time:78825ms step_avg:57.49ms
step:1372/2330 train_time:78885ms step_avg:57.50ms
step:1373/2330 train_time:78942ms step_avg:57.50ms
step:1374/2330 train_time:79000ms step_avg:57.50ms
step:1375/2330 train_time:79056ms step_avg:57.50ms
step:1376/2330 train_time:79116ms step_avg:57.50ms
step:1377/2330 train_time:79172ms step_avg:57.50ms
step:1378/2330 train_time:79232ms step_avg:57.50ms
step:1379/2330 train_time:79289ms step_avg:57.50ms
step:1380/2330 train_time:79349ms step_avg:57.50ms
step:1381/2330 train_time:79406ms step_avg:57.50ms
step:1382/2330 train_time:79466ms step_avg:57.50ms
step:1383/2330 train_time:79523ms step_avg:57.50ms
step:1384/2330 train_time:79582ms step_avg:57.50ms
step:1385/2330 train_time:79639ms step_avg:57.50ms
step:1386/2330 train_time:79698ms step_avg:57.50ms
step:1387/2330 train_time:79753ms step_avg:57.50ms
step:1388/2330 train_time:79813ms step_avg:57.50ms
step:1389/2330 train_time:79869ms step_avg:57.50ms
step:1390/2330 train_time:79929ms step_avg:57.50ms
step:1391/2330 train_time:79986ms step_avg:57.50ms
step:1392/2330 train_time:80046ms step_avg:57.50ms
step:1393/2330 train_time:80102ms step_avg:57.50ms
step:1394/2330 train_time:80162ms step_avg:57.50ms
step:1395/2330 train_time:80218ms step_avg:57.50ms
step:1396/2330 train_time:80278ms step_avg:57.51ms
step:1397/2330 train_time:80334ms step_avg:57.50ms
step:1398/2330 train_time:80395ms step_avg:57.51ms
step:1399/2330 train_time:80451ms step_avg:57.51ms
step:1400/2330 train_time:80511ms step_avg:57.51ms
step:1401/2330 train_time:80568ms step_avg:57.51ms
step:1402/2330 train_time:80628ms step_avg:57.51ms
step:1403/2330 train_time:80684ms step_avg:57.51ms
step:1404/2330 train_time:80744ms step_avg:57.51ms
step:1405/2330 train_time:80800ms step_avg:57.51ms
step:1406/2330 train_time:80860ms step_avg:57.51ms
step:1407/2330 train_time:80916ms step_avg:57.51ms
step:1408/2330 train_time:80976ms step_avg:57.51ms
step:1409/2330 train_time:81032ms step_avg:57.51ms
step:1410/2330 train_time:81092ms step_avg:57.51ms
step:1411/2330 train_time:81148ms step_avg:57.51ms
step:1412/2330 train_time:81208ms step_avg:57.51ms
step:1413/2330 train_time:81265ms step_avg:57.51ms
step:1414/2330 train_time:81325ms step_avg:57.51ms
step:1415/2330 train_time:81381ms step_avg:57.51ms
step:1416/2330 train_time:81441ms step_avg:57.51ms
step:1417/2330 train_time:81497ms step_avg:57.51ms
step:1418/2330 train_time:81556ms step_avg:57.51ms
step:1419/2330 train_time:81612ms step_avg:57.51ms
step:1420/2330 train_time:81671ms step_avg:57.51ms
step:1421/2330 train_time:81728ms step_avg:57.51ms
step:1422/2330 train_time:81789ms step_avg:57.52ms
step:1423/2330 train_time:81845ms step_avg:57.52ms
step:1424/2330 train_time:81905ms step_avg:57.52ms
step:1425/2330 train_time:81961ms step_avg:57.52ms
step:1426/2330 train_time:82021ms step_avg:57.52ms
step:1427/2330 train_time:82077ms step_avg:57.52ms
step:1428/2330 train_time:82137ms step_avg:57.52ms
step:1429/2330 train_time:82193ms step_avg:57.52ms
step:1430/2330 train_time:82253ms step_avg:57.52ms
step:1431/2330 train_time:82309ms step_avg:57.52ms
step:1432/2330 train_time:82370ms step_avg:57.52ms
step:1433/2330 train_time:82426ms step_avg:57.52ms
step:1434/2330 train_time:82487ms step_avg:57.52ms
step:1435/2330 train_time:82543ms step_avg:57.52ms
step:1436/2330 train_time:82603ms step_avg:57.52ms
step:1437/2330 train_time:82659ms step_avg:57.52ms
step:1438/2330 train_time:82719ms step_avg:57.52ms
step:1439/2330 train_time:82775ms step_avg:57.52ms
step:1440/2330 train_time:82834ms step_avg:57.52ms
step:1441/2330 train_time:82891ms step_avg:57.52ms
step:1442/2330 train_time:82951ms step_avg:57.52ms
step:1443/2330 train_time:83008ms step_avg:57.52ms
step:1444/2330 train_time:83067ms step_avg:57.53ms
step:1445/2330 train_time:83124ms step_avg:57.53ms
step:1446/2330 train_time:83184ms step_avg:57.53ms
step:1447/2330 train_time:83240ms step_avg:57.53ms
step:1448/2330 train_time:83299ms step_avg:57.53ms
step:1449/2330 train_time:83355ms step_avg:57.53ms
step:1450/2330 train_time:83415ms step_avg:57.53ms
step:1451/2330 train_time:83471ms step_avg:57.53ms
step:1452/2330 train_time:83532ms step_avg:57.53ms
step:1453/2330 train_time:83589ms step_avg:57.53ms
step:1454/2330 train_time:83648ms step_avg:57.53ms
step:1455/2330 train_time:83704ms step_avg:57.53ms
step:1456/2330 train_time:83763ms step_avg:57.53ms
step:1457/2330 train_time:83819ms step_avg:57.53ms
step:1458/2330 train_time:83879ms step_avg:57.53ms
step:1459/2330 train_time:83936ms step_avg:57.53ms
step:1460/2330 train_time:83995ms step_avg:57.53ms
step:1461/2330 train_time:84051ms step_avg:57.53ms
step:1462/2330 train_time:84112ms step_avg:57.53ms
step:1463/2330 train_time:84168ms step_avg:57.53ms
step:1464/2330 train_time:84228ms step_avg:57.53ms
step:1465/2330 train_time:84285ms step_avg:57.53ms
step:1466/2330 train_time:84346ms step_avg:57.53ms
step:1467/2330 train_time:84402ms step_avg:57.53ms
step:1468/2330 train_time:84461ms step_avg:57.53ms
step:1469/2330 train_time:84518ms step_avg:57.53ms
step:1470/2330 train_time:84577ms step_avg:57.54ms
step:1471/2330 train_time:84633ms step_avg:57.53ms
step:1472/2330 train_time:84693ms step_avg:57.54ms
step:1473/2330 train_time:84749ms step_avg:57.54ms
step:1474/2330 train_time:84809ms step_avg:57.54ms
step:1475/2330 train_time:84866ms step_avg:57.54ms
step:1476/2330 train_time:84925ms step_avg:57.54ms
step:1477/2330 train_time:84982ms step_avg:57.54ms
step:1478/2330 train_time:85041ms step_avg:57.54ms
step:1479/2330 train_time:85097ms step_avg:57.54ms
step:1480/2330 train_time:85156ms step_avg:57.54ms
step:1481/2330 train_time:85211ms step_avg:57.54ms
step:1482/2330 train_time:85272ms step_avg:57.54ms
step:1483/2330 train_time:85328ms step_avg:57.54ms
step:1484/2330 train_time:85388ms step_avg:57.54ms
step:1485/2330 train_time:85446ms step_avg:57.54ms
step:1486/2330 train_time:85505ms step_avg:57.54ms
step:1487/2330 train_time:85561ms step_avg:57.54ms
step:1488/2330 train_time:85619ms step_avg:57.54ms
step:1489/2330 train_time:85676ms step_avg:57.54ms
step:1490/2330 train_time:85735ms step_avg:57.54ms
step:1491/2330 train_time:85791ms step_avg:57.54ms
step:1492/2330 train_time:85851ms step_avg:57.54ms
step:1493/2330 train_time:85908ms step_avg:57.54ms
step:1494/2330 train_time:85968ms step_avg:57.54ms
step:1495/2330 train_time:86025ms step_avg:57.54ms
step:1496/2330 train_time:86085ms step_avg:57.54ms
step:1497/2330 train_time:86141ms step_avg:57.54ms
step:1498/2330 train_time:86200ms step_avg:57.54ms
step:1499/2330 train_time:86256ms step_avg:57.54ms
step:1500/2330 train_time:86315ms step_avg:57.54ms
step:1500/2330 val_loss:4.2256 train_time:86396ms step_avg:57.60ms
step:1501/2330 train_time:86416ms step_avg:57.57ms
step:1502/2330 train_time:86437ms step_avg:57.55ms
step:1503/2330 train_time:86492ms step_avg:57.55ms
step:1504/2330 train_time:86554ms step_avg:57.55ms
step:1505/2330 train_time:86611ms step_avg:57.55ms
step:1506/2330 train_time:86671ms step_avg:57.55ms
step:1507/2330 train_time:86727ms step_avg:57.55ms
step:1508/2330 train_time:86786ms step_avg:57.55ms
step:1509/2330 train_time:86842ms step_avg:57.55ms
step:1510/2330 train_time:86901ms step_avg:57.55ms
step:1511/2330 train_time:86957ms step_avg:57.55ms
step:1512/2330 train_time:87016ms step_avg:57.55ms
step:1513/2330 train_time:87072ms step_avg:57.55ms
step:1514/2330 train_time:87130ms step_avg:57.55ms
step:1515/2330 train_time:87186ms step_avg:57.55ms
step:1516/2330 train_time:87245ms step_avg:57.55ms
step:1517/2330 train_time:87301ms step_avg:57.55ms
step:1518/2330 train_time:87361ms step_avg:57.55ms
step:1519/2330 train_time:87418ms step_avg:57.55ms
step:1520/2330 train_time:87480ms step_avg:57.55ms
step:1521/2330 train_time:87536ms step_avg:57.55ms
step:1522/2330 train_time:87599ms step_avg:57.56ms
step:1523/2330 train_time:87655ms step_avg:57.55ms
step:1524/2330 train_time:87715ms step_avg:57.56ms
step:1525/2330 train_time:87772ms step_avg:57.56ms
step:1526/2330 train_time:87830ms step_avg:57.56ms
step:1527/2330 train_time:87886ms step_avg:57.55ms
step:1528/2330 train_time:87946ms step_avg:57.56ms
step:1529/2330 train_time:88004ms step_avg:57.56ms
step:1530/2330 train_time:88062ms step_avg:57.56ms
step:1531/2330 train_time:88119ms step_avg:57.56ms
step:1532/2330 train_time:88179ms step_avg:57.56ms
step:1533/2330 train_time:88235ms step_avg:57.56ms
step:1534/2330 train_time:88295ms step_avg:57.56ms
step:1535/2330 train_time:88352ms step_avg:57.56ms
step:1536/2330 train_time:88411ms step_avg:57.56ms
step:1537/2330 train_time:88468ms step_avg:57.56ms
step:1538/2330 train_time:88530ms step_avg:57.56ms
step:1539/2330 train_time:88587ms step_avg:57.56ms
step:1540/2330 train_time:88648ms step_avg:57.56ms
step:1541/2330 train_time:88705ms step_avg:57.56ms
step:1542/2330 train_time:88765ms step_avg:57.57ms
step:1543/2330 train_time:88822ms step_avg:57.56ms
step:1544/2330 train_time:88882ms step_avg:57.57ms
step:1545/2330 train_time:88938ms step_avg:57.57ms
step:1546/2330 train_time:88998ms step_avg:57.57ms
step:1547/2330 train_time:89055ms step_avg:57.57ms
step:1548/2330 train_time:89115ms step_avg:57.57ms
step:1549/2330 train_time:89171ms step_avg:57.57ms
step:1550/2330 train_time:89231ms step_avg:57.57ms
step:1551/2330 train_time:89287ms step_avg:57.57ms
step:1552/2330 train_time:89346ms step_avg:57.57ms
step:1553/2330 train_time:89403ms step_avg:57.57ms
step:1554/2330 train_time:89464ms step_avg:57.57ms
step:1555/2330 train_time:89521ms step_avg:57.57ms
step:1556/2330 train_time:89581ms step_avg:57.57ms
step:1557/2330 train_time:89639ms step_avg:57.57ms
step:1558/2330 train_time:89698ms step_avg:57.57ms
step:1559/2330 train_time:89755ms step_avg:57.57ms
step:1560/2330 train_time:89815ms step_avg:57.57ms
step:1561/2330 train_time:89872ms step_avg:57.57ms
step:1562/2330 train_time:89931ms step_avg:57.57ms
step:1563/2330 train_time:89988ms step_avg:57.57ms
step:1564/2330 train_time:90047ms step_avg:57.57ms
step:1565/2330 train_time:90104ms step_avg:57.57ms
step:1566/2330 train_time:90165ms step_avg:57.58ms
step:1567/2330 train_time:90222ms step_avg:57.58ms
step:1568/2330 train_time:90281ms step_avg:57.58ms
step:1569/2330 train_time:90337ms step_avg:57.58ms
step:1570/2330 train_time:90397ms step_avg:57.58ms
step:1571/2330 train_time:90455ms step_avg:57.58ms
step:1572/2330 train_time:90514ms step_avg:57.58ms
step:1573/2330 train_time:90571ms step_avg:57.58ms
step:1574/2330 train_time:90631ms step_avg:57.58ms
step:1575/2330 train_time:90687ms step_avg:57.58ms
step:1576/2330 train_time:90749ms step_avg:57.58ms
step:1577/2330 train_time:90806ms step_avg:57.58ms
step:1578/2330 train_time:90866ms step_avg:57.58ms
step:1579/2330 train_time:90923ms step_avg:57.58ms
step:1580/2330 train_time:90983ms step_avg:57.58ms
step:1581/2330 train_time:91040ms step_avg:57.58ms
step:1582/2330 train_time:91100ms step_avg:57.59ms
step:1583/2330 train_time:91157ms step_avg:57.58ms
step:1584/2330 train_time:91217ms step_avg:57.59ms
step:1585/2330 train_time:91274ms step_avg:57.59ms
step:1586/2330 train_time:91333ms step_avg:57.59ms
step:1587/2330 train_time:91389ms step_avg:57.59ms
step:1588/2330 train_time:91450ms step_avg:57.59ms
step:1589/2330 train_time:91507ms step_avg:57.59ms
step:1590/2330 train_time:91568ms step_avg:57.59ms
step:1591/2330 train_time:91625ms step_avg:57.59ms
step:1592/2330 train_time:91684ms step_avg:57.59ms
step:1593/2330 train_time:91741ms step_avg:57.59ms
step:1594/2330 train_time:91802ms step_avg:57.59ms
step:1595/2330 train_time:91859ms step_avg:57.59ms
step:1596/2330 train_time:91919ms step_avg:57.59ms
step:1597/2330 train_time:91976ms step_avg:57.59ms
step:1598/2330 train_time:92036ms step_avg:57.59ms
step:1599/2330 train_time:92092ms step_avg:57.59ms
step:1600/2330 train_time:92152ms step_avg:57.59ms
step:1601/2330 train_time:92209ms step_avg:57.59ms
step:1602/2330 train_time:92268ms step_avg:57.60ms
step:1603/2330 train_time:92325ms step_avg:57.60ms
step:1604/2330 train_time:92386ms step_avg:57.60ms
step:1605/2330 train_time:92442ms step_avg:57.60ms
step:1606/2330 train_time:92503ms step_avg:57.60ms
step:1607/2330 train_time:92560ms step_avg:57.60ms
step:1608/2330 train_time:92620ms step_avg:57.60ms
step:1609/2330 train_time:92678ms step_avg:57.60ms
step:1610/2330 train_time:92737ms step_avg:57.60ms
step:1611/2330 train_time:92793ms step_avg:57.60ms
step:1612/2330 train_time:92853ms step_avg:57.60ms
step:1613/2330 train_time:92910ms step_avg:57.60ms
step:1614/2330 train_time:92970ms step_avg:57.60ms
step:1615/2330 train_time:93027ms step_avg:57.60ms
step:1616/2330 train_time:93087ms step_avg:57.60ms
step:1617/2330 train_time:93144ms step_avg:57.60ms
step:1618/2330 train_time:93204ms step_avg:57.60ms
step:1619/2330 train_time:93262ms step_avg:57.60ms
step:1620/2330 train_time:93322ms step_avg:57.61ms
step:1621/2330 train_time:93379ms step_avg:57.61ms
step:1622/2330 train_time:93439ms step_avg:57.61ms
step:1623/2330 train_time:93496ms step_avg:57.61ms
step:1624/2330 train_time:93556ms step_avg:57.61ms
step:1625/2330 train_time:93614ms step_avg:57.61ms
step:1626/2330 train_time:93674ms step_avg:57.61ms
step:1627/2330 train_time:93731ms step_avg:57.61ms
step:1628/2330 train_time:93790ms step_avg:57.61ms
step:1629/2330 train_time:93847ms step_avg:57.61ms
step:1630/2330 train_time:93907ms step_avg:57.61ms
step:1631/2330 train_time:93964ms step_avg:57.61ms
step:1632/2330 train_time:94024ms step_avg:57.61ms
step:1633/2330 train_time:94081ms step_avg:57.61ms
step:1634/2330 train_time:94140ms step_avg:57.61ms
step:1635/2330 train_time:94197ms step_avg:57.61ms
step:1636/2330 train_time:94257ms step_avg:57.61ms
step:1637/2330 train_time:94314ms step_avg:57.61ms
step:1638/2330 train_time:94374ms step_avg:57.62ms
step:1639/2330 train_time:94431ms step_avg:57.61ms
step:1640/2330 train_time:94490ms step_avg:57.62ms
step:1641/2330 train_time:94546ms step_avg:57.62ms
step:1642/2330 train_time:94608ms step_avg:57.62ms
step:1643/2330 train_time:94664ms step_avg:57.62ms
step:1644/2330 train_time:94725ms step_avg:57.62ms
step:1645/2330 train_time:94782ms step_avg:57.62ms
step:1646/2330 train_time:94843ms step_avg:57.62ms
step:1647/2330 train_time:94900ms step_avg:57.62ms
step:1648/2330 train_time:94961ms step_avg:57.62ms
step:1649/2330 train_time:95018ms step_avg:57.62ms
step:1650/2330 train_time:95078ms step_avg:57.62ms
step:1651/2330 train_time:95134ms step_avg:57.62ms
step:1652/2330 train_time:95194ms step_avg:57.62ms
step:1653/2330 train_time:95250ms step_avg:57.62ms
step:1654/2330 train_time:95310ms step_avg:57.62ms
step:1655/2330 train_time:95367ms step_avg:57.62ms
step:1656/2330 train_time:95427ms step_avg:57.63ms
step:1657/2330 train_time:95484ms step_avg:57.62ms
step:1658/2330 train_time:95545ms step_avg:57.63ms
step:1659/2330 train_time:95602ms step_avg:57.63ms
step:1660/2330 train_time:95663ms step_avg:57.63ms
step:1661/2330 train_time:95720ms step_avg:57.63ms
step:1662/2330 train_time:95779ms step_avg:57.63ms
step:1663/2330 train_time:95835ms step_avg:57.63ms
step:1664/2330 train_time:95896ms step_avg:57.63ms
step:1665/2330 train_time:95953ms step_avg:57.63ms
step:1666/2330 train_time:96012ms step_avg:57.63ms
step:1667/2330 train_time:96069ms step_avg:57.63ms
step:1668/2330 train_time:96129ms step_avg:57.63ms
step:1669/2330 train_time:96186ms step_avg:57.63ms
step:1670/2330 train_time:96245ms step_avg:57.63ms
step:1671/2330 train_time:96302ms step_avg:57.63ms
step:1672/2330 train_time:96363ms step_avg:57.63ms
step:1673/2330 train_time:96420ms step_avg:57.63ms
step:1674/2330 train_time:96480ms step_avg:57.63ms
step:1675/2330 train_time:96536ms step_avg:57.63ms
step:1676/2330 train_time:96596ms step_avg:57.64ms
step:1677/2330 train_time:96653ms step_avg:57.63ms
step:1678/2330 train_time:96713ms step_avg:57.64ms
step:1679/2330 train_time:96770ms step_avg:57.64ms
step:1680/2330 train_time:96830ms step_avg:57.64ms
step:1681/2330 train_time:96887ms step_avg:57.64ms
step:1682/2330 train_time:96947ms step_avg:57.64ms
step:1683/2330 train_time:97004ms step_avg:57.64ms
step:1684/2330 train_time:97065ms step_avg:57.64ms
step:1685/2330 train_time:97123ms step_avg:57.64ms
step:1686/2330 train_time:97182ms step_avg:57.64ms
step:1687/2330 train_time:97239ms step_avg:57.64ms
step:1688/2330 train_time:97299ms step_avg:57.64ms
step:1689/2330 train_time:97356ms step_avg:57.64ms
step:1690/2330 train_time:97416ms step_avg:57.64ms
step:1691/2330 train_time:97473ms step_avg:57.64ms
step:1692/2330 train_time:97532ms step_avg:57.64ms
step:1693/2330 train_time:97589ms step_avg:57.64ms
step:1694/2330 train_time:97649ms step_avg:57.64ms
step:1695/2330 train_time:97706ms step_avg:57.64ms
step:1696/2330 train_time:97766ms step_avg:57.65ms
step:1697/2330 train_time:97823ms step_avg:57.64ms
step:1698/2330 train_time:97883ms step_avg:57.65ms
step:1699/2330 train_time:97941ms step_avg:57.65ms
step:1700/2330 train_time:98000ms step_avg:57.65ms
step:1701/2330 train_time:98057ms step_avg:57.65ms
step:1702/2330 train_time:98116ms step_avg:57.65ms
step:1703/2330 train_time:98172ms step_avg:57.65ms
step:1704/2330 train_time:98232ms step_avg:57.65ms
step:1705/2330 train_time:98288ms step_avg:57.65ms
step:1706/2330 train_time:98348ms step_avg:57.65ms
step:1707/2330 train_time:98404ms step_avg:57.65ms
step:1708/2330 train_time:98466ms step_avg:57.65ms
step:1709/2330 train_time:98523ms step_avg:57.65ms
step:1710/2330 train_time:98583ms step_avg:57.65ms
step:1711/2330 train_time:98640ms step_avg:57.65ms
step:1712/2330 train_time:98700ms step_avg:57.65ms
step:1713/2330 train_time:98756ms step_avg:57.65ms
step:1714/2330 train_time:98817ms step_avg:57.65ms
step:1715/2330 train_time:98873ms step_avg:57.65ms
step:1716/2330 train_time:98933ms step_avg:57.65ms
step:1717/2330 train_time:98989ms step_avg:57.65ms
step:1718/2330 train_time:99049ms step_avg:57.65ms
step:1719/2330 train_time:99106ms step_avg:57.65ms
step:1720/2330 train_time:99166ms step_avg:57.65ms
step:1721/2330 train_time:99223ms step_avg:57.65ms
step:1722/2330 train_time:99283ms step_avg:57.66ms
step:1723/2330 train_time:99340ms step_avg:57.66ms
step:1724/2330 train_time:99400ms step_avg:57.66ms
step:1725/2330 train_time:99456ms step_avg:57.66ms
step:1726/2330 train_time:99517ms step_avg:57.66ms
step:1727/2330 train_time:99574ms step_avg:57.66ms
step:1728/2330 train_time:99634ms step_avg:57.66ms
step:1729/2330 train_time:99690ms step_avg:57.66ms
step:1730/2330 train_time:99750ms step_avg:57.66ms
step:1731/2330 train_time:99807ms step_avg:57.66ms
step:1732/2330 train_time:99867ms step_avg:57.66ms
step:1733/2330 train_time:99923ms step_avg:57.66ms
step:1734/2330 train_time:99983ms step_avg:57.66ms
step:1735/2330 train_time:100040ms step_avg:57.66ms
step:1736/2330 train_time:100101ms step_avg:57.66ms
step:1737/2330 train_time:100158ms step_avg:57.66ms
step:1738/2330 train_time:100218ms step_avg:57.66ms
step:1739/2330 train_time:100275ms step_avg:57.66ms
step:1740/2330 train_time:100334ms step_avg:57.66ms
step:1741/2330 train_time:100390ms step_avg:57.66ms
step:1742/2330 train_time:100450ms step_avg:57.66ms
step:1743/2330 train_time:100507ms step_avg:57.66ms
step:1744/2330 train_time:100568ms step_avg:57.66ms
step:1745/2330 train_time:100624ms step_avg:57.66ms
step:1746/2330 train_time:100684ms step_avg:57.67ms
step:1747/2330 train_time:100741ms step_avg:57.67ms
step:1748/2330 train_time:100801ms step_avg:57.67ms
step:1749/2330 train_time:100858ms step_avg:57.67ms
step:1750/2330 train_time:100918ms step_avg:57.67ms
step:1750/2330 val_loss:4.1322 train_time:101000ms step_avg:57.71ms
step:1751/2330 train_time:101021ms step_avg:57.69ms
step:1752/2330 train_time:101041ms step_avg:57.67ms
step:1753/2330 train_time:101099ms step_avg:57.67ms
step:1754/2330 train_time:101162ms step_avg:57.67ms
step:1755/2330 train_time:101218ms step_avg:57.67ms
step:1756/2330 train_time:101279ms step_avg:57.68ms
step:1757/2330 train_time:101336ms step_avg:57.68ms
step:1758/2330 train_time:101396ms step_avg:57.68ms
step:1759/2330 train_time:101453ms step_avg:57.68ms
step:1760/2330 train_time:101513ms step_avg:57.68ms
step:1761/2330 train_time:101569ms step_avg:57.68ms
step:1762/2330 train_time:101628ms step_avg:57.68ms
step:1763/2330 train_time:101684ms step_avg:57.68ms
step:1764/2330 train_time:101743ms step_avg:57.68ms
step:1765/2330 train_time:101800ms step_avg:57.68ms
step:1766/2330 train_time:101859ms step_avg:57.68ms
step:1767/2330 train_time:101915ms step_avg:57.68ms
step:1768/2330 train_time:101977ms step_avg:57.68ms
step:1769/2330 train_time:102034ms step_avg:57.68ms
step:1770/2330 train_time:102096ms step_avg:57.68ms
step:1771/2330 train_time:102155ms step_avg:57.68ms
step:1772/2330 train_time:102215ms step_avg:57.68ms
step:1773/2330 train_time:102272ms step_avg:57.68ms
step:1774/2330 train_time:102332ms step_avg:57.68ms
step:1775/2330 train_time:102389ms step_avg:57.68ms
step:1776/2330 train_time:102449ms step_avg:57.69ms
step:1777/2330 train_time:102505ms step_avg:57.68ms
step:1778/2330 train_time:102565ms step_avg:57.69ms
step:1779/2330 train_time:102623ms step_avg:57.69ms
step:1780/2330 train_time:102682ms step_avg:57.69ms
step:1781/2330 train_time:102738ms step_avg:57.69ms
step:1782/2330 train_time:102798ms step_avg:57.69ms
step:1783/2330 train_time:102854ms step_avg:57.69ms
step:1784/2330 train_time:102914ms step_avg:57.69ms
step:1785/2330 train_time:102971ms step_avg:57.69ms
step:1786/2330 train_time:103031ms step_avg:57.69ms
step:1787/2330 train_time:103088ms step_avg:57.69ms
step:1788/2330 train_time:103149ms step_avg:57.69ms
step:1789/2330 train_time:103205ms step_avg:57.69ms
step:1790/2330 train_time:103265ms step_avg:57.69ms
step:1791/2330 train_time:103322ms step_avg:57.69ms
step:1792/2330 train_time:103383ms step_avg:57.69ms
step:1793/2330 train_time:103440ms step_avg:57.69ms
step:1794/2330 train_time:103500ms step_avg:57.69ms
step:1795/2330 train_time:103556ms step_avg:57.69ms
step:1796/2330 train_time:103616ms step_avg:57.69ms
step:1797/2330 train_time:103673ms step_avg:57.69ms
step:1798/2330 train_time:103732ms step_avg:57.69ms
step:1799/2330 train_time:103788ms step_avg:57.69ms
step:1800/2330 train_time:103848ms step_avg:57.69ms
step:1801/2330 train_time:103904ms step_avg:57.69ms
step:1802/2330 train_time:103964ms step_avg:57.69ms
step:1803/2330 train_time:104021ms step_avg:57.69ms
step:1804/2330 train_time:104080ms step_avg:57.69ms
step:1805/2330 train_time:104137ms step_avg:57.69ms
step:1806/2330 train_time:104198ms step_avg:57.70ms
step:1807/2330 train_time:104255ms step_avg:57.69ms
step:1808/2330 train_time:104315ms step_avg:57.70ms
step:1809/2330 train_time:104373ms step_avg:57.70ms
step:1810/2330 train_time:104433ms step_avg:57.70ms
step:1811/2330 train_time:104490ms step_avg:57.70ms
step:1812/2330 train_time:104550ms step_avg:57.70ms
step:1813/2330 train_time:104607ms step_avg:57.70ms
step:1814/2330 train_time:104666ms step_avg:57.70ms
step:1815/2330 train_time:104723ms step_avg:57.70ms
step:1816/2330 train_time:104782ms step_avg:57.70ms
step:1817/2330 train_time:104839ms step_avg:57.70ms
step:1818/2330 train_time:104899ms step_avg:57.70ms
step:1819/2330 train_time:104956ms step_avg:57.70ms
step:1820/2330 train_time:105015ms step_avg:57.70ms
step:1821/2330 train_time:105073ms step_avg:57.70ms
step:1822/2330 train_time:105133ms step_avg:57.70ms
step:1823/2330 train_time:105190ms step_avg:57.70ms
step:1824/2330 train_time:105250ms step_avg:57.70ms
step:1825/2330 train_time:105307ms step_avg:57.70ms
step:1826/2330 train_time:105367ms step_avg:57.70ms
step:1827/2330 train_time:105424ms step_avg:57.70ms
step:1828/2330 train_time:105484ms step_avg:57.70ms
step:1829/2330 train_time:105540ms step_avg:57.70ms
step:1830/2330 train_time:105601ms step_avg:57.71ms
step:1831/2330 train_time:105658ms step_avg:57.70ms
step:1832/2330 train_time:105717ms step_avg:57.71ms
step:1833/2330 train_time:105774ms step_avg:57.71ms
step:1834/2330 train_time:105834ms step_avg:57.71ms
step:1835/2330 train_time:105891ms step_avg:57.71ms
step:1836/2330 train_time:105951ms step_avg:57.71ms
step:1837/2330 train_time:106008ms step_avg:57.71ms
step:1838/2330 train_time:106067ms step_avg:57.71ms
step:1839/2330 train_time:106124ms step_avg:57.71ms
step:1840/2330 train_time:106185ms step_avg:57.71ms
step:1841/2330 train_time:106241ms step_avg:57.71ms
step:1842/2330 train_time:106301ms step_avg:57.71ms
step:1843/2330 train_time:106358ms step_avg:57.71ms
step:1844/2330 train_time:106419ms step_avg:57.71ms
step:1845/2330 train_time:106476ms step_avg:57.71ms
step:1846/2330 train_time:106536ms step_avg:57.71ms
step:1847/2330 train_time:106592ms step_avg:57.71ms
step:1848/2330 train_time:106653ms step_avg:57.71ms
step:1849/2330 train_time:106710ms step_avg:57.71ms
step:1850/2330 train_time:106770ms step_avg:57.71ms
step:1851/2330 train_time:106827ms step_avg:57.71ms
step:1852/2330 train_time:106887ms step_avg:57.71ms
step:1853/2330 train_time:106943ms step_avg:57.71ms
step:1854/2330 train_time:107003ms step_avg:57.71ms
step:1855/2330 train_time:107060ms step_avg:57.71ms
step:1856/2330 train_time:107120ms step_avg:57.72ms
step:1857/2330 train_time:107176ms step_avg:57.71ms
step:1858/2330 train_time:107237ms step_avg:57.72ms
step:1859/2330 train_time:107293ms step_avg:57.72ms
step:1860/2330 train_time:107356ms step_avg:57.72ms
step:1861/2330 train_time:107413ms step_avg:57.72ms
step:1862/2330 train_time:107473ms step_avg:57.72ms
step:1863/2330 train_time:107530ms step_avg:57.72ms
step:1864/2330 train_time:107590ms step_avg:57.72ms
step:1865/2330 train_time:107647ms step_avg:57.72ms
step:1866/2330 train_time:107706ms step_avg:57.72ms
step:1867/2330 train_time:107763ms step_avg:57.72ms
step:1868/2330 train_time:107823ms step_avg:57.72ms
step:1869/2330 train_time:107880ms step_avg:57.72ms
step:1870/2330 train_time:107940ms step_avg:57.72ms
step:1871/2330 train_time:107996ms step_avg:57.72ms
step:1872/2330 train_time:108057ms step_avg:57.72ms
step:1873/2330 train_time:108114ms step_avg:57.72ms
step:1874/2330 train_time:108173ms step_avg:57.72ms
step:1875/2330 train_time:108230ms step_avg:57.72ms
step:1876/2330 train_time:108290ms step_avg:57.72ms
step:1877/2330 train_time:108347ms step_avg:57.72ms
step:1878/2330 train_time:108408ms step_avg:57.73ms
step:1879/2330 train_time:108464ms step_avg:57.72ms
step:1880/2330 train_time:108524ms step_avg:57.73ms
step:1881/2330 train_time:108580ms step_avg:57.72ms
step:1882/2330 train_time:108641ms step_avg:57.73ms
step:1883/2330 train_time:108697ms step_avg:57.73ms
step:1884/2330 train_time:108757ms step_avg:57.73ms
step:1885/2330 train_time:108815ms step_avg:57.73ms
step:1886/2330 train_time:108875ms step_avg:57.73ms
step:1887/2330 train_time:108931ms step_avg:57.73ms
step:1888/2330 train_time:108992ms step_avg:57.73ms
step:1889/2330 train_time:109049ms step_avg:57.73ms
step:1890/2330 train_time:109109ms step_avg:57.73ms
step:1891/2330 train_time:109166ms step_avg:57.73ms
step:1892/2330 train_time:109225ms step_avg:57.73ms
step:1893/2330 train_time:109282ms step_avg:57.73ms
step:1894/2330 train_time:109342ms step_avg:57.73ms
step:1895/2330 train_time:109399ms step_avg:57.73ms
step:1896/2330 train_time:109459ms step_avg:57.73ms
step:1897/2330 train_time:109515ms step_avg:57.73ms
step:1898/2330 train_time:109575ms step_avg:57.73ms
step:1899/2330 train_time:109632ms step_avg:57.73ms
step:1900/2330 train_time:109692ms step_avg:57.73ms
step:1901/2330 train_time:109750ms step_avg:57.73ms
step:1902/2330 train_time:109810ms step_avg:57.73ms
step:1903/2330 train_time:109866ms step_avg:57.73ms
step:1904/2330 train_time:109926ms step_avg:57.73ms
step:1905/2330 train_time:109983ms step_avg:57.73ms
step:1906/2330 train_time:110042ms step_avg:57.73ms
step:1907/2330 train_time:110098ms step_avg:57.73ms
step:1908/2330 train_time:110159ms step_avg:57.74ms
step:1909/2330 train_time:110216ms step_avg:57.74ms
step:1910/2330 train_time:110276ms step_avg:57.74ms
step:1911/2330 train_time:110333ms step_avg:57.74ms
step:1912/2330 train_time:110393ms step_avg:57.74ms
step:1913/2330 train_time:110450ms step_avg:57.74ms
step:1914/2330 train_time:110511ms step_avg:57.74ms
step:1915/2330 train_time:110567ms step_avg:57.74ms
step:1916/2330 train_time:110627ms step_avg:57.74ms
step:1917/2330 train_time:110684ms step_avg:57.74ms
step:1918/2330 train_time:110743ms step_avg:57.74ms
step:1919/2330 train_time:110801ms step_avg:57.74ms
step:1920/2330 train_time:110860ms step_avg:57.74ms
step:1921/2330 train_time:110917ms step_avg:57.74ms
step:1922/2330 train_time:110977ms step_avg:57.74ms
step:1923/2330 train_time:111034ms step_avg:57.74ms
step:1924/2330 train_time:111094ms step_avg:57.74ms
step:1925/2330 train_time:111151ms step_avg:57.74ms
step:1926/2330 train_time:111211ms step_avg:57.74ms
step:1927/2330 train_time:111267ms step_avg:57.74ms
step:1928/2330 train_time:111328ms step_avg:57.74ms
step:1929/2330 train_time:111385ms step_avg:57.74ms
step:1930/2330 train_time:111446ms step_avg:57.74ms
step:1931/2330 train_time:111503ms step_avg:57.74ms
step:1932/2330 train_time:111562ms step_avg:57.74ms
step:1933/2330 train_time:111619ms step_avg:57.74ms
step:1934/2330 train_time:111678ms step_avg:57.74ms
step:1935/2330 train_time:111735ms step_avg:57.74ms
step:1936/2330 train_time:111797ms step_avg:57.75ms
step:1937/2330 train_time:111854ms step_avg:57.75ms
step:1938/2330 train_time:111913ms step_avg:57.75ms
step:1939/2330 train_time:111971ms step_avg:57.75ms
step:1940/2330 train_time:112030ms step_avg:57.75ms
step:1941/2330 train_time:112087ms step_avg:57.75ms
step:1942/2330 train_time:112147ms step_avg:57.75ms
step:1943/2330 train_time:112203ms step_avg:57.75ms
step:1944/2330 train_time:112264ms step_avg:57.75ms
step:1945/2330 train_time:112321ms step_avg:57.75ms
step:1946/2330 train_time:112380ms step_avg:57.75ms
step:1947/2330 train_time:112437ms step_avg:57.75ms
step:1948/2330 train_time:112497ms step_avg:57.75ms
step:1949/2330 train_time:112554ms step_avg:57.75ms
step:1950/2330 train_time:112614ms step_avg:57.75ms
step:1951/2330 train_time:112671ms step_avg:57.75ms
step:1952/2330 train_time:112731ms step_avg:57.75ms
step:1953/2330 train_time:112787ms step_avg:57.75ms
step:1954/2330 train_time:112848ms step_avg:57.75ms
step:1955/2330 train_time:112904ms step_avg:57.75ms
step:1956/2330 train_time:112965ms step_avg:57.75ms
step:1957/2330 train_time:113021ms step_avg:57.75ms
step:1958/2330 train_time:113081ms step_avg:57.75ms
step:1959/2330 train_time:113138ms step_avg:57.75ms
step:1960/2330 train_time:113198ms step_avg:57.75ms
step:1961/2330 train_time:113255ms step_avg:57.75ms
step:1962/2330 train_time:113316ms step_avg:57.76ms
step:1963/2330 train_time:113373ms step_avg:57.75ms
step:1964/2330 train_time:113432ms step_avg:57.76ms
step:1965/2330 train_time:113489ms step_avg:57.76ms
step:1966/2330 train_time:113549ms step_avg:57.76ms
step:1967/2330 train_time:113606ms step_avg:57.76ms
step:1968/2330 train_time:113666ms step_avg:57.76ms
step:1969/2330 train_time:113723ms step_avg:57.76ms
step:1970/2330 train_time:113782ms step_avg:57.76ms
step:1971/2330 train_time:113839ms step_avg:57.76ms
step:1972/2330 train_time:113899ms step_avg:57.76ms
step:1973/2330 train_time:113956ms step_avg:57.76ms
step:1974/2330 train_time:114016ms step_avg:57.76ms
step:1975/2330 train_time:114073ms step_avg:57.76ms
step:1976/2330 train_time:114134ms step_avg:57.76ms
step:1977/2330 train_time:114190ms step_avg:57.76ms
step:1978/2330 train_time:114250ms step_avg:57.76ms
step:1979/2330 train_time:114307ms step_avg:57.76ms
step:1980/2330 train_time:114367ms step_avg:57.76ms
step:1981/2330 train_time:114424ms step_avg:57.76ms
step:1982/2330 train_time:114483ms step_avg:57.76ms
step:1983/2330 train_time:114541ms step_avg:57.76ms
step:1984/2330 train_time:114600ms step_avg:57.76ms
step:1985/2330 train_time:114657ms step_avg:57.76ms
step:1986/2330 train_time:114717ms step_avg:57.76ms
step:1987/2330 train_time:114774ms step_avg:57.76ms
step:1988/2330 train_time:114833ms step_avg:57.76ms
step:1989/2330 train_time:114890ms step_avg:57.76ms
step:1990/2330 train_time:114950ms step_avg:57.76ms
step:1991/2330 train_time:115007ms step_avg:57.76ms
step:1992/2330 train_time:115067ms step_avg:57.76ms
step:1993/2330 train_time:115123ms step_avg:57.76ms
step:1994/2330 train_time:115183ms step_avg:57.76ms
step:1995/2330 train_time:115239ms step_avg:57.76ms
step:1996/2330 train_time:115299ms step_avg:57.77ms
step:1997/2330 train_time:115356ms step_avg:57.76ms
step:1998/2330 train_time:115417ms step_avg:57.77ms
step:1999/2330 train_time:115474ms step_avg:57.77ms
step:2000/2330 train_time:115535ms step_avg:57.77ms
step:2000/2330 val_loss:4.0676 train_time:115615ms step_avg:57.81ms
step:2001/2330 train_time:115635ms step_avg:57.79ms
step:2002/2330 train_time:115655ms step_avg:57.77ms
step:2003/2330 train_time:115713ms step_avg:57.77ms
step:2004/2330 train_time:115776ms step_avg:57.77ms
step:2005/2330 train_time:115832ms step_avg:57.77ms
step:2006/2330 train_time:115892ms step_avg:57.77ms
step:2007/2330 train_time:115949ms step_avg:57.77ms
step:2008/2330 train_time:116008ms step_avg:57.77ms
step:2009/2330 train_time:116064ms step_avg:57.77ms
step:2010/2330 train_time:116125ms step_avg:57.77ms
step:2011/2330 train_time:116181ms step_avg:57.77ms
step:2012/2330 train_time:116240ms step_avg:57.77ms
step:2013/2330 train_time:116296ms step_avg:57.77ms
step:2014/2330 train_time:116355ms step_avg:57.77ms
step:2015/2330 train_time:116411ms step_avg:57.77ms
step:2016/2330 train_time:116470ms step_avg:57.77ms
step:2017/2330 train_time:116527ms step_avg:57.77ms
step:2018/2330 train_time:116587ms step_avg:57.77ms
step:2019/2330 train_time:116645ms step_avg:57.77ms
step:2020/2330 train_time:116707ms step_avg:57.78ms
step:2021/2330 train_time:116765ms step_avg:57.78ms
step:2022/2330 train_time:116825ms step_avg:57.78ms
step:2023/2330 train_time:116883ms step_avg:57.78ms
step:2024/2330 train_time:116943ms step_avg:57.78ms
step:2025/2330 train_time:117000ms step_avg:57.78ms
step:2026/2330 train_time:117060ms step_avg:57.78ms
step:2027/2330 train_time:117116ms step_avg:57.78ms
step:2028/2330 train_time:117176ms step_avg:57.78ms
step:2029/2330 train_time:117231ms step_avg:57.78ms
step:2030/2330 train_time:117291ms step_avg:57.78ms
step:2031/2330 train_time:117348ms step_avg:57.78ms
step:2032/2330 train_time:117407ms step_avg:57.78ms
step:2033/2330 train_time:117463ms step_avg:57.78ms
step:2034/2330 train_time:117523ms step_avg:57.78ms
step:2035/2330 train_time:117581ms step_avg:57.78ms
step:2036/2330 train_time:117642ms step_avg:57.78ms
step:2037/2330 train_time:117699ms step_avg:57.78ms
step:2038/2330 train_time:117759ms step_avg:57.78ms
step:2039/2330 train_time:117816ms step_avg:57.78ms
step:2040/2330 train_time:117876ms step_avg:57.78ms
step:2041/2330 train_time:117933ms step_avg:57.78ms
step:2042/2330 train_time:117993ms step_avg:57.78ms
step:2043/2330 train_time:118050ms step_avg:57.78ms
step:2044/2330 train_time:118109ms step_avg:57.78ms
step:2045/2330 train_time:118166ms step_avg:57.78ms
step:2046/2330 train_time:118227ms step_avg:57.78ms
step:2047/2330 train_time:118284ms step_avg:57.78ms
step:2048/2330 train_time:118343ms step_avg:57.78ms
step:2049/2330 train_time:118400ms step_avg:57.78ms
step:2050/2330 train_time:118459ms step_avg:57.79ms
step:2051/2330 train_time:118516ms step_avg:57.78ms
step:2052/2330 train_time:118576ms step_avg:57.79ms
step:2053/2330 train_time:118633ms step_avg:57.79ms
step:2054/2330 train_time:118693ms step_avg:57.79ms
step:2055/2330 train_time:118749ms step_avg:57.79ms
step:2056/2330 train_time:118811ms step_avg:57.79ms
step:2057/2330 train_time:118868ms step_avg:57.79ms
step:2058/2330 train_time:118928ms step_avg:57.79ms
step:2059/2330 train_time:118985ms step_avg:57.79ms
step:2060/2330 train_time:119046ms step_avg:57.79ms
step:2061/2330 train_time:119102ms step_avg:57.79ms
step:2062/2330 train_time:119162ms step_avg:57.79ms
step:2063/2330 train_time:119220ms step_avg:57.79ms
step:2064/2330 train_time:119280ms step_avg:57.79ms
step:2065/2330 train_time:119336ms step_avg:57.79ms
step:2066/2330 train_time:119395ms step_avg:57.79ms
step:2067/2330 train_time:119451ms step_avg:57.79ms
step:2068/2330 train_time:119512ms step_avg:57.79ms
step:2069/2330 train_time:119568ms step_avg:57.79ms
step:2070/2330 train_time:119629ms step_avg:57.79ms
step:2071/2330 train_time:119686ms step_avg:57.79ms
step:2072/2330 train_time:119747ms step_avg:57.79ms
step:2073/2330 train_time:119804ms step_avg:57.79ms
step:2074/2330 train_time:119864ms step_avg:57.79ms
step:2075/2330 train_time:119922ms step_avg:57.79ms
step:2076/2330 train_time:119982ms step_avg:57.79ms
step:2077/2330 train_time:120039ms step_avg:57.79ms
step:2078/2330 train_time:120098ms step_avg:57.80ms
step:2079/2330 train_time:120155ms step_avg:57.79ms
step:2080/2330 train_time:120214ms step_avg:57.80ms
step:2081/2330 train_time:120270ms step_avg:57.79ms
step:2082/2330 train_time:120330ms step_avg:57.80ms
step:2083/2330 train_time:120387ms step_avg:57.80ms
step:2084/2330 train_time:120447ms step_avg:57.80ms
step:2085/2330 train_time:120504ms step_avg:57.80ms
step:2086/2330 train_time:120564ms step_avg:57.80ms
step:2087/2330 train_time:120621ms step_avg:57.80ms
step:2088/2330 train_time:120681ms step_avg:57.80ms
step:2089/2330 train_time:120739ms step_avg:57.80ms
step:2090/2330 train_time:120799ms step_avg:57.80ms
step:2091/2330 train_time:120856ms step_avg:57.80ms
step:2092/2330 train_time:120915ms step_avg:57.80ms
step:2093/2330 train_time:120972ms step_avg:57.80ms
step:2094/2330 train_time:121032ms step_avg:57.80ms
step:2095/2330 train_time:121089ms step_avg:57.80ms
step:2096/2330 train_time:121148ms step_avg:57.80ms
step:2097/2330 train_time:121204ms step_avg:57.80ms
step:2098/2330 train_time:121265ms step_avg:57.80ms
step:2099/2330 train_time:121321ms step_avg:57.80ms
step:2100/2330 train_time:121381ms step_avg:57.80ms
step:2101/2330 train_time:121438ms step_avg:57.80ms
step:2102/2330 train_time:121498ms step_avg:57.80ms
step:2103/2330 train_time:121555ms step_avg:57.80ms
step:2104/2330 train_time:121615ms step_avg:57.80ms
step:2105/2330 train_time:121672ms step_avg:57.80ms
step:2106/2330 train_time:121731ms step_avg:57.80ms
step:2107/2330 train_time:121787ms step_avg:57.80ms
step:2108/2330 train_time:121849ms step_avg:57.80ms
step:2109/2330 train_time:121905ms step_avg:57.80ms
step:2110/2330 train_time:121966ms step_avg:57.80ms
step:2111/2330 train_time:122023ms step_avg:57.80ms
step:2112/2330 train_time:122082ms step_avg:57.80ms
step:2113/2330 train_time:122140ms step_avg:57.80ms
step:2114/2330 train_time:122200ms step_avg:57.81ms
step:2115/2330 train_time:122258ms step_avg:57.81ms
step:2116/2330 train_time:122317ms step_avg:57.81ms
step:2117/2330 train_time:122373ms step_avg:57.81ms
step:2118/2330 train_time:122433ms step_avg:57.81ms
step:2119/2330 train_time:122489ms step_avg:57.81ms
step:2120/2330 train_time:122549ms step_avg:57.81ms
step:2121/2330 train_time:122606ms step_avg:57.81ms
step:2122/2330 train_time:122665ms step_avg:57.81ms
step:2123/2330 train_time:122722ms step_avg:57.81ms
step:2124/2330 train_time:122782ms step_avg:57.81ms
step:2125/2330 train_time:122840ms step_avg:57.81ms
step:2126/2330 train_time:122900ms step_avg:57.81ms
step:2127/2330 train_time:122957ms step_avg:57.81ms
step:2128/2330 train_time:123016ms step_avg:57.81ms
step:2129/2330 train_time:123072ms step_avg:57.81ms
step:2130/2330 train_time:123133ms step_avg:57.81ms
step:2131/2330 train_time:123189ms step_avg:57.81ms
step:2132/2330 train_time:123250ms step_avg:57.81ms
step:2133/2330 train_time:123307ms step_avg:57.81ms
step:2134/2330 train_time:123367ms step_avg:57.81ms
step:2135/2330 train_time:123423ms step_avg:57.81ms
step:2136/2330 train_time:123483ms step_avg:57.81ms
step:2137/2330 train_time:123541ms step_avg:57.81ms
step:2138/2330 train_time:123600ms step_avg:57.81ms
step:2139/2330 train_time:123657ms step_avg:57.81ms
step:2140/2330 train_time:123717ms step_avg:57.81ms
step:2141/2330 train_time:123773ms step_avg:57.81ms
step:2142/2330 train_time:123833ms step_avg:57.81ms
step:2143/2330 train_time:123891ms step_avg:57.81ms
step:2144/2330 train_time:123950ms step_avg:57.81ms
step:2145/2330 train_time:124006ms step_avg:57.81ms
step:2146/2330 train_time:124066ms step_avg:57.81ms
step:2147/2330 train_time:124123ms step_avg:57.81ms
step:2148/2330 train_time:124183ms step_avg:57.81ms
step:2149/2330 train_time:124241ms step_avg:57.81ms
step:2150/2330 train_time:124300ms step_avg:57.81ms
step:2151/2330 train_time:124357ms step_avg:57.81ms
step:2152/2330 train_time:124416ms step_avg:57.81ms
step:2153/2330 train_time:124473ms step_avg:57.81ms
step:2154/2330 train_time:124531ms step_avg:57.81ms
step:2155/2330 train_time:124589ms step_avg:57.81ms
step:2156/2330 train_time:124649ms step_avg:57.81ms
step:2157/2330 train_time:124706ms step_avg:57.81ms
step:2158/2330 train_time:124766ms step_avg:57.82ms
step:2159/2330 train_time:124822ms step_avg:57.81ms
step:2160/2330 train_time:124883ms step_avg:57.82ms
step:2161/2330 train_time:124940ms step_avg:57.82ms
step:2162/2330 train_time:125001ms step_avg:57.82ms
step:2163/2330 train_time:125057ms step_avg:57.82ms
step:2164/2330 train_time:125117ms step_avg:57.82ms
step:2165/2330 train_time:125174ms step_avg:57.82ms
step:2166/2330 train_time:125233ms step_avg:57.82ms
step:2167/2330 train_time:125290ms step_avg:57.82ms
step:2168/2330 train_time:125349ms step_avg:57.82ms
step:2169/2330 train_time:125406ms step_avg:57.82ms
step:2170/2330 train_time:125466ms step_avg:57.82ms
step:2171/2330 train_time:125523ms step_avg:57.82ms
step:2172/2330 train_time:125584ms step_avg:57.82ms
step:2173/2330 train_time:125641ms step_avg:57.82ms
step:2174/2330 train_time:125700ms step_avg:57.82ms
step:2175/2330 train_time:125757ms step_avg:57.82ms
step:2176/2330 train_time:125817ms step_avg:57.82ms
step:2177/2330 train_time:125874ms step_avg:57.82ms
step:2178/2330 train_time:125933ms step_avg:57.82ms
step:2179/2330 train_time:125990ms step_avg:57.82ms
step:2180/2330 train_time:126050ms step_avg:57.82ms
step:2181/2330 train_time:126107ms step_avg:57.82ms
step:2182/2330 train_time:126167ms step_avg:57.82ms
step:2183/2330 train_time:126225ms step_avg:57.82ms
step:2184/2330 train_time:126284ms step_avg:57.82ms
step:2185/2330 train_time:126341ms step_avg:57.82ms
step:2186/2330 train_time:126401ms step_avg:57.82ms
step:2187/2330 train_time:126458ms step_avg:57.82ms
step:2188/2330 train_time:126518ms step_avg:57.82ms
step:2189/2330 train_time:126575ms step_avg:57.82ms
step:2190/2330 train_time:126634ms step_avg:57.82ms
step:2191/2330 train_time:126690ms step_avg:57.82ms
step:2192/2330 train_time:126750ms step_avg:57.82ms
step:2193/2330 train_time:126807ms step_avg:57.82ms
step:2194/2330 train_time:126867ms step_avg:57.82ms
step:2195/2330 train_time:126923ms step_avg:57.82ms
step:2196/2330 train_time:126984ms step_avg:57.83ms
step:2197/2330 train_time:127042ms step_avg:57.82ms
step:2198/2330 train_time:127101ms step_avg:57.83ms
step:2199/2330 train_time:127158ms step_avg:57.83ms
step:2200/2330 train_time:127218ms step_avg:57.83ms
step:2201/2330 train_time:127275ms step_avg:57.83ms
step:2202/2330 train_time:127334ms step_avg:57.83ms
step:2203/2330 train_time:127390ms step_avg:57.83ms
step:2204/2330 train_time:127450ms step_avg:57.83ms
step:2205/2330 train_time:127507ms step_avg:57.83ms
step:2206/2330 train_time:127566ms step_avg:57.83ms
step:2207/2330 train_time:127623ms step_avg:57.83ms
step:2208/2330 train_time:127683ms step_avg:57.83ms
step:2209/2330 train_time:127740ms step_avg:57.83ms
step:2210/2330 train_time:127800ms step_avg:57.83ms
step:2211/2330 train_time:127857ms step_avg:57.83ms
step:2212/2330 train_time:127916ms step_avg:57.83ms
step:2213/2330 train_time:127974ms step_avg:57.83ms
step:2214/2330 train_time:128033ms step_avg:57.83ms
step:2215/2330 train_time:128090ms step_avg:57.83ms
step:2216/2330 train_time:128150ms step_avg:57.83ms
step:2217/2330 train_time:128207ms step_avg:57.83ms
step:2218/2330 train_time:128267ms step_avg:57.83ms
step:2219/2330 train_time:128323ms step_avg:57.83ms
step:2220/2330 train_time:128384ms step_avg:57.83ms
step:2221/2330 train_time:128441ms step_avg:57.83ms
step:2222/2330 train_time:128501ms step_avg:57.83ms
step:2223/2330 train_time:128557ms step_avg:57.83ms
step:2224/2330 train_time:128617ms step_avg:57.83ms
step:2225/2330 train_time:128673ms step_avg:57.83ms
step:2226/2330 train_time:128733ms step_avg:57.83ms
step:2227/2330 train_time:128790ms step_avg:57.83ms
step:2228/2330 train_time:128850ms step_avg:57.83ms
step:2229/2330 train_time:128906ms step_avg:57.83ms
step:2230/2330 train_time:128966ms step_avg:57.83ms
step:2231/2330 train_time:129023ms step_avg:57.83ms
step:2232/2330 train_time:129083ms step_avg:57.83ms
step:2233/2330 train_time:129140ms step_avg:57.83ms
step:2234/2330 train_time:129201ms step_avg:57.83ms
step:2235/2330 train_time:129257ms step_avg:57.83ms
step:2236/2330 train_time:129317ms step_avg:57.83ms
step:2237/2330 train_time:129373ms step_avg:57.83ms
step:2238/2330 train_time:129433ms step_avg:57.83ms
step:2239/2330 train_time:129489ms step_avg:57.83ms
step:2240/2330 train_time:129549ms step_avg:57.83ms
step:2241/2330 train_time:129605ms step_avg:57.83ms
step:2242/2330 train_time:129665ms step_avg:57.83ms
step:2243/2330 train_time:129722ms step_avg:57.83ms
step:2244/2330 train_time:129782ms step_avg:57.84ms
step:2245/2330 train_time:129840ms step_avg:57.83ms
step:2246/2330 train_time:129899ms step_avg:57.84ms
step:2247/2330 train_time:129955ms step_avg:57.83ms
step:2248/2330 train_time:130016ms step_avg:57.84ms
step:2249/2330 train_time:130072ms step_avg:57.84ms
step:2250/2330 train_time:130132ms step_avg:57.84ms
step:2250/2330 val_loss:4.0195 train_time:130213ms step_avg:57.87ms
step:2251/2330 train_time:130232ms step_avg:57.86ms
step:2252/2330 train_time:130252ms step_avg:57.84ms
step:2253/2330 train_time:130310ms step_avg:57.84ms
step:2254/2330 train_time:130376ms step_avg:57.84ms
step:2255/2330 train_time:130433ms step_avg:57.84ms
step:2256/2330 train_time:130494ms step_avg:57.84ms
step:2257/2330 train_time:130550ms step_avg:57.84ms
step:2258/2330 train_time:130611ms step_avg:57.84ms
step:2259/2330 train_time:130667ms step_avg:57.84ms
step:2260/2330 train_time:130727ms step_avg:57.84ms
step:2261/2330 train_time:130783ms step_avg:57.84ms
step:2262/2330 train_time:130843ms step_avg:57.84ms
step:2263/2330 train_time:130898ms step_avg:57.84ms
step:2264/2330 train_time:130958ms step_avg:57.84ms
step:2265/2330 train_time:131014ms step_avg:57.84ms
step:2266/2330 train_time:131073ms step_avg:57.84ms
step:2267/2330 train_time:131129ms step_avg:57.84ms
step:2268/2330 train_time:131190ms step_avg:57.84ms
step:2269/2330 train_time:131247ms step_avg:57.84ms
step:2270/2330 train_time:131310ms step_avg:57.85ms
step:2271/2330 train_time:131368ms step_avg:57.85ms
step:2272/2330 train_time:131430ms step_avg:57.85ms
step:2273/2330 train_time:131487ms step_avg:57.85ms
step:2274/2330 train_time:131547ms step_avg:57.85ms
step:2275/2330 train_time:131604ms step_avg:57.85ms
step:2276/2330 train_time:131664ms step_avg:57.85ms
step:2277/2330 train_time:131720ms step_avg:57.85ms
step:2278/2330 train_time:131780ms step_avg:57.85ms
step:2279/2330 train_time:131836ms step_avg:57.85ms
step:2280/2330 train_time:131896ms step_avg:57.85ms
step:2281/2330 train_time:131951ms step_avg:57.85ms
step:2282/2330 train_time:132011ms step_avg:57.85ms
step:2283/2330 train_time:132067ms step_avg:57.85ms
step:2284/2330 train_time:132127ms step_avg:57.85ms
step:2285/2330 train_time:132184ms step_avg:57.85ms
step:2286/2330 train_time:132244ms step_avg:57.85ms
step:2287/2330 train_time:132301ms step_avg:57.85ms
step:2288/2330 train_time:132362ms step_avg:57.85ms
step:2289/2330 train_time:132420ms step_avg:57.85ms
step:2290/2330 train_time:132480ms step_avg:57.85ms
step:2291/2330 train_time:132536ms step_avg:57.85ms
step:2292/2330 train_time:132596ms step_avg:57.85ms
step:2293/2330 train_time:132652ms step_avg:57.85ms
step:2294/2330 train_time:132713ms step_avg:57.85ms
step:2295/2330 train_time:132770ms step_avg:57.85ms
step:2296/2330 train_time:132829ms step_avg:57.85ms
step:2297/2330 train_time:132886ms step_avg:57.85ms
step:2298/2330 train_time:132945ms step_avg:57.85ms
step:2299/2330 train_time:133002ms step_avg:57.85ms
step:2300/2330 train_time:133062ms step_avg:57.85ms
step:2301/2330 train_time:133119ms step_avg:57.85ms
step:2302/2330 train_time:133179ms step_avg:57.85ms
step:2303/2330 train_time:133235ms step_avg:57.85ms
step:2304/2330 train_time:133295ms step_avg:57.85ms
step:2305/2330 train_time:133352ms step_avg:57.85ms
step:2306/2330 train_time:133412ms step_avg:57.85ms
step:2307/2330 train_time:133469ms step_avg:57.85ms
step:2308/2330 train_time:133529ms step_avg:57.86ms
step:2309/2330 train_time:133586ms step_avg:57.85ms
step:2310/2330 train_time:133647ms step_avg:57.86ms
step:2311/2330 train_time:133703ms step_avg:57.86ms
step:2312/2330 train_time:133764ms step_avg:57.86ms
step:2313/2330 train_time:133820ms step_avg:57.86ms
step:2314/2330 train_time:133880ms step_avg:57.86ms
step:2315/2330 train_time:133936ms step_avg:57.86ms
step:2316/2330 train_time:133996ms step_avg:57.86ms
step:2317/2330 train_time:134053ms step_avg:57.86ms
step:2318/2330 train_time:134112ms step_avg:57.86ms
step:2319/2330 train_time:134168ms step_avg:57.86ms
step:2320/2330 train_time:134228ms step_avg:57.86ms
step:2321/2330 train_time:134285ms step_avg:57.86ms
step:2322/2330 train_time:134345ms step_avg:57.86ms
step:2323/2330 train_time:134403ms step_avg:57.86ms
step:2324/2330 train_time:134463ms step_avg:57.86ms
step:2325/2330 train_time:134520ms step_avg:57.86ms
step:2326/2330 train_time:134580ms step_avg:57.86ms
step:2327/2330 train_time:134636ms step_avg:57.86ms
step:2328/2330 train_time:134696ms step_avg:57.86ms
step:2329/2330 train_time:134753ms step_avg:57.86ms
step:2330/2330 train_time:134812ms step_avg:57.86ms
step:2330/2330 val_loss:4.0054 train_time:134893ms step_avg:57.89ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
