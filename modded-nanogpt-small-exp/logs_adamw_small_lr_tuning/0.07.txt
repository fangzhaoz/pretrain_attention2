import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--adamw_lr', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_lr_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_lr_tuning/{args2.adamw_lr}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=float(args2.adamw_lr),  betas=(0.85, 0.85), eps=1e-10, weight_decay=0.0, fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_lr_tuning/{args2.adamw_lr}", exist_ok=True)
            torch.save(log, f"logs_adamw_small_lr_tuning/{args2.adamw_lr}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 02:42:59 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   32C    P0             114W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   31C    P0             115W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   27C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   28C    P0             116W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   26C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   30C    P0             116W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   27C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:91ms step_avg:91.12ms
step:2/2330 train_time:182ms step_avg:91.21ms
step:3/2330 train_time:202ms step_avg:67.24ms
step:4/2330 train_time:221ms step_avg:55.15ms
step:5/2330 train_time:273ms step_avg:54.67ms
step:6/2330 train_time:331ms step_avg:55.16ms
step:7/2330 train_time:385ms step_avg:54.93ms
step:8/2330 train_time:441ms step_avg:55.18ms
step:9/2330 train_time:495ms step_avg:55.05ms
step:10/2330 train_time:552ms step_avg:55.22ms
step:11/2330 train_time:606ms step_avg:55.09ms
step:12/2330 train_time:663ms step_avg:55.24ms
step:13/2330 train_time:717ms step_avg:55.14ms
step:14/2330 train_time:774ms step_avg:55.27ms
step:15/2330 train_time:828ms step_avg:55.18ms
step:16/2330 train_time:884ms step_avg:55.28ms
step:17/2330 train_time:938ms step_avg:55.20ms
step:18/2330 train_time:995ms step_avg:55.28ms
step:19/2330 train_time:1049ms step_avg:55.23ms
step:20/2330 train_time:1107ms step_avg:55.33ms
step:21/2330 train_time:1161ms step_avg:55.31ms
step:22/2330 train_time:1219ms step_avg:55.39ms
step:23/2330 train_time:1273ms step_avg:55.34ms
step:24/2330 train_time:1331ms step_avg:55.46ms
step:25/2330 train_time:1386ms step_avg:55.42ms
step:26/2330 train_time:1442ms step_avg:55.48ms
step:27/2330 train_time:1496ms step_avg:55.42ms
step:28/2330 train_time:1554ms step_avg:55.51ms
step:29/2330 train_time:1608ms step_avg:55.46ms
step:30/2330 train_time:1665ms step_avg:55.51ms
step:31/2330 train_time:1719ms step_avg:55.47ms
step:32/2330 train_time:1776ms step_avg:55.51ms
step:33/2330 train_time:1831ms step_avg:55.49ms
step:34/2330 train_time:1888ms step_avg:55.54ms
step:35/2330 train_time:1943ms step_avg:55.50ms
step:36/2330 train_time:2000ms step_avg:55.55ms
step:37/2330 train_time:2054ms step_avg:55.52ms
step:38/2330 train_time:2112ms step_avg:55.58ms
step:39/2330 train_time:2167ms step_avg:55.55ms
step:40/2330 train_time:2224ms step_avg:55.59ms
step:41/2330 train_time:2278ms step_avg:55.55ms
step:42/2330 train_time:2336ms step_avg:55.62ms
step:43/2330 train_time:2390ms step_avg:55.59ms
step:44/2330 train_time:2448ms step_avg:55.63ms
step:45/2330 train_time:2502ms step_avg:55.60ms
step:46/2330 train_time:2561ms step_avg:55.68ms
step:47/2330 train_time:2615ms step_avg:55.65ms
step:48/2330 train_time:2674ms step_avg:55.70ms
step:49/2330 train_time:2728ms step_avg:55.67ms
step:50/2330 train_time:2787ms step_avg:55.74ms
step:51/2330 train_time:2841ms step_avg:55.71ms
step:52/2330 train_time:2899ms step_avg:55.75ms
step:53/2330 train_time:2953ms step_avg:55.72ms
step:54/2330 train_time:3011ms step_avg:55.76ms
step:55/2330 train_time:3065ms step_avg:55.73ms
step:56/2330 train_time:3123ms step_avg:55.77ms
step:57/2330 train_time:3177ms step_avg:55.74ms
step:58/2330 train_time:3235ms step_avg:55.77ms
step:59/2330 train_time:3289ms step_avg:55.75ms
step:60/2330 train_time:3347ms step_avg:55.78ms
step:61/2330 train_time:3402ms step_avg:55.76ms
step:62/2330 train_time:3460ms step_avg:55.80ms
step:63/2330 train_time:3515ms step_avg:55.79ms
step:64/2330 train_time:3572ms step_avg:55.81ms
step:65/2330 train_time:3627ms step_avg:55.80ms
step:66/2330 train_time:3685ms step_avg:55.83ms
step:67/2330 train_time:3740ms step_avg:55.81ms
step:68/2330 train_time:3797ms step_avg:55.84ms
step:69/2330 train_time:3852ms step_avg:55.83ms
step:70/2330 train_time:3910ms step_avg:55.86ms
step:71/2330 train_time:3965ms step_avg:55.85ms
step:72/2330 train_time:4022ms step_avg:55.87ms
step:73/2330 train_time:4077ms step_avg:55.85ms
step:74/2330 train_time:4135ms step_avg:55.88ms
step:75/2330 train_time:4189ms step_avg:55.86ms
step:76/2330 train_time:4248ms step_avg:55.89ms
step:77/2330 train_time:4303ms step_avg:55.88ms
step:78/2330 train_time:4361ms step_avg:55.91ms
step:79/2330 train_time:4416ms step_avg:55.90ms
step:80/2330 train_time:4474ms step_avg:55.93ms
step:81/2330 train_time:4529ms step_avg:55.91ms
step:82/2330 train_time:4587ms step_avg:55.94ms
step:83/2330 train_time:4642ms step_avg:55.93ms
step:84/2330 train_time:4701ms step_avg:55.96ms
step:85/2330 train_time:4756ms step_avg:55.95ms
step:86/2330 train_time:4814ms step_avg:55.98ms
step:87/2330 train_time:4869ms step_avg:55.97ms
step:88/2330 train_time:4927ms step_avg:55.99ms
step:89/2330 train_time:4981ms step_avg:55.97ms
step:90/2330 train_time:5040ms step_avg:56.00ms
step:91/2330 train_time:5095ms step_avg:55.99ms
step:92/2330 train_time:5153ms step_avg:56.01ms
step:93/2330 train_time:5207ms step_avg:55.99ms
step:94/2330 train_time:5266ms step_avg:56.02ms
step:95/2330 train_time:5320ms step_avg:56.00ms
step:96/2330 train_time:5379ms step_avg:56.03ms
step:97/2330 train_time:5434ms step_avg:56.02ms
step:98/2330 train_time:5492ms step_avg:56.04ms
step:99/2330 train_time:5546ms step_avg:56.02ms
step:100/2330 train_time:5605ms step_avg:56.05ms
step:101/2330 train_time:5660ms step_avg:56.04ms
step:102/2330 train_time:5719ms step_avg:56.07ms
step:103/2330 train_time:5774ms step_avg:56.06ms
step:104/2330 train_time:5833ms step_avg:56.09ms
step:105/2330 train_time:5887ms step_avg:56.07ms
step:106/2330 train_time:5946ms step_avg:56.09ms
step:107/2330 train_time:6001ms step_avg:56.08ms
step:108/2330 train_time:6060ms step_avg:56.11ms
step:109/2330 train_time:6115ms step_avg:56.10ms
step:110/2330 train_time:6173ms step_avg:56.12ms
step:111/2330 train_time:6227ms step_avg:56.10ms
step:112/2330 train_time:6286ms step_avg:56.12ms
step:113/2330 train_time:6340ms step_avg:56.11ms
step:114/2330 train_time:6400ms step_avg:56.14ms
step:115/2330 train_time:6455ms step_avg:56.13ms
step:116/2330 train_time:6513ms step_avg:56.14ms
step:117/2330 train_time:6568ms step_avg:56.13ms
step:118/2330 train_time:6625ms step_avg:56.15ms
step:119/2330 train_time:6680ms step_avg:56.13ms
step:120/2330 train_time:6739ms step_avg:56.16ms
step:121/2330 train_time:6794ms step_avg:56.15ms
step:122/2330 train_time:6853ms step_avg:56.17ms
step:123/2330 train_time:6907ms step_avg:56.16ms
step:124/2330 train_time:6966ms step_avg:56.18ms
step:125/2330 train_time:7020ms step_avg:56.16ms
step:126/2330 train_time:7079ms step_avg:56.19ms
step:127/2330 train_time:7135ms step_avg:56.18ms
step:128/2330 train_time:7193ms step_avg:56.19ms
step:129/2330 train_time:7248ms step_avg:56.19ms
step:130/2330 train_time:7307ms step_avg:56.20ms
step:131/2330 train_time:7361ms step_avg:56.19ms
step:132/2330 train_time:7419ms step_avg:56.21ms
step:133/2330 train_time:7475ms step_avg:56.20ms
step:134/2330 train_time:7533ms step_avg:56.21ms
step:135/2330 train_time:7588ms step_avg:56.20ms
step:136/2330 train_time:7646ms step_avg:56.22ms
step:137/2330 train_time:7701ms step_avg:56.21ms
step:138/2330 train_time:7760ms step_avg:56.23ms
step:139/2330 train_time:7815ms step_avg:56.22ms
step:140/2330 train_time:7873ms step_avg:56.24ms
step:141/2330 train_time:7928ms step_avg:56.23ms
step:142/2330 train_time:7987ms step_avg:56.24ms
step:143/2330 train_time:8041ms step_avg:56.23ms
step:144/2330 train_time:8100ms step_avg:56.25ms
step:145/2330 train_time:8155ms step_avg:56.24ms
step:146/2330 train_time:8214ms step_avg:56.26ms
step:147/2330 train_time:8269ms step_avg:56.25ms
step:148/2330 train_time:8328ms step_avg:56.27ms
step:149/2330 train_time:8383ms step_avg:56.26ms
step:150/2330 train_time:8443ms step_avg:56.28ms
step:151/2330 train_time:8497ms step_avg:56.27ms
step:152/2330 train_time:8556ms step_avg:56.29ms
step:153/2330 train_time:8611ms step_avg:56.28ms
step:154/2330 train_time:8670ms step_avg:56.30ms
step:155/2330 train_time:8725ms step_avg:56.29ms
step:156/2330 train_time:8783ms step_avg:56.30ms
step:157/2330 train_time:8838ms step_avg:56.29ms
step:158/2330 train_time:8897ms step_avg:56.31ms
step:159/2330 train_time:8952ms step_avg:56.30ms
step:160/2330 train_time:9011ms step_avg:56.32ms
step:161/2330 train_time:9067ms step_avg:56.31ms
step:162/2330 train_time:9124ms step_avg:56.32ms
step:163/2330 train_time:9180ms step_avg:56.32ms
step:164/2330 train_time:9238ms step_avg:56.33ms
step:165/2330 train_time:9294ms step_avg:56.32ms
step:166/2330 train_time:9352ms step_avg:56.34ms
step:167/2330 train_time:9407ms step_avg:56.33ms
step:168/2330 train_time:9466ms step_avg:56.35ms
step:169/2330 train_time:9521ms step_avg:56.34ms
step:170/2330 train_time:9580ms step_avg:56.35ms
step:171/2330 train_time:9636ms step_avg:56.35ms
step:172/2330 train_time:9694ms step_avg:56.36ms
step:173/2330 train_time:9749ms step_avg:56.35ms
step:174/2330 train_time:9809ms step_avg:56.37ms
step:175/2330 train_time:9864ms step_avg:56.36ms
step:176/2330 train_time:9923ms step_avg:56.38ms
step:177/2330 train_time:9977ms step_avg:56.37ms
step:178/2330 train_time:10038ms step_avg:56.39ms
step:179/2330 train_time:10094ms step_avg:56.39ms
step:180/2330 train_time:10152ms step_avg:56.40ms
step:181/2330 train_time:10207ms step_avg:56.39ms
step:182/2330 train_time:10265ms step_avg:56.40ms
step:183/2330 train_time:10321ms step_avg:56.40ms
step:184/2330 train_time:10380ms step_avg:56.41ms
step:185/2330 train_time:10436ms step_avg:56.41ms
step:186/2330 train_time:10494ms step_avg:56.42ms
step:187/2330 train_time:10549ms step_avg:56.41ms
step:188/2330 train_time:10609ms step_avg:56.43ms
step:189/2330 train_time:10664ms step_avg:56.42ms
step:190/2330 train_time:10724ms step_avg:56.44ms
step:191/2330 train_time:10779ms step_avg:56.43ms
step:192/2330 train_time:10837ms step_avg:56.44ms
step:193/2330 train_time:10893ms step_avg:56.44ms
step:194/2330 train_time:10951ms step_avg:56.45ms
step:195/2330 train_time:11006ms step_avg:56.44ms
step:196/2330 train_time:11065ms step_avg:56.45ms
step:197/2330 train_time:11120ms step_avg:56.45ms
step:198/2330 train_time:11179ms step_avg:56.46ms
step:199/2330 train_time:11234ms step_avg:56.45ms
step:200/2330 train_time:11293ms step_avg:56.46ms
step:201/2330 train_time:11348ms step_avg:56.46ms
step:202/2330 train_time:11407ms step_avg:56.47ms
step:203/2330 train_time:11462ms step_avg:56.47ms
step:204/2330 train_time:11522ms step_avg:56.48ms
step:205/2330 train_time:11577ms step_avg:56.47ms
step:206/2330 train_time:11636ms step_avg:56.49ms
step:207/2330 train_time:11692ms step_avg:56.48ms
step:208/2330 train_time:11750ms step_avg:56.49ms
step:209/2330 train_time:11804ms step_avg:56.48ms
step:210/2330 train_time:11866ms step_avg:56.50ms
step:211/2330 train_time:11921ms step_avg:56.50ms
step:212/2330 train_time:11980ms step_avg:56.51ms
step:213/2330 train_time:12035ms step_avg:56.50ms
step:214/2330 train_time:12094ms step_avg:56.51ms
step:215/2330 train_time:12149ms step_avg:56.51ms
step:216/2330 train_time:12208ms step_avg:56.52ms
step:217/2330 train_time:12263ms step_avg:56.51ms
step:218/2330 train_time:12322ms step_avg:56.52ms
step:219/2330 train_time:12378ms step_avg:56.52ms
step:220/2330 train_time:12437ms step_avg:56.53ms
step:221/2330 train_time:12493ms step_avg:56.53ms
step:222/2330 train_time:12551ms step_avg:56.54ms
step:223/2330 train_time:12606ms step_avg:56.53ms
step:224/2330 train_time:12665ms step_avg:56.54ms
step:225/2330 train_time:12720ms step_avg:56.53ms
step:226/2330 train_time:12780ms step_avg:56.55ms
step:227/2330 train_time:12836ms step_avg:56.55ms
step:228/2330 train_time:12894ms step_avg:56.55ms
step:229/2330 train_time:12950ms step_avg:56.55ms
step:230/2330 train_time:13009ms step_avg:56.56ms
step:231/2330 train_time:13064ms step_avg:56.55ms
step:232/2330 train_time:13123ms step_avg:56.57ms
step:233/2330 train_time:13179ms step_avg:56.56ms
step:234/2330 train_time:13237ms step_avg:56.57ms
step:235/2330 train_time:13292ms step_avg:56.56ms
step:236/2330 train_time:13351ms step_avg:56.57ms
step:237/2330 train_time:13406ms step_avg:56.57ms
step:238/2330 train_time:13465ms step_avg:56.58ms
step:239/2330 train_time:13521ms step_avg:56.57ms
step:240/2330 train_time:13581ms step_avg:56.59ms
step:241/2330 train_time:13637ms step_avg:56.58ms
step:242/2330 train_time:13695ms step_avg:56.59ms
step:243/2330 train_time:13750ms step_avg:56.59ms
step:244/2330 train_time:13811ms step_avg:56.60ms
step:245/2330 train_time:13866ms step_avg:56.60ms
step:246/2330 train_time:13925ms step_avg:56.60ms
step:247/2330 train_time:13979ms step_avg:56.60ms
step:248/2330 train_time:14040ms step_avg:56.61ms
step:249/2330 train_time:14096ms step_avg:56.61ms
step:250/2330 train_time:14155ms step_avg:56.62ms
step:250/2330 val_loss:6.4358 train_time:14234ms step_avg:56.94ms
step:251/2330 train_time:14253ms step_avg:56.78ms
step:252/2330 train_time:14272ms step_avg:56.63ms
step:253/2330 train_time:14326ms step_avg:56.63ms
step:254/2330 train_time:14388ms step_avg:56.65ms
step:255/2330 train_time:14443ms step_avg:56.64ms
step:256/2330 train_time:14505ms step_avg:56.66ms
step:257/2330 train_time:14559ms step_avg:56.65ms
step:258/2330 train_time:14620ms step_avg:56.67ms
step:259/2330 train_time:14676ms step_avg:56.66ms
step:260/2330 train_time:14735ms step_avg:56.67ms
step:261/2330 train_time:14790ms step_avg:56.67ms
step:262/2330 train_time:14848ms step_avg:56.67ms
step:263/2330 train_time:14903ms step_avg:56.66ms
step:264/2330 train_time:14962ms step_avg:56.67ms
step:265/2330 train_time:15018ms step_avg:56.67ms
step:266/2330 train_time:15076ms step_avg:56.68ms
step:267/2330 train_time:15132ms step_avg:56.67ms
step:268/2330 train_time:15192ms step_avg:56.69ms
step:269/2330 train_time:15248ms step_avg:56.68ms
step:270/2330 train_time:15307ms step_avg:56.69ms
step:271/2330 train_time:15363ms step_avg:56.69ms
step:272/2330 train_time:15423ms step_avg:56.70ms
step:273/2330 train_time:15479ms step_avg:56.70ms
step:274/2330 train_time:15538ms step_avg:56.71ms
step:275/2330 train_time:15593ms step_avg:56.70ms
step:276/2330 train_time:15654ms step_avg:56.72ms
step:277/2330 train_time:15709ms step_avg:56.71ms
step:278/2330 train_time:15768ms step_avg:56.72ms
step:279/2330 train_time:15823ms step_avg:56.71ms
step:280/2330 train_time:15882ms step_avg:56.72ms
step:281/2330 train_time:15937ms step_avg:56.72ms
step:282/2330 train_time:15995ms step_avg:56.72ms
step:283/2330 train_time:16050ms step_avg:56.71ms
step:284/2330 train_time:16109ms step_avg:56.72ms
step:285/2330 train_time:16165ms step_avg:56.72ms
step:286/2330 train_time:16224ms step_avg:56.73ms
step:287/2330 train_time:16280ms step_avg:56.73ms
step:288/2330 train_time:16339ms step_avg:56.73ms
step:289/2330 train_time:16396ms step_avg:56.73ms
step:290/2330 train_time:16455ms step_avg:56.74ms
step:291/2330 train_time:16511ms step_avg:56.74ms
step:292/2330 train_time:16571ms step_avg:56.75ms
step:293/2330 train_time:16627ms step_avg:56.75ms
step:294/2330 train_time:16687ms step_avg:56.76ms
step:295/2330 train_time:16742ms step_avg:56.75ms
step:296/2330 train_time:16802ms step_avg:56.76ms
step:297/2330 train_time:16857ms step_avg:56.76ms
step:298/2330 train_time:16916ms step_avg:56.76ms
step:299/2330 train_time:16971ms step_avg:56.76ms
step:300/2330 train_time:17030ms step_avg:56.77ms
step:301/2330 train_time:17085ms step_avg:56.76ms
step:302/2330 train_time:17144ms step_avg:56.77ms
step:303/2330 train_time:17200ms step_avg:56.76ms
step:304/2330 train_time:17259ms step_avg:56.77ms
step:305/2330 train_time:17315ms step_avg:56.77ms
step:306/2330 train_time:17374ms step_avg:56.78ms
step:307/2330 train_time:17430ms step_avg:56.77ms
step:308/2330 train_time:17490ms step_avg:56.78ms
step:309/2330 train_time:17545ms step_avg:56.78ms
step:310/2330 train_time:17605ms step_avg:56.79ms
step:311/2330 train_time:17660ms step_avg:56.79ms
step:312/2330 train_time:17719ms step_avg:56.79ms
step:313/2330 train_time:17775ms step_avg:56.79ms
step:314/2330 train_time:17833ms step_avg:56.79ms
step:315/2330 train_time:17888ms step_avg:56.79ms
step:316/2330 train_time:17948ms step_avg:56.80ms
step:317/2330 train_time:18003ms step_avg:56.79ms
step:318/2330 train_time:18063ms step_avg:56.80ms
step:319/2330 train_time:18118ms step_avg:56.80ms
step:320/2330 train_time:18177ms step_avg:56.80ms
step:321/2330 train_time:18232ms step_avg:56.80ms
step:322/2330 train_time:18291ms step_avg:56.81ms
step:323/2330 train_time:18347ms step_avg:56.80ms
step:324/2330 train_time:18407ms step_avg:56.81ms
step:325/2330 train_time:18463ms step_avg:56.81ms
step:326/2330 train_time:18523ms step_avg:56.82ms
step:327/2330 train_time:18578ms step_avg:56.81ms
step:328/2330 train_time:18637ms step_avg:56.82ms
step:329/2330 train_time:18693ms step_avg:56.82ms
step:330/2330 train_time:18751ms step_avg:56.82ms
step:331/2330 train_time:18806ms step_avg:56.82ms
step:332/2330 train_time:18865ms step_avg:56.82ms
step:333/2330 train_time:18921ms step_avg:56.82ms
step:334/2330 train_time:18980ms step_avg:56.83ms
step:335/2330 train_time:19036ms step_avg:56.82ms
step:336/2330 train_time:19095ms step_avg:56.83ms
step:337/2330 train_time:19150ms step_avg:56.83ms
step:338/2330 train_time:19211ms step_avg:56.84ms
step:339/2330 train_time:19266ms step_avg:56.83ms
step:340/2330 train_time:19326ms step_avg:56.84ms
step:341/2330 train_time:19381ms step_avg:56.84ms
step:342/2330 train_time:19441ms step_avg:56.85ms
step:343/2330 train_time:19497ms step_avg:56.84ms
step:344/2330 train_time:19556ms step_avg:56.85ms
step:345/2330 train_time:19612ms step_avg:56.85ms
step:346/2330 train_time:19673ms step_avg:56.86ms
step:347/2330 train_time:19728ms step_avg:56.85ms
step:348/2330 train_time:19788ms step_avg:56.86ms
step:349/2330 train_time:19843ms step_avg:56.86ms
step:350/2330 train_time:19903ms step_avg:56.87ms
step:351/2330 train_time:19959ms step_avg:56.86ms
step:352/2330 train_time:20018ms step_avg:56.87ms
step:353/2330 train_time:20073ms step_avg:56.86ms
step:354/2330 train_time:20133ms step_avg:56.87ms
step:355/2330 train_time:20188ms step_avg:56.87ms
step:356/2330 train_time:20248ms step_avg:56.88ms
step:357/2330 train_time:20303ms step_avg:56.87ms
step:358/2330 train_time:20363ms step_avg:56.88ms
step:359/2330 train_time:20419ms step_avg:56.88ms
step:360/2330 train_time:20478ms step_avg:56.88ms
step:361/2330 train_time:20534ms step_avg:56.88ms
step:362/2330 train_time:20593ms step_avg:56.89ms
step:363/2330 train_time:20649ms step_avg:56.88ms
step:364/2330 train_time:20708ms step_avg:56.89ms
step:365/2330 train_time:20764ms step_avg:56.89ms
step:366/2330 train_time:20822ms step_avg:56.89ms
step:367/2330 train_time:20878ms step_avg:56.89ms
step:368/2330 train_time:20937ms step_avg:56.89ms
step:369/2330 train_time:20993ms step_avg:56.89ms
step:370/2330 train_time:21051ms step_avg:56.90ms
step:371/2330 train_time:21107ms step_avg:56.89ms
step:372/2330 train_time:21165ms step_avg:56.90ms
step:373/2330 train_time:21221ms step_avg:56.89ms
step:374/2330 train_time:21281ms step_avg:56.90ms
step:375/2330 train_time:21337ms step_avg:56.90ms
step:376/2330 train_time:21396ms step_avg:56.90ms
step:377/2330 train_time:21451ms step_avg:56.90ms
step:378/2330 train_time:21511ms step_avg:56.91ms
step:379/2330 train_time:21567ms step_avg:56.90ms
step:380/2330 train_time:21626ms step_avg:56.91ms
step:381/2330 train_time:21681ms step_avg:56.91ms
step:382/2330 train_time:21741ms step_avg:56.91ms
step:383/2330 train_time:21797ms step_avg:56.91ms
step:384/2330 train_time:21856ms step_avg:56.92ms
step:385/2330 train_time:21911ms step_avg:56.91ms
step:386/2330 train_time:21970ms step_avg:56.92ms
step:387/2330 train_time:22026ms step_avg:56.91ms
step:388/2330 train_time:22086ms step_avg:56.92ms
step:389/2330 train_time:22141ms step_avg:56.92ms
step:390/2330 train_time:22200ms step_avg:56.92ms
step:391/2330 train_time:22256ms step_avg:56.92ms
step:392/2330 train_time:22315ms step_avg:56.93ms
step:393/2330 train_time:22371ms step_avg:56.92ms
step:394/2330 train_time:22431ms step_avg:56.93ms
step:395/2330 train_time:22486ms step_avg:56.93ms
step:396/2330 train_time:22546ms step_avg:56.93ms
step:397/2330 train_time:22601ms step_avg:56.93ms
step:398/2330 train_time:22660ms step_avg:56.94ms
step:399/2330 train_time:22716ms step_avg:56.93ms
step:400/2330 train_time:22776ms step_avg:56.94ms
step:401/2330 train_time:22832ms step_avg:56.94ms
step:402/2330 train_time:22891ms step_avg:56.94ms
step:403/2330 train_time:22947ms step_avg:56.94ms
step:404/2330 train_time:23006ms step_avg:56.95ms
step:405/2330 train_time:23062ms step_avg:56.94ms
step:406/2330 train_time:23121ms step_avg:56.95ms
step:407/2330 train_time:23177ms step_avg:56.95ms
step:408/2330 train_time:23235ms step_avg:56.95ms
step:409/2330 train_time:23291ms step_avg:56.95ms
step:410/2330 train_time:23350ms step_avg:56.95ms
step:411/2330 train_time:23405ms step_avg:56.95ms
step:412/2330 train_time:23464ms step_avg:56.95ms
step:413/2330 train_time:23520ms step_avg:56.95ms
step:414/2330 train_time:23578ms step_avg:56.95ms
step:415/2330 train_time:23634ms step_avg:56.95ms
step:416/2330 train_time:23694ms step_avg:56.96ms
step:417/2330 train_time:23749ms step_avg:56.95ms
step:418/2330 train_time:23811ms step_avg:56.96ms
step:419/2330 train_time:23866ms step_avg:56.96ms
step:420/2330 train_time:23926ms step_avg:56.97ms
step:421/2330 train_time:23981ms step_avg:56.96ms
step:422/2330 train_time:24040ms step_avg:56.97ms
step:423/2330 train_time:24096ms step_avg:56.96ms
step:424/2330 train_time:24155ms step_avg:56.97ms
step:425/2330 train_time:24211ms step_avg:56.97ms
step:426/2330 train_time:24270ms step_avg:56.97ms
step:427/2330 train_time:24326ms step_avg:56.97ms
step:428/2330 train_time:24384ms step_avg:56.97ms
step:429/2330 train_time:24440ms step_avg:56.97ms
step:430/2330 train_time:24499ms step_avg:56.97ms
step:431/2330 train_time:24555ms step_avg:56.97ms
step:432/2330 train_time:24614ms step_avg:56.98ms
step:433/2330 train_time:24670ms step_avg:56.97ms
step:434/2330 train_time:24730ms step_avg:56.98ms
step:435/2330 train_time:24785ms step_avg:56.98ms
step:436/2330 train_time:24845ms step_avg:56.98ms
step:437/2330 train_time:24900ms step_avg:56.98ms
step:438/2330 train_time:24960ms step_avg:56.99ms
step:439/2330 train_time:25015ms step_avg:56.98ms
step:440/2330 train_time:25075ms step_avg:56.99ms
step:441/2330 train_time:25130ms step_avg:56.98ms
step:442/2330 train_time:25191ms step_avg:56.99ms
step:443/2330 train_time:25246ms step_avg:56.99ms
step:444/2330 train_time:25305ms step_avg:56.99ms
step:445/2330 train_time:25360ms step_avg:56.99ms
step:446/2330 train_time:25420ms step_avg:57.00ms
step:447/2330 train_time:25476ms step_avg:56.99ms
step:448/2330 train_time:25535ms step_avg:57.00ms
step:449/2330 train_time:25591ms step_avg:57.00ms
step:450/2330 train_time:25650ms step_avg:57.00ms
step:451/2330 train_time:25706ms step_avg:57.00ms
step:452/2330 train_time:25765ms step_avg:57.00ms
step:453/2330 train_time:25820ms step_avg:57.00ms
step:454/2330 train_time:25880ms step_avg:57.00ms
step:455/2330 train_time:25936ms step_avg:57.00ms
step:456/2330 train_time:25995ms step_avg:57.01ms
step:457/2330 train_time:26051ms step_avg:57.00ms
step:458/2330 train_time:26109ms step_avg:57.01ms
step:459/2330 train_time:26164ms step_avg:57.00ms
step:460/2330 train_time:26224ms step_avg:57.01ms
step:461/2330 train_time:26280ms step_avg:57.01ms
step:462/2330 train_time:26338ms step_avg:57.01ms
step:463/2330 train_time:26394ms step_avg:57.01ms
step:464/2330 train_time:26453ms step_avg:57.01ms
step:465/2330 train_time:26509ms step_avg:57.01ms
step:466/2330 train_time:26569ms step_avg:57.01ms
step:467/2330 train_time:26624ms step_avg:57.01ms
step:468/2330 train_time:26684ms step_avg:57.02ms
step:469/2330 train_time:26740ms step_avg:57.01ms
step:470/2330 train_time:26799ms step_avg:57.02ms
step:471/2330 train_time:26855ms step_avg:57.02ms
step:472/2330 train_time:26914ms step_avg:57.02ms
step:473/2330 train_time:26969ms step_avg:57.02ms
step:474/2330 train_time:27028ms step_avg:57.02ms
step:475/2330 train_time:27083ms step_avg:57.02ms
step:476/2330 train_time:27142ms step_avg:57.02ms
step:477/2330 train_time:27198ms step_avg:57.02ms
step:478/2330 train_time:27257ms step_avg:57.02ms
step:479/2330 train_time:27312ms step_avg:57.02ms
step:480/2330 train_time:27372ms step_avg:57.02ms
step:481/2330 train_time:27428ms step_avg:57.02ms
step:482/2330 train_time:27486ms step_avg:57.02ms
step:483/2330 train_time:27541ms step_avg:57.02ms
step:484/2330 train_time:27600ms step_avg:57.03ms
step:485/2330 train_time:27656ms step_avg:57.02ms
step:486/2330 train_time:27716ms step_avg:57.03ms
step:487/2330 train_time:27771ms step_avg:57.03ms
step:488/2330 train_time:27830ms step_avg:57.03ms
step:489/2330 train_time:27886ms step_avg:57.03ms
step:490/2330 train_time:27945ms step_avg:57.03ms
step:491/2330 train_time:28000ms step_avg:57.03ms
step:492/2330 train_time:28060ms step_avg:57.03ms
step:493/2330 train_time:28116ms step_avg:57.03ms
step:494/2330 train_time:28175ms step_avg:57.03ms
step:495/2330 train_time:28231ms step_avg:57.03ms
step:496/2330 train_time:28290ms step_avg:57.04ms
step:497/2330 train_time:28346ms step_avg:57.03ms
step:498/2330 train_time:28405ms step_avg:57.04ms
step:499/2330 train_time:28460ms step_avg:57.03ms
step:500/2330 train_time:28520ms step_avg:57.04ms
step:500/2330 val_loss:5.7640 train_time:28598ms step_avg:57.20ms
step:501/2330 train_time:28616ms step_avg:57.12ms
step:502/2330 train_time:28636ms step_avg:57.04ms
step:503/2330 train_time:28694ms step_avg:57.05ms
step:504/2330 train_time:28758ms step_avg:57.06ms
step:505/2330 train_time:28814ms step_avg:57.06ms
step:506/2330 train_time:28874ms step_avg:57.06ms
step:507/2330 train_time:28930ms step_avg:57.06ms
step:508/2330 train_time:28990ms step_avg:57.07ms
step:509/2330 train_time:29045ms step_avg:57.06ms
step:510/2330 train_time:29104ms step_avg:57.07ms
step:511/2330 train_time:29159ms step_avg:57.06ms
step:512/2330 train_time:29217ms step_avg:57.07ms
step:513/2330 train_time:29273ms step_avg:57.06ms
step:514/2330 train_time:29332ms step_avg:57.07ms
step:515/2330 train_time:29387ms step_avg:57.06ms
step:516/2330 train_time:29446ms step_avg:57.07ms
step:517/2330 train_time:29501ms step_avg:57.06ms
step:518/2330 train_time:29560ms step_avg:57.07ms
step:519/2330 train_time:29616ms step_avg:57.06ms
step:520/2330 train_time:29678ms step_avg:57.07ms
step:521/2330 train_time:29735ms step_avg:57.07ms
step:522/2330 train_time:29795ms step_avg:57.08ms
step:523/2330 train_time:29852ms step_avg:57.08ms
step:524/2330 train_time:29912ms step_avg:57.08ms
step:525/2330 train_time:29967ms step_avg:57.08ms
step:526/2330 train_time:30028ms step_avg:57.09ms
step:527/2330 train_time:30083ms step_avg:57.08ms
step:528/2330 train_time:30142ms step_avg:57.09ms
step:529/2330 train_time:30198ms step_avg:57.08ms
step:530/2330 train_time:30256ms step_avg:57.09ms
step:531/2330 train_time:30312ms step_avg:57.08ms
step:532/2330 train_time:30371ms step_avg:57.09ms
step:533/2330 train_time:30427ms step_avg:57.09ms
step:534/2330 train_time:30484ms step_avg:57.09ms
step:535/2330 train_time:30540ms step_avg:57.08ms
step:536/2330 train_time:30599ms step_avg:57.09ms
step:537/2330 train_time:30655ms step_avg:57.09ms
step:538/2330 train_time:30715ms step_avg:57.09ms
step:539/2330 train_time:30772ms step_avg:57.09ms
step:540/2330 train_time:30832ms step_avg:57.10ms
step:541/2330 train_time:30887ms step_avg:57.09ms
step:542/2330 train_time:30948ms step_avg:57.10ms
step:543/2330 train_time:31004ms step_avg:57.10ms
step:544/2330 train_time:31063ms step_avg:57.10ms
step:545/2330 train_time:31119ms step_avg:57.10ms
step:546/2330 train_time:31178ms step_avg:57.10ms
step:547/2330 train_time:31233ms step_avg:57.10ms
step:548/2330 train_time:31292ms step_avg:57.10ms
step:549/2330 train_time:31348ms step_avg:57.10ms
step:550/2330 train_time:31406ms step_avg:57.10ms
step:551/2330 train_time:31462ms step_avg:57.10ms
step:552/2330 train_time:31521ms step_avg:57.10ms
step:553/2330 train_time:31576ms step_avg:57.10ms
step:554/2330 train_time:31635ms step_avg:57.10ms
step:555/2330 train_time:31691ms step_avg:57.10ms
step:556/2330 train_time:31751ms step_avg:57.11ms
step:557/2330 train_time:31807ms step_avg:57.10ms
step:558/2330 train_time:31868ms step_avg:57.11ms
step:559/2330 train_time:31924ms step_avg:57.11ms
step:560/2330 train_time:31983ms step_avg:57.11ms
step:561/2330 train_time:32039ms step_avg:57.11ms
step:562/2330 train_time:32099ms step_avg:57.12ms
step:563/2330 train_time:32154ms step_avg:57.11ms
step:564/2330 train_time:32214ms step_avg:57.12ms
step:565/2330 train_time:32270ms step_avg:57.11ms
step:566/2330 train_time:32331ms step_avg:57.12ms
step:567/2330 train_time:32386ms step_avg:57.12ms
step:568/2330 train_time:32445ms step_avg:57.12ms
step:569/2330 train_time:32500ms step_avg:57.12ms
step:570/2330 train_time:32559ms step_avg:57.12ms
step:571/2330 train_time:32615ms step_avg:57.12ms
step:572/2330 train_time:32674ms step_avg:57.12ms
step:573/2330 train_time:32730ms step_avg:57.12ms
step:574/2330 train_time:32790ms step_avg:57.12ms
step:575/2330 train_time:32845ms step_avg:57.12ms
step:576/2330 train_time:32905ms step_avg:57.13ms
step:577/2330 train_time:32962ms step_avg:57.13ms
step:578/2330 train_time:33020ms step_avg:57.13ms
step:579/2330 train_time:33077ms step_avg:57.13ms
step:580/2330 train_time:33136ms step_avg:57.13ms
step:581/2330 train_time:33191ms step_avg:57.13ms
step:582/2330 train_time:33251ms step_avg:57.13ms
step:583/2330 train_time:33306ms step_avg:57.13ms
step:584/2330 train_time:33366ms step_avg:57.13ms
step:585/2330 train_time:33421ms step_avg:57.13ms
step:586/2330 train_time:33481ms step_avg:57.13ms
step:587/2330 train_time:33536ms step_avg:57.13ms
step:588/2330 train_time:33595ms step_avg:57.13ms
step:589/2330 train_time:33650ms step_avg:57.13ms
step:590/2330 train_time:33709ms step_avg:57.13ms
step:591/2330 train_time:33766ms step_avg:57.13ms
step:592/2330 train_time:33825ms step_avg:57.14ms
step:593/2330 train_time:33880ms step_avg:57.13ms
step:594/2330 train_time:33940ms step_avg:57.14ms
step:595/2330 train_time:33996ms step_avg:57.14ms
step:596/2330 train_time:34056ms step_avg:57.14ms
step:597/2330 train_time:34112ms step_avg:57.14ms
step:598/2330 train_time:34172ms step_avg:57.14ms
step:599/2330 train_time:34227ms step_avg:57.14ms
step:600/2330 train_time:34287ms step_avg:57.14ms
step:601/2330 train_time:34343ms step_avg:57.14ms
step:602/2330 train_time:34401ms step_avg:57.15ms
step:603/2330 train_time:34458ms step_avg:57.14ms
step:604/2330 train_time:34517ms step_avg:57.15ms
step:605/2330 train_time:34572ms step_avg:57.14ms
step:606/2330 train_time:34631ms step_avg:57.15ms
step:607/2330 train_time:34686ms step_avg:57.14ms
step:608/2330 train_time:34746ms step_avg:57.15ms
step:609/2330 train_time:34801ms step_avg:57.15ms
step:610/2330 train_time:34862ms step_avg:57.15ms
step:611/2330 train_time:34918ms step_avg:57.15ms
step:612/2330 train_time:34977ms step_avg:57.15ms
step:613/2330 train_time:35033ms step_avg:57.15ms
step:614/2330 train_time:35093ms step_avg:57.16ms
step:615/2330 train_time:35149ms step_avg:57.15ms
step:616/2330 train_time:35209ms step_avg:57.16ms
step:617/2330 train_time:35265ms step_avg:57.16ms
step:618/2330 train_time:35325ms step_avg:57.16ms
step:619/2330 train_time:35381ms step_avg:57.16ms
step:620/2330 train_time:35439ms step_avg:57.16ms
step:621/2330 train_time:35495ms step_avg:57.16ms
step:622/2330 train_time:35555ms step_avg:57.16ms
step:623/2330 train_time:35611ms step_avg:57.16ms
step:624/2330 train_time:35671ms step_avg:57.16ms
step:625/2330 train_time:35726ms step_avg:57.16ms
step:626/2330 train_time:35785ms step_avg:57.17ms
step:627/2330 train_time:35841ms step_avg:57.16ms
step:628/2330 train_time:35900ms step_avg:57.17ms
step:629/2330 train_time:35956ms step_avg:57.16ms
step:630/2330 train_time:36016ms step_avg:57.17ms
step:631/2330 train_time:36072ms step_avg:57.17ms
step:632/2330 train_time:36132ms step_avg:57.17ms
step:633/2330 train_time:36188ms step_avg:57.17ms
step:634/2330 train_time:36248ms step_avg:57.17ms
step:635/2330 train_time:36304ms step_avg:57.17ms
step:636/2330 train_time:36364ms step_avg:57.18ms
step:637/2330 train_time:36420ms step_avg:57.17ms
step:638/2330 train_time:36478ms step_avg:57.18ms
step:639/2330 train_time:36535ms step_avg:57.17ms
step:640/2330 train_time:36595ms step_avg:57.18ms
step:641/2330 train_time:36652ms step_avg:57.18ms
step:642/2330 train_time:36711ms step_avg:57.18ms
step:643/2330 train_time:36767ms step_avg:57.18ms
step:644/2330 train_time:36826ms step_avg:57.18ms
step:645/2330 train_time:36881ms step_avg:57.18ms
step:646/2330 train_time:36941ms step_avg:57.18ms
step:647/2330 train_time:36997ms step_avg:57.18ms
step:648/2330 train_time:37057ms step_avg:57.19ms
step:649/2330 train_time:37113ms step_avg:57.19ms
step:650/2330 train_time:37172ms step_avg:57.19ms
step:651/2330 train_time:37228ms step_avg:57.19ms
step:652/2330 train_time:37287ms step_avg:57.19ms
step:653/2330 train_time:37343ms step_avg:57.19ms
step:654/2330 train_time:37402ms step_avg:57.19ms
step:655/2330 train_time:37458ms step_avg:57.19ms
step:656/2330 train_time:37517ms step_avg:57.19ms
step:657/2330 train_time:37573ms step_avg:57.19ms
step:658/2330 train_time:37633ms step_avg:57.19ms
step:659/2330 train_time:37690ms step_avg:57.19ms
step:660/2330 train_time:37748ms step_avg:57.19ms
step:661/2330 train_time:37804ms step_avg:57.19ms
step:662/2330 train_time:37864ms step_avg:57.20ms
step:663/2330 train_time:37919ms step_avg:57.19ms
step:664/2330 train_time:37978ms step_avg:57.20ms
step:665/2330 train_time:38034ms step_avg:57.19ms
step:666/2330 train_time:38094ms step_avg:57.20ms
step:667/2330 train_time:38150ms step_avg:57.20ms
step:668/2330 train_time:38211ms step_avg:57.20ms
step:669/2330 train_time:38266ms step_avg:57.20ms
step:670/2330 train_time:38325ms step_avg:57.20ms
step:671/2330 train_time:38381ms step_avg:57.20ms
step:672/2330 train_time:38441ms step_avg:57.20ms
step:673/2330 train_time:38498ms step_avg:57.20ms
step:674/2330 train_time:38557ms step_avg:57.21ms
step:675/2330 train_time:38613ms step_avg:57.20ms
step:676/2330 train_time:38673ms step_avg:57.21ms
step:677/2330 train_time:38729ms step_avg:57.21ms
step:678/2330 train_time:38788ms step_avg:57.21ms
step:679/2330 train_time:38844ms step_avg:57.21ms
step:680/2330 train_time:38903ms step_avg:57.21ms
step:681/2330 train_time:38959ms step_avg:57.21ms
step:682/2330 train_time:39018ms step_avg:57.21ms
step:683/2330 train_time:39075ms step_avg:57.21ms
step:684/2330 train_time:39136ms step_avg:57.22ms
step:685/2330 train_time:39192ms step_avg:57.22ms
step:686/2330 train_time:39251ms step_avg:57.22ms
step:687/2330 train_time:39307ms step_avg:57.22ms
step:688/2330 train_time:39366ms step_avg:57.22ms
step:689/2330 train_time:39422ms step_avg:57.22ms
step:690/2330 train_time:39481ms step_avg:57.22ms
step:691/2330 train_time:39538ms step_avg:57.22ms
step:692/2330 train_time:39597ms step_avg:57.22ms
step:693/2330 train_time:39653ms step_avg:57.22ms
step:694/2330 train_time:39712ms step_avg:57.22ms
step:695/2330 train_time:39768ms step_avg:57.22ms
step:696/2330 train_time:39828ms step_avg:57.22ms
step:697/2330 train_time:39884ms step_avg:57.22ms
step:698/2330 train_time:39944ms step_avg:57.23ms
step:699/2330 train_time:40000ms step_avg:57.22ms
step:700/2330 train_time:40059ms step_avg:57.23ms
step:701/2330 train_time:40115ms step_avg:57.23ms
step:702/2330 train_time:40174ms step_avg:57.23ms
step:703/2330 train_time:40230ms step_avg:57.23ms
step:704/2330 train_time:40289ms step_avg:57.23ms
step:705/2330 train_time:40345ms step_avg:57.23ms
step:706/2330 train_time:40404ms step_avg:57.23ms
step:707/2330 train_time:40460ms step_avg:57.23ms
step:708/2330 train_time:40520ms step_avg:57.23ms
step:709/2330 train_time:40576ms step_avg:57.23ms
step:710/2330 train_time:40636ms step_avg:57.23ms
step:711/2330 train_time:40692ms step_avg:57.23ms
step:712/2330 train_time:40751ms step_avg:57.23ms
step:713/2330 train_time:40806ms step_avg:57.23ms
step:714/2330 train_time:40867ms step_avg:57.24ms
step:715/2330 train_time:40922ms step_avg:57.23ms
step:716/2330 train_time:40982ms step_avg:57.24ms
step:717/2330 train_time:41039ms step_avg:57.24ms
step:718/2330 train_time:41098ms step_avg:57.24ms
step:719/2330 train_time:41154ms step_avg:57.24ms
step:720/2330 train_time:41214ms step_avg:57.24ms
step:721/2330 train_time:41270ms step_avg:57.24ms
step:722/2330 train_time:41330ms step_avg:57.24ms
step:723/2330 train_time:41385ms step_avg:57.24ms
step:724/2330 train_time:41445ms step_avg:57.24ms
step:725/2330 train_time:41501ms step_avg:57.24ms
step:726/2330 train_time:41560ms step_avg:57.25ms
step:727/2330 train_time:41616ms step_avg:57.24ms
step:728/2330 train_time:41676ms step_avg:57.25ms
step:729/2330 train_time:41732ms step_avg:57.25ms
step:730/2330 train_time:41793ms step_avg:57.25ms
step:731/2330 train_time:41848ms step_avg:57.25ms
step:732/2330 train_time:41909ms step_avg:57.25ms
step:733/2330 train_time:41964ms step_avg:57.25ms
step:734/2330 train_time:42024ms step_avg:57.25ms
step:735/2330 train_time:42080ms step_avg:57.25ms
step:736/2330 train_time:42139ms step_avg:57.25ms
step:737/2330 train_time:42195ms step_avg:57.25ms
step:738/2330 train_time:42255ms step_avg:57.26ms
step:739/2330 train_time:42311ms step_avg:57.25ms
step:740/2330 train_time:42370ms step_avg:57.26ms
step:741/2330 train_time:42426ms step_avg:57.26ms
step:742/2330 train_time:42485ms step_avg:57.26ms
step:743/2330 train_time:42541ms step_avg:57.26ms
step:744/2330 train_time:42601ms step_avg:57.26ms
step:745/2330 train_time:42658ms step_avg:57.26ms
step:746/2330 train_time:42717ms step_avg:57.26ms
step:747/2330 train_time:42774ms step_avg:57.26ms
step:748/2330 train_time:42833ms step_avg:57.26ms
step:749/2330 train_time:42890ms step_avg:57.26ms
step:750/2330 train_time:42951ms step_avg:57.27ms
step:750/2330 val_loss:5.3258 train_time:43031ms step_avg:57.37ms
step:751/2330 train_time:43049ms step_avg:57.32ms
step:752/2330 train_time:43069ms step_avg:57.27ms
step:753/2330 train_time:43126ms step_avg:57.27ms
step:754/2330 train_time:43191ms step_avg:57.28ms
step:755/2330 train_time:43246ms step_avg:57.28ms
step:756/2330 train_time:43308ms step_avg:57.29ms
step:757/2330 train_time:43364ms step_avg:57.28ms
step:758/2330 train_time:43424ms step_avg:57.29ms
step:759/2330 train_time:43479ms step_avg:57.28ms
step:760/2330 train_time:43538ms step_avg:57.29ms
step:761/2330 train_time:43594ms step_avg:57.28ms
step:762/2330 train_time:43653ms step_avg:57.29ms
step:763/2330 train_time:43709ms step_avg:57.29ms
step:764/2330 train_time:43767ms step_avg:57.29ms
step:765/2330 train_time:43824ms step_avg:57.29ms
step:766/2330 train_time:43882ms step_avg:57.29ms
step:767/2330 train_time:43938ms step_avg:57.29ms
step:768/2330 train_time:43999ms step_avg:57.29ms
step:769/2330 train_time:44056ms step_avg:57.29ms
step:770/2330 train_time:44118ms step_avg:57.30ms
step:771/2330 train_time:44176ms step_avg:57.30ms
step:772/2330 train_time:44237ms step_avg:57.30ms
step:773/2330 train_time:44295ms step_avg:57.30ms
step:774/2330 train_time:44356ms step_avg:57.31ms
step:775/2330 train_time:44414ms step_avg:57.31ms
step:776/2330 train_time:44473ms step_avg:57.31ms
step:777/2330 train_time:44529ms step_avg:57.31ms
step:778/2330 train_time:44591ms step_avg:57.31ms
step:779/2330 train_time:44647ms step_avg:57.31ms
step:780/2330 train_time:44707ms step_avg:57.32ms
step:781/2330 train_time:44763ms step_avg:57.31ms
step:782/2330 train_time:44822ms step_avg:57.32ms
step:783/2330 train_time:44878ms step_avg:57.32ms
step:784/2330 train_time:44938ms step_avg:57.32ms
step:785/2330 train_time:44995ms step_avg:57.32ms
step:786/2330 train_time:45055ms step_avg:57.32ms
step:787/2330 train_time:45112ms step_avg:57.32ms
step:788/2330 train_time:45173ms step_avg:57.33ms
step:789/2330 train_time:45230ms step_avg:57.33ms
step:790/2330 train_time:45291ms step_avg:57.33ms
step:791/2330 train_time:45347ms step_avg:57.33ms
step:792/2330 train_time:45409ms step_avg:57.33ms
step:793/2330 train_time:45465ms step_avg:57.33ms
step:794/2330 train_time:45525ms step_avg:57.34ms
step:795/2330 train_time:45582ms step_avg:57.34ms
step:796/2330 train_time:45641ms step_avg:57.34ms
step:797/2330 train_time:45698ms step_avg:57.34ms
step:798/2330 train_time:45758ms step_avg:57.34ms
step:799/2330 train_time:45814ms step_avg:57.34ms
step:800/2330 train_time:45874ms step_avg:57.34ms
step:801/2330 train_time:45930ms step_avg:57.34ms
step:802/2330 train_time:45991ms step_avg:57.35ms
step:803/2330 train_time:46048ms step_avg:57.34ms
step:804/2330 train_time:46108ms step_avg:57.35ms
step:805/2330 train_time:46164ms step_avg:57.35ms
step:806/2330 train_time:46225ms step_avg:57.35ms
step:807/2330 train_time:46282ms step_avg:57.35ms
step:808/2330 train_time:46342ms step_avg:57.35ms
step:809/2330 train_time:46399ms step_avg:57.35ms
step:810/2330 train_time:46459ms step_avg:57.36ms
step:811/2330 train_time:46516ms step_avg:57.36ms
step:812/2330 train_time:46577ms step_avg:57.36ms
step:813/2330 train_time:46633ms step_avg:57.36ms
step:814/2330 train_time:46694ms step_avg:57.36ms
step:815/2330 train_time:46750ms step_avg:57.36ms
step:816/2330 train_time:46810ms step_avg:57.37ms
step:817/2330 train_time:46867ms step_avg:57.36ms
step:818/2330 train_time:46926ms step_avg:57.37ms
step:819/2330 train_time:46982ms step_avg:57.37ms
step:820/2330 train_time:47043ms step_avg:57.37ms
step:821/2330 train_time:47100ms step_avg:57.37ms
step:822/2330 train_time:47160ms step_avg:57.37ms
step:823/2330 train_time:47217ms step_avg:57.37ms
step:824/2330 train_time:47278ms step_avg:57.38ms
step:825/2330 train_time:47335ms step_avg:57.38ms
step:826/2330 train_time:47396ms step_avg:57.38ms
step:827/2330 train_time:47453ms step_avg:57.38ms
step:828/2330 train_time:47513ms step_avg:57.38ms
step:829/2330 train_time:47570ms step_avg:57.38ms
step:830/2330 train_time:47630ms step_avg:57.39ms
step:831/2330 train_time:47686ms step_avg:57.38ms
step:832/2330 train_time:47746ms step_avg:57.39ms
step:833/2330 train_time:47803ms step_avg:57.39ms
step:834/2330 train_time:47863ms step_avg:57.39ms
step:835/2330 train_time:47919ms step_avg:57.39ms
step:836/2330 train_time:47979ms step_avg:57.39ms
step:837/2330 train_time:48036ms step_avg:57.39ms
step:838/2330 train_time:48097ms step_avg:57.39ms
step:839/2330 train_time:48154ms step_avg:57.39ms
step:840/2330 train_time:48214ms step_avg:57.40ms
step:841/2330 train_time:48270ms step_avg:57.40ms
step:842/2330 train_time:48331ms step_avg:57.40ms
step:843/2330 train_time:48387ms step_avg:57.40ms
step:844/2330 train_time:48448ms step_avg:57.40ms
step:845/2330 train_time:48505ms step_avg:57.40ms
step:846/2330 train_time:48565ms step_avg:57.41ms
step:847/2330 train_time:48621ms step_avg:57.40ms
step:848/2330 train_time:48682ms step_avg:57.41ms
step:849/2330 train_time:48738ms step_avg:57.41ms
step:850/2330 train_time:48799ms step_avg:57.41ms
step:851/2330 train_time:48856ms step_avg:57.41ms
step:852/2330 train_time:48916ms step_avg:57.41ms
step:853/2330 train_time:48972ms step_avg:57.41ms
step:854/2330 train_time:49033ms step_avg:57.42ms
step:855/2330 train_time:49089ms step_avg:57.41ms
step:856/2330 train_time:49150ms step_avg:57.42ms
step:857/2330 train_time:49206ms step_avg:57.42ms
step:858/2330 train_time:49268ms step_avg:57.42ms
step:859/2330 train_time:49324ms step_avg:57.42ms
step:860/2330 train_time:49384ms step_avg:57.42ms
step:861/2330 train_time:49440ms step_avg:57.42ms
step:862/2330 train_time:49501ms step_avg:57.43ms
step:863/2330 train_time:49557ms step_avg:57.42ms
step:864/2330 train_time:49618ms step_avg:57.43ms
step:865/2330 train_time:49674ms step_avg:57.43ms
step:866/2330 train_time:49735ms step_avg:57.43ms
step:867/2330 train_time:49792ms step_avg:57.43ms
step:868/2330 train_time:49852ms step_avg:57.43ms
step:869/2330 train_time:49909ms step_avg:57.43ms
step:870/2330 train_time:49969ms step_avg:57.44ms
step:871/2330 train_time:50025ms step_avg:57.43ms
step:872/2330 train_time:50086ms step_avg:57.44ms
step:873/2330 train_time:50143ms step_avg:57.44ms
step:874/2330 train_time:50204ms step_avg:57.44ms
step:875/2330 train_time:50260ms step_avg:57.44ms
step:876/2330 train_time:50320ms step_avg:57.44ms
step:877/2330 train_time:50377ms step_avg:57.44ms
step:878/2330 train_time:50438ms step_avg:57.45ms
step:879/2330 train_time:50495ms step_avg:57.45ms
step:880/2330 train_time:50555ms step_avg:57.45ms
step:881/2330 train_time:50612ms step_avg:57.45ms
step:882/2330 train_time:50671ms step_avg:57.45ms
step:883/2330 train_time:50727ms step_avg:57.45ms
step:884/2330 train_time:50787ms step_avg:57.45ms
step:885/2330 train_time:50844ms step_avg:57.45ms
step:886/2330 train_time:50904ms step_avg:57.45ms
step:887/2330 train_time:50962ms step_avg:57.45ms
step:888/2330 train_time:51022ms step_avg:57.46ms
step:889/2330 train_time:51079ms step_avg:57.46ms
step:890/2330 train_time:51139ms step_avg:57.46ms
step:891/2330 train_time:51195ms step_avg:57.46ms
step:892/2330 train_time:51256ms step_avg:57.46ms
step:893/2330 train_time:51312ms step_avg:57.46ms
step:894/2330 train_time:51373ms step_avg:57.46ms
step:895/2330 train_time:51430ms step_avg:57.46ms
step:896/2330 train_time:51490ms step_avg:57.47ms
step:897/2330 train_time:51546ms step_avg:57.47ms
step:898/2330 train_time:51607ms step_avg:57.47ms
step:899/2330 train_time:51663ms step_avg:57.47ms
step:900/2330 train_time:51724ms step_avg:57.47ms
step:901/2330 train_time:51780ms step_avg:57.47ms
step:902/2330 train_time:51840ms step_avg:57.47ms
step:903/2330 train_time:51897ms step_avg:57.47ms
step:904/2330 train_time:51957ms step_avg:57.47ms
step:905/2330 train_time:52015ms step_avg:57.47ms
step:906/2330 train_time:52074ms step_avg:57.48ms
step:907/2330 train_time:52132ms step_avg:57.48ms
step:908/2330 train_time:52192ms step_avg:57.48ms
step:909/2330 train_time:52250ms step_avg:57.48ms
step:910/2330 train_time:52309ms step_avg:57.48ms
step:911/2330 train_time:52365ms step_avg:57.48ms
step:912/2330 train_time:52426ms step_avg:57.48ms
step:913/2330 train_time:52483ms step_avg:57.48ms
step:914/2330 train_time:52543ms step_avg:57.49ms
step:915/2330 train_time:52600ms step_avg:57.49ms
step:916/2330 train_time:52660ms step_avg:57.49ms
step:917/2330 train_time:52717ms step_avg:57.49ms
step:918/2330 train_time:52776ms step_avg:57.49ms
step:919/2330 train_time:52832ms step_avg:57.49ms
step:920/2330 train_time:52894ms step_avg:57.49ms
step:921/2330 train_time:52950ms step_avg:57.49ms
step:922/2330 train_time:53011ms step_avg:57.50ms
step:923/2330 train_time:53068ms step_avg:57.50ms
step:924/2330 train_time:53128ms step_avg:57.50ms
step:925/2330 train_time:53185ms step_avg:57.50ms
step:926/2330 train_time:53246ms step_avg:57.50ms
step:927/2330 train_time:53302ms step_avg:57.50ms
step:928/2330 train_time:53362ms step_avg:57.50ms
step:929/2330 train_time:53419ms step_avg:57.50ms
step:930/2330 train_time:53478ms step_avg:57.50ms
step:931/2330 train_time:53535ms step_avg:57.50ms
step:932/2330 train_time:53596ms step_avg:57.51ms
step:933/2330 train_time:53653ms step_avg:57.51ms
step:934/2330 train_time:53713ms step_avg:57.51ms
step:935/2330 train_time:53769ms step_avg:57.51ms
step:936/2330 train_time:53829ms step_avg:57.51ms
step:937/2330 train_time:53886ms step_avg:57.51ms
step:938/2330 train_time:53946ms step_avg:57.51ms
step:939/2330 train_time:54003ms step_avg:57.51ms
step:940/2330 train_time:54063ms step_avg:57.51ms
step:941/2330 train_time:54119ms step_avg:57.51ms
step:942/2330 train_time:54179ms step_avg:57.52ms
step:943/2330 train_time:54236ms step_avg:57.51ms
step:944/2330 train_time:54297ms step_avg:57.52ms
step:945/2330 train_time:54354ms step_avg:57.52ms
step:946/2330 train_time:54415ms step_avg:57.52ms
step:947/2330 train_time:54472ms step_avg:57.52ms
step:948/2330 train_time:54531ms step_avg:57.52ms
step:949/2330 train_time:54587ms step_avg:57.52ms
step:950/2330 train_time:54648ms step_avg:57.52ms
step:951/2330 train_time:54704ms step_avg:57.52ms
step:952/2330 train_time:54765ms step_avg:57.53ms
step:953/2330 train_time:54821ms step_avg:57.52ms
step:954/2330 train_time:54882ms step_avg:57.53ms
step:955/2330 train_time:54939ms step_avg:57.53ms
step:956/2330 train_time:54999ms step_avg:57.53ms
step:957/2330 train_time:55056ms step_avg:57.53ms
step:958/2330 train_time:55116ms step_avg:57.53ms
step:959/2330 train_time:55173ms step_avg:57.53ms
step:960/2330 train_time:55232ms step_avg:57.53ms
step:961/2330 train_time:55290ms step_avg:57.53ms
step:962/2330 train_time:55351ms step_avg:57.54ms
step:963/2330 train_time:55407ms step_avg:57.54ms
step:964/2330 train_time:55467ms step_avg:57.54ms
step:965/2330 train_time:55523ms step_avg:57.54ms
step:966/2330 train_time:55583ms step_avg:57.54ms
step:967/2330 train_time:55640ms step_avg:57.54ms
step:968/2330 train_time:55700ms step_avg:57.54ms
step:969/2330 train_time:55757ms step_avg:57.54ms
step:970/2330 train_time:55817ms step_avg:57.54ms
step:971/2330 train_time:55874ms step_avg:57.54ms
step:972/2330 train_time:55934ms step_avg:57.55ms
step:973/2330 train_time:55992ms step_avg:57.55ms
step:974/2330 train_time:56051ms step_avg:57.55ms
step:975/2330 train_time:56109ms step_avg:57.55ms
step:976/2330 train_time:56168ms step_avg:57.55ms
step:977/2330 train_time:56225ms step_avg:57.55ms
step:978/2330 train_time:56285ms step_avg:57.55ms
step:979/2330 train_time:56341ms step_avg:57.55ms
step:980/2330 train_time:56402ms step_avg:57.55ms
step:981/2330 train_time:56458ms step_avg:57.55ms
step:982/2330 train_time:56518ms step_avg:57.55ms
step:983/2330 train_time:56575ms step_avg:57.55ms
step:984/2330 train_time:56635ms step_avg:57.56ms
step:985/2330 train_time:56692ms step_avg:57.56ms
step:986/2330 train_time:56752ms step_avg:57.56ms
step:987/2330 train_time:56808ms step_avg:57.56ms
step:988/2330 train_time:56869ms step_avg:57.56ms
step:989/2330 train_time:56926ms step_avg:57.56ms
step:990/2330 train_time:56986ms step_avg:57.56ms
step:991/2330 train_time:57043ms step_avg:57.56ms
step:992/2330 train_time:57103ms step_avg:57.56ms
step:993/2330 train_time:57159ms step_avg:57.56ms
step:994/2330 train_time:57219ms step_avg:57.56ms
step:995/2330 train_time:57277ms step_avg:57.56ms
step:996/2330 train_time:57337ms step_avg:57.57ms
step:997/2330 train_time:57393ms step_avg:57.57ms
step:998/2330 train_time:57455ms step_avg:57.57ms
step:999/2330 train_time:57512ms step_avg:57.57ms
step:1000/2330 train_time:57571ms step_avg:57.57ms
step:1000/2330 val_loss:4.9913 train_time:57652ms step_avg:57.65ms
step:1001/2330 train_time:57672ms step_avg:57.61ms
step:1002/2330 train_time:57693ms step_avg:57.58ms
step:1003/2330 train_time:57745ms step_avg:57.57ms
step:1004/2330 train_time:57812ms step_avg:57.58ms
step:1005/2330 train_time:57868ms step_avg:57.58ms
step:1006/2330 train_time:57931ms step_avg:57.59ms
step:1007/2330 train_time:57988ms step_avg:57.58ms
step:1008/2330 train_time:58047ms step_avg:57.59ms
step:1009/2330 train_time:58103ms step_avg:57.58ms
step:1010/2330 train_time:58162ms step_avg:57.59ms
step:1011/2330 train_time:58218ms step_avg:57.58ms
step:1012/2330 train_time:58277ms step_avg:57.59ms
step:1013/2330 train_time:58333ms step_avg:57.58ms
step:1014/2330 train_time:58392ms step_avg:57.59ms
step:1015/2330 train_time:58448ms step_avg:57.58ms
step:1016/2330 train_time:58508ms step_avg:57.59ms
step:1017/2330 train_time:58564ms step_avg:57.59ms
step:1018/2330 train_time:58627ms step_avg:57.59ms
step:1019/2330 train_time:58685ms step_avg:57.59ms
step:1020/2330 train_time:58746ms step_avg:57.59ms
step:1021/2330 train_time:58803ms step_avg:57.59ms
step:1022/2330 train_time:58864ms step_avg:57.60ms
step:1023/2330 train_time:58921ms step_avg:57.60ms
step:1024/2330 train_time:58982ms step_avg:57.60ms
step:1025/2330 train_time:59039ms step_avg:57.60ms
step:1026/2330 train_time:59098ms step_avg:57.60ms
step:1027/2330 train_time:59155ms step_avg:57.60ms
step:1028/2330 train_time:59214ms step_avg:57.60ms
step:1029/2330 train_time:59271ms step_avg:57.60ms
step:1030/2330 train_time:59330ms step_avg:57.60ms
step:1031/2330 train_time:59386ms step_avg:57.60ms
step:1032/2330 train_time:59445ms step_avg:57.60ms
step:1033/2330 train_time:59502ms step_avg:57.60ms
step:1034/2330 train_time:59562ms step_avg:57.60ms
step:1035/2330 train_time:59620ms step_avg:57.60ms
step:1036/2330 train_time:59681ms step_avg:57.61ms
step:1037/2330 train_time:59739ms step_avg:57.61ms
step:1038/2330 train_time:59799ms step_avg:57.61ms
step:1039/2330 train_time:59857ms step_avg:57.61ms
step:1040/2330 train_time:59917ms step_avg:57.61ms
step:1041/2330 train_time:59974ms step_avg:57.61ms
step:1042/2330 train_time:60034ms step_avg:57.61ms
step:1043/2330 train_time:60091ms step_avg:57.61ms
step:1044/2330 train_time:60149ms step_avg:57.61ms
step:1045/2330 train_time:60205ms step_avg:57.61ms
step:1046/2330 train_time:60266ms step_avg:57.62ms
step:1047/2330 train_time:60323ms step_avg:57.61ms
step:1048/2330 train_time:60382ms step_avg:57.62ms
step:1049/2330 train_time:60438ms step_avg:57.61ms
step:1050/2330 train_time:60498ms step_avg:57.62ms
step:1051/2330 train_time:60554ms step_avg:57.62ms
step:1052/2330 train_time:60616ms step_avg:57.62ms
step:1053/2330 train_time:60672ms step_avg:57.62ms
step:1054/2330 train_time:60734ms step_avg:57.62ms
step:1055/2330 train_time:60790ms step_avg:57.62ms
step:1056/2330 train_time:60852ms step_avg:57.62ms
step:1057/2330 train_time:60909ms step_avg:57.62ms
step:1058/2330 train_time:60969ms step_avg:57.63ms
step:1059/2330 train_time:61025ms step_avg:57.63ms
step:1060/2330 train_time:61086ms step_avg:57.63ms
step:1061/2330 train_time:61142ms step_avg:57.63ms
step:1062/2330 train_time:61202ms step_avg:57.63ms
step:1063/2330 train_time:61259ms step_avg:57.63ms
step:1064/2330 train_time:61319ms step_avg:57.63ms
step:1065/2330 train_time:61376ms step_avg:57.63ms
step:1066/2330 train_time:61435ms step_avg:57.63ms
step:1067/2330 train_time:61492ms step_avg:57.63ms
step:1068/2330 train_time:61552ms step_avg:57.63ms
step:1069/2330 train_time:61608ms step_avg:57.63ms
step:1070/2330 train_time:61669ms step_avg:57.63ms
step:1071/2330 train_time:61726ms step_avg:57.63ms
step:1072/2330 train_time:61786ms step_avg:57.64ms
step:1073/2330 train_time:61843ms step_avg:57.64ms
step:1074/2330 train_time:61903ms step_avg:57.64ms
step:1075/2330 train_time:61960ms step_avg:57.64ms
step:1076/2330 train_time:62022ms step_avg:57.64ms
step:1077/2330 train_time:62079ms step_avg:57.64ms
step:1078/2330 train_time:62139ms step_avg:57.64ms
step:1079/2330 train_time:62195ms step_avg:57.64ms
step:1080/2330 train_time:62255ms step_avg:57.64ms
step:1081/2330 train_time:62311ms step_avg:57.64ms
step:1082/2330 train_time:62371ms step_avg:57.64ms
step:1083/2330 train_time:62427ms step_avg:57.64ms
step:1084/2330 train_time:62489ms step_avg:57.65ms
step:1085/2330 train_time:62545ms step_avg:57.65ms
step:1086/2330 train_time:62605ms step_avg:57.65ms
step:1087/2330 train_time:62661ms step_avg:57.65ms
step:1088/2330 train_time:62722ms step_avg:57.65ms
step:1089/2330 train_time:62780ms step_avg:57.65ms
step:1090/2330 train_time:62840ms step_avg:57.65ms
step:1091/2330 train_time:62897ms step_avg:57.65ms
step:1092/2330 train_time:62958ms step_avg:57.65ms
step:1093/2330 train_time:63014ms step_avg:57.65ms
step:1094/2330 train_time:63075ms step_avg:57.66ms
step:1095/2330 train_time:63132ms step_avg:57.65ms
step:1096/2330 train_time:63192ms step_avg:57.66ms
step:1097/2330 train_time:63249ms step_avg:57.66ms
step:1098/2330 train_time:63309ms step_avg:57.66ms
step:1099/2330 train_time:63365ms step_avg:57.66ms
step:1100/2330 train_time:63425ms step_avg:57.66ms
step:1101/2330 train_time:63482ms step_avg:57.66ms
step:1102/2330 train_time:63541ms step_avg:57.66ms
step:1103/2330 train_time:63598ms step_avg:57.66ms
step:1104/2330 train_time:63659ms step_avg:57.66ms
step:1105/2330 train_time:63716ms step_avg:57.66ms
step:1106/2330 train_time:63777ms step_avg:57.66ms
step:1107/2330 train_time:63833ms step_avg:57.66ms
step:1108/2330 train_time:63894ms step_avg:57.67ms
step:1109/2330 train_time:63950ms step_avg:57.66ms
step:1110/2330 train_time:64010ms step_avg:57.67ms
step:1111/2330 train_time:64067ms step_avg:57.67ms
step:1112/2330 train_time:64128ms step_avg:57.67ms
step:1113/2330 train_time:64185ms step_avg:57.67ms
step:1114/2330 train_time:64243ms step_avg:57.67ms
step:1115/2330 train_time:64300ms step_avg:57.67ms
step:1116/2330 train_time:64361ms step_avg:57.67ms
step:1117/2330 train_time:64418ms step_avg:57.67ms
step:1118/2330 train_time:64479ms step_avg:57.67ms
step:1119/2330 train_time:64535ms step_avg:57.67ms
step:1120/2330 train_time:64596ms step_avg:57.68ms
step:1121/2330 train_time:64653ms step_avg:57.67ms
step:1122/2330 train_time:64714ms step_avg:57.68ms
step:1123/2330 train_time:64770ms step_avg:57.68ms
step:1124/2330 train_time:64831ms step_avg:57.68ms
step:1125/2330 train_time:64888ms step_avg:57.68ms
step:1126/2330 train_time:64947ms step_avg:57.68ms
step:1127/2330 train_time:65004ms step_avg:57.68ms
step:1128/2330 train_time:65064ms step_avg:57.68ms
step:1129/2330 train_time:65121ms step_avg:57.68ms
step:1130/2330 train_time:65181ms step_avg:57.68ms
step:1131/2330 train_time:65238ms step_avg:57.68ms
step:1132/2330 train_time:65298ms step_avg:57.68ms
step:1133/2330 train_time:65355ms step_avg:57.68ms
step:1134/2330 train_time:65415ms step_avg:57.68ms
step:1135/2330 train_time:65472ms step_avg:57.68ms
step:1136/2330 train_time:65531ms step_avg:57.69ms
step:1137/2330 train_time:65589ms step_avg:57.69ms
step:1138/2330 train_time:65648ms step_avg:57.69ms
step:1139/2330 train_time:65704ms step_avg:57.69ms
step:1140/2330 train_time:65764ms step_avg:57.69ms
step:1141/2330 train_time:65821ms step_avg:57.69ms
step:1142/2330 train_time:65881ms step_avg:57.69ms
step:1143/2330 train_time:65938ms step_avg:57.69ms
step:1144/2330 train_time:65999ms step_avg:57.69ms
step:1145/2330 train_time:66055ms step_avg:57.69ms
step:1146/2330 train_time:66115ms step_avg:57.69ms
step:1147/2330 train_time:66172ms step_avg:57.69ms
step:1148/2330 train_time:66232ms step_avg:57.69ms
step:1149/2330 train_time:66289ms step_avg:57.69ms
step:1150/2330 train_time:66348ms step_avg:57.69ms
step:1151/2330 train_time:66405ms step_avg:57.69ms
step:1152/2330 train_time:66465ms step_avg:57.70ms
step:1153/2330 train_time:66522ms step_avg:57.69ms
step:1154/2330 train_time:66582ms step_avg:57.70ms
step:1155/2330 train_time:66639ms step_avg:57.70ms
step:1156/2330 train_time:66699ms step_avg:57.70ms
step:1157/2330 train_time:66756ms step_avg:57.70ms
step:1158/2330 train_time:66816ms step_avg:57.70ms
step:1159/2330 train_time:66873ms step_avg:57.70ms
step:1160/2330 train_time:66934ms step_avg:57.70ms
step:1161/2330 train_time:66990ms step_avg:57.70ms
step:1162/2330 train_time:67049ms step_avg:57.70ms
step:1163/2330 train_time:67106ms step_avg:57.70ms
step:1164/2330 train_time:67167ms step_avg:57.70ms
step:1165/2330 train_time:67223ms step_avg:57.70ms
step:1166/2330 train_time:67283ms step_avg:57.70ms
step:1167/2330 train_time:67340ms step_avg:57.70ms
step:1168/2330 train_time:67400ms step_avg:57.71ms
step:1169/2330 train_time:67457ms step_avg:57.70ms
step:1170/2330 train_time:67517ms step_avg:57.71ms
step:1171/2330 train_time:67573ms step_avg:57.71ms
step:1172/2330 train_time:67634ms step_avg:57.71ms
step:1173/2330 train_time:67690ms step_avg:57.71ms
step:1174/2330 train_time:67751ms step_avg:57.71ms
step:1175/2330 train_time:67808ms step_avg:57.71ms
step:1176/2330 train_time:67868ms step_avg:57.71ms
step:1177/2330 train_time:67925ms step_avg:57.71ms
step:1178/2330 train_time:67984ms step_avg:57.71ms
step:1179/2330 train_time:68042ms step_avg:57.71ms
step:1180/2330 train_time:68101ms step_avg:57.71ms
step:1181/2330 train_time:68159ms step_avg:57.71ms
step:1182/2330 train_time:68219ms step_avg:57.71ms
step:1183/2330 train_time:68275ms step_avg:57.71ms
step:1184/2330 train_time:68336ms step_avg:57.72ms
step:1185/2330 train_time:68392ms step_avg:57.71ms
step:1186/2330 train_time:68452ms step_avg:57.72ms
step:1187/2330 train_time:68509ms step_avg:57.72ms
step:1188/2330 train_time:68570ms step_avg:57.72ms
step:1189/2330 train_time:68626ms step_avg:57.72ms
step:1190/2330 train_time:68686ms step_avg:57.72ms
step:1191/2330 train_time:68743ms step_avg:57.72ms
step:1192/2330 train_time:68804ms step_avg:57.72ms
step:1193/2330 train_time:68861ms step_avg:57.72ms
step:1194/2330 train_time:68920ms step_avg:57.72ms
step:1195/2330 train_time:68978ms step_avg:57.72ms
step:1196/2330 train_time:69037ms step_avg:57.72ms
step:1197/2330 train_time:69094ms step_avg:57.72ms
step:1198/2330 train_time:69153ms step_avg:57.72ms
step:1199/2330 train_time:69210ms step_avg:57.72ms
step:1200/2330 train_time:69269ms step_avg:57.72ms
step:1201/2330 train_time:69327ms step_avg:57.72ms
step:1202/2330 train_time:69386ms step_avg:57.73ms
step:1203/2330 train_time:69443ms step_avg:57.73ms
step:1204/2330 train_time:69503ms step_avg:57.73ms
step:1205/2330 train_time:69560ms step_avg:57.73ms
step:1206/2330 train_time:69620ms step_avg:57.73ms
step:1207/2330 train_time:69677ms step_avg:57.73ms
step:1208/2330 train_time:69737ms step_avg:57.73ms
step:1209/2330 train_time:69794ms step_avg:57.73ms
step:1210/2330 train_time:69854ms step_avg:57.73ms
step:1211/2330 train_time:69910ms step_avg:57.73ms
step:1212/2330 train_time:69971ms step_avg:57.73ms
step:1213/2330 train_time:70028ms step_avg:57.73ms
step:1214/2330 train_time:70088ms step_avg:57.73ms
step:1215/2330 train_time:70144ms step_avg:57.73ms
step:1216/2330 train_time:70204ms step_avg:57.73ms
step:1217/2330 train_time:70260ms step_avg:57.73ms
step:1218/2330 train_time:70321ms step_avg:57.73ms
step:1219/2330 train_time:70378ms step_avg:57.73ms
step:1220/2330 train_time:70438ms step_avg:57.74ms
step:1221/2330 train_time:70495ms step_avg:57.74ms
step:1222/2330 train_time:70555ms step_avg:57.74ms
step:1223/2330 train_time:70611ms step_avg:57.74ms
step:1224/2330 train_time:70672ms step_avg:57.74ms
step:1225/2330 train_time:70729ms step_avg:57.74ms
step:1226/2330 train_time:70789ms step_avg:57.74ms
step:1227/2330 train_time:70845ms step_avg:57.74ms
step:1228/2330 train_time:70906ms step_avg:57.74ms
step:1229/2330 train_time:70963ms step_avg:57.74ms
step:1230/2330 train_time:71023ms step_avg:57.74ms
step:1231/2330 train_time:71080ms step_avg:57.74ms
step:1232/2330 train_time:71139ms step_avg:57.74ms
step:1233/2330 train_time:71196ms step_avg:57.74ms
step:1234/2330 train_time:71256ms step_avg:57.74ms
step:1235/2330 train_time:71313ms step_avg:57.74ms
step:1236/2330 train_time:71373ms step_avg:57.75ms
step:1237/2330 train_time:71430ms step_avg:57.74ms
step:1238/2330 train_time:71490ms step_avg:57.75ms
step:1239/2330 train_time:71546ms step_avg:57.74ms
step:1240/2330 train_time:71607ms step_avg:57.75ms
step:1241/2330 train_time:71663ms step_avg:57.75ms
step:1242/2330 train_time:71724ms step_avg:57.75ms
step:1243/2330 train_time:71781ms step_avg:57.75ms
step:1244/2330 train_time:71840ms step_avg:57.75ms
step:1245/2330 train_time:71897ms step_avg:57.75ms
step:1246/2330 train_time:71957ms step_avg:57.75ms
step:1247/2330 train_time:72014ms step_avg:57.75ms
step:1248/2330 train_time:72074ms step_avg:57.75ms
step:1249/2330 train_time:72131ms step_avg:57.75ms
step:1250/2330 train_time:72191ms step_avg:57.75ms
step:1250/2330 val_loss:4.7251 train_time:72272ms step_avg:57.82ms
step:1251/2330 train_time:72292ms step_avg:57.79ms
step:1252/2330 train_time:72312ms step_avg:57.76ms
step:1253/2330 train_time:72370ms step_avg:57.76ms
step:1254/2330 train_time:72432ms step_avg:57.76ms
step:1255/2330 train_time:72489ms step_avg:57.76ms
step:1256/2330 train_time:72550ms step_avg:57.76ms
step:1257/2330 train_time:72606ms step_avg:57.76ms
step:1258/2330 train_time:72665ms step_avg:57.76ms
step:1259/2330 train_time:72722ms step_avg:57.76ms
step:1260/2330 train_time:72781ms step_avg:57.76ms
step:1261/2330 train_time:72837ms step_avg:57.76ms
step:1262/2330 train_time:72897ms step_avg:57.76ms
step:1263/2330 train_time:72952ms step_avg:57.76ms
step:1264/2330 train_time:73012ms step_avg:57.76ms
step:1265/2330 train_time:73068ms step_avg:57.76ms
step:1266/2330 train_time:73127ms step_avg:57.76ms
step:1267/2330 train_time:73183ms step_avg:57.76ms
step:1268/2330 train_time:73244ms step_avg:57.76ms
step:1269/2330 train_time:73303ms step_avg:57.76ms
step:1270/2330 train_time:73364ms step_avg:57.77ms
step:1271/2330 train_time:73421ms step_avg:57.77ms
step:1272/2330 train_time:73483ms step_avg:57.77ms
step:1273/2330 train_time:73540ms step_avg:57.77ms
step:1274/2330 train_time:73600ms step_avg:57.77ms
step:1275/2330 train_time:73656ms step_avg:57.77ms
step:1276/2330 train_time:73717ms step_avg:57.77ms
step:1277/2330 train_time:73772ms step_avg:57.77ms
step:1278/2330 train_time:73833ms step_avg:57.77ms
step:1279/2330 train_time:73889ms step_avg:57.77ms
step:1280/2330 train_time:73948ms step_avg:57.77ms
step:1281/2330 train_time:74005ms step_avg:57.77ms
step:1282/2330 train_time:74064ms step_avg:57.77ms
step:1283/2330 train_time:74120ms step_avg:57.77ms
step:1284/2330 train_time:74181ms step_avg:57.77ms
step:1285/2330 train_time:74237ms step_avg:57.77ms
step:1286/2330 train_time:74298ms step_avg:57.77ms
step:1287/2330 train_time:74355ms step_avg:57.77ms
step:1288/2330 train_time:74416ms step_avg:57.78ms
step:1289/2330 train_time:74472ms step_avg:57.78ms
step:1290/2330 train_time:74534ms step_avg:57.78ms
step:1291/2330 train_time:74590ms step_avg:57.78ms
step:1292/2330 train_time:74650ms step_avg:57.78ms
step:1293/2330 train_time:74707ms step_avg:57.78ms
step:1294/2330 train_time:74767ms step_avg:57.78ms
step:1295/2330 train_time:74823ms step_avg:57.78ms
step:1296/2330 train_time:74883ms step_avg:57.78ms
step:1297/2330 train_time:74939ms step_avg:57.78ms
step:1298/2330 train_time:74999ms step_avg:57.78ms
step:1299/2330 train_time:75055ms step_avg:57.78ms
step:1300/2330 train_time:75114ms step_avg:57.78ms
step:1301/2330 train_time:75171ms step_avg:57.78ms
step:1302/2330 train_time:75231ms step_avg:57.78ms
step:1303/2330 train_time:75288ms step_avg:57.78ms
step:1304/2330 train_time:75348ms step_avg:57.78ms
step:1305/2330 train_time:75405ms step_avg:57.78ms
step:1306/2330 train_time:75467ms step_avg:57.78ms
step:1307/2330 train_time:75523ms step_avg:57.78ms
step:1308/2330 train_time:75585ms step_avg:57.79ms
step:1309/2330 train_time:75641ms step_avg:57.79ms
step:1310/2330 train_time:75702ms step_avg:57.79ms
step:1311/2330 train_time:75759ms step_avg:57.79ms
step:1312/2330 train_time:75819ms step_avg:57.79ms
step:1313/2330 train_time:75874ms step_avg:57.79ms
step:1314/2330 train_time:75934ms step_avg:57.79ms
step:1315/2330 train_time:75990ms step_avg:57.79ms
step:1316/2330 train_time:76050ms step_avg:57.79ms
step:1317/2330 train_time:76107ms step_avg:57.79ms
step:1318/2330 train_time:76166ms step_avg:57.79ms
step:1319/2330 train_time:76223ms step_avg:57.79ms
step:1320/2330 train_time:76284ms step_avg:57.79ms
step:1321/2330 train_time:76341ms step_avg:57.79ms
step:1322/2330 train_time:76402ms step_avg:57.79ms
step:1323/2330 train_time:76460ms step_avg:57.79ms
step:1324/2330 train_time:76519ms step_avg:57.79ms
step:1325/2330 train_time:76575ms step_avg:57.79ms
step:1326/2330 train_time:76635ms step_avg:57.79ms
step:1327/2330 train_time:76692ms step_avg:57.79ms
step:1328/2330 train_time:76752ms step_avg:57.80ms
step:1329/2330 train_time:76809ms step_avg:57.79ms
step:1330/2330 train_time:76869ms step_avg:57.80ms
step:1331/2330 train_time:76925ms step_avg:57.80ms
step:1332/2330 train_time:76985ms step_avg:57.80ms
step:1333/2330 train_time:77042ms step_avg:57.80ms
step:1334/2330 train_time:77101ms step_avg:57.80ms
step:1335/2330 train_time:77158ms step_avg:57.80ms
step:1336/2330 train_time:77217ms step_avg:57.80ms
step:1337/2330 train_time:77273ms step_avg:57.80ms
step:1338/2330 train_time:77333ms step_avg:57.80ms
step:1339/2330 train_time:77390ms step_avg:57.80ms
step:1340/2330 train_time:77451ms step_avg:57.80ms
step:1341/2330 train_time:77507ms step_avg:57.80ms
step:1342/2330 train_time:77568ms step_avg:57.80ms
step:1343/2330 train_time:77625ms step_avg:57.80ms
step:1344/2330 train_time:77685ms step_avg:57.80ms
step:1345/2330 train_time:77742ms step_avg:57.80ms
step:1346/2330 train_time:77802ms step_avg:57.80ms
step:1347/2330 train_time:77858ms step_avg:57.80ms
step:1348/2330 train_time:77919ms step_avg:57.80ms
step:1349/2330 train_time:77975ms step_avg:57.80ms
step:1350/2330 train_time:78036ms step_avg:57.80ms
step:1351/2330 train_time:78092ms step_avg:57.80ms
step:1352/2330 train_time:78152ms step_avg:57.80ms
step:1353/2330 train_time:78208ms step_avg:57.80ms
step:1354/2330 train_time:78268ms step_avg:57.80ms
step:1355/2330 train_time:78324ms step_avg:57.80ms
step:1356/2330 train_time:78386ms step_avg:57.81ms
step:1357/2330 train_time:78443ms step_avg:57.81ms
step:1358/2330 train_time:78502ms step_avg:57.81ms
step:1359/2330 train_time:78559ms step_avg:57.81ms
step:1360/2330 train_time:78619ms step_avg:57.81ms
step:1361/2330 train_time:78676ms step_avg:57.81ms
step:1362/2330 train_time:78735ms step_avg:57.81ms
step:1363/2330 train_time:78792ms step_avg:57.81ms
step:1364/2330 train_time:78852ms step_avg:57.81ms
step:1365/2330 train_time:78908ms step_avg:57.81ms
step:1366/2330 train_time:78968ms step_avg:57.81ms
step:1367/2330 train_time:79025ms step_avg:57.81ms
step:1368/2330 train_time:79085ms step_avg:57.81ms
step:1369/2330 train_time:79141ms step_avg:57.81ms
step:1370/2330 train_time:79201ms step_avg:57.81ms
step:1371/2330 train_time:79258ms step_avg:57.81ms
step:1372/2330 train_time:79318ms step_avg:57.81ms
step:1373/2330 train_time:79374ms step_avg:57.81ms
step:1374/2330 train_time:79435ms step_avg:57.81ms
step:1375/2330 train_time:79491ms step_avg:57.81ms
step:1376/2330 train_time:79552ms step_avg:57.81ms
step:1377/2330 train_time:79608ms step_avg:57.81ms
step:1378/2330 train_time:79668ms step_avg:57.81ms
step:1379/2330 train_time:79725ms step_avg:57.81ms
step:1380/2330 train_time:79786ms step_avg:57.82ms
step:1381/2330 train_time:79842ms step_avg:57.81ms
step:1382/2330 train_time:79902ms step_avg:57.82ms
step:1383/2330 train_time:79959ms step_avg:57.82ms
step:1384/2330 train_time:80018ms step_avg:57.82ms
step:1385/2330 train_time:80075ms step_avg:57.82ms
step:1386/2330 train_time:80133ms step_avg:57.82ms
step:1387/2330 train_time:80190ms step_avg:57.82ms
step:1388/2330 train_time:80250ms step_avg:57.82ms
step:1389/2330 train_time:80307ms step_avg:57.82ms
step:1390/2330 train_time:80366ms step_avg:57.82ms
step:1391/2330 train_time:80423ms step_avg:57.82ms
step:1392/2330 train_time:80483ms step_avg:57.82ms
step:1393/2330 train_time:80540ms step_avg:57.82ms
step:1394/2330 train_time:80600ms step_avg:57.82ms
step:1395/2330 train_time:80656ms step_avg:57.82ms
step:1396/2330 train_time:80717ms step_avg:57.82ms
step:1397/2330 train_time:80773ms step_avg:57.82ms
step:1398/2330 train_time:80833ms step_avg:57.82ms
step:1399/2330 train_time:80889ms step_avg:57.82ms
step:1400/2330 train_time:80950ms step_avg:57.82ms
step:1401/2330 train_time:81007ms step_avg:57.82ms
step:1402/2330 train_time:81066ms step_avg:57.82ms
step:1403/2330 train_time:81123ms step_avg:57.82ms
step:1404/2330 train_time:81183ms step_avg:57.82ms
step:1405/2330 train_time:81239ms step_avg:57.82ms
step:1406/2330 train_time:81299ms step_avg:57.82ms
step:1407/2330 train_time:81355ms step_avg:57.82ms
step:1408/2330 train_time:81415ms step_avg:57.82ms
step:1409/2330 train_time:81471ms step_avg:57.82ms
step:1410/2330 train_time:81531ms step_avg:57.82ms
step:1411/2330 train_time:81588ms step_avg:57.82ms
step:1412/2330 train_time:81648ms step_avg:57.82ms
step:1413/2330 train_time:81705ms step_avg:57.82ms
step:1414/2330 train_time:81764ms step_avg:57.82ms
step:1415/2330 train_time:81822ms step_avg:57.82ms
step:1416/2330 train_time:81882ms step_avg:57.83ms
step:1417/2330 train_time:81939ms step_avg:57.83ms
step:1418/2330 train_time:81998ms step_avg:57.83ms
step:1419/2330 train_time:82055ms step_avg:57.83ms
step:1420/2330 train_time:82114ms step_avg:57.83ms
step:1421/2330 train_time:82171ms step_avg:57.83ms
step:1422/2330 train_time:82231ms step_avg:57.83ms
step:1423/2330 train_time:82288ms step_avg:57.83ms
step:1424/2330 train_time:82347ms step_avg:57.83ms
step:1425/2330 train_time:82404ms step_avg:57.83ms
step:1426/2330 train_time:82464ms step_avg:57.83ms
step:1427/2330 train_time:82520ms step_avg:57.83ms
step:1428/2330 train_time:82581ms step_avg:57.83ms
step:1429/2330 train_time:82638ms step_avg:57.83ms
step:1430/2330 train_time:82698ms step_avg:57.83ms
step:1431/2330 train_time:82755ms step_avg:57.83ms
step:1432/2330 train_time:82815ms step_avg:57.83ms
step:1433/2330 train_time:82871ms step_avg:57.83ms
step:1434/2330 train_time:82931ms step_avg:57.83ms
step:1435/2330 train_time:82988ms step_avg:57.83ms
step:1436/2330 train_time:83049ms step_avg:57.83ms
step:1437/2330 train_time:83105ms step_avg:57.83ms
step:1438/2330 train_time:83165ms step_avg:57.83ms
step:1439/2330 train_time:83221ms step_avg:57.83ms
step:1440/2330 train_time:83282ms step_avg:57.83ms
step:1441/2330 train_time:83338ms step_avg:57.83ms
step:1442/2330 train_time:83398ms step_avg:57.84ms
step:1443/2330 train_time:83455ms step_avg:57.83ms
step:1444/2330 train_time:83515ms step_avg:57.84ms
step:1445/2330 train_time:83571ms step_avg:57.83ms
step:1446/2330 train_time:83632ms step_avg:57.84ms
step:1447/2330 train_time:83689ms step_avg:57.84ms
step:1448/2330 train_time:83749ms step_avg:57.84ms
step:1449/2330 train_time:83805ms step_avg:57.84ms
step:1450/2330 train_time:83866ms step_avg:57.84ms
step:1451/2330 train_time:83923ms step_avg:57.84ms
step:1452/2330 train_time:83983ms step_avg:57.84ms
step:1453/2330 train_time:84040ms step_avg:57.84ms
step:1454/2330 train_time:84100ms step_avg:57.84ms
step:1455/2330 train_time:84157ms step_avg:57.84ms
step:1456/2330 train_time:84216ms step_avg:57.84ms
step:1457/2330 train_time:84272ms step_avg:57.84ms
step:1458/2330 train_time:84333ms step_avg:57.84ms
step:1459/2330 train_time:84389ms step_avg:57.84ms
step:1460/2330 train_time:84450ms step_avg:57.84ms
step:1461/2330 train_time:84506ms step_avg:57.84ms
step:1462/2330 train_time:84566ms step_avg:57.84ms
step:1463/2330 train_time:84623ms step_avg:57.84ms
step:1464/2330 train_time:84683ms step_avg:57.84ms
step:1465/2330 train_time:84740ms step_avg:57.84ms
step:1466/2330 train_time:84801ms step_avg:57.84ms
step:1467/2330 train_time:84857ms step_avg:57.84ms
step:1468/2330 train_time:84917ms step_avg:57.85ms
step:1469/2330 train_time:84973ms step_avg:57.84ms
step:1470/2330 train_time:85033ms step_avg:57.85ms
step:1471/2330 train_time:85090ms step_avg:57.84ms
step:1472/2330 train_time:85150ms step_avg:57.85ms
step:1473/2330 train_time:85207ms step_avg:57.85ms
step:1474/2330 train_time:85267ms step_avg:57.85ms
step:1475/2330 train_time:85324ms step_avg:57.85ms
step:1476/2330 train_time:85383ms step_avg:57.85ms
step:1477/2330 train_time:85440ms step_avg:57.85ms
step:1478/2330 train_time:85501ms step_avg:57.85ms
step:1479/2330 train_time:85557ms step_avg:57.85ms
step:1480/2330 train_time:85618ms step_avg:57.85ms
step:1481/2330 train_time:85674ms step_avg:57.85ms
step:1482/2330 train_time:85734ms step_avg:57.85ms
step:1483/2330 train_time:85791ms step_avg:57.85ms
step:1484/2330 train_time:85851ms step_avg:57.85ms
step:1485/2330 train_time:85907ms step_avg:57.85ms
step:1486/2330 train_time:85968ms step_avg:57.85ms
step:1487/2330 train_time:86024ms step_avg:57.85ms
step:1488/2330 train_time:86085ms step_avg:57.85ms
step:1489/2330 train_time:86142ms step_avg:57.85ms
step:1490/2330 train_time:86202ms step_avg:57.85ms
step:1491/2330 train_time:86259ms step_avg:57.85ms
step:1492/2330 train_time:86319ms step_avg:57.85ms
step:1493/2330 train_time:86375ms step_avg:57.85ms
step:1494/2330 train_time:86435ms step_avg:57.86ms
step:1495/2330 train_time:86492ms step_avg:57.85ms
step:1496/2330 train_time:86552ms step_avg:57.86ms
step:1497/2330 train_time:86608ms step_avg:57.85ms
step:1498/2330 train_time:86668ms step_avg:57.86ms
step:1499/2330 train_time:86725ms step_avg:57.86ms
step:1500/2330 train_time:86784ms step_avg:57.86ms
step:1500/2330 val_loss:4.5408 train_time:86865ms step_avg:57.91ms
step:1501/2330 train_time:86884ms step_avg:57.88ms
step:1502/2330 train_time:86906ms step_avg:57.86ms
step:1503/2330 train_time:86965ms step_avg:57.86ms
step:1504/2330 train_time:87027ms step_avg:57.86ms
step:1505/2330 train_time:87085ms step_avg:57.86ms
step:1506/2330 train_time:87147ms step_avg:57.87ms
step:1507/2330 train_time:87203ms step_avg:57.87ms
step:1508/2330 train_time:87263ms step_avg:57.87ms
step:1509/2330 train_time:87319ms step_avg:57.87ms
step:1510/2330 train_time:87378ms step_avg:57.87ms
step:1511/2330 train_time:87434ms step_avg:57.87ms
step:1512/2330 train_time:87493ms step_avg:57.87ms
step:1513/2330 train_time:87550ms step_avg:57.87ms
step:1514/2330 train_time:87609ms step_avg:57.87ms
step:1515/2330 train_time:87665ms step_avg:57.86ms
step:1516/2330 train_time:87725ms step_avg:57.87ms
step:1517/2330 train_time:87781ms step_avg:57.86ms
step:1518/2330 train_time:87842ms step_avg:57.87ms
step:1519/2330 train_time:87899ms step_avg:57.87ms
step:1520/2330 train_time:87961ms step_avg:57.87ms
step:1521/2330 train_time:88018ms step_avg:57.87ms
step:1522/2330 train_time:88080ms step_avg:57.87ms
step:1523/2330 train_time:88137ms step_avg:57.87ms
step:1524/2330 train_time:88198ms step_avg:57.87ms
step:1525/2330 train_time:88255ms step_avg:57.87ms
step:1526/2330 train_time:88314ms step_avg:57.87ms
step:1527/2330 train_time:88370ms step_avg:57.87ms
step:1528/2330 train_time:88430ms step_avg:57.87ms
step:1529/2330 train_time:88488ms step_avg:57.87ms
step:1530/2330 train_time:88547ms step_avg:57.87ms
step:1531/2330 train_time:88604ms step_avg:57.87ms
step:1532/2330 train_time:88664ms step_avg:57.87ms
step:1533/2330 train_time:88720ms step_avg:57.87ms
step:1534/2330 train_time:88781ms step_avg:57.88ms
step:1535/2330 train_time:88837ms step_avg:57.87ms
step:1536/2330 train_time:88898ms step_avg:57.88ms
step:1537/2330 train_time:88955ms step_avg:57.88ms
step:1538/2330 train_time:89017ms step_avg:57.88ms
step:1539/2330 train_time:89075ms step_avg:57.88ms
step:1540/2330 train_time:89135ms step_avg:57.88ms
step:1541/2330 train_time:89193ms step_avg:57.88ms
step:1542/2330 train_time:89255ms step_avg:57.88ms
step:1543/2330 train_time:89312ms step_avg:57.88ms
step:1544/2330 train_time:89372ms step_avg:57.88ms
step:1545/2330 train_time:89429ms step_avg:57.88ms
step:1546/2330 train_time:89490ms step_avg:57.88ms
step:1547/2330 train_time:89547ms step_avg:57.88ms
step:1548/2330 train_time:89607ms step_avg:57.89ms
step:1549/2330 train_time:89665ms step_avg:57.89ms
step:1550/2330 train_time:89725ms step_avg:57.89ms
step:1551/2330 train_time:89783ms step_avg:57.89ms
step:1552/2330 train_time:89844ms step_avg:57.89ms
step:1553/2330 train_time:89900ms step_avg:57.89ms
step:1554/2330 train_time:89963ms step_avg:57.89ms
step:1555/2330 train_time:90019ms step_avg:57.89ms
step:1556/2330 train_time:90080ms step_avg:57.89ms
step:1557/2330 train_time:90137ms step_avg:57.89ms
step:1558/2330 train_time:90199ms step_avg:57.89ms
step:1559/2330 train_time:90256ms step_avg:57.89ms
step:1560/2330 train_time:90317ms step_avg:57.90ms
step:1561/2330 train_time:90374ms step_avg:57.89ms
step:1562/2330 train_time:90435ms step_avg:57.90ms
step:1563/2330 train_time:90492ms step_avg:57.90ms
step:1564/2330 train_time:90553ms step_avg:57.90ms
step:1565/2330 train_time:90610ms step_avg:57.90ms
step:1566/2330 train_time:90671ms step_avg:57.90ms
step:1567/2330 train_time:90729ms step_avg:57.90ms
step:1568/2330 train_time:90789ms step_avg:57.90ms
step:1569/2330 train_time:90847ms step_avg:57.90ms
step:1570/2330 train_time:90907ms step_avg:57.90ms
step:1571/2330 train_time:90964ms step_avg:57.90ms
step:1572/2330 train_time:91026ms step_avg:57.90ms
step:1573/2330 train_time:91083ms step_avg:57.90ms
step:1574/2330 train_time:91145ms step_avg:57.91ms
step:1575/2330 train_time:91202ms step_avg:57.91ms
step:1576/2330 train_time:91264ms step_avg:57.91ms
step:1577/2330 train_time:91321ms step_avg:57.91ms
step:1578/2330 train_time:91383ms step_avg:57.91ms
step:1579/2330 train_time:91440ms step_avg:57.91ms
step:1580/2330 train_time:91500ms step_avg:57.91ms
step:1581/2330 train_time:91556ms step_avg:57.91ms
step:1582/2330 train_time:91617ms step_avg:57.91ms
step:1583/2330 train_time:91674ms step_avg:57.91ms
step:1584/2330 train_time:91736ms step_avg:57.91ms
step:1585/2330 train_time:91792ms step_avg:57.91ms
step:1586/2330 train_time:91853ms step_avg:57.91ms
step:1587/2330 train_time:91910ms step_avg:57.91ms
step:1588/2330 train_time:91972ms step_avg:57.92ms
step:1589/2330 train_time:92030ms step_avg:57.92ms
step:1590/2330 train_time:92091ms step_avg:57.92ms
step:1591/2330 train_time:92149ms step_avg:57.92ms
step:1592/2330 train_time:92209ms step_avg:57.92ms
step:1593/2330 train_time:92266ms step_avg:57.92ms
step:1594/2330 train_time:92327ms step_avg:57.92ms
step:1595/2330 train_time:92385ms step_avg:57.92ms
step:1596/2330 train_time:92445ms step_avg:57.92ms
step:1597/2330 train_time:92501ms step_avg:57.92ms
step:1598/2330 train_time:92563ms step_avg:57.92ms
step:1599/2330 train_time:92619ms step_avg:57.92ms
step:1600/2330 train_time:92680ms step_avg:57.93ms
step:1601/2330 train_time:92737ms step_avg:57.92ms
step:1602/2330 train_time:92797ms step_avg:57.93ms
step:1603/2330 train_time:92853ms step_avg:57.92ms
step:1604/2330 train_time:92914ms step_avg:57.93ms
step:1605/2330 train_time:92972ms step_avg:57.93ms
step:1606/2330 train_time:93032ms step_avg:57.93ms
step:1607/2330 train_time:93090ms step_avg:57.93ms
step:1608/2330 train_time:93150ms step_avg:57.93ms
step:1609/2330 train_time:93208ms step_avg:57.93ms
step:1610/2330 train_time:93268ms step_avg:57.93ms
step:1611/2330 train_time:93326ms step_avg:57.93ms
step:1612/2330 train_time:93387ms step_avg:57.93ms
step:1613/2330 train_time:93443ms step_avg:57.93ms
step:1614/2330 train_time:93505ms step_avg:57.93ms
step:1615/2330 train_time:93562ms step_avg:57.93ms
step:1616/2330 train_time:93624ms step_avg:57.94ms
step:1617/2330 train_time:93681ms step_avg:57.93ms
step:1618/2330 train_time:93743ms step_avg:57.94ms
step:1619/2330 train_time:93800ms step_avg:57.94ms
step:1620/2330 train_time:93860ms step_avg:57.94ms
step:1621/2330 train_time:93916ms step_avg:57.94ms
step:1622/2330 train_time:93978ms step_avg:57.94ms
step:1623/2330 train_time:94034ms step_avg:57.94ms
step:1624/2330 train_time:94096ms step_avg:57.94ms
step:1625/2330 train_time:94152ms step_avg:57.94ms
step:1626/2330 train_time:94215ms step_avg:57.94ms
step:1627/2330 train_time:94272ms step_avg:57.94ms
step:1628/2330 train_time:94332ms step_avg:57.94ms
step:1629/2330 train_time:94390ms step_avg:57.94ms
step:1630/2330 train_time:94452ms step_avg:57.95ms
step:1631/2330 train_time:94509ms step_avg:57.95ms
step:1632/2330 train_time:94570ms step_avg:57.95ms
step:1633/2330 train_time:94627ms step_avg:57.95ms
step:1634/2330 train_time:94688ms step_avg:57.95ms
step:1635/2330 train_time:94745ms step_avg:57.95ms
step:1636/2330 train_time:94807ms step_avg:57.95ms
step:1637/2330 train_time:94864ms step_avg:57.95ms
step:1638/2330 train_time:94924ms step_avg:57.95ms
step:1639/2330 train_time:94981ms step_avg:57.95ms
step:1640/2330 train_time:95044ms step_avg:57.95ms
step:1641/2330 train_time:95101ms step_avg:57.95ms
step:1642/2330 train_time:95163ms step_avg:57.96ms
step:1643/2330 train_time:95219ms step_avg:57.95ms
step:1644/2330 train_time:95281ms step_avg:57.96ms
step:1645/2330 train_time:95337ms step_avg:57.96ms
step:1646/2330 train_time:95399ms step_avg:57.96ms
step:1647/2330 train_time:95455ms step_avg:57.96ms
step:1648/2330 train_time:95517ms step_avg:57.96ms
step:1649/2330 train_time:95574ms step_avg:57.96ms
step:1650/2330 train_time:95635ms step_avg:57.96ms
step:1651/2330 train_time:95692ms step_avg:57.96ms
step:1652/2330 train_time:95753ms step_avg:57.96ms
step:1653/2330 train_time:95811ms step_avg:57.96ms
step:1654/2330 train_time:95872ms step_avg:57.96ms
step:1655/2330 train_time:95931ms step_avg:57.96ms
step:1656/2330 train_time:95990ms step_avg:57.97ms
step:1657/2330 train_time:96048ms step_avg:57.96ms
step:1658/2330 train_time:96110ms step_avg:57.97ms
step:1659/2330 train_time:96166ms step_avg:57.97ms
step:1660/2330 train_time:96228ms step_avg:57.97ms
step:1661/2330 train_time:96285ms step_avg:57.97ms
step:1662/2330 train_time:96345ms step_avg:57.97ms
step:1663/2330 train_time:96402ms step_avg:57.97ms
step:1664/2330 train_time:96463ms step_avg:57.97ms
step:1665/2330 train_time:96519ms step_avg:57.97ms
step:1666/2330 train_time:96580ms step_avg:57.97ms
step:1667/2330 train_time:96637ms step_avg:57.97ms
step:1668/2330 train_time:96697ms step_avg:57.97ms
step:1669/2330 train_time:96754ms step_avg:57.97ms
step:1670/2330 train_time:96815ms step_avg:57.97ms
step:1671/2330 train_time:96873ms step_avg:57.97ms
step:1672/2330 train_time:96933ms step_avg:57.97ms
step:1673/2330 train_time:96991ms step_avg:57.97ms
step:1674/2330 train_time:97052ms step_avg:57.98ms
step:1675/2330 train_time:97109ms step_avg:57.98ms
step:1676/2330 train_time:97171ms step_avg:57.98ms
step:1677/2330 train_time:97228ms step_avg:57.98ms
step:1678/2330 train_time:97289ms step_avg:57.98ms
step:1679/2330 train_time:97346ms step_avg:57.98ms
step:1680/2330 train_time:97407ms step_avg:57.98ms
step:1681/2330 train_time:97464ms step_avg:57.98ms
step:1682/2330 train_time:97525ms step_avg:57.98ms
step:1683/2330 train_time:97582ms step_avg:57.98ms
step:1684/2330 train_time:97646ms step_avg:57.98ms
step:1685/2330 train_time:97702ms step_avg:57.98ms
step:1686/2330 train_time:97764ms step_avg:57.99ms
step:1687/2330 train_time:97821ms step_avg:57.98ms
step:1688/2330 train_time:97882ms step_avg:57.99ms
step:1689/2330 train_time:97938ms step_avg:57.99ms
step:1690/2330 train_time:97999ms step_avg:57.99ms
step:1691/2330 train_time:98056ms step_avg:57.99ms
step:1692/2330 train_time:98117ms step_avg:57.99ms
step:1693/2330 train_time:98174ms step_avg:57.99ms
step:1694/2330 train_time:98235ms step_avg:57.99ms
step:1695/2330 train_time:98292ms step_avg:57.99ms
step:1696/2330 train_time:98353ms step_avg:57.99ms
step:1697/2330 train_time:98410ms step_avg:57.99ms
step:1698/2330 train_time:98472ms step_avg:57.99ms
step:1699/2330 train_time:98531ms step_avg:57.99ms
step:1700/2330 train_time:98591ms step_avg:57.99ms
step:1701/2330 train_time:98648ms step_avg:57.99ms
step:1702/2330 train_time:98709ms step_avg:58.00ms
step:1703/2330 train_time:98767ms step_avg:58.00ms
step:1704/2330 train_time:98827ms step_avg:58.00ms
step:1705/2330 train_time:98884ms step_avg:58.00ms
step:1706/2330 train_time:98945ms step_avg:58.00ms
step:1707/2330 train_time:99001ms step_avg:58.00ms
step:1708/2330 train_time:99063ms step_avg:58.00ms
step:1709/2330 train_time:99120ms step_avg:58.00ms
step:1710/2330 train_time:99181ms step_avg:58.00ms
step:1711/2330 train_time:99238ms step_avg:58.00ms
step:1712/2330 train_time:99299ms step_avg:58.00ms
step:1713/2330 train_time:99356ms step_avg:58.00ms
step:1714/2330 train_time:99417ms step_avg:58.00ms
step:1715/2330 train_time:99475ms step_avg:58.00ms
step:1716/2330 train_time:99536ms step_avg:58.00ms
step:1717/2330 train_time:99593ms step_avg:58.00ms
step:1718/2330 train_time:99653ms step_avg:58.01ms
step:1719/2330 train_time:99710ms step_avg:58.00ms
step:1720/2330 train_time:99771ms step_avg:58.01ms
step:1721/2330 train_time:99829ms step_avg:58.01ms
step:1722/2330 train_time:99890ms step_avg:58.01ms
step:1723/2330 train_time:99948ms step_avg:58.01ms
step:1724/2330 train_time:100009ms step_avg:58.01ms
step:1725/2330 train_time:100067ms step_avg:58.01ms
step:1726/2330 train_time:100128ms step_avg:58.01ms
step:1727/2330 train_time:100185ms step_avg:58.01ms
step:1728/2330 train_time:100246ms step_avg:58.01ms
step:1729/2330 train_time:100303ms step_avg:58.01ms
step:1730/2330 train_time:100365ms step_avg:58.01ms
step:1731/2330 train_time:100421ms step_avg:58.01ms
step:1732/2330 train_time:100483ms step_avg:58.02ms
step:1733/2330 train_time:100539ms step_avg:58.01ms
step:1734/2330 train_time:100600ms step_avg:58.02ms
step:1735/2330 train_time:100656ms step_avg:58.02ms
step:1736/2330 train_time:100717ms step_avg:58.02ms
step:1737/2330 train_time:100775ms step_avg:58.02ms
step:1738/2330 train_time:100834ms step_avg:58.02ms
step:1739/2330 train_time:100892ms step_avg:58.02ms
step:1740/2330 train_time:100953ms step_avg:58.02ms
step:1741/2330 train_time:101011ms step_avg:58.02ms
step:1742/2330 train_time:101071ms step_avg:58.02ms
step:1743/2330 train_time:101129ms step_avg:58.02ms
step:1744/2330 train_time:101190ms step_avg:58.02ms
step:1745/2330 train_time:101248ms step_avg:58.02ms
step:1746/2330 train_time:101308ms step_avg:58.02ms
step:1747/2330 train_time:101365ms step_avg:58.02ms
step:1748/2330 train_time:101427ms step_avg:58.02ms
step:1749/2330 train_time:101484ms step_avg:58.02ms
step:1750/2330 train_time:101544ms step_avg:58.03ms
step:1750/2330 val_loss:4.4106 train_time:101626ms step_avg:58.07ms
step:1751/2330 train_time:101647ms step_avg:58.05ms
step:1752/2330 train_time:101668ms step_avg:58.03ms
step:1753/2330 train_time:101719ms step_avg:58.03ms
step:1754/2330 train_time:101789ms step_avg:58.03ms
step:1755/2330 train_time:101845ms step_avg:58.03ms
step:1756/2330 train_time:101911ms step_avg:58.04ms
step:1757/2330 train_time:101967ms step_avg:58.03ms
step:1758/2330 train_time:102028ms step_avg:58.04ms
step:1759/2330 train_time:102084ms step_avg:58.04ms
step:1760/2330 train_time:102144ms step_avg:58.04ms
step:1761/2330 train_time:102201ms step_avg:58.04ms
step:1762/2330 train_time:102260ms step_avg:58.04ms
step:1763/2330 train_time:102317ms step_avg:58.04ms
step:1764/2330 train_time:102376ms step_avg:58.04ms
step:1765/2330 train_time:102433ms step_avg:58.04ms
step:1766/2330 train_time:102492ms step_avg:58.04ms
step:1767/2330 train_time:102550ms step_avg:58.04ms
step:1768/2330 train_time:102611ms step_avg:58.04ms
step:1769/2330 train_time:102669ms step_avg:58.04ms
step:1770/2330 train_time:102732ms step_avg:58.04ms
step:1771/2330 train_time:102790ms step_avg:58.04ms
step:1772/2330 train_time:102851ms step_avg:58.04ms
step:1773/2330 train_time:102908ms step_avg:58.04ms
step:1774/2330 train_time:102969ms step_avg:58.04ms
step:1775/2330 train_time:103026ms step_avg:58.04ms
step:1776/2330 train_time:103087ms step_avg:58.04ms
step:1777/2330 train_time:103144ms step_avg:58.04ms
step:1778/2330 train_time:103204ms step_avg:58.04ms
step:1779/2330 train_time:103260ms step_avg:58.04ms
step:1780/2330 train_time:103320ms step_avg:58.05ms
step:1781/2330 train_time:103377ms step_avg:58.04ms
step:1782/2330 train_time:103436ms step_avg:58.05ms
step:1783/2330 train_time:103494ms step_avg:58.04ms
step:1784/2330 train_time:103553ms step_avg:58.05ms
step:1785/2330 train_time:103611ms step_avg:58.05ms
step:1786/2330 train_time:103672ms step_avg:58.05ms
step:1787/2330 train_time:103729ms step_avg:58.05ms
step:1788/2330 train_time:103792ms step_avg:58.05ms
step:1789/2330 train_time:103849ms step_avg:58.05ms
step:1790/2330 train_time:103910ms step_avg:58.05ms
step:1791/2330 train_time:103967ms step_avg:58.05ms
step:1792/2330 train_time:104028ms step_avg:58.05ms
step:1793/2330 train_time:104085ms step_avg:58.05ms
step:1794/2330 train_time:104146ms step_avg:58.05ms
step:1795/2330 train_time:104202ms step_avg:58.05ms
step:1796/2330 train_time:104262ms step_avg:58.05ms
step:1797/2330 train_time:104318ms step_avg:58.05ms
step:1798/2330 train_time:104379ms step_avg:58.05ms
step:1799/2330 train_time:104436ms step_avg:58.05ms
step:1800/2330 train_time:104495ms step_avg:58.05ms
step:1801/2330 train_time:104552ms step_avg:58.05ms
step:1802/2330 train_time:104614ms step_avg:58.05ms
step:1803/2330 train_time:104671ms step_avg:58.05ms
step:1804/2330 train_time:104733ms step_avg:58.06ms
step:1805/2330 train_time:104792ms step_avg:58.06ms
step:1806/2330 train_time:104852ms step_avg:58.06ms
step:1807/2330 train_time:104910ms step_avg:58.06ms
step:1808/2330 train_time:104971ms step_avg:58.06ms
step:1809/2330 train_time:105029ms step_avg:58.06ms
step:1810/2330 train_time:105091ms step_avg:58.06ms
step:1811/2330 train_time:105149ms step_avg:58.06ms
step:1812/2330 train_time:105209ms step_avg:58.06ms
step:1813/2330 train_time:105265ms step_avg:58.06ms
step:1814/2330 train_time:105327ms step_avg:58.06ms
step:1815/2330 train_time:105384ms step_avg:58.06ms
step:1816/2330 train_time:105444ms step_avg:58.06ms
step:1817/2330 train_time:105500ms step_avg:58.06ms
step:1818/2330 train_time:105560ms step_avg:58.06ms
step:1819/2330 train_time:105617ms step_avg:58.06ms
step:1820/2330 train_time:105677ms step_avg:58.06ms
step:1821/2330 train_time:105735ms step_avg:58.06ms
step:1822/2330 train_time:105795ms step_avg:58.07ms
step:1823/2330 train_time:105852ms step_avg:58.06ms
step:1824/2330 train_time:105913ms step_avg:58.07ms
step:1825/2330 train_time:105971ms step_avg:58.07ms
step:1826/2330 train_time:106032ms step_avg:58.07ms
step:1827/2330 train_time:106090ms step_avg:58.07ms
step:1828/2330 train_time:106152ms step_avg:58.07ms
step:1829/2330 train_time:106209ms step_avg:58.07ms
step:1830/2330 train_time:106270ms step_avg:58.07ms
step:1831/2330 train_time:106327ms step_avg:58.07ms
step:1832/2330 train_time:106388ms step_avg:58.07ms
step:1833/2330 train_time:106445ms step_avg:58.07ms
step:1834/2330 train_time:106507ms step_avg:58.07ms
step:1835/2330 train_time:106563ms step_avg:58.07ms
step:1836/2330 train_time:106624ms step_avg:58.07ms
step:1837/2330 train_time:106681ms step_avg:58.07ms
step:1838/2330 train_time:106741ms step_avg:58.07ms
step:1839/2330 train_time:106797ms step_avg:58.07ms
step:1840/2330 train_time:106858ms step_avg:58.08ms
step:1841/2330 train_time:106915ms step_avg:58.07ms
step:1842/2330 train_time:106977ms step_avg:58.08ms
step:1843/2330 train_time:107034ms step_avg:58.08ms
step:1844/2330 train_time:107095ms step_avg:58.08ms
step:1845/2330 train_time:107153ms step_avg:58.08ms
step:1846/2330 train_time:107214ms step_avg:58.08ms
step:1847/2330 train_time:107272ms step_avg:58.08ms
step:1848/2330 train_time:107334ms step_avg:58.08ms
step:1849/2330 train_time:107391ms step_avg:58.08ms
step:1850/2330 train_time:107452ms step_avg:58.08ms
step:1851/2330 train_time:107510ms step_avg:58.08ms
step:1852/2330 train_time:107569ms step_avg:58.08ms
step:1853/2330 train_time:107625ms step_avg:58.08ms
step:1854/2330 train_time:107688ms step_avg:58.08ms
step:1855/2330 train_time:107745ms step_avg:58.08ms
step:1856/2330 train_time:107806ms step_avg:58.09ms
step:1857/2330 train_time:107862ms step_avg:58.08ms
step:1858/2330 train_time:107923ms step_avg:58.09ms
step:1859/2330 train_time:107979ms step_avg:58.08ms
step:1860/2330 train_time:108041ms step_avg:58.09ms
step:1861/2330 train_time:108098ms step_avg:58.09ms
step:1862/2330 train_time:108159ms step_avg:58.09ms
step:1863/2330 train_time:108216ms step_avg:58.09ms
step:1864/2330 train_time:108278ms step_avg:58.09ms
step:1865/2330 train_time:108335ms step_avg:58.09ms
step:1866/2330 train_time:108396ms step_avg:58.09ms
step:1867/2330 train_time:108454ms step_avg:58.09ms
step:1868/2330 train_time:108515ms step_avg:58.09ms
step:1869/2330 train_time:108572ms step_avg:58.09ms
step:1870/2330 train_time:108633ms step_avg:58.09ms
step:1871/2330 train_time:108691ms step_avg:58.09ms
step:1872/2330 train_time:108751ms step_avg:58.09ms
step:1873/2330 train_time:108809ms step_avg:58.09ms
step:1874/2330 train_time:108869ms step_avg:58.09ms
step:1875/2330 train_time:108926ms step_avg:58.09ms
step:1876/2330 train_time:108987ms step_avg:58.10ms
step:1877/2330 train_time:109043ms step_avg:58.09ms
step:1878/2330 train_time:109104ms step_avg:58.10ms
step:1879/2330 train_time:109160ms step_avg:58.09ms
step:1880/2330 train_time:109221ms step_avg:58.10ms
step:1881/2330 train_time:109277ms step_avg:58.10ms
step:1882/2330 train_time:109340ms step_avg:58.10ms
step:1883/2330 train_time:109397ms step_avg:58.10ms
step:1884/2330 train_time:109458ms step_avg:58.10ms
step:1885/2330 train_time:109515ms step_avg:58.10ms
step:1886/2330 train_time:109577ms step_avg:58.10ms
step:1887/2330 train_time:109634ms step_avg:58.10ms
step:1888/2330 train_time:109695ms step_avg:58.10ms
step:1889/2330 train_time:109752ms step_avg:58.10ms
step:1890/2330 train_time:109813ms step_avg:58.10ms
step:1891/2330 train_time:109871ms step_avg:58.10ms
step:1892/2330 train_time:109933ms step_avg:58.10ms
step:1893/2330 train_time:109991ms step_avg:58.10ms
step:1894/2330 train_time:110051ms step_avg:58.10ms
step:1895/2330 train_time:110107ms step_avg:58.10ms
step:1896/2330 train_time:110169ms step_avg:58.11ms
step:1897/2330 train_time:110226ms step_avg:58.11ms
step:1898/2330 train_time:110287ms step_avg:58.11ms
step:1899/2330 train_time:110344ms step_avg:58.11ms
step:1900/2330 train_time:110404ms step_avg:58.11ms
step:1901/2330 train_time:110461ms step_avg:58.11ms
step:1902/2330 train_time:110523ms step_avg:58.11ms
step:1903/2330 train_time:110579ms step_avg:58.11ms
step:1904/2330 train_time:110641ms step_avg:58.11ms
step:1905/2330 train_time:110697ms step_avg:58.11ms
step:1906/2330 train_time:110759ms step_avg:58.11ms
step:1907/2330 train_time:110815ms step_avg:58.11ms
step:1908/2330 train_time:110877ms step_avg:58.11ms
step:1909/2330 train_time:110934ms step_avg:58.11ms
step:1910/2330 train_time:110995ms step_avg:58.11ms
step:1911/2330 train_time:111053ms step_avg:58.11ms
step:1912/2330 train_time:111113ms step_avg:58.11ms
step:1913/2330 train_time:111171ms step_avg:58.11ms
step:1914/2330 train_time:111231ms step_avg:58.11ms
step:1915/2330 train_time:111289ms step_avg:58.11ms
step:1916/2330 train_time:111350ms step_avg:58.12ms
step:1917/2330 train_time:111407ms step_avg:58.12ms
step:1918/2330 train_time:111468ms step_avg:58.12ms
step:1919/2330 train_time:111525ms step_avg:58.12ms
step:1920/2330 train_time:111586ms step_avg:58.12ms
step:1921/2330 train_time:111642ms step_avg:58.12ms
step:1922/2330 train_time:111703ms step_avg:58.12ms
step:1923/2330 train_time:111759ms step_avg:58.12ms
step:1924/2330 train_time:111820ms step_avg:58.12ms
step:1925/2330 train_time:111876ms step_avg:58.12ms
step:1926/2330 train_time:111938ms step_avg:58.12ms
step:1927/2330 train_time:111996ms step_avg:58.12ms
step:1928/2330 train_time:112056ms step_avg:58.12ms
step:1929/2330 train_time:112113ms step_avg:58.12ms
step:1930/2330 train_time:112174ms step_avg:58.12ms
step:1931/2330 train_time:112232ms step_avg:58.12ms
step:1932/2330 train_time:112293ms step_avg:58.12ms
step:1933/2330 train_time:112351ms step_avg:58.12ms
step:1934/2330 train_time:112412ms step_avg:58.12ms
step:1935/2330 train_time:112469ms step_avg:58.12ms
step:1936/2330 train_time:112531ms step_avg:58.13ms
step:1937/2330 train_time:112588ms step_avg:58.12ms
step:1938/2330 train_time:112648ms step_avg:58.13ms
step:1939/2330 train_time:112704ms step_avg:58.12ms
step:1940/2330 train_time:112767ms step_avg:58.13ms
step:1941/2330 train_time:112824ms step_avg:58.13ms
step:1942/2330 train_time:112885ms step_avg:58.13ms
step:1943/2330 train_time:112942ms step_avg:58.13ms
step:1944/2330 train_time:113003ms step_avg:58.13ms
step:1945/2330 train_time:113058ms step_avg:58.13ms
step:1946/2330 train_time:113121ms step_avg:58.13ms
step:1947/2330 train_time:113177ms step_avg:58.13ms
step:1948/2330 train_time:113239ms step_avg:58.13ms
step:1949/2330 train_time:113296ms step_avg:58.13ms
step:1950/2330 train_time:113356ms step_avg:58.13ms
step:1951/2330 train_time:113414ms step_avg:58.13ms
step:1952/2330 train_time:113475ms step_avg:58.13ms
step:1953/2330 train_time:113533ms step_avg:58.13ms
step:1954/2330 train_time:113593ms step_avg:58.13ms
step:1955/2330 train_time:113651ms step_avg:58.13ms
step:1956/2330 train_time:113711ms step_avg:58.13ms
step:1957/2330 train_time:113769ms step_avg:58.13ms
step:1958/2330 train_time:113829ms step_avg:58.14ms
step:1959/2330 train_time:113886ms step_avg:58.13ms
step:1960/2330 train_time:113948ms step_avg:58.14ms
step:1961/2330 train_time:114004ms step_avg:58.14ms
step:1962/2330 train_time:114065ms step_avg:58.14ms
step:1963/2330 train_time:114122ms step_avg:58.14ms
step:1964/2330 train_time:114182ms step_avg:58.14ms
step:1965/2330 train_time:114239ms step_avg:58.14ms
step:1966/2330 train_time:114299ms step_avg:58.14ms
step:1967/2330 train_time:114356ms step_avg:58.14ms
step:1968/2330 train_time:114418ms step_avg:58.14ms
step:1969/2330 train_time:114475ms step_avg:58.14ms
step:1970/2330 train_time:114537ms step_avg:58.14ms
step:1971/2330 train_time:114594ms step_avg:58.14ms
step:1972/2330 train_time:114654ms step_avg:58.14ms
step:1973/2330 train_time:114712ms step_avg:58.14ms
step:1974/2330 train_time:114773ms step_avg:58.14ms
step:1975/2330 train_time:114831ms step_avg:58.14ms
step:1976/2330 train_time:114892ms step_avg:58.14ms
step:1977/2330 train_time:114949ms step_avg:58.14ms
step:1978/2330 train_time:115010ms step_avg:58.14ms
step:1979/2330 train_time:115067ms step_avg:58.14ms
step:1980/2330 train_time:115128ms step_avg:58.15ms
step:1981/2330 train_time:115185ms step_avg:58.14ms
step:1982/2330 train_time:115246ms step_avg:58.15ms
step:1983/2330 train_time:115303ms step_avg:58.15ms
step:1984/2330 train_time:115363ms step_avg:58.15ms
step:1985/2330 train_time:115419ms step_avg:58.15ms
step:1986/2330 train_time:115480ms step_avg:58.15ms
step:1987/2330 train_time:115537ms step_avg:58.15ms
step:1988/2330 train_time:115597ms step_avg:58.15ms
step:1989/2330 train_time:115654ms step_avg:58.15ms
step:1990/2330 train_time:115716ms step_avg:58.15ms
step:1991/2330 train_time:115773ms step_avg:58.15ms
step:1992/2330 train_time:115834ms step_avg:58.15ms
step:1993/2330 train_time:115893ms step_avg:58.15ms
step:1994/2330 train_time:115953ms step_avg:58.15ms
step:1995/2330 train_time:116012ms step_avg:58.15ms
step:1996/2330 train_time:116073ms step_avg:58.15ms
step:1997/2330 train_time:116130ms step_avg:58.15ms
step:1998/2330 train_time:116191ms step_avg:58.15ms
step:1999/2330 train_time:116249ms step_avg:58.15ms
step:2000/2330 train_time:116310ms step_avg:58.16ms
step:2000/2330 val_loss:4.3216 train_time:116392ms step_avg:58.20ms
step:2001/2330 train_time:116412ms step_avg:58.18ms
step:2002/2330 train_time:116433ms step_avg:58.16ms
step:2003/2330 train_time:116490ms step_avg:58.16ms
step:2004/2330 train_time:116557ms step_avg:58.16ms
step:2005/2330 train_time:116614ms step_avg:58.16ms
step:2006/2330 train_time:116675ms step_avg:58.16ms
step:2007/2330 train_time:116733ms step_avg:58.16ms
step:2008/2330 train_time:116792ms step_avg:58.16ms
step:2009/2330 train_time:116849ms step_avg:58.16ms
step:2010/2330 train_time:116909ms step_avg:58.16ms
step:2011/2330 train_time:116965ms step_avg:58.16ms
step:2012/2330 train_time:117024ms step_avg:58.16ms
step:2013/2330 train_time:117080ms step_avg:58.16ms
step:2014/2330 train_time:117140ms step_avg:58.16ms
step:2015/2330 train_time:117197ms step_avg:58.16ms
step:2016/2330 train_time:117257ms step_avg:58.16ms
step:2017/2330 train_time:117313ms step_avg:58.16ms
step:2018/2330 train_time:117374ms step_avg:58.16ms
step:2019/2330 train_time:117431ms step_avg:58.16ms
step:2020/2330 train_time:117495ms step_avg:58.17ms
step:2021/2330 train_time:117552ms step_avg:58.17ms
step:2022/2330 train_time:117614ms step_avg:58.17ms
step:2023/2330 train_time:117672ms step_avg:58.17ms
step:2024/2330 train_time:117733ms step_avg:58.17ms
step:2025/2330 train_time:117790ms step_avg:58.17ms
step:2026/2330 train_time:117851ms step_avg:58.17ms
step:2027/2330 train_time:117908ms step_avg:58.17ms
step:2028/2330 train_time:117967ms step_avg:58.17ms
step:2029/2330 train_time:118025ms step_avg:58.17ms
step:2030/2330 train_time:118084ms step_avg:58.17ms
step:2031/2330 train_time:118140ms step_avg:58.17ms
step:2032/2330 train_time:118201ms step_avg:58.17ms
step:2033/2330 train_time:118257ms step_avg:58.17ms
step:2034/2330 train_time:118317ms step_avg:58.17ms
step:2035/2330 train_time:118374ms step_avg:58.17ms
step:2036/2330 train_time:118435ms step_avg:58.17ms
step:2037/2330 train_time:118493ms step_avg:58.17ms
step:2038/2330 train_time:118555ms step_avg:58.17ms
step:2039/2330 train_time:118613ms step_avg:58.17ms
step:2040/2330 train_time:118673ms step_avg:58.17ms
step:2041/2330 train_time:118730ms step_avg:58.17ms
step:2042/2330 train_time:118792ms step_avg:58.17ms
step:2043/2330 train_time:118849ms step_avg:58.17ms
step:2044/2330 train_time:118909ms step_avg:58.17ms
step:2045/2330 train_time:118967ms step_avg:58.17ms
step:2046/2330 train_time:119027ms step_avg:58.18ms
step:2047/2330 train_time:119083ms step_avg:58.17ms
step:2048/2330 train_time:119145ms step_avg:58.18ms
step:2049/2330 train_time:119202ms step_avg:58.18ms
step:2050/2330 train_time:119263ms step_avg:58.18ms
step:2051/2330 train_time:119320ms step_avg:58.18ms
step:2052/2330 train_time:119381ms step_avg:58.18ms
step:2053/2330 train_time:119437ms step_avg:58.18ms
step:2054/2330 train_time:119501ms step_avg:58.18ms
step:2055/2330 train_time:119557ms step_avg:58.18ms
step:2056/2330 train_time:119620ms step_avg:58.18ms
step:2057/2330 train_time:119677ms step_avg:58.18ms
step:2058/2330 train_time:119739ms step_avg:58.18ms
step:2059/2330 train_time:119796ms step_avg:58.18ms
step:2060/2330 train_time:119857ms step_avg:58.18ms
step:2061/2330 train_time:119914ms step_avg:58.18ms
step:2062/2330 train_time:119976ms step_avg:58.18ms
step:2063/2330 train_time:120033ms step_avg:58.18ms
step:2064/2330 train_time:120094ms step_avg:58.18ms
step:2065/2330 train_time:120151ms step_avg:58.18ms
step:2066/2330 train_time:120212ms step_avg:58.19ms
step:2067/2330 train_time:120269ms step_avg:58.19ms
step:2068/2330 train_time:120329ms step_avg:58.19ms
step:2069/2330 train_time:120387ms step_avg:58.19ms
step:2070/2330 train_time:120448ms step_avg:58.19ms
step:2071/2330 train_time:120505ms step_avg:58.19ms
step:2072/2330 train_time:120567ms step_avg:58.19ms
step:2073/2330 train_time:120625ms step_avg:58.19ms
step:2074/2330 train_time:120685ms step_avg:58.19ms
step:2075/2330 train_time:120743ms step_avg:58.19ms
step:2076/2330 train_time:120805ms step_avg:58.19ms
step:2077/2330 train_time:120861ms step_avg:58.19ms
step:2078/2330 train_time:120922ms step_avg:58.19ms
step:2079/2330 train_time:120979ms step_avg:58.19ms
step:2080/2330 train_time:121039ms step_avg:58.19ms
step:2081/2330 train_time:121096ms step_avg:58.19ms
step:2082/2330 train_time:121156ms step_avg:58.19ms
step:2083/2330 train_time:121213ms step_avg:58.19ms
step:2084/2330 train_time:121273ms step_avg:58.19ms
step:2085/2330 train_time:121330ms step_avg:58.19ms
step:2086/2330 train_time:121392ms step_avg:58.19ms
step:2087/2330 train_time:121450ms step_avg:58.19ms
step:2088/2330 train_time:121510ms step_avg:58.19ms
step:2089/2330 train_time:121568ms step_avg:58.19ms
step:2090/2330 train_time:121628ms step_avg:58.20ms
step:2091/2330 train_time:121686ms step_avg:58.20ms
step:2092/2330 train_time:121747ms step_avg:58.20ms
step:2093/2330 train_time:121805ms step_avg:58.20ms
step:2094/2330 train_time:121866ms step_avg:58.20ms
step:2095/2330 train_time:121924ms step_avg:58.20ms
step:2096/2330 train_time:121984ms step_avg:58.20ms
step:2097/2330 train_time:122040ms step_avg:58.20ms
step:2098/2330 train_time:122101ms step_avg:58.20ms
step:2099/2330 train_time:122158ms step_avg:58.20ms
step:2100/2330 train_time:122219ms step_avg:58.20ms
step:2101/2330 train_time:122275ms step_avg:58.20ms
step:2102/2330 train_time:122336ms step_avg:58.20ms
step:2103/2330 train_time:122393ms step_avg:58.20ms
step:2104/2330 train_time:122455ms step_avg:58.20ms
step:2105/2330 train_time:122513ms step_avg:58.20ms
step:2106/2330 train_time:122573ms step_avg:58.20ms
step:2107/2330 train_time:122630ms step_avg:58.20ms
step:2108/2330 train_time:122692ms step_avg:58.20ms
step:2109/2330 train_time:122749ms step_avg:58.20ms
step:2110/2330 train_time:122810ms step_avg:58.20ms
step:2111/2330 train_time:122867ms step_avg:58.20ms
step:2112/2330 train_time:122929ms step_avg:58.20ms
step:2113/2330 train_time:122986ms step_avg:58.20ms
step:2114/2330 train_time:123047ms step_avg:58.21ms
step:2115/2330 train_time:123104ms step_avg:58.21ms
step:2116/2330 train_time:123164ms step_avg:58.21ms
step:2117/2330 train_time:123221ms step_avg:58.21ms
step:2118/2330 train_time:123282ms step_avg:58.21ms
step:2119/2330 train_time:123339ms step_avg:58.21ms
step:2120/2330 train_time:123401ms step_avg:58.21ms
step:2121/2330 train_time:123457ms step_avg:58.21ms
step:2122/2330 train_time:123519ms step_avg:58.21ms
step:2123/2330 train_time:123575ms step_avg:58.21ms
step:2124/2330 train_time:123638ms step_avg:58.21ms
step:2125/2330 train_time:123694ms step_avg:58.21ms
step:2126/2330 train_time:123756ms step_avg:58.21ms
step:2127/2330 train_time:123812ms step_avg:58.21ms
step:2128/2330 train_time:123875ms step_avg:58.21ms
step:2129/2330 train_time:123931ms step_avg:58.21ms
step:2130/2330 train_time:123993ms step_avg:58.21ms
step:2131/2330 train_time:124050ms step_avg:58.21ms
step:2132/2330 train_time:124111ms step_avg:58.21ms
step:2133/2330 train_time:124169ms step_avg:58.21ms
step:2134/2330 train_time:124229ms step_avg:58.21ms
step:2135/2330 train_time:124287ms step_avg:58.21ms
step:2136/2330 train_time:124348ms step_avg:58.22ms
step:2137/2330 train_time:124406ms step_avg:58.22ms
step:2138/2330 train_time:124466ms step_avg:58.22ms
step:2139/2330 train_time:124523ms step_avg:58.22ms
step:2140/2330 train_time:124584ms step_avg:58.22ms
step:2141/2330 train_time:124641ms step_avg:58.22ms
step:2142/2330 train_time:124703ms step_avg:58.22ms
step:2143/2330 train_time:124760ms step_avg:58.22ms
step:2144/2330 train_time:124821ms step_avg:58.22ms
step:2145/2330 train_time:124877ms step_avg:58.22ms
step:2146/2330 train_time:124938ms step_avg:58.22ms
step:2147/2330 train_time:124995ms step_avg:58.22ms
step:2148/2330 train_time:125056ms step_avg:58.22ms
step:2149/2330 train_time:125113ms step_avg:58.22ms
step:2150/2330 train_time:125174ms step_avg:58.22ms
step:2151/2330 train_time:125231ms step_avg:58.22ms
step:2152/2330 train_time:125293ms step_avg:58.22ms
step:2153/2330 train_time:125350ms step_avg:58.22ms
step:2154/2330 train_time:125410ms step_avg:58.22ms
step:2155/2330 train_time:125467ms step_avg:58.22ms
step:2156/2330 train_time:125529ms step_avg:58.22ms
step:2157/2330 train_time:125586ms step_avg:58.22ms
step:2158/2330 train_time:125647ms step_avg:58.22ms
step:2159/2330 train_time:125705ms step_avg:58.22ms
step:2160/2330 train_time:125767ms step_avg:58.23ms
step:2161/2330 train_time:125824ms step_avg:58.23ms
step:2162/2330 train_time:125886ms step_avg:58.23ms
step:2163/2330 train_time:125944ms step_avg:58.23ms
step:2164/2330 train_time:126004ms step_avg:58.23ms
step:2165/2330 train_time:126061ms step_avg:58.23ms
step:2166/2330 train_time:126123ms step_avg:58.23ms
step:2167/2330 train_time:126179ms step_avg:58.23ms
step:2168/2330 train_time:126240ms step_avg:58.23ms
step:2169/2330 train_time:126296ms step_avg:58.23ms
step:2170/2330 train_time:126359ms step_avg:58.23ms
step:2171/2330 train_time:126415ms step_avg:58.23ms
step:2172/2330 train_time:126477ms step_avg:58.23ms
step:2173/2330 train_time:126534ms step_avg:58.23ms
step:2174/2330 train_time:126595ms step_avg:58.23ms
step:2175/2330 train_time:126653ms step_avg:58.23ms
step:2176/2330 train_time:126713ms step_avg:58.23ms
step:2177/2330 train_time:126770ms step_avg:58.23ms
step:2178/2330 train_time:126831ms step_avg:58.23ms
step:2179/2330 train_time:126888ms step_avg:58.23ms
step:2180/2330 train_time:126950ms step_avg:58.23ms
step:2181/2330 train_time:127009ms step_avg:58.23ms
step:2182/2330 train_time:127068ms step_avg:58.23ms
step:2183/2330 train_time:127126ms step_avg:58.23ms
step:2184/2330 train_time:127187ms step_avg:58.24ms
step:2185/2330 train_time:127244ms step_avg:58.24ms
step:2186/2330 train_time:127306ms step_avg:58.24ms
step:2187/2330 train_time:127363ms step_avg:58.24ms
step:2188/2330 train_time:127425ms step_avg:58.24ms
step:2189/2330 train_time:127481ms step_avg:58.24ms
step:2190/2330 train_time:127542ms step_avg:58.24ms
step:2191/2330 train_time:127599ms step_avg:58.24ms
step:2192/2330 train_time:127662ms step_avg:58.24ms
step:2193/2330 train_time:127718ms step_avg:58.24ms
step:2194/2330 train_time:127780ms step_avg:58.24ms
step:2195/2330 train_time:127836ms step_avg:58.24ms
step:2196/2330 train_time:127897ms step_avg:58.24ms
step:2197/2330 train_time:127954ms step_avg:58.24ms
step:2198/2330 train_time:128014ms step_avg:58.24ms
step:2199/2330 train_time:128072ms step_avg:58.24ms
step:2200/2330 train_time:128133ms step_avg:58.24ms
step:2201/2330 train_time:128190ms step_avg:58.24ms
step:2202/2330 train_time:128251ms step_avg:58.24ms
step:2203/2330 train_time:128309ms step_avg:58.24ms
step:2204/2330 train_time:128369ms step_avg:58.24ms
step:2205/2330 train_time:128427ms step_avg:58.24ms
step:2206/2330 train_time:128488ms step_avg:58.24ms
step:2207/2330 train_time:128546ms step_avg:58.24ms
step:2208/2330 train_time:128607ms step_avg:58.25ms
step:2209/2330 train_time:128665ms step_avg:58.25ms
step:2210/2330 train_time:128726ms step_avg:58.25ms
step:2211/2330 train_time:128783ms step_avg:58.25ms
step:2212/2330 train_time:128844ms step_avg:58.25ms
step:2213/2330 train_time:128900ms step_avg:58.25ms
step:2214/2330 train_time:128962ms step_avg:58.25ms
step:2215/2330 train_time:129019ms step_avg:58.25ms
step:2216/2330 train_time:129080ms step_avg:58.25ms
step:2217/2330 train_time:129136ms step_avg:58.25ms
step:2218/2330 train_time:129197ms step_avg:58.25ms
step:2219/2330 train_time:129253ms step_avg:58.25ms
step:2220/2330 train_time:129314ms step_avg:58.25ms
step:2221/2330 train_time:129371ms step_avg:58.25ms
step:2222/2330 train_time:129432ms step_avg:58.25ms
step:2223/2330 train_time:129488ms step_avg:58.25ms
step:2224/2330 train_time:129551ms step_avg:58.25ms
step:2225/2330 train_time:129609ms step_avg:58.25ms
step:2226/2330 train_time:129670ms step_avg:58.25ms
step:2227/2330 train_time:129727ms step_avg:58.25ms
step:2228/2330 train_time:129789ms step_avg:58.25ms
step:2229/2330 train_time:129847ms step_avg:58.25ms
step:2230/2330 train_time:129907ms step_avg:58.25ms
step:2231/2330 train_time:129965ms step_avg:58.25ms
step:2232/2330 train_time:130025ms step_avg:58.26ms
step:2233/2330 train_time:130082ms step_avg:58.25ms
step:2234/2330 train_time:130143ms step_avg:58.26ms
step:2235/2330 train_time:130200ms step_avg:58.26ms
step:2236/2330 train_time:130262ms step_avg:58.26ms
step:2237/2330 train_time:130318ms step_avg:58.26ms
step:2238/2330 train_time:130379ms step_avg:58.26ms
step:2239/2330 train_time:130436ms step_avg:58.26ms
step:2240/2330 train_time:130497ms step_avg:58.26ms
step:2241/2330 train_time:130554ms step_avg:58.26ms
step:2242/2330 train_time:130615ms step_avg:58.26ms
step:2243/2330 train_time:130672ms step_avg:58.26ms
step:2244/2330 train_time:130733ms step_avg:58.26ms
step:2245/2330 train_time:130790ms step_avg:58.26ms
step:2246/2330 train_time:130851ms step_avg:58.26ms
step:2247/2330 train_time:130909ms step_avg:58.26ms
step:2248/2330 train_time:130969ms step_avg:58.26ms
step:2249/2330 train_time:131027ms step_avg:58.26ms
step:2250/2330 train_time:131088ms step_avg:58.26ms
step:2250/2330 val_loss:4.2589 train_time:131170ms step_avg:58.30ms
step:2251/2330 train_time:131189ms step_avg:58.28ms
step:2252/2330 train_time:131209ms step_avg:58.26ms
step:2253/2330 train_time:131269ms step_avg:58.26ms
step:2254/2330 train_time:131334ms step_avg:58.27ms
step:2255/2330 train_time:131391ms step_avg:58.27ms
step:2256/2330 train_time:131453ms step_avg:58.27ms
step:2257/2330 train_time:131510ms step_avg:58.27ms
step:2258/2330 train_time:131569ms step_avg:58.27ms
step:2259/2330 train_time:131626ms step_avg:58.27ms
step:2260/2330 train_time:131685ms step_avg:58.27ms
step:2261/2330 train_time:131742ms step_avg:58.27ms
step:2262/2330 train_time:131802ms step_avg:58.27ms
step:2263/2330 train_time:131858ms step_avg:58.27ms
step:2264/2330 train_time:131919ms step_avg:58.27ms
step:2265/2330 train_time:131975ms step_avg:58.27ms
step:2266/2330 train_time:132036ms step_avg:58.27ms
step:2267/2330 train_time:132092ms step_avg:58.27ms
step:2268/2330 train_time:132154ms step_avg:58.27ms
step:2269/2330 train_time:132212ms step_avg:58.27ms
step:2270/2330 train_time:132276ms step_avg:58.27ms
step:2271/2330 train_time:132333ms step_avg:58.27ms
step:2272/2330 train_time:132396ms step_avg:58.27ms
step:2273/2330 train_time:132453ms step_avg:58.27ms
step:2274/2330 train_time:132514ms step_avg:58.27ms
step:2275/2330 train_time:132571ms step_avg:58.27ms
step:2276/2330 train_time:132631ms step_avg:58.27ms
step:2277/2330 train_time:132687ms step_avg:58.27ms
step:2278/2330 train_time:132747ms step_avg:58.27ms
step:2279/2330 train_time:132804ms step_avg:58.27ms
step:2280/2330 train_time:132864ms step_avg:58.27ms
step:2281/2330 train_time:132921ms step_avg:58.27ms
step:2282/2330 train_time:132981ms step_avg:58.27ms
step:2283/2330 train_time:133038ms step_avg:58.27ms
step:2284/2330 train_time:133099ms step_avg:58.27ms
step:2285/2330 train_time:133157ms step_avg:58.27ms
step:2286/2330 train_time:133219ms step_avg:58.28ms
step:2287/2330 train_time:133276ms step_avg:58.28ms
step:2288/2330 train_time:133339ms step_avg:58.28ms
step:2289/2330 train_time:133396ms step_avg:58.28ms
step:2290/2330 train_time:133458ms step_avg:58.28ms
step:2291/2330 train_time:133514ms step_avg:58.28ms
step:2292/2330 train_time:133576ms step_avg:58.28ms
step:2293/2330 train_time:133633ms step_avg:58.28ms
step:2294/2330 train_time:133693ms step_avg:58.28ms
step:2295/2330 train_time:133750ms step_avg:58.28ms
step:2296/2330 train_time:133810ms step_avg:58.28ms
step:2297/2330 train_time:133867ms step_avg:58.28ms
step:2298/2330 train_time:133926ms step_avg:58.28ms
step:2299/2330 train_time:133983ms step_avg:58.28ms
step:2300/2330 train_time:134043ms step_avg:58.28ms
step:2301/2330 train_time:134100ms step_avg:58.28ms
step:2302/2330 train_time:134161ms step_avg:58.28ms
step:2303/2330 train_time:134221ms step_avg:58.28ms
step:2304/2330 train_time:134281ms step_avg:58.28ms
step:2305/2330 train_time:134339ms step_avg:58.28ms
step:2306/2330 train_time:134400ms step_avg:58.28ms
step:2307/2330 train_time:134458ms step_avg:58.28ms
step:2308/2330 train_time:134520ms step_avg:58.28ms
step:2309/2330 train_time:134578ms step_avg:58.28ms
step:2310/2330 train_time:134638ms step_avg:58.28ms
step:2311/2330 train_time:134695ms step_avg:58.28ms
step:2312/2330 train_time:134756ms step_avg:58.29ms
step:2313/2330 train_time:134812ms step_avg:58.28ms
step:2314/2330 train_time:134873ms step_avg:58.29ms
step:2315/2330 train_time:134929ms step_avg:58.28ms
step:2316/2330 train_time:134989ms step_avg:58.29ms
step:2317/2330 train_time:135046ms step_avg:58.28ms
step:2318/2330 train_time:135106ms step_avg:58.29ms
step:2319/2330 train_time:135163ms step_avg:58.29ms
step:2320/2330 train_time:135225ms step_avg:58.29ms
step:2321/2330 train_time:135283ms step_avg:58.29ms
step:2322/2330 train_time:135343ms step_avg:58.29ms
step:2323/2330 train_time:135402ms step_avg:58.29ms
step:2324/2330 train_time:135463ms step_avg:58.29ms
step:2325/2330 train_time:135522ms step_avg:58.29ms
step:2326/2330 train_time:135583ms step_avg:58.29ms
step:2327/2330 train_time:135641ms step_avg:58.29ms
step:2328/2330 train_time:135702ms step_avg:58.29ms
step:2329/2330 train_time:135760ms step_avg:58.29ms
step:2330/2330 train_time:135820ms step_avg:58.29ms
step:2330/2330 val_loss:4.2410 train_time:135902ms step_avg:58.33ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
