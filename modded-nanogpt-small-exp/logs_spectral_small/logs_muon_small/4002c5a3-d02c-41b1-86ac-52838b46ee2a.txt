import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_muon_small", exist_ok=True)
    logfile = f"logs_muon_small/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_muon_small/{run_id}", exist_ok=True)
            torch.save(log, f"logs_muon_small/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

if master_process:
    import pickle, re, os

    sv_out = {}
    hidden_ids = {id(p) for p in hidden_matrix_params}

    def pretty_name(full_name: str, idx: int | None = None) -> str:
        # attn qkv/o
        m = re.match(r"^blocks\.(\d+)\.attn\.qkvo_w$", full_name)
        if m and idx is not None:
            tag = ["q", "k", "v", "o"][idx]
            return f"layer{m.group(1)}_{tag}"
        # mlp fc/proj
        m = re.match(r"^blocks\.(\d+)\.mlp\.(c_fc|c_proj)$", full_name)
        if m:
            tag = "mlp_fc" if m.group(2) == "c_fc" else "mlp_proj"
            return f"layer{m.group(1)}_{tag}"
        # fallback
        return full_name if idx is None else f"{full_name}_{idx}"

    with torch.no_grad():
        for full_name, p in model.named_parameters():
            if id(p) not in hidden_ids:
                continue

            # ATTENTION: qkvo_w is stored as (hdim, dim*4); split into 4 matrices
            if full_name.endswith("attn.qkvo_w") and p.ndim == 2 and (p.size(1) % 4 == 0):
                W = p.detach().to(dtype=torch.float32, device="cpu").view(4, p.size(0), p.size(1) // 4)
                for i in range(4):
                    s = torch.linalg.svdvals(W[i])
                    sv_out[pretty_name(full_name, idx=i)] = s
                continue

            # Standard 2D weights (e.g., mlp.c_fc [dim, 4*dim], mlp.c_proj [dim, 4*dim])
            if p.ndim == 2:
                W = p.detach().to(dtype=torch.float32, device="cpu")
                s = torch.linalg.svdvals(W)
                sv_out[pretty_name(full_name)] = s
                continue

            # Rare fallback for unexpected >2D tensors: flatten to (out,in)
            W = p.detach().to(dtype=torch.float32, device="cpu")
            out_dim, in_dim = W.shape[-2], W.shape[-1]
            W2 = W.reshape(-1, in_dim) if out_dim != W.numel() // in_dim else W.view(out_dim, in_dim)
            s = torch.linalg.svdvals(W2)
            sv_out[pretty_name(full_name)] = s

    out_dir = f"logs_muon_small/{run_id}"
    os.makedirs(out_dir, exist_ok=True)
    sv_path = f"{out_dir}/singular_values.pkl"
    with open(sv_path, "wb") as f:
        pickle.dump(sv_out, f)

    print0(f"[SV] Saved singular values for {len(sv_out)} params to {sv_path}", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 00:37:38 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   27C    P0             113W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   26C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   25C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   25C    P0             114W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   24C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   25C    P0             113W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   24C    P0             109W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   24C    P0             109W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:90ms step_avg:89.90ms
step:2/2330 train_time:189ms step_avg:94.30ms
step:3/2330 train_time:211ms step_avg:70.33ms
step:4/2330 train_time:245ms step_avg:61.30ms
step:5/2330 train_time:302ms step_avg:60.48ms
step:6/2330 train_time:363ms step_avg:60.49ms
step:7/2330 train_time:421ms step_avg:60.18ms
step:8/2330 train_time:482ms step_avg:60.22ms
step:9/2330 train_time:540ms step_avg:60.02ms
step:10/2330 train_time:601ms step_avg:60.08ms
step:11/2330 train_time:660ms step_avg:59.98ms
step:12/2330 train_time:721ms step_avg:60.05ms
step:13/2330 train_time:779ms step_avg:59.92ms
step:14/2330 train_time:839ms step_avg:59.95ms
step:15/2330 train_time:898ms step_avg:59.87ms
step:16/2330 train_time:959ms step_avg:59.94ms
step:17/2330 train_time:1021ms step_avg:60.08ms
step:18/2330 train_time:1086ms step_avg:60.32ms
step:19/2330 train_time:1149ms step_avg:60.45ms
step:20/2330 train_time:1211ms step_avg:60.57ms
step:21/2330 train_time:1271ms step_avg:60.54ms
step:22/2330 train_time:1333ms step_avg:60.57ms
step:23/2330 train_time:1392ms step_avg:60.53ms
step:24/2330 train_time:1453ms step_avg:60.56ms
step:25/2330 train_time:1512ms step_avg:60.50ms
step:26/2330 train_time:1574ms step_avg:60.55ms
step:27/2330 train_time:1633ms step_avg:60.48ms
step:28/2330 train_time:1694ms step_avg:60.50ms
step:29/2330 train_time:1753ms step_avg:60.45ms
step:30/2330 train_time:1814ms step_avg:60.48ms
step:31/2330 train_time:1873ms step_avg:60.43ms
step:32/2330 train_time:1935ms step_avg:60.48ms
step:33/2330 train_time:1995ms step_avg:60.44ms
step:34/2330 train_time:2056ms step_avg:60.48ms
step:35/2330 train_time:2116ms step_avg:60.46ms
step:36/2330 train_time:2178ms step_avg:60.49ms
step:37/2330 train_time:2237ms step_avg:60.46ms
step:38/2330 train_time:2299ms step_avg:60.51ms
step:39/2330 train_time:2360ms step_avg:60.50ms
step:40/2330 train_time:2422ms step_avg:60.54ms
step:41/2330 train_time:2480ms step_avg:60.50ms
step:42/2330 train_time:2541ms step_avg:60.51ms
step:43/2330 train_time:2601ms step_avg:60.48ms
step:44/2330 train_time:2663ms step_avg:60.51ms
step:45/2330 train_time:2722ms step_avg:60.48ms
step:46/2330 train_time:2783ms step_avg:60.50ms
step:47/2330 train_time:2842ms step_avg:60.48ms
step:48/2330 train_time:2904ms step_avg:60.50ms
step:49/2330 train_time:2963ms step_avg:60.47ms
step:50/2330 train_time:3025ms step_avg:60.50ms
step:51/2330 train_time:3084ms step_avg:60.48ms
step:52/2330 train_time:3146ms step_avg:60.51ms
step:53/2330 train_time:3207ms step_avg:60.50ms
step:54/2330 train_time:3269ms step_avg:60.53ms
step:55/2330 train_time:3329ms step_avg:60.52ms
step:56/2330 train_time:3390ms step_avg:60.54ms
step:57/2330 train_time:3450ms step_avg:60.53ms
step:58/2330 train_time:3512ms step_avg:60.55ms
step:59/2330 train_time:3573ms step_avg:60.56ms
step:60/2330 train_time:3635ms step_avg:60.58ms
step:61/2330 train_time:3694ms step_avg:60.55ms
step:62/2330 train_time:3755ms step_avg:60.56ms
step:63/2330 train_time:3814ms step_avg:60.54ms
step:64/2330 train_time:3875ms step_avg:60.55ms
step:65/2330 train_time:3935ms step_avg:60.53ms
step:66/2330 train_time:3996ms step_avg:60.54ms
step:67/2330 train_time:4054ms step_avg:60.51ms
step:68/2330 train_time:4115ms step_avg:60.52ms
step:69/2330 train_time:4175ms step_avg:60.50ms
step:70/2330 train_time:4236ms step_avg:60.51ms
step:71/2330 train_time:4295ms step_avg:60.50ms
step:72/2330 train_time:4357ms step_avg:60.51ms
step:73/2330 train_time:4416ms step_avg:60.50ms
step:74/2330 train_time:4478ms step_avg:60.51ms
step:75/2330 train_time:4537ms step_avg:60.50ms
step:76/2330 train_time:4598ms step_avg:60.50ms
step:77/2330 train_time:4657ms step_avg:60.48ms
step:78/2330 train_time:4718ms step_avg:60.49ms
step:79/2330 train_time:4778ms step_avg:60.48ms
step:80/2330 train_time:4839ms step_avg:60.48ms
step:81/2330 train_time:4898ms step_avg:60.47ms
step:82/2330 train_time:4959ms step_avg:60.48ms
step:83/2330 train_time:5018ms step_avg:60.46ms
step:84/2330 train_time:5079ms step_avg:60.47ms
step:85/2330 train_time:5139ms step_avg:60.46ms
step:86/2330 train_time:5200ms step_avg:60.46ms
step:87/2330 train_time:5259ms step_avg:60.45ms
step:88/2330 train_time:5321ms step_avg:60.46ms
step:89/2330 train_time:5381ms step_avg:60.46ms
step:90/2330 train_time:5442ms step_avg:60.47ms
step:91/2330 train_time:5502ms step_avg:60.46ms
step:92/2330 train_time:5563ms step_avg:60.47ms
step:93/2330 train_time:5623ms step_avg:60.46ms
step:94/2330 train_time:5684ms step_avg:60.47ms
step:95/2330 train_time:5743ms step_avg:60.46ms
step:96/2330 train_time:5805ms step_avg:60.47ms
step:97/2330 train_time:5864ms step_avg:60.45ms
step:98/2330 train_time:5925ms step_avg:60.46ms
step:99/2330 train_time:5984ms step_avg:60.45ms
step:100/2330 train_time:6046ms step_avg:60.46ms
step:101/2330 train_time:6105ms step_avg:60.45ms
step:102/2330 train_time:6167ms step_avg:60.46ms
step:103/2330 train_time:6227ms step_avg:60.45ms
step:104/2330 train_time:6288ms step_avg:60.46ms
step:105/2330 train_time:6348ms step_avg:60.45ms
step:106/2330 train_time:6409ms step_avg:60.46ms
step:107/2330 train_time:6468ms step_avg:60.45ms
step:108/2330 train_time:6530ms step_avg:60.47ms
step:109/2330 train_time:6589ms step_avg:60.45ms
step:110/2330 train_time:6651ms step_avg:60.46ms
step:111/2330 train_time:6710ms step_avg:60.45ms
step:112/2330 train_time:6772ms step_avg:60.47ms
step:113/2330 train_time:6831ms step_avg:60.45ms
step:114/2330 train_time:6893ms step_avg:60.46ms
step:115/2330 train_time:6952ms step_avg:60.45ms
step:116/2330 train_time:7013ms step_avg:60.46ms
step:117/2330 train_time:7072ms step_avg:60.44ms
step:118/2330 train_time:7133ms step_avg:60.45ms
step:119/2330 train_time:7192ms step_avg:60.44ms
step:120/2330 train_time:7253ms step_avg:60.44ms
step:121/2330 train_time:7312ms step_avg:60.43ms
step:122/2330 train_time:7373ms step_avg:60.44ms
step:123/2330 train_time:7432ms step_avg:60.43ms
step:124/2330 train_time:7494ms step_avg:60.44ms
step:125/2330 train_time:7553ms step_avg:60.43ms
step:126/2330 train_time:7614ms step_avg:60.43ms
step:127/2330 train_time:7673ms step_avg:60.42ms
step:128/2330 train_time:7735ms step_avg:60.43ms
step:129/2330 train_time:7793ms step_avg:60.41ms
step:130/2330 train_time:7854ms step_avg:60.42ms
step:131/2330 train_time:7913ms step_avg:60.40ms
step:132/2330 train_time:7975ms step_avg:60.41ms
step:133/2330 train_time:8033ms step_avg:60.40ms
step:134/2330 train_time:8094ms step_avg:60.40ms
step:135/2330 train_time:8153ms step_avg:60.39ms
step:136/2330 train_time:8214ms step_avg:60.40ms
step:137/2330 train_time:8273ms step_avg:60.38ms
step:138/2330 train_time:8333ms step_avg:60.39ms
step:139/2330 train_time:8392ms step_avg:60.37ms
step:140/2330 train_time:8454ms step_avg:60.38ms
step:141/2330 train_time:8512ms step_avg:60.37ms
step:142/2330 train_time:8574ms step_avg:60.38ms
step:143/2330 train_time:8633ms step_avg:60.37ms
step:144/2330 train_time:8693ms step_avg:60.37ms
step:145/2330 train_time:8752ms step_avg:60.36ms
step:146/2330 train_time:8813ms step_avg:60.37ms
step:147/2330 train_time:8872ms step_avg:60.36ms
step:148/2330 train_time:8933ms step_avg:60.36ms
step:149/2330 train_time:8992ms step_avg:60.35ms
step:150/2330 train_time:9053ms step_avg:60.36ms
step:151/2330 train_time:9112ms step_avg:60.34ms
step:152/2330 train_time:9173ms step_avg:60.35ms
step:153/2330 train_time:9232ms step_avg:60.34ms
step:154/2330 train_time:9293ms step_avg:60.34ms
step:155/2330 train_time:9352ms step_avg:60.34ms
step:156/2330 train_time:9413ms step_avg:60.34ms
step:157/2330 train_time:9472ms step_avg:60.33ms
step:158/2330 train_time:9533ms step_avg:60.34ms
step:159/2330 train_time:9592ms step_avg:60.33ms
step:160/2330 train_time:9653ms step_avg:60.33ms
step:161/2330 train_time:9712ms step_avg:60.32ms
step:162/2330 train_time:9773ms step_avg:60.33ms
step:163/2330 train_time:9832ms step_avg:60.32ms
step:164/2330 train_time:9893ms step_avg:60.32ms
step:165/2330 train_time:9952ms step_avg:60.32ms
step:166/2330 train_time:10013ms step_avg:60.32ms
step:167/2330 train_time:10072ms step_avg:60.31ms
step:168/2330 train_time:10134ms step_avg:60.32ms
step:169/2330 train_time:10193ms step_avg:60.32ms
step:170/2330 train_time:10254ms step_avg:60.32ms
step:171/2330 train_time:10313ms step_avg:60.31ms
step:172/2330 train_time:10374ms step_avg:60.32ms
step:173/2330 train_time:10433ms step_avg:60.31ms
step:174/2330 train_time:10494ms step_avg:60.31ms
step:175/2330 train_time:10553ms step_avg:60.30ms
step:176/2330 train_time:10614ms step_avg:60.30ms
step:177/2330 train_time:10673ms step_avg:60.30ms
step:178/2330 train_time:10734ms step_avg:60.30ms
step:179/2330 train_time:10792ms step_avg:60.29ms
step:180/2330 train_time:10854ms step_avg:60.30ms
step:181/2330 train_time:10913ms step_avg:60.29ms
step:182/2330 train_time:10974ms step_avg:60.30ms
step:183/2330 train_time:11032ms step_avg:60.29ms
step:184/2330 train_time:11094ms step_avg:60.29ms
step:185/2330 train_time:11153ms step_avg:60.29ms
step:186/2330 train_time:11214ms step_avg:60.29ms
step:187/2330 train_time:11273ms step_avg:60.28ms
step:188/2330 train_time:11334ms step_avg:60.29ms
step:189/2330 train_time:11392ms step_avg:60.28ms
step:190/2330 train_time:11453ms step_avg:60.28ms
step:191/2330 train_time:11512ms step_avg:60.27ms
step:192/2330 train_time:11573ms step_avg:60.27ms
step:193/2330 train_time:11631ms step_avg:60.27ms
step:194/2330 train_time:11692ms step_avg:60.27ms
step:195/2330 train_time:11752ms step_avg:60.27ms
step:196/2330 train_time:11813ms step_avg:60.27ms
step:197/2330 train_time:11872ms step_avg:60.26ms
step:198/2330 train_time:11933ms step_avg:60.27ms
step:199/2330 train_time:11992ms step_avg:60.26ms
step:200/2330 train_time:12053ms step_avg:60.27ms
step:201/2330 train_time:12112ms step_avg:60.26ms
step:202/2330 train_time:12174ms step_avg:60.27ms
step:203/2330 train_time:12233ms step_avg:60.26ms
step:204/2330 train_time:12294ms step_avg:60.26ms
step:205/2330 train_time:12352ms step_avg:60.25ms
step:206/2330 train_time:12413ms step_avg:60.26ms
step:207/2330 train_time:12472ms step_avg:60.25ms
step:208/2330 train_time:12533ms step_avg:60.26ms
step:209/2330 train_time:12592ms step_avg:60.25ms
step:210/2330 train_time:12653ms step_avg:60.25ms
step:211/2330 train_time:12712ms step_avg:60.25ms
step:212/2330 train_time:12774ms step_avg:60.25ms
step:213/2330 train_time:12832ms step_avg:60.25ms
step:214/2330 train_time:12893ms step_avg:60.25ms
step:215/2330 train_time:12952ms step_avg:60.24ms
step:216/2330 train_time:13013ms step_avg:60.25ms
step:217/2330 train_time:13074ms step_avg:60.25ms
step:218/2330 train_time:13135ms step_avg:60.25ms
step:219/2330 train_time:13195ms step_avg:60.25ms
step:220/2330 train_time:13255ms step_avg:60.25ms
step:221/2330 train_time:13314ms step_avg:60.24ms
step:222/2330 train_time:13375ms step_avg:60.25ms
step:223/2330 train_time:13434ms step_avg:60.24ms
step:224/2330 train_time:13495ms step_avg:60.24ms
step:225/2330 train_time:13553ms step_avg:60.24ms
step:226/2330 train_time:13614ms step_avg:60.24ms
step:227/2330 train_time:13673ms step_avg:60.23ms
step:228/2330 train_time:13734ms step_avg:60.24ms
step:229/2330 train_time:13793ms step_avg:60.23ms
step:230/2330 train_time:13854ms step_avg:60.23ms
step:231/2330 train_time:13912ms step_avg:60.23ms
step:232/2330 train_time:13973ms step_avg:60.23ms
step:233/2330 train_time:14032ms step_avg:60.22ms
step:234/2330 train_time:14094ms step_avg:60.23ms
step:235/2330 train_time:14153ms step_avg:60.22ms
step:236/2330 train_time:14214ms step_avg:60.23ms
step:237/2330 train_time:14273ms step_avg:60.22ms
step:238/2330 train_time:14334ms step_avg:60.23ms
step:239/2330 train_time:14393ms step_avg:60.22ms
step:240/2330 train_time:14454ms step_avg:60.22ms
step:241/2330 train_time:14513ms step_avg:60.22ms
step:242/2330 train_time:14575ms step_avg:60.23ms
step:243/2330 train_time:14633ms step_avg:60.22ms
step:244/2330 train_time:14694ms step_avg:60.22ms
step:245/2330 train_time:14752ms step_avg:60.21ms
step:246/2330 train_time:14813ms step_avg:60.22ms
step:247/2330 train_time:14872ms step_avg:60.21ms
step:248/2330 train_time:14933ms step_avg:60.21ms
step:249/2330 train_time:14992ms step_avg:60.21ms
step:250/2330 train_time:15052ms step_avg:60.21ms
step:250/2330 val_loss:4.0819 train_time:15116ms step_avg:60.46ms
step:251/2330 train_time:15139ms step_avg:60.32ms
step:252/2330 train_time:15176ms step_avg:60.22ms
step:253/2330 train_time:15240ms step_avg:60.24ms
step:254/2330 train_time:15303ms step_avg:60.25ms
step:255/2330 train_time:15362ms step_avg:60.24ms
step:256/2330 train_time:15424ms step_avg:60.25ms
step:257/2330 train_time:15483ms step_avg:60.24ms
step:258/2330 train_time:15543ms step_avg:60.24ms
step:259/2330 train_time:15601ms step_avg:60.24ms
step:260/2330 train_time:15661ms step_avg:60.24ms
step:261/2330 train_time:15720ms step_avg:60.23ms
step:262/2330 train_time:15780ms step_avg:60.23ms
step:263/2330 train_time:15838ms step_avg:60.22ms
step:264/2330 train_time:15898ms step_avg:60.22ms
step:265/2330 train_time:15956ms step_avg:60.21ms
step:266/2330 train_time:16017ms step_avg:60.22ms
step:267/2330 train_time:16078ms step_avg:60.22ms
step:268/2330 train_time:16139ms step_avg:60.22ms
step:269/2330 train_time:16200ms step_avg:60.22ms
step:270/2330 train_time:16262ms step_avg:60.23ms
step:271/2330 train_time:16322ms step_avg:60.23ms
step:272/2330 train_time:16384ms step_avg:60.23ms
step:273/2330 train_time:16442ms step_avg:60.23ms
step:274/2330 train_time:16503ms step_avg:60.23ms
step:275/2330 train_time:16562ms step_avg:60.23ms
step:276/2330 train_time:16624ms step_avg:60.23ms
step:277/2330 train_time:16682ms step_avg:60.22ms
step:278/2330 train_time:16743ms step_avg:60.23ms
step:279/2330 train_time:16802ms step_avg:60.22ms
step:280/2330 train_time:16862ms step_avg:60.22ms
step:281/2330 train_time:16921ms step_avg:60.22ms
step:282/2330 train_time:16982ms step_avg:60.22ms
step:283/2330 train_time:17041ms step_avg:60.22ms
step:284/2330 train_time:17103ms step_avg:60.22ms
step:285/2330 train_time:17163ms step_avg:60.22ms
step:286/2330 train_time:17225ms step_avg:60.23ms
step:287/2330 train_time:17286ms step_avg:60.23ms
step:288/2330 train_time:17347ms step_avg:60.23ms
step:289/2330 train_time:17406ms step_avg:60.23ms
step:290/2330 train_time:17467ms step_avg:60.23ms
step:291/2330 train_time:17527ms step_avg:60.23ms
step:292/2330 train_time:17588ms step_avg:60.23ms
step:293/2330 train_time:17647ms step_avg:60.23ms
step:294/2330 train_time:17709ms step_avg:60.23ms
step:295/2330 train_time:17767ms step_avg:60.23ms
step:296/2330 train_time:17828ms step_avg:60.23ms
step:297/2330 train_time:17887ms step_avg:60.23ms
step:298/2330 train_time:17948ms step_avg:60.23ms
step:299/2330 train_time:18008ms step_avg:60.23ms
step:300/2330 train_time:18069ms step_avg:60.23ms
step:301/2330 train_time:18129ms step_avg:60.23ms
step:302/2330 train_time:18190ms step_avg:60.23ms
step:303/2330 train_time:18250ms step_avg:60.23ms
step:304/2330 train_time:18311ms step_avg:60.23ms
step:305/2330 train_time:18370ms step_avg:60.23ms
step:306/2330 train_time:18431ms step_avg:60.23ms
step:307/2330 train_time:18491ms step_avg:60.23ms
step:308/2330 train_time:18552ms step_avg:60.23ms
step:309/2330 train_time:18611ms step_avg:60.23ms
step:310/2330 train_time:18672ms step_avg:60.23ms
step:311/2330 train_time:18731ms step_avg:60.23ms
step:312/2330 train_time:18792ms step_avg:60.23ms
step:313/2330 train_time:18851ms step_avg:60.23ms
step:314/2330 train_time:18913ms step_avg:60.23ms
step:315/2330 train_time:18972ms step_avg:60.23ms
step:316/2330 train_time:19033ms step_avg:60.23ms
step:317/2330 train_time:19092ms step_avg:60.23ms
step:318/2330 train_time:19154ms step_avg:60.23ms
step:319/2330 train_time:19214ms step_avg:60.23ms
step:320/2330 train_time:19274ms step_avg:60.23ms
step:321/2330 train_time:19333ms step_avg:60.23ms
step:322/2330 train_time:19393ms step_avg:60.23ms
step:323/2330 train_time:19452ms step_avg:60.22ms
step:324/2330 train_time:19514ms step_avg:60.23ms
step:325/2330 train_time:19572ms step_avg:60.22ms
step:326/2330 train_time:19634ms step_avg:60.23ms
step:327/2330 train_time:19693ms step_avg:60.22ms
step:328/2330 train_time:19754ms step_avg:60.23ms
step:329/2330 train_time:19813ms step_avg:60.22ms
step:330/2330 train_time:19874ms step_avg:60.23ms
step:331/2330 train_time:19934ms step_avg:60.22ms
step:332/2330 train_time:19996ms step_avg:60.23ms
step:333/2330 train_time:20054ms step_avg:60.22ms
step:334/2330 train_time:20115ms step_avg:60.22ms
step:335/2330 train_time:20175ms step_avg:60.22ms
step:336/2330 train_time:20236ms step_avg:60.23ms
step:337/2330 train_time:20295ms step_avg:60.22ms
step:338/2330 train_time:20356ms step_avg:60.23ms
step:339/2330 train_time:20415ms step_avg:60.22ms
step:340/2330 train_time:20476ms step_avg:60.22ms
step:341/2330 train_time:20535ms step_avg:60.22ms
step:342/2330 train_time:20596ms step_avg:60.22ms
step:343/2330 train_time:20655ms step_avg:60.22ms
step:344/2330 train_time:20715ms step_avg:60.22ms
step:345/2330 train_time:20774ms step_avg:60.21ms
step:346/2330 train_time:20836ms step_avg:60.22ms
step:347/2330 train_time:20895ms step_avg:60.22ms
step:348/2330 train_time:20956ms step_avg:60.22ms
step:349/2330 train_time:21015ms step_avg:60.21ms
step:350/2330 train_time:21076ms step_avg:60.22ms
step:351/2330 train_time:21135ms step_avg:60.21ms
step:352/2330 train_time:21195ms step_avg:60.21ms
step:353/2330 train_time:21254ms step_avg:60.21ms
step:354/2330 train_time:21315ms step_avg:60.21ms
step:355/2330 train_time:21374ms step_avg:60.21ms
step:356/2330 train_time:21436ms step_avg:60.21ms
step:357/2330 train_time:21495ms step_avg:60.21ms
step:358/2330 train_time:21556ms step_avg:60.21ms
step:359/2330 train_time:21614ms step_avg:60.21ms
step:360/2330 train_time:21675ms step_avg:60.21ms
step:361/2330 train_time:21734ms step_avg:60.20ms
step:362/2330 train_time:21795ms step_avg:60.21ms
step:363/2330 train_time:21854ms step_avg:60.20ms
step:364/2330 train_time:21915ms step_avg:60.21ms
step:365/2330 train_time:21974ms step_avg:60.20ms
step:366/2330 train_time:22035ms step_avg:60.21ms
step:367/2330 train_time:22095ms step_avg:60.20ms
step:368/2330 train_time:22156ms step_avg:60.21ms
step:369/2330 train_time:22215ms step_avg:60.20ms
step:370/2330 train_time:22276ms step_avg:60.21ms
step:371/2330 train_time:22335ms step_avg:60.20ms
step:372/2330 train_time:22395ms step_avg:60.20ms
step:373/2330 train_time:22454ms step_avg:60.20ms
step:374/2330 train_time:22515ms step_avg:60.20ms
step:375/2330 train_time:22573ms step_avg:60.20ms
step:376/2330 train_time:22634ms step_avg:60.20ms
step:377/2330 train_time:22692ms step_avg:60.19ms
step:378/2330 train_time:22754ms step_avg:60.20ms
step:379/2330 train_time:22813ms step_avg:60.19ms
step:380/2330 train_time:22874ms step_avg:60.20ms
step:381/2330 train_time:22934ms step_avg:60.19ms
step:382/2330 train_time:22995ms step_avg:60.20ms
step:383/2330 train_time:23054ms step_avg:60.19ms
step:384/2330 train_time:23115ms step_avg:60.19ms
step:385/2330 train_time:23174ms step_avg:60.19ms
step:386/2330 train_time:23236ms step_avg:60.20ms
step:387/2330 train_time:23295ms step_avg:60.19ms
step:388/2330 train_time:23356ms step_avg:60.20ms
step:389/2330 train_time:23415ms step_avg:60.19ms
step:390/2330 train_time:23476ms step_avg:60.20ms
step:391/2330 train_time:23535ms step_avg:60.19ms
step:392/2330 train_time:23597ms step_avg:60.20ms
step:393/2330 train_time:23656ms step_avg:60.19ms
step:394/2330 train_time:23717ms step_avg:60.19ms
step:395/2330 train_time:23776ms step_avg:60.19ms
step:396/2330 train_time:23837ms step_avg:60.19ms
step:397/2330 train_time:23896ms step_avg:60.19ms
step:398/2330 train_time:23957ms step_avg:60.19ms
step:399/2330 train_time:24015ms step_avg:60.19ms
step:400/2330 train_time:24076ms step_avg:60.19ms
step:401/2330 train_time:24136ms step_avg:60.19ms
step:402/2330 train_time:24197ms step_avg:60.19ms
step:403/2330 train_time:24256ms step_avg:60.19ms
step:404/2330 train_time:24317ms step_avg:60.19ms
step:405/2330 train_time:24375ms step_avg:60.19ms
step:406/2330 train_time:24437ms step_avg:60.19ms
step:407/2330 train_time:24495ms step_avg:60.19ms
step:408/2330 train_time:24557ms step_avg:60.19ms
step:409/2330 train_time:24616ms step_avg:60.18ms
step:410/2330 train_time:24676ms step_avg:60.19ms
step:411/2330 train_time:24735ms step_avg:60.18ms
step:412/2330 train_time:24796ms step_avg:60.18ms
step:413/2330 train_time:24855ms step_avg:60.18ms
step:414/2330 train_time:24916ms step_avg:60.18ms
step:415/2330 train_time:24975ms step_avg:60.18ms
step:416/2330 train_time:25037ms step_avg:60.18ms
step:417/2330 train_time:25096ms step_avg:60.18ms
step:418/2330 train_time:25156ms step_avg:60.18ms
step:419/2330 train_time:25215ms step_avg:60.18ms
step:420/2330 train_time:25277ms step_avg:60.18ms
step:421/2330 train_time:25336ms step_avg:60.18ms
step:422/2330 train_time:25397ms step_avg:60.18ms
step:423/2330 train_time:25456ms step_avg:60.18ms
step:424/2330 train_time:25516ms step_avg:60.18ms
step:425/2330 train_time:25575ms step_avg:60.18ms
step:426/2330 train_time:25636ms step_avg:60.18ms
step:427/2330 train_time:25695ms step_avg:60.18ms
step:428/2330 train_time:25756ms step_avg:60.18ms
step:429/2330 train_time:25815ms step_avg:60.18ms
step:430/2330 train_time:25876ms step_avg:60.18ms
step:431/2330 train_time:25936ms step_avg:60.18ms
step:432/2330 train_time:25996ms step_avg:60.18ms
step:433/2330 train_time:26055ms step_avg:60.17ms
step:434/2330 train_time:26117ms step_avg:60.18ms
step:435/2330 train_time:26176ms step_avg:60.17ms
step:436/2330 train_time:26237ms step_avg:60.18ms
step:437/2330 train_time:26296ms step_avg:60.17ms
step:438/2330 train_time:26357ms step_avg:60.18ms
step:439/2330 train_time:26416ms step_avg:60.17ms
step:440/2330 train_time:26476ms step_avg:60.17ms
step:441/2330 train_time:26535ms step_avg:60.17ms
step:442/2330 train_time:26597ms step_avg:60.17ms
step:443/2330 train_time:26656ms step_avg:60.17ms
step:444/2330 train_time:26716ms step_avg:60.17ms
step:445/2330 train_time:26775ms step_avg:60.17ms
step:446/2330 train_time:26836ms step_avg:60.17ms
step:447/2330 train_time:26896ms step_avg:60.17ms
step:448/2330 train_time:26956ms step_avg:60.17ms
step:449/2330 train_time:27015ms step_avg:60.17ms
step:450/2330 train_time:27076ms step_avg:60.17ms
step:451/2330 train_time:27135ms step_avg:60.17ms
step:452/2330 train_time:27196ms step_avg:60.17ms
step:453/2330 train_time:27255ms step_avg:60.17ms
step:454/2330 train_time:27315ms step_avg:60.17ms
step:455/2330 train_time:27374ms step_avg:60.16ms
step:456/2330 train_time:27435ms step_avg:60.17ms
step:457/2330 train_time:27494ms step_avg:60.16ms
step:458/2330 train_time:27555ms step_avg:60.16ms
step:459/2330 train_time:27614ms step_avg:60.16ms
step:460/2330 train_time:27675ms step_avg:60.16ms
step:461/2330 train_time:27734ms step_avg:60.16ms
step:462/2330 train_time:27795ms step_avg:60.16ms
step:463/2330 train_time:27855ms step_avg:60.16ms
step:464/2330 train_time:27916ms step_avg:60.16ms
step:465/2330 train_time:27975ms step_avg:60.16ms
step:466/2330 train_time:28036ms step_avg:60.16ms
step:467/2330 train_time:28096ms step_avg:60.16ms
step:468/2330 train_time:28156ms step_avg:60.16ms
step:469/2330 train_time:28215ms step_avg:60.16ms
step:470/2330 train_time:28276ms step_avg:60.16ms
step:471/2330 train_time:28335ms step_avg:60.16ms
step:472/2330 train_time:28396ms step_avg:60.16ms
step:473/2330 train_time:28455ms step_avg:60.16ms
step:474/2330 train_time:28517ms step_avg:60.16ms
step:475/2330 train_time:28576ms step_avg:60.16ms
step:476/2330 train_time:28637ms step_avg:60.16ms
step:477/2330 train_time:28696ms step_avg:60.16ms
step:478/2330 train_time:28756ms step_avg:60.16ms
step:479/2330 train_time:28815ms step_avg:60.16ms
step:480/2330 train_time:28876ms step_avg:60.16ms
step:481/2330 train_time:28935ms step_avg:60.16ms
step:482/2330 train_time:28996ms step_avg:60.16ms
step:483/2330 train_time:29055ms step_avg:60.16ms
step:484/2330 train_time:29116ms step_avg:60.16ms
step:485/2330 train_time:29175ms step_avg:60.15ms
step:486/2330 train_time:29236ms step_avg:60.16ms
step:487/2330 train_time:29295ms step_avg:60.15ms
step:488/2330 train_time:29357ms step_avg:60.16ms
step:489/2330 train_time:29415ms step_avg:60.15ms
step:490/2330 train_time:29476ms step_avg:60.15ms
step:491/2330 train_time:29535ms step_avg:60.15ms
step:492/2330 train_time:29597ms step_avg:60.16ms
step:493/2330 train_time:29656ms step_avg:60.15ms
step:494/2330 train_time:29717ms step_avg:60.16ms
step:495/2330 train_time:29775ms step_avg:60.15ms
step:496/2330 train_time:29836ms step_avg:60.15ms
step:497/2330 train_time:29895ms step_avg:60.15ms
step:498/2330 train_time:29956ms step_avg:60.15ms
step:499/2330 train_time:30014ms step_avg:60.15ms
step:500/2330 train_time:30075ms step_avg:60.15ms
step:500/2330 val_loss:3.8193 train_time:30138ms step_avg:60.28ms
step:501/2330 train_time:30163ms step_avg:60.21ms
step:502/2330 train_time:30198ms step_avg:60.16ms
step:503/2330 train_time:30261ms step_avg:60.16ms
step:504/2330 train_time:30325ms step_avg:60.17ms
step:505/2330 train_time:30384ms step_avg:60.17ms
step:506/2330 train_time:30445ms step_avg:60.17ms
step:507/2330 train_time:30504ms step_avg:60.16ms
step:508/2330 train_time:30564ms step_avg:60.17ms
step:509/2330 train_time:30623ms step_avg:60.16ms
step:510/2330 train_time:30683ms step_avg:60.16ms
step:511/2330 train_time:30741ms step_avg:60.16ms
step:512/2330 train_time:30802ms step_avg:60.16ms
step:513/2330 train_time:30861ms step_avg:60.16ms
step:514/2330 train_time:30922ms step_avg:60.16ms
step:515/2330 train_time:30981ms step_avg:60.16ms
step:516/2330 train_time:31042ms step_avg:60.16ms
step:517/2330 train_time:31102ms step_avg:60.16ms
step:518/2330 train_time:31165ms step_avg:60.16ms
step:519/2330 train_time:31225ms step_avg:60.16ms
step:520/2330 train_time:31287ms step_avg:60.17ms
step:521/2330 train_time:31347ms step_avg:60.17ms
step:522/2330 train_time:31408ms step_avg:60.17ms
step:523/2330 train_time:31467ms step_avg:60.17ms
step:524/2330 train_time:31528ms step_avg:60.17ms
step:525/2330 train_time:31587ms step_avg:60.17ms
step:526/2330 train_time:31648ms step_avg:60.17ms
step:527/2330 train_time:31707ms step_avg:60.16ms
step:528/2330 train_time:31767ms step_avg:60.17ms
step:529/2330 train_time:31826ms step_avg:60.16ms
step:530/2330 train_time:31887ms step_avg:60.16ms
step:531/2330 train_time:31946ms step_avg:60.16ms
step:532/2330 train_time:32008ms step_avg:60.16ms
step:533/2330 train_time:32067ms step_avg:60.16ms
step:534/2330 train_time:32129ms step_avg:60.17ms
step:535/2330 train_time:32189ms step_avg:60.17ms
step:536/2330 train_time:32250ms step_avg:60.17ms
step:537/2330 train_time:32310ms step_avg:60.17ms
step:538/2330 train_time:32371ms step_avg:60.17ms
step:539/2330 train_time:32431ms step_avg:60.17ms
step:540/2330 train_time:32492ms step_avg:60.17ms
step:541/2330 train_time:32551ms step_avg:60.17ms
step:542/2330 train_time:32612ms step_avg:60.17ms
step:543/2330 train_time:32671ms step_avg:60.17ms
step:544/2330 train_time:32733ms step_avg:60.17ms
step:545/2330 train_time:32791ms step_avg:60.17ms
step:546/2330 train_time:32853ms step_avg:60.17ms
step:547/2330 train_time:32912ms step_avg:60.17ms
step:548/2330 train_time:32974ms step_avg:60.17ms
step:549/2330 train_time:33033ms step_avg:60.17ms
step:550/2330 train_time:33094ms step_avg:60.17ms
step:551/2330 train_time:33154ms step_avg:60.17ms
step:552/2330 train_time:33215ms step_avg:60.17ms
step:553/2330 train_time:33275ms step_avg:60.17ms
step:554/2330 train_time:33336ms step_avg:60.17ms
step:555/2330 train_time:33395ms step_avg:60.17ms
step:556/2330 train_time:33456ms step_avg:60.17ms
step:557/2330 train_time:33515ms step_avg:60.17ms
step:558/2330 train_time:33576ms step_avg:60.17ms
step:559/2330 train_time:33634ms step_avg:60.17ms
step:560/2330 train_time:33695ms step_avg:60.17ms
step:561/2330 train_time:33754ms step_avg:60.17ms
step:562/2330 train_time:33815ms step_avg:60.17ms
step:563/2330 train_time:33875ms step_avg:60.17ms
step:564/2330 train_time:33936ms step_avg:60.17ms
step:565/2330 train_time:33995ms step_avg:60.17ms
step:566/2330 train_time:34057ms step_avg:60.17ms
step:567/2330 train_time:34115ms step_avg:60.17ms
step:568/2330 train_time:34176ms step_avg:60.17ms
step:569/2330 train_time:34236ms step_avg:60.17ms
step:570/2330 train_time:34297ms step_avg:60.17ms
step:571/2330 train_time:34356ms step_avg:60.17ms
step:572/2330 train_time:34417ms step_avg:60.17ms
step:573/2330 train_time:34476ms step_avg:60.17ms
step:574/2330 train_time:34537ms step_avg:60.17ms
step:575/2330 train_time:34596ms step_avg:60.17ms
step:576/2330 train_time:34657ms step_avg:60.17ms
step:577/2330 train_time:34715ms step_avg:60.17ms
step:578/2330 train_time:34777ms step_avg:60.17ms
step:579/2330 train_time:34836ms step_avg:60.17ms
step:580/2330 train_time:34897ms step_avg:60.17ms
step:581/2330 train_time:34956ms step_avg:60.17ms
step:582/2330 train_time:35017ms step_avg:60.17ms
step:583/2330 train_time:35076ms step_avg:60.17ms
step:584/2330 train_time:35137ms step_avg:60.17ms
step:585/2330 train_time:35196ms step_avg:60.16ms
step:586/2330 train_time:35257ms step_avg:60.17ms
step:587/2330 train_time:35316ms step_avg:60.16ms
step:588/2330 train_time:35377ms step_avg:60.16ms
step:589/2330 train_time:35436ms step_avg:60.16ms
step:590/2330 train_time:35496ms step_avg:60.16ms
step:591/2330 train_time:35555ms step_avg:60.16ms
step:592/2330 train_time:35616ms step_avg:60.16ms
step:593/2330 train_time:35675ms step_avg:60.16ms
step:594/2330 train_time:35737ms step_avg:60.16ms
step:595/2330 train_time:35796ms step_avg:60.16ms
step:596/2330 train_time:35856ms step_avg:60.16ms
step:597/2330 train_time:35915ms step_avg:60.16ms
step:598/2330 train_time:35976ms step_avg:60.16ms
step:599/2330 train_time:36035ms step_avg:60.16ms
step:600/2330 train_time:36097ms step_avg:60.16ms
step:601/2330 train_time:36156ms step_avg:60.16ms
step:602/2330 train_time:36217ms step_avg:60.16ms
step:603/2330 train_time:36277ms step_avg:60.16ms
step:604/2330 train_time:36337ms step_avg:60.16ms
step:605/2330 train_time:36396ms step_avg:60.16ms
step:606/2330 train_time:36457ms step_avg:60.16ms
step:607/2330 train_time:36516ms step_avg:60.16ms
step:608/2330 train_time:36577ms step_avg:60.16ms
step:609/2330 train_time:36636ms step_avg:60.16ms
step:610/2330 train_time:36697ms step_avg:60.16ms
step:611/2330 train_time:36756ms step_avg:60.16ms
step:612/2330 train_time:36817ms step_avg:60.16ms
step:613/2330 train_time:36876ms step_avg:60.16ms
step:614/2330 train_time:36937ms step_avg:60.16ms
step:615/2330 train_time:36995ms step_avg:60.16ms
step:616/2330 train_time:37057ms step_avg:60.16ms
step:617/2330 train_time:37116ms step_avg:60.16ms
step:618/2330 train_time:37177ms step_avg:60.16ms
step:619/2330 train_time:37236ms step_avg:60.16ms
step:620/2330 train_time:37297ms step_avg:60.16ms
step:621/2330 train_time:37356ms step_avg:60.15ms
step:622/2330 train_time:37416ms step_avg:60.16ms
step:623/2330 train_time:37475ms step_avg:60.15ms
step:624/2330 train_time:37537ms step_avg:60.15ms
step:625/2330 train_time:37596ms step_avg:60.15ms
step:626/2330 train_time:37657ms step_avg:60.16ms
step:627/2330 train_time:37716ms step_avg:60.15ms
step:628/2330 train_time:37778ms step_avg:60.16ms
step:629/2330 train_time:37836ms step_avg:60.15ms
step:630/2330 train_time:37897ms step_avg:60.15ms
step:631/2330 train_time:37956ms step_avg:60.15ms
step:632/2330 train_time:38017ms step_avg:60.15ms
step:633/2330 train_time:38077ms step_avg:60.15ms
step:634/2330 train_time:38138ms step_avg:60.15ms
step:635/2330 train_time:38197ms step_avg:60.15ms
step:636/2330 train_time:38258ms step_avg:60.15ms
step:637/2330 train_time:38317ms step_avg:60.15ms
step:638/2330 train_time:38378ms step_avg:60.15ms
step:639/2330 train_time:38437ms step_avg:60.15ms
step:640/2330 train_time:38499ms step_avg:60.15ms
step:641/2330 train_time:38557ms step_avg:60.15ms
step:642/2330 train_time:38618ms step_avg:60.15ms
step:643/2330 train_time:38677ms step_avg:60.15ms
step:644/2330 train_time:38738ms step_avg:60.15ms
step:645/2330 train_time:38797ms step_avg:60.15ms
step:646/2330 train_time:38858ms step_avg:60.15ms
step:647/2330 train_time:38917ms step_avg:60.15ms
step:648/2330 train_time:38978ms step_avg:60.15ms
step:649/2330 train_time:39037ms step_avg:60.15ms
step:650/2330 train_time:39098ms step_avg:60.15ms
step:651/2330 train_time:39157ms step_avg:60.15ms
step:652/2330 train_time:39218ms step_avg:60.15ms
step:653/2330 train_time:39278ms step_avg:60.15ms
step:654/2330 train_time:39339ms step_avg:60.15ms
step:655/2330 train_time:39398ms step_avg:60.15ms
step:656/2330 train_time:39459ms step_avg:60.15ms
step:657/2330 train_time:39519ms step_avg:60.15ms
step:658/2330 train_time:39580ms step_avg:60.15ms
step:659/2330 train_time:39640ms step_avg:60.15ms
step:660/2330 train_time:39701ms step_avg:60.15ms
step:661/2330 train_time:39759ms step_avg:60.15ms
step:662/2330 train_time:39820ms step_avg:60.15ms
step:663/2330 train_time:39879ms step_avg:60.15ms
step:664/2330 train_time:39940ms step_avg:60.15ms
step:665/2330 train_time:40000ms step_avg:60.15ms
step:666/2330 train_time:40061ms step_avg:60.15ms
step:667/2330 train_time:40120ms step_avg:60.15ms
step:668/2330 train_time:40182ms step_avg:60.15ms
step:669/2330 train_time:40240ms step_avg:60.15ms
step:670/2330 train_time:40301ms step_avg:60.15ms
step:671/2330 train_time:40360ms step_avg:60.15ms
step:672/2330 train_time:40421ms step_avg:60.15ms
step:673/2330 train_time:40480ms step_avg:60.15ms
step:674/2330 train_time:40542ms step_avg:60.15ms
step:675/2330 train_time:40602ms step_avg:60.15ms
step:676/2330 train_time:40663ms step_avg:60.15ms
step:677/2330 train_time:40723ms step_avg:60.15ms
step:678/2330 train_time:40784ms step_avg:60.15ms
step:679/2330 train_time:40843ms step_avg:60.15ms
step:680/2330 train_time:40904ms step_avg:60.15ms
step:681/2330 train_time:40964ms step_avg:60.15ms
step:682/2330 train_time:41025ms step_avg:60.15ms
step:683/2330 train_time:41084ms step_avg:60.15ms
step:684/2330 train_time:41146ms step_avg:60.15ms
step:685/2330 train_time:41205ms step_avg:60.15ms
step:686/2330 train_time:41266ms step_avg:60.15ms
step:687/2330 train_time:41326ms step_avg:60.15ms
step:688/2330 train_time:41387ms step_avg:60.16ms
step:689/2330 train_time:41447ms step_avg:60.15ms
step:690/2330 train_time:41508ms step_avg:60.16ms
step:691/2330 train_time:41568ms step_avg:60.16ms
step:692/2330 train_time:41629ms step_avg:60.16ms
step:693/2330 train_time:41688ms step_avg:60.16ms
step:694/2330 train_time:41750ms step_avg:60.16ms
step:695/2330 train_time:41809ms step_avg:60.16ms
step:696/2330 train_time:41870ms step_avg:60.16ms
step:697/2330 train_time:41929ms step_avg:60.16ms
step:698/2330 train_time:41991ms step_avg:60.16ms
step:699/2330 train_time:42049ms step_avg:60.16ms
step:700/2330 train_time:42111ms step_avg:60.16ms
step:701/2330 train_time:42170ms step_avg:60.16ms
step:702/2330 train_time:42232ms step_avg:60.16ms
step:703/2330 train_time:42291ms step_avg:60.16ms
step:704/2330 train_time:42352ms step_avg:60.16ms
step:705/2330 train_time:42412ms step_avg:60.16ms
step:706/2330 train_time:42473ms step_avg:60.16ms
step:707/2330 train_time:42533ms step_avg:60.16ms
step:708/2330 train_time:42593ms step_avg:60.16ms
step:709/2330 train_time:42653ms step_avg:60.16ms
step:710/2330 train_time:42713ms step_avg:60.16ms
step:711/2330 train_time:42773ms step_avg:60.16ms
step:712/2330 train_time:42834ms step_avg:60.16ms
step:713/2330 train_time:42893ms step_avg:60.16ms
step:714/2330 train_time:42954ms step_avg:60.16ms
step:715/2330 train_time:43013ms step_avg:60.16ms
step:716/2330 train_time:43075ms step_avg:60.16ms
step:717/2330 train_time:43134ms step_avg:60.16ms
step:718/2330 train_time:43195ms step_avg:60.16ms
step:719/2330 train_time:43254ms step_avg:60.16ms
step:720/2330 train_time:43315ms step_avg:60.16ms
step:721/2330 train_time:43374ms step_avg:60.16ms
step:722/2330 train_time:43435ms step_avg:60.16ms
step:723/2330 train_time:43494ms step_avg:60.16ms
step:724/2330 train_time:43555ms step_avg:60.16ms
step:725/2330 train_time:43614ms step_avg:60.16ms
step:726/2330 train_time:43675ms step_avg:60.16ms
step:727/2330 train_time:43734ms step_avg:60.16ms
step:728/2330 train_time:43796ms step_avg:60.16ms
step:729/2330 train_time:43855ms step_avg:60.16ms
step:730/2330 train_time:43916ms step_avg:60.16ms
step:731/2330 train_time:43975ms step_avg:60.16ms
step:732/2330 train_time:44036ms step_avg:60.16ms
step:733/2330 train_time:44095ms step_avg:60.16ms
step:734/2330 train_time:44156ms step_avg:60.16ms
step:735/2330 train_time:44215ms step_avg:60.16ms
step:736/2330 train_time:44277ms step_avg:60.16ms
step:737/2330 train_time:44336ms step_avg:60.16ms
step:738/2330 train_time:44397ms step_avg:60.16ms
step:739/2330 train_time:44456ms step_avg:60.16ms
step:740/2330 train_time:44517ms step_avg:60.16ms
step:741/2330 train_time:44575ms step_avg:60.16ms
step:742/2330 train_time:44637ms step_avg:60.16ms
step:743/2330 train_time:44696ms step_avg:60.16ms
step:744/2330 train_time:44756ms step_avg:60.16ms
step:745/2330 train_time:44815ms step_avg:60.15ms
step:746/2330 train_time:44877ms step_avg:60.16ms
step:747/2330 train_time:44936ms step_avg:60.16ms
step:748/2330 train_time:44997ms step_avg:60.16ms
step:749/2330 train_time:45055ms step_avg:60.15ms
step:750/2330 train_time:45116ms step_avg:60.16ms
step:750/2330 val_loss:3.6895 train_time:45180ms step_avg:60.24ms
step:751/2330 train_time:45205ms step_avg:60.19ms
step:752/2330 train_time:45241ms step_avg:60.16ms
step:753/2330 train_time:45303ms step_avg:60.16ms
step:754/2330 train_time:45367ms step_avg:60.17ms
step:755/2330 train_time:45426ms step_avg:60.17ms
step:756/2330 train_time:45488ms step_avg:60.17ms
step:757/2330 train_time:45547ms step_avg:60.17ms
step:758/2330 train_time:45607ms step_avg:60.17ms
step:759/2330 train_time:45666ms step_avg:60.17ms
step:760/2330 train_time:45727ms step_avg:60.17ms
step:761/2330 train_time:45785ms step_avg:60.16ms
step:762/2330 train_time:45846ms step_avg:60.17ms
step:763/2330 train_time:45904ms step_avg:60.16ms
step:764/2330 train_time:45965ms step_avg:60.16ms
step:765/2330 train_time:46024ms step_avg:60.16ms
step:766/2330 train_time:46085ms step_avg:60.16ms
step:767/2330 train_time:46145ms step_avg:60.16ms
step:768/2330 train_time:46208ms step_avg:60.17ms
step:769/2330 train_time:46271ms step_avg:60.17ms
step:770/2330 train_time:46334ms step_avg:60.17ms
step:771/2330 train_time:46395ms step_avg:60.17ms
step:772/2330 train_time:46457ms step_avg:60.18ms
step:773/2330 train_time:46517ms step_avg:60.18ms
step:774/2330 train_time:46579ms step_avg:60.18ms
step:775/2330 train_time:46638ms step_avg:60.18ms
step:776/2330 train_time:46700ms step_avg:60.18ms
step:777/2330 train_time:46760ms step_avg:60.18ms
step:778/2330 train_time:46822ms step_avg:60.18ms
step:779/2330 train_time:46882ms step_avg:60.18ms
step:780/2330 train_time:46943ms step_avg:60.18ms
step:781/2330 train_time:47002ms step_avg:60.18ms
step:782/2330 train_time:47064ms step_avg:60.18ms
step:783/2330 train_time:47125ms step_avg:60.19ms
step:784/2330 train_time:47187ms step_avg:60.19ms
step:785/2330 train_time:47248ms step_avg:60.19ms
step:786/2330 train_time:47311ms step_avg:60.19ms
step:787/2330 train_time:47371ms step_avg:60.19ms
step:788/2330 train_time:47433ms step_avg:60.19ms
step:789/2330 train_time:47493ms step_avg:60.19ms
step:790/2330 train_time:47555ms step_avg:60.20ms
step:791/2330 train_time:47615ms step_avg:60.20ms
step:792/2330 train_time:47677ms step_avg:60.20ms
step:793/2330 train_time:47736ms step_avg:60.20ms
step:794/2330 train_time:47798ms step_avg:60.20ms
step:795/2330 train_time:47857ms step_avg:60.20ms
step:796/2330 train_time:47918ms step_avg:60.20ms
step:797/2330 train_time:47978ms step_avg:60.20ms
step:798/2330 train_time:48041ms step_avg:60.20ms
step:799/2330 train_time:48101ms step_avg:60.20ms
step:800/2330 train_time:48164ms step_avg:60.20ms
step:801/2330 train_time:48224ms step_avg:60.20ms
step:802/2330 train_time:48286ms step_avg:60.21ms
step:803/2330 train_time:48347ms step_avg:60.21ms
step:804/2330 train_time:48409ms step_avg:60.21ms
step:805/2330 train_time:48470ms step_avg:60.21ms
step:806/2330 train_time:48532ms step_avg:60.21ms
step:807/2330 train_time:48591ms step_avg:60.21ms
step:808/2330 train_time:48653ms step_avg:60.21ms
step:809/2330 train_time:48714ms step_avg:60.21ms
step:810/2330 train_time:48775ms step_avg:60.22ms
step:811/2330 train_time:48835ms step_avg:60.22ms
step:812/2330 train_time:48897ms step_avg:60.22ms
step:813/2330 train_time:48956ms step_avg:60.22ms
step:814/2330 train_time:49018ms step_avg:60.22ms
step:815/2330 train_time:49077ms step_avg:60.22ms
step:816/2330 train_time:49139ms step_avg:60.22ms
step:817/2330 train_time:49200ms step_avg:60.22ms
step:818/2330 train_time:49263ms step_avg:60.22ms
step:819/2330 train_time:49323ms step_avg:60.22ms
step:820/2330 train_time:49386ms step_avg:60.23ms
step:821/2330 train_time:49447ms step_avg:60.23ms
step:822/2330 train_time:49509ms step_avg:60.23ms
step:823/2330 train_time:49569ms step_avg:60.23ms
step:824/2330 train_time:49631ms step_avg:60.23ms
step:825/2330 train_time:49691ms step_avg:60.23ms
step:826/2330 train_time:49753ms step_avg:60.23ms
step:827/2330 train_time:49813ms step_avg:60.23ms
step:828/2330 train_time:49875ms step_avg:60.24ms
step:829/2330 train_time:49935ms step_avg:60.24ms
step:830/2330 train_time:49997ms step_avg:60.24ms
step:831/2330 train_time:50057ms step_avg:60.24ms
step:832/2330 train_time:50119ms step_avg:60.24ms
step:833/2330 train_time:50178ms step_avg:60.24ms
step:834/2330 train_time:50240ms step_avg:60.24ms
step:835/2330 train_time:50300ms step_avg:60.24ms
step:836/2330 train_time:50363ms step_avg:60.24ms
step:837/2330 train_time:50423ms step_avg:60.24ms
step:838/2330 train_time:50485ms step_avg:60.24ms
step:839/2330 train_time:50545ms step_avg:60.24ms
step:840/2330 train_time:50608ms step_avg:60.25ms
step:841/2330 train_time:50668ms step_avg:60.25ms
step:842/2330 train_time:50730ms step_avg:60.25ms
step:843/2330 train_time:50790ms step_avg:60.25ms
step:844/2330 train_time:50852ms step_avg:60.25ms
step:845/2330 train_time:50912ms step_avg:60.25ms
step:846/2330 train_time:50974ms step_avg:60.25ms
step:847/2330 train_time:51034ms step_avg:60.25ms
step:848/2330 train_time:51096ms step_avg:60.26ms
step:849/2330 train_time:51156ms step_avg:60.25ms
step:850/2330 train_time:51218ms step_avg:60.26ms
step:851/2330 train_time:51278ms step_avg:60.26ms
step:852/2330 train_time:51340ms step_avg:60.26ms
step:853/2330 train_time:51400ms step_avg:60.26ms
step:854/2330 train_time:51462ms step_avg:60.26ms
step:855/2330 train_time:51523ms step_avg:60.26ms
step:856/2330 train_time:51585ms step_avg:60.26ms
step:857/2330 train_time:51646ms step_avg:60.26ms
step:858/2330 train_time:51708ms step_avg:60.27ms
step:859/2330 train_time:51769ms step_avg:60.27ms
step:860/2330 train_time:51831ms step_avg:60.27ms
step:861/2330 train_time:51892ms step_avg:60.27ms
step:862/2330 train_time:51954ms step_avg:60.27ms
step:863/2330 train_time:52013ms step_avg:60.27ms
step:864/2330 train_time:52075ms step_avg:60.27ms
step:865/2330 train_time:52135ms step_avg:60.27ms
step:866/2330 train_time:52196ms step_avg:60.27ms
step:867/2330 train_time:52256ms step_avg:60.27ms
step:868/2330 train_time:52318ms step_avg:60.27ms
step:869/2330 train_time:52377ms step_avg:60.27ms
step:870/2330 train_time:52440ms step_avg:60.28ms
step:871/2330 train_time:52500ms step_avg:60.28ms
step:872/2330 train_time:52563ms step_avg:60.28ms
step:873/2330 train_time:52624ms step_avg:60.28ms
step:874/2330 train_time:52685ms step_avg:60.28ms
step:875/2330 train_time:52746ms step_avg:60.28ms
step:876/2330 train_time:52809ms step_avg:60.28ms
step:877/2330 train_time:52869ms step_avg:60.28ms
step:878/2330 train_time:52931ms step_avg:60.29ms
step:879/2330 train_time:52991ms step_avg:60.29ms
step:880/2330 train_time:53054ms step_avg:60.29ms
step:881/2330 train_time:53114ms step_avg:60.29ms
step:882/2330 train_time:53175ms step_avg:60.29ms
step:883/2330 train_time:53235ms step_avg:60.29ms
step:884/2330 train_time:53298ms step_avg:60.29ms
step:885/2330 train_time:53358ms step_avg:60.29ms
step:886/2330 train_time:53419ms step_avg:60.29ms
step:887/2330 train_time:53479ms step_avg:60.29ms
step:888/2330 train_time:53541ms step_avg:60.29ms
step:889/2330 train_time:53600ms step_avg:60.29ms
step:890/2330 train_time:53662ms step_avg:60.29ms
step:891/2330 train_time:53723ms step_avg:60.29ms
step:892/2330 train_time:53785ms step_avg:60.30ms
step:893/2330 train_time:53846ms step_avg:60.30ms
step:894/2330 train_time:53908ms step_avg:60.30ms
step:895/2330 train_time:53968ms step_avg:60.30ms
step:896/2330 train_time:54031ms step_avg:60.30ms
step:897/2330 train_time:54091ms step_avg:60.30ms
step:898/2330 train_time:54153ms step_avg:60.30ms
step:899/2330 train_time:54213ms step_avg:60.30ms
step:900/2330 train_time:54275ms step_avg:60.31ms
step:901/2330 train_time:54334ms step_avg:60.30ms
step:902/2330 train_time:54397ms step_avg:60.31ms
step:903/2330 train_time:54457ms step_avg:60.31ms
step:904/2330 train_time:54518ms step_avg:60.31ms
step:905/2330 train_time:54578ms step_avg:60.31ms
step:906/2330 train_time:54639ms step_avg:60.31ms
step:907/2330 train_time:54699ms step_avg:60.31ms
step:908/2330 train_time:54761ms step_avg:60.31ms
step:909/2330 train_time:54821ms step_avg:60.31ms
step:910/2330 train_time:54884ms step_avg:60.31ms
step:911/2330 train_time:54944ms step_avg:60.31ms
step:912/2330 train_time:55007ms step_avg:60.31ms
step:913/2330 train_time:55067ms step_avg:60.31ms
step:914/2330 train_time:55130ms step_avg:60.32ms
step:915/2330 train_time:55190ms step_avg:60.32ms
step:916/2330 train_time:55252ms step_avg:60.32ms
step:917/2330 train_time:55312ms step_avg:60.32ms
step:918/2330 train_time:55374ms step_avg:60.32ms
step:919/2330 train_time:55435ms step_avg:60.32ms
step:920/2330 train_time:55497ms step_avg:60.32ms
step:921/2330 train_time:55556ms step_avg:60.32ms
step:922/2330 train_time:55618ms step_avg:60.32ms
step:923/2330 train_time:55678ms step_avg:60.32ms
step:924/2330 train_time:55740ms step_avg:60.32ms
step:925/2330 train_time:55800ms step_avg:60.32ms
step:926/2330 train_time:55862ms step_avg:60.33ms
step:927/2330 train_time:55923ms step_avg:60.33ms
step:928/2330 train_time:55985ms step_avg:60.33ms
step:929/2330 train_time:56045ms step_avg:60.33ms
step:930/2330 train_time:56107ms step_avg:60.33ms
step:931/2330 train_time:56168ms step_avg:60.33ms
step:932/2330 train_time:56229ms step_avg:60.33ms
step:933/2330 train_time:56290ms step_avg:60.33ms
step:934/2330 train_time:56353ms step_avg:60.33ms
step:935/2330 train_time:56413ms step_avg:60.33ms
step:936/2330 train_time:56475ms step_avg:60.34ms
step:937/2330 train_time:56535ms step_avg:60.34ms
step:938/2330 train_time:56597ms step_avg:60.34ms
step:939/2330 train_time:56656ms step_avg:60.34ms
step:940/2330 train_time:56719ms step_avg:60.34ms
step:941/2330 train_time:56779ms step_avg:60.34ms
step:942/2330 train_time:56840ms step_avg:60.34ms
step:943/2330 train_time:56900ms step_avg:60.34ms
step:944/2330 train_time:56963ms step_avg:60.34ms
step:945/2330 train_time:57024ms step_avg:60.34ms
step:946/2330 train_time:57086ms step_avg:60.35ms
step:947/2330 train_time:57146ms step_avg:60.34ms
step:948/2330 train_time:57208ms step_avg:60.35ms
step:949/2330 train_time:57268ms step_avg:60.35ms
step:950/2330 train_time:57330ms step_avg:60.35ms
step:951/2330 train_time:57390ms step_avg:60.35ms
step:952/2330 train_time:57453ms step_avg:60.35ms
step:953/2330 train_time:57513ms step_avg:60.35ms
step:954/2330 train_time:57575ms step_avg:60.35ms
step:955/2330 train_time:57635ms step_avg:60.35ms
step:956/2330 train_time:57697ms step_avg:60.35ms
step:957/2330 train_time:57757ms step_avg:60.35ms
step:958/2330 train_time:57819ms step_avg:60.35ms
step:959/2330 train_time:57879ms step_avg:60.35ms
step:960/2330 train_time:57940ms step_avg:60.35ms
step:961/2330 train_time:58000ms step_avg:60.35ms
step:962/2330 train_time:58062ms step_avg:60.36ms
step:963/2330 train_time:58122ms step_avg:60.36ms
step:964/2330 train_time:58184ms step_avg:60.36ms
step:965/2330 train_time:58245ms step_avg:60.36ms
step:966/2330 train_time:58308ms step_avg:60.36ms
step:967/2330 train_time:58368ms step_avg:60.36ms
step:968/2330 train_time:58431ms step_avg:60.36ms
step:969/2330 train_time:58492ms step_avg:60.36ms
step:970/2330 train_time:58553ms step_avg:60.36ms
step:971/2330 train_time:58615ms step_avg:60.37ms
step:972/2330 train_time:58676ms step_avg:60.37ms
step:973/2330 train_time:58736ms step_avg:60.37ms
step:974/2330 train_time:58798ms step_avg:60.37ms
step:975/2330 train_time:58858ms step_avg:60.37ms
step:976/2330 train_time:58920ms step_avg:60.37ms
step:977/2330 train_time:58979ms step_avg:60.37ms
step:978/2330 train_time:59041ms step_avg:60.37ms
step:979/2330 train_time:59101ms step_avg:60.37ms
step:980/2330 train_time:59164ms step_avg:60.37ms
step:981/2330 train_time:59224ms step_avg:60.37ms
step:982/2330 train_time:59287ms step_avg:60.37ms
step:983/2330 train_time:59347ms step_avg:60.37ms
step:984/2330 train_time:59409ms step_avg:60.38ms
step:985/2330 train_time:59470ms step_avg:60.38ms
step:986/2330 train_time:59532ms step_avg:60.38ms
step:987/2330 train_time:59592ms step_avg:60.38ms
step:988/2330 train_time:59655ms step_avg:60.38ms
step:989/2330 train_time:59715ms step_avg:60.38ms
step:990/2330 train_time:59778ms step_avg:60.38ms
step:991/2330 train_time:59837ms step_avg:60.38ms
step:992/2330 train_time:59899ms step_avg:60.38ms
step:993/2330 train_time:59958ms step_avg:60.38ms
step:994/2330 train_time:60019ms step_avg:60.38ms
step:995/2330 train_time:60079ms step_avg:60.38ms
step:996/2330 train_time:60142ms step_avg:60.38ms
step:997/2330 train_time:60202ms step_avg:60.38ms
step:998/2330 train_time:60265ms step_avg:60.39ms
step:999/2330 train_time:60326ms step_avg:60.39ms
step:1000/2330 train_time:60387ms step_avg:60.39ms
step:1000/2330 val_loss:3.5808 train_time:60452ms step_avg:60.45ms
step:1001/2330 train_time:60478ms step_avg:60.42ms
step:1002/2330 train_time:60511ms step_avg:60.39ms
step:1003/2330 train_time:60574ms step_avg:60.39ms
step:1004/2330 train_time:60638ms step_avg:60.40ms
step:1005/2330 train_time:60697ms step_avg:60.40ms
step:1006/2330 train_time:60759ms step_avg:60.40ms
step:1007/2330 train_time:60818ms step_avg:60.40ms
step:1008/2330 train_time:60879ms step_avg:60.40ms
step:1009/2330 train_time:60938ms step_avg:60.39ms
step:1010/2330 train_time:60999ms step_avg:60.39ms
step:1011/2330 train_time:61058ms step_avg:60.39ms
step:1012/2330 train_time:61118ms step_avg:60.39ms
step:1013/2330 train_time:61177ms step_avg:60.39ms
step:1014/2330 train_time:61237ms step_avg:60.39ms
step:1015/2330 train_time:61296ms step_avg:60.39ms
step:1016/2330 train_time:61358ms step_avg:60.39ms
step:1017/2330 train_time:61421ms step_avg:60.39ms
step:1018/2330 train_time:61484ms step_avg:60.40ms
step:1019/2330 train_time:61545ms step_avg:60.40ms
step:1020/2330 train_time:61607ms step_avg:60.40ms
step:1021/2330 train_time:61668ms step_avg:60.40ms
step:1022/2330 train_time:61730ms step_avg:60.40ms
step:1023/2330 train_time:61789ms step_avg:60.40ms
step:1024/2330 train_time:61851ms step_avg:60.40ms
step:1025/2330 train_time:61910ms step_avg:60.40ms
step:1026/2330 train_time:61972ms step_avg:60.40ms
step:1027/2330 train_time:62031ms step_avg:60.40ms
step:1028/2330 train_time:62092ms step_avg:60.40ms
step:1029/2330 train_time:62151ms step_avg:60.40ms
step:1030/2330 train_time:62213ms step_avg:60.40ms
step:1031/2330 train_time:62272ms step_avg:60.40ms
step:1032/2330 train_time:62334ms step_avg:60.40ms
step:1033/2330 train_time:62396ms step_avg:60.40ms
step:1034/2330 train_time:62459ms step_avg:60.40ms
step:1035/2330 train_time:62518ms step_avg:60.40ms
step:1036/2330 train_time:62579ms step_avg:60.40ms
step:1037/2330 train_time:62639ms step_avg:60.40ms
step:1038/2330 train_time:62701ms step_avg:60.41ms
step:1039/2330 train_time:62760ms step_avg:60.40ms
step:1040/2330 train_time:62822ms step_avg:60.41ms
step:1041/2330 train_time:62881ms step_avg:60.40ms
step:1042/2330 train_time:62944ms step_avg:60.41ms
step:1043/2330 train_time:63003ms step_avg:60.41ms
step:1044/2330 train_time:63065ms step_avg:60.41ms
step:1045/2330 train_time:63125ms step_avg:60.41ms
step:1046/2330 train_time:63187ms step_avg:60.41ms
step:1047/2330 train_time:63247ms step_avg:60.41ms
step:1048/2330 train_time:63309ms step_avg:60.41ms
step:1049/2330 train_time:63369ms step_avg:60.41ms
step:1050/2330 train_time:63432ms step_avg:60.41ms
step:1051/2330 train_time:63492ms step_avg:60.41ms
step:1052/2330 train_time:63555ms step_avg:60.41ms
step:1053/2330 train_time:63616ms step_avg:60.41ms
step:1054/2330 train_time:63678ms step_avg:60.42ms
step:1055/2330 train_time:63737ms step_avg:60.41ms
step:1056/2330 train_time:63799ms step_avg:60.42ms
step:1057/2330 train_time:63859ms step_avg:60.41ms
step:1058/2330 train_time:63920ms step_avg:60.42ms
step:1059/2330 train_time:63979ms step_avg:60.41ms
step:1060/2330 train_time:64041ms step_avg:60.42ms
step:1061/2330 train_time:64101ms step_avg:60.42ms
step:1062/2330 train_time:64163ms step_avg:60.42ms
step:1063/2330 train_time:64222ms step_avg:60.42ms
step:1064/2330 train_time:64284ms step_avg:60.42ms
step:1065/2330 train_time:64343ms step_avg:60.42ms
step:1066/2330 train_time:64406ms step_avg:60.42ms
step:1067/2330 train_time:64466ms step_avg:60.42ms
step:1068/2330 train_time:64528ms step_avg:60.42ms
step:1069/2330 train_time:64588ms step_avg:60.42ms
step:1070/2330 train_time:64651ms step_avg:60.42ms
step:1071/2330 train_time:64711ms step_avg:60.42ms
step:1072/2330 train_time:64774ms step_avg:60.42ms
step:1073/2330 train_time:64834ms step_avg:60.42ms
step:1074/2330 train_time:64895ms step_avg:60.42ms
step:1075/2330 train_time:64955ms step_avg:60.42ms
step:1076/2330 train_time:65017ms step_avg:60.42ms
step:1077/2330 train_time:65077ms step_avg:60.42ms
step:1078/2330 train_time:65138ms step_avg:60.42ms
step:1079/2330 train_time:65197ms step_avg:60.42ms
step:1080/2330 train_time:65259ms step_avg:60.43ms
step:1081/2330 train_time:65318ms step_avg:60.42ms
step:1082/2330 train_time:65380ms step_avg:60.43ms
step:1083/2330 train_time:65440ms step_avg:60.42ms
step:1084/2330 train_time:65502ms step_avg:60.43ms
step:1085/2330 train_time:65561ms step_avg:60.43ms
step:1086/2330 train_time:65624ms step_avg:60.43ms
step:1087/2330 train_time:65685ms step_avg:60.43ms
step:1088/2330 train_time:65747ms step_avg:60.43ms
step:1089/2330 train_time:65807ms step_avg:60.43ms
step:1090/2330 train_time:65868ms step_avg:60.43ms
step:1091/2330 train_time:65928ms step_avg:60.43ms
step:1092/2330 train_time:65990ms step_avg:60.43ms
step:1093/2330 train_time:66050ms step_avg:60.43ms
step:1094/2330 train_time:66112ms step_avg:60.43ms
step:1095/2330 train_time:66172ms step_avg:60.43ms
step:1096/2330 train_time:66233ms step_avg:60.43ms
step:1097/2330 train_time:66294ms step_avg:60.43ms
step:1098/2330 train_time:66356ms step_avg:60.43ms
step:1099/2330 train_time:66416ms step_avg:60.43ms
step:1100/2330 train_time:66478ms step_avg:60.43ms
step:1101/2330 train_time:66538ms step_avg:60.43ms
step:1102/2330 train_time:66599ms step_avg:60.44ms
step:1103/2330 train_time:66659ms step_avg:60.43ms
step:1104/2330 train_time:66721ms step_avg:60.44ms
step:1105/2330 train_time:66781ms step_avg:60.43ms
step:1106/2330 train_time:66843ms step_avg:60.44ms
step:1107/2330 train_time:66903ms step_avg:60.44ms
step:1108/2330 train_time:66965ms step_avg:60.44ms
step:1109/2330 train_time:67025ms step_avg:60.44ms
step:1110/2330 train_time:67087ms step_avg:60.44ms
step:1111/2330 train_time:67147ms step_avg:60.44ms
step:1112/2330 train_time:67209ms step_avg:60.44ms
step:1113/2330 train_time:67269ms step_avg:60.44ms
step:1114/2330 train_time:67331ms step_avg:60.44ms
step:1115/2330 train_time:67391ms step_avg:60.44ms
step:1116/2330 train_time:67454ms step_avg:60.44ms
step:1117/2330 train_time:67514ms step_avg:60.44ms
step:1118/2330 train_time:67576ms step_avg:60.44ms
step:1119/2330 train_time:67636ms step_avg:60.44ms
step:1120/2330 train_time:67697ms step_avg:60.44ms
step:1121/2330 train_time:67757ms step_avg:60.44ms
step:1122/2330 train_time:67819ms step_avg:60.44ms
step:1123/2330 train_time:67878ms step_avg:60.44ms
step:1124/2330 train_time:67939ms step_avg:60.44ms
step:1125/2330 train_time:67999ms step_avg:60.44ms
step:1126/2330 train_time:68062ms step_avg:60.45ms
step:1127/2330 train_time:68122ms step_avg:60.45ms
step:1128/2330 train_time:68184ms step_avg:60.45ms
step:1129/2330 train_time:68244ms step_avg:60.45ms
step:1130/2330 train_time:68306ms step_avg:60.45ms
step:1131/2330 train_time:68366ms step_avg:60.45ms
step:1132/2330 train_time:68428ms step_avg:60.45ms
step:1133/2330 train_time:68489ms step_avg:60.45ms
step:1134/2330 train_time:68551ms step_avg:60.45ms
step:1135/2330 train_time:68611ms step_avg:60.45ms
step:1136/2330 train_time:68673ms step_avg:60.45ms
step:1137/2330 train_time:68734ms step_avg:60.45ms
step:1138/2330 train_time:68795ms step_avg:60.45ms
step:1139/2330 train_time:68856ms step_avg:60.45ms
step:1140/2330 train_time:68918ms step_avg:60.45ms
step:1141/2330 train_time:68977ms step_avg:60.45ms
step:1142/2330 train_time:69039ms step_avg:60.45ms
step:1143/2330 train_time:69098ms step_avg:60.45ms
step:1144/2330 train_time:69159ms step_avg:60.45ms
step:1145/2330 train_time:69219ms step_avg:60.45ms
step:1146/2330 train_time:69280ms step_avg:60.45ms
step:1147/2330 train_time:69341ms step_avg:60.45ms
step:1148/2330 train_time:69403ms step_avg:60.46ms
step:1149/2330 train_time:69464ms step_avg:60.46ms
step:1150/2330 train_time:69526ms step_avg:60.46ms
step:1151/2330 train_time:69586ms step_avg:60.46ms
step:1152/2330 train_time:69648ms step_avg:60.46ms
step:1153/2330 train_time:69709ms step_avg:60.46ms
step:1154/2330 train_time:69770ms step_avg:60.46ms
step:1155/2330 train_time:69831ms step_avg:60.46ms
step:1156/2330 train_time:69893ms step_avg:60.46ms
step:1157/2330 train_time:69953ms step_avg:60.46ms
step:1158/2330 train_time:70015ms step_avg:60.46ms
step:1159/2330 train_time:70076ms step_avg:60.46ms
step:1160/2330 train_time:70138ms step_avg:60.46ms
step:1161/2330 train_time:70198ms step_avg:60.46ms
step:1162/2330 train_time:70259ms step_avg:60.46ms
step:1163/2330 train_time:70318ms step_avg:60.46ms
step:1164/2330 train_time:70380ms step_avg:60.46ms
step:1165/2330 train_time:70440ms step_avg:60.46ms
step:1166/2330 train_time:70501ms step_avg:60.46ms
step:1167/2330 train_time:70562ms step_avg:60.46ms
step:1168/2330 train_time:70624ms step_avg:60.47ms
step:1169/2330 train_time:70684ms step_avg:60.47ms
step:1170/2330 train_time:70746ms step_avg:60.47ms
step:1171/2330 train_time:70806ms step_avg:60.47ms
step:1172/2330 train_time:70868ms step_avg:60.47ms
step:1173/2330 train_time:70928ms step_avg:60.47ms
step:1174/2330 train_time:70991ms step_avg:60.47ms
step:1175/2330 train_time:71052ms step_avg:60.47ms
step:1176/2330 train_time:71114ms step_avg:60.47ms
step:1177/2330 train_time:71174ms step_avg:60.47ms
step:1178/2330 train_time:71236ms step_avg:60.47ms
step:1179/2330 train_time:71296ms step_avg:60.47ms
step:1180/2330 train_time:71358ms step_avg:60.47ms
step:1181/2330 train_time:71418ms step_avg:60.47ms
step:1182/2330 train_time:71480ms step_avg:60.47ms
step:1183/2330 train_time:71539ms step_avg:60.47ms
step:1184/2330 train_time:71601ms step_avg:60.47ms
step:1185/2330 train_time:71660ms step_avg:60.47ms
step:1186/2330 train_time:71722ms step_avg:60.47ms
step:1187/2330 train_time:71783ms step_avg:60.47ms
step:1188/2330 train_time:71846ms step_avg:60.48ms
step:1189/2330 train_time:71906ms step_avg:60.48ms
step:1190/2330 train_time:71968ms step_avg:60.48ms
step:1191/2330 train_time:72028ms step_avg:60.48ms
step:1192/2330 train_time:72090ms step_avg:60.48ms
step:1193/2330 train_time:72150ms step_avg:60.48ms
step:1194/2330 train_time:72212ms step_avg:60.48ms
step:1195/2330 train_time:72272ms step_avg:60.48ms
step:1196/2330 train_time:72334ms step_avg:60.48ms
step:1197/2330 train_time:72394ms step_avg:60.48ms
step:1198/2330 train_time:72457ms step_avg:60.48ms
step:1199/2330 train_time:72517ms step_avg:60.48ms
step:1200/2330 train_time:72579ms step_avg:60.48ms
step:1201/2330 train_time:72639ms step_avg:60.48ms
step:1202/2330 train_time:72700ms step_avg:60.48ms
step:1203/2330 train_time:72760ms step_avg:60.48ms
step:1204/2330 train_time:72821ms step_avg:60.48ms
step:1205/2330 train_time:72881ms step_avg:60.48ms
step:1206/2330 train_time:72943ms step_avg:60.48ms
step:1207/2330 train_time:73004ms step_avg:60.48ms
step:1208/2330 train_time:73066ms step_avg:60.49ms
step:1209/2330 train_time:73127ms step_avg:60.49ms
step:1210/2330 train_time:73188ms step_avg:60.49ms
step:1211/2330 train_time:73248ms step_avg:60.49ms
step:1212/2330 train_time:73310ms step_avg:60.49ms
step:1213/2330 train_time:73370ms step_avg:60.49ms
step:1214/2330 train_time:73433ms step_avg:60.49ms
step:1215/2330 train_time:73494ms step_avg:60.49ms
step:1216/2330 train_time:73556ms step_avg:60.49ms
step:1217/2330 train_time:73616ms step_avg:60.49ms
step:1218/2330 train_time:73678ms step_avg:60.49ms
step:1219/2330 train_time:73738ms step_avg:60.49ms
step:1220/2330 train_time:73799ms step_avg:60.49ms
step:1221/2330 train_time:73858ms step_avg:60.49ms
step:1222/2330 train_time:73920ms step_avg:60.49ms
step:1223/2330 train_time:73979ms step_avg:60.49ms
step:1224/2330 train_time:74041ms step_avg:60.49ms
step:1225/2330 train_time:74102ms step_avg:60.49ms
step:1226/2330 train_time:74164ms step_avg:60.49ms
step:1227/2330 train_time:74224ms step_avg:60.49ms
step:1228/2330 train_time:74287ms step_avg:60.49ms
step:1229/2330 train_time:74347ms step_avg:60.49ms
step:1230/2330 train_time:74409ms step_avg:60.50ms
step:1231/2330 train_time:74469ms step_avg:60.49ms
step:1232/2330 train_time:74531ms step_avg:60.50ms
step:1233/2330 train_time:74592ms step_avg:60.50ms
step:1234/2330 train_time:74654ms step_avg:60.50ms
step:1235/2330 train_time:74715ms step_avg:60.50ms
step:1236/2330 train_time:74777ms step_avg:60.50ms
step:1237/2330 train_time:74837ms step_avg:60.50ms
step:1238/2330 train_time:74898ms step_avg:60.50ms
step:1239/2330 train_time:74959ms step_avg:60.50ms
step:1240/2330 train_time:75020ms step_avg:60.50ms
step:1241/2330 train_time:75079ms step_avg:60.50ms
step:1242/2330 train_time:75142ms step_avg:60.50ms
step:1243/2330 train_time:75202ms step_avg:60.50ms
step:1244/2330 train_time:75264ms step_avg:60.50ms
step:1245/2330 train_time:75324ms step_avg:60.50ms
step:1246/2330 train_time:75386ms step_avg:60.50ms
step:1247/2330 train_time:75446ms step_avg:60.50ms
step:1248/2330 train_time:75508ms step_avg:60.50ms
step:1249/2330 train_time:75569ms step_avg:60.50ms
step:1250/2330 train_time:75631ms step_avg:60.51ms
step:1250/2330 val_loss:3.5203 train_time:75696ms step_avg:60.56ms
step:1251/2330 train_time:75722ms step_avg:60.53ms
step:1252/2330 train_time:75757ms step_avg:60.51ms
step:1253/2330 train_time:75824ms step_avg:60.51ms
step:1254/2330 train_time:75889ms step_avg:60.52ms
step:1255/2330 train_time:75948ms step_avg:60.52ms
step:1256/2330 train_time:76012ms step_avg:60.52ms
step:1257/2330 train_time:76070ms step_avg:60.52ms
step:1258/2330 train_time:76131ms step_avg:60.52ms
step:1259/2330 train_time:76190ms step_avg:60.52ms
step:1260/2330 train_time:76251ms step_avg:60.52ms
step:1261/2330 train_time:76309ms step_avg:60.52ms
step:1262/2330 train_time:76371ms step_avg:60.52ms
step:1263/2330 train_time:76429ms step_avg:60.51ms
step:1264/2330 train_time:76491ms step_avg:60.51ms
step:1265/2330 train_time:76549ms step_avg:60.51ms
step:1266/2330 train_time:76612ms step_avg:60.52ms
step:1267/2330 train_time:76673ms step_avg:60.52ms
step:1268/2330 train_time:76737ms step_avg:60.52ms
step:1269/2330 train_time:76799ms step_avg:60.52ms
step:1270/2330 train_time:76861ms step_avg:60.52ms
step:1271/2330 train_time:76921ms step_avg:60.52ms
step:1272/2330 train_time:76984ms step_avg:60.52ms
step:1273/2330 train_time:77044ms step_avg:60.52ms
step:1274/2330 train_time:77106ms step_avg:60.52ms
step:1275/2330 train_time:77165ms step_avg:60.52ms
step:1276/2330 train_time:77227ms step_avg:60.52ms
step:1277/2330 train_time:77287ms step_avg:60.52ms
step:1278/2330 train_time:77348ms step_avg:60.52ms
step:1279/2330 train_time:77407ms step_avg:60.52ms
step:1280/2330 train_time:77468ms step_avg:60.52ms
step:1281/2330 train_time:77527ms step_avg:60.52ms
step:1282/2330 train_time:77589ms step_avg:60.52ms
step:1283/2330 train_time:77649ms step_avg:60.52ms
step:1284/2330 train_time:77712ms step_avg:60.52ms
step:1285/2330 train_time:77773ms step_avg:60.52ms
step:1286/2330 train_time:77836ms step_avg:60.53ms
step:1287/2330 train_time:77897ms step_avg:60.53ms
step:1288/2330 train_time:77959ms step_avg:60.53ms
step:1289/2330 train_time:78018ms step_avg:60.53ms
step:1290/2330 train_time:78081ms step_avg:60.53ms
step:1291/2330 train_time:78140ms step_avg:60.53ms
step:1292/2330 train_time:78203ms step_avg:60.53ms
step:1293/2330 train_time:78263ms step_avg:60.53ms
step:1294/2330 train_time:78324ms step_avg:60.53ms
step:1295/2330 train_time:78384ms step_avg:60.53ms
step:1296/2330 train_time:78445ms step_avg:60.53ms
step:1297/2330 train_time:78505ms step_avg:60.53ms
step:1298/2330 train_time:78567ms step_avg:60.53ms
step:1299/2330 train_time:78627ms step_avg:60.53ms
step:1300/2330 train_time:78689ms step_avg:60.53ms
step:1301/2330 train_time:78749ms step_avg:60.53ms
step:1302/2330 train_time:78811ms step_avg:60.53ms
step:1303/2330 train_time:78871ms step_avg:60.53ms
step:1304/2330 train_time:78933ms step_avg:60.53ms
step:1305/2330 train_time:78993ms step_avg:60.53ms
step:1306/2330 train_time:79055ms step_avg:60.53ms
step:1307/2330 train_time:79115ms step_avg:60.53ms
step:1308/2330 train_time:79177ms step_avg:60.53ms
step:1309/2330 train_time:79238ms step_avg:60.53ms
step:1310/2330 train_time:79300ms step_avg:60.53ms
step:1311/2330 train_time:79360ms step_avg:60.53ms
step:1312/2330 train_time:79422ms step_avg:60.54ms
step:1313/2330 train_time:79482ms step_avg:60.53ms
step:1314/2330 train_time:79544ms step_avg:60.54ms
step:1315/2330 train_time:79604ms step_avg:60.54ms
step:1316/2330 train_time:79666ms step_avg:60.54ms
step:1317/2330 train_time:79727ms step_avg:60.54ms
step:1318/2330 train_time:79789ms step_avg:60.54ms
step:1319/2330 train_time:79849ms step_avg:60.54ms
step:1320/2330 train_time:79911ms step_avg:60.54ms
step:1321/2330 train_time:79970ms step_avg:60.54ms
step:1322/2330 train_time:80032ms step_avg:60.54ms
step:1323/2330 train_time:80092ms step_avg:60.54ms
step:1324/2330 train_time:80154ms step_avg:60.54ms
step:1325/2330 train_time:80215ms step_avg:60.54ms
step:1326/2330 train_time:80278ms step_avg:60.54ms
step:1327/2330 train_time:80337ms step_avg:60.54ms
step:1328/2330 train_time:80400ms step_avg:60.54ms
step:1329/2330 train_time:80461ms step_avg:60.54ms
step:1330/2330 train_time:80523ms step_avg:60.54ms
step:1331/2330 train_time:80583ms step_avg:60.54ms
step:1332/2330 train_time:80646ms step_avg:60.54ms
step:1333/2330 train_time:80706ms step_avg:60.54ms
step:1334/2330 train_time:80768ms step_avg:60.55ms
step:1335/2330 train_time:80828ms step_avg:60.55ms
step:1336/2330 train_time:80890ms step_avg:60.55ms
step:1337/2330 train_time:80949ms step_avg:60.55ms
step:1338/2330 train_time:81010ms step_avg:60.55ms
step:1339/2330 train_time:81070ms step_avg:60.55ms
step:1340/2330 train_time:81132ms step_avg:60.55ms
step:1341/2330 train_time:81192ms step_avg:60.55ms
step:1342/2330 train_time:81254ms step_avg:60.55ms
step:1343/2330 train_time:81314ms step_avg:60.55ms
step:1344/2330 train_time:81377ms step_avg:60.55ms
step:1345/2330 train_time:81438ms step_avg:60.55ms
step:1346/2330 train_time:81500ms step_avg:60.55ms
step:1347/2330 train_time:81561ms step_avg:60.55ms
step:1348/2330 train_time:81623ms step_avg:60.55ms
step:1349/2330 train_time:81683ms step_avg:60.55ms
step:1350/2330 train_time:81745ms step_avg:60.55ms
step:1351/2330 train_time:81806ms step_avg:60.55ms
step:1352/2330 train_time:81868ms step_avg:60.55ms
step:1353/2330 train_time:81927ms step_avg:60.55ms
step:1354/2330 train_time:81990ms step_avg:60.55ms
step:1355/2330 train_time:82049ms step_avg:60.55ms
step:1356/2330 train_time:82111ms step_avg:60.55ms
step:1357/2330 train_time:82170ms step_avg:60.55ms
step:1358/2330 train_time:82232ms step_avg:60.55ms
step:1359/2330 train_time:82292ms step_avg:60.55ms
step:1360/2330 train_time:82354ms step_avg:60.55ms
step:1361/2330 train_time:82414ms step_avg:60.55ms
step:1362/2330 train_time:82476ms step_avg:60.56ms
step:1363/2330 train_time:82537ms step_avg:60.56ms
step:1364/2330 train_time:82600ms step_avg:60.56ms
step:1365/2330 train_time:82661ms step_avg:60.56ms
step:1366/2330 train_time:82722ms step_avg:60.56ms
step:1367/2330 train_time:82782ms step_avg:60.56ms
step:1368/2330 train_time:82845ms step_avg:60.56ms
step:1369/2330 train_time:82906ms step_avg:60.56ms
step:1370/2330 train_time:82968ms step_avg:60.56ms
step:1371/2330 train_time:83027ms step_avg:60.56ms
step:1372/2330 train_time:83089ms step_avg:60.56ms
step:1373/2330 train_time:83149ms step_avg:60.56ms
step:1374/2330 train_time:83210ms step_avg:60.56ms
step:1375/2330 train_time:83270ms step_avg:60.56ms
step:1376/2330 train_time:83332ms step_avg:60.56ms
step:1377/2330 train_time:83392ms step_avg:60.56ms
step:1378/2330 train_time:83454ms step_avg:60.56ms
step:1379/2330 train_time:83514ms step_avg:60.56ms
step:1380/2330 train_time:83576ms step_avg:60.56ms
step:1381/2330 train_time:83636ms step_avg:60.56ms
step:1382/2330 train_time:83699ms step_avg:60.56ms
step:1383/2330 train_time:83759ms step_avg:60.56ms
step:1384/2330 train_time:83821ms step_avg:60.56ms
step:1385/2330 train_time:83881ms step_avg:60.56ms
step:1386/2330 train_time:83944ms step_avg:60.57ms
step:1387/2330 train_time:84004ms step_avg:60.57ms
step:1388/2330 train_time:84066ms step_avg:60.57ms
step:1389/2330 train_time:84126ms step_avg:60.57ms
step:1390/2330 train_time:84189ms step_avg:60.57ms
step:1391/2330 train_time:84248ms step_avg:60.57ms
step:1392/2330 train_time:84309ms step_avg:60.57ms
step:1393/2330 train_time:84369ms step_avg:60.57ms
step:1394/2330 train_time:84431ms step_avg:60.57ms
step:1395/2330 train_time:84491ms step_avg:60.57ms
step:1396/2330 train_time:84553ms step_avg:60.57ms
step:1397/2330 train_time:84614ms step_avg:60.57ms
step:1398/2330 train_time:84676ms step_avg:60.57ms
step:1399/2330 train_time:84736ms step_avg:60.57ms
step:1400/2330 train_time:84799ms step_avg:60.57ms
step:1401/2330 train_time:84859ms step_avg:60.57ms
step:1402/2330 train_time:84920ms step_avg:60.57ms
step:1403/2330 train_time:84981ms step_avg:60.57ms
step:1404/2330 train_time:85043ms step_avg:60.57ms
step:1405/2330 train_time:85103ms step_avg:60.57ms
step:1406/2330 train_time:85165ms step_avg:60.57ms
step:1407/2330 train_time:85225ms step_avg:60.57ms
step:1408/2330 train_time:85288ms step_avg:60.57ms
step:1409/2330 train_time:85347ms step_avg:60.57ms
step:1410/2330 train_time:85409ms step_avg:60.57ms
step:1411/2330 train_time:85469ms step_avg:60.57ms
step:1412/2330 train_time:85530ms step_avg:60.57ms
step:1413/2330 train_time:85591ms step_avg:60.57ms
step:1414/2330 train_time:85653ms step_avg:60.57ms
step:1415/2330 train_time:85714ms step_avg:60.57ms
step:1416/2330 train_time:85776ms step_avg:60.58ms
step:1417/2330 train_time:85836ms step_avg:60.58ms
step:1418/2330 train_time:85898ms step_avg:60.58ms
step:1419/2330 train_time:85958ms step_avg:60.58ms
step:1420/2330 train_time:86020ms step_avg:60.58ms
step:1421/2330 train_time:86080ms step_avg:60.58ms
step:1422/2330 train_time:86143ms step_avg:60.58ms
step:1423/2330 train_time:86203ms step_avg:60.58ms
step:1424/2330 train_time:86265ms step_avg:60.58ms
step:1425/2330 train_time:86325ms step_avg:60.58ms
step:1426/2330 train_time:86387ms step_avg:60.58ms
step:1427/2330 train_time:86447ms step_avg:60.58ms
step:1428/2330 train_time:86508ms step_avg:60.58ms
step:1429/2330 train_time:86568ms step_avg:60.58ms
step:1430/2330 train_time:86630ms step_avg:60.58ms
step:1431/2330 train_time:86689ms step_avg:60.58ms
step:1432/2330 train_time:86751ms step_avg:60.58ms
step:1433/2330 train_time:86811ms step_avg:60.58ms
step:1434/2330 train_time:86874ms step_avg:60.58ms
step:1435/2330 train_time:86934ms step_avg:60.58ms
step:1436/2330 train_time:86996ms step_avg:60.58ms
step:1437/2330 train_time:87056ms step_avg:60.58ms
step:1438/2330 train_time:87118ms step_avg:60.58ms
step:1439/2330 train_time:87178ms step_avg:60.58ms
step:1440/2330 train_time:87241ms step_avg:60.58ms
step:1441/2330 train_time:87301ms step_avg:60.58ms
step:1442/2330 train_time:87363ms step_avg:60.58ms
step:1443/2330 train_time:87423ms step_avg:60.58ms
step:1444/2330 train_time:87486ms step_avg:60.59ms
step:1445/2330 train_time:87546ms step_avg:60.59ms
step:1446/2330 train_time:87608ms step_avg:60.59ms
step:1447/2330 train_time:87667ms step_avg:60.59ms
step:1448/2330 train_time:87728ms step_avg:60.59ms
step:1449/2330 train_time:87788ms step_avg:60.59ms
step:1450/2330 train_time:87850ms step_avg:60.59ms
step:1451/2330 train_time:87910ms step_avg:60.59ms
step:1452/2330 train_time:87973ms step_avg:60.59ms
step:1453/2330 train_time:88033ms step_avg:60.59ms
step:1454/2330 train_time:88096ms step_avg:60.59ms
step:1455/2330 train_time:88156ms step_avg:60.59ms
step:1456/2330 train_time:88218ms step_avg:60.59ms
step:1457/2330 train_time:88278ms step_avg:60.59ms
step:1458/2330 train_time:88341ms step_avg:60.59ms
step:1459/2330 train_time:88401ms step_avg:60.59ms
step:1460/2330 train_time:88464ms step_avg:60.59ms
step:1461/2330 train_time:88524ms step_avg:60.59ms
step:1462/2330 train_time:88587ms step_avg:60.59ms
step:1463/2330 train_time:88646ms step_avg:60.59ms
step:1464/2330 train_time:88708ms step_avg:60.59ms
step:1465/2330 train_time:88768ms step_avg:60.59ms
step:1466/2330 train_time:88829ms step_avg:60.59ms
step:1467/2330 train_time:88889ms step_avg:60.59ms
step:1468/2330 train_time:88950ms step_avg:60.59ms
step:1469/2330 train_time:89011ms step_avg:60.59ms
step:1470/2330 train_time:89074ms step_avg:60.59ms
step:1471/2330 train_time:89133ms step_avg:60.59ms
step:1472/2330 train_time:89196ms step_avg:60.60ms
step:1473/2330 train_time:89257ms step_avg:60.60ms
step:1474/2330 train_time:89318ms step_avg:60.60ms
step:1475/2330 train_time:89378ms step_avg:60.60ms
step:1476/2330 train_time:89441ms step_avg:60.60ms
step:1477/2330 train_time:89501ms step_avg:60.60ms
step:1478/2330 train_time:89563ms step_avg:60.60ms
step:1479/2330 train_time:89624ms step_avg:60.60ms
step:1480/2330 train_time:89687ms step_avg:60.60ms
step:1481/2330 train_time:89747ms step_avg:60.60ms
step:1482/2330 train_time:89808ms step_avg:60.60ms
step:1483/2330 train_time:89868ms step_avg:60.60ms
step:1484/2330 train_time:89929ms step_avg:60.60ms
step:1485/2330 train_time:89989ms step_avg:60.60ms
step:1486/2330 train_time:90050ms step_avg:60.60ms
step:1487/2330 train_time:90110ms step_avg:60.60ms
step:1488/2330 train_time:90172ms step_avg:60.60ms
step:1489/2330 train_time:90232ms step_avg:60.60ms
step:1490/2330 train_time:90294ms step_avg:60.60ms
step:1491/2330 train_time:90355ms step_avg:60.60ms
step:1492/2330 train_time:90417ms step_avg:60.60ms
step:1493/2330 train_time:90478ms step_avg:60.60ms
step:1494/2330 train_time:90540ms step_avg:60.60ms
step:1495/2330 train_time:90601ms step_avg:60.60ms
step:1496/2330 train_time:90664ms step_avg:60.60ms
step:1497/2330 train_time:90724ms step_avg:60.60ms
step:1498/2330 train_time:90786ms step_avg:60.61ms
step:1499/2330 train_time:90846ms step_avg:60.60ms
step:1500/2330 train_time:90909ms step_avg:60.61ms
step:1500/2330 val_loss:3.4563 train_time:90973ms step_avg:60.65ms
step:1501/2330 train_time:90999ms step_avg:60.63ms
step:1502/2330 train_time:91034ms step_avg:60.61ms
step:1503/2330 train_time:91099ms step_avg:60.61ms
step:1504/2330 train_time:91164ms step_avg:60.61ms
step:1505/2330 train_time:91225ms step_avg:60.61ms
step:1506/2330 train_time:91287ms step_avg:60.62ms
step:1507/2330 train_time:91348ms step_avg:60.62ms
step:1508/2330 train_time:91409ms step_avg:60.62ms
step:1509/2330 train_time:91468ms step_avg:60.61ms
step:1510/2330 train_time:91529ms step_avg:60.61ms
step:1511/2330 train_time:91588ms step_avg:60.61ms
step:1512/2330 train_time:91649ms step_avg:60.61ms
step:1513/2330 train_time:91708ms step_avg:60.61ms
step:1514/2330 train_time:91768ms step_avg:60.61ms
step:1515/2330 train_time:91827ms step_avg:60.61ms
step:1516/2330 train_time:91889ms step_avg:60.61ms
step:1517/2330 train_time:91949ms step_avg:60.61ms
step:1518/2330 train_time:92013ms step_avg:60.61ms
step:1519/2330 train_time:92074ms step_avg:60.62ms
step:1520/2330 train_time:92137ms step_avg:60.62ms
step:1521/2330 train_time:92199ms step_avg:60.62ms
step:1522/2330 train_time:92261ms step_avg:60.62ms
step:1523/2330 train_time:92322ms step_avg:60.62ms
step:1524/2330 train_time:92384ms step_avg:60.62ms
step:1525/2330 train_time:92444ms step_avg:60.62ms
step:1526/2330 train_time:92506ms step_avg:60.62ms
step:1527/2330 train_time:92566ms step_avg:60.62ms
step:1528/2330 train_time:92627ms step_avg:60.62ms
step:1529/2330 train_time:92687ms step_avg:60.62ms
step:1530/2330 train_time:92749ms step_avg:60.62ms
step:1531/2330 train_time:92809ms step_avg:60.62ms
step:1532/2330 train_time:92870ms step_avg:60.62ms
step:1533/2330 train_time:92930ms step_avg:60.62ms
step:1534/2330 train_time:92993ms step_avg:60.62ms
step:1535/2330 train_time:93054ms step_avg:60.62ms
step:1536/2330 train_time:93117ms step_avg:60.62ms
step:1537/2330 train_time:93178ms step_avg:60.62ms
step:1538/2330 train_time:93242ms step_avg:60.63ms
step:1539/2330 train_time:93302ms step_avg:60.63ms
step:1540/2330 train_time:93365ms step_avg:60.63ms
step:1541/2330 train_time:93425ms step_avg:60.63ms
step:1542/2330 train_time:93487ms step_avg:60.63ms
step:1543/2330 train_time:93548ms step_avg:60.63ms
step:1544/2330 train_time:93610ms step_avg:60.63ms
step:1545/2330 train_time:93670ms step_avg:60.63ms
step:1546/2330 train_time:93732ms step_avg:60.63ms
step:1547/2330 train_time:93791ms step_avg:60.63ms
step:1548/2330 train_time:93853ms step_avg:60.63ms
step:1549/2330 train_time:93913ms step_avg:60.63ms
step:1550/2330 train_time:93975ms step_avg:60.63ms
step:1551/2330 train_time:94035ms step_avg:60.63ms
step:1552/2330 train_time:94098ms step_avg:60.63ms
step:1553/2330 train_time:94159ms step_avg:60.63ms
step:1554/2330 train_time:94221ms step_avg:60.63ms
step:1555/2330 train_time:94282ms step_avg:60.63ms
step:1556/2330 train_time:94345ms step_avg:60.63ms
step:1557/2330 train_time:94406ms step_avg:60.63ms
step:1558/2330 train_time:94469ms step_avg:60.63ms
step:1559/2330 train_time:94529ms step_avg:60.63ms
step:1560/2330 train_time:94591ms step_avg:60.64ms
step:1561/2330 train_time:94651ms step_avg:60.63ms
step:1562/2330 train_time:94713ms step_avg:60.64ms
step:1563/2330 train_time:94772ms step_avg:60.63ms
step:1564/2330 train_time:94834ms step_avg:60.64ms
step:1565/2330 train_time:94895ms step_avg:60.64ms
step:1566/2330 train_time:94957ms step_avg:60.64ms
step:1567/2330 train_time:95017ms step_avg:60.64ms
step:1568/2330 train_time:95079ms step_avg:60.64ms
step:1569/2330 train_time:95140ms step_avg:60.64ms
step:1570/2330 train_time:95203ms step_avg:60.64ms
step:1571/2330 train_time:95264ms step_avg:60.64ms
step:1572/2330 train_time:95326ms step_avg:60.64ms
step:1573/2330 train_time:95386ms step_avg:60.64ms
step:1574/2330 train_time:95449ms step_avg:60.64ms
step:1575/2330 train_time:95510ms step_avg:60.64ms
step:1576/2330 train_time:95573ms step_avg:60.64ms
step:1577/2330 train_time:95633ms step_avg:60.64ms
step:1578/2330 train_time:95695ms step_avg:60.64ms
step:1579/2330 train_time:95756ms step_avg:60.64ms
step:1580/2330 train_time:95818ms step_avg:60.64ms
step:1581/2330 train_time:95878ms step_avg:60.64ms
step:1582/2330 train_time:95940ms step_avg:60.65ms
step:1583/2330 train_time:96001ms step_avg:60.64ms
step:1584/2330 train_time:96063ms step_avg:60.65ms
step:1585/2330 train_time:96124ms step_avg:60.65ms
step:1586/2330 train_time:96186ms step_avg:60.65ms
step:1587/2330 train_time:96246ms step_avg:60.65ms
step:1588/2330 train_time:96310ms step_avg:60.65ms
step:1589/2330 train_time:96370ms step_avg:60.65ms
step:1590/2330 train_time:96432ms step_avg:60.65ms
step:1591/2330 train_time:96492ms step_avg:60.65ms
step:1592/2330 train_time:96555ms step_avg:60.65ms
step:1593/2330 train_time:96615ms step_avg:60.65ms
step:1594/2330 train_time:96679ms step_avg:60.65ms
step:1595/2330 train_time:96739ms step_avg:60.65ms
step:1596/2330 train_time:96801ms step_avg:60.65ms
step:1597/2330 train_time:96861ms step_avg:60.65ms
step:1598/2330 train_time:96924ms step_avg:60.65ms
step:1599/2330 train_time:96985ms step_avg:60.65ms
step:1600/2330 train_time:97047ms step_avg:60.65ms
step:1601/2330 train_time:97107ms step_avg:60.65ms
step:1602/2330 train_time:97169ms step_avg:60.65ms
step:1603/2330 train_time:97229ms step_avg:60.65ms
step:1604/2330 train_time:97292ms step_avg:60.66ms
step:1605/2330 train_time:97352ms step_avg:60.66ms
step:1606/2330 train_time:97415ms step_avg:60.66ms
step:1607/2330 train_time:97475ms step_avg:60.66ms
step:1608/2330 train_time:97537ms step_avg:60.66ms
step:1609/2330 train_time:97597ms step_avg:60.66ms
step:1610/2330 train_time:97659ms step_avg:60.66ms
step:1611/2330 train_time:97720ms step_avg:60.66ms
step:1612/2330 train_time:97782ms step_avg:60.66ms
step:1613/2330 train_time:97843ms step_avg:60.66ms
step:1614/2330 train_time:97906ms step_avg:60.66ms
step:1615/2330 train_time:97966ms step_avg:60.66ms
step:1616/2330 train_time:98028ms step_avg:60.66ms
step:1617/2330 train_time:98089ms step_avg:60.66ms
step:1618/2330 train_time:98150ms step_avg:60.66ms
step:1619/2330 train_time:98211ms step_avg:60.66ms
step:1620/2330 train_time:98273ms step_avg:60.66ms
step:1621/2330 train_time:98333ms step_avg:60.66ms
step:1622/2330 train_time:98395ms step_avg:60.66ms
step:1623/2330 train_time:98455ms step_avg:60.66ms
step:1624/2330 train_time:98517ms step_avg:60.66ms
step:1625/2330 train_time:98577ms step_avg:60.66ms
step:1626/2330 train_time:98640ms step_avg:60.66ms
step:1627/2330 train_time:98700ms step_avg:60.66ms
step:1628/2330 train_time:98762ms step_avg:60.66ms
step:1629/2330 train_time:98823ms step_avg:60.66ms
step:1630/2330 train_time:98886ms step_avg:60.67ms
step:1631/2330 train_time:98947ms step_avg:60.67ms
step:1632/2330 train_time:99010ms step_avg:60.67ms
step:1633/2330 train_time:99069ms step_avg:60.67ms
step:1634/2330 train_time:99131ms step_avg:60.67ms
step:1635/2330 train_time:99191ms step_avg:60.67ms
step:1636/2330 train_time:99253ms step_avg:60.67ms
step:1637/2330 train_time:99314ms step_avg:60.67ms
step:1638/2330 train_time:99376ms step_avg:60.67ms
step:1639/2330 train_time:99436ms step_avg:60.67ms
step:1640/2330 train_time:99499ms step_avg:60.67ms
step:1641/2330 train_time:99559ms step_avg:60.67ms
step:1642/2330 train_time:99621ms step_avg:60.67ms
step:1643/2330 train_time:99681ms step_avg:60.67ms
step:1644/2330 train_time:99744ms step_avg:60.67ms
step:1645/2330 train_time:99805ms step_avg:60.67ms
step:1646/2330 train_time:99868ms step_avg:60.67ms
step:1647/2330 train_time:99929ms step_avg:60.67ms
step:1648/2330 train_time:99991ms step_avg:60.67ms
step:1649/2330 train_time:100051ms step_avg:60.67ms
step:1650/2330 train_time:100114ms step_avg:60.67ms
step:1651/2330 train_time:100174ms step_avg:60.67ms
step:1652/2330 train_time:100236ms step_avg:60.68ms
step:1653/2330 train_time:100297ms step_avg:60.68ms
step:1654/2330 train_time:100359ms step_avg:60.68ms
step:1655/2330 train_time:100420ms step_avg:60.68ms
step:1656/2330 train_time:100482ms step_avg:60.68ms
step:1657/2330 train_time:100543ms step_avg:60.68ms
step:1658/2330 train_time:100605ms step_avg:60.68ms
step:1659/2330 train_time:100666ms step_avg:60.68ms
step:1660/2330 train_time:100728ms step_avg:60.68ms
step:1661/2330 train_time:100788ms step_avg:60.68ms
step:1662/2330 train_time:100851ms step_avg:60.68ms
step:1663/2330 train_time:100911ms step_avg:60.68ms
step:1664/2330 train_time:100974ms step_avg:60.68ms
step:1665/2330 train_time:101034ms step_avg:60.68ms
step:1666/2330 train_time:101096ms step_avg:60.68ms
step:1667/2330 train_time:101156ms step_avg:60.68ms
step:1668/2330 train_time:101219ms step_avg:60.68ms
step:1669/2330 train_time:101279ms step_avg:60.68ms
step:1670/2330 train_time:101341ms step_avg:60.68ms
step:1671/2330 train_time:101401ms step_avg:60.68ms
step:1672/2330 train_time:101464ms step_avg:60.68ms
step:1673/2330 train_time:101524ms step_avg:60.68ms
step:1674/2330 train_time:101586ms step_avg:60.68ms
step:1675/2330 train_time:101646ms step_avg:60.68ms
step:1676/2330 train_time:101708ms step_avg:60.69ms
step:1677/2330 train_time:101769ms step_avg:60.69ms
step:1678/2330 train_time:101832ms step_avg:60.69ms
step:1679/2330 train_time:101892ms step_avg:60.69ms
step:1680/2330 train_time:101955ms step_avg:60.69ms
step:1681/2330 train_time:102015ms step_avg:60.69ms
step:1682/2330 train_time:102077ms step_avg:60.69ms
step:1683/2330 train_time:102137ms step_avg:60.69ms
step:1684/2330 train_time:102200ms step_avg:60.69ms
step:1685/2330 train_time:102261ms step_avg:60.69ms
step:1686/2330 train_time:102323ms step_avg:60.69ms
step:1687/2330 train_time:102383ms step_avg:60.69ms
step:1688/2330 train_time:102445ms step_avg:60.69ms
step:1689/2330 train_time:102506ms step_avg:60.69ms
step:1690/2330 train_time:102568ms step_avg:60.69ms
step:1691/2330 train_time:102628ms step_avg:60.69ms
step:1692/2330 train_time:102691ms step_avg:60.69ms
step:1693/2330 train_time:102751ms step_avg:60.69ms
step:1694/2330 train_time:102813ms step_avg:60.69ms
step:1695/2330 train_time:102874ms step_avg:60.69ms
step:1696/2330 train_time:102936ms step_avg:60.69ms
step:1697/2330 train_time:102997ms step_avg:60.69ms
step:1698/2330 train_time:103059ms step_avg:60.69ms
step:1699/2330 train_time:103119ms step_avg:60.69ms
step:1700/2330 train_time:103181ms step_avg:60.69ms
step:1701/2330 train_time:103242ms step_avg:60.69ms
step:1702/2330 train_time:103304ms step_avg:60.70ms
step:1703/2330 train_time:103364ms step_avg:60.69ms
step:1704/2330 train_time:103426ms step_avg:60.70ms
step:1705/2330 train_time:103486ms step_avg:60.70ms
step:1706/2330 train_time:103548ms step_avg:60.70ms
step:1707/2330 train_time:103609ms step_avg:60.70ms
step:1708/2330 train_time:103672ms step_avg:60.70ms
step:1709/2330 train_time:103731ms step_avg:60.70ms
step:1710/2330 train_time:103794ms step_avg:60.70ms
step:1711/2330 train_time:103854ms step_avg:60.70ms
step:1712/2330 train_time:103917ms step_avg:60.70ms
step:1713/2330 train_time:103978ms step_avg:60.70ms
step:1714/2330 train_time:104040ms step_avg:60.70ms
step:1715/2330 train_time:104101ms step_avg:60.70ms
step:1716/2330 train_time:104163ms step_avg:60.70ms
step:1717/2330 train_time:104224ms step_avg:60.70ms
step:1718/2330 train_time:104286ms step_avg:60.70ms
step:1719/2330 train_time:104346ms step_avg:60.70ms
step:1720/2330 train_time:104409ms step_avg:60.70ms
step:1721/2330 train_time:104470ms step_avg:60.70ms
step:1722/2330 train_time:104532ms step_avg:60.70ms
step:1723/2330 train_time:104592ms step_avg:60.70ms
step:1724/2330 train_time:104654ms step_avg:60.70ms
step:1725/2330 train_time:104715ms step_avg:60.70ms
step:1726/2330 train_time:104777ms step_avg:60.71ms
step:1727/2330 train_time:104838ms step_avg:60.71ms
step:1728/2330 train_time:104900ms step_avg:60.71ms
step:1729/2330 train_time:104961ms step_avg:60.71ms
step:1730/2330 train_time:105024ms step_avg:60.71ms
step:1731/2330 train_time:105085ms step_avg:60.71ms
step:1732/2330 train_time:105147ms step_avg:60.71ms
step:1733/2330 train_time:105208ms step_avg:60.71ms
step:1734/2330 train_time:105270ms step_avg:60.71ms
step:1735/2330 train_time:105331ms step_avg:60.71ms
step:1736/2330 train_time:105393ms step_avg:60.71ms
step:1737/2330 train_time:105453ms step_avg:60.71ms
step:1738/2330 train_time:105515ms step_avg:60.71ms
step:1739/2330 train_time:105575ms step_avg:60.71ms
step:1740/2330 train_time:105638ms step_avg:60.71ms
step:1741/2330 train_time:105699ms step_avg:60.71ms
step:1742/2330 train_time:105760ms step_avg:60.71ms
step:1743/2330 train_time:105821ms step_avg:60.71ms
step:1744/2330 train_time:105884ms step_avg:60.71ms
step:1745/2330 train_time:105944ms step_avg:60.71ms
step:1746/2330 train_time:106007ms step_avg:60.71ms
step:1747/2330 train_time:106067ms step_avg:60.71ms
step:1748/2330 train_time:106129ms step_avg:60.71ms
step:1749/2330 train_time:106189ms step_avg:60.71ms
step:1750/2330 train_time:106252ms step_avg:60.72ms
step:1750/2330 val_loss:3.3872 train_time:106316ms step_avg:60.75ms
step:1751/2330 train_time:106342ms step_avg:60.73ms
step:1752/2330 train_time:106376ms step_avg:60.72ms
step:1753/2330 train_time:106443ms step_avg:60.72ms
step:1754/2330 train_time:106508ms step_avg:60.72ms
step:1755/2330 train_time:106568ms step_avg:60.72ms
step:1756/2330 train_time:106630ms step_avg:60.72ms
step:1757/2330 train_time:106689ms step_avg:60.72ms
step:1758/2330 train_time:106750ms step_avg:60.72ms
step:1759/2330 train_time:106809ms step_avg:60.72ms
step:1760/2330 train_time:106871ms step_avg:60.72ms
step:1761/2330 train_time:106930ms step_avg:60.72ms
step:1762/2330 train_time:106991ms step_avg:60.72ms
step:1763/2330 train_time:107050ms step_avg:60.72ms
step:1764/2330 train_time:107111ms step_avg:60.72ms
step:1765/2330 train_time:107171ms step_avg:60.72ms
step:1766/2330 train_time:107237ms step_avg:60.72ms
step:1767/2330 train_time:107300ms step_avg:60.72ms
step:1768/2330 train_time:107365ms step_avg:60.73ms
step:1769/2330 train_time:107427ms step_avg:60.73ms
step:1770/2330 train_time:107490ms step_avg:60.73ms
step:1771/2330 train_time:107550ms step_avg:60.73ms
step:1772/2330 train_time:107612ms step_avg:60.73ms
step:1773/2330 train_time:107672ms step_avg:60.73ms
step:1774/2330 train_time:107733ms step_avg:60.73ms
step:1775/2330 train_time:107792ms step_avg:60.73ms
step:1776/2330 train_time:107853ms step_avg:60.73ms
step:1777/2330 train_time:107913ms step_avg:60.73ms
step:1778/2330 train_time:107974ms step_avg:60.73ms
step:1779/2330 train_time:108033ms step_avg:60.73ms
step:1780/2330 train_time:108095ms step_avg:60.73ms
step:1781/2330 train_time:108155ms step_avg:60.73ms
step:1782/2330 train_time:108218ms step_avg:60.73ms
step:1783/2330 train_time:108279ms step_avg:60.73ms
step:1784/2330 train_time:108342ms step_avg:60.73ms
step:1785/2330 train_time:108403ms step_avg:60.73ms
step:1786/2330 train_time:108466ms step_avg:60.73ms
step:1787/2330 train_time:108526ms step_avg:60.73ms
step:1788/2330 train_time:108589ms step_avg:60.73ms
step:1789/2330 train_time:108648ms step_avg:60.73ms
step:1790/2330 train_time:108711ms step_avg:60.73ms
step:1791/2330 train_time:108770ms step_avg:60.73ms
step:1792/2330 train_time:108832ms step_avg:60.73ms
step:1793/2330 train_time:108892ms step_avg:60.73ms
step:1794/2330 train_time:108954ms step_avg:60.73ms
step:1795/2330 train_time:109013ms step_avg:60.73ms
step:1796/2330 train_time:109075ms step_avg:60.73ms
step:1797/2330 train_time:109135ms step_avg:60.73ms
step:1798/2330 train_time:109197ms step_avg:60.73ms
step:1799/2330 train_time:109258ms step_avg:60.73ms
step:1800/2330 train_time:109321ms step_avg:60.73ms
step:1801/2330 train_time:109382ms step_avg:60.73ms
step:1802/2330 train_time:109444ms step_avg:60.73ms
step:1803/2330 train_time:109506ms step_avg:60.74ms
step:1804/2330 train_time:109568ms step_avg:60.74ms
step:1805/2330 train_time:109628ms step_avg:60.74ms
step:1806/2330 train_time:109690ms step_avg:60.74ms
step:1807/2330 train_time:109750ms step_avg:60.74ms
step:1808/2330 train_time:109812ms step_avg:60.74ms
step:1809/2330 train_time:109872ms step_avg:60.74ms
step:1810/2330 train_time:109933ms step_avg:60.74ms
step:1811/2330 train_time:109993ms step_avg:60.74ms
step:1812/2330 train_time:110055ms step_avg:60.74ms
step:1813/2330 train_time:110115ms step_avg:60.74ms
step:1814/2330 train_time:110177ms step_avg:60.74ms
step:1815/2330 train_time:110237ms step_avg:60.74ms
step:1816/2330 train_time:110299ms step_avg:60.74ms
step:1817/2330 train_time:110360ms step_avg:60.74ms
step:1818/2330 train_time:110423ms step_avg:60.74ms
step:1819/2330 train_time:110483ms step_avg:60.74ms
step:1820/2330 train_time:110545ms step_avg:60.74ms
step:1821/2330 train_time:110606ms step_avg:60.74ms
step:1822/2330 train_time:110669ms step_avg:60.74ms
step:1823/2330 train_time:110729ms step_avg:60.74ms
step:1824/2330 train_time:110792ms step_avg:60.74ms
step:1825/2330 train_time:110852ms step_avg:60.74ms
step:1826/2330 train_time:110914ms step_avg:60.74ms
step:1827/2330 train_time:110973ms step_avg:60.74ms
step:1828/2330 train_time:111035ms step_avg:60.74ms
step:1829/2330 train_time:111096ms step_avg:60.74ms
step:1830/2330 train_time:111157ms step_avg:60.74ms
step:1831/2330 train_time:111217ms step_avg:60.74ms
step:1832/2330 train_time:111279ms step_avg:60.74ms
step:1833/2330 train_time:111340ms step_avg:60.74ms
step:1834/2330 train_time:111402ms step_avg:60.74ms
step:1835/2330 train_time:111462ms step_avg:60.74ms
step:1836/2330 train_time:111525ms step_avg:60.74ms
step:1837/2330 train_time:111585ms step_avg:60.74ms
step:1838/2330 train_time:111648ms step_avg:60.74ms
step:1839/2330 train_time:111708ms step_avg:60.74ms
step:1840/2330 train_time:111771ms step_avg:60.74ms
step:1841/2330 train_time:111831ms step_avg:60.74ms
step:1842/2330 train_time:111893ms step_avg:60.75ms
step:1843/2330 train_time:111953ms step_avg:60.74ms
step:1844/2330 train_time:112015ms step_avg:60.75ms
step:1845/2330 train_time:112076ms step_avg:60.75ms
step:1846/2330 train_time:112138ms step_avg:60.75ms
step:1847/2330 train_time:112198ms step_avg:60.75ms
step:1848/2330 train_time:112260ms step_avg:60.75ms
step:1849/2330 train_time:112320ms step_avg:60.75ms
step:1850/2330 train_time:112382ms step_avg:60.75ms
step:1851/2330 train_time:112442ms step_avg:60.75ms
step:1852/2330 train_time:112505ms step_avg:60.75ms
step:1853/2330 train_time:112565ms step_avg:60.75ms
step:1854/2330 train_time:112627ms step_avg:60.75ms
step:1855/2330 train_time:112688ms step_avg:60.75ms
step:1856/2330 train_time:112750ms step_avg:60.75ms
step:1857/2330 train_time:112810ms step_avg:60.75ms
step:1858/2330 train_time:112873ms step_avg:60.75ms
step:1859/2330 train_time:112934ms step_avg:60.75ms
step:1860/2330 train_time:112996ms step_avg:60.75ms
step:1861/2330 train_time:113056ms step_avg:60.75ms
step:1862/2330 train_time:113119ms step_avg:60.75ms
step:1863/2330 train_time:113179ms step_avg:60.75ms
step:1864/2330 train_time:113241ms step_avg:60.75ms
step:1865/2330 train_time:113301ms step_avg:60.75ms
step:1866/2330 train_time:113363ms step_avg:60.75ms
step:1867/2330 train_time:113424ms step_avg:60.75ms
step:1868/2330 train_time:113486ms step_avg:60.75ms
step:1869/2330 train_time:113546ms step_avg:60.75ms
step:1870/2330 train_time:113608ms step_avg:60.75ms
step:1871/2330 train_time:113668ms step_avg:60.75ms
step:1872/2330 train_time:113730ms step_avg:60.75ms
step:1873/2330 train_time:113791ms step_avg:60.75ms
step:1874/2330 train_time:113853ms step_avg:60.75ms
step:1875/2330 train_time:113913ms step_avg:60.75ms
step:1876/2330 train_time:113975ms step_avg:60.75ms
step:1877/2330 train_time:114036ms step_avg:60.75ms
step:1878/2330 train_time:114098ms step_avg:60.76ms
step:1879/2330 train_time:114158ms step_avg:60.75ms
step:1880/2330 train_time:114220ms step_avg:60.76ms
step:1881/2330 train_time:114280ms step_avg:60.75ms
step:1882/2330 train_time:114342ms step_avg:60.76ms
step:1883/2330 train_time:114403ms step_avg:60.76ms
step:1884/2330 train_time:114465ms step_avg:60.76ms
step:1885/2330 train_time:114526ms step_avg:60.76ms
step:1886/2330 train_time:114588ms step_avg:60.76ms
step:1887/2330 train_time:114648ms step_avg:60.76ms
step:1888/2330 train_time:114711ms step_avg:60.76ms
step:1889/2330 train_time:114771ms step_avg:60.76ms
step:1890/2330 train_time:114833ms step_avg:60.76ms
step:1891/2330 train_time:114894ms step_avg:60.76ms
step:1892/2330 train_time:114956ms step_avg:60.76ms
step:1893/2330 train_time:115016ms step_avg:60.76ms
step:1894/2330 train_time:115078ms step_avg:60.76ms
step:1895/2330 train_time:115139ms step_avg:60.76ms
step:1896/2330 train_time:115201ms step_avg:60.76ms
step:1897/2330 train_time:115260ms step_avg:60.76ms
step:1898/2330 train_time:115322ms step_avg:60.76ms
step:1899/2330 train_time:115382ms step_avg:60.76ms
step:1900/2330 train_time:115445ms step_avg:60.76ms
step:1901/2330 train_time:115506ms step_avg:60.76ms
step:1902/2330 train_time:115568ms step_avg:60.76ms
step:1903/2330 train_time:115629ms step_avg:60.76ms
step:1904/2330 train_time:115691ms step_avg:60.76ms
step:1905/2330 train_time:115751ms step_avg:60.76ms
step:1906/2330 train_time:115814ms step_avg:60.76ms
step:1907/2330 train_time:115875ms step_avg:60.76ms
step:1908/2330 train_time:115936ms step_avg:60.76ms
step:1909/2330 train_time:115996ms step_avg:60.76ms
step:1910/2330 train_time:116058ms step_avg:60.76ms
step:1911/2330 train_time:116119ms step_avg:60.76ms
step:1912/2330 train_time:116181ms step_avg:60.76ms
step:1913/2330 train_time:116241ms step_avg:60.76ms
step:1914/2330 train_time:116303ms step_avg:60.76ms
step:1915/2330 train_time:116363ms step_avg:60.76ms
step:1916/2330 train_time:116425ms step_avg:60.76ms
step:1917/2330 train_time:116485ms step_avg:60.76ms
step:1918/2330 train_time:116548ms step_avg:60.77ms
step:1919/2330 train_time:116608ms step_avg:60.76ms
step:1920/2330 train_time:116671ms step_avg:60.77ms
step:1921/2330 train_time:116731ms step_avg:60.77ms
step:1922/2330 train_time:116794ms step_avg:60.77ms
step:1923/2330 train_time:116854ms step_avg:60.77ms
step:1924/2330 train_time:116916ms step_avg:60.77ms
step:1925/2330 train_time:116976ms step_avg:60.77ms
step:1926/2330 train_time:117039ms step_avg:60.77ms
step:1927/2330 train_time:117099ms step_avg:60.77ms
step:1928/2330 train_time:117161ms step_avg:60.77ms
step:1929/2330 train_time:117221ms step_avg:60.77ms
step:1930/2330 train_time:117283ms step_avg:60.77ms
step:1931/2330 train_time:117344ms step_avg:60.77ms
step:1932/2330 train_time:117407ms step_avg:60.77ms
step:1933/2330 train_time:117467ms step_avg:60.77ms
step:1934/2330 train_time:117529ms step_avg:60.77ms
step:1935/2330 train_time:117589ms step_avg:60.77ms
step:1936/2330 train_time:117652ms step_avg:60.77ms
step:1937/2330 train_time:117712ms step_avg:60.77ms
step:1938/2330 train_time:117775ms step_avg:60.77ms
step:1939/2330 train_time:117836ms step_avg:60.77ms
step:1940/2330 train_time:117898ms step_avg:60.77ms
step:1941/2330 train_time:117958ms step_avg:60.77ms
step:1942/2330 train_time:118020ms step_avg:60.77ms
step:1943/2330 train_time:118081ms step_avg:60.77ms
step:1944/2330 train_time:118143ms step_avg:60.77ms
step:1945/2330 train_time:118203ms step_avg:60.77ms
step:1946/2330 train_time:118266ms step_avg:60.77ms
step:1947/2330 train_time:118326ms step_avg:60.77ms
step:1948/2330 train_time:118388ms step_avg:60.77ms
step:1949/2330 train_time:118448ms step_avg:60.77ms
step:1950/2330 train_time:118511ms step_avg:60.77ms
step:1951/2330 train_time:118570ms step_avg:60.77ms
step:1952/2330 train_time:118633ms step_avg:60.77ms
step:1953/2330 train_time:118692ms step_avg:60.77ms
step:1954/2330 train_time:118755ms step_avg:60.78ms
step:1955/2330 train_time:118815ms step_avg:60.78ms
step:1956/2330 train_time:118878ms step_avg:60.78ms
step:1957/2330 train_time:118938ms step_avg:60.78ms
step:1958/2330 train_time:119000ms step_avg:60.78ms
step:1959/2330 train_time:119060ms step_avg:60.78ms
step:1960/2330 train_time:119123ms step_avg:60.78ms
step:1961/2330 train_time:119183ms step_avg:60.78ms
step:1962/2330 train_time:119245ms step_avg:60.78ms
step:1963/2330 train_time:119305ms step_avg:60.78ms
step:1964/2330 train_time:119367ms step_avg:60.78ms
step:1965/2330 train_time:119427ms step_avg:60.78ms
step:1966/2330 train_time:119490ms step_avg:60.78ms
step:1967/2330 train_time:119550ms step_avg:60.78ms
step:1968/2330 train_time:119613ms step_avg:60.78ms
step:1969/2330 train_time:119673ms step_avg:60.78ms
step:1970/2330 train_time:119735ms step_avg:60.78ms
step:1971/2330 train_time:119796ms step_avg:60.78ms
step:1972/2330 train_time:119858ms step_avg:60.78ms
step:1973/2330 train_time:119918ms step_avg:60.78ms
step:1974/2330 train_time:119980ms step_avg:60.78ms
step:1975/2330 train_time:120040ms step_avg:60.78ms
step:1976/2330 train_time:120103ms step_avg:60.78ms
step:1977/2330 train_time:120163ms step_avg:60.78ms
step:1978/2330 train_time:120225ms step_avg:60.78ms
step:1979/2330 train_time:120286ms step_avg:60.78ms
step:1980/2330 train_time:120348ms step_avg:60.78ms
step:1981/2330 train_time:120408ms step_avg:60.78ms
step:1982/2330 train_time:120471ms step_avg:60.78ms
step:1983/2330 train_time:120531ms step_avg:60.78ms
step:1984/2330 train_time:120593ms step_avg:60.78ms
step:1985/2330 train_time:120654ms step_avg:60.78ms
step:1986/2330 train_time:120716ms step_avg:60.78ms
step:1987/2330 train_time:120776ms step_avg:60.78ms
step:1988/2330 train_time:120838ms step_avg:60.78ms
step:1989/2330 train_time:120898ms step_avg:60.78ms
step:1990/2330 train_time:120960ms step_avg:60.78ms
step:1991/2330 train_time:121020ms step_avg:60.78ms
step:1992/2330 train_time:121083ms step_avg:60.78ms
step:1993/2330 train_time:121143ms step_avg:60.78ms
step:1994/2330 train_time:121205ms step_avg:60.78ms
step:1995/2330 train_time:121265ms step_avg:60.78ms
step:1996/2330 train_time:121327ms step_avg:60.79ms
step:1997/2330 train_time:121388ms step_avg:60.79ms
step:1998/2330 train_time:121451ms step_avg:60.79ms
step:1999/2330 train_time:121512ms step_avg:60.79ms
step:2000/2330 train_time:121575ms step_avg:60.79ms
step:2000/2330 val_loss:3.3380 train_time:121639ms step_avg:60.82ms
step:2001/2330 train_time:121665ms step_avg:60.80ms
step:2002/2330 train_time:121700ms step_avg:60.79ms
step:2003/2330 train_time:121763ms step_avg:60.79ms
step:2004/2330 train_time:121829ms step_avg:60.79ms
step:2005/2330 train_time:121889ms step_avg:60.79ms
step:2006/2330 train_time:121952ms step_avg:60.79ms
step:2007/2330 train_time:122012ms step_avg:60.79ms
step:2008/2330 train_time:122074ms step_avg:60.79ms
step:2009/2330 train_time:122134ms step_avg:60.79ms
step:2010/2330 train_time:122195ms step_avg:60.79ms
step:2011/2330 train_time:122254ms step_avg:60.79ms
step:2012/2330 train_time:122316ms step_avg:60.79ms
step:2013/2330 train_time:122376ms step_avg:60.79ms
step:2014/2330 train_time:122438ms step_avg:60.79ms
step:2015/2330 train_time:122497ms step_avg:60.79ms
step:2016/2330 train_time:122559ms step_avg:60.79ms
step:2017/2330 train_time:122620ms step_avg:60.79ms
step:2018/2330 train_time:122683ms step_avg:60.79ms
step:2019/2330 train_time:122745ms step_avg:60.79ms
step:2020/2330 train_time:122808ms step_avg:60.80ms
step:2021/2330 train_time:122869ms step_avg:60.80ms
step:2022/2330 train_time:122932ms step_avg:60.80ms
step:2023/2330 train_time:122992ms step_avg:60.80ms
step:2024/2330 train_time:123054ms step_avg:60.80ms
step:2025/2330 train_time:123114ms step_avg:60.80ms
step:2026/2330 train_time:123175ms step_avg:60.80ms
step:2027/2330 train_time:123235ms step_avg:60.80ms
step:2028/2330 train_time:123296ms step_avg:60.80ms
step:2029/2330 train_time:123355ms step_avg:60.80ms
step:2030/2330 train_time:123417ms step_avg:60.80ms
step:2031/2330 train_time:123477ms step_avg:60.80ms
step:2032/2330 train_time:123539ms step_avg:60.80ms
step:2033/2330 train_time:123599ms step_avg:60.80ms
step:2034/2330 train_time:123662ms step_avg:60.80ms
step:2035/2330 train_time:123722ms step_avg:60.80ms
step:2036/2330 train_time:123784ms step_avg:60.80ms
step:2037/2330 train_time:123845ms step_avg:60.80ms
step:2038/2330 train_time:123909ms step_avg:60.80ms
step:2039/2330 train_time:123970ms step_avg:60.80ms
step:2040/2330 train_time:124032ms step_avg:60.80ms
step:2041/2330 train_time:124092ms step_avg:60.80ms
step:2042/2330 train_time:124154ms step_avg:60.80ms
step:2043/2330 train_time:124215ms step_avg:60.80ms
step:2044/2330 train_time:124276ms step_avg:60.80ms
step:2045/2330 train_time:124336ms step_avg:60.80ms
step:2046/2330 train_time:124398ms step_avg:60.80ms
step:2047/2330 train_time:124458ms step_avg:60.80ms
step:2048/2330 train_time:124520ms step_avg:60.80ms
step:2049/2330 train_time:124581ms step_avg:60.80ms
step:2050/2330 train_time:124644ms step_avg:60.80ms
step:2051/2330 train_time:124704ms step_avg:60.80ms
step:2052/2330 train_time:124767ms step_avg:60.80ms
step:2053/2330 train_time:124827ms step_avg:60.80ms
step:2054/2330 train_time:124890ms step_avg:60.80ms
step:2055/2330 train_time:124950ms step_avg:60.80ms
step:2056/2330 train_time:125013ms step_avg:60.80ms
step:2057/2330 train_time:125072ms step_avg:60.80ms
step:2058/2330 train_time:125135ms step_avg:60.80ms
step:2059/2330 train_time:125195ms step_avg:60.80ms
step:2060/2330 train_time:125257ms step_avg:60.80ms
step:2061/2330 train_time:125317ms step_avg:60.80ms
step:2062/2330 train_time:125379ms step_avg:60.80ms
step:2063/2330 train_time:125440ms step_avg:60.80ms
step:2064/2330 train_time:125501ms step_avg:60.80ms
step:2065/2330 train_time:125561ms step_avg:60.80ms
step:2066/2330 train_time:125623ms step_avg:60.81ms
step:2067/2330 train_time:125683ms step_avg:60.80ms
step:2068/2330 train_time:125746ms step_avg:60.81ms
step:2069/2330 train_time:125806ms step_avg:60.81ms
step:2070/2330 train_time:125868ms step_avg:60.81ms
step:2071/2330 train_time:125929ms step_avg:60.81ms
step:2072/2330 train_time:125991ms step_avg:60.81ms
step:2073/2330 train_time:126052ms step_avg:60.81ms
step:2074/2330 train_time:126114ms step_avg:60.81ms
step:2075/2330 train_time:126175ms step_avg:60.81ms
step:2076/2330 train_time:126236ms step_avg:60.81ms
step:2077/2330 train_time:126297ms step_avg:60.81ms
step:2078/2330 train_time:126359ms step_avg:60.81ms
step:2079/2330 train_time:126419ms step_avg:60.81ms
step:2080/2330 train_time:126481ms step_avg:60.81ms
step:2081/2330 train_time:126542ms step_avg:60.81ms
step:2082/2330 train_time:126603ms step_avg:60.81ms
step:2083/2330 train_time:126664ms step_avg:60.81ms
step:2084/2330 train_time:126726ms step_avg:60.81ms
step:2085/2330 train_time:126786ms step_avg:60.81ms
step:2086/2330 train_time:126848ms step_avg:60.81ms
step:2087/2330 train_time:126909ms step_avg:60.81ms
step:2088/2330 train_time:126971ms step_avg:60.81ms
step:2089/2330 train_time:127031ms step_avg:60.81ms
step:2090/2330 train_time:127094ms step_avg:60.81ms
step:2091/2330 train_time:127154ms step_avg:60.81ms
step:2092/2330 train_time:127216ms step_avg:60.81ms
step:2093/2330 train_time:127276ms step_avg:60.81ms
step:2094/2330 train_time:127339ms step_avg:60.81ms
step:2095/2330 train_time:127399ms step_avg:60.81ms
step:2096/2330 train_time:127461ms step_avg:60.81ms
step:2097/2330 train_time:127521ms step_avg:60.81ms
step:2098/2330 train_time:127583ms step_avg:60.81ms
step:2099/2330 train_time:127643ms step_avg:60.81ms
step:2100/2330 train_time:127705ms step_avg:60.81ms
step:2101/2330 train_time:127765ms step_avg:60.81ms
step:2102/2330 train_time:127827ms step_avg:60.81ms
step:2103/2330 train_time:127889ms step_avg:60.81ms
step:2104/2330 train_time:127951ms step_avg:60.81ms
step:2105/2330 train_time:128011ms step_avg:60.81ms
step:2106/2330 train_time:128073ms step_avg:60.81ms
step:2107/2330 train_time:128134ms step_avg:60.81ms
step:2108/2330 train_time:128196ms step_avg:60.81ms
step:2109/2330 train_time:128257ms step_avg:60.81ms
step:2110/2330 train_time:128319ms step_avg:60.81ms
step:2111/2330 train_time:128379ms step_avg:60.81ms
step:2112/2330 train_time:128441ms step_avg:60.82ms
step:2113/2330 train_time:128501ms step_avg:60.81ms
step:2114/2330 train_time:128563ms step_avg:60.82ms
step:2115/2330 train_time:128623ms step_avg:60.81ms
step:2116/2330 train_time:128685ms step_avg:60.82ms
step:2117/2330 train_time:128745ms step_avg:60.81ms
step:2118/2330 train_time:128808ms step_avg:60.82ms
step:2119/2330 train_time:128868ms step_avg:60.82ms
step:2120/2330 train_time:128931ms step_avg:60.82ms
step:2121/2330 train_time:128991ms step_avg:60.82ms
step:2122/2330 train_time:129053ms step_avg:60.82ms
step:2123/2330 train_time:129113ms step_avg:60.82ms
step:2124/2330 train_time:129176ms step_avg:60.82ms
step:2125/2330 train_time:129236ms step_avg:60.82ms
step:2126/2330 train_time:129298ms step_avg:60.82ms
step:2127/2330 train_time:129359ms step_avg:60.82ms
step:2128/2330 train_time:129421ms step_avg:60.82ms
step:2129/2330 train_time:129481ms step_avg:60.82ms
step:2130/2330 train_time:129544ms step_avg:60.82ms
step:2131/2330 train_time:129604ms step_avg:60.82ms
step:2132/2330 train_time:129666ms step_avg:60.82ms
step:2133/2330 train_time:129726ms step_avg:60.82ms
step:2134/2330 train_time:129789ms step_avg:60.82ms
step:2135/2330 train_time:129849ms step_avg:60.82ms
step:2136/2330 train_time:129911ms step_avg:60.82ms
step:2137/2330 train_time:129971ms step_avg:60.82ms
step:2138/2330 train_time:130034ms step_avg:60.82ms
step:2139/2330 train_time:130094ms step_avg:60.82ms
step:2140/2330 train_time:130157ms step_avg:60.82ms
step:2141/2330 train_time:130218ms step_avg:60.82ms
step:2142/2330 train_time:130280ms step_avg:60.82ms
step:2143/2330 train_time:130340ms step_avg:60.82ms
step:2144/2330 train_time:130402ms step_avg:60.82ms
step:2145/2330 train_time:130462ms step_avg:60.82ms
step:2146/2330 train_time:130524ms step_avg:60.82ms
step:2147/2330 train_time:130584ms step_avg:60.82ms
step:2148/2330 train_time:130646ms step_avg:60.82ms
step:2149/2330 train_time:130707ms step_avg:60.82ms
step:2150/2330 train_time:130769ms step_avg:60.82ms
step:2151/2330 train_time:130830ms step_avg:60.82ms
step:2152/2330 train_time:130892ms step_avg:60.82ms
step:2153/2330 train_time:130952ms step_avg:60.82ms
step:2154/2330 train_time:131015ms step_avg:60.82ms
step:2155/2330 train_time:131076ms step_avg:60.82ms
step:2156/2330 train_time:131138ms step_avg:60.82ms
step:2157/2330 train_time:131198ms step_avg:60.82ms
step:2158/2330 train_time:131261ms step_avg:60.83ms
step:2159/2330 train_time:131321ms step_avg:60.83ms
step:2160/2330 train_time:131383ms step_avg:60.83ms
step:2161/2330 train_time:131444ms step_avg:60.83ms
step:2162/2330 train_time:131506ms step_avg:60.83ms
step:2163/2330 train_time:131566ms step_avg:60.83ms
step:2164/2330 train_time:131629ms step_avg:60.83ms
step:2165/2330 train_time:131689ms step_avg:60.83ms
step:2166/2330 train_time:131752ms step_avg:60.83ms
step:2167/2330 train_time:131811ms step_avg:60.83ms
step:2168/2330 train_time:131874ms step_avg:60.83ms
step:2169/2330 train_time:131934ms step_avg:60.83ms
step:2170/2330 train_time:131997ms step_avg:60.83ms
step:2171/2330 train_time:132057ms step_avg:60.83ms
step:2172/2330 train_time:132120ms step_avg:60.83ms
step:2173/2330 train_time:132180ms step_avg:60.83ms
step:2174/2330 train_time:132242ms step_avg:60.83ms
step:2175/2330 train_time:132302ms step_avg:60.83ms
step:2176/2330 train_time:132364ms step_avg:60.83ms
step:2177/2330 train_time:132424ms step_avg:60.83ms
step:2178/2330 train_time:132487ms step_avg:60.83ms
step:2179/2330 train_time:132547ms step_avg:60.83ms
step:2180/2330 train_time:132609ms step_avg:60.83ms
step:2181/2330 train_time:132669ms step_avg:60.83ms
step:2182/2330 train_time:132732ms step_avg:60.83ms
step:2183/2330 train_time:132792ms step_avg:60.83ms
step:2184/2330 train_time:132855ms step_avg:60.83ms
step:2185/2330 train_time:132915ms step_avg:60.83ms
step:2186/2330 train_time:132977ms step_avg:60.83ms
step:2187/2330 train_time:133038ms step_avg:60.83ms
step:2188/2330 train_time:133100ms step_avg:60.83ms
step:2189/2330 train_time:133160ms step_avg:60.83ms
step:2190/2330 train_time:133223ms step_avg:60.83ms
step:2191/2330 train_time:133282ms step_avg:60.83ms
step:2192/2330 train_time:133345ms step_avg:60.83ms
step:2193/2330 train_time:133405ms step_avg:60.83ms
step:2194/2330 train_time:133468ms step_avg:60.83ms
step:2195/2330 train_time:133528ms step_avg:60.83ms
step:2196/2330 train_time:133590ms step_avg:60.83ms
step:2197/2330 train_time:133650ms step_avg:60.83ms
step:2198/2330 train_time:133712ms step_avg:60.83ms
step:2199/2330 train_time:133772ms step_avg:60.83ms
step:2200/2330 train_time:133834ms step_avg:60.83ms
step:2201/2330 train_time:133894ms step_avg:60.83ms
step:2202/2330 train_time:133957ms step_avg:60.83ms
step:2203/2330 train_time:134017ms step_avg:60.83ms
step:2204/2330 train_time:134079ms step_avg:60.83ms
step:2205/2330 train_time:134139ms step_avg:60.83ms
step:2206/2330 train_time:134202ms step_avg:60.83ms
step:2207/2330 train_time:134262ms step_avg:60.83ms
step:2208/2330 train_time:134324ms step_avg:60.84ms
step:2209/2330 train_time:134384ms step_avg:60.83ms
step:2210/2330 train_time:134447ms step_avg:60.84ms
step:2211/2330 train_time:134507ms step_avg:60.84ms
step:2212/2330 train_time:134570ms step_avg:60.84ms
step:2213/2330 train_time:134630ms step_avg:60.84ms
step:2214/2330 train_time:134692ms step_avg:60.84ms
step:2215/2330 train_time:134753ms step_avg:60.84ms
step:2216/2330 train_time:134815ms step_avg:60.84ms
step:2217/2330 train_time:134876ms step_avg:60.84ms
step:2218/2330 train_time:134938ms step_avg:60.84ms
step:2219/2330 train_time:134998ms step_avg:60.84ms
step:2220/2330 train_time:135060ms step_avg:60.84ms
step:2221/2330 train_time:135120ms step_avg:60.84ms
step:2222/2330 train_time:135181ms step_avg:60.84ms
step:2223/2330 train_time:135241ms step_avg:60.84ms
step:2224/2330 train_time:135304ms step_avg:60.84ms
step:2225/2330 train_time:135364ms step_avg:60.84ms
step:2226/2330 train_time:135426ms step_avg:60.84ms
step:2227/2330 train_time:135486ms step_avg:60.84ms
step:2228/2330 train_time:135549ms step_avg:60.84ms
step:2229/2330 train_time:135608ms step_avg:60.84ms
step:2230/2330 train_time:135670ms step_avg:60.84ms
step:2231/2330 train_time:135731ms step_avg:60.84ms
step:2232/2330 train_time:135794ms step_avg:60.84ms
step:2233/2330 train_time:135855ms step_avg:60.84ms
step:2234/2330 train_time:135917ms step_avg:60.84ms
step:2235/2330 train_time:135978ms step_avg:60.84ms
step:2236/2330 train_time:136040ms step_avg:60.84ms
step:2237/2330 train_time:136101ms step_avg:60.84ms
step:2238/2330 train_time:136163ms step_avg:60.84ms
step:2239/2330 train_time:136223ms step_avg:60.84ms
step:2240/2330 train_time:136286ms step_avg:60.84ms
step:2241/2330 train_time:136345ms step_avg:60.84ms
step:2242/2330 train_time:136407ms step_avg:60.84ms
step:2243/2330 train_time:136468ms step_avg:60.84ms
step:2244/2330 train_time:136531ms step_avg:60.84ms
step:2245/2330 train_time:136591ms step_avg:60.84ms
step:2246/2330 train_time:136654ms step_avg:60.84ms
step:2247/2330 train_time:136714ms step_avg:60.84ms
step:2248/2330 train_time:136776ms step_avg:60.84ms
step:2249/2330 train_time:136837ms step_avg:60.84ms
step:2250/2330 train_time:136900ms step_avg:60.84ms
step:2250/2330 val_loss:3.2987 train_time:136964ms step_avg:60.87ms
step:2251/2330 train_time:136990ms step_avg:60.86ms
step:2252/2330 train_time:137025ms step_avg:60.85ms
step:2253/2330 train_time:137090ms step_avg:60.85ms
step:2254/2330 train_time:137154ms step_avg:60.85ms
step:2255/2330 train_time:137215ms step_avg:60.85ms
step:2256/2330 train_time:137278ms step_avg:60.85ms
step:2257/2330 train_time:137337ms step_avg:60.85ms
step:2258/2330 train_time:137399ms step_avg:60.85ms
step:2259/2330 train_time:137458ms step_avg:60.85ms
step:2260/2330 train_time:137519ms step_avg:60.85ms
step:2261/2330 train_time:137579ms step_avg:60.85ms
step:2262/2330 train_time:137641ms step_avg:60.85ms
step:2263/2330 train_time:137701ms step_avg:60.85ms
step:2264/2330 train_time:137763ms step_avg:60.85ms
step:2265/2330 train_time:137822ms step_avg:60.85ms
step:2266/2330 train_time:137885ms step_avg:60.85ms
step:2267/2330 train_time:137946ms step_avg:60.85ms
step:2268/2330 train_time:138010ms step_avg:60.85ms
step:2269/2330 train_time:138073ms step_avg:60.85ms
step:2270/2330 train_time:138137ms step_avg:60.85ms
step:2271/2330 train_time:138198ms step_avg:60.85ms
step:2272/2330 train_time:138261ms step_avg:60.85ms
step:2273/2330 train_time:138321ms step_avg:60.85ms
step:2274/2330 train_time:138383ms step_avg:60.85ms
step:2275/2330 train_time:138443ms step_avg:60.85ms
step:2276/2330 train_time:138505ms step_avg:60.85ms
step:2277/2330 train_time:138564ms step_avg:60.85ms
step:2278/2330 train_time:138626ms step_avg:60.85ms
step:2279/2330 train_time:138686ms step_avg:60.85ms
step:2280/2330 train_time:138748ms step_avg:60.85ms
step:2281/2330 train_time:138808ms step_avg:60.85ms
step:2282/2330 train_time:138870ms step_avg:60.85ms
step:2283/2330 train_time:138930ms step_avg:60.85ms
step:2284/2330 train_time:138993ms step_avg:60.86ms
step:2285/2330 train_time:139055ms step_avg:60.86ms
step:2286/2330 train_time:139118ms step_avg:60.86ms
step:2287/2330 train_time:139179ms step_avg:60.86ms
step:2288/2330 train_time:139243ms step_avg:60.86ms
step:2289/2330 train_time:139303ms step_avg:60.86ms
step:2290/2330 train_time:139365ms step_avg:60.86ms
step:2291/2330 train_time:139426ms step_avg:60.86ms
step:2292/2330 train_time:139488ms step_avg:60.86ms
step:2293/2330 train_time:139548ms step_avg:60.86ms
step:2294/2330 train_time:139610ms step_avg:60.86ms
step:2295/2330 train_time:139670ms step_avg:60.86ms
step:2296/2330 train_time:139731ms step_avg:60.86ms
step:2297/2330 train_time:139792ms step_avg:60.86ms
step:2298/2330 train_time:139853ms step_avg:60.86ms
step:2299/2330 train_time:139913ms step_avg:60.86ms
step:2300/2330 train_time:139975ms step_avg:60.86ms
step:2301/2330 train_time:140036ms step_avg:60.86ms
step:2302/2330 train_time:140099ms step_avg:60.86ms
step:2303/2330 train_time:140160ms step_avg:60.86ms
step:2304/2330 train_time:140222ms step_avg:60.86ms
step:2305/2330 train_time:140283ms step_avg:60.86ms
step:2306/2330 train_time:140345ms step_avg:60.86ms
step:2307/2330 train_time:140406ms step_avg:60.86ms
step:2308/2330 train_time:140468ms step_avg:60.86ms
step:2309/2330 train_time:140528ms step_avg:60.86ms
step:2310/2330 train_time:140590ms step_avg:60.86ms
step:2311/2330 train_time:140650ms step_avg:60.86ms
step:2312/2330 train_time:140712ms step_avg:60.86ms
step:2313/2330 train_time:140772ms step_avg:60.86ms
step:2314/2330 train_time:140834ms step_avg:60.86ms
step:2315/2330 train_time:140894ms step_avg:60.86ms
step:2316/2330 train_time:140956ms step_avg:60.86ms
step:2317/2330 train_time:141016ms step_avg:60.86ms
step:2318/2330 train_time:141078ms step_avg:60.86ms
step:2319/2330 train_time:141138ms step_avg:60.86ms
step:2320/2330 train_time:141201ms step_avg:60.86ms
step:2321/2330 train_time:141262ms step_avg:60.86ms
step:2322/2330 train_time:141325ms step_avg:60.86ms
step:2323/2330 train_time:141385ms step_avg:60.86ms
step:2324/2330 train_time:141448ms step_avg:60.86ms
step:2325/2330 train_time:141508ms step_avg:60.86ms
step:2326/2330 train_time:141570ms step_avg:60.86ms
step:2327/2330 train_time:141630ms step_avg:60.86ms
step:2328/2330 train_time:141692ms step_avg:60.86ms
step:2329/2330 train_time:141752ms step_avg:60.86ms
step:2330/2330 train_time:141814ms step_avg:60.86ms
step:2330/2330 val_loss:3.2851 train_time:141879ms step_avg:60.89ms
peak memory allocated: 29512 MiB reserved: 44076 MiB
[SV] Saved singular values for 62 params to logs_muon_small/4002c5a3-d02c-41b1-86ac-52838b46ee2a/singular_values.pkl
