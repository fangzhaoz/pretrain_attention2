import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--beta1', type=str, required=True)
parser.add_argument('--beta2', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_beta_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=6e-4,  betas=(float(args2.beta1), float(args2.beta2)), eps=1e-10, weight_decay=0.0, fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt", exist_ok=True)
            torch.save(log, f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 04:17:19 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   26C    P0             115W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   25C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   27C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:108ms step_avg:108.25ms
step:2/2330 train_time:200ms step_avg:99.93ms
step:3/2330 train_time:218ms step_avg:72.68ms
step:4/2330 train_time:237ms step_avg:59.18ms
step:5/2330 train_time:290ms step_avg:58.08ms
step:6/2330 train_time:349ms step_avg:58.10ms
step:7/2330 train_time:403ms step_avg:57.56ms
step:8/2330 train_time:462ms step_avg:57.76ms
step:9/2330 train_time:517ms step_avg:57.48ms
step:10/2330 train_time:577ms step_avg:57.65ms
step:11/2330 train_time:632ms step_avg:57.43ms
step:12/2330 train_time:690ms step_avg:57.53ms
step:13/2330 train_time:746ms step_avg:57.39ms
step:14/2330 train_time:804ms step_avg:57.43ms
step:15/2330 train_time:859ms step_avg:57.25ms
step:16/2330 train_time:918ms step_avg:57.37ms
step:17/2330 train_time:973ms step_avg:57.24ms
step:18/2330 train_time:1033ms step_avg:57.39ms
step:19/2330 train_time:1091ms step_avg:57.43ms
step:20/2330 train_time:1152ms step_avg:57.61ms
step:21/2330 train_time:1209ms step_avg:57.56ms
step:22/2330 train_time:1271ms step_avg:57.76ms
step:23/2330 train_time:1326ms step_avg:57.67ms
step:24/2330 train_time:1386ms step_avg:57.74ms
step:25/2330 train_time:1441ms step_avg:57.63ms
step:26/2330 train_time:1500ms step_avg:57.68ms
step:27/2330 train_time:1555ms step_avg:57.60ms
step:28/2330 train_time:1614ms step_avg:57.64ms
step:29/2330 train_time:1669ms step_avg:57.55ms
step:30/2330 train_time:1728ms step_avg:57.59ms
step:31/2330 train_time:1783ms step_avg:57.53ms
step:32/2330 train_time:1842ms step_avg:57.56ms
step:33/2330 train_time:1897ms step_avg:57.49ms
step:34/2330 train_time:1957ms step_avg:57.56ms
step:35/2330 train_time:2013ms step_avg:57.52ms
step:36/2330 train_time:2073ms step_avg:57.57ms
step:37/2330 train_time:2130ms step_avg:57.57ms
step:38/2330 train_time:2189ms step_avg:57.61ms
step:39/2330 train_time:2246ms step_avg:57.59ms
step:40/2330 train_time:2305ms step_avg:57.64ms
step:41/2330 train_time:2361ms step_avg:57.60ms
step:42/2330 train_time:2421ms step_avg:57.65ms
step:43/2330 train_time:2477ms step_avg:57.60ms
step:44/2330 train_time:2535ms step_avg:57.61ms
step:45/2330 train_time:2590ms step_avg:57.55ms
step:46/2330 train_time:2649ms step_avg:57.60ms
step:47/2330 train_time:2704ms step_avg:57.54ms
step:48/2330 train_time:2763ms step_avg:57.57ms
step:49/2330 train_time:2818ms step_avg:57.51ms
step:50/2330 train_time:2877ms step_avg:57.54ms
step:51/2330 train_time:2933ms step_avg:57.51ms
step:52/2330 train_time:2992ms step_avg:57.53ms
step:53/2330 train_time:3047ms step_avg:57.50ms
step:54/2330 train_time:3107ms step_avg:57.54ms
step:55/2330 train_time:3164ms step_avg:57.52ms
step:56/2330 train_time:3223ms step_avg:57.56ms
step:57/2330 train_time:3280ms step_avg:57.54ms
step:58/2330 train_time:3339ms step_avg:57.57ms
step:59/2330 train_time:3396ms step_avg:57.56ms
step:60/2330 train_time:3454ms step_avg:57.57ms
step:61/2330 train_time:3510ms step_avg:57.53ms
step:62/2330 train_time:3568ms step_avg:57.55ms
step:63/2330 train_time:3624ms step_avg:57.52ms
step:64/2330 train_time:3683ms step_avg:57.55ms
step:65/2330 train_time:3739ms step_avg:57.52ms
step:66/2330 train_time:3797ms step_avg:57.52ms
step:67/2330 train_time:3851ms step_avg:57.48ms
step:68/2330 train_time:3911ms step_avg:57.51ms
step:69/2330 train_time:3966ms step_avg:57.48ms
step:70/2330 train_time:4025ms step_avg:57.50ms
step:71/2330 train_time:4081ms step_avg:57.47ms
step:72/2330 train_time:4141ms step_avg:57.51ms
step:73/2330 train_time:4197ms step_avg:57.49ms
step:74/2330 train_time:4257ms step_avg:57.52ms
step:75/2330 train_time:4313ms step_avg:57.50ms
step:76/2330 train_time:4372ms step_avg:57.52ms
step:77/2330 train_time:4427ms step_avg:57.50ms
step:78/2330 train_time:4486ms step_avg:57.51ms
step:79/2330 train_time:4541ms step_avg:57.48ms
step:80/2330 train_time:4603ms step_avg:57.53ms
step:81/2330 train_time:4658ms step_avg:57.51ms
step:82/2330 train_time:4717ms step_avg:57.52ms
step:83/2330 train_time:4773ms step_avg:57.50ms
step:84/2330 train_time:4831ms step_avg:57.51ms
step:85/2330 train_time:4886ms step_avg:57.48ms
step:86/2330 train_time:4945ms step_avg:57.50ms
step:87/2330 train_time:5000ms step_avg:57.48ms
step:88/2330 train_time:5061ms step_avg:57.51ms
step:89/2330 train_time:5117ms step_avg:57.49ms
step:90/2330 train_time:5176ms step_avg:57.51ms
step:91/2330 train_time:5233ms step_avg:57.51ms
step:92/2330 train_time:5292ms step_avg:57.52ms
step:93/2330 train_time:5347ms step_avg:57.50ms
step:94/2330 train_time:5406ms step_avg:57.51ms
step:95/2330 train_time:5462ms step_avg:57.49ms
step:96/2330 train_time:5521ms step_avg:57.51ms
step:97/2330 train_time:5578ms step_avg:57.50ms
step:98/2330 train_time:5637ms step_avg:57.52ms
step:99/2330 train_time:5693ms step_avg:57.51ms
step:100/2330 train_time:5751ms step_avg:57.51ms
step:101/2330 train_time:5807ms step_avg:57.49ms
step:102/2330 train_time:5865ms step_avg:57.50ms
step:103/2330 train_time:5921ms step_avg:57.49ms
step:104/2330 train_time:5980ms step_avg:57.50ms
step:105/2330 train_time:6035ms step_avg:57.48ms
step:106/2330 train_time:6094ms step_avg:57.49ms
step:107/2330 train_time:6149ms step_avg:57.47ms
step:108/2330 train_time:6209ms step_avg:57.49ms
step:109/2330 train_time:6265ms step_avg:57.48ms
step:110/2330 train_time:6324ms step_avg:57.50ms
step:111/2330 train_time:6380ms step_avg:57.48ms
step:112/2330 train_time:6440ms step_avg:57.50ms
step:113/2330 train_time:6496ms step_avg:57.49ms
step:114/2330 train_time:6555ms step_avg:57.50ms
step:115/2330 train_time:6611ms step_avg:57.49ms
step:116/2330 train_time:6670ms step_avg:57.50ms
step:117/2330 train_time:6725ms step_avg:57.48ms
step:118/2330 train_time:6785ms step_avg:57.50ms
step:119/2330 train_time:6840ms step_avg:57.48ms
step:120/2330 train_time:6899ms step_avg:57.49ms
step:121/2330 train_time:6955ms step_avg:57.48ms
step:122/2330 train_time:7013ms step_avg:57.49ms
step:123/2330 train_time:7069ms step_avg:57.47ms
step:124/2330 train_time:7128ms step_avg:57.48ms
step:125/2330 train_time:7184ms step_avg:57.47ms
step:126/2330 train_time:7242ms step_avg:57.48ms
step:127/2330 train_time:7298ms step_avg:57.46ms
step:128/2330 train_time:7357ms step_avg:57.48ms
step:129/2330 train_time:7413ms step_avg:57.46ms
step:130/2330 train_time:7472ms step_avg:57.47ms
step:131/2330 train_time:7528ms step_avg:57.46ms
step:132/2330 train_time:7587ms step_avg:57.48ms
step:133/2330 train_time:7643ms step_avg:57.46ms
step:134/2330 train_time:7702ms step_avg:57.48ms
step:135/2330 train_time:7757ms step_avg:57.46ms
step:136/2330 train_time:7817ms step_avg:57.47ms
step:137/2330 train_time:7872ms step_avg:57.46ms
step:138/2330 train_time:7931ms step_avg:57.47ms
step:139/2330 train_time:7986ms step_avg:57.45ms
step:140/2330 train_time:8046ms step_avg:57.47ms
step:141/2330 train_time:8101ms step_avg:57.46ms
step:142/2330 train_time:8161ms step_avg:57.47ms
step:143/2330 train_time:8217ms step_avg:57.46ms
step:144/2330 train_time:8275ms step_avg:57.47ms
step:145/2330 train_time:8331ms step_avg:57.46ms
step:146/2330 train_time:8390ms step_avg:57.47ms
step:147/2330 train_time:8445ms step_avg:57.45ms
step:148/2330 train_time:8504ms step_avg:57.46ms
step:149/2330 train_time:8560ms step_avg:57.45ms
step:150/2330 train_time:8620ms step_avg:57.47ms
step:151/2330 train_time:8676ms step_avg:57.46ms
step:152/2330 train_time:8735ms step_avg:57.47ms
step:153/2330 train_time:8790ms step_avg:57.45ms
step:154/2330 train_time:8850ms step_avg:57.47ms
step:155/2330 train_time:8905ms step_avg:57.45ms
step:156/2330 train_time:8964ms step_avg:57.46ms
step:157/2330 train_time:9020ms step_avg:57.45ms
step:158/2330 train_time:9079ms step_avg:57.46ms
step:159/2330 train_time:9135ms step_avg:57.45ms
step:160/2330 train_time:9193ms step_avg:57.46ms
step:161/2330 train_time:9249ms step_avg:57.45ms
step:162/2330 train_time:9308ms step_avg:57.46ms
step:163/2330 train_time:9364ms step_avg:57.45ms
step:164/2330 train_time:9423ms step_avg:57.46ms
step:165/2330 train_time:9479ms step_avg:57.45ms
step:166/2330 train_time:9539ms step_avg:57.46ms
step:167/2330 train_time:9595ms step_avg:57.45ms
step:168/2330 train_time:9654ms step_avg:57.46ms
step:169/2330 train_time:9709ms step_avg:57.45ms
step:170/2330 train_time:9768ms step_avg:57.46ms
step:171/2330 train_time:9824ms step_avg:57.45ms
step:172/2330 train_time:9883ms step_avg:57.46ms
step:173/2330 train_time:9938ms step_avg:57.45ms
step:174/2330 train_time:9998ms step_avg:57.46ms
step:175/2330 train_time:10054ms step_avg:57.45ms
step:176/2330 train_time:10112ms step_avg:57.46ms
step:177/2330 train_time:10168ms step_avg:57.45ms
step:178/2330 train_time:10227ms step_avg:57.45ms
step:179/2330 train_time:10283ms step_avg:57.45ms
step:180/2330 train_time:10342ms step_avg:57.45ms
step:181/2330 train_time:10397ms step_avg:57.44ms
step:182/2330 train_time:10458ms step_avg:57.46ms
step:183/2330 train_time:10514ms step_avg:57.45ms
step:184/2330 train_time:10573ms step_avg:57.46ms
step:185/2330 train_time:10629ms step_avg:57.45ms
step:186/2330 train_time:10688ms step_avg:57.46ms
step:187/2330 train_time:10744ms step_avg:57.46ms
step:188/2330 train_time:10803ms step_avg:57.46ms
step:189/2330 train_time:10859ms step_avg:57.46ms
step:190/2330 train_time:10918ms step_avg:57.46ms
step:191/2330 train_time:10975ms step_avg:57.46ms
step:192/2330 train_time:11034ms step_avg:57.47ms
step:193/2330 train_time:11090ms step_avg:57.46ms
step:194/2330 train_time:11148ms step_avg:57.46ms
step:195/2330 train_time:11204ms step_avg:57.46ms
step:196/2330 train_time:11263ms step_avg:57.46ms
step:197/2330 train_time:11318ms step_avg:57.45ms
step:198/2330 train_time:11378ms step_avg:57.46ms
step:199/2330 train_time:11434ms step_avg:57.46ms
step:200/2330 train_time:11493ms step_avg:57.47ms
step:201/2330 train_time:11549ms step_avg:57.46ms
step:202/2330 train_time:11608ms step_avg:57.46ms
step:203/2330 train_time:11664ms step_avg:57.46ms
step:204/2330 train_time:11723ms step_avg:57.47ms
step:205/2330 train_time:11779ms step_avg:57.46ms
step:206/2330 train_time:11839ms step_avg:57.47ms
step:207/2330 train_time:11894ms step_avg:57.46ms
step:208/2330 train_time:11954ms step_avg:57.47ms
step:209/2330 train_time:12010ms step_avg:57.46ms
step:210/2330 train_time:12068ms step_avg:57.47ms
step:211/2330 train_time:12124ms step_avg:57.46ms
step:212/2330 train_time:12183ms step_avg:57.47ms
step:213/2330 train_time:12239ms step_avg:57.46ms
step:214/2330 train_time:12298ms step_avg:57.47ms
step:215/2330 train_time:12354ms step_avg:57.46ms
step:216/2330 train_time:12412ms step_avg:57.46ms
step:217/2330 train_time:12468ms step_avg:57.46ms
step:218/2330 train_time:12527ms step_avg:57.46ms
step:219/2330 train_time:12583ms step_avg:57.46ms
step:220/2330 train_time:12643ms step_avg:57.47ms
step:221/2330 train_time:12698ms step_avg:57.46ms
step:222/2330 train_time:12757ms step_avg:57.46ms
step:223/2330 train_time:12813ms step_avg:57.46ms
step:224/2330 train_time:12872ms step_avg:57.46ms
step:225/2330 train_time:12927ms step_avg:57.45ms
step:226/2330 train_time:12987ms step_avg:57.47ms
step:227/2330 train_time:13043ms step_avg:57.46ms
step:228/2330 train_time:13103ms step_avg:57.47ms
step:229/2330 train_time:13158ms step_avg:57.46ms
step:230/2330 train_time:13218ms step_avg:57.47ms
step:231/2330 train_time:13273ms step_avg:57.46ms
step:232/2330 train_time:13332ms step_avg:57.47ms
step:233/2330 train_time:13388ms step_avg:57.46ms
step:234/2330 train_time:13447ms step_avg:57.46ms
step:235/2330 train_time:13503ms step_avg:57.46ms
step:236/2330 train_time:13561ms step_avg:57.46ms
step:237/2330 train_time:13617ms step_avg:57.46ms
step:238/2330 train_time:13677ms step_avg:57.47ms
step:239/2330 train_time:13733ms step_avg:57.46ms
step:240/2330 train_time:13793ms step_avg:57.47ms
step:241/2330 train_time:13849ms step_avg:57.46ms
step:242/2330 train_time:13907ms step_avg:57.47ms
step:243/2330 train_time:13963ms step_avg:57.46ms
step:244/2330 train_time:14023ms step_avg:57.47ms
step:245/2330 train_time:14079ms step_avg:57.47ms
step:246/2330 train_time:14138ms step_avg:57.47ms
step:247/2330 train_time:14193ms step_avg:57.46ms
step:248/2330 train_time:14253ms step_avg:57.47ms
step:249/2330 train_time:14308ms step_avg:57.46ms
step:250/2330 train_time:14367ms step_avg:57.47ms
step:250/2330 val_loss:4.8985 train_time:14446ms step_avg:57.78ms
step:251/2330 train_time:14463ms step_avg:57.62ms
step:252/2330 train_time:14483ms step_avg:57.47ms
step:253/2330 train_time:14537ms step_avg:57.46ms
step:254/2330 train_time:14604ms step_avg:57.50ms
step:255/2330 train_time:14659ms step_avg:57.49ms
step:256/2330 train_time:14723ms step_avg:57.51ms
step:257/2330 train_time:14778ms step_avg:57.50ms
step:258/2330 train_time:14838ms step_avg:57.51ms
step:259/2330 train_time:14893ms step_avg:57.50ms
step:260/2330 train_time:14952ms step_avg:57.51ms
step:261/2330 train_time:15007ms step_avg:57.50ms
step:262/2330 train_time:15066ms step_avg:57.50ms
step:263/2330 train_time:15121ms step_avg:57.49ms
step:264/2330 train_time:15180ms step_avg:57.50ms
step:265/2330 train_time:15235ms step_avg:57.49ms
step:266/2330 train_time:15293ms step_avg:57.49ms
step:267/2330 train_time:15351ms step_avg:57.49ms
step:268/2330 train_time:15411ms step_avg:57.50ms
step:269/2330 train_time:15468ms step_avg:57.50ms
step:270/2330 train_time:15528ms step_avg:57.51ms
step:271/2330 train_time:15584ms step_avg:57.50ms
step:272/2330 train_time:15645ms step_avg:57.52ms
step:273/2330 train_time:15700ms step_avg:57.51ms
step:274/2330 train_time:15762ms step_avg:57.52ms
step:275/2330 train_time:15816ms step_avg:57.51ms
step:276/2330 train_time:15876ms step_avg:57.52ms
step:277/2330 train_time:15931ms step_avg:57.51ms
step:278/2330 train_time:15990ms step_avg:57.52ms
step:279/2330 train_time:16045ms step_avg:57.51ms
step:280/2330 train_time:16104ms step_avg:57.52ms
step:281/2330 train_time:16159ms step_avg:57.51ms
step:282/2330 train_time:16218ms step_avg:57.51ms
step:283/2330 train_time:16275ms step_avg:57.51ms
step:284/2330 train_time:16333ms step_avg:57.51ms
step:285/2330 train_time:16389ms step_avg:57.50ms
step:286/2330 train_time:16447ms step_avg:57.51ms
step:287/2330 train_time:16503ms step_avg:57.50ms
step:288/2330 train_time:16564ms step_avg:57.52ms
step:289/2330 train_time:16620ms step_avg:57.51ms
step:290/2330 train_time:16680ms step_avg:57.52ms
step:291/2330 train_time:16736ms step_avg:57.51ms
step:292/2330 train_time:16795ms step_avg:57.52ms
step:293/2330 train_time:16852ms step_avg:57.52ms
step:294/2330 train_time:16911ms step_avg:57.52ms
step:295/2330 train_time:16967ms step_avg:57.51ms
step:296/2330 train_time:17025ms step_avg:57.52ms
step:297/2330 train_time:17080ms step_avg:57.51ms
step:298/2330 train_time:17140ms step_avg:57.52ms
step:299/2330 train_time:17196ms step_avg:57.51ms
step:300/2330 train_time:17255ms step_avg:57.52ms
step:301/2330 train_time:17311ms step_avg:57.51ms
step:302/2330 train_time:17370ms step_avg:57.52ms
step:303/2330 train_time:17426ms step_avg:57.51ms
step:304/2330 train_time:17485ms step_avg:57.52ms
step:305/2330 train_time:17542ms step_avg:57.51ms
step:306/2330 train_time:17601ms step_avg:57.52ms
step:307/2330 train_time:17657ms step_avg:57.51ms
step:308/2330 train_time:17718ms step_avg:57.53ms
step:309/2330 train_time:17774ms step_avg:57.52ms
step:310/2330 train_time:17833ms step_avg:57.53ms
step:311/2330 train_time:17890ms step_avg:57.52ms
step:312/2330 train_time:17948ms step_avg:57.53ms
step:313/2330 train_time:18004ms step_avg:57.52ms
step:314/2330 train_time:18062ms step_avg:57.52ms
step:315/2330 train_time:18117ms step_avg:57.52ms
step:316/2330 train_time:18177ms step_avg:57.52ms
step:317/2330 train_time:18233ms step_avg:57.52ms
step:318/2330 train_time:18292ms step_avg:57.52ms
step:319/2330 train_time:18347ms step_avg:57.52ms
step:320/2330 train_time:18407ms step_avg:57.52ms
step:321/2330 train_time:18462ms step_avg:57.52ms
step:322/2330 train_time:18523ms step_avg:57.52ms
step:323/2330 train_time:18578ms step_avg:57.52ms
step:324/2330 train_time:18638ms step_avg:57.53ms
step:325/2330 train_time:18694ms step_avg:57.52ms
step:326/2330 train_time:18754ms step_avg:57.53ms
step:327/2330 train_time:18810ms step_avg:57.52ms
step:328/2330 train_time:18869ms step_avg:57.53ms
step:329/2330 train_time:18926ms step_avg:57.52ms
step:330/2330 train_time:18984ms step_avg:57.53ms
step:331/2330 train_time:19039ms step_avg:57.52ms
step:332/2330 train_time:19099ms step_avg:57.53ms
step:333/2330 train_time:19155ms step_avg:57.52ms
step:334/2330 train_time:19214ms step_avg:57.53ms
step:335/2330 train_time:19271ms step_avg:57.53ms
step:336/2330 train_time:19329ms step_avg:57.53ms
step:337/2330 train_time:19385ms step_avg:57.52ms
step:338/2330 train_time:19444ms step_avg:57.53ms
step:339/2330 train_time:19500ms step_avg:57.52ms
step:340/2330 train_time:19559ms step_avg:57.53ms
step:341/2330 train_time:19615ms step_avg:57.52ms
step:342/2330 train_time:19674ms step_avg:57.53ms
step:343/2330 train_time:19730ms step_avg:57.52ms
step:344/2330 train_time:19790ms step_avg:57.53ms
step:345/2330 train_time:19846ms step_avg:57.52ms
step:346/2330 train_time:19905ms step_avg:57.53ms
step:347/2330 train_time:19960ms step_avg:57.52ms
step:348/2330 train_time:20020ms step_avg:57.53ms
step:349/2330 train_time:20076ms step_avg:57.52ms
step:350/2330 train_time:20135ms step_avg:57.53ms
step:351/2330 train_time:20191ms step_avg:57.52ms
step:352/2330 train_time:20251ms step_avg:57.53ms
step:353/2330 train_time:20307ms step_avg:57.53ms
step:354/2330 train_time:20366ms step_avg:57.53ms
step:355/2330 train_time:20422ms step_avg:57.53ms
step:356/2330 train_time:20480ms step_avg:57.53ms
step:357/2330 train_time:20537ms step_avg:57.53ms
step:358/2330 train_time:20596ms step_avg:57.53ms
step:359/2330 train_time:20652ms step_avg:57.53ms
step:360/2330 train_time:20711ms step_avg:57.53ms
step:361/2330 train_time:20768ms step_avg:57.53ms
step:362/2330 train_time:20827ms step_avg:57.53ms
step:363/2330 train_time:20883ms step_avg:57.53ms
step:364/2330 train_time:20942ms step_avg:57.53ms
step:365/2330 train_time:20998ms step_avg:57.53ms
step:366/2330 train_time:21058ms step_avg:57.54ms
step:367/2330 train_time:21114ms step_avg:57.53ms
step:368/2330 train_time:21174ms step_avg:57.54ms
step:369/2330 train_time:21230ms step_avg:57.53ms
step:370/2330 train_time:21289ms step_avg:57.54ms
step:371/2330 train_time:21344ms step_avg:57.53ms
step:372/2330 train_time:21403ms step_avg:57.53ms
step:373/2330 train_time:21458ms step_avg:57.53ms
step:374/2330 train_time:21518ms step_avg:57.54ms
step:375/2330 train_time:21574ms step_avg:57.53ms
step:376/2330 train_time:21634ms step_avg:57.54ms
step:377/2330 train_time:21690ms step_avg:57.53ms
step:378/2330 train_time:21748ms step_avg:57.54ms
step:379/2330 train_time:21804ms step_avg:57.53ms
step:380/2330 train_time:21864ms step_avg:57.54ms
step:381/2330 train_time:21919ms step_avg:57.53ms
step:382/2330 train_time:21980ms step_avg:57.54ms
step:383/2330 train_time:22037ms step_avg:57.54ms
step:384/2330 train_time:22095ms step_avg:57.54ms
step:385/2330 train_time:22150ms step_avg:57.53ms
step:386/2330 train_time:22211ms step_avg:57.54ms
step:387/2330 train_time:22266ms step_avg:57.54ms
step:388/2330 train_time:22325ms step_avg:57.54ms
step:389/2330 train_time:22380ms step_avg:57.53ms
step:390/2330 train_time:22440ms step_avg:57.54ms
step:391/2330 train_time:22495ms step_avg:57.53ms
step:392/2330 train_time:22556ms step_avg:57.54ms
step:393/2330 train_time:22612ms step_avg:57.54ms
step:394/2330 train_time:22671ms step_avg:57.54ms
step:395/2330 train_time:22727ms step_avg:57.54ms
step:396/2330 train_time:22785ms step_avg:57.54ms
step:397/2330 train_time:22841ms step_avg:57.53ms
step:398/2330 train_time:22901ms step_avg:57.54ms
step:399/2330 train_time:22957ms step_avg:57.54ms
step:400/2330 train_time:23017ms step_avg:57.54ms
step:401/2330 train_time:23073ms step_avg:57.54ms
step:402/2330 train_time:23132ms step_avg:57.54ms
step:403/2330 train_time:23188ms step_avg:57.54ms
step:404/2330 train_time:23247ms step_avg:57.54ms
step:405/2330 train_time:23304ms step_avg:57.54ms
step:406/2330 train_time:23363ms step_avg:57.54ms
step:407/2330 train_time:23418ms step_avg:57.54ms
step:408/2330 train_time:23479ms step_avg:57.55ms
step:409/2330 train_time:23535ms step_avg:57.54ms
step:410/2330 train_time:23594ms step_avg:57.55ms
step:411/2330 train_time:23651ms step_avg:57.54ms
step:412/2330 train_time:23709ms step_avg:57.55ms
step:413/2330 train_time:23765ms step_avg:57.54ms
step:414/2330 train_time:23824ms step_avg:57.55ms
step:415/2330 train_time:23879ms step_avg:57.54ms
step:416/2330 train_time:23940ms step_avg:57.55ms
step:417/2330 train_time:23996ms step_avg:57.54ms
step:418/2330 train_time:24056ms step_avg:57.55ms
step:419/2330 train_time:24112ms step_avg:57.55ms
step:420/2330 train_time:24171ms step_avg:57.55ms
step:421/2330 train_time:24228ms step_avg:57.55ms
step:422/2330 train_time:24286ms step_avg:57.55ms
step:423/2330 train_time:24342ms step_avg:57.55ms
step:424/2330 train_time:24401ms step_avg:57.55ms
step:425/2330 train_time:24457ms step_avg:57.55ms
step:426/2330 train_time:24518ms step_avg:57.55ms
step:427/2330 train_time:24574ms step_avg:57.55ms
step:428/2330 train_time:24633ms step_avg:57.55ms
step:429/2330 train_time:24689ms step_avg:57.55ms
step:430/2330 train_time:24748ms step_avg:57.55ms
step:431/2330 train_time:24804ms step_avg:57.55ms
step:432/2330 train_time:24864ms step_avg:57.56ms
step:433/2330 train_time:24920ms step_avg:57.55ms
step:434/2330 train_time:24980ms step_avg:57.56ms
step:435/2330 train_time:25035ms step_avg:57.55ms
step:436/2330 train_time:25095ms step_avg:57.56ms
step:437/2330 train_time:25151ms step_avg:57.55ms
step:438/2330 train_time:25210ms step_avg:57.56ms
step:439/2330 train_time:25266ms step_avg:57.55ms
step:440/2330 train_time:25326ms step_avg:57.56ms
step:441/2330 train_time:25381ms step_avg:57.55ms
step:442/2330 train_time:25441ms step_avg:57.56ms
step:443/2330 train_time:25496ms step_avg:57.55ms
step:444/2330 train_time:25557ms step_avg:57.56ms
step:445/2330 train_time:25613ms step_avg:57.56ms
step:446/2330 train_time:25672ms step_avg:57.56ms
step:447/2330 train_time:25728ms step_avg:57.56ms
step:448/2330 train_time:25787ms step_avg:57.56ms
step:449/2330 train_time:25843ms step_avg:57.56ms
step:450/2330 train_time:25903ms step_avg:57.56ms
step:451/2330 train_time:25958ms step_avg:57.56ms
step:452/2330 train_time:26019ms step_avg:57.56ms
step:453/2330 train_time:26075ms step_avg:57.56ms
step:454/2330 train_time:26134ms step_avg:57.56ms
step:455/2330 train_time:26191ms step_avg:57.56ms
step:456/2330 train_time:26250ms step_avg:57.57ms
step:457/2330 train_time:26306ms step_avg:57.56ms
step:458/2330 train_time:26365ms step_avg:57.56ms
step:459/2330 train_time:26420ms step_avg:57.56ms
step:460/2330 train_time:26481ms step_avg:57.57ms
step:461/2330 train_time:26536ms step_avg:57.56ms
step:462/2330 train_time:26596ms step_avg:57.57ms
step:463/2330 train_time:26652ms step_avg:57.56ms
step:464/2330 train_time:26711ms step_avg:57.57ms
step:465/2330 train_time:26767ms step_avg:57.56ms
step:466/2330 train_time:26826ms step_avg:57.57ms
step:467/2330 train_time:26882ms step_avg:57.56ms
step:468/2330 train_time:26941ms step_avg:57.57ms
step:469/2330 train_time:26997ms step_avg:57.56ms
step:470/2330 train_time:27058ms step_avg:57.57ms
step:471/2330 train_time:27114ms step_avg:57.57ms
step:472/2330 train_time:27173ms step_avg:57.57ms
step:473/2330 train_time:27229ms step_avg:57.57ms
step:474/2330 train_time:27288ms step_avg:57.57ms
step:475/2330 train_time:27344ms step_avg:57.57ms
step:476/2330 train_time:27403ms step_avg:57.57ms
step:477/2330 train_time:27459ms step_avg:57.57ms
step:478/2330 train_time:27520ms step_avg:57.57ms
step:479/2330 train_time:27575ms step_avg:57.57ms
step:480/2330 train_time:27635ms step_avg:57.57ms
step:481/2330 train_time:27690ms step_avg:57.57ms
step:482/2330 train_time:27750ms step_avg:57.57ms
step:483/2330 train_time:27806ms step_avg:57.57ms
step:484/2330 train_time:27865ms step_avg:57.57ms
step:485/2330 train_time:27920ms step_avg:57.57ms
step:486/2330 train_time:27981ms step_avg:57.57ms
step:487/2330 train_time:28037ms step_avg:57.57ms
step:488/2330 train_time:28097ms step_avg:57.58ms
step:489/2330 train_time:28154ms step_avg:57.57ms
step:490/2330 train_time:28213ms step_avg:57.58ms
step:491/2330 train_time:28269ms step_avg:57.57ms
step:492/2330 train_time:28328ms step_avg:57.58ms
step:493/2330 train_time:28383ms step_avg:57.57ms
step:494/2330 train_time:28442ms step_avg:57.58ms
step:495/2330 train_time:28498ms step_avg:57.57ms
step:496/2330 train_time:28559ms step_avg:57.58ms
step:497/2330 train_time:28615ms step_avg:57.57ms
step:498/2330 train_time:28673ms step_avg:57.58ms
step:499/2330 train_time:28729ms step_avg:57.57ms
step:500/2330 train_time:28787ms step_avg:57.57ms
step:500/2330 val_loss:4.4091 train_time:28867ms step_avg:57.73ms
step:501/2330 train_time:28885ms step_avg:57.65ms
step:502/2330 train_time:28907ms step_avg:57.58ms
step:503/2330 train_time:28964ms step_avg:57.58ms
step:504/2330 train_time:29028ms step_avg:57.60ms
step:505/2330 train_time:29086ms step_avg:57.60ms
step:506/2330 train_time:29144ms step_avg:57.60ms
step:507/2330 train_time:29200ms step_avg:57.59ms
step:508/2330 train_time:29261ms step_avg:57.60ms
step:509/2330 train_time:29316ms step_avg:57.60ms
step:510/2330 train_time:29375ms step_avg:57.60ms
step:511/2330 train_time:29430ms step_avg:57.59ms
step:512/2330 train_time:29488ms step_avg:57.59ms
step:513/2330 train_time:29544ms step_avg:57.59ms
step:514/2330 train_time:29602ms step_avg:57.59ms
step:515/2330 train_time:29658ms step_avg:57.59ms
step:516/2330 train_time:29716ms step_avg:57.59ms
step:517/2330 train_time:29772ms step_avg:57.59ms
step:518/2330 train_time:29830ms step_avg:57.59ms
step:519/2330 train_time:29886ms step_avg:57.58ms
step:520/2330 train_time:29947ms step_avg:57.59ms
step:521/2330 train_time:30004ms step_avg:57.59ms
step:522/2330 train_time:30065ms step_avg:57.60ms
step:523/2330 train_time:30121ms step_avg:57.59ms
step:524/2330 train_time:30182ms step_avg:57.60ms
step:525/2330 train_time:30237ms step_avg:57.59ms
step:526/2330 train_time:30298ms step_avg:57.60ms
step:527/2330 train_time:30354ms step_avg:57.60ms
step:528/2330 train_time:30413ms step_avg:57.60ms
step:529/2330 train_time:30468ms step_avg:57.60ms
step:530/2330 train_time:30526ms step_avg:57.60ms
step:531/2330 train_time:30582ms step_avg:57.59ms
step:532/2330 train_time:30641ms step_avg:57.60ms
step:533/2330 train_time:30696ms step_avg:57.59ms
step:534/2330 train_time:30755ms step_avg:57.59ms
step:535/2330 train_time:30812ms step_avg:57.59ms
step:536/2330 train_time:30870ms step_avg:57.59ms
step:537/2330 train_time:30927ms step_avg:57.59ms
step:538/2330 train_time:30986ms step_avg:57.60ms
step:539/2330 train_time:31043ms step_avg:57.59ms
step:540/2330 train_time:31102ms step_avg:57.60ms
step:541/2330 train_time:31158ms step_avg:57.59ms
step:542/2330 train_time:31219ms step_avg:57.60ms
step:543/2330 train_time:31275ms step_avg:57.60ms
step:544/2330 train_time:31335ms step_avg:57.60ms
step:545/2330 train_time:31392ms step_avg:57.60ms
step:546/2330 train_time:31451ms step_avg:57.60ms
step:547/2330 train_time:31506ms step_avg:57.60ms
step:548/2330 train_time:31564ms step_avg:57.60ms
step:549/2330 train_time:31620ms step_avg:57.59ms
step:550/2330 train_time:31679ms step_avg:57.60ms
step:551/2330 train_time:31735ms step_avg:57.60ms
step:552/2330 train_time:31794ms step_avg:57.60ms
step:553/2330 train_time:31851ms step_avg:57.60ms
step:554/2330 train_time:31910ms step_avg:57.60ms
step:555/2330 train_time:31966ms step_avg:57.60ms
step:556/2330 train_time:32025ms step_avg:57.60ms
step:557/2330 train_time:32081ms step_avg:57.60ms
step:558/2330 train_time:32141ms step_avg:57.60ms
step:559/2330 train_time:32197ms step_avg:57.60ms
step:560/2330 train_time:32257ms step_avg:57.60ms
step:561/2330 train_time:32313ms step_avg:57.60ms
step:562/2330 train_time:32373ms step_avg:57.60ms
step:563/2330 train_time:32428ms step_avg:57.60ms
step:564/2330 train_time:32487ms step_avg:57.60ms
step:565/2330 train_time:32542ms step_avg:57.60ms
step:566/2330 train_time:32602ms step_avg:57.60ms
step:567/2330 train_time:32658ms step_avg:57.60ms
step:568/2330 train_time:32718ms step_avg:57.60ms
step:569/2330 train_time:32773ms step_avg:57.60ms
step:570/2330 train_time:32833ms step_avg:57.60ms
step:571/2330 train_time:32889ms step_avg:57.60ms
step:572/2330 train_time:32948ms step_avg:57.60ms
step:573/2330 train_time:33005ms step_avg:57.60ms
step:574/2330 train_time:33063ms step_avg:57.60ms
step:575/2330 train_time:33119ms step_avg:57.60ms
step:576/2330 train_time:33180ms step_avg:57.60ms
step:577/2330 train_time:33236ms step_avg:57.60ms
step:578/2330 train_time:33296ms step_avg:57.61ms
step:579/2330 train_time:33354ms step_avg:57.61ms
step:580/2330 train_time:33413ms step_avg:57.61ms
step:581/2330 train_time:33469ms step_avg:57.61ms
step:582/2330 train_time:33527ms step_avg:57.61ms
step:583/2330 train_time:33582ms step_avg:57.60ms
step:584/2330 train_time:33642ms step_avg:57.61ms
step:585/2330 train_time:33697ms step_avg:57.60ms
step:586/2330 train_time:33757ms step_avg:57.61ms
step:587/2330 train_time:33813ms step_avg:57.60ms
step:588/2330 train_time:33872ms step_avg:57.61ms
step:589/2330 train_time:33928ms step_avg:57.60ms
step:590/2330 train_time:33987ms step_avg:57.60ms
step:591/2330 train_time:34043ms step_avg:57.60ms
step:592/2330 train_time:34103ms step_avg:57.61ms
step:593/2330 train_time:34158ms step_avg:57.60ms
step:594/2330 train_time:34219ms step_avg:57.61ms
step:595/2330 train_time:34276ms step_avg:57.61ms
step:596/2330 train_time:34335ms step_avg:57.61ms
step:597/2330 train_time:34392ms step_avg:57.61ms
step:598/2330 train_time:34451ms step_avg:57.61ms
step:599/2330 train_time:34507ms step_avg:57.61ms
step:600/2330 train_time:34566ms step_avg:57.61ms
step:601/2330 train_time:34622ms step_avg:57.61ms
step:602/2330 train_time:34681ms step_avg:57.61ms
step:603/2330 train_time:34737ms step_avg:57.61ms
step:604/2330 train_time:34797ms step_avg:57.61ms
step:605/2330 train_time:34853ms step_avg:57.61ms
step:606/2330 train_time:34911ms step_avg:57.61ms
step:607/2330 train_time:34968ms step_avg:57.61ms
step:608/2330 train_time:35026ms step_avg:57.61ms
step:609/2330 train_time:35082ms step_avg:57.61ms
step:610/2330 train_time:35142ms step_avg:57.61ms
step:611/2330 train_time:35198ms step_avg:57.61ms
step:612/2330 train_time:35258ms step_avg:57.61ms
step:613/2330 train_time:35314ms step_avg:57.61ms
step:614/2330 train_time:35375ms step_avg:57.61ms
step:615/2330 train_time:35431ms step_avg:57.61ms
step:616/2330 train_time:35490ms step_avg:57.61ms
step:617/2330 train_time:35546ms step_avg:57.61ms
step:618/2330 train_time:35604ms step_avg:57.61ms
step:619/2330 train_time:35660ms step_avg:57.61ms
step:620/2330 train_time:35720ms step_avg:57.61ms
step:621/2330 train_time:35776ms step_avg:57.61ms
step:622/2330 train_time:35835ms step_avg:57.61ms
step:623/2330 train_time:35891ms step_avg:57.61ms
step:624/2330 train_time:35950ms step_avg:57.61ms
step:625/2330 train_time:36007ms step_avg:57.61ms
step:626/2330 train_time:36065ms step_avg:57.61ms
step:627/2330 train_time:36121ms step_avg:57.61ms
step:628/2330 train_time:36182ms step_avg:57.62ms
step:629/2330 train_time:36238ms step_avg:57.61ms
step:630/2330 train_time:36299ms step_avg:57.62ms
step:631/2330 train_time:36355ms step_avg:57.61ms
step:632/2330 train_time:36414ms step_avg:57.62ms
step:633/2330 train_time:36470ms step_avg:57.61ms
step:634/2330 train_time:36529ms step_avg:57.62ms
step:635/2330 train_time:36584ms step_avg:57.61ms
step:636/2330 train_time:36644ms step_avg:57.62ms
step:637/2330 train_time:36700ms step_avg:57.61ms
step:638/2330 train_time:36760ms step_avg:57.62ms
step:639/2330 train_time:36816ms step_avg:57.61ms
step:640/2330 train_time:36874ms step_avg:57.62ms
step:641/2330 train_time:36931ms step_avg:57.61ms
step:642/2330 train_time:36990ms step_avg:57.62ms
step:643/2330 train_time:37046ms step_avg:57.61ms
step:644/2330 train_time:37104ms step_avg:57.62ms
step:645/2330 train_time:37160ms step_avg:57.61ms
step:646/2330 train_time:37220ms step_avg:57.62ms
step:647/2330 train_time:37277ms step_avg:57.61ms
step:648/2330 train_time:37336ms step_avg:57.62ms
step:649/2330 train_time:37393ms step_avg:57.62ms
step:650/2330 train_time:37451ms step_avg:57.62ms
step:651/2330 train_time:37508ms step_avg:57.62ms
step:652/2330 train_time:37567ms step_avg:57.62ms
step:653/2330 train_time:37623ms step_avg:57.62ms
step:654/2330 train_time:37683ms step_avg:57.62ms
step:655/2330 train_time:37738ms step_avg:57.62ms
step:656/2330 train_time:37799ms step_avg:57.62ms
step:657/2330 train_time:37855ms step_avg:57.62ms
step:658/2330 train_time:37914ms step_avg:57.62ms
step:659/2330 train_time:37970ms step_avg:57.62ms
step:660/2330 train_time:38028ms step_avg:57.62ms
step:661/2330 train_time:38085ms step_avg:57.62ms
step:662/2330 train_time:38144ms step_avg:57.62ms
step:663/2330 train_time:38199ms step_avg:57.62ms
step:664/2330 train_time:38259ms step_avg:57.62ms
step:665/2330 train_time:38315ms step_avg:57.62ms
step:666/2330 train_time:38375ms step_avg:57.62ms
step:667/2330 train_time:38431ms step_avg:57.62ms
step:668/2330 train_time:38490ms step_avg:57.62ms
step:669/2330 train_time:38546ms step_avg:57.62ms
step:670/2330 train_time:38605ms step_avg:57.62ms
step:671/2330 train_time:38660ms step_avg:57.62ms
step:672/2330 train_time:38721ms step_avg:57.62ms
step:673/2330 train_time:38777ms step_avg:57.62ms
step:674/2330 train_time:38837ms step_avg:57.62ms
step:675/2330 train_time:38894ms step_avg:57.62ms
step:676/2330 train_time:38952ms step_avg:57.62ms
step:677/2330 train_time:39008ms step_avg:57.62ms
step:678/2330 train_time:39067ms step_avg:57.62ms
step:679/2330 train_time:39123ms step_avg:57.62ms
step:680/2330 train_time:39182ms step_avg:57.62ms
step:681/2330 train_time:39238ms step_avg:57.62ms
step:682/2330 train_time:39297ms step_avg:57.62ms
step:683/2330 train_time:39353ms step_avg:57.62ms
step:684/2330 train_time:39413ms step_avg:57.62ms
step:685/2330 train_time:39469ms step_avg:57.62ms
step:686/2330 train_time:39528ms step_avg:57.62ms
step:687/2330 train_time:39584ms step_avg:57.62ms
step:688/2330 train_time:39643ms step_avg:57.62ms
step:689/2330 train_time:39699ms step_avg:57.62ms
step:690/2330 train_time:39759ms step_avg:57.62ms
step:691/2330 train_time:39815ms step_avg:57.62ms
step:692/2330 train_time:39875ms step_avg:57.62ms
step:693/2330 train_time:39932ms step_avg:57.62ms
step:694/2330 train_time:39991ms step_avg:57.62ms
step:695/2330 train_time:40047ms step_avg:57.62ms
step:696/2330 train_time:40106ms step_avg:57.62ms
step:697/2330 train_time:40162ms step_avg:57.62ms
step:698/2330 train_time:40221ms step_avg:57.62ms
step:699/2330 train_time:40277ms step_avg:57.62ms
step:700/2330 train_time:40337ms step_avg:57.62ms
step:701/2330 train_time:40394ms step_avg:57.62ms
step:702/2330 train_time:40453ms step_avg:57.62ms
step:703/2330 train_time:40510ms step_avg:57.62ms
step:704/2330 train_time:40568ms step_avg:57.63ms
step:705/2330 train_time:40625ms step_avg:57.62ms
step:706/2330 train_time:40684ms step_avg:57.63ms
step:707/2330 train_time:40739ms step_avg:57.62ms
step:708/2330 train_time:40801ms step_avg:57.63ms
step:709/2330 train_time:40857ms step_avg:57.63ms
step:710/2330 train_time:40916ms step_avg:57.63ms
step:711/2330 train_time:40973ms step_avg:57.63ms
step:712/2330 train_time:41032ms step_avg:57.63ms
step:713/2330 train_time:41088ms step_avg:57.63ms
step:714/2330 train_time:41147ms step_avg:57.63ms
step:715/2330 train_time:41203ms step_avg:57.63ms
step:716/2330 train_time:41262ms step_avg:57.63ms
step:717/2330 train_time:41318ms step_avg:57.63ms
step:718/2330 train_time:41379ms step_avg:57.63ms
step:719/2330 train_time:41434ms step_avg:57.63ms
step:720/2330 train_time:41494ms step_avg:57.63ms
step:721/2330 train_time:41550ms step_avg:57.63ms
step:722/2330 train_time:41609ms step_avg:57.63ms
step:723/2330 train_time:41665ms step_avg:57.63ms
step:724/2330 train_time:41724ms step_avg:57.63ms
step:725/2330 train_time:41780ms step_avg:57.63ms
step:726/2330 train_time:41840ms step_avg:57.63ms
step:727/2330 train_time:41896ms step_avg:57.63ms
step:728/2330 train_time:41956ms step_avg:57.63ms
step:729/2330 train_time:42012ms step_avg:57.63ms
step:730/2330 train_time:42072ms step_avg:57.63ms
step:731/2330 train_time:42128ms step_avg:57.63ms
step:732/2330 train_time:42187ms step_avg:57.63ms
step:733/2330 train_time:42243ms step_avg:57.63ms
step:734/2330 train_time:42302ms step_avg:57.63ms
step:735/2330 train_time:42358ms step_avg:57.63ms
step:736/2330 train_time:42418ms step_avg:57.63ms
step:737/2330 train_time:42474ms step_avg:57.63ms
step:738/2330 train_time:42533ms step_avg:57.63ms
step:739/2330 train_time:42590ms step_avg:57.63ms
step:740/2330 train_time:42648ms step_avg:57.63ms
step:741/2330 train_time:42705ms step_avg:57.63ms
step:742/2330 train_time:42763ms step_avg:57.63ms
step:743/2330 train_time:42819ms step_avg:57.63ms
step:744/2330 train_time:42880ms step_avg:57.63ms
step:745/2330 train_time:42936ms step_avg:57.63ms
step:746/2330 train_time:42995ms step_avg:57.63ms
step:747/2330 train_time:43051ms step_avg:57.63ms
step:748/2330 train_time:43111ms step_avg:57.63ms
step:749/2330 train_time:43167ms step_avg:57.63ms
step:750/2330 train_time:43225ms step_avg:57.63ms
step:750/2330 val_loss:4.2117 train_time:43306ms step_avg:57.74ms
step:751/2330 train_time:43324ms step_avg:57.69ms
step:752/2330 train_time:43344ms step_avg:57.64ms
step:753/2330 train_time:43400ms step_avg:57.64ms
step:754/2330 train_time:43464ms step_avg:57.64ms
step:755/2330 train_time:43520ms step_avg:57.64ms
step:756/2330 train_time:43585ms step_avg:57.65ms
step:757/2330 train_time:43641ms step_avg:57.65ms
step:758/2330 train_time:43701ms step_avg:57.65ms
step:759/2330 train_time:43756ms step_avg:57.65ms
step:760/2330 train_time:43817ms step_avg:57.65ms
step:761/2330 train_time:43872ms step_avg:57.65ms
step:762/2330 train_time:43931ms step_avg:57.65ms
step:763/2330 train_time:43986ms step_avg:57.65ms
step:764/2330 train_time:44044ms step_avg:57.65ms
step:765/2330 train_time:44101ms step_avg:57.65ms
step:766/2330 train_time:44159ms step_avg:57.65ms
step:767/2330 train_time:44215ms step_avg:57.65ms
step:768/2330 train_time:44276ms step_avg:57.65ms
step:769/2330 train_time:44335ms step_avg:57.65ms
step:770/2330 train_time:44396ms step_avg:57.66ms
step:771/2330 train_time:44454ms step_avg:57.66ms
step:772/2330 train_time:44516ms step_avg:57.66ms
step:773/2330 train_time:44574ms step_avg:57.66ms
step:774/2330 train_time:44634ms step_avg:57.67ms
step:775/2330 train_time:44691ms step_avg:57.67ms
step:776/2330 train_time:44750ms step_avg:57.67ms
step:777/2330 train_time:44807ms step_avg:57.67ms
step:778/2330 train_time:44866ms step_avg:57.67ms
step:779/2330 train_time:44923ms step_avg:57.67ms
step:780/2330 train_time:44982ms step_avg:57.67ms
step:781/2330 train_time:45039ms step_avg:57.67ms
step:782/2330 train_time:45098ms step_avg:57.67ms
step:783/2330 train_time:45155ms step_avg:57.67ms
step:784/2330 train_time:45215ms step_avg:57.67ms
step:785/2330 train_time:45272ms step_avg:57.67ms
step:786/2330 train_time:45333ms step_avg:57.67ms
step:787/2330 train_time:45390ms step_avg:57.68ms
step:788/2330 train_time:45450ms step_avg:57.68ms
step:789/2330 train_time:45507ms step_avg:57.68ms
step:790/2330 train_time:45568ms step_avg:57.68ms
step:791/2330 train_time:45625ms step_avg:57.68ms
step:792/2330 train_time:45686ms step_avg:57.68ms
step:793/2330 train_time:45743ms step_avg:57.68ms
step:794/2330 train_time:45802ms step_avg:57.69ms
step:795/2330 train_time:45858ms step_avg:57.68ms
step:796/2330 train_time:45919ms step_avg:57.69ms
step:797/2330 train_time:45975ms step_avg:57.69ms
step:798/2330 train_time:46036ms step_avg:57.69ms
step:799/2330 train_time:46093ms step_avg:57.69ms
step:800/2330 train_time:46152ms step_avg:57.69ms
step:801/2330 train_time:46209ms step_avg:57.69ms
step:802/2330 train_time:46268ms step_avg:57.69ms
step:803/2330 train_time:46324ms step_avg:57.69ms
step:804/2330 train_time:46386ms step_avg:57.69ms
step:805/2330 train_time:46442ms step_avg:57.69ms
step:806/2330 train_time:46505ms step_avg:57.70ms
step:807/2330 train_time:46561ms step_avg:57.70ms
step:808/2330 train_time:46623ms step_avg:57.70ms
step:809/2330 train_time:46679ms step_avg:57.70ms
step:810/2330 train_time:46740ms step_avg:57.70ms
step:811/2330 train_time:46797ms step_avg:57.70ms
step:812/2330 train_time:46858ms step_avg:57.71ms
step:813/2330 train_time:46914ms step_avg:57.71ms
step:814/2330 train_time:46975ms step_avg:57.71ms
step:815/2330 train_time:47032ms step_avg:57.71ms
step:816/2330 train_time:47092ms step_avg:57.71ms
step:817/2330 train_time:47148ms step_avg:57.71ms
step:818/2330 train_time:47208ms step_avg:57.71ms
step:819/2330 train_time:47265ms step_avg:57.71ms
step:820/2330 train_time:47325ms step_avg:57.71ms
step:821/2330 train_time:47383ms step_avg:57.71ms
step:822/2330 train_time:47444ms step_avg:57.72ms
step:823/2330 train_time:47501ms step_avg:57.72ms
step:824/2330 train_time:47561ms step_avg:57.72ms
step:825/2330 train_time:47618ms step_avg:57.72ms
step:826/2330 train_time:47679ms step_avg:57.72ms
step:827/2330 train_time:47736ms step_avg:57.72ms
step:828/2330 train_time:47797ms step_avg:57.73ms
step:829/2330 train_time:47854ms step_avg:57.73ms
step:830/2330 train_time:47913ms step_avg:57.73ms
step:831/2330 train_time:47970ms step_avg:57.73ms
step:832/2330 train_time:48030ms step_avg:57.73ms
step:833/2330 train_time:48087ms step_avg:57.73ms
step:834/2330 train_time:48146ms step_avg:57.73ms
step:835/2330 train_time:48202ms step_avg:57.73ms
step:836/2330 train_time:48263ms step_avg:57.73ms
step:837/2330 train_time:48320ms step_avg:57.73ms
step:838/2330 train_time:48381ms step_avg:57.73ms
step:839/2330 train_time:48439ms step_avg:57.73ms
step:840/2330 train_time:48499ms step_avg:57.74ms
step:841/2330 train_time:48556ms step_avg:57.74ms
step:842/2330 train_time:48617ms step_avg:57.74ms
step:843/2330 train_time:48673ms step_avg:57.74ms
step:844/2330 train_time:48735ms step_avg:57.74ms
step:845/2330 train_time:48791ms step_avg:57.74ms
step:846/2330 train_time:48851ms step_avg:57.74ms
step:847/2330 train_time:48908ms step_avg:57.74ms
step:848/2330 train_time:48967ms step_avg:57.74ms
step:849/2330 train_time:49025ms step_avg:57.74ms
step:850/2330 train_time:49084ms step_avg:57.75ms
step:851/2330 train_time:49141ms step_avg:57.74ms
step:852/2330 train_time:49202ms step_avg:57.75ms
step:853/2330 train_time:49258ms step_avg:57.75ms
step:854/2330 train_time:49319ms step_avg:57.75ms
step:855/2330 train_time:49376ms step_avg:57.75ms
step:856/2330 train_time:49437ms step_avg:57.75ms
step:857/2330 train_time:49495ms step_avg:57.75ms
step:858/2330 train_time:49553ms step_avg:57.75ms
step:859/2330 train_time:49611ms step_avg:57.75ms
step:860/2330 train_time:49671ms step_avg:57.76ms
step:861/2330 train_time:49728ms step_avg:57.76ms
step:862/2330 train_time:49788ms step_avg:57.76ms
step:863/2330 train_time:49844ms step_avg:57.76ms
step:864/2330 train_time:49905ms step_avg:57.76ms
step:865/2330 train_time:49961ms step_avg:57.76ms
step:866/2330 train_time:50023ms step_avg:57.76ms
step:867/2330 train_time:50079ms step_avg:57.76ms
step:868/2330 train_time:50140ms step_avg:57.77ms
step:869/2330 train_time:50197ms step_avg:57.76ms
step:870/2330 train_time:50258ms step_avg:57.77ms
step:871/2330 train_time:50315ms step_avg:57.77ms
step:872/2330 train_time:50375ms step_avg:57.77ms
step:873/2330 train_time:50432ms step_avg:57.77ms
step:874/2330 train_time:50493ms step_avg:57.77ms
step:875/2330 train_time:50550ms step_avg:57.77ms
step:876/2330 train_time:50610ms step_avg:57.77ms
step:877/2330 train_time:50666ms step_avg:57.77ms
step:878/2330 train_time:50726ms step_avg:57.77ms
step:879/2330 train_time:50784ms step_avg:57.77ms
step:880/2330 train_time:50843ms step_avg:57.78ms
step:881/2330 train_time:50900ms step_avg:57.78ms
step:882/2330 train_time:50960ms step_avg:57.78ms
step:883/2330 train_time:51017ms step_avg:57.78ms
step:884/2330 train_time:51077ms step_avg:57.78ms
step:885/2330 train_time:51133ms step_avg:57.78ms
step:886/2330 train_time:51194ms step_avg:57.78ms
step:887/2330 train_time:51251ms step_avg:57.78ms
step:888/2330 train_time:51310ms step_avg:57.78ms
step:889/2330 train_time:51367ms step_avg:57.78ms
step:890/2330 train_time:51427ms step_avg:57.78ms
step:891/2330 train_time:51483ms step_avg:57.78ms
step:892/2330 train_time:51544ms step_avg:57.79ms
step:893/2330 train_time:51600ms step_avg:57.78ms
step:894/2330 train_time:51662ms step_avg:57.79ms
step:895/2330 train_time:51719ms step_avg:57.79ms
step:896/2330 train_time:51780ms step_avg:57.79ms
step:897/2330 train_time:51837ms step_avg:57.79ms
step:898/2330 train_time:51898ms step_avg:57.79ms
step:899/2330 train_time:51955ms step_avg:57.79ms
step:900/2330 train_time:52015ms step_avg:57.79ms
step:901/2330 train_time:52071ms step_avg:57.79ms
step:902/2330 train_time:52131ms step_avg:57.80ms
step:903/2330 train_time:52188ms step_avg:57.79ms
step:904/2330 train_time:52247ms step_avg:57.80ms
step:905/2330 train_time:52304ms step_avg:57.79ms
step:906/2330 train_time:52364ms step_avg:57.80ms
step:907/2330 train_time:52421ms step_avg:57.80ms
step:908/2330 train_time:52481ms step_avg:57.80ms
step:909/2330 train_time:52537ms step_avg:57.80ms
step:910/2330 train_time:52599ms step_avg:57.80ms
step:911/2330 train_time:52656ms step_avg:57.80ms
step:912/2330 train_time:52716ms step_avg:57.80ms
step:913/2330 train_time:52773ms step_avg:57.80ms
step:914/2330 train_time:52833ms step_avg:57.80ms
step:915/2330 train_time:52890ms step_avg:57.80ms
step:916/2330 train_time:52949ms step_avg:57.81ms
step:917/2330 train_time:53006ms step_avg:57.80ms
step:918/2330 train_time:53066ms step_avg:57.81ms
step:919/2330 train_time:53123ms step_avg:57.80ms
step:920/2330 train_time:53183ms step_avg:57.81ms
step:921/2330 train_time:53240ms step_avg:57.81ms
step:922/2330 train_time:53300ms step_avg:57.81ms
step:923/2330 train_time:53357ms step_avg:57.81ms
step:924/2330 train_time:53418ms step_avg:57.81ms
step:925/2330 train_time:53474ms step_avg:57.81ms
step:926/2330 train_time:53534ms step_avg:57.81ms
step:927/2330 train_time:53592ms step_avg:57.81ms
step:928/2330 train_time:53651ms step_avg:57.81ms
step:929/2330 train_time:53707ms step_avg:57.81ms
step:930/2330 train_time:53768ms step_avg:57.81ms
step:931/2330 train_time:53825ms step_avg:57.81ms
step:932/2330 train_time:53885ms step_avg:57.82ms
step:933/2330 train_time:53942ms step_avg:57.82ms
step:934/2330 train_time:54003ms step_avg:57.82ms
step:935/2330 train_time:54059ms step_avg:57.82ms
step:936/2330 train_time:54120ms step_avg:57.82ms
step:937/2330 train_time:54177ms step_avg:57.82ms
step:938/2330 train_time:54237ms step_avg:57.82ms
step:939/2330 train_time:54293ms step_avg:57.82ms
step:940/2330 train_time:54354ms step_avg:57.82ms
step:941/2330 train_time:54411ms step_avg:57.82ms
step:942/2330 train_time:54470ms step_avg:57.82ms
step:943/2330 train_time:54526ms step_avg:57.82ms
step:944/2330 train_time:54586ms step_avg:57.82ms
step:945/2330 train_time:54643ms step_avg:57.82ms
step:946/2330 train_time:54703ms step_avg:57.83ms
step:947/2330 train_time:54760ms step_avg:57.82ms
step:948/2330 train_time:54822ms step_avg:57.83ms
step:949/2330 train_time:54879ms step_avg:57.83ms
step:950/2330 train_time:54940ms step_avg:57.83ms
step:951/2330 train_time:54997ms step_avg:57.83ms
step:952/2330 train_time:55058ms step_avg:57.83ms
step:953/2330 train_time:55115ms step_avg:57.83ms
step:954/2330 train_time:55175ms step_avg:57.84ms
step:955/2330 train_time:55232ms step_avg:57.83ms
step:956/2330 train_time:55292ms step_avg:57.84ms
step:957/2330 train_time:55348ms step_avg:57.84ms
step:958/2330 train_time:55408ms step_avg:57.84ms
step:959/2330 train_time:55465ms step_avg:57.84ms
step:960/2330 train_time:55525ms step_avg:57.84ms
step:961/2330 train_time:55581ms step_avg:57.84ms
step:962/2330 train_time:55642ms step_avg:57.84ms
step:963/2330 train_time:55699ms step_avg:57.84ms
step:964/2330 train_time:55760ms step_avg:57.84ms
step:965/2330 train_time:55816ms step_avg:57.84ms
step:966/2330 train_time:55877ms step_avg:57.84ms
step:967/2330 train_time:55934ms step_avg:57.84ms
step:968/2330 train_time:55993ms step_avg:57.84ms
step:969/2330 train_time:56050ms step_avg:57.84ms
step:970/2330 train_time:56110ms step_avg:57.85ms
step:971/2330 train_time:56166ms step_avg:57.84ms
step:972/2330 train_time:56228ms step_avg:57.85ms
step:973/2330 train_time:56284ms step_avg:57.85ms
step:974/2330 train_time:56345ms step_avg:57.85ms
step:975/2330 train_time:56401ms step_avg:57.85ms
step:976/2330 train_time:56462ms step_avg:57.85ms
step:977/2330 train_time:56519ms step_avg:57.85ms
step:978/2330 train_time:56578ms step_avg:57.85ms
step:979/2330 train_time:56635ms step_avg:57.85ms
step:980/2330 train_time:56696ms step_avg:57.85ms
step:981/2330 train_time:56753ms step_avg:57.85ms
step:982/2330 train_time:56813ms step_avg:57.85ms
step:983/2330 train_time:56869ms step_avg:57.85ms
step:984/2330 train_time:56930ms step_avg:57.86ms
step:985/2330 train_time:56986ms step_avg:57.85ms
step:986/2330 train_time:57046ms step_avg:57.86ms
step:987/2330 train_time:57102ms step_avg:57.85ms
step:988/2330 train_time:57164ms step_avg:57.86ms
step:989/2330 train_time:57221ms step_avg:57.86ms
step:990/2330 train_time:57281ms step_avg:57.86ms
step:991/2330 train_time:57338ms step_avg:57.86ms
step:992/2330 train_time:57399ms step_avg:57.86ms
step:993/2330 train_time:57455ms step_avg:57.86ms
step:994/2330 train_time:57516ms step_avg:57.86ms
step:995/2330 train_time:57573ms step_avg:57.86ms
step:996/2330 train_time:57633ms step_avg:57.86ms
step:997/2330 train_time:57690ms step_avg:57.86ms
step:998/2330 train_time:57750ms step_avg:57.87ms
step:999/2330 train_time:57806ms step_avg:57.86ms
step:1000/2330 train_time:57866ms step_avg:57.87ms
step:1000/2330 val_loss:4.0673 train_time:57947ms step_avg:57.95ms
step:1001/2330 train_time:57967ms step_avg:57.91ms
step:1002/2330 train_time:57987ms step_avg:57.87ms
step:1003/2330 train_time:58039ms step_avg:57.87ms
step:1004/2330 train_time:58107ms step_avg:57.88ms
step:1005/2330 train_time:58163ms step_avg:57.87ms
step:1006/2330 train_time:58227ms step_avg:57.88ms
step:1007/2330 train_time:58283ms step_avg:57.88ms
step:1008/2330 train_time:58343ms step_avg:57.88ms
step:1009/2330 train_time:58400ms step_avg:57.88ms
step:1010/2330 train_time:58458ms step_avg:57.88ms
step:1011/2330 train_time:58514ms step_avg:57.88ms
step:1012/2330 train_time:58574ms step_avg:57.88ms
step:1013/2330 train_time:58630ms step_avg:57.88ms
step:1014/2330 train_time:58690ms step_avg:57.88ms
step:1015/2330 train_time:58746ms step_avg:57.88ms
step:1016/2330 train_time:58805ms step_avg:57.88ms
step:1017/2330 train_time:58865ms step_avg:57.88ms
step:1018/2330 train_time:58928ms step_avg:57.89ms
step:1019/2330 train_time:58987ms step_avg:57.89ms
step:1020/2330 train_time:59048ms step_avg:57.89ms
step:1021/2330 train_time:59105ms step_avg:57.89ms
step:1022/2330 train_time:59166ms step_avg:57.89ms
step:1023/2330 train_time:59223ms step_avg:57.89ms
step:1024/2330 train_time:59283ms step_avg:57.89ms
step:1025/2330 train_time:59340ms step_avg:57.89ms
step:1026/2330 train_time:59398ms step_avg:57.89ms
step:1027/2330 train_time:59455ms step_avg:57.89ms
step:1028/2330 train_time:59514ms step_avg:57.89ms
step:1029/2330 train_time:59571ms step_avg:57.89ms
step:1030/2330 train_time:59630ms step_avg:57.89ms
step:1031/2330 train_time:59687ms step_avg:57.89ms
step:1032/2330 train_time:59746ms step_avg:57.89ms
step:1033/2330 train_time:59803ms step_avg:57.89ms
step:1034/2330 train_time:59864ms step_avg:57.90ms
step:1035/2330 train_time:59922ms step_avg:57.90ms
step:1036/2330 train_time:59982ms step_avg:57.90ms
step:1037/2330 train_time:60040ms step_avg:57.90ms
step:1038/2330 train_time:60100ms step_avg:57.90ms
step:1039/2330 train_time:60158ms step_avg:57.90ms
step:1040/2330 train_time:60218ms step_avg:57.90ms
step:1041/2330 train_time:60275ms step_avg:57.90ms
step:1042/2330 train_time:60335ms step_avg:57.90ms
step:1043/2330 train_time:60392ms step_avg:57.90ms
step:1044/2330 train_time:60452ms step_avg:57.90ms
step:1045/2330 train_time:60507ms step_avg:57.90ms
step:1046/2330 train_time:60568ms step_avg:57.90ms
step:1047/2330 train_time:60625ms step_avg:57.90ms
step:1048/2330 train_time:60685ms step_avg:57.91ms
step:1049/2330 train_time:60743ms step_avg:57.91ms
step:1050/2330 train_time:60802ms step_avg:57.91ms
step:1051/2330 train_time:60860ms step_avg:57.91ms
step:1052/2330 train_time:60920ms step_avg:57.91ms
step:1053/2330 train_time:60976ms step_avg:57.91ms
step:1054/2330 train_time:61037ms step_avg:57.91ms
step:1055/2330 train_time:61094ms step_avg:57.91ms
step:1056/2330 train_time:61156ms step_avg:57.91ms
step:1057/2330 train_time:61212ms step_avg:57.91ms
step:1058/2330 train_time:61274ms step_avg:57.91ms
step:1059/2330 train_time:61330ms step_avg:57.91ms
step:1060/2330 train_time:61390ms step_avg:57.92ms
step:1061/2330 train_time:61446ms step_avg:57.91ms
step:1062/2330 train_time:61508ms step_avg:57.92ms
step:1063/2330 train_time:61564ms step_avg:57.92ms
step:1064/2330 train_time:61623ms step_avg:57.92ms
step:1065/2330 train_time:61680ms step_avg:57.92ms
step:1066/2330 train_time:61740ms step_avg:57.92ms
step:1067/2330 train_time:61797ms step_avg:57.92ms
step:1068/2330 train_time:61856ms step_avg:57.92ms
step:1069/2330 train_time:61913ms step_avg:57.92ms
step:1070/2330 train_time:61974ms step_avg:57.92ms
step:1071/2330 train_time:62030ms step_avg:57.92ms
step:1072/2330 train_time:62091ms step_avg:57.92ms
step:1073/2330 train_time:62148ms step_avg:57.92ms
step:1074/2330 train_time:62210ms step_avg:57.92ms
step:1075/2330 train_time:62267ms step_avg:57.92ms
step:1076/2330 train_time:62327ms step_avg:57.92ms
step:1077/2330 train_time:62384ms step_avg:57.92ms
step:1078/2330 train_time:62444ms step_avg:57.93ms
step:1079/2330 train_time:62501ms step_avg:57.93ms
step:1080/2330 train_time:62561ms step_avg:57.93ms
step:1081/2330 train_time:62617ms step_avg:57.93ms
step:1082/2330 train_time:62677ms step_avg:57.93ms
step:1083/2330 train_time:62735ms step_avg:57.93ms
step:1084/2330 train_time:62794ms step_avg:57.93ms
step:1085/2330 train_time:62851ms step_avg:57.93ms
step:1086/2330 train_time:62912ms step_avg:57.93ms
step:1087/2330 train_time:62969ms step_avg:57.93ms
step:1088/2330 train_time:63030ms step_avg:57.93ms
step:1089/2330 train_time:63087ms step_avg:57.93ms
step:1090/2330 train_time:63149ms step_avg:57.93ms
step:1091/2330 train_time:63205ms step_avg:57.93ms
step:1092/2330 train_time:63265ms step_avg:57.94ms
step:1093/2330 train_time:63322ms step_avg:57.93ms
step:1094/2330 train_time:63382ms step_avg:57.94ms
step:1095/2330 train_time:63439ms step_avg:57.94ms
step:1096/2330 train_time:63499ms step_avg:57.94ms
step:1097/2330 train_time:63557ms step_avg:57.94ms
step:1098/2330 train_time:63616ms step_avg:57.94ms
step:1099/2330 train_time:63673ms step_avg:57.94ms
step:1100/2330 train_time:63732ms step_avg:57.94ms
step:1101/2330 train_time:63790ms step_avg:57.94ms
step:1102/2330 train_time:63851ms step_avg:57.94ms
step:1103/2330 train_time:63908ms step_avg:57.94ms
step:1104/2330 train_time:63968ms step_avg:57.94ms
step:1105/2330 train_time:64025ms step_avg:57.94ms
step:1106/2330 train_time:64085ms step_avg:57.94ms
step:1107/2330 train_time:64142ms step_avg:57.94ms
step:1108/2330 train_time:64203ms step_avg:57.94ms
step:1109/2330 train_time:64259ms step_avg:57.94ms
step:1110/2330 train_time:64320ms step_avg:57.95ms
step:1111/2330 train_time:64377ms step_avg:57.95ms
step:1112/2330 train_time:64437ms step_avg:57.95ms
step:1113/2330 train_time:64494ms step_avg:57.95ms
step:1114/2330 train_time:64553ms step_avg:57.95ms
step:1115/2330 train_time:64610ms step_avg:57.95ms
step:1116/2330 train_time:64670ms step_avg:57.95ms
step:1117/2330 train_time:64728ms step_avg:57.95ms
step:1118/2330 train_time:64788ms step_avg:57.95ms
step:1119/2330 train_time:64845ms step_avg:57.95ms
step:1120/2330 train_time:64905ms step_avg:57.95ms
step:1121/2330 train_time:64962ms step_avg:57.95ms
step:1122/2330 train_time:65021ms step_avg:57.95ms
step:1123/2330 train_time:65078ms step_avg:57.95ms
step:1124/2330 train_time:65138ms step_avg:57.95ms
step:1125/2330 train_time:65196ms step_avg:57.95ms
step:1126/2330 train_time:65255ms step_avg:57.95ms
step:1127/2330 train_time:65312ms step_avg:57.95ms
step:1128/2330 train_time:65373ms step_avg:57.95ms
step:1129/2330 train_time:65429ms step_avg:57.95ms
step:1130/2330 train_time:65491ms step_avg:57.96ms
step:1131/2330 train_time:65547ms step_avg:57.95ms
step:1132/2330 train_time:65608ms step_avg:57.96ms
step:1133/2330 train_time:65665ms step_avg:57.96ms
step:1134/2330 train_time:65725ms step_avg:57.96ms
step:1135/2330 train_time:65782ms step_avg:57.96ms
step:1136/2330 train_time:65841ms step_avg:57.96ms
step:1137/2330 train_time:65898ms step_avg:57.96ms
step:1138/2330 train_time:65958ms step_avg:57.96ms
step:1139/2330 train_time:66015ms step_avg:57.96ms
step:1140/2330 train_time:66075ms step_avg:57.96ms
step:1141/2330 train_time:66132ms step_avg:57.96ms
step:1142/2330 train_time:66192ms step_avg:57.96ms
step:1143/2330 train_time:66249ms step_avg:57.96ms
step:1144/2330 train_time:66310ms step_avg:57.96ms
step:1145/2330 train_time:66367ms step_avg:57.96ms
step:1146/2330 train_time:66846ms step_avg:58.33ms
step:1147/2330 train_time:66901ms step_avg:58.33ms
step:1148/2330 train_time:66960ms step_avg:58.33ms
step:1149/2330 train_time:67016ms step_avg:58.33ms
step:1150/2330 train_time:67075ms step_avg:58.33ms
step:1151/2330 train_time:67131ms step_avg:58.32ms
step:1152/2330 train_time:67191ms step_avg:58.33ms
step:1153/2330 train_time:67246ms step_avg:58.32ms
step:1154/2330 train_time:67306ms step_avg:58.32ms
step:1155/2330 train_time:67362ms step_avg:58.32ms
step:1156/2330 train_time:67422ms step_avg:58.32ms
step:1157/2330 train_time:67478ms step_avg:58.32ms
step:1158/2330 train_time:67537ms step_avg:58.32ms
step:1159/2330 train_time:67593ms step_avg:58.32ms
step:1160/2330 train_time:67653ms step_avg:58.32ms
step:1161/2330 train_time:67712ms step_avg:58.32ms
step:1162/2330 train_time:67779ms step_avg:58.33ms
step:1163/2330 train_time:67835ms step_avg:58.33ms
step:1164/2330 train_time:67899ms step_avg:58.33ms
step:1165/2330 train_time:67954ms step_avg:58.33ms
step:1166/2330 train_time:68015ms step_avg:58.33ms
step:1167/2330 train_time:68071ms step_avg:58.33ms
step:1168/2330 train_time:68132ms step_avg:58.33ms
step:1169/2330 train_time:68189ms step_avg:58.33ms
step:1170/2330 train_time:68249ms step_avg:58.33ms
step:1171/2330 train_time:68305ms step_avg:58.33ms
step:1172/2330 train_time:68365ms step_avg:58.33ms
step:1173/2330 train_time:68421ms step_avg:58.33ms
step:1174/2330 train_time:68480ms step_avg:58.33ms
step:1175/2330 train_time:68537ms step_avg:58.33ms
step:1176/2330 train_time:68596ms step_avg:58.33ms
step:1177/2330 train_time:68654ms step_avg:58.33ms
step:1178/2330 train_time:68714ms step_avg:58.33ms
step:1179/2330 train_time:68774ms step_avg:58.33ms
step:1180/2330 train_time:68834ms step_avg:58.33ms
step:1181/2330 train_time:68892ms step_avg:58.33ms
step:1182/2330 train_time:68953ms step_avg:58.34ms
step:1183/2330 train_time:69009ms step_avg:58.33ms
step:1184/2330 train_time:69071ms step_avg:58.34ms
step:1185/2330 train_time:69127ms step_avg:58.34ms
step:1186/2330 train_time:69188ms step_avg:58.34ms
step:1187/2330 train_time:69244ms step_avg:58.34ms
step:1188/2330 train_time:69304ms step_avg:58.34ms
step:1189/2330 train_time:69360ms step_avg:58.33ms
step:1190/2330 train_time:69419ms step_avg:58.34ms
step:1191/2330 train_time:69475ms step_avg:58.33ms
step:1192/2330 train_time:69535ms step_avg:58.33ms
step:1193/2330 train_time:69592ms step_avg:58.33ms
step:1194/2330 train_time:69652ms step_avg:58.33ms
step:1195/2330 train_time:69708ms step_avg:58.33ms
step:1196/2330 train_time:69771ms step_avg:58.34ms
step:1197/2330 train_time:69830ms step_avg:58.34ms
step:1198/2330 train_time:69891ms step_avg:58.34ms
step:1199/2330 train_time:69948ms step_avg:58.34ms
step:1200/2330 train_time:70009ms step_avg:58.34ms
step:1201/2330 train_time:70066ms step_avg:58.34ms
step:1202/2330 train_time:70125ms step_avg:58.34ms
step:1203/2330 train_time:70182ms step_avg:58.34ms
step:1204/2330 train_time:70241ms step_avg:58.34ms
step:1205/2330 train_time:70298ms step_avg:58.34ms
step:1206/2330 train_time:70357ms step_avg:58.34ms
step:1207/2330 train_time:70413ms step_avg:58.34ms
step:1208/2330 train_time:70475ms step_avg:58.34ms
step:1209/2330 train_time:70531ms step_avg:58.34ms
step:1210/2330 train_time:70592ms step_avg:58.34ms
step:1211/2330 train_time:70649ms step_avg:58.34ms
step:1212/2330 train_time:70710ms step_avg:58.34ms
step:1213/2330 train_time:70767ms step_avg:58.34ms
step:1214/2330 train_time:70830ms step_avg:58.34ms
step:1215/2330 train_time:70888ms step_avg:58.34ms
step:1216/2330 train_time:70948ms step_avg:58.35ms
step:1217/2330 train_time:71005ms step_avg:58.34ms
step:1218/2330 train_time:71065ms step_avg:58.35ms
step:1219/2330 train_time:71121ms step_avg:58.34ms
step:1220/2330 train_time:71182ms step_avg:58.35ms
step:1221/2330 train_time:71239ms step_avg:58.34ms
step:1222/2330 train_time:71299ms step_avg:58.35ms
step:1223/2330 train_time:71356ms step_avg:58.35ms
step:1224/2330 train_time:71415ms step_avg:58.35ms
step:1225/2330 train_time:71472ms step_avg:58.34ms
step:1226/2330 train_time:71532ms step_avg:58.35ms
step:1227/2330 train_time:71588ms step_avg:58.34ms
step:1228/2330 train_time:71649ms step_avg:58.35ms
step:1229/2330 train_time:71706ms step_avg:58.35ms
step:1230/2330 train_time:71767ms step_avg:58.35ms
step:1231/2330 train_time:71825ms step_avg:58.35ms
step:1232/2330 train_time:71885ms step_avg:58.35ms
step:1233/2330 train_time:71942ms step_avg:58.35ms
step:1234/2330 train_time:72002ms step_avg:58.35ms
step:1235/2330 train_time:72059ms step_avg:58.35ms
step:1236/2330 train_time:72119ms step_avg:58.35ms
step:1237/2330 train_time:72175ms step_avg:58.35ms
step:1238/2330 train_time:72236ms step_avg:58.35ms
step:1239/2330 train_time:72293ms step_avg:58.35ms
step:1240/2330 train_time:72352ms step_avg:58.35ms
step:1241/2330 train_time:72408ms step_avg:58.35ms
step:1242/2330 train_time:72470ms step_avg:58.35ms
step:1243/2330 train_time:72527ms step_avg:58.35ms
step:1244/2330 train_time:72587ms step_avg:58.35ms
step:1245/2330 train_time:72644ms step_avg:58.35ms
step:1246/2330 train_time:72704ms step_avg:58.35ms
step:1247/2330 train_time:72762ms step_avg:58.35ms
step:1248/2330 train_time:72822ms step_avg:58.35ms
step:1249/2330 train_time:72879ms step_avg:58.35ms
step:1250/2330 train_time:72939ms step_avg:58.35ms
step:1250/2330 val_loss:3.9912 train_time:73020ms step_avg:58.42ms
step:1251/2330 train_time:73040ms step_avg:58.39ms
step:1252/2330 train_time:73059ms step_avg:58.35ms
step:1253/2330 train_time:73116ms step_avg:58.35ms
step:1254/2330 train_time:73186ms step_avg:58.36ms
step:1255/2330 train_time:73242ms step_avg:58.36ms
step:1256/2330 train_time:73304ms step_avg:58.36ms
step:1257/2330 train_time:73360ms step_avg:58.36ms
step:1258/2330 train_time:73421ms step_avg:58.36ms
step:1259/2330 train_time:73477ms step_avg:58.36ms
step:1260/2330 train_time:73537ms step_avg:58.36ms
step:1261/2330 train_time:73593ms step_avg:58.36ms
step:1262/2330 train_time:73652ms step_avg:58.36ms
step:1263/2330 train_time:73709ms step_avg:58.36ms
step:1264/2330 train_time:73768ms step_avg:58.36ms
step:1265/2330 train_time:73824ms step_avg:58.36ms
step:1266/2330 train_time:73883ms step_avg:58.36ms
step:1267/2330 train_time:73939ms step_avg:58.36ms
step:1268/2330 train_time:74000ms step_avg:58.36ms
step:1269/2330 train_time:74065ms step_avg:58.36ms
step:1270/2330 train_time:74122ms step_avg:58.36ms
step:1271/2330 train_time:74180ms step_avg:58.36ms
step:1272/2330 train_time:74241ms step_avg:58.37ms
step:1273/2330 train_time:74298ms step_avg:58.36ms
step:1274/2330 train_time:74360ms step_avg:58.37ms
step:1275/2330 train_time:74416ms step_avg:58.37ms
step:1276/2330 train_time:74477ms step_avg:58.37ms
step:1277/2330 train_time:74533ms step_avg:58.37ms
step:1278/2330 train_time:74594ms step_avg:58.37ms
step:1279/2330 train_time:74650ms step_avg:58.37ms
step:1280/2330 train_time:74710ms step_avg:58.37ms
step:1281/2330 train_time:74767ms step_avg:58.37ms
step:1282/2330 train_time:74826ms step_avg:58.37ms
step:1283/2330 train_time:74883ms step_avg:58.37ms
step:1284/2330 train_time:74943ms step_avg:58.37ms
step:1285/2330 train_time:75000ms step_avg:58.37ms
step:1286/2330 train_time:75061ms step_avg:58.37ms
step:1287/2330 train_time:75118ms step_avg:58.37ms
step:1288/2330 train_time:75179ms step_avg:58.37ms
step:1289/2330 train_time:75237ms step_avg:58.37ms
step:1290/2330 train_time:75714ms step_avg:58.69ms
step:1291/2330 train_time:75734ms step_avg:58.66ms
step:1292/2330 train_time:75775ms step_avg:58.65ms
step:1293/2330 train_time:75832ms step_avg:58.65ms
step:1294/2330 train_time:75890ms step_avg:58.65ms
step:1295/2330 train_time:75946ms step_avg:58.65ms
step:1296/2330 train_time:76005ms step_avg:58.65ms
step:1297/2330 train_time:76062ms step_avg:58.64ms
step:1298/2330 train_time:76121ms step_avg:58.64ms
step:1299/2330 train_time:76177ms step_avg:58.64ms
step:1300/2330 train_time:76236ms step_avg:58.64ms
step:1301/2330 train_time:76292ms step_avg:58.64ms
step:1302/2330 train_time:76352ms step_avg:58.64ms
step:1303/2330 train_time:76408ms step_avg:58.64ms
step:1304/2330 train_time:76467ms step_avg:58.64ms
step:1305/2330 train_time:76523ms step_avg:58.64ms
step:1306/2330 train_time:76584ms step_avg:58.64ms
step:1307/2330 train_time:76646ms step_avg:58.64ms
step:1308/2330 train_time:76708ms step_avg:58.65ms
step:1309/2330 train_time:76766ms step_avg:58.64ms
step:1310/2330 train_time:76828ms step_avg:58.65ms
step:1311/2330 train_time:76885ms step_avg:58.65ms
step:1312/2330 train_time:76945ms step_avg:58.65ms
step:1313/2330 train_time:77002ms step_avg:58.65ms
step:1314/2330 train_time:77061ms step_avg:58.65ms
step:1315/2330 train_time:77118ms step_avg:58.64ms
step:1316/2330 train_time:77177ms step_avg:58.65ms
step:1317/2330 train_time:77233ms step_avg:58.64ms
step:1318/2330 train_time:77293ms step_avg:58.64ms
step:1319/2330 train_time:77349ms step_avg:58.64ms
step:1320/2330 train_time:77408ms step_avg:58.64ms
step:1321/2330 train_time:77465ms step_avg:58.64ms
step:1322/2330 train_time:77525ms step_avg:58.64ms
step:1323/2330 train_time:77583ms step_avg:58.64ms
step:1324/2330 train_time:77643ms step_avg:58.64ms
step:1325/2330 train_time:77700ms step_avg:58.64ms
step:1326/2330 train_time:77763ms step_avg:58.64ms
step:1327/2330 train_time:77820ms step_avg:58.64ms
step:1328/2330 train_time:77882ms step_avg:58.65ms
step:1329/2330 train_time:77938ms step_avg:58.64ms
step:1330/2330 train_time:78000ms step_avg:58.65ms
step:1331/2330 train_time:78057ms step_avg:58.65ms
step:1332/2330 train_time:78117ms step_avg:58.65ms
step:1333/2330 train_time:78173ms step_avg:58.64ms
step:1334/2330 train_time:78232ms step_avg:58.65ms
step:1335/2330 train_time:78289ms step_avg:58.64ms
step:1336/2330 train_time:78349ms step_avg:58.64ms
step:1337/2330 train_time:78405ms step_avg:58.64ms
step:1338/2330 train_time:78465ms step_avg:58.64ms
step:1339/2330 train_time:78523ms step_avg:58.64ms
step:1340/2330 train_time:78582ms step_avg:58.64ms
step:1341/2330 train_time:78640ms step_avg:58.64ms
step:1342/2330 train_time:78702ms step_avg:58.65ms
step:1343/2330 train_time:78759ms step_avg:58.64ms
step:1344/2330 train_time:78821ms step_avg:58.65ms
step:1345/2330 train_time:78877ms step_avg:58.64ms
step:1346/2330 train_time:78940ms step_avg:58.65ms
step:1347/2330 train_time:78996ms step_avg:58.65ms
step:1348/2330 train_time:79057ms step_avg:58.65ms
step:1349/2330 train_time:79114ms step_avg:58.65ms
step:1350/2330 train_time:79174ms step_avg:58.65ms
step:1351/2330 train_time:79231ms step_avg:58.65ms
step:1352/2330 train_time:79291ms step_avg:58.65ms
step:1353/2330 train_time:79347ms step_avg:58.65ms
step:1354/2330 train_time:79407ms step_avg:58.65ms
step:1355/2330 train_time:79465ms step_avg:58.65ms
step:1356/2330 train_time:79524ms step_avg:58.65ms
step:1357/2330 train_time:79582ms step_avg:58.65ms
step:1358/2330 train_time:79642ms step_avg:58.65ms
step:1359/2330 train_time:79699ms step_avg:58.65ms
step:1360/2330 train_time:79760ms step_avg:58.65ms
step:1361/2330 train_time:79818ms step_avg:58.65ms
step:1362/2330 train_time:79879ms step_avg:58.65ms
step:1363/2330 train_time:79935ms step_avg:58.65ms
step:1364/2330 train_time:79997ms step_avg:58.65ms
step:1365/2330 train_time:80054ms step_avg:58.65ms
step:1366/2330 train_time:80114ms step_avg:58.65ms
step:1367/2330 train_time:80171ms step_avg:58.65ms
step:1368/2330 train_time:80230ms step_avg:58.65ms
step:1369/2330 train_time:80286ms step_avg:58.65ms
step:1370/2330 train_time:80347ms step_avg:58.65ms
step:1371/2330 train_time:80403ms step_avg:58.65ms
step:1372/2330 train_time:80463ms step_avg:58.65ms
step:1373/2330 train_time:80521ms step_avg:58.65ms
step:1374/2330 train_time:80580ms step_avg:58.65ms
step:1375/2330 train_time:80636ms step_avg:58.64ms
step:1376/2330 train_time:80698ms step_avg:58.65ms
step:1377/2330 train_time:80755ms step_avg:58.65ms
step:1378/2330 train_time:80815ms step_avg:58.65ms
step:1379/2330 train_time:80873ms step_avg:58.65ms
step:1380/2330 train_time:80933ms step_avg:58.65ms
step:1381/2330 train_time:80990ms step_avg:58.65ms
step:1382/2330 train_time:81050ms step_avg:58.65ms
step:1383/2330 train_time:81107ms step_avg:58.65ms
step:1384/2330 train_time:81167ms step_avg:58.65ms
step:1385/2330 train_time:81224ms step_avg:58.65ms
step:1386/2330 train_time:81283ms step_avg:58.65ms
step:1387/2330 train_time:81339ms step_avg:58.64ms
step:1388/2330 train_time:81400ms step_avg:58.65ms
step:1389/2330 train_time:81457ms step_avg:58.64ms
step:1390/2330 train_time:81518ms step_avg:58.65ms
step:1391/2330 train_time:81575ms step_avg:58.64ms
step:1392/2330 train_time:81636ms step_avg:58.65ms
step:1393/2330 train_time:81693ms step_avg:58.65ms
step:1394/2330 train_time:81753ms step_avg:58.65ms
step:1395/2330 train_time:81810ms step_avg:58.64ms
step:1396/2330 train_time:81870ms step_avg:58.65ms
step:1397/2330 train_time:81926ms step_avg:58.64ms
step:1398/2330 train_time:81987ms step_avg:58.65ms
step:1399/2330 train_time:82044ms step_avg:58.64ms
step:1400/2330 train_time:82105ms step_avg:58.65ms
step:1401/2330 train_time:82162ms step_avg:58.65ms
step:1402/2330 train_time:82222ms step_avg:58.65ms
step:1403/2330 train_time:82279ms step_avg:58.65ms
step:1404/2330 train_time:82340ms step_avg:58.65ms
step:1405/2330 train_time:82396ms step_avg:58.65ms
step:1406/2330 train_time:82457ms step_avg:58.65ms
step:1407/2330 train_time:82514ms step_avg:58.65ms
step:1408/2330 train_time:82575ms step_avg:58.65ms
step:1409/2330 train_time:82632ms step_avg:58.65ms
step:1410/2330 train_time:82693ms step_avg:58.65ms
step:1411/2330 train_time:82750ms step_avg:58.65ms
step:1412/2330 train_time:82810ms step_avg:58.65ms
step:1413/2330 train_time:82867ms step_avg:58.65ms
step:1414/2330 train_time:82927ms step_avg:58.65ms
step:1415/2330 train_time:82983ms step_avg:58.65ms
step:1416/2330 train_time:83043ms step_avg:58.65ms
step:1417/2330 train_time:83100ms step_avg:58.65ms
step:1418/2330 train_time:83161ms step_avg:58.65ms
step:1419/2330 train_time:83218ms step_avg:58.65ms
step:1420/2330 train_time:83278ms step_avg:58.65ms
step:1421/2330 train_time:83335ms step_avg:58.65ms
step:1422/2330 train_time:83396ms step_avg:58.65ms
step:1423/2330 train_time:83452ms step_avg:58.65ms
step:1424/2330 train_time:83512ms step_avg:58.65ms
step:1425/2330 train_time:83570ms step_avg:58.65ms
step:1426/2330 train_time:83629ms step_avg:58.65ms
step:1427/2330 train_time:83686ms step_avg:58.64ms
step:1428/2330 train_time:83746ms step_avg:58.65ms
step:1429/2330 train_time:83804ms step_avg:58.64ms
step:1430/2330 train_time:83864ms step_avg:58.65ms
step:1431/2330 train_time:83921ms step_avg:58.65ms
step:1432/2330 train_time:83982ms step_avg:58.65ms
step:1433/2330 train_time:84039ms step_avg:58.65ms
step:1434/2330 train_time:84099ms step_avg:58.65ms
step:1435/2330 train_time:84156ms step_avg:58.65ms
step:1436/2330 train_time:84217ms step_avg:58.65ms
step:1437/2330 train_time:84275ms step_avg:58.65ms
step:1438/2330 train_time:84334ms step_avg:58.65ms
step:1439/2330 train_time:84390ms step_avg:58.65ms
step:1440/2330 train_time:84451ms step_avg:58.65ms
step:1441/2330 train_time:84509ms step_avg:58.65ms
step:1442/2330 train_time:84569ms step_avg:58.65ms
step:1443/2330 train_time:84626ms step_avg:58.65ms
step:1444/2330 train_time:84685ms step_avg:58.65ms
step:1445/2330 train_time:84743ms step_avg:58.65ms
step:1446/2330 train_time:84803ms step_avg:58.65ms
step:1447/2330 train_time:84860ms step_avg:58.65ms
step:1448/2330 train_time:84920ms step_avg:58.65ms
step:1449/2330 train_time:84977ms step_avg:58.64ms
step:1450/2330 train_time:85038ms step_avg:58.65ms
step:1451/2330 train_time:85095ms step_avg:58.65ms
step:1452/2330 train_time:85155ms step_avg:58.65ms
step:1453/2330 train_time:85213ms step_avg:58.65ms
step:1454/2330 train_time:85272ms step_avg:58.65ms
step:1455/2330 train_time:85329ms step_avg:58.65ms
step:1456/2330 train_time:85389ms step_avg:58.65ms
step:1457/2330 train_time:85446ms step_avg:58.65ms
step:1458/2330 train_time:85506ms step_avg:58.65ms
step:1459/2330 train_time:85563ms step_avg:58.65ms
step:1460/2330 train_time:85624ms step_avg:58.65ms
step:1461/2330 train_time:85681ms step_avg:58.65ms
step:1462/2330 train_time:85741ms step_avg:58.65ms
step:1463/2330 train_time:85798ms step_avg:58.65ms
step:1464/2330 train_time:85859ms step_avg:58.65ms
step:1465/2330 train_time:85916ms step_avg:58.65ms
step:1466/2330 train_time:85977ms step_avg:58.65ms
step:1467/2330 train_time:86033ms step_avg:58.65ms
step:1468/2330 train_time:86094ms step_avg:58.65ms
step:1469/2330 train_time:86151ms step_avg:58.65ms
step:1470/2330 train_time:86211ms step_avg:58.65ms
step:1471/2330 train_time:86267ms step_avg:58.65ms
step:1472/2330 train_time:86327ms step_avg:58.65ms
step:1473/2330 train_time:86383ms step_avg:58.64ms
step:1474/2330 train_time:86444ms step_avg:58.65ms
step:1475/2330 train_time:86501ms step_avg:58.64ms
step:1476/2330 train_time:86561ms step_avg:58.65ms
step:1477/2330 train_time:86618ms step_avg:58.64ms
step:1478/2330 train_time:86679ms step_avg:58.65ms
step:1479/2330 train_time:86736ms step_avg:58.64ms
step:1480/2330 train_time:86797ms step_avg:58.65ms
step:1481/2330 train_time:86853ms step_avg:58.64ms
step:1482/2330 train_time:86915ms step_avg:58.65ms
step:1483/2330 train_time:86972ms step_avg:58.65ms
step:1484/2330 train_time:87032ms step_avg:58.65ms
step:1485/2330 train_time:87089ms step_avg:58.65ms
step:1486/2330 train_time:87149ms step_avg:58.65ms
step:1487/2330 train_time:87206ms step_avg:58.65ms
step:1488/2330 train_time:87266ms step_avg:58.65ms
step:1489/2330 train_time:87322ms step_avg:58.64ms
step:1490/2330 train_time:87383ms step_avg:58.65ms
step:1491/2330 train_time:87440ms step_avg:58.64ms
step:1492/2330 train_time:87501ms step_avg:58.65ms
step:1493/2330 train_time:87557ms step_avg:58.65ms
step:1494/2330 train_time:87618ms step_avg:58.65ms
step:1495/2330 train_time:87675ms step_avg:58.65ms
step:1496/2330 train_time:87737ms step_avg:58.65ms
step:1497/2330 train_time:87793ms step_avg:58.65ms
step:1498/2330 train_time:87854ms step_avg:58.65ms
step:1499/2330 train_time:87912ms step_avg:58.65ms
step:1500/2330 train_time:87972ms step_avg:58.65ms
step:1500/2330 val_loss:3.9069 train_time:88051ms step_avg:58.70ms
step:1501/2330 train_time:88072ms step_avg:58.68ms
step:1502/2330 train_time:88093ms step_avg:58.65ms
step:1503/2330 train_time:88151ms step_avg:58.65ms
step:1504/2330 train_time:88216ms step_avg:58.65ms
step:1505/2330 train_time:88273ms step_avg:58.65ms
step:1506/2330 train_time:88336ms step_avg:58.66ms
step:1507/2330 train_time:88393ms step_avg:58.65ms
step:1508/2330 train_time:88454ms step_avg:58.66ms
step:1509/2330 train_time:88510ms step_avg:58.65ms
step:1510/2330 train_time:88569ms step_avg:58.66ms
step:1511/2330 train_time:88625ms step_avg:58.65ms
step:1512/2330 train_time:88685ms step_avg:58.65ms
step:1513/2330 train_time:88742ms step_avg:58.65ms
step:1514/2330 train_time:88801ms step_avg:58.65ms
step:1515/2330 train_time:88857ms step_avg:58.65ms
step:1516/2330 train_time:88916ms step_avg:58.65ms
step:1517/2330 train_time:88972ms step_avg:58.65ms
step:1518/2330 train_time:89032ms step_avg:58.65ms
step:1519/2330 train_time:89091ms step_avg:58.65ms
step:1520/2330 train_time:89155ms step_avg:58.65ms
step:1521/2330 train_time:89213ms step_avg:58.65ms
step:1522/2330 train_time:89276ms step_avg:58.66ms
step:1523/2330 train_time:89333ms step_avg:58.66ms
step:1524/2330 train_time:89394ms step_avg:58.66ms
step:1525/2330 train_time:89451ms step_avg:58.66ms
step:1526/2330 train_time:89512ms step_avg:58.66ms
step:1527/2330 train_time:89568ms step_avg:58.66ms
step:1528/2330 train_time:89628ms step_avg:58.66ms
step:1529/2330 train_time:89686ms step_avg:58.66ms
step:1530/2330 train_time:89745ms step_avg:58.66ms
step:1531/2330 train_time:89802ms step_avg:58.66ms
step:1532/2330 train_time:89861ms step_avg:58.66ms
step:1533/2330 train_time:89919ms step_avg:58.66ms
step:1534/2330 train_time:89978ms step_avg:58.66ms
step:1535/2330 train_time:90036ms step_avg:58.66ms
step:1536/2330 train_time:90097ms step_avg:58.66ms
step:1537/2330 train_time:90156ms step_avg:58.66ms
step:1538/2330 train_time:90217ms step_avg:58.66ms
step:1539/2330 train_time:90275ms step_avg:58.66ms
step:1540/2330 train_time:90337ms step_avg:58.66ms
step:1541/2330 train_time:90394ms step_avg:58.66ms
step:1542/2330 train_time:90456ms step_avg:58.66ms
step:1543/2330 train_time:90513ms step_avg:58.66ms
step:1544/2330 train_time:90574ms step_avg:58.66ms
step:1545/2330 train_time:90630ms step_avg:58.66ms
step:1546/2330 train_time:90692ms step_avg:58.66ms
step:1547/2330 train_time:90749ms step_avg:58.66ms
step:1548/2330 train_time:90810ms step_avg:58.66ms
step:1549/2330 train_time:90866ms step_avg:58.66ms
step:1550/2330 train_time:90928ms step_avg:58.66ms
step:1551/2330 train_time:90985ms step_avg:58.66ms
step:1552/2330 train_time:91047ms step_avg:58.66ms
step:1553/2330 train_time:91105ms step_avg:58.66ms
step:1554/2330 train_time:91166ms step_avg:58.67ms
step:1555/2330 train_time:91225ms step_avg:58.67ms
step:1556/2330 train_time:91286ms step_avg:58.67ms
step:1557/2330 train_time:91345ms step_avg:58.67ms
step:1558/2330 train_time:91405ms step_avg:58.67ms
step:1559/2330 train_time:91463ms step_avg:58.67ms
step:1560/2330 train_time:91523ms step_avg:58.67ms
step:1561/2330 train_time:91580ms step_avg:58.67ms
step:1562/2330 train_time:91641ms step_avg:58.67ms
step:1563/2330 train_time:91698ms step_avg:58.67ms
step:1564/2330 train_time:91757ms step_avg:58.67ms
step:1565/2330 train_time:91814ms step_avg:58.67ms
step:1566/2330 train_time:91874ms step_avg:58.67ms
step:1567/2330 train_time:91932ms step_avg:58.67ms
step:1568/2330 train_time:91992ms step_avg:58.67ms
step:1569/2330 train_time:92049ms step_avg:58.67ms
step:1570/2330 train_time:92110ms step_avg:58.67ms
step:1571/2330 train_time:92167ms step_avg:58.67ms
step:1572/2330 train_time:92231ms step_avg:58.67ms
step:1573/2330 train_time:92289ms step_avg:58.67ms
step:1574/2330 train_time:92352ms step_avg:58.67ms
step:1575/2330 train_time:92409ms step_avg:58.67ms
step:1576/2330 train_time:92470ms step_avg:58.67ms
step:1577/2330 train_time:92526ms step_avg:58.67ms
step:1578/2330 train_time:92589ms step_avg:58.67ms
step:1579/2330 train_time:92646ms step_avg:58.67ms
step:1580/2330 train_time:92707ms step_avg:58.68ms
step:1581/2330 train_time:92765ms step_avg:58.67ms
step:1582/2330 train_time:92825ms step_avg:58.68ms
step:1583/2330 train_time:92882ms step_avg:58.67ms
step:1584/2330 train_time:92944ms step_avg:58.68ms
step:1585/2330 train_time:93000ms step_avg:58.68ms
step:1586/2330 train_time:93061ms step_avg:58.68ms
step:1587/2330 train_time:93118ms step_avg:58.68ms
step:1588/2330 train_time:93179ms step_avg:58.68ms
step:1589/2330 train_time:93237ms step_avg:58.68ms
step:1590/2330 train_time:93297ms step_avg:58.68ms
step:1591/2330 train_time:93354ms step_avg:58.68ms
step:1592/2330 train_time:93416ms step_avg:58.68ms
step:1593/2330 train_time:93473ms step_avg:58.68ms
step:1594/2330 train_time:93536ms step_avg:58.68ms
step:1595/2330 train_time:93593ms step_avg:58.68ms
step:1596/2330 train_time:93654ms step_avg:58.68ms
step:1597/2330 train_time:93711ms step_avg:58.68ms
step:1598/2330 train_time:93772ms step_avg:58.68ms
step:1599/2330 train_time:93829ms step_avg:58.68ms
step:1600/2330 train_time:93891ms step_avg:58.68ms
step:1601/2330 train_time:93948ms step_avg:58.68ms
step:1602/2330 train_time:94009ms step_avg:58.68ms
step:1603/2330 train_time:94065ms step_avg:58.68ms
step:1604/2330 train_time:94128ms step_avg:58.68ms
step:1605/2330 train_time:94185ms step_avg:58.68ms
step:1606/2330 train_time:94248ms step_avg:58.69ms
step:1607/2330 train_time:94305ms step_avg:58.68ms
step:1608/2330 train_time:94367ms step_avg:58.69ms
step:1609/2330 train_time:94425ms step_avg:58.69ms
step:1610/2330 train_time:94486ms step_avg:58.69ms
step:1611/2330 train_time:94544ms step_avg:58.69ms
step:1612/2330 train_time:94604ms step_avg:58.69ms
step:1613/2330 train_time:94662ms step_avg:58.69ms
step:1614/2330 train_time:94722ms step_avg:58.69ms
step:1615/2330 train_time:94780ms step_avg:58.69ms
step:1616/2330 train_time:94840ms step_avg:58.69ms
step:1617/2330 train_time:94898ms step_avg:58.69ms
step:1618/2330 train_time:94958ms step_avg:58.69ms
step:1619/2330 train_time:95015ms step_avg:58.69ms
step:1620/2330 train_time:95076ms step_avg:58.69ms
step:1621/2330 train_time:95133ms step_avg:58.69ms
step:1622/2330 train_time:95195ms step_avg:58.69ms
step:1623/2330 train_time:95251ms step_avg:58.69ms
step:1624/2330 train_time:95312ms step_avg:58.69ms
step:1625/2330 train_time:95369ms step_avg:58.69ms
step:1626/2330 train_time:95434ms step_avg:58.69ms
step:1627/2330 train_time:95490ms step_avg:58.69ms
step:1628/2330 train_time:95553ms step_avg:58.69ms
step:1629/2330 train_time:95609ms step_avg:58.69ms
step:1630/2330 train_time:95672ms step_avg:58.69ms
step:1631/2330 train_time:95728ms step_avg:58.69ms
step:1632/2330 train_time:95790ms step_avg:58.69ms
step:1633/2330 train_time:95847ms step_avg:58.69ms
step:1634/2330 train_time:95909ms step_avg:58.70ms
step:1635/2330 train_time:95965ms step_avg:58.69ms
step:1636/2330 train_time:96028ms step_avg:58.70ms
step:1637/2330 train_time:96085ms step_avg:58.70ms
step:1638/2330 train_time:96148ms step_avg:58.70ms
step:1639/2330 train_time:96205ms step_avg:58.70ms
step:1640/2330 train_time:96266ms step_avg:58.70ms
step:1641/2330 train_time:96324ms step_avg:58.70ms
step:1642/2330 train_time:96385ms step_avg:58.70ms
step:1643/2330 train_time:96443ms step_avg:58.70ms
step:1644/2330 train_time:96504ms step_avg:58.70ms
step:1645/2330 train_time:96561ms step_avg:58.70ms
step:1646/2330 train_time:96622ms step_avg:58.70ms
step:1647/2330 train_time:96680ms step_avg:58.70ms
step:1648/2330 train_time:96740ms step_avg:58.70ms
step:1649/2330 train_time:96798ms step_avg:58.70ms
step:1650/2330 train_time:96858ms step_avg:58.70ms
step:1651/2330 train_time:96915ms step_avg:58.70ms
step:1652/2330 train_time:96975ms step_avg:58.70ms
step:1653/2330 train_time:97032ms step_avg:58.70ms
step:1654/2330 train_time:97094ms step_avg:58.70ms
step:1655/2330 train_time:97151ms step_avg:58.70ms
step:1656/2330 train_time:97212ms step_avg:58.70ms
step:1657/2330 train_time:97269ms step_avg:58.70ms
step:1658/2330 train_time:97332ms step_avg:58.70ms
step:1659/2330 train_time:97389ms step_avg:58.70ms
step:1660/2330 train_time:97451ms step_avg:58.71ms
step:1661/2330 train_time:97508ms step_avg:58.70ms
step:1662/2330 train_time:97570ms step_avg:58.71ms
step:1663/2330 train_time:97626ms step_avg:58.70ms
step:1664/2330 train_time:97689ms step_avg:58.71ms
step:1665/2330 train_time:97745ms step_avg:58.71ms
step:1666/2330 train_time:97808ms step_avg:58.71ms
step:1667/2330 train_time:97865ms step_avg:58.71ms
step:1668/2330 train_time:97925ms step_avg:58.71ms
step:1669/2330 train_time:97983ms step_avg:58.71ms
step:1670/2330 train_time:98043ms step_avg:58.71ms
step:1671/2330 train_time:98102ms step_avg:58.71ms
step:1672/2330 train_time:98162ms step_avg:58.71ms
step:1673/2330 train_time:98221ms step_avg:58.71ms
step:1674/2330 train_time:98281ms step_avg:58.71ms
step:1675/2330 train_time:98338ms step_avg:58.71ms
step:1676/2330 train_time:98398ms step_avg:58.71ms
step:1677/2330 train_time:98455ms step_avg:58.71ms
step:1678/2330 train_time:98516ms step_avg:58.71ms
step:1679/2330 train_time:98572ms step_avg:58.71ms
step:1680/2330 train_time:98634ms step_avg:58.71ms
step:1681/2330 train_time:98691ms step_avg:58.71ms
step:1682/2330 train_time:98753ms step_avg:58.71ms
step:1683/2330 train_time:98809ms step_avg:58.71ms
step:1684/2330 train_time:98872ms step_avg:58.71ms
step:1685/2330 train_time:98928ms step_avg:58.71ms
step:1686/2330 train_time:98991ms step_avg:58.71ms
step:1687/2330 train_time:99048ms step_avg:58.71ms
step:1688/2330 train_time:99111ms step_avg:58.71ms
step:1689/2330 train_time:99168ms step_avg:58.71ms
step:1690/2330 train_time:99231ms step_avg:58.72ms
step:1691/2330 train_time:99288ms step_avg:58.72ms
step:1692/2330 train_time:99351ms step_avg:58.72ms
step:1693/2330 train_time:99407ms step_avg:58.72ms
step:1694/2330 train_time:99469ms step_avg:58.72ms
step:1695/2330 train_time:99526ms step_avg:58.72ms
step:1696/2330 train_time:99588ms step_avg:58.72ms
step:1697/2330 train_time:99645ms step_avg:58.72ms
step:1698/2330 train_time:99706ms step_avg:58.72ms
step:1699/2330 train_time:99764ms step_avg:58.72ms
step:1700/2330 train_time:99824ms step_avg:58.72ms
step:1701/2330 train_time:99881ms step_avg:58.72ms
step:1702/2330 train_time:99943ms step_avg:58.72ms
step:1703/2330 train_time:100000ms step_avg:58.72ms
step:1704/2330 train_time:100060ms step_avg:58.72ms
step:1705/2330 train_time:100118ms step_avg:58.72ms
step:1706/2330 train_time:100178ms step_avg:58.72ms
step:1707/2330 train_time:100235ms step_avg:58.72ms
step:1708/2330 train_time:100296ms step_avg:58.72ms
step:1709/2330 train_time:100354ms step_avg:58.72ms
step:1710/2330 train_time:100415ms step_avg:58.72ms
step:1711/2330 train_time:100471ms step_avg:58.72ms
step:1712/2330 train_time:100533ms step_avg:58.72ms
step:1713/2330 train_time:100589ms step_avg:58.72ms
step:1714/2330 train_time:100651ms step_avg:58.72ms
step:1715/2330 train_time:100708ms step_avg:58.72ms
step:1716/2330 train_time:100771ms step_avg:58.72ms
step:1717/2330 train_time:100827ms step_avg:58.72ms
step:1718/2330 train_time:100890ms step_avg:58.73ms
step:1719/2330 train_time:100947ms step_avg:58.72ms
step:1720/2330 train_time:101009ms step_avg:58.73ms
step:1721/2330 train_time:101066ms step_avg:58.72ms
step:1722/2330 train_time:101129ms step_avg:58.73ms
step:1723/2330 train_time:101185ms step_avg:58.73ms
step:1724/2330 train_time:101249ms step_avg:58.73ms
step:1725/2330 train_time:101305ms step_avg:58.73ms
step:1726/2330 train_time:101367ms step_avg:58.73ms
step:1727/2330 train_time:101425ms step_avg:58.73ms
step:1728/2330 train_time:101485ms step_avg:58.73ms
step:1729/2330 train_time:101543ms step_avg:58.73ms
step:1730/2330 train_time:101604ms step_avg:58.73ms
step:1731/2330 train_time:101662ms step_avg:58.73ms
step:1732/2330 train_time:101721ms step_avg:58.73ms
step:1733/2330 train_time:101779ms step_avg:58.73ms
step:1734/2330 train_time:101839ms step_avg:58.73ms
step:1735/2330 train_time:101897ms step_avg:58.73ms
step:1736/2330 train_time:101957ms step_avg:58.73ms
step:1737/2330 train_time:102014ms step_avg:58.73ms
step:1738/2330 train_time:102075ms step_avg:58.73ms
step:1739/2330 train_time:102132ms step_avg:58.73ms
step:1740/2330 train_time:102194ms step_avg:58.73ms
step:1741/2330 train_time:102251ms step_avg:58.73ms
step:1742/2330 train_time:102312ms step_avg:58.73ms
step:1743/2330 train_time:102369ms step_avg:58.73ms
step:1744/2330 train_time:102433ms step_avg:58.73ms
step:1745/2330 train_time:102489ms step_avg:58.73ms
step:1746/2330 train_time:102551ms step_avg:58.74ms
step:1747/2330 train_time:102608ms step_avg:58.73ms
step:1748/2330 train_time:102670ms step_avg:58.74ms
step:1749/2330 train_time:102727ms step_avg:58.73ms
step:1750/2330 train_time:102789ms step_avg:58.74ms
step:1750/2330 val_loss:3.8242 train_time:102872ms step_avg:58.78ms
step:1751/2330 train_time:102889ms step_avg:58.76ms
step:1752/2330 train_time:102910ms step_avg:58.74ms
step:1753/2330 train_time:102967ms step_avg:58.74ms
step:1754/2330 train_time:103035ms step_avg:58.74ms
step:1755/2330 train_time:103092ms step_avg:58.74ms
step:1756/2330 train_time:103157ms step_avg:58.75ms
step:1757/2330 train_time:103215ms step_avg:58.74ms
step:1758/2330 train_time:103274ms step_avg:58.75ms
step:1759/2330 train_time:103331ms step_avg:58.74ms
step:1760/2330 train_time:103392ms step_avg:58.75ms
step:1761/2330 train_time:103448ms step_avg:58.74ms
step:1762/2330 train_time:103508ms step_avg:58.74ms
step:1763/2330 train_time:103564ms step_avg:58.74ms
step:1764/2330 train_time:103625ms step_avg:58.74ms
step:1765/2330 train_time:103681ms step_avg:58.74ms
step:1766/2330 train_time:103741ms step_avg:58.74ms
step:1767/2330 train_time:103797ms step_avg:58.74ms
step:1768/2330 train_time:103863ms step_avg:58.75ms
step:1769/2330 train_time:103921ms step_avg:58.75ms
step:1770/2330 train_time:103982ms step_avg:58.75ms
step:1771/2330 train_time:104039ms step_avg:58.75ms
step:1772/2330 train_time:104103ms step_avg:58.75ms
step:1773/2330 train_time:104160ms step_avg:58.75ms
step:1774/2330 train_time:104222ms step_avg:58.75ms
step:1775/2330 train_time:104279ms step_avg:58.75ms
step:1776/2330 train_time:104341ms step_avg:58.75ms
step:1777/2330 train_time:104398ms step_avg:58.75ms
step:1778/2330 train_time:104459ms step_avg:58.75ms
step:1779/2330 train_time:104517ms step_avg:58.75ms
step:1780/2330 train_time:104577ms step_avg:58.75ms
step:1781/2330 train_time:104634ms step_avg:58.75ms
step:1782/2330 train_time:104693ms step_avg:58.75ms
step:1783/2330 train_time:104751ms step_avg:58.75ms
step:1784/2330 train_time:104812ms step_avg:58.75ms
step:1785/2330 train_time:104870ms step_avg:58.75ms
step:1786/2330 train_time:104931ms step_avg:58.75ms
step:1787/2330 train_time:104989ms step_avg:58.75ms
step:1788/2330 train_time:105050ms step_avg:58.75ms
step:1789/2330 train_time:105108ms step_avg:58.75ms
step:1790/2330 train_time:105169ms step_avg:58.75ms
step:1791/2330 train_time:105227ms step_avg:58.75ms
step:1792/2330 train_time:105287ms step_avg:58.75ms
step:1793/2330 train_time:105344ms step_avg:58.75ms
step:1794/2330 train_time:105405ms step_avg:58.75ms
step:1795/2330 train_time:105462ms step_avg:58.75ms
step:1796/2330 train_time:105523ms step_avg:58.75ms
step:1797/2330 train_time:105579ms step_avg:58.75ms
step:1798/2330 train_time:105642ms step_avg:58.76ms
step:1799/2330 train_time:105698ms step_avg:58.75ms
step:1800/2330 train_time:105760ms step_avg:58.76ms
step:1801/2330 train_time:105818ms step_avg:58.76ms
step:1802/2330 train_time:105879ms step_avg:58.76ms
step:1803/2330 train_time:105936ms step_avg:58.76ms
step:1804/2330 train_time:105998ms step_avg:58.76ms
step:1805/2330 train_time:106056ms step_avg:58.76ms
step:1806/2330 train_time:106117ms step_avg:58.76ms
step:1807/2330 train_time:106176ms step_avg:58.76ms
step:1808/2330 train_time:106237ms step_avg:58.76ms
step:1809/2330 train_time:106294ms step_avg:58.76ms
step:1810/2330 train_time:106355ms step_avg:58.76ms
step:1811/2330 train_time:106412ms step_avg:58.76ms
step:1812/2330 train_time:106473ms step_avg:58.76ms
step:1813/2330 train_time:106531ms step_avg:58.76ms
step:1814/2330 train_time:106591ms step_avg:58.76ms
step:1815/2330 train_time:106648ms step_avg:58.76ms
step:1816/2330 train_time:106709ms step_avg:58.76ms
step:1817/2330 train_time:106766ms step_avg:58.76ms
step:1818/2330 train_time:106827ms step_avg:58.76ms
step:1819/2330 train_time:106884ms step_avg:58.76ms
step:1820/2330 train_time:106945ms step_avg:58.76ms
step:1821/2330 train_time:107002ms step_avg:58.76ms
step:1822/2330 train_time:107065ms step_avg:58.76ms
step:1823/2330 train_time:107122ms step_avg:58.76ms
step:1824/2330 train_time:107184ms step_avg:58.76ms
step:1825/2330 train_time:107241ms step_avg:58.76ms
step:1826/2330 train_time:107303ms step_avg:58.76ms
step:1827/2330 train_time:107360ms step_avg:58.76ms
step:1828/2330 train_time:107422ms step_avg:58.76ms
step:1829/2330 train_time:107478ms step_avg:58.76ms
step:1830/2330 train_time:107541ms step_avg:58.77ms
step:1831/2330 train_time:107597ms step_avg:58.76ms
step:1832/2330 train_time:107660ms step_avg:58.77ms
step:1833/2330 train_time:107717ms step_avg:58.77ms
step:1834/2330 train_time:107777ms step_avg:58.77ms
step:1835/2330 train_time:107835ms step_avg:58.77ms
step:1836/2330 train_time:107895ms step_avg:58.77ms
step:1837/2330 train_time:107954ms step_avg:58.77ms
step:1838/2330 train_time:108015ms step_avg:58.77ms
step:1839/2330 train_time:108073ms step_avg:58.77ms
step:1840/2330 train_time:108133ms step_avg:58.77ms
step:1841/2330 train_time:108190ms step_avg:58.77ms
step:1842/2330 train_time:108250ms step_avg:58.77ms
step:1843/2330 train_time:108308ms step_avg:58.77ms
step:1844/2330 train_time:108369ms step_avg:58.77ms
step:1845/2330 train_time:108427ms step_avg:58.77ms
step:1846/2330 train_time:108487ms step_avg:58.77ms
step:1847/2330 train_time:108544ms step_avg:58.77ms
step:1848/2330 train_time:108605ms step_avg:58.77ms
step:1849/2330 train_time:108661ms step_avg:58.77ms
step:1850/2330 train_time:108723ms step_avg:58.77ms
step:1851/2330 train_time:108780ms step_avg:58.77ms
step:1852/2330 train_time:108843ms step_avg:58.77ms
step:1853/2330 train_time:108899ms step_avg:58.77ms
step:1854/2330 train_time:108963ms step_avg:58.77ms
step:1855/2330 train_time:109019ms step_avg:58.77ms
step:1856/2330 train_time:109082ms step_avg:58.77ms
step:1857/2330 train_time:109139ms step_avg:58.77ms
step:1858/2330 train_time:109201ms step_avg:58.77ms
step:1859/2330 train_time:109258ms step_avg:58.77ms
step:1860/2330 train_time:109321ms step_avg:58.77ms
step:1861/2330 train_time:109379ms step_avg:58.77ms
step:1862/2330 train_time:109439ms step_avg:58.77ms
step:1863/2330 train_time:109496ms step_avg:58.77ms
step:1864/2330 train_time:109556ms step_avg:58.77ms
step:1865/2330 train_time:109615ms step_avg:58.77ms
step:1866/2330 train_time:109675ms step_avg:58.78ms
step:1867/2330 train_time:109733ms step_avg:58.78ms
step:1868/2330 train_time:109793ms step_avg:58.78ms
step:1869/2330 train_time:109851ms step_avg:58.78ms
step:1870/2330 train_time:109911ms step_avg:58.78ms
step:1871/2330 train_time:109969ms step_avg:58.78ms
step:1872/2330 train_time:110030ms step_avg:58.78ms
step:1873/2330 train_time:110086ms step_avg:58.78ms
step:1874/2330 train_time:110147ms step_avg:58.78ms
step:1875/2330 train_time:110205ms step_avg:58.78ms
step:1876/2330 train_time:110266ms step_avg:58.78ms
step:1877/2330 train_time:110323ms step_avg:58.78ms
step:1878/2330 train_time:110384ms step_avg:58.78ms
step:1879/2330 train_time:110441ms step_avg:58.78ms
step:1880/2330 train_time:110503ms step_avg:58.78ms
step:1881/2330 train_time:110560ms step_avg:58.78ms
step:1882/2330 train_time:110621ms step_avg:58.78ms
step:1883/2330 train_time:110678ms step_avg:58.78ms
step:1884/2330 train_time:110741ms step_avg:58.78ms
step:1885/2330 train_time:110798ms step_avg:58.78ms
step:1886/2330 train_time:110860ms step_avg:58.78ms
step:1887/2330 train_time:110917ms step_avg:58.78ms
step:1888/2330 train_time:110978ms step_avg:58.78ms
step:1889/2330 train_time:111035ms step_avg:58.78ms
step:1890/2330 train_time:111096ms step_avg:58.78ms
step:1891/2330 train_time:111155ms step_avg:58.78ms
step:1892/2330 train_time:111215ms step_avg:58.78ms
step:1893/2330 train_time:111273ms step_avg:58.78ms
step:1894/2330 train_time:111333ms step_avg:58.78ms
step:1895/2330 train_time:111390ms step_avg:58.78ms
step:1896/2330 train_time:111451ms step_avg:58.78ms
step:1897/2330 train_time:111509ms step_avg:58.78ms
step:1898/2330 train_time:111569ms step_avg:58.78ms
step:1899/2330 train_time:111626ms step_avg:58.78ms
step:1900/2330 train_time:111687ms step_avg:58.78ms
step:1901/2330 train_time:111744ms step_avg:58.78ms
step:1902/2330 train_time:111807ms step_avg:58.78ms
step:1903/2330 train_time:111864ms step_avg:58.78ms
step:1904/2330 train_time:111926ms step_avg:58.78ms
step:1905/2330 train_time:111982ms step_avg:58.78ms
step:1906/2330 train_time:112046ms step_avg:58.79ms
step:1907/2330 train_time:112102ms step_avg:58.78ms
step:1908/2330 train_time:112165ms step_avg:58.79ms
step:1909/2330 train_time:112221ms step_avg:58.79ms
step:1910/2330 train_time:112283ms step_avg:58.79ms
step:1911/2330 train_time:112340ms step_avg:58.79ms
step:1912/2330 train_time:112402ms step_avg:58.79ms
step:1913/2330 train_time:112459ms step_avg:58.79ms
step:1914/2330 train_time:112521ms step_avg:58.79ms
step:1915/2330 train_time:112578ms step_avg:58.79ms
step:1916/2330 train_time:112639ms step_avg:58.79ms
step:1917/2330 train_time:112697ms step_avg:58.79ms
step:1918/2330 train_time:112758ms step_avg:58.79ms
step:1919/2330 train_time:112817ms step_avg:58.79ms
step:1920/2330 train_time:112877ms step_avg:58.79ms
step:1921/2330 train_time:112935ms step_avg:58.79ms
step:1922/2330 train_time:112995ms step_avg:58.79ms
step:1923/2330 train_time:113052ms step_avg:58.79ms
step:1924/2330 train_time:113113ms step_avg:58.79ms
step:1925/2330 train_time:113171ms step_avg:58.79ms
step:1926/2330 train_time:113232ms step_avg:58.79ms
step:1927/2330 train_time:113288ms step_avg:58.79ms
step:1928/2330 train_time:113349ms step_avg:58.79ms
step:1929/2330 train_time:113407ms step_avg:58.79ms
step:1930/2330 train_time:113467ms step_avg:58.79ms
step:1931/2330 train_time:113524ms step_avg:58.79ms
step:1932/2330 train_time:113585ms step_avg:58.79ms
step:1933/2330 train_time:113642ms step_avg:58.79ms
step:1934/2330 train_time:113703ms step_avg:58.79ms
step:1935/2330 train_time:113761ms step_avg:58.79ms
step:1936/2330 train_time:113823ms step_avg:58.79ms
step:1937/2330 train_time:113880ms step_avg:58.79ms
step:1938/2330 train_time:113941ms step_avg:58.79ms
step:1939/2330 train_time:113998ms step_avg:58.79ms
step:1940/2330 train_time:114061ms step_avg:58.79ms
step:1941/2330 train_time:114118ms step_avg:58.79ms
step:1942/2330 train_time:114179ms step_avg:58.79ms
step:1943/2330 train_time:114236ms step_avg:58.79ms
step:1944/2330 train_time:114297ms step_avg:58.79ms
step:1945/2330 train_time:114355ms step_avg:58.79ms
step:1946/2330 train_time:114415ms step_avg:58.80ms
step:1947/2330 train_time:114473ms step_avg:58.79ms
step:1948/2330 train_time:114533ms step_avg:58.80ms
step:1949/2330 train_time:114592ms step_avg:58.80ms
step:1950/2330 train_time:114653ms step_avg:58.80ms
step:1951/2330 train_time:114711ms step_avg:58.80ms
step:1952/2330 train_time:114771ms step_avg:58.80ms
step:1953/2330 train_time:114829ms step_avg:58.80ms
step:1954/2330 train_time:114889ms step_avg:58.80ms
step:1955/2330 train_time:114946ms step_avg:58.80ms
step:1956/2330 train_time:115008ms step_avg:58.80ms
step:1957/2330 train_time:115065ms step_avg:58.80ms
step:1958/2330 train_time:115126ms step_avg:58.80ms
step:1959/2330 train_time:115183ms step_avg:58.80ms
step:1960/2330 train_time:115245ms step_avg:58.80ms
step:1961/2330 train_time:115302ms step_avg:58.80ms
step:1962/2330 train_time:115363ms step_avg:58.80ms
step:1963/2330 train_time:115420ms step_avg:58.80ms
step:1964/2330 train_time:115482ms step_avg:58.80ms
step:1965/2330 train_time:115539ms step_avg:58.80ms
step:1966/2330 train_time:115602ms step_avg:58.80ms
step:1967/2330 train_time:115659ms step_avg:58.80ms
step:1968/2330 train_time:115721ms step_avg:58.80ms
step:1969/2330 train_time:115778ms step_avg:58.80ms
step:1970/2330 train_time:115840ms step_avg:58.80ms
step:1971/2330 train_time:115898ms step_avg:58.80ms
step:1972/2330 train_time:115958ms step_avg:58.80ms
step:1973/2330 train_time:116016ms step_avg:58.80ms
step:1974/2330 train_time:116077ms step_avg:58.80ms
step:1975/2330 train_time:116135ms step_avg:58.80ms
step:1976/2330 train_time:116195ms step_avg:58.80ms
step:1977/2330 train_time:116252ms step_avg:58.80ms
step:1978/2330 train_time:116313ms step_avg:58.80ms
step:1979/2330 train_time:116370ms step_avg:58.80ms
step:1980/2330 train_time:116430ms step_avg:58.80ms
step:1981/2330 train_time:116487ms step_avg:58.80ms
step:1982/2330 train_time:116548ms step_avg:58.80ms
step:1983/2330 train_time:116606ms step_avg:58.80ms
step:1984/2330 train_time:116666ms step_avg:58.80ms
step:1985/2330 train_time:116723ms step_avg:58.80ms
step:1986/2330 train_time:116784ms step_avg:58.80ms
step:1987/2330 train_time:116841ms step_avg:58.80ms
step:1988/2330 train_time:116902ms step_avg:58.80ms
step:1989/2330 train_time:116959ms step_avg:58.80ms
step:1990/2330 train_time:117023ms step_avg:58.81ms
step:1991/2330 train_time:117079ms step_avg:58.80ms
step:1992/2330 train_time:117142ms step_avg:58.81ms
step:1993/2330 train_time:117199ms step_avg:58.81ms
step:1994/2330 train_time:117262ms step_avg:58.81ms
step:1995/2330 train_time:117319ms step_avg:58.81ms
step:1996/2330 train_time:117381ms step_avg:58.81ms
step:1997/2330 train_time:117438ms step_avg:58.81ms
step:1998/2330 train_time:117499ms step_avg:58.81ms
step:1999/2330 train_time:117556ms step_avg:58.81ms
step:2000/2330 train_time:117618ms step_avg:58.81ms
step:2000/2330 val_loss:3.7625 train_time:117700ms step_avg:58.85ms
step:2001/2330 train_time:117717ms step_avg:58.83ms
step:2002/2330 train_time:117738ms step_avg:58.81ms
step:2003/2330 train_time:117799ms step_avg:58.81ms
step:2004/2330 train_time:117866ms step_avg:58.82ms
step:2005/2330 train_time:117924ms step_avg:58.81ms
step:2006/2330 train_time:117984ms step_avg:58.82ms
step:2007/2330 train_time:118041ms step_avg:58.81ms
step:2008/2330 train_time:118101ms step_avg:58.82ms
step:2009/2330 train_time:118158ms step_avg:58.81ms
step:2010/2330 train_time:118217ms step_avg:58.81ms
step:2011/2330 train_time:118275ms step_avg:58.81ms
step:2012/2330 train_time:118334ms step_avg:58.81ms
step:2013/2330 train_time:118391ms step_avg:58.81ms
step:2014/2330 train_time:118451ms step_avg:58.81ms
step:2015/2330 train_time:118507ms step_avg:58.81ms
step:2016/2330 train_time:118567ms step_avg:58.81ms
step:2017/2330 train_time:118623ms step_avg:58.81ms
step:2018/2330 train_time:118686ms step_avg:58.81ms
step:2019/2330 train_time:118745ms step_avg:58.81ms
step:2020/2330 train_time:118810ms step_avg:58.82ms
step:2021/2330 train_time:118868ms step_avg:58.82ms
step:2022/2330 train_time:118930ms step_avg:58.82ms
step:2023/2330 train_time:118988ms step_avg:58.82ms
step:2024/2330 train_time:119048ms step_avg:58.82ms
step:2025/2330 train_time:119104ms step_avg:58.82ms
step:2026/2330 train_time:119166ms step_avg:58.82ms
step:2027/2330 train_time:119223ms step_avg:58.82ms
step:2028/2330 train_time:119284ms step_avg:58.82ms
step:2029/2330 train_time:119340ms step_avg:58.82ms
step:2030/2330 train_time:119401ms step_avg:58.82ms
step:2031/2330 train_time:119458ms step_avg:58.82ms
step:2032/2330 train_time:119518ms step_avg:58.82ms
step:2033/2330 train_time:119575ms step_avg:58.82ms
step:2034/2330 train_time:119635ms step_avg:58.82ms
step:2035/2330 train_time:119694ms step_avg:58.82ms
step:2036/2330 train_time:119755ms step_avg:58.82ms
step:2037/2330 train_time:119814ms step_avg:58.82ms
step:2038/2330 train_time:119875ms step_avg:58.82ms
step:2039/2330 train_time:119934ms step_avg:58.82ms
step:2040/2330 train_time:119995ms step_avg:58.82ms
step:2041/2330 train_time:120054ms step_avg:58.82ms
step:2042/2330 train_time:120114ms step_avg:58.82ms
step:2043/2330 train_time:120173ms step_avg:58.82ms
step:2044/2330 train_time:120233ms step_avg:58.82ms
step:2045/2330 train_time:120290ms step_avg:58.82ms
step:2046/2330 train_time:120350ms step_avg:58.82ms
step:2047/2330 train_time:120406ms step_avg:58.82ms
step:2048/2330 train_time:120467ms step_avg:58.82ms
step:2049/2330 train_time:120523ms step_avg:58.82ms
step:2050/2330 train_time:120584ms step_avg:58.82ms
step:2051/2330 train_time:120642ms step_avg:58.82ms
step:2052/2330 train_time:120704ms step_avg:58.82ms
step:2053/2330 train_time:120761ms step_avg:58.82ms
step:2054/2330 train_time:120826ms step_avg:58.82ms
step:2055/2330 train_time:120883ms step_avg:58.82ms
step:2056/2330 train_time:120946ms step_avg:58.83ms
step:2057/2330 train_time:121003ms step_avg:58.82ms
step:2058/2330 train_time:121065ms step_avg:58.83ms
step:2059/2330 train_time:121122ms step_avg:58.83ms
step:2060/2330 train_time:121184ms step_avg:58.83ms
step:2061/2330 train_time:121241ms step_avg:58.83ms
step:2062/2330 train_time:121302ms step_avg:58.83ms
step:2063/2330 train_time:121359ms step_avg:58.83ms
step:2064/2330 train_time:121419ms step_avg:58.83ms
step:2065/2330 train_time:121477ms step_avg:58.83ms
step:2066/2330 train_time:121537ms step_avg:58.83ms
step:2067/2330 train_time:121595ms step_avg:58.83ms
step:2068/2330 train_time:121655ms step_avg:58.83ms
step:2069/2330 train_time:121713ms step_avg:58.83ms
step:2070/2330 train_time:121773ms step_avg:58.83ms
step:2071/2330 train_time:121831ms step_avg:58.83ms
step:2072/2330 train_time:121892ms step_avg:58.83ms
step:2073/2330 train_time:121951ms step_avg:58.83ms
step:2074/2330 train_time:122011ms step_avg:58.83ms
step:2075/2330 train_time:122068ms step_avg:58.83ms
step:2076/2330 train_time:122129ms step_avg:58.83ms
step:2077/2330 train_time:122187ms step_avg:58.83ms
step:2078/2330 train_time:122247ms step_avg:58.83ms
step:2079/2330 train_time:122304ms step_avg:58.83ms
step:2080/2330 train_time:122366ms step_avg:58.83ms
step:2081/2330 train_time:122422ms step_avg:58.83ms
step:2082/2330 train_time:122484ms step_avg:58.83ms
step:2083/2330 train_time:122541ms step_avg:58.83ms
step:2084/2330 train_time:122603ms step_avg:58.83ms
step:2085/2330 train_time:122660ms step_avg:58.83ms
step:2086/2330 train_time:122722ms step_avg:58.83ms
step:2087/2330 train_time:122779ms step_avg:58.83ms
step:2088/2330 train_time:122840ms step_avg:58.83ms
step:2089/2330 train_time:122898ms step_avg:58.83ms
step:2090/2330 train_time:122958ms step_avg:58.83ms
step:2091/2330 train_time:123016ms step_avg:58.83ms
step:2092/2330 train_time:123077ms step_avg:58.83ms
step:2093/2330 train_time:123136ms step_avg:58.83ms
step:2094/2330 train_time:123197ms step_avg:58.83ms
step:2095/2330 train_time:123255ms step_avg:58.83ms
step:2096/2330 train_time:123315ms step_avg:58.83ms
step:2097/2330 train_time:123372ms step_avg:58.83ms
step:2098/2330 train_time:123432ms step_avg:58.83ms
step:2099/2330 train_time:123490ms step_avg:58.83ms
step:2100/2330 train_time:123551ms step_avg:58.83ms
step:2101/2330 train_time:123608ms step_avg:58.83ms
step:2102/2330 train_time:123669ms step_avg:58.83ms
step:2103/2330 train_time:123727ms step_avg:58.83ms
step:2104/2330 train_time:123788ms step_avg:58.83ms
step:2105/2330 train_time:123846ms step_avg:58.83ms
step:2106/2330 train_time:123907ms step_avg:58.84ms
step:2107/2330 train_time:123964ms step_avg:58.83ms
step:2108/2330 train_time:124026ms step_avg:58.84ms
step:2109/2330 train_time:124083ms step_avg:58.84ms
step:2110/2330 train_time:124145ms step_avg:58.84ms
step:2111/2330 train_time:124202ms step_avg:58.84ms
step:2112/2330 train_time:124265ms step_avg:58.84ms
step:2113/2330 train_time:124321ms step_avg:58.84ms
step:2114/2330 train_time:124382ms step_avg:58.84ms
step:2115/2330 train_time:124440ms step_avg:58.84ms
step:2116/2330 train_time:124501ms step_avg:58.84ms
step:2117/2330 train_time:124558ms step_avg:58.84ms
step:2118/2330 train_time:124618ms step_avg:58.84ms
step:2119/2330 train_time:124676ms step_avg:58.84ms
step:2120/2330 train_time:124737ms step_avg:58.84ms
step:2121/2330 train_time:124795ms step_avg:58.84ms
step:2122/2330 train_time:124855ms step_avg:58.84ms
step:2123/2330 train_time:124913ms step_avg:58.84ms
step:2124/2330 train_time:124974ms step_avg:58.84ms
step:2125/2330 train_time:125033ms step_avg:58.84ms
step:2126/2330 train_time:125093ms step_avg:58.84ms
step:2127/2330 train_time:125150ms step_avg:58.84ms
step:2128/2330 train_time:125212ms step_avg:58.84ms
step:2129/2330 train_time:125269ms step_avg:58.84ms
step:2130/2330 train_time:125330ms step_avg:58.84ms
step:2131/2330 train_time:125387ms step_avg:58.84ms
step:2132/2330 train_time:125448ms step_avg:58.84ms
step:2133/2330 train_time:125505ms step_avg:58.84ms
step:2134/2330 train_time:125568ms step_avg:58.84ms
step:2135/2330 train_time:125624ms step_avg:58.84ms
step:2136/2330 train_time:125687ms step_avg:58.84ms
step:2137/2330 train_time:125744ms step_avg:58.84ms
step:2138/2330 train_time:125805ms step_avg:58.84ms
step:2139/2330 train_time:125862ms step_avg:58.84ms
step:2140/2330 train_time:125925ms step_avg:58.84ms
step:2141/2330 train_time:125982ms step_avg:58.84ms
step:2142/2330 train_time:126044ms step_avg:58.84ms
step:2143/2330 train_time:126101ms step_avg:58.84ms
step:2144/2330 train_time:126162ms step_avg:58.84ms
step:2145/2330 train_time:126219ms step_avg:58.84ms
step:2146/2330 train_time:126280ms step_avg:58.84ms
step:2147/2330 train_time:126337ms step_avg:58.84ms
step:2148/2330 train_time:126398ms step_avg:58.84ms
step:2149/2330 train_time:126456ms step_avg:58.84ms
step:2150/2330 train_time:126516ms step_avg:58.84ms
step:2151/2330 train_time:126575ms step_avg:58.84ms
step:2152/2330 train_time:126635ms step_avg:58.85ms
step:2153/2330 train_time:126693ms step_avg:58.85ms
step:2154/2330 train_time:126754ms step_avg:58.85ms
step:2155/2330 train_time:126811ms step_avg:58.84ms
step:2156/2330 train_time:126871ms step_avg:58.85ms
step:2157/2330 train_time:126929ms step_avg:58.85ms
step:2158/2330 train_time:126990ms step_avg:58.85ms
step:2159/2330 train_time:127048ms step_avg:58.85ms
step:2160/2330 train_time:127109ms step_avg:58.85ms
step:2161/2330 train_time:127165ms step_avg:58.85ms
step:2162/2330 train_time:127229ms step_avg:58.85ms
step:2163/2330 train_time:127286ms step_avg:58.85ms
step:2164/2330 train_time:127347ms step_avg:58.85ms
step:2165/2330 train_time:127404ms step_avg:58.85ms
step:2166/2330 train_time:127468ms step_avg:58.85ms
step:2167/2330 train_time:127525ms step_avg:58.85ms
step:2168/2330 train_time:127587ms step_avg:58.85ms
step:2169/2330 train_time:127644ms step_avg:58.85ms
step:2170/2330 train_time:127705ms step_avg:58.85ms
step:2171/2330 train_time:127762ms step_avg:58.85ms
step:2172/2330 train_time:127824ms step_avg:58.85ms
step:2173/2330 train_time:127881ms step_avg:58.85ms
step:2174/2330 train_time:127943ms step_avg:58.85ms
step:2175/2330 train_time:128001ms step_avg:58.85ms
step:2176/2330 train_time:128062ms step_avg:58.85ms
step:2177/2330 train_time:128119ms step_avg:58.85ms
step:2178/2330 train_time:128180ms step_avg:58.85ms
step:2179/2330 train_time:128238ms step_avg:58.85ms
step:2180/2330 train_time:128298ms step_avg:58.85ms
step:2181/2330 train_time:128356ms step_avg:58.85ms
step:2182/2330 train_time:128418ms step_avg:58.85ms
step:2183/2330 train_time:128476ms step_avg:58.85ms
step:2184/2330 train_time:128536ms step_avg:58.85ms
step:2185/2330 train_time:128594ms step_avg:58.85ms
step:2186/2330 train_time:128654ms step_avg:58.85ms
step:2187/2330 train_time:128712ms step_avg:58.85ms
step:2188/2330 train_time:128772ms step_avg:58.85ms
step:2189/2330 train_time:128829ms step_avg:58.85ms
step:2190/2330 train_time:128889ms step_avg:58.85ms
step:2191/2330 train_time:128946ms step_avg:58.85ms
step:2192/2330 train_time:129008ms step_avg:58.85ms
step:2193/2330 train_time:129064ms step_avg:58.85ms
step:2194/2330 train_time:129126ms step_avg:58.85ms
step:2195/2330 train_time:129183ms step_avg:58.85ms
step:2196/2330 train_time:129246ms step_avg:58.86ms
step:2197/2330 train_time:129302ms step_avg:58.85ms
step:2198/2330 train_time:129365ms step_avg:58.86ms
step:2199/2330 train_time:129422ms step_avg:58.85ms
step:2200/2330 train_time:129485ms step_avg:58.86ms
step:2201/2330 train_time:129542ms step_avg:58.86ms
step:2202/2330 train_time:129604ms step_avg:58.86ms
step:2203/2330 train_time:129660ms step_avg:58.86ms
step:2204/2330 train_time:129722ms step_avg:58.86ms
step:2205/2330 train_time:129780ms step_avg:58.86ms
step:2206/2330 train_time:129840ms step_avg:58.86ms
step:2207/2330 train_time:129898ms step_avg:58.86ms
step:2208/2330 train_time:129958ms step_avg:58.86ms
step:2209/2330 train_time:130016ms step_avg:58.86ms
step:2210/2330 train_time:130077ms step_avg:58.86ms
step:2211/2330 train_time:130134ms step_avg:58.86ms
step:2212/2330 train_time:130195ms step_avg:58.86ms
step:2213/2330 train_time:130254ms step_avg:58.86ms
step:2214/2330 train_time:130314ms step_avg:58.86ms
step:2215/2330 train_time:130372ms step_avg:58.86ms
step:2216/2330 train_time:130432ms step_avg:58.86ms
step:2217/2330 train_time:130491ms step_avg:58.86ms
step:2218/2330 train_time:130551ms step_avg:58.86ms
step:2219/2330 train_time:130607ms step_avg:58.86ms
step:2220/2330 train_time:130668ms step_avg:58.86ms
step:2221/2330 train_time:130724ms step_avg:58.86ms
step:2222/2330 train_time:130786ms step_avg:58.86ms
step:2223/2330 train_time:130842ms step_avg:58.86ms
step:2224/2330 train_time:130904ms step_avg:58.86ms
step:2225/2330 train_time:130961ms step_avg:58.86ms
step:2226/2330 train_time:131023ms step_avg:58.86ms
step:2227/2330 train_time:131081ms step_avg:58.86ms
step:2228/2330 train_time:131144ms step_avg:58.86ms
step:2229/2330 train_time:131201ms step_avg:58.86ms
step:2230/2330 train_time:131263ms step_avg:58.86ms
step:2231/2330 train_time:131320ms step_avg:58.86ms
step:2232/2330 train_time:131381ms step_avg:58.86ms
step:2233/2330 train_time:131439ms step_avg:58.86ms
step:2234/2330 train_time:131499ms step_avg:58.86ms
step:2235/2330 train_time:131557ms step_avg:58.86ms
step:2236/2330 train_time:131618ms step_avg:58.86ms
step:2237/2330 train_time:131676ms step_avg:58.86ms
step:2238/2330 train_time:131736ms step_avg:58.86ms
step:2239/2330 train_time:131794ms step_avg:58.86ms
step:2240/2330 train_time:131854ms step_avg:58.86ms
step:2241/2330 train_time:131910ms step_avg:58.86ms
step:2242/2330 train_time:131971ms step_avg:58.86ms
step:2243/2330 train_time:132028ms step_avg:58.86ms
step:2244/2330 train_time:132089ms step_avg:58.86ms
step:2245/2330 train_time:132146ms step_avg:58.86ms
step:2246/2330 train_time:132208ms step_avg:58.86ms
step:2247/2330 train_time:132265ms step_avg:58.86ms
step:2248/2330 train_time:132327ms step_avg:58.86ms
step:2249/2330 train_time:132383ms step_avg:58.86ms
step:2250/2330 train_time:132446ms step_avg:58.86ms
step:2250/2330 val_loss:3.7151 train_time:132529ms step_avg:58.90ms
step:2251/2330 train_time:132547ms step_avg:58.88ms
step:2252/2330 train_time:132567ms step_avg:58.87ms
step:2253/2330 train_time:132625ms step_avg:58.87ms
step:2254/2330 train_time:132696ms step_avg:58.87ms
step:2255/2330 train_time:132754ms step_avg:58.87ms
step:2256/2330 train_time:132814ms step_avg:58.87ms
step:2257/2330 train_time:132871ms step_avg:58.87ms
step:2258/2330 train_time:132932ms step_avg:58.87ms
step:2259/2330 train_time:132989ms step_avg:58.87ms
step:2260/2330 train_time:133050ms step_avg:58.87ms
step:2261/2330 train_time:133107ms step_avg:58.87ms
step:2262/2330 train_time:133167ms step_avg:58.87ms
step:2263/2330 train_time:133223ms step_avg:58.87ms
step:2264/2330 train_time:133283ms step_avg:58.87ms
step:2265/2330 train_time:133339ms step_avg:58.87ms
step:2266/2330 train_time:133399ms step_avg:58.87ms
step:2267/2330 train_time:133456ms step_avg:58.87ms
step:2268/2330 train_time:133517ms step_avg:58.87ms
step:2269/2330 train_time:133576ms step_avg:58.87ms
step:2270/2330 train_time:133639ms step_avg:58.87ms
step:2271/2330 train_time:133698ms step_avg:58.87ms
step:2272/2330 train_time:133761ms step_avg:58.87ms
step:2273/2330 train_time:133818ms step_avg:58.87ms
step:2274/2330 train_time:133881ms step_avg:58.87ms
step:2275/2330 train_time:133937ms step_avg:58.87ms
step:2276/2330 train_time:133999ms step_avg:58.87ms
step:2277/2330 train_time:134056ms step_avg:58.87ms
step:2278/2330 train_time:134117ms step_avg:58.88ms
step:2279/2330 train_time:134174ms step_avg:58.87ms
step:2280/2330 train_time:134235ms step_avg:58.88ms
step:2281/2330 train_time:134292ms step_avg:58.87ms
step:2282/2330 train_time:134353ms step_avg:58.87ms
step:2283/2330 train_time:134410ms step_avg:58.87ms
step:2284/2330 train_time:134470ms step_avg:58.87ms
step:2285/2330 train_time:134528ms step_avg:58.87ms
step:2286/2330 train_time:134589ms step_avg:58.88ms
step:2287/2330 train_time:134647ms step_avg:58.87ms
step:2288/2330 train_time:134708ms step_avg:58.88ms
step:2289/2330 train_time:134766ms step_avg:58.88ms
step:2290/2330 train_time:134827ms step_avg:58.88ms
step:2291/2330 train_time:134884ms step_avg:58.88ms
step:2292/2330 train_time:134946ms step_avg:58.88ms
step:2293/2330 train_time:135003ms step_avg:58.88ms
step:2294/2330 train_time:135064ms step_avg:58.88ms
step:2295/2330 train_time:135121ms step_avg:58.88ms
step:2296/2330 train_time:135181ms step_avg:58.88ms
step:2297/2330 train_time:135238ms step_avg:58.88ms
step:2298/2330 train_time:135300ms step_avg:58.88ms
step:2299/2330 train_time:135356ms step_avg:58.88ms
step:2300/2330 train_time:135418ms step_avg:58.88ms
step:2301/2330 train_time:135475ms step_avg:58.88ms
step:2302/2330 train_time:135536ms step_avg:58.88ms
step:2303/2330 train_time:135594ms step_avg:58.88ms
step:2304/2330 train_time:135655ms step_avg:58.88ms
step:2305/2330 train_time:135714ms step_avg:58.88ms
step:2306/2330 train_time:135775ms step_avg:58.88ms
step:2307/2330 train_time:135833ms step_avg:58.88ms
step:2308/2330 train_time:135895ms step_avg:58.88ms
step:2309/2330 train_time:135953ms step_avg:58.88ms
step:2310/2330 train_time:136013ms step_avg:58.88ms
step:2311/2330 train_time:136071ms step_avg:58.88ms
step:2312/2330 train_time:136131ms step_avg:58.88ms
step:2313/2330 train_time:136188ms step_avg:58.88ms
step:2314/2330 train_time:136248ms step_avg:58.88ms
step:2315/2330 train_time:136304ms step_avg:58.88ms
step:2316/2330 train_time:136366ms step_avg:58.88ms
step:2317/2330 train_time:136422ms step_avg:58.88ms
step:2318/2330 train_time:136483ms step_avg:58.88ms
step:2319/2330 train_time:136540ms step_avg:58.88ms
step:2320/2330 train_time:136602ms step_avg:58.88ms
step:2321/2330 train_time:136660ms step_avg:58.88ms
step:2322/2330 train_time:136722ms step_avg:58.88ms
step:2323/2330 train_time:136778ms step_avg:58.88ms
step:2324/2330 train_time:136841ms step_avg:58.88ms
step:2325/2330 train_time:136899ms step_avg:58.88ms
step:2326/2330 train_time:136961ms step_avg:58.88ms
step:2327/2330 train_time:137017ms step_avg:58.88ms
step:2328/2330 train_time:137080ms step_avg:58.88ms
step:2329/2330 train_time:137137ms step_avg:58.88ms
step:2330/2330 train_time:137198ms step_avg:58.88ms
step:2330/2330 val_loss:3.7000 train_time:137279ms step_avg:58.92ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
