import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--beta1', type=str, required=True)
parser.add_argument('--beta2', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_beta_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=6e-4,  betas=(float(args2.beta1), float(args2.beta2)), eps=1e-10, weight_decay=0.0, fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt", exist_ok=True)
            torch.save(log, f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 04:41:13 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   27C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   26C    P0             116W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   25C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   27C    P0             113W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   25C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   26C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:85ms step_avg:85.39ms
step:2/2330 train_time:182ms step_avg:91.14ms
step:3/2330 train_time:201ms step_avg:66.97ms
step:4/2330 train_time:221ms step_avg:55.36ms
step:5/2330 train_time:273ms step_avg:54.67ms
step:6/2330 train_time:331ms step_avg:55.16ms
step:7/2330 train_time:386ms step_avg:55.08ms
step:8/2330 train_time:445ms step_avg:55.63ms
step:9/2330 train_time:500ms step_avg:55.58ms
step:10/2330 train_time:559ms step_avg:55.91ms
step:11/2330 train_time:615ms step_avg:55.88ms
step:12/2330 train_time:673ms step_avg:56.06ms
step:13/2330 train_time:727ms step_avg:55.95ms
step:14/2330 train_time:787ms step_avg:56.23ms
step:15/2330 train_time:842ms step_avg:56.16ms
step:16/2330 train_time:901ms step_avg:56.30ms
step:17/2330 train_time:957ms step_avg:56.27ms
step:18/2330 train_time:1015ms step_avg:56.41ms
step:19/2330 train_time:1074ms step_avg:56.52ms
step:20/2330 train_time:1136ms step_avg:56.80ms
step:21/2330 train_time:1195ms step_avg:56.88ms
step:22/2330 train_time:1255ms step_avg:57.03ms
step:23/2330 train_time:1310ms step_avg:56.97ms
step:24/2330 train_time:1370ms step_avg:57.08ms
step:25/2330 train_time:1425ms step_avg:57.00ms
step:26/2330 train_time:1485ms step_avg:57.11ms
step:27/2330 train_time:1540ms step_avg:57.04ms
step:28/2330 train_time:1599ms step_avg:57.12ms
step:29/2330 train_time:1655ms step_avg:57.08ms
step:30/2330 train_time:1714ms step_avg:57.12ms
step:31/2330 train_time:1769ms step_avg:57.06ms
step:32/2330 train_time:1828ms step_avg:57.13ms
step:33/2330 train_time:1884ms step_avg:57.08ms
step:34/2330 train_time:1942ms step_avg:57.12ms
step:35/2330 train_time:1998ms step_avg:57.08ms
step:36/2330 train_time:2057ms step_avg:57.14ms
step:37/2330 train_time:2114ms step_avg:57.12ms
step:38/2330 train_time:2174ms step_avg:57.21ms
step:39/2330 train_time:2230ms step_avg:57.18ms
step:40/2330 train_time:2290ms step_avg:57.25ms
step:41/2330 train_time:2346ms step_avg:57.22ms
step:42/2330 train_time:2406ms step_avg:57.28ms
step:43/2330 train_time:2461ms step_avg:57.24ms
step:44/2330 train_time:2520ms step_avg:57.28ms
step:45/2330 train_time:2576ms step_avg:57.24ms
step:46/2330 train_time:2635ms step_avg:57.29ms
step:47/2330 train_time:2691ms step_avg:57.25ms
step:48/2330 train_time:2749ms step_avg:57.28ms
step:49/2330 train_time:2805ms step_avg:57.24ms
step:50/2330 train_time:2863ms step_avg:57.27ms
step:51/2330 train_time:2919ms step_avg:57.23ms
step:52/2330 train_time:2977ms step_avg:57.25ms
step:53/2330 train_time:3033ms step_avg:57.22ms
step:54/2330 train_time:3092ms step_avg:57.27ms
step:55/2330 train_time:3149ms step_avg:57.25ms
step:56/2330 train_time:3208ms step_avg:57.29ms
step:57/2330 train_time:3264ms step_avg:57.27ms
step:58/2330 train_time:3324ms step_avg:57.30ms
step:59/2330 train_time:3379ms step_avg:57.27ms
step:60/2330 train_time:3440ms step_avg:57.33ms
step:61/2330 train_time:3496ms step_avg:57.31ms
step:62/2330 train_time:3555ms step_avg:57.34ms
step:63/2330 train_time:3610ms step_avg:57.31ms
step:64/2330 train_time:3670ms step_avg:57.34ms
step:65/2330 train_time:3725ms step_avg:57.30ms
step:66/2330 train_time:3783ms step_avg:57.32ms
step:67/2330 train_time:3838ms step_avg:57.28ms
step:68/2330 train_time:3897ms step_avg:57.31ms
step:69/2330 train_time:3953ms step_avg:57.29ms
step:70/2330 train_time:4012ms step_avg:57.31ms
step:71/2330 train_time:4068ms step_avg:57.30ms
step:72/2330 train_time:4128ms step_avg:57.33ms
step:73/2330 train_time:4184ms step_avg:57.31ms
step:74/2330 train_time:4243ms step_avg:57.34ms
step:75/2330 train_time:4299ms step_avg:57.32ms
step:76/2330 train_time:4359ms step_avg:57.35ms
step:77/2330 train_time:4415ms step_avg:57.33ms
step:78/2330 train_time:4473ms step_avg:57.35ms
step:79/2330 train_time:4529ms step_avg:57.33ms
step:80/2330 train_time:4588ms step_avg:57.35ms
step:81/2330 train_time:4643ms step_avg:57.32ms
step:82/2330 train_time:4702ms step_avg:57.34ms
step:83/2330 train_time:4757ms step_avg:57.32ms
step:84/2330 train_time:4816ms step_avg:57.34ms
step:85/2330 train_time:4872ms step_avg:57.32ms
step:86/2330 train_time:4931ms step_avg:57.34ms
step:87/2330 train_time:4986ms step_avg:57.31ms
step:88/2330 train_time:5045ms step_avg:57.33ms
step:89/2330 train_time:5102ms step_avg:57.32ms
step:90/2330 train_time:5160ms step_avg:57.34ms
step:91/2330 train_time:5216ms step_avg:57.32ms
step:92/2330 train_time:5275ms step_avg:57.34ms
step:93/2330 train_time:5331ms step_avg:57.32ms
step:94/2330 train_time:5391ms step_avg:57.35ms
step:95/2330 train_time:5446ms step_avg:57.33ms
step:96/2330 train_time:5506ms step_avg:57.35ms
step:97/2330 train_time:5562ms step_avg:57.34ms
step:98/2330 train_time:5621ms step_avg:57.35ms
step:99/2330 train_time:5677ms step_avg:57.34ms
step:100/2330 train_time:5736ms step_avg:57.36ms
step:101/2330 train_time:5792ms step_avg:57.34ms
step:102/2330 train_time:5850ms step_avg:57.35ms
step:103/2330 train_time:5906ms step_avg:57.34ms
step:104/2330 train_time:5965ms step_avg:57.36ms
step:105/2330 train_time:6021ms step_avg:57.34ms
step:106/2330 train_time:6081ms step_avg:57.37ms
step:107/2330 train_time:6137ms step_avg:57.35ms
step:108/2330 train_time:6197ms step_avg:57.38ms
step:109/2330 train_time:6253ms step_avg:57.37ms
step:110/2330 train_time:6312ms step_avg:57.38ms
step:111/2330 train_time:6368ms step_avg:57.37ms
step:112/2330 train_time:6427ms step_avg:57.38ms
step:113/2330 train_time:6483ms step_avg:57.37ms
step:114/2330 train_time:6542ms step_avg:57.38ms
step:115/2330 train_time:6597ms step_avg:57.37ms
step:116/2330 train_time:6656ms step_avg:57.38ms
step:117/2330 train_time:6712ms step_avg:57.37ms
step:118/2330 train_time:6771ms step_avg:57.38ms
step:119/2330 train_time:6827ms step_avg:57.37ms
step:120/2330 train_time:6886ms step_avg:57.38ms
step:121/2330 train_time:6941ms step_avg:57.37ms
step:122/2330 train_time:7001ms step_avg:57.39ms
step:123/2330 train_time:7057ms step_avg:57.37ms
step:124/2330 train_time:7116ms step_avg:57.39ms
step:125/2330 train_time:7171ms step_avg:57.37ms
step:126/2330 train_time:7230ms step_avg:57.38ms
step:127/2330 train_time:7286ms step_avg:57.37ms
step:128/2330 train_time:7345ms step_avg:57.39ms
step:129/2330 train_time:7402ms step_avg:57.38ms
step:130/2330 train_time:7461ms step_avg:57.39ms
step:131/2330 train_time:7517ms step_avg:57.38ms
step:132/2330 train_time:7575ms step_avg:57.39ms
step:133/2330 train_time:7631ms step_avg:57.38ms
step:134/2330 train_time:7690ms step_avg:57.39ms
step:135/2330 train_time:7746ms step_avg:57.38ms
step:136/2330 train_time:7804ms step_avg:57.39ms
step:137/2330 train_time:7860ms step_avg:57.38ms
step:138/2330 train_time:7919ms step_avg:57.38ms
step:139/2330 train_time:7976ms step_avg:57.38ms
step:140/2330 train_time:8035ms step_avg:57.39ms
step:141/2330 train_time:8091ms step_avg:57.38ms
step:142/2330 train_time:8149ms step_avg:57.39ms
step:143/2330 train_time:8205ms step_avg:57.38ms
step:144/2330 train_time:8265ms step_avg:57.40ms
step:145/2330 train_time:8321ms step_avg:57.38ms
step:146/2330 train_time:8382ms step_avg:57.41ms
step:147/2330 train_time:8437ms step_avg:57.40ms
step:148/2330 train_time:8496ms step_avg:57.40ms
step:149/2330 train_time:8551ms step_avg:57.39ms
step:150/2330 train_time:8610ms step_avg:57.40ms
step:151/2330 train_time:8666ms step_avg:57.39ms
step:152/2330 train_time:8724ms step_avg:57.40ms
step:153/2330 train_time:8780ms step_avg:57.39ms
step:154/2330 train_time:8839ms step_avg:57.40ms
step:155/2330 train_time:8895ms step_avg:57.39ms
step:156/2330 train_time:8954ms step_avg:57.40ms
step:157/2330 train_time:9010ms step_avg:57.39ms
step:158/2330 train_time:9069ms step_avg:57.40ms
step:159/2330 train_time:9125ms step_avg:57.39ms
step:160/2330 train_time:9184ms step_avg:57.40ms
step:161/2330 train_time:9240ms step_avg:57.39ms
step:162/2330 train_time:9299ms step_avg:57.40ms
step:163/2330 train_time:9355ms step_avg:57.39ms
step:164/2330 train_time:9414ms step_avg:57.40ms
step:165/2330 train_time:9470ms step_avg:57.39ms
step:166/2330 train_time:9529ms step_avg:57.40ms
step:167/2330 train_time:9584ms step_avg:57.39ms
step:168/2330 train_time:9643ms step_avg:57.40ms
step:169/2330 train_time:9699ms step_avg:57.39ms
step:170/2330 train_time:9758ms step_avg:57.40ms
step:171/2330 train_time:9814ms step_avg:57.39ms
step:172/2330 train_time:9873ms step_avg:57.40ms
step:173/2330 train_time:9929ms step_avg:57.39ms
step:174/2330 train_time:9987ms step_avg:57.40ms
step:175/2330 train_time:10043ms step_avg:57.39ms
step:176/2330 train_time:10102ms step_avg:57.40ms
step:177/2330 train_time:10158ms step_avg:57.39ms
step:178/2330 train_time:10217ms step_avg:57.40ms
step:179/2330 train_time:10274ms step_avg:57.40ms
step:180/2330 train_time:10332ms step_avg:57.40ms
step:181/2330 train_time:10388ms step_avg:57.39ms
step:182/2330 train_time:10448ms step_avg:57.41ms
step:183/2330 train_time:10504ms step_avg:57.40ms
step:184/2330 train_time:10563ms step_avg:57.41ms
step:185/2330 train_time:10619ms step_avg:57.40ms
step:186/2330 train_time:10679ms step_avg:57.41ms
step:187/2330 train_time:10734ms step_avg:57.40ms
step:188/2330 train_time:10793ms step_avg:57.41ms
step:189/2330 train_time:10849ms step_avg:57.40ms
step:190/2330 train_time:10909ms step_avg:57.41ms
step:191/2330 train_time:10965ms step_avg:57.41ms
step:192/2330 train_time:11023ms step_avg:57.41ms
step:193/2330 train_time:11079ms step_avg:57.40ms
step:194/2330 train_time:11139ms step_avg:57.42ms
step:195/2330 train_time:11195ms step_avg:57.41ms
step:196/2330 train_time:11254ms step_avg:57.42ms
step:197/2330 train_time:11310ms step_avg:57.41ms
step:198/2330 train_time:11369ms step_avg:57.42ms
step:199/2330 train_time:11424ms step_avg:57.41ms
step:200/2330 train_time:11483ms step_avg:57.42ms
step:201/2330 train_time:11538ms step_avg:57.40ms
step:202/2330 train_time:11598ms step_avg:57.41ms
step:203/2330 train_time:11654ms step_avg:57.41ms
step:204/2330 train_time:11713ms step_avg:57.42ms
step:205/2330 train_time:11769ms step_avg:57.41ms
step:206/2330 train_time:11827ms step_avg:57.41ms
step:207/2330 train_time:11883ms step_avg:57.41ms
step:208/2330 train_time:11942ms step_avg:57.41ms
step:209/2330 train_time:11998ms step_avg:57.41ms
step:210/2330 train_time:12058ms step_avg:57.42ms
step:211/2330 train_time:12114ms step_avg:57.41ms
step:212/2330 train_time:12172ms step_avg:57.42ms
step:213/2330 train_time:12229ms step_avg:57.41ms
step:214/2330 train_time:12287ms step_avg:57.42ms
step:215/2330 train_time:12343ms step_avg:57.41ms
step:216/2330 train_time:12402ms step_avg:57.42ms
step:217/2330 train_time:12459ms step_avg:57.41ms
step:218/2330 train_time:12517ms step_avg:57.42ms
step:219/2330 train_time:12574ms step_avg:57.41ms
step:220/2330 train_time:12633ms step_avg:57.42ms
step:221/2330 train_time:12688ms step_avg:57.41ms
step:222/2330 train_time:12747ms step_avg:57.42ms
step:223/2330 train_time:12803ms step_avg:57.41ms
step:224/2330 train_time:12861ms step_avg:57.42ms
step:225/2330 train_time:12918ms step_avg:57.41ms
step:226/2330 train_time:12977ms step_avg:57.42ms
step:227/2330 train_time:13032ms step_avg:57.41ms
step:228/2330 train_time:13092ms step_avg:57.42ms
step:229/2330 train_time:13147ms step_avg:57.41ms
step:230/2330 train_time:13207ms step_avg:57.42ms
step:231/2330 train_time:13263ms step_avg:57.41ms
step:232/2330 train_time:13322ms step_avg:57.42ms
step:233/2330 train_time:13378ms step_avg:57.42ms
step:234/2330 train_time:13437ms step_avg:57.42ms
step:235/2330 train_time:13493ms step_avg:57.42ms
step:236/2330 train_time:13552ms step_avg:57.42ms
step:237/2330 train_time:13608ms step_avg:57.42ms
step:238/2330 train_time:13667ms step_avg:57.42ms
step:239/2330 train_time:13723ms step_avg:57.42ms
step:240/2330 train_time:13782ms step_avg:57.42ms
step:241/2330 train_time:13837ms step_avg:57.42ms
step:242/2330 train_time:13897ms step_avg:57.42ms
step:243/2330 train_time:13953ms step_avg:57.42ms
step:244/2330 train_time:14012ms step_avg:57.43ms
step:245/2330 train_time:14067ms step_avg:57.42ms
step:246/2330 train_time:14128ms step_avg:57.43ms
step:247/2330 train_time:14184ms step_avg:57.42ms
step:248/2330 train_time:14243ms step_avg:57.43ms
step:249/2330 train_time:14298ms step_avg:57.42ms
step:250/2330 train_time:14358ms step_avg:57.43ms
step:250/2330 val_loss:4.8881 train_time:14437ms step_avg:57.75ms
step:251/2330 train_time:14455ms step_avg:57.59ms
step:252/2330 train_time:14474ms step_avg:57.44ms
step:253/2330 train_time:14529ms step_avg:57.43ms
step:254/2330 train_time:14592ms step_avg:57.45ms
step:255/2330 train_time:14648ms step_avg:57.44ms
step:256/2330 train_time:14716ms step_avg:57.49ms
step:257/2330 train_time:14772ms step_avg:57.48ms
step:258/2330 train_time:14832ms step_avg:57.49ms
step:259/2330 train_time:14887ms step_avg:57.48ms
step:260/2330 train_time:14945ms step_avg:57.48ms
step:261/2330 train_time:15001ms step_avg:57.47ms
step:262/2330 train_time:15059ms step_avg:57.48ms
step:263/2330 train_time:15114ms step_avg:57.47ms
step:264/2330 train_time:15172ms step_avg:57.47ms
step:265/2330 train_time:15227ms step_avg:57.46ms
step:266/2330 train_time:15286ms step_avg:57.47ms
step:267/2330 train_time:15342ms step_avg:57.46ms
step:268/2330 train_time:15402ms step_avg:57.47ms
step:269/2330 train_time:15458ms step_avg:57.47ms
step:270/2330 train_time:15520ms step_avg:57.48ms
step:271/2330 train_time:15576ms step_avg:57.48ms
step:272/2330 train_time:15638ms step_avg:57.49ms
step:273/2330 train_time:15694ms step_avg:57.49ms
step:274/2330 train_time:15754ms step_avg:57.50ms
step:275/2330 train_time:15810ms step_avg:57.49ms
step:276/2330 train_time:15869ms step_avg:57.50ms
step:277/2330 train_time:15925ms step_avg:57.49ms
step:278/2330 train_time:15984ms step_avg:57.50ms
step:279/2330 train_time:16040ms step_avg:57.49ms
step:280/2330 train_time:16099ms step_avg:57.50ms
step:281/2330 train_time:16155ms step_avg:57.49ms
step:282/2330 train_time:16213ms step_avg:57.49ms
step:283/2330 train_time:16269ms step_avg:57.49ms
step:284/2330 train_time:16328ms step_avg:57.49ms
step:285/2330 train_time:16383ms step_avg:57.49ms
step:286/2330 train_time:16444ms step_avg:57.50ms
step:287/2330 train_time:16500ms step_avg:57.49ms
step:288/2330 train_time:16560ms step_avg:57.50ms
step:289/2330 train_time:16616ms step_avg:57.49ms
step:290/2330 train_time:16676ms step_avg:57.50ms
step:291/2330 train_time:16732ms step_avg:57.50ms
step:292/2330 train_time:16793ms step_avg:57.51ms
step:293/2330 train_time:16848ms step_avg:57.50ms
step:294/2330 train_time:16908ms step_avg:57.51ms
step:295/2330 train_time:16963ms step_avg:57.50ms
step:296/2330 train_time:17023ms step_avg:57.51ms
step:297/2330 train_time:17078ms step_avg:57.50ms
step:298/2330 train_time:17137ms step_avg:57.51ms
step:299/2330 train_time:17193ms step_avg:57.50ms
step:300/2330 train_time:17251ms step_avg:57.50ms
step:301/2330 train_time:17307ms step_avg:57.50ms
step:302/2330 train_time:17366ms step_avg:57.50ms
step:303/2330 train_time:17422ms step_avg:57.50ms
step:304/2330 train_time:17481ms step_avg:57.50ms
step:305/2330 train_time:17537ms step_avg:57.50ms
step:306/2330 train_time:17597ms step_avg:57.51ms
step:307/2330 train_time:17653ms step_avg:57.50ms
step:308/2330 train_time:17714ms step_avg:57.51ms
step:309/2330 train_time:17770ms step_avg:57.51ms
step:310/2330 train_time:17829ms step_avg:57.51ms
step:311/2330 train_time:17885ms step_avg:57.51ms
step:312/2330 train_time:17945ms step_avg:57.51ms
step:313/2330 train_time:18000ms step_avg:57.51ms
step:314/2330 train_time:18059ms step_avg:57.51ms
step:315/2330 train_time:18115ms step_avg:57.51ms
step:316/2330 train_time:18174ms step_avg:57.51ms
step:317/2330 train_time:18231ms step_avg:57.51ms
step:318/2330 train_time:18290ms step_avg:57.52ms
step:319/2330 train_time:18346ms step_avg:57.51ms
step:320/2330 train_time:18405ms step_avg:57.51ms
step:321/2330 train_time:18461ms step_avg:57.51ms
step:322/2330 train_time:18520ms step_avg:57.51ms
step:323/2330 train_time:18576ms step_avg:57.51ms
step:324/2330 train_time:18635ms step_avg:57.52ms
step:325/2330 train_time:18692ms step_avg:57.51ms
step:326/2330 train_time:18751ms step_avg:57.52ms
step:327/2330 train_time:18807ms step_avg:57.51ms
step:328/2330 train_time:18867ms step_avg:57.52ms
step:329/2330 train_time:18923ms step_avg:57.52ms
step:330/2330 train_time:18982ms step_avg:57.52ms
step:331/2330 train_time:19037ms step_avg:57.51ms
step:332/2330 train_time:19097ms step_avg:57.52ms
step:333/2330 train_time:19153ms step_avg:57.52ms
step:334/2330 train_time:19213ms step_avg:57.52ms
step:335/2330 train_time:19268ms step_avg:57.52ms
step:336/2330 train_time:19327ms step_avg:57.52ms
step:337/2330 train_time:19382ms step_avg:57.51ms
step:338/2330 train_time:19441ms step_avg:57.52ms
step:339/2330 train_time:19497ms step_avg:57.51ms
step:340/2330 train_time:19557ms step_avg:57.52ms
step:341/2330 train_time:19613ms step_avg:57.52ms
step:342/2330 train_time:19672ms step_avg:57.52ms
step:343/2330 train_time:19728ms step_avg:57.52ms
step:344/2330 train_time:19787ms step_avg:57.52ms
step:345/2330 train_time:19844ms step_avg:57.52ms
step:346/2330 train_time:19904ms step_avg:57.53ms
step:347/2330 train_time:19959ms step_avg:57.52ms
step:348/2330 train_time:20020ms step_avg:57.53ms
step:349/2330 train_time:20075ms step_avg:57.52ms
step:350/2330 train_time:20136ms step_avg:57.53ms
step:351/2330 train_time:20191ms step_avg:57.53ms
step:352/2330 train_time:20250ms step_avg:57.53ms
step:353/2330 train_time:20306ms step_avg:57.52ms
step:354/2330 train_time:20366ms step_avg:57.53ms
step:355/2330 train_time:20422ms step_avg:57.53ms
step:356/2330 train_time:20481ms step_avg:57.53ms
step:357/2330 train_time:20536ms step_avg:57.52ms
step:358/2330 train_time:20596ms step_avg:57.53ms
step:359/2330 train_time:20653ms step_avg:57.53ms
step:360/2330 train_time:20712ms step_avg:57.53ms
step:361/2330 train_time:20768ms step_avg:57.53ms
step:362/2330 train_time:20827ms step_avg:57.53ms
step:363/2330 train_time:20883ms step_avg:57.53ms
step:364/2330 train_time:20943ms step_avg:57.53ms
step:365/2330 train_time:20998ms step_avg:57.53ms
step:366/2330 train_time:21058ms step_avg:57.53ms
step:367/2330 train_time:21113ms step_avg:57.53ms
step:368/2330 train_time:21173ms step_avg:57.54ms
step:369/2330 train_time:21229ms step_avg:57.53ms
step:370/2330 train_time:21288ms step_avg:57.53ms
step:371/2330 train_time:21345ms step_avg:57.53ms
step:372/2330 train_time:21404ms step_avg:57.54ms
step:373/2330 train_time:21460ms step_avg:57.53ms
step:374/2330 train_time:21518ms step_avg:57.54ms
step:375/2330 train_time:21574ms step_avg:57.53ms
step:376/2330 train_time:21634ms step_avg:57.54ms
step:377/2330 train_time:21690ms step_avg:57.53ms
step:378/2330 train_time:21749ms step_avg:57.54ms
step:379/2330 train_time:21805ms step_avg:57.53ms
step:380/2330 train_time:21863ms step_avg:57.54ms
step:381/2330 train_time:21919ms step_avg:57.53ms
step:382/2330 train_time:21979ms step_avg:57.54ms
step:383/2330 train_time:22035ms step_avg:57.53ms
step:384/2330 train_time:22095ms step_avg:57.54ms
step:385/2330 train_time:22151ms step_avg:57.54ms
step:386/2330 train_time:22210ms step_avg:57.54ms
step:387/2330 train_time:22266ms step_avg:57.53ms
step:388/2330 train_time:22325ms step_avg:57.54ms
step:389/2330 train_time:22381ms step_avg:57.53ms
step:390/2330 train_time:22440ms step_avg:57.54ms
step:391/2330 train_time:22495ms step_avg:57.53ms
step:392/2330 train_time:22555ms step_avg:57.54ms
step:393/2330 train_time:22611ms step_avg:57.54ms
step:394/2330 train_time:22671ms step_avg:57.54ms
step:395/2330 train_time:22727ms step_avg:57.54ms
step:396/2330 train_time:22786ms step_avg:57.54ms
step:397/2330 train_time:22842ms step_avg:57.54ms
step:398/2330 train_time:22901ms step_avg:57.54ms
step:399/2330 train_time:22957ms step_avg:57.54ms
step:400/2330 train_time:23016ms step_avg:57.54ms
step:401/2330 train_time:23072ms step_avg:57.54ms
step:402/2330 train_time:23131ms step_avg:57.54ms
step:403/2330 train_time:23186ms step_avg:57.53ms
step:404/2330 train_time:23245ms step_avg:57.54ms
step:405/2330 train_time:23301ms step_avg:57.53ms
step:406/2330 train_time:23360ms step_avg:57.54ms
step:407/2330 train_time:23416ms step_avg:57.53ms
step:408/2330 train_time:23476ms step_avg:57.54ms
step:409/2330 train_time:23531ms step_avg:57.53ms
step:410/2330 train_time:23591ms step_avg:57.54ms
step:411/2330 train_time:23647ms step_avg:57.54ms
step:412/2330 train_time:23706ms step_avg:57.54ms
step:413/2330 train_time:23762ms step_avg:57.54ms
step:414/2330 train_time:23821ms step_avg:57.54ms
step:415/2330 train_time:23877ms step_avg:57.54ms
step:416/2330 train_time:23938ms step_avg:57.54ms
step:417/2330 train_time:23994ms step_avg:57.54ms
step:418/2330 train_time:24053ms step_avg:57.54ms
step:419/2330 train_time:24110ms step_avg:57.54ms
step:420/2330 train_time:24168ms step_avg:57.54ms
step:421/2330 train_time:24224ms step_avg:57.54ms
step:422/2330 train_time:24284ms step_avg:57.54ms
step:423/2330 train_time:24339ms step_avg:57.54ms
step:424/2330 train_time:24399ms step_avg:57.54ms
step:425/2330 train_time:24454ms step_avg:57.54ms
step:426/2330 train_time:24515ms step_avg:57.55ms
step:427/2330 train_time:24572ms step_avg:57.54ms
step:428/2330 train_time:24631ms step_avg:57.55ms
step:429/2330 train_time:24687ms step_avg:57.54ms
step:430/2330 train_time:24746ms step_avg:57.55ms
step:431/2330 train_time:24802ms step_avg:57.54ms
step:432/2330 train_time:24862ms step_avg:57.55ms
step:433/2330 train_time:24918ms step_avg:57.55ms
step:434/2330 train_time:24976ms step_avg:57.55ms
step:435/2330 train_time:25033ms step_avg:57.55ms
step:436/2330 train_time:25092ms step_avg:57.55ms
step:437/2330 train_time:25148ms step_avg:57.55ms
step:438/2330 train_time:25207ms step_avg:57.55ms
step:439/2330 train_time:25263ms step_avg:57.55ms
step:440/2330 train_time:25322ms step_avg:57.55ms
step:441/2330 train_time:25377ms step_avg:57.54ms
step:442/2330 train_time:25438ms step_avg:57.55ms
step:443/2330 train_time:25493ms step_avg:57.55ms
step:444/2330 train_time:25554ms step_avg:57.55ms
step:445/2330 train_time:25610ms step_avg:57.55ms
step:446/2330 train_time:25669ms step_avg:57.55ms
step:447/2330 train_time:25725ms step_avg:57.55ms
step:448/2330 train_time:25784ms step_avg:57.55ms
step:449/2330 train_time:25839ms step_avg:57.55ms
step:450/2330 train_time:25899ms step_avg:57.55ms
step:451/2330 train_time:25955ms step_avg:57.55ms
step:452/2330 train_time:26014ms step_avg:57.55ms
step:453/2330 train_time:26070ms step_avg:57.55ms
step:454/2330 train_time:26129ms step_avg:57.55ms
step:455/2330 train_time:26186ms step_avg:57.55ms
step:456/2330 train_time:26245ms step_avg:57.55ms
step:457/2330 train_time:26300ms step_avg:57.55ms
step:458/2330 train_time:26360ms step_avg:57.55ms
step:459/2330 train_time:26416ms step_avg:57.55ms
step:460/2330 train_time:26475ms step_avg:57.55ms
step:461/2330 train_time:26532ms step_avg:57.55ms
step:462/2330 train_time:26591ms step_avg:57.56ms
step:463/2330 train_time:26647ms step_avg:57.55ms
step:464/2330 train_time:26706ms step_avg:57.55ms
step:465/2330 train_time:26762ms step_avg:57.55ms
step:466/2330 train_time:26821ms step_avg:57.56ms
step:467/2330 train_time:26876ms step_avg:57.55ms
step:468/2330 train_time:26936ms step_avg:57.56ms
step:469/2330 train_time:26992ms step_avg:57.55ms
step:470/2330 train_time:27052ms step_avg:57.56ms
step:471/2330 train_time:27109ms step_avg:57.56ms
step:472/2330 train_time:27168ms step_avg:57.56ms
step:473/2330 train_time:27223ms step_avg:57.55ms
step:474/2330 train_time:27282ms step_avg:57.56ms
step:475/2330 train_time:27338ms step_avg:57.55ms
step:476/2330 train_time:27397ms step_avg:57.56ms
step:477/2330 train_time:27453ms step_avg:57.55ms
step:478/2330 train_time:27512ms step_avg:57.56ms
step:479/2330 train_time:27568ms step_avg:57.55ms
step:480/2330 train_time:27627ms step_avg:57.56ms
step:481/2330 train_time:27683ms step_avg:57.55ms
step:482/2330 train_time:27743ms step_avg:57.56ms
step:483/2330 train_time:27799ms step_avg:57.55ms
step:484/2330 train_time:27858ms step_avg:57.56ms
step:485/2330 train_time:27914ms step_avg:57.55ms
step:486/2330 train_time:27973ms step_avg:57.56ms
step:487/2330 train_time:28029ms step_avg:57.55ms
step:488/2330 train_time:28088ms step_avg:57.56ms
step:489/2330 train_time:28144ms step_avg:57.55ms
step:490/2330 train_time:28203ms step_avg:57.56ms
step:491/2330 train_time:28258ms step_avg:57.55ms
step:492/2330 train_time:28319ms step_avg:57.56ms
step:493/2330 train_time:28375ms step_avg:57.56ms
step:494/2330 train_time:28436ms step_avg:57.56ms
step:495/2330 train_time:28492ms step_avg:57.56ms
step:496/2330 train_time:28551ms step_avg:57.56ms
step:497/2330 train_time:28607ms step_avg:57.56ms
step:498/2330 train_time:28665ms step_avg:57.56ms
step:499/2330 train_time:28721ms step_avg:57.56ms
step:500/2330 train_time:28780ms step_avg:57.56ms
step:500/2330 val_loss:4.4035 train_time:28860ms step_avg:57.72ms
step:501/2330 train_time:28879ms step_avg:57.64ms
step:502/2330 train_time:28898ms step_avg:57.57ms
step:503/2330 train_time:28954ms step_avg:57.56ms
step:504/2330 train_time:29019ms step_avg:57.58ms
step:505/2330 train_time:29076ms step_avg:57.58ms
step:506/2330 train_time:29136ms step_avg:57.58ms
step:507/2330 train_time:29192ms step_avg:57.58ms
step:508/2330 train_time:29252ms step_avg:57.58ms
step:509/2330 train_time:29307ms step_avg:57.58ms
step:510/2330 train_time:29367ms step_avg:57.58ms
step:511/2330 train_time:29423ms step_avg:57.58ms
step:512/2330 train_time:29481ms step_avg:57.58ms
step:513/2330 train_time:29537ms step_avg:57.58ms
step:514/2330 train_time:29595ms step_avg:57.58ms
step:515/2330 train_time:29650ms step_avg:57.57ms
step:516/2330 train_time:29709ms step_avg:57.58ms
step:517/2330 train_time:29765ms step_avg:57.57ms
step:518/2330 train_time:29825ms step_avg:57.58ms
step:519/2330 train_time:29882ms step_avg:57.58ms
step:520/2330 train_time:29942ms step_avg:57.58ms
step:521/2330 train_time:29999ms step_avg:57.58ms
step:522/2330 train_time:30059ms step_avg:57.58ms
step:523/2330 train_time:30116ms step_avg:57.58ms
step:524/2330 train_time:30176ms step_avg:57.59ms
step:525/2330 train_time:30232ms step_avg:57.58ms
step:526/2330 train_time:30290ms step_avg:57.59ms
step:527/2330 train_time:30346ms step_avg:57.58ms
step:528/2330 train_time:30406ms step_avg:57.59ms
step:529/2330 train_time:30463ms step_avg:57.59ms
step:530/2330 train_time:30522ms step_avg:57.59ms
step:531/2330 train_time:30578ms step_avg:57.59ms
step:532/2330 train_time:30636ms step_avg:57.59ms
step:533/2330 train_time:30692ms step_avg:57.58ms
step:534/2330 train_time:30750ms step_avg:57.58ms
step:535/2330 train_time:30806ms step_avg:57.58ms
step:536/2330 train_time:30867ms step_avg:57.59ms
step:537/2330 train_time:30923ms step_avg:57.58ms
step:538/2330 train_time:30984ms step_avg:57.59ms
step:539/2330 train_time:31040ms step_avg:57.59ms
step:540/2330 train_time:31100ms step_avg:57.59ms
step:541/2330 train_time:31156ms step_avg:57.59ms
step:542/2330 train_time:31215ms step_avg:57.59ms
step:543/2330 train_time:31271ms step_avg:57.59ms
step:544/2330 train_time:31330ms step_avg:57.59ms
step:545/2330 train_time:31386ms step_avg:57.59ms
step:546/2330 train_time:31447ms step_avg:57.59ms
step:547/2330 train_time:31503ms step_avg:57.59ms
step:548/2330 train_time:31563ms step_avg:57.60ms
step:549/2330 train_time:31619ms step_avg:57.59ms
step:550/2330 train_time:31678ms step_avg:57.60ms
step:551/2330 train_time:31733ms step_avg:57.59ms
step:552/2330 train_time:31792ms step_avg:57.59ms
step:553/2330 train_time:31847ms step_avg:57.59ms
step:554/2330 train_time:31907ms step_avg:57.59ms
step:555/2330 train_time:31963ms step_avg:57.59ms
step:556/2330 train_time:32025ms step_avg:57.60ms
step:557/2330 train_time:32081ms step_avg:57.60ms
step:558/2330 train_time:32142ms step_avg:57.60ms
step:559/2330 train_time:32199ms step_avg:57.60ms
step:560/2330 train_time:32258ms step_avg:57.60ms
step:561/2330 train_time:32315ms step_avg:57.60ms
step:562/2330 train_time:32375ms step_avg:57.61ms
step:563/2330 train_time:32431ms step_avg:57.60ms
step:564/2330 train_time:32490ms step_avg:57.61ms
step:565/2330 train_time:32546ms step_avg:57.60ms
step:566/2330 train_time:32606ms step_avg:57.61ms
step:567/2330 train_time:32662ms step_avg:57.60ms
step:568/2330 train_time:32720ms step_avg:57.61ms
step:569/2330 train_time:32776ms step_avg:57.60ms
step:570/2330 train_time:32835ms step_avg:57.60ms
step:571/2330 train_time:32891ms step_avg:57.60ms
step:572/2330 train_time:32951ms step_avg:57.61ms
step:573/2330 train_time:33006ms step_avg:57.60ms
step:574/2330 train_time:33068ms step_avg:57.61ms
step:575/2330 train_time:33124ms step_avg:57.61ms
step:576/2330 train_time:33184ms step_avg:57.61ms
step:577/2330 train_time:33240ms step_avg:57.61ms
step:578/2330 train_time:33299ms step_avg:57.61ms
step:579/2330 train_time:33356ms step_avg:57.61ms
step:580/2330 train_time:33415ms step_avg:57.61ms
step:581/2330 train_time:33470ms step_avg:57.61ms
step:582/2330 train_time:33530ms step_avg:57.61ms
step:583/2330 train_time:33585ms step_avg:57.61ms
step:584/2330 train_time:33646ms step_avg:57.61ms
step:585/2330 train_time:33702ms step_avg:57.61ms
step:586/2330 train_time:33760ms step_avg:57.61ms
step:587/2330 train_time:33816ms step_avg:57.61ms
step:588/2330 train_time:33875ms step_avg:57.61ms
step:589/2330 train_time:33930ms step_avg:57.61ms
step:590/2330 train_time:33990ms step_avg:57.61ms
step:591/2330 train_time:34046ms step_avg:57.61ms
step:592/2330 train_time:34107ms step_avg:57.61ms
step:593/2330 train_time:34162ms step_avg:57.61ms
step:594/2330 train_time:34223ms step_avg:57.61ms
step:595/2330 train_time:34280ms step_avg:57.61ms
step:596/2330 train_time:34339ms step_avg:57.62ms
step:597/2330 train_time:34396ms step_avg:57.61ms
step:598/2330 train_time:34454ms step_avg:57.62ms
step:599/2330 train_time:34510ms step_avg:57.61ms
step:600/2330 train_time:34569ms step_avg:57.62ms
step:601/2330 train_time:34626ms step_avg:57.61ms
step:602/2330 train_time:34685ms step_avg:57.62ms
step:603/2330 train_time:34742ms step_avg:57.61ms
step:604/2330 train_time:34801ms step_avg:57.62ms
step:605/2330 train_time:34857ms step_avg:57.62ms
step:606/2330 train_time:34916ms step_avg:57.62ms
step:607/2330 train_time:34971ms step_avg:57.61ms
step:608/2330 train_time:35030ms step_avg:57.62ms
step:609/2330 train_time:35086ms step_avg:57.61ms
step:610/2330 train_time:35146ms step_avg:57.62ms
step:611/2330 train_time:35202ms step_avg:57.61ms
step:612/2330 train_time:35262ms step_avg:57.62ms
step:613/2330 train_time:35318ms step_avg:57.61ms
step:614/2330 train_time:35377ms step_avg:57.62ms
step:615/2330 train_time:35433ms step_avg:57.61ms
step:616/2330 train_time:35493ms step_avg:57.62ms
step:617/2330 train_time:35549ms step_avg:57.62ms
step:618/2330 train_time:35609ms step_avg:57.62ms
step:619/2330 train_time:35665ms step_avg:57.62ms
step:620/2330 train_time:35724ms step_avg:57.62ms
step:621/2330 train_time:35780ms step_avg:57.62ms
step:622/2330 train_time:35840ms step_avg:57.62ms
step:623/2330 train_time:35896ms step_avg:57.62ms
step:624/2330 train_time:35955ms step_avg:57.62ms
step:625/2330 train_time:36011ms step_avg:57.62ms
step:626/2330 train_time:36071ms step_avg:57.62ms
step:627/2330 train_time:36127ms step_avg:57.62ms
step:628/2330 train_time:36188ms step_avg:57.62ms
step:629/2330 train_time:36244ms step_avg:57.62ms
step:630/2330 train_time:36305ms step_avg:57.63ms
step:631/2330 train_time:36361ms step_avg:57.62ms
step:632/2330 train_time:36420ms step_avg:57.63ms
step:633/2330 train_time:36476ms step_avg:57.62ms
step:634/2330 train_time:36535ms step_avg:57.63ms
step:635/2330 train_time:36590ms step_avg:57.62ms
step:636/2330 train_time:36651ms step_avg:57.63ms
step:637/2330 train_time:36706ms step_avg:57.62ms
step:638/2330 train_time:36767ms step_avg:57.63ms
step:639/2330 train_time:36823ms step_avg:57.63ms
step:640/2330 train_time:36883ms step_avg:57.63ms
step:641/2330 train_time:36939ms step_avg:57.63ms
step:642/2330 train_time:36998ms step_avg:57.63ms
step:643/2330 train_time:37053ms step_avg:57.63ms
step:644/2330 train_time:37114ms step_avg:57.63ms
step:645/2330 train_time:37169ms step_avg:57.63ms
step:646/2330 train_time:37229ms step_avg:57.63ms
step:647/2330 train_time:37285ms step_avg:57.63ms
step:648/2330 train_time:37347ms step_avg:57.63ms
step:649/2330 train_time:37403ms step_avg:57.63ms
step:650/2330 train_time:37463ms step_avg:57.64ms
step:651/2330 train_time:37519ms step_avg:57.63ms
step:652/2330 train_time:37578ms step_avg:57.64ms
step:653/2330 train_time:37634ms step_avg:57.63ms
step:654/2330 train_time:37693ms step_avg:57.63ms
step:655/2330 train_time:37749ms step_avg:57.63ms
step:656/2330 train_time:37808ms step_avg:57.63ms
step:657/2330 train_time:37864ms step_avg:57.63ms
step:658/2330 train_time:37924ms step_avg:57.64ms
step:659/2330 train_time:37981ms step_avg:57.63ms
step:660/2330 train_time:38040ms step_avg:57.64ms
step:661/2330 train_time:38096ms step_avg:57.63ms
step:662/2330 train_time:38156ms step_avg:57.64ms
step:663/2330 train_time:38212ms step_avg:57.63ms
step:664/2330 train_time:38270ms step_avg:57.64ms
step:665/2330 train_time:38326ms step_avg:57.63ms
step:666/2330 train_time:38387ms step_avg:57.64ms
step:667/2330 train_time:38443ms step_avg:57.64ms
step:668/2330 train_time:38503ms step_avg:57.64ms
step:669/2330 train_time:38559ms step_avg:57.64ms
step:670/2330 train_time:38619ms step_avg:57.64ms
step:671/2330 train_time:38675ms step_avg:57.64ms
step:672/2330 train_time:38733ms step_avg:57.64ms
step:673/2330 train_time:38789ms step_avg:57.64ms
step:674/2330 train_time:38848ms step_avg:57.64ms
step:675/2330 train_time:38904ms step_avg:57.64ms
step:676/2330 train_time:38965ms step_avg:57.64ms
step:677/2330 train_time:39021ms step_avg:57.64ms
step:678/2330 train_time:39080ms step_avg:57.64ms
step:679/2330 train_time:39136ms step_avg:57.64ms
step:680/2330 train_time:39195ms step_avg:57.64ms
step:681/2330 train_time:39251ms step_avg:57.64ms
step:682/2330 train_time:39310ms step_avg:57.64ms
step:683/2330 train_time:39366ms step_avg:57.64ms
step:684/2330 train_time:39426ms step_avg:57.64ms
step:685/2330 train_time:39483ms step_avg:57.64ms
step:686/2330 train_time:39542ms step_avg:57.64ms
step:687/2330 train_time:39599ms step_avg:57.64ms
step:688/2330 train_time:39658ms step_avg:57.64ms
step:689/2330 train_time:39715ms step_avg:57.64ms
step:690/2330 train_time:39773ms step_avg:57.64ms
step:691/2330 train_time:39828ms step_avg:57.64ms
step:692/2330 train_time:39888ms step_avg:57.64ms
step:693/2330 train_time:39944ms step_avg:57.64ms
step:694/2330 train_time:40005ms step_avg:57.64ms
step:695/2330 train_time:40061ms step_avg:57.64ms
step:696/2330 train_time:40120ms step_avg:57.64ms
step:697/2330 train_time:40176ms step_avg:57.64ms
step:698/2330 train_time:40235ms step_avg:57.64ms
step:699/2330 train_time:40291ms step_avg:57.64ms
step:700/2330 train_time:40351ms step_avg:57.64ms
step:701/2330 train_time:40407ms step_avg:57.64ms
step:702/2330 train_time:40467ms step_avg:57.65ms
step:703/2330 train_time:40523ms step_avg:57.64ms
step:704/2330 train_time:40583ms step_avg:57.65ms
step:705/2330 train_time:40639ms step_avg:57.64ms
step:706/2330 train_time:40697ms step_avg:57.65ms
step:707/2330 train_time:40754ms step_avg:57.64ms
step:708/2330 train_time:40813ms step_avg:57.65ms
step:709/2330 train_time:40869ms step_avg:57.64ms
step:710/2330 train_time:40929ms step_avg:57.65ms
step:711/2330 train_time:40985ms step_avg:57.64ms
step:712/2330 train_time:41045ms step_avg:57.65ms
step:713/2330 train_time:41101ms step_avg:57.65ms
step:714/2330 train_time:41160ms step_avg:57.65ms
step:715/2330 train_time:41216ms step_avg:57.65ms
step:716/2330 train_time:41275ms step_avg:57.65ms
step:717/2330 train_time:41331ms step_avg:57.64ms
step:718/2330 train_time:41390ms step_avg:57.65ms
step:719/2330 train_time:41446ms step_avg:57.64ms
step:720/2330 train_time:41506ms step_avg:57.65ms
step:721/2330 train_time:41562ms step_avg:57.64ms
step:722/2330 train_time:41621ms step_avg:57.65ms
step:723/2330 train_time:41678ms step_avg:57.65ms
step:724/2330 train_time:41737ms step_avg:57.65ms
step:725/2330 train_time:41793ms step_avg:57.65ms
step:726/2330 train_time:41853ms step_avg:57.65ms
step:727/2330 train_time:41909ms step_avg:57.65ms
step:728/2330 train_time:41968ms step_avg:57.65ms
step:729/2330 train_time:42024ms step_avg:57.65ms
step:730/2330 train_time:42085ms step_avg:57.65ms
step:731/2330 train_time:42140ms step_avg:57.65ms
step:732/2330 train_time:42200ms step_avg:57.65ms
step:733/2330 train_time:42257ms step_avg:57.65ms
step:734/2330 train_time:42316ms step_avg:57.65ms
step:735/2330 train_time:42372ms step_avg:57.65ms
step:736/2330 train_time:42431ms step_avg:57.65ms
step:737/2330 train_time:42487ms step_avg:57.65ms
step:738/2330 train_time:42547ms step_avg:57.65ms
step:739/2330 train_time:42603ms step_avg:57.65ms
step:740/2330 train_time:42664ms step_avg:57.65ms
step:741/2330 train_time:42720ms step_avg:57.65ms
step:742/2330 train_time:42780ms step_avg:57.65ms
step:743/2330 train_time:42836ms step_avg:57.65ms
step:744/2330 train_time:42895ms step_avg:57.65ms
step:745/2330 train_time:42950ms step_avg:57.65ms
step:746/2330 train_time:43010ms step_avg:57.65ms
step:747/2330 train_time:43065ms step_avg:57.65ms
step:748/2330 train_time:43126ms step_avg:57.65ms
step:749/2330 train_time:43181ms step_avg:57.65ms
step:750/2330 train_time:43241ms step_avg:57.65ms
step:750/2330 val_loss:4.2055 train_time:43320ms step_avg:57.76ms
step:751/2330 train_time:43339ms step_avg:57.71ms
step:752/2330 train_time:43362ms step_avg:57.66ms
step:753/2330 train_time:43418ms step_avg:57.66ms
step:754/2330 train_time:43480ms step_avg:57.67ms
step:755/2330 train_time:43537ms step_avg:57.66ms
step:756/2330 train_time:43597ms step_avg:57.67ms
step:757/2330 train_time:43654ms step_avg:57.67ms
step:758/2330 train_time:43713ms step_avg:57.67ms
step:759/2330 train_time:43768ms step_avg:57.67ms
step:760/2330 train_time:43828ms step_avg:57.67ms
step:761/2330 train_time:43883ms step_avg:57.67ms
step:762/2330 train_time:43941ms step_avg:57.67ms
step:763/2330 train_time:43996ms step_avg:57.66ms
step:764/2330 train_time:44055ms step_avg:57.66ms
step:765/2330 train_time:44112ms step_avg:57.66ms
step:766/2330 train_time:44169ms step_avg:57.66ms
step:767/2330 train_time:44225ms step_avg:57.66ms
step:768/2330 train_time:44286ms step_avg:57.66ms
step:769/2330 train_time:44345ms step_avg:57.67ms
step:770/2330 train_time:44405ms step_avg:57.67ms
step:771/2330 train_time:44464ms step_avg:57.67ms
step:772/2330 train_time:44525ms step_avg:57.67ms
step:773/2330 train_time:44583ms step_avg:57.68ms
step:774/2330 train_time:44644ms step_avg:57.68ms
step:775/2330 train_time:44701ms step_avg:57.68ms
step:776/2330 train_time:44761ms step_avg:57.68ms
step:777/2330 train_time:44818ms step_avg:57.68ms
step:778/2330 train_time:44877ms step_avg:57.68ms
step:779/2330 train_time:44934ms step_avg:57.68ms
step:780/2330 train_time:44993ms step_avg:57.68ms
step:781/2330 train_time:45049ms step_avg:57.68ms
step:782/2330 train_time:45109ms step_avg:57.68ms
step:783/2330 train_time:45165ms step_avg:57.68ms
step:784/2330 train_time:45225ms step_avg:57.68ms
step:785/2330 train_time:45281ms step_avg:57.68ms
step:786/2330 train_time:45341ms step_avg:57.69ms
step:787/2330 train_time:45399ms step_avg:57.69ms
step:788/2330 train_time:45461ms step_avg:57.69ms
step:789/2330 train_time:45518ms step_avg:57.69ms
step:790/2330 train_time:45579ms step_avg:57.69ms
step:791/2330 train_time:45636ms step_avg:57.69ms
step:792/2330 train_time:45697ms step_avg:57.70ms
step:793/2330 train_time:45753ms step_avg:57.70ms
step:794/2330 train_time:45813ms step_avg:57.70ms
step:795/2330 train_time:45868ms step_avg:57.70ms
step:796/2330 train_time:45929ms step_avg:57.70ms
step:797/2330 train_time:45985ms step_avg:57.70ms
step:798/2330 train_time:46045ms step_avg:57.70ms
step:799/2330 train_time:46101ms step_avg:57.70ms
step:800/2330 train_time:46162ms step_avg:57.70ms
step:801/2330 train_time:46218ms step_avg:57.70ms
step:802/2330 train_time:46278ms step_avg:57.70ms
step:803/2330 train_time:46335ms step_avg:57.70ms
step:804/2330 train_time:46397ms step_avg:57.71ms
step:805/2330 train_time:46454ms step_avg:57.71ms
step:806/2330 train_time:46516ms step_avg:57.71ms
step:807/2330 train_time:46572ms step_avg:57.71ms
step:808/2330 train_time:46634ms step_avg:57.72ms
step:809/2330 train_time:46691ms step_avg:57.71ms
step:810/2330 train_time:46752ms step_avg:57.72ms
step:811/2330 train_time:46808ms step_avg:57.72ms
step:812/2330 train_time:46868ms step_avg:57.72ms
step:813/2330 train_time:46925ms step_avg:57.72ms
step:814/2330 train_time:46985ms step_avg:57.72ms
step:815/2330 train_time:47042ms step_avg:57.72ms
step:816/2330 train_time:47102ms step_avg:57.72ms
step:817/2330 train_time:47159ms step_avg:57.72ms
step:818/2330 train_time:47219ms step_avg:57.72ms
step:819/2330 train_time:47276ms step_avg:57.72ms
step:820/2330 train_time:47335ms step_avg:57.73ms
step:821/2330 train_time:47393ms step_avg:57.73ms
step:822/2330 train_time:47452ms step_avg:57.73ms
step:823/2330 train_time:47509ms step_avg:57.73ms
step:824/2330 train_time:47571ms step_avg:57.73ms
step:825/2330 train_time:47628ms step_avg:57.73ms
step:826/2330 train_time:47688ms step_avg:57.73ms
step:827/2330 train_time:47746ms step_avg:57.73ms
step:828/2330 train_time:47805ms step_avg:57.74ms
step:829/2330 train_time:47862ms step_avg:57.73ms
step:830/2330 train_time:47922ms step_avg:57.74ms
step:831/2330 train_time:47978ms step_avg:57.74ms
step:832/2330 train_time:48038ms step_avg:57.74ms
step:833/2330 train_time:48095ms step_avg:57.74ms
step:834/2330 train_time:48154ms step_avg:57.74ms
step:835/2330 train_time:48211ms step_avg:57.74ms
step:836/2330 train_time:48271ms step_avg:57.74ms
step:837/2330 train_time:48329ms step_avg:57.74ms
step:838/2330 train_time:48389ms step_avg:57.74ms
step:839/2330 train_time:48446ms step_avg:57.74ms
step:840/2330 train_time:48506ms step_avg:57.75ms
step:841/2330 train_time:48564ms step_avg:57.75ms
step:842/2330 train_time:48624ms step_avg:57.75ms
step:843/2330 train_time:48682ms step_avg:57.75ms
step:844/2330 train_time:48741ms step_avg:57.75ms
step:845/2330 train_time:48798ms step_avg:57.75ms
step:846/2330 train_time:48857ms step_avg:57.75ms
step:847/2330 train_time:48914ms step_avg:57.75ms
step:848/2330 train_time:48974ms step_avg:57.75ms
step:849/2330 train_time:49031ms step_avg:57.75ms
step:850/2330 train_time:49091ms step_avg:57.75ms
step:851/2330 train_time:49147ms step_avg:57.75ms
step:852/2330 train_time:49207ms step_avg:57.76ms
step:853/2330 train_time:49264ms step_avg:57.75ms
step:854/2330 train_time:49325ms step_avg:57.76ms
step:855/2330 train_time:49382ms step_avg:57.76ms
step:856/2330 train_time:49442ms step_avg:57.76ms
step:857/2330 train_time:49498ms step_avg:57.76ms
step:858/2330 train_time:49559ms step_avg:57.76ms
step:859/2330 train_time:49615ms step_avg:57.76ms
step:860/2330 train_time:49676ms step_avg:57.76ms
step:861/2330 train_time:49733ms step_avg:57.76ms
step:862/2330 train_time:49793ms step_avg:57.76ms
step:863/2330 train_time:49850ms step_avg:57.76ms
step:864/2330 train_time:49910ms step_avg:57.77ms
step:865/2330 train_time:49967ms step_avg:57.76ms
step:866/2330 train_time:50026ms step_avg:57.77ms
step:867/2330 train_time:50083ms step_avg:57.77ms
step:868/2330 train_time:50143ms step_avg:57.77ms
step:869/2330 train_time:50201ms step_avg:57.77ms
step:870/2330 train_time:50260ms step_avg:57.77ms
step:871/2330 train_time:50317ms step_avg:57.77ms
step:872/2330 train_time:50377ms step_avg:57.77ms
step:873/2330 train_time:50434ms step_avg:57.77ms
step:874/2330 train_time:50493ms step_avg:57.77ms
step:875/2330 train_time:50551ms step_avg:57.77ms
step:876/2330 train_time:50611ms step_avg:57.78ms
step:877/2330 train_time:50668ms step_avg:57.77ms
step:878/2330 train_time:50729ms step_avg:57.78ms
step:879/2330 train_time:50785ms step_avg:57.78ms
step:880/2330 train_time:50845ms step_avg:57.78ms
step:881/2330 train_time:50902ms step_avg:57.78ms
step:882/2330 train_time:50962ms step_avg:57.78ms
step:883/2330 train_time:51018ms step_avg:57.78ms
step:884/2330 train_time:51079ms step_avg:57.78ms
step:885/2330 train_time:51135ms step_avg:57.78ms
step:886/2330 train_time:51196ms step_avg:57.78ms
step:887/2330 train_time:51252ms step_avg:57.78ms
step:888/2330 train_time:51313ms step_avg:57.79ms
step:889/2330 train_time:51369ms step_avg:57.78ms
step:890/2330 train_time:51430ms step_avg:57.79ms
step:891/2330 train_time:51487ms step_avg:57.79ms
step:892/2330 train_time:51548ms step_avg:57.79ms
step:893/2330 train_time:51605ms step_avg:57.79ms
step:894/2330 train_time:51665ms step_avg:57.79ms
step:895/2330 train_time:51722ms step_avg:57.79ms
step:896/2330 train_time:51782ms step_avg:57.79ms
step:897/2330 train_time:51841ms step_avg:57.79ms
step:898/2330 train_time:51901ms step_avg:57.80ms
step:899/2330 train_time:51959ms step_avg:57.80ms
step:900/2330 train_time:52018ms step_avg:57.80ms
step:901/2330 train_time:52075ms step_avg:57.80ms
step:902/2330 train_time:52134ms step_avg:57.80ms
step:903/2330 train_time:52190ms step_avg:57.80ms
step:904/2330 train_time:52251ms step_avg:57.80ms
step:905/2330 train_time:52307ms step_avg:57.80ms
step:906/2330 train_time:52368ms step_avg:57.80ms
step:907/2330 train_time:52424ms step_avg:57.80ms
step:908/2330 train_time:52484ms step_avg:57.80ms
step:909/2330 train_time:52541ms step_avg:57.80ms
step:910/2330 train_time:52601ms step_avg:57.80ms
step:911/2330 train_time:52658ms step_avg:57.80ms
step:912/2330 train_time:52718ms step_avg:57.80ms
step:913/2330 train_time:52775ms step_avg:57.80ms
step:914/2330 train_time:52836ms step_avg:57.81ms
step:915/2330 train_time:52892ms step_avg:57.81ms
step:916/2330 train_time:52953ms step_avg:57.81ms
step:917/2330 train_time:53009ms step_avg:57.81ms
step:918/2330 train_time:53070ms step_avg:57.81ms
step:919/2330 train_time:53128ms step_avg:57.81ms
step:920/2330 train_time:53188ms step_avg:57.81ms
step:921/2330 train_time:53245ms step_avg:57.81ms
step:922/2330 train_time:53305ms step_avg:57.81ms
step:923/2330 train_time:53361ms step_avg:57.81ms
step:924/2330 train_time:53422ms step_avg:57.82ms
step:925/2330 train_time:53479ms step_avg:57.82ms
step:926/2330 train_time:53539ms step_avg:57.82ms
step:927/2330 train_time:53596ms step_avg:57.82ms
step:928/2330 train_time:53656ms step_avg:57.82ms
step:929/2330 train_time:53713ms step_avg:57.82ms
step:930/2330 train_time:53772ms step_avg:57.82ms
step:931/2330 train_time:53829ms step_avg:57.82ms
step:932/2330 train_time:53890ms step_avg:57.82ms
step:933/2330 train_time:53947ms step_avg:57.82ms
step:934/2330 train_time:54008ms step_avg:57.82ms
step:935/2330 train_time:54065ms step_avg:57.82ms
step:936/2330 train_time:54125ms step_avg:57.83ms
step:937/2330 train_time:54182ms step_avg:57.83ms
step:938/2330 train_time:54242ms step_avg:57.83ms
step:939/2330 train_time:54298ms step_avg:57.83ms
step:940/2330 train_time:54358ms step_avg:57.83ms
step:941/2330 train_time:54415ms step_avg:57.83ms
step:942/2330 train_time:54474ms step_avg:57.83ms
step:943/2330 train_time:54531ms step_avg:57.83ms
step:944/2330 train_time:54592ms step_avg:57.83ms
step:945/2330 train_time:54649ms step_avg:57.83ms
step:946/2330 train_time:54710ms step_avg:57.83ms
step:947/2330 train_time:54766ms step_avg:57.83ms
step:948/2330 train_time:54827ms step_avg:57.83ms
step:949/2330 train_time:54884ms step_avg:57.83ms
step:950/2330 train_time:54944ms step_avg:57.84ms
step:951/2330 train_time:55001ms step_avg:57.83ms
step:952/2330 train_time:55061ms step_avg:57.84ms
step:953/2330 train_time:55118ms step_avg:57.84ms
step:954/2330 train_time:55178ms step_avg:57.84ms
step:955/2330 train_time:55234ms step_avg:57.84ms
step:956/2330 train_time:55294ms step_avg:57.84ms
step:957/2330 train_time:55351ms step_avg:57.84ms
step:958/2330 train_time:55410ms step_avg:57.84ms
step:959/2330 train_time:55467ms step_avg:57.84ms
step:960/2330 train_time:55528ms step_avg:57.84ms
step:961/2330 train_time:55586ms step_avg:57.84ms
step:962/2330 train_time:55646ms step_avg:57.84ms
step:963/2330 train_time:55703ms step_avg:57.84ms
step:964/2330 train_time:55763ms step_avg:57.85ms
step:965/2330 train_time:55820ms step_avg:57.85ms
step:966/2330 train_time:55880ms step_avg:57.85ms
step:967/2330 train_time:55937ms step_avg:57.85ms
step:968/2330 train_time:55997ms step_avg:57.85ms
step:969/2330 train_time:56054ms step_avg:57.85ms
step:970/2330 train_time:56114ms step_avg:57.85ms
step:971/2330 train_time:56170ms step_avg:57.85ms
step:972/2330 train_time:56231ms step_avg:57.85ms
step:973/2330 train_time:56288ms step_avg:57.85ms
step:974/2330 train_time:56347ms step_avg:57.85ms
step:975/2330 train_time:56404ms step_avg:57.85ms
step:976/2330 train_time:56465ms step_avg:57.85ms
step:977/2330 train_time:56522ms step_avg:57.85ms
step:978/2330 train_time:56582ms step_avg:57.85ms
step:979/2330 train_time:56639ms step_avg:57.85ms
step:980/2330 train_time:56699ms step_avg:57.86ms
step:981/2330 train_time:56756ms step_avg:57.85ms
step:982/2330 train_time:56816ms step_avg:57.86ms
step:983/2330 train_time:56872ms step_avg:57.86ms
step:984/2330 train_time:56932ms step_avg:57.86ms
step:985/2330 train_time:56989ms step_avg:57.86ms
step:986/2330 train_time:57050ms step_avg:57.86ms
step:987/2330 train_time:57107ms step_avg:57.86ms
step:988/2330 train_time:57168ms step_avg:57.86ms
step:989/2330 train_time:57225ms step_avg:57.86ms
step:990/2330 train_time:57285ms step_avg:57.86ms
step:991/2330 train_time:57342ms step_avg:57.86ms
step:992/2330 train_time:57402ms step_avg:57.86ms
step:993/2330 train_time:57458ms step_avg:57.86ms
step:994/2330 train_time:57517ms step_avg:57.86ms
step:995/2330 train_time:57574ms step_avg:57.86ms
step:996/2330 train_time:57634ms step_avg:57.87ms
step:997/2330 train_time:57691ms step_avg:57.86ms
step:998/2330 train_time:57752ms step_avg:57.87ms
step:999/2330 train_time:57809ms step_avg:57.87ms
step:1000/2330 train_time:57868ms step_avg:57.87ms
step:1000/2330 val_loss:4.0660 train_time:57948ms step_avg:57.95ms
step:1001/2330 train_time:57968ms step_avg:57.91ms
step:1002/2330 train_time:57988ms step_avg:57.87ms
step:1003/2330 train_time:58041ms step_avg:57.87ms
step:1004/2330 train_time:58109ms step_avg:57.88ms
step:1005/2330 train_time:58165ms step_avg:57.88ms
step:1006/2330 train_time:58230ms step_avg:57.88ms
step:1007/2330 train_time:58286ms step_avg:57.88ms
step:1008/2330 train_time:58346ms step_avg:57.88ms
step:1009/2330 train_time:58403ms step_avg:57.88ms
step:1010/2330 train_time:58464ms step_avg:57.88ms
step:1011/2330 train_time:58520ms step_avg:57.88ms
step:1012/2330 train_time:58579ms step_avg:57.88ms
step:1013/2330 train_time:58636ms step_avg:57.88ms
step:1014/2330 train_time:58694ms step_avg:57.88ms
step:1015/2330 train_time:58750ms step_avg:57.88ms
step:1016/2330 train_time:58809ms step_avg:57.88ms
step:1017/2330 train_time:58870ms step_avg:57.89ms
step:1018/2330 train_time:58932ms step_avg:57.89ms
step:1019/2330 train_time:58989ms step_avg:57.89ms
step:1020/2330 train_time:59049ms step_avg:57.89ms
step:1021/2330 train_time:59106ms step_avg:57.89ms
step:1022/2330 train_time:59170ms step_avg:57.90ms
step:1023/2330 train_time:59227ms step_avg:57.90ms
step:1024/2330 train_time:59287ms step_avg:57.90ms
step:1025/2330 train_time:59343ms step_avg:57.90ms
step:1026/2330 train_time:59405ms step_avg:57.90ms
step:1027/2330 train_time:59462ms step_avg:57.90ms
step:1028/2330 train_time:59522ms step_avg:57.90ms
step:1029/2330 train_time:59579ms step_avg:57.90ms
step:1030/2330 train_time:59639ms step_avg:57.90ms
step:1031/2330 train_time:59695ms step_avg:57.90ms
step:1032/2330 train_time:59754ms step_avg:57.90ms
step:1033/2330 train_time:59812ms step_avg:57.90ms
step:1034/2330 train_time:59872ms step_avg:57.90ms
step:1035/2330 train_time:59930ms step_avg:57.90ms
step:1036/2330 train_time:59989ms step_avg:57.90ms
step:1037/2330 train_time:60046ms step_avg:57.90ms
step:1038/2330 train_time:60107ms step_avg:57.91ms
step:1039/2330 train_time:60164ms step_avg:57.91ms
step:1040/2330 train_time:60227ms step_avg:57.91ms
step:1041/2330 train_time:60283ms step_avg:57.91ms
step:1042/2330 train_time:60345ms step_avg:57.91ms
step:1043/2330 train_time:60402ms step_avg:57.91ms
step:1044/2330 train_time:60463ms step_avg:57.91ms
step:1045/2330 train_time:60519ms step_avg:57.91ms
step:1046/2330 train_time:60579ms step_avg:57.92ms
step:1047/2330 train_time:60637ms step_avg:57.91ms
step:1048/2330 train_time:60695ms step_avg:57.92ms
step:1049/2330 train_time:60752ms step_avg:57.91ms
step:1050/2330 train_time:60811ms step_avg:57.92ms
step:1051/2330 train_time:60868ms step_avg:57.91ms
step:1052/2330 train_time:60928ms step_avg:57.92ms
step:1053/2330 train_time:60984ms step_avg:57.91ms
step:1054/2330 train_time:61045ms step_avg:57.92ms
step:1055/2330 train_time:61103ms step_avg:57.92ms
step:1056/2330 train_time:61163ms step_avg:57.92ms
step:1057/2330 train_time:61221ms step_avg:57.92ms
step:1058/2330 train_time:61281ms step_avg:57.92ms
step:1059/2330 train_time:61338ms step_avg:57.92ms
step:1060/2330 train_time:61399ms step_avg:57.92ms
step:1061/2330 train_time:61455ms step_avg:57.92ms
step:1062/2330 train_time:61516ms step_avg:57.92ms
step:1063/2330 train_time:61573ms step_avg:57.92ms
step:1064/2330 train_time:61632ms step_avg:57.93ms
step:1065/2330 train_time:61688ms step_avg:57.92ms
step:1066/2330 train_time:61748ms step_avg:57.93ms
step:1067/2330 train_time:61805ms step_avg:57.92ms
step:1068/2330 train_time:61866ms step_avg:57.93ms
step:1069/2330 train_time:61923ms step_avg:57.93ms
step:1070/2330 train_time:61983ms step_avg:57.93ms
step:1071/2330 train_time:62041ms step_avg:57.93ms
step:1072/2330 train_time:62101ms step_avg:57.93ms
step:1073/2330 train_time:62158ms step_avg:57.93ms
step:1074/2330 train_time:62218ms step_avg:57.93ms
step:1075/2330 train_time:62275ms step_avg:57.93ms
step:1076/2330 train_time:62335ms step_avg:57.93ms
step:1077/2330 train_time:62391ms step_avg:57.93ms
step:1078/2330 train_time:62452ms step_avg:57.93ms
step:1079/2330 train_time:62509ms step_avg:57.93ms
step:1080/2330 train_time:62569ms step_avg:57.93ms
step:1081/2330 train_time:62627ms step_avg:57.93ms
step:1082/2330 train_time:62686ms step_avg:57.94ms
step:1083/2330 train_time:62743ms step_avg:57.93ms
step:1084/2330 train_time:62803ms step_avg:57.94ms
step:1085/2330 train_time:62860ms step_avg:57.94ms
step:1086/2330 train_time:62920ms step_avg:57.94ms
step:1087/2330 train_time:62977ms step_avg:57.94ms
step:1088/2330 train_time:63038ms step_avg:57.94ms
step:1089/2330 train_time:63094ms step_avg:57.94ms
step:1090/2330 train_time:63153ms step_avg:57.94ms
step:1091/2330 train_time:63210ms step_avg:57.94ms
step:1092/2330 train_time:63272ms step_avg:57.94ms
step:1093/2330 train_time:63329ms step_avg:57.94ms
step:1094/2330 train_time:63389ms step_avg:57.94ms
step:1095/2330 train_time:63446ms step_avg:57.94ms
step:1096/2330 train_time:63507ms step_avg:57.94ms
step:1097/2330 train_time:63563ms step_avg:57.94ms
step:1098/2330 train_time:63624ms step_avg:57.95ms
step:1099/2330 train_time:63681ms step_avg:57.94ms
step:1100/2330 train_time:63740ms step_avg:57.95ms
step:1101/2330 train_time:63797ms step_avg:57.94ms
step:1102/2330 train_time:63858ms step_avg:57.95ms
step:1103/2330 train_time:63914ms step_avg:57.95ms
step:1104/2330 train_time:63974ms step_avg:57.95ms
step:1105/2330 train_time:64031ms step_avg:57.95ms
step:1106/2330 train_time:64091ms step_avg:57.95ms
step:1107/2330 train_time:64148ms step_avg:57.95ms
step:1108/2330 train_time:64209ms step_avg:57.95ms
step:1109/2330 train_time:64265ms step_avg:57.95ms
step:1110/2330 train_time:64327ms step_avg:57.95ms
step:1111/2330 train_time:64383ms step_avg:57.95ms
step:1112/2330 train_time:64444ms step_avg:57.95ms
step:1113/2330 train_time:64502ms step_avg:57.95ms
step:1114/2330 train_time:64562ms step_avg:57.95ms
step:1115/2330 train_time:64619ms step_avg:57.95ms
step:1116/2330 train_time:64678ms step_avg:57.96ms
step:1117/2330 train_time:64735ms step_avg:57.95ms
step:1118/2330 train_time:64795ms step_avg:57.96ms
step:1119/2330 train_time:64851ms step_avg:57.95ms
step:1120/2330 train_time:64912ms step_avg:57.96ms
step:1121/2330 train_time:64968ms step_avg:57.96ms
step:1122/2330 train_time:65029ms step_avg:57.96ms
step:1123/2330 train_time:65086ms step_avg:57.96ms
step:1124/2330 train_time:65146ms step_avg:57.96ms
step:1125/2330 train_time:65202ms step_avg:57.96ms
step:1126/2330 train_time:65263ms step_avg:57.96ms
step:1127/2330 train_time:65320ms step_avg:57.96ms
step:1128/2330 train_time:65380ms step_avg:57.96ms
step:1129/2330 train_time:65438ms step_avg:57.96ms
step:1130/2330 train_time:65498ms step_avg:57.96ms
step:1131/2330 train_time:65555ms step_avg:57.96ms
step:1132/2330 train_time:65615ms step_avg:57.96ms
step:1133/2330 train_time:65671ms step_avg:57.96ms
step:1134/2330 train_time:65731ms step_avg:57.96ms
step:1135/2330 train_time:65788ms step_avg:57.96ms
step:1136/2330 train_time:65848ms step_avg:57.96ms
step:1137/2330 train_time:65905ms step_avg:57.96ms
step:1138/2330 train_time:65965ms step_avg:57.97ms
step:1139/2330 train_time:66022ms step_avg:57.97ms
step:1140/2330 train_time:66082ms step_avg:57.97ms
step:1141/2330 train_time:66139ms step_avg:57.97ms
step:1142/2330 train_time:66199ms step_avg:57.97ms
step:1143/2330 train_time:66256ms step_avg:57.97ms
step:1144/2330 train_time:66316ms step_avg:57.97ms
step:1145/2330 train_time:66374ms step_avg:57.97ms
step:1146/2330 train_time:66819ms step_avg:58.31ms
step:1147/2330 train_time:66875ms step_avg:58.30ms
step:1148/2330 train_time:66934ms step_avg:58.30ms
step:1149/2330 train_time:66990ms step_avg:58.30ms
step:1150/2330 train_time:67049ms step_avg:58.30ms
step:1151/2330 train_time:67105ms step_avg:58.30ms
step:1152/2330 train_time:67165ms step_avg:58.30ms
step:1153/2330 train_time:67221ms step_avg:58.30ms
step:1154/2330 train_time:67280ms step_avg:58.30ms
step:1155/2330 train_time:67336ms step_avg:58.30ms
step:1156/2330 train_time:67396ms step_avg:58.30ms
step:1157/2330 train_time:67451ms step_avg:58.30ms
step:1158/2330 train_time:67511ms step_avg:58.30ms
step:1159/2330 train_time:67567ms step_avg:58.30ms
step:1160/2330 train_time:67626ms step_avg:58.30ms
step:1161/2330 train_time:67685ms step_avg:58.30ms
step:1162/2330 train_time:67752ms step_avg:58.31ms
step:1163/2330 train_time:67809ms step_avg:58.31ms
step:1164/2330 train_time:67871ms step_avg:58.31ms
step:1165/2330 train_time:67928ms step_avg:58.31ms
step:1166/2330 train_time:67989ms step_avg:58.31ms
step:1167/2330 train_time:68045ms step_avg:58.31ms
step:1168/2330 train_time:68105ms step_avg:58.31ms
step:1169/2330 train_time:68161ms step_avg:58.31ms
step:1170/2330 train_time:68221ms step_avg:58.31ms
step:1171/2330 train_time:68278ms step_avg:58.31ms
step:1172/2330 train_time:68337ms step_avg:58.31ms
step:1173/2330 train_time:68393ms step_avg:58.31ms
step:1174/2330 train_time:68452ms step_avg:58.31ms
step:1175/2330 train_time:68509ms step_avg:58.31ms
step:1176/2330 train_time:68568ms step_avg:58.31ms
step:1177/2330 train_time:68626ms step_avg:58.31ms
step:1178/2330 train_time:68687ms step_avg:58.31ms
step:1179/2330 train_time:68745ms step_avg:58.31ms
step:1180/2330 train_time:68806ms step_avg:58.31ms
step:1181/2330 train_time:68864ms step_avg:58.31ms
step:1182/2330 train_time:68926ms step_avg:58.31ms
step:1183/2330 train_time:68983ms step_avg:58.31ms
step:1184/2330 train_time:69044ms step_avg:58.31ms
step:1185/2330 train_time:69101ms step_avg:58.31ms
step:1186/2330 train_time:69160ms step_avg:58.31ms
step:1187/2330 train_time:69217ms step_avg:58.31ms
step:1188/2330 train_time:69277ms step_avg:58.31ms
step:1189/2330 train_time:69334ms step_avg:58.31ms
step:1190/2330 train_time:69393ms step_avg:58.31ms
step:1191/2330 train_time:69449ms step_avg:58.31ms
step:1192/2330 train_time:69508ms step_avg:58.31ms
step:1193/2330 train_time:69565ms step_avg:58.31ms
step:1194/2330 train_time:69626ms step_avg:58.31ms
step:1195/2330 train_time:69683ms step_avg:58.31ms
step:1196/2330 train_time:69746ms step_avg:58.32ms
step:1197/2330 train_time:69803ms step_avg:58.32ms
step:1198/2330 train_time:69865ms step_avg:58.32ms
step:1199/2330 train_time:69922ms step_avg:58.32ms
step:1200/2330 train_time:69982ms step_avg:58.32ms
step:1201/2330 train_time:70039ms step_avg:58.32ms
step:1202/2330 train_time:70100ms step_avg:58.32ms
step:1203/2330 train_time:70157ms step_avg:58.32ms
step:1204/2330 train_time:70216ms step_avg:58.32ms
step:1205/2330 train_time:70273ms step_avg:58.32ms
step:1206/2330 train_time:70332ms step_avg:58.32ms
step:1207/2330 train_time:70389ms step_avg:58.32ms
step:1208/2330 train_time:70448ms step_avg:58.32ms
step:1209/2330 train_time:70504ms step_avg:58.32ms
step:1210/2330 train_time:70564ms step_avg:58.32ms
step:1211/2330 train_time:70622ms step_avg:58.32ms
step:1212/2330 train_time:70682ms step_avg:58.32ms
step:1213/2330 train_time:70739ms step_avg:58.32ms
step:1214/2330 train_time:70801ms step_avg:58.32ms
step:1215/2330 train_time:70858ms step_avg:58.32ms
step:1216/2330 train_time:70919ms step_avg:58.32ms
step:1217/2330 train_time:70976ms step_avg:58.32ms
step:1218/2330 train_time:71036ms step_avg:58.32ms
step:1219/2330 train_time:71092ms step_avg:58.32ms
step:1220/2330 train_time:71152ms step_avg:58.32ms
step:1221/2330 train_time:71209ms step_avg:58.32ms
step:1222/2330 train_time:71269ms step_avg:58.32ms
step:1223/2330 train_time:71326ms step_avg:58.32ms
step:1224/2330 train_time:71385ms step_avg:58.32ms
step:1225/2330 train_time:71442ms step_avg:58.32ms
step:1226/2330 train_time:71501ms step_avg:58.32ms
step:1227/2330 train_time:71558ms step_avg:58.32ms
step:1228/2330 train_time:71618ms step_avg:58.32ms
step:1229/2330 train_time:71676ms step_avg:58.32ms
step:1230/2330 train_time:71736ms step_avg:58.32ms
step:1231/2330 train_time:71793ms step_avg:58.32ms
step:1232/2330 train_time:71854ms step_avg:58.32ms
step:1233/2330 train_time:71911ms step_avg:58.32ms
step:1234/2330 train_time:71972ms step_avg:58.32ms
step:1235/2330 train_time:72029ms step_avg:58.32ms
step:1236/2330 train_time:72089ms step_avg:58.32ms
step:1237/2330 train_time:72146ms step_avg:58.32ms
step:1238/2330 train_time:72206ms step_avg:58.32ms
step:1239/2330 train_time:72263ms step_avg:58.32ms
step:1240/2330 train_time:72324ms step_avg:58.33ms
step:1241/2330 train_time:72381ms step_avg:58.32ms
step:1242/2330 train_time:72440ms step_avg:58.33ms
step:1243/2330 train_time:72498ms step_avg:58.32ms
step:1244/2330 train_time:72557ms step_avg:58.33ms
step:1245/2330 train_time:72613ms step_avg:58.32ms
step:1246/2330 train_time:72674ms step_avg:58.33ms
step:1247/2330 train_time:72731ms step_avg:58.33ms
step:1248/2330 train_time:72791ms step_avg:58.33ms
step:1249/2330 train_time:72848ms step_avg:58.33ms
step:1250/2330 train_time:72910ms step_avg:58.33ms
step:1250/2330 val_loss:3.9826 train_time:72992ms step_avg:58.39ms
step:1251/2330 train_time:73011ms step_avg:58.36ms
step:1252/2330 train_time:73031ms step_avg:58.33ms
step:1253/2330 train_time:73089ms step_avg:58.33ms
step:1254/2330 train_time:73152ms step_avg:58.33ms
step:1255/2330 train_time:73209ms step_avg:58.33ms
step:1256/2330 train_time:73272ms step_avg:58.34ms
step:1257/2330 train_time:73328ms step_avg:58.34ms
step:1258/2330 train_time:73389ms step_avg:58.34ms
step:1259/2330 train_time:73445ms step_avg:58.34ms
step:1260/2330 train_time:73505ms step_avg:58.34ms
step:1261/2330 train_time:73561ms step_avg:58.34ms
step:1262/2330 train_time:73620ms step_avg:58.34ms
step:1263/2330 train_time:73677ms step_avg:58.33ms
step:1264/2330 train_time:73736ms step_avg:58.34ms
step:1265/2330 train_time:73792ms step_avg:58.33ms
step:1266/2330 train_time:73851ms step_avg:58.33ms
step:1267/2330 train_time:73907ms step_avg:58.33ms
step:1268/2330 train_time:73969ms step_avg:58.33ms
step:1269/2330 train_time:74027ms step_avg:58.33ms
step:1270/2330 train_time:74087ms step_avg:58.34ms
step:1271/2330 train_time:74144ms step_avg:58.34ms
step:1272/2330 train_time:74205ms step_avg:58.34ms
step:1273/2330 train_time:74263ms step_avg:58.34ms
step:1274/2330 train_time:74324ms step_avg:58.34ms
step:1275/2330 train_time:74380ms step_avg:58.34ms
step:1276/2330 train_time:74440ms step_avg:58.34ms
step:1277/2330 train_time:74496ms step_avg:58.34ms
step:1278/2330 train_time:74556ms step_avg:58.34ms
step:1279/2330 train_time:74612ms step_avg:58.34ms
step:1280/2330 train_time:74672ms step_avg:58.34ms
step:1281/2330 train_time:74729ms step_avg:58.34ms
step:1282/2330 train_time:74788ms step_avg:58.34ms
step:1283/2330 train_time:74845ms step_avg:58.34ms
step:1284/2330 train_time:74905ms step_avg:58.34ms
step:1285/2330 train_time:74963ms step_avg:58.34ms
step:1286/2330 train_time:75023ms step_avg:58.34ms
step:1287/2330 train_time:75080ms step_avg:58.34ms
step:1288/2330 train_time:75140ms step_avg:58.34ms
step:1289/2330 train_time:75197ms step_avg:58.34ms
step:1290/2330 train_time:75258ms step_avg:58.34ms
step:1291/2330 train_time:75315ms step_avg:58.34ms
step:1292/2330 train_time:75853ms step_avg:58.71ms
step:1293/2330 train_time:75870ms step_avg:58.68ms
step:1294/2330 train_time:75915ms step_avg:58.67ms
step:1295/2330 train_time:75970ms step_avg:58.66ms
step:1296/2330 train_time:76029ms step_avg:58.66ms
step:1297/2330 train_time:76086ms step_avg:58.66ms
step:1298/2330 train_time:76144ms step_avg:58.66ms
step:1299/2330 train_time:76201ms step_avg:58.66ms
step:1300/2330 train_time:76260ms step_avg:58.66ms
step:1301/2330 train_time:76316ms step_avg:58.66ms
step:1302/2330 train_time:76375ms step_avg:58.66ms
step:1303/2330 train_time:76432ms step_avg:58.66ms
step:1304/2330 train_time:76491ms step_avg:58.66ms
step:1305/2330 train_time:76547ms step_avg:58.66ms
step:1306/2330 train_time:76606ms step_avg:58.66ms
step:1307/2330 train_time:76663ms step_avg:58.66ms
step:1308/2330 train_time:76722ms step_avg:58.66ms
step:1309/2330 train_time:76784ms step_avg:58.66ms
step:1310/2330 train_time:76849ms step_avg:58.66ms
step:1311/2330 train_time:76907ms step_avg:58.66ms
step:1312/2330 train_time:76968ms step_avg:58.66ms
step:1313/2330 train_time:77024ms step_avg:58.66ms
step:1314/2330 train_time:77084ms step_avg:58.66ms
step:1315/2330 train_time:77140ms step_avg:58.66ms
step:1316/2330 train_time:77199ms step_avg:58.66ms
step:1317/2330 train_time:77256ms step_avg:58.66ms
step:1318/2330 train_time:77315ms step_avg:58.66ms
step:1319/2330 train_time:77371ms step_avg:58.66ms
step:1320/2330 train_time:77430ms step_avg:58.66ms
step:1321/2330 train_time:77486ms step_avg:58.66ms
step:1322/2330 train_time:77545ms step_avg:58.66ms
step:1323/2330 train_time:77602ms step_avg:58.66ms
step:1324/2330 train_time:77662ms step_avg:58.66ms
step:1325/2330 train_time:77720ms step_avg:58.66ms
step:1326/2330 train_time:77780ms step_avg:58.66ms
step:1327/2330 train_time:77839ms step_avg:58.66ms
step:1328/2330 train_time:77901ms step_avg:58.66ms
step:1329/2330 train_time:77958ms step_avg:58.66ms
step:1330/2330 train_time:78018ms step_avg:58.66ms
step:1331/2330 train_time:78074ms step_avg:58.66ms
step:1332/2330 train_time:78136ms step_avg:58.66ms
step:1333/2330 train_time:78193ms step_avg:58.66ms
step:1334/2330 train_time:78252ms step_avg:58.66ms
step:1335/2330 train_time:78308ms step_avg:58.66ms
step:1336/2330 train_time:78368ms step_avg:58.66ms
step:1337/2330 train_time:78424ms step_avg:58.66ms
step:1338/2330 train_time:78484ms step_avg:58.66ms
step:1339/2330 train_time:78540ms step_avg:58.66ms
step:1340/2330 train_time:78599ms step_avg:58.66ms
step:1341/2330 train_time:78656ms step_avg:58.65ms
step:1342/2330 train_time:78717ms step_avg:58.66ms
step:1343/2330 train_time:78775ms step_avg:58.66ms
step:1344/2330 train_time:78835ms step_avg:58.66ms
step:1345/2330 train_time:78892ms step_avg:58.66ms
step:1346/2330 train_time:78954ms step_avg:58.66ms
step:1347/2330 train_time:79010ms step_avg:58.66ms
step:1348/2330 train_time:79072ms step_avg:58.66ms
step:1349/2330 train_time:79129ms step_avg:58.66ms
step:1350/2330 train_time:79190ms step_avg:58.66ms
step:1351/2330 train_time:79247ms step_avg:58.66ms
step:1352/2330 train_time:79306ms step_avg:58.66ms
step:1353/2330 train_time:79363ms step_avg:58.66ms
step:1354/2330 train_time:79422ms step_avg:58.66ms
step:1355/2330 train_time:79479ms step_avg:58.66ms
step:1356/2330 train_time:79538ms step_avg:58.66ms
step:1357/2330 train_time:79595ms step_avg:58.66ms
step:1358/2330 train_time:79654ms step_avg:58.66ms
step:1359/2330 train_time:79711ms step_avg:58.65ms
step:1360/2330 train_time:79771ms step_avg:58.66ms
step:1361/2330 train_time:79829ms step_avg:58.65ms
step:1362/2330 train_time:79890ms step_avg:58.66ms
step:1363/2330 train_time:79948ms step_avg:58.66ms
step:1364/2330 train_time:80008ms step_avg:58.66ms
step:1365/2330 train_time:80065ms step_avg:58.66ms
step:1366/2330 train_time:80125ms step_avg:58.66ms
step:1367/2330 train_time:80182ms step_avg:58.66ms
step:1368/2330 train_time:80242ms step_avg:58.66ms
step:1369/2330 train_time:80299ms step_avg:58.65ms
step:1370/2330 train_time:80358ms step_avg:58.66ms
step:1371/2330 train_time:80415ms step_avg:58.65ms
step:1372/2330 train_time:80475ms step_avg:58.66ms
step:1373/2330 train_time:80531ms step_avg:58.65ms
step:1374/2330 train_time:80591ms step_avg:58.65ms
step:1375/2330 train_time:80648ms step_avg:58.65ms
step:1376/2330 train_time:80709ms step_avg:58.65ms
step:1377/2330 train_time:80766ms step_avg:58.65ms
step:1378/2330 train_time:80826ms step_avg:58.65ms
step:1379/2330 train_time:80883ms step_avg:58.65ms
step:1380/2330 train_time:80944ms step_avg:58.66ms
step:1381/2330 train_time:81002ms step_avg:58.65ms
step:1382/2330 train_time:81062ms step_avg:58.66ms
step:1383/2330 train_time:81119ms step_avg:58.65ms
step:1384/2330 train_time:81178ms step_avg:58.65ms
step:1385/2330 train_time:81235ms step_avg:58.65ms
step:1386/2330 train_time:81294ms step_avg:58.65ms
step:1387/2330 train_time:81352ms step_avg:58.65ms
step:1388/2330 train_time:81411ms step_avg:58.65ms
step:1389/2330 train_time:81468ms step_avg:58.65ms
step:1390/2330 train_time:81527ms step_avg:58.65ms
step:1391/2330 train_time:81584ms step_avg:58.65ms
step:1392/2330 train_time:81644ms step_avg:58.65ms
step:1393/2330 train_time:81701ms step_avg:58.65ms
step:1394/2330 train_time:81762ms step_avg:58.65ms
step:1395/2330 train_time:81819ms step_avg:58.65ms
step:1396/2330 train_time:81879ms step_avg:58.65ms
step:1397/2330 train_time:81935ms step_avg:58.65ms
step:1398/2330 train_time:81996ms step_avg:58.65ms
step:1399/2330 train_time:82054ms step_avg:58.65ms
step:1400/2330 train_time:82114ms step_avg:58.65ms
step:1401/2330 train_time:82170ms step_avg:58.65ms
step:1402/2330 train_time:82231ms step_avg:58.65ms
step:1403/2330 train_time:82289ms step_avg:58.65ms
step:1404/2330 train_time:82348ms step_avg:58.65ms
step:1405/2330 train_time:82405ms step_avg:58.65ms
step:1406/2330 train_time:82464ms step_avg:58.65ms
step:1407/2330 train_time:82522ms step_avg:58.65ms
step:1408/2330 train_time:82581ms step_avg:58.65ms
step:1409/2330 train_time:82638ms step_avg:58.65ms
step:1410/2330 train_time:82698ms step_avg:58.65ms
step:1411/2330 train_time:82755ms step_avg:58.65ms
step:1412/2330 train_time:82815ms step_avg:58.65ms
step:1413/2330 train_time:82872ms step_avg:58.65ms
step:1414/2330 train_time:82933ms step_avg:58.65ms
step:1415/2330 train_time:82990ms step_avg:58.65ms
step:1416/2330 train_time:83050ms step_avg:58.65ms
step:1417/2330 train_time:83107ms step_avg:58.65ms
step:1418/2330 train_time:83167ms step_avg:58.65ms
step:1419/2330 train_time:83223ms step_avg:58.65ms
step:1420/2330 train_time:83283ms step_avg:58.65ms
step:1421/2330 train_time:83340ms step_avg:58.65ms
step:1422/2330 train_time:83400ms step_avg:58.65ms
step:1423/2330 train_time:83457ms step_avg:58.65ms
step:1424/2330 train_time:83516ms step_avg:58.65ms
step:1425/2330 train_time:83572ms step_avg:58.65ms
step:1426/2330 train_time:83634ms step_avg:58.65ms
step:1427/2330 train_time:83690ms step_avg:58.65ms
step:1428/2330 train_time:83750ms step_avg:58.65ms
step:1429/2330 train_time:83807ms step_avg:58.65ms
step:1430/2330 train_time:83868ms step_avg:58.65ms
step:1431/2330 train_time:83925ms step_avg:58.65ms
step:1432/2330 train_time:83985ms step_avg:58.65ms
step:1433/2330 train_time:84042ms step_avg:58.65ms
step:1434/2330 train_time:84102ms step_avg:58.65ms
step:1435/2330 train_time:84159ms step_avg:58.65ms
step:1436/2330 train_time:84219ms step_avg:58.65ms
step:1437/2330 train_time:84275ms step_avg:58.65ms
step:1438/2330 train_time:84335ms step_avg:58.65ms
step:1439/2330 train_time:84392ms step_avg:58.65ms
step:1440/2330 train_time:84452ms step_avg:58.65ms
step:1441/2330 train_time:84509ms step_avg:58.65ms
step:1442/2330 train_time:84569ms step_avg:58.65ms
step:1443/2330 train_time:84627ms step_avg:58.65ms
step:1444/2330 train_time:84687ms step_avg:58.65ms
step:1445/2330 train_time:84744ms step_avg:58.65ms
step:1446/2330 train_time:84804ms step_avg:58.65ms
step:1447/2330 train_time:84861ms step_avg:58.65ms
step:1448/2330 train_time:84921ms step_avg:58.65ms
step:1449/2330 train_time:84979ms step_avg:58.65ms
step:1450/2330 train_time:85038ms step_avg:58.65ms
step:1451/2330 train_time:85095ms step_avg:58.65ms
step:1452/2330 train_time:85155ms step_avg:58.65ms
step:1453/2330 train_time:85212ms step_avg:58.65ms
step:1454/2330 train_time:85272ms step_avg:58.65ms
step:1455/2330 train_time:85329ms step_avg:58.65ms
step:1456/2330 train_time:85390ms step_avg:58.65ms
step:1457/2330 train_time:85446ms step_avg:58.65ms
step:1458/2330 train_time:85508ms step_avg:58.65ms
step:1459/2330 train_time:85565ms step_avg:58.65ms
step:1460/2330 train_time:85625ms step_avg:58.65ms
step:1461/2330 train_time:85682ms step_avg:58.65ms
step:1462/2330 train_time:85742ms step_avg:58.65ms
step:1463/2330 train_time:85798ms step_avg:58.65ms
step:1464/2330 train_time:85858ms step_avg:58.65ms
step:1465/2330 train_time:85915ms step_avg:58.65ms
step:1466/2330 train_time:85976ms step_avg:58.65ms
step:1467/2330 train_time:86033ms step_avg:58.65ms
step:1468/2330 train_time:86092ms step_avg:58.65ms
step:1469/2330 train_time:86150ms step_avg:58.65ms
step:1470/2330 train_time:86210ms step_avg:58.65ms
step:1471/2330 train_time:86268ms step_avg:58.65ms
step:1472/2330 train_time:86327ms step_avg:58.65ms
step:1473/2330 train_time:86384ms step_avg:58.64ms
step:1474/2330 train_time:86444ms step_avg:58.65ms
step:1475/2330 train_time:86502ms step_avg:58.65ms
step:1476/2330 train_time:86562ms step_avg:58.65ms
step:1477/2330 train_time:86619ms step_avg:58.65ms
step:1478/2330 train_time:86679ms step_avg:58.65ms
step:1479/2330 train_time:86736ms step_avg:58.65ms
step:1480/2330 train_time:86795ms step_avg:58.65ms
step:1481/2330 train_time:86852ms step_avg:58.64ms
step:1482/2330 train_time:86912ms step_avg:58.65ms
step:1483/2330 train_time:86969ms step_avg:58.64ms
step:1484/2330 train_time:87030ms step_avg:58.65ms
step:1485/2330 train_time:87086ms step_avg:58.64ms
step:1486/2330 train_time:87147ms step_avg:58.65ms
step:1487/2330 train_time:87204ms step_avg:58.64ms
step:1488/2330 train_time:87264ms step_avg:58.65ms
step:1489/2330 train_time:87321ms step_avg:58.64ms
step:1490/2330 train_time:87380ms step_avg:58.64ms
step:1491/2330 train_time:87437ms step_avg:58.64ms
step:1492/2330 train_time:87497ms step_avg:58.64ms
step:1493/2330 train_time:87554ms step_avg:58.64ms
step:1494/2330 train_time:87614ms step_avg:58.64ms
step:1495/2330 train_time:87670ms step_avg:58.64ms
step:1496/2330 train_time:87731ms step_avg:58.64ms
step:1497/2330 train_time:87788ms step_avg:58.64ms
step:1498/2330 train_time:87848ms step_avg:58.64ms
step:1499/2330 train_time:87905ms step_avg:58.64ms
step:1500/2330 train_time:87964ms step_avg:58.64ms
step:1500/2330 val_loss:3.9017 train_time:88045ms step_avg:58.70ms
step:1501/2330 train_time:88065ms step_avg:58.67ms
step:1502/2330 train_time:88084ms step_avg:58.64ms
step:1503/2330 train_time:88144ms step_avg:58.65ms
step:1504/2330 train_time:88208ms step_avg:58.65ms
step:1505/2330 train_time:88265ms step_avg:58.65ms
step:1506/2330 train_time:88327ms step_avg:58.65ms
step:1507/2330 train_time:88384ms step_avg:58.65ms
step:1508/2330 train_time:88443ms step_avg:58.65ms
step:1509/2330 train_time:88500ms step_avg:58.65ms
step:1510/2330 train_time:88560ms step_avg:58.65ms
step:1511/2330 train_time:88616ms step_avg:58.65ms
step:1512/2330 train_time:88675ms step_avg:58.65ms
step:1513/2330 train_time:88731ms step_avg:58.65ms
step:1514/2330 train_time:88790ms step_avg:58.65ms
step:1515/2330 train_time:88846ms step_avg:58.64ms
step:1516/2330 train_time:88906ms step_avg:58.65ms
step:1517/2330 train_time:88963ms step_avg:58.64ms
step:1518/2330 train_time:89024ms step_avg:58.65ms
step:1519/2330 train_time:89083ms step_avg:58.65ms
step:1520/2330 train_time:89144ms step_avg:58.65ms
step:1521/2330 train_time:89202ms step_avg:58.65ms
step:1522/2330 train_time:89263ms step_avg:58.65ms
step:1523/2330 train_time:89320ms step_avg:58.65ms
step:1524/2330 train_time:89380ms step_avg:58.65ms
step:1525/2330 train_time:89437ms step_avg:58.65ms
step:1526/2330 train_time:89496ms step_avg:58.65ms
step:1527/2330 train_time:89553ms step_avg:58.65ms
step:1528/2330 train_time:89612ms step_avg:58.65ms
step:1529/2330 train_time:89670ms step_avg:58.65ms
step:1530/2330 train_time:89729ms step_avg:58.65ms
step:1531/2330 train_time:89786ms step_avg:58.65ms
step:1532/2330 train_time:89846ms step_avg:58.65ms
step:1533/2330 train_time:89903ms step_avg:58.65ms
step:1534/2330 train_time:89964ms step_avg:58.65ms
step:1535/2330 train_time:90022ms step_avg:58.65ms
step:1536/2330 train_time:90083ms step_avg:58.65ms
step:1537/2330 train_time:90142ms step_avg:58.65ms
step:1538/2330 train_time:90202ms step_avg:58.65ms
step:1539/2330 train_time:90260ms step_avg:58.65ms
step:1540/2330 train_time:90321ms step_avg:58.65ms
step:1541/2330 train_time:90379ms step_avg:58.65ms
step:1542/2330 train_time:90439ms step_avg:58.65ms
step:1543/2330 train_time:90497ms step_avg:58.65ms
step:1544/2330 train_time:90557ms step_avg:58.65ms
step:1545/2330 train_time:90613ms step_avg:58.65ms
step:1546/2330 train_time:90674ms step_avg:58.65ms
step:1547/2330 train_time:90731ms step_avg:58.65ms
step:1548/2330 train_time:90792ms step_avg:58.65ms
step:1549/2330 train_time:90849ms step_avg:58.65ms
step:1550/2330 train_time:90910ms step_avg:58.65ms
step:1551/2330 train_time:90966ms step_avg:58.65ms
step:1552/2330 train_time:91029ms step_avg:58.65ms
step:1553/2330 train_time:91086ms step_avg:58.65ms
step:1554/2330 train_time:91149ms step_avg:58.65ms
step:1555/2330 train_time:91206ms step_avg:58.65ms
step:1556/2330 train_time:91268ms step_avg:58.66ms
step:1557/2330 train_time:91326ms step_avg:58.65ms
step:1558/2330 train_time:91388ms step_avg:58.66ms
step:1559/2330 train_time:91446ms step_avg:58.66ms
step:1560/2330 train_time:91507ms step_avg:58.66ms
step:1561/2330 train_time:91565ms step_avg:58.66ms
step:1562/2330 train_time:91626ms step_avg:58.66ms
step:1563/2330 train_time:91684ms step_avg:58.66ms
step:1564/2330 train_time:91744ms step_avg:58.66ms
step:1565/2330 train_time:91802ms step_avg:58.66ms
step:1566/2330 train_time:91862ms step_avg:58.66ms
step:1567/2330 train_time:91919ms step_avg:58.66ms
step:1568/2330 train_time:91980ms step_avg:58.66ms
step:1569/2330 train_time:92037ms step_avg:58.66ms
step:1570/2330 train_time:92099ms step_avg:58.66ms
step:1571/2330 train_time:92157ms step_avg:58.66ms
step:1572/2330 train_time:92217ms step_avg:58.66ms
step:1573/2330 train_time:92275ms step_avg:58.66ms
step:1574/2330 train_time:92336ms step_avg:58.66ms
step:1575/2330 train_time:92393ms step_avg:58.66ms
step:1576/2330 train_time:92454ms step_avg:58.66ms
step:1577/2330 train_time:92511ms step_avg:58.66ms
step:1578/2330 train_time:92572ms step_avg:58.66ms
step:1579/2330 train_time:92629ms step_avg:58.66ms
step:1580/2330 train_time:92691ms step_avg:58.67ms
step:1581/2330 train_time:92748ms step_avg:58.66ms
step:1582/2330 train_time:92808ms step_avg:58.67ms
step:1583/2330 train_time:92865ms step_avg:58.66ms
step:1584/2330 train_time:92927ms step_avg:58.67ms
step:1585/2330 train_time:92985ms step_avg:58.67ms
step:1586/2330 train_time:93045ms step_avg:58.67ms
step:1587/2330 train_time:93103ms step_avg:58.67ms
step:1588/2330 train_time:93163ms step_avg:58.67ms
step:1589/2330 train_time:93221ms step_avg:58.67ms
step:1590/2330 train_time:93281ms step_avg:58.67ms
step:1591/2330 train_time:93341ms step_avg:58.67ms
step:1592/2330 train_time:93402ms step_avg:58.67ms
step:1593/2330 train_time:93459ms step_avg:58.67ms
step:1594/2330 train_time:93520ms step_avg:58.67ms
step:1595/2330 train_time:93577ms step_avg:58.67ms
step:1596/2330 train_time:93638ms step_avg:58.67ms
step:1597/2330 train_time:93695ms step_avg:58.67ms
step:1598/2330 train_time:93756ms step_avg:58.67ms
step:1599/2330 train_time:93812ms step_avg:58.67ms
step:1600/2330 train_time:93875ms step_avg:58.67ms
step:1601/2330 train_time:93932ms step_avg:58.67ms
step:1602/2330 train_time:93992ms step_avg:58.67ms
step:1603/2330 train_time:94049ms step_avg:58.67ms
step:1604/2330 train_time:94110ms step_avg:58.67ms
step:1605/2330 train_time:94167ms step_avg:58.67ms
step:1606/2330 train_time:94229ms step_avg:58.67ms
step:1607/2330 train_time:94287ms step_avg:58.67ms
step:1608/2330 train_time:94348ms step_avg:58.67ms
step:1609/2330 train_time:94406ms step_avg:58.67ms
step:1610/2330 train_time:94467ms step_avg:58.68ms
step:1611/2330 train_time:94525ms step_avg:58.67ms
step:1612/2330 train_time:94586ms step_avg:58.68ms
step:1613/2330 train_time:94643ms step_avg:58.68ms
step:1614/2330 train_time:94704ms step_avg:58.68ms
step:1615/2330 train_time:94762ms step_avg:58.68ms
step:1616/2330 train_time:94822ms step_avg:58.68ms
step:1617/2330 train_time:94880ms step_avg:58.68ms
step:1618/2330 train_time:94940ms step_avg:58.68ms
step:1619/2330 train_time:94998ms step_avg:58.68ms
step:1620/2330 train_time:95058ms step_avg:58.68ms
step:1621/2330 train_time:95115ms step_avg:58.68ms
step:1622/2330 train_time:95177ms step_avg:58.68ms
step:1623/2330 train_time:95233ms step_avg:58.68ms
step:1624/2330 train_time:95296ms step_avg:58.68ms
step:1625/2330 train_time:95352ms step_avg:58.68ms
step:1626/2330 train_time:95414ms step_avg:58.68ms
step:1627/2330 train_time:95471ms step_avg:58.68ms
step:1628/2330 train_time:95533ms step_avg:58.68ms
step:1629/2330 train_time:95590ms step_avg:58.68ms
step:1630/2330 train_time:95652ms step_avg:58.68ms
step:1631/2330 train_time:95709ms step_avg:58.68ms
step:1632/2330 train_time:95770ms step_avg:58.68ms
step:1633/2330 train_time:95827ms step_avg:58.68ms
step:1634/2330 train_time:95889ms step_avg:58.68ms
step:1635/2330 train_time:95946ms step_avg:58.68ms
step:1636/2330 train_time:96007ms step_avg:58.68ms
step:1637/2330 train_time:96066ms step_avg:58.68ms
step:1638/2330 train_time:96126ms step_avg:58.68ms
step:1639/2330 train_time:96183ms step_avg:58.68ms
step:1640/2330 train_time:96244ms step_avg:58.69ms
step:1641/2330 train_time:96302ms step_avg:58.69ms
step:1642/2330 train_time:96363ms step_avg:58.69ms
step:1643/2330 train_time:96421ms step_avg:58.69ms
step:1644/2330 train_time:96481ms step_avg:58.69ms
step:1645/2330 train_time:96540ms step_avg:58.69ms
step:1646/2330 train_time:96601ms step_avg:58.69ms
step:1647/2330 train_time:96658ms step_avg:58.69ms
step:1648/2330 train_time:96719ms step_avg:58.69ms
step:1649/2330 train_time:96776ms step_avg:58.69ms
step:1650/2330 train_time:96837ms step_avg:58.69ms
step:1651/2330 train_time:96894ms step_avg:58.69ms
step:1652/2330 train_time:96955ms step_avg:58.69ms
step:1653/2330 train_time:97012ms step_avg:58.69ms
step:1654/2330 train_time:97074ms step_avg:58.69ms
step:1655/2330 train_time:97131ms step_avg:58.69ms
step:1656/2330 train_time:97192ms step_avg:58.69ms
step:1657/2330 train_time:97249ms step_avg:58.69ms
step:1658/2330 train_time:97312ms step_avg:58.69ms
step:1659/2330 train_time:97369ms step_avg:58.69ms
step:1660/2330 train_time:97431ms step_avg:58.69ms
step:1661/2330 train_time:97488ms step_avg:58.69ms
step:1662/2330 train_time:97550ms step_avg:58.69ms
step:1663/2330 train_time:97607ms step_avg:58.69ms
step:1664/2330 train_time:97670ms step_avg:58.70ms
step:1665/2330 train_time:97727ms step_avg:58.69ms
step:1666/2330 train_time:97788ms step_avg:58.70ms
step:1667/2330 train_time:97846ms step_avg:58.70ms
step:1668/2330 train_time:97906ms step_avg:58.70ms
step:1669/2330 train_time:97965ms step_avg:58.70ms
step:1670/2330 train_time:98025ms step_avg:58.70ms
step:1671/2330 train_time:98082ms step_avg:58.70ms
step:1672/2330 train_time:98143ms step_avg:58.70ms
step:1673/2330 train_time:98202ms step_avg:58.70ms
step:1674/2330 train_time:98262ms step_avg:58.70ms
step:1675/2330 train_time:98319ms step_avg:58.70ms
step:1676/2330 train_time:98378ms step_avg:58.70ms
step:1677/2330 train_time:98435ms step_avg:58.70ms
step:1678/2330 train_time:98497ms step_avg:58.70ms
step:1679/2330 train_time:98553ms step_avg:58.70ms
step:1680/2330 train_time:98616ms step_avg:58.70ms
step:1681/2330 train_time:98673ms step_avg:58.70ms
step:1682/2330 train_time:98735ms step_avg:58.70ms
step:1683/2330 train_time:98791ms step_avg:58.70ms
step:1684/2330 train_time:98853ms step_avg:58.70ms
step:1685/2330 train_time:98909ms step_avg:58.70ms
step:1686/2330 train_time:98973ms step_avg:58.70ms
step:1687/2330 train_time:99029ms step_avg:58.70ms
step:1688/2330 train_time:99090ms step_avg:58.70ms
step:1689/2330 train_time:99147ms step_avg:58.70ms
step:1690/2330 train_time:99210ms step_avg:58.70ms
step:1691/2330 train_time:99266ms step_avg:58.70ms
step:1692/2330 train_time:99328ms step_avg:58.70ms
step:1693/2330 train_time:99387ms step_avg:58.70ms
step:1694/2330 train_time:99447ms step_avg:58.71ms
step:1695/2330 train_time:99507ms step_avg:58.71ms
step:1696/2330 train_time:99567ms step_avg:58.71ms
step:1697/2330 train_time:99625ms step_avg:58.71ms
step:1698/2330 train_time:99686ms step_avg:58.71ms
step:1699/2330 train_time:99744ms step_avg:58.71ms
step:1700/2330 train_time:99804ms step_avg:58.71ms
step:1701/2330 train_time:99861ms step_avg:58.71ms
step:1702/2330 train_time:99922ms step_avg:58.71ms
step:1703/2330 train_time:99981ms step_avg:58.71ms
step:1704/2330 train_time:100041ms step_avg:58.71ms
step:1705/2330 train_time:100099ms step_avg:58.71ms
step:1706/2330 train_time:100158ms step_avg:58.71ms
step:1707/2330 train_time:100215ms step_avg:58.71ms
step:1708/2330 train_time:100277ms step_avg:58.71ms
step:1709/2330 train_time:100334ms step_avg:58.71ms
step:1710/2330 train_time:100395ms step_avg:58.71ms
step:1711/2330 train_time:100452ms step_avg:58.71ms
step:1712/2330 train_time:100513ms step_avg:58.71ms
step:1713/2330 train_time:100570ms step_avg:58.71ms
step:1714/2330 train_time:100632ms step_avg:58.71ms
step:1715/2330 train_time:100690ms step_avg:58.71ms
step:1716/2330 train_time:100751ms step_avg:58.71ms
step:1717/2330 train_time:100808ms step_avg:58.71ms
step:1718/2330 train_time:100869ms step_avg:58.71ms
step:1719/2330 train_time:100926ms step_avg:58.71ms
step:1720/2330 train_time:100987ms step_avg:58.71ms
step:1721/2330 train_time:101045ms step_avg:58.71ms
step:1722/2330 train_time:101105ms step_avg:58.71ms
step:1723/2330 train_time:101162ms step_avg:58.71ms
step:1724/2330 train_time:101223ms step_avg:58.71ms
step:1725/2330 train_time:101282ms step_avg:58.71ms
step:1726/2330 train_time:101342ms step_avg:58.71ms
step:1727/2330 train_time:101400ms step_avg:58.71ms
step:1728/2330 train_time:101460ms step_avg:58.72ms
step:1729/2330 train_time:101518ms step_avg:58.72ms
step:1730/2330 train_time:101578ms step_avg:58.72ms
step:1731/2330 train_time:101635ms step_avg:58.71ms
step:1732/2330 train_time:101697ms step_avg:58.72ms
step:1733/2330 train_time:101753ms step_avg:58.72ms
step:1734/2330 train_time:101816ms step_avg:58.72ms
step:1735/2330 train_time:101873ms step_avg:58.72ms
step:1736/2330 train_time:101934ms step_avg:58.72ms
step:1737/2330 train_time:101991ms step_avg:58.72ms
step:1738/2330 train_time:102052ms step_avg:58.72ms
step:1739/2330 train_time:102108ms step_avg:58.72ms
step:1740/2330 train_time:102172ms step_avg:58.72ms
step:1741/2330 train_time:102229ms step_avg:58.72ms
step:1742/2330 train_time:102291ms step_avg:58.72ms
step:1743/2330 train_time:102348ms step_avg:58.72ms
step:1744/2330 train_time:102410ms step_avg:58.72ms
step:1745/2330 train_time:102468ms step_avg:58.72ms
step:1746/2330 train_time:102529ms step_avg:58.72ms
step:1747/2330 train_time:102588ms step_avg:58.72ms
step:1748/2330 train_time:102648ms step_avg:58.72ms
step:1749/2330 train_time:102707ms step_avg:58.72ms
step:1750/2330 train_time:102767ms step_avg:58.72ms
step:1750/2330 val_loss:3.8163 train_time:102848ms step_avg:58.77ms
step:1751/2330 train_time:102867ms step_avg:58.75ms
step:1752/2330 train_time:102887ms step_avg:58.73ms
step:1753/2330 train_time:102945ms step_avg:58.73ms
step:1754/2330 train_time:103010ms step_avg:58.73ms
step:1755/2330 train_time:103067ms step_avg:58.73ms
step:1756/2330 train_time:103130ms step_avg:58.73ms
step:1757/2330 train_time:103186ms step_avg:58.73ms
step:1758/2330 train_time:103249ms step_avg:58.73ms
step:1759/2330 train_time:103305ms step_avg:58.73ms
step:1760/2330 train_time:103366ms step_avg:58.73ms
step:1761/2330 train_time:103422ms step_avg:58.73ms
step:1762/2330 train_time:103483ms step_avg:58.73ms
step:1763/2330 train_time:103539ms step_avg:58.73ms
step:1764/2330 train_time:103601ms step_avg:58.73ms
step:1765/2330 train_time:103657ms step_avg:58.73ms
step:1766/2330 train_time:103717ms step_avg:58.73ms
step:1767/2330 train_time:103775ms step_avg:58.73ms
step:1768/2330 train_time:103839ms step_avg:58.73ms
step:1769/2330 train_time:103898ms step_avg:58.73ms
step:1770/2330 train_time:103959ms step_avg:58.73ms
step:1771/2330 train_time:104016ms step_avg:58.73ms
step:1772/2330 train_time:104078ms step_avg:58.73ms
step:1773/2330 train_time:104135ms step_avg:58.73ms
step:1774/2330 train_time:104198ms step_avg:58.74ms
step:1775/2330 train_time:104256ms step_avg:58.74ms
step:1776/2330 train_time:104316ms step_avg:58.74ms
step:1777/2330 train_time:104374ms step_avg:58.74ms
step:1778/2330 train_time:104434ms step_avg:58.74ms
step:1779/2330 train_time:104491ms step_avg:58.74ms
step:1780/2330 train_time:104551ms step_avg:58.74ms
step:1781/2330 train_time:104608ms step_avg:58.74ms
step:1782/2330 train_time:104667ms step_avg:58.74ms
step:1783/2330 train_time:104723ms step_avg:58.73ms
step:1784/2330 train_time:104785ms step_avg:58.74ms
step:1785/2330 train_time:104842ms step_avg:58.74ms
step:1786/2330 train_time:104905ms step_avg:58.74ms
step:1787/2330 train_time:104961ms step_avg:58.74ms
step:1788/2330 train_time:105024ms step_avg:58.74ms
step:1789/2330 train_time:105081ms step_avg:58.74ms
step:1790/2330 train_time:105143ms step_avg:58.74ms
step:1791/2330 train_time:105200ms step_avg:58.74ms
step:1792/2330 train_time:105262ms step_avg:58.74ms
step:1793/2330 train_time:105318ms step_avg:58.74ms
step:1794/2330 train_time:105380ms step_avg:58.74ms
step:1795/2330 train_time:105436ms step_avg:58.74ms
step:1796/2330 train_time:105498ms step_avg:58.74ms
step:1797/2330 train_time:105555ms step_avg:58.74ms
step:1798/2330 train_time:105616ms step_avg:58.74ms
step:1799/2330 train_time:105674ms step_avg:58.74ms
step:1800/2330 train_time:105735ms step_avg:58.74ms
step:1801/2330 train_time:105793ms step_avg:58.74ms
step:1802/2330 train_time:105854ms step_avg:58.74ms
step:1803/2330 train_time:105913ms step_avg:58.74ms
step:1804/2330 train_time:105973ms step_avg:58.74ms
step:1805/2330 train_time:106030ms step_avg:58.74ms
step:1806/2330 train_time:106091ms step_avg:58.74ms
step:1807/2330 train_time:106148ms step_avg:58.74ms
step:1808/2330 train_time:106209ms step_avg:58.74ms
step:1809/2330 train_time:106266ms step_avg:58.74ms
step:1810/2330 train_time:106328ms step_avg:58.74ms
step:1811/2330 train_time:106384ms step_avg:58.74ms
step:1812/2330 train_time:106446ms step_avg:58.75ms
step:1813/2330 train_time:106502ms step_avg:58.74ms
step:1814/2330 train_time:106565ms step_avg:58.75ms
step:1815/2330 train_time:106621ms step_avg:58.74ms
step:1816/2330 train_time:106683ms step_avg:58.75ms
step:1817/2330 train_time:106739ms step_avg:58.74ms
step:1818/2330 train_time:106802ms step_avg:58.75ms
step:1819/2330 train_time:106859ms step_avg:58.75ms
step:1820/2330 train_time:106920ms step_avg:58.75ms
step:1821/2330 train_time:106978ms step_avg:58.75ms
step:1822/2330 train_time:107040ms step_avg:58.75ms
step:1823/2330 train_time:107097ms step_avg:58.75ms
step:1824/2330 train_time:107159ms step_avg:58.75ms
step:1825/2330 train_time:107216ms step_avg:58.75ms
step:1826/2330 train_time:107278ms step_avg:58.75ms
step:1827/2330 train_time:107336ms step_avg:58.75ms
step:1828/2330 train_time:107396ms step_avg:58.75ms
step:1829/2330 train_time:107454ms step_avg:58.75ms
step:1830/2330 train_time:107515ms step_avg:58.75ms
step:1831/2330 train_time:107573ms step_avg:58.75ms
step:1832/2330 train_time:107632ms step_avg:58.75ms
step:1833/2330 train_time:107689ms step_avg:58.75ms
step:1834/2330 train_time:107749ms step_avg:58.75ms
step:1835/2330 train_time:107806ms step_avg:58.75ms
step:1836/2330 train_time:107867ms step_avg:58.75ms
step:1837/2330 train_time:107924ms step_avg:58.75ms
step:1838/2330 train_time:107985ms step_avg:58.75ms
step:1839/2330 train_time:108042ms step_avg:58.75ms
step:1840/2330 train_time:108104ms step_avg:58.75ms
step:1841/2330 train_time:108161ms step_avg:58.75ms
step:1842/2330 train_time:108223ms step_avg:58.75ms
step:1843/2330 train_time:108280ms step_avg:58.75ms
step:1844/2330 train_time:108342ms step_avg:58.75ms
step:1845/2330 train_time:108398ms step_avg:58.75ms
step:1846/2330 train_time:108460ms step_avg:58.75ms
step:1847/2330 train_time:108517ms step_avg:58.75ms
step:1848/2330 train_time:108579ms step_avg:58.75ms
step:1849/2330 train_time:108636ms step_avg:58.75ms
step:1850/2330 train_time:108698ms step_avg:58.76ms
step:1851/2330 train_time:108756ms step_avg:58.76ms
step:1852/2330 train_time:108816ms step_avg:58.76ms
step:1853/2330 train_time:108874ms step_avg:58.76ms
step:1854/2330 train_time:108935ms step_avg:58.76ms
step:1855/2330 train_time:108993ms step_avg:58.76ms
step:1856/2330 train_time:109053ms step_avg:58.76ms
step:1857/2330 train_time:109112ms step_avg:58.76ms
step:1858/2330 train_time:109172ms step_avg:58.76ms
step:1859/2330 train_time:109229ms step_avg:58.76ms
step:1860/2330 train_time:109290ms step_avg:58.76ms
step:1861/2330 train_time:109347ms step_avg:58.76ms
step:1862/2330 train_time:109408ms step_avg:58.76ms
step:1863/2330 train_time:109465ms step_avg:58.76ms
step:1864/2330 train_time:109527ms step_avg:58.76ms
step:1865/2330 train_time:109583ms step_avg:58.76ms
step:1866/2330 train_time:109645ms step_avg:58.76ms
step:1867/2330 train_time:109701ms step_avg:58.76ms
step:1868/2330 train_time:109764ms step_avg:58.76ms
step:1869/2330 train_time:109820ms step_avg:58.76ms
step:1870/2330 train_time:109883ms step_avg:58.76ms
step:1871/2330 train_time:109939ms step_avg:58.76ms
step:1872/2330 train_time:110000ms step_avg:58.76ms
step:1873/2330 train_time:110057ms step_avg:58.76ms
step:1874/2330 train_time:110120ms step_avg:58.76ms
step:1875/2330 train_time:110177ms step_avg:58.76ms
step:1876/2330 train_time:110239ms step_avg:58.76ms
step:1877/2330 train_time:110296ms step_avg:58.76ms
step:1878/2330 train_time:110357ms step_avg:58.76ms
step:1879/2330 train_time:110415ms step_avg:58.76ms
step:1880/2330 train_time:110476ms step_avg:58.76ms
step:1881/2330 train_time:110533ms step_avg:58.76ms
step:1882/2330 train_time:110594ms step_avg:58.76ms
step:1883/2330 train_time:110651ms step_avg:58.76ms
step:1884/2330 train_time:110711ms step_avg:58.76ms
step:1885/2330 train_time:110769ms step_avg:58.76ms
step:1886/2330 train_time:110829ms step_avg:58.76ms
step:1887/2330 train_time:110886ms step_avg:58.76ms
step:1888/2330 train_time:110947ms step_avg:58.76ms
step:1889/2330 train_time:111003ms step_avg:58.76ms
step:1890/2330 train_time:111066ms step_avg:58.76ms
step:1891/2330 train_time:111122ms step_avg:58.76ms
step:1892/2330 train_time:111185ms step_avg:58.77ms
step:1893/2330 train_time:111242ms step_avg:58.76ms
step:1894/2330 train_time:111304ms step_avg:58.77ms
step:1895/2330 train_time:111360ms step_avg:58.77ms
step:1896/2330 train_time:111422ms step_avg:58.77ms
step:1897/2330 train_time:111478ms step_avg:58.77ms
step:1898/2330 train_time:111542ms step_avg:58.77ms
step:1899/2330 train_time:111598ms step_avg:58.77ms
step:1900/2330 train_time:111660ms step_avg:58.77ms
step:1901/2330 train_time:111717ms step_avg:58.77ms
step:1902/2330 train_time:111780ms step_avg:58.77ms
step:1903/2330 train_time:111837ms step_avg:58.77ms
step:1904/2330 train_time:111898ms step_avg:58.77ms
step:1905/2330 train_time:111956ms step_avg:58.77ms
step:1906/2330 train_time:112017ms step_avg:58.77ms
step:1907/2330 train_time:112075ms step_avg:58.77ms
step:1908/2330 train_time:112135ms step_avg:58.77ms
step:1909/2330 train_time:112192ms step_avg:58.77ms
step:1910/2330 train_time:112253ms step_avg:58.77ms
step:1911/2330 train_time:112310ms step_avg:58.77ms
step:1912/2330 train_time:112371ms step_avg:58.77ms
step:1913/2330 train_time:112428ms step_avg:58.77ms
step:1914/2330 train_time:112489ms step_avg:58.77ms
step:1915/2330 train_time:112547ms step_avg:58.77ms
step:1916/2330 train_time:112607ms step_avg:58.77ms
step:1917/2330 train_time:112664ms step_avg:58.77ms
step:1918/2330 train_time:112725ms step_avg:58.77ms
step:1919/2330 train_time:112782ms step_avg:58.77ms
step:1920/2330 train_time:112844ms step_avg:58.77ms
step:1921/2330 train_time:112900ms step_avg:58.77ms
step:1922/2330 train_time:112962ms step_avg:58.77ms
step:1923/2330 train_time:113018ms step_avg:58.77ms
step:1924/2330 train_time:113082ms step_avg:58.77ms
step:1925/2330 train_time:113138ms step_avg:58.77ms
step:1926/2330 train_time:113200ms step_avg:58.77ms
step:1927/2330 train_time:113257ms step_avg:58.77ms
step:1928/2330 train_time:113320ms step_avg:58.78ms
step:1929/2330 train_time:113377ms step_avg:58.77ms
step:1930/2330 train_time:113440ms step_avg:58.78ms
step:1931/2330 train_time:113497ms step_avg:58.78ms
step:1932/2330 train_time:113558ms step_avg:58.78ms
step:1933/2330 train_time:113616ms step_avg:58.78ms
step:1934/2330 train_time:113677ms step_avg:58.78ms
step:1935/2330 train_time:113734ms step_avg:58.78ms
step:1936/2330 train_time:113795ms step_avg:58.78ms
step:1937/2330 train_time:113853ms step_avg:58.78ms
step:1938/2330 train_time:113913ms step_avg:58.78ms
step:1939/2330 train_time:113971ms step_avg:58.78ms
step:1940/2330 train_time:114032ms step_avg:58.78ms
step:1941/2330 train_time:114089ms step_avg:58.78ms
step:1942/2330 train_time:114149ms step_avg:58.78ms
step:1943/2330 train_time:114206ms step_avg:58.78ms
step:1944/2330 train_time:114267ms step_avg:58.78ms
step:1945/2330 train_time:114323ms step_avg:58.78ms
step:1946/2330 train_time:114386ms step_avg:58.78ms
step:1947/2330 train_time:114442ms step_avg:58.78ms
step:1948/2330 train_time:114504ms step_avg:58.78ms
step:1949/2330 train_time:114561ms step_avg:58.78ms
step:1950/2330 train_time:114622ms step_avg:58.78ms
step:1951/2330 train_time:114679ms step_avg:58.78ms
step:1952/2330 train_time:114740ms step_avg:58.78ms
step:1953/2330 train_time:114797ms step_avg:58.78ms
step:1954/2330 train_time:114858ms step_avg:58.78ms
step:1955/2330 train_time:114915ms step_avg:58.78ms
step:1956/2330 train_time:114978ms step_avg:58.78ms
step:1957/2330 train_time:115035ms step_avg:58.78ms
step:1958/2330 train_time:115096ms step_avg:58.78ms
step:1959/2330 train_time:115154ms step_avg:58.78ms
step:1960/2330 train_time:115214ms step_avg:58.78ms
step:1961/2330 train_time:115272ms step_avg:58.78ms
step:1962/2330 train_time:115332ms step_avg:58.78ms
step:1963/2330 train_time:115391ms step_avg:58.78ms
step:1964/2330 train_time:115452ms step_avg:58.78ms
step:1965/2330 train_time:115510ms step_avg:58.78ms
step:1966/2330 train_time:115570ms step_avg:58.78ms
step:1967/2330 train_time:115627ms step_avg:58.78ms
step:1968/2330 train_time:115688ms step_avg:58.78ms
step:1969/2330 train_time:115746ms step_avg:58.78ms
step:1970/2330 train_time:115808ms step_avg:58.79ms
step:1971/2330 train_time:115864ms step_avg:58.78ms
step:1972/2330 train_time:115926ms step_avg:58.79ms
step:1973/2330 train_time:115982ms step_avg:58.78ms
step:1974/2330 train_time:116045ms step_avg:58.79ms
step:1975/2330 train_time:116101ms step_avg:58.79ms
step:1976/2330 train_time:116163ms step_avg:58.79ms
step:1977/2330 train_time:116220ms step_avg:58.79ms
step:1978/2330 train_time:116282ms step_avg:58.79ms
step:1979/2330 train_time:116338ms step_avg:58.79ms
step:1980/2330 train_time:116401ms step_avg:58.79ms
step:1981/2330 train_time:116458ms step_avg:58.79ms
step:1982/2330 train_time:116519ms step_avg:58.79ms
step:1983/2330 train_time:116577ms step_avg:58.79ms
step:1984/2330 train_time:116637ms step_avg:58.79ms
step:1985/2330 train_time:116695ms step_avg:58.79ms
step:1986/2330 train_time:116757ms step_avg:58.79ms
step:1987/2330 train_time:116815ms step_avg:58.79ms
step:1988/2330 train_time:116875ms step_avg:58.79ms
step:1989/2330 train_time:116932ms step_avg:58.79ms
step:1990/2330 train_time:116992ms step_avg:58.79ms
step:1991/2330 train_time:117049ms step_avg:58.79ms
step:1992/2330 train_time:117109ms step_avg:58.79ms
step:1993/2330 train_time:117166ms step_avg:58.79ms
step:1994/2330 train_time:117227ms step_avg:58.79ms
step:1995/2330 train_time:117284ms step_avg:58.79ms
step:1996/2330 train_time:117346ms step_avg:58.79ms
step:1997/2330 train_time:117402ms step_avg:58.79ms
step:1998/2330 train_time:117465ms step_avg:58.79ms
step:1999/2330 train_time:117521ms step_avg:58.79ms
step:2000/2330 train_time:117584ms step_avg:58.79ms
step:2000/2330 val_loss:3.7539 train_time:117668ms step_avg:58.83ms
step:2001/2330 train_time:117686ms step_avg:58.81ms
step:2002/2330 train_time:117707ms step_avg:58.79ms
step:2003/2330 train_time:117767ms step_avg:58.80ms
step:2004/2330 train_time:117834ms step_avg:58.80ms
step:2005/2330 train_time:117892ms step_avg:58.80ms
step:2006/2330 train_time:117952ms step_avg:58.80ms
step:2007/2330 train_time:118009ms step_avg:58.80ms
step:2008/2330 train_time:118070ms step_avg:58.80ms
step:2009/2330 train_time:118126ms step_avg:58.80ms
step:2010/2330 train_time:118188ms step_avg:58.80ms
step:2011/2330 train_time:118245ms step_avg:58.80ms
step:2012/2330 train_time:118304ms step_avg:58.80ms
step:2013/2330 train_time:118361ms step_avg:58.80ms
step:2014/2330 train_time:118421ms step_avg:58.80ms
step:2015/2330 train_time:118477ms step_avg:58.80ms
step:2016/2330 train_time:118537ms step_avg:58.80ms
step:2017/2330 train_time:118594ms step_avg:58.80ms
step:2018/2330 train_time:118656ms step_avg:58.80ms
step:2019/2330 train_time:118715ms step_avg:58.80ms
step:2020/2330 train_time:118778ms step_avg:58.80ms
step:2021/2330 train_time:118837ms step_avg:58.80ms
step:2022/2330 train_time:118899ms step_avg:58.80ms
step:2023/2330 train_time:118957ms step_avg:58.80ms
step:2024/2330 train_time:119020ms step_avg:58.80ms
step:2025/2330 train_time:119077ms step_avg:58.80ms
step:2026/2330 train_time:119138ms step_avg:58.80ms
step:2027/2330 train_time:119195ms step_avg:58.80ms
step:2028/2330 train_time:119255ms step_avg:58.80ms
step:2029/2330 train_time:119313ms step_avg:58.80ms
step:2030/2330 train_time:119372ms step_avg:58.80ms
step:2031/2330 train_time:119430ms step_avg:58.80ms
step:2032/2330 train_time:119489ms step_avg:58.80ms
step:2033/2330 train_time:119546ms step_avg:58.80ms
step:2034/2330 train_time:119606ms step_avg:58.80ms
step:2035/2330 train_time:119664ms step_avg:58.80ms
step:2036/2330 train_time:119726ms step_avg:58.80ms
step:2037/2330 train_time:119784ms step_avg:58.80ms
step:2038/2330 train_time:119847ms step_avg:58.81ms
step:2039/2330 train_time:119904ms step_avg:58.81ms
step:2040/2330 train_time:119966ms step_avg:58.81ms
step:2041/2330 train_time:120024ms step_avg:58.81ms
step:2042/2330 train_time:120086ms step_avg:58.81ms
step:2043/2330 train_time:120142ms step_avg:58.81ms
step:2044/2330 train_time:120204ms step_avg:58.81ms
step:2045/2330 train_time:120260ms step_avg:58.81ms
step:2046/2330 train_time:120322ms step_avg:58.81ms
step:2047/2330 train_time:120378ms step_avg:58.81ms
step:2048/2330 train_time:120440ms step_avg:58.81ms
step:2049/2330 train_time:120496ms step_avg:58.81ms
step:2050/2330 train_time:120559ms step_avg:58.81ms
step:2051/2330 train_time:120615ms step_avg:58.81ms
step:2052/2330 train_time:120677ms step_avg:58.81ms
step:2053/2330 train_time:120735ms step_avg:58.81ms
step:2054/2330 train_time:120796ms step_avg:58.81ms
step:2055/2330 train_time:120853ms step_avg:58.81ms
step:2056/2330 train_time:120916ms step_avg:58.81ms
step:2057/2330 train_time:120973ms step_avg:58.81ms
step:2058/2330 train_time:121034ms step_avg:58.81ms
step:2059/2330 train_time:121092ms step_avg:58.81ms
step:2060/2330 train_time:121153ms step_avg:58.81ms
step:2061/2330 train_time:121211ms step_avg:58.81ms
step:2062/2330 train_time:121271ms step_avg:58.81ms
step:2063/2330 train_time:121328ms step_avg:58.81ms
step:2064/2330 train_time:121388ms step_avg:58.81ms
step:2065/2330 train_time:121445ms step_avg:58.81ms
step:2066/2330 train_time:121506ms step_avg:58.81ms
step:2067/2330 train_time:121563ms step_avg:58.81ms
step:2068/2330 train_time:121624ms step_avg:58.81ms
step:2069/2330 train_time:121681ms step_avg:58.81ms
step:2070/2330 train_time:121743ms step_avg:58.81ms
step:2071/2330 train_time:121800ms step_avg:58.81ms
step:2072/2330 train_time:121863ms step_avg:58.81ms
step:2073/2330 train_time:121920ms step_avg:58.81ms
step:2074/2330 train_time:121982ms step_avg:58.81ms
step:2075/2330 train_time:122038ms step_avg:58.81ms
step:2076/2330 train_time:122101ms step_avg:58.82ms
step:2077/2330 train_time:122159ms step_avg:58.81ms
step:2078/2330 train_time:122220ms step_avg:58.82ms
step:2079/2330 train_time:122277ms step_avg:58.82ms
step:2080/2330 train_time:122338ms step_avg:58.82ms
step:2081/2330 train_time:122396ms step_avg:58.82ms
step:2082/2330 train_time:122456ms step_avg:58.82ms
step:2083/2330 train_time:122514ms step_avg:58.82ms
step:2084/2330 train_time:122574ms step_avg:58.82ms
step:2085/2330 train_time:122631ms step_avg:58.82ms
step:2086/2330 train_time:122692ms step_avg:58.82ms
step:2087/2330 train_time:122749ms step_avg:58.82ms
step:2088/2330 train_time:122810ms step_avg:58.82ms
step:2089/2330 train_time:122867ms step_avg:58.82ms
step:2090/2330 train_time:122928ms step_avg:58.82ms
step:2091/2330 train_time:122986ms step_avg:58.82ms
step:2092/2330 train_time:123048ms step_avg:58.82ms
step:2093/2330 train_time:123105ms step_avg:58.82ms
step:2094/2330 train_time:123166ms step_avg:58.82ms
step:2095/2330 train_time:123222ms step_avg:58.82ms
step:2096/2330 train_time:123284ms step_avg:58.82ms
step:2097/2330 train_time:123341ms step_avg:58.82ms
step:2098/2330 train_time:123402ms step_avg:58.82ms
step:2099/2330 train_time:123458ms step_avg:58.82ms
step:2100/2330 train_time:123520ms step_avg:58.82ms
step:2101/2330 train_time:123577ms step_avg:58.82ms
step:2102/2330 train_time:123639ms step_avg:58.82ms
step:2103/2330 train_time:123696ms step_avg:58.82ms
step:2104/2330 train_time:123758ms step_avg:58.82ms
step:2105/2330 train_time:123816ms step_avg:58.82ms
step:2106/2330 train_time:123877ms step_avg:58.82ms
step:2107/2330 train_time:123934ms step_avg:58.82ms
step:2108/2330 train_time:123996ms step_avg:58.82ms
step:2109/2330 train_time:124054ms step_avg:58.82ms
step:2110/2330 train_time:124115ms step_avg:58.82ms
step:2111/2330 train_time:124172ms step_avg:58.82ms
step:2112/2330 train_time:124232ms step_avg:58.82ms
step:2113/2330 train_time:124290ms step_avg:58.82ms
step:2114/2330 train_time:124351ms step_avg:58.82ms
step:2115/2330 train_time:124409ms step_avg:58.82ms
step:2116/2330 train_time:124469ms step_avg:58.82ms
step:2117/2330 train_time:124527ms step_avg:58.82ms
step:2118/2330 train_time:124586ms step_avg:58.82ms
step:2119/2330 train_time:124643ms step_avg:58.82ms
step:2120/2330 train_time:124704ms step_avg:58.82ms
step:2121/2330 train_time:124762ms step_avg:58.82ms
step:2122/2330 train_time:124823ms step_avg:58.82ms
step:2123/2330 train_time:124880ms step_avg:58.82ms
step:2124/2330 train_time:124941ms step_avg:58.82ms
step:2125/2330 train_time:124998ms step_avg:58.82ms
step:2126/2330 train_time:125060ms step_avg:58.82ms
step:2127/2330 train_time:125117ms step_avg:58.82ms
step:2128/2330 train_time:125180ms step_avg:58.82ms
step:2129/2330 train_time:125236ms step_avg:58.82ms
step:2130/2330 train_time:125298ms step_avg:58.83ms
step:2131/2330 train_time:125355ms step_avg:58.82ms
step:2132/2330 train_time:125417ms step_avg:58.83ms
step:2133/2330 train_time:125474ms step_avg:58.83ms
step:2134/2330 train_time:125535ms step_avg:58.83ms
step:2135/2330 train_time:125593ms step_avg:58.83ms
step:2136/2330 train_time:125654ms step_avg:58.83ms
step:2137/2330 train_time:125711ms step_avg:58.83ms
step:2138/2330 train_time:125771ms step_avg:58.83ms
step:2139/2330 train_time:125829ms step_avg:58.83ms
step:2140/2330 train_time:125889ms step_avg:58.83ms
step:2141/2330 train_time:125947ms step_avg:58.83ms
step:2142/2330 train_time:126008ms step_avg:58.83ms
step:2143/2330 train_time:126065ms step_avg:58.83ms
step:2144/2330 train_time:126126ms step_avg:58.83ms
step:2145/2330 train_time:126183ms step_avg:58.83ms
step:2146/2330 train_time:126245ms step_avg:58.83ms
step:2147/2330 train_time:126301ms step_avg:58.83ms
step:2148/2330 train_time:126363ms step_avg:58.83ms
step:2149/2330 train_time:126420ms step_avg:58.83ms
step:2150/2330 train_time:126481ms step_avg:58.83ms
step:2151/2330 train_time:126537ms step_avg:58.83ms
step:2152/2330 train_time:126600ms step_avg:58.83ms
step:2153/2330 train_time:126657ms step_avg:58.83ms
step:2154/2330 train_time:126718ms step_avg:58.83ms
step:2155/2330 train_time:126775ms step_avg:58.83ms
step:2156/2330 train_time:126838ms step_avg:58.83ms
step:2157/2330 train_time:126896ms step_avg:58.83ms
step:2158/2330 train_time:126957ms step_avg:58.83ms
step:2159/2330 train_time:127015ms step_avg:58.83ms
step:2160/2330 train_time:127076ms step_avg:58.83ms
step:2161/2330 train_time:127133ms step_avg:58.83ms
step:2162/2330 train_time:127193ms step_avg:58.83ms
step:2163/2330 train_time:127251ms step_avg:58.83ms
step:2164/2330 train_time:127311ms step_avg:58.83ms
step:2165/2330 train_time:127370ms step_avg:58.83ms
step:2166/2330 train_time:127430ms step_avg:58.83ms
step:2167/2330 train_time:127489ms step_avg:58.83ms
step:2168/2330 train_time:127549ms step_avg:58.83ms
step:2169/2330 train_time:127606ms step_avg:58.83ms
step:2170/2330 train_time:127667ms step_avg:58.83ms
step:2171/2330 train_time:127725ms step_avg:58.83ms
step:2172/2330 train_time:127785ms step_avg:58.83ms
step:2173/2330 train_time:127842ms step_avg:58.83ms
step:2174/2330 train_time:127906ms step_avg:58.83ms
step:2175/2330 train_time:127962ms step_avg:58.83ms
step:2176/2330 train_time:128024ms step_avg:58.83ms
step:2177/2330 train_time:128081ms step_avg:58.83ms
step:2178/2330 train_time:128141ms step_avg:58.83ms
step:2179/2330 train_time:128198ms step_avg:58.83ms
step:2180/2330 train_time:128261ms step_avg:58.84ms
step:2181/2330 train_time:128318ms step_avg:58.83ms
step:2182/2330 train_time:128380ms step_avg:58.84ms
step:2183/2330 train_time:128437ms step_avg:58.83ms
step:2184/2330 train_time:128499ms step_avg:58.84ms
step:2185/2330 train_time:128556ms step_avg:58.84ms
step:2186/2330 train_time:128618ms step_avg:58.84ms
step:2187/2330 train_time:128675ms step_avg:58.84ms
step:2188/2330 train_time:128738ms step_avg:58.84ms
step:2189/2330 train_time:128796ms step_avg:58.84ms
step:2190/2330 train_time:128856ms step_avg:58.84ms
step:2191/2330 train_time:128913ms step_avg:58.84ms
step:2192/2330 train_time:128975ms step_avg:58.84ms
step:2193/2330 train_time:129032ms step_avg:58.84ms
step:2194/2330 train_time:129092ms step_avg:58.84ms
step:2195/2330 train_time:129149ms step_avg:58.84ms
step:2196/2330 train_time:129209ms step_avg:58.84ms
step:2197/2330 train_time:129266ms step_avg:58.84ms
step:2198/2330 train_time:129327ms step_avg:58.84ms
step:2199/2330 train_time:129384ms step_avg:58.84ms
step:2200/2330 train_time:129446ms step_avg:58.84ms
step:2201/2330 train_time:129502ms step_avg:58.84ms
step:2202/2330 train_time:129565ms step_avg:58.84ms
step:2203/2330 train_time:129622ms step_avg:58.84ms
step:2204/2330 train_time:129684ms step_avg:58.84ms
step:2205/2330 train_time:129740ms step_avg:58.84ms
step:2206/2330 train_time:129801ms step_avg:58.84ms
step:2207/2330 train_time:129858ms step_avg:58.84ms
step:2208/2330 train_time:129922ms step_avg:58.84ms
step:2209/2330 train_time:129978ms step_avg:58.84ms
step:2210/2330 train_time:130041ms step_avg:58.84ms
step:2211/2330 train_time:130097ms step_avg:58.84ms
step:2212/2330 train_time:130160ms step_avg:58.84ms
step:2213/2330 train_time:130217ms step_avg:58.84ms
step:2214/2330 train_time:130279ms step_avg:58.84ms
step:2215/2330 train_time:130337ms step_avg:58.84ms
step:2216/2330 train_time:130399ms step_avg:58.84ms
step:2217/2330 train_time:130456ms step_avg:58.84ms
step:2218/2330 train_time:130519ms step_avg:58.85ms
step:2219/2330 train_time:130576ms step_avg:58.84ms
step:2220/2330 train_time:130637ms step_avg:58.85ms
step:2221/2330 train_time:130694ms step_avg:58.84ms
step:2222/2330 train_time:130755ms step_avg:58.85ms
step:2223/2330 train_time:130813ms step_avg:58.85ms
step:2224/2330 train_time:130873ms step_avg:58.85ms
step:2225/2330 train_time:130931ms step_avg:58.85ms
step:2226/2330 train_time:130991ms step_avg:58.85ms
step:2227/2330 train_time:131048ms step_avg:58.85ms
step:2228/2330 train_time:131109ms step_avg:58.85ms
step:2229/2330 train_time:131166ms step_avg:58.85ms
step:2230/2330 train_time:131227ms step_avg:58.85ms
step:2231/2330 train_time:131284ms step_avg:58.85ms
step:2232/2330 train_time:131345ms step_avg:58.85ms
step:2233/2330 train_time:131402ms step_avg:58.85ms
step:2234/2330 train_time:131464ms step_avg:58.85ms
step:2235/2330 train_time:131522ms step_avg:58.85ms
step:2236/2330 train_time:131583ms step_avg:58.85ms
step:2237/2330 train_time:131640ms step_avg:58.85ms
step:2238/2330 train_time:131702ms step_avg:58.85ms
step:2239/2330 train_time:131758ms step_avg:58.85ms
step:2240/2330 train_time:131821ms step_avg:58.85ms
step:2241/2330 train_time:131877ms step_avg:58.85ms
step:2242/2330 train_time:131939ms step_avg:58.85ms
step:2243/2330 train_time:131996ms step_avg:58.85ms
step:2244/2330 train_time:132059ms step_avg:58.85ms
step:2245/2330 train_time:132116ms step_avg:58.85ms
step:2246/2330 train_time:132178ms step_avg:58.85ms
step:2247/2330 train_time:132235ms step_avg:58.85ms
step:2248/2330 train_time:132297ms step_avg:58.85ms
step:2249/2330 train_time:132355ms step_avg:58.85ms
step:2250/2330 train_time:132415ms step_avg:58.85ms
step:2250/2330 val_loss:3.7063 train_time:132496ms step_avg:58.89ms
step:2251/2330 train_time:132516ms step_avg:58.87ms
step:2252/2330 train_time:132536ms step_avg:58.85ms
step:2253/2330 train_time:132593ms step_avg:58.85ms
step:2254/2330 train_time:132657ms step_avg:58.85ms
step:2255/2330 train_time:132713ms step_avg:58.85ms
step:2256/2330 train_time:132778ms step_avg:58.86ms
step:2257/2330 train_time:132834ms step_avg:58.85ms
step:2258/2330 train_time:132896ms step_avg:58.86ms
step:2259/2330 train_time:132953ms step_avg:58.85ms
step:2260/2330 train_time:133013ms step_avg:58.86ms
step:2261/2330 train_time:133070ms step_avg:58.85ms
step:2262/2330 train_time:133130ms step_avg:58.85ms
step:2263/2330 train_time:133186ms step_avg:58.85ms
step:2264/2330 train_time:133247ms step_avg:58.85ms
step:2265/2330 train_time:133304ms step_avg:58.85ms
step:2266/2330 train_time:133363ms step_avg:58.85ms
step:2267/2330 train_time:133421ms step_avg:58.85ms
step:2268/2330 train_time:133483ms step_avg:58.86ms
step:2269/2330 train_time:133542ms step_avg:58.86ms
step:2270/2330 train_time:133605ms step_avg:58.86ms
step:2271/2330 train_time:133662ms step_avg:58.86ms
step:2272/2330 train_time:133728ms step_avg:58.86ms
step:2273/2330 train_time:133783ms step_avg:58.86ms
step:2274/2330 train_time:133846ms step_avg:58.86ms
step:2275/2330 train_time:133903ms step_avg:58.86ms
step:2276/2330 train_time:133964ms step_avg:58.86ms
step:2277/2330 train_time:134020ms step_avg:58.86ms
step:2278/2330 train_time:134082ms step_avg:58.86ms
step:2279/2330 train_time:134138ms step_avg:58.86ms
step:2280/2330 train_time:134199ms step_avg:58.86ms
step:2281/2330 train_time:134256ms step_avg:58.86ms
step:2282/2330 train_time:134317ms step_avg:58.86ms
step:2283/2330 train_time:134373ms step_avg:58.86ms
step:2284/2330 train_time:134435ms step_avg:58.86ms
step:2285/2330 train_time:134493ms step_avg:58.86ms
step:2286/2330 train_time:134554ms step_avg:58.86ms
step:2287/2330 train_time:134613ms step_avg:58.86ms
step:2288/2330 train_time:134674ms step_avg:58.86ms
step:2289/2330 train_time:134733ms step_avg:58.86ms
step:2290/2330 train_time:134794ms step_avg:58.86ms
step:2291/2330 train_time:134852ms step_avg:58.86ms
step:2292/2330 train_time:134912ms step_avg:58.86ms
step:2293/2330 train_time:134969ms step_avg:58.86ms
step:2294/2330 train_time:135030ms step_avg:58.86ms
step:2295/2330 train_time:135088ms step_avg:58.86ms
step:2296/2330 train_time:135148ms step_avg:58.86ms
step:2297/2330 train_time:135204ms step_avg:58.86ms
step:2298/2330 train_time:135265ms step_avg:58.86ms
step:2299/2330 train_time:135322ms step_avg:58.86ms
step:2300/2330 train_time:135383ms step_avg:58.86ms
step:2301/2330 train_time:135440ms step_avg:58.86ms
step:2302/2330 train_time:135502ms step_avg:58.86ms
step:2303/2330 train_time:135559ms step_avg:58.86ms
step:2304/2330 train_time:135623ms step_avg:58.86ms
step:2305/2330 train_time:135680ms step_avg:58.86ms
step:2306/2330 train_time:135745ms step_avg:58.87ms
step:2307/2330 train_time:135801ms step_avg:58.86ms
step:2308/2330 train_time:135863ms step_avg:58.87ms
step:2309/2330 train_time:135920ms step_avg:58.87ms
step:2310/2330 train_time:135982ms step_avg:58.87ms
step:2311/2330 train_time:136039ms step_avg:58.87ms
step:2312/2330 train_time:136100ms step_avg:58.87ms
step:2313/2330 train_time:136157ms step_avg:58.87ms
step:2314/2330 train_time:136218ms step_avg:58.87ms
step:2315/2330 train_time:136274ms step_avg:58.87ms
step:2316/2330 train_time:136336ms step_avg:58.87ms
step:2317/2330 train_time:136394ms step_avg:58.87ms
step:2318/2330 train_time:136453ms step_avg:58.87ms
step:2319/2330 train_time:136511ms step_avg:58.87ms
step:2320/2330 train_time:136572ms step_avg:58.87ms
step:2321/2330 train_time:136631ms step_avg:58.87ms
step:2322/2330 train_time:136692ms step_avg:58.87ms
step:2323/2330 train_time:136751ms step_avg:58.87ms
step:2324/2330 train_time:136811ms step_avg:58.87ms
step:2325/2330 train_time:136869ms step_avg:58.87ms
step:2326/2330 train_time:136929ms step_avg:58.87ms
step:2327/2330 train_time:136986ms step_avg:58.87ms
step:2328/2330 train_time:137047ms step_avg:58.87ms
step:2329/2330 train_time:137104ms step_avg:58.87ms
step:2330/2330 train_time:137165ms step_avg:58.87ms
step:2330/2330 val_loss:3.6912 train_time:137248ms step_avg:58.90ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
