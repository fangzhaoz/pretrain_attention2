import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--beta1', type=str, required=True)
parser.add_argument('--beta2', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_beta_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=6e-4,  betas=(float(args2.beta1), float(args2.beta2)), eps=1e-10, weight_decay=0.0, fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt", exist_ok=True)
            torch.save(log, f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 03:58:44 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   36C    P0             117W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   35C    P0             118W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             118W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   28C    P0             115W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   34C    P0             118W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             114W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.05ms
step:1/2330 train_time:82ms step_avg:82.31ms
step:2/2330 train_time:181ms step_avg:90.42ms
step:3/2330 train_time:200ms step_avg:66.74ms
step:4/2330 train_time:220ms step_avg:54.97ms
step:5/2330 train_time:272ms step_avg:54.30ms
step:6/2330 train_time:329ms step_avg:54.87ms
step:7/2330 train_time:384ms step_avg:54.84ms
step:8/2330 train_time:442ms step_avg:55.30ms
step:9/2330 train_time:498ms step_avg:55.28ms
step:10/2330 train_time:556ms step_avg:55.61ms
step:11/2330 train_time:612ms step_avg:55.62ms
step:12/2330 train_time:670ms step_avg:55.86ms
step:13/2330 train_time:726ms step_avg:55.81ms
step:14/2330 train_time:784ms step_avg:55.97ms
step:15/2330 train_time:839ms step_avg:55.92ms
step:16/2330 train_time:898ms step_avg:56.11ms
step:17/2330 train_time:953ms step_avg:56.06ms
step:18/2330 train_time:1012ms step_avg:56.20ms
step:19/2330 train_time:1068ms step_avg:56.20ms
step:20/2330 train_time:1132ms step_avg:56.59ms
step:21/2330 train_time:1188ms step_avg:56.59ms
step:22/2330 train_time:1249ms step_avg:56.76ms
step:23/2330 train_time:1304ms step_avg:56.71ms
step:24/2330 train_time:1363ms step_avg:56.80ms
step:25/2330 train_time:1419ms step_avg:56.77ms
step:26/2330 train_time:1478ms step_avg:56.84ms
step:27/2330 train_time:1533ms step_avg:56.76ms
step:28/2330 train_time:1592ms step_avg:56.85ms
step:29/2330 train_time:1647ms step_avg:56.78ms
step:30/2330 train_time:1705ms step_avg:56.83ms
step:31/2330 train_time:1761ms step_avg:56.81ms
step:32/2330 train_time:1819ms step_avg:56.85ms
step:33/2330 train_time:1874ms step_avg:56.80ms
step:34/2330 train_time:1932ms step_avg:56.83ms
step:35/2330 train_time:1987ms step_avg:56.77ms
step:36/2330 train_time:2048ms step_avg:56.89ms
step:37/2330 train_time:2105ms step_avg:56.89ms
step:38/2330 train_time:2165ms step_avg:56.97ms
step:39/2330 train_time:2222ms step_avg:56.97ms
step:40/2330 train_time:2280ms step_avg:57.01ms
step:41/2330 train_time:2337ms step_avg:56.99ms
step:42/2330 train_time:2395ms step_avg:57.03ms
step:43/2330 train_time:2451ms step_avg:57.00ms
step:44/2330 train_time:2510ms step_avg:57.04ms
step:45/2330 train_time:2565ms step_avg:57.00ms
step:46/2330 train_time:2623ms step_avg:57.03ms
step:47/2330 train_time:2678ms step_avg:56.99ms
step:48/2330 train_time:2736ms step_avg:57.01ms
step:49/2330 train_time:2791ms step_avg:56.97ms
step:50/2330 train_time:2850ms step_avg:57.01ms
step:51/2330 train_time:2906ms step_avg:56.97ms
step:52/2330 train_time:2965ms step_avg:57.01ms
step:53/2330 train_time:3020ms step_avg:56.98ms
step:54/2330 train_time:3080ms step_avg:57.03ms
step:55/2330 train_time:3136ms step_avg:57.01ms
step:56/2330 train_time:3195ms step_avg:57.05ms
step:57/2330 train_time:3251ms step_avg:57.03ms
step:58/2330 train_time:3311ms step_avg:57.08ms
step:59/2330 train_time:3367ms step_avg:57.07ms
step:60/2330 train_time:3426ms step_avg:57.10ms
step:61/2330 train_time:3482ms step_avg:57.08ms
step:62/2330 train_time:3540ms step_avg:57.10ms
step:63/2330 train_time:3596ms step_avg:57.08ms
step:64/2330 train_time:3654ms step_avg:57.09ms
step:65/2330 train_time:3709ms step_avg:57.07ms
step:66/2330 train_time:3768ms step_avg:57.09ms
step:67/2330 train_time:3823ms step_avg:57.07ms
step:68/2330 train_time:3881ms step_avg:57.08ms
step:69/2330 train_time:3937ms step_avg:57.06ms
step:70/2330 train_time:3996ms step_avg:57.08ms
step:71/2330 train_time:4051ms step_avg:57.06ms
step:72/2330 train_time:4111ms step_avg:57.10ms
step:73/2330 train_time:4167ms step_avg:57.08ms
step:74/2330 train_time:4226ms step_avg:57.11ms
step:75/2330 train_time:4283ms step_avg:57.11ms
step:76/2330 train_time:4341ms step_avg:57.12ms
step:77/2330 train_time:4397ms step_avg:57.10ms
step:78/2330 train_time:4456ms step_avg:57.13ms
step:79/2330 train_time:4511ms step_avg:57.11ms
step:80/2330 train_time:4571ms step_avg:57.14ms
step:81/2330 train_time:4626ms step_avg:57.11ms
step:82/2330 train_time:4685ms step_avg:57.14ms
step:83/2330 train_time:4740ms step_avg:57.11ms
step:84/2330 train_time:4799ms step_avg:57.13ms
step:85/2330 train_time:4854ms step_avg:57.11ms
step:86/2330 train_time:4913ms step_avg:57.13ms
step:87/2330 train_time:4969ms step_avg:57.11ms
step:88/2330 train_time:5028ms step_avg:57.13ms
step:89/2330 train_time:5084ms step_avg:57.12ms
step:90/2330 train_time:5142ms step_avg:57.14ms
step:91/2330 train_time:5198ms step_avg:57.12ms
step:92/2330 train_time:5257ms step_avg:57.14ms
step:93/2330 train_time:5313ms step_avg:57.13ms
step:94/2330 train_time:5372ms step_avg:57.15ms
step:95/2330 train_time:5428ms step_avg:57.13ms
step:96/2330 train_time:5487ms step_avg:57.15ms
step:97/2330 train_time:5543ms step_avg:57.14ms
step:98/2330 train_time:5601ms step_avg:57.15ms
step:99/2330 train_time:5657ms step_avg:57.14ms
step:100/2330 train_time:5715ms step_avg:57.15ms
step:101/2330 train_time:5771ms step_avg:57.14ms
step:102/2330 train_time:5829ms step_avg:57.15ms
step:103/2330 train_time:5885ms step_avg:57.13ms
step:104/2330 train_time:5943ms step_avg:57.15ms
step:105/2330 train_time:5999ms step_avg:57.13ms
step:106/2330 train_time:6059ms step_avg:57.16ms
step:107/2330 train_time:6114ms step_avg:57.14ms
step:108/2330 train_time:6174ms step_avg:57.16ms
step:109/2330 train_time:6229ms step_avg:57.15ms
step:110/2330 train_time:6289ms step_avg:57.18ms
step:111/2330 train_time:6345ms step_avg:57.16ms
step:112/2330 train_time:6405ms step_avg:57.19ms
step:113/2330 train_time:6462ms step_avg:57.18ms
step:114/2330 train_time:6520ms step_avg:57.19ms
step:115/2330 train_time:6576ms step_avg:57.18ms
step:116/2330 train_time:6634ms step_avg:57.19ms
step:117/2330 train_time:6690ms step_avg:57.18ms
step:118/2330 train_time:6749ms step_avg:57.19ms
step:119/2330 train_time:6805ms step_avg:57.18ms
step:120/2330 train_time:6863ms step_avg:57.19ms
step:121/2330 train_time:6918ms step_avg:57.18ms
step:122/2330 train_time:6977ms step_avg:57.19ms
step:123/2330 train_time:7033ms step_avg:57.17ms
step:124/2330 train_time:7091ms step_avg:57.19ms
step:125/2330 train_time:7146ms step_avg:57.17ms
step:126/2330 train_time:7206ms step_avg:57.19ms
step:127/2330 train_time:7262ms step_avg:57.18ms
step:128/2330 train_time:7320ms step_avg:57.19ms
step:129/2330 train_time:7376ms step_avg:57.18ms
step:130/2330 train_time:7436ms step_avg:57.20ms
step:131/2330 train_time:7491ms step_avg:57.19ms
step:132/2330 train_time:7550ms step_avg:57.19ms
step:133/2330 train_time:7605ms step_avg:57.18ms
step:134/2330 train_time:7664ms step_avg:57.19ms
step:135/2330 train_time:7719ms step_avg:57.18ms
step:136/2330 train_time:7778ms step_avg:57.19ms
step:137/2330 train_time:7833ms step_avg:57.17ms
step:138/2330 train_time:7893ms step_avg:57.19ms
step:139/2330 train_time:7948ms step_avg:57.18ms
step:140/2330 train_time:8007ms step_avg:57.19ms
step:141/2330 train_time:8063ms step_avg:57.18ms
step:142/2330 train_time:8122ms step_avg:57.19ms
step:143/2330 train_time:8177ms step_avg:57.18ms
step:144/2330 train_time:8236ms step_avg:57.20ms
step:145/2330 train_time:8292ms step_avg:57.19ms
step:146/2330 train_time:8351ms step_avg:57.20ms
step:147/2330 train_time:8406ms step_avg:57.18ms
step:148/2330 train_time:8466ms step_avg:57.20ms
step:149/2330 train_time:8522ms step_avg:57.19ms
step:150/2330 train_time:8580ms step_avg:57.20ms
step:151/2330 train_time:8636ms step_avg:57.19ms
step:152/2330 train_time:8694ms step_avg:57.20ms
step:153/2330 train_time:8750ms step_avg:57.19ms
step:154/2330 train_time:8810ms step_avg:57.20ms
step:155/2330 train_time:8865ms step_avg:57.19ms
step:156/2330 train_time:8924ms step_avg:57.20ms
step:157/2330 train_time:8979ms step_avg:57.19ms
step:158/2330 train_time:9038ms step_avg:57.20ms
step:159/2330 train_time:9093ms step_avg:57.19ms
step:160/2330 train_time:9153ms step_avg:57.21ms
step:161/2330 train_time:9209ms step_avg:57.20ms
step:162/2330 train_time:9268ms step_avg:57.21ms
step:163/2330 train_time:9324ms step_avg:57.20ms
step:164/2330 train_time:9383ms step_avg:57.21ms
step:165/2330 train_time:9438ms step_avg:57.20ms
step:166/2330 train_time:9497ms step_avg:57.21ms
step:167/2330 train_time:9552ms step_avg:57.20ms
step:168/2330 train_time:9612ms step_avg:57.21ms
step:169/2330 train_time:9667ms step_avg:57.20ms
step:170/2330 train_time:9727ms step_avg:57.21ms
step:171/2330 train_time:9782ms step_avg:57.20ms
step:172/2330 train_time:9840ms step_avg:57.21ms
step:173/2330 train_time:9895ms step_avg:57.20ms
step:174/2330 train_time:9955ms step_avg:57.21ms
step:175/2330 train_time:10010ms step_avg:57.20ms
step:176/2330 train_time:10070ms step_avg:57.21ms
step:177/2330 train_time:10125ms step_avg:57.20ms
step:178/2330 train_time:10184ms step_avg:57.21ms
step:179/2330 train_time:10240ms step_avg:57.21ms
step:180/2330 train_time:10298ms step_avg:57.21ms
step:181/2330 train_time:10354ms step_avg:57.20ms
step:182/2330 train_time:10414ms step_avg:57.22ms
step:183/2330 train_time:10470ms step_avg:57.21ms
step:184/2330 train_time:10528ms step_avg:57.22ms
step:185/2330 train_time:10584ms step_avg:57.21ms
step:186/2330 train_time:10642ms step_avg:57.22ms
step:187/2330 train_time:10698ms step_avg:57.21ms
step:188/2330 train_time:10757ms step_avg:57.22ms
step:189/2330 train_time:10812ms step_avg:57.21ms
step:190/2330 train_time:10871ms step_avg:57.21ms
step:191/2330 train_time:10926ms step_avg:57.20ms
step:192/2330 train_time:10985ms step_avg:57.21ms
step:193/2330 train_time:11040ms step_avg:57.20ms
step:194/2330 train_time:11099ms step_avg:57.21ms
step:195/2330 train_time:11155ms step_avg:57.21ms
step:196/2330 train_time:11214ms step_avg:57.21ms
step:197/2330 train_time:11269ms step_avg:57.20ms
step:198/2330 train_time:11329ms step_avg:57.22ms
step:199/2330 train_time:11385ms step_avg:57.21ms
step:200/2330 train_time:11444ms step_avg:57.22ms
step:201/2330 train_time:11500ms step_avg:57.22ms
step:202/2330 train_time:11559ms step_avg:57.22ms
step:203/2330 train_time:11615ms step_avg:57.21ms
step:204/2330 train_time:11673ms step_avg:57.22ms
step:205/2330 train_time:11729ms step_avg:57.22ms
step:206/2330 train_time:11788ms step_avg:57.22ms
step:207/2330 train_time:11844ms step_avg:57.22ms
step:208/2330 train_time:11902ms step_avg:57.22ms
step:209/2330 train_time:11957ms step_avg:57.21ms
step:210/2330 train_time:12016ms step_avg:57.22ms
step:211/2330 train_time:12072ms step_avg:57.21ms
step:212/2330 train_time:12131ms step_avg:57.22ms
step:213/2330 train_time:12186ms step_avg:57.21ms
step:214/2330 train_time:12246ms step_avg:57.23ms
step:215/2330 train_time:12302ms step_avg:57.22ms
step:216/2330 train_time:12360ms step_avg:57.22ms
step:217/2330 train_time:12416ms step_avg:57.22ms
step:218/2330 train_time:12476ms step_avg:57.23ms
step:219/2330 train_time:12531ms step_avg:57.22ms
step:220/2330 train_time:12590ms step_avg:57.23ms
step:221/2330 train_time:12645ms step_avg:57.22ms
step:222/2330 train_time:12705ms step_avg:57.23ms
step:223/2330 train_time:12761ms step_avg:57.22ms
step:224/2330 train_time:12819ms step_avg:57.23ms
step:225/2330 train_time:12875ms step_avg:57.22ms
step:226/2330 train_time:12934ms step_avg:57.23ms
step:227/2330 train_time:12990ms step_avg:57.22ms
step:228/2330 train_time:13049ms step_avg:57.23ms
step:229/2330 train_time:13104ms step_avg:57.22ms
step:230/2330 train_time:13164ms step_avg:57.23ms
step:231/2330 train_time:13220ms step_avg:57.23ms
step:232/2330 train_time:13278ms step_avg:57.23ms
step:233/2330 train_time:13334ms step_avg:57.23ms
step:234/2330 train_time:13393ms step_avg:57.23ms
step:235/2330 train_time:13448ms step_avg:57.23ms
step:236/2330 train_time:13508ms step_avg:57.24ms
step:237/2330 train_time:13563ms step_avg:57.23ms
step:238/2330 train_time:13623ms step_avg:57.24ms
step:239/2330 train_time:13679ms step_avg:57.23ms
step:240/2330 train_time:13738ms step_avg:57.24ms
step:241/2330 train_time:13793ms step_avg:57.23ms
step:242/2330 train_time:13853ms step_avg:57.24ms
step:243/2330 train_time:13909ms step_avg:57.24ms
step:244/2330 train_time:13968ms step_avg:57.24ms
step:245/2330 train_time:14023ms step_avg:57.24ms
step:246/2330 train_time:14082ms step_avg:57.24ms
step:247/2330 train_time:14137ms step_avg:57.23ms
step:248/2330 train_time:14196ms step_avg:57.24ms
step:249/2330 train_time:14252ms step_avg:57.24ms
step:250/2330 train_time:14311ms step_avg:57.24ms
step:250/2330 val_loss:4.9552 train_time:14389ms step_avg:57.56ms
step:251/2330 train_time:14409ms step_avg:57.41ms
step:252/2330 train_time:14428ms step_avg:57.25ms
step:253/2330 train_time:14483ms step_avg:57.25ms
step:254/2330 train_time:14551ms step_avg:57.29ms
step:255/2330 train_time:14606ms step_avg:57.28ms
step:256/2330 train_time:14670ms step_avg:57.30ms
step:257/2330 train_time:14724ms step_avg:57.29ms
step:258/2330 train_time:14784ms step_avg:57.30ms
step:259/2330 train_time:14839ms step_avg:57.29ms
step:260/2330 train_time:14898ms step_avg:57.30ms
step:261/2330 train_time:14953ms step_avg:57.29ms
step:262/2330 train_time:15011ms step_avg:57.30ms
step:263/2330 train_time:15067ms step_avg:57.29ms
step:264/2330 train_time:15125ms step_avg:57.29ms
step:265/2330 train_time:15180ms step_avg:57.28ms
step:266/2330 train_time:15238ms step_avg:57.29ms
step:267/2330 train_time:15294ms step_avg:57.28ms
step:268/2330 train_time:15353ms step_avg:57.29ms
step:269/2330 train_time:15409ms step_avg:57.28ms
step:270/2330 train_time:15469ms step_avg:57.29ms
step:271/2330 train_time:15525ms step_avg:57.29ms
step:272/2330 train_time:15586ms step_avg:57.30ms
step:273/2330 train_time:15642ms step_avg:57.30ms
step:274/2330 train_time:15702ms step_avg:57.31ms
step:275/2330 train_time:15758ms step_avg:57.30ms
step:276/2330 train_time:15817ms step_avg:57.31ms
step:277/2330 train_time:15873ms step_avg:57.30ms
step:278/2330 train_time:15931ms step_avg:57.31ms
step:279/2330 train_time:15986ms step_avg:57.30ms
step:280/2330 train_time:16045ms step_avg:57.30ms
step:281/2330 train_time:16101ms step_avg:57.30ms
step:282/2330 train_time:16159ms step_avg:57.30ms
step:283/2330 train_time:16214ms step_avg:57.29ms
step:284/2330 train_time:16274ms step_avg:57.30ms
step:285/2330 train_time:16331ms step_avg:57.30ms
step:286/2330 train_time:16389ms step_avg:57.31ms
step:287/2330 train_time:16446ms step_avg:57.30ms
step:288/2330 train_time:16505ms step_avg:57.31ms
step:289/2330 train_time:16561ms step_avg:57.30ms
step:290/2330 train_time:16621ms step_avg:57.31ms
step:291/2330 train_time:16677ms step_avg:57.31ms
step:292/2330 train_time:16738ms step_avg:57.32ms
step:293/2330 train_time:16793ms step_avg:57.32ms
step:294/2330 train_time:16852ms step_avg:57.32ms
step:295/2330 train_time:16908ms step_avg:57.32ms
step:296/2330 train_time:16967ms step_avg:57.32ms
step:297/2330 train_time:17022ms step_avg:57.31ms
step:298/2330 train_time:17081ms step_avg:57.32ms
step:299/2330 train_time:17136ms step_avg:57.31ms
step:300/2330 train_time:17196ms step_avg:57.32ms
step:301/2330 train_time:17252ms step_avg:57.31ms
step:302/2330 train_time:17311ms step_avg:57.32ms
step:303/2330 train_time:17366ms step_avg:57.31ms
step:304/2330 train_time:17424ms step_avg:57.32ms
step:305/2330 train_time:17481ms step_avg:57.31ms
step:306/2330 train_time:17540ms step_avg:57.32ms
step:307/2330 train_time:17596ms step_avg:57.32ms
step:308/2330 train_time:17656ms step_avg:57.32ms
step:309/2330 train_time:17712ms step_avg:57.32ms
step:310/2330 train_time:17771ms step_avg:57.32ms
step:311/2330 train_time:17826ms step_avg:57.32ms
step:312/2330 train_time:17886ms step_avg:57.33ms
step:313/2330 train_time:17941ms step_avg:57.32ms
step:314/2330 train_time:18000ms step_avg:57.32ms
step:315/2330 train_time:18055ms step_avg:57.32ms
step:316/2330 train_time:18114ms step_avg:57.32ms
step:317/2330 train_time:18170ms step_avg:57.32ms
step:318/2330 train_time:18228ms step_avg:57.32ms
step:319/2330 train_time:18284ms step_avg:57.32ms
step:320/2330 train_time:18343ms step_avg:57.32ms
step:321/2330 train_time:18398ms step_avg:57.32ms
step:322/2330 train_time:18459ms step_avg:57.32ms
step:323/2330 train_time:18514ms step_avg:57.32ms
step:324/2330 train_time:18574ms step_avg:57.33ms
step:325/2330 train_time:18630ms step_avg:57.32ms
step:326/2330 train_time:18689ms step_avg:57.33ms
step:327/2330 train_time:18745ms step_avg:57.32ms
step:328/2330 train_time:18804ms step_avg:57.33ms
step:329/2330 train_time:18859ms step_avg:57.32ms
step:330/2330 train_time:18919ms step_avg:57.33ms
step:331/2330 train_time:18974ms step_avg:57.32ms
step:332/2330 train_time:19033ms step_avg:57.33ms
step:333/2330 train_time:19088ms step_avg:57.32ms
step:334/2330 train_time:19147ms step_avg:57.33ms
step:335/2330 train_time:19202ms step_avg:57.32ms
step:336/2330 train_time:19261ms step_avg:57.32ms
step:337/2330 train_time:19317ms step_avg:57.32ms
step:338/2330 train_time:19376ms step_avg:57.32ms
step:339/2330 train_time:19431ms step_avg:57.32ms
step:340/2330 train_time:19490ms step_avg:57.32ms
step:341/2330 train_time:19546ms step_avg:57.32ms
step:342/2330 train_time:19606ms step_avg:57.33ms
step:343/2330 train_time:19662ms step_avg:57.32ms
step:344/2330 train_time:19722ms step_avg:57.33ms
step:345/2330 train_time:19777ms step_avg:57.33ms
step:346/2330 train_time:19837ms step_avg:57.33ms
step:347/2330 train_time:19893ms step_avg:57.33ms
step:348/2330 train_time:19952ms step_avg:57.33ms
step:349/2330 train_time:20007ms step_avg:57.33ms
step:350/2330 train_time:20066ms step_avg:57.33ms
step:351/2330 train_time:20121ms step_avg:57.33ms
step:352/2330 train_time:20180ms step_avg:57.33ms
step:353/2330 train_time:20236ms step_avg:57.33ms
step:354/2330 train_time:20295ms step_avg:57.33ms
step:355/2330 train_time:20351ms step_avg:57.33ms
step:356/2330 train_time:20410ms step_avg:57.33ms
step:357/2330 train_time:20465ms step_avg:57.33ms
step:358/2330 train_time:20525ms step_avg:57.33ms
step:359/2330 train_time:20581ms step_avg:57.33ms
step:360/2330 train_time:20640ms step_avg:57.33ms
step:361/2330 train_time:20695ms step_avg:57.33ms
step:362/2330 train_time:20755ms step_avg:57.33ms
step:363/2330 train_time:20811ms step_avg:57.33ms
step:364/2330 train_time:20869ms step_avg:57.33ms
step:365/2330 train_time:20925ms step_avg:57.33ms
step:366/2330 train_time:20984ms step_avg:57.33ms
step:367/2330 train_time:21039ms step_avg:57.33ms
step:368/2330 train_time:21099ms step_avg:57.33ms
step:369/2330 train_time:21154ms step_avg:57.33ms
step:370/2330 train_time:21213ms step_avg:57.33ms
step:371/2330 train_time:21269ms step_avg:57.33ms
step:372/2330 train_time:21328ms step_avg:57.33ms
step:373/2330 train_time:21383ms step_avg:57.33ms
step:374/2330 train_time:21442ms step_avg:57.33ms
step:375/2330 train_time:21499ms step_avg:57.33ms
step:376/2330 train_time:21558ms step_avg:57.33ms
step:377/2330 train_time:21613ms step_avg:57.33ms
step:378/2330 train_time:21673ms step_avg:57.34ms
step:379/2330 train_time:21729ms step_avg:57.33ms
step:380/2330 train_time:21788ms step_avg:57.34ms
step:381/2330 train_time:21843ms step_avg:57.33ms
step:382/2330 train_time:21903ms step_avg:57.34ms
step:383/2330 train_time:21959ms step_avg:57.33ms
step:384/2330 train_time:22018ms step_avg:57.34ms
step:385/2330 train_time:22073ms step_avg:57.33ms
step:386/2330 train_time:22132ms step_avg:57.34ms
step:387/2330 train_time:22189ms step_avg:57.34ms
step:388/2330 train_time:22247ms step_avg:57.34ms
step:389/2330 train_time:22303ms step_avg:57.33ms
step:390/2330 train_time:22362ms step_avg:57.34ms
step:391/2330 train_time:22418ms step_avg:57.34ms
step:392/2330 train_time:22478ms step_avg:57.34ms
step:393/2330 train_time:22534ms step_avg:57.34ms
step:394/2330 train_time:22593ms step_avg:57.34ms
step:395/2330 train_time:22649ms step_avg:57.34ms
step:396/2330 train_time:22707ms step_avg:57.34ms
step:397/2330 train_time:22763ms step_avg:57.34ms
step:398/2330 train_time:22823ms step_avg:57.34ms
step:399/2330 train_time:22878ms step_avg:57.34ms
step:400/2330 train_time:22937ms step_avg:57.34ms
step:401/2330 train_time:22993ms step_avg:57.34ms
step:402/2330 train_time:23052ms step_avg:57.34ms
step:403/2330 train_time:23108ms step_avg:57.34ms
step:404/2330 train_time:23167ms step_avg:57.34ms
step:405/2330 train_time:23223ms step_avg:57.34ms
step:406/2330 train_time:23282ms step_avg:57.34ms
step:407/2330 train_time:23338ms step_avg:57.34ms
step:408/2330 train_time:23397ms step_avg:57.34ms
step:409/2330 train_time:23452ms step_avg:57.34ms
step:410/2330 train_time:23511ms step_avg:57.34ms
step:411/2330 train_time:23567ms step_avg:57.34ms
step:412/2330 train_time:23626ms step_avg:57.34ms
step:413/2330 train_time:23682ms step_avg:57.34ms
step:414/2330 train_time:23741ms step_avg:57.35ms
step:415/2330 train_time:23797ms step_avg:57.34ms
step:416/2330 train_time:23856ms step_avg:57.35ms
step:417/2330 train_time:23912ms step_avg:57.34ms
step:418/2330 train_time:23970ms step_avg:57.35ms
step:419/2330 train_time:24026ms step_avg:57.34ms
step:420/2330 train_time:24085ms step_avg:57.35ms
step:421/2330 train_time:24141ms step_avg:57.34ms
step:422/2330 train_time:24200ms step_avg:57.35ms
step:423/2330 train_time:24256ms step_avg:57.34ms
step:424/2330 train_time:24315ms step_avg:57.35ms
step:425/2330 train_time:24370ms step_avg:57.34ms
step:426/2330 train_time:24429ms step_avg:57.34ms
step:427/2330 train_time:24484ms step_avg:57.34ms
step:428/2330 train_time:24544ms step_avg:57.35ms
step:429/2330 train_time:24600ms step_avg:57.34ms
step:430/2330 train_time:24660ms step_avg:57.35ms
step:431/2330 train_time:24715ms step_avg:57.34ms
step:432/2330 train_time:24775ms step_avg:57.35ms
step:433/2330 train_time:24831ms step_avg:57.35ms
step:434/2330 train_time:24890ms step_avg:57.35ms
step:435/2330 train_time:24945ms step_avg:57.35ms
step:436/2330 train_time:25004ms step_avg:57.35ms
step:437/2330 train_time:25061ms step_avg:57.35ms
step:438/2330 train_time:25119ms step_avg:57.35ms
step:439/2330 train_time:25175ms step_avg:57.35ms
step:440/2330 train_time:25234ms step_avg:57.35ms
step:441/2330 train_time:25290ms step_avg:57.35ms
step:442/2330 train_time:25349ms step_avg:57.35ms
step:443/2330 train_time:25405ms step_avg:57.35ms
step:444/2330 train_time:25463ms step_avg:57.35ms
step:445/2330 train_time:25519ms step_avg:57.35ms
step:446/2330 train_time:25578ms step_avg:57.35ms
step:447/2330 train_time:25634ms step_avg:57.35ms
step:448/2330 train_time:25693ms step_avg:57.35ms
step:449/2330 train_time:25750ms step_avg:57.35ms
step:450/2330 train_time:25808ms step_avg:57.35ms
step:451/2330 train_time:25864ms step_avg:57.35ms
step:452/2330 train_time:25923ms step_avg:57.35ms
step:453/2330 train_time:25979ms step_avg:57.35ms
step:454/2330 train_time:26039ms step_avg:57.35ms
step:455/2330 train_time:26094ms step_avg:57.35ms
step:456/2330 train_time:26154ms step_avg:57.35ms
step:457/2330 train_time:26210ms step_avg:57.35ms
step:458/2330 train_time:26268ms step_avg:57.35ms
step:459/2330 train_time:26324ms step_avg:57.35ms
step:460/2330 train_time:26383ms step_avg:57.36ms
step:461/2330 train_time:26439ms step_avg:57.35ms
step:462/2330 train_time:26498ms step_avg:57.35ms
step:463/2330 train_time:26554ms step_avg:57.35ms
step:464/2330 train_time:26613ms step_avg:57.36ms
step:465/2330 train_time:26670ms step_avg:57.35ms
step:466/2330 train_time:26729ms step_avg:57.36ms
step:467/2330 train_time:26784ms step_avg:57.35ms
step:468/2330 train_time:26843ms step_avg:57.36ms
step:469/2330 train_time:26899ms step_avg:57.35ms
step:470/2330 train_time:26959ms step_avg:57.36ms
step:471/2330 train_time:27014ms step_avg:57.35ms
step:472/2330 train_time:27073ms step_avg:57.36ms
step:473/2330 train_time:27129ms step_avg:57.36ms
step:474/2330 train_time:27188ms step_avg:57.36ms
step:475/2330 train_time:27243ms step_avg:57.35ms
step:476/2330 train_time:27303ms step_avg:57.36ms
step:477/2330 train_time:27359ms step_avg:57.36ms
step:478/2330 train_time:27418ms step_avg:57.36ms
step:479/2330 train_time:27473ms step_avg:57.36ms
step:480/2330 train_time:27532ms step_avg:57.36ms
step:481/2330 train_time:27589ms step_avg:57.36ms
step:482/2330 train_time:27647ms step_avg:57.36ms
step:483/2330 train_time:27703ms step_avg:57.36ms
step:484/2330 train_time:27762ms step_avg:57.36ms
step:485/2330 train_time:27818ms step_avg:57.36ms
step:486/2330 train_time:27877ms step_avg:57.36ms
step:487/2330 train_time:27932ms step_avg:57.36ms
step:488/2330 train_time:27992ms step_avg:57.36ms
step:489/2330 train_time:28048ms step_avg:57.36ms
step:490/2330 train_time:28107ms step_avg:57.36ms
step:491/2330 train_time:28163ms step_avg:57.36ms
step:492/2330 train_time:28222ms step_avg:57.36ms
step:493/2330 train_time:28278ms step_avg:57.36ms
step:494/2330 train_time:28337ms step_avg:57.36ms
step:495/2330 train_time:28393ms step_avg:57.36ms
step:496/2330 train_time:28452ms step_avg:57.36ms
step:497/2330 train_time:28508ms step_avg:57.36ms
step:498/2330 train_time:28566ms step_avg:57.36ms
step:499/2330 train_time:28622ms step_avg:57.36ms
step:500/2330 train_time:28682ms step_avg:57.36ms
step:500/2330 val_loss:4.4742 train_time:28761ms step_avg:57.52ms
step:501/2330 train_time:28780ms step_avg:57.45ms
step:502/2330 train_time:28799ms step_avg:57.37ms
step:503/2330 train_time:28855ms step_avg:57.37ms
step:504/2330 train_time:28919ms step_avg:57.38ms
step:505/2330 train_time:28974ms step_avg:57.38ms
step:506/2330 train_time:29036ms step_avg:57.38ms
step:507/2330 train_time:29092ms step_avg:57.38ms
step:508/2330 train_time:29151ms step_avg:57.38ms
step:509/2330 train_time:29207ms step_avg:57.38ms
step:510/2330 train_time:29265ms step_avg:57.38ms
step:511/2330 train_time:29320ms step_avg:57.38ms
step:512/2330 train_time:29379ms step_avg:57.38ms
step:513/2330 train_time:29435ms step_avg:57.38ms
step:514/2330 train_time:29493ms step_avg:57.38ms
step:515/2330 train_time:29549ms step_avg:57.38ms
step:516/2330 train_time:29607ms step_avg:57.38ms
step:517/2330 train_time:29662ms step_avg:57.37ms
step:518/2330 train_time:29721ms step_avg:57.38ms
step:519/2330 train_time:29778ms step_avg:57.38ms
step:520/2330 train_time:29838ms step_avg:57.38ms
step:521/2330 train_time:29895ms step_avg:57.38ms
step:522/2330 train_time:29955ms step_avg:57.38ms
step:523/2330 train_time:30011ms step_avg:57.38ms
step:524/2330 train_time:30070ms step_avg:57.39ms
step:525/2330 train_time:30126ms step_avg:57.38ms
step:526/2330 train_time:30186ms step_avg:57.39ms
step:527/2330 train_time:30242ms step_avg:57.38ms
step:528/2330 train_time:30301ms step_avg:57.39ms
step:529/2330 train_time:30357ms step_avg:57.39ms
step:530/2330 train_time:30416ms step_avg:57.39ms
step:531/2330 train_time:30471ms step_avg:57.38ms
step:532/2330 train_time:30529ms step_avg:57.39ms
step:533/2330 train_time:30585ms step_avg:57.38ms
step:534/2330 train_time:30644ms step_avg:57.39ms
step:535/2330 train_time:30700ms step_avg:57.38ms
step:536/2330 train_time:30760ms step_avg:57.39ms
step:537/2330 train_time:30816ms step_avg:57.39ms
step:538/2330 train_time:30875ms step_avg:57.39ms
step:539/2330 train_time:30932ms step_avg:57.39ms
step:540/2330 train_time:30991ms step_avg:57.39ms
step:541/2330 train_time:31048ms step_avg:57.39ms
step:542/2330 train_time:31108ms step_avg:57.39ms
step:543/2330 train_time:31163ms step_avg:57.39ms
step:544/2330 train_time:31223ms step_avg:57.40ms
step:545/2330 train_time:31279ms step_avg:57.39ms
step:546/2330 train_time:31339ms step_avg:57.40ms
step:547/2330 train_time:31395ms step_avg:57.39ms
step:548/2330 train_time:31453ms step_avg:57.40ms
step:549/2330 train_time:31509ms step_avg:57.39ms
step:550/2330 train_time:31567ms step_avg:57.40ms
step:551/2330 train_time:31623ms step_avg:57.39ms
step:552/2330 train_time:31683ms step_avg:57.40ms
step:553/2330 train_time:31738ms step_avg:57.39ms
step:554/2330 train_time:31798ms step_avg:57.40ms
step:555/2330 train_time:31855ms step_avg:57.40ms
step:556/2330 train_time:31915ms step_avg:57.40ms
step:557/2330 train_time:31972ms step_avg:57.40ms
step:558/2330 train_time:32031ms step_avg:57.40ms
step:559/2330 train_time:32087ms step_avg:57.40ms
step:560/2330 train_time:32147ms step_avg:57.41ms
step:561/2330 train_time:32203ms step_avg:57.40ms
step:562/2330 train_time:32263ms step_avg:57.41ms
step:563/2330 train_time:32319ms step_avg:57.40ms
step:564/2330 train_time:32377ms step_avg:57.41ms
step:565/2330 train_time:32433ms step_avg:57.40ms
step:566/2330 train_time:32491ms step_avg:57.40ms
step:567/2330 train_time:32547ms step_avg:57.40ms
step:568/2330 train_time:32607ms step_avg:57.41ms
step:569/2330 train_time:32662ms step_avg:57.40ms
step:570/2330 train_time:32723ms step_avg:57.41ms
step:571/2330 train_time:32778ms step_avg:57.40ms
step:572/2330 train_time:32838ms step_avg:57.41ms
step:573/2330 train_time:32894ms step_avg:57.41ms
step:574/2330 train_time:32953ms step_avg:57.41ms
step:575/2330 train_time:33010ms step_avg:57.41ms
step:576/2330 train_time:33069ms step_avg:57.41ms
step:577/2330 train_time:33125ms step_avg:57.41ms
step:578/2330 train_time:33185ms step_avg:57.41ms
step:579/2330 train_time:33241ms step_avg:57.41ms
step:580/2330 train_time:33301ms step_avg:57.42ms
step:581/2330 train_time:33356ms step_avg:57.41ms
step:582/2330 train_time:33415ms step_avg:57.41ms
step:583/2330 train_time:33471ms step_avg:57.41ms
step:584/2330 train_time:33529ms step_avg:57.41ms
step:585/2330 train_time:33585ms step_avg:57.41ms
step:586/2330 train_time:33644ms step_avg:57.41ms
step:587/2330 train_time:33699ms step_avg:57.41ms
step:588/2330 train_time:33759ms step_avg:57.41ms
step:589/2330 train_time:33814ms step_avg:57.41ms
step:590/2330 train_time:33874ms step_avg:57.41ms
step:591/2330 train_time:33929ms step_avg:57.41ms
step:592/2330 train_time:33989ms step_avg:57.41ms
step:593/2330 train_time:34045ms step_avg:57.41ms
step:594/2330 train_time:34105ms step_avg:57.42ms
step:595/2330 train_time:34160ms step_avg:57.41ms
step:596/2330 train_time:34221ms step_avg:57.42ms
step:597/2330 train_time:34276ms step_avg:57.41ms
step:598/2330 train_time:34336ms step_avg:57.42ms
step:599/2330 train_time:34391ms step_avg:57.41ms
step:600/2330 train_time:34450ms step_avg:57.42ms
step:601/2330 train_time:34506ms step_avg:57.41ms
step:602/2330 train_time:34565ms step_avg:57.42ms
step:603/2330 train_time:34621ms step_avg:57.41ms
step:604/2330 train_time:34679ms step_avg:57.42ms
step:605/2330 train_time:34735ms step_avg:57.41ms
step:606/2330 train_time:34795ms step_avg:57.42ms
step:607/2330 train_time:34851ms step_avg:57.42ms
step:608/2330 train_time:34910ms step_avg:57.42ms
step:609/2330 train_time:34967ms step_avg:57.42ms
step:610/2330 train_time:35026ms step_avg:57.42ms
step:611/2330 train_time:35082ms step_avg:57.42ms
step:612/2330 train_time:35142ms step_avg:57.42ms
step:613/2330 train_time:35198ms step_avg:57.42ms
step:614/2330 train_time:35257ms step_avg:57.42ms
step:615/2330 train_time:35313ms step_avg:57.42ms
step:616/2330 train_time:35372ms step_avg:57.42ms
step:617/2330 train_time:35428ms step_avg:57.42ms
step:618/2330 train_time:35486ms step_avg:57.42ms
step:619/2330 train_time:35542ms step_avg:57.42ms
step:620/2330 train_time:35602ms step_avg:57.42ms
step:621/2330 train_time:35657ms step_avg:57.42ms
step:622/2330 train_time:35717ms step_avg:57.42ms
step:623/2330 train_time:35773ms step_avg:57.42ms
step:624/2330 train_time:35831ms step_avg:57.42ms
step:625/2330 train_time:35887ms step_avg:57.42ms
step:626/2330 train_time:35947ms step_avg:57.42ms
step:627/2330 train_time:36003ms step_avg:57.42ms
step:628/2330 train_time:36062ms step_avg:57.42ms
step:629/2330 train_time:36118ms step_avg:57.42ms
step:630/2330 train_time:36178ms step_avg:57.43ms
step:631/2330 train_time:36235ms step_avg:57.42ms
step:632/2330 train_time:36294ms step_avg:57.43ms
step:633/2330 train_time:36350ms step_avg:57.43ms
step:634/2330 train_time:36409ms step_avg:57.43ms
step:635/2330 train_time:36465ms step_avg:57.43ms
step:636/2330 train_time:36524ms step_avg:57.43ms
step:637/2330 train_time:36579ms step_avg:57.42ms
step:638/2330 train_time:36639ms step_avg:57.43ms
step:639/2330 train_time:36695ms step_avg:57.42ms
step:640/2330 train_time:36754ms step_avg:57.43ms
step:641/2330 train_time:36810ms step_avg:57.43ms
step:642/2330 train_time:36868ms step_avg:57.43ms
step:643/2330 train_time:36924ms step_avg:57.42ms
step:644/2330 train_time:36984ms step_avg:57.43ms
step:645/2330 train_time:37040ms step_avg:57.43ms
step:646/2330 train_time:37100ms step_avg:57.43ms
step:647/2330 train_time:37156ms step_avg:57.43ms
step:648/2330 train_time:37215ms step_avg:57.43ms
step:649/2330 train_time:37271ms step_avg:57.43ms
step:650/2330 train_time:37330ms step_avg:57.43ms
step:651/2330 train_time:37386ms step_avg:57.43ms
step:652/2330 train_time:37445ms step_avg:57.43ms
step:653/2330 train_time:37500ms step_avg:57.43ms
step:654/2330 train_time:37560ms step_avg:57.43ms
step:655/2330 train_time:37615ms step_avg:57.43ms
step:656/2330 train_time:37675ms step_avg:57.43ms
step:657/2330 train_time:37731ms step_avg:57.43ms
step:658/2330 train_time:37790ms step_avg:57.43ms
step:659/2330 train_time:37845ms step_avg:57.43ms
step:660/2330 train_time:37905ms step_avg:57.43ms
step:661/2330 train_time:37961ms step_avg:57.43ms
step:662/2330 train_time:38021ms step_avg:57.43ms
step:663/2330 train_time:38077ms step_avg:57.43ms
step:664/2330 train_time:38136ms step_avg:57.43ms
step:665/2330 train_time:38192ms step_avg:57.43ms
step:666/2330 train_time:38250ms step_avg:57.43ms
step:667/2330 train_time:38306ms step_avg:57.43ms
step:668/2330 train_time:38365ms step_avg:57.43ms
step:669/2330 train_time:38421ms step_avg:57.43ms
step:670/2330 train_time:38481ms step_avg:57.43ms
step:671/2330 train_time:38537ms step_avg:57.43ms
step:672/2330 train_time:38596ms step_avg:57.43ms
step:673/2330 train_time:38652ms step_avg:57.43ms
step:674/2330 train_time:38711ms step_avg:57.43ms
step:675/2330 train_time:38766ms step_avg:57.43ms
step:676/2330 train_time:38826ms step_avg:57.43ms
step:677/2330 train_time:38882ms step_avg:57.43ms
step:678/2330 train_time:38940ms step_avg:57.43ms
step:679/2330 train_time:38996ms step_avg:57.43ms
step:680/2330 train_time:39055ms step_avg:57.43ms
step:681/2330 train_time:39111ms step_avg:57.43ms
step:682/2330 train_time:39170ms step_avg:57.43ms
step:683/2330 train_time:39226ms step_avg:57.43ms
step:684/2330 train_time:39287ms step_avg:57.44ms
step:685/2330 train_time:39343ms step_avg:57.43ms
step:686/2330 train_time:39402ms step_avg:57.44ms
step:687/2330 train_time:39457ms step_avg:57.43ms
step:688/2330 train_time:39516ms step_avg:57.44ms
step:689/2330 train_time:39572ms step_avg:57.43ms
step:690/2330 train_time:39631ms step_avg:57.44ms
step:691/2330 train_time:39687ms step_avg:57.43ms
step:692/2330 train_time:39747ms step_avg:57.44ms
step:693/2330 train_time:39802ms step_avg:57.43ms
step:694/2330 train_time:39862ms step_avg:57.44ms
step:695/2330 train_time:39917ms step_avg:57.43ms
step:696/2330 train_time:39977ms step_avg:57.44ms
step:697/2330 train_time:40033ms step_avg:57.44ms
step:698/2330 train_time:40092ms step_avg:57.44ms
step:699/2330 train_time:40147ms step_avg:57.44ms
step:700/2330 train_time:40208ms step_avg:57.44ms
step:701/2330 train_time:40264ms step_avg:57.44ms
step:702/2330 train_time:40324ms step_avg:57.44ms
step:703/2330 train_time:40379ms step_avg:57.44ms
step:704/2330 train_time:40440ms step_avg:57.44ms
step:705/2330 train_time:40497ms step_avg:57.44ms
step:706/2330 train_time:40555ms step_avg:57.44ms
step:707/2330 train_time:40611ms step_avg:57.44ms
step:708/2330 train_time:40669ms step_avg:57.44ms
step:709/2330 train_time:40726ms step_avg:57.44ms
step:710/2330 train_time:40786ms step_avg:57.44ms
step:711/2330 train_time:40841ms step_avg:57.44ms
step:712/2330 train_time:40902ms step_avg:57.45ms
step:713/2330 train_time:40957ms step_avg:57.44ms
step:714/2330 train_time:41016ms step_avg:57.45ms
step:715/2330 train_time:41073ms step_avg:57.44ms
step:716/2330 train_time:41131ms step_avg:57.45ms
step:717/2330 train_time:41187ms step_avg:57.44ms
step:718/2330 train_time:41247ms step_avg:57.45ms
step:719/2330 train_time:41302ms step_avg:57.44ms
step:720/2330 train_time:41363ms step_avg:57.45ms
step:721/2330 train_time:41418ms step_avg:57.45ms
step:722/2330 train_time:41477ms step_avg:57.45ms
step:723/2330 train_time:41533ms step_avg:57.45ms
step:724/2330 train_time:41592ms step_avg:57.45ms
step:725/2330 train_time:41649ms step_avg:57.45ms
step:726/2330 train_time:41708ms step_avg:57.45ms
step:727/2330 train_time:41764ms step_avg:57.45ms
step:728/2330 train_time:41824ms step_avg:57.45ms
step:729/2330 train_time:41879ms step_avg:57.45ms
step:730/2330 train_time:41939ms step_avg:57.45ms
step:731/2330 train_time:41995ms step_avg:57.45ms
step:732/2330 train_time:42054ms step_avg:57.45ms
step:733/2330 train_time:42110ms step_avg:57.45ms
step:734/2330 train_time:42169ms step_avg:57.45ms
step:735/2330 train_time:42225ms step_avg:57.45ms
step:736/2330 train_time:42285ms step_avg:57.45ms
step:737/2330 train_time:42340ms step_avg:57.45ms
step:738/2330 train_time:42401ms step_avg:57.45ms
step:739/2330 train_time:42456ms step_avg:57.45ms
step:740/2330 train_time:42515ms step_avg:57.45ms
step:741/2330 train_time:42572ms step_avg:57.45ms
step:742/2330 train_time:42631ms step_avg:57.45ms
step:743/2330 train_time:42687ms step_avg:57.45ms
step:744/2330 train_time:42746ms step_avg:57.45ms
step:745/2330 train_time:42802ms step_avg:57.45ms
step:746/2330 train_time:42862ms step_avg:57.46ms
step:747/2330 train_time:42918ms step_avg:57.45ms
step:748/2330 train_time:42977ms step_avg:57.46ms
step:749/2330 train_time:43033ms step_avg:57.45ms
step:750/2330 train_time:43092ms step_avg:57.46ms
step:750/2330 val_loss:4.2536 train_time:43172ms step_avg:57.56ms
step:751/2330 train_time:43191ms step_avg:57.51ms
step:752/2330 train_time:43210ms step_avg:57.46ms
step:753/2330 train_time:43267ms step_avg:57.46ms
step:754/2330 train_time:43331ms step_avg:57.47ms
step:755/2330 train_time:43386ms step_avg:57.46ms
step:756/2330 train_time:43448ms step_avg:57.47ms
step:757/2330 train_time:43503ms step_avg:57.47ms
step:758/2330 train_time:43564ms step_avg:57.47ms
step:759/2330 train_time:43619ms step_avg:57.47ms
step:760/2330 train_time:43678ms step_avg:57.47ms
step:761/2330 train_time:43733ms step_avg:57.47ms
step:762/2330 train_time:43791ms step_avg:57.47ms
step:763/2330 train_time:43847ms step_avg:57.47ms
step:764/2330 train_time:43906ms step_avg:57.47ms
step:765/2330 train_time:43963ms step_avg:57.47ms
step:766/2330 train_time:44021ms step_avg:57.47ms
step:767/2330 train_time:44077ms step_avg:57.47ms
step:768/2330 train_time:44137ms step_avg:57.47ms
step:769/2330 train_time:44195ms step_avg:57.47ms
step:770/2330 train_time:44256ms step_avg:57.48ms
step:771/2330 train_time:44313ms step_avg:57.47ms
step:772/2330 train_time:44375ms step_avg:57.48ms
step:773/2330 train_time:44431ms step_avg:57.48ms
step:774/2330 train_time:44493ms step_avg:57.48ms
step:775/2330 train_time:44549ms step_avg:57.48ms
step:776/2330 train_time:44610ms step_avg:57.49ms
step:777/2330 train_time:44666ms step_avg:57.48ms
step:778/2330 train_time:44727ms step_avg:57.49ms
step:779/2330 train_time:44783ms step_avg:57.49ms
step:780/2330 train_time:44843ms step_avg:57.49ms
step:781/2330 train_time:44899ms step_avg:57.49ms
step:782/2330 train_time:44959ms step_avg:57.49ms
step:783/2330 train_time:45015ms step_avg:57.49ms
step:784/2330 train_time:45075ms step_avg:57.49ms
step:785/2330 train_time:45132ms step_avg:57.49ms
step:786/2330 train_time:45193ms step_avg:57.50ms
step:787/2330 train_time:45250ms step_avg:57.50ms
step:788/2330 train_time:45311ms step_avg:57.50ms
step:789/2330 train_time:45368ms step_avg:57.50ms
step:790/2330 train_time:45429ms step_avg:57.50ms
step:791/2330 train_time:45486ms step_avg:57.50ms
step:792/2330 train_time:45547ms step_avg:57.51ms
step:793/2330 train_time:45603ms step_avg:57.51ms
step:794/2330 train_time:45662ms step_avg:57.51ms
step:795/2330 train_time:45719ms step_avg:57.51ms
step:796/2330 train_time:45778ms step_avg:57.51ms
step:797/2330 train_time:45834ms step_avg:57.51ms
step:798/2330 train_time:45895ms step_avg:57.51ms
step:799/2330 train_time:45951ms step_avg:57.51ms
step:800/2330 train_time:46012ms step_avg:57.51ms
step:801/2330 train_time:46068ms step_avg:57.51ms
step:802/2330 train_time:46129ms step_avg:57.52ms
step:803/2330 train_time:46186ms step_avg:57.52ms
step:804/2330 train_time:46246ms step_avg:57.52ms
step:805/2330 train_time:46303ms step_avg:57.52ms
step:806/2330 train_time:46364ms step_avg:57.52ms
step:807/2330 train_time:46421ms step_avg:57.52ms
step:808/2330 train_time:46481ms step_avg:57.53ms
step:809/2330 train_time:46538ms step_avg:57.53ms
step:810/2330 train_time:46598ms step_avg:57.53ms
step:811/2330 train_time:46655ms step_avg:57.53ms
step:812/2330 train_time:46714ms step_avg:57.53ms
step:813/2330 train_time:46771ms step_avg:57.53ms
step:814/2330 train_time:46831ms step_avg:57.53ms
step:815/2330 train_time:46887ms step_avg:57.53ms
step:816/2330 train_time:46948ms step_avg:57.53ms
step:817/2330 train_time:47003ms step_avg:57.53ms
step:818/2330 train_time:47064ms step_avg:57.54ms
step:819/2330 train_time:47121ms step_avg:57.53ms
step:820/2330 train_time:47181ms step_avg:57.54ms
step:821/2330 train_time:47239ms step_avg:57.54ms
step:822/2330 train_time:47298ms step_avg:57.54ms
step:823/2330 train_time:47355ms step_avg:57.54ms
step:824/2330 train_time:47415ms step_avg:57.54ms
step:825/2330 train_time:47472ms step_avg:57.54ms
step:826/2330 train_time:47532ms step_avg:57.55ms
step:827/2330 train_time:47589ms step_avg:57.54ms
step:828/2330 train_time:47650ms step_avg:57.55ms
step:829/2330 train_time:47706ms step_avg:57.55ms
step:830/2330 train_time:47767ms step_avg:57.55ms
step:831/2330 train_time:47823ms step_avg:57.55ms
step:832/2330 train_time:47884ms step_avg:57.55ms
step:833/2330 train_time:47940ms step_avg:57.55ms
step:834/2330 train_time:47999ms step_avg:57.55ms
step:835/2330 train_time:48054ms step_avg:57.55ms
step:836/2330 train_time:48116ms step_avg:57.55ms
step:837/2330 train_time:48171ms step_avg:57.55ms
step:838/2330 train_time:48233ms step_avg:57.56ms
step:839/2330 train_time:48289ms step_avg:57.56ms
step:840/2330 train_time:48351ms step_avg:57.56ms
step:841/2330 train_time:48407ms step_avg:57.56ms
step:842/2330 train_time:48469ms step_avg:57.56ms
step:843/2330 train_time:48525ms step_avg:57.56ms
step:844/2330 train_time:48587ms step_avg:57.57ms
step:845/2330 train_time:48643ms step_avg:57.57ms
step:846/2330 train_time:48703ms step_avg:57.57ms
step:847/2330 train_time:48760ms step_avg:57.57ms
step:848/2330 train_time:48819ms step_avg:57.57ms
step:849/2330 train_time:48876ms step_avg:57.57ms
step:850/2330 train_time:48936ms step_avg:57.57ms
step:851/2330 train_time:48993ms step_avg:57.57ms
step:852/2330 train_time:49052ms step_avg:57.57ms
step:853/2330 train_time:49108ms step_avg:57.57ms
step:854/2330 train_time:49169ms step_avg:57.58ms
step:855/2330 train_time:49226ms step_avg:57.57ms
step:856/2330 train_time:49286ms step_avg:57.58ms
step:857/2330 train_time:49343ms step_avg:57.58ms
step:858/2330 train_time:49403ms step_avg:57.58ms
step:859/2330 train_time:49460ms step_avg:57.58ms
step:860/2330 train_time:49520ms step_avg:57.58ms
step:861/2330 train_time:49577ms step_avg:57.58ms
step:862/2330 train_time:49637ms step_avg:57.58ms
step:863/2330 train_time:49693ms step_avg:57.58ms
step:864/2330 train_time:49754ms step_avg:57.59ms
step:865/2330 train_time:49810ms step_avg:57.58ms
step:866/2330 train_time:49871ms step_avg:57.59ms
step:867/2330 train_time:49927ms step_avg:57.59ms
step:868/2330 train_time:49988ms step_avg:57.59ms
step:869/2330 train_time:50044ms step_avg:57.59ms
step:870/2330 train_time:50104ms step_avg:57.59ms
step:871/2330 train_time:50161ms step_avg:57.59ms
step:872/2330 train_time:50221ms step_avg:57.59ms
step:873/2330 train_time:50277ms step_avg:57.59ms
step:874/2330 train_time:50337ms step_avg:57.59ms
step:875/2330 train_time:50394ms step_avg:57.59ms
step:876/2330 train_time:50455ms step_avg:57.60ms
step:877/2330 train_time:50511ms step_avg:57.59ms
step:878/2330 train_time:50572ms step_avg:57.60ms
step:879/2330 train_time:50628ms step_avg:57.60ms
step:880/2330 train_time:50690ms step_avg:57.60ms
step:881/2330 train_time:50746ms step_avg:57.60ms
step:882/2330 train_time:50806ms step_avg:57.60ms
step:883/2330 train_time:50863ms step_avg:57.60ms
step:884/2330 train_time:50922ms step_avg:57.60ms
step:885/2330 train_time:50979ms step_avg:57.60ms
step:886/2330 train_time:51039ms step_avg:57.61ms
step:887/2330 train_time:51095ms step_avg:57.60ms
step:888/2330 train_time:51156ms step_avg:57.61ms
step:889/2330 train_time:51213ms step_avg:57.61ms
step:890/2330 train_time:51273ms step_avg:57.61ms
step:891/2330 train_time:51329ms step_avg:57.61ms
step:892/2330 train_time:51390ms step_avg:57.61ms
step:893/2330 train_time:51447ms step_avg:57.61ms
step:894/2330 train_time:51508ms step_avg:57.61ms
step:895/2330 train_time:51564ms step_avg:57.61ms
step:896/2330 train_time:51625ms step_avg:57.62ms
step:897/2330 train_time:51682ms step_avg:57.62ms
step:898/2330 train_time:51741ms step_avg:57.62ms
step:899/2330 train_time:51798ms step_avg:57.62ms
step:900/2330 train_time:51858ms step_avg:57.62ms
step:901/2330 train_time:51915ms step_avg:57.62ms
step:902/2330 train_time:51975ms step_avg:57.62ms
step:903/2330 train_time:52032ms step_avg:57.62ms
step:904/2330 train_time:52092ms step_avg:57.62ms
step:905/2330 train_time:52148ms step_avg:57.62ms
step:906/2330 train_time:52209ms step_avg:57.63ms
step:907/2330 train_time:52266ms step_avg:57.62ms
step:908/2330 train_time:52327ms step_avg:57.63ms
step:909/2330 train_time:52384ms step_avg:57.63ms
step:910/2330 train_time:52444ms step_avg:57.63ms
step:911/2330 train_time:52500ms step_avg:57.63ms
step:912/2330 train_time:52561ms step_avg:57.63ms
step:913/2330 train_time:52618ms step_avg:57.63ms
step:914/2330 train_time:52678ms step_avg:57.63ms
step:915/2330 train_time:52734ms step_avg:57.63ms
step:916/2330 train_time:52795ms step_avg:57.64ms
step:917/2330 train_time:52851ms step_avg:57.63ms
step:918/2330 train_time:52912ms step_avg:57.64ms
step:919/2330 train_time:52968ms step_avg:57.64ms
step:920/2330 train_time:53029ms step_avg:57.64ms
step:921/2330 train_time:53086ms step_avg:57.64ms
step:922/2330 train_time:53146ms step_avg:57.64ms
step:923/2330 train_time:53202ms step_avg:57.64ms
step:924/2330 train_time:53262ms step_avg:57.64ms
step:925/2330 train_time:53318ms step_avg:57.64ms
step:926/2330 train_time:53379ms step_avg:57.64ms
step:927/2330 train_time:53435ms step_avg:57.64ms
step:928/2330 train_time:53496ms step_avg:57.65ms
step:929/2330 train_time:53553ms step_avg:57.65ms
step:930/2330 train_time:53614ms step_avg:57.65ms
step:931/2330 train_time:53670ms step_avg:57.65ms
step:932/2330 train_time:53731ms step_avg:57.65ms
step:933/2330 train_time:53788ms step_avg:57.65ms
step:934/2330 train_time:53849ms step_avg:57.65ms
step:935/2330 train_time:53904ms step_avg:57.65ms
step:936/2330 train_time:53965ms step_avg:57.65ms
step:937/2330 train_time:54022ms step_avg:57.65ms
step:938/2330 train_time:54083ms step_avg:57.66ms
step:939/2330 train_time:54140ms step_avg:57.66ms
step:940/2330 train_time:54200ms step_avg:57.66ms
step:941/2330 train_time:54257ms step_avg:57.66ms
step:942/2330 train_time:54316ms step_avg:57.66ms
step:943/2330 train_time:54373ms step_avg:57.66ms
step:944/2330 train_time:54433ms step_avg:57.66ms
step:945/2330 train_time:54490ms step_avg:57.66ms
step:946/2330 train_time:54549ms step_avg:57.66ms
step:947/2330 train_time:54606ms step_avg:57.66ms
step:948/2330 train_time:54667ms step_avg:57.67ms
step:949/2330 train_time:54724ms step_avg:57.66ms
step:950/2330 train_time:54784ms step_avg:57.67ms
step:951/2330 train_time:54841ms step_avg:57.67ms
step:952/2330 train_time:54900ms step_avg:57.67ms
step:953/2330 train_time:54959ms step_avg:57.67ms
step:954/2330 train_time:55017ms step_avg:57.67ms
step:955/2330 train_time:55074ms step_avg:57.67ms
step:956/2330 train_time:55135ms step_avg:57.67ms
step:957/2330 train_time:55191ms step_avg:57.67ms
step:958/2330 train_time:55252ms step_avg:57.67ms
step:959/2330 train_time:55309ms step_avg:57.67ms
step:960/2330 train_time:55370ms step_avg:57.68ms
step:961/2330 train_time:55427ms step_avg:57.68ms
step:962/2330 train_time:55488ms step_avg:57.68ms
step:963/2330 train_time:55544ms step_avg:57.68ms
step:964/2330 train_time:55605ms step_avg:57.68ms
step:965/2330 train_time:55662ms step_avg:57.68ms
step:966/2330 train_time:55722ms step_avg:57.68ms
step:967/2330 train_time:55779ms step_avg:57.68ms
step:968/2330 train_time:55838ms step_avg:57.68ms
step:969/2330 train_time:55895ms step_avg:57.68ms
step:970/2330 train_time:55955ms step_avg:57.69ms
step:971/2330 train_time:56012ms step_avg:57.68ms
step:972/2330 train_time:56072ms step_avg:57.69ms
step:973/2330 train_time:56130ms step_avg:57.69ms
step:974/2330 train_time:56190ms step_avg:57.69ms
step:975/2330 train_time:56246ms step_avg:57.69ms
step:976/2330 train_time:56306ms step_avg:57.69ms
step:977/2330 train_time:56363ms step_avg:57.69ms
step:978/2330 train_time:56423ms step_avg:57.69ms
step:979/2330 train_time:56480ms step_avg:57.69ms
step:980/2330 train_time:56540ms step_avg:57.69ms
step:981/2330 train_time:56596ms step_avg:57.69ms
step:982/2330 train_time:56657ms step_avg:57.70ms
step:983/2330 train_time:56713ms step_avg:57.69ms
step:984/2330 train_time:56774ms step_avg:57.70ms
step:985/2330 train_time:56830ms step_avg:57.70ms
step:986/2330 train_time:56891ms step_avg:57.70ms
step:987/2330 train_time:56947ms step_avg:57.70ms
step:988/2330 train_time:57008ms step_avg:57.70ms
step:989/2330 train_time:57065ms step_avg:57.70ms
step:990/2330 train_time:57125ms step_avg:57.70ms
step:991/2330 train_time:57182ms step_avg:57.70ms
step:992/2330 train_time:57242ms step_avg:57.70ms
step:993/2330 train_time:57299ms step_avg:57.70ms
step:994/2330 train_time:57358ms step_avg:57.70ms
step:995/2330 train_time:57414ms step_avg:57.70ms
step:996/2330 train_time:57475ms step_avg:57.71ms
step:997/2330 train_time:57532ms step_avg:57.71ms
step:998/2330 train_time:57594ms step_avg:57.71ms
step:999/2330 train_time:57651ms step_avg:57.71ms
step:1000/2330 train_time:57711ms step_avg:57.71ms
step:1000/2330 val_loss:4.1015 train_time:57793ms step_avg:57.79ms
step:1001/2330 train_time:57813ms step_avg:57.76ms
step:1002/2330 train_time:57835ms step_avg:57.72ms
step:1003/2330 train_time:57888ms step_avg:57.71ms
step:1004/2330 train_time:57947ms step_avg:57.72ms
step:1005/2330 train_time:58003ms step_avg:57.71ms
step:1006/2330 train_time:58066ms step_avg:57.72ms
step:1007/2330 train_time:58121ms step_avg:57.72ms
step:1008/2330 train_time:58181ms step_avg:57.72ms
step:1009/2330 train_time:58236ms step_avg:57.72ms
step:1010/2330 train_time:58296ms step_avg:57.72ms
step:1011/2330 train_time:58352ms step_avg:57.72ms
step:1012/2330 train_time:58412ms step_avg:57.72ms
step:1013/2330 train_time:58467ms step_avg:57.72ms
step:1014/2330 train_time:58527ms step_avg:57.72ms
step:1015/2330 train_time:58583ms step_avg:57.72ms
step:1016/2330 train_time:58642ms step_avg:57.72ms
step:1017/2330 train_time:58700ms step_avg:57.72ms
step:1018/2330 train_time:58764ms step_avg:57.73ms
step:1019/2330 train_time:58822ms step_avg:57.72ms
step:1020/2330 train_time:58884ms step_avg:57.73ms
step:1021/2330 train_time:58940ms step_avg:57.73ms
step:1022/2330 train_time:59001ms step_avg:57.73ms
step:1023/2330 train_time:59057ms step_avg:57.73ms
step:1024/2330 train_time:59117ms step_avg:57.73ms
step:1025/2330 train_time:59173ms step_avg:57.73ms
step:1026/2330 train_time:59232ms step_avg:57.73ms
step:1027/2330 train_time:59288ms step_avg:57.73ms
step:1028/2330 train_time:59348ms step_avg:57.73ms
step:1029/2330 train_time:59404ms step_avg:57.73ms
step:1030/2330 train_time:59465ms step_avg:57.73ms
step:1031/2330 train_time:59520ms step_avg:57.73ms
step:1032/2330 train_time:59580ms step_avg:57.73ms
step:1033/2330 train_time:59636ms step_avg:57.73ms
step:1034/2330 train_time:59698ms step_avg:57.73ms
step:1035/2330 train_time:59756ms step_avg:57.73ms
step:1036/2330 train_time:59817ms step_avg:57.74ms
step:1037/2330 train_time:59875ms step_avg:57.74ms
step:1038/2330 train_time:59935ms step_avg:57.74ms
step:1039/2330 train_time:59992ms step_avg:57.74ms
step:1040/2330 train_time:60053ms step_avg:57.74ms
step:1041/2330 train_time:60108ms step_avg:57.74ms
step:1042/2330 train_time:60169ms step_avg:57.74ms
step:1043/2330 train_time:60225ms step_avg:57.74ms
step:1044/2330 train_time:60286ms step_avg:57.75ms
step:1045/2330 train_time:60342ms step_avg:57.74ms
step:1046/2330 train_time:60403ms step_avg:57.75ms
step:1047/2330 train_time:60459ms step_avg:57.75ms
step:1048/2330 train_time:60518ms step_avg:57.75ms
step:1049/2330 train_time:60574ms step_avg:57.74ms
step:1050/2330 train_time:60635ms step_avg:57.75ms
step:1051/2330 train_time:60691ms step_avg:57.75ms
step:1052/2330 train_time:60754ms step_avg:57.75ms
step:1053/2330 train_time:60810ms step_avg:57.75ms
step:1054/2330 train_time:60872ms step_avg:57.75ms
step:1055/2330 train_time:60927ms step_avg:57.75ms
step:1056/2330 train_time:60990ms step_avg:57.76ms
step:1057/2330 train_time:61046ms step_avg:57.75ms
step:1058/2330 train_time:61107ms step_avg:57.76ms
step:1059/2330 train_time:61163ms step_avg:57.76ms
step:1060/2330 train_time:61225ms step_avg:57.76ms
step:1061/2330 train_time:61280ms step_avg:57.76ms
step:1062/2330 train_time:61341ms step_avg:57.76ms
step:1063/2330 train_time:61397ms step_avg:57.76ms
step:1064/2330 train_time:61457ms step_avg:57.76ms
step:1065/2330 train_time:61513ms step_avg:57.76ms
step:1066/2330 train_time:61573ms step_avg:57.76ms
step:1067/2330 train_time:61630ms step_avg:57.76ms
step:1068/2330 train_time:61691ms step_avg:57.76ms
step:1069/2330 train_time:61747ms step_avg:57.76ms
step:1070/2330 train_time:61809ms step_avg:57.77ms
step:1071/2330 train_time:61866ms step_avg:57.76ms
step:1072/2330 train_time:61926ms step_avg:57.77ms
step:1073/2330 train_time:61982ms step_avg:57.77ms
step:1074/2330 train_time:62044ms step_avg:57.77ms
step:1075/2330 train_time:62101ms step_avg:57.77ms
step:1076/2330 train_time:62161ms step_avg:57.77ms
step:1077/2330 train_time:62218ms step_avg:57.77ms
step:1078/2330 train_time:62277ms step_avg:57.77ms
step:1079/2330 train_time:62333ms step_avg:57.77ms
step:1080/2330 train_time:62393ms step_avg:57.77ms
step:1081/2330 train_time:62449ms step_avg:57.77ms
step:1082/2330 train_time:62510ms step_avg:57.77ms
step:1083/2330 train_time:62566ms step_avg:57.77ms
step:1084/2330 train_time:62627ms step_avg:57.77ms
step:1085/2330 train_time:62683ms step_avg:57.77ms
step:1086/2330 train_time:62744ms step_avg:57.78ms
step:1087/2330 train_time:62801ms step_avg:57.77ms
step:1088/2330 train_time:62862ms step_avg:57.78ms
step:1089/2330 train_time:62917ms step_avg:57.78ms
step:1090/2330 train_time:62978ms step_avg:57.78ms
step:1091/2330 train_time:63035ms step_avg:57.78ms
step:1092/2330 train_time:63096ms step_avg:57.78ms
step:1093/2330 train_time:63152ms step_avg:57.78ms
step:1094/2330 train_time:63213ms step_avg:57.78ms
step:1095/2330 train_time:63269ms step_avg:57.78ms
step:1096/2330 train_time:63330ms step_avg:57.78ms
step:1097/2330 train_time:63386ms step_avg:57.78ms
step:1098/2330 train_time:63447ms step_avg:57.78ms
step:1099/2330 train_time:63503ms step_avg:57.78ms
step:1100/2330 train_time:63563ms step_avg:57.78ms
step:1101/2330 train_time:63619ms step_avg:57.78ms
step:1102/2330 train_time:63680ms step_avg:57.79ms
step:1103/2330 train_time:63737ms step_avg:57.78ms
step:1104/2330 train_time:63798ms step_avg:57.79ms
step:1105/2330 train_time:63854ms step_avg:57.79ms
step:1106/2330 train_time:63914ms step_avg:57.79ms
step:1107/2330 train_time:63970ms step_avg:57.79ms
step:1108/2330 train_time:64031ms step_avg:57.79ms
step:1109/2330 train_time:64087ms step_avg:57.79ms
step:1110/2330 train_time:64148ms step_avg:57.79ms
step:1111/2330 train_time:64205ms step_avg:57.79ms
step:1112/2330 train_time:64265ms step_avg:57.79ms
step:1113/2330 train_time:64321ms step_avg:57.79ms
step:1114/2330 train_time:64382ms step_avg:57.79ms
step:1115/2330 train_time:64439ms step_avg:57.79ms
step:1116/2330 train_time:64498ms step_avg:57.79ms
step:1117/2330 train_time:64555ms step_avg:57.79ms
step:1118/2330 train_time:64615ms step_avg:57.80ms
step:1119/2330 train_time:64671ms step_avg:57.79ms
step:1120/2330 train_time:64732ms step_avg:57.80ms
step:1121/2330 train_time:64789ms step_avg:57.80ms
step:1122/2330 train_time:64850ms step_avg:57.80ms
step:1123/2330 train_time:64906ms step_avg:57.80ms
step:1124/2330 train_time:64968ms step_avg:57.80ms
step:1125/2330 train_time:65024ms step_avg:57.80ms
step:1126/2330 train_time:65086ms step_avg:57.80ms
step:1127/2330 train_time:65142ms step_avg:57.80ms
step:1128/2330 train_time:65203ms step_avg:57.80ms
step:1129/2330 train_time:65260ms step_avg:57.80ms
step:1130/2330 train_time:65319ms step_avg:57.80ms
step:1131/2330 train_time:65376ms step_avg:57.80ms
step:1132/2330 train_time:65436ms step_avg:57.81ms
step:1133/2330 train_time:65492ms step_avg:57.80ms
step:1134/2330 train_time:65552ms step_avg:57.81ms
step:1135/2330 train_time:65608ms step_avg:57.80ms
step:1136/2330 train_time:65669ms step_avg:57.81ms
step:1137/2330 train_time:65726ms step_avg:57.81ms
step:1138/2330 train_time:65786ms step_avg:57.81ms
step:1139/2330 train_time:65842ms step_avg:57.81ms
step:1140/2330 train_time:65904ms step_avg:57.81ms
step:1141/2330 train_time:65961ms step_avg:57.81ms
step:1142/2330 train_time:66020ms step_avg:57.81ms
step:1143/2330 train_time:66076ms step_avg:57.81ms
step:1144/2330 train_time:66137ms step_avg:57.81ms
step:1145/2330 train_time:66193ms step_avg:57.81ms
step:1146/2330 train_time:66701ms step_avg:58.20ms
step:1147/2330 train_time:66756ms step_avg:58.20ms
step:1148/2330 train_time:66816ms step_avg:58.20ms
step:1149/2330 train_time:66872ms step_avg:58.20ms
step:1150/2330 train_time:66931ms step_avg:58.20ms
step:1151/2330 train_time:66986ms step_avg:58.20ms
step:1152/2330 train_time:67047ms step_avg:58.20ms
step:1153/2330 train_time:67102ms step_avg:58.20ms
step:1154/2330 train_time:67162ms step_avg:58.20ms
step:1155/2330 train_time:67218ms step_avg:58.20ms
step:1156/2330 train_time:67277ms step_avg:58.20ms
step:1157/2330 train_time:67333ms step_avg:58.20ms
step:1158/2330 train_time:67392ms step_avg:58.20ms
step:1159/2330 train_time:67448ms step_avg:58.19ms
step:1160/2330 train_time:67507ms step_avg:58.20ms
step:1161/2330 train_time:67567ms step_avg:58.20ms
step:1162/2330 train_time:67631ms step_avg:58.20ms
step:1163/2330 train_time:67689ms step_avg:58.20ms
step:1164/2330 train_time:67753ms step_avg:58.21ms
step:1165/2330 train_time:67809ms step_avg:58.21ms
step:1166/2330 train_time:67871ms step_avg:58.21ms
step:1167/2330 train_time:67927ms step_avg:58.21ms
step:1168/2330 train_time:67987ms step_avg:58.21ms
step:1169/2330 train_time:68043ms step_avg:58.21ms
step:1170/2330 train_time:68103ms step_avg:58.21ms
step:1171/2330 train_time:68159ms step_avg:58.21ms
step:1172/2330 train_time:68219ms step_avg:58.21ms
step:1173/2330 train_time:68275ms step_avg:58.21ms
step:1174/2330 train_time:68334ms step_avg:58.21ms
step:1175/2330 train_time:68390ms step_avg:58.20ms
step:1176/2330 train_time:68450ms step_avg:58.21ms
step:1177/2330 train_time:68507ms step_avg:58.20ms
step:1178/2330 train_time:68567ms step_avg:58.21ms
step:1179/2330 train_time:68624ms step_avg:58.21ms
step:1180/2330 train_time:68687ms step_avg:58.21ms
step:1181/2330 train_time:68745ms step_avg:58.21ms
step:1182/2330 train_time:68805ms step_avg:58.21ms
step:1183/2330 train_time:68862ms step_avg:58.21ms
step:1184/2330 train_time:68922ms step_avg:58.21ms
step:1185/2330 train_time:68979ms step_avg:58.21ms
step:1186/2330 train_time:69039ms step_avg:58.21ms
step:1187/2330 train_time:69095ms step_avg:58.21ms
step:1188/2330 train_time:69155ms step_avg:58.21ms
step:1189/2330 train_time:69210ms step_avg:58.21ms
step:1190/2330 train_time:69270ms step_avg:58.21ms
step:1191/2330 train_time:69326ms step_avg:58.21ms
step:1192/2330 train_time:69387ms step_avg:58.21ms
step:1193/2330 train_time:69443ms step_avg:58.21ms
step:1194/2330 train_time:69504ms step_avg:58.21ms
step:1195/2330 train_time:69561ms step_avg:58.21ms
step:1196/2330 train_time:69622ms step_avg:58.21ms
step:1197/2330 train_time:69680ms step_avg:58.21ms
step:1198/2330 train_time:69740ms step_avg:58.21ms
step:1199/2330 train_time:69797ms step_avg:58.21ms
step:1200/2330 train_time:69858ms step_avg:58.21ms
step:1201/2330 train_time:69914ms step_avg:58.21ms
step:1202/2330 train_time:69974ms step_avg:58.21ms
step:1203/2330 train_time:70030ms step_avg:58.21ms
step:1204/2330 train_time:70091ms step_avg:58.22ms
step:1205/2330 train_time:70147ms step_avg:58.21ms
step:1206/2330 train_time:70207ms step_avg:58.22ms
step:1207/2330 train_time:70264ms step_avg:58.21ms
step:1208/2330 train_time:70323ms step_avg:58.21ms
step:1209/2330 train_time:70380ms step_avg:58.21ms
step:1210/2330 train_time:70440ms step_avg:58.21ms
step:1211/2330 train_time:70497ms step_avg:58.21ms
step:1212/2330 train_time:70556ms step_avg:58.21ms
step:1213/2330 train_time:70614ms step_avg:58.21ms
step:1214/2330 train_time:70675ms step_avg:58.22ms
step:1215/2330 train_time:70732ms step_avg:58.22ms
step:1216/2330 train_time:70793ms step_avg:58.22ms
step:1217/2330 train_time:70849ms step_avg:58.22ms
step:1218/2330 train_time:70910ms step_avg:58.22ms
step:1219/2330 train_time:70967ms step_avg:58.22ms
step:1220/2330 train_time:71028ms step_avg:58.22ms
step:1221/2330 train_time:71084ms step_avg:58.22ms
step:1222/2330 train_time:71145ms step_avg:58.22ms
step:1223/2330 train_time:71201ms step_avg:58.22ms
step:1224/2330 train_time:71262ms step_avg:58.22ms
step:1225/2330 train_time:71318ms step_avg:58.22ms
step:1226/2330 train_time:71378ms step_avg:58.22ms
step:1227/2330 train_time:71435ms step_avg:58.22ms
step:1228/2330 train_time:71494ms step_avg:58.22ms
step:1229/2330 train_time:71550ms step_avg:58.22ms
step:1230/2330 train_time:71611ms step_avg:58.22ms
step:1231/2330 train_time:71668ms step_avg:58.22ms
step:1232/2330 train_time:71728ms step_avg:58.22ms
step:1233/2330 train_time:71786ms step_avg:58.22ms
step:1234/2330 train_time:71847ms step_avg:58.22ms
step:1235/2330 train_time:71903ms step_avg:58.22ms
step:1236/2330 train_time:71964ms step_avg:58.22ms
step:1237/2330 train_time:72020ms step_avg:58.22ms
step:1238/2330 train_time:72081ms step_avg:58.22ms
step:1239/2330 train_time:72137ms step_avg:58.22ms
step:1240/2330 train_time:72197ms step_avg:58.22ms
step:1241/2330 train_time:72253ms step_avg:58.22ms
step:1242/2330 train_time:72313ms step_avg:58.22ms
step:1243/2330 train_time:72369ms step_avg:58.22ms
step:1244/2330 train_time:72430ms step_avg:58.22ms
step:1245/2330 train_time:72486ms step_avg:58.22ms
step:1246/2330 train_time:72548ms step_avg:58.22ms
step:1247/2330 train_time:72605ms step_avg:58.22ms
step:1248/2330 train_time:72665ms step_avg:58.23ms
step:1249/2330 train_time:72722ms step_avg:58.22ms
step:1250/2330 train_time:72782ms step_avg:58.23ms
step:1250/2330 val_loss:4.0234 train_time:72863ms step_avg:58.29ms
step:1251/2330 train_time:72883ms step_avg:58.26ms
step:1252/2330 train_time:72905ms step_avg:58.23ms
step:1253/2330 train_time:72960ms step_avg:58.23ms
step:1254/2330 train_time:73025ms step_avg:58.23ms
step:1255/2330 train_time:73082ms step_avg:58.23ms
step:1256/2330 train_time:73144ms step_avg:58.24ms
step:1257/2330 train_time:73200ms step_avg:58.23ms
step:1258/2330 train_time:73261ms step_avg:58.24ms
step:1259/2330 train_time:73317ms step_avg:58.23ms
step:1260/2330 train_time:73377ms step_avg:58.24ms
step:1261/2330 train_time:73434ms step_avg:58.23ms
step:1262/2330 train_time:73493ms step_avg:58.24ms
step:1263/2330 train_time:73549ms step_avg:58.23ms
step:1264/2330 train_time:73608ms step_avg:58.23ms
step:1265/2330 train_time:73664ms step_avg:58.23ms
step:1266/2330 train_time:73724ms step_avg:58.23ms
step:1267/2330 train_time:73780ms step_avg:58.23ms
step:1268/2330 train_time:73840ms step_avg:58.23ms
step:1269/2330 train_time:73899ms step_avg:58.23ms
step:1270/2330 train_time:73960ms step_avg:58.24ms
step:1271/2330 train_time:74018ms step_avg:58.24ms
step:1272/2330 train_time:74079ms step_avg:58.24ms
step:1273/2330 train_time:74137ms step_avg:58.24ms
step:1274/2330 train_time:74196ms step_avg:58.24ms
step:1275/2330 train_time:74253ms step_avg:58.24ms
step:1276/2330 train_time:74313ms step_avg:58.24ms
step:1277/2330 train_time:74370ms step_avg:58.24ms
step:1278/2330 train_time:74429ms step_avg:58.24ms
step:1279/2330 train_time:74485ms step_avg:58.24ms
step:1280/2330 train_time:74545ms step_avg:58.24ms
step:1281/2330 train_time:74601ms step_avg:58.24ms
step:1282/2330 train_time:74661ms step_avg:58.24ms
step:1283/2330 train_time:74717ms step_avg:58.24ms
step:1284/2330 train_time:74778ms step_avg:58.24ms
step:1285/2330 train_time:74834ms step_avg:58.24ms
step:1286/2330 train_time:74894ms step_avg:58.24ms
step:1287/2330 train_time:74952ms step_avg:58.24ms
step:1288/2330 train_time:75012ms step_avg:58.24ms
step:1289/2330 train_time:75070ms step_avg:58.24ms
step:1290/2330 train_time:75130ms step_avg:58.24ms
step:1291/2330 train_time:75188ms step_avg:58.24ms
step:1292/2330 train_time:75248ms step_avg:58.24ms
step:1293/2330 train_time:75305ms step_avg:58.24ms
step:1294/2330 train_time:75366ms step_avg:58.24ms
step:1295/2330 train_time:75423ms step_avg:58.24ms
step:1296/2330 train_time:75482ms step_avg:58.24ms
step:1297/2330 train_time:75539ms step_avg:58.24ms
step:1298/2330 train_time:75598ms step_avg:58.24ms
step:1299/2330 train_time:75655ms step_avg:58.24ms
step:1300/2330 train_time:75714ms step_avg:58.24ms
step:1301/2330 train_time:75771ms step_avg:58.24ms
step:1302/2330 train_time:75831ms step_avg:58.24ms
step:1303/2330 train_time:75888ms step_avg:58.24ms
step:1304/2330 train_time:75948ms step_avg:58.24ms
step:1305/2330 train_time:76005ms step_avg:58.24ms
step:1306/2330 train_time:76066ms step_avg:58.24ms
step:1307/2330 train_time:76124ms step_avg:58.24ms
step:1308/2330 train_time:76184ms step_avg:58.24ms
step:1309/2330 train_time:76241ms step_avg:58.24ms
step:1310/2330 train_time:76302ms step_avg:58.25ms
step:1311/2330 train_time:76358ms step_avg:58.24ms
step:1312/2330 train_time:76419ms step_avg:58.25ms
step:1313/2330 train_time:76475ms step_avg:58.24ms
step:1314/2330 train_time:76534ms step_avg:58.25ms
step:1315/2330 train_time:76591ms step_avg:58.24ms
step:1316/2330 train_time:76650ms step_avg:58.24ms
step:1317/2330 train_time:76707ms step_avg:58.24ms
step:1318/2330 train_time:76767ms step_avg:58.24ms
step:1319/2330 train_time:76823ms step_avg:58.24ms
step:1320/2330 train_time:76884ms step_avg:58.25ms
step:1321/2330 train_time:76941ms step_avg:58.24ms
step:1322/2330 train_time:77002ms step_avg:58.25ms
step:1323/2330 train_time:77059ms step_avg:58.25ms
step:1324/2330 train_time:77120ms step_avg:58.25ms
step:1325/2330 train_time:77178ms step_avg:58.25ms
step:1326/2330 train_time:77238ms step_avg:58.25ms
step:1327/2330 train_time:77295ms step_avg:58.25ms
step:1328/2330 train_time:77355ms step_avg:58.25ms
step:1329/2330 train_time:77411ms step_avg:58.25ms
step:1330/2330 train_time:77471ms step_avg:58.25ms
step:1331/2330 train_time:77527ms step_avg:58.25ms
step:1332/2330 train_time:77587ms step_avg:58.25ms
step:1333/2330 train_time:77644ms step_avg:58.25ms
step:1334/2330 train_time:77704ms step_avg:58.25ms
step:1335/2330 train_time:77760ms step_avg:58.25ms
step:1336/2330 train_time:77821ms step_avg:58.25ms
step:1337/2330 train_time:77877ms step_avg:58.25ms
step:1338/2330 train_time:77938ms step_avg:58.25ms
step:1339/2330 train_time:77995ms step_avg:58.25ms
step:1340/2330 train_time:78055ms step_avg:58.25ms
step:1341/2330 train_time:78112ms step_avg:58.25ms
step:1342/2330 train_time:78173ms step_avg:58.25ms
step:1343/2330 train_time:78230ms step_avg:58.25ms
step:1344/2330 train_time:78290ms step_avg:58.25ms
step:1345/2330 train_time:78347ms step_avg:58.25ms
step:1346/2330 train_time:78407ms step_avg:58.25ms
step:1347/2330 train_time:78463ms step_avg:58.25ms
step:1348/2330 train_time:78523ms step_avg:58.25ms
step:1349/2330 train_time:78580ms step_avg:58.25ms
step:1350/2330 train_time:78642ms step_avg:58.25ms
step:1351/2330 train_time:78698ms step_avg:58.25ms
step:1352/2330 train_time:78758ms step_avg:58.25ms
step:1353/2330 train_time:78815ms step_avg:58.25ms
step:1354/2330 train_time:78875ms step_avg:58.25ms
step:1355/2330 train_time:78933ms step_avg:58.25ms
step:1356/2330 train_time:78992ms step_avg:58.25ms
step:1357/2330 train_time:79049ms step_avg:58.25ms
step:1358/2330 train_time:79109ms step_avg:58.25ms
step:1359/2330 train_time:79166ms step_avg:58.25ms
step:1360/2330 train_time:79226ms step_avg:58.25ms
step:1361/2330 train_time:79283ms step_avg:58.25ms
step:1362/2330 train_time:79344ms step_avg:58.26ms
step:1363/2330 train_time:79400ms step_avg:58.25ms
step:1364/2330 train_time:79460ms step_avg:58.26ms
step:1365/2330 train_time:79517ms step_avg:58.25ms
step:1366/2330 train_time:79578ms step_avg:58.26ms
step:1367/2330 train_time:79636ms step_avg:58.26ms
step:1368/2330 train_time:79696ms step_avg:58.26ms
step:1369/2330 train_time:79753ms step_avg:58.26ms
step:1370/2330 train_time:79812ms step_avg:58.26ms
step:1371/2330 train_time:79869ms step_avg:58.26ms
step:1372/2330 train_time:79929ms step_avg:58.26ms
step:1373/2330 train_time:79986ms step_avg:58.26ms
step:1374/2330 train_time:80045ms step_avg:58.26ms
step:1375/2330 train_time:80101ms step_avg:58.26ms
step:1376/2330 train_time:80162ms step_avg:58.26ms
step:1377/2330 train_time:80219ms step_avg:58.26ms
step:1378/2330 train_time:80280ms step_avg:58.26ms
step:1379/2330 train_time:80337ms step_avg:58.26ms
step:1380/2330 train_time:80397ms step_avg:58.26ms
step:1381/2330 train_time:80453ms step_avg:58.26ms
step:1382/2330 train_time:80513ms step_avg:58.26ms
step:1383/2330 train_time:80569ms step_avg:58.26ms
step:1384/2330 train_time:80631ms step_avg:58.26ms
step:1385/2330 train_time:80688ms step_avg:58.26ms
step:1386/2330 train_time:80748ms step_avg:58.26ms
step:1387/2330 train_time:80804ms step_avg:58.26ms
step:1388/2330 train_time:80864ms step_avg:58.26ms
step:1389/2330 train_time:80921ms step_avg:58.26ms
step:1390/2330 train_time:80981ms step_avg:58.26ms
step:1391/2330 train_time:81037ms step_avg:58.26ms
step:1392/2330 train_time:81098ms step_avg:58.26ms
step:1393/2330 train_time:81155ms step_avg:58.26ms
step:1394/2330 train_time:81214ms step_avg:58.26ms
step:1395/2330 train_time:81271ms step_avg:58.26ms
step:1396/2330 train_time:81331ms step_avg:58.26ms
step:1397/2330 train_time:81387ms step_avg:58.26ms
step:1398/2330 train_time:81448ms step_avg:58.26ms
step:1399/2330 train_time:81504ms step_avg:58.26ms
step:1400/2330 train_time:81565ms step_avg:58.26ms
step:1401/2330 train_time:81621ms step_avg:58.26ms
step:1402/2330 train_time:81683ms step_avg:58.26ms
step:1403/2330 train_time:81739ms step_avg:58.26ms
step:1404/2330 train_time:81800ms step_avg:58.26ms
step:1405/2330 train_time:81857ms step_avg:58.26ms
step:1406/2330 train_time:81916ms step_avg:58.26ms
step:1407/2330 train_time:81972ms step_avg:58.26ms
step:1408/2330 train_time:82032ms step_avg:58.26ms
step:1409/2330 train_time:82088ms step_avg:58.26ms
step:1410/2330 train_time:82149ms step_avg:58.26ms
step:1411/2330 train_time:82206ms step_avg:58.26ms
step:1412/2330 train_time:82266ms step_avg:58.26ms
step:1413/2330 train_time:82323ms step_avg:58.26ms
step:1414/2330 train_time:82383ms step_avg:58.26ms
step:1415/2330 train_time:82440ms step_avg:58.26ms
step:1416/2330 train_time:82500ms step_avg:58.26ms
step:1417/2330 train_time:82557ms step_avg:58.26ms
step:1418/2330 train_time:82618ms step_avg:58.26ms
step:1419/2330 train_time:82674ms step_avg:58.26ms
step:1420/2330 train_time:82734ms step_avg:58.26ms
step:1421/2330 train_time:82791ms step_avg:58.26ms
step:1422/2330 train_time:82850ms step_avg:58.26ms
step:1423/2330 train_time:82907ms step_avg:58.26ms
step:1424/2330 train_time:82967ms step_avg:58.26ms
step:1425/2330 train_time:83023ms step_avg:58.26ms
step:1426/2330 train_time:83084ms step_avg:58.26ms
step:1427/2330 train_time:83140ms step_avg:58.26ms
step:1428/2330 train_time:83201ms step_avg:58.26ms
step:1429/2330 train_time:83258ms step_avg:58.26ms
step:1430/2330 train_time:83318ms step_avg:58.26ms
step:1431/2330 train_time:83374ms step_avg:58.26ms
step:1432/2330 train_time:83434ms step_avg:58.26ms
step:1433/2330 train_time:83491ms step_avg:58.26ms
step:1434/2330 train_time:83551ms step_avg:58.26ms
step:1435/2330 train_time:83608ms step_avg:58.26ms
step:1436/2330 train_time:83667ms step_avg:58.26ms
step:1437/2330 train_time:83724ms step_avg:58.26ms
step:1438/2330 train_time:83785ms step_avg:58.26ms
step:1439/2330 train_time:83842ms step_avg:58.26ms
step:1440/2330 train_time:83902ms step_avg:58.27ms
step:1441/2330 train_time:83958ms step_avg:58.26ms
step:1442/2330 train_time:84019ms step_avg:58.27ms
step:1443/2330 train_time:84075ms step_avg:58.26ms
step:1444/2330 train_time:84136ms step_avg:58.27ms
step:1445/2330 train_time:84193ms step_avg:58.26ms
step:1446/2330 train_time:84253ms step_avg:58.27ms
step:1447/2330 train_time:84310ms step_avg:58.27ms
step:1448/2330 train_time:84369ms step_avg:58.27ms
step:1449/2330 train_time:84425ms step_avg:58.26ms
step:1450/2330 train_time:84486ms step_avg:58.27ms
step:1451/2330 train_time:84543ms step_avg:58.27ms
step:1452/2330 train_time:84604ms step_avg:58.27ms
step:1453/2330 train_time:84661ms step_avg:58.27ms
step:1454/2330 train_time:84722ms step_avg:58.27ms
step:1455/2330 train_time:84778ms step_avg:58.27ms
step:1456/2330 train_time:84840ms step_avg:58.27ms
step:1457/2330 train_time:84896ms step_avg:58.27ms
step:1458/2330 train_time:84956ms step_avg:58.27ms
step:1459/2330 train_time:85012ms step_avg:58.27ms
step:1460/2330 train_time:85072ms step_avg:58.27ms
step:1461/2330 train_time:85130ms step_avg:58.27ms
step:1462/2330 train_time:85190ms step_avg:58.27ms
step:1463/2330 train_time:85248ms step_avg:58.27ms
step:1464/2330 train_time:85307ms step_avg:58.27ms
step:1465/2330 train_time:85364ms step_avg:58.27ms
step:1466/2330 train_time:85424ms step_avg:58.27ms
step:1467/2330 train_time:85482ms step_avg:58.27ms
step:1468/2330 train_time:85541ms step_avg:58.27ms
step:1469/2330 train_time:85598ms step_avg:58.27ms
step:1470/2330 train_time:85657ms step_avg:58.27ms
step:1471/2330 train_time:85715ms step_avg:58.27ms
step:1472/2330 train_time:85774ms step_avg:58.27ms
step:1473/2330 train_time:85830ms step_avg:58.27ms
step:1474/2330 train_time:85891ms step_avg:58.27ms
step:1475/2330 train_time:85948ms step_avg:58.27ms
step:1476/2330 train_time:86008ms step_avg:58.27ms
step:1477/2330 train_time:86064ms step_avg:58.27ms
step:1478/2330 train_time:86125ms step_avg:58.27ms
step:1479/2330 train_time:86182ms step_avg:58.27ms
step:1480/2330 train_time:86242ms step_avg:58.27ms
step:1481/2330 train_time:86299ms step_avg:58.27ms
step:1482/2330 train_time:86360ms step_avg:58.27ms
step:1483/2330 train_time:86416ms step_avg:58.27ms
step:1484/2330 train_time:86477ms step_avg:58.27ms
step:1485/2330 train_time:86533ms step_avg:58.27ms
step:1486/2330 train_time:86593ms step_avg:58.27ms
step:1487/2330 train_time:86650ms step_avg:58.27ms
step:1488/2330 train_time:86709ms step_avg:58.27ms
step:1489/2330 train_time:86766ms step_avg:58.27ms
step:1490/2330 train_time:86825ms step_avg:58.27ms
step:1491/2330 train_time:86882ms step_avg:58.27ms
step:1492/2330 train_time:86942ms step_avg:58.27ms
step:1493/2330 train_time:87000ms step_avg:58.27ms
step:1494/2330 train_time:87059ms step_avg:58.27ms
step:1495/2330 train_time:87116ms step_avg:58.27ms
step:1496/2330 train_time:87176ms step_avg:58.27ms
step:1497/2330 train_time:87233ms step_avg:58.27ms
step:1498/2330 train_time:87293ms step_avg:58.27ms
step:1499/2330 train_time:87350ms step_avg:58.27ms
step:1500/2330 train_time:87411ms step_avg:58.27ms
step:1500/2330 val_loss:3.9376 train_time:87490ms step_avg:58.33ms
step:1501/2330 train_time:87511ms step_avg:58.30ms
step:1502/2330 train_time:87532ms step_avg:58.28ms
step:1503/2330 train_time:87588ms step_avg:58.28ms
step:1504/2330 train_time:87653ms step_avg:58.28ms
step:1505/2330 train_time:87709ms step_avg:58.28ms
step:1506/2330 train_time:87770ms step_avg:58.28ms
step:1507/2330 train_time:87828ms step_avg:58.28ms
step:1508/2330 train_time:87887ms step_avg:58.28ms
step:1509/2330 train_time:87943ms step_avg:58.28ms
step:1510/2330 train_time:88002ms step_avg:58.28ms
step:1511/2330 train_time:88058ms step_avg:58.28ms
step:1512/2330 train_time:88118ms step_avg:58.28ms
step:1513/2330 train_time:88173ms step_avg:58.28ms
step:1514/2330 train_time:88233ms step_avg:58.28ms
step:1515/2330 train_time:88289ms step_avg:58.28ms
step:1516/2330 train_time:88349ms step_avg:58.28ms
step:1517/2330 train_time:88406ms step_avg:58.28ms
step:1518/2330 train_time:88465ms step_avg:58.28ms
step:1519/2330 train_time:88524ms step_avg:58.28ms
step:1520/2330 train_time:88586ms step_avg:58.28ms
step:1521/2330 train_time:88644ms step_avg:58.28ms
step:1522/2330 train_time:88705ms step_avg:58.28ms
step:1523/2330 train_time:88761ms step_avg:58.28ms
step:1524/2330 train_time:88824ms step_avg:58.28ms
step:1525/2330 train_time:88881ms step_avg:58.28ms
step:1526/2330 train_time:88941ms step_avg:58.28ms
step:1527/2330 train_time:88997ms step_avg:58.28ms
step:1528/2330 train_time:89057ms step_avg:58.28ms
step:1529/2330 train_time:89115ms step_avg:58.28ms
step:1530/2330 train_time:89174ms step_avg:58.28ms
step:1531/2330 train_time:89230ms step_avg:58.28ms
step:1532/2330 train_time:89290ms step_avg:58.28ms
step:1533/2330 train_time:89346ms step_avg:58.28ms
step:1534/2330 train_time:89407ms step_avg:58.28ms
step:1535/2330 train_time:89465ms step_avg:58.28ms
step:1536/2330 train_time:89526ms step_avg:58.28ms
step:1537/2330 train_time:89584ms step_avg:58.28ms
step:1538/2330 train_time:89645ms step_avg:58.29ms
step:1539/2330 train_time:89703ms step_avg:58.29ms
step:1540/2330 train_time:89764ms step_avg:58.29ms
step:1541/2330 train_time:89823ms step_avg:58.29ms
step:1542/2330 train_time:89884ms step_avg:58.29ms
step:1543/2330 train_time:89941ms step_avg:58.29ms
step:1544/2330 train_time:90001ms step_avg:58.29ms
step:1545/2330 train_time:90058ms step_avg:58.29ms
step:1546/2330 train_time:90119ms step_avg:58.29ms
step:1547/2330 train_time:90176ms step_avg:58.29ms
step:1548/2330 train_time:90237ms step_avg:58.29ms
step:1549/2330 train_time:90294ms step_avg:58.29ms
step:1550/2330 train_time:90355ms step_avg:58.29ms
step:1551/2330 train_time:90411ms step_avg:58.29ms
step:1552/2330 train_time:90473ms step_avg:58.29ms
step:1553/2330 train_time:90530ms step_avg:58.29ms
step:1554/2330 train_time:90591ms step_avg:58.30ms
step:1555/2330 train_time:90648ms step_avg:58.29ms
step:1556/2330 train_time:90709ms step_avg:58.30ms
step:1557/2330 train_time:90768ms step_avg:58.30ms
step:1558/2330 train_time:90829ms step_avg:58.30ms
step:1559/2330 train_time:90887ms step_avg:58.30ms
step:1560/2330 train_time:90947ms step_avg:58.30ms
step:1561/2330 train_time:91006ms step_avg:58.30ms
step:1562/2330 train_time:91067ms step_avg:58.30ms
step:1563/2330 train_time:91125ms step_avg:58.30ms
step:1564/2330 train_time:91185ms step_avg:58.30ms
step:1565/2330 train_time:91242ms step_avg:58.30ms
step:1566/2330 train_time:91302ms step_avg:58.30ms
step:1567/2330 train_time:91358ms step_avg:58.30ms
step:1568/2330 train_time:91421ms step_avg:58.30ms
step:1569/2330 train_time:91477ms step_avg:58.30ms
step:1570/2330 train_time:91539ms step_avg:58.31ms
step:1571/2330 train_time:91595ms step_avg:58.30ms
step:1572/2330 train_time:91658ms step_avg:58.31ms
step:1573/2330 train_time:91715ms step_avg:58.31ms
step:1574/2330 train_time:91777ms step_avg:58.31ms
step:1575/2330 train_time:91834ms step_avg:58.31ms
step:1576/2330 train_time:91896ms step_avg:58.31ms
step:1577/2330 train_time:91953ms step_avg:58.31ms
step:1578/2330 train_time:92014ms step_avg:58.31ms
step:1579/2330 train_time:92072ms step_avg:58.31ms
step:1580/2330 train_time:92132ms step_avg:58.31ms
step:1581/2330 train_time:92190ms step_avg:58.31ms
step:1582/2330 train_time:92249ms step_avg:58.31ms
step:1583/2330 train_time:92307ms step_avg:58.31ms
step:1584/2330 train_time:92368ms step_avg:58.31ms
step:1585/2330 train_time:92426ms step_avg:58.31ms
step:1586/2330 train_time:92486ms step_avg:58.31ms
step:1587/2330 train_time:92543ms step_avg:58.31ms
step:1588/2330 train_time:92603ms step_avg:58.31ms
step:1589/2330 train_time:92661ms step_avg:58.31ms
step:1590/2330 train_time:92722ms step_avg:58.32ms
step:1591/2330 train_time:92779ms step_avg:58.31ms
step:1592/2330 train_time:92841ms step_avg:58.32ms
step:1593/2330 train_time:92897ms step_avg:58.32ms
step:1594/2330 train_time:92960ms step_avg:58.32ms
step:1595/2330 train_time:93017ms step_avg:58.32ms
step:1596/2330 train_time:93078ms step_avg:58.32ms
step:1597/2330 train_time:93135ms step_avg:58.32ms
step:1598/2330 train_time:93197ms step_avg:58.32ms
step:1599/2330 train_time:93253ms step_avg:58.32ms
step:1600/2330 train_time:93315ms step_avg:58.32ms
step:1601/2330 train_time:93372ms step_avg:58.32ms
step:1602/2330 train_time:93433ms step_avg:58.32ms
step:1603/2330 train_time:93490ms step_avg:58.32ms
step:1604/2330 train_time:93550ms step_avg:58.32ms
step:1605/2330 train_time:93607ms step_avg:58.32ms
step:1606/2330 train_time:93667ms step_avg:58.32ms
step:1607/2330 train_time:93726ms step_avg:58.32ms
step:1608/2330 train_time:93786ms step_avg:58.32ms
step:1609/2330 train_time:93844ms step_avg:58.32ms
step:1610/2330 train_time:93905ms step_avg:58.33ms
step:1611/2330 train_time:93961ms step_avg:58.32ms
step:1612/2330 train_time:94022ms step_avg:58.33ms
step:1613/2330 train_time:94080ms step_avg:58.33ms
step:1614/2330 train_time:94140ms step_avg:58.33ms
step:1615/2330 train_time:94197ms step_avg:58.33ms
step:1616/2330 train_time:94259ms step_avg:58.33ms
step:1617/2330 train_time:94315ms step_avg:58.33ms
step:1618/2330 train_time:94377ms step_avg:58.33ms
step:1619/2330 train_time:94434ms step_avg:58.33ms
step:1620/2330 train_time:94496ms step_avg:58.33ms
step:1621/2330 train_time:94552ms step_avg:58.33ms
step:1622/2330 train_time:94614ms step_avg:58.33ms
step:1623/2330 train_time:94671ms step_avg:58.33ms
step:1624/2330 train_time:94732ms step_avg:58.33ms
step:1625/2330 train_time:94789ms step_avg:58.33ms
step:1626/2330 train_time:94850ms step_avg:58.33ms
step:1627/2330 train_time:94907ms step_avg:58.33ms
step:1628/2330 train_time:94967ms step_avg:58.33ms
step:1629/2330 train_time:95025ms step_avg:58.33ms
step:1630/2330 train_time:95086ms step_avg:58.33ms
step:1631/2330 train_time:95144ms step_avg:58.33ms
step:1632/2330 train_time:95204ms step_avg:58.34ms
step:1633/2330 train_time:95262ms step_avg:58.34ms
step:1634/2330 train_time:95322ms step_avg:58.34ms
step:1635/2330 train_time:95379ms step_avg:58.34ms
step:1636/2330 train_time:95440ms step_avg:58.34ms
step:1637/2330 train_time:95497ms step_avg:58.34ms
step:1638/2330 train_time:95559ms step_avg:58.34ms
step:1639/2330 train_time:95616ms step_avg:58.34ms
step:1640/2330 train_time:95678ms step_avg:58.34ms
step:1641/2330 train_time:95735ms step_avg:58.34ms
step:1642/2330 train_time:95796ms step_avg:58.34ms
step:1643/2330 train_time:95853ms step_avg:58.34ms
step:1644/2330 train_time:95916ms step_avg:58.34ms
step:1645/2330 train_time:95973ms step_avg:58.34ms
step:1646/2330 train_time:96034ms step_avg:58.34ms
step:1647/2330 train_time:96091ms step_avg:58.34ms
step:1648/2330 train_time:96152ms step_avg:58.34ms
step:1649/2330 train_time:96210ms step_avg:58.34ms
step:1650/2330 train_time:96271ms step_avg:58.35ms
step:1651/2330 train_time:96329ms step_avg:58.35ms
step:1652/2330 train_time:96389ms step_avg:58.35ms
step:1653/2330 train_time:96447ms step_avg:58.35ms
step:1654/2330 train_time:96508ms step_avg:58.35ms
step:1655/2330 train_time:96565ms step_avg:58.35ms
step:1656/2330 train_time:96625ms step_avg:58.35ms
step:1657/2330 train_time:96683ms step_avg:58.35ms
step:1658/2330 train_time:96743ms step_avg:58.35ms
step:1659/2330 train_time:96801ms step_avg:58.35ms
step:1660/2330 train_time:96862ms step_avg:58.35ms
step:1661/2330 train_time:96919ms step_avg:58.35ms
step:1662/2330 train_time:96980ms step_avg:58.35ms
step:1663/2330 train_time:97037ms step_avg:58.35ms
step:1664/2330 train_time:97098ms step_avg:58.35ms
step:1665/2330 train_time:97154ms step_avg:58.35ms
step:1666/2330 train_time:97217ms step_avg:58.35ms
step:1667/2330 train_time:97273ms step_avg:58.35ms
step:1668/2330 train_time:97335ms step_avg:58.35ms
step:1669/2330 train_time:97391ms step_avg:58.35ms
step:1670/2330 train_time:97453ms step_avg:58.35ms
step:1671/2330 train_time:97510ms step_avg:58.35ms
step:1672/2330 train_time:97571ms step_avg:58.36ms
step:1673/2330 train_time:97628ms step_avg:58.35ms
step:1674/2330 train_time:97688ms step_avg:58.36ms
step:1675/2330 train_time:97746ms step_avg:58.36ms
step:1676/2330 train_time:97806ms step_avg:58.36ms
step:1677/2330 train_time:97864ms step_avg:58.36ms
step:1678/2330 train_time:97923ms step_avg:58.36ms
step:1679/2330 train_time:97982ms step_avg:58.36ms
step:1680/2330 train_time:98042ms step_avg:58.36ms
step:1681/2330 train_time:98099ms step_avg:58.36ms
step:1682/2330 train_time:98160ms step_avg:58.36ms
step:1683/2330 train_time:98216ms step_avg:58.36ms
step:1684/2330 train_time:98279ms step_avg:58.36ms
step:1685/2330 train_time:98335ms step_avg:58.36ms
step:1686/2330 train_time:98398ms step_avg:58.36ms
step:1687/2330 train_time:98454ms step_avg:58.36ms
step:1688/2330 train_time:98518ms step_avg:58.36ms
step:1689/2330 train_time:98575ms step_avg:58.36ms
step:1690/2330 train_time:98636ms step_avg:58.36ms
step:1691/2330 train_time:98692ms step_avg:58.36ms
step:1692/2330 train_time:98754ms step_avg:58.37ms
step:1693/2330 train_time:98811ms step_avg:58.36ms
step:1694/2330 train_time:98872ms step_avg:58.37ms
step:1695/2330 train_time:98930ms step_avg:58.37ms
step:1696/2330 train_time:98990ms step_avg:58.37ms
step:1697/2330 train_time:99047ms step_avg:58.37ms
step:1698/2330 train_time:99108ms step_avg:58.37ms
step:1699/2330 train_time:99166ms step_avg:58.37ms
step:1700/2330 train_time:99227ms step_avg:58.37ms
step:1701/2330 train_time:99285ms step_avg:58.37ms
step:1702/2330 train_time:99345ms step_avg:58.37ms
step:1703/2330 train_time:99403ms step_avg:58.37ms
step:1704/2330 train_time:99463ms step_avg:58.37ms
step:1705/2330 train_time:99520ms step_avg:58.37ms
step:1706/2330 train_time:99581ms step_avg:58.37ms
step:1707/2330 train_time:99637ms step_avg:58.37ms
step:1708/2330 train_time:99699ms step_avg:58.37ms
step:1709/2330 train_time:99756ms step_avg:58.37ms
step:1710/2330 train_time:99818ms step_avg:58.37ms
step:1711/2330 train_time:99874ms step_avg:58.37ms
step:1712/2330 train_time:99936ms step_avg:58.37ms
step:1713/2330 train_time:99993ms step_avg:58.37ms
step:1714/2330 train_time:100055ms step_avg:58.37ms
step:1715/2330 train_time:100111ms step_avg:58.37ms
step:1716/2330 train_time:100173ms step_avg:58.38ms
step:1717/2330 train_time:100230ms step_avg:58.38ms
step:1718/2330 train_time:100290ms step_avg:58.38ms
step:1719/2330 train_time:100348ms step_avg:58.38ms
step:1720/2330 train_time:100408ms step_avg:58.38ms
step:1721/2330 train_time:100466ms step_avg:58.38ms
step:1722/2330 train_time:100527ms step_avg:58.38ms
step:1723/2330 train_time:100585ms step_avg:58.38ms
step:1724/2330 train_time:100646ms step_avg:58.38ms
step:1725/2330 train_time:100703ms step_avg:58.38ms
step:1726/2330 train_time:100764ms step_avg:58.38ms
step:1727/2330 train_time:100823ms step_avg:58.38ms
step:1728/2330 train_time:100883ms step_avg:58.38ms
step:1729/2330 train_time:100941ms step_avg:58.38ms
step:1730/2330 train_time:101002ms step_avg:58.38ms
step:1731/2330 train_time:101059ms step_avg:58.38ms
step:1732/2330 train_time:101120ms step_avg:58.38ms
step:1733/2330 train_time:101176ms step_avg:58.38ms
step:1734/2330 train_time:101239ms step_avg:58.38ms
step:1735/2330 train_time:101295ms step_avg:58.38ms
step:1736/2330 train_time:101357ms step_avg:58.39ms
step:1737/2330 train_time:101413ms step_avg:58.38ms
step:1738/2330 train_time:101475ms step_avg:58.39ms
step:1739/2330 train_time:101531ms step_avg:58.38ms
step:1740/2330 train_time:101594ms step_avg:58.39ms
step:1741/2330 train_time:101650ms step_avg:58.39ms
step:1742/2330 train_time:101711ms step_avg:58.39ms
step:1743/2330 train_time:101768ms step_avg:58.39ms
step:1744/2330 train_time:101829ms step_avg:58.39ms
step:1745/2330 train_time:101886ms step_avg:58.39ms
step:1746/2330 train_time:101947ms step_avg:58.39ms
step:1747/2330 train_time:102005ms step_avg:58.39ms
step:1748/2330 train_time:102066ms step_avg:58.39ms
step:1749/2330 train_time:102125ms step_avg:58.39ms
step:1750/2330 train_time:102186ms step_avg:58.39ms
step:1750/2330 val_loss:3.8544 train_time:102268ms step_avg:58.44ms
step:1751/2330 train_time:102287ms step_avg:58.42ms
step:1752/2330 train_time:102307ms step_avg:58.39ms
step:1753/2330 train_time:102367ms step_avg:58.40ms
step:1754/2330 train_time:102433ms step_avg:58.40ms
step:1755/2330 train_time:102491ms step_avg:58.40ms
step:1756/2330 train_time:102553ms step_avg:58.40ms
step:1757/2330 train_time:102610ms step_avg:58.40ms
step:1758/2330 train_time:102670ms step_avg:58.40ms
step:1759/2330 train_time:102727ms step_avg:58.40ms
step:1760/2330 train_time:102787ms step_avg:58.40ms
step:1761/2330 train_time:102844ms step_avg:58.40ms
step:1762/2330 train_time:102904ms step_avg:58.40ms
step:1763/2330 train_time:102960ms step_avg:58.40ms
step:1764/2330 train_time:103021ms step_avg:58.40ms
step:1765/2330 train_time:103078ms step_avg:58.40ms
step:1766/2330 train_time:103138ms step_avg:58.40ms
step:1767/2330 train_time:103195ms step_avg:58.40ms
step:1768/2330 train_time:103255ms step_avg:58.40ms
step:1769/2330 train_time:103315ms step_avg:58.40ms
step:1770/2330 train_time:103376ms step_avg:58.40ms
step:1771/2330 train_time:103435ms step_avg:58.40ms
step:1772/2330 train_time:103495ms step_avg:58.41ms
step:1773/2330 train_time:103552ms step_avg:58.40ms
step:1774/2330 train_time:103613ms step_avg:58.41ms
step:1775/2330 train_time:103670ms step_avg:58.41ms
step:1776/2330 train_time:103731ms step_avg:58.41ms
step:1777/2330 train_time:103788ms step_avg:58.41ms
step:1778/2330 train_time:103848ms step_avg:58.41ms
step:1779/2330 train_time:103905ms step_avg:58.41ms
step:1780/2330 train_time:103965ms step_avg:58.41ms
step:1781/2330 train_time:104022ms step_avg:58.41ms
step:1782/2330 train_time:104082ms step_avg:58.41ms
step:1783/2330 train_time:104138ms step_avg:58.41ms
step:1784/2330 train_time:104198ms step_avg:58.41ms
step:1785/2330 train_time:104255ms step_avg:58.41ms
step:1786/2330 train_time:104317ms step_avg:58.41ms
step:1787/2330 train_time:104376ms step_avg:58.41ms
step:1788/2330 train_time:104436ms step_avg:58.41ms
step:1789/2330 train_time:104493ms step_avg:58.41ms
step:1790/2330 train_time:104555ms step_avg:58.41ms
step:1791/2330 train_time:104612ms step_avg:58.41ms
step:1792/2330 train_time:104672ms step_avg:58.41ms
step:1793/2330 train_time:104729ms step_avg:58.41ms
step:1794/2330 train_time:104789ms step_avg:58.41ms
step:1795/2330 train_time:104846ms step_avg:58.41ms
step:1796/2330 train_time:104906ms step_avg:58.41ms
step:1797/2330 train_time:104963ms step_avg:58.41ms
step:1798/2330 train_time:105024ms step_avg:58.41ms
step:1799/2330 train_time:105080ms step_avg:58.41ms
step:1800/2330 train_time:105141ms step_avg:58.41ms
step:1801/2330 train_time:105198ms step_avg:58.41ms
step:1802/2330 train_time:105260ms step_avg:58.41ms
step:1803/2330 train_time:105317ms step_avg:58.41ms
step:1804/2330 train_time:105378ms step_avg:58.41ms
step:1805/2330 train_time:105434ms step_avg:58.41ms
step:1806/2330 train_time:105496ms step_avg:58.41ms
step:1807/2330 train_time:105553ms step_avg:58.41ms
step:1808/2330 train_time:105614ms step_avg:58.41ms
step:1809/2330 train_time:105671ms step_avg:58.41ms
step:1810/2330 train_time:105732ms step_avg:58.42ms
step:1811/2330 train_time:105790ms step_avg:58.42ms
step:1812/2330 train_time:105850ms step_avg:58.42ms
step:1813/2330 train_time:105908ms step_avg:58.42ms
step:1814/2330 train_time:105968ms step_avg:58.42ms
step:1815/2330 train_time:106025ms step_avg:58.42ms
step:1816/2330 train_time:106086ms step_avg:58.42ms
step:1817/2330 train_time:106143ms step_avg:58.42ms
step:1818/2330 train_time:106204ms step_avg:58.42ms
step:1819/2330 train_time:106261ms step_avg:58.42ms
step:1820/2330 train_time:106323ms step_avg:58.42ms
step:1821/2330 train_time:106379ms step_avg:58.42ms
step:1822/2330 train_time:106441ms step_avg:58.42ms
step:1823/2330 train_time:106498ms step_avg:58.42ms
step:1824/2330 train_time:106559ms step_avg:58.42ms
step:1825/2330 train_time:106617ms step_avg:58.42ms
step:1826/2330 train_time:106678ms step_avg:58.42ms
step:1827/2330 train_time:106736ms step_avg:58.42ms
step:1828/2330 train_time:106796ms step_avg:58.42ms
step:1829/2330 train_time:106853ms step_avg:58.42ms
step:1830/2330 train_time:106915ms step_avg:58.42ms
step:1831/2330 train_time:106973ms step_avg:58.42ms
step:1832/2330 train_time:107032ms step_avg:58.42ms
step:1833/2330 train_time:107090ms step_avg:58.42ms
step:1834/2330 train_time:107150ms step_avg:58.42ms
step:1835/2330 train_time:107207ms step_avg:58.42ms
step:1836/2330 train_time:107268ms step_avg:58.42ms
step:1837/2330 train_time:107325ms step_avg:58.42ms
step:1838/2330 train_time:107387ms step_avg:58.43ms
step:1839/2330 train_time:107443ms step_avg:58.42ms
step:1840/2330 train_time:107506ms step_avg:58.43ms
step:1841/2330 train_time:107563ms step_avg:58.43ms
step:1842/2330 train_time:107625ms step_avg:58.43ms
step:1843/2330 train_time:107681ms step_avg:58.43ms
step:1844/2330 train_time:107744ms step_avg:58.43ms
step:1845/2330 train_time:107800ms step_avg:58.43ms
step:1846/2330 train_time:107863ms step_avg:58.43ms
step:1847/2330 train_time:107919ms step_avg:58.43ms
step:1848/2330 train_time:107980ms step_avg:58.43ms
step:1849/2330 train_time:108037ms step_avg:58.43ms
step:1850/2330 train_time:108098ms step_avg:58.43ms
step:1851/2330 train_time:108156ms step_avg:58.43ms
step:1852/2330 train_time:108215ms step_avg:58.43ms
step:1853/2330 train_time:108274ms step_avg:58.43ms
step:1854/2330 train_time:108334ms step_avg:58.43ms
step:1855/2330 train_time:108391ms step_avg:58.43ms
step:1856/2330 train_time:108451ms step_avg:58.43ms
step:1857/2330 train_time:108509ms step_avg:58.43ms
step:1858/2330 train_time:108569ms step_avg:58.43ms
step:1859/2330 train_time:108625ms step_avg:58.43ms
step:1860/2330 train_time:108688ms step_avg:58.43ms
step:1861/2330 train_time:108745ms step_avg:58.43ms
step:1862/2330 train_time:108806ms step_avg:58.43ms
step:1863/2330 train_time:108862ms step_avg:58.43ms
step:1864/2330 train_time:108924ms step_avg:58.44ms
step:1865/2330 train_time:108981ms step_avg:58.44ms
step:1866/2330 train_time:109043ms step_avg:58.44ms
step:1867/2330 train_time:109099ms step_avg:58.44ms
step:1868/2330 train_time:109161ms step_avg:58.44ms
step:1869/2330 train_time:109218ms step_avg:58.44ms
step:1870/2330 train_time:109278ms step_avg:58.44ms
step:1871/2330 train_time:109335ms step_avg:58.44ms
step:1872/2330 train_time:109395ms step_avg:58.44ms
step:1873/2330 train_time:109453ms step_avg:58.44ms
step:1874/2330 train_time:109513ms step_avg:58.44ms
step:1875/2330 train_time:109571ms step_avg:58.44ms
step:1876/2330 train_time:109632ms step_avg:58.44ms
step:1877/2330 train_time:109690ms step_avg:58.44ms
step:1878/2330 train_time:109750ms step_avg:58.44ms
step:1879/2330 train_time:109808ms step_avg:58.44ms
step:1880/2330 train_time:109869ms step_avg:58.44ms
step:1881/2330 train_time:109926ms step_avg:58.44ms
step:1882/2330 train_time:109986ms step_avg:58.44ms
step:1883/2330 train_time:110043ms step_avg:58.44ms
step:1884/2330 train_time:110105ms step_avg:58.44ms
step:1885/2330 train_time:110161ms step_avg:58.44ms
step:1886/2330 train_time:110222ms step_avg:58.44ms
step:1887/2330 train_time:110279ms step_avg:58.44ms
step:1888/2330 train_time:110341ms step_avg:58.44ms
step:1889/2330 train_time:110398ms step_avg:58.44ms
step:1890/2330 train_time:110460ms step_avg:58.44ms
step:1891/2330 train_time:110517ms step_avg:58.44ms
step:1892/2330 train_time:110577ms step_avg:58.44ms
step:1893/2330 train_time:110635ms step_avg:58.44ms
step:1894/2330 train_time:110695ms step_avg:58.44ms
step:1895/2330 train_time:110753ms step_avg:58.44ms
step:1896/2330 train_time:110813ms step_avg:58.45ms
step:1897/2330 train_time:110871ms step_avg:58.45ms
step:1898/2330 train_time:110931ms step_avg:58.45ms
step:1899/2330 train_time:110989ms step_avg:58.45ms
step:1900/2330 train_time:111049ms step_avg:58.45ms
step:1901/2330 train_time:111106ms step_avg:58.45ms
step:1902/2330 train_time:111168ms step_avg:58.45ms
step:1903/2330 train_time:111225ms step_avg:58.45ms
step:1904/2330 train_time:111286ms step_avg:58.45ms
step:1905/2330 train_time:111342ms step_avg:58.45ms
step:1906/2330 train_time:111404ms step_avg:58.45ms
step:1907/2330 train_time:111460ms step_avg:58.45ms
step:1908/2330 train_time:111523ms step_avg:58.45ms
step:1909/2330 train_time:111579ms step_avg:58.45ms
step:1910/2330 train_time:111642ms step_avg:58.45ms
step:1911/2330 train_time:111699ms step_avg:58.45ms
step:1912/2330 train_time:111760ms step_avg:58.45ms
step:1913/2330 train_time:111817ms step_avg:58.45ms
step:1914/2330 train_time:111878ms step_avg:58.45ms
step:1915/2330 train_time:111935ms step_avg:58.45ms
step:1916/2330 train_time:111996ms step_avg:58.45ms
step:1917/2330 train_time:112054ms step_avg:58.45ms
step:1918/2330 train_time:112114ms step_avg:58.45ms
step:1919/2330 train_time:112173ms step_avg:58.45ms
step:1920/2330 train_time:112233ms step_avg:58.45ms
step:1921/2330 train_time:112292ms step_avg:58.45ms
step:1922/2330 train_time:112352ms step_avg:58.46ms
step:1923/2330 train_time:112409ms step_avg:58.45ms
step:1924/2330 train_time:112469ms step_avg:58.46ms
step:1925/2330 train_time:112526ms step_avg:58.46ms
step:1926/2330 train_time:112588ms step_avg:58.46ms
step:1927/2330 train_time:112644ms step_avg:58.46ms
step:1928/2330 train_time:112707ms step_avg:58.46ms
step:1929/2330 train_time:112763ms step_avg:58.46ms
step:1930/2330 train_time:112826ms step_avg:58.46ms
step:1931/2330 train_time:112882ms step_avg:58.46ms
step:1932/2330 train_time:112944ms step_avg:58.46ms
step:1933/2330 train_time:113000ms step_avg:58.46ms
step:1934/2330 train_time:113062ms step_avg:58.46ms
step:1935/2330 train_time:113119ms step_avg:58.46ms
step:1936/2330 train_time:113181ms step_avg:58.46ms
step:1937/2330 train_time:113238ms step_avg:58.46ms
step:1938/2330 train_time:113298ms step_avg:58.46ms
step:1939/2330 train_time:113355ms step_avg:58.46ms
step:1940/2330 train_time:113416ms step_avg:58.46ms
step:1941/2330 train_time:113475ms step_avg:58.46ms
step:1942/2330 train_time:113535ms step_avg:58.46ms
step:1943/2330 train_time:113593ms step_avg:58.46ms
step:1944/2330 train_time:113652ms step_avg:58.46ms
step:1945/2330 train_time:113710ms step_avg:58.46ms
step:1946/2330 train_time:113770ms step_avg:58.46ms
step:1947/2330 train_time:113828ms step_avg:58.46ms
step:1948/2330 train_time:113888ms step_avg:58.46ms
step:1949/2330 train_time:113945ms step_avg:58.46ms
step:1950/2330 train_time:114006ms step_avg:58.46ms
step:1951/2330 train_time:114062ms step_avg:58.46ms
step:1952/2330 train_time:114126ms step_avg:58.47ms
step:1953/2330 train_time:114182ms step_avg:58.46ms
step:1954/2330 train_time:114244ms step_avg:58.47ms
step:1955/2330 train_time:114300ms step_avg:58.47ms
step:1956/2330 train_time:114363ms step_avg:58.47ms
step:1957/2330 train_time:114420ms step_avg:58.47ms
step:1958/2330 train_time:114482ms step_avg:58.47ms
step:1959/2330 train_time:114539ms step_avg:58.47ms
step:1960/2330 train_time:114600ms step_avg:58.47ms
step:1961/2330 train_time:114657ms step_avg:58.47ms
step:1962/2330 train_time:114717ms step_avg:58.47ms
step:1963/2330 train_time:114775ms step_avg:58.47ms
step:1964/2330 train_time:114836ms step_avg:58.47ms
step:1965/2330 train_time:114893ms step_avg:58.47ms
step:1966/2330 train_time:114953ms step_avg:58.47ms
step:1967/2330 train_time:115010ms step_avg:58.47ms
step:1968/2330 train_time:115070ms step_avg:58.47ms
step:1969/2330 train_time:115127ms step_avg:58.47ms
step:1970/2330 train_time:115189ms step_avg:58.47ms
step:1971/2330 train_time:115246ms step_avg:58.47ms
step:1972/2330 train_time:115307ms step_avg:58.47ms
step:1973/2330 train_time:115364ms step_avg:58.47ms
step:1974/2330 train_time:115426ms step_avg:58.47ms
step:1975/2330 train_time:115483ms step_avg:58.47ms
step:1976/2330 train_time:115545ms step_avg:58.47ms
step:1977/2330 train_time:115601ms step_avg:58.47ms
step:1978/2330 train_time:115663ms step_avg:58.47ms
step:1979/2330 train_time:115720ms step_avg:58.47ms
step:1980/2330 train_time:115782ms step_avg:58.48ms
step:1981/2330 train_time:115838ms step_avg:58.47ms
step:1982/2330 train_time:115900ms step_avg:58.48ms
step:1983/2330 train_time:115957ms step_avg:58.48ms
step:1984/2330 train_time:116017ms step_avg:58.48ms
step:1985/2330 train_time:116075ms step_avg:58.48ms
step:1986/2330 train_time:116135ms step_avg:58.48ms
step:1987/2330 train_time:116193ms step_avg:58.48ms
step:1988/2330 train_time:116253ms step_avg:58.48ms
step:1989/2330 train_time:116311ms step_avg:58.48ms
step:1990/2330 train_time:116370ms step_avg:58.48ms
step:1991/2330 train_time:116427ms step_avg:58.48ms
step:1992/2330 train_time:116488ms step_avg:58.48ms
step:1993/2330 train_time:116546ms step_avg:58.48ms
step:1994/2330 train_time:116607ms step_avg:58.48ms
step:1995/2330 train_time:116663ms step_avg:58.48ms
step:1996/2330 train_time:116725ms step_avg:58.48ms
step:1997/2330 train_time:116782ms step_avg:58.48ms
step:1998/2330 train_time:116844ms step_avg:58.48ms
step:1999/2330 train_time:116900ms step_avg:58.48ms
step:2000/2330 train_time:116962ms step_avg:58.48ms
step:2000/2330 val_loss:3.7937 train_time:117045ms step_avg:58.52ms
step:2001/2330 train_time:117063ms step_avg:58.50ms
step:2002/2330 train_time:117083ms step_avg:58.48ms
step:2003/2330 train_time:117142ms step_avg:58.48ms
step:2004/2330 train_time:117207ms step_avg:58.49ms
step:2005/2330 train_time:117264ms step_avg:58.49ms
step:2006/2330 train_time:117328ms step_avg:58.49ms
step:2007/2330 train_time:117384ms step_avg:58.49ms
step:2008/2330 train_time:117446ms step_avg:58.49ms
step:2009/2330 train_time:117502ms step_avg:58.49ms
step:2010/2330 train_time:117563ms step_avg:58.49ms
step:2011/2330 train_time:117620ms step_avg:58.49ms
step:2012/2330 train_time:117680ms step_avg:58.49ms
step:2013/2330 train_time:117736ms step_avg:58.49ms
step:2014/2330 train_time:117797ms step_avg:58.49ms
step:2015/2330 train_time:117853ms step_avg:58.49ms
step:2016/2330 train_time:117912ms step_avg:58.49ms
step:2017/2330 train_time:117969ms step_avg:58.49ms
step:2018/2330 train_time:118030ms step_avg:58.49ms
step:2019/2330 train_time:118089ms step_avg:58.49ms
step:2020/2330 train_time:118152ms step_avg:58.49ms
step:2021/2330 train_time:118210ms step_avg:58.49ms
step:2022/2330 train_time:118272ms step_avg:58.49ms
step:2023/2330 train_time:118330ms step_avg:58.49ms
step:2024/2330 train_time:118391ms step_avg:58.49ms
step:2025/2330 train_time:118448ms step_avg:58.49ms
step:2026/2330 train_time:118508ms step_avg:58.49ms
step:2027/2330 train_time:118566ms step_avg:58.49ms
step:2028/2330 train_time:118625ms step_avg:58.49ms
step:2029/2330 train_time:118681ms step_avg:58.49ms
step:2030/2330 train_time:118742ms step_avg:58.49ms
step:2031/2330 train_time:118798ms step_avg:58.49ms
step:2032/2330 train_time:118860ms step_avg:58.49ms
step:2033/2330 train_time:118916ms step_avg:58.49ms
step:2034/2330 train_time:118978ms step_avg:58.49ms
step:2035/2330 train_time:119035ms step_avg:58.49ms
step:2036/2330 train_time:119095ms step_avg:58.49ms
step:2037/2330 train_time:119153ms step_avg:58.49ms
step:2038/2330 train_time:119213ms step_avg:58.50ms
step:2039/2330 train_time:119272ms step_avg:58.50ms
step:2040/2330 train_time:119332ms step_avg:58.50ms
step:2041/2330 train_time:119389ms step_avg:58.50ms
step:2042/2330 train_time:119450ms step_avg:58.50ms
step:2043/2330 train_time:119508ms step_avg:58.50ms
step:2044/2330 train_time:119568ms step_avg:58.50ms
step:2045/2330 train_time:119625ms step_avg:58.50ms
step:2046/2330 train_time:119686ms step_avg:58.50ms
step:2047/2330 train_time:119743ms step_avg:58.50ms
step:2048/2330 train_time:119803ms step_avg:58.50ms
step:2049/2330 train_time:119859ms step_avg:58.50ms
step:2050/2330 train_time:119921ms step_avg:58.50ms
step:2051/2330 train_time:119978ms step_avg:58.50ms
step:2052/2330 train_time:120040ms step_avg:58.50ms
step:2053/2330 train_time:120097ms step_avg:58.50ms
step:2054/2330 train_time:120158ms step_avg:58.50ms
step:2055/2330 train_time:120215ms step_avg:58.50ms
step:2056/2330 train_time:120276ms step_avg:58.50ms
step:2057/2330 train_time:120333ms step_avg:58.50ms
step:2058/2330 train_time:120394ms step_avg:58.50ms
step:2059/2330 train_time:120452ms step_avg:58.50ms
step:2060/2330 train_time:120512ms step_avg:58.50ms
step:2061/2330 train_time:120570ms step_avg:58.50ms
step:2062/2330 train_time:120631ms step_avg:58.50ms
step:2063/2330 train_time:120689ms step_avg:58.50ms
step:2064/2330 train_time:120748ms step_avg:58.50ms
step:2065/2330 train_time:120806ms step_avg:58.50ms
step:2066/2330 train_time:120867ms step_avg:58.50ms
step:2067/2330 train_time:120924ms step_avg:58.50ms
step:2068/2330 train_time:120986ms step_avg:58.50ms
step:2069/2330 train_time:121043ms step_avg:58.50ms
step:2070/2330 train_time:121105ms step_avg:58.51ms
step:2071/2330 train_time:121162ms step_avg:58.50ms
step:2072/2330 train_time:121225ms step_avg:58.51ms
step:2073/2330 train_time:121282ms step_avg:58.51ms
step:2074/2330 train_time:121344ms step_avg:58.51ms
step:2075/2330 train_time:121401ms step_avg:58.51ms
step:2076/2330 train_time:121462ms step_avg:58.51ms
step:2077/2330 train_time:121518ms step_avg:58.51ms
step:2078/2330 train_time:121580ms step_avg:58.51ms
step:2079/2330 train_time:121637ms step_avg:58.51ms
step:2080/2330 train_time:121697ms step_avg:58.51ms
step:2081/2330 train_time:121754ms step_avg:58.51ms
step:2082/2330 train_time:121814ms step_avg:58.51ms
step:2083/2330 train_time:121871ms step_avg:58.51ms
step:2084/2330 train_time:121931ms step_avg:58.51ms
step:2085/2330 train_time:121989ms step_avg:58.51ms
step:2086/2330 train_time:122049ms step_avg:58.51ms
step:2087/2330 train_time:122107ms step_avg:58.51ms
step:2088/2330 train_time:122168ms step_avg:58.51ms
step:2089/2330 train_time:122224ms step_avg:58.51ms
step:2090/2330 train_time:122286ms step_avg:58.51ms
step:2091/2330 train_time:122343ms step_avg:58.51ms
step:2092/2330 train_time:122404ms step_avg:58.51ms
step:2093/2330 train_time:122461ms step_avg:58.51ms
step:2094/2330 train_time:122523ms step_avg:58.51ms
step:2095/2330 train_time:122579ms step_avg:58.51ms
step:2096/2330 train_time:122641ms step_avg:58.51ms
step:2097/2330 train_time:122698ms step_avg:58.51ms
step:2098/2330 train_time:122759ms step_avg:58.51ms
step:2099/2330 train_time:122816ms step_avg:58.51ms
step:2100/2330 train_time:122876ms step_avg:58.51ms
step:2101/2330 train_time:122933ms step_avg:58.51ms
step:2102/2330 train_time:122993ms step_avg:58.51ms
step:2103/2330 train_time:123052ms step_avg:58.51ms
step:2104/2330 train_time:123112ms step_avg:58.51ms
step:2105/2330 train_time:123170ms step_avg:58.51ms
step:2106/2330 train_time:123230ms step_avg:58.51ms
step:2107/2330 train_time:123288ms step_avg:58.51ms
step:2108/2330 train_time:123348ms step_avg:58.51ms
step:2109/2330 train_time:123406ms step_avg:58.51ms
step:2110/2330 train_time:123466ms step_avg:58.51ms
step:2111/2330 train_time:123522ms step_avg:58.51ms
step:2112/2330 train_time:123584ms step_avg:58.52ms
step:2113/2330 train_time:123641ms step_avg:58.51ms
step:2114/2330 train_time:123702ms step_avg:58.52ms
step:2115/2330 train_time:123759ms step_avg:58.51ms
step:2116/2330 train_time:123821ms step_avg:58.52ms
step:2117/2330 train_time:123878ms step_avg:58.52ms
step:2118/2330 train_time:123940ms step_avg:58.52ms
step:2119/2330 train_time:123997ms step_avg:58.52ms
step:2120/2330 train_time:124058ms step_avg:58.52ms
step:2121/2330 train_time:124116ms step_avg:58.52ms
step:2122/2330 train_time:124176ms step_avg:58.52ms
step:2123/2330 train_time:124234ms step_avg:58.52ms
step:2124/2330 train_time:124295ms step_avg:58.52ms
step:2125/2330 train_time:124353ms step_avg:58.52ms
step:2126/2330 train_time:124413ms step_avg:58.52ms
step:2127/2330 train_time:124471ms step_avg:58.52ms
step:2128/2330 train_time:124531ms step_avg:58.52ms
step:2129/2330 train_time:124588ms step_avg:58.52ms
step:2130/2330 train_time:124649ms step_avg:58.52ms
step:2131/2330 train_time:124706ms step_avg:58.52ms
step:2132/2330 train_time:124766ms step_avg:58.52ms
step:2133/2330 train_time:124823ms step_avg:58.52ms
step:2134/2330 train_time:124885ms step_avg:58.52ms
step:2135/2330 train_time:124942ms step_avg:58.52ms
step:2136/2330 train_time:125003ms step_avg:58.52ms
step:2137/2330 train_time:125060ms step_avg:58.52ms
step:2138/2330 train_time:125123ms step_avg:58.52ms
step:2139/2330 train_time:125179ms step_avg:58.52ms
step:2140/2330 train_time:125241ms step_avg:58.52ms
step:2141/2330 train_time:125298ms step_avg:58.52ms
step:2142/2330 train_time:125359ms step_avg:58.52ms
step:2143/2330 train_time:125417ms step_avg:58.52ms
step:2144/2330 train_time:125477ms step_avg:58.52ms
step:2145/2330 train_time:125534ms step_avg:58.52ms
step:2146/2330 train_time:125594ms step_avg:58.52ms
step:2147/2330 train_time:125652ms step_avg:58.52ms
step:2148/2330 train_time:125712ms step_avg:58.53ms
step:2149/2330 train_time:125770ms step_avg:58.52ms
step:2150/2330 train_time:125829ms step_avg:58.53ms
step:2151/2330 train_time:125887ms step_avg:58.52ms
step:2152/2330 train_time:125948ms step_avg:58.53ms
step:2153/2330 train_time:126005ms step_avg:58.53ms
step:2154/2330 train_time:126066ms step_avg:58.53ms
step:2155/2330 train_time:126122ms step_avg:58.53ms
step:2156/2330 train_time:126184ms step_avg:58.53ms
step:2157/2330 train_time:126241ms step_avg:58.53ms
step:2158/2330 train_time:126303ms step_avg:58.53ms
step:2159/2330 train_time:126359ms step_avg:58.53ms
step:2160/2330 train_time:126422ms step_avg:58.53ms
step:2161/2330 train_time:126479ms step_avg:58.53ms
step:2162/2330 train_time:126541ms step_avg:58.53ms
step:2163/2330 train_time:126597ms step_avg:58.53ms
step:2164/2330 train_time:126659ms step_avg:58.53ms
step:2165/2330 train_time:126716ms step_avg:58.53ms
step:2166/2330 train_time:126776ms step_avg:58.53ms
step:2167/2330 train_time:126834ms step_avg:58.53ms
step:2168/2330 train_time:126894ms step_avg:58.53ms
step:2169/2330 train_time:126952ms step_avg:58.53ms
step:2170/2330 train_time:127012ms step_avg:58.53ms
step:2171/2330 train_time:127070ms step_avg:58.53ms
step:2172/2330 train_time:127130ms step_avg:58.53ms
step:2173/2330 train_time:127188ms step_avg:58.53ms
step:2174/2330 train_time:127248ms step_avg:58.53ms
step:2175/2330 train_time:127305ms step_avg:58.53ms
step:2176/2330 train_time:127366ms step_avg:58.53ms
step:2177/2330 train_time:127423ms step_avg:58.53ms
step:2178/2330 train_time:127485ms step_avg:58.53ms
step:2179/2330 train_time:127541ms step_avg:58.53ms
step:2180/2330 train_time:127603ms step_avg:58.53ms
step:2181/2330 train_time:127660ms step_avg:58.53ms
step:2182/2330 train_time:127722ms step_avg:58.53ms
step:2183/2330 train_time:127778ms step_avg:58.53ms
step:2184/2330 train_time:127840ms step_avg:58.53ms
step:2185/2330 train_time:127896ms step_avg:58.53ms
step:2186/2330 train_time:127958ms step_avg:58.54ms
step:2187/2330 train_time:128015ms step_avg:58.53ms
step:2188/2330 train_time:128075ms step_avg:58.54ms
step:2189/2330 train_time:128133ms step_avg:58.53ms
step:2190/2330 train_time:128193ms step_avg:58.54ms
step:2191/2330 train_time:128251ms step_avg:58.54ms
step:2192/2330 train_time:128312ms step_avg:58.54ms
step:2193/2330 train_time:128369ms step_avg:58.54ms
step:2194/2330 train_time:128429ms step_avg:58.54ms
step:2195/2330 train_time:128487ms step_avg:58.54ms
step:2196/2330 train_time:128546ms step_avg:58.54ms
step:2197/2330 train_time:128603ms step_avg:58.54ms
step:2198/2330 train_time:128664ms step_avg:58.54ms
step:2199/2330 train_time:128720ms step_avg:58.54ms
step:2200/2330 train_time:128783ms step_avg:58.54ms
step:2201/2330 train_time:128839ms step_avg:58.54ms
step:2202/2330 train_time:128902ms step_avg:58.54ms
step:2203/2330 train_time:128958ms step_avg:58.54ms
step:2204/2330 train_time:129020ms step_avg:58.54ms
step:2205/2330 train_time:129077ms step_avg:58.54ms
step:2206/2330 train_time:129139ms step_avg:58.54ms
step:2207/2330 train_time:129196ms step_avg:58.54ms
step:2208/2330 train_time:129257ms step_avg:58.54ms
step:2209/2330 train_time:129314ms step_avg:58.54ms
step:2210/2330 train_time:129375ms step_avg:58.54ms
step:2211/2330 train_time:129432ms step_avg:58.54ms
step:2212/2330 train_time:129493ms step_avg:58.54ms
step:2213/2330 train_time:129552ms step_avg:58.54ms
step:2214/2330 train_time:129612ms step_avg:58.54ms
step:2215/2330 train_time:129669ms step_avg:58.54ms
step:2216/2330 train_time:129729ms step_avg:58.54ms
step:2217/2330 train_time:129787ms step_avg:58.54ms
step:2218/2330 train_time:129848ms step_avg:58.54ms
step:2219/2330 train_time:129904ms step_avg:58.54ms
step:2220/2330 train_time:129965ms step_avg:58.54ms
step:2221/2330 train_time:130021ms step_avg:58.54ms
step:2222/2330 train_time:130084ms step_avg:58.54ms
step:2223/2330 train_time:130140ms step_avg:58.54ms
step:2224/2330 train_time:130202ms step_avg:58.54ms
step:2225/2330 train_time:130258ms step_avg:58.54ms
step:2226/2330 train_time:130320ms step_avg:58.54ms
step:2227/2330 train_time:130376ms step_avg:58.54ms
step:2228/2330 train_time:130438ms step_avg:58.54ms
step:2229/2330 train_time:130495ms step_avg:58.54ms
step:2230/2330 train_time:130556ms step_avg:58.55ms
step:2231/2330 train_time:130614ms step_avg:58.54ms
step:2232/2330 train_time:130674ms step_avg:58.55ms
step:2233/2330 train_time:130731ms step_avg:58.55ms
step:2234/2330 train_time:130792ms step_avg:58.55ms
step:2235/2330 train_time:130850ms step_avg:58.55ms
step:2236/2330 train_time:130910ms step_avg:58.55ms
step:2237/2330 train_time:130968ms step_avg:58.55ms
step:2238/2330 train_time:131027ms step_avg:58.55ms
step:2239/2330 train_time:131084ms step_avg:58.55ms
step:2240/2330 train_time:131146ms step_avg:58.55ms
step:2241/2330 train_time:131202ms step_avg:58.55ms
step:2242/2330 train_time:131264ms step_avg:58.55ms
step:2243/2330 train_time:131320ms step_avg:58.55ms
step:2244/2330 train_time:131383ms step_avg:58.55ms
step:2245/2330 train_time:131439ms step_avg:58.55ms
step:2246/2330 train_time:131501ms step_avg:58.55ms
step:2247/2330 train_time:131557ms step_avg:58.55ms
step:2248/2330 train_time:131620ms step_avg:58.55ms
step:2249/2330 train_time:131676ms step_avg:58.55ms
step:2250/2330 train_time:131738ms step_avg:58.55ms
step:2250/2330 val_loss:3.7478 train_time:131818ms step_avg:58.59ms
step:2251/2330 train_time:131837ms step_avg:58.57ms
step:2252/2330 train_time:131859ms step_avg:58.55ms
step:2253/2330 train_time:131920ms step_avg:58.55ms
step:2254/2330 train_time:131982ms step_avg:58.55ms
step:2255/2330 train_time:132039ms step_avg:58.55ms
step:2256/2330 train_time:132100ms step_avg:58.56ms
step:2257/2330 train_time:132157ms step_avg:58.55ms
step:2258/2330 train_time:132217ms step_avg:58.55ms
step:2259/2330 train_time:132274ms step_avg:58.55ms
step:2260/2330 train_time:132334ms step_avg:58.55ms
step:2261/2330 train_time:132390ms step_avg:58.55ms
step:2262/2330 train_time:132450ms step_avg:58.55ms
step:2263/2330 train_time:132506ms step_avg:58.55ms
step:2264/2330 train_time:132567ms step_avg:58.55ms
step:2265/2330 train_time:132623ms step_avg:58.55ms
step:2266/2330 train_time:132684ms step_avg:58.55ms
step:2267/2330 train_time:132740ms step_avg:58.55ms
step:2268/2330 train_time:132803ms step_avg:58.56ms
step:2269/2330 train_time:132862ms step_avg:58.56ms
step:2270/2330 train_time:132925ms step_avg:58.56ms
step:2271/2330 train_time:132982ms step_avg:58.56ms
step:2272/2330 train_time:133045ms step_avg:58.56ms
step:2273/2330 train_time:133102ms step_avg:58.56ms
step:2274/2330 train_time:133162ms step_avg:58.56ms
step:2275/2330 train_time:133220ms step_avg:58.56ms
step:2276/2330 train_time:133279ms step_avg:58.56ms
step:2277/2330 train_time:133336ms step_avg:58.56ms
step:2278/2330 train_time:133397ms step_avg:58.56ms
step:2279/2330 train_time:133453ms step_avg:58.56ms
step:2280/2330 train_time:133513ms step_avg:58.56ms
step:2281/2330 train_time:133569ms step_avg:58.56ms
step:2282/2330 train_time:133629ms step_avg:58.56ms
step:2283/2330 train_time:133686ms step_avg:58.56ms
step:2284/2330 train_time:133747ms step_avg:58.56ms
step:2285/2330 train_time:133804ms step_avg:58.56ms
step:2286/2330 train_time:133868ms step_avg:58.56ms
step:2287/2330 train_time:133925ms step_avg:58.56ms
step:2288/2330 train_time:133986ms step_avg:58.56ms
step:2289/2330 train_time:134043ms step_avg:58.56ms
step:2290/2330 train_time:134105ms step_avg:58.56ms
step:2291/2330 train_time:134162ms step_avg:58.56ms
step:2292/2330 train_time:134224ms step_avg:58.56ms
step:2293/2330 train_time:134281ms step_avg:58.56ms
step:2294/2330 train_time:134342ms step_avg:58.56ms
step:2295/2330 train_time:134399ms step_avg:58.56ms
step:2296/2330 train_time:134459ms step_avg:58.56ms
step:2297/2330 train_time:134516ms step_avg:58.56ms
step:2298/2330 train_time:134576ms step_avg:58.56ms
step:2299/2330 train_time:134634ms step_avg:58.56ms
step:2300/2330 train_time:134694ms step_avg:58.56ms
step:2301/2330 train_time:134752ms step_avg:58.56ms
step:2302/2330 train_time:134812ms step_avg:58.56ms
step:2303/2330 train_time:134869ms step_avg:58.56ms
step:2304/2330 train_time:134931ms step_avg:58.56ms
step:2305/2330 train_time:134987ms step_avg:58.56ms
step:2306/2330 train_time:135050ms step_avg:58.56ms
step:2307/2330 train_time:135107ms step_avg:58.56ms
step:2308/2330 train_time:135170ms step_avg:58.57ms
step:2309/2330 train_time:135227ms step_avg:58.57ms
step:2310/2330 train_time:135288ms step_avg:58.57ms
step:2311/2330 train_time:135344ms step_avg:58.57ms
step:2312/2330 train_time:135406ms step_avg:58.57ms
step:2313/2330 train_time:135463ms step_avg:58.57ms
step:2314/2330 train_time:135524ms step_avg:58.57ms
step:2315/2330 train_time:135581ms step_avg:58.57ms
step:2316/2330 train_time:135640ms step_avg:58.57ms
step:2317/2330 train_time:135698ms step_avg:58.57ms
step:2318/2330 train_time:135758ms step_avg:58.57ms
step:2319/2330 train_time:135816ms step_avg:58.57ms
step:2320/2330 train_time:135876ms step_avg:58.57ms
step:2321/2330 train_time:135935ms step_avg:58.57ms
step:2322/2330 train_time:135995ms step_avg:58.57ms
step:2323/2330 train_time:136052ms step_avg:58.57ms
step:2324/2330 train_time:136113ms step_avg:58.57ms
step:2325/2330 train_time:136170ms step_avg:58.57ms
step:2326/2330 train_time:136231ms step_avg:58.57ms
step:2327/2330 train_time:136288ms step_avg:58.57ms
step:2328/2330 train_time:136350ms step_avg:58.57ms
step:2329/2330 train_time:136406ms step_avg:58.57ms
step:2330/2330 train_time:136468ms step_avg:58.57ms
step:2330/2330 val_loss:3.7329 train_time:136551ms step_avg:58.61ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
