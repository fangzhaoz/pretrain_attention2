import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--beta1', type=str, required=True)
parser.add_argument('--beta2', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_beta_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=6e-4,  betas=(float(args2.beta1), float(args2.beta2)), eps=1e-10, weight_decay=0.0, fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt", exist_ok=True)
            torch.save(log, f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 05:26:14 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   27C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   26C    P0             116W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   25C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   27C    P0             113W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   25C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   26C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:85ms step_avg:85.38ms
step:2/2330 train_time:187ms step_avg:93.74ms
step:3/2330 train_time:206ms step_avg:68.55ms
step:4/2330 train_time:225ms step_avg:56.18ms
step:5/2330 train_time:278ms step_avg:55.70ms
step:6/2330 train_time:337ms step_avg:56.09ms
step:7/2330 train_time:392ms step_avg:55.95ms
step:8/2330 train_time:451ms step_avg:56.33ms
step:9/2330 train_time:506ms step_avg:56.21ms
step:10/2330 train_time:564ms step_avg:56.41ms
step:11/2330 train_time:620ms step_avg:56.32ms
step:12/2330 train_time:678ms step_avg:56.51ms
step:13/2330 train_time:734ms step_avg:56.46ms
step:14/2330 train_time:792ms step_avg:56.57ms
step:15/2330 train_time:848ms step_avg:56.51ms
step:16/2330 train_time:906ms step_avg:56.61ms
step:17/2330 train_time:961ms step_avg:56.55ms
step:18/2330 train_time:1020ms step_avg:56.65ms
step:19/2330 train_time:1077ms step_avg:56.68ms
step:20/2330 train_time:1139ms step_avg:56.93ms
step:21/2330 train_time:1197ms step_avg:56.99ms
step:22/2330 train_time:1257ms step_avg:57.13ms
step:23/2330 train_time:1313ms step_avg:57.07ms
step:24/2330 train_time:1372ms step_avg:57.16ms
step:25/2330 train_time:1427ms step_avg:57.09ms
step:26/2330 train_time:1486ms step_avg:57.17ms
step:27/2330 train_time:1543ms step_avg:57.13ms
step:28/2330 train_time:1601ms step_avg:57.17ms
step:29/2330 train_time:1656ms step_avg:57.11ms
step:30/2330 train_time:1714ms step_avg:57.14ms
step:31/2330 train_time:1769ms step_avg:57.08ms
step:32/2330 train_time:1828ms step_avg:57.13ms
step:33/2330 train_time:1884ms step_avg:57.09ms
step:34/2330 train_time:1943ms step_avg:57.13ms
step:35/2330 train_time:2000ms step_avg:57.13ms
step:36/2330 train_time:2059ms step_avg:57.20ms
step:37/2330 train_time:2116ms step_avg:57.19ms
step:38/2330 train_time:2177ms step_avg:57.28ms
step:39/2330 train_time:2234ms step_avg:57.27ms
step:40/2330 train_time:2292ms step_avg:57.31ms
step:41/2330 train_time:2349ms step_avg:57.29ms
step:42/2330 train_time:2408ms step_avg:57.34ms
step:43/2330 train_time:2464ms step_avg:57.31ms
step:44/2330 train_time:2523ms step_avg:57.34ms
step:45/2330 train_time:2579ms step_avg:57.31ms
step:46/2330 train_time:2637ms step_avg:57.34ms
step:47/2330 train_time:2693ms step_avg:57.30ms
step:48/2330 train_time:2751ms step_avg:57.32ms
step:49/2330 train_time:2807ms step_avg:57.28ms
step:50/2330 train_time:2866ms step_avg:57.32ms
step:51/2330 train_time:2922ms step_avg:57.29ms
step:52/2330 train_time:2980ms step_avg:57.32ms
step:53/2330 train_time:3037ms step_avg:57.30ms
step:54/2330 train_time:3096ms step_avg:57.34ms
step:55/2330 train_time:3154ms step_avg:57.34ms
step:56/2330 train_time:3213ms step_avg:57.37ms
step:57/2330 train_time:3269ms step_avg:57.35ms
step:58/2330 train_time:3329ms step_avg:57.39ms
step:59/2330 train_time:3385ms step_avg:57.37ms
step:60/2330 train_time:3444ms step_avg:57.39ms
step:61/2330 train_time:3499ms step_avg:57.36ms
step:62/2330 train_time:3559ms step_avg:57.40ms
step:63/2330 train_time:3615ms step_avg:57.37ms
step:64/2330 train_time:3674ms step_avg:57.40ms
step:65/2330 train_time:3730ms step_avg:57.38ms
step:66/2330 train_time:3788ms step_avg:57.39ms
step:67/2330 train_time:3844ms step_avg:57.37ms
step:68/2330 train_time:3903ms step_avg:57.39ms
step:69/2330 train_time:3959ms step_avg:57.37ms
step:70/2330 train_time:4018ms step_avg:57.39ms
step:71/2330 train_time:4074ms step_avg:57.38ms
step:72/2330 train_time:4133ms step_avg:57.41ms
step:73/2330 train_time:4190ms step_avg:57.40ms
step:74/2330 train_time:4249ms step_avg:57.43ms
step:75/2330 train_time:4306ms step_avg:57.41ms
step:76/2330 train_time:4365ms step_avg:57.43ms
step:77/2330 train_time:4421ms step_avg:57.41ms
step:78/2330 train_time:4480ms step_avg:57.44ms
step:79/2330 train_time:4536ms step_avg:57.41ms
step:80/2330 train_time:4595ms step_avg:57.44ms
step:81/2330 train_time:4651ms step_avg:57.42ms
step:82/2330 train_time:4709ms step_avg:57.43ms
step:83/2330 train_time:4765ms step_avg:57.41ms
step:84/2330 train_time:4823ms step_avg:57.42ms
step:85/2330 train_time:4879ms step_avg:57.40ms
step:86/2330 train_time:4937ms step_avg:57.41ms
step:87/2330 train_time:4994ms step_avg:57.40ms
step:88/2330 train_time:5052ms step_avg:57.41ms
step:89/2330 train_time:5108ms step_avg:57.39ms
step:90/2330 train_time:5168ms step_avg:57.42ms
step:91/2330 train_time:5224ms step_avg:57.41ms
step:92/2330 train_time:5283ms step_avg:57.43ms
step:93/2330 train_time:5339ms step_avg:57.41ms
step:94/2330 train_time:5399ms step_avg:57.44ms
step:95/2330 train_time:5456ms step_avg:57.43ms
step:96/2330 train_time:5515ms step_avg:57.44ms
step:97/2330 train_time:5570ms step_avg:57.42ms
step:98/2330 train_time:5629ms step_avg:57.44ms
step:99/2330 train_time:5684ms step_avg:57.42ms
step:100/2330 train_time:5744ms step_avg:57.44ms
step:101/2330 train_time:5800ms step_avg:57.43ms
step:102/2330 train_time:5859ms step_avg:57.44ms
step:103/2330 train_time:5915ms step_avg:57.43ms
step:104/2330 train_time:5973ms step_avg:57.44ms
step:105/2330 train_time:6030ms step_avg:57.43ms
step:106/2330 train_time:6089ms step_avg:57.44ms
step:107/2330 train_time:6146ms step_avg:57.44ms
step:108/2330 train_time:6204ms step_avg:57.45ms
step:109/2330 train_time:6261ms step_avg:57.44ms
step:110/2330 train_time:6319ms step_avg:57.45ms
step:111/2330 train_time:6376ms step_avg:57.44ms
step:112/2330 train_time:6435ms step_avg:57.45ms
step:113/2330 train_time:6490ms step_avg:57.43ms
step:114/2330 train_time:6550ms step_avg:57.46ms
step:115/2330 train_time:6605ms step_avg:57.44ms
step:116/2330 train_time:6664ms step_avg:57.45ms
step:117/2330 train_time:6720ms step_avg:57.44ms
step:118/2330 train_time:6779ms step_avg:57.45ms
step:119/2330 train_time:6834ms step_avg:57.43ms
step:120/2330 train_time:6893ms step_avg:57.44ms
step:121/2330 train_time:6949ms step_avg:57.43ms
step:122/2330 train_time:7008ms step_avg:57.44ms
step:123/2330 train_time:7065ms step_avg:57.44ms
step:124/2330 train_time:7123ms step_avg:57.45ms
step:125/2330 train_time:7179ms step_avg:57.43ms
step:126/2330 train_time:7238ms step_avg:57.44ms
step:127/2330 train_time:7295ms step_avg:57.44ms
step:128/2330 train_time:7354ms step_avg:57.45ms
step:129/2330 train_time:7410ms step_avg:57.44ms
step:130/2330 train_time:7469ms step_avg:57.45ms
step:131/2330 train_time:7525ms step_avg:57.44ms
step:132/2330 train_time:7584ms step_avg:57.45ms
step:133/2330 train_time:7640ms step_avg:57.44ms
step:134/2330 train_time:7698ms step_avg:57.45ms
step:135/2330 train_time:7754ms step_avg:57.44ms
step:136/2330 train_time:7813ms step_avg:57.45ms
step:137/2330 train_time:7868ms step_avg:57.43ms
step:138/2330 train_time:7928ms step_avg:57.45ms
step:139/2330 train_time:7983ms step_avg:57.44ms
step:140/2330 train_time:8044ms step_avg:57.45ms
step:141/2330 train_time:8100ms step_avg:57.44ms
step:142/2330 train_time:8159ms step_avg:57.46ms
step:143/2330 train_time:8214ms step_avg:57.44ms
step:144/2330 train_time:8273ms step_avg:57.45ms
step:145/2330 train_time:8329ms step_avg:57.44ms
step:146/2330 train_time:8388ms step_avg:57.45ms
step:147/2330 train_time:8445ms step_avg:57.45ms
step:148/2330 train_time:8504ms step_avg:57.46ms
step:149/2330 train_time:8559ms step_avg:57.44ms
step:150/2330 train_time:8618ms step_avg:57.45ms
step:151/2330 train_time:8673ms step_avg:57.44ms
step:152/2330 train_time:8733ms step_avg:57.46ms
step:153/2330 train_time:8789ms step_avg:57.44ms
step:154/2330 train_time:8848ms step_avg:57.46ms
step:155/2330 train_time:8904ms step_avg:57.44ms
step:156/2330 train_time:8963ms step_avg:57.46ms
step:157/2330 train_time:9019ms step_avg:57.44ms
step:158/2330 train_time:9078ms step_avg:57.46ms
step:159/2330 train_time:9134ms step_avg:57.45ms
step:160/2330 train_time:9193ms step_avg:57.45ms
step:161/2330 train_time:9249ms step_avg:57.45ms
step:162/2330 train_time:9308ms step_avg:57.46ms
step:163/2330 train_time:9365ms step_avg:57.45ms
step:164/2330 train_time:9424ms step_avg:57.46ms
step:165/2330 train_time:9480ms step_avg:57.45ms
step:166/2330 train_time:9538ms step_avg:57.46ms
step:167/2330 train_time:9595ms step_avg:57.45ms
step:168/2330 train_time:9654ms step_avg:57.46ms
step:169/2330 train_time:9710ms step_avg:57.46ms
step:170/2330 train_time:9769ms step_avg:57.46ms
step:171/2330 train_time:9825ms step_avg:57.46ms
step:172/2330 train_time:9884ms step_avg:57.46ms
step:173/2330 train_time:9940ms step_avg:57.46ms
step:174/2330 train_time:9999ms step_avg:57.46ms
step:175/2330 train_time:10054ms step_avg:57.45ms
step:176/2330 train_time:10114ms step_avg:57.46ms
step:177/2330 train_time:10169ms step_avg:57.45ms
step:178/2330 train_time:10228ms step_avg:57.46ms
step:179/2330 train_time:10284ms step_avg:57.45ms
step:180/2330 train_time:10343ms step_avg:57.46ms
step:181/2330 train_time:10400ms step_avg:57.46ms
step:182/2330 train_time:10459ms step_avg:57.47ms
step:183/2330 train_time:10516ms step_avg:57.46ms
step:184/2330 train_time:10575ms step_avg:57.47ms
step:185/2330 train_time:10631ms step_avg:57.46ms
step:186/2330 train_time:10689ms step_avg:57.47ms
step:187/2330 train_time:10745ms step_avg:57.46ms
step:188/2330 train_time:10804ms step_avg:57.47ms
step:189/2330 train_time:10861ms step_avg:57.46ms
step:190/2330 train_time:10919ms step_avg:57.47ms
step:191/2330 train_time:10975ms step_avg:57.46ms
step:192/2330 train_time:11034ms step_avg:57.47ms
step:193/2330 train_time:11090ms step_avg:57.46ms
step:194/2330 train_time:11149ms step_avg:57.47ms
step:195/2330 train_time:11206ms step_avg:57.46ms
step:196/2330 train_time:11264ms step_avg:57.47ms
step:197/2330 train_time:11320ms step_avg:57.46ms
step:198/2330 train_time:11379ms step_avg:57.47ms
step:199/2330 train_time:11435ms step_avg:57.46ms
step:200/2330 train_time:11494ms step_avg:57.47ms
step:201/2330 train_time:11551ms step_avg:57.47ms
step:202/2330 train_time:11610ms step_avg:57.47ms
step:203/2330 train_time:11665ms step_avg:57.46ms
step:204/2330 train_time:11724ms step_avg:57.47ms
step:205/2330 train_time:11780ms step_avg:57.46ms
step:206/2330 train_time:11839ms step_avg:57.47ms
step:207/2330 train_time:11895ms step_avg:57.46ms
step:208/2330 train_time:11953ms step_avg:57.47ms
step:209/2330 train_time:12009ms step_avg:57.46ms
step:210/2330 train_time:12069ms step_avg:57.47ms
step:211/2330 train_time:12124ms step_avg:57.46ms
step:212/2330 train_time:12183ms step_avg:57.47ms
step:213/2330 train_time:12240ms step_avg:57.46ms
step:214/2330 train_time:12299ms step_avg:57.47ms
step:215/2330 train_time:12355ms step_avg:57.46ms
step:216/2330 train_time:12413ms step_avg:57.47ms
step:217/2330 train_time:12469ms step_avg:57.46ms
step:218/2330 train_time:12528ms step_avg:57.47ms
step:219/2330 train_time:12585ms step_avg:57.47ms
step:220/2330 train_time:12644ms step_avg:57.47ms
step:221/2330 train_time:12700ms step_avg:57.46ms
step:222/2330 train_time:12759ms step_avg:57.47ms
step:223/2330 train_time:12815ms step_avg:57.46ms
step:224/2330 train_time:12874ms step_avg:57.47ms
step:225/2330 train_time:12929ms step_avg:57.46ms
step:226/2330 train_time:12989ms step_avg:57.47ms
step:227/2330 train_time:13045ms step_avg:57.47ms
step:228/2330 train_time:13104ms step_avg:57.47ms
step:229/2330 train_time:13160ms step_avg:57.47ms
step:230/2330 train_time:13218ms step_avg:57.47ms
step:231/2330 train_time:13275ms step_avg:57.47ms
step:232/2330 train_time:13333ms step_avg:57.47ms
step:233/2330 train_time:13389ms step_avg:57.46ms
step:234/2330 train_time:13448ms step_avg:57.47ms
step:235/2330 train_time:13505ms step_avg:57.47ms
step:236/2330 train_time:13564ms step_avg:57.47ms
step:237/2330 train_time:13620ms step_avg:57.47ms
step:238/2330 train_time:13679ms step_avg:57.48ms
step:239/2330 train_time:13735ms step_avg:57.47ms
step:240/2330 train_time:13794ms step_avg:57.48ms
step:241/2330 train_time:13851ms step_avg:57.47ms
step:242/2330 train_time:13909ms step_avg:57.48ms
step:243/2330 train_time:13965ms step_avg:57.47ms
step:244/2330 train_time:14024ms step_avg:57.48ms
step:245/2330 train_time:14079ms step_avg:57.47ms
step:246/2330 train_time:14138ms step_avg:57.47ms
step:247/2330 train_time:14194ms step_avg:57.47ms
step:248/2330 train_time:14254ms step_avg:57.48ms
step:249/2330 train_time:14310ms step_avg:57.47ms
step:250/2330 train_time:14369ms step_avg:57.48ms
step:250/2330 val_loss:4.8945 train_time:14448ms step_avg:57.79ms
step:251/2330 train_time:14466ms step_avg:57.63ms
step:252/2330 train_time:14485ms step_avg:57.48ms
step:253/2330 train_time:14541ms step_avg:57.47ms
step:254/2330 train_time:14606ms step_avg:57.50ms
step:255/2330 train_time:14661ms step_avg:57.49ms
step:256/2330 train_time:14724ms step_avg:57.51ms
step:257/2330 train_time:14779ms step_avg:57.51ms
step:258/2330 train_time:14840ms step_avg:57.52ms
step:259/2330 train_time:14895ms step_avg:57.51ms
step:260/2330 train_time:14955ms step_avg:57.52ms
step:261/2330 train_time:15010ms step_avg:57.51ms
step:262/2330 train_time:15069ms step_avg:57.51ms
step:263/2330 train_time:15124ms step_avg:57.51ms
step:264/2330 train_time:15182ms step_avg:57.51ms
step:265/2330 train_time:15237ms step_avg:57.50ms
step:266/2330 train_time:15296ms step_avg:57.50ms
step:267/2330 train_time:15352ms step_avg:57.50ms
step:268/2330 train_time:15412ms step_avg:57.51ms
step:269/2330 train_time:15469ms step_avg:57.51ms
step:270/2330 train_time:15529ms step_avg:57.52ms
step:271/2330 train_time:15586ms step_avg:57.51ms
step:272/2330 train_time:15646ms step_avg:57.52ms
step:273/2330 train_time:15702ms step_avg:57.52ms
step:274/2330 train_time:15762ms step_avg:57.52ms
step:275/2330 train_time:15818ms step_avg:57.52ms
step:276/2330 train_time:15878ms step_avg:57.53ms
step:277/2330 train_time:15934ms step_avg:57.52ms
step:278/2330 train_time:15993ms step_avg:57.53ms
step:279/2330 train_time:16049ms step_avg:57.52ms
step:280/2330 train_time:16107ms step_avg:57.53ms
step:281/2330 train_time:16163ms step_avg:57.52ms
step:282/2330 train_time:16221ms step_avg:57.52ms
step:283/2330 train_time:16277ms step_avg:57.51ms
step:284/2330 train_time:16336ms step_avg:57.52ms
step:285/2330 train_time:16392ms step_avg:57.52ms
step:286/2330 train_time:16451ms step_avg:57.52ms
step:287/2330 train_time:16508ms step_avg:57.52ms
step:288/2330 train_time:16567ms step_avg:57.52ms
step:289/2330 train_time:16623ms step_avg:57.52ms
step:290/2330 train_time:16683ms step_avg:57.53ms
step:291/2330 train_time:16740ms step_avg:57.52ms
step:292/2330 train_time:16799ms step_avg:57.53ms
step:293/2330 train_time:16856ms step_avg:57.53ms
step:294/2330 train_time:16914ms step_avg:57.53ms
step:295/2330 train_time:16971ms step_avg:57.53ms
step:296/2330 train_time:17030ms step_avg:57.53ms
step:297/2330 train_time:17086ms step_avg:57.53ms
step:298/2330 train_time:17144ms step_avg:57.53ms
step:299/2330 train_time:17200ms step_avg:57.52ms
step:300/2330 train_time:17258ms step_avg:57.53ms
step:301/2330 train_time:17313ms step_avg:57.52ms
step:302/2330 train_time:17372ms step_avg:57.52ms
step:303/2330 train_time:17428ms step_avg:57.52ms
step:304/2330 train_time:17488ms step_avg:57.53ms
step:305/2330 train_time:17545ms step_avg:57.52ms
step:306/2330 train_time:17604ms step_avg:57.53ms
step:307/2330 train_time:17660ms step_avg:57.52ms
step:308/2330 train_time:17720ms step_avg:57.53ms
step:309/2330 train_time:17777ms step_avg:57.53ms
step:310/2330 train_time:17836ms step_avg:57.53ms
step:311/2330 train_time:17892ms step_avg:57.53ms
step:312/2330 train_time:17951ms step_avg:57.54ms
step:313/2330 train_time:18007ms step_avg:57.53ms
step:314/2330 train_time:18066ms step_avg:57.53ms
step:315/2330 train_time:18122ms step_avg:57.53ms
step:316/2330 train_time:18180ms step_avg:57.53ms
step:317/2330 train_time:18236ms step_avg:57.53ms
step:318/2330 train_time:18294ms step_avg:57.53ms
step:319/2330 train_time:18350ms step_avg:57.52ms
step:320/2330 train_time:18409ms step_avg:57.53ms
step:321/2330 train_time:18465ms step_avg:57.52ms
step:322/2330 train_time:18524ms step_avg:57.53ms
step:323/2330 train_time:18580ms step_avg:57.52ms
step:324/2330 train_time:18639ms step_avg:57.53ms
step:325/2330 train_time:18696ms step_avg:57.53ms
step:326/2330 train_time:18756ms step_avg:57.53ms
step:327/2330 train_time:18812ms step_avg:57.53ms
step:328/2330 train_time:18870ms step_avg:57.53ms
step:329/2330 train_time:18926ms step_avg:57.53ms
step:330/2330 train_time:18986ms step_avg:57.53ms
step:331/2330 train_time:19041ms step_avg:57.53ms
step:332/2330 train_time:19100ms step_avg:57.53ms
step:333/2330 train_time:19155ms step_avg:57.52ms
step:334/2330 train_time:19215ms step_avg:57.53ms
step:335/2330 train_time:19271ms step_avg:57.53ms
step:336/2330 train_time:19330ms step_avg:57.53ms
step:337/2330 train_time:19386ms step_avg:57.53ms
step:338/2330 train_time:19445ms step_avg:57.53ms
step:339/2330 train_time:19502ms step_avg:57.53ms
step:340/2330 train_time:19561ms step_avg:57.53ms
step:341/2330 train_time:19617ms step_avg:57.53ms
step:342/2330 train_time:19677ms step_avg:57.53ms
step:343/2330 train_time:19733ms step_avg:57.53ms
step:344/2330 train_time:19793ms step_avg:57.54ms
step:345/2330 train_time:19849ms step_avg:57.53ms
step:346/2330 train_time:19909ms step_avg:57.54ms
step:347/2330 train_time:19964ms step_avg:57.53ms
step:348/2330 train_time:20023ms step_avg:57.54ms
step:349/2330 train_time:20079ms step_avg:57.53ms
step:350/2330 train_time:20138ms step_avg:57.54ms
step:351/2330 train_time:20195ms step_avg:57.53ms
step:352/2330 train_time:20253ms step_avg:57.54ms
step:353/2330 train_time:20309ms step_avg:57.53ms
step:354/2330 train_time:20368ms step_avg:57.54ms
step:355/2330 train_time:20424ms step_avg:57.53ms
step:356/2330 train_time:20483ms step_avg:57.54ms
step:357/2330 train_time:20539ms step_avg:57.53ms
step:358/2330 train_time:20598ms step_avg:57.54ms
step:359/2330 train_time:20654ms step_avg:57.53ms
step:360/2330 train_time:20713ms step_avg:57.54ms
step:361/2330 train_time:20770ms step_avg:57.53ms
step:362/2330 train_time:20829ms step_avg:57.54ms
step:363/2330 train_time:20885ms step_avg:57.54ms
step:364/2330 train_time:20944ms step_avg:57.54ms
step:365/2330 train_time:21000ms step_avg:57.53ms
step:366/2330 train_time:21059ms step_avg:57.54ms
step:367/2330 train_time:21115ms step_avg:57.53ms
step:368/2330 train_time:21175ms step_avg:57.54ms
step:369/2330 train_time:21231ms step_avg:57.54ms
step:370/2330 train_time:21290ms step_avg:57.54ms
step:371/2330 train_time:21346ms step_avg:57.54ms
step:372/2330 train_time:21405ms step_avg:57.54ms
step:373/2330 train_time:21461ms step_avg:57.54ms
step:374/2330 train_time:21521ms step_avg:57.54ms
step:375/2330 train_time:21576ms step_avg:57.54ms
step:376/2330 train_time:21635ms step_avg:57.54ms
step:377/2330 train_time:21691ms step_avg:57.54ms
step:378/2330 train_time:21752ms step_avg:57.54ms
step:379/2330 train_time:21807ms step_avg:57.54ms
step:380/2330 train_time:21867ms step_avg:57.55ms
step:381/2330 train_time:21923ms step_avg:57.54ms
step:382/2330 train_time:21982ms step_avg:57.54ms
step:383/2330 train_time:22038ms step_avg:57.54ms
step:384/2330 train_time:22097ms step_avg:57.54ms
step:385/2330 train_time:22153ms step_avg:57.54ms
step:386/2330 train_time:22211ms step_avg:57.54ms
step:387/2330 train_time:22268ms step_avg:57.54ms
step:388/2330 train_time:22326ms step_avg:57.54ms
step:389/2330 train_time:22383ms step_avg:57.54ms
step:390/2330 train_time:22441ms step_avg:57.54ms
step:391/2330 train_time:22498ms step_avg:57.54ms
step:392/2330 train_time:22557ms step_avg:57.54ms
step:393/2330 train_time:22614ms step_avg:57.54ms
step:394/2330 train_time:22673ms step_avg:57.55ms
step:395/2330 train_time:22729ms step_avg:57.54ms
step:396/2330 train_time:22788ms step_avg:57.55ms
step:397/2330 train_time:22844ms step_avg:57.54ms
step:398/2330 train_time:22903ms step_avg:57.55ms
step:399/2330 train_time:22959ms step_avg:57.54ms
step:400/2330 train_time:23018ms step_avg:57.54ms
step:401/2330 train_time:23074ms step_avg:57.54ms
step:402/2330 train_time:23133ms step_avg:57.55ms
step:403/2330 train_time:23189ms step_avg:57.54ms
step:404/2330 train_time:23248ms step_avg:57.54ms
step:405/2330 train_time:23304ms step_avg:57.54ms
step:406/2330 train_time:23362ms step_avg:57.54ms
step:407/2330 train_time:23418ms step_avg:57.54ms
step:408/2330 train_time:23477ms step_avg:57.54ms
step:409/2330 train_time:23533ms step_avg:57.54ms
step:410/2330 train_time:23592ms step_avg:57.54ms
step:411/2330 train_time:23648ms step_avg:57.54ms
step:412/2330 train_time:23707ms step_avg:57.54ms
step:413/2330 train_time:23764ms step_avg:57.54ms
step:414/2330 train_time:23823ms step_avg:57.54ms
step:415/2330 train_time:23879ms step_avg:57.54ms
step:416/2330 train_time:23938ms step_avg:57.54ms
step:417/2330 train_time:23995ms step_avg:57.54ms
step:418/2330 train_time:24053ms step_avg:57.54ms
step:419/2330 train_time:24109ms step_avg:57.54ms
step:420/2330 train_time:24168ms step_avg:57.54ms
step:421/2330 train_time:24224ms step_avg:57.54ms
step:422/2330 train_time:24283ms step_avg:57.54ms
step:423/2330 train_time:24339ms step_avg:57.54ms
step:424/2330 train_time:24399ms step_avg:57.54ms
step:425/2330 train_time:24455ms step_avg:57.54ms
step:426/2330 train_time:24514ms step_avg:57.54ms
step:427/2330 train_time:24570ms step_avg:57.54ms
step:428/2330 train_time:24631ms step_avg:57.55ms
step:429/2330 train_time:24687ms step_avg:57.55ms
step:430/2330 train_time:24746ms step_avg:57.55ms
step:431/2330 train_time:24802ms step_avg:57.54ms
step:432/2330 train_time:24862ms step_avg:57.55ms
step:433/2330 train_time:24917ms step_avg:57.55ms
step:434/2330 train_time:24977ms step_avg:57.55ms
step:435/2330 train_time:25033ms step_avg:57.55ms
step:436/2330 train_time:25093ms step_avg:57.55ms
step:437/2330 train_time:25149ms step_avg:57.55ms
step:438/2330 train_time:25208ms step_avg:57.55ms
step:439/2330 train_time:25265ms step_avg:57.55ms
step:440/2330 train_time:25323ms step_avg:57.55ms
step:441/2330 train_time:25379ms step_avg:57.55ms
step:442/2330 train_time:25438ms step_avg:57.55ms
step:443/2330 train_time:25494ms step_avg:57.55ms
step:444/2330 train_time:25554ms step_avg:57.55ms
step:445/2330 train_time:25610ms step_avg:57.55ms
step:446/2330 train_time:25670ms step_avg:57.56ms
step:447/2330 train_time:25726ms step_avg:57.55ms
step:448/2330 train_time:25785ms step_avg:57.56ms
step:449/2330 train_time:25841ms step_avg:57.55ms
step:450/2330 train_time:25900ms step_avg:57.56ms
step:451/2330 train_time:25956ms step_avg:57.55ms
step:452/2330 train_time:26016ms step_avg:57.56ms
step:453/2330 train_time:26072ms step_avg:57.55ms
step:454/2330 train_time:26132ms step_avg:57.56ms
step:455/2330 train_time:26188ms step_avg:57.56ms
step:456/2330 train_time:26248ms step_avg:57.56ms
step:457/2330 train_time:26303ms step_avg:57.56ms
step:458/2330 train_time:26364ms step_avg:57.56ms
step:459/2330 train_time:26419ms step_avg:57.56ms
step:460/2330 train_time:26478ms step_avg:57.56ms
step:461/2330 train_time:26534ms step_avg:57.56ms
step:462/2330 train_time:26594ms step_avg:57.56ms
step:463/2330 train_time:26651ms step_avg:57.56ms
step:464/2330 train_time:26710ms step_avg:57.56ms
step:465/2330 train_time:26766ms step_avg:57.56ms
step:466/2330 train_time:26825ms step_avg:57.57ms
step:467/2330 train_time:26881ms step_avg:57.56ms
step:468/2330 train_time:26940ms step_avg:57.56ms
step:469/2330 train_time:26997ms step_avg:57.56ms
step:470/2330 train_time:27056ms step_avg:57.57ms
step:471/2330 train_time:27113ms step_avg:57.56ms
step:472/2330 train_time:27172ms step_avg:57.57ms
step:473/2330 train_time:27228ms step_avg:57.56ms
step:474/2330 train_time:27287ms step_avg:57.57ms
step:475/2330 train_time:27343ms step_avg:57.56ms
step:476/2330 train_time:27402ms step_avg:57.57ms
step:477/2330 train_time:27458ms step_avg:57.56ms
step:478/2330 train_time:27517ms step_avg:57.57ms
step:479/2330 train_time:27573ms step_avg:57.56ms
step:480/2330 train_time:27632ms step_avg:57.57ms
step:481/2330 train_time:27688ms step_avg:57.56ms
step:482/2330 train_time:27748ms step_avg:57.57ms
step:483/2330 train_time:27804ms step_avg:57.56ms
step:484/2330 train_time:27863ms step_avg:57.57ms
step:485/2330 train_time:27919ms step_avg:57.56ms
step:486/2330 train_time:27978ms step_avg:57.57ms
step:487/2330 train_time:28035ms step_avg:57.57ms
step:488/2330 train_time:28094ms step_avg:57.57ms
step:489/2330 train_time:28149ms step_avg:57.57ms
step:490/2330 train_time:28209ms step_avg:57.57ms
step:491/2330 train_time:28265ms step_avg:57.57ms
step:492/2330 train_time:28324ms step_avg:57.57ms
step:493/2330 train_time:28379ms step_avg:57.56ms
step:494/2330 train_time:28439ms step_avg:57.57ms
step:495/2330 train_time:28496ms step_avg:57.57ms
step:496/2330 train_time:28556ms step_avg:57.57ms
step:497/2330 train_time:28612ms step_avg:57.57ms
step:498/2330 train_time:28671ms step_avg:57.57ms
step:499/2330 train_time:28727ms step_avg:57.57ms
step:500/2330 train_time:28786ms step_avg:57.57ms
step:500/2330 val_loss:4.4100 train_time:28866ms step_avg:57.73ms
step:501/2330 train_time:28884ms step_avg:57.65ms
step:502/2330 train_time:28904ms step_avg:57.58ms
step:503/2330 train_time:28962ms step_avg:57.58ms
step:504/2330 train_time:29024ms step_avg:57.59ms
step:505/2330 train_time:29081ms step_avg:57.59ms
step:506/2330 train_time:29142ms step_avg:57.59ms
step:507/2330 train_time:29198ms step_avg:57.59ms
step:508/2330 train_time:29257ms step_avg:57.59ms
step:509/2330 train_time:29312ms step_avg:57.59ms
step:510/2330 train_time:29371ms step_avg:57.59ms
step:511/2330 train_time:29426ms step_avg:57.59ms
step:512/2330 train_time:29485ms step_avg:57.59ms
step:513/2330 train_time:29540ms step_avg:57.58ms
step:514/2330 train_time:29599ms step_avg:57.59ms
step:515/2330 train_time:29654ms step_avg:57.58ms
step:516/2330 train_time:29713ms step_avg:57.58ms
step:517/2330 train_time:29768ms step_avg:57.58ms
step:518/2330 train_time:29827ms step_avg:57.58ms
step:519/2330 train_time:29884ms step_avg:57.58ms
step:520/2330 train_time:29943ms step_avg:57.58ms
step:521/2330 train_time:30000ms step_avg:57.58ms
step:522/2330 train_time:30061ms step_avg:57.59ms
step:523/2330 train_time:30118ms step_avg:57.59ms
step:524/2330 train_time:30178ms step_avg:57.59ms
step:525/2330 train_time:30234ms step_avg:57.59ms
step:526/2330 train_time:30292ms step_avg:57.59ms
step:527/2330 train_time:30347ms step_avg:57.59ms
step:528/2330 train_time:30407ms step_avg:57.59ms
step:529/2330 train_time:30463ms step_avg:57.59ms
step:530/2330 train_time:30522ms step_avg:57.59ms
step:531/2330 train_time:30578ms step_avg:57.59ms
step:532/2330 train_time:30636ms step_avg:57.59ms
step:533/2330 train_time:30692ms step_avg:57.58ms
step:534/2330 train_time:30750ms step_avg:57.59ms
step:535/2330 train_time:30806ms step_avg:57.58ms
step:536/2330 train_time:30866ms step_avg:57.58ms
step:537/2330 train_time:30923ms step_avg:57.58ms
step:538/2330 train_time:30983ms step_avg:57.59ms
step:539/2330 train_time:31040ms step_avg:57.59ms
step:540/2330 train_time:31099ms step_avg:57.59ms
step:541/2330 train_time:31155ms step_avg:57.59ms
step:542/2330 train_time:31215ms step_avg:57.59ms
step:543/2330 train_time:31272ms step_avg:57.59ms
step:544/2330 train_time:31330ms step_avg:57.59ms
step:545/2330 train_time:31386ms step_avg:57.59ms
step:546/2330 train_time:31446ms step_avg:57.59ms
step:547/2330 train_time:31501ms step_avg:57.59ms
step:548/2330 train_time:31560ms step_avg:57.59ms
step:549/2330 train_time:31616ms step_avg:57.59ms
step:550/2330 train_time:31675ms step_avg:57.59ms
step:551/2330 train_time:31732ms step_avg:57.59ms
step:552/2330 train_time:31790ms step_avg:57.59ms
step:553/2330 train_time:31846ms step_avg:57.59ms
step:554/2330 train_time:31905ms step_avg:57.59ms
step:555/2330 train_time:31962ms step_avg:57.59ms
step:556/2330 train_time:32022ms step_avg:57.59ms
step:557/2330 train_time:32079ms step_avg:57.59ms
step:558/2330 train_time:32138ms step_avg:57.60ms
step:559/2330 train_time:32195ms step_avg:57.59ms
step:560/2330 train_time:32254ms step_avg:57.60ms
step:561/2330 train_time:32309ms step_avg:57.59ms
step:562/2330 train_time:32369ms step_avg:57.60ms
step:563/2330 train_time:32425ms step_avg:57.59ms
step:564/2330 train_time:32485ms step_avg:57.60ms
step:565/2330 train_time:32541ms step_avg:57.59ms
step:566/2330 train_time:32600ms step_avg:57.60ms
step:567/2330 train_time:32656ms step_avg:57.59ms
step:568/2330 train_time:32715ms step_avg:57.60ms
step:569/2330 train_time:32771ms step_avg:57.59ms
step:570/2330 train_time:32831ms step_avg:57.60ms
step:571/2330 train_time:32886ms step_avg:57.59ms
step:572/2330 train_time:32946ms step_avg:57.60ms
step:573/2330 train_time:33003ms step_avg:57.60ms
step:574/2330 train_time:33062ms step_avg:57.60ms
step:575/2330 train_time:33118ms step_avg:57.60ms
step:576/2330 train_time:33178ms step_avg:57.60ms
step:577/2330 train_time:33234ms step_avg:57.60ms
step:578/2330 train_time:33294ms step_avg:57.60ms
step:579/2330 train_time:33350ms step_avg:57.60ms
step:580/2330 train_time:33409ms step_avg:57.60ms
step:581/2330 train_time:33466ms step_avg:57.60ms
step:582/2330 train_time:33525ms step_avg:57.60ms
step:583/2330 train_time:33582ms step_avg:57.60ms
step:584/2330 train_time:33641ms step_avg:57.60ms
step:585/2330 train_time:33697ms step_avg:57.60ms
step:586/2330 train_time:33755ms step_avg:57.60ms
step:587/2330 train_time:33811ms step_avg:57.60ms
step:588/2330 train_time:33871ms step_avg:57.60ms
step:589/2330 train_time:33926ms step_avg:57.60ms
step:590/2330 train_time:33986ms step_avg:57.60ms
step:591/2330 train_time:34042ms step_avg:57.60ms
step:592/2330 train_time:34102ms step_avg:57.60ms
step:593/2330 train_time:34159ms step_avg:57.60ms
step:594/2330 train_time:34218ms step_avg:57.61ms
step:595/2330 train_time:34273ms step_avg:57.60ms
step:596/2330 train_time:34333ms step_avg:57.61ms
step:597/2330 train_time:34389ms step_avg:57.60ms
step:598/2330 train_time:34448ms step_avg:57.61ms
step:599/2330 train_time:34505ms step_avg:57.60ms
step:600/2330 train_time:34564ms step_avg:57.61ms
step:601/2330 train_time:34621ms step_avg:57.61ms
step:602/2330 train_time:34681ms step_avg:57.61ms
step:603/2330 train_time:34737ms step_avg:57.61ms
step:604/2330 train_time:34796ms step_avg:57.61ms
step:605/2330 train_time:34852ms step_avg:57.61ms
step:606/2330 train_time:34911ms step_avg:57.61ms
step:607/2330 train_time:34967ms step_avg:57.61ms
step:608/2330 train_time:35026ms step_avg:57.61ms
step:609/2330 train_time:35082ms step_avg:57.61ms
step:610/2330 train_time:35142ms step_avg:57.61ms
step:611/2330 train_time:35198ms step_avg:57.61ms
step:612/2330 train_time:35257ms step_avg:57.61ms
step:613/2330 train_time:35313ms step_avg:57.61ms
step:614/2330 train_time:35373ms step_avg:57.61ms
step:615/2330 train_time:35429ms step_avg:57.61ms
step:616/2330 train_time:35488ms step_avg:57.61ms
step:617/2330 train_time:35545ms step_avg:57.61ms
step:618/2330 train_time:35604ms step_avg:57.61ms
step:619/2330 train_time:35660ms step_avg:57.61ms
step:620/2330 train_time:35719ms step_avg:57.61ms
step:621/2330 train_time:35775ms step_avg:57.61ms
step:622/2330 train_time:35834ms step_avg:57.61ms
step:623/2330 train_time:35890ms step_avg:57.61ms
step:624/2330 train_time:35950ms step_avg:57.61ms
step:625/2330 train_time:36005ms step_avg:57.61ms
step:626/2330 train_time:36065ms step_avg:57.61ms
step:627/2330 train_time:36122ms step_avg:57.61ms
step:628/2330 train_time:36181ms step_avg:57.61ms
step:629/2330 train_time:36238ms step_avg:57.61ms
step:630/2330 train_time:36296ms step_avg:57.61ms
step:631/2330 train_time:36352ms step_avg:57.61ms
step:632/2330 train_time:36411ms step_avg:57.61ms
step:633/2330 train_time:36467ms step_avg:57.61ms
step:634/2330 train_time:36527ms step_avg:57.61ms
step:635/2330 train_time:36583ms step_avg:57.61ms
step:636/2330 train_time:36643ms step_avg:57.61ms
step:637/2330 train_time:36699ms step_avg:57.61ms
step:638/2330 train_time:36759ms step_avg:57.62ms
step:639/2330 train_time:36814ms step_avg:57.61ms
step:640/2330 train_time:36874ms step_avg:57.62ms
step:641/2330 train_time:36929ms step_avg:57.61ms
step:642/2330 train_time:36988ms step_avg:57.61ms
step:643/2330 train_time:37044ms step_avg:57.61ms
step:644/2330 train_time:37104ms step_avg:57.62ms
step:645/2330 train_time:37160ms step_avg:57.61ms
step:646/2330 train_time:37220ms step_avg:57.62ms
step:647/2330 train_time:37276ms step_avg:57.61ms
step:648/2330 train_time:37335ms step_avg:57.62ms
step:649/2330 train_time:37391ms step_avg:57.61ms
step:650/2330 train_time:37450ms step_avg:57.62ms
step:651/2330 train_time:37506ms step_avg:57.61ms
step:652/2330 train_time:37566ms step_avg:57.62ms
step:653/2330 train_time:37622ms step_avg:57.61ms
step:654/2330 train_time:37682ms step_avg:57.62ms
step:655/2330 train_time:37739ms step_avg:57.62ms
step:656/2330 train_time:37798ms step_avg:57.62ms
step:657/2330 train_time:37854ms step_avg:57.62ms
step:658/2330 train_time:37912ms step_avg:57.62ms
step:659/2330 train_time:37968ms step_avg:57.62ms
step:660/2330 train_time:38028ms step_avg:57.62ms
step:661/2330 train_time:38085ms step_avg:57.62ms
step:662/2330 train_time:38143ms step_avg:57.62ms
step:663/2330 train_time:38199ms step_avg:57.62ms
step:664/2330 train_time:38259ms step_avg:57.62ms
step:665/2330 train_time:38315ms step_avg:57.62ms
step:666/2330 train_time:38375ms step_avg:57.62ms
step:667/2330 train_time:38430ms step_avg:57.62ms
step:668/2330 train_time:38490ms step_avg:57.62ms
step:669/2330 train_time:38546ms step_avg:57.62ms
step:670/2330 train_time:38606ms step_avg:57.62ms
step:671/2330 train_time:38662ms step_avg:57.62ms
step:672/2330 train_time:38722ms step_avg:57.62ms
step:673/2330 train_time:38779ms step_avg:57.62ms
step:674/2330 train_time:38838ms step_avg:57.62ms
step:675/2330 train_time:38894ms step_avg:57.62ms
step:676/2330 train_time:38953ms step_avg:57.62ms
step:677/2330 train_time:39008ms step_avg:57.62ms
step:678/2330 train_time:39069ms step_avg:57.62ms
step:679/2330 train_time:39125ms step_avg:57.62ms
step:680/2330 train_time:39184ms step_avg:57.62ms
step:681/2330 train_time:39240ms step_avg:57.62ms
step:682/2330 train_time:39299ms step_avg:57.62ms
step:683/2330 train_time:39356ms step_avg:57.62ms
step:684/2330 train_time:39414ms step_avg:57.62ms
step:685/2330 train_time:39470ms step_avg:57.62ms
step:686/2330 train_time:39529ms step_avg:57.62ms
step:687/2330 train_time:39585ms step_avg:57.62ms
step:688/2330 train_time:39645ms step_avg:57.62ms
step:689/2330 train_time:39701ms step_avg:57.62ms
step:690/2330 train_time:39762ms step_avg:57.63ms
step:691/2330 train_time:39818ms step_avg:57.62ms
step:692/2330 train_time:39878ms step_avg:57.63ms
step:693/2330 train_time:39933ms step_avg:57.62ms
step:694/2330 train_time:39993ms step_avg:57.63ms
step:695/2330 train_time:40049ms step_avg:57.62ms
step:696/2330 train_time:40109ms step_avg:57.63ms
step:697/2330 train_time:40165ms step_avg:57.63ms
step:698/2330 train_time:40224ms step_avg:57.63ms
step:699/2330 train_time:40281ms step_avg:57.63ms
step:700/2330 train_time:40340ms step_avg:57.63ms
step:701/2330 train_time:40396ms step_avg:57.63ms
step:702/2330 train_time:40455ms step_avg:57.63ms
step:703/2330 train_time:40511ms step_avg:57.63ms
step:704/2330 train_time:40570ms step_avg:57.63ms
step:705/2330 train_time:40626ms step_avg:57.63ms
step:706/2330 train_time:40686ms step_avg:57.63ms
step:707/2330 train_time:40742ms step_avg:57.63ms
step:708/2330 train_time:40802ms step_avg:57.63ms
step:709/2330 train_time:40858ms step_avg:57.63ms
step:710/2330 train_time:40917ms step_avg:57.63ms
step:711/2330 train_time:40974ms step_avg:57.63ms
step:712/2330 train_time:41032ms step_avg:57.63ms
step:713/2330 train_time:41087ms step_avg:57.63ms
step:714/2330 train_time:41147ms step_avg:57.63ms
step:715/2330 train_time:41203ms step_avg:57.63ms
step:716/2330 train_time:41262ms step_avg:57.63ms
step:717/2330 train_time:41319ms step_avg:57.63ms
step:718/2330 train_time:41378ms step_avg:57.63ms
step:719/2330 train_time:41434ms step_avg:57.63ms
step:720/2330 train_time:41493ms step_avg:57.63ms
step:721/2330 train_time:41548ms step_avg:57.63ms
step:722/2330 train_time:41608ms step_avg:57.63ms
step:723/2330 train_time:41665ms step_avg:57.63ms
step:724/2330 train_time:41724ms step_avg:57.63ms
step:725/2330 train_time:41780ms step_avg:57.63ms
step:726/2330 train_time:41840ms step_avg:57.63ms
step:727/2330 train_time:41896ms step_avg:57.63ms
step:728/2330 train_time:41955ms step_avg:57.63ms
step:729/2330 train_time:42011ms step_avg:57.63ms
step:730/2330 train_time:42070ms step_avg:57.63ms
step:731/2330 train_time:42126ms step_avg:57.63ms
step:732/2330 train_time:42186ms step_avg:57.63ms
step:733/2330 train_time:42244ms step_avg:57.63ms
step:734/2330 train_time:42303ms step_avg:57.63ms
step:735/2330 train_time:42359ms step_avg:57.63ms
step:736/2330 train_time:42418ms step_avg:57.63ms
step:737/2330 train_time:42473ms step_avg:57.63ms
step:738/2330 train_time:42533ms step_avg:57.63ms
step:739/2330 train_time:42589ms step_avg:57.63ms
step:740/2330 train_time:42648ms step_avg:57.63ms
step:741/2330 train_time:42704ms step_avg:57.63ms
step:742/2330 train_time:42764ms step_avg:57.63ms
step:743/2330 train_time:42821ms step_avg:57.63ms
step:744/2330 train_time:42881ms step_avg:57.64ms
step:745/2330 train_time:42937ms step_avg:57.63ms
step:746/2330 train_time:42996ms step_avg:57.64ms
step:747/2330 train_time:43052ms step_avg:57.63ms
step:748/2330 train_time:43111ms step_avg:57.63ms
step:749/2330 train_time:43167ms step_avg:57.63ms
step:750/2330 train_time:43226ms step_avg:57.64ms
step:750/2330 val_loss:4.2367 train_time:43307ms step_avg:57.74ms
step:751/2330 train_time:43323ms step_avg:57.69ms
step:752/2330 train_time:43345ms step_avg:57.64ms
step:753/2330 train_time:43401ms step_avg:57.64ms
step:754/2330 train_time:43467ms step_avg:57.65ms
step:755/2330 train_time:43523ms step_avg:57.65ms
step:756/2330 train_time:43583ms step_avg:57.65ms
step:757/2330 train_time:43638ms step_avg:57.65ms
step:758/2330 train_time:43697ms step_avg:57.65ms
step:759/2330 train_time:43753ms step_avg:57.65ms
step:760/2330 train_time:43811ms step_avg:57.65ms
step:761/2330 train_time:43867ms step_avg:57.64ms
step:762/2330 train_time:43925ms step_avg:57.65ms
step:763/2330 train_time:43981ms step_avg:57.64ms
step:764/2330 train_time:44039ms step_avg:57.64ms
step:765/2330 train_time:44096ms step_avg:57.64ms
step:766/2330 train_time:44154ms step_avg:57.64ms
step:767/2330 train_time:44211ms step_avg:57.64ms
step:768/2330 train_time:44271ms step_avg:57.64ms
step:769/2330 train_time:44329ms step_avg:57.65ms
step:770/2330 train_time:44391ms step_avg:57.65ms
step:771/2330 train_time:44449ms step_avg:57.65ms
step:772/2330 train_time:44510ms step_avg:57.66ms
step:773/2330 train_time:44568ms step_avg:57.66ms
step:774/2330 train_time:44628ms step_avg:57.66ms
step:775/2330 train_time:44686ms step_avg:57.66ms
step:776/2330 train_time:44746ms step_avg:57.66ms
step:777/2330 train_time:44803ms step_avg:57.66ms
step:778/2330 train_time:44862ms step_avg:57.66ms
step:779/2330 train_time:44919ms step_avg:57.66ms
step:780/2330 train_time:44978ms step_avg:57.66ms
step:781/2330 train_time:45034ms step_avg:57.66ms
step:782/2330 train_time:45094ms step_avg:57.66ms
step:783/2330 train_time:45151ms step_avg:57.66ms
step:784/2330 train_time:45210ms step_avg:57.67ms
step:785/2330 train_time:45267ms step_avg:57.67ms
step:786/2330 train_time:45328ms step_avg:57.67ms
step:787/2330 train_time:45385ms step_avg:57.67ms
step:788/2330 train_time:45446ms step_avg:57.67ms
step:789/2330 train_time:45502ms step_avg:57.67ms
step:790/2330 train_time:45564ms step_avg:57.68ms
step:791/2330 train_time:45621ms step_avg:57.67ms
step:792/2330 train_time:45681ms step_avg:57.68ms
step:793/2330 train_time:45738ms step_avg:57.68ms
step:794/2330 train_time:45797ms step_avg:57.68ms
step:795/2330 train_time:45854ms step_avg:57.68ms
step:796/2330 train_time:45914ms step_avg:57.68ms
step:797/2330 train_time:45971ms step_avg:57.68ms
step:798/2330 train_time:46031ms step_avg:57.68ms
step:799/2330 train_time:46087ms step_avg:57.68ms
step:800/2330 train_time:46147ms step_avg:57.68ms
step:801/2330 train_time:46203ms step_avg:57.68ms
step:802/2330 train_time:46264ms step_avg:57.69ms
step:803/2330 train_time:46321ms step_avg:57.68ms
step:804/2330 train_time:46380ms step_avg:57.69ms
step:805/2330 train_time:46438ms step_avg:57.69ms
step:806/2330 train_time:46498ms step_avg:57.69ms
step:807/2330 train_time:46555ms step_avg:57.69ms
step:808/2330 train_time:46616ms step_avg:57.69ms
step:809/2330 train_time:46673ms step_avg:57.69ms
step:810/2330 train_time:46733ms step_avg:57.69ms
step:811/2330 train_time:46790ms step_avg:57.69ms
step:812/2330 train_time:46849ms step_avg:57.70ms
step:813/2330 train_time:46906ms step_avg:57.70ms
step:814/2330 train_time:46966ms step_avg:57.70ms
step:815/2330 train_time:47022ms step_avg:57.70ms
step:816/2330 train_time:47082ms step_avg:57.70ms
step:817/2330 train_time:47138ms step_avg:57.70ms
step:818/2330 train_time:47198ms step_avg:57.70ms
step:819/2330 train_time:47254ms step_avg:57.70ms
step:820/2330 train_time:47315ms step_avg:57.70ms
step:821/2330 train_time:47372ms step_avg:57.70ms
step:822/2330 train_time:47432ms step_avg:57.70ms
step:823/2330 train_time:47489ms step_avg:57.70ms
step:824/2330 train_time:47550ms step_avg:57.71ms
step:825/2330 train_time:47607ms step_avg:57.71ms
step:826/2330 train_time:47667ms step_avg:57.71ms
step:827/2330 train_time:47725ms step_avg:57.71ms
step:828/2330 train_time:47785ms step_avg:57.71ms
step:829/2330 train_time:47842ms step_avg:57.71ms
step:830/2330 train_time:47901ms step_avg:57.71ms
step:831/2330 train_time:47958ms step_avg:57.71ms
step:832/2330 train_time:48018ms step_avg:57.71ms
step:833/2330 train_time:48074ms step_avg:57.71ms
step:834/2330 train_time:48134ms step_avg:57.71ms
step:835/2330 train_time:48191ms step_avg:57.71ms
step:836/2330 train_time:48252ms step_avg:57.72ms
step:837/2330 train_time:48309ms step_avg:57.72ms
step:838/2330 train_time:48368ms step_avg:57.72ms
step:839/2330 train_time:48426ms step_avg:57.72ms
step:840/2330 train_time:48485ms step_avg:57.72ms
step:841/2330 train_time:48542ms step_avg:57.72ms
step:842/2330 train_time:48602ms step_avg:57.72ms
step:843/2330 train_time:48658ms step_avg:57.72ms
step:844/2330 train_time:48719ms step_avg:57.72ms
step:845/2330 train_time:48776ms step_avg:57.72ms
step:846/2330 train_time:48836ms step_avg:57.73ms
step:847/2330 train_time:48893ms step_avg:57.72ms
step:848/2330 train_time:48952ms step_avg:57.73ms
step:849/2330 train_time:49010ms step_avg:57.73ms
step:850/2330 train_time:49069ms step_avg:57.73ms
step:851/2330 train_time:49127ms step_avg:57.73ms
step:852/2330 train_time:49186ms step_avg:57.73ms
step:853/2330 train_time:49244ms step_avg:57.73ms
step:854/2330 train_time:49303ms step_avg:57.73ms
step:855/2330 train_time:49359ms step_avg:57.73ms
step:856/2330 train_time:49419ms step_avg:57.73ms
step:857/2330 train_time:49476ms step_avg:57.73ms
step:858/2330 train_time:49536ms step_avg:57.73ms
step:859/2330 train_time:49593ms step_avg:57.73ms
step:860/2330 train_time:49653ms step_avg:57.74ms
step:861/2330 train_time:49711ms step_avg:57.74ms
step:862/2330 train_time:49771ms step_avg:57.74ms
step:863/2330 train_time:49828ms step_avg:57.74ms
step:864/2330 train_time:49888ms step_avg:57.74ms
step:865/2330 train_time:49945ms step_avg:57.74ms
step:866/2330 train_time:50004ms step_avg:57.74ms
step:867/2330 train_time:50062ms step_avg:57.74ms
step:868/2330 train_time:50121ms step_avg:57.74ms
step:869/2330 train_time:50178ms step_avg:57.74ms
step:870/2330 train_time:50237ms step_avg:57.74ms
step:871/2330 train_time:50294ms step_avg:57.74ms
step:872/2330 train_time:50354ms step_avg:57.75ms
step:873/2330 train_time:50412ms step_avg:57.75ms
step:874/2330 train_time:50472ms step_avg:57.75ms
step:875/2330 train_time:50529ms step_avg:57.75ms
step:876/2330 train_time:50589ms step_avg:57.75ms
step:877/2330 train_time:50646ms step_avg:57.75ms
step:878/2330 train_time:50706ms step_avg:57.75ms
step:879/2330 train_time:50762ms step_avg:57.75ms
step:880/2330 train_time:50823ms step_avg:57.75ms
step:881/2330 train_time:50879ms step_avg:57.75ms
step:882/2330 train_time:50941ms step_avg:57.76ms
step:883/2330 train_time:50997ms step_avg:57.75ms
step:884/2330 train_time:51057ms step_avg:57.76ms
step:885/2330 train_time:51114ms step_avg:57.76ms
step:886/2330 train_time:51174ms step_avg:57.76ms
step:887/2330 train_time:51231ms step_avg:57.76ms
step:888/2330 train_time:51291ms step_avg:57.76ms
step:889/2330 train_time:51348ms step_avg:57.76ms
step:890/2330 train_time:51408ms step_avg:57.76ms
step:891/2330 train_time:51465ms step_avg:57.76ms
step:892/2330 train_time:51525ms step_avg:57.76ms
step:893/2330 train_time:51582ms step_avg:57.76ms
step:894/2330 train_time:51640ms step_avg:57.76ms
step:895/2330 train_time:51697ms step_avg:57.76ms
step:896/2330 train_time:51758ms step_avg:57.77ms
step:897/2330 train_time:51815ms step_avg:57.76ms
step:898/2330 train_time:51875ms step_avg:57.77ms
step:899/2330 train_time:51932ms step_avg:57.77ms
step:900/2330 train_time:51992ms step_avg:57.77ms
step:901/2330 train_time:52049ms step_avg:57.77ms
step:902/2330 train_time:52109ms step_avg:57.77ms
step:903/2330 train_time:52166ms step_avg:57.77ms
step:904/2330 train_time:52226ms step_avg:57.77ms
step:905/2330 train_time:52283ms step_avg:57.77ms
step:906/2330 train_time:52342ms step_avg:57.77ms
step:907/2330 train_time:52400ms step_avg:57.77ms
step:908/2330 train_time:52459ms step_avg:57.77ms
step:909/2330 train_time:52516ms step_avg:57.77ms
step:910/2330 train_time:52575ms step_avg:57.77ms
step:911/2330 train_time:52632ms step_avg:57.77ms
step:912/2330 train_time:52693ms step_avg:57.78ms
step:913/2330 train_time:52750ms step_avg:57.78ms
step:914/2330 train_time:52810ms step_avg:57.78ms
step:915/2330 train_time:52868ms step_avg:57.78ms
step:916/2330 train_time:52928ms step_avg:57.78ms
step:917/2330 train_time:52985ms step_avg:57.78ms
step:918/2330 train_time:53045ms step_avg:57.78ms
step:919/2330 train_time:53102ms step_avg:57.78ms
step:920/2330 train_time:53162ms step_avg:57.78ms
step:921/2330 train_time:53218ms step_avg:57.78ms
step:922/2330 train_time:53278ms step_avg:57.79ms
step:923/2330 train_time:53335ms step_avg:57.78ms
step:924/2330 train_time:53395ms step_avg:57.79ms
step:925/2330 train_time:53452ms step_avg:57.79ms
step:926/2330 train_time:53511ms step_avg:57.79ms
step:927/2330 train_time:53568ms step_avg:57.79ms
step:928/2330 train_time:53628ms step_avg:57.79ms
step:929/2330 train_time:53685ms step_avg:57.79ms
step:930/2330 train_time:53745ms step_avg:57.79ms
step:931/2330 train_time:53803ms step_avg:57.79ms
step:932/2330 train_time:53862ms step_avg:57.79ms
step:933/2330 train_time:53920ms step_avg:57.79ms
step:934/2330 train_time:53979ms step_avg:57.79ms
step:935/2330 train_time:54036ms step_avg:57.79ms
step:936/2330 train_time:54096ms step_avg:57.80ms
step:937/2330 train_time:54154ms step_avg:57.79ms
step:938/2330 train_time:54213ms step_avg:57.80ms
step:939/2330 train_time:54271ms step_avg:57.80ms
step:940/2330 train_time:54331ms step_avg:57.80ms
step:941/2330 train_time:54388ms step_avg:57.80ms
step:942/2330 train_time:54448ms step_avg:57.80ms
step:943/2330 train_time:54504ms step_avg:57.80ms
step:944/2330 train_time:54564ms step_avg:57.80ms
step:945/2330 train_time:54622ms step_avg:57.80ms
step:946/2330 train_time:54681ms step_avg:57.80ms
step:947/2330 train_time:54739ms step_avg:57.80ms
step:948/2330 train_time:54798ms step_avg:57.80ms
step:949/2330 train_time:54854ms step_avg:57.80ms
step:950/2330 train_time:54915ms step_avg:57.81ms
step:951/2330 train_time:54972ms step_avg:57.80ms
step:952/2330 train_time:55032ms step_avg:57.81ms
step:953/2330 train_time:55090ms step_avg:57.81ms
step:954/2330 train_time:55149ms step_avg:57.81ms
step:955/2330 train_time:55206ms step_avg:57.81ms
step:956/2330 train_time:55266ms step_avg:57.81ms
step:957/2330 train_time:55322ms step_avg:57.81ms
step:958/2330 train_time:55382ms step_avg:57.81ms
step:959/2330 train_time:55439ms step_avg:57.81ms
step:960/2330 train_time:55498ms step_avg:57.81ms
step:961/2330 train_time:55555ms step_avg:57.81ms
step:962/2330 train_time:55615ms step_avg:57.81ms
step:963/2330 train_time:55673ms step_avg:57.81ms
step:964/2330 train_time:55733ms step_avg:57.81ms
step:965/2330 train_time:55790ms step_avg:57.81ms
step:966/2330 train_time:55850ms step_avg:57.82ms
step:967/2330 train_time:55907ms step_avg:57.81ms
step:968/2330 train_time:55967ms step_avg:57.82ms
step:969/2330 train_time:56025ms step_avg:57.82ms
step:970/2330 train_time:56085ms step_avg:57.82ms
step:971/2330 train_time:56142ms step_avg:57.82ms
step:972/2330 train_time:56201ms step_avg:57.82ms
step:973/2330 train_time:56258ms step_avg:57.82ms
step:974/2330 train_time:56318ms step_avg:57.82ms
step:975/2330 train_time:56374ms step_avg:57.82ms
step:976/2330 train_time:56435ms step_avg:57.82ms
step:977/2330 train_time:56492ms step_avg:57.82ms
step:978/2330 train_time:56552ms step_avg:57.82ms
step:979/2330 train_time:56609ms step_avg:57.82ms
step:980/2330 train_time:56669ms step_avg:57.83ms
step:981/2330 train_time:56727ms step_avg:57.83ms
step:982/2330 train_time:56787ms step_avg:57.83ms
step:983/2330 train_time:56844ms step_avg:57.83ms
step:984/2330 train_time:56904ms step_avg:57.83ms
step:985/2330 train_time:56960ms step_avg:57.83ms
step:986/2330 train_time:57021ms step_avg:57.83ms
step:987/2330 train_time:57078ms step_avg:57.83ms
step:988/2330 train_time:57139ms step_avg:57.83ms
step:989/2330 train_time:57196ms step_avg:57.83ms
step:990/2330 train_time:57257ms step_avg:57.83ms
step:991/2330 train_time:57313ms step_avg:57.83ms
step:992/2330 train_time:57373ms step_avg:57.84ms
step:993/2330 train_time:57431ms step_avg:57.84ms
step:994/2330 train_time:57490ms step_avg:57.84ms
step:995/2330 train_time:57547ms step_avg:57.84ms
step:996/2330 train_time:57607ms step_avg:57.84ms
step:997/2330 train_time:57664ms step_avg:57.84ms
step:998/2330 train_time:57724ms step_avg:57.84ms
step:999/2330 train_time:57782ms step_avg:57.84ms
step:1000/2330 train_time:57841ms step_avg:57.84ms
step:1000/2330 val_loss:4.0709 train_time:57922ms step_avg:57.92ms
step:1001/2330 train_time:57941ms step_avg:57.88ms
step:1002/2330 train_time:57961ms step_avg:57.85ms
step:1003/2330 train_time:58015ms step_avg:57.84ms
step:1004/2330 train_time:58078ms step_avg:57.85ms
step:1005/2330 train_time:58134ms step_avg:57.84ms
step:1006/2330 train_time:58198ms step_avg:57.85ms
step:1007/2330 train_time:58254ms step_avg:57.85ms
step:1008/2330 train_time:58314ms step_avg:57.85ms
step:1009/2330 train_time:58369ms step_avg:57.85ms
step:1010/2330 train_time:58429ms step_avg:57.85ms
step:1011/2330 train_time:58485ms step_avg:57.85ms
step:1012/2330 train_time:58544ms step_avg:57.85ms
step:1013/2330 train_time:58601ms step_avg:57.85ms
step:1014/2330 train_time:58660ms step_avg:57.85ms
step:1015/2330 train_time:58716ms step_avg:57.85ms
step:1016/2330 train_time:58775ms step_avg:57.85ms
step:1017/2330 train_time:58833ms step_avg:57.85ms
step:1018/2330 train_time:58900ms step_avg:57.86ms
step:1019/2330 train_time:58958ms step_avg:57.86ms
step:1020/2330 train_time:59019ms step_avg:57.86ms
step:1021/2330 train_time:59076ms step_avg:57.86ms
step:1022/2330 train_time:59137ms step_avg:57.86ms
step:1023/2330 train_time:59194ms step_avg:57.86ms
step:1024/2330 train_time:59253ms step_avg:57.86ms
step:1025/2330 train_time:59310ms step_avg:57.86ms
step:1026/2330 train_time:59369ms step_avg:57.86ms
step:1027/2330 train_time:59426ms step_avg:57.86ms
step:1028/2330 train_time:59484ms step_avg:57.86ms
step:1029/2330 train_time:59541ms step_avg:57.86ms
step:1030/2330 train_time:59600ms step_avg:57.86ms
step:1031/2330 train_time:59656ms step_avg:57.86ms
step:1032/2330 train_time:59716ms step_avg:57.86ms
step:1033/2330 train_time:59773ms step_avg:57.86ms
step:1034/2330 train_time:59834ms step_avg:57.87ms
step:1035/2330 train_time:59892ms step_avg:57.87ms
step:1036/2330 train_time:59953ms step_avg:57.87ms
step:1037/2330 train_time:60010ms step_avg:57.87ms
step:1038/2330 train_time:60071ms step_avg:57.87ms
step:1039/2330 train_time:60128ms step_avg:57.87ms
step:1040/2330 train_time:60189ms step_avg:57.87ms
step:1041/2330 train_time:60246ms step_avg:57.87ms
step:1042/2330 train_time:60307ms step_avg:57.88ms
step:1043/2330 train_time:60363ms step_avg:57.87ms
step:1044/2330 train_time:60423ms step_avg:57.88ms
step:1045/2330 train_time:60480ms step_avg:57.88ms
step:1046/2330 train_time:60540ms step_avg:57.88ms
step:1047/2330 train_time:60596ms step_avg:57.88ms
step:1048/2330 train_time:60656ms step_avg:57.88ms
step:1049/2330 train_time:60713ms step_avg:57.88ms
step:1050/2330 train_time:60773ms step_avg:57.88ms
step:1051/2330 train_time:60831ms step_avg:57.88ms
step:1052/2330 train_time:60891ms step_avg:57.88ms
step:1053/2330 train_time:60948ms step_avg:57.88ms
step:1054/2330 train_time:61009ms step_avg:57.88ms
step:1055/2330 train_time:61065ms step_avg:57.88ms
step:1056/2330 train_time:61128ms step_avg:57.89ms
step:1057/2330 train_time:61185ms step_avg:57.89ms
step:1058/2330 train_time:61246ms step_avg:57.89ms
step:1059/2330 train_time:61302ms step_avg:57.89ms
step:1060/2330 train_time:61363ms step_avg:57.89ms
step:1061/2330 train_time:61420ms step_avg:57.89ms
step:1062/2330 train_time:61481ms step_avg:57.89ms
step:1063/2330 train_time:61537ms step_avg:57.89ms
step:1064/2330 train_time:61596ms step_avg:57.89ms
step:1065/2330 train_time:61653ms step_avg:57.89ms
step:1066/2330 train_time:61712ms step_avg:57.89ms
step:1067/2330 train_time:61769ms step_avg:57.89ms
step:1068/2330 train_time:61830ms step_avg:57.89ms
step:1069/2330 train_time:61886ms step_avg:57.89ms
step:1070/2330 train_time:61947ms step_avg:57.89ms
step:1071/2330 train_time:62004ms step_avg:57.89ms
step:1072/2330 train_time:62064ms step_avg:57.90ms
step:1073/2330 train_time:62121ms step_avg:57.89ms
step:1074/2330 train_time:62183ms step_avg:57.90ms
step:1075/2330 train_time:62241ms step_avg:57.90ms
step:1076/2330 train_time:62300ms step_avg:57.90ms
step:1077/2330 train_time:62357ms step_avg:57.90ms
step:1078/2330 train_time:62417ms step_avg:57.90ms
step:1079/2330 train_time:62474ms step_avg:57.90ms
step:1080/2330 train_time:62533ms step_avg:57.90ms
step:1081/2330 train_time:62590ms step_avg:57.90ms
step:1082/2330 train_time:62650ms step_avg:57.90ms
step:1083/2330 train_time:62707ms step_avg:57.90ms
step:1084/2330 train_time:62767ms step_avg:57.90ms
step:1085/2330 train_time:62824ms step_avg:57.90ms
step:1086/2330 train_time:62885ms step_avg:57.91ms
step:1087/2330 train_time:62943ms step_avg:57.90ms
step:1088/2330 train_time:63003ms step_avg:57.91ms
step:1089/2330 train_time:63060ms step_avg:57.91ms
step:1090/2330 train_time:63120ms step_avg:57.91ms
step:1091/2330 train_time:63177ms step_avg:57.91ms
step:1092/2330 train_time:63238ms step_avg:57.91ms
step:1093/2330 train_time:63295ms step_avg:57.91ms
step:1094/2330 train_time:63355ms step_avg:57.91ms
step:1095/2330 train_time:63412ms step_avg:57.91ms
step:1096/2330 train_time:63472ms step_avg:57.91ms
step:1097/2330 train_time:63529ms step_avg:57.91ms
step:1098/2330 train_time:63588ms step_avg:57.91ms
step:1099/2330 train_time:63646ms step_avg:57.91ms
step:1100/2330 train_time:63706ms step_avg:57.91ms
step:1101/2330 train_time:63763ms step_avg:57.91ms
step:1102/2330 train_time:63823ms step_avg:57.92ms
step:1103/2330 train_time:63880ms step_avg:57.91ms
step:1104/2330 train_time:63941ms step_avg:57.92ms
step:1105/2330 train_time:63999ms step_avg:57.92ms
step:1106/2330 train_time:64059ms step_avg:57.92ms
step:1107/2330 train_time:64116ms step_avg:57.92ms
step:1108/2330 train_time:64177ms step_avg:57.92ms
step:1109/2330 train_time:64233ms step_avg:57.92ms
step:1110/2330 train_time:64294ms step_avg:57.92ms
step:1111/2330 train_time:64351ms step_avg:57.92ms
step:1112/2330 train_time:64410ms step_avg:57.92ms
step:1113/2330 train_time:64467ms step_avg:57.92ms
step:1114/2330 train_time:64527ms step_avg:57.92ms
step:1115/2330 train_time:64583ms step_avg:57.92ms
step:1116/2330 train_time:64643ms step_avg:57.92ms
step:1117/2330 train_time:64701ms step_avg:57.92ms
step:1118/2330 train_time:64760ms step_avg:57.93ms
step:1119/2330 train_time:64818ms step_avg:57.92ms
step:1120/2330 train_time:64878ms step_avg:57.93ms
step:1121/2330 train_time:64935ms step_avg:57.93ms
step:1122/2330 train_time:64995ms step_avg:57.93ms
step:1123/2330 train_time:65052ms step_avg:57.93ms
step:1124/2330 train_time:65112ms step_avg:57.93ms
step:1125/2330 train_time:65169ms step_avg:57.93ms
step:1126/2330 train_time:65228ms step_avg:57.93ms
step:1127/2330 train_time:65286ms step_avg:57.93ms
step:1128/2330 train_time:65346ms step_avg:57.93ms
step:1129/2330 train_time:65403ms step_avg:57.93ms
step:1130/2330 train_time:65463ms step_avg:57.93ms
step:1131/2330 train_time:65521ms step_avg:57.93ms
step:1132/2330 train_time:65580ms step_avg:57.93ms
step:1133/2330 train_time:65637ms step_avg:57.93ms
step:1134/2330 train_time:65697ms step_avg:57.93ms
step:1135/2330 train_time:65754ms step_avg:57.93ms
step:1136/2330 train_time:65814ms step_avg:57.94ms
step:1137/2330 train_time:65871ms step_avg:57.93ms
step:1138/2330 train_time:65931ms step_avg:57.94ms
step:1139/2330 train_time:65987ms step_avg:57.93ms
step:1140/2330 train_time:66048ms step_avg:57.94ms
step:1141/2330 train_time:66105ms step_avg:57.94ms
step:1142/2330 train_time:66166ms step_avg:57.94ms
step:1143/2330 train_time:66223ms step_avg:57.94ms
step:1144/2330 train_time:66282ms step_avg:57.94ms
step:1145/2330 train_time:66340ms step_avg:57.94ms
step:1146/2330 train_time:66399ms step_avg:57.94ms
step:1147/2330 train_time:66457ms step_avg:57.94ms
step:1148/2330 train_time:66517ms step_avg:57.94ms
step:1149/2330 train_time:66574ms step_avg:57.94ms
step:1150/2330 train_time:66633ms step_avg:57.94ms
step:1151/2330 train_time:66690ms step_avg:57.94ms
step:1152/2330 train_time:66750ms step_avg:57.94ms
step:1153/2330 train_time:66807ms step_avg:57.94ms
step:1154/2330 train_time:66867ms step_avg:57.94ms
step:1155/2330 train_time:66924ms step_avg:57.94ms
step:1156/2330 train_time:66984ms step_avg:57.94ms
step:1157/2330 train_time:67041ms step_avg:57.94ms
step:1158/2330 train_time:67100ms step_avg:57.95ms
step:1159/2330 train_time:67158ms step_avg:57.95ms
step:1160/2330 train_time:67218ms step_avg:57.95ms
step:1161/2330 train_time:67275ms step_avg:57.95ms
step:1162/2330 train_time:67335ms step_avg:57.95ms
step:1163/2330 train_time:67392ms step_avg:57.95ms
step:1164/2330 train_time:67453ms step_avg:57.95ms
step:1165/2330 train_time:67510ms step_avg:57.95ms
step:1166/2330 train_time:67569ms step_avg:57.95ms
step:1167/2330 train_time:67626ms step_avg:57.95ms
step:1168/2330 train_time:67686ms step_avg:57.95ms
step:1169/2330 train_time:67743ms step_avg:57.95ms
step:1170/2330 train_time:67803ms step_avg:57.95ms
step:1171/2330 train_time:67860ms step_avg:57.95ms
step:1172/2330 train_time:67920ms step_avg:57.95ms
step:1173/2330 train_time:67978ms step_avg:57.95ms
step:1174/2330 train_time:68038ms step_avg:57.95ms
step:1175/2330 train_time:68095ms step_avg:57.95ms
step:1176/2330 train_time:68156ms step_avg:57.96ms
step:1177/2330 train_time:68212ms step_avg:57.95ms
step:1178/2330 train_time:68272ms step_avg:57.96ms
step:1179/2330 train_time:68329ms step_avg:57.95ms
step:1180/2330 train_time:68388ms step_avg:57.96ms
step:1181/2330 train_time:68446ms step_avg:57.96ms
step:1182/2330 train_time:68506ms step_avg:57.96ms
step:1183/2330 train_time:68563ms step_avg:57.96ms
step:1184/2330 train_time:68623ms step_avg:57.96ms
step:1185/2330 train_time:68680ms step_avg:57.96ms
step:1186/2330 train_time:68740ms step_avg:57.96ms
step:1187/2330 train_time:68797ms step_avg:57.96ms
step:1188/2330 train_time:68858ms step_avg:57.96ms
step:1189/2330 train_time:68914ms step_avg:57.96ms
step:1190/2330 train_time:68974ms step_avg:57.96ms
step:1191/2330 train_time:69031ms step_avg:57.96ms
step:1192/2330 train_time:69092ms step_avg:57.96ms
step:1193/2330 train_time:69149ms step_avg:57.96ms
step:1194/2330 train_time:69209ms step_avg:57.96ms
step:1195/2330 train_time:69265ms step_avg:57.96ms
step:1196/2330 train_time:69326ms step_avg:57.96ms
step:1197/2330 train_time:69383ms step_avg:57.96ms
step:1198/2330 train_time:69443ms step_avg:57.97ms
step:1199/2330 train_time:69501ms step_avg:57.97ms
step:1200/2330 train_time:69560ms step_avg:57.97ms
step:1201/2330 train_time:69617ms step_avg:57.97ms
step:1202/2330 train_time:69676ms step_avg:57.97ms
step:1203/2330 train_time:69733ms step_avg:57.97ms
step:1204/2330 train_time:69794ms step_avg:57.97ms
step:1205/2330 train_time:69851ms step_avg:57.97ms
step:1206/2330 train_time:69910ms step_avg:57.97ms
step:1207/2330 train_time:69966ms step_avg:57.97ms
step:1208/2330 train_time:70028ms step_avg:57.97ms
step:1209/2330 train_time:70085ms step_avg:57.97ms
step:1210/2330 train_time:70145ms step_avg:57.97ms
step:1211/2330 train_time:70202ms step_avg:57.97ms
step:1212/2330 train_time:70262ms step_avg:57.97ms
step:1213/2330 train_time:70319ms step_avg:57.97ms
step:1214/2330 train_time:70378ms step_avg:57.97ms
step:1215/2330 train_time:70435ms step_avg:57.97ms
step:1216/2330 train_time:70495ms step_avg:57.97ms
step:1217/2330 train_time:70552ms step_avg:57.97ms
step:1218/2330 train_time:70612ms step_avg:57.97ms
step:1219/2330 train_time:70669ms step_avg:57.97ms
step:1220/2330 train_time:70728ms step_avg:57.97ms
step:1221/2330 train_time:70784ms step_avg:57.97ms
step:1222/2330 train_time:70844ms step_avg:57.97ms
step:1223/2330 train_time:70901ms step_avg:57.97ms
step:1224/2330 train_time:70962ms step_avg:57.98ms
step:1225/2330 train_time:71019ms step_avg:57.97ms
step:1226/2330 train_time:71079ms step_avg:57.98ms
step:1227/2330 train_time:71136ms step_avg:57.98ms
step:1228/2330 train_time:71196ms step_avg:57.98ms
step:1229/2330 train_time:71253ms step_avg:57.98ms
step:1230/2330 train_time:71314ms step_avg:57.98ms
step:1231/2330 train_time:71370ms step_avg:57.98ms
step:1232/2330 train_time:71431ms step_avg:57.98ms
step:1233/2330 train_time:71488ms step_avg:57.98ms
step:1234/2330 train_time:71548ms step_avg:57.98ms
step:1235/2330 train_time:71605ms step_avg:57.98ms
step:1236/2330 train_time:71665ms step_avg:57.98ms
step:1237/2330 train_time:71722ms step_avg:57.98ms
step:1238/2330 train_time:71782ms step_avg:57.98ms
step:1239/2330 train_time:71839ms step_avg:57.98ms
step:1240/2330 train_time:71900ms step_avg:57.98ms
step:1241/2330 train_time:71957ms step_avg:57.98ms
step:1242/2330 train_time:72017ms step_avg:57.98ms
step:1243/2330 train_time:72073ms step_avg:57.98ms
step:1244/2330 train_time:72133ms step_avg:57.98ms
step:1245/2330 train_time:72190ms step_avg:57.98ms
step:1246/2330 train_time:72250ms step_avg:57.99ms
step:1247/2330 train_time:72307ms step_avg:57.99ms
step:1248/2330 train_time:72367ms step_avg:57.99ms
step:1249/2330 train_time:72424ms step_avg:57.99ms
step:1250/2330 train_time:72483ms step_avg:57.99ms
step:1250/2330 val_loss:3.9926 train_time:72564ms step_avg:58.05ms
step:1251/2330 train_time:72583ms step_avg:58.02ms
step:1252/2330 train_time:72603ms step_avg:57.99ms
step:1253/2330 train_time:72662ms step_avg:57.99ms
step:1254/2330 train_time:72725ms step_avg:57.99ms
step:1255/2330 train_time:72783ms step_avg:57.99ms
step:1256/2330 train_time:72845ms step_avg:58.00ms
step:1257/2330 train_time:72901ms step_avg:58.00ms
step:1258/2330 train_time:72960ms step_avg:58.00ms
step:1259/2330 train_time:73017ms step_avg:58.00ms
step:1260/2330 train_time:73077ms step_avg:58.00ms
step:1261/2330 train_time:73133ms step_avg:58.00ms
step:1262/2330 train_time:73191ms step_avg:58.00ms
step:1263/2330 train_time:73248ms step_avg:58.00ms
step:1264/2330 train_time:73308ms step_avg:58.00ms
step:1265/2330 train_time:73364ms step_avg:58.00ms
step:1266/2330 train_time:73423ms step_avg:58.00ms
step:1267/2330 train_time:73479ms step_avg:57.99ms
step:1268/2330 train_time:73540ms step_avg:58.00ms
step:1269/2330 train_time:73598ms step_avg:58.00ms
step:1270/2330 train_time:73661ms step_avg:58.00ms
step:1271/2330 train_time:73718ms step_avg:58.00ms
step:1272/2330 train_time:73781ms step_avg:58.00ms
step:1273/2330 train_time:73837ms step_avg:58.00ms
step:1274/2330 train_time:73898ms step_avg:58.01ms
step:1275/2330 train_time:73955ms step_avg:58.00ms
step:1276/2330 train_time:74015ms step_avg:58.01ms
step:1277/2330 train_time:74071ms step_avg:58.00ms
step:1278/2330 train_time:74132ms step_avg:58.01ms
step:1279/2330 train_time:74189ms step_avg:58.01ms
step:1280/2330 train_time:74248ms step_avg:58.01ms
step:1281/2330 train_time:74305ms step_avg:58.01ms
step:1282/2330 train_time:74364ms step_avg:58.01ms
step:1283/2330 train_time:74421ms step_avg:58.01ms
step:1284/2330 train_time:74480ms step_avg:58.01ms
step:1285/2330 train_time:74538ms step_avg:58.01ms
step:1286/2330 train_time:74598ms step_avg:58.01ms
step:1287/2330 train_time:74655ms step_avg:58.01ms
step:1288/2330 train_time:74718ms step_avg:58.01ms
step:1289/2330 train_time:74775ms step_avg:58.01ms
step:1290/2330 train_time:74836ms step_avg:58.01ms
step:1291/2330 train_time:74892ms step_avg:58.01ms
step:1292/2330 train_time:74954ms step_avg:58.01ms
step:1293/2330 train_time:75010ms step_avg:58.01ms
step:1294/2330 train_time:75071ms step_avg:58.01ms
step:1295/2330 train_time:75127ms step_avg:58.01ms
step:1296/2330 train_time:75186ms step_avg:58.01ms
step:1297/2330 train_time:75243ms step_avg:58.01ms
step:1298/2330 train_time:75302ms step_avg:58.01ms
step:1299/2330 train_time:75358ms step_avg:58.01ms
step:1300/2330 train_time:75418ms step_avg:58.01ms
step:1301/2330 train_time:75476ms step_avg:58.01ms
step:1302/2330 train_time:75536ms step_avg:58.02ms
step:1303/2330 train_time:75592ms step_avg:58.01ms
step:1304/2330 train_time:75654ms step_avg:58.02ms
step:1305/2330 train_time:75711ms step_avg:58.02ms
step:1306/2330 train_time:75773ms step_avg:58.02ms
step:1307/2330 train_time:75830ms step_avg:58.02ms
step:1308/2330 train_time:75890ms step_avg:58.02ms
step:1309/2330 train_time:75947ms step_avg:58.02ms
step:1310/2330 train_time:76008ms step_avg:58.02ms
step:1311/2330 train_time:76064ms step_avg:58.02ms
step:1312/2330 train_time:76124ms step_avg:58.02ms
step:1313/2330 train_time:76180ms step_avg:58.02ms
step:1314/2330 train_time:76240ms step_avg:58.02ms
step:1315/2330 train_time:76297ms step_avg:58.02ms
step:1316/2330 train_time:76356ms step_avg:58.02ms
step:1317/2330 train_time:76412ms step_avg:58.02ms
step:1318/2330 train_time:76473ms step_avg:58.02ms
step:1319/2330 train_time:76530ms step_avg:58.02ms
step:1320/2330 train_time:76591ms step_avg:58.02ms
step:1321/2330 train_time:76647ms step_avg:58.02ms
step:1322/2330 train_time:76708ms step_avg:58.02ms
step:1323/2330 train_time:76765ms step_avg:58.02ms
step:1324/2330 train_time:76825ms step_avg:58.03ms
step:1325/2330 train_time:76883ms step_avg:58.02ms
step:1326/2330 train_time:76943ms step_avg:58.03ms
step:1327/2330 train_time:76999ms step_avg:58.02ms
step:1328/2330 train_time:77059ms step_avg:58.03ms
step:1329/2330 train_time:77115ms step_avg:58.03ms
step:1330/2330 train_time:77175ms step_avg:58.03ms
step:1331/2330 train_time:77232ms step_avg:58.03ms
step:1332/2330 train_time:77291ms step_avg:58.03ms
step:1333/2330 train_time:77348ms step_avg:58.03ms
step:1334/2330 train_time:77409ms step_avg:58.03ms
step:1335/2330 train_time:77467ms step_avg:58.03ms
step:1336/2330 train_time:77527ms step_avg:58.03ms
step:1337/2330 train_time:77583ms step_avg:58.03ms
step:1338/2330 train_time:77644ms step_avg:58.03ms
step:1339/2330 train_time:77700ms step_avg:58.03ms
step:1340/2330 train_time:77760ms step_avg:58.03ms
step:1341/2330 train_time:77818ms step_avg:58.03ms
step:1342/2330 train_time:77877ms step_avg:58.03ms
step:1343/2330 train_time:77934ms step_avg:58.03ms
step:1344/2330 train_time:77996ms step_avg:58.03ms
step:1345/2330 train_time:78052ms step_avg:58.03ms
step:1346/2330 train_time:78112ms step_avg:58.03ms
step:1347/2330 train_time:78169ms step_avg:58.03ms
step:1348/2330 train_time:78230ms step_avg:58.03ms
step:1349/2330 train_time:78286ms step_avg:58.03ms
step:1350/2330 train_time:78347ms step_avg:58.03ms
step:1351/2330 train_time:78404ms step_avg:58.03ms
step:1352/2330 train_time:78464ms step_avg:58.04ms
step:1353/2330 train_time:78521ms step_avg:58.03ms
step:1354/2330 train_time:78581ms step_avg:58.04ms
step:1355/2330 train_time:78638ms step_avg:58.04ms
step:1356/2330 train_time:78698ms step_avg:58.04ms
step:1357/2330 train_time:78755ms step_avg:58.04ms
step:1358/2330 train_time:78815ms step_avg:58.04ms
step:1359/2330 train_time:78872ms step_avg:58.04ms
step:1360/2330 train_time:78933ms step_avg:58.04ms
step:1361/2330 train_time:78990ms step_avg:58.04ms
step:1362/2330 train_time:79050ms step_avg:58.04ms
step:1363/2330 train_time:79107ms step_avg:58.04ms
step:1364/2330 train_time:79167ms step_avg:58.04ms
step:1365/2330 train_time:79224ms step_avg:58.04ms
step:1366/2330 train_time:79284ms step_avg:58.04ms
step:1367/2330 train_time:79340ms step_avg:58.04ms
step:1368/2330 train_time:79400ms step_avg:58.04ms
step:1369/2330 train_time:79457ms step_avg:58.04ms
step:1370/2330 train_time:79517ms step_avg:58.04ms
step:1371/2330 train_time:79575ms step_avg:58.04ms
step:1372/2330 train_time:79635ms step_avg:58.04ms
step:1373/2330 train_time:79692ms step_avg:58.04ms
step:1374/2330 train_time:79753ms step_avg:58.04ms
step:1375/2330 train_time:79810ms step_avg:58.04ms
step:1376/2330 train_time:79872ms step_avg:58.05ms
step:1377/2330 train_time:79929ms step_avg:58.05ms
step:1378/2330 train_time:79989ms step_avg:58.05ms
step:1379/2330 train_time:80045ms step_avg:58.05ms
step:1380/2330 train_time:80106ms step_avg:58.05ms
step:1381/2330 train_time:80162ms step_avg:58.05ms
step:1382/2330 train_time:80223ms step_avg:58.05ms
step:1383/2330 train_time:80280ms step_avg:58.05ms
step:1384/2330 train_time:80339ms step_avg:58.05ms
step:1385/2330 train_time:80396ms step_avg:58.05ms
step:1386/2330 train_time:80456ms step_avg:58.05ms
step:1387/2330 train_time:80513ms step_avg:58.05ms
step:1388/2330 train_time:80574ms step_avg:58.05ms
step:1389/2330 train_time:80631ms step_avg:58.05ms
step:1390/2330 train_time:80691ms step_avg:58.05ms
step:1391/2330 train_time:80749ms step_avg:58.05ms
step:1392/2330 train_time:80809ms step_avg:58.05ms
step:1393/2330 train_time:80866ms step_avg:58.05ms
step:1394/2330 train_time:80927ms step_avg:58.05ms
step:1395/2330 train_time:80983ms step_avg:58.05ms
step:1396/2330 train_time:81043ms step_avg:58.05ms
step:1397/2330 train_time:81099ms step_avg:58.05ms
step:1398/2330 train_time:81160ms step_avg:58.05ms
step:1399/2330 train_time:81217ms step_avg:58.05ms
step:1400/2330 train_time:81277ms step_avg:58.05ms
step:1401/2330 train_time:81332ms step_avg:58.05ms
step:1402/2330 train_time:81393ms step_avg:58.06ms
step:1403/2330 train_time:81450ms step_avg:58.05ms
step:1404/2330 train_time:81511ms step_avg:58.06ms
step:1405/2330 train_time:81568ms step_avg:58.06ms
step:1406/2330 train_time:81628ms step_avg:58.06ms
step:1407/2330 train_time:81685ms step_avg:58.06ms
step:1408/2330 train_time:81746ms step_avg:58.06ms
step:1409/2330 train_time:81803ms step_avg:58.06ms
step:1410/2330 train_time:81863ms step_avg:58.06ms
step:1411/2330 train_time:81921ms step_avg:58.06ms
step:1412/2330 train_time:81980ms step_avg:58.06ms
step:1413/2330 train_time:82037ms step_avg:58.06ms
step:1414/2330 train_time:82097ms step_avg:58.06ms
step:1415/2330 train_time:82153ms step_avg:58.06ms
step:1416/2330 train_time:82215ms step_avg:58.06ms
step:1417/2330 train_time:82271ms step_avg:58.06ms
step:1418/2330 train_time:82331ms step_avg:58.06ms
step:1419/2330 train_time:82387ms step_avg:58.06ms
step:1420/2330 train_time:82449ms step_avg:58.06ms
step:1421/2330 train_time:82505ms step_avg:58.06ms
step:1422/2330 train_time:82566ms step_avg:58.06ms
step:1423/2330 train_time:82622ms step_avg:58.06ms
step:1424/2330 train_time:82683ms step_avg:58.06ms
step:1425/2330 train_time:82739ms step_avg:58.06ms
step:1426/2330 train_time:82799ms step_avg:58.06ms
step:1427/2330 train_time:82856ms step_avg:58.06ms
step:1428/2330 train_time:82917ms step_avg:58.07ms
step:1429/2330 train_time:82974ms step_avg:58.06ms
step:1430/2330 train_time:83036ms step_avg:58.07ms
step:1431/2330 train_time:83092ms step_avg:58.07ms
step:1432/2330 train_time:83154ms step_avg:58.07ms
step:1433/2330 train_time:83211ms step_avg:58.07ms
step:1434/2330 train_time:83271ms step_avg:58.07ms
step:1435/2330 train_time:83328ms step_avg:58.07ms
step:1436/2330 train_time:83389ms step_avg:58.07ms
step:1437/2330 train_time:83446ms step_avg:58.07ms
step:1438/2330 train_time:83506ms step_avg:58.07ms
step:1439/2330 train_time:83564ms step_avg:58.07ms
step:1440/2330 train_time:83623ms step_avg:58.07ms
step:1441/2330 train_time:83681ms step_avg:58.07ms
step:1442/2330 train_time:83741ms step_avg:58.07ms
step:1443/2330 train_time:83798ms step_avg:58.07ms
step:1444/2330 train_time:83857ms step_avg:58.07ms
step:1445/2330 train_time:83913ms step_avg:58.07ms
step:1446/2330 train_time:83975ms step_avg:58.07ms
step:1447/2330 train_time:84031ms step_avg:58.07ms
step:1448/2330 train_time:84092ms step_avg:58.07ms
step:1449/2330 train_time:84148ms step_avg:58.07ms
step:1450/2330 train_time:84209ms step_avg:58.08ms
step:1451/2330 train_time:84266ms step_avg:58.07ms
step:1452/2330 train_time:84326ms step_avg:58.08ms
step:1453/2330 train_time:84383ms step_avg:58.07ms
step:1454/2330 train_time:84443ms step_avg:58.08ms
step:1455/2330 train_time:84500ms step_avg:58.08ms
step:1456/2330 train_time:84560ms step_avg:58.08ms
step:1457/2330 train_time:84617ms step_avg:58.08ms
step:1458/2330 train_time:84677ms step_avg:58.08ms
step:1459/2330 train_time:84734ms step_avg:58.08ms
step:1460/2330 train_time:84795ms step_avg:58.08ms
step:1461/2330 train_time:84851ms step_avg:58.08ms
step:1462/2330 train_time:84912ms step_avg:58.08ms
step:1463/2330 train_time:84969ms step_avg:58.08ms
step:1464/2330 train_time:85029ms step_avg:58.08ms
step:1465/2330 train_time:85087ms step_avg:58.08ms
step:1466/2330 train_time:85147ms step_avg:58.08ms
step:1467/2330 train_time:85204ms step_avg:58.08ms
step:1468/2330 train_time:85263ms step_avg:58.08ms
step:1469/2330 train_time:85320ms step_avg:58.08ms
step:1470/2330 train_time:85381ms step_avg:58.08ms
step:1471/2330 train_time:85438ms step_avg:58.08ms
step:1472/2330 train_time:85497ms step_avg:58.08ms
step:1473/2330 train_time:85554ms step_avg:58.08ms
step:1474/2330 train_time:85615ms step_avg:58.08ms
step:1475/2330 train_time:85672ms step_avg:58.08ms
step:1476/2330 train_time:85732ms step_avg:58.08ms
step:1477/2330 train_time:85789ms step_avg:58.08ms
step:1478/2330 train_time:85849ms step_avg:58.08ms
step:1479/2330 train_time:85905ms step_avg:58.08ms
step:1480/2330 train_time:85966ms step_avg:58.09ms
step:1481/2330 train_time:86022ms step_avg:58.08ms
step:1482/2330 train_time:86083ms step_avg:58.09ms
step:1483/2330 train_time:86139ms step_avg:58.08ms
step:1484/2330 train_time:86199ms step_avg:58.09ms
step:1485/2330 train_time:86256ms step_avg:58.09ms
step:1486/2330 train_time:86317ms step_avg:58.09ms
step:1487/2330 train_time:86374ms step_avg:58.09ms
step:1488/2330 train_time:86434ms step_avg:58.09ms
step:1489/2330 train_time:86492ms step_avg:58.09ms
step:1490/2330 train_time:86552ms step_avg:58.09ms
step:1491/2330 train_time:86610ms step_avg:58.09ms
step:1492/2330 train_time:86670ms step_avg:58.09ms
step:1493/2330 train_time:86727ms step_avg:58.09ms
step:1494/2330 train_time:86787ms step_avg:58.09ms
step:1495/2330 train_time:86844ms step_avg:58.09ms
step:1496/2330 train_time:86903ms step_avg:58.09ms
step:1497/2330 train_time:86960ms step_avg:58.09ms
step:1498/2330 train_time:87020ms step_avg:58.09ms
step:1499/2330 train_time:87078ms step_avg:58.09ms
step:1500/2330 train_time:87138ms step_avg:58.09ms
step:1500/2330 val_loss:3.9059 train_time:87219ms step_avg:58.15ms
step:1501/2330 train_time:87237ms step_avg:58.12ms
step:1502/2330 train_time:87260ms step_avg:58.10ms
step:1503/2330 train_time:87318ms step_avg:58.10ms
step:1504/2330 train_time:87382ms step_avg:58.10ms
step:1505/2330 train_time:87439ms step_avg:58.10ms
step:1506/2330 train_time:87501ms step_avg:58.10ms
step:1507/2330 train_time:87558ms step_avg:58.10ms
step:1508/2330 train_time:87617ms step_avg:58.10ms
step:1509/2330 train_time:87674ms step_avg:58.10ms
step:1510/2330 train_time:87733ms step_avg:58.10ms
step:1511/2330 train_time:87789ms step_avg:58.10ms
step:1512/2330 train_time:87848ms step_avg:58.10ms
step:1513/2330 train_time:87904ms step_avg:58.10ms
step:1514/2330 train_time:87963ms step_avg:58.10ms
step:1515/2330 train_time:88020ms step_avg:58.10ms
step:1516/2330 train_time:88079ms step_avg:58.10ms
step:1517/2330 train_time:88136ms step_avg:58.10ms
step:1518/2330 train_time:88196ms step_avg:58.10ms
step:1519/2330 train_time:88254ms step_avg:58.10ms
step:1520/2330 train_time:88316ms step_avg:58.10ms
step:1521/2330 train_time:88374ms step_avg:58.10ms
step:1522/2330 train_time:88437ms step_avg:58.11ms
step:1523/2330 train_time:88493ms step_avg:58.10ms
step:1524/2330 train_time:88556ms step_avg:58.11ms
step:1525/2330 train_time:88612ms step_avg:58.11ms
step:1526/2330 train_time:88672ms step_avg:58.11ms
step:1527/2330 train_time:88728ms step_avg:58.11ms
step:1528/2330 train_time:88788ms step_avg:58.11ms
step:1529/2330 train_time:88846ms step_avg:58.11ms
step:1530/2330 train_time:88904ms step_avg:58.11ms
step:1531/2330 train_time:88961ms step_avg:58.11ms
step:1532/2330 train_time:89021ms step_avg:58.11ms
step:1533/2330 train_time:89078ms step_avg:58.11ms
step:1534/2330 train_time:89138ms step_avg:58.11ms
step:1535/2330 train_time:89197ms step_avg:58.11ms
step:1536/2330 train_time:89258ms step_avg:58.11ms
step:1537/2330 train_time:89316ms step_avg:58.11ms
step:1538/2330 train_time:89377ms step_avg:58.11ms
step:1539/2330 train_time:89434ms step_avg:58.11ms
step:1540/2330 train_time:89495ms step_avg:58.11ms
step:1541/2330 train_time:89554ms step_avg:58.11ms
step:1542/2330 train_time:89614ms step_avg:58.12ms
step:1543/2330 train_time:89672ms step_avg:58.12ms
step:1544/2330 train_time:89732ms step_avg:58.12ms
step:1545/2330 train_time:89789ms step_avg:58.12ms
step:1546/2330 train_time:89850ms step_avg:58.12ms
step:1547/2330 train_time:89906ms step_avg:58.12ms
step:1548/2330 train_time:89967ms step_avg:58.12ms
step:1549/2330 train_time:90024ms step_avg:58.12ms
step:1550/2330 train_time:90085ms step_avg:58.12ms
step:1551/2330 train_time:90141ms step_avg:58.12ms
step:1552/2330 train_time:90203ms step_avg:58.12ms
step:1553/2330 train_time:90261ms step_avg:58.12ms
step:1554/2330 train_time:90323ms step_avg:58.12ms
step:1555/2330 train_time:90381ms step_avg:58.12ms
step:1556/2330 train_time:90442ms step_avg:58.12ms
step:1557/2330 train_time:90501ms step_avg:58.13ms
step:1558/2330 train_time:90562ms step_avg:58.13ms
step:1559/2330 train_time:90621ms step_avg:58.13ms
step:1560/2330 train_time:90681ms step_avg:58.13ms
step:1561/2330 train_time:90739ms step_avg:58.13ms
step:1562/2330 train_time:90800ms step_avg:58.13ms
step:1563/2330 train_time:90858ms step_avg:58.13ms
step:1564/2330 train_time:90918ms step_avg:58.13ms
step:1565/2330 train_time:90975ms step_avg:58.13ms
step:1566/2330 train_time:91036ms step_avg:58.13ms
step:1567/2330 train_time:91092ms step_avg:58.13ms
step:1568/2330 train_time:91154ms step_avg:58.13ms
step:1569/2330 train_time:91211ms step_avg:58.13ms
step:1570/2330 train_time:91273ms step_avg:58.14ms
step:1571/2330 train_time:91329ms step_avg:58.13ms
step:1572/2330 train_time:91391ms step_avg:58.14ms
step:1573/2330 train_time:91449ms step_avg:58.14ms
step:1574/2330 train_time:91510ms step_avg:58.14ms
step:1575/2330 train_time:91567ms step_avg:58.14ms
step:1576/2330 train_time:91628ms step_avg:58.14ms
step:1577/2330 train_time:91685ms step_avg:58.14ms
step:1578/2330 train_time:91747ms step_avg:58.14ms
step:1579/2330 train_time:91804ms step_avg:58.14ms
step:1580/2330 train_time:91865ms step_avg:58.14ms
step:1581/2330 train_time:91923ms step_avg:58.14ms
step:1582/2330 train_time:91982ms step_avg:58.14ms
step:1583/2330 train_time:92039ms step_avg:58.14ms
step:1584/2330 train_time:92101ms step_avg:58.14ms
step:1585/2330 train_time:92158ms step_avg:58.14ms
step:1586/2330 train_time:92219ms step_avg:58.15ms
step:1587/2330 train_time:92276ms step_avg:58.14ms
step:1588/2330 train_time:92337ms step_avg:58.15ms
step:1589/2330 train_time:92394ms step_avg:58.15ms
step:1590/2330 train_time:92454ms step_avg:58.15ms
step:1591/2330 train_time:92512ms step_avg:58.15ms
step:1592/2330 train_time:92572ms step_avg:58.15ms
step:1593/2330 train_time:92629ms step_avg:58.15ms
step:1594/2330 train_time:92692ms step_avg:58.15ms
step:1595/2330 train_time:92749ms step_avg:58.15ms
step:1596/2330 train_time:92810ms step_avg:58.15ms
step:1597/2330 train_time:92867ms step_avg:58.15ms
step:1598/2330 train_time:92927ms step_avg:58.15ms
step:1599/2330 train_time:92984ms step_avg:58.15ms
step:1600/2330 train_time:93046ms step_avg:58.15ms
step:1601/2330 train_time:93104ms step_avg:58.15ms
step:1602/2330 train_time:93164ms step_avg:58.15ms
step:1603/2330 train_time:93221ms step_avg:58.15ms
step:1604/2330 train_time:93282ms step_avg:58.16ms
step:1605/2330 train_time:93339ms step_avg:58.15ms
step:1606/2330 train_time:93400ms step_avg:58.16ms
step:1607/2330 train_time:93458ms step_avg:58.16ms
step:1608/2330 train_time:93518ms step_avg:58.16ms
step:1609/2330 train_time:93577ms step_avg:58.16ms
step:1610/2330 train_time:93639ms step_avg:58.16ms
step:1611/2330 train_time:93697ms step_avg:58.16ms
step:1612/2330 train_time:93757ms step_avg:58.16ms
step:1613/2330 train_time:93814ms step_avg:58.16ms
step:1614/2330 train_time:93876ms step_avg:58.16ms
step:1615/2330 train_time:93932ms step_avg:58.16ms
step:1616/2330 train_time:93993ms step_avg:58.16ms
step:1617/2330 train_time:94050ms step_avg:58.16ms
step:1618/2330 train_time:94112ms step_avg:58.17ms
step:1619/2330 train_time:94169ms step_avg:58.16ms
step:1620/2330 train_time:94230ms step_avg:58.17ms
step:1621/2330 train_time:94286ms step_avg:58.17ms
step:1622/2330 train_time:94348ms step_avg:58.17ms
step:1623/2330 train_time:94405ms step_avg:58.17ms
step:1624/2330 train_time:94467ms step_avg:58.17ms
step:1625/2330 train_time:94524ms step_avg:58.17ms
step:1626/2330 train_time:94586ms step_avg:58.17ms
step:1627/2330 train_time:94643ms step_avg:58.17ms
step:1628/2330 train_time:94704ms step_avg:58.17ms
step:1629/2330 train_time:94762ms step_avg:58.17ms
step:1630/2330 train_time:94822ms step_avg:58.17ms
step:1631/2330 train_time:94881ms step_avg:58.17ms
step:1632/2330 train_time:94940ms step_avg:58.17ms
step:1633/2330 train_time:94999ms step_avg:58.17ms
step:1634/2330 train_time:95060ms step_avg:58.18ms
step:1635/2330 train_time:95117ms step_avg:58.18ms
step:1636/2330 train_time:95178ms step_avg:58.18ms
step:1637/2330 train_time:95234ms step_avg:58.18ms
step:1638/2330 train_time:95295ms step_avg:58.18ms
step:1639/2330 train_time:95351ms step_avg:58.18ms
step:1640/2330 train_time:95413ms step_avg:58.18ms
step:1641/2330 train_time:95470ms step_avg:58.18ms
step:1642/2330 train_time:95531ms step_avg:58.18ms
step:1643/2330 train_time:95588ms step_avg:58.18ms
step:1644/2330 train_time:95650ms step_avg:58.18ms
step:1645/2330 train_time:95707ms step_avg:58.18ms
step:1646/2330 train_time:95768ms step_avg:58.18ms
step:1647/2330 train_time:95825ms step_avg:58.18ms
step:1648/2330 train_time:95888ms step_avg:58.18ms
step:1649/2330 train_time:95945ms step_avg:58.18ms
step:1650/2330 train_time:96005ms step_avg:58.19ms
step:1651/2330 train_time:96062ms step_avg:58.18ms
step:1652/2330 train_time:96124ms step_avg:58.19ms
step:1653/2330 train_time:96181ms step_avg:58.19ms
step:1654/2330 train_time:96241ms step_avg:58.19ms
step:1655/2330 train_time:96298ms step_avg:58.19ms
step:1656/2330 train_time:96359ms step_avg:58.19ms
step:1657/2330 train_time:96418ms step_avg:58.19ms
step:1658/2330 train_time:96479ms step_avg:58.19ms
step:1659/2330 train_time:96537ms step_avg:58.19ms
step:1660/2330 train_time:96597ms step_avg:58.19ms
step:1661/2330 train_time:96655ms step_avg:58.19ms
step:1662/2330 train_time:96716ms step_avg:58.19ms
step:1663/2330 train_time:96772ms step_avg:58.19ms
step:1664/2330 train_time:96834ms step_avg:58.19ms
step:1665/2330 train_time:96890ms step_avg:58.19ms
step:1666/2330 train_time:96952ms step_avg:58.19ms
step:1667/2330 train_time:97009ms step_avg:58.19ms
step:1668/2330 train_time:97071ms step_avg:58.20ms
step:1669/2330 train_time:97127ms step_avg:58.19ms
step:1670/2330 train_time:97188ms step_avg:58.20ms
step:1671/2330 train_time:97244ms step_avg:58.20ms
step:1672/2330 train_time:97306ms step_avg:58.20ms
step:1673/2330 train_time:97363ms step_avg:58.20ms
step:1674/2330 train_time:97423ms step_avg:58.20ms
step:1675/2330 train_time:97481ms step_avg:58.20ms
step:1676/2330 train_time:97542ms step_avg:58.20ms
step:1677/2330 train_time:97600ms step_avg:58.20ms
step:1678/2330 train_time:97662ms step_avg:58.20ms
step:1679/2330 train_time:97720ms step_avg:58.20ms
step:1680/2330 train_time:97781ms step_avg:58.20ms
step:1681/2330 train_time:97839ms step_avg:58.20ms
step:1682/2330 train_time:97899ms step_avg:58.20ms
step:1683/2330 train_time:97958ms step_avg:58.20ms
step:1684/2330 train_time:98019ms step_avg:58.21ms
step:1685/2330 train_time:98077ms step_avg:58.21ms
step:1686/2330 train_time:98137ms step_avg:58.21ms
step:1687/2330 train_time:98194ms step_avg:58.21ms
step:1688/2330 train_time:98255ms step_avg:58.21ms
step:1689/2330 train_time:98312ms step_avg:58.21ms
step:1690/2330 train_time:98373ms step_avg:58.21ms
step:1691/2330 train_time:98430ms step_avg:58.21ms
step:1692/2330 train_time:98491ms step_avg:58.21ms
step:1693/2330 train_time:98548ms step_avg:58.21ms
step:1694/2330 train_time:98609ms step_avg:58.21ms
step:1695/2330 train_time:98666ms step_avg:58.21ms
step:1696/2330 train_time:98728ms step_avg:58.21ms
step:1697/2330 train_time:98784ms step_avg:58.21ms
step:1698/2330 train_time:98845ms step_avg:58.21ms
step:1699/2330 train_time:98903ms step_avg:58.21ms
step:1700/2330 train_time:98964ms step_avg:58.21ms
step:1701/2330 train_time:99020ms step_avg:58.21ms
step:1702/2330 train_time:99082ms step_avg:58.22ms
step:1703/2330 train_time:99140ms step_avg:58.21ms
step:1704/2330 train_time:99200ms step_avg:58.22ms
step:1705/2330 train_time:99258ms step_avg:58.22ms
step:1706/2330 train_time:99320ms step_avg:58.22ms
step:1707/2330 train_time:99378ms step_avg:58.22ms
step:1708/2330 train_time:99439ms step_avg:58.22ms
step:1709/2330 train_time:99496ms step_avg:58.22ms
step:1710/2330 train_time:99558ms step_avg:58.22ms
step:1711/2330 train_time:99616ms step_avg:58.22ms
step:1712/2330 train_time:99676ms step_avg:58.22ms
step:1713/2330 train_time:99733ms step_avg:58.22ms
step:1714/2330 train_time:99793ms step_avg:58.22ms
step:1715/2330 train_time:99850ms step_avg:58.22ms
step:1716/2330 train_time:99913ms step_avg:58.22ms
step:1717/2330 train_time:99970ms step_avg:58.22ms
step:1718/2330 train_time:100030ms step_avg:58.22ms
step:1719/2330 train_time:100086ms step_avg:58.22ms
step:1720/2330 train_time:100148ms step_avg:58.23ms
step:1721/2330 train_time:100205ms step_avg:58.22ms
step:1722/2330 train_time:100266ms step_avg:58.23ms
step:1723/2330 train_time:100324ms step_avg:58.23ms
step:1724/2330 train_time:100385ms step_avg:58.23ms
step:1725/2330 train_time:100441ms step_avg:58.23ms
step:1726/2330 train_time:100503ms step_avg:58.23ms
step:1727/2330 train_time:100560ms step_avg:58.23ms
step:1728/2330 train_time:100621ms step_avg:58.23ms
step:1729/2330 train_time:100678ms step_avg:58.23ms
step:1730/2330 train_time:100740ms step_avg:58.23ms
step:1731/2330 train_time:100798ms step_avg:58.23ms
step:1732/2330 train_time:100859ms step_avg:58.23ms
step:1733/2330 train_time:100918ms step_avg:58.23ms
step:1734/2330 train_time:100978ms step_avg:58.23ms
step:1735/2330 train_time:101036ms step_avg:58.23ms
step:1736/2330 train_time:101096ms step_avg:58.24ms
step:1737/2330 train_time:101153ms step_avg:58.23ms
step:1738/2330 train_time:101214ms step_avg:58.24ms
step:1739/2330 train_time:101271ms step_avg:58.24ms
step:1740/2330 train_time:101332ms step_avg:58.24ms
step:1741/2330 train_time:101388ms step_avg:58.24ms
step:1742/2330 train_time:101449ms step_avg:58.24ms
step:1743/2330 train_time:101506ms step_avg:58.24ms
step:1744/2330 train_time:101567ms step_avg:58.24ms
step:1745/2330 train_time:101624ms step_avg:58.24ms
step:1746/2330 train_time:101684ms step_avg:58.24ms
step:1747/2330 train_time:101742ms step_avg:58.24ms
step:1748/2330 train_time:101803ms step_avg:58.24ms
step:1749/2330 train_time:101862ms step_avg:58.24ms
step:1750/2330 train_time:101922ms step_avg:58.24ms
step:1750/2330 val_loss:3.8225 train_time:102004ms step_avg:58.29ms
step:1751/2330 train_time:102022ms step_avg:58.26ms
step:1752/2330 train_time:102042ms step_avg:58.24ms
step:1753/2330 train_time:102097ms step_avg:58.24ms
step:1754/2330 train_time:102169ms step_avg:58.25ms
step:1755/2330 train_time:102225ms step_avg:58.25ms
step:1756/2330 train_time:102291ms step_avg:58.25ms
step:1757/2330 train_time:102347ms step_avg:58.25ms
step:1758/2330 train_time:102408ms step_avg:58.25ms
step:1759/2330 train_time:102464ms step_avg:58.25ms
step:1760/2330 train_time:102525ms step_avg:58.25ms
step:1761/2330 train_time:102581ms step_avg:58.25ms
step:1762/2330 train_time:102641ms step_avg:58.25ms
step:1763/2330 train_time:102697ms step_avg:58.25ms
step:1764/2330 train_time:102757ms step_avg:58.25ms
step:1765/2330 train_time:102814ms step_avg:58.25ms
step:1766/2330 train_time:102873ms step_avg:58.25ms
step:1767/2330 train_time:102933ms step_avg:58.25ms
step:1768/2330 train_time:102997ms step_avg:58.26ms
step:1769/2330 train_time:103056ms step_avg:58.26ms
step:1770/2330 train_time:103116ms step_avg:58.26ms
step:1771/2330 train_time:103174ms step_avg:58.26ms
step:1772/2330 train_time:103236ms step_avg:58.26ms
step:1773/2330 train_time:103293ms step_avg:58.26ms
step:1774/2330 train_time:103356ms step_avg:58.26ms
step:1775/2330 train_time:103412ms step_avg:58.26ms
step:1776/2330 train_time:103474ms step_avg:58.26ms
step:1777/2330 train_time:103530ms step_avg:58.26ms
step:1778/2330 train_time:103591ms step_avg:58.26ms
step:1779/2330 train_time:103647ms step_avg:58.26ms
step:1780/2330 train_time:103707ms step_avg:58.26ms
step:1781/2330 train_time:103763ms step_avg:58.26ms
step:1782/2330 train_time:103824ms step_avg:58.26ms
step:1783/2330 train_time:103882ms step_avg:58.26ms
step:1784/2330 train_time:103945ms step_avg:58.27ms
step:1785/2330 train_time:104003ms step_avg:58.27ms
step:1786/2330 train_time:104067ms step_avg:58.27ms
step:1787/2330 train_time:104125ms step_avg:58.27ms
step:1788/2330 train_time:104187ms step_avg:58.27ms
step:1789/2330 train_time:104245ms step_avg:58.27ms
step:1790/2330 train_time:104307ms step_avg:58.27ms
step:1791/2330 train_time:104365ms step_avg:58.27ms
step:1792/2330 train_time:104426ms step_avg:58.27ms
step:1793/2330 train_time:104484ms step_avg:58.27ms
step:1794/2330 train_time:104544ms step_avg:58.27ms
step:1795/2330 train_time:104600ms step_avg:58.27ms
step:1796/2330 train_time:104660ms step_avg:58.27ms
step:1797/2330 train_time:104716ms step_avg:58.27ms
step:1798/2330 train_time:104778ms step_avg:58.27ms
step:1799/2330 train_time:104834ms step_avg:58.27ms
step:1800/2330 train_time:104896ms step_avg:58.28ms
step:1801/2330 train_time:104954ms step_avg:58.28ms
step:1802/2330 train_time:105014ms step_avg:58.28ms
step:1803/2330 train_time:105072ms step_avg:58.28ms
step:1804/2330 train_time:105133ms step_avg:58.28ms
step:1805/2330 train_time:105190ms step_avg:58.28ms
step:1806/2330 train_time:105253ms step_avg:58.28ms
step:1807/2330 train_time:105310ms step_avg:58.28ms
step:1808/2330 train_time:105371ms step_avg:58.28ms
step:1809/2330 train_time:105427ms step_avg:58.28ms
step:1810/2330 train_time:105489ms step_avg:58.28ms
step:1811/2330 train_time:105546ms step_avg:58.28ms
step:1812/2330 train_time:105607ms step_avg:58.28ms
step:1813/2330 train_time:105664ms step_avg:58.28ms
step:1814/2330 train_time:105725ms step_avg:58.28ms
step:1815/2330 train_time:105782ms step_avg:58.28ms
step:1816/2330 train_time:105843ms step_avg:58.28ms
step:1817/2330 train_time:105901ms step_avg:58.28ms
step:1818/2330 train_time:105962ms step_avg:58.29ms
step:1819/2330 train_time:106020ms step_avg:58.28ms
step:1820/2330 train_time:106080ms step_avg:58.29ms
step:1821/2330 train_time:106138ms step_avg:58.29ms
step:1822/2330 train_time:106198ms step_avg:58.29ms
step:1823/2330 train_time:106256ms step_avg:58.29ms
step:1824/2330 train_time:106318ms step_avg:58.29ms
step:1825/2330 train_time:106375ms step_avg:58.29ms
step:1826/2330 train_time:106437ms step_avg:58.29ms
step:1827/2330 train_time:106493ms step_avg:58.29ms
step:1828/2330 train_time:106556ms step_avg:58.29ms
step:1829/2330 train_time:106612ms step_avg:58.29ms
step:1830/2330 train_time:106673ms step_avg:58.29ms
step:1831/2330 train_time:106729ms step_avg:58.29ms
step:1832/2330 train_time:106792ms step_avg:58.29ms
step:1833/2330 train_time:106849ms step_avg:58.29ms
step:1834/2330 train_time:106911ms step_avg:58.29ms
step:1835/2330 train_time:106968ms step_avg:58.29ms
step:1836/2330 train_time:107029ms step_avg:58.29ms
step:1837/2330 train_time:107087ms step_avg:58.29ms
step:1838/2330 train_time:107148ms step_avg:58.30ms
step:1839/2330 train_time:107206ms step_avg:58.30ms
step:1840/2330 train_time:107267ms step_avg:58.30ms
step:1841/2330 train_time:107326ms step_avg:58.30ms
step:1842/2330 train_time:107387ms step_avg:58.30ms
step:1843/2330 train_time:107445ms step_avg:58.30ms
step:1844/2330 train_time:107506ms step_avg:58.30ms
step:1845/2330 train_time:107564ms step_avg:58.30ms
step:1846/2330 train_time:107624ms step_avg:58.30ms
step:1847/2330 train_time:107681ms step_avg:58.30ms
step:1848/2330 train_time:107742ms step_avg:58.30ms
step:1849/2330 train_time:107799ms step_avg:58.30ms
step:1850/2330 train_time:107860ms step_avg:58.30ms
step:1851/2330 train_time:107917ms step_avg:58.30ms
step:1852/2330 train_time:107978ms step_avg:58.30ms
step:1853/2330 train_time:108034ms step_avg:58.30ms
step:1854/2330 train_time:108096ms step_avg:58.30ms
step:1855/2330 train_time:108152ms step_avg:58.30ms
step:1856/2330 train_time:108214ms step_avg:58.30ms
step:1857/2330 train_time:108271ms step_avg:58.30ms
step:1858/2330 train_time:108332ms step_avg:58.31ms
step:1859/2330 train_time:108389ms step_avg:58.30ms
step:1860/2330 train_time:108453ms step_avg:58.31ms
step:1861/2330 train_time:108510ms step_avg:58.31ms
step:1862/2330 train_time:108570ms step_avg:58.31ms
step:1863/2330 train_time:108628ms step_avg:58.31ms
step:1864/2330 train_time:108689ms step_avg:58.31ms
step:1865/2330 train_time:108746ms step_avg:58.31ms
step:1866/2330 train_time:108807ms step_avg:58.31ms
step:1867/2330 train_time:108865ms step_avg:58.31ms
step:1868/2330 train_time:108927ms step_avg:58.31ms
step:1869/2330 train_time:108984ms step_avg:58.31ms
step:1870/2330 train_time:109046ms step_avg:58.31ms
step:1871/2330 train_time:109103ms step_avg:58.31ms
step:1872/2330 train_time:109164ms step_avg:58.31ms
step:1873/2330 train_time:109223ms step_avg:58.31ms
step:1874/2330 train_time:109284ms step_avg:58.32ms
step:1875/2330 train_time:109342ms step_avg:58.32ms
step:1876/2330 train_time:109402ms step_avg:58.32ms
step:1877/2330 train_time:109459ms step_avg:58.32ms
step:1878/2330 train_time:109521ms step_avg:58.32ms
step:1879/2330 train_time:109579ms step_avg:58.32ms
step:1880/2330 train_time:109639ms step_avg:58.32ms
step:1881/2330 train_time:109696ms step_avg:58.32ms
step:1882/2330 train_time:109757ms step_avg:58.32ms
step:1883/2330 train_time:109814ms step_avg:58.32ms
step:1884/2330 train_time:109875ms step_avg:58.32ms
step:1885/2330 train_time:109931ms step_avg:58.32ms
step:1886/2330 train_time:109993ms step_avg:58.32ms
step:1887/2330 train_time:110049ms step_avg:58.32ms
step:1888/2330 train_time:110111ms step_avg:58.32ms
step:1889/2330 train_time:110168ms step_avg:58.32ms
step:1890/2330 train_time:110230ms step_avg:58.32ms
step:1891/2330 train_time:110287ms step_avg:58.32ms
step:1892/2330 train_time:110348ms step_avg:58.32ms
step:1893/2330 train_time:110406ms step_avg:58.32ms
step:1894/2330 train_time:110468ms step_avg:58.33ms
step:1895/2330 train_time:110526ms step_avg:58.33ms
step:1896/2330 train_time:110587ms step_avg:58.33ms
step:1897/2330 train_time:110645ms step_avg:58.33ms
step:1898/2330 train_time:110706ms step_avg:58.33ms
step:1899/2330 train_time:110764ms step_avg:58.33ms
step:1900/2330 train_time:110825ms step_avg:58.33ms
step:1901/2330 train_time:110883ms step_avg:58.33ms
step:1902/2330 train_time:110944ms step_avg:58.33ms
step:1903/2330 train_time:111001ms step_avg:58.33ms
step:1904/2330 train_time:111061ms step_avg:58.33ms
step:1905/2330 train_time:111118ms step_avg:58.33ms
step:1906/2330 train_time:111179ms step_avg:58.33ms
step:1907/2330 train_time:111236ms step_avg:58.33ms
step:1908/2330 train_time:111297ms step_avg:58.33ms
step:1909/2330 train_time:111353ms step_avg:58.33ms
step:1910/2330 train_time:111415ms step_avg:58.33ms
step:1911/2330 train_time:111472ms step_avg:58.33ms
step:1912/2330 train_time:111534ms step_avg:58.33ms
step:1913/2330 train_time:111590ms step_avg:58.33ms
step:1914/2330 train_time:111652ms step_avg:58.33ms
step:1915/2330 train_time:111709ms step_avg:58.33ms
step:1916/2330 train_time:111771ms step_avg:58.34ms
step:1917/2330 train_time:111828ms step_avg:58.33ms
step:1918/2330 train_time:111889ms step_avg:58.34ms
step:1919/2330 train_time:111946ms step_avg:58.34ms
step:1920/2330 train_time:112007ms step_avg:58.34ms
step:1921/2330 train_time:112066ms step_avg:58.34ms
step:1922/2330 train_time:112125ms step_avg:58.34ms
step:1923/2330 train_time:112184ms step_avg:58.34ms
step:1924/2330 train_time:112244ms step_avg:58.34ms
step:1925/2330 train_time:112301ms step_avg:58.34ms
step:1926/2330 train_time:112362ms step_avg:58.34ms
step:1927/2330 train_time:112420ms step_avg:58.34ms
step:1928/2330 train_time:112482ms step_avg:58.34ms
step:1929/2330 train_time:112539ms step_avg:58.34ms
step:1930/2330 train_time:112601ms step_avg:58.34ms
step:1931/2330 train_time:112658ms step_avg:58.34ms
step:1932/2330 train_time:112720ms step_avg:58.34ms
step:1933/2330 train_time:112777ms step_avg:58.34ms
step:1934/2330 train_time:112838ms step_avg:58.34ms
step:1935/2330 train_time:112895ms step_avg:58.34ms
step:1936/2330 train_time:112956ms step_avg:58.35ms
step:1937/2330 train_time:113014ms step_avg:58.34ms
step:1938/2330 train_time:113074ms step_avg:58.35ms
step:1939/2330 train_time:113130ms step_avg:58.34ms
step:1940/2330 train_time:113192ms step_avg:58.35ms
step:1941/2330 train_time:113250ms step_avg:58.35ms
step:1942/2330 train_time:113312ms step_avg:58.35ms
step:1943/2330 train_time:113369ms step_avg:58.35ms
step:1944/2330 train_time:113432ms step_avg:58.35ms
step:1945/2330 train_time:113488ms step_avg:58.35ms
step:1946/2330 train_time:113551ms step_avg:58.35ms
step:1947/2330 train_time:113609ms step_avg:58.35ms
step:1948/2330 train_time:113669ms step_avg:58.35ms
step:1949/2330 train_time:113727ms step_avg:58.35ms
step:1950/2330 train_time:113788ms step_avg:58.35ms
step:1951/2330 train_time:113845ms step_avg:58.35ms
step:1952/2330 train_time:113906ms step_avg:58.35ms
step:1953/2330 train_time:113963ms step_avg:58.35ms
step:1954/2330 train_time:114024ms step_avg:58.35ms
step:1955/2330 train_time:114082ms step_avg:58.35ms
step:1956/2330 train_time:114142ms step_avg:58.35ms
step:1957/2330 train_time:114200ms step_avg:58.35ms
step:1958/2330 train_time:114260ms step_avg:58.36ms
step:1959/2330 train_time:114317ms step_avg:58.35ms
step:1960/2330 train_time:114378ms step_avg:58.36ms
step:1961/2330 train_time:114434ms step_avg:58.36ms
step:1962/2330 train_time:114495ms step_avg:58.36ms
step:1963/2330 train_time:114552ms step_avg:58.36ms
step:1964/2330 train_time:114613ms step_avg:58.36ms
step:1965/2330 train_time:114670ms step_avg:58.36ms
step:1966/2330 train_time:114732ms step_avg:58.36ms
step:1967/2330 train_time:114789ms step_avg:58.36ms
step:1968/2330 train_time:114851ms step_avg:58.36ms
step:1969/2330 train_time:114908ms step_avg:58.36ms
step:1970/2330 train_time:114970ms step_avg:58.36ms
step:1971/2330 train_time:115028ms step_avg:58.36ms
step:1972/2330 train_time:115089ms step_avg:58.36ms
step:1973/2330 train_time:115146ms step_avg:58.36ms
step:1974/2330 train_time:115208ms step_avg:58.36ms
step:1975/2330 train_time:115266ms step_avg:58.36ms
step:1976/2330 train_time:115327ms step_avg:58.36ms
step:1977/2330 train_time:115385ms step_avg:58.36ms
step:1978/2330 train_time:115446ms step_avg:58.36ms
step:1979/2330 train_time:115503ms step_avg:58.36ms
step:1980/2330 train_time:115565ms step_avg:58.37ms
step:1981/2330 train_time:115623ms step_avg:58.37ms
step:1982/2330 train_time:115684ms step_avg:58.37ms
step:1983/2330 train_time:115742ms step_avg:58.37ms
step:1984/2330 train_time:115802ms step_avg:58.37ms
step:1985/2330 train_time:115860ms step_avg:58.37ms
step:1986/2330 train_time:115920ms step_avg:58.37ms
step:1987/2330 train_time:115977ms step_avg:58.37ms
step:1988/2330 train_time:116038ms step_avg:58.37ms
step:1989/2330 train_time:116094ms step_avg:58.37ms
step:1990/2330 train_time:116155ms step_avg:58.37ms
step:1991/2330 train_time:116212ms step_avg:58.37ms
step:1992/2330 train_time:116273ms step_avg:58.37ms
step:1993/2330 train_time:116329ms step_avg:58.37ms
step:1994/2330 train_time:116391ms step_avg:58.37ms
step:1995/2330 train_time:116448ms step_avg:58.37ms
step:1996/2330 train_time:116510ms step_avg:58.37ms
step:1997/2330 train_time:116566ms step_avg:58.37ms
step:1998/2330 train_time:116628ms step_avg:58.37ms
step:1999/2330 train_time:116685ms step_avg:58.37ms
step:2000/2330 train_time:116748ms step_avg:58.37ms
step:2000/2330 val_loss:3.7613 train_time:116831ms step_avg:58.42ms
step:2001/2330 train_time:116850ms step_avg:58.40ms
step:2002/2330 train_time:116872ms step_avg:58.38ms
step:2003/2330 train_time:116932ms step_avg:58.38ms
step:2004/2330 train_time:116996ms step_avg:58.38ms
step:2005/2330 train_time:117053ms step_avg:58.38ms
step:2006/2330 train_time:117117ms step_avg:58.38ms
step:2007/2330 train_time:117174ms step_avg:58.38ms
step:2008/2330 train_time:117234ms step_avg:58.38ms
step:2009/2330 train_time:117291ms step_avg:58.38ms
step:2010/2330 train_time:117350ms step_avg:58.38ms
step:2011/2330 train_time:117407ms step_avg:58.38ms
step:2012/2330 train_time:117467ms step_avg:58.38ms
step:2013/2330 train_time:117523ms step_avg:58.38ms
step:2014/2330 train_time:117583ms step_avg:58.38ms
step:2015/2330 train_time:117640ms step_avg:58.38ms
step:2016/2330 train_time:117699ms step_avg:58.38ms
step:2017/2330 train_time:117756ms step_avg:58.38ms
step:2018/2330 train_time:117818ms step_avg:58.38ms
step:2019/2330 train_time:117878ms step_avg:58.38ms
step:2020/2330 train_time:117941ms step_avg:58.39ms
step:2021/2330 train_time:117998ms step_avg:58.39ms
step:2022/2330 train_time:118060ms step_avg:58.39ms
step:2023/2330 train_time:118117ms step_avg:58.39ms
step:2024/2330 train_time:118180ms step_avg:58.39ms
step:2025/2330 train_time:118236ms step_avg:58.39ms
step:2026/2330 train_time:118298ms step_avg:58.39ms
step:2027/2330 train_time:118354ms step_avg:58.39ms
step:2028/2330 train_time:118414ms step_avg:58.39ms
step:2029/2330 train_time:118471ms step_avg:58.39ms
step:2030/2330 train_time:118532ms step_avg:58.39ms
step:2031/2330 train_time:118589ms step_avg:58.39ms
step:2032/2330 train_time:118649ms step_avg:58.39ms
step:2033/2330 train_time:118705ms step_avg:58.39ms
step:2034/2330 train_time:118766ms step_avg:58.39ms
step:2035/2330 train_time:118826ms step_avg:58.39ms
step:2036/2330 train_time:118887ms step_avg:58.39ms
step:2037/2330 train_time:118946ms step_avg:58.39ms
step:2038/2330 train_time:119007ms step_avg:58.39ms
step:2039/2330 train_time:119066ms step_avg:58.39ms
step:2040/2330 train_time:119126ms step_avg:58.40ms
step:2041/2330 train_time:119185ms step_avg:58.40ms
step:2042/2330 train_time:119246ms step_avg:58.40ms
step:2043/2330 train_time:119304ms step_avg:58.40ms
step:2044/2330 train_time:119364ms step_avg:58.40ms
step:2045/2330 train_time:119421ms step_avg:58.40ms
step:2046/2330 train_time:119482ms step_avg:58.40ms
step:2047/2330 train_time:119539ms step_avg:58.40ms
step:2048/2330 train_time:119598ms step_avg:58.40ms
step:2049/2330 train_time:119655ms step_avg:58.40ms
step:2050/2330 train_time:119715ms step_avg:58.40ms
step:2051/2330 train_time:119772ms step_avg:58.40ms
step:2052/2330 train_time:119835ms step_avg:58.40ms
step:2053/2330 train_time:119892ms step_avg:58.40ms
step:2054/2330 train_time:119953ms step_avg:58.40ms
step:2055/2330 train_time:120011ms step_avg:58.40ms
step:2056/2330 train_time:120073ms step_avg:58.40ms
step:2057/2330 train_time:120131ms step_avg:58.40ms
step:2058/2330 train_time:120193ms step_avg:58.40ms
step:2059/2330 train_time:120250ms step_avg:58.40ms
step:2060/2330 train_time:120312ms step_avg:58.40ms
step:2061/2330 train_time:120369ms step_avg:58.40ms
step:2062/2330 train_time:120429ms step_avg:58.40ms
step:2063/2330 train_time:120487ms step_avg:58.40ms
step:2064/2330 train_time:120549ms step_avg:58.41ms
step:2065/2330 train_time:120606ms step_avg:58.41ms
step:2066/2330 train_time:120666ms step_avg:58.41ms
step:2067/2330 train_time:120724ms step_avg:58.41ms
step:2068/2330 train_time:120785ms step_avg:58.41ms
step:2069/2330 train_time:120843ms step_avg:58.41ms
step:2070/2330 train_time:120904ms step_avg:58.41ms
step:2071/2330 train_time:120961ms step_avg:58.41ms
step:2072/2330 train_time:121023ms step_avg:58.41ms
step:2073/2330 train_time:121081ms step_avg:58.41ms
step:2074/2330 train_time:121143ms step_avg:58.41ms
step:2075/2330 train_time:121200ms step_avg:58.41ms
step:2076/2330 train_time:121261ms step_avg:58.41ms
step:2077/2330 train_time:121319ms step_avg:58.41ms
step:2078/2330 train_time:121379ms step_avg:58.41ms
step:2079/2330 train_time:121436ms step_avg:58.41ms
step:2080/2330 train_time:121497ms step_avg:58.41ms
step:2081/2330 train_time:121553ms step_avg:58.41ms
step:2082/2330 train_time:121615ms step_avg:58.41ms
step:2083/2330 train_time:121672ms step_avg:58.41ms
step:2084/2330 train_time:121733ms step_avg:58.41ms
step:2085/2330 train_time:121790ms step_avg:58.41ms
step:2086/2330 train_time:121851ms step_avg:58.41ms
step:2087/2330 train_time:121908ms step_avg:58.41ms
step:2088/2330 train_time:121970ms step_avg:58.41ms
step:2089/2330 train_time:122028ms step_avg:58.41ms
step:2090/2330 train_time:122089ms step_avg:58.42ms
step:2091/2330 train_time:122148ms step_avg:58.42ms
step:2092/2330 train_time:122209ms step_avg:58.42ms
step:2093/2330 train_time:122267ms step_avg:58.42ms
step:2094/2330 train_time:122328ms step_avg:58.42ms
step:2095/2330 train_time:122387ms step_avg:58.42ms
step:2096/2330 train_time:122447ms step_avg:58.42ms
step:2097/2330 train_time:122505ms step_avg:58.42ms
step:2098/2330 train_time:122564ms step_avg:58.42ms
step:2099/2330 train_time:122621ms step_avg:58.42ms
step:2100/2330 train_time:122681ms step_avg:58.42ms
step:2101/2330 train_time:122738ms step_avg:58.42ms
step:2102/2330 train_time:122799ms step_avg:58.42ms
step:2103/2330 train_time:122855ms step_avg:58.42ms
step:2104/2330 train_time:122917ms step_avg:58.42ms
step:2105/2330 train_time:122974ms step_avg:58.42ms
step:2106/2330 train_time:123036ms step_avg:58.42ms
step:2107/2330 train_time:123093ms step_avg:58.42ms
step:2108/2330 train_time:123154ms step_avg:58.42ms
step:2109/2330 train_time:123211ms step_avg:58.42ms
step:2110/2330 train_time:123273ms step_avg:58.42ms
step:2111/2330 train_time:123331ms step_avg:58.42ms
step:2112/2330 train_time:123391ms step_avg:58.42ms
step:2113/2330 train_time:123449ms step_avg:58.42ms
step:2114/2330 train_time:123510ms step_avg:58.42ms
step:2115/2330 train_time:123567ms step_avg:58.42ms
step:2116/2330 train_time:123627ms step_avg:58.43ms
step:2117/2330 train_time:123685ms step_avg:58.42ms
step:2118/2330 train_time:123745ms step_avg:58.43ms
step:2119/2330 train_time:123802ms step_avg:58.42ms
step:2120/2330 train_time:123863ms step_avg:58.43ms
step:2121/2330 train_time:123920ms step_avg:58.43ms
step:2122/2330 train_time:123982ms step_avg:58.43ms
step:2123/2330 train_time:124039ms step_avg:58.43ms
step:2124/2330 train_time:124100ms step_avg:58.43ms
step:2125/2330 train_time:124157ms step_avg:58.43ms
step:2126/2330 train_time:124218ms step_avg:58.43ms
step:2127/2330 train_time:124275ms step_avg:58.43ms
step:2128/2330 train_time:124339ms step_avg:58.43ms
step:2129/2330 train_time:124395ms step_avg:58.43ms
step:2130/2330 train_time:124457ms step_avg:58.43ms
step:2131/2330 train_time:124514ms step_avg:58.43ms
step:2132/2330 train_time:124576ms step_avg:58.43ms
step:2133/2330 train_time:124632ms step_avg:58.43ms
step:2134/2330 train_time:124694ms step_avg:58.43ms
step:2135/2330 train_time:124751ms step_avg:58.43ms
step:2136/2330 train_time:124813ms step_avg:58.43ms
step:2137/2330 train_time:124870ms step_avg:58.43ms
step:2138/2330 train_time:124931ms step_avg:58.43ms
step:2139/2330 train_time:124989ms step_avg:58.43ms
step:2140/2330 train_time:125051ms step_avg:58.43ms
step:2141/2330 train_time:125109ms step_avg:58.43ms
step:2142/2330 train_time:125170ms step_avg:58.44ms
step:2143/2330 train_time:125228ms step_avg:58.44ms
step:2144/2330 train_time:125288ms step_avg:58.44ms
step:2145/2330 train_time:125346ms step_avg:58.44ms
step:2146/2330 train_time:125407ms step_avg:58.44ms
step:2147/2330 train_time:125464ms step_avg:58.44ms
step:2148/2330 train_time:125525ms step_avg:58.44ms
step:2149/2330 train_time:125582ms step_avg:58.44ms
step:2150/2330 train_time:125644ms step_avg:58.44ms
step:2151/2330 train_time:125702ms step_avg:58.44ms
step:2152/2330 train_time:125762ms step_avg:58.44ms
step:2153/2330 train_time:125819ms step_avg:58.44ms
step:2154/2330 train_time:125879ms step_avg:58.44ms
step:2155/2330 train_time:125937ms step_avg:58.44ms
step:2156/2330 train_time:125998ms step_avg:58.44ms
step:2157/2330 train_time:126054ms step_avg:58.44ms
step:2158/2330 train_time:126116ms step_avg:58.44ms
step:2159/2330 train_time:126173ms step_avg:58.44ms
step:2160/2330 train_time:126234ms step_avg:58.44ms
step:2161/2330 train_time:126291ms step_avg:58.44ms
step:2162/2330 train_time:126352ms step_avg:58.44ms
step:2163/2330 train_time:126409ms step_avg:58.44ms
step:2164/2330 train_time:126470ms step_avg:58.44ms
step:2165/2330 train_time:126527ms step_avg:58.44ms
step:2166/2330 train_time:126588ms step_avg:58.44ms
step:2167/2330 train_time:126645ms step_avg:58.44ms
step:2168/2330 train_time:126707ms step_avg:58.44ms
step:2169/2330 train_time:126765ms step_avg:58.44ms
step:2170/2330 train_time:126825ms step_avg:58.44ms
step:2171/2330 train_time:126884ms step_avg:58.44ms
step:2172/2330 train_time:126944ms step_avg:58.45ms
step:2173/2330 train_time:127003ms step_avg:58.45ms
step:2174/2330 train_time:127064ms step_avg:58.45ms
step:2175/2330 train_time:127122ms step_avg:58.45ms
step:2176/2330 train_time:127182ms step_avg:58.45ms
step:2177/2330 train_time:127239ms step_avg:58.45ms
step:2178/2330 train_time:127300ms step_avg:58.45ms
step:2179/2330 train_time:127356ms step_avg:58.45ms
step:2180/2330 train_time:127418ms step_avg:58.45ms
step:2181/2330 train_time:127475ms step_avg:58.45ms
step:2182/2330 train_time:127536ms step_avg:58.45ms
step:2183/2330 train_time:127593ms step_avg:58.45ms
step:2184/2330 train_time:127654ms step_avg:58.45ms
step:2185/2330 train_time:127711ms step_avg:58.45ms
step:2186/2330 train_time:127772ms step_avg:58.45ms
step:2187/2330 train_time:127829ms step_avg:58.45ms
step:2188/2330 train_time:127892ms step_avg:58.45ms
step:2189/2330 train_time:127949ms step_avg:58.45ms
step:2190/2330 train_time:128010ms step_avg:58.45ms
step:2191/2330 train_time:128068ms step_avg:58.45ms
step:2192/2330 train_time:128129ms step_avg:58.45ms
step:2193/2330 train_time:128187ms step_avg:58.45ms
step:2194/2330 train_time:128248ms step_avg:58.45ms
step:2195/2330 train_time:128306ms step_avg:58.45ms
step:2196/2330 train_time:128366ms step_avg:58.45ms
step:2197/2330 train_time:128423ms step_avg:58.45ms
step:2198/2330 train_time:128485ms step_avg:58.46ms
step:2199/2330 train_time:128543ms step_avg:58.46ms
step:2200/2330 train_time:128603ms step_avg:58.46ms
step:2201/2330 train_time:128660ms step_avg:58.46ms
step:2202/2330 train_time:128721ms step_avg:58.46ms
step:2203/2330 train_time:128777ms step_avg:58.46ms
step:2204/2330 train_time:128839ms step_avg:58.46ms
step:2205/2330 train_time:128896ms step_avg:58.46ms
step:2206/2330 train_time:128957ms step_avg:58.46ms
step:2207/2330 train_time:129014ms step_avg:58.46ms
step:2208/2330 train_time:129075ms step_avg:58.46ms
step:2209/2330 train_time:129132ms step_avg:58.46ms
step:2210/2330 train_time:129194ms step_avg:58.46ms
step:2211/2330 train_time:129251ms step_avg:58.46ms
step:2212/2330 train_time:129312ms step_avg:58.46ms
step:2213/2330 train_time:129369ms step_avg:58.46ms
step:2214/2330 train_time:129431ms step_avg:58.46ms
step:2215/2330 train_time:129489ms step_avg:58.46ms
step:2216/2330 train_time:129549ms step_avg:58.46ms
step:2217/2330 train_time:129607ms step_avg:58.46ms
step:2218/2330 train_time:129668ms step_avg:58.46ms
step:2219/2330 train_time:129725ms step_avg:58.46ms
step:2220/2330 train_time:129786ms step_avg:58.46ms
step:2221/2330 train_time:129844ms step_avg:58.46ms
step:2222/2330 train_time:129905ms step_avg:58.46ms
step:2223/2330 train_time:129962ms step_avg:58.46ms
step:2224/2330 train_time:130022ms step_avg:58.46ms
step:2225/2330 train_time:130080ms step_avg:58.46ms
step:2226/2330 train_time:130141ms step_avg:58.46ms
step:2227/2330 train_time:130198ms step_avg:58.46ms
step:2228/2330 train_time:130259ms step_avg:58.46ms
step:2229/2330 train_time:130316ms step_avg:58.46ms
step:2230/2330 train_time:130377ms step_avg:58.47ms
step:2231/2330 train_time:130434ms step_avg:58.46ms
step:2232/2330 train_time:130496ms step_avg:58.47ms
step:2233/2330 train_time:130552ms step_avg:58.46ms
step:2234/2330 train_time:130614ms step_avg:58.47ms
step:2235/2330 train_time:130671ms step_avg:58.47ms
step:2236/2330 train_time:130733ms step_avg:58.47ms
step:2237/2330 train_time:130790ms step_avg:58.47ms
step:2238/2330 train_time:130851ms step_avg:58.47ms
step:2239/2330 train_time:130909ms step_avg:58.47ms
step:2240/2330 train_time:130970ms step_avg:58.47ms
step:2241/2330 train_time:131027ms step_avg:58.47ms
step:2242/2330 train_time:131089ms step_avg:58.47ms
step:2243/2330 train_time:131147ms step_avg:58.47ms
step:2244/2330 train_time:131207ms step_avg:58.47ms
step:2245/2330 train_time:131265ms step_avg:58.47ms
step:2246/2330 train_time:131326ms step_avg:58.47ms
step:2247/2330 train_time:131384ms step_avg:58.47ms
step:2248/2330 train_time:131446ms step_avg:58.47ms
step:2249/2330 train_time:131503ms step_avg:58.47ms
step:2250/2330 train_time:131564ms step_avg:58.47ms
step:2250/2330 val_loss:3.7121 train_time:131645ms step_avg:58.51ms
step:2251/2330 train_time:131663ms step_avg:58.49ms
step:2252/2330 train_time:131686ms step_avg:58.48ms
step:2253/2330 train_time:131745ms step_avg:58.48ms
step:2254/2330 train_time:131806ms step_avg:58.48ms
step:2255/2330 train_time:131864ms step_avg:58.48ms
step:2256/2330 train_time:131925ms step_avg:58.48ms
step:2257/2330 train_time:131982ms step_avg:58.48ms
step:2258/2330 train_time:132042ms step_avg:58.48ms
step:2259/2330 train_time:132099ms step_avg:58.48ms
step:2260/2330 train_time:132159ms step_avg:58.48ms
step:2261/2330 train_time:132216ms step_avg:58.48ms
step:2262/2330 train_time:132276ms step_avg:58.48ms
step:2263/2330 train_time:132333ms step_avg:58.48ms
step:2264/2330 train_time:132392ms step_avg:58.48ms
step:2265/2330 train_time:132449ms step_avg:58.48ms
step:2266/2330 train_time:132509ms step_avg:58.48ms
step:2267/2330 train_time:132566ms step_avg:58.48ms
step:2268/2330 train_time:132629ms step_avg:58.48ms
step:2269/2330 train_time:132687ms step_avg:58.48ms
step:2270/2330 train_time:132751ms step_avg:58.48ms
step:2271/2330 train_time:132810ms step_avg:58.48ms
step:2272/2330 train_time:132873ms step_avg:58.48ms
step:2273/2330 train_time:132931ms step_avg:58.48ms
step:2274/2330 train_time:132992ms step_avg:58.48ms
step:2275/2330 train_time:133049ms step_avg:58.48ms
step:2276/2330 train_time:133110ms step_avg:58.48ms
step:2277/2330 train_time:133166ms step_avg:58.48ms
step:2278/2330 train_time:133228ms step_avg:58.48ms
step:2279/2330 train_time:133284ms step_avg:58.48ms
step:2280/2330 train_time:133345ms step_avg:58.48ms
step:2281/2330 train_time:133401ms step_avg:58.48ms
step:2282/2330 train_time:133463ms step_avg:58.49ms
step:2283/2330 train_time:133520ms step_avg:58.48ms
step:2284/2330 train_time:133580ms step_avg:58.49ms
step:2285/2330 train_time:133638ms step_avg:58.48ms
step:2286/2330 train_time:133699ms step_avg:58.49ms
step:2287/2330 train_time:133756ms step_avg:58.49ms
step:2288/2330 train_time:133820ms step_avg:58.49ms
step:2289/2330 train_time:133877ms step_avg:58.49ms
step:2290/2330 train_time:133938ms step_avg:58.49ms
step:2291/2330 train_time:133996ms step_avg:58.49ms
step:2292/2330 train_time:134056ms step_avg:58.49ms
step:2293/2330 train_time:134114ms step_avg:58.49ms
step:2294/2330 train_time:134174ms step_avg:58.49ms
step:2295/2330 train_time:134233ms step_avg:58.49ms
step:2296/2330 train_time:134293ms step_avg:58.49ms
step:2297/2330 train_time:134350ms step_avg:58.49ms
step:2298/2330 train_time:134410ms step_avg:58.49ms
step:2299/2330 train_time:134469ms step_avg:58.49ms
step:2300/2330 train_time:134529ms step_avg:58.49ms
step:2301/2330 train_time:134587ms step_avg:58.49ms
step:2302/2330 train_time:134647ms step_avg:58.49ms
step:2303/2330 train_time:134705ms step_avg:58.49ms
step:2304/2330 train_time:134767ms step_avg:58.49ms
step:2305/2330 train_time:134825ms step_avg:58.49ms
step:2306/2330 train_time:134886ms step_avg:58.49ms
step:2307/2330 train_time:134943ms step_avg:58.49ms
step:2308/2330 train_time:135005ms step_avg:58.49ms
step:2309/2330 train_time:135062ms step_avg:58.49ms
step:2310/2330 train_time:135124ms step_avg:58.50ms
step:2311/2330 train_time:135180ms step_avg:58.49ms
step:2312/2330 train_time:135241ms step_avg:58.50ms
step:2313/2330 train_time:135297ms step_avg:58.49ms
step:2314/2330 train_time:135359ms step_avg:58.50ms
step:2315/2330 train_time:135416ms step_avg:58.50ms
step:2316/2330 train_time:135476ms step_avg:58.50ms
step:2317/2330 train_time:135534ms step_avg:58.50ms
step:2318/2330 train_time:135594ms step_avg:58.50ms
step:2319/2330 train_time:135653ms step_avg:58.50ms
step:2320/2330 train_time:135714ms step_avg:58.50ms
step:2321/2330 train_time:135771ms step_avg:58.50ms
step:2322/2330 train_time:135833ms step_avg:58.50ms
step:2323/2330 train_time:135891ms step_avg:58.50ms
step:2324/2330 train_time:135952ms step_avg:58.50ms
step:2325/2330 train_time:136011ms step_avg:58.50ms
step:2326/2330 train_time:136072ms step_avg:58.50ms
step:2327/2330 train_time:136129ms step_avg:58.50ms
step:2328/2330 train_time:136190ms step_avg:58.50ms
step:2329/2330 train_time:136246ms step_avg:58.50ms
step:2330/2330 train_time:136307ms step_avg:58.50ms
step:2330/2330 val_loss:3.6966 train_time:136389ms step_avg:58.54ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
