import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--beta1', type=str, required=True)
parser.add_argument('--beta2', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_beta_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=6e-4,  betas=(float(args2.beta1), float(args2.beta2)), eps=1e-10, weight_decay=0.0, fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt", exist_ok=True)
            torch.save(log, f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 04:26:13 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   27C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   25C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   25C    P0             115W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   24C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   26C    P0             113W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   25C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   25C    P0             109W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:78ms step_avg:77.81ms
step:2/2330 train_time:199ms step_avg:99.44ms
step:3/2330 train_time:218ms step_avg:72.53ms
step:4/2330 train_time:237ms step_avg:59.32ms
step:5/2330 train_time:290ms step_avg:57.92ms
step:6/2330 train_time:347ms step_avg:57.91ms
step:7/2330 train_time:403ms step_avg:57.59ms
step:8/2330 train_time:461ms step_avg:57.67ms
step:9/2330 train_time:517ms step_avg:57.45ms
step:10/2330 train_time:576ms step_avg:57.56ms
step:11/2330 train_time:631ms step_avg:57.36ms
step:12/2330 train_time:689ms step_avg:57.40ms
step:13/2330 train_time:744ms step_avg:57.22ms
step:14/2330 train_time:803ms step_avg:57.33ms
step:15/2330 train_time:858ms step_avg:57.17ms
step:16/2330 train_time:915ms step_avg:57.20ms
step:17/2330 train_time:970ms step_avg:57.07ms
step:18/2330 train_time:1029ms step_avg:57.16ms
step:19/2330 train_time:1086ms step_avg:57.14ms
step:20/2330 train_time:1149ms step_avg:57.44ms
step:21/2330 train_time:1207ms step_avg:57.48ms
step:22/2330 train_time:1269ms step_avg:57.69ms
step:23/2330 train_time:1325ms step_avg:57.62ms
step:24/2330 train_time:1384ms step_avg:57.67ms
step:25/2330 train_time:1439ms step_avg:57.56ms
step:26/2330 train_time:1499ms step_avg:57.65ms
step:27/2330 train_time:1555ms step_avg:57.59ms
step:28/2330 train_time:1613ms step_avg:57.62ms
step:29/2330 train_time:1668ms step_avg:57.53ms
step:30/2330 train_time:1727ms step_avg:57.57ms
step:31/2330 train_time:1782ms step_avg:57.50ms
step:32/2330 train_time:1841ms step_avg:57.52ms
step:33/2330 train_time:1896ms step_avg:57.46ms
step:34/2330 train_time:1954ms step_avg:57.48ms
step:35/2330 train_time:2010ms step_avg:57.43ms
step:36/2330 train_time:2069ms step_avg:57.47ms
step:37/2330 train_time:2126ms step_avg:57.46ms
step:38/2330 train_time:2187ms step_avg:57.54ms
step:39/2330 train_time:2244ms step_avg:57.53ms
step:40/2330 train_time:2304ms step_avg:57.59ms
step:41/2330 train_time:2359ms step_avg:57.54ms
step:42/2330 train_time:2419ms step_avg:57.59ms
step:43/2330 train_time:2475ms step_avg:57.56ms
step:44/2330 train_time:2533ms step_avg:57.58ms
step:45/2330 train_time:2589ms step_avg:57.53ms
step:46/2330 train_time:2647ms step_avg:57.55ms
step:47/2330 train_time:2702ms step_avg:57.50ms
step:48/2330 train_time:2761ms step_avg:57.53ms
step:49/2330 train_time:2817ms step_avg:57.49ms
step:50/2330 train_time:2875ms step_avg:57.51ms
step:51/2330 train_time:2931ms step_avg:57.47ms
step:52/2330 train_time:2990ms step_avg:57.50ms
step:53/2330 train_time:3046ms step_avg:57.48ms
step:54/2330 train_time:3106ms step_avg:57.52ms
step:55/2330 train_time:3162ms step_avg:57.50ms
step:56/2330 train_time:3224ms step_avg:57.56ms
step:57/2330 train_time:3280ms step_avg:57.54ms
step:58/2330 train_time:3340ms step_avg:57.58ms
step:59/2330 train_time:3396ms step_avg:57.56ms
step:60/2330 train_time:3455ms step_avg:57.59ms
step:61/2330 train_time:3511ms step_avg:57.56ms
step:62/2330 train_time:3570ms step_avg:57.58ms
step:63/2330 train_time:3626ms step_avg:57.55ms
step:64/2330 train_time:3685ms step_avg:57.58ms
step:65/2330 train_time:3740ms step_avg:57.54ms
step:66/2330 train_time:3800ms step_avg:57.57ms
step:67/2330 train_time:3856ms step_avg:57.55ms
step:68/2330 train_time:3914ms step_avg:57.56ms
step:69/2330 train_time:3970ms step_avg:57.54ms
step:70/2330 train_time:4029ms step_avg:57.55ms
step:71/2330 train_time:4085ms step_avg:57.53ms
step:72/2330 train_time:4144ms step_avg:57.56ms
step:73/2330 train_time:4199ms step_avg:57.53ms
step:74/2330 train_time:4260ms step_avg:57.56ms
step:75/2330 train_time:4317ms step_avg:57.55ms
step:76/2330 train_time:4376ms step_avg:57.57ms
step:77/2330 train_time:4431ms step_avg:57.55ms
step:78/2330 train_time:4491ms step_avg:57.58ms
step:79/2330 train_time:4547ms step_avg:57.55ms
step:80/2330 train_time:4606ms step_avg:57.58ms
step:81/2330 train_time:4661ms step_avg:57.55ms
step:82/2330 train_time:4722ms step_avg:57.59ms
step:83/2330 train_time:4778ms step_avg:57.56ms
step:84/2330 train_time:4836ms step_avg:57.58ms
step:85/2330 train_time:4893ms step_avg:57.56ms
step:86/2330 train_time:4951ms step_avg:57.57ms
step:87/2330 train_time:5008ms step_avg:57.56ms
step:88/2330 train_time:5067ms step_avg:57.58ms
step:89/2330 train_time:5123ms step_avg:57.56ms
step:90/2330 train_time:5182ms step_avg:57.58ms
step:91/2330 train_time:5238ms step_avg:57.56ms
step:92/2330 train_time:5298ms step_avg:57.58ms
step:93/2330 train_time:5354ms step_avg:57.57ms
step:94/2330 train_time:5413ms step_avg:57.59ms
step:95/2330 train_time:5470ms step_avg:57.58ms
step:96/2330 train_time:5528ms step_avg:57.59ms
step:97/2330 train_time:5584ms step_avg:57.57ms
step:98/2330 train_time:5643ms step_avg:57.58ms
step:99/2330 train_time:5700ms step_avg:57.57ms
step:100/2330 train_time:5758ms step_avg:57.58ms
step:101/2330 train_time:5814ms step_avg:57.56ms
step:102/2330 train_time:5873ms step_avg:57.58ms
step:103/2330 train_time:5929ms step_avg:57.56ms
step:104/2330 train_time:5988ms step_avg:57.58ms
step:105/2330 train_time:6044ms step_avg:57.56ms
step:106/2330 train_time:6103ms step_avg:57.58ms
step:107/2330 train_time:6159ms step_avg:57.56ms
step:108/2330 train_time:6219ms step_avg:57.58ms
step:109/2330 train_time:6274ms step_avg:57.56ms
step:110/2330 train_time:6334ms step_avg:57.58ms
step:111/2330 train_time:6390ms step_avg:57.57ms
step:112/2330 train_time:6449ms step_avg:57.58ms
step:113/2330 train_time:6506ms step_avg:57.57ms
step:114/2330 train_time:6565ms step_avg:57.58ms
step:115/2330 train_time:6620ms step_avg:57.56ms
step:116/2330 train_time:6680ms step_avg:57.58ms
step:117/2330 train_time:6736ms step_avg:57.57ms
step:118/2330 train_time:6795ms step_avg:57.59ms
step:119/2330 train_time:6851ms step_avg:57.57ms
step:120/2330 train_time:6910ms step_avg:57.58ms
step:121/2330 train_time:6965ms step_avg:57.57ms
step:122/2330 train_time:7025ms step_avg:57.58ms
step:123/2330 train_time:7081ms step_avg:57.57ms
step:124/2330 train_time:7141ms step_avg:57.59ms
step:125/2330 train_time:7196ms step_avg:57.57ms
step:126/2330 train_time:7256ms step_avg:57.58ms
step:127/2330 train_time:7311ms step_avg:57.57ms
step:128/2330 train_time:7371ms step_avg:57.58ms
step:129/2330 train_time:7426ms step_avg:57.57ms
step:130/2330 train_time:7486ms step_avg:57.58ms
step:131/2330 train_time:7541ms step_avg:57.56ms
step:132/2330 train_time:7601ms step_avg:57.58ms
step:133/2330 train_time:7657ms step_avg:57.57ms
step:134/2330 train_time:7716ms step_avg:57.58ms
step:135/2330 train_time:7772ms step_avg:57.57ms
step:136/2330 train_time:7831ms step_avg:57.58ms
step:137/2330 train_time:7887ms step_avg:57.57ms
step:138/2330 train_time:7946ms step_avg:57.58ms
step:139/2330 train_time:8001ms step_avg:57.56ms
step:140/2330 train_time:8060ms step_avg:57.57ms
step:141/2330 train_time:8117ms step_avg:57.57ms
step:142/2330 train_time:8175ms step_avg:57.57ms
step:143/2330 train_time:8231ms step_avg:57.56ms
step:144/2330 train_time:8291ms step_avg:57.58ms
step:145/2330 train_time:8346ms step_avg:57.56ms
step:146/2330 train_time:8406ms step_avg:57.58ms
step:147/2330 train_time:8462ms step_avg:57.56ms
step:148/2330 train_time:8522ms step_avg:57.58ms
step:149/2330 train_time:8578ms step_avg:57.57ms
step:150/2330 train_time:8636ms step_avg:57.57ms
step:151/2330 train_time:8692ms step_avg:57.56ms
step:152/2330 train_time:8751ms step_avg:57.57ms
step:153/2330 train_time:8807ms step_avg:57.56ms
step:154/2330 train_time:8865ms step_avg:57.57ms
step:155/2330 train_time:8921ms step_avg:57.55ms
step:156/2330 train_time:8980ms step_avg:57.57ms
step:157/2330 train_time:9036ms step_avg:57.56ms
step:158/2330 train_time:9095ms step_avg:57.57ms
step:159/2330 train_time:9151ms step_avg:57.55ms
step:160/2330 train_time:9211ms step_avg:57.57ms
step:161/2330 train_time:9266ms step_avg:57.55ms
step:162/2330 train_time:9326ms step_avg:57.57ms
step:163/2330 train_time:9381ms step_avg:57.55ms
step:164/2330 train_time:9440ms step_avg:57.56ms
step:165/2330 train_time:9496ms step_avg:57.55ms
step:166/2330 train_time:9554ms step_avg:57.55ms
step:167/2330 train_time:9610ms step_avg:57.54ms
step:168/2330 train_time:9669ms step_avg:57.55ms
step:169/2330 train_time:9725ms step_avg:57.54ms
step:170/2330 train_time:9785ms step_avg:57.56ms
step:171/2330 train_time:9840ms step_avg:57.54ms
step:172/2330 train_time:9899ms step_avg:57.55ms
step:173/2330 train_time:9955ms step_avg:57.55ms
step:174/2330 train_time:10014ms step_avg:57.55ms
step:175/2330 train_time:10070ms step_avg:57.54ms
step:176/2330 train_time:10129ms step_avg:57.55ms
step:177/2330 train_time:10185ms step_avg:57.54ms
step:178/2330 train_time:10245ms step_avg:57.55ms
step:179/2330 train_time:10300ms step_avg:57.54ms
step:180/2330 train_time:10360ms step_avg:57.55ms
step:181/2330 train_time:10416ms step_avg:57.55ms
step:182/2330 train_time:10474ms step_avg:57.55ms
step:183/2330 train_time:10530ms step_avg:57.54ms
step:184/2330 train_time:10589ms step_avg:57.55ms
step:185/2330 train_time:10645ms step_avg:57.54ms
step:186/2330 train_time:10704ms step_avg:57.55ms
step:187/2330 train_time:10760ms step_avg:57.54ms
step:188/2330 train_time:10819ms step_avg:57.55ms
step:189/2330 train_time:10875ms step_avg:57.54ms
step:190/2330 train_time:10934ms step_avg:57.55ms
step:191/2330 train_time:10990ms step_avg:57.54ms
step:192/2330 train_time:11048ms step_avg:57.54ms
step:193/2330 train_time:11105ms step_avg:57.54ms
step:194/2330 train_time:11163ms step_avg:57.54ms
step:195/2330 train_time:11219ms step_avg:57.53ms
step:196/2330 train_time:11278ms step_avg:57.54ms
step:197/2330 train_time:11334ms step_avg:57.53ms
step:198/2330 train_time:11393ms step_avg:57.54ms
step:199/2330 train_time:11449ms step_avg:57.53ms
step:200/2330 train_time:11507ms step_avg:57.53ms
step:201/2330 train_time:11563ms step_avg:57.53ms
step:202/2330 train_time:11622ms step_avg:57.54ms
step:203/2330 train_time:11678ms step_avg:57.53ms
step:204/2330 train_time:11737ms step_avg:57.53ms
step:205/2330 train_time:11793ms step_avg:57.53ms
step:206/2330 train_time:11851ms step_avg:57.53ms
step:207/2330 train_time:11907ms step_avg:57.52ms
step:208/2330 train_time:11966ms step_avg:57.53ms
step:209/2330 train_time:12021ms step_avg:57.52ms
step:210/2330 train_time:12082ms step_avg:57.53ms
step:211/2330 train_time:12139ms step_avg:57.53ms
step:212/2330 train_time:12197ms step_avg:57.53ms
step:213/2330 train_time:12253ms step_avg:57.52ms
step:214/2330 train_time:12312ms step_avg:57.53ms
step:215/2330 train_time:12368ms step_avg:57.53ms
step:216/2330 train_time:12427ms step_avg:57.53ms
step:217/2330 train_time:12483ms step_avg:57.52ms
step:218/2330 train_time:12542ms step_avg:57.53ms
step:219/2330 train_time:12598ms step_avg:57.53ms
step:220/2330 train_time:12657ms step_avg:57.53ms
step:221/2330 train_time:12713ms step_avg:57.52ms
step:222/2330 train_time:12772ms step_avg:57.53ms
step:223/2330 train_time:12828ms step_avg:57.52ms
step:224/2330 train_time:12886ms step_avg:57.53ms
step:225/2330 train_time:12941ms step_avg:57.52ms
step:226/2330 train_time:13002ms step_avg:57.53ms
step:227/2330 train_time:13058ms step_avg:57.52ms
step:228/2330 train_time:13117ms step_avg:57.53ms
step:229/2330 train_time:13172ms step_avg:57.52ms
step:230/2330 train_time:13231ms step_avg:57.53ms
step:231/2330 train_time:13287ms step_avg:57.52ms
step:232/2330 train_time:13346ms step_avg:57.53ms
step:233/2330 train_time:13402ms step_avg:57.52ms
step:234/2330 train_time:13461ms step_avg:57.53ms
step:235/2330 train_time:13517ms step_avg:57.52ms
step:236/2330 train_time:13576ms step_avg:57.53ms
step:237/2330 train_time:13632ms step_avg:57.52ms
step:238/2330 train_time:13691ms step_avg:57.53ms
step:239/2330 train_time:13747ms step_avg:57.52ms
step:240/2330 train_time:13806ms step_avg:57.52ms
step:241/2330 train_time:13861ms step_avg:57.51ms
step:242/2330 train_time:13921ms step_avg:57.53ms
step:243/2330 train_time:13977ms step_avg:57.52ms
step:244/2330 train_time:14036ms step_avg:57.52ms
step:245/2330 train_time:14092ms step_avg:57.52ms
step:246/2330 train_time:14151ms step_avg:57.52ms
step:247/2330 train_time:14206ms step_avg:57.51ms
step:248/2330 train_time:14266ms step_avg:57.52ms
step:249/2330 train_time:14322ms step_avg:57.52ms
step:250/2330 train_time:14381ms step_avg:57.52ms
step:250/2330 val_loss:4.9470 train_time:14460ms step_avg:57.84ms
step:251/2330 train_time:14478ms step_avg:57.68ms
step:252/2330 train_time:14498ms step_avg:57.53ms
step:253/2330 train_time:14553ms step_avg:57.52ms
step:254/2330 train_time:14616ms step_avg:57.54ms
step:255/2330 train_time:14671ms step_avg:57.53ms
step:256/2330 train_time:14735ms step_avg:57.56ms
step:257/2330 train_time:14790ms step_avg:57.55ms
step:258/2330 train_time:14851ms step_avg:57.56ms
step:259/2330 train_time:14906ms step_avg:57.55ms
step:260/2330 train_time:14965ms step_avg:57.56ms
step:261/2330 train_time:15020ms step_avg:57.55ms
step:262/2330 train_time:15078ms step_avg:57.55ms
step:263/2330 train_time:15134ms step_avg:57.54ms
step:264/2330 train_time:15192ms step_avg:57.54ms
step:265/2330 train_time:15247ms step_avg:57.53ms
step:266/2330 train_time:15305ms step_avg:57.54ms
step:267/2330 train_time:15361ms step_avg:57.53ms
step:268/2330 train_time:15421ms step_avg:57.54ms
step:269/2330 train_time:15479ms step_avg:57.54ms
step:270/2330 train_time:15538ms step_avg:57.55ms
step:271/2330 train_time:15595ms step_avg:57.55ms
step:272/2330 train_time:15655ms step_avg:57.55ms
step:273/2330 train_time:15711ms step_avg:57.55ms
step:274/2330 train_time:15770ms step_avg:57.56ms
step:275/2330 train_time:15826ms step_avg:57.55ms
step:276/2330 train_time:15885ms step_avg:57.56ms
step:277/2330 train_time:15941ms step_avg:57.55ms
step:278/2330 train_time:16000ms step_avg:57.55ms
step:279/2330 train_time:16056ms step_avg:57.55ms
step:280/2330 train_time:16114ms step_avg:57.55ms
step:281/2330 train_time:16170ms step_avg:57.54ms
step:282/2330 train_time:16228ms step_avg:57.55ms
step:283/2330 train_time:16283ms step_avg:57.54ms
step:284/2330 train_time:16343ms step_avg:57.54ms
step:285/2330 train_time:16398ms step_avg:57.54ms
step:286/2330 train_time:16457ms step_avg:57.54ms
step:287/2330 train_time:16513ms step_avg:57.54ms
step:288/2330 train_time:16574ms step_avg:57.55ms
step:289/2330 train_time:16630ms step_avg:57.54ms
step:290/2330 train_time:16690ms step_avg:57.55ms
step:291/2330 train_time:16745ms step_avg:57.54ms
step:292/2330 train_time:16805ms step_avg:57.55ms
step:293/2330 train_time:16861ms step_avg:57.55ms
step:294/2330 train_time:16921ms step_avg:57.55ms
step:295/2330 train_time:16977ms step_avg:57.55ms
step:296/2330 train_time:17036ms step_avg:57.55ms
step:297/2330 train_time:17092ms step_avg:57.55ms
step:298/2330 train_time:17151ms step_avg:57.55ms
step:299/2330 train_time:17206ms step_avg:57.55ms
step:300/2330 train_time:17265ms step_avg:57.55ms
step:301/2330 train_time:17320ms step_avg:57.54ms
step:302/2330 train_time:17379ms step_avg:57.55ms
step:303/2330 train_time:17435ms step_avg:57.54ms
step:304/2330 train_time:17493ms step_avg:57.54ms
step:305/2330 train_time:17549ms step_avg:57.54ms
step:306/2330 train_time:17609ms step_avg:57.54ms
step:307/2330 train_time:17664ms step_avg:57.54ms
step:308/2330 train_time:17725ms step_avg:57.55ms
step:309/2330 train_time:17781ms step_avg:57.54ms
step:310/2330 train_time:17840ms step_avg:57.55ms
step:311/2330 train_time:17896ms step_avg:57.54ms
step:312/2330 train_time:17955ms step_avg:57.55ms
step:313/2330 train_time:18010ms step_avg:57.54ms
step:314/2330 train_time:18070ms step_avg:57.55ms
step:315/2330 train_time:18126ms step_avg:57.54ms
step:316/2330 train_time:18184ms step_avg:57.55ms
step:317/2330 train_time:18240ms step_avg:57.54ms
step:318/2330 train_time:18300ms step_avg:57.55ms
step:319/2330 train_time:18355ms step_avg:57.54ms
step:320/2330 train_time:18414ms step_avg:57.54ms
step:321/2330 train_time:18470ms step_avg:57.54ms
step:322/2330 train_time:18529ms step_avg:57.54ms
step:323/2330 train_time:18585ms step_avg:57.54ms
step:324/2330 train_time:18644ms step_avg:57.54ms
step:325/2330 train_time:18700ms step_avg:57.54ms
step:326/2330 train_time:18760ms step_avg:57.55ms
step:327/2330 train_time:18817ms step_avg:57.54ms
step:328/2330 train_time:18876ms step_avg:57.55ms
step:329/2330 train_time:18932ms step_avg:57.54ms
step:330/2330 train_time:18992ms step_avg:57.55ms
step:331/2330 train_time:19047ms step_avg:57.54ms
step:332/2330 train_time:19105ms step_avg:57.55ms
step:333/2330 train_time:19161ms step_avg:57.54ms
step:334/2330 train_time:19221ms step_avg:57.55ms
step:335/2330 train_time:19277ms step_avg:57.54ms
step:336/2330 train_time:19336ms step_avg:57.55ms
step:337/2330 train_time:19393ms step_avg:57.54ms
step:338/2330 train_time:19451ms step_avg:57.55ms
step:339/2330 train_time:19508ms step_avg:57.54ms
step:340/2330 train_time:19566ms step_avg:57.55ms
step:341/2330 train_time:19622ms step_avg:57.54ms
step:342/2330 train_time:19682ms step_avg:57.55ms
step:343/2330 train_time:19738ms step_avg:57.55ms
step:344/2330 train_time:19797ms step_avg:57.55ms
step:345/2330 train_time:19853ms step_avg:57.55ms
step:346/2330 train_time:19913ms step_avg:57.55ms
step:347/2330 train_time:19968ms step_avg:57.55ms
step:348/2330 train_time:20028ms step_avg:57.55ms
step:349/2330 train_time:20083ms step_avg:57.54ms
step:350/2330 train_time:20142ms step_avg:57.55ms
step:351/2330 train_time:20197ms step_avg:57.54ms
step:352/2330 train_time:20256ms step_avg:57.55ms
step:353/2330 train_time:20312ms step_avg:57.54ms
step:354/2330 train_time:20372ms step_avg:57.55ms
step:355/2330 train_time:20427ms step_avg:57.54ms
step:356/2330 train_time:20487ms step_avg:57.55ms
step:357/2330 train_time:20543ms step_avg:57.54ms
step:358/2330 train_time:20603ms step_avg:57.55ms
step:359/2330 train_time:20659ms step_avg:57.55ms
step:360/2330 train_time:20719ms step_avg:57.55ms
step:361/2330 train_time:20775ms step_avg:57.55ms
step:362/2330 train_time:20834ms step_avg:57.55ms
step:363/2330 train_time:20891ms step_avg:57.55ms
step:364/2330 train_time:20949ms step_avg:57.55ms
step:365/2330 train_time:21005ms step_avg:57.55ms
step:366/2330 train_time:21064ms step_avg:57.55ms
step:367/2330 train_time:21120ms step_avg:57.55ms
step:368/2330 train_time:21179ms step_avg:57.55ms
step:369/2330 train_time:21235ms step_avg:57.55ms
step:370/2330 train_time:21294ms step_avg:57.55ms
step:371/2330 train_time:21350ms step_avg:57.55ms
step:372/2330 train_time:21409ms step_avg:57.55ms
step:373/2330 train_time:21465ms step_avg:57.55ms
step:374/2330 train_time:21524ms step_avg:57.55ms
step:375/2330 train_time:21579ms step_avg:57.54ms
step:376/2330 train_time:21641ms step_avg:57.56ms
step:377/2330 train_time:21697ms step_avg:57.55ms
step:378/2330 train_time:21757ms step_avg:57.56ms
step:379/2330 train_time:21813ms step_avg:57.55ms
step:380/2330 train_time:21872ms step_avg:57.56ms
step:381/2330 train_time:21928ms step_avg:57.55ms
step:382/2330 train_time:21987ms step_avg:57.56ms
step:383/2330 train_time:22043ms step_avg:57.55ms
step:384/2330 train_time:22102ms step_avg:57.56ms
step:385/2330 train_time:22158ms step_avg:57.55ms
step:386/2330 train_time:22217ms step_avg:57.56ms
step:387/2330 train_time:22273ms step_avg:57.55ms
step:388/2330 train_time:22331ms step_avg:57.55ms
step:389/2330 train_time:22388ms step_avg:57.55ms
step:390/2330 train_time:22447ms step_avg:57.56ms
step:391/2330 train_time:22502ms step_avg:57.55ms
step:392/2330 train_time:22561ms step_avg:57.55ms
step:393/2330 train_time:22616ms step_avg:57.55ms
step:394/2330 train_time:22676ms step_avg:57.55ms
step:395/2330 train_time:22731ms step_avg:57.55ms
step:396/2330 train_time:22791ms step_avg:57.55ms
step:397/2330 train_time:22847ms step_avg:57.55ms
step:398/2330 train_time:22906ms step_avg:57.55ms
step:399/2330 train_time:22962ms step_avg:57.55ms
step:400/2330 train_time:23022ms step_avg:57.55ms
step:401/2330 train_time:23078ms step_avg:57.55ms
step:402/2330 train_time:23138ms step_avg:57.56ms
step:403/2330 train_time:23194ms step_avg:57.55ms
step:404/2330 train_time:23253ms step_avg:57.56ms
step:405/2330 train_time:23309ms step_avg:57.55ms
step:406/2330 train_time:23367ms step_avg:57.55ms
step:407/2330 train_time:23423ms step_avg:57.55ms
step:408/2330 train_time:23482ms step_avg:57.55ms
step:409/2330 train_time:23538ms step_avg:57.55ms
step:410/2330 train_time:23597ms step_avg:57.55ms
step:411/2330 train_time:23653ms step_avg:57.55ms
step:412/2330 train_time:23712ms step_avg:57.55ms
step:413/2330 train_time:23768ms step_avg:57.55ms
step:414/2330 train_time:23828ms step_avg:57.55ms
step:415/2330 train_time:23884ms step_avg:57.55ms
step:416/2330 train_time:23943ms step_avg:57.55ms
step:417/2330 train_time:23998ms step_avg:57.55ms
step:418/2330 train_time:24057ms step_avg:57.55ms
step:419/2330 train_time:24113ms step_avg:57.55ms
step:420/2330 train_time:24172ms step_avg:57.55ms
step:421/2330 train_time:24228ms step_avg:57.55ms
step:422/2330 train_time:24287ms step_avg:57.55ms
step:423/2330 train_time:24343ms step_avg:57.55ms
step:424/2330 train_time:24402ms step_avg:57.55ms
step:425/2330 train_time:24458ms step_avg:57.55ms
step:426/2330 train_time:24517ms step_avg:57.55ms
step:427/2330 train_time:24573ms step_avg:57.55ms
step:428/2330 train_time:24632ms step_avg:57.55ms
step:429/2330 train_time:24688ms step_avg:57.55ms
step:430/2330 train_time:24748ms step_avg:57.55ms
step:431/2330 train_time:24804ms step_avg:57.55ms
step:432/2330 train_time:24863ms step_avg:57.55ms
step:433/2330 train_time:24918ms step_avg:57.55ms
step:434/2330 train_time:24978ms step_avg:57.55ms
step:435/2330 train_time:25034ms step_avg:57.55ms
step:436/2330 train_time:25093ms step_avg:57.55ms
step:437/2330 train_time:25150ms step_avg:57.55ms
step:438/2330 train_time:25208ms step_avg:57.55ms
step:439/2330 train_time:25264ms step_avg:57.55ms
step:440/2330 train_time:25323ms step_avg:57.55ms
step:441/2330 train_time:25379ms step_avg:57.55ms
step:442/2330 train_time:25438ms step_avg:57.55ms
step:443/2330 train_time:25493ms step_avg:57.55ms
step:444/2330 train_time:25553ms step_avg:57.55ms
step:445/2330 train_time:25609ms step_avg:57.55ms
step:446/2330 train_time:25668ms step_avg:57.55ms
step:447/2330 train_time:25724ms step_avg:57.55ms
step:448/2330 train_time:25783ms step_avg:57.55ms
step:449/2330 train_time:25839ms step_avg:57.55ms
step:450/2330 train_time:25898ms step_avg:57.55ms
step:451/2330 train_time:25953ms step_avg:57.55ms
step:452/2330 train_time:26013ms step_avg:57.55ms
step:453/2330 train_time:26069ms step_avg:57.55ms
step:454/2330 train_time:26129ms step_avg:57.55ms
step:455/2330 train_time:26185ms step_avg:57.55ms
step:456/2330 train_time:26246ms step_avg:57.56ms
step:457/2330 train_time:26302ms step_avg:57.55ms
step:458/2330 train_time:26362ms step_avg:57.56ms
step:459/2330 train_time:26417ms step_avg:57.55ms
step:460/2330 train_time:26476ms step_avg:57.56ms
step:461/2330 train_time:26532ms step_avg:57.55ms
step:462/2330 train_time:26591ms step_avg:57.56ms
step:463/2330 train_time:26647ms step_avg:57.55ms
step:464/2330 train_time:26706ms step_avg:57.56ms
step:465/2330 train_time:26761ms step_avg:57.55ms
step:466/2330 train_time:26821ms step_avg:57.56ms
step:467/2330 train_time:26877ms step_avg:57.55ms
step:468/2330 train_time:26937ms step_avg:57.56ms
step:469/2330 train_time:26994ms step_avg:57.56ms
step:470/2330 train_time:27052ms step_avg:57.56ms
step:471/2330 train_time:27109ms step_avg:57.56ms
step:472/2330 train_time:27168ms step_avg:57.56ms
step:473/2330 train_time:27224ms step_avg:57.56ms
step:474/2330 train_time:27283ms step_avg:57.56ms
step:475/2330 train_time:27340ms step_avg:57.56ms
step:476/2330 train_time:27398ms step_avg:57.56ms
step:477/2330 train_time:27454ms step_avg:57.56ms
step:478/2330 train_time:27513ms step_avg:57.56ms
step:479/2330 train_time:27569ms step_avg:57.56ms
step:480/2330 train_time:27628ms step_avg:57.56ms
step:481/2330 train_time:27685ms step_avg:57.56ms
step:482/2330 train_time:27743ms step_avg:57.56ms
step:483/2330 train_time:27798ms step_avg:57.55ms
step:484/2330 train_time:27858ms step_avg:57.56ms
step:485/2330 train_time:27914ms step_avg:57.55ms
step:486/2330 train_time:27974ms step_avg:57.56ms
step:487/2330 train_time:28030ms step_avg:57.56ms
step:488/2330 train_time:28089ms step_avg:57.56ms
step:489/2330 train_time:28145ms step_avg:57.56ms
step:490/2330 train_time:28204ms step_avg:57.56ms
step:491/2330 train_time:28259ms step_avg:57.55ms
step:492/2330 train_time:28320ms step_avg:57.56ms
step:493/2330 train_time:28377ms step_avg:57.56ms
step:494/2330 train_time:28436ms step_avg:57.56ms
step:495/2330 train_time:28492ms step_avg:57.56ms
step:496/2330 train_time:28551ms step_avg:57.56ms
step:497/2330 train_time:28607ms step_avg:57.56ms
step:498/2330 train_time:28665ms step_avg:57.56ms
step:499/2330 train_time:28721ms step_avg:57.56ms
step:500/2330 train_time:28781ms step_avg:57.56ms
step:500/2330 val_loss:4.4478 train_time:28860ms step_avg:57.72ms
step:501/2330 train_time:28880ms step_avg:57.64ms
step:502/2330 train_time:28900ms step_avg:57.57ms
step:503/2330 train_time:28958ms step_avg:57.57ms
step:504/2330 train_time:29021ms step_avg:57.58ms
step:505/2330 train_time:29078ms step_avg:57.58ms
step:506/2330 train_time:29137ms step_avg:57.58ms
step:507/2330 train_time:29194ms step_avg:57.58ms
step:508/2330 train_time:29253ms step_avg:57.58ms
step:509/2330 train_time:29310ms step_avg:57.58ms
step:510/2330 train_time:29368ms step_avg:57.58ms
step:511/2330 train_time:29423ms step_avg:57.58ms
step:512/2330 train_time:29482ms step_avg:57.58ms
step:513/2330 train_time:29538ms step_avg:57.58ms
step:514/2330 train_time:29596ms step_avg:57.58ms
step:515/2330 train_time:29651ms step_avg:57.57ms
step:516/2330 train_time:29710ms step_avg:57.58ms
step:517/2330 train_time:29765ms step_avg:57.57ms
step:518/2330 train_time:29825ms step_avg:57.58ms
step:519/2330 train_time:29880ms step_avg:57.57ms
step:520/2330 train_time:29941ms step_avg:57.58ms
step:521/2330 train_time:29998ms step_avg:57.58ms
step:522/2330 train_time:30059ms step_avg:57.58ms
step:523/2330 train_time:30115ms step_avg:57.58ms
step:524/2330 train_time:30175ms step_avg:57.59ms
step:525/2330 train_time:30231ms step_avg:57.58ms
step:526/2330 train_time:30291ms step_avg:57.59ms
step:527/2330 train_time:30346ms step_avg:57.58ms
step:528/2330 train_time:30405ms step_avg:57.59ms
step:529/2330 train_time:30461ms step_avg:57.58ms
step:530/2330 train_time:30520ms step_avg:57.58ms
step:531/2330 train_time:30576ms step_avg:57.58ms
step:532/2330 train_time:30634ms step_avg:57.58ms
step:533/2330 train_time:30690ms step_avg:57.58ms
step:534/2330 train_time:30748ms step_avg:57.58ms
step:535/2330 train_time:30804ms step_avg:57.58ms
step:536/2330 train_time:30863ms step_avg:57.58ms
step:537/2330 train_time:30920ms step_avg:57.58ms
step:538/2330 train_time:30980ms step_avg:57.58ms
step:539/2330 train_time:31036ms step_avg:57.58ms
step:540/2330 train_time:31097ms step_avg:57.59ms
step:541/2330 train_time:31154ms step_avg:57.59ms
step:542/2330 train_time:31213ms step_avg:57.59ms
step:543/2330 train_time:31270ms step_avg:57.59ms
step:544/2330 train_time:31328ms step_avg:57.59ms
step:545/2330 train_time:31384ms step_avg:57.59ms
step:546/2330 train_time:31444ms step_avg:57.59ms
step:547/2330 train_time:31499ms step_avg:57.59ms
step:548/2330 train_time:31559ms step_avg:57.59ms
step:549/2330 train_time:31614ms step_avg:57.59ms
step:550/2330 train_time:31673ms step_avg:57.59ms
step:551/2330 train_time:31729ms step_avg:57.59ms
step:552/2330 train_time:31787ms step_avg:57.59ms
step:553/2330 train_time:31843ms step_avg:57.58ms
step:554/2330 train_time:31905ms step_avg:57.59ms
step:555/2330 train_time:31960ms step_avg:57.59ms
step:556/2330 train_time:32022ms step_avg:57.59ms
step:557/2330 train_time:32078ms step_avg:57.59ms
step:558/2330 train_time:32137ms step_avg:57.59ms
step:559/2330 train_time:32194ms step_avg:57.59ms
step:560/2330 train_time:32254ms step_avg:57.60ms
step:561/2330 train_time:32310ms step_avg:57.59ms
step:562/2330 train_time:32369ms step_avg:57.60ms
step:563/2330 train_time:32425ms step_avg:57.59ms
step:564/2330 train_time:32484ms step_avg:57.59ms
step:565/2330 train_time:32539ms step_avg:57.59ms
step:566/2330 train_time:32600ms step_avg:57.60ms
step:567/2330 train_time:32656ms step_avg:57.59ms
step:568/2330 train_time:32715ms step_avg:57.60ms
step:569/2330 train_time:32770ms step_avg:57.59ms
step:570/2330 train_time:32830ms step_avg:57.60ms
step:571/2330 train_time:32886ms step_avg:57.59ms
step:572/2330 train_time:32946ms step_avg:57.60ms
step:573/2330 train_time:33001ms step_avg:57.59ms
step:574/2330 train_time:33063ms step_avg:57.60ms
step:575/2330 train_time:33119ms step_avg:57.60ms
step:576/2330 train_time:33179ms step_avg:57.60ms
step:577/2330 train_time:33236ms step_avg:57.60ms
step:578/2330 train_time:33295ms step_avg:57.60ms
step:579/2330 train_time:33351ms step_avg:57.60ms
step:580/2330 train_time:33409ms step_avg:57.60ms
step:581/2330 train_time:33465ms step_avg:57.60ms
step:582/2330 train_time:33524ms step_avg:57.60ms
step:583/2330 train_time:33580ms step_avg:57.60ms
step:584/2330 train_time:33641ms step_avg:57.60ms
step:585/2330 train_time:33697ms step_avg:57.60ms
step:586/2330 train_time:33756ms step_avg:57.60ms
step:587/2330 train_time:33812ms step_avg:57.60ms
step:588/2330 train_time:33871ms step_avg:57.60ms
step:589/2330 train_time:33927ms step_avg:57.60ms
step:590/2330 train_time:33987ms step_avg:57.60ms
step:591/2330 train_time:34042ms step_avg:57.60ms
step:592/2330 train_time:34103ms step_avg:57.61ms
step:593/2330 train_time:34159ms step_avg:57.60ms
step:594/2330 train_time:34220ms step_avg:57.61ms
step:595/2330 train_time:34276ms step_avg:57.61ms
step:596/2330 train_time:34336ms step_avg:57.61ms
step:597/2330 train_time:34392ms step_avg:57.61ms
step:598/2330 train_time:34451ms step_avg:57.61ms
step:599/2330 train_time:34506ms step_avg:57.61ms
step:600/2330 train_time:34566ms step_avg:57.61ms
step:601/2330 train_time:34622ms step_avg:57.61ms
step:602/2330 train_time:34683ms step_avg:57.61ms
step:603/2330 train_time:34739ms step_avg:57.61ms
step:604/2330 train_time:34797ms step_avg:57.61ms
step:605/2330 train_time:34853ms step_avg:57.61ms
step:606/2330 train_time:34912ms step_avg:57.61ms
step:607/2330 train_time:34968ms step_avg:57.61ms
step:608/2330 train_time:35028ms step_avg:57.61ms
step:609/2330 train_time:35084ms step_avg:57.61ms
step:610/2330 train_time:35145ms step_avg:57.61ms
step:611/2330 train_time:35200ms step_avg:57.61ms
step:612/2330 train_time:35262ms step_avg:57.62ms
step:613/2330 train_time:35318ms step_avg:57.61ms
step:614/2330 train_time:35377ms step_avg:57.62ms
step:615/2330 train_time:35433ms step_avg:57.61ms
step:616/2330 train_time:35492ms step_avg:57.62ms
step:617/2330 train_time:35548ms step_avg:57.61ms
step:618/2330 train_time:35608ms step_avg:57.62ms
step:619/2330 train_time:35664ms step_avg:57.61ms
step:620/2330 train_time:35724ms step_avg:57.62ms
step:621/2330 train_time:35780ms step_avg:57.62ms
step:622/2330 train_time:35839ms step_avg:57.62ms
step:623/2330 train_time:35895ms step_avg:57.62ms
step:624/2330 train_time:35955ms step_avg:57.62ms
step:625/2330 train_time:36011ms step_avg:57.62ms
step:626/2330 train_time:36070ms step_avg:57.62ms
step:627/2330 train_time:36126ms step_avg:57.62ms
step:628/2330 train_time:36186ms step_avg:57.62ms
step:629/2330 train_time:36242ms step_avg:57.62ms
step:630/2330 train_time:36302ms step_avg:57.62ms
step:631/2330 train_time:36358ms step_avg:57.62ms
step:632/2330 train_time:36417ms step_avg:57.62ms
step:633/2330 train_time:36474ms step_avg:57.62ms
step:634/2330 train_time:36533ms step_avg:57.62ms
step:635/2330 train_time:36589ms step_avg:57.62ms
step:636/2330 train_time:36648ms step_avg:57.62ms
step:637/2330 train_time:36704ms step_avg:57.62ms
step:638/2330 train_time:36764ms step_avg:57.62ms
step:639/2330 train_time:36820ms step_avg:57.62ms
step:640/2330 train_time:36879ms step_avg:57.62ms
step:641/2330 train_time:36935ms step_avg:57.62ms
step:642/2330 train_time:36995ms step_avg:57.62ms
step:643/2330 train_time:37051ms step_avg:57.62ms
step:644/2330 train_time:37111ms step_avg:57.63ms
step:645/2330 train_time:37166ms step_avg:57.62ms
step:646/2330 train_time:37226ms step_avg:57.63ms
step:647/2330 train_time:37282ms step_avg:57.62ms
step:648/2330 train_time:37343ms step_avg:57.63ms
step:649/2330 train_time:37399ms step_avg:57.63ms
step:650/2330 train_time:37459ms step_avg:57.63ms
step:651/2330 train_time:37515ms step_avg:57.63ms
step:652/2330 train_time:37575ms step_avg:57.63ms
step:653/2330 train_time:37630ms step_avg:57.63ms
step:654/2330 train_time:37689ms step_avg:57.63ms
step:655/2330 train_time:37745ms step_avg:57.63ms
step:656/2330 train_time:37806ms step_avg:57.63ms
step:657/2330 train_time:37862ms step_avg:57.63ms
step:658/2330 train_time:37921ms step_avg:57.63ms
step:659/2330 train_time:37978ms step_avg:57.63ms
step:660/2330 train_time:38037ms step_avg:57.63ms
step:661/2330 train_time:38093ms step_avg:57.63ms
step:662/2330 train_time:38152ms step_avg:57.63ms
step:663/2330 train_time:38208ms step_avg:57.63ms
step:664/2330 train_time:38268ms step_avg:57.63ms
step:665/2330 train_time:38324ms step_avg:57.63ms
step:666/2330 train_time:38385ms step_avg:57.63ms
step:667/2330 train_time:38440ms step_avg:57.63ms
step:668/2330 train_time:38501ms step_avg:57.64ms
step:669/2330 train_time:38557ms step_avg:57.63ms
step:670/2330 train_time:38616ms step_avg:57.64ms
step:671/2330 train_time:38672ms step_avg:57.63ms
step:672/2330 train_time:38731ms step_avg:57.64ms
step:673/2330 train_time:38787ms step_avg:57.63ms
step:674/2330 train_time:38847ms step_avg:57.64ms
step:675/2330 train_time:38902ms step_avg:57.63ms
step:676/2330 train_time:38963ms step_avg:57.64ms
step:677/2330 train_time:39019ms step_avg:57.64ms
step:678/2330 train_time:39078ms step_avg:57.64ms
step:679/2330 train_time:39135ms step_avg:57.64ms
step:680/2330 train_time:39194ms step_avg:57.64ms
step:681/2330 train_time:39250ms step_avg:57.64ms
step:682/2330 train_time:39309ms step_avg:57.64ms
step:683/2330 train_time:39364ms step_avg:57.63ms
step:684/2330 train_time:39425ms step_avg:57.64ms
step:685/2330 train_time:39480ms step_avg:57.64ms
step:686/2330 train_time:39541ms step_avg:57.64ms
step:687/2330 train_time:39597ms step_avg:57.64ms
step:688/2330 train_time:39657ms step_avg:57.64ms
step:689/2330 train_time:39713ms step_avg:57.64ms
step:690/2330 train_time:39772ms step_avg:57.64ms
step:691/2330 train_time:39829ms step_avg:57.64ms
step:692/2330 train_time:39887ms step_avg:57.64ms
step:693/2330 train_time:39943ms step_avg:57.64ms
step:694/2330 train_time:40003ms step_avg:57.64ms
step:695/2330 train_time:40059ms step_avg:57.64ms
step:696/2330 train_time:40119ms step_avg:57.64ms
step:697/2330 train_time:40176ms step_avg:57.64ms
step:698/2330 train_time:40236ms step_avg:57.64ms
step:699/2330 train_time:40292ms step_avg:57.64ms
step:700/2330 train_time:40350ms step_avg:57.64ms
step:701/2330 train_time:40406ms step_avg:57.64ms
step:702/2330 train_time:40466ms step_avg:57.64ms
step:703/2330 train_time:40521ms step_avg:57.64ms
step:704/2330 train_time:40583ms step_avg:57.65ms
step:705/2330 train_time:40638ms step_avg:57.64ms
step:706/2330 train_time:40699ms step_avg:57.65ms
step:707/2330 train_time:40755ms step_avg:57.64ms
step:708/2330 train_time:40815ms step_avg:57.65ms
step:709/2330 train_time:40871ms step_avg:57.65ms
step:710/2330 train_time:40929ms step_avg:57.65ms
step:711/2330 train_time:40985ms step_avg:57.64ms
step:712/2330 train_time:41044ms step_avg:57.65ms
step:713/2330 train_time:41100ms step_avg:57.64ms
step:714/2330 train_time:41162ms step_avg:57.65ms
step:715/2330 train_time:41218ms step_avg:57.65ms
step:716/2330 train_time:41277ms step_avg:57.65ms
step:717/2330 train_time:41333ms step_avg:57.65ms
step:718/2330 train_time:41392ms step_avg:57.65ms
step:719/2330 train_time:41448ms step_avg:57.65ms
step:720/2330 train_time:41507ms step_avg:57.65ms
step:721/2330 train_time:41563ms step_avg:57.65ms
step:722/2330 train_time:41623ms step_avg:57.65ms
step:723/2330 train_time:41679ms step_avg:57.65ms
step:724/2330 train_time:41739ms step_avg:57.65ms
step:725/2330 train_time:41795ms step_avg:57.65ms
step:726/2330 train_time:41854ms step_avg:57.65ms
step:727/2330 train_time:41911ms step_avg:57.65ms
step:728/2330 train_time:41970ms step_avg:57.65ms
step:729/2330 train_time:42025ms step_avg:57.65ms
step:730/2330 train_time:42085ms step_avg:57.65ms
step:731/2330 train_time:42141ms step_avg:57.65ms
step:732/2330 train_time:42201ms step_avg:57.65ms
step:733/2330 train_time:42257ms step_avg:57.65ms
step:734/2330 train_time:42316ms step_avg:57.65ms
step:735/2330 train_time:42372ms step_avg:57.65ms
step:736/2330 train_time:42431ms step_avg:57.65ms
step:737/2330 train_time:42487ms step_avg:57.65ms
step:738/2330 train_time:42547ms step_avg:57.65ms
step:739/2330 train_time:42602ms step_avg:57.65ms
step:740/2330 train_time:42663ms step_avg:57.65ms
step:741/2330 train_time:42718ms step_avg:57.65ms
step:742/2330 train_time:42778ms step_avg:57.65ms
step:743/2330 train_time:42835ms step_avg:57.65ms
step:744/2330 train_time:42894ms step_avg:57.65ms
step:745/2330 train_time:42949ms step_avg:57.65ms
step:746/2330 train_time:43009ms step_avg:57.65ms
step:747/2330 train_time:43065ms step_avg:57.65ms
step:748/2330 train_time:43125ms step_avg:57.65ms
step:749/2330 train_time:43181ms step_avg:57.65ms
step:750/2330 train_time:43241ms step_avg:57.65ms
step:750/2330 val_loss:4.2306 train_time:43321ms step_avg:57.76ms
step:751/2330 train_time:43340ms step_avg:57.71ms
step:752/2330 train_time:43360ms step_avg:57.66ms
step:753/2330 train_time:43415ms step_avg:57.66ms
step:754/2330 train_time:43478ms step_avg:57.66ms
step:755/2330 train_time:43534ms step_avg:57.66ms
step:756/2330 train_time:43595ms step_avg:57.67ms
step:757/2330 train_time:43651ms step_avg:57.66ms
step:758/2330 train_time:43710ms step_avg:57.67ms
step:759/2330 train_time:43767ms step_avg:57.66ms
step:760/2330 train_time:43825ms step_avg:57.66ms
step:761/2330 train_time:43879ms step_avg:57.66ms
step:762/2330 train_time:43939ms step_avg:57.66ms
step:763/2330 train_time:43994ms step_avg:57.66ms
step:764/2330 train_time:44053ms step_avg:57.66ms
step:765/2330 train_time:44110ms step_avg:57.66ms
step:766/2330 train_time:44168ms step_avg:57.66ms
step:767/2330 train_time:44225ms step_avg:57.66ms
step:768/2330 train_time:44284ms step_avg:57.66ms
step:769/2330 train_time:44343ms step_avg:57.66ms
step:770/2330 train_time:44404ms step_avg:57.67ms
step:771/2330 train_time:44461ms step_avg:57.67ms
step:772/2330 train_time:44524ms step_avg:57.67ms
step:773/2330 train_time:44581ms step_avg:57.67ms
step:774/2330 train_time:44643ms step_avg:57.68ms
step:775/2330 train_time:44700ms step_avg:57.68ms
step:776/2330 train_time:44760ms step_avg:57.68ms
step:777/2330 train_time:44816ms step_avg:57.68ms
step:778/2330 train_time:44876ms step_avg:57.68ms
step:779/2330 train_time:44932ms step_avg:57.68ms
step:780/2330 train_time:44991ms step_avg:57.68ms
step:781/2330 train_time:45048ms step_avg:57.68ms
step:782/2330 train_time:45108ms step_avg:57.68ms
step:783/2330 train_time:45165ms step_avg:57.68ms
step:784/2330 train_time:45224ms step_avg:57.68ms
step:785/2330 train_time:45281ms step_avg:57.68ms
step:786/2330 train_time:45341ms step_avg:57.69ms
step:787/2330 train_time:45398ms step_avg:57.69ms
step:788/2330 train_time:45460ms step_avg:57.69ms
step:789/2330 train_time:45517ms step_avg:57.69ms
step:790/2330 train_time:45577ms step_avg:57.69ms
step:791/2330 train_time:45635ms step_avg:57.69ms
step:792/2330 train_time:45695ms step_avg:57.70ms
step:793/2330 train_time:45751ms step_avg:57.69ms
step:794/2330 train_time:45811ms step_avg:57.70ms
step:795/2330 train_time:45868ms step_avg:57.70ms
step:796/2330 train_time:45926ms step_avg:57.70ms
step:797/2330 train_time:45982ms step_avg:57.69ms
step:798/2330 train_time:46043ms step_avg:57.70ms
step:799/2330 train_time:46099ms step_avg:57.70ms
step:800/2330 train_time:46160ms step_avg:57.70ms
step:801/2330 train_time:46217ms step_avg:57.70ms
step:802/2330 train_time:46276ms step_avg:57.70ms
step:803/2330 train_time:46333ms step_avg:57.70ms
step:804/2330 train_time:46394ms step_avg:57.70ms
step:805/2330 train_time:46451ms step_avg:57.70ms
step:806/2330 train_time:46512ms step_avg:57.71ms
step:807/2330 train_time:46568ms step_avg:57.71ms
step:808/2330 train_time:46629ms step_avg:57.71ms
step:809/2330 train_time:46685ms step_avg:57.71ms
step:810/2330 train_time:46747ms step_avg:57.71ms
step:811/2330 train_time:46803ms step_avg:57.71ms
step:812/2330 train_time:46863ms step_avg:57.71ms
step:813/2330 train_time:46920ms step_avg:57.71ms
step:814/2330 train_time:46981ms step_avg:57.72ms
step:815/2330 train_time:47037ms step_avg:57.71ms
step:816/2330 train_time:47096ms step_avg:57.72ms
step:817/2330 train_time:47153ms step_avg:57.71ms
step:818/2330 train_time:47212ms step_avg:57.72ms
step:819/2330 train_time:47269ms step_avg:57.72ms
step:820/2330 train_time:47329ms step_avg:57.72ms
step:821/2330 train_time:47385ms step_avg:57.72ms
step:822/2330 train_time:47447ms step_avg:57.72ms
step:823/2330 train_time:47503ms step_avg:57.72ms
step:824/2330 train_time:47564ms step_avg:57.72ms
step:825/2330 train_time:47621ms step_avg:57.72ms
step:826/2330 train_time:47682ms step_avg:57.73ms
step:827/2330 train_time:47739ms step_avg:57.73ms
step:828/2330 train_time:47799ms step_avg:57.73ms
step:829/2330 train_time:47856ms step_avg:57.73ms
step:830/2330 train_time:47916ms step_avg:57.73ms
step:831/2330 train_time:47973ms step_avg:57.73ms
step:832/2330 train_time:48031ms step_avg:57.73ms
step:833/2330 train_time:48088ms step_avg:57.73ms
step:834/2330 train_time:48148ms step_avg:57.73ms
step:835/2330 train_time:48205ms step_avg:57.73ms
step:836/2330 train_time:48265ms step_avg:57.73ms
step:837/2330 train_time:48321ms step_avg:57.73ms
step:838/2330 train_time:48382ms step_avg:57.74ms
step:839/2330 train_time:48440ms step_avg:57.73ms
step:840/2330 train_time:48501ms step_avg:57.74ms
step:841/2330 train_time:48557ms step_avg:57.74ms
step:842/2330 train_time:48618ms step_avg:57.74ms
step:843/2330 train_time:48674ms step_avg:57.74ms
step:844/2330 train_time:48735ms step_avg:57.74ms
step:845/2330 train_time:48791ms step_avg:57.74ms
step:846/2330 train_time:48851ms step_avg:57.74ms
step:847/2330 train_time:48908ms step_avg:57.74ms
step:848/2330 train_time:48968ms step_avg:57.75ms
step:849/2330 train_time:49025ms step_avg:57.74ms
step:850/2330 train_time:49085ms step_avg:57.75ms
step:851/2330 train_time:49141ms step_avg:57.75ms
step:852/2330 train_time:49202ms step_avg:57.75ms
step:853/2330 train_time:49259ms step_avg:57.75ms
step:854/2330 train_time:49319ms step_avg:57.75ms
step:855/2330 train_time:49375ms step_avg:57.75ms
step:856/2330 train_time:49436ms step_avg:57.75ms
step:857/2330 train_time:49493ms step_avg:57.75ms
step:858/2330 train_time:49553ms step_avg:57.75ms
step:859/2330 train_time:49610ms step_avg:57.75ms
step:860/2330 train_time:49670ms step_avg:57.76ms
step:861/2330 train_time:49726ms step_avg:57.75ms
step:862/2330 train_time:49787ms step_avg:57.76ms
step:863/2330 train_time:49844ms step_avg:57.76ms
step:864/2330 train_time:49905ms step_avg:57.76ms
step:865/2330 train_time:49962ms step_avg:57.76ms
step:866/2330 train_time:50021ms step_avg:57.76ms
step:867/2330 train_time:50078ms step_avg:57.76ms
step:868/2330 train_time:50139ms step_avg:57.76ms
step:869/2330 train_time:50195ms step_avg:57.76ms
step:870/2330 train_time:50255ms step_avg:57.76ms
step:871/2330 train_time:50312ms step_avg:57.76ms
step:872/2330 train_time:50372ms step_avg:57.77ms
step:873/2330 train_time:50428ms step_avg:57.76ms
step:874/2330 train_time:50489ms step_avg:57.77ms
step:875/2330 train_time:50545ms step_avg:57.77ms
step:876/2330 train_time:50606ms step_avg:57.77ms
step:877/2330 train_time:50662ms step_avg:57.77ms
step:878/2330 train_time:50724ms step_avg:57.77ms
step:879/2330 train_time:50780ms step_avg:57.77ms
step:880/2330 train_time:50841ms step_avg:57.77ms
step:881/2330 train_time:50898ms step_avg:57.77ms
step:882/2330 train_time:50959ms step_avg:57.78ms
step:883/2330 train_time:51016ms step_avg:57.78ms
step:884/2330 train_time:51075ms step_avg:57.78ms
step:885/2330 train_time:51132ms step_avg:57.78ms
step:886/2330 train_time:51192ms step_avg:57.78ms
step:887/2330 train_time:51249ms step_avg:57.78ms
step:888/2330 train_time:51309ms step_avg:57.78ms
step:889/2330 train_time:51365ms step_avg:57.78ms
step:890/2330 train_time:51426ms step_avg:57.78ms
step:891/2330 train_time:51482ms step_avg:57.78ms
step:892/2330 train_time:51542ms step_avg:57.78ms
step:893/2330 train_time:51599ms step_avg:57.78ms
step:894/2330 train_time:51660ms step_avg:57.79ms
step:895/2330 train_time:51716ms step_avg:57.78ms
step:896/2330 train_time:51776ms step_avg:57.79ms
step:897/2330 train_time:51833ms step_avg:57.79ms
step:898/2330 train_time:51893ms step_avg:57.79ms
step:899/2330 train_time:51949ms step_avg:57.79ms
step:900/2330 train_time:52008ms step_avg:57.79ms
step:901/2330 train_time:52066ms step_avg:57.79ms
step:902/2330 train_time:52126ms step_avg:57.79ms
step:903/2330 train_time:52182ms step_avg:57.79ms
step:904/2330 train_time:52243ms step_avg:57.79ms
step:905/2330 train_time:52299ms step_avg:57.79ms
step:906/2330 train_time:52360ms step_avg:57.79ms
step:907/2330 train_time:52418ms step_avg:57.79ms
step:908/2330 train_time:52477ms step_avg:57.79ms
step:909/2330 train_time:52534ms step_avg:57.79ms
step:910/2330 train_time:52594ms step_avg:57.80ms
step:911/2330 train_time:52649ms step_avg:57.79ms
step:912/2330 train_time:52710ms step_avg:57.80ms
step:913/2330 train_time:52767ms step_avg:57.80ms
step:914/2330 train_time:52827ms step_avg:57.80ms
step:915/2330 train_time:52883ms step_avg:57.80ms
step:916/2330 train_time:52943ms step_avg:57.80ms
step:917/2330 train_time:53000ms step_avg:57.80ms
step:918/2330 train_time:53060ms step_avg:57.80ms
step:919/2330 train_time:53117ms step_avg:57.80ms
step:920/2330 train_time:53178ms step_avg:57.80ms
step:921/2330 train_time:53235ms step_avg:57.80ms
step:922/2330 train_time:53295ms step_avg:57.80ms
step:923/2330 train_time:53352ms step_avg:57.80ms
step:924/2330 train_time:53412ms step_avg:57.80ms
step:925/2330 train_time:53468ms step_avg:57.80ms
step:926/2330 train_time:53529ms step_avg:57.81ms
step:927/2330 train_time:53585ms step_avg:57.81ms
step:928/2330 train_time:53646ms step_avg:57.81ms
step:929/2330 train_time:53702ms step_avg:57.81ms
step:930/2330 train_time:53763ms step_avg:57.81ms
step:931/2330 train_time:53819ms step_avg:57.81ms
step:932/2330 train_time:53880ms step_avg:57.81ms
step:933/2330 train_time:53936ms step_avg:57.81ms
step:934/2330 train_time:53996ms step_avg:57.81ms
step:935/2330 train_time:54053ms step_avg:57.81ms
step:936/2330 train_time:54113ms step_avg:57.81ms
step:937/2330 train_time:54170ms step_avg:57.81ms
step:938/2330 train_time:54229ms step_avg:57.81ms
step:939/2330 train_time:54286ms step_avg:57.81ms
step:940/2330 train_time:54347ms step_avg:57.82ms
step:941/2330 train_time:54405ms step_avg:57.82ms
step:942/2330 train_time:54464ms step_avg:57.82ms
step:943/2330 train_time:54520ms step_avg:57.82ms
step:944/2330 train_time:54582ms step_avg:57.82ms
step:945/2330 train_time:54638ms step_avg:57.82ms
step:946/2330 train_time:54698ms step_avg:57.82ms
step:947/2330 train_time:54755ms step_avg:57.82ms
step:948/2330 train_time:54815ms step_avg:57.82ms
step:949/2330 train_time:54872ms step_avg:57.82ms
step:950/2330 train_time:54932ms step_avg:57.82ms
step:951/2330 train_time:54988ms step_avg:57.82ms
step:952/2330 train_time:55048ms step_avg:57.82ms
step:953/2330 train_time:55105ms step_avg:57.82ms
step:954/2330 train_time:55166ms step_avg:57.83ms
step:955/2330 train_time:55222ms step_avg:57.82ms
step:956/2330 train_time:55284ms step_avg:57.83ms
step:957/2330 train_time:55340ms step_avg:57.83ms
step:958/2330 train_time:55401ms step_avg:57.83ms
step:959/2330 train_time:55458ms step_avg:57.83ms
step:960/2330 train_time:55518ms step_avg:57.83ms
step:961/2330 train_time:55575ms step_avg:57.83ms
step:962/2330 train_time:55636ms step_avg:57.83ms
step:963/2330 train_time:55692ms step_avg:57.83ms
step:964/2330 train_time:55752ms step_avg:57.83ms
step:965/2330 train_time:55809ms step_avg:57.83ms
step:966/2330 train_time:55868ms step_avg:57.83ms
step:967/2330 train_time:55925ms step_avg:57.83ms
step:968/2330 train_time:55985ms step_avg:57.84ms
step:969/2330 train_time:56042ms step_avg:57.83ms
step:970/2330 train_time:56103ms step_avg:57.84ms
step:971/2330 train_time:56159ms step_avg:57.84ms
step:972/2330 train_time:56221ms step_avg:57.84ms
step:973/2330 train_time:56278ms step_avg:57.84ms
step:974/2330 train_time:56338ms step_avg:57.84ms
step:975/2330 train_time:56395ms step_avg:57.84ms
step:976/2330 train_time:56455ms step_avg:57.84ms
step:977/2330 train_time:56512ms step_avg:57.84ms
step:978/2330 train_time:56571ms step_avg:57.84ms
step:979/2330 train_time:56627ms step_avg:57.84ms
step:980/2330 train_time:56689ms step_avg:57.85ms
step:981/2330 train_time:56745ms step_avg:57.84ms
step:982/2330 train_time:56806ms step_avg:57.85ms
step:983/2330 train_time:56863ms step_avg:57.85ms
step:984/2330 train_time:56923ms step_avg:57.85ms
step:985/2330 train_time:56980ms step_avg:57.85ms
step:986/2330 train_time:57041ms step_avg:57.85ms
step:987/2330 train_time:57097ms step_avg:57.85ms
step:988/2330 train_time:57159ms step_avg:57.85ms
step:989/2330 train_time:57216ms step_avg:57.85ms
step:990/2330 train_time:57275ms step_avg:57.85ms
step:991/2330 train_time:57332ms step_avg:57.85ms
step:992/2330 train_time:57392ms step_avg:57.85ms
step:993/2330 train_time:57448ms step_avg:57.85ms
step:994/2330 train_time:57508ms step_avg:57.86ms
step:995/2330 train_time:57565ms step_avg:57.85ms
step:996/2330 train_time:57627ms step_avg:57.86ms
step:997/2330 train_time:57683ms step_avg:57.86ms
step:998/2330 train_time:57744ms step_avg:57.86ms
step:999/2330 train_time:57801ms step_avg:57.86ms
step:1000/2330 train_time:57861ms step_avg:57.86ms
step:1000/2330 val_loss:4.0826 train_time:57941ms step_avg:57.94ms
step:1001/2330 train_time:57961ms step_avg:57.90ms
step:1002/2330 train_time:57982ms step_avg:57.87ms
step:1003/2330 train_time:58036ms step_avg:57.86ms
step:1004/2330 train_time:58101ms step_avg:57.87ms
step:1005/2330 train_time:58157ms step_avg:57.87ms
step:1006/2330 train_time:58220ms step_avg:57.87ms
step:1007/2330 train_time:58277ms step_avg:57.87ms
step:1008/2330 train_time:58336ms step_avg:57.87ms
step:1009/2330 train_time:58392ms step_avg:57.87ms
step:1010/2330 train_time:58451ms step_avg:57.87ms
step:1011/2330 train_time:58507ms step_avg:57.87ms
step:1012/2330 train_time:58566ms step_avg:57.87ms
step:1013/2330 train_time:58622ms step_avg:57.87ms
step:1014/2330 train_time:58681ms step_avg:57.87ms
step:1015/2330 train_time:58737ms step_avg:57.87ms
step:1016/2330 train_time:58796ms step_avg:57.87ms
step:1017/2330 train_time:58853ms step_avg:57.87ms
step:1018/2330 train_time:58918ms step_avg:57.88ms
step:1019/2330 train_time:58975ms step_avg:57.88ms
step:1020/2330 train_time:59038ms step_avg:57.88ms
step:1021/2330 train_time:59096ms step_avg:57.88ms
step:1022/2330 train_time:59157ms step_avg:57.88ms
step:1023/2330 train_time:59214ms step_avg:57.88ms
step:1024/2330 train_time:59275ms step_avg:57.89ms
step:1025/2330 train_time:59332ms step_avg:57.88ms
step:1026/2330 train_time:59392ms step_avg:57.89ms
step:1027/2330 train_time:59448ms step_avg:57.89ms
step:1028/2330 train_time:59508ms step_avg:57.89ms
step:1029/2330 train_time:59565ms step_avg:57.89ms
step:1030/2330 train_time:59624ms step_avg:57.89ms
step:1031/2330 train_time:59681ms step_avg:57.89ms
step:1032/2330 train_time:59740ms step_avg:57.89ms
step:1033/2330 train_time:59796ms step_avg:57.89ms
step:1034/2330 train_time:59857ms step_avg:57.89ms
step:1035/2330 train_time:59914ms step_avg:57.89ms
step:1036/2330 train_time:59977ms step_avg:57.89ms
step:1037/2330 train_time:60034ms step_avg:57.89ms
step:1038/2330 train_time:60096ms step_avg:57.90ms
step:1039/2330 train_time:60153ms step_avg:57.90ms
step:1040/2330 train_time:60215ms step_avg:57.90ms
step:1041/2330 train_time:60272ms step_avg:57.90ms
step:1042/2330 train_time:60332ms step_avg:57.90ms
step:1043/2330 train_time:60388ms step_avg:57.90ms
step:1044/2330 train_time:60448ms step_avg:57.90ms
step:1045/2330 train_time:60504ms step_avg:57.90ms
step:1046/2330 train_time:60564ms step_avg:57.90ms
step:1047/2330 train_time:60620ms step_avg:57.90ms
step:1048/2330 train_time:60679ms step_avg:57.90ms
step:1049/2330 train_time:60736ms step_avg:57.90ms
step:1050/2330 train_time:60796ms step_avg:57.90ms
step:1051/2330 train_time:60853ms step_avg:57.90ms
step:1052/2330 train_time:60913ms step_avg:57.90ms
step:1053/2330 train_time:60970ms step_avg:57.90ms
step:1054/2330 train_time:61031ms step_avg:57.90ms
step:1055/2330 train_time:61088ms step_avg:57.90ms
step:1056/2330 train_time:61148ms step_avg:57.91ms
step:1057/2330 train_time:61206ms step_avg:57.91ms
step:1058/2330 train_time:61265ms step_avg:57.91ms
step:1059/2330 train_time:61322ms step_avg:57.91ms
step:1060/2330 train_time:61382ms step_avg:57.91ms
step:1061/2330 train_time:61438ms step_avg:57.91ms
step:1062/2330 train_time:61500ms step_avg:57.91ms
step:1063/2330 train_time:61556ms step_avg:57.91ms
step:1064/2330 train_time:61616ms step_avg:57.91ms
step:1065/2330 train_time:61672ms step_avg:57.91ms
step:1066/2330 train_time:61732ms step_avg:57.91ms
step:1067/2330 train_time:61789ms step_avg:57.91ms
step:1068/2330 train_time:61849ms step_avg:57.91ms
step:1069/2330 train_time:61907ms step_avg:57.91ms
step:1070/2330 train_time:61967ms step_avg:57.91ms
step:1071/2330 train_time:62024ms step_avg:57.91ms
step:1072/2330 train_time:62083ms step_avg:57.91ms
step:1073/2330 train_time:62140ms step_avg:57.91ms
step:1074/2330 train_time:62202ms step_avg:57.92ms
step:1075/2330 train_time:62259ms step_avg:57.92ms
step:1076/2330 train_time:62319ms step_avg:57.92ms
step:1077/2330 train_time:62376ms step_avg:57.92ms
step:1078/2330 train_time:62436ms step_avg:57.92ms
step:1079/2330 train_time:62492ms step_avg:57.92ms
step:1080/2330 train_time:62552ms step_avg:57.92ms
step:1081/2330 train_time:62609ms step_avg:57.92ms
step:1082/2330 train_time:62669ms step_avg:57.92ms
step:1083/2330 train_time:62726ms step_avg:57.92ms
step:1084/2330 train_time:62785ms step_avg:57.92ms
step:1085/2330 train_time:62841ms step_avg:57.92ms
step:1086/2330 train_time:62902ms step_avg:57.92ms
step:1087/2330 train_time:62959ms step_avg:57.92ms
step:1088/2330 train_time:63019ms step_avg:57.92ms
step:1089/2330 train_time:63077ms step_avg:57.92ms
step:1090/2330 train_time:63137ms step_avg:57.92ms
step:1091/2330 train_time:63194ms step_avg:57.92ms
step:1092/2330 train_time:63255ms step_avg:57.93ms
step:1093/2330 train_time:63312ms step_avg:57.93ms
step:1094/2330 train_time:63372ms step_avg:57.93ms
step:1095/2330 train_time:63429ms step_avg:57.93ms
step:1096/2330 train_time:63490ms step_avg:57.93ms
step:1097/2330 train_time:63547ms step_avg:57.93ms
step:1098/2330 train_time:63606ms step_avg:57.93ms
step:1099/2330 train_time:63663ms step_avg:57.93ms
step:1100/2330 train_time:63723ms step_avg:57.93ms
step:1101/2330 train_time:63780ms step_avg:57.93ms
step:1102/2330 train_time:63839ms step_avg:57.93ms
step:1103/2330 train_time:63897ms step_avg:57.93ms
step:1104/2330 train_time:63958ms step_avg:57.93ms
step:1105/2330 train_time:64015ms step_avg:57.93ms
step:1106/2330 train_time:64075ms step_avg:57.93ms
step:1107/2330 train_time:64132ms step_avg:57.93ms
step:1108/2330 train_time:64192ms step_avg:57.93ms
step:1109/2330 train_time:64249ms step_avg:57.93ms
step:1110/2330 train_time:64309ms step_avg:57.94ms
step:1111/2330 train_time:64366ms step_avg:57.94ms
step:1112/2330 train_time:64425ms step_avg:57.94ms
step:1113/2330 train_time:64482ms step_avg:57.94ms
step:1114/2330 train_time:64542ms step_avg:57.94ms
step:1115/2330 train_time:64598ms step_avg:57.94ms
step:1116/2330 train_time:64659ms step_avg:57.94ms
step:1117/2330 train_time:64715ms step_avg:57.94ms
step:1118/2330 train_time:64776ms step_avg:57.94ms
step:1119/2330 train_time:64833ms step_avg:57.94ms
step:1120/2330 train_time:64894ms step_avg:57.94ms
step:1121/2330 train_time:64952ms step_avg:57.94ms
step:1122/2330 train_time:65012ms step_avg:57.94ms
step:1123/2330 train_time:65069ms step_avg:57.94ms
step:1124/2330 train_time:65129ms step_avg:57.94ms
step:1125/2330 train_time:65186ms step_avg:57.94ms
step:1126/2330 train_time:65246ms step_avg:57.94ms
step:1127/2330 train_time:65304ms step_avg:57.94ms
step:1128/2330 train_time:65363ms step_avg:57.95ms
step:1129/2330 train_time:65419ms step_avg:57.94ms
step:1130/2330 train_time:65479ms step_avg:57.95ms
step:1131/2330 train_time:65535ms step_avg:57.94ms
step:1132/2330 train_time:65596ms step_avg:57.95ms
step:1133/2330 train_time:65653ms step_avg:57.95ms
step:1134/2330 train_time:65713ms step_avg:57.95ms
step:1135/2330 train_time:65770ms step_avg:57.95ms
step:1136/2330 train_time:65830ms step_avg:57.95ms
step:1137/2330 train_time:65888ms step_avg:57.95ms
step:1138/2330 train_time:65948ms step_avg:57.95ms
step:1139/2330 train_time:66004ms step_avg:57.95ms
step:1140/2330 train_time:66064ms step_avg:57.95ms
step:1141/2330 train_time:66121ms step_avg:57.95ms
step:1142/2330 train_time:66183ms step_avg:57.95ms
step:1143/2330 train_time:66239ms step_avg:57.95ms
step:1144/2330 train_time:66300ms step_avg:57.95ms
step:1145/2330 train_time:66357ms step_avg:57.95ms
step:1146/2330 train_time:66782ms step_avg:58.27ms
step:1147/2330 train_time:66837ms step_avg:58.27ms
step:1148/2330 train_time:66896ms step_avg:58.27ms
step:1149/2330 train_time:66952ms step_avg:58.27ms
step:1150/2330 train_time:67011ms step_avg:58.27ms
step:1151/2330 train_time:67067ms step_avg:58.27ms
step:1152/2330 train_time:67126ms step_avg:58.27ms
step:1153/2330 train_time:67183ms step_avg:58.27ms
step:1154/2330 train_time:67242ms step_avg:58.27ms
step:1155/2330 train_time:67297ms step_avg:58.27ms
step:1156/2330 train_time:67357ms step_avg:58.27ms
step:1157/2330 train_time:67413ms step_avg:58.27ms
step:1158/2330 train_time:67473ms step_avg:58.27ms
step:1159/2330 train_time:67529ms step_avg:58.26ms
step:1160/2330 train_time:67588ms step_avg:58.27ms
step:1161/2330 train_time:67648ms step_avg:58.27ms
step:1162/2330 train_time:67714ms step_avg:58.27ms
step:1163/2330 train_time:67773ms step_avg:58.27ms
step:1164/2330 train_time:67834ms step_avg:58.28ms
step:1165/2330 train_time:67891ms step_avg:58.28ms
step:1166/2330 train_time:67951ms step_avg:58.28ms
step:1167/2330 train_time:68007ms step_avg:58.28ms
step:1168/2330 train_time:68066ms step_avg:58.28ms
step:1169/2330 train_time:68122ms step_avg:58.27ms
step:1170/2330 train_time:68181ms step_avg:58.27ms
step:1171/2330 train_time:68237ms step_avg:58.27ms
step:1172/2330 train_time:68297ms step_avg:58.27ms
step:1173/2330 train_time:68353ms step_avg:58.27ms
step:1174/2330 train_time:68413ms step_avg:58.27ms
step:1175/2330 train_time:68469ms step_avg:58.27ms
step:1176/2330 train_time:68528ms step_avg:58.27ms
step:1177/2330 train_time:68586ms step_avg:58.27ms
step:1178/2330 train_time:68647ms step_avg:58.27ms
step:1179/2330 train_time:68705ms step_avg:58.27ms
step:1180/2330 train_time:68766ms step_avg:58.28ms
step:1181/2330 train_time:68823ms step_avg:58.28ms
step:1182/2330 train_time:68885ms step_avg:58.28ms
step:1183/2330 train_time:68941ms step_avg:58.28ms
step:1184/2330 train_time:69003ms step_avg:58.28ms
step:1185/2330 train_time:69059ms step_avg:58.28ms
step:1186/2330 train_time:69119ms step_avg:58.28ms
step:1187/2330 train_time:69175ms step_avg:58.28ms
step:1188/2330 train_time:69236ms step_avg:58.28ms
step:1189/2330 train_time:69292ms step_avg:58.28ms
step:1190/2330 train_time:69352ms step_avg:58.28ms
step:1191/2330 train_time:69408ms step_avg:58.28ms
step:1192/2330 train_time:69467ms step_avg:58.28ms
step:1193/2330 train_time:69524ms step_avg:58.28ms
step:1194/2330 train_time:69584ms step_avg:58.28ms
step:1195/2330 train_time:69641ms step_avg:58.28ms
step:1196/2330 train_time:69702ms step_avg:58.28ms
step:1197/2330 train_time:69760ms step_avg:58.28ms
step:1198/2330 train_time:69820ms step_avg:58.28ms
step:1199/2330 train_time:69877ms step_avg:58.28ms
step:1200/2330 train_time:69939ms step_avg:58.28ms
step:1201/2330 train_time:69996ms step_avg:58.28ms
step:1202/2330 train_time:70057ms step_avg:58.28ms
step:1203/2330 train_time:70113ms step_avg:58.28ms
step:1204/2330 train_time:70173ms step_avg:58.28ms
step:1205/2330 train_time:70229ms step_avg:58.28ms
step:1206/2330 train_time:70289ms step_avg:58.28ms
step:1207/2330 train_time:70345ms step_avg:58.28ms
step:1208/2330 train_time:70405ms step_avg:58.28ms
step:1209/2330 train_time:70461ms step_avg:58.28ms
step:1210/2330 train_time:70521ms step_avg:58.28ms
step:1211/2330 train_time:70578ms step_avg:58.28ms
step:1212/2330 train_time:70639ms step_avg:58.28ms
step:1213/2330 train_time:70695ms step_avg:58.28ms
step:1214/2330 train_time:70757ms step_avg:58.28ms
step:1215/2330 train_time:70814ms step_avg:58.28ms
step:1216/2330 train_time:70875ms step_avg:58.29ms
step:1217/2330 train_time:70933ms step_avg:58.28ms
step:1218/2330 train_time:70993ms step_avg:58.29ms
step:1219/2330 train_time:71050ms step_avg:58.29ms
step:1220/2330 train_time:71109ms step_avg:58.29ms
step:1221/2330 train_time:71167ms step_avg:58.29ms
step:1222/2330 train_time:71227ms step_avg:58.29ms
step:1223/2330 train_time:71283ms step_avg:58.29ms
step:1224/2330 train_time:71342ms step_avg:58.29ms
step:1225/2330 train_time:71399ms step_avg:58.28ms
step:1226/2330 train_time:71459ms step_avg:58.29ms
step:1227/2330 train_time:71515ms step_avg:58.28ms
step:1228/2330 train_time:71576ms step_avg:58.29ms
step:1229/2330 train_time:71632ms step_avg:58.28ms
step:1230/2330 train_time:71693ms step_avg:58.29ms
step:1231/2330 train_time:71750ms step_avg:58.29ms
step:1232/2330 train_time:71811ms step_avg:58.29ms
step:1233/2330 train_time:71869ms step_avg:58.29ms
step:1234/2330 train_time:71929ms step_avg:58.29ms
step:1235/2330 train_time:71986ms step_avg:58.29ms
step:1236/2330 train_time:72046ms step_avg:58.29ms
step:1237/2330 train_time:72103ms step_avg:58.29ms
step:1238/2330 train_time:72163ms step_avg:58.29ms
step:1239/2330 train_time:72219ms step_avg:58.29ms
step:1240/2330 train_time:72280ms step_avg:58.29ms
step:1241/2330 train_time:72335ms step_avg:58.29ms
step:1242/2330 train_time:72396ms step_avg:58.29ms
step:1243/2330 train_time:72452ms step_avg:58.29ms
step:1244/2330 train_time:72513ms step_avg:58.29ms
step:1245/2330 train_time:72569ms step_avg:58.29ms
step:1246/2330 train_time:72630ms step_avg:58.29ms
step:1247/2330 train_time:72686ms step_avg:58.29ms
step:1248/2330 train_time:72746ms step_avg:58.29ms
step:1249/2330 train_time:72804ms step_avg:58.29ms
step:1250/2330 train_time:72864ms step_avg:58.29ms
step:1250/2330 val_loss:4.0021 train_time:72945ms step_avg:58.36ms
step:1251/2330 train_time:72964ms step_avg:58.32ms
step:1252/2330 train_time:72983ms step_avg:58.29ms
step:1253/2330 train_time:73042ms step_avg:58.29ms
step:1254/2330 train_time:73111ms step_avg:58.30ms
step:1255/2330 train_time:73166ms step_avg:58.30ms
step:1256/2330 train_time:73229ms step_avg:58.30ms
step:1257/2330 train_time:73285ms step_avg:58.30ms
step:1258/2330 train_time:73345ms step_avg:58.30ms
step:1259/2330 train_time:73401ms step_avg:58.30ms
step:1260/2330 train_time:73461ms step_avg:58.30ms
step:1261/2330 train_time:73518ms step_avg:58.30ms
step:1262/2330 train_time:73577ms step_avg:58.30ms
step:1263/2330 train_time:73633ms step_avg:58.30ms
step:1264/2330 train_time:73693ms step_avg:58.30ms
step:1265/2330 train_time:73749ms step_avg:58.30ms
step:1266/2330 train_time:73808ms step_avg:58.30ms
step:1267/2330 train_time:73864ms step_avg:58.30ms
step:1268/2330 train_time:73926ms step_avg:58.30ms
step:1269/2330 train_time:73984ms step_avg:58.30ms
step:1270/2330 train_time:74048ms step_avg:58.31ms
step:1271/2330 train_time:74105ms step_avg:58.30ms
step:1272/2330 train_time:74168ms step_avg:58.31ms
step:1273/2330 train_time:74224ms step_avg:58.31ms
step:1274/2330 train_time:74286ms step_avg:58.31ms
step:1275/2330 train_time:74342ms step_avg:58.31ms
step:1276/2330 train_time:74404ms step_avg:58.31ms
step:1277/2330 train_time:74459ms step_avg:58.31ms
step:1278/2330 train_time:74520ms step_avg:58.31ms
step:1279/2330 train_time:74576ms step_avg:58.31ms
step:1280/2330 train_time:74636ms step_avg:58.31ms
step:1281/2330 train_time:74692ms step_avg:58.31ms
step:1282/2330 train_time:74752ms step_avg:58.31ms
step:1283/2330 train_time:74808ms step_avg:58.31ms
step:1284/2330 train_time:74867ms step_avg:58.31ms
step:1285/2330 train_time:74924ms step_avg:58.31ms
step:1286/2330 train_time:74986ms step_avg:58.31ms
step:1287/2330 train_time:75043ms step_avg:58.31ms
step:1288/2330 train_time:75105ms step_avg:58.31ms
step:1289/2330 train_time:75162ms step_avg:58.31ms
step:1290/2330 train_time:75225ms step_avg:58.31ms
step:1291/2330 train_time:75281ms step_avg:58.31ms
step:1292/2330 train_time:75342ms step_avg:58.31ms
step:1293/2330 train_time:75399ms step_avg:58.31ms
step:1294/2330 train_time:75459ms step_avg:58.31ms
step:1295/2330 train_time:75515ms step_avg:58.31ms
step:1296/2330 train_time:75574ms step_avg:58.31ms
step:1297/2330 train_time:75630ms step_avg:58.31ms
step:1298/2330 train_time:75690ms step_avg:58.31ms
step:1299/2330 train_time:75746ms step_avg:58.31ms
step:1300/2330 train_time:75807ms step_avg:58.31ms
step:1301/2330 train_time:75863ms step_avg:58.31ms
step:1302/2330 train_time:75924ms step_avg:58.31ms
step:1303/2330 train_time:75981ms step_avg:58.31ms
step:1304/2330 train_time:76042ms step_avg:58.31ms
step:1305/2330 train_time:76100ms step_avg:58.31ms
step:1306/2330 train_time:76160ms step_avg:58.32ms
step:1307/2330 train_time:76217ms step_avg:58.31ms
step:1308/2330 train_time:76277ms step_avg:58.32ms
step:1309/2330 train_time:76334ms step_avg:58.31ms
step:1310/2330 train_time:76394ms step_avg:58.32ms
step:1311/2330 train_time:76451ms step_avg:58.31ms
step:1312/2330 train_time:76511ms step_avg:58.32ms
step:1313/2330 train_time:76566ms step_avg:58.31ms
step:1314/2330 train_time:76628ms step_avg:58.32ms
step:1315/2330 train_time:76684ms step_avg:58.31ms
step:1316/2330 train_time:76745ms step_avg:58.32ms
step:1317/2330 train_time:76801ms step_avg:58.31ms
step:1318/2330 train_time:76861ms step_avg:58.32ms
step:1319/2330 train_time:76918ms step_avg:58.32ms
step:1320/2330 train_time:76979ms step_avg:58.32ms
step:1321/2330 train_time:77036ms step_avg:58.32ms
step:1322/2330 train_time:77096ms step_avg:58.32ms
step:1323/2330 train_time:77154ms step_avg:58.32ms
step:1324/2330 train_time:77213ms step_avg:58.32ms
step:1325/2330 train_time:77270ms step_avg:58.32ms
step:1326/2330 train_time:77332ms step_avg:58.32ms
step:1327/2330 train_time:77388ms step_avg:58.32ms
step:1328/2330 train_time:77449ms step_avg:58.32ms
step:1329/2330 train_time:77505ms step_avg:58.32ms
step:1330/2330 train_time:77565ms step_avg:58.32ms
step:1331/2330 train_time:77621ms step_avg:58.32ms
step:1332/2330 train_time:77681ms step_avg:58.32ms
step:1333/2330 train_time:77738ms step_avg:58.32ms
step:1334/2330 train_time:77798ms step_avg:58.32ms
step:1335/2330 train_time:77854ms step_avg:58.32ms
step:1336/2330 train_time:77914ms step_avg:58.32ms
step:1337/2330 train_time:77970ms step_avg:58.32ms
step:1338/2330 train_time:78031ms step_avg:58.32ms
step:1339/2330 train_time:78088ms step_avg:58.32ms
step:1340/2330 train_time:78150ms step_avg:58.32ms
step:1341/2330 train_time:78207ms step_avg:58.32ms
step:1342/2330 train_time:78268ms step_avg:58.32ms
step:1343/2330 train_time:78324ms step_avg:58.32ms
step:1344/2330 train_time:78385ms step_avg:58.32ms
step:1345/2330 train_time:78442ms step_avg:58.32ms
step:1346/2330 train_time:78502ms step_avg:58.32ms
step:1347/2330 train_time:78559ms step_avg:58.32ms
step:1348/2330 train_time:78618ms step_avg:58.32ms
step:1349/2330 train_time:78674ms step_avg:58.32ms
step:1350/2330 train_time:78735ms step_avg:58.32ms
step:1351/2330 train_time:78791ms step_avg:58.32ms
step:1352/2330 train_time:78851ms step_avg:58.32ms
step:1353/2330 train_time:78908ms step_avg:58.32ms
step:1354/2330 train_time:78968ms step_avg:58.32ms
step:1355/2330 train_time:79025ms step_avg:58.32ms
step:1356/2330 train_time:79086ms step_avg:58.32ms
step:1357/2330 train_time:79143ms step_avg:58.32ms
step:1358/2330 train_time:79204ms step_avg:58.32ms
step:1359/2330 train_time:79261ms step_avg:58.32ms
step:1360/2330 train_time:79321ms step_avg:58.32ms
step:1361/2330 train_time:79378ms step_avg:58.32ms
step:1362/2330 train_time:79438ms step_avg:58.32ms
step:1363/2330 train_time:79494ms step_avg:58.32ms
step:1364/2330 train_time:79554ms step_avg:58.32ms
step:1365/2330 train_time:79610ms step_avg:58.32ms
step:1366/2330 train_time:79670ms step_avg:58.32ms
step:1367/2330 train_time:79727ms step_avg:58.32ms
step:1368/2330 train_time:79788ms step_avg:58.32ms
step:1369/2330 train_time:79844ms step_avg:58.32ms
step:1370/2330 train_time:79906ms step_avg:58.33ms
step:1371/2330 train_time:79962ms step_avg:58.32ms
step:1372/2330 train_time:80023ms step_avg:58.33ms
step:1373/2330 train_time:80080ms step_avg:58.33ms
step:1374/2330 train_time:80141ms step_avg:58.33ms
step:1375/2330 train_time:80197ms step_avg:58.33ms
step:1376/2330 train_time:80259ms step_avg:58.33ms
step:1377/2330 train_time:80316ms step_avg:58.33ms
step:1378/2330 train_time:80376ms step_avg:58.33ms
step:1379/2330 train_time:80433ms step_avg:58.33ms
step:1380/2330 train_time:80493ms step_avg:58.33ms
step:1381/2330 train_time:80550ms step_avg:58.33ms
step:1382/2330 train_time:80610ms step_avg:58.33ms
step:1383/2330 train_time:80666ms step_avg:58.33ms
step:1384/2330 train_time:80727ms step_avg:58.33ms
step:1385/2330 train_time:80783ms step_avg:58.33ms
step:1386/2330 train_time:80843ms step_avg:58.33ms
step:1387/2330 train_time:80900ms step_avg:58.33ms
step:1388/2330 train_time:80961ms step_avg:58.33ms
step:1389/2330 train_time:81017ms step_avg:58.33ms
step:1390/2330 train_time:81077ms step_avg:58.33ms
step:1391/2330 train_time:81134ms step_avg:58.33ms
step:1392/2330 train_time:81194ms step_avg:58.33ms
step:1393/2330 train_time:81251ms step_avg:58.33ms
step:1394/2330 train_time:81312ms step_avg:58.33ms
step:1395/2330 train_time:81367ms step_avg:58.33ms
step:1396/2330 train_time:81429ms step_avg:58.33ms
step:1397/2330 train_time:81486ms step_avg:58.33ms
step:1398/2330 train_time:81547ms step_avg:58.33ms
step:1399/2330 train_time:81604ms step_avg:58.33ms
step:1400/2330 train_time:81664ms step_avg:58.33ms
step:1401/2330 train_time:81721ms step_avg:58.33ms
step:1402/2330 train_time:81781ms step_avg:58.33ms
step:1403/2330 train_time:81838ms step_avg:58.33ms
step:1404/2330 train_time:81898ms step_avg:58.33ms
step:1405/2330 train_time:81955ms step_avg:58.33ms
step:1406/2330 train_time:82014ms step_avg:58.33ms
step:1407/2330 train_time:82070ms step_avg:58.33ms
step:1408/2330 train_time:82130ms step_avg:58.33ms
step:1409/2330 train_time:82187ms step_avg:58.33ms
step:1410/2330 train_time:82248ms step_avg:58.33ms
step:1411/2330 train_time:82305ms step_avg:58.33ms
step:1412/2330 train_time:82365ms step_avg:58.33ms
step:1413/2330 train_time:82422ms step_avg:58.33ms
step:1414/2330 train_time:82482ms step_avg:58.33ms
step:1415/2330 train_time:82539ms step_avg:58.33ms
step:1416/2330 train_time:82599ms step_avg:58.33ms
step:1417/2330 train_time:82656ms step_avg:58.33ms
step:1418/2330 train_time:82716ms step_avg:58.33ms
step:1419/2330 train_time:82773ms step_avg:58.33ms
step:1420/2330 train_time:82832ms step_avg:58.33ms
step:1421/2330 train_time:82888ms step_avg:58.33ms
step:1422/2330 train_time:82949ms step_avg:58.33ms
step:1423/2330 train_time:83006ms step_avg:58.33ms
step:1424/2330 train_time:83067ms step_avg:58.33ms
step:1425/2330 train_time:83123ms step_avg:58.33ms
step:1426/2330 train_time:83184ms step_avg:58.33ms
step:1427/2330 train_time:83242ms step_avg:58.33ms
step:1428/2330 train_time:83302ms step_avg:58.33ms
step:1429/2330 train_time:83359ms step_avg:58.33ms
step:1430/2330 train_time:83420ms step_avg:58.34ms
step:1431/2330 train_time:83477ms step_avg:58.33ms
step:1432/2330 train_time:83536ms step_avg:58.34ms
step:1433/2330 train_time:83593ms step_avg:58.33ms
step:1434/2330 train_time:83653ms step_avg:58.34ms
step:1435/2330 train_time:83710ms step_avg:58.33ms
step:1436/2330 train_time:83770ms step_avg:58.34ms
step:1437/2330 train_time:83826ms step_avg:58.33ms
step:1438/2330 train_time:83888ms step_avg:58.34ms
step:1439/2330 train_time:83944ms step_avg:58.33ms
step:1440/2330 train_time:84006ms step_avg:58.34ms
step:1441/2330 train_time:84062ms step_avg:58.34ms
step:1442/2330 train_time:84123ms step_avg:58.34ms
step:1443/2330 train_time:84179ms step_avg:58.34ms
step:1444/2330 train_time:84241ms step_avg:58.34ms
step:1445/2330 train_time:84298ms step_avg:58.34ms
step:1446/2330 train_time:84357ms step_avg:58.34ms
step:1447/2330 train_time:84414ms step_avg:58.34ms
step:1448/2330 train_time:84474ms step_avg:58.34ms
step:1449/2330 train_time:84530ms step_avg:58.34ms
step:1450/2330 train_time:84591ms step_avg:58.34ms
step:1451/2330 train_time:84647ms step_avg:58.34ms
step:1452/2330 train_time:84708ms step_avg:58.34ms
step:1453/2330 train_time:84764ms step_avg:58.34ms
step:1454/2330 train_time:84825ms step_avg:58.34ms
step:1455/2330 train_time:84881ms step_avg:58.34ms
step:1456/2330 train_time:84942ms step_avg:58.34ms
step:1457/2330 train_time:84999ms step_avg:58.34ms
step:1458/2330 train_time:85059ms step_avg:58.34ms
step:1459/2330 train_time:85115ms step_avg:58.34ms
step:1460/2330 train_time:85175ms step_avg:58.34ms
step:1461/2330 train_time:85232ms step_avg:58.34ms
step:1462/2330 train_time:85292ms step_avg:58.34ms
step:1463/2330 train_time:85348ms step_avg:58.34ms
step:1464/2330 train_time:85410ms step_avg:58.34ms
step:1465/2330 train_time:85466ms step_avg:58.34ms
step:1466/2330 train_time:85527ms step_avg:58.34ms
step:1467/2330 train_time:85584ms step_avg:58.34ms
step:1468/2330 train_time:85645ms step_avg:58.34ms
step:1469/2330 train_time:85702ms step_avg:58.34ms
step:1470/2330 train_time:85762ms step_avg:58.34ms
step:1471/2330 train_time:85819ms step_avg:58.34ms
step:1472/2330 train_time:85879ms step_avg:58.34ms
step:1473/2330 train_time:85936ms step_avg:58.34ms
step:1474/2330 train_time:85996ms step_avg:58.34ms
step:1475/2330 train_time:86053ms step_avg:58.34ms
step:1476/2330 train_time:86113ms step_avg:58.34ms
step:1477/2330 train_time:86169ms step_avg:58.34ms
step:1478/2330 train_time:86230ms step_avg:58.34ms
step:1479/2330 train_time:86287ms step_avg:58.34ms
step:1480/2330 train_time:86349ms step_avg:58.34ms
step:1481/2330 train_time:86405ms step_avg:58.34ms
step:1482/2330 train_time:86466ms step_avg:58.34ms
step:1483/2330 train_time:86522ms step_avg:58.34ms
step:1484/2330 train_time:86584ms step_avg:58.34ms
step:1485/2330 train_time:86640ms step_avg:58.34ms
step:1486/2330 train_time:86701ms step_avg:58.35ms
step:1487/2330 train_time:86758ms step_avg:58.34ms
step:1488/2330 train_time:86819ms step_avg:58.35ms
step:1489/2330 train_time:86875ms step_avg:58.34ms
step:1490/2330 train_time:86936ms step_avg:58.35ms
step:1491/2330 train_time:86992ms step_avg:58.34ms
step:1492/2330 train_time:87052ms step_avg:58.35ms
step:1493/2330 train_time:87109ms step_avg:58.35ms
step:1494/2330 train_time:87169ms step_avg:58.35ms
step:1495/2330 train_time:87226ms step_avg:58.34ms
step:1496/2330 train_time:87287ms step_avg:58.35ms
step:1497/2330 train_time:87343ms step_avg:58.35ms
step:1498/2330 train_time:87404ms step_avg:58.35ms
step:1499/2330 train_time:87461ms step_avg:58.35ms
step:1500/2330 train_time:87522ms step_avg:58.35ms
step:1500/2330 val_loss:3.9170 train_time:87602ms step_avg:58.40ms
step:1501/2330 train_time:87622ms step_avg:58.38ms
step:1502/2330 train_time:87642ms step_avg:58.35ms
step:1503/2330 train_time:87701ms step_avg:58.35ms
step:1504/2330 train_time:87766ms step_avg:58.36ms
step:1505/2330 train_time:87823ms step_avg:58.35ms
step:1506/2330 train_time:87885ms step_avg:58.36ms
step:1507/2330 train_time:87942ms step_avg:58.36ms
step:1508/2330 train_time:88001ms step_avg:58.36ms
step:1509/2330 train_time:88057ms step_avg:58.35ms
step:1510/2330 train_time:88116ms step_avg:58.36ms
step:1511/2330 train_time:88172ms step_avg:58.35ms
step:1512/2330 train_time:88233ms step_avg:58.36ms
step:1513/2330 train_time:88289ms step_avg:58.35ms
step:1514/2330 train_time:88349ms step_avg:58.35ms
step:1515/2330 train_time:88405ms step_avg:58.35ms
step:1516/2330 train_time:88465ms step_avg:58.35ms
step:1517/2330 train_time:88521ms step_avg:58.35ms
step:1518/2330 train_time:88582ms step_avg:58.35ms
step:1519/2330 train_time:88641ms step_avg:58.35ms
step:1520/2330 train_time:88702ms step_avg:58.36ms
step:1521/2330 train_time:88759ms step_avg:58.36ms
step:1522/2330 train_time:88823ms step_avg:58.36ms
step:1523/2330 train_time:88880ms step_avg:58.36ms
step:1524/2330 train_time:88940ms step_avg:58.36ms
step:1525/2330 train_time:88997ms step_avg:58.36ms
step:1526/2330 train_time:89057ms step_avg:58.36ms
step:1527/2330 train_time:89113ms step_avg:58.36ms
step:1528/2330 train_time:89174ms step_avg:58.36ms
step:1529/2330 train_time:89231ms step_avg:58.36ms
step:1530/2330 train_time:89291ms step_avg:58.36ms
step:1531/2330 train_time:89347ms step_avg:58.36ms
step:1532/2330 train_time:89408ms step_avg:58.36ms
step:1533/2330 train_time:89465ms step_avg:58.36ms
step:1534/2330 train_time:89526ms step_avg:58.36ms
step:1535/2330 train_time:89585ms step_avg:58.36ms
step:1536/2330 train_time:89645ms step_avg:58.36ms
step:1537/2330 train_time:89703ms step_avg:58.36ms
step:1538/2330 train_time:89764ms step_avg:58.36ms
step:1539/2330 train_time:89822ms step_avg:58.36ms
step:1540/2330 train_time:89883ms step_avg:58.37ms
step:1541/2330 train_time:89941ms step_avg:58.37ms
step:1542/2330 train_time:90001ms step_avg:58.37ms
step:1543/2330 train_time:90058ms step_avg:58.37ms
step:1544/2330 train_time:90120ms step_avg:58.37ms
step:1545/2330 train_time:90176ms step_avg:58.37ms
step:1546/2330 train_time:90238ms step_avg:58.37ms
step:1547/2330 train_time:90294ms step_avg:58.37ms
step:1548/2330 train_time:90356ms step_avg:58.37ms
step:1549/2330 train_time:90412ms step_avg:58.37ms
step:1550/2330 train_time:90474ms step_avg:58.37ms
step:1551/2330 train_time:90531ms step_avg:58.37ms
step:1552/2330 train_time:90593ms step_avg:58.37ms
step:1553/2330 train_time:90651ms step_avg:58.37ms
step:1554/2330 train_time:90712ms step_avg:58.37ms
step:1555/2330 train_time:90770ms step_avg:58.37ms
step:1556/2330 train_time:90832ms step_avg:58.38ms
step:1557/2330 train_time:90889ms step_avg:58.37ms
step:1558/2330 train_time:90951ms step_avg:58.38ms
step:1559/2330 train_time:91009ms step_avg:58.38ms
step:1560/2330 train_time:91069ms step_avg:58.38ms
step:1561/2330 train_time:91126ms step_avg:58.38ms
step:1562/2330 train_time:91187ms step_avg:58.38ms
step:1563/2330 train_time:91245ms step_avg:58.38ms
step:1564/2330 train_time:91304ms step_avg:58.38ms
step:1565/2330 train_time:91362ms step_avg:58.38ms
step:1566/2330 train_time:91422ms step_avg:58.38ms
step:1567/2330 train_time:91479ms step_avg:58.38ms
step:1568/2330 train_time:91540ms step_avg:58.38ms
step:1569/2330 train_time:91597ms step_avg:58.38ms
step:1570/2330 train_time:91658ms step_avg:58.38ms
step:1571/2330 train_time:91715ms step_avg:58.38ms
step:1572/2330 train_time:91778ms step_avg:58.38ms
step:1573/2330 train_time:91836ms step_avg:58.38ms
step:1574/2330 train_time:91897ms step_avg:58.38ms
step:1575/2330 train_time:91955ms step_avg:58.38ms
step:1576/2330 train_time:92016ms step_avg:58.39ms
step:1577/2330 train_time:92073ms step_avg:58.39ms
step:1578/2330 train_time:92135ms step_avg:58.39ms
step:1579/2330 train_time:92192ms step_avg:58.39ms
step:1580/2330 train_time:92254ms step_avg:58.39ms
step:1581/2330 train_time:92312ms step_avg:58.39ms
step:1582/2330 train_time:92372ms step_avg:58.39ms
step:1583/2330 train_time:92430ms step_avg:58.39ms
step:1584/2330 train_time:92490ms step_avg:58.39ms
step:1585/2330 train_time:92547ms step_avg:58.39ms
step:1586/2330 train_time:92607ms step_avg:58.39ms
step:1587/2330 train_time:92664ms step_avg:58.39ms
step:1588/2330 train_time:92725ms step_avg:58.39ms
step:1589/2330 train_time:92782ms step_avg:58.39ms
step:1590/2330 train_time:92844ms step_avg:58.39ms
step:1591/2330 train_time:92901ms step_avg:58.39ms
step:1592/2330 train_time:92963ms step_avg:58.39ms
step:1593/2330 train_time:93020ms step_avg:58.39ms
step:1594/2330 train_time:93082ms step_avg:58.40ms
step:1595/2330 train_time:93138ms step_avg:58.39ms
step:1596/2330 train_time:93199ms step_avg:58.40ms
step:1597/2330 train_time:93255ms step_avg:58.39ms
step:1598/2330 train_time:93318ms step_avg:58.40ms
step:1599/2330 train_time:93374ms step_avg:58.40ms
step:1600/2330 train_time:93437ms step_avg:58.40ms
step:1601/2330 train_time:93493ms step_avg:58.40ms
step:1602/2330 train_time:93555ms step_avg:58.40ms
step:1603/2330 train_time:93612ms step_avg:58.40ms
step:1604/2330 train_time:93675ms step_avg:58.40ms
step:1605/2330 train_time:93732ms step_avg:58.40ms
step:1606/2330 train_time:93794ms step_avg:58.40ms
step:1607/2330 train_time:93851ms step_avg:58.40ms
step:1608/2330 train_time:93912ms step_avg:58.40ms
step:1609/2330 train_time:93971ms step_avg:58.40ms
step:1610/2330 train_time:94031ms step_avg:58.40ms
step:1611/2330 train_time:94089ms step_avg:58.40ms
step:1612/2330 train_time:94150ms step_avg:58.41ms
step:1613/2330 train_time:94208ms step_avg:58.41ms
step:1614/2330 train_time:94268ms step_avg:58.41ms
step:1615/2330 train_time:94326ms step_avg:58.41ms
step:1616/2330 train_time:94386ms step_avg:58.41ms
step:1617/2330 train_time:94443ms step_avg:58.41ms
step:1618/2330 train_time:94503ms step_avg:58.41ms
step:1619/2330 train_time:94560ms step_avg:58.41ms
step:1620/2330 train_time:94621ms step_avg:58.41ms
step:1621/2330 train_time:94678ms step_avg:58.41ms
step:1622/2330 train_time:94740ms step_avg:58.41ms
step:1623/2330 train_time:94796ms step_avg:58.41ms
step:1624/2330 train_time:94859ms step_avg:58.41ms
step:1625/2330 train_time:94917ms step_avg:58.41ms
step:1626/2330 train_time:94978ms step_avg:58.41ms
step:1627/2330 train_time:95035ms step_avg:58.41ms
step:1628/2330 train_time:95098ms step_avg:58.41ms
step:1629/2330 train_time:95155ms step_avg:58.41ms
step:1630/2330 train_time:95217ms step_avg:58.42ms
step:1631/2330 train_time:95273ms step_avg:58.41ms
step:1632/2330 train_time:95335ms step_avg:58.42ms
step:1633/2330 train_time:95392ms step_avg:58.42ms
step:1634/2330 train_time:95453ms step_avg:58.42ms
step:1635/2330 train_time:95511ms step_avg:58.42ms
step:1636/2330 train_time:95572ms step_avg:58.42ms
step:1637/2330 train_time:95630ms step_avg:58.42ms
step:1638/2330 train_time:95690ms step_avg:58.42ms
step:1639/2330 train_time:95747ms step_avg:58.42ms
step:1640/2330 train_time:95808ms step_avg:58.42ms
step:1641/2330 train_time:95866ms step_avg:58.42ms
step:1642/2330 train_time:95927ms step_avg:58.42ms
step:1643/2330 train_time:95984ms step_avg:58.42ms
step:1644/2330 train_time:96045ms step_avg:58.42ms
step:1645/2330 train_time:96103ms step_avg:58.42ms
step:1646/2330 train_time:96163ms step_avg:58.42ms
step:1647/2330 train_time:96220ms step_avg:58.42ms
step:1648/2330 train_time:96281ms step_avg:58.42ms
step:1649/2330 train_time:96339ms step_avg:58.42ms
step:1650/2330 train_time:96399ms step_avg:58.42ms
step:1651/2330 train_time:96455ms step_avg:58.42ms
step:1652/2330 train_time:96519ms step_avg:58.43ms
step:1653/2330 train_time:96575ms step_avg:58.42ms
step:1654/2330 train_time:96639ms step_avg:58.43ms
step:1655/2330 train_time:96695ms step_avg:58.43ms
step:1656/2330 train_time:96758ms step_avg:58.43ms
step:1657/2330 train_time:96814ms step_avg:58.43ms
step:1658/2330 train_time:96877ms step_avg:58.43ms
step:1659/2330 train_time:96934ms step_avg:58.43ms
step:1660/2330 train_time:96995ms step_avg:58.43ms
step:1661/2330 train_time:97052ms step_avg:58.43ms
step:1662/2330 train_time:97114ms step_avg:58.43ms
step:1663/2330 train_time:97171ms step_avg:58.43ms
step:1664/2330 train_time:97232ms step_avg:58.43ms
step:1665/2330 train_time:97290ms step_avg:58.43ms
step:1666/2330 train_time:97350ms step_avg:58.43ms
step:1667/2330 train_time:97407ms step_avg:58.43ms
step:1668/2330 train_time:97469ms step_avg:58.43ms
step:1669/2330 train_time:97526ms step_avg:58.43ms
step:1670/2330 train_time:97587ms step_avg:58.44ms
step:1671/2330 train_time:97645ms step_avg:58.43ms
step:1672/2330 train_time:97705ms step_avg:58.44ms
step:1673/2330 train_time:97764ms step_avg:58.44ms
step:1674/2330 train_time:97824ms step_avg:58.44ms
step:1675/2330 train_time:97881ms step_avg:58.44ms
step:1676/2330 train_time:97942ms step_avg:58.44ms
step:1677/2330 train_time:97998ms step_avg:58.44ms
step:1678/2330 train_time:98060ms step_avg:58.44ms
step:1679/2330 train_time:98117ms step_avg:58.44ms
step:1680/2330 train_time:98179ms step_avg:58.44ms
step:1681/2330 train_time:98236ms step_avg:58.44ms
step:1682/2330 train_time:98297ms step_avg:58.44ms
step:1683/2330 train_time:98354ms step_avg:58.44ms
step:1684/2330 train_time:98416ms step_avg:58.44ms
step:1685/2330 train_time:98473ms step_avg:58.44ms
step:1686/2330 train_time:98535ms step_avg:58.44ms
step:1687/2330 train_time:98592ms step_avg:58.44ms
step:1688/2330 train_time:98654ms step_avg:58.44ms
step:1689/2330 train_time:98711ms step_avg:58.44ms
step:1690/2330 train_time:98773ms step_avg:58.45ms
step:1691/2330 train_time:98832ms step_avg:58.45ms
step:1692/2330 train_time:98892ms step_avg:58.45ms
step:1693/2330 train_time:98950ms step_avg:58.45ms
step:1694/2330 train_time:99010ms step_avg:58.45ms
step:1695/2330 train_time:99068ms step_avg:58.45ms
step:1696/2330 train_time:99128ms step_avg:58.45ms
step:1697/2330 train_time:99186ms step_avg:58.45ms
step:1698/2330 train_time:99247ms step_avg:58.45ms
step:1699/2330 train_time:99304ms step_avg:58.45ms
step:1700/2330 train_time:99364ms step_avg:58.45ms
step:1701/2330 train_time:99421ms step_avg:58.45ms
step:1702/2330 train_time:99483ms step_avg:58.45ms
step:1703/2330 train_time:99540ms step_avg:58.45ms
step:1704/2330 train_time:99601ms step_avg:58.45ms
step:1705/2330 train_time:99658ms step_avg:58.45ms
step:1706/2330 train_time:99721ms step_avg:58.45ms
step:1707/2330 train_time:99777ms step_avg:58.45ms
step:1708/2330 train_time:99839ms step_avg:58.45ms
step:1709/2330 train_time:99896ms step_avg:58.45ms
step:1710/2330 train_time:99958ms step_avg:58.46ms
step:1711/2330 train_time:100015ms step_avg:58.45ms
step:1712/2330 train_time:100077ms step_avg:58.46ms
step:1713/2330 train_time:100134ms step_avg:58.46ms
step:1714/2330 train_time:100196ms step_avg:58.46ms
step:1715/2330 train_time:100252ms step_avg:58.46ms
step:1716/2330 train_time:100315ms step_avg:58.46ms
step:1717/2330 train_time:100373ms step_avg:58.46ms
step:1718/2330 train_time:100434ms step_avg:58.46ms
step:1719/2330 train_time:100492ms step_avg:58.46ms
step:1720/2330 train_time:100552ms step_avg:58.46ms
step:1721/2330 train_time:100610ms step_avg:58.46ms
step:1722/2330 train_time:100671ms step_avg:58.46ms
step:1723/2330 train_time:100729ms step_avg:58.46ms
step:1724/2330 train_time:100790ms step_avg:58.46ms
step:1725/2330 train_time:100847ms step_avg:58.46ms
step:1726/2330 train_time:100908ms step_avg:58.46ms
step:1727/2330 train_time:100966ms step_avg:58.46ms
step:1728/2330 train_time:101026ms step_avg:58.46ms
step:1729/2330 train_time:101084ms step_avg:58.46ms
step:1730/2330 train_time:101144ms step_avg:58.46ms
step:1731/2330 train_time:101201ms step_avg:58.46ms
step:1732/2330 train_time:101261ms step_avg:58.47ms
step:1733/2330 train_time:101318ms step_avg:58.46ms
step:1734/2330 train_time:101380ms step_avg:58.47ms
step:1735/2330 train_time:101438ms step_avg:58.47ms
step:1736/2330 train_time:101498ms step_avg:58.47ms
step:1737/2330 train_time:101555ms step_avg:58.47ms
step:1738/2330 train_time:101617ms step_avg:58.47ms
step:1739/2330 train_time:101674ms step_avg:58.47ms
step:1740/2330 train_time:101736ms step_avg:58.47ms
step:1741/2330 train_time:101793ms step_avg:58.47ms
step:1742/2330 train_time:101854ms step_avg:58.47ms
step:1743/2330 train_time:101910ms step_avg:58.47ms
step:1744/2330 train_time:101974ms step_avg:58.47ms
step:1745/2330 train_time:102032ms step_avg:58.47ms
step:1746/2330 train_time:102092ms step_avg:58.47ms
step:1747/2330 train_time:102149ms step_avg:58.47ms
step:1748/2330 train_time:102211ms step_avg:58.47ms
step:1749/2330 train_time:102270ms step_avg:58.47ms
step:1750/2330 train_time:102329ms step_avg:58.47ms
step:1750/2330 val_loss:3.8303 train_time:102411ms step_avg:58.52ms
step:1751/2330 train_time:102430ms step_avg:58.50ms
step:1752/2330 train_time:102450ms step_avg:58.48ms
step:1753/2330 train_time:102512ms step_avg:58.48ms
step:1754/2330 train_time:102577ms step_avg:58.48ms
step:1755/2330 train_time:102633ms step_avg:58.48ms
step:1756/2330 train_time:102696ms step_avg:58.48ms
step:1757/2330 train_time:102753ms step_avg:58.48ms
step:1758/2330 train_time:102814ms step_avg:58.48ms
step:1759/2330 train_time:102870ms step_avg:58.48ms
step:1760/2330 train_time:102932ms step_avg:58.48ms
step:1761/2330 train_time:102988ms step_avg:58.48ms
step:1762/2330 train_time:103050ms step_avg:58.48ms
step:1763/2330 train_time:103106ms step_avg:58.48ms
step:1764/2330 train_time:103167ms step_avg:58.48ms
step:1765/2330 train_time:103223ms step_avg:58.48ms
step:1766/2330 train_time:103283ms step_avg:58.48ms
step:1767/2330 train_time:103339ms step_avg:58.48ms
step:1768/2330 train_time:103401ms step_avg:58.48ms
step:1769/2330 train_time:103459ms step_avg:58.48ms
step:1770/2330 train_time:103521ms step_avg:58.49ms
step:1771/2330 train_time:103579ms step_avg:58.49ms
step:1772/2330 train_time:103641ms step_avg:58.49ms
step:1773/2330 train_time:103699ms step_avg:58.49ms
step:1774/2330 train_time:103760ms step_avg:58.49ms
step:1775/2330 train_time:103816ms step_avg:58.49ms
step:1776/2330 train_time:103877ms step_avg:58.49ms
step:1777/2330 train_time:103933ms step_avg:58.49ms
step:1778/2330 train_time:103995ms step_avg:58.49ms
step:1779/2330 train_time:104051ms step_avg:58.49ms
step:1780/2330 train_time:104112ms step_avg:58.49ms
step:1781/2330 train_time:104168ms step_avg:58.49ms
step:1782/2330 train_time:104229ms step_avg:58.49ms
step:1783/2330 train_time:104286ms step_avg:58.49ms
step:1784/2330 train_time:104347ms step_avg:58.49ms
step:1785/2330 train_time:104403ms step_avg:58.49ms
step:1786/2330 train_time:104467ms step_avg:58.49ms
step:1787/2330 train_time:104525ms step_avg:58.49ms
step:1788/2330 train_time:104588ms step_avg:58.49ms
step:1789/2330 train_time:104647ms step_avg:58.49ms
step:1790/2330 train_time:104707ms step_avg:58.50ms
step:1791/2330 train_time:104764ms step_avg:58.49ms
step:1792/2330 train_time:104825ms step_avg:58.50ms
step:1793/2330 train_time:104883ms step_avg:58.50ms
step:1794/2330 train_time:104944ms step_avg:58.50ms
step:1795/2330 train_time:105001ms step_avg:58.50ms
step:1796/2330 train_time:105061ms step_avg:58.50ms
step:1797/2330 train_time:105118ms step_avg:58.50ms
step:1798/2330 train_time:105178ms step_avg:58.50ms
step:1799/2330 train_time:105235ms step_avg:58.50ms
step:1800/2330 train_time:105296ms step_avg:58.50ms
step:1801/2330 train_time:105352ms step_avg:58.50ms
step:1802/2330 train_time:105415ms step_avg:58.50ms
step:1803/2330 train_time:105472ms step_avg:58.50ms
step:1804/2330 train_time:105534ms step_avg:58.50ms
step:1805/2330 train_time:105591ms step_avg:58.50ms
step:1806/2330 train_time:105654ms step_avg:58.50ms
step:1807/2330 train_time:105710ms step_avg:58.50ms
step:1808/2330 train_time:105773ms step_avg:58.50ms
step:1809/2330 train_time:105830ms step_avg:58.50ms
step:1810/2330 train_time:105892ms step_avg:58.50ms
step:1811/2330 train_time:105948ms step_avg:58.50ms
step:1812/2330 train_time:106010ms step_avg:58.50ms
step:1813/2330 train_time:106067ms step_avg:58.50ms
step:1814/2330 train_time:106128ms step_avg:58.50ms
step:1815/2330 train_time:106186ms step_avg:58.50ms
step:1816/2330 train_time:106246ms step_avg:58.51ms
step:1817/2330 train_time:106303ms step_avg:58.50ms
step:1818/2330 train_time:106363ms step_avg:58.51ms
step:1819/2330 train_time:106421ms step_avg:58.51ms
step:1820/2330 train_time:106481ms step_avg:58.51ms
step:1821/2330 train_time:106539ms step_avg:58.51ms
step:1822/2330 train_time:106599ms step_avg:58.51ms
step:1823/2330 train_time:106657ms step_avg:58.51ms
step:1824/2330 train_time:106718ms step_avg:58.51ms
step:1825/2330 train_time:106775ms step_avg:58.51ms
step:1826/2330 train_time:106836ms step_avg:58.51ms
step:1827/2330 train_time:106893ms step_avg:58.51ms
step:1828/2330 train_time:106955ms step_avg:58.51ms
step:1829/2330 train_time:107012ms step_avg:58.51ms
step:1830/2330 train_time:107073ms step_avg:58.51ms
step:1831/2330 train_time:107130ms step_avg:58.51ms
step:1832/2330 train_time:107192ms step_avg:58.51ms
step:1833/2330 train_time:107249ms step_avg:58.51ms
step:1834/2330 train_time:107310ms step_avg:58.51ms
step:1835/2330 train_time:107366ms step_avg:58.51ms
step:1836/2330 train_time:107429ms step_avg:58.51ms
step:1837/2330 train_time:107487ms step_avg:58.51ms
step:1838/2330 train_time:107547ms step_avg:58.51ms
step:1839/2330 train_time:107605ms step_avg:58.51ms
step:1840/2330 train_time:107667ms step_avg:58.51ms
step:1841/2330 train_time:107724ms step_avg:58.51ms
step:1842/2330 train_time:107786ms step_avg:58.52ms
step:1843/2330 train_time:107844ms step_avg:58.52ms
step:1844/2330 train_time:107904ms step_avg:58.52ms
step:1845/2330 train_time:107961ms step_avg:58.52ms
step:1846/2330 train_time:108022ms step_avg:58.52ms
step:1847/2330 train_time:108080ms step_avg:58.52ms
step:1848/2330 train_time:108139ms step_avg:58.52ms
step:1849/2330 train_time:108197ms step_avg:58.52ms
step:1850/2330 train_time:108257ms step_avg:58.52ms
step:1851/2330 train_time:108314ms step_avg:58.52ms
step:1852/2330 train_time:108376ms step_avg:58.52ms
step:1853/2330 train_time:108432ms step_avg:58.52ms
step:1854/2330 train_time:108494ms step_avg:58.52ms
step:1855/2330 train_time:108550ms step_avg:58.52ms
step:1856/2330 train_time:108613ms step_avg:58.52ms
step:1857/2330 train_time:108670ms step_avg:58.52ms
step:1858/2330 train_time:108733ms step_avg:58.52ms
step:1859/2330 train_time:108790ms step_avg:58.52ms
step:1860/2330 train_time:108853ms step_avg:58.52ms
step:1861/2330 train_time:108909ms step_avg:58.52ms
step:1862/2330 train_time:108972ms step_avg:58.52ms
step:1863/2330 train_time:109029ms step_avg:58.52ms
step:1864/2330 train_time:109090ms step_avg:58.52ms
step:1865/2330 train_time:109148ms step_avg:58.52ms
step:1866/2330 train_time:109208ms step_avg:58.53ms
step:1867/2330 train_time:109266ms step_avg:58.53ms
step:1868/2330 train_time:109327ms step_avg:58.53ms
step:1869/2330 train_time:109384ms step_avg:58.53ms
step:1870/2330 train_time:109444ms step_avg:58.53ms
step:1871/2330 train_time:109502ms step_avg:58.53ms
step:1872/2330 train_time:109562ms step_avg:58.53ms
step:1873/2330 train_time:109619ms step_avg:58.53ms
step:1874/2330 train_time:109680ms step_avg:58.53ms
step:1875/2330 train_time:109737ms step_avg:58.53ms
step:1876/2330 train_time:109798ms step_avg:58.53ms
step:1877/2330 train_time:109855ms step_avg:58.53ms
step:1878/2330 train_time:109916ms step_avg:58.53ms
step:1879/2330 train_time:109972ms step_avg:58.53ms
step:1880/2330 train_time:110034ms step_avg:58.53ms
step:1881/2330 train_time:110090ms step_avg:58.53ms
step:1882/2330 train_time:110152ms step_avg:58.53ms
step:1883/2330 train_time:110209ms step_avg:58.53ms
step:1884/2330 train_time:110272ms step_avg:58.53ms
step:1885/2330 train_time:110329ms step_avg:58.53ms
step:1886/2330 train_time:110391ms step_avg:58.53ms
step:1887/2330 train_time:110448ms step_avg:58.53ms
step:1888/2330 train_time:110510ms step_avg:58.53ms
step:1889/2330 train_time:110567ms step_avg:58.53ms
step:1890/2330 train_time:110629ms step_avg:58.53ms
step:1891/2330 train_time:110686ms step_avg:58.53ms
step:1892/2330 train_time:110749ms step_avg:58.54ms
step:1893/2330 train_time:110807ms step_avg:58.54ms
step:1894/2330 train_time:110868ms step_avg:58.54ms
step:1895/2330 train_time:110926ms step_avg:58.54ms
step:1896/2330 train_time:110987ms step_avg:58.54ms
step:1897/2330 train_time:111044ms step_avg:58.54ms
step:1898/2330 train_time:111104ms step_avg:58.54ms
step:1899/2330 train_time:111162ms step_avg:58.54ms
step:1900/2330 train_time:111223ms step_avg:58.54ms
step:1901/2330 train_time:111281ms step_avg:58.54ms
step:1902/2330 train_time:111341ms step_avg:58.54ms
step:1903/2330 train_time:111397ms step_avg:58.54ms
step:1904/2330 train_time:111457ms step_avg:58.54ms
step:1905/2330 train_time:111514ms step_avg:58.54ms
step:1906/2330 train_time:111576ms step_avg:58.54ms
step:1907/2330 train_time:111633ms step_avg:58.54ms
step:1908/2330 train_time:111695ms step_avg:58.54ms
step:1909/2330 train_time:111751ms step_avg:58.54ms
step:1910/2330 train_time:111813ms step_avg:58.54ms
step:1911/2330 train_time:111870ms step_avg:58.54ms
step:1912/2330 train_time:111933ms step_avg:58.54ms
step:1913/2330 train_time:111990ms step_avg:58.54ms
step:1914/2330 train_time:112052ms step_avg:58.54ms
step:1915/2330 train_time:112108ms step_avg:58.54ms
step:1916/2330 train_time:112172ms step_avg:58.54ms
step:1917/2330 train_time:112229ms step_avg:58.54ms
step:1918/2330 train_time:112291ms step_avg:58.55ms
step:1919/2330 train_time:112348ms step_avg:58.55ms
step:1920/2330 train_time:112408ms step_avg:58.55ms
step:1921/2330 train_time:112466ms step_avg:58.55ms
step:1922/2330 train_time:112525ms step_avg:58.55ms
step:1923/2330 train_time:112583ms step_avg:58.55ms
step:1924/2330 train_time:112644ms step_avg:58.55ms
step:1925/2330 train_time:112702ms step_avg:58.55ms
step:1926/2330 train_time:112762ms step_avg:58.55ms
step:1927/2330 train_time:112820ms step_avg:58.55ms
step:1928/2330 train_time:112880ms step_avg:58.55ms
step:1929/2330 train_time:112938ms step_avg:58.55ms
step:1930/2330 train_time:112999ms step_avg:58.55ms
step:1931/2330 train_time:113056ms step_avg:58.55ms
step:1932/2330 train_time:113116ms step_avg:58.55ms
step:1933/2330 train_time:113173ms step_avg:58.55ms
step:1934/2330 train_time:113234ms step_avg:58.55ms
step:1935/2330 train_time:113291ms step_avg:58.55ms
step:1936/2330 train_time:113353ms step_avg:58.55ms
step:1937/2330 train_time:113409ms step_avg:58.55ms
step:1938/2330 train_time:113471ms step_avg:58.55ms
step:1939/2330 train_time:113527ms step_avg:58.55ms
step:1940/2330 train_time:113589ms step_avg:58.55ms
step:1941/2330 train_time:113646ms step_avg:58.55ms
step:1942/2330 train_time:113708ms step_avg:58.55ms
step:1943/2330 train_time:113765ms step_avg:58.55ms
step:1944/2330 train_time:113828ms step_avg:58.55ms
step:1945/2330 train_time:113886ms step_avg:58.55ms
step:1946/2330 train_time:113947ms step_avg:58.55ms
step:1947/2330 train_time:114005ms step_avg:58.55ms
step:1948/2330 train_time:114066ms step_avg:58.56ms
step:1949/2330 train_time:114123ms step_avg:58.55ms
step:1950/2330 train_time:114183ms step_avg:58.56ms
step:1951/2330 train_time:114241ms step_avg:58.55ms
step:1952/2330 train_time:114302ms step_avg:58.56ms
step:1953/2330 train_time:114360ms step_avg:58.56ms
step:1954/2330 train_time:114420ms step_avg:58.56ms
step:1955/2330 train_time:114477ms step_avg:58.56ms
step:1956/2330 train_time:114538ms step_avg:58.56ms
step:1957/2330 train_time:114595ms step_avg:58.56ms
step:1958/2330 train_time:114656ms step_avg:58.56ms
step:1959/2330 train_time:114712ms step_avg:58.56ms
step:1960/2330 train_time:114775ms step_avg:58.56ms
step:1961/2330 train_time:114831ms step_avg:58.56ms
step:1962/2330 train_time:114894ms step_avg:58.56ms
step:1963/2330 train_time:114951ms step_avg:58.56ms
step:1964/2330 train_time:115013ms step_avg:58.56ms
step:1965/2330 train_time:115070ms step_avg:58.56ms
step:1966/2330 train_time:115133ms step_avg:58.56ms
step:1967/2330 train_time:115189ms step_avg:58.56ms
step:1968/2330 train_time:115253ms step_avg:58.56ms
step:1969/2330 train_time:115309ms step_avg:58.56ms
step:1970/2330 train_time:115371ms step_avg:58.56ms
step:1971/2330 train_time:115428ms step_avg:58.56ms
step:1972/2330 train_time:115490ms step_avg:58.56ms
step:1973/2330 train_time:115547ms step_avg:58.56ms
step:1974/2330 train_time:115609ms step_avg:58.57ms
step:1975/2330 train_time:115666ms step_avg:58.57ms
step:1976/2330 train_time:115727ms step_avg:58.57ms
step:1977/2330 train_time:115784ms step_avg:58.57ms
step:1978/2330 train_time:115846ms step_avg:58.57ms
step:1979/2330 train_time:115905ms step_avg:58.57ms
step:1980/2330 train_time:115965ms step_avg:58.57ms
step:1981/2330 train_time:116023ms step_avg:58.57ms
step:1982/2330 train_time:116084ms step_avg:58.57ms
step:1983/2330 train_time:116143ms step_avg:58.57ms
step:1984/2330 train_time:116203ms step_avg:58.57ms
step:1985/2330 train_time:116260ms step_avg:58.57ms
step:1986/2330 train_time:116320ms step_avg:58.57ms
step:1987/2330 train_time:116377ms step_avg:58.57ms
step:1988/2330 train_time:116438ms step_avg:58.57ms
step:1989/2330 train_time:116495ms step_avg:58.57ms
step:1990/2330 train_time:116556ms step_avg:58.57ms
step:1991/2330 train_time:116612ms step_avg:58.57ms
step:1992/2330 train_time:116675ms step_avg:58.57ms
step:1993/2330 train_time:116732ms step_avg:58.57ms
step:1994/2330 train_time:116795ms step_avg:58.57ms
step:1995/2330 train_time:116851ms step_avg:58.57ms
step:1996/2330 train_time:116915ms step_avg:58.57ms
step:1997/2330 train_time:116972ms step_avg:58.57ms
step:1998/2330 train_time:117035ms step_avg:58.58ms
step:1999/2330 train_time:117091ms step_avg:58.57ms
step:2000/2330 train_time:117154ms step_avg:58.58ms
step:2000/2330 val_loss:3.7675 train_time:117237ms step_avg:58.62ms
step:2001/2330 train_time:117256ms step_avg:58.60ms
step:2002/2330 train_time:117275ms step_avg:58.58ms
step:2003/2330 train_time:117333ms step_avg:58.58ms
step:2004/2330 train_time:117401ms step_avg:58.58ms
step:2005/2330 train_time:117458ms step_avg:58.58ms
step:2006/2330 train_time:117522ms step_avg:58.59ms
step:2007/2330 train_time:117578ms step_avg:58.58ms
step:2008/2330 train_time:117639ms step_avg:58.59ms
step:2009/2330 train_time:117695ms step_avg:58.58ms
step:2010/2330 train_time:117755ms step_avg:58.58ms
step:2011/2330 train_time:117812ms step_avg:58.58ms
step:2012/2330 train_time:117872ms step_avg:58.58ms
step:2013/2330 train_time:117928ms step_avg:58.58ms
step:2014/2330 train_time:117989ms step_avg:58.58ms
step:2015/2330 train_time:118045ms step_avg:58.58ms
step:2016/2330 train_time:118105ms step_avg:58.58ms
step:2017/2330 train_time:118162ms step_avg:58.58ms
step:2018/2330 train_time:118223ms step_avg:58.58ms
step:2019/2330 train_time:118281ms step_avg:58.58ms
step:2020/2330 train_time:118344ms step_avg:58.59ms
step:2021/2330 train_time:118403ms step_avg:58.59ms
step:2022/2330 train_time:118465ms step_avg:58.59ms
step:2023/2330 train_time:118522ms step_avg:58.59ms
step:2024/2330 train_time:118582ms step_avg:58.59ms
step:2025/2330 train_time:118638ms step_avg:58.59ms
step:2026/2330 train_time:118700ms step_avg:58.59ms
step:2027/2330 train_time:118756ms step_avg:58.59ms
step:2028/2330 train_time:118817ms step_avg:58.59ms
step:2029/2330 train_time:118873ms step_avg:58.59ms
step:2030/2330 train_time:118934ms step_avg:58.59ms
step:2031/2330 train_time:118990ms step_avg:58.59ms
step:2032/2330 train_time:119051ms step_avg:58.59ms
step:2033/2330 train_time:119108ms step_avg:58.59ms
step:2034/2330 train_time:119168ms step_avg:58.59ms
step:2035/2330 train_time:119226ms step_avg:58.59ms
step:2036/2330 train_time:119288ms step_avg:58.59ms
step:2037/2330 train_time:119346ms step_avg:58.59ms
step:2038/2330 train_time:119409ms step_avg:58.59ms
step:2039/2330 train_time:119467ms step_avg:58.59ms
step:2040/2330 train_time:119527ms step_avg:58.59ms
step:2041/2330 train_time:119586ms step_avg:58.59ms
step:2042/2330 train_time:119646ms step_avg:58.59ms
step:2043/2330 train_time:119704ms step_avg:58.59ms
step:2044/2330 train_time:119763ms step_avg:58.59ms
step:2045/2330 train_time:119820ms step_avg:58.59ms
step:2046/2330 train_time:119881ms step_avg:58.59ms
step:2047/2330 train_time:119938ms step_avg:58.59ms
step:2048/2330 train_time:119998ms step_avg:58.59ms
step:2049/2330 train_time:120055ms step_avg:58.59ms
step:2050/2330 train_time:120117ms step_avg:58.59ms
step:2051/2330 train_time:120173ms step_avg:58.59ms
step:2052/2330 train_time:120235ms step_avg:58.59ms
step:2053/2330 train_time:120293ms step_avg:58.59ms
step:2054/2330 train_time:120356ms step_avg:58.60ms
step:2055/2330 train_time:120413ms step_avg:58.59ms
step:2056/2330 train_time:120475ms step_avg:58.60ms
step:2057/2330 train_time:120532ms step_avg:58.60ms
step:2058/2330 train_time:120594ms step_avg:58.60ms
step:2059/2330 train_time:120652ms step_avg:58.60ms
step:2060/2330 train_time:120713ms step_avg:58.60ms
step:2061/2330 train_time:120770ms step_avg:58.60ms
step:2062/2330 train_time:120831ms step_avg:58.60ms
step:2063/2330 train_time:120888ms step_avg:58.60ms
step:2064/2330 train_time:120948ms step_avg:58.60ms
step:2065/2330 train_time:121006ms step_avg:58.60ms
step:2066/2330 train_time:121066ms step_avg:58.60ms
step:2067/2330 train_time:121123ms step_avg:58.60ms
step:2068/2330 train_time:121184ms step_avg:58.60ms
step:2069/2330 train_time:121241ms step_avg:58.60ms
step:2070/2330 train_time:121303ms step_avg:58.60ms
step:2071/2330 train_time:121360ms step_avg:58.60ms
step:2072/2330 train_time:121422ms step_avg:58.60ms
step:2073/2330 train_time:121478ms step_avg:58.60ms
step:2074/2330 train_time:121541ms step_avg:58.60ms
step:2075/2330 train_time:121598ms step_avg:58.60ms
step:2076/2330 train_time:121659ms step_avg:58.60ms
step:2077/2330 train_time:121716ms step_avg:58.60ms
step:2078/2330 train_time:121778ms step_avg:58.60ms
step:2079/2330 train_time:121834ms step_avg:58.60ms
step:2080/2330 train_time:121896ms step_avg:58.60ms
step:2081/2330 train_time:121952ms step_avg:58.60ms
step:2082/2330 train_time:122015ms step_avg:58.60ms
step:2083/2330 train_time:122073ms step_avg:58.60ms
step:2084/2330 train_time:122132ms step_avg:58.60ms
step:2085/2330 train_time:122190ms step_avg:58.60ms
step:2086/2330 train_time:122251ms step_avg:58.61ms
step:2087/2330 train_time:122309ms step_avg:58.61ms
step:2088/2330 train_time:122369ms step_avg:58.61ms
step:2089/2330 train_time:122427ms step_avg:58.61ms
step:2090/2330 train_time:122488ms step_avg:58.61ms
step:2091/2330 train_time:122545ms step_avg:58.61ms
step:2092/2330 train_time:122606ms step_avg:58.61ms
step:2093/2330 train_time:122664ms step_avg:58.61ms
step:2094/2330 train_time:122724ms step_avg:58.61ms
step:2095/2330 train_time:122781ms step_avg:58.61ms
step:2096/2330 train_time:122842ms step_avg:58.61ms
step:2097/2330 train_time:122899ms step_avg:58.61ms
step:2098/2330 train_time:122959ms step_avg:58.61ms
step:2099/2330 train_time:123016ms step_avg:58.61ms
step:2100/2330 train_time:123077ms step_avg:58.61ms
step:2101/2330 train_time:123134ms step_avg:58.61ms
step:2102/2330 train_time:123196ms step_avg:58.61ms
step:2103/2330 train_time:123253ms step_avg:58.61ms
step:2104/2330 train_time:123315ms step_avg:58.61ms
step:2105/2330 train_time:123373ms step_avg:58.61ms
step:2106/2330 train_time:123434ms step_avg:58.61ms
step:2107/2330 train_time:123491ms step_avg:58.61ms
step:2108/2330 train_time:123553ms step_avg:58.61ms
step:2109/2330 train_time:123611ms step_avg:58.61ms
step:2110/2330 train_time:123671ms step_avg:58.61ms
step:2111/2330 train_time:123728ms step_avg:58.61ms
step:2112/2330 train_time:123789ms step_avg:58.61ms
step:2113/2330 train_time:123847ms step_avg:58.61ms
step:2114/2330 train_time:123908ms step_avg:58.61ms
step:2115/2330 train_time:123966ms step_avg:58.61ms
step:2116/2330 train_time:124026ms step_avg:58.61ms
step:2117/2330 train_time:124084ms step_avg:58.61ms
step:2118/2330 train_time:124143ms step_avg:58.61ms
step:2119/2330 train_time:124200ms step_avg:58.61ms
step:2120/2330 train_time:124261ms step_avg:58.61ms
step:2121/2330 train_time:124318ms step_avg:58.61ms
step:2122/2330 train_time:124379ms step_avg:58.61ms
step:2123/2330 train_time:124436ms step_avg:58.61ms
step:2124/2330 train_time:124498ms step_avg:58.61ms
step:2125/2330 train_time:124554ms step_avg:58.61ms
step:2126/2330 train_time:124617ms step_avg:58.62ms
step:2127/2330 train_time:124673ms step_avg:58.61ms
step:2128/2330 train_time:124736ms step_avg:58.62ms
step:2129/2330 train_time:124792ms step_avg:58.62ms
step:2130/2330 train_time:124855ms step_avg:58.62ms
step:2131/2330 train_time:124912ms step_avg:58.62ms
step:2132/2330 train_time:124975ms step_avg:58.62ms
step:2133/2330 train_time:125032ms step_avg:58.62ms
step:2134/2330 train_time:125094ms step_avg:58.62ms
step:2135/2330 train_time:125152ms step_avg:58.62ms
step:2136/2330 train_time:125212ms step_avg:58.62ms
step:2137/2330 train_time:125271ms step_avg:58.62ms
step:2138/2330 train_time:125330ms step_avg:58.62ms
step:2139/2330 train_time:125388ms step_avg:58.62ms
step:2140/2330 train_time:125449ms step_avg:58.62ms
step:2141/2330 train_time:125507ms step_avg:58.62ms
step:2142/2330 train_time:125567ms step_avg:58.62ms
step:2143/2330 train_time:125625ms step_avg:58.62ms
step:2144/2330 train_time:125685ms step_avg:58.62ms
step:2145/2330 train_time:125742ms step_avg:58.62ms
step:2146/2330 train_time:125803ms step_avg:58.62ms
step:2147/2330 train_time:125860ms step_avg:58.62ms
step:2148/2330 train_time:125921ms step_avg:58.62ms
step:2149/2330 train_time:125978ms step_avg:58.62ms
step:2150/2330 train_time:126039ms step_avg:58.62ms
step:2151/2330 train_time:126096ms step_avg:58.62ms
step:2152/2330 train_time:126157ms step_avg:58.62ms
step:2153/2330 train_time:126214ms step_avg:58.62ms
step:2154/2330 train_time:126276ms step_avg:58.62ms
step:2155/2330 train_time:126333ms step_avg:58.62ms
step:2156/2330 train_time:126395ms step_avg:58.62ms
step:2157/2330 train_time:126452ms step_avg:58.62ms
step:2158/2330 train_time:126514ms step_avg:58.63ms
step:2159/2330 train_time:126571ms step_avg:58.62ms
step:2160/2330 train_time:126632ms step_avg:58.63ms
step:2161/2330 train_time:126690ms step_avg:58.63ms
step:2162/2330 train_time:126751ms step_avg:58.63ms
step:2163/2330 train_time:126808ms step_avg:58.63ms
step:2164/2330 train_time:126870ms step_avg:58.63ms
step:2165/2330 train_time:126927ms step_avg:58.63ms
step:2166/2330 train_time:126988ms step_avg:58.63ms
step:2167/2330 train_time:127047ms step_avg:58.63ms
step:2168/2330 train_time:127106ms step_avg:58.63ms
step:2169/2330 train_time:127163ms step_avg:58.63ms
step:2170/2330 train_time:127224ms step_avg:58.63ms
step:2171/2330 train_time:127281ms step_avg:58.63ms
step:2172/2330 train_time:127342ms step_avg:58.63ms
step:2173/2330 train_time:127399ms step_avg:58.63ms
step:2174/2330 train_time:127461ms step_avg:58.63ms
step:2175/2330 train_time:127518ms step_avg:58.63ms
step:2176/2330 train_time:127580ms step_avg:58.63ms
step:2177/2330 train_time:127637ms step_avg:58.63ms
step:2178/2330 train_time:127699ms step_avg:58.63ms
step:2179/2330 train_time:127755ms step_avg:58.63ms
step:2180/2330 train_time:127818ms step_avg:58.63ms
step:2181/2330 train_time:127875ms step_avg:58.63ms
step:2182/2330 train_time:127937ms step_avg:58.63ms
step:2183/2330 train_time:127994ms step_avg:58.63ms
step:2184/2330 train_time:128056ms step_avg:58.63ms
step:2185/2330 train_time:128113ms step_avg:58.63ms
step:2186/2330 train_time:128174ms step_avg:58.63ms
step:2187/2330 train_time:128231ms step_avg:58.63ms
step:2188/2330 train_time:128294ms step_avg:58.64ms
step:2189/2330 train_time:128352ms step_avg:58.63ms
step:2190/2330 train_time:128413ms step_avg:58.64ms
step:2191/2330 train_time:128470ms step_avg:58.64ms
step:2192/2330 train_time:128531ms step_avg:58.64ms
step:2193/2330 train_time:128589ms step_avg:58.64ms
step:2194/2330 train_time:128650ms step_avg:58.64ms
step:2195/2330 train_time:128707ms step_avg:58.64ms
step:2196/2330 train_time:128767ms step_avg:58.64ms
step:2197/2330 train_time:128824ms step_avg:58.64ms
step:2198/2330 train_time:128885ms step_avg:58.64ms
step:2199/2330 train_time:128941ms step_avg:58.64ms
step:2200/2330 train_time:129002ms step_avg:58.64ms
step:2201/2330 train_time:129059ms step_avg:58.64ms
step:2202/2330 train_time:129120ms step_avg:58.64ms
step:2203/2330 train_time:129177ms step_avg:58.64ms
step:2204/2330 train_time:129239ms step_avg:58.64ms
step:2205/2330 train_time:129296ms step_avg:58.64ms
step:2206/2330 train_time:129357ms step_avg:58.64ms
step:2207/2330 train_time:129414ms step_avg:58.64ms
step:2208/2330 train_time:129476ms step_avg:58.64ms
step:2209/2330 train_time:129532ms step_avg:58.64ms
step:2210/2330 train_time:129594ms step_avg:58.64ms
step:2211/2330 train_time:129651ms step_avg:58.64ms
step:2212/2330 train_time:129714ms step_avg:58.64ms
step:2213/2330 train_time:129772ms step_avg:58.64ms
step:2214/2330 train_time:129833ms step_avg:58.64ms
step:2215/2330 train_time:129890ms step_avg:58.64ms
step:2216/2330 train_time:129952ms step_avg:58.64ms
step:2217/2330 train_time:130010ms step_avg:58.64ms
step:2218/2330 train_time:130071ms step_avg:58.64ms
step:2219/2330 train_time:130127ms step_avg:58.64ms
step:2220/2330 train_time:130189ms step_avg:58.64ms
step:2221/2330 train_time:130247ms step_avg:58.64ms
step:2222/2330 train_time:130307ms step_avg:58.64ms
step:2223/2330 train_time:130366ms step_avg:58.64ms
step:2224/2330 train_time:130426ms step_avg:58.64ms
step:2225/2330 train_time:130482ms step_avg:58.64ms
step:2226/2330 train_time:130543ms step_avg:58.64ms
step:2227/2330 train_time:130599ms step_avg:58.64ms
step:2228/2330 train_time:130662ms step_avg:58.65ms
step:2229/2330 train_time:130719ms step_avg:58.64ms
step:2230/2330 train_time:130779ms step_avg:58.65ms
step:2231/2330 train_time:130836ms step_avg:58.64ms
step:2232/2330 train_time:130898ms step_avg:58.65ms
step:2233/2330 train_time:130955ms step_avg:58.65ms
step:2234/2330 train_time:131017ms step_avg:58.65ms
step:2235/2330 train_time:131073ms step_avg:58.65ms
step:2236/2330 train_time:131137ms step_avg:58.65ms
step:2237/2330 train_time:131194ms step_avg:58.65ms
step:2238/2330 train_time:131256ms step_avg:58.65ms
step:2239/2330 train_time:131313ms step_avg:58.65ms
step:2240/2330 train_time:131375ms step_avg:58.65ms
step:2241/2330 train_time:131431ms step_avg:58.65ms
step:2242/2330 train_time:131492ms step_avg:58.65ms
step:2243/2330 train_time:131550ms step_avg:58.65ms
step:2244/2330 train_time:131611ms step_avg:58.65ms
step:2245/2330 train_time:131669ms step_avg:58.65ms
step:2246/2330 train_time:131728ms step_avg:58.65ms
step:2247/2330 train_time:131785ms step_avg:58.65ms
step:2248/2330 train_time:131847ms step_avg:58.65ms
step:2249/2330 train_time:131904ms step_avg:58.65ms
step:2250/2330 train_time:131964ms step_avg:58.65ms
step:2250/2330 val_loss:3.7174 train_time:132047ms step_avg:58.69ms
step:2251/2330 train_time:132066ms step_avg:58.67ms
step:2252/2330 train_time:132086ms step_avg:58.65ms
step:2253/2330 train_time:132145ms step_avg:58.65ms
step:2254/2330 train_time:132209ms step_avg:58.66ms
step:2255/2330 train_time:132266ms step_avg:58.65ms
step:2256/2330 train_time:132328ms step_avg:58.66ms
step:2257/2330 train_time:132385ms step_avg:58.66ms
step:2258/2330 train_time:132445ms step_avg:58.66ms
step:2259/2330 train_time:132503ms step_avg:58.66ms
step:2260/2330 train_time:132562ms step_avg:58.66ms
step:2261/2330 train_time:132619ms step_avg:58.66ms
step:2262/2330 train_time:132678ms step_avg:58.66ms
step:2263/2330 train_time:132735ms step_avg:58.65ms
step:2264/2330 train_time:132795ms step_avg:58.66ms
step:2265/2330 train_time:132852ms step_avg:58.65ms
step:2266/2330 train_time:132911ms step_avg:58.65ms
step:2267/2330 train_time:132969ms step_avg:58.65ms
step:2268/2330 train_time:133031ms step_avg:58.66ms
step:2269/2330 train_time:133091ms step_avg:58.66ms
step:2270/2330 train_time:133152ms step_avg:58.66ms
step:2271/2330 train_time:133210ms step_avg:58.66ms
step:2272/2330 train_time:133272ms step_avg:58.66ms
step:2273/2330 train_time:133329ms step_avg:58.66ms
step:2274/2330 train_time:133391ms step_avg:58.66ms
step:2275/2330 train_time:133448ms step_avg:58.66ms
step:2276/2330 train_time:133509ms step_avg:58.66ms
step:2277/2330 train_time:133566ms step_avg:58.66ms
step:2278/2330 train_time:133627ms step_avg:58.66ms
step:2279/2330 train_time:133684ms step_avg:58.66ms
step:2280/2330 train_time:133744ms step_avg:58.66ms
step:2281/2330 train_time:133801ms step_avg:58.66ms
step:2282/2330 train_time:133861ms step_avg:58.66ms
step:2283/2330 train_time:133917ms step_avg:58.66ms
step:2284/2330 train_time:133978ms step_avg:58.66ms
step:2285/2330 train_time:134035ms step_avg:58.66ms
step:2286/2330 train_time:134097ms step_avg:58.66ms
step:2287/2330 train_time:134155ms step_avg:58.66ms
step:2288/2330 train_time:134217ms step_avg:58.66ms
step:2289/2330 train_time:134274ms step_avg:58.66ms
step:2290/2330 train_time:134336ms step_avg:58.66ms
step:2291/2330 train_time:134393ms step_avg:58.66ms
step:2292/2330 train_time:134456ms step_avg:58.66ms
step:2293/2330 train_time:134512ms step_avg:58.66ms
step:2294/2330 train_time:134575ms step_avg:58.66ms
step:2295/2330 train_time:134631ms step_avg:58.66ms
step:2296/2330 train_time:134693ms step_avg:58.66ms
step:2297/2330 train_time:134749ms step_avg:58.66ms
step:2298/2330 train_time:134811ms step_avg:58.66ms
step:2299/2330 train_time:134867ms step_avg:58.66ms
step:2300/2330 train_time:134928ms step_avg:58.66ms
step:2301/2330 train_time:134986ms step_avg:58.66ms
step:2302/2330 train_time:135047ms step_avg:58.66ms
step:2303/2330 train_time:135105ms step_avg:58.66ms
step:2304/2330 train_time:135166ms step_avg:58.67ms
step:2305/2330 train_time:135225ms step_avg:58.67ms
step:2306/2330 train_time:135286ms step_avg:58.67ms
step:2307/2330 train_time:135344ms step_avg:58.67ms
step:2308/2330 train_time:135405ms step_avg:58.67ms
step:2309/2330 train_time:135463ms step_avg:58.67ms
step:2310/2330 train_time:135524ms step_avg:58.67ms
step:2311/2330 train_time:135582ms step_avg:58.67ms
step:2312/2330 train_time:135641ms step_avg:58.67ms
step:2313/2330 train_time:135697ms step_avg:58.67ms
step:2314/2330 train_time:135758ms step_avg:58.67ms
step:2315/2330 train_time:135815ms step_avg:58.67ms
step:2316/2330 train_time:135876ms step_avg:58.67ms
step:2317/2330 train_time:135932ms step_avg:58.67ms
step:2318/2330 train_time:135994ms step_avg:58.67ms
step:2319/2330 train_time:136050ms step_avg:58.67ms
step:2320/2330 train_time:136112ms step_avg:58.67ms
step:2321/2330 train_time:136170ms step_avg:58.67ms
step:2322/2330 train_time:136233ms step_avg:58.67ms
step:2323/2330 train_time:136291ms step_avg:58.67ms
step:2324/2330 train_time:136352ms step_avg:58.67ms
step:2325/2330 train_time:136410ms step_avg:58.67ms
step:2326/2330 train_time:136471ms step_avg:58.67ms
step:2327/2330 train_time:136529ms step_avg:58.67ms
step:2328/2330 train_time:136590ms step_avg:58.67ms
step:2329/2330 train_time:136648ms step_avg:58.67ms
step:2330/2330 train_time:136708ms step_avg:58.67ms
step:2330/2330 val_loss:3.7014 train_time:136789ms step_avg:58.71ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
