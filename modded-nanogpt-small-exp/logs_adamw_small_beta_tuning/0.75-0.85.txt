import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--beta1', type=str, required=True)
parser.add_argument('--beta2', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_beta_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=6e-4,  betas=(float(args2.beta1), float(args2.beta2)), eps=1e-10, weight_decay=0.0, fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt", exist_ok=True)
            torch.save(log, f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 05:01:13 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   27C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   26C    P0             115W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   24C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   27C    P0             113W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   25C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   26C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:86ms step_avg:85.79ms
step:2/2330 train_time:178ms step_avg:88.97ms
step:3/2330 train_time:195ms step_avg:65.09ms
step:4/2330 train_time:215ms step_avg:53.65ms
step:5/2330 train_time:269ms step_avg:53.72ms
step:6/2330 train_time:327ms step_avg:54.46ms
step:7/2330 train_time:382ms step_avg:54.53ms
step:8/2330 train_time:440ms step_avg:55.04ms
step:9/2330 train_time:496ms step_avg:55.06ms
step:10/2330 train_time:554ms step_avg:55.43ms
step:11/2330 train_time:609ms step_avg:55.39ms
step:12/2330 train_time:668ms step_avg:55.63ms
step:13/2330 train_time:723ms step_avg:55.63ms
step:14/2330 train_time:781ms step_avg:55.81ms
step:15/2330 train_time:837ms step_avg:55.79ms
step:16/2330 train_time:895ms step_avg:55.94ms
step:17/2330 train_time:950ms step_avg:55.90ms
step:18/2330 train_time:1009ms step_avg:56.04ms
step:19/2330 train_time:1067ms step_avg:56.14ms
step:20/2330 train_time:1127ms step_avg:56.34ms
step:21/2330 train_time:1184ms step_avg:56.39ms
step:22/2330 train_time:1245ms step_avg:56.60ms
step:23/2330 train_time:1300ms step_avg:56.54ms
step:24/2330 train_time:1361ms step_avg:56.70ms
step:25/2330 train_time:1416ms step_avg:56.65ms
step:26/2330 train_time:1475ms step_avg:56.75ms
step:27/2330 train_time:1531ms step_avg:56.71ms
step:28/2330 train_time:1590ms step_avg:56.78ms
step:29/2330 train_time:1645ms step_avg:56.73ms
step:30/2330 train_time:1704ms step_avg:56.79ms
step:31/2330 train_time:1759ms step_avg:56.74ms
step:32/2330 train_time:1818ms step_avg:56.83ms
step:33/2330 train_time:1874ms step_avg:56.78ms
step:34/2330 train_time:1933ms step_avg:56.86ms
step:35/2330 train_time:1989ms step_avg:56.84ms
step:36/2330 train_time:2049ms step_avg:56.91ms
step:37/2330 train_time:2106ms step_avg:56.91ms
step:38/2330 train_time:2165ms step_avg:56.98ms
step:39/2330 train_time:2222ms step_avg:56.98ms
step:40/2330 train_time:2282ms step_avg:57.05ms
step:41/2330 train_time:2338ms step_avg:57.02ms
step:42/2330 train_time:2397ms step_avg:57.08ms
step:43/2330 train_time:2454ms step_avg:57.07ms
step:44/2330 train_time:2513ms step_avg:57.11ms
step:45/2330 train_time:2569ms step_avg:57.09ms
step:46/2330 train_time:2627ms step_avg:57.11ms
step:47/2330 train_time:2683ms step_avg:57.08ms
step:48/2330 train_time:2741ms step_avg:57.11ms
step:49/2330 train_time:2796ms step_avg:57.07ms
step:50/2330 train_time:2857ms step_avg:57.14ms
step:51/2330 train_time:2912ms step_avg:57.10ms
step:52/2330 train_time:2972ms step_avg:57.15ms
step:53/2330 train_time:3028ms step_avg:57.13ms
step:54/2330 train_time:3088ms step_avg:57.18ms
step:55/2330 train_time:3144ms step_avg:57.16ms
step:56/2330 train_time:3203ms step_avg:57.20ms
step:57/2330 train_time:3260ms step_avg:57.19ms
step:58/2330 train_time:3320ms step_avg:57.24ms
step:59/2330 train_time:3376ms step_avg:57.22ms
step:60/2330 train_time:3435ms step_avg:57.25ms
step:61/2330 train_time:3491ms step_avg:57.23ms
step:62/2330 train_time:3550ms step_avg:57.26ms
step:63/2330 train_time:3606ms step_avg:57.24ms
step:64/2330 train_time:3665ms step_avg:57.26ms
step:65/2330 train_time:3721ms step_avg:57.24ms
step:66/2330 train_time:3779ms step_avg:57.26ms
step:67/2330 train_time:3835ms step_avg:57.24ms
step:68/2330 train_time:3894ms step_avg:57.26ms
step:69/2330 train_time:3950ms step_avg:57.24ms
step:70/2330 train_time:4008ms step_avg:57.26ms
step:71/2330 train_time:4065ms step_avg:57.25ms
step:72/2330 train_time:4124ms step_avg:57.28ms
step:73/2330 train_time:4180ms step_avg:57.26ms
step:74/2330 train_time:4240ms step_avg:57.30ms
step:75/2330 train_time:4296ms step_avg:57.28ms
step:76/2330 train_time:4355ms step_avg:57.31ms
step:77/2330 train_time:4411ms step_avg:57.29ms
step:78/2330 train_time:4470ms step_avg:57.31ms
step:79/2330 train_time:4527ms step_avg:57.30ms
step:80/2330 train_time:4586ms step_avg:57.32ms
step:81/2330 train_time:4642ms step_avg:57.31ms
step:82/2330 train_time:4700ms step_avg:57.32ms
step:83/2330 train_time:4756ms step_avg:57.30ms
step:84/2330 train_time:4815ms step_avg:57.32ms
step:85/2330 train_time:4870ms step_avg:57.30ms
step:86/2330 train_time:4929ms step_avg:57.31ms
step:87/2330 train_time:4984ms step_avg:57.29ms
step:88/2330 train_time:5044ms step_avg:57.31ms
step:89/2330 train_time:5099ms step_avg:57.30ms
step:90/2330 train_time:5159ms step_avg:57.32ms
step:91/2330 train_time:5215ms step_avg:57.31ms
step:92/2330 train_time:5275ms step_avg:57.34ms
step:93/2330 train_time:5332ms step_avg:57.33ms
step:94/2330 train_time:5391ms step_avg:57.35ms
step:95/2330 train_time:5447ms step_avg:57.34ms
step:96/2330 train_time:5505ms step_avg:57.35ms
step:97/2330 train_time:5563ms step_avg:57.35ms
step:98/2330 train_time:5621ms step_avg:57.35ms
step:99/2330 train_time:5676ms step_avg:57.34ms
step:100/2330 train_time:5735ms step_avg:57.35ms
step:101/2330 train_time:5791ms step_avg:57.34ms
step:102/2330 train_time:5850ms step_avg:57.36ms
step:103/2330 train_time:5906ms step_avg:57.34ms
step:104/2330 train_time:5965ms step_avg:57.36ms
step:105/2330 train_time:6021ms step_avg:57.34ms
step:106/2330 train_time:6080ms step_avg:57.36ms
step:107/2330 train_time:6136ms step_avg:57.34ms
step:108/2330 train_time:6195ms step_avg:57.36ms
step:109/2330 train_time:6251ms step_avg:57.35ms
step:110/2330 train_time:6311ms step_avg:57.37ms
step:111/2330 train_time:6367ms step_avg:57.36ms
step:112/2330 train_time:6426ms step_avg:57.38ms
step:113/2330 train_time:6483ms step_avg:57.37ms
step:114/2330 train_time:6541ms step_avg:57.38ms
step:115/2330 train_time:6597ms step_avg:57.36ms
step:116/2330 train_time:6657ms step_avg:57.39ms
step:117/2330 train_time:6713ms step_avg:57.38ms
step:118/2330 train_time:6772ms step_avg:57.39ms
step:119/2330 train_time:6828ms step_avg:57.38ms
step:120/2330 train_time:6887ms step_avg:57.39ms
step:121/2330 train_time:6942ms step_avg:57.37ms
step:122/2330 train_time:7001ms step_avg:57.39ms
step:123/2330 train_time:7057ms step_avg:57.37ms
step:124/2330 train_time:7117ms step_avg:57.39ms
step:125/2330 train_time:7173ms step_avg:57.38ms
step:126/2330 train_time:7231ms step_avg:57.39ms
step:127/2330 train_time:7288ms step_avg:57.39ms
step:128/2330 train_time:7347ms step_avg:57.39ms
step:129/2330 train_time:7402ms step_avg:57.38ms
step:130/2330 train_time:7461ms step_avg:57.39ms
step:131/2330 train_time:7516ms step_avg:57.38ms
step:132/2330 train_time:7576ms step_avg:57.40ms
step:133/2330 train_time:7633ms step_avg:57.39ms
step:134/2330 train_time:7692ms step_avg:57.40ms
step:135/2330 train_time:7747ms step_avg:57.39ms
step:136/2330 train_time:7806ms step_avg:57.40ms
step:137/2330 train_time:7862ms step_avg:57.39ms
step:138/2330 train_time:7921ms step_avg:57.40ms
step:139/2330 train_time:7977ms step_avg:57.39ms
step:140/2330 train_time:8036ms step_avg:57.40ms
step:141/2330 train_time:8092ms step_avg:57.39ms
step:142/2330 train_time:8151ms step_avg:57.40ms
step:143/2330 train_time:8207ms step_avg:57.39ms
step:144/2330 train_time:8266ms step_avg:57.40ms
step:145/2330 train_time:8322ms step_avg:57.39ms
step:146/2330 train_time:8380ms step_avg:57.40ms
step:147/2330 train_time:8436ms step_avg:57.38ms
step:148/2330 train_time:8496ms step_avg:57.41ms
step:149/2330 train_time:8552ms step_avg:57.40ms
step:150/2330 train_time:8612ms step_avg:57.41ms
step:151/2330 train_time:8667ms step_avg:57.40ms
step:152/2330 train_time:8726ms step_avg:57.41ms
step:153/2330 train_time:8782ms step_avg:57.40ms
step:154/2330 train_time:8841ms step_avg:57.41ms
step:155/2330 train_time:8896ms step_avg:57.40ms
step:156/2330 train_time:8956ms step_avg:57.41ms
step:157/2330 train_time:9011ms step_avg:57.40ms
step:158/2330 train_time:9070ms step_avg:57.40ms
step:159/2330 train_time:9125ms step_avg:57.39ms
step:160/2330 train_time:9185ms step_avg:57.40ms
step:161/2330 train_time:9241ms step_avg:57.40ms
step:162/2330 train_time:9300ms step_avg:57.41ms
step:163/2330 train_time:9356ms step_avg:57.40ms
step:164/2330 train_time:9416ms step_avg:57.41ms
step:165/2330 train_time:9472ms step_avg:57.41ms
step:166/2330 train_time:9531ms step_avg:57.41ms
step:167/2330 train_time:9587ms step_avg:57.41ms
step:168/2330 train_time:9645ms step_avg:57.41ms
step:169/2330 train_time:9701ms step_avg:57.40ms
step:170/2330 train_time:9760ms step_avg:57.41ms
step:171/2330 train_time:9816ms step_avg:57.40ms
step:172/2330 train_time:9875ms step_avg:57.42ms
step:173/2330 train_time:9932ms step_avg:57.41ms
step:174/2330 train_time:9991ms step_avg:57.42ms
step:175/2330 train_time:10047ms step_avg:57.41ms
step:176/2330 train_time:10106ms step_avg:57.42ms
step:177/2330 train_time:10162ms step_avg:57.41ms
step:178/2330 train_time:10221ms step_avg:57.42ms
step:179/2330 train_time:10277ms step_avg:57.41ms
step:180/2330 train_time:10337ms step_avg:57.43ms
step:181/2330 train_time:10394ms step_avg:57.42ms
step:182/2330 train_time:10452ms step_avg:57.43ms
step:183/2330 train_time:10508ms step_avg:57.42ms
step:184/2330 train_time:10567ms step_avg:57.43ms
step:185/2330 train_time:10623ms step_avg:57.42ms
step:186/2330 train_time:10682ms step_avg:57.43ms
step:187/2330 train_time:10738ms step_avg:57.42ms
step:188/2330 train_time:10797ms step_avg:57.43ms
step:189/2330 train_time:10854ms step_avg:57.43ms
step:190/2330 train_time:10913ms step_avg:57.43ms
step:191/2330 train_time:10969ms step_avg:57.43ms
step:192/2330 train_time:11027ms step_avg:57.43ms
step:193/2330 train_time:11083ms step_avg:57.42ms
step:194/2330 train_time:11142ms step_avg:57.43ms
step:195/2330 train_time:11197ms step_avg:57.42ms
step:196/2330 train_time:11257ms step_avg:57.43ms
step:197/2330 train_time:11312ms step_avg:57.42ms
step:198/2330 train_time:11372ms step_avg:57.44ms
step:199/2330 train_time:11428ms step_avg:57.43ms
step:200/2330 train_time:11488ms step_avg:57.44ms
step:201/2330 train_time:11543ms step_avg:57.43ms
step:202/2330 train_time:11602ms step_avg:57.44ms
step:203/2330 train_time:11658ms step_avg:57.43ms
step:204/2330 train_time:11718ms step_avg:57.44ms
step:205/2330 train_time:11774ms step_avg:57.43ms
step:206/2330 train_time:11833ms step_avg:57.44ms
step:207/2330 train_time:11889ms step_avg:57.44ms
step:208/2330 train_time:11948ms step_avg:57.44ms
step:209/2330 train_time:12004ms step_avg:57.44ms
step:210/2330 train_time:12062ms step_avg:57.44ms
step:211/2330 train_time:12118ms step_avg:57.43ms
step:212/2330 train_time:12177ms step_avg:57.44ms
step:213/2330 train_time:12233ms step_avg:57.43ms
step:214/2330 train_time:12292ms step_avg:57.44ms
step:215/2330 train_time:12348ms step_avg:57.43ms
step:216/2330 train_time:12407ms step_avg:57.44ms
step:217/2330 train_time:12464ms step_avg:57.44ms
step:218/2330 train_time:12522ms step_avg:57.44ms
step:219/2330 train_time:12578ms step_avg:57.43ms
step:220/2330 train_time:12637ms step_avg:57.44ms
step:221/2330 train_time:12693ms step_avg:57.44ms
step:222/2330 train_time:12752ms step_avg:57.44ms
step:223/2330 train_time:12808ms step_avg:57.43ms
step:224/2330 train_time:12867ms step_avg:57.44ms
step:225/2330 train_time:12923ms step_avg:57.43ms
step:226/2330 train_time:12982ms step_avg:57.44ms
step:227/2330 train_time:13038ms step_avg:57.44ms
step:228/2330 train_time:13097ms step_avg:57.44ms
step:229/2330 train_time:13153ms step_avg:57.44ms
step:230/2330 train_time:13211ms step_avg:57.44ms
step:231/2330 train_time:13267ms step_avg:57.43ms
step:232/2330 train_time:13327ms step_avg:57.44ms
step:233/2330 train_time:13383ms step_avg:57.44ms
step:234/2330 train_time:13442ms step_avg:57.44ms
step:235/2330 train_time:13497ms step_avg:57.44ms
step:236/2330 train_time:13558ms step_avg:57.45ms
step:237/2330 train_time:13614ms step_avg:57.44ms
step:238/2330 train_time:13672ms step_avg:57.45ms
step:239/2330 train_time:13729ms step_avg:57.44ms
step:240/2330 train_time:13787ms step_avg:57.45ms
step:241/2330 train_time:13843ms step_avg:57.44ms
step:242/2330 train_time:13902ms step_avg:57.44ms
step:243/2330 train_time:13957ms step_avg:57.44ms
step:244/2330 train_time:14017ms step_avg:57.45ms
step:245/2330 train_time:14073ms step_avg:57.44ms
step:246/2330 train_time:14131ms step_avg:57.44ms
step:247/2330 train_time:14187ms step_avg:57.44ms
step:248/2330 train_time:14247ms step_avg:57.45ms
step:249/2330 train_time:14303ms step_avg:57.44ms
step:250/2330 train_time:14362ms step_avg:57.45ms
step:250/2330 val_loss:4.8898 train_time:14441ms step_avg:57.76ms
step:251/2330 train_time:14459ms step_avg:57.61ms
step:252/2330 train_time:14479ms step_avg:57.46ms
step:253/2330 train_time:14536ms step_avg:57.46ms
step:254/2330 train_time:14601ms step_avg:57.48ms
step:255/2330 train_time:14658ms step_avg:57.48ms
step:256/2330 train_time:14717ms step_avg:57.49ms
step:257/2330 train_time:14773ms step_avg:57.48ms
step:258/2330 train_time:14831ms step_avg:57.49ms
step:259/2330 train_time:14887ms step_avg:57.48ms
step:260/2330 train_time:14946ms step_avg:57.48ms
step:261/2330 train_time:15001ms step_avg:57.48ms
step:262/2330 train_time:15060ms step_avg:57.48ms
step:263/2330 train_time:15115ms step_avg:57.47ms
step:264/2330 train_time:15173ms step_avg:57.48ms
step:265/2330 train_time:15228ms step_avg:57.47ms
step:266/2330 train_time:15287ms step_avg:57.47ms
step:267/2330 train_time:15343ms step_avg:57.47ms
step:268/2330 train_time:15402ms step_avg:57.47ms
step:269/2330 train_time:15459ms step_avg:57.47ms
step:270/2330 train_time:15520ms step_avg:57.48ms
step:271/2330 train_time:15576ms step_avg:57.48ms
step:272/2330 train_time:15637ms step_avg:57.49ms
step:273/2330 train_time:15693ms step_avg:57.48ms
step:274/2330 train_time:15752ms step_avg:57.49ms
step:275/2330 train_time:15808ms step_avg:57.48ms
step:276/2330 train_time:15867ms step_avg:57.49ms
step:277/2330 train_time:15923ms step_avg:57.48ms
step:278/2330 train_time:15982ms step_avg:57.49ms
step:279/2330 train_time:16038ms step_avg:57.48ms
step:280/2330 train_time:16096ms step_avg:57.49ms
step:281/2330 train_time:16152ms step_avg:57.48ms
step:282/2330 train_time:16210ms step_avg:57.48ms
step:283/2330 train_time:16265ms step_avg:57.48ms
step:284/2330 train_time:16324ms step_avg:57.48ms
step:285/2330 train_time:16381ms step_avg:57.48ms
step:286/2330 train_time:16439ms step_avg:57.48ms
step:287/2330 train_time:16496ms step_avg:57.48ms
step:288/2330 train_time:16555ms step_avg:57.48ms
step:289/2330 train_time:16611ms step_avg:57.48ms
step:290/2330 train_time:16672ms step_avg:57.49ms
step:291/2330 train_time:16728ms step_avg:57.48ms
step:292/2330 train_time:16788ms step_avg:57.49ms
step:293/2330 train_time:16843ms step_avg:57.49ms
step:294/2330 train_time:16903ms step_avg:57.49ms
step:295/2330 train_time:16958ms step_avg:57.49ms
step:296/2330 train_time:17018ms step_avg:57.49ms
step:297/2330 train_time:17074ms step_avg:57.49ms
step:298/2330 train_time:17132ms step_avg:57.49ms
step:299/2330 train_time:17188ms step_avg:57.48ms
step:300/2330 train_time:17246ms step_avg:57.49ms
step:301/2330 train_time:17302ms step_avg:57.48ms
step:302/2330 train_time:17361ms step_avg:57.49ms
step:303/2330 train_time:17417ms step_avg:57.48ms
step:304/2330 train_time:17476ms step_avg:57.49ms
step:305/2330 train_time:17533ms step_avg:57.48ms
step:306/2330 train_time:17592ms step_avg:57.49ms
step:307/2330 train_time:17649ms step_avg:57.49ms
step:308/2330 train_time:17708ms step_avg:57.49ms
step:309/2330 train_time:17765ms step_avg:57.49ms
step:310/2330 train_time:17824ms step_avg:57.50ms
step:311/2330 train_time:17881ms step_avg:57.49ms
step:312/2330 train_time:17939ms step_avg:57.50ms
step:313/2330 train_time:17995ms step_avg:57.49ms
step:314/2330 train_time:18054ms step_avg:57.50ms
step:315/2330 train_time:18110ms step_avg:57.49ms
step:316/2330 train_time:18168ms step_avg:57.49ms
step:317/2330 train_time:18225ms step_avg:57.49ms
step:318/2330 train_time:18283ms step_avg:57.49ms
step:319/2330 train_time:18340ms step_avg:57.49ms
step:320/2330 train_time:18399ms step_avg:57.50ms
step:321/2330 train_time:18455ms step_avg:57.49ms
step:322/2330 train_time:18514ms step_avg:57.50ms
step:323/2330 train_time:18570ms step_avg:57.49ms
step:324/2330 train_time:18629ms step_avg:57.50ms
step:325/2330 train_time:18685ms step_avg:57.49ms
step:326/2330 train_time:18746ms step_avg:57.50ms
step:327/2330 train_time:18801ms step_avg:57.50ms
step:328/2330 train_time:18861ms step_avg:57.50ms
step:329/2330 train_time:18918ms step_avg:57.50ms
step:330/2330 train_time:18976ms step_avg:57.50ms
step:331/2330 train_time:19032ms step_avg:57.50ms
step:332/2330 train_time:19090ms step_avg:57.50ms
step:333/2330 train_time:19146ms step_avg:57.49ms
step:334/2330 train_time:19206ms step_avg:57.50ms
step:335/2330 train_time:19262ms step_avg:57.50ms
step:336/2330 train_time:19322ms step_avg:57.50ms
step:337/2330 train_time:19378ms step_avg:57.50ms
step:338/2330 train_time:19436ms step_avg:57.50ms
step:339/2330 train_time:19492ms step_avg:57.50ms
step:340/2330 train_time:19551ms step_avg:57.50ms
step:341/2330 train_time:19607ms step_avg:57.50ms
step:342/2330 train_time:19667ms step_avg:57.51ms
step:343/2330 train_time:19723ms step_avg:57.50ms
step:344/2330 train_time:19783ms step_avg:57.51ms
step:345/2330 train_time:19839ms step_avg:57.50ms
step:346/2330 train_time:19898ms step_avg:57.51ms
step:347/2330 train_time:19955ms step_avg:57.51ms
step:348/2330 train_time:20014ms step_avg:57.51ms
step:349/2330 train_time:20070ms step_avg:57.51ms
step:350/2330 train_time:20129ms step_avg:57.51ms
step:351/2330 train_time:20184ms step_avg:57.51ms
step:352/2330 train_time:20244ms step_avg:57.51ms
step:353/2330 train_time:20300ms step_avg:57.51ms
step:354/2330 train_time:20359ms step_avg:57.51ms
step:355/2330 train_time:20416ms step_avg:57.51ms
step:356/2330 train_time:20475ms step_avg:57.51ms
step:357/2330 train_time:20531ms step_avg:57.51ms
step:358/2330 train_time:20590ms step_avg:57.51ms
step:359/2330 train_time:20646ms step_avg:57.51ms
step:360/2330 train_time:20705ms step_avg:57.51ms
step:361/2330 train_time:20761ms step_avg:57.51ms
step:362/2330 train_time:20820ms step_avg:57.51ms
step:363/2330 train_time:20876ms step_avg:57.51ms
step:364/2330 train_time:20936ms step_avg:57.52ms
step:365/2330 train_time:20991ms step_avg:57.51ms
step:366/2330 train_time:21050ms step_avg:57.51ms
step:367/2330 train_time:21106ms step_avg:57.51ms
step:368/2330 train_time:21166ms step_avg:57.52ms
step:369/2330 train_time:21222ms step_avg:57.51ms
step:370/2330 train_time:21281ms step_avg:57.52ms
step:371/2330 train_time:21337ms step_avg:57.51ms
step:372/2330 train_time:21397ms step_avg:57.52ms
step:373/2330 train_time:21453ms step_avg:57.52ms
step:374/2330 train_time:21512ms step_avg:57.52ms
step:375/2330 train_time:21568ms step_avg:57.51ms
step:376/2330 train_time:21628ms step_avg:57.52ms
step:377/2330 train_time:21683ms step_avg:57.52ms
step:378/2330 train_time:21744ms step_avg:57.52ms
step:379/2330 train_time:21800ms step_avg:57.52ms
step:380/2330 train_time:21859ms step_avg:57.52ms
step:381/2330 train_time:21916ms step_avg:57.52ms
step:382/2330 train_time:21974ms step_avg:57.52ms
step:383/2330 train_time:22030ms step_avg:57.52ms
step:384/2330 train_time:22089ms step_avg:57.52ms
step:385/2330 train_time:22145ms step_avg:57.52ms
step:386/2330 train_time:22205ms step_avg:57.52ms
step:387/2330 train_time:22260ms step_avg:57.52ms
step:388/2330 train_time:22320ms step_avg:57.53ms
step:389/2330 train_time:22376ms step_avg:57.52ms
step:390/2330 train_time:22435ms step_avg:57.53ms
step:391/2330 train_time:22491ms step_avg:57.52ms
step:392/2330 train_time:22550ms step_avg:57.53ms
step:393/2330 train_time:22606ms step_avg:57.52ms
step:394/2330 train_time:22665ms step_avg:57.53ms
step:395/2330 train_time:22721ms step_avg:57.52ms
step:396/2330 train_time:22781ms step_avg:57.53ms
step:397/2330 train_time:22837ms step_avg:57.52ms
step:398/2330 train_time:22896ms step_avg:57.53ms
step:399/2330 train_time:22952ms step_avg:57.52ms
step:400/2330 train_time:23010ms step_avg:57.53ms
step:401/2330 train_time:23066ms step_avg:57.52ms
step:402/2330 train_time:23126ms step_avg:57.53ms
step:403/2330 train_time:23182ms step_avg:57.52ms
step:404/2330 train_time:23241ms step_avg:57.53ms
step:405/2330 train_time:23297ms step_avg:57.52ms
step:406/2330 train_time:23357ms step_avg:57.53ms
step:407/2330 train_time:23413ms step_avg:57.53ms
step:408/2330 train_time:23471ms step_avg:57.53ms
step:409/2330 train_time:23527ms step_avg:57.52ms
step:410/2330 train_time:23586ms step_avg:57.53ms
step:411/2330 train_time:23643ms step_avg:57.53ms
step:412/2330 train_time:23702ms step_avg:57.53ms
step:413/2330 train_time:23758ms step_avg:57.53ms
step:414/2330 train_time:23817ms step_avg:57.53ms
step:415/2330 train_time:23873ms step_avg:57.53ms
step:416/2330 train_time:23931ms step_avg:57.53ms
step:417/2330 train_time:23987ms step_avg:57.52ms
step:418/2330 train_time:24047ms step_avg:57.53ms
step:419/2330 train_time:24103ms step_avg:57.52ms
step:420/2330 train_time:24162ms step_avg:57.53ms
step:421/2330 train_time:24218ms step_avg:57.53ms
step:422/2330 train_time:24278ms step_avg:57.53ms
step:423/2330 train_time:24334ms step_avg:57.53ms
step:424/2330 train_time:24393ms step_avg:57.53ms
step:425/2330 train_time:24449ms step_avg:57.53ms
step:426/2330 train_time:24507ms step_avg:57.53ms
step:427/2330 train_time:24564ms step_avg:57.53ms
step:428/2330 train_time:24623ms step_avg:57.53ms
step:429/2330 train_time:24679ms step_avg:57.53ms
step:430/2330 train_time:24739ms step_avg:57.53ms
step:431/2330 train_time:24795ms step_avg:57.53ms
step:432/2330 train_time:24854ms step_avg:57.53ms
step:433/2330 train_time:24909ms step_avg:57.53ms
step:434/2330 train_time:24969ms step_avg:57.53ms
step:435/2330 train_time:25025ms step_avg:57.53ms
step:436/2330 train_time:25085ms step_avg:57.53ms
step:437/2330 train_time:25141ms step_avg:57.53ms
step:438/2330 train_time:25201ms step_avg:57.54ms
step:439/2330 train_time:25257ms step_avg:57.53ms
step:440/2330 train_time:25317ms step_avg:57.54ms
step:441/2330 train_time:25372ms step_avg:57.53ms
step:442/2330 train_time:25432ms step_avg:57.54ms
step:443/2330 train_time:25487ms step_avg:57.53ms
step:444/2330 train_time:25547ms step_avg:57.54ms
step:445/2330 train_time:25603ms step_avg:57.54ms
step:446/2330 train_time:25664ms step_avg:57.54ms
step:447/2330 train_time:25720ms step_avg:57.54ms
step:448/2330 train_time:25780ms step_avg:57.54ms
step:449/2330 train_time:25835ms step_avg:57.54ms
step:450/2330 train_time:25895ms step_avg:57.54ms
step:451/2330 train_time:25950ms step_avg:57.54ms
step:452/2330 train_time:26010ms step_avg:57.54ms
step:453/2330 train_time:26065ms step_avg:57.54ms
step:454/2330 train_time:26126ms step_avg:57.55ms
step:455/2330 train_time:26183ms step_avg:57.55ms
step:456/2330 train_time:26242ms step_avg:57.55ms
step:457/2330 train_time:26299ms step_avg:57.55ms
step:458/2330 train_time:26358ms step_avg:57.55ms
step:459/2330 train_time:26415ms step_avg:57.55ms
step:460/2330 train_time:26473ms step_avg:57.55ms
step:461/2330 train_time:26529ms step_avg:57.55ms
step:462/2330 train_time:26588ms step_avg:57.55ms
step:463/2330 train_time:26644ms step_avg:57.55ms
step:464/2330 train_time:26704ms step_avg:57.55ms
step:465/2330 train_time:26760ms step_avg:57.55ms
step:466/2330 train_time:26820ms step_avg:57.55ms
step:467/2330 train_time:26876ms step_avg:57.55ms
step:468/2330 train_time:26935ms step_avg:57.55ms
step:469/2330 train_time:26992ms step_avg:57.55ms
step:470/2330 train_time:27050ms step_avg:57.55ms
step:471/2330 train_time:27106ms step_avg:57.55ms
step:472/2330 train_time:27166ms step_avg:57.55ms
step:473/2330 train_time:27221ms step_avg:57.55ms
step:474/2330 train_time:27281ms step_avg:57.56ms
step:475/2330 train_time:27338ms step_avg:57.55ms
step:476/2330 train_time:27396ms step_avg:57.56ms
step:477/2330 train_time:27453ms step_avg:57.55ms
step:478/2330 train_time:27511ms step_avg:57.55ms
step:479/2330 train_time:27567ms step_avg:57.55ms
step:480/2330 train_time:27628ms step_avg:57.56ms
step:481/2330 train_time:27683ms step_avg:57.55ms
step:482/2330 train_time:27744ms step_avg:57.56ms
step:483/2330 train_time:27800ms step_avg:57.56ms
step:484/2330 train_time:27860ms step_avg:57.56ms
step:485/2330 train_time:27916ms step_avg:57.56ms
step:486/2330 train_time:27974ms step_avg:57.56ms
step:487/2330 train_time:28031ms step_avg:57.56ms
step:488/2330 train_time:28089ms step_avg:57.56ms
step:489/2330 train_time:28145ms step_avg:57.56ms
step:490/2330 train_time:28205ms step_avg:57.56ms
step:491/2330 train_time:28261ms step_avg:57.56ms
step:492/2330 train_time:28321ms step_avg:57.56ms
step:493/2330 train_time:28376ms step_avg:57.56ms
step:494/2330 train_time:28435ms step_avg:57.56ms
step:495/2330 train_time:28491ms step_avg:57.56ms
step:496/2330 train_time:28550ms step_avg:57.56ms
step:497/2330 train_time:28606ms step_avg:57.56ms
step:498/2330 train_time:28666ms step_avg:57.56ms
step:499/2330 train_time:28724ms step_avg:57.56ms
step:500/2330 train_time:28783ms step_avg:57.57ms
step:500/2330 val_loss:4.4087 train_time:28862ms step_avg:57.72ms
step:501/2330 train_time:28880ms step_avg:57.65ms
step:502/2330 train_time:28901ms step_avg:57.57ms
step:503/2330 train_time:28957ms step_avg:57.57ms
step:504/2330 train_time:29020ms step_avg:57.58ms
step:505/2330 train_time:29076ms step_avg:57.58ms
step:506/2330 train_time:29136ms step_avg:57.58ms
step:507/2330 train_time:29192ms step_avg:57.58ms
step:508/2330 train_time:29252ms step_avg:57.58ms
step:509/2330 train_time:29307ms step_avg:57.58ms
step:510/2330 train_time:29367ms step_avg:57.58ms
step:511/2330 train_time:29423ms step_avg:57.58ms
step:512/2330 train_time:29481ms step_avg:57.58ms
step:513/2330 train_time:29536ms step_avg:57.58ms
step:514/2330 train_time:29594ms step_avg:57.58ms
step:515/2330 train_time:29650ms step_avg:57.57ms
step:516/2330 train_time:29709ms step_avg:57.57ms
step:517/2330 train_time:29764ms step_avg:57.57ms
step:518/2330 train_time:29824ms step_avg:57.57ms
step:519/2330 train_time:29880ms step_avg:57.57ms
step:520/2330 train_time:29942ms step_avg:57.58ms
step:521/2330 train_time:29998ms step_avg:57.58ms
step:522/2330 train_time:30059ms step_avg:57.58ms
step:523/2330 train_time:30115ms step_avg:57.58ms
step:524/2330 train_time:30176ms step_avg:57.59ms
step:525/2330 train_time:30231ms step_avg:57.58ms
step:526/2330 train_time:30290ms step_avg:57.59ms
step:527/2330 train_time:30346ms step_avg:57.58ms
step:528/2330 train_time:30406ms step_avg:57.59ms
step:529/2330 train_time:30462ms step_avg:57.58ms
step:530/2330 train_time:30521ms step_avg:57.59ms
step:531/2330 train_time:30577ms step_avg:57.58ms
step:532/2330 train_time:30635ms step_avg:57.59ms
step:533/2330 train_time:30691ms step_avg:57.58ms
step:534/2330 train_time:30749ms step_avg:57.58ms
step:535/2330 train_time:30805ms step_avg:57.58ms
step:536/2330 train_time:30866ms step_avg:57.59ms
step:537/2330 train_time:30923ms step_avg:57.58ms
step:538/2330 train_time:30983ms step_avg:57.59ms
step:539/2330 train_time:31040ms step_avg:57.59ms
step:540/2330 train_time:31100ms step_avg:57.59ms
step:541/2330 train_time:31157ms step_avg:57.59ms
step:542/2330 train_time:31216ms step_avg:57.59ms
step:543/2330 train_time:31272ms step_avg:57.59ms
step:544/2330 train_time:31331ms step_avg:57.59ms
step:545/2330 train_time:31387ms step_avg:57.59ms
step:546/2330 train_time:31447ms step_avg:57.60ms
step:547/2330 train_time:31503ms step_avg:57.59ms
step:548/2330 train_time:31562ms step_avg:57.59ms
step:549/2330 train_time:31617ms step_avg:57.59ms
step:550/2330 train_time:31676ms step_avg:57.59ms
step:551/2330 train_time:31732ms step_avg:57.59ms
step:552/2330 train_time:31791ms step_avg:57.59ms
step:553/2330 train_time:31846ms step_avg:57.59ms
step:554/2330 train_time:31906ms step_avg:57.59ms
step:555/2330 train_time:31962ms step_avg:57.59ms
step:556/2330 train_time:32023ms step_avg:57.60ms
step:557/2330 train_time:32080ms step_avg:57.59ms
step:558/2330 train_time:32140ms step_avg:57.60ms
step:559/2330 train_time:32197ms step_avg:57.60ms
step:560/2330 train_time:32256ms step_avg:57.60ms
step:561/2330 train_time:32312ms step_avg:57.60ms
step:562/2330 train_time:32371ms step_avg:57.60ms
step:563/2330 train_time:32426ms step_avg:57.60ms
step:564/2330 train_time:32487ms step_avg:57.60ms
step:565/2330 train_time:32542ms step_avg:57.60ms
step:566/2330 train_time:32602ms step_avg:57.60ms
step:567/2330 train_time:32658ms step_avg:57.60ms
step:568/2330 train_time:32717ms step_avg:57.60ms
step:569/2330 train_time:32773ms step_avg:57.60ms
step:570/2330 train_time:32832ms step_avg:57.60ms
step:571/2330 train_time:32888ms step_avg:57.60ms
step:572/2330 train_time:32947ms step_avg:57.60ms
step:573/2330 train_time:33004ms step_avg:57.60ms
step:574/2330 train_time:33064ms step_avg:57.60ms
step:575/2330 train_time:33120ms step_avg:57.60ms
step:576/2330 train_time:33180ms step_avg:57.60ms
step:577/2330 train_time:33236ms step_avg:57.60ms
step:578/2330 train_time:33295ms step_avg:57.60ms
step:579/2330 train_time:33351ms step_avg:57.60ms
step:580/2330 train_time:33410ms step_avg:57.60ms
step:581/2330 train_time:33466ms step_avg:57.60ms
step:582/2330 train_time:33527ms step_avg:57.61ms
step:583/2330 train_time:33583ms step_avg:57.60ms
step:584/2330 train_time:33642ms step_avg:57.61ms
step:585/2330 train_time:33698ms step_avg:57.60ms
step:586/2330 train_time:33757ms step_avg:57.61ms
step:587/2330 train_time:33812ms step_avg:57.60ms
step:588/2330 train_time:33872ms step_avg:57.61ms
step:589/2330 train_time:33928ms step_avg:57.60ms
step:590/2330 train_time:33989ms step_avg:57.61ms
step:591/2330 train_time:34045ms step_avg:57.61ms
step:592/2330 train_time:34105ms step_avg:57.61ms
step:593/2330 train_time:34161ms step_avg:57.61ms
step:594/2330 train_time:34222ms step_avg:57.61ms
step:595/2330 train_time:34278ms step_avg:57.61ms
step:596/2330 train_time:34337ms step_avg:57.61ms
step:597/2330 train_time:34392ms step_avg:57.61ms
step:598/2330 train_time:34451ms step_avg:57.61ms
step:599/2330 train_time:34507ms step_avg:57.61ms
step:600/2330 train_time:34567ms step_avg:57.61ms
step:601/2330 train_time:34623ms step_avg:57.61ms
step:602/2330 train_time:34683ms step_avg:57.61ms
step:603/2330 train_time:34739ms step_avg:57.61ms
step:604/2330 train_time:34798ms step_avg:57.61ms
step:605/2330 train_time:34854ms step_avg:57.61ms
step:606/2330 train_time:34913ms step_avg:57.61ms
step:607/2330 train_time:34969ms step_avg:57.61ms
step:608/2330 train_time:35028ms step_avg:57.61ms
step:609/2330 train_time:35084ms step_avg:57.61ms
step:610/2330 train_time:35144ms step_avg:57.61ms
step:611/2330 train_time:35201ms step_avg:57.61ms
step:612/2330 train_time:35260ms step_avg:57.61ms
step:613/2330 train_time:35316ms step_avg:57.61ms
step:614/2330 train_time:35374ms step_avg:57.61ms
step:615/2330 train_time:35430ms step_avg:57.61ms
step:616/2330 train_time:35489ms step_avg:57.61ms
step:617/2330 train_time:35546ms step_avg:57.61ms
step:618/2330 train_time:35606ms step_avg:57.61ms
step:619/2330 train_time:35661ms step_avg:57.61ms
step:620/2330 train_time:35721ms step_avg:57.61ms
step:621/2330 train_time:35777ms step_avg:57.61ms
step:622/2330 train_time:35836ms step_avg:57.61ms
step:623/2330 train_time:35891ms step_avg:57.61ms
step:624/2330 train_time:35951ms step_avg:57.61ms
step:625/2330 train_time:36006ms step_avg:57.61ms
step:626/2330 train_time:36066ms step_avg:57.61ms
step:627/2330 train_time:36122ms step_avg:57.61ms
step:628/2330 train_time:36183ms step_avg:57.62ms
step:629/2330 train_time:36239ms step_avg:57.61ms
step:630/2330 train_time:36299ms step_avg:57.62ms
step:631/2330 train_time:36355ms step_avg:57.62ms
step:632/2330 train_time:36414ms step_avg:57.62ms
step:633/2330 train_time:36469ms step_avg:57.61ms
step:634/2330 train_time:36529ms step_avg:57.62ms
step:635/2330 train_time:36584ms step_avg:57.61ms
step:636/2330 train_time:36645ms step_avg:57.62ms
step:637/2330 train_time:36702ms step_avg:57.62ms
step:638/2330 train_time:36762ms step_avg:57.62ms
step:639/2330 train_time:36818ms step_avg:57.62ms
step:640/2330 train_time:36876ms step_avg:57.62ms
step:641/2330 train_time:36932ms step_avg:57.62ms
step:642/2330 train_time:36991ms step_avg:57.62ms
step:643/2330 train_time:37047ms step_avg:57.62ms
step:644/2330 train_time:37107ms step_avg:57.62ms
step:645/2330 train_time:37163ms step_avg:57.62ms
step:646/2330 train_time:37224ms step_avg:57.62ms
step:647/2330 train_time:37280ms step_avg:57.62ms
step:648/2330 train_time:37340ms step_avg:57.62ms
step:649/2330 train_time:37396ms step_avg:57.62ms
step:650/2330 train_time:37455ms step_avg:57.62ms
step:651/2330 train_time:37511ms step_avg:57.62ms
step:652/2330 train_time:37570ms step_avg:57.62ms
step:653/2330 train_time:37626ms step_avg:57.62ms
step:654/2330 train_time:37685ms step_avg:57.62ms
step:655/2330 train_time:37742ms step_avg:57.62ms
step:656/2330 train_time:37801ms step_avg:57.62ms
step:657/2330 train_time:37857ms step_avg:57.62ms
step:658/2330 train_time:37916ms step_avg:57.62ms
step:659/2330 train_time:37972ms step_avg:57.62ms
step:660/2330 train_time:38031ms step_avg:57.62ms
step:661/2330 train_time:38087ms step_avg:57.62ms
step:662/2330 train_time:38147ms step_avg:57.62ms
step:663/2330 train_time:38203ms step_avg:57.62ms
step:664/2330 train_time:38263ms step_avg:57.63ms
step:665/2330 train_time:38319ms step_avg:57.62ms
step:666/2330 train_time:38378ms step_avg:57.63ms
step:667/2330 train_time:38434ms step_avg:57.62ms
step:668/2330 train_time:38493ms step_avg:57.62ms
step:669/2330 train_time:38549ms step_avg:57.62ms
step:670/2330 train_time:38608ms step_avg:57.62ms
step:671/2330 train_time:38664ms step_avg:57.62ms
step:672/2330 train_time:38724ms step_avg:57.63ms
step:673/2330 train_time:38781ms step_avg:57.62ms
step:674/2330 train_time:38839ms step_avg:57.63ms
step:675/2330 train_time:38895ms step_avg:57.62ms
step:676/2330 train_time:38955ms step_avg:57.63ms
step:677/2330 train_time:39010ms step_avg:57.62ms
step:678/2330 train_time:39070ms step_avg:57.63ms
step:679/2330 train_time:39126ms step_avg:57.62ms
step:680/2330 train_time:39187ms step_avg:57.63ms
step:681/2330 train_time:39243ms step_avg:57.62ms
step:682/2330 train_time:39303ms step_avg:57.63ms
step:683/2330 train_time:39359ms step_avg:57.63ms
step:684/2330 train_time:39418ms step_avg:57.63ms
step:685/2330 train_time:39475ms step_avg:57.63ms
step:686/2330 train_time:39533ms step_avg:57.63ms
step:687/2330 train_time:39590ms step_avg:57.63ms
step:688/2330 train_time:39649ms step_avg:57.63ms
step:689/2330 train_time:39705ms step_avg:57.63ms
step:690/2330 train_time:39765ms step_avg:57.63ms
step:691/2330 train_time:39821ms step_avg:57.63ms
step:692/2330 train_time:39880ms step_avg:57.63ms
step:693/2330 train_time:39937ms step_avg:57.63ms
step:694/2330 train_time:39996ms step_avg:57.63ms
step:695/2330 train_time:40052ms step_avg:57.63ms
step:696/2330 train_time:40110ms step_avg:57.63ms
step:697/2330 train_time:40166ms step_avg:57.63ms
step:698/2330 train_time:40227ms step_avg:57.63ms
step:699/2330 train_time:40283ms step_avg:57.63ms
step:700/2330 train_time:40343ms step_avg:57.63ms
step:701/2330 train_time:40399ms step_avg:57.63ms
step:702/2330 train_time:40458ms step_avg:57.63ms
step:703/2330 train_time:40514ms step_avg:57.63ms
step:704/2330 train_time:40572ms step_avg:57.63ms
step:705/2330 train_time:40628ms step_avg:57.63ms
step:706/2330 train_time:40688ms step_avg:57.63ms
step:707/2330 train_time:40745ms step_avg:57.63ms
step:708/2330 train_time:40804ms step_avg:57.63ms
step:709/2330 train_time:40860ms step_avg:57.63ms
step:710/2330 train_time:40920ms step_avg:57.63ms
step:711/2330 train_time:40976ms step_avg:57.63ms
step:712/2330 train_time:41034ms step_avg:57.63ms
step:713/2330 train_time:41090ms step_avg:57.63ms
step:714/2330 train_time:41150ms step_avg:57.63ms
step:715/2330 train_time:41206ms step_avg:57.63ms
step:716/2330 train_time:41266ms step_avg:57.63ms
step:717/2330 train_time:41323ms step_avg:57.63ms
step:718/2330 train_time:41382ms step_avg:57.63ms
step:719/2330 train_time:41438ms step_avg:57.63ms
step:720/2330 train_time:41496ms step_avg:57.63ms
step:721/2330 train_time:41553ms step_avg:57.63ms
step:722/2330 train_time:41611ms step_avg:57.63ms
step:723/2330 train_time:41666ms step_avg:57.63ms
step:724/2330 train_time:41727ms step_avg:57.63ms
step:725/2330 train_time:41785ms step_avg:57.63ms
step:726/2330 train_time:41844ms step_avg:57.64ms
step:727/2330 train_time:41900ms step_avg:57.63ms
step:728/2330 train_time:41959ms step_avg:57.64ms
step:729/2330 train_time:42016ms step_avg:57.63ms
step:730/2330 train_time:42074ms step_avg:57.64ms
step:731/2330 train_time:42131ms step_avg:57.63ms
step:732/2330 train_time:42190ms step_avg:57.64ms
step:733/2330 train_time:42246ms step_avg:57.63ms
step:734/2330 train_time:42306ms step_avg:57.64ms
step:735/2330 train_time:42362ms step_avg:57.64ms
step:736/2330 train_time:42421ms step_avg:57.64ms
step:737/2330 train_time:42477ms step_avg:57.64ms
step:738/2330 train_time:42536ms step_avg:57.64ms
step:739/2330 train_time:42592ms step_avg:57.64ms
step:740/2330 train_time:42651ms step_avg:57.64ms
step:741/2330 train_time:42707ms step_avg:57.63ms
step:742/2330 train_time:42767ms step_avg:57.64ms
step:743/2330 train_time:42823ms step_avg:57.64ms
step:744/2330 train_time:42883ms step_avg:57.64ms
step:745/2330 train_time:42940ms step_avg:57.64ms
step:746/2330 train_time:43000ms step_avg:57.64ms
step:747/2330 train_time:43056ms step_avg:57.64ms
step:748/2330 train_time:43115ms step_avg:57.64ms
step:749/2330 train_time:43171ms step_avg:57.64ms
step:750/2330 train_time:43231ms step_avg:57.64ms
step:750/2330 val_loss:4.2205 train_time:43313ms step_avg:57.75ms
step:751/2330 train_time:43331ms step_avg:57.70ms
step:752/2330 train_time:43351ms step_avg:57.65ms
step:753/2330 train_time:43411ms step_avg:57.65ms
step:754/2330 train_time:43476ms step_avg:57.66ms
step:755/2330 train_time:43534ms step_avg:57.66ms
step:756/2330 train_time:43596ms step_avg:57.67ms
step:757/2330 train_time:43651ms step_avg:57.66ms
step:758/2330 train_time:43712ms step_avg:57.67ms
step:759/2330 train_time:43768ms step_avg:57.66ms
step:760/2330 train_time:43826ms step_avg:57.67ms
step:761/2330 train_time:43882ms step_avg:57.66ms
step:762/2330 train_time:43940ms step_avg:57.66ms
step:763/2330 train_time:43996ms step_avg:57.66ms
step:764/2330 train_time:44054ms step_avg:57.66ms
step:765/2330 train_time:44111ms step_avg:57.66ms
step:766/2330 train_time:44169ms step_avg:57.66ms
step:767/2330 train_time:44225ms step_avg:57.66ms
step:768/2330 train_time:44285ms step_avg:57.66ms
step:769/2330 train_time:44342ms step_avg:57.66ms
step:770/2330 train_time:44404ms step_avg:57.67ms
step:771/2330 train_time:44462ms step_avg:57.67ms
step:772/2330 train_time:44522ms step_avg:57.67ms
step:773/2330 train_time:44580ms step_avg:57.67ms
step:774/2330 train_time:44640ms step_avg:57.67ms
step:775/2330 train_time:44697ms step_avg:57.67ms
step:776/2330 train_time:44757ms step_avg:57.68ms
step:777/2330 train_time:44813ms step_avg:57.67ms
step:778/2330 train_time:44874ms step_avg:57.68ms
step:779/2330 train_time:44931ms step_avg:57.68ms
step:780/2330 train_time:44991ms step_avg:57.68ms
step:781/2330 train_time:45047ms step_avg:57.68ms
step:782/2330 train_time:45107ms step_avg:57.68ms
step:783/2330 train_time:45163ms step_avg:57.68ms
step:784/2330 train_time:45222ms step_avg:57.68ms
step:785/2330 train_time:45279ms step_avg:57.68ms
step:786/2330 train_time:45339ms step_avg:57.68ms
step:787/2330 train_time:45396ms step_avg:57.68ms
step:788/2330 train_time:45457ms step_avg:57.69ms
step:789/2330 train_time:45514ms step_avg:57.69ms
step:790/2330 train_time:45576ms step_avg:57.69ms
step:791/2330 train_time:45633ms step_avg:57.69ms
step:792/2330 train_time:45694ms step_avg:57.69ms
step:793/2330 train_time:45750ms step_avg:57.69ms
step:794/2330 train_time:45811ms step_avg:57.70ms
step:795/2330 train_time:45868ms step_avg:57.70ms
step:796/2330 train_time:45928ms step_avg:57.70ms
step:797/2330 train_time:45985ms step_avg:57.70ms
step:798/2330 train_time:46044ms step_avg:57.70ms
step:799/2330 train_time:46101ms step_avg:57.70ms
step:800/2330 train_time:46160ms step_avg:57.70ms
step:801/2330 train_time:46216ms step_avg:57.70ms
step:802/2330 train_time:46277ms step_avg:57.70ms
step:803/2330 train_time:46334ms step_avg:57.70ms
step:804/2330 train_time:46395ms step_avg:57.70ms
step:805/2330 train_time:46451ms step_avg:57.70ms
step:806/2330 train_time:46512ms step_avg:57.71ms
step:807/2330 train_time:46570ms step_avg:57.71ms
step:808/2330 train_time:46630ms step_avg:57.71ms
step:809/2330 train_time:46688ms step_avg:57.71ms
step:810/2330 train_time:46747ms step_avg:57.71ms
step:811/2330 train_time:46805ms step_avg:57.71ms
step:812/2330 train_time:46864ms step_avg:57.71ms
step:813/2330 train_time:46921ms step_avg:57.71ms
step:814/2330 train_time:46980ms step_avg:57.71ms
step:815/2330 train_time:47036ms step_avg:57.71ms
step:816/2330 train_time:47097ms step_avg:57.72ms
step:817/2330 train_time:47153ms step_avg:57.71ms
step:818/2330 train_time:47215ms step_avg:57.72ms
step:819/2330 train_time:47271ms step_avg:57.72ms
step:820/2330 train_time:47333ms step_avg:57.72ms
step:821/2330 train_time:47391ms step_avg:57.72ms
step:822/2330 train_time:47451ms step_avg:57.73ms
step:823/2330 train_time:47508ms step_avg:57.73ms
step:824/2330 train_time:47568ms step_avg:57.73ms
step:825/2330 train_time:47626ms step_avg:57.73ms
step:826/2330 train_time:47686ms step_avg:57.73ms
step:827/2330 train_time:47744ms step_avg:57.73ms
step:828/2330 train_time:47804ms step_avg:57.73ms
step:829/2330 train_time:47860ms step_avg:57.73ms
step:830/2330 train_time:47919ms step_avg:57.73ms
step:831/2330 train_time:47976ms step_avg:57.73ms
step:832/2330 train_time:48035ms step_avg:57.73ms
step:833/2330 train_time:48092ms step_avg:57.73ms
step:834/2330 train_time:48152ms step_avg:57.74ms
step:835/2330 train_time:48208ms step_avg:57.73ms
step:836/2330 train_time:48269ms step_avg:57.74ms
step:837/2330 train_time:48326ms step_avg:57.74ms
step:838/2330 train_time:48387ms step_avg:57.74ms
step:839/2330 train_time:48444ms step_avg:57.74ms
step:840/2330 train_time:48503ms step_avg:57.74ms
step:841/2330 train_time:48560ms step_avg:57.74ms
step:842/2330 train_time:48620ms step_avg:57.74ms
step:843/2330 train_time:48677ms step_avg:57.74ms
step:844/2330 train_time:48738ms step_avg:57.75ms
step:845/2330 train_time:48794ms step_avg:57.74ms
step:846/2330 train_time:48857ms step_avg:57.75ms
step:847/2330 train_time:48914ms step_avg:57.75ms
step:848/2330 train_time:48974ms step_avg:57.75ms
step:849/2330 train_time:49032ms step_avg:57.75ms
step:850/2330 train_time:49091ms step_avg:57.75ms
step:851/2330 train_time:49147ms step_avg:57.75ms
step:852/2330 train_time:49207ms step_avg:57.76ms
step:853/2330 train_time:49265ms step_avg:57.75ms
step:854/2330 train_time:49325ms step_avg:57.76ms
step:855/2330 train_time:49382ms step_avg:57.76ms
step:856/2330 train_time:49441ms step_avg:57.76ms
step:857/2330 train_time:49498ms step_avg:57.76ms
step:858/2330 train_time:49558ms step_avg:57.76ms
step:859/2330 train_time:49615ms step_avg:57.76ms
step:860/2330 train_time:49675ms step_avg:57.76ms
step:861/2330 train_time:49733ms step_avg:57.76ms
step:862/2330 train_time:49794ms step_avg:57.77ms
step:863/2330 train_time:49851ms step_avg:57.76ms
step:864/2330 train_time:49911ms step_avg:57.77ms
step:865/2330 train_time:49969ms step_avg:57.77ms
step:866/2330 train_time:50028ms step_avg:57.77ms
step:867/2330 train_time:50085ms step_avg:57.77ms
step:868/2330 train_time:50144ms step_avg:57.77ms
step:869/2330 train_time:50201ms step_avg:57.77ms
step:870/2330 train_time:50260ms step_avg:57.77ms
step:871/2330 train_time:50317ms step_avg:57.77ms
step:872/2330 train_time:50378ms step_avg:57.77ms
step:873/2330 train_time:50435ms step_avg:57.77ms
step:874/2330 train_time:50495ms step_avg:57.77ms
step:875/2330 train_time:50552ms step_avg:57.77ms
step:876/2330 train_time:50612ms step_avg:57.78ms
step:877/2330 train_time:50670ms step_avg:57.78ms
step:878/2330 train_time:50730ms step_avg:57.78ms
step:879/2330 train_time:50787ms step_avg:57.78ms
step:880/2330 train_time:50847ms step_avg:57.78ms
step:881/2330 train_time:50905ms step_avg:57.78ms
step:882/2330 train_time:50964ms step_avg:57.78ms
step:883/2330 train_time:51020ms step_avg:57.78ms
step:884/2330 train_time:51080ms step_avg:57.78ms
step:885/2330 train_time:51137ms step_avg:57.78ms
step:886/2330 train_time:51197ms step_avg:57.78ms
step:887/2330 train_time:51252ms step_avg:57.78ms
step:888/2330 train_time:51315ms step_avg:57.79ms
step:889/2330 train_time:51372ms step_avg:57.79ms
step:890/2330 train_time:51432ms step_avg:57.79ms
step:891/2330 train_time:51490ms step_avg:57.79ms
step:892/2330 train_time:51549ms step_avg:57.79ms
step:893/2330 train_time:51606ms step_avg:57.79ms
step:894/2330 train_time:51666ms step_avg:57.79ms
step:895/2330 train_time:51723ms step_avg:57.79ms
step:896/2330 train_time:51783ms step_avg:57.79ms
step:897/2330 train_time:51839ms step_avg:57.79ms
step:898/2330 train_time:51900ms step_avg:57.79ms
step:899/2330 train_time:51956ms step_avg:57.79ms
step:900/2330 train_time:52017ms step_avg:57.80ms
step:901/2330 train_time:52074ms step_avg:57.80ms
step:902/2330 train_time:52134ms step_avg:57.80ms
step:903/2330 train_time:52190ms step_avg:57.80ms
step:904/2330 train_time:52251ms step_avg:57.80ms
step:905/2330 train_time:52308ms step_avg:57.80ms
step:906/2330 train_time:52368ms step_avg:57.80ms
step:907/2330 train_time:52426ms step_avg:57.80ms
step:908/2330 train_time:52486ms step_avg:57.80ms
step:909/2330 train_time:52544ms step_avg:57.80ms
step:910/2330 train_time:52602ms step_avg:57.80ms
step:911/2330 train_time:52659ms step_avg:57.80ms
step:912/2330 train_time:52720ms step_avg:57.81ms
step:913/2330 train_time:52776ms step_avg:57.81ms
step:914/2330 train_time:52838ms step_avg:57.81ms
step:915/2330 train_time:52895ms step_avg:57.81ms
step:916/2330 train_time:52955ms step_avg:57.81ms
step:917/2330 train_time:53011ms step_avg:57.81ms
step:918/2330 train_time:53072ms step_avg:57.81ms
step:919/2330 train_time:53128ms step_avg:57.81ms
step:920/2330 train_time:53189ms step_avg:57.81ms
step:921/2330 train_time:53246ms step_avg:57.81ms
step:922/2330 train_time:53307ms step_avg:57.82ms
step:923/2330 train_time:53363ms step_avg:57.82ms
step:924/2330 train_time:53424ms step_avg:57.82ms
step:925/2330 train_time:53481ms step_avg:57.82ms
step:926/2330 train_time:53540ms step_avg:57.82ms
step:927/2330 train_time:53597ms step_avg:57.82ms
step:928/2330 train_time:53656ms step_avg:57.82ms
step:929/2330 train_time:53713ms step_avg:57.82ms
step:930/2330 train_time:53774ms step_avg:57.82ms
step:931/2330 train_time:53831ms step_avg:57.82ms
step:932/2330 train_time:53893ms step_avg:57.82ms
step:933/2330 train_time:53950ms step_avg:57.82ms
step:934/2330 train_time:54010ms step_avg:57.83ms
step:935/2330 train_time:54067ms step_avg:57.83ms
step:936/2330 train_time:54127ms step_avg:57.83ms
step:937/2330 train_time:54184ms step_avg:57.83ms
step:938/2330 train_time:54243ms step_avg:57.83ms
step:939/2330 train_time:54300ms step_avg:57.83ms
step:940/2330 train_time:54360ms step_avg:57.83ms
step:941/2330 train_time:54416ms step_avg:57.83ms
step:942/2330 train_time:54477ms step_avg:57.83ms
step:943/2330 train_time:54534ms step_avg:57.83ms
step:944/2330 train_time:54595ms step_avg:57.83ms
step:945/2330 train_time:54652ms step_avg:57.83ms
step:946/2330 train_time:54711ms step_avg:57.83ms
step:947/2330 train_time:54768ms step_avg:57.83ms
step:948/2330 train_time:54830ms step_avg:57.84ms
step:949/2330 train_time:54887ms step_avg:57.84ms
step:950/2330 train_time:54947ms step_avg:57.84ms
step:951/2330 train_time:55003ms step_avg:57.84ms
step:952/2330 train_time:55063ms step_avg:57.84ms
step:953/2330 train_time:55119ms step_avg:57.84ms
step:954/2330 train_time:55180ms step_avg:57.84ms
step:955/2330 train_time:55237ms step_avg:57.84ms
step:956/2330 train_time:55297ms step_avg:57.84ms
step:957/2330 train_time:55353ms step_avg:57.84ms
step:958/2330 train_time:55416ms step_avg:57.85ms
step:959/2330 train_time:55473ms step_avg:57.84ms
step:960/2330 train_time:55533ms step_avg:57.85ms
step:961/2330 train_time:55590ms step_avg:57.85ms
step:962/2330 train_time:55651ms step_avg:57.85ms
step:963/2330 train_time:55708ms step_avg:57.85ms
step:964/2330 train_time:55768ms step_avg:57.85ms
step:965/2330 train_time:55825ms step_avg:57.85ms
step:966/2330 train_time:55886ms step_avg:57.85ms
step:967/2330 train_time:55942ms step_avg:57.85ms
step:968/2330 train_time:56003ms step_avg:57.85ms
step:969/2330 train_time:56060ms step_avg:57.85ms
step:970/2330 train_time:56120ms step_avg:57.86ms
step:971/2330 train_time:56177ms step_avg:57.85ms
step:972/2330 train_time:56237ms step_avg:57.86ms
step:973/2330 train_time:56294ms step_avg:57.86ms
step:974/2330 train_time:56355ms step_avg:57.86ms
step:975/2330 train_time:56411ms step_avg:57.86ms
step:976/2330 train_time:56472ms step_avg:57.86ms
step:977/2330 train_time:56529ms step_avg:57.86ms
step:978/2330 train_time:56588ms step_avg:57.86ms
step:979/2330 train_time:56646ms step_avg:57.86ms
step:980/2330 train_time:56705ms step_avg:57.86ms
step:981/2330 train_time:56760ms step_avg:57.86ms
step:982/2330 train_time:56821ms step_avg:57.86ms
step:983/2330 train_time:56878ms step_avg:57.86ms
step:984/2330 train_time:56939ms step_avg:57.86ms
step:985/2330 train_time:56996ms step_avg:57.86ms
step:986/2330 train_time:57056ms step_avg:57.87ms
step:987/2330 train_time:57113ms step_avg:57.87ms
step:988/2330 train_time:57174ms step_avg:57.87ms
step:989/2330 train_time:57231ms step_avg:57.87ms
step:990/2330 train_time:57291ms step_avg:57.87ms
step:991/2330 train_time:57348ms step_avg:57.87ms
step:992/2330 train_time:57409ms step_avg:57.87ms
step:993/2330 train_time:57466ms step_avg:57.87ms
step:994/2330 train_time:57526ms step_avg:57.87ms
step:995/2330 train_time:57583ms step_avg:57.87ms
step:996/2330 train_time:57642ms step_avg:57.87ms
step:997/2330 train_time:57699ms step_avg:57.87ms
step:998/2330 train_time:57759ms step_avg:57.87ms
step:999/2330 train_time:57816ms step_avg:57.87ms
step:1000/2330 train_time:57877ms step_avg:57.88ms
step:1000/2330 val_loss:4.0728 train_time:57958ms step_avg:57.96ms
step:1001/2330 train_time:57977ms step_avg:57.92ms
step:1002/2330 train_time:57996ms step_avg:57.88ms
step:1003/2330 train_time:58051ms step_avg:57.88ms
step:1004/2330 train_time:58119ms step_avg:57.89ms
step:1005/2330 train_time:58174ms step_avg:57.88ms
step:1006/2330 train_time:58236ms step_avg:57.89ms
step:1007/2330 train_time:58292ms step_avg:57.89ms
step:1008/2330 train_time:58351ms step_avg:57.89ms
step:1009/2330 train_time:58408ms step_avg:57.89ms
step:1010/2330 train_time:58467ms step_avg:57.89ms
step:1011/2330 train_time:58523ms step_avg:57.89ms
step:1012/2330 train_time:58582ms step_avg:57.89ms
step:1013/2330 train_time:58638ms step_avg:57.89ms
step:1014/2330 train_time:58697ms step_avg:57.89ms
step:1015/2330 train_time:58753ms step_avg:57.88ms
step:1016/2330 train_time:58812ms step_avg:57.89ms
step:1017/2330 train_time:58871ms step_avg:57.89ms
step:1018/2330 train_time:58935ms step_avg:57.89ms
step:1019/2330 train_time:58993ms step_avg:57.89ms
step:1020/2330 train_time:59056ms step_avg:57.90ms
step:1021/2330 train_time:59113ms step_avg:57.90ms
step:1022/2330 train_time:59174ms step_avg:57.90ms
step:1023/2330 train_time:59230ms step_avg:57.90ms
step:1024/2330 train_time:59290ms step_avg:57.90ms
step:1025/2330 train_time:59346ms step_avg:57.90ms
step:1026/2330 train_time:59406ms step_avg:57.90ms
step:1027/2330 train_time:59463ms step_avg:57.90ms
step:1028/2330 train_time:59522ms step_avg:57.90ms
step:1029/2330 train_time:59578ms step_avg:57.90ms
step:1030/2330 train_time:59637ms step_avg:57.90ms
step:1031/2330 train_time:59693ms step_avg:57.90ms
step:1032/2330 train_time:59752ms step_avg:57.90ms
step:1033/2330 train_time:59810ms step_avg:57.90ms
step:1034/2330 train_time:59871ms step_avg:57.90ms
step:1035/2330 train_time:59928ms step_avg:57.90ms
step:1036/2330 train_time:59991ms step_avg:57.91ms
step:1037/2330 train_time:60048ms step_avg:57.91ms
step:1038/2330 train_time:60110ms step_avg:57.91ms
step:1039/2330 train_time:60166ms step_avg:57.91ms
step:1040/2330 train_time:60227ms step_avg:57.91ms
step:1041/2330 train_time:60284ms step_avg:57.91ms
step:1042/2330 train_time:60343ms step_avg:57.91ms
step:1043/2330 train_time:60400ms step_avg:57.91ms
step:1044/2330 train_time:60460ms step_avg:57.91ms
step:1045/2330 train_time:60516ms step_avg:57.91ms
step:1046/2330 train_time:60576ms step_avg:57.91ms
step:1047/2330 train_time:60632ms step_avg:57.91ms
step:1048/2330 train_time:60692ms step_avg:57.91ms
step:1049/2330 train_time:60749ms step_avg:57.91ms
step:1050/2330 train_time:60809ms step_avg:57.91ms
step:1051/2330 train_time:60867ms step_avg:57.91ms
step:1052/2330 train_time:60928ms step_avg:57.92ms
step:1053/2330 train_time:60985ms step_avg:57.92ms
step:1054/2330 train_time:61046ms step_avg:57.92ms
step:1055/2330 train_time:61104ms step_avg:57.92ms
step:1056/2330 train_time:61165ms step_avg:57.92ms
step:1057/2330 train_time:61222ms step_avg:57.92ms
step:1058/2330 train_time:61283ms step_avg:57.92ms
step:1059/2330 train_time:61339ms step_avg:57.92ms
step:1060/2330 train_time:61400ms step_avg:57.92ms
step:1061/2330 train_time:61456ms step_avg:57.92ms
step:1062/2330 train_time:61516ms step_avg:57.92ms
step:1063/2330 train_time:61572ms step_avg:57.92ms
step:1064/2330 train_time:61631ms step_avg:57.92ms
step:1065/2330 train_time:61688ms step_avg:57.92ms
step:1066/2330 train_time:61748ms step_avg:57.92ms
step:1067/2330 train_time:61804ms step_avg:57.92ms
step:1068/2330 train_time:61865ms step_avg:57.93ms
step:1069/2330 train_time:61923ms step_avg:57.93ms
step:1070/2330 train_time:61984ms step_avg:57.93ms
step:1071/2330 train_time:62042ms step_avg:57.93ms
step:1072/2330 train_time:62102ms step_avg:57.93ms
step:1073/2330 train_time:62160ms step_avg:57.93ms
step:1074/2330 train_time:62219ms step_avg:57.93ms
step:1075/2330 train_time:62277ms step_avg:57.93ms
step:1076/2330 train_time:62337ms step_avg:57.93ms
step:1077/2330 train_time:62394ms step_avg:57.93ms
step:1078/2330 train_time:62453ms step_avg:57.93ms
step:1079/2330 train_time:62511ms step_avg:57.93ms
step:1080/2330 train_time:62570ms step_avg:57.93ms
step:1081/2330 train_time:62626ms step_avg:57.93ms
step:1082/2330 train_time:62686ms step_avg:57.94ms
step:1083/2330 train_time:62743ms step_avg:57.93ms
step:1084/2330 train_time:62804ms step_avg:57.94ms
step:1085/2330 train_time:62861ms step_avg:57.94ms
step:1086/2330 train_time:62921ms step_avg:57.94ms
step:1087/2330 train_time:62979ms step_avg:57.94ms
step:1088/2330 train_time:63039ms step_avg:57.94ms
step:1089/2330 train_time:63096ms step_avg:57.94ms
step:1090/2330 train_time:63156ms step_avg:57.94ms
step:1091/2330 train_time:63212ms step_avg:57.94ms
step:1092/2330 train_time:63273ms step_avg:57.94ms
step:1093/2330 train_time:63330ms step_avg:57.94ms
step:1094/2330 train_time:63390ms step_avg:57.94ms
step:1095/2330 train_time:63448ms step_avg:57.94ms
step:1096/2330 train_time:63508ms step_avg:57.95ms
step:1097/2330 train_time:63565ms step_avg:57.94ms
step:1098/2330 train_time:63625ms step_avg:57.95ms
step:1099/2330 train_time:63682ms step_avg:57.95ms
step:1100/2330 train_time:63742ms step_avg:57.95ms
step:1101/2330 train_time:63800ms step_avg:57.95ms
step:1102/2330 train_time:63860ms step_avg:57.95ms
step:1103/2330 train_time:63917ms step_avg:57.95ms
step:1104/2330 train_time:63977ms step_avg:57.95ms
step:1105/2330 train_time:64034ms step_avg:57.95ms
step:1106/2330 train_time:64094ms step_avg:57.95ms
step:1107/2330 train_time:64152ms step_avg:57.95ms
step:1108/2330 train_time:64211ms step_avg:57.95ms
step:1109/2330 train_time:64269ms step_avg:57.95ms
step:1110/2330 train_time:64329ms step_avg:57.95ms
step:1111/2330 train_time:64385ms step_avg:57.95ms
step:1112/2330 train_time:64445ms step_avg:57.95ms
step:1113/2330 train_time:64502ms step_avg:57.95ms
step:1114/2330 train_time:64562ms step_avg:57.96ms
step:1115/2330 train_time:64619ms step_avg:57.95ms
step:1116/2330 train_time:64679ms step_avg:57.96ms
step:1117/2330 train_time:64736ms step_avg:57.96ms
step:1118/2330 train_time:64795ms step_avg:57.96ms
step:1119/2330 train_time:64852ms step_avg:57.96ms
step:1120/2330 train_time:64912ms step_avg:57.96ms
step:1121/2330 train_time:64969ms step_avg:57.96ms
step:1122/2330 train_time:65029ms step_avg:57.96ms
step:1123/2330 train_time:65086ms step_avg:57.96ms
step:1124/2330 train_time:65147ms step_avg:57.96ms
step:1125/2330 train_time:65204ms step_avg:57.96ms
step:1126/2330 train_time:65265ms step_avg:57.96ms
step:1127/2330 train_time:65322ms step_avg:57.96ms
step:1128/2330 train_time:65382ms step_avg:57.96ms
step:1129/2330 train_time:65440ms step_avg:57.96ms
step:1130/2330 train_time:65500ms step_avg:57.96ms
step:1131/2330 train_time:65557ms step_avg:57.96ms
step:1132/2330 train_time:65617ms step_avg:57.97ms
step:1133/2330 train_time:65674ms step_avg:57.96ms
step:1134/2330 train_time:65734ms step_avg:57.97ms
step:1135/2330 train_time:65790ms step_avg:57.97ms
step:1136/2330 train_time:65851ms step_avg:57.97ms
step:1137/2330 train_time:65907ms step_avg:57.97ms
step:1138/2330 train_time:65968ms step_avg:57.97ms
step:1139/2330 train_time:66025ms step_avg:57.97ms
step:1140/2330 train_time:66086ms step_avg:57.97ms
step:1141/2330 train_time:66143ms step_avg:57.97ms
step:1142/2330 train_time:66204ms step_avg:57.97ms
step:1143/2330 train_time:66261ms step_avg:57.97ms
step:1144/2330 train_time:66320ms step_avg:57.97ms
step:1145/2330 train_time:66378ms step_avg:57.97ms
step:1146/2330 train_time:66437ms step_avg:57.97ms
step:1147/2330 train_time:66494ms step_avg:57.97ms
step:1148/2330 train_time:66554ms step_avg:57.97ms
step:1149/2330 train_time:66610ms step_avg:57.97ms
step:1150/2330 train_time:66670ms step_avg:57.97ms
step:1151/2330 train_time:66727ms step_avg:57.97ms
step:1152/2330 train_time:66788ms step_avg:57.98ms
step:1153/2330 train_time:66844ms step_avg:57.97ms
step:1154/2330 train_time:66906ms step_avg:57.98ms
step:1155/2330 train_time:66963ms step_avg:57.98ms
step:1156/2330 train_time:67023ms step_avg:57.98ms
step:1157/2330 train_time:67080ms step_avg:57.98ms
step:1158/2330 train_time:67141ms step_avg:57.98ms
step:1159/2330 train_time:67197ms step_avg:57.98ms
step:1160/2330 train_time:67258ms step_avg:57.98ms
step:1161/2330 train_time:67314ms step_avg:57.98ms
step:1162/2330 train_time:67375ms step_avg:57.98ms
step:1163/2330 train_time:67431ms step_avg:57.98ms
step:1164/2330 train_time:67491ms step_avg:57.98ms
step:1165/2330 train_time:67548ms step_avg:57.98ms
step:1166/2330 train_time:67608ms step_avg:57.98ms
step:1167/2330 train_time:67665ms step_avg:57.98ms
step:1168/2330 train_time:67726ms step_avg:57.98ms
step:1169/2330 train_time:67784ms step_avg:57.98ms
step:1170/2330 train_time:67844ms step_avg:57.99ms
step:1171/2330 train_time:67901ms step_avg:57.99ms
step:1172/2330 train_time:67961ms step_avg:57.99ms
step:1173/2330 train_time:68018ms step_avg:57.99ms
step:1174/2330 train_time:68078ms step_avg:57.99ms
step:1175/2330 train_time:68135ms step_avg:57.99ms
step:1176/2330 train_time:68195ms step_avg:57.99ms
step:1177/2330 train_time:68252ms step_avg:57.99ms
step:1178/2330 train_time:68311ms step_avg:57.99ms
step:1179/2330 train_time:68368ms step_avg:57.99ms
step:1180/2330 train_time:68427ms step_avg:57.99ms
step:1181/2330 train_time:68484ms step_avg:57.99ms
step:1182/2330 train_time:68546ms step_avg:57.99ms
step:1183/2330 train_time:68603ms step_avg:57.99ms
step:1184/2330 train_time:68664ms step_avg:57.99ms
step:1185/2330 train_time:68722ms step_avg:57.99ms
step:1186/2330 train_time:68782ms step_avg:58.00ms
step:1187/2330 train_time:68840ms step_avg:57.99ms
step:1188/2330 train_time:68899ms step_avg:58.00ms
step:1189/2330 train_time:68956ms step_avg:57.99ms
step:1190/2330 train_time:69015ms step_avg:58.00ms
step:1191/2330 train_time:69072ms step_avg:58.00ms
step:1192/2330 train_time:69132ms step_avg:58.00ms
step:1193/2330 train_time:69188ms step_avg:57.99ms
step:1194/2330 train_time:69249ms step_avg:58.00ms
step:1195/2330 train_time:69307ms step_avg:58.00ms
step:1196/2330 train_time:69367ms step_avg:58.00ms
step:1197/2330 train_time:69424ms step_avg:58.00ms
step:1198/2330 train_time:69483ms step_avg:58.00ms
step:1199/2330 train_time:69540ms step_avg:58.00ms
step:1200/2330 train_time:69601ms step_avg:58.00ms
step:1201/2330 train_time:69658ms step_avg:58.00ms
step:1202/2330 train_time:69718ms step_avg:58.00ms
step:1203/2330 train_time:69775ms step_avg:58.00ms
step:1204/2330 train_time:69835ms step_avg:58.00ms
step:1205/2330 train_time:69891ms step_avg:58.00ms
step:1206/2330 train_time:69952ms step_avg:58.00ms
step:1207/2330 train_time:70009ms step_avg:58.00ms
step:1208/2330 train_time:70070ms step_avg:58.01ms
step:1209/2330 train_time:70127ms step_avg:58.00ms
step:1210/2330 train_time:70189ms step_avg:58.01ms
step:1211/2330 train_time:70245ms step_avg:58.01ms
step:1212/2330 train_time:70307ms step_avg:58.01ms
step:1213/2330 train_time:70363ms step_avg:58.01ms
step:1214/2330 train_time:70423ms step_avg:58.01ms
step:1215/2330 train_time:70480ms step_avg:58.01ms
step:1216/2330 train_time:70541ms step_avg:58.01ms
step:1217/2330 train_time:70598ms step_avg:58.01ms
step:1218/2330 train_time:70658ms step_avg:58.01ms
step:1219/2330 train_time:70716ms step_avg:58.01ms
step:1220/2330 train_time:70775ms step_avg:58.01ms
step:1221/2330 train_time:70832ms step_avg:58.01ms
step:1222/2330 train_time:70892ms step_avg:58.01ms
step:1223/2330 train_time:70950ms step_avg:58.01ms
step:1224/2330 train_time:71010ms step_avg:58.01ms
step:1225/2330 train_time:71067ms step_avg:58.01ms
step:1226/2330 train_time:71127ms step_avg:58.02ms
step:1227/2330 train_time:71184ms step_avg:58.01ms
step:1228/2330 train_time:71245ms step_avg:58.02ms
step:1229/2330 train_time:71302ms step_avg:58.02ms
step:1230/2330 train_time:71362ms step_avg:58.02ms
step:1231/2330 train_time:71419ms step_avg:58.02ms
step:1232/2330 train_time:71480ms step_avg:58.02ms
step:1233/2330 train_time:71538ms step_avg:58.02ms
step:1234/2330 train_time:71597ms step_avg:58.02ms
step:1235/2330 train_time:71654ms step_avg:58.02ms
step:1236/2330 train_time:71714ms step_avg:58.02ms
step:1237/2330 train_time:71771ms step_avg:58.02ms
step:1238/2330 train_time:71830ms step_avg:58.02ms
step:1239/2330 train_time:71887ms step_avg:58.02ms
step:1240/2330 train_time:71947ms step_avg:58.02ms
step:1241/2330 train_time:72004ms step_avg:58.02ms
step:1242/2330 train_time:72066ms step_avg:58.02ms
step:1243/2330 train_time:72123ms step_avg:58.02ms
step:1244/2330 train_time:72183ms step_avg:58.02ms
step:1245/2330 train_time:72240ms step_avg:58.02ms
step:1246/2330 train_time:72301ms step_avg:58.03ms
step:1247/2330 train_time:72358ms step_avg:58.03ms
step:1248/2330 train_time:72418ms step_avg:58.03ms
step:1249/2330 train_time:72475ms step_avg:58.03ms
step:1250/2330 train_time:72535ms step_avg:58.03ms
step:1250/2330 val_loss:3.9881 train_time:72615ms step_avg:58.09ms
step:1251/2330 train_time:72633ms step_avg:58.06ms
step:1252/2330 train_time:72654ms step_avg:58.03ms
step:1253/2330 train_time:72715ms step_avg:58.03ms
step:1254/2330 train_time:72778ms step_avg:58.04ms
step:1255/2330 train_time:72835ms step_avg:58.04ms
step:1256/2330 train_time:72897ms step_avg:58.04ms
step:1257/2330 train_time:72953ms step_avg:58.04ms
step:1258/2330 train_time:73013ms step_avg:58.04ms
step:1259/2330 train_time:73070ms step_avg:58.04ms
step:1260/2330 train_time:73130ms step_avg:58.04ms
step:1261/2330 train_time:73186ms step_avg:58.04ms
step:1262/2330 train_time:73245ms step_avg:58.04ms
step:1263/2330 train_time:73302ms step_avg:58.04ms
step:1264/2330 train_time:73361ms step_avg:58.04ms
step:1265/2330 train_time:73417ms step_avg:58.04ms
step:1266/2330 train_time:73477ms step_avg:58.04ms
step:1267/2330 train_time:73533ms step_avg:58.04ms
step:1268/2330 train_time:73594ms step_avg:58.04ms
step:1269/2330 train_time:73652ms step_avg:58.04ms
step:1270/2330 train_time:73715ms step_avg:58.04ms
step:1271/2330 train_time:73773ms step_avg:58.04ms
step:1272/2330 train_time:73836ms step_avg:58.05ms
step:1273/2330 train_time:73893ms step_avg:58.05ms
step:1274/2330 train_time:73955ms step_avg:58.05ms
step:1275/2330 train_time:74011ms step_avg:58.05ms
step:1276/2330 train_time:74073ms step_avg:58.05ms
step:1277/2330 train_time:74129ms step_avg:58.05ms
step:1278/2330 train_time:74190ms step_avg:58.05ms
step:1279/2330 train_time:74247ms step_avg:58.05ms
step:1280/2330 train_time:74307ms step_avg:58.05ms
step:1281/2330 train_time:74363ms step_avg:58.05ms
step:1282/2330 train_time:74423ms step_avg:58.05ms
step:1283/2330 train_time:74479ms step_avg:58.05ms
step:1284/2330 train_time:74539ms step_avg:58.05ms
step:1285/2330 train_time:74596ms step_avg:58.05ms
step:1286/2330 train_time:74657ms step_avg:58.05ms
step:1287/2330 train_time:74714ms step_avg:58.05ms
step:1288/2330 train_time:74776ms step_avg:58.06ms
step:1289/2330 train_time:74833ms step_avg:58.06ms
step:1290/2330 train_time:74895ms step_avg:58.06ms
step:1291/2330 train_time:74951ms step_avg:58.06ms
step:1292/2330 train_time:75014ms step_avg:58.06ms
step:1293/2330 train_time:75071ms step_avg:58.06ms
step:1294/2330 train_time:75132ms step_avg:58.06ms
step:1295/2330 train_time:75188ms step_avg:58.06ms
step:1296/2330 train_time:75250ms step_avg:58.06ms
step:1297/2330 train_time:75306ms step_avg:58.06ms
step:1298/2330 train_time:75365ms step_avg:58.06ms
step:1299/2330 train_time:75422ms step_avg:58.06ms
step:1300/2330 train_time:75482ms step_avg:58.06ms
step:1301/2330 train_time:75538ms step_avg:58.06ms
step:1302/2330 train_time:75598ms step_avg:58.06ms
step:1303/2330 train_time:75655ms step_avg:58.06ms
step:1304/2330 train_time:75716ms step_avg:58.06ms
step:1305/2330 train_time:75773ms step_avg:58.06ms
step:1306/2330 train_time:75835ms step_avg:58.07ms
step:1307/2330 train_time:75892ms step_avg:58.07ms
step:1308/2330 train_time:75953ms step_avg:58.07ms
step:1309/2330 train_time:76011ms step_avg:58.07ms
step:1310/2330 train_time:76071ms step_avg:58.07ms
step:1311/2330 train_time:76128ms step_avg:58.07ms
step:1312/2330 train_time:76188ms step_avg:58.07ms
step:1313/2330 train_time:76244ms step_avg:58.07ms
step:1314/2330 train_time:76304ms step_avg:58.07ms
step:1315/2330 train_time:76361ms step_avg:58.07ms
step:1316/2330 train_time:76420ms step_avg:58.07ms
step:1317/2330 train_time:76477ms step_avg:58.07ms
step:1318/2330 train_time:76537ms step_avg:58.07ms
step:1319/2330 train_time:76594ms step_avg:58.07ms
step:1320/2330 train_time:76656ms step_avg:58.07ms
step:1321/2330 train_time:76713ms step_avg:58.07ms
step:1322/2330 train_time:76774ms step_avg:58.07ms
step:1323/2330 train_time:76831ms step_avg:58.07ms
step:1324/2330 train_time:76892ms step_avg:58.08ms
step:1325/2330 train_time:76949ms step_avg:58.08ms
step:1326/2330 train_time:77010ms step_avg:58.08ms
step:1327/2330 train_time:77067ms step_avg:58.08ms
step:1328/2330 train_time:77127ms step_avg:58.08ms
step:1329/2330 train_time:77184ms step_avg:58.08ms
step:1330/2330 train_time:77244ms step_avg:58.08ms
step:1331/2330 train_time:77302ms step_avg:58.08ms
step:1332/2330 train_time:77361ms step_avg:58.08ms
step:1333/2330 train_time:77418ms step_avg:58.08ms
step:1334/2330 train_time:77478ms step_avg:58.08ms
step:1335/2330 train_time:77534ms step_avg:58.08ms
step:1336/2330 train_time:77594ms step_avg:58.08ms
step:1337/2330 train_time:77651ms step_avg:58.08ms
step:1338/2330 train_time:77712ms step_avg:58.08ms
step:1339/2330 train_time:77769ms step_avg:58.08ms
step:1340/2330 train_time:77829ms step_avg:58.08ms
step:1341/2330 train_time:77887ms step_avg:58.08ms
step:1342/2330 train_time:77947ms step_avg:58.08ms
step:1343/2330 train_time:78004ms step_avg:58.08ms
step:1344/2330 train_time:78064ms step_avg:58.08ms
step:1345/2330 train_time:78120ms step_avg:58.08ms
step:1346/2330 train_time:78182ms step_avg:58.08ms
step:1347/2330 train_time:78238ms step_avg:58.08ms
step:1348/2330 train_time:78299ms step_avg:58.09ms
step:1349/2330 train_time:78355ms step_avg:58.08ms
step:1350/2330 train_time:78416ms step_avg:58.09ms
step:1351/2330 train_time:78473ms step_avg:58.08ms
step:1352/2330 train_time:78532ms step_avg:58.09ms
step:1353/2330 train_time:78590ms step_avg:58.09ms
step:1354/2330 train_time:78650ms step_avg:58.09ms
step:1355/2330 train_time:78707ms step_avg:58.09ms
step:1356/2330 train_time:78768ms step_avg:58.09ms
step:1357/2330 train_time:78825ms step_avg:58.09ms
step:1358/2330 train_time:78884ms step_avg:58.09ms
step:1359/2330 train_time:78942ms step_avg:58.09ms
step:1360/2330 train_time:79002ms step_avg:58.09ms
step:1361/2330 train_time:79059ms step_avg:58.09ms
step:1362/2330 train_time:79118ms step_avg:58.09ms
step:1363/2330 train_time:79175ms step_avg:58.09ms
step:1364/2330 train_time:79236ms step_avg:58.09ms
step:1365/2330 train_time:79293ms step_avg:58.09ms
step:1366/2330 train_time:79354ms step_avg:58.09ms
step:1367/2330 train_time:79410ms step_avg:58.09ms
step:1368/2330 train_time:79471ms step_avg:58.09ms
step:1369/2330 train_time:79528ms step_avg:58.09ms
step:1370/2330 train_time:79588ms step_avg:58.09ms
step:1371/2330 train_time:79645ms step_avg:58.09ms
step:1372/2330 train_time:79705ms step_avg:58.09ms
step:1373/2330 train_time:79762ms step_avg:58.09ms
step:1374/2330 train_time:79822ms step_avg:58.09ms
step:1375/2330 train_time:79879ms step_avg:58.09ms
step:1376/2330 train_time:79938ms step_avg:58.09ms
step:1377/2330 train_time:79996ms step_avg:58.09ms
step:1378/2330 train_time:80057ms step_avg:58.10ms
step:1379/2330 train_time:80113ms step_avg:58.10ms
step:1380/2330 train_time:80174ms step_avg:58.10ms
step:1381/2330 train_time:80231ms step_avg:58.10ms
step:1382/2330 train_time:80291ms step_avg:58.10ms
step:1383/2330 train_time:80348ms step_avg:58.10ms
step:1384/2330 train_time:80408ms step_avg:58.10ms
step:1385/2330 train_time:80465ms step_avg:58.10ms
step:1386/2330 train_time:80525ms step_avg:58.10ms
step:1387/2330 train_time:80582ms step_avg:58.10ms
step:1388/2330 train_time:80642ms step_avg:58.10ms
step:1389/2330 train_time:80700ms step_avg:58.10ms
step:1390/2330 train_time:80759ms step_avg:58.10ms
step:1391/2330 train_time:80815ms step_avg:58.10ms
step:1392/2330 train_time:80877ms step_avg:58.10ms
step:1393/2330 train_time:80934ms step_avg:58.10ms
step:1394/2330 train_time:80995ms step_avg:58.10ms
step:1395/2330 train_time:81052ms step_avg:58.10ms
step:1396/2330 train_time:81112ms step_avg:58.10ms
step:1397/2330 train_time:81169ms step_avg:58.10ms
step:1398/2330 train_time:81229ms step_avg:58.10ms
step:1399/2330 train_time:81287ms step_avg:58.10ms
step:1400/2330 train_time:81347ms step_avg:58.10ms
step:1401/2330 train_time:81403ms step_avg:58.10ms
step:1402/2330 train_time:81463ms step_avg:58.11ms
step:1403/2330 train_time:81520ms step_avg:58.10ms
step:1404/2330 train_time:81581ms step_avg:58.11ms
step:1405/2330 train_time:81637ms step_avg:58.10ms
step:1406/2330 train_time:81698ms step_avg:58.11ms
step:1407/2330 train_time:81754ms step_avg:58.11ms
step:1408/2330 train_time:81815ms step_avg:58.11ms
step:1409/2330 train_time:81872ms step_avg:58.11ms
step:1410/2330 train_time:81932ms step_avg:58.11ms
step:1411/2330 train_time:81990ms step_avg:58.11ms
step:1412/2330 train_time:82050ms step_avg:58.11ms
step:1413/2330 train_time:82107ms step_avg:58.11ms
step:1414/2330 train_time:82167ms step_avg:58.11ms
step:1415/2330 train_time:82225ms step_avg:58.11ms
step:1416/2330 train_time:82285ms step_avg:58.11ms
step:1417/2330 train_time:82342ms step_avg:58.11ms
step:1418/2330 train_time:82401ms step_avg:58.11ms
step:1419/2330 train_time:82458ms step_avg:58.11ms
step:1420/2330 train_time:82518ms step_avg:58.11ms
step:1421/2330 train_time:82575ms step_avg:58.11ms
step:1422/2330 train_time:82635ms step_avg:58.11ms
step:1423/2330 train_time:82692ms step_avg:58.11ms
step:1424/2330 train_time:82753ms step_avg:58.11ms
step:1425/2330 train_time:82809ms step_avg:58.11ms
step:1426/2330 train_time:82870ms step_avg:58.11ms
step:1427/2330 train_time:82927ms step_avg:58.11ms
step:1428/2330 train_time:82987ms step_avg:58.11ms
step:1429/2330 train_time:83045ms step_avg:58.11ms
step:1430/2330 train_time:83104ms step_avg:58.11ms
step:1431/2330 train_time:83160ms step_avg:58.11ms
step:1432/2330 train_time:83221ms step_avg:58.12ms
step:1433/2330 train_time:83277ms step_avg:58.11ms
step:1434/2330 train_time:83338ms step_avg:58.12ms
step:1435/2330 train_time:83395ms step_avg:58.12ms
step:1436/2330 train_time:83456ms step_avg:58.12ms
step:1437/2330 train_time:83512ms step_avg:58.12ms
step:1438/2330 train_time:83572ms step_avg:58.12ms
step:1439/2330 train_time:83629ms step_avg:58.12ms
step:1440/2330 train_time:83691ms step_avg:58.12ms
step:1441/2330 train_time:83749ms step_avg:58.12ms
step:1442/2330 train_time:83808ms step_avg:58.12ms
step:1443/2330 train_time:83866ms step_avg:58.12ms
step:1444/2330 train_time:83926ms step_avg:58.12ms
step:1445/2330 train_time:83982ms step_avg:58.12ms
step:1446/2330 train_time:84042ms step_avg:58.12ms
step:1447/2330 train_time:84099ms step_avg:58.12ms
step:1448/2330 train_time:84159ms step_avg:58.12ms
step:1449/2330 train_time:84216ms step_avg:58.12ms
step:1450/2330 train_time:84277ms step_avg:58.12ms
step:1451/2330 train_time:84334ms step_avg:58.12ms
step:1452/2330 train_time:84394ms step_avg:58.12ms
step:1453/2330 train_time:84452ms step_avg:58.12ms
step:1454/2330 train_time:84511ms step_avg:58.12ms
step:1455/2330 train_time:84569ms step_avg:58.12ms
step:1456/2330 train_time:84628ms step_avg:58.12ms
step:1457/2330 train_time:84686ms step_avg:58.12ms
step:1458/2330 train_time:84745ms step_avg:58.12ms
step:1459/2330 train_time:84803ms step_avg:58.12ms
step:1460/2330 train_time:84862ms step_avg:58.12ms
step:1461/2330 train_time:84919ms step_avg:58.12ms
step:1462/2330 train_time:84979ms step_avg:58.13ms
step:1463/2330 train_time:85037ms step_avg:58.12ms
step:1464/2330 train_time:85097ms step_avg:58.13ms
step:1465/2330 train_time:85154ms step_avg:58.13ms
step:1466/2330 train_time:85215ms step_avg:58.13ms
step:1467/2330 train_time:85272ms step_avg:58.13ms
step:1468/2330 train_time:85332ms step_avg:58.13ms
step:1469/2330 train_time:85390ms step_avg:58.13ms
step:1470/2330 train_time:85449ms step_avg:58.13ms
step:1471/2330 train_time:85506ms step_avg:58.13ms
step:1472/2330 train_time:85566ms step_avg:58.13ms
step:1473/2330 train_time:85622ms step_avg:58.13ms
step:1474/2330 train_time:85682ms step_avg:58.13ms
step:1475/2330 train_time:85739ms step_avg:58.13ms
step:1476/2330 train_time:85800ms step_avg:58.13ms
step:1477/2330 train_time:85857ms step_avg:58.13ms
step:1478/2330 train_time:85917ms step_avg:58.13ms
step:1479/2330 train_time:85974ms step_avg:58.13ms
step:1480/2330 train_time:86035ms step_avg:58.13ms
step:1481/2330 train_time:86092ms step_avg:58.13ms
step:1482/2330 train_time:86153ms step_avg:58.13ms
step:1483/2330 train_time:86210ms step_avg:58.13ms
step:1484/2330 train_time:86270ms step_avg:58.13ms
step:1485/2330 train_time:86327ms step_avg:58.13ms
step:1486/2330 train_time:86387ms step_avg:58.13ms
step:1487/2330 train_time:86445ms step_avg:58.13ms
step:1488/2330 train_time:86504ms step_avg:58.13ms
step:1489/2330 train_time:86561ms step_avg:58.13ms
step:1490/2330 train_time:86620ms step_avg:58.13ms
step:1491/2330 train_time:86678ms step_avg:58.13ms
step:1492/2330 train_time:86737ms step_avg:58.13ms
step:1493/2330 train_time:86795ms step_avg:58.13ms
step:1494/2330 train_time:86855ms step_avg:58.14ms
step:1495/2330 train_time:86912ms step_avg:58.14ms
step:1496/2330 train_time:86972ms step_avg:58.14ms
step:1497/2330 train_time:87030ms step_avg:58.14ms
step:1498/2330 train_time:87090ms step_avg:58.14ms
step:1499/2330 train_time:87147ms step_avg:58.14ms
step:1500/2330 train_time:87207ms step_avg:58.14ms
step:1500/2330 val_loss:3.9049 train_time:87287ms step_avg:58.19ms
step:1501/2330 train_time:87306ms step_avg:58.17ms
step:1502/2330 train_time:87326ms step_avg:58.14ms
step:1503/2330 train_time:87383ms step_avg:58.14ms
step:1504/2330 train_time:87448ms step_avg:58.14ms
step:1505/2330 train_time:87506ms step_avg:58.14ms
step:1506/2330 train_time:87568ms step_avg:58.15ms
step:1507/2330 train_time:87624ms step_avg:58.14ms
step:1508/2330 train_time:87685ms step_avg:58.15ms
step:1509/2330 train_time:87742ms step_avg:58.15ms
step:1510/2330 train_time:87801ms step_avg:58.15ms
step:1511/2330 train_time:87858ms step_avg:58.15ms
step:1512/2330 train_time:87917ms step_avg:58.15ms
step:1513/2330 train_time:87973ms step_avg:58.15ms
step:1514/2330 train_time:88032ms step_avg:58.15ms
step:1515/2330 train_time:88088ms step_avg:58.14ms
step:1516/2330 train_time:88148ms step_avg:58.14ms
step:1517/2330 train_time:88204ms step_avg:58.14ms
step:1518/2330 train_time:88265ms step_avg:58.15ms
step:1519/2330 train_time:88324ms step_avg:58.15ms
step:1520/2330 train_time:88386ms step_avg:58.15ms
step:1521/2330 train_time:88443ms step_avg:58.15ms
step:1522/2330 train_time:88507ms step_avg:58.15ms
step:1523/2330 train_time:88564ms step_avg:58.15ms
step:1524/2330 train_time:88625ms step_avg:58.15ms
step:1525/2330 train_time:88682ms step_avg:58.15ms
step:1526/2330 train_time:88742ms step_avg:58.15ms
step:1527/2330 train_time:88799ms step_avg:58.15ms
step:1528/2330 train_time:88859ms step_avg:58.15ms
step:1529/2330 train_time:88917ms step_avg:58.15ms
step:1530/2330 train_time:88976ms step_avg:58.15ms
step:1531/2330 train_time:89033ms step_avg:58.15ms
step:1532/2330 train_time:89092ms step_avg:58.15ms
step:1533/2330 train_time:89149ms step_avg:58.15ms
step:1534/2330 train_time:89209ms step_avg:58.15ms
step:1535/2330 train_time:89266ms step_avg:58.15ms
step:1536/2330 train_time:89328ms step_avg:58.16ms
step:1537/2330 train_time:89385ms step_avg:58.16ms
step:1538/2330 train_time:89447ms step_avg:58.16ms
step:1539/2330 train_time:89505ms step_avg:58.16ms
step:1540/2330 train_time:89567ms step_avg:58.16ms
step:1541/2330 train_time:89625ms step_avg:58.16ms
step:1542/2330 train_time:89685ms step_avg:58.16ms
step:1543/2330 train_time:89742ms step_avg:58.16ms
step:1544/2330 train_time:89803ms step_avg:58.16ms
step:1545/2330 train_time:89860ms step_avg:58.16ms
step:1546/2330 train_time:89922ms step_avg:58.16ms
step:1547/2330 train_time:89979ms step_avg:58.16ms
step:1548/2330 train_time:90040ms step_avg:58.17ms
step:1549/2330 train_time:90097ms step_avg:58.16ms
step:1550/2330 train_time:90158ms step_avg:58.17ms
step:1551/2330 train_time:90216ms step_avg:58.17ms
step:1552/2330 train_time:90276ms step_avg:58.17ms
step:1553/2330 train_time:90334ms step_avg:58.17ms
step:1554/2330 train_time:90395ms step_avg:58.17ms
step:1555/2330 train_time:90452ms step_avg:58.17ms
step:1556/2330 train_time:90514ms step_avg:58.17ms
step:1557/2330 train_time:90571ms step_avg:58.17ms
step:1558/2330 train_time:90633ms step_avg:58.17ms
step:1559/2330 train_time:90689ms step_avg:58.17ms
step:1560/2330 train_time:90750ms step_avg:58.17ms
step:1561/2330 train_time:90807ms step_avg:58.17ms
step:1562/2330 train_time:90869ms step_avg:58.17ms
step:1563/2330 train_time:90925ms step_avg:58.17ms
step:1564/2330 train_time:90987ms step_avg:58.18ms
step:1565/2330 train_time:91043ms step_avg:58.17ms
step:1566/2330 train_time:91106ms step_avg:58.18ms
step:1567/2330 train_time:91163ms step_avg:58.18ms
step:1568/2330 train_time:91225ms step_avg:58.18ms
step:1569/2330 train_time:91283ms step_avg:58.18ms
step:1570/2330 train_time:91344ms step_avg:58.18ms
step:1571/2330 train_time:91401ms step_avg:58.18ms
step:1572/2330 train_time:91463ms step_avg:58.18ms
step:1573/2330 train_time:91521ms step_avg:58.18ms
step:1574/2330 train_time:91582ms step_avg:58.18ms
step:1575/2330 train_time:91639ms step_avg:58.18ms
step:1576/2330 train_time:91700ms step_avg:58.19ms
step:1577/2330 train_time:91757ms step_avg:58.18ms
step:1578/2330 train_time:91818ms step_avg:58.19ms
step:1579/2330 train_time:91875ms step_avg:58.19ms
step:1580/2330 train_time:91935ms step_avg:58.19ms
step:1581/2330 train_time:91991ms step_avg:58.19ms
step:1582/2330 train_time:92051ms step_avg:58.19ms
step:1583/2330 train_time:92108ms step_avg:58.19ms
step:1584/2330 train_time:92171ms step_avg:58.19ms
step:1585/2330 train_time:92227ms step_avg:58.19ms
step:1586/2330 train_time:92288ms step_avg:58.19ms
step:1587/2330 train_time:92344ms step_avg:58.19ms
step:1588/2330 train_time:92406ms step_avg:58.19ms
step:1589/2330 train_time:92464ms step_avg:58.19ms
step:1590/2330 train_time:92526ms step_avg:58.19ms
step:1591/2330 train_time:92583ms step_avg:58.19ms
step:1592/2330 train_time:92645ms step_avg:58.19ms
step:1593/2330 train_time:92702ms step_avg:58.19ms
step:1594/2330 train_time:92764ms step_avg:58.20ms
step:1595/2330 train_time:92822ms step_avg:58.20ms
step:1596/2330 train_time:92883ms step_avg:58.20ms
step:1597/2330 train_time:92941ms step_avg:58.20ms
step:1598/2330 train_time:93001ms step_avg:58.20ms
step:1599/2330 train_time:93058ms step_avg:58.20ms
step:1600/2330 train_time:93119ms step_avg:58.20ms
step:1601/2330 train_time:93176ms step_avg:58.20ms
step:1602/2330 train_time:93236ms step_avg:58.20ms
step:1603/2330 train_time:93293ms step_avg:58.20ms
step:1604/2330 train_time:93353ms step_avg:58.20ms
step:1605/2330 train_time:93411ms step_avg:58.20ms
step:1606/2330 train_time:93470ms step_avg:58.20ms
step:1607/2330 train_time:93528ms step_avg:58.20ms
step:1608/2330 train_time:93589ms step_avg:58.20ms
step:1609/2330 train_time:93645ms step_avg:58.20ms
step:1610/2330 train_time:93707ms step_avg:58.20ms
step:1611/2330 train_time:93764ms step_avg:58.20ms
step:1612/2330 train_time:93826ms step_avg:58.20ms
step:1613/2330 train_time:93883ms step_avg:58.20ms
step:1614/2330 train_time:93944ms step_avg:58.21ms
step:1615/2330 train_time:94002ms step_avg:58.21ms
step:1616/2330 train_time:94064ms step_avg:58.21ms
step:1617/2330 train_time:94121ms step_avg:58.21ms
step:1618/2330 train_time:94183ms step_avg:58.21ms
step:1619/2330 train_time:94241ms step_avg:58.21ms
step:1620/2330 train_time:94302ms step_avg:58.21ms
step:1621/2330 train_time:94360ms step_avg:58.21ms
step:1622/2330 train_time:94420ms step_avg:58.21ms
step:1623/2330 train_time:94478ms step_avg:58.21ms
step:1624/2330 train_time:94538ms step_avg:58.21ms
step:1625/2330 train_time:94596ms step_avg:58.21ms
step:1626/2330 train_time:94657ms step_avg:58.21ms
step:1627/2330 train_time:94714ms step_avg:58.21ms
step:1628/2330 train_time:94775ms step_avg:58.22ms
step:1629/2330 train_time:94832ms step_avg:58.21ms
step:1630/2330 train_time:94893ms step_avg:58.22ms
step:1631/2330 train_time:94949ms step_avg:58.22ms
step:1632/2330 train_time:95011ms step_avg:58.22ms
step:1633/2330 train_time:95068ms step_avg:58.22ms
step:1634/2330 train_time:95129ms step_avg:58.22ms
step:1635/2330 train_time:95185ms step_avg:58.22ms
step:1636/2330 train_time:95248ms step_avg:58.22ms
step:1637/2330 train_time:95305ms step_avg:58.22ms
step:1638/2330 train_time:95367ms step_avg:58.22ms
step:1639/2330 train_time:95424ms step_avg:58.22ms
step:1640/2330 train_time:95485ms step_avg:58.22ms
step:1641/2330 train_time:95543ms step_avg:58.22ms
step:1642/2330 train_time:95604ms step_avg:58.22ms
step:1643/2330 train_time:95662ms step_avg:58.22ms
step:1644/2330 train_time:95723ms step_avg:58.23ms
step:1645/2330 train_time:95780ms step_avg:58.23ms
step:1646/2330 train_time:95843ms step_avg:58.23ms
step:1647/2330 train_time:95900ms step_avg:58.23ms
step:1648/2330 train_time:95961ms step_avg:58.23ms
step:1649/2330 train_time:96019ms step_avg:58.23ms
step:1650/2330 train_time:96079ms step_avg:58.23ms
step:1651/2330 train_time:96137ms step_avg:58.23ms
step:1652/2330 train_time:96197ms step_avg:58.23ms
step:1653/2330 train_time:96254ms step_avg:58.23ms
step:1654/2330 train_time:96314ms step_avg:58.23ms
step:1655/2330 train_time:96372ms step_avg:58.23ms
step:1656/2330 train_time:96432ms step_avg:58.23ms
step:1657/2330 train_time:96489ms step_avg:58.23ms
step:1658/2330 train_time:96550ms step_avg:58.23ms
step:1659/2330 train_time:96607ms step_avg:58.23ms
step:1660/2330 train_time:96668ms step_avg:58.23ms
step:1661/2330 train_time:96724ms step_avg:58.23ms
step:1662/2330 train_time:96786ms step_avg:58.23ms
step:1663/2330 train_time:96843ms step_avg:58.23ms
step:1664/2330 train_time:96905ms step_avg:58.24ms
step:1665/2330 train_time:96962ms step_avg:58.24ms
step:1666/2330 train_time:97024ms step_avg:58.24ms
step:1667/2330 train_time:97082ms step_avg:58.24ms
step:1668/2330 train_time:97142ms step_avg:58.24ms
step:1669/2330 train_time:97200ms step_avg:58.24ms
step:1670/2330 train_time:97260ms step_avg:58.24ms
step:1671/2330 train_time:97319ms step_avg:58.24ms
step:1672/2330 train_time:97379ms step_avg:58.24ms
step:1673/2330 train_time:97438ms step_avg:58.24ms
step:1674/2330 train_time:97499ms step_avg:58.24ms
step:1675/2330 train_time:97556ms step_avg:58.24ms
step:1676/2330 train_time:97615ms step_avg:58.24ms
step:1677/2330 train_time:97672ms step_avg:58.24ms
step:1678/2330 train_time:97733ms step_avg:58.24ms
step:1679/2330 train_time:97789ms step_avg:58.24ms
step:1680/2330 train_time:97850ms step_avg:58.24ms
step:1681/2330 train_time:97907ms step_avg:58.24ms
step:1682/2330 train_time:97969ms step_avg:58.25ms
step:1683/2330 train_time:98026ms step_avg:58.24ms
step:1684/2330 train_time:98089ms step_avg:58.25ms
step:1685/2330 train_time:98145ms step_avg:58.25ms
step:1686/2330 train_time:98208ms step_avg:58.25ms
step:1687/2330 train_time:98265ms step_avg:58.25ms
step:1688/2330 train_time:98328ms step_avg:58.25ms
step:1689/2330 train_time:98385ms step_avg:58.25ms
step:1690/2330 train_time:98447ms step_avg:58.25ms
step:1691/2330 train_time:98504ms step_avg:58.25ms
step:1692/2330 train_time:98565ms step_avg:58.25ms
step:1693/2330 train_time:98622ms step_avg:58.25ms
step:1694/2330 train_time:98683ms step_avg:58.25ms
step:1695/2330 train_time:98742ms step_avg:58.25ms
step:1696/2330 train_time:98802ms step_avg:58.26ms
step:1697/2330 train_time:98860ms step_avg:58.26ms
step:1698/2330 train_time:98920ms step_avg:58.26ms
step:1699/2330 train_time:98979ms step_avg:58.26ms
step:1700/2330 train_time:99039ms step_avg:58.26ms
step:1701/2330 train_time:99096ms step_avg:58.26ms
step:1702/2330 train_time:99156ms step_avg:58.26ms
step:1703/2330 train_time:99213ms step_avg:58.26ms
step:1704/2330 train_time:99275ms step_avg:58.26ms
step:1705/2330 train_time:99332ms step_avg:58.26ms
step:1706/2330 train_time:99393ms step_avg:58.26ms
step:1707/2330 train_time:99449ms step_avg:58.26ms
step:1708/2330 train_time:99510ms step_avg:58.26ms
step:1709/2330 train_time:99567ms step_avg:58.26ms
step:1710/2330 train_time:99629ms step_avg:58.26ms
step:1711/2330 train_time:99686ms step_avg:58.26ms
step:1712/2330 train_time:99747ms step_avg:58.26ms
step:1713/2330 train_time:99804ms step_avg:58.26ms
step:1714/2330 train_time:99865ms step_avg:58.26ms
step:1715/2330 train_time:99923ms step_avg:58.26ms
step:1716/2330 train_time:99984ms step_avg:58.27ms
step:1717/2330 train_time:100042ms step_avg:58.27ms
step:1718/2330 train_time:100102ms step_avg:58.27ms
step:1719/2330 train_time:100159ms step_avg:58.27ms
step:1720/2330 train_time:100220ms step_avg:58.27ms
step:1721/2330 train_time:100278ms step_avg:58.27ms
step:1722/2330 train_time:100339ms step_avg:58.27ms
step:1723/2330 train_time:100397ms step_avg:58.27ms
step:1724/2330 train_time:100457ms step_avg:58.27ms
step:1725/2330 train_time:100514ms step_avg:58.27ms
step:1726/2330 train_time:100574ms step_avg:58.27ms
step:1727/2330 train_time:100631ms step_avg:58.27ms
step:1728/2330 train_time:100691ms step_avg:58.27ms
step:1729/2330 train_time:100748ms step_avg:58.27ms
step:1730/2330 train_time:100809ms step_avg:58.27ms
step:1731/2330 train_time:100866ms step_avg:58.27ms
step:1732/2330 train_time:100927ms step_avg:58.27ms
step:1733/2330 train_time:100984ms step_avg:58.27ms
step:1734/2330 train_time:101046ms step_avg:58.27ms
step:1735/2330 train_time:101103ms step_avg:58.27ms
step:1736/2330 train_time:101165ms step_avg:58.27ms
step:1737/2330 train_time:101222ms step_avg:58.27ms
step:1738/2330 train_time:101284ms step_avg:58.28ms
step:1739/2330 train_time:101342ms step_avg:58.28ms
step:1740/2330 train_time:101404ms step_avg:58.28ms
step:1741/2330 train_time:101461ms step_avg:58.28ms
step:1742/2330 train_time:101522ms step_avg:58.28ms
step:1743/2330 train_time:101580ms step_avg:58.28ms
step:1744/2330 train_time:101640ms step_avg:58.28ms
step:1745/2330 train_time:101700ms step_avg:58.28ms
step:1746/2330 train_time:101760ms step_avg:58.28ms
step:1747/2330 train_time:101817ms step_avg:58.28ms
step:1748/2330 train_time:101877ms step_avg:58.28ms
step:1749/2330 train_time:101935ms step_avg:58.28ms
step:1750/2330 train_time:101995ms step_avg:58.28ms
step:1750/2330 val_loss:3.8209 train_time:102077ms step_avg:58.33ms
step:1751/2330 train_time:102096ms step_avg:58.31ms
step:1752/2330 train_time:102115ms step_avg:58.28ms
step:1753/2330 train_time:102173ms step_avg:58.28ms
step:1754/2330 train_time:102244ms step_avg:58.29ms
step:1755/2330 train_time:102301ms step_avg:58.29ms
step:1756/2330 train_time:102363ms step_avg:58.29ms
step:1757/2330 train_time:102419ms step_avg:58.29ms
step:1758/2330 train_time:102482ms step_avg:58.29ms
step:1759/2330 train_time:102539ms step_avg:58.29ms
step:1760/2330 train_time:102599ms step_avg:58.30ms
step:1761/2330 train_time:102656ms step_avg:58.29ms
step:1762/2330 train_time:102716ms step_avg:58.29ms
step:1763/2330 train_time:102773ms step_avg:58.29ms
step:1764/2330 train_time:102832ms step_avg:58.29ms
step:1765/2330 train_time:102889ms step_avg:58.29ms
step:1766/2330 train_time:102948ms step_avg:58.29ms
step:1767/2330 train_time:103006ms step_avg:58.29ms
step:1768/2330 train_time:103070ms step_avg:58.30ms
step:1769/2330 train_time:103128ms step_avg:58.30ms
step:1770/2330 train_time:103191ms step_avg:58.30ms
step:1771/2330 train_time:103248ms step_avg:58.30ms
step:1772/2330 train_time:103309ms step_avg:58.30ms
step:1773/2330 train_time:103367ms step_avg:58.30ms
step:1774/2330 train_time:103428ms step_avg:58.30ms
step:1775/2330 train_time:103485ms step_avg:58.30ms
step:1776/2330 train_time:103545ms step_avg:58.30ms
step:1777/2330 train_time:103602ms step_avg:58.30ms
step:1778/2330 train_time:103663ms step_avg:58.30ms
step:1779/2330 train_time:103719ms step_avg:58.30ms
step:1780/2330 train_time:103780ms step_avg:58.30ms
step:1781/2330 train_time:103836ms step_avg:58.30ms
step:1782/2330 train_time:103897ms step_avg:58.30ms
step:1783/2330 train_time:103954ms step_avg:58.30ms
step:1784/2330 train_time:104017ms step_avg:58.31ms
step:1785/2330 train_time:104075ms step_avg:58.31ms
step:1786/2330 train_time:104138ms step_avg:58.31ms
step:1787/2330 train_time:104197ms step_avg:58.31ms
step:1788/2330 train_time:104260ms step_avg:58.31ms
step:1789/2330 train_time:104318ms step_avg:58.31ms
step:1790/2330 train_time:104379ms step_avg:58.31ms
step:1791/2330 train_time:104436ms step_avg:58.31ms
step:1792/2330 train_time:104497ms step_avg:58.31ms
step:1793/2330 train_time:104555ms step_avg:58.31ms
step:1794/2330 train_time:104615ms step_avg:58.31ms
step:1795/2330 train_time:104673ms step_avg:58.31ms
step:1796/2330 train_time:104732ms step_avg:58.31ms
step:1797/2330 train_time:104788ms step_avg:58.31ms
step:1798/2330 train_time:104849ms step_avg:58.31ms
step:1799/2330 train_time:104905ms step_avg:58.31ms
step:1800/2330 train_time:104967ms step_avg:58.31ms
step:1801/2330 train_time:105024ms step_avg:58.31ms
step:1802/2330 train_time:105086ms step_avg:58.32ms
step:1803/2330 train_time:105143ms step_avg:58.32ms
step:1804/2330 train_time:105207ms step_avg:58.32ms
step:1805/2330 train_time:105264ms step_avg:58.32ms
step:1806/2330 train_time:105326ms step_avg:58.32ms
step:1807/2330 train_time:105382ms step_avg:58.32ms
step:1808/2330 train_time:105444ms step_avg:58.32ms
step:1809/2330 train_time:105501ms step_avg:58.32ms
step:1810/2330 train_time:105563ms step_avg:58.32ms
step:1811/2330 train_time:105620ms step_avg:58.32ms
step:1812/2330 train_time:105681ms step_avg:58.32ms
step:1813/2330 train_time:105738ms step_avg:58.32ms
step:1814/2330 train_time:105798ms step_avg:58.32ms
step:1815/2330 train_time:105856ms step_avg:58.32ms
step:1816/2330 train_time:105917ms step_avg:58.32ms
step:1817/2330 train_time:105975ms step_avg:58.32ms
step:1818/2330 train_time:106036ms step_avg:58.33ms
step:1819/2330 train_time:106095ms step_avg:58.33ms
step:1820/2330 train_time:106156ms step_avg:58.33ms
step:1821/2330 train_time:106214ms step_avg:58.33ms
step:1822/2330 train_time:106274ms step_avg:58.33ms
step:1823/2330 train_time:106332ms step_avg:58.33ms
step:1824/2330 train_time:106392ms step_avg:58.33ms
step:1825/2330 train_time:106449ms step_avg:58.33ms
step:1826/2330 train_time:106509ms step_avg:58.33ms
step:1827/2330 train_time:106567ms step_avg:58.33ms
step:1828/2330 train_time:106628ms step_avg:58.33ms
step:1829/2330 train_time:106684ms step_avg:58.33ms
step:1830/2330 train_time:106744ms step_avg:58.33ms
step:1831/2330 train_time:106801ms step_avg:58.33ms
step:1832/2330 train_time:106862ms step_avg:58.33ms
step:1833/2330 train_time:106919ms step_avg:58.33ms
step:1834/2330 train_time:106982ms step_avg:58.33ms
step:1835/2330 train_time:107039ms step_avg:58.33ms
step:1836/2330 train_time:107102ms step_avg:58.33ms
step:1837/2330 train_time:107159ms step_avg:58.33ms
step:1838/2330 train_time:107220ms step_avg:58.33ms
step:1839/2330 train_time:107278ms step_avg:58.33ms
step:1840/2330 train_time:107340ms step_avg:58.34ms
step:1841/2330 train_time:107398ms step_avg:58.34ms
step:1842/2330 train_time:107459ms step_avg:58.34ms
step:1843/2330 train_time:107517ms step_avg:58.34ms
step:1844/2330 train_time:107578ms step_avg:58.34ms
step:1845/2330 train_time:107636ms step_avg:58.34ms
step:1846/2330 train_time:107696ms step_avg:58.34ms
step:1847/2330 train_time:107753ms step_avg:58.34ms
step:1848/2330 train_time:107813ms step_avg:58.34ms
step:1849/2330 train_time:107870ms step_avg:58.34ms
step:1850/2330 train_time:107930ms step_avg:58.34ms
step:1851/2330 train_time:107987ms step_avg:58.34ms
step:1852/2330 train_time:108048ms step_avg:58.34ms
step:1853/2330 train_time:108104ms step_avg:58.34ms
step:1854/2330 train_time:108167ms step_avg:58.34ms
step:1855/2330 train_time:108223ms step_avg:58.34ms
step:1856/2330 train_time:108286ms step_avg:58.34ms
step:1857/2330 train_time:108343ms step_avg:58.34ms
step:1858/2330 train_time:108405ms step_avg:58.34ms
step:1859/2330 train_time:108462ms step_avg:58.34ms
step:1860/2330 train_time:108523ms step_avg:58.35ms
step:1861/2330 train_time:108580ms step_avg:58.35ms
step:1862/2330 train_time:108641ms step_avg:58.35ms
step:1863/2330 train_time:108698ms step_avg:58.35ms
step:1864/2330 train_time:108760ms step_avg:58.35ms
step:1865/2330 train_time:108818ms step_avg:58.35ms
step:1866/2330 train_time:108879ms step_avg:58.35ms
step:1867/2330 train_time:108936ms step_avg:58.35ms
step:1868/2330 train_time:108997ms step_avg:58.35ms
step:1869/2330 train_time:109055ms step_avg:58.35ms
step:1870/2330 train_time:109116ms step_avg:58.35ms
step:1871/2330 train_time:109174ms step_avg:58.35ms
step:1872/2330 train_time:109233ms step_avg:58.35ms
step:1873/2330 train_time:109291ms step_avg:58.35ms
step:1874/2330 train_time:109352ms step_avg:58.35ms
step:1875/2330 train_time:109409ms step_avg:58.35ms
step:1876/2330 train_time:109469ms step_avg:58.35ms
step:1877/2330 train_time:109526ms step_avg:58.35ms
step:1878/2330 train_time:109587ms step_avg:58.35ms
step:1879/2330 train_time:109644ms step_avg:58.35ms
step:1880/2330 train_time:109705ms step_avg:58.35ms
step:1881/2330 train_time:109762ms step_avg:58.35ms
step:1882/2330 train_time:109823ms step_avg:58.35ms
step:1883/2330 train_time:109879ms step_avg:58.35ms
step:1884/2330 train_time:109941ms step_avg:58.35ms
step:1885/2330 train_time:109998ms step_avg:58.35ms
step:1886/2330 train_time:110060ms step_avg:58.36ms
step:1887/2330 train_time:110117ms step_avg:58.36ms
step:1888/2330 train_time:110179ms step_avg:58.36ms
step:1889/2330 train_time:110237ms step_avg:58.36ms
step:1890/2330 train_time:110299ms step_avg:58.36ms
step:1891/2330 train_time:110356ms step_avg:58.36ms
step:1892/2330 train_time:110418ms step_avg:58.36ms
step:1893/2330 train_time:110477ms step_avg:58.36ms
step:1894/2330 train_time:110537ms step_avg:58.36ms
step:1895/2330 train_time:110595ms step_avg:58.36ms
step:1896/2330 train_time:110655ms step_avg:58.36ms
step:1897/2330 train_time:110712ms step_avg:58.36ms
step:1898/2330 train_time:110773ms step_avg:58.36ms
step:1899/2330 train_time:110831ms step_avg:58.36ms
step:1900/2330 train_time:110890ms step_avg:58.36ms
step:1901/2330 train_time:110948ms step_avg:58.36ms
step:1902/2330 train_time:111009ms step_avg:58.36ms
step:1903/2330 train_time:111065ms step_avg:58.36ms
step:1904/2330 train_time:111125ms step_avg:58.36ms
step:1905/2330 train_time:111183ms step_avg:58.36ms
step:1906/2330 train_time:111244ms step_avg:58.37ms
step:1907/2330 train_time:111301ms step_avg:58.36ms
step:1908/2330 train_time:111364ms step_avg:58.37ms
step:1909/2330 train_time:111421ms step_avg:58.37ms
step:1910/2330 train_time:111484ms step_avg:58.37ms
step:1911/2330 train_time:111540ms step_avg:58.37ms
step:1912/2330 train_time:111603ms step_avg:58.37ms
step:1913/2330 train_time:111659ms step_avg:58.37ms
step:1914/2330 train_time:111722ms step_avg:58.37ms
step:1915/2330 train_time:111779ms step_avg:58.37ms
step:1916/2330 train_time:111840ms step_avg:58.37ms
step:1917/2330 train_time:111898ms step_avg:58.37ms
step:1918/2330 train_time:111959ms step_avg:58.37ms
step:1919/2330 train_time:112017ms step_avg:58.37ms
step:1920/2330 train_time:112077ms step_avg:58.37ms
step:1921/2330 train_time:112134ms step_avg:58.37ms
step:1922/2330 train_time:112196ms step_avg:58.37ms
step:1923/2330 train_time:112253ms step_avg:58.37ms
step:1924/2330 train_time:112315ms step_avg:58.38ms
step:1925/2330 train_time:112372ms step_avg:58.38ms
step:1926/2330 train_time:112433ms step_avg:58.38ms
step:1927/2330 train_time:112490ms step_avg:58.38ms
step:1928/2330 train_time:112551ms step_avg:58.38ms
step:1929/2330 train_time:112608ms step_avg:58.38ms
step:1930/2330 train_time:112668ms step_avg:58.38ms
step:1931/2330 train_time:112726ms step_avg:58.38ms
step:1932/2330 train_time:112786ms step_avg:58.38ms
step:1933/2330 train_time:112843ms step_avg:58.38ms
step:1934/2330 train_time:112905ms step_avg:58.38ms
step:1935/2330 train_time:112962ms step_avg:58.38ms
step:1936/2330 train_time:113023ms step_avg:58.38ms
step:1937/2330 train_time:113079ms step_avg:58.38ms
step:1938/2330 train_time:113142ms step_avg:58.38ms
step:1939/2330 train_time:113198ms step_avg:58.38ms
step:1940/2330 train_time:113261ms step_avg:58.38ms
step:1941/2330 train_time:113319ms step_avg:58.38ms
step:1942/2330 train_time:113380ms step_avg:58.38ms
step:1943/2330 train_time:113437ms step_avg:58.38ms
step:1944/2330 train_time:113500ms step_avg:58.38ms
step:1945/2330 train_time:113557ms step_avg:58.38ms
step:1946/2330 train_time:113620ms step_avg:58.39ms
step:1947/2330 train_time:113678ms step_avg:58.39ms
step:1948/2330 train_time:113738ms step_avg:58.39ms
step:1949/2330 train_time:113796ms step_avg:58.39ms
step:1950/2330 train_time:113856ms step_avg:58.39ms
step:1951/2330 train_time:113914ms step_avg:58.39ms
step:1952/2330 train_time:113975ms step_avg:58.39ms
step:1953/2330 train_time:114033ms step_avg:58.39ms
step:1954/2330 train_time:114093ms step_avg:58.39ms
step:1955/2330 train_time:114149ms step_avg:58.39ms
step:1956/2330 train_time:114210ms step_avg:58.39ms
step:1957/2330 train_time:114268ms step_avg:58.39ms
step:1958/2330 train_time:114328ms step_avg:58.39ms
step:1959/2330 train_time:114385ms step_avg:58.39ms
step:1960/2330 train_time:114446ms step_avg:58.39ms
step:1961/2330 train_time:114503ms step_avg:58.39ms
step:1962/2330 train_time:114565ms step_avg:58.39ms
step:1963/2330 train_time:114622ms step_avg:58.39ms
step:1964/2330 train_time:114683ms step_avg:58.39ms
step:1965/2330 train_time:114740ms step_avg:58.39ms
step:1966/2330 train_time:114801ms step_avg:58.39ms
step:1967/2330 train_time:114858ms step_avg:58.39ms
step:1968/2330 train_time:114920ms step_avg:58.39ms
step:1969/2330 train_time:114977ms step_avg:58.39ms
step:1970/2330 train_time:115039ms step_avg:58.40ms
step:1971/2330 train_time:115096ms step_avg:58.39ms
step:1972/2330 train_time:115158ms step_avg:58.40ms
step:1973/2330 train_time:115216ms step_avg:58.40ms
step:1974/2330 train_time:115278ms step_avg:58.40ms
step:1975/2330 train_time:115336ms step_avg:58.40ms
step:1976/2330 train_time:115396ms step_avg:58.40ms
step:1977/2330 train_time:115454ms step_avg:58.40ms
step:1978/2330 train_time:115515ms step_avg:58.40ms
step:1979/2330 train_time:115573ms step_avg:58.40ms
step:1980/2330 train_time:115633ms step_avg:58.40ms
step:1981/2330 train_time:115690ms step_avg:58.40ms
step:1982/2330 train_time:115750ms step_avg:58.40ms
step:1983/2330 train_time:115808ms step_avg:58.40ms
step:1984/2330 train_time:115869ms step_avg:58.40ms
step:1985/2330 train_time:115926ms step_avg:58.40ms
step:1986/2330 train_time:115986ms step_avg:58.40ms
step:1987/2330 train_time:116043ms step_avg:58.40ms
step:1988/2330 train_time:116105ms step_avg:58.40ms
step:1989/2330 train_time:116161ms step_avg:58.40ms
step:1990/2330 train_time:116224ms step_avg:58.40ms
step:1991/2330 train_time:116281ms step_avg:58.40ms
step:1992/2330 train_time:116342ms step_avg:58.40ms
step:1993/2330 train_time:116400ms step_avg:58.40ms
step:1994/2330 train_time:116462ms step_avg:58.41ms
step:1995/2330 train_time:116519ms step_avg:58.41ms
step:1996/2330 train_time:116580ms step_avg:58.41ms
step:1997/2330 train_time:116638ms step_avg:58.41ms
step:1998/2330 train_time:116699ms step_avg:58.41ms
step:1999/2330 train_time:116756ms step_avg:58.41ms
step:2000/2330 train_time:116818ms step_avg:58.41ms
step:2000/2330 val_loss:3.7602 train_time:116900ms step_avg:58.45ms
step:2001/2330 train_time:116918ms step_avg:58.43ms
step:2002/2330 train_time:116941ms step_avg:58.41ms
step:2003/2330 train_time:116999ms step_avg:58.41ms
step:2004/2330 train_time:117062ms step_avg:58.41ms
step:2005/2330 train_time:117119ms step_avg:58.41ms
step:2006/2330 train_time:117181ms step_avg:58.42ms
step:2007/2330 train_time:117237ms step_avg:58.41ms
step:2008/2330 train_time:117298ms step_avg:58.42ms
step:2009/2330 train_time:117354ms step_avg:58.41ms
step:2010/2330 train_time:117416ms step_avg:58.42ms
step:2011/2330 train_time:117472ms step_avg:58.41ms
step:2012/2330 train_time:117532ms step_avg:58.42ms
step:2013/2330 train_time:117588ms step_avg:58.41ms
step:2014/2330 train_time:117649ms step_avg:58.42ms
step:2015/2330 train_time:117705ms step_avg:58.41ms
step:2016/2330 train_time:117765ms step_avg:58.41ms
step:2017/2330 train_time:117822ms step_avg:58.41ms
step:2018/2330 train_time:117883ms step_avg:58.42ms
step:2019/2330 train_time:117943ms step_avg:58.42ms
step:2020/2330 train_time:118005ms step_avg:58.42ms
step:2021/2330 train_time:118064ms step_avg:58.42ms
step:2022/2330 train_time:118125ms step_avg:58.42ms
step:2023/2330 train_time:118184ms step_avg:58.42ms
step:2024/2330 train_time:118244ms step_avg:58.42ms
step:2025/2330 train_time:118300ms step_avg:58.42ms
step:2026/2330 train_time:118360ms step_avg:58.42ms
step:2027/2330 train_time:118417ms step_avg:58.42ms
step:2028/2330 train_time:118477ms step_avg:58.42ms
step:2029/2330 train_time:118533ms step_avg:58.42ms
step:2030/2330 train_time:118594ms step_avg:58.42ms
step:2031/2330 train_time:118650ms step_avg:58.42ms
step:2032/2330 train_time:118712ms step_avg:58.42ms
step:2033/2330 train_time:118768ms step_avg:58.42ms
step:2034/2330 train_time:118830ms step_avg:58.42ms
step:2035/2330 train_time:118887ms step_avg:58.42ms
step:2036/2330 train_time:118951ms step_avg:58.42ms
step:2037/2330 train_time:119010ms step_avg:58.42ms
step:2038/2330 train_time:119072ms step_avg:58.43ms
step:2039/2330 train_time:119131ms step_avg:58.43ms
step:2040/2330 train_time:119192ms step_avg:58.43ms
step:2041/2330 train_time:119249ms step_avg:58.43ms
step:2042/2330 train_time:119311ms step_avg:58.43ms
step:2043/2330 train_time:119368ms step_avg:58.43ms
step:2044/2330 train_time:119428ms step_avg:58.43ms
step:2045/2330 train_time:119486ms step_avg:58.43ms
step:2046/2330 train_time:119547ms step_avg:58.43ms
step:2047/2330 train_time:119604ms step_avg:58.43ms
step:2048/2330 train_time:119663ms step_avg:58.43ms
step:2049/2330 train_time:119720ms step_avg:58.43ms
step:2050/2330 train_time:119780ms step_avg:58.43ms
step:2051/2330 train_time:119838ms step_avg:58.43ms
step:2052/2330 train_time:119900ms step_avg:58.43ms
step:2053/2330 train_time:119957ms step_avg:58.43ms
step:2054/2330 train_time:120021ms step_avg:58.43ms
step:2055/2330 train_time:120078ms step_avg:58.43ms
step:2056/2330 train_time:120141ms step_avg:58.43ms
step:2057/2330 train_time:120197ms step_avg:58.43ms
step:2058/2330 train_time:120260ms step_avg:58.44ms
step:2059/2330 train_time:120316ms step_avg:58.43ms
step:2060/2330 train_time:120377ms step_avg:58.44ms
step:2061/2330 train_time:120434ms step_avg:58.43ms
step:2062/2330 train_time:120495ms step_avg:58.44ms
step:2063/2330 train_time:120551ms step_avg:58.43ms
step:2064/2330 train_time:120613ms step_avg:58.44ms
step:2065/2330 train_time:120670ms step_avg:58.44ms
step:2066/2330 train_time:120731ms step_avg:58.44ms
step:2067/2330 train_time:120788ms step_avg:58.44ms
step:2068/2330 train_time:120849ms step_avg:58.44ms
step:2069/2330 train_time:120907ms step_avg:58.44ms
step:2070/2330 train_time:120968ms step_avg:58.44ms
step:2071/2330 train_time:121028ms step_avg:58.44ms
step:2072/2330 train_time:121089ms step_avg:58.44ms
step:2073/2330 train_time:121147ms step_avg:58.44ms
step:2074/2330 train_time:121207ms step_avg:58.44ms
step:2075/2330 train_time:121264ms step_avg:58.44ms
step:2076/2330 train_time:121325ms step_avg:58.44ms
step:2077/2330 train_time:121382ms step_avg:58.44ms
step:2078/2330 train_time:121441ms step_avg:58.44ms
step:2079/2330 train_time:121499ms step_avg:58.44ms
step:2080/2330 train_time:121559ms step_avg:58.44ms
step:2081/2330 train_time:121616ms step_avg:58.44ms
step:2082/2330 train_time:121676ms step_avg:58.44ms
step:2083/2330 train_time:121733ms step_avg:58.44ms
step:2084/2330 train_time:121794ms step_avg:58.44ms
step:2085/2330 train_time:121851ms step_avg:58.44ms
step:2086/2330 train_time:121915ms step_avg:58.44ms
step:2087/2330 train_time:121973ms step_avg:58.44ms
step:2088/2330 train_time:122037ms step_avg:58.45ms
step:2089/2330 train_time:122094ms step_avg:58.45ms
step:2090/2330 train_time:122156ms step_avg:58.45ms
step:2091/2330 train_time:122213ms step_avg:58.45ms
step:2092/2330 train_time:122275ms step_avg:58.45ms
step:2093/2330 train_time:122332ms step_avg:58.45ms
step:2094/2330 train_time:122394ms step_avg:58.45ms
step:2095/2330 train_time:122451ms step_avg:58.45ms
step:2096/2330 train_time:122512ms step_avg:58.45ms
step:2097/2330 train_time:122570ms step_avg:58.45ms
step:2098/2330 train_time:122630ms step_avg:58.45ms
step:2099/2330 train_time:122688ms step_avg:58.45ms
step:2100/2330 train_time:122748ms step_avg:58.45ms
step:2101/2330 train_time:122806ms step_avg:58.45ms
step:2102/2330 train_time:122866ms step_avg:58.45ms
step:2103/2330 train_time:122924ms step_avg:58.45ms
step:2104/2330 train_time:122985ms step_avg:58.45ms
step:2105/2330 train_time:123043ms step_avg:58.45ms
step:2106/2330 train_time:123103ms step_avg:58.45ms
step:2107/2330 train_time:123160ms step_avg:58.45ms
step:2108/2330 train_time:123221ms step_avg:58.45ms
step:2109/2330 train_time:123279ms step_avg:58.45ms
step:2110/2330 train_time:123338ms step_avg:58.45ms
step:2111/2330 train_time:123395ms step_avg:58.45ms
step:2112/2330 train_time:123456ms step_avg:58.45ms
step:2113/2330 train_time:123513ms step_avg:58.45ms
step:2114/2330 train_time:123574ms step_avg:58.46ms
step:2115/2330 train_time:123631ms step_avg:58.45ms
step:2116/2330 train_time:123693ms step_avg:58.46ms
step:2117/2330 train_time:123750ms step_avg:58.46ms
step:2118/2330 train_time:123812ms step_avg:58.46ms
step:2119/2330 train_time:123869ms step_avg:58.46ms
step:2120/2330 train_time:123932ms step_avg:58.46ms
step:2121/2330 train_time:123990ms step_avg:58.46ms
step:2122/2330 train_time:124051ms step_avg:58.46ms
step:2123/2330 train_time:124109ms step_avg:58.46ms
step:2124/2330 train_time:124171ms step_avg:58.46ms
step:2125/2330 train_time:124228ms step_avg:58.46ms
step:2126/2330 train_time:124290ms step_avg:58.46ms
step:2127/2330 train_time:124348ms step_avg:58.46ms
step:2128/2330 train_time:124409ms step_avg:58.46ms
step:2129/2330 train_time:124466ms step_avg:58.46ms
step:2130/2330 train_time:124527ms step_avg:58.46ms
step:2131/2330 train_time:124585ms step_avg:58.46ms
step:2132/2330 train_time:124644ms step_avg:58.46ms
step:2133/2330 train_time:124702ms step_avg:58.46ms
step:2134/2330 train_time:124762ms step_avg:58.46ms
step:2135/2330 train_time:124819ms step_avg:58.46ms
step:2136/2330 train_time:124880ms step_avg:58.46ms
step:2137/2330 train_time:124937ms step_avg:58.46ms
step:2138/2330 train_time:125000ms step_avg:58.47ms
step:2139/2330 train_time:125056ms step_avg:58.46ms
step:2140/2330 train_time:125119ms step_avg:58.47ms
step:2141/2330 train_time:125175ms step_avg:58.47ms
step:2142/2330 train_time:125237ms step_avg:58.47ms
step:2143/2330 train_time:125294ms step_avg:58.47ms
step:2144/2330 train_time:125355ms step_avg:58.47ms
step:2145/2330 train_time:125412ms step_avg:58.47ms
step:2146/2330 train_time:125474ms step_avg:58.47ms
step:2147/2330 train_time:125531ms step_avg:58.47ms
step:2148/2330 train_time:125593ms step_avg:58.47ms
step:2149/2330 train_time:125650ms step_avg:58.47ms
step:2150/2330 train_time:125712ms step_avg:58.47ms
step:2151/2330 train_time:125769ms step_avg:58.47ms
step:2152/2330 train_time:125831ms step_avg:58.47ms
step:2153/2330 train_time:125889ms step_avg:58.47ms
step:2154/2330 train_time:125950ms step_avg:58.47ms
step:2155/2330 train_time:126007ms step_avg:58.47ms
step:2156/2330 train_time:126068ms step_avg:58.47ms
step:2157/2330 train_time:126127ms step_avg:58.47ms
step:2158/2330 train_time:126187ms step_avg:58.47ms
step:2159/2330 train_time:126244ms step_avg:58.47ms
step:2160/2330 train_time:126306ms step_avg:58.47ms
step:2161/2330 train_time:126364ms step_avg:58.47ms
step:2162/2330 train_time:126424ms step_avg:58.48ms
step:2163/2330 train_time:126481ms step_avg:58.47ms
step:2164/2330 train_time:126541ms step_avg:58.48ms
step:2165/2330 train_time:126598ms step_avg:58.47ms
step:2166/2330 train_time:126659ms step_avg:58.48ms
step:2167/2330 train_time:126716ms step_avg:58.48ms
step:2168/2330 train_time:126777ms step_avg:58.48ms
step:2169/2330 train_time:126834ms step_avg:58.48ms
step:2170/2330 train_time:126897ms step_avg:58.48ms
step:2171/2330 train_time:126954ms step_avg:58.48ms
step:2172/2330 train_time:127016ms step_avg:58.48ms
step:2173/2330 train_time:127073ms step_avg:58.48ms
step:2174/2330 train_time:127136ms step_avg:58.48ms
step:2175/2330 train_time:127193ms step_avg:58.48ms
step:2176/2330 train_time:127255ms step_avg:58.48ms
step:2177/2330 train_time:127311ms step_avg:58.48ms
step:2178/2330 train_time:127374ms step_avg:58.48ms
step:2179/2330 train_time:127431ms step_avg:58.48ms
step:2180/2330 train_time:127494ms step_avg:58.48ms
step:2181/2330 train_time:127551ms step_avg:58.48ms
step:2182/2330 train_time:127612ms step_avg:58.48ms
step:2183/2330 train_time:127669ms step_avg:58.48ms
step:2184/2330 train_time:127731ms step_avg:58.49ms
step:2185/2330 train_time:127790ms step_avg:58.48ms
step:2186/2330 train_time:127851ms step_avg:58.49ms
step:2187/2330 train_time:127908ms step_avg:58.49ms
step:2188/2330 train_time:127970ms step_avg:58.49ms
step:2189/2330 train_time:128029ms step_avg:58.49ms
step:2190/2330 train_time:128089ms step_avg:58.49ms
step:2191/2330 train_time:128147ms step_avg:58.49ms
step:2192/2330 train_time:128208ms step_avg:58.49ms
step:2193/2330 train_time:128266ms step_avg:58.49ms
step:2194/2330 train_time:128326ms step_avg:58.49ms
step:2195/2330 train_time:128385ms step_avg:58.49ms
step:2196/2330 train_time:128445ms step_avg:58.49ms
step:2197/2330 train_time:128502ms step_avg:58.49ms
step:2198/2330 train_time:128562ms step_avg:58.49ms
step:2199/2330 train_time:128619ms step_avg:58.49ms
step:2200/2330 train_time:128680ms step_avg:58.49ms
step:2201/2330 train_time:128737ms step_avg:58.49ms
step:2202/2330 train_time:128798ms step_avg:58.49ms
step:2203/2330 train_time:128855ms step_avg:58.49ms
step:2204/2330 train_time:128918ms step_avg:58.49ms
step:2205/2330 train_time:128975ms step_avg:58.49ms
step:2206/2330 train_time:129037ms step_avg:58.49ms
step:2207/2330 train_time:129094ms step_avg:58.49ms
step:2208/2330 train_time:129156ms step_avg:58.49ms
step:2209/2330 train_time:129212ms step_avg:58.49ms
step:2210/2330 train_time:129276ms step_avg:58.50ms
step:2211/2330 train_time:129333ms step_avg:58.50ms
step:2212/2330 train_time:129395ms step_avg:58.50ms
step:2213/2330 train_time:129452ms step_avg:58.50ms
step:2214/2330 train_time:129514ms step_avg:58.50ms
step:2215/2330 train_time:129571ms step_avg:58.50ms
step:2216/2330 train_time:129632ms step_avg:58.50ms
step:2217/2330 train_time:129690ms step_avg:58.50ms
step:2218/2330 train_time:129751ms step_avg:58.50ms
step:2219/2330 train_time:129808ms step_avg:58.50ms
step:2220/2330 train_time:129868ms step_avg:58.50ms
step:2221/2330 train_time:129926ms step_avg:58.50ms
step:2222/2330 train_time:129987ms step_avg:58.50ms
step:2223/2330 train_time:130045ms step_avg:58.50ms
step:2224/2330 train_time:130105ms step_avg:58.50ms
step:2225/2330 train_time:130163ms step_avg:58.50ms
step:2226/2330 train_time:130223ms step_avg:58.50ms
step:2227/2330 train_time:130280ms step_avg:58.50ms
step:2228/2330 train_time:130341ms step_avg:58.50ms
step:2229/2330 train_time:130398ms step_avg:58.50ms
step:2230/2330 train_time:130458ms step_avg:58.50ms
step:2231/2330 train_time:130515ms step_avg:58.50ms
step:2232/2330 train_time:130576ms step_avg:58.50ms
step:2233/2330 train_time:130633ms step_avg:58.50ms
step:2234/2330 train_time:130695ms step_avg:58.50ms
step:2235/2330 train_time:130751ms step_avg:58.50ms
step:2236/2330 train_time:130814ms step_avg:58.50ms
step:2237/2330 train_time:130870ms step_avg:58.50ms
step:2238/2330 train_time:130934ms step_avg:58.50ms
step:2239/2330 train_time:130991ms step_avg:58.50ms
step:2240/2330 train_time:131053ms step_avg:58.51ms
step:2241/2330 train_time:131111ms step_avg:58.51ms
step:2242/2330 train_time:131173ms step_avg:58.51ms
step:2243/2330 train_time:131230ms step_avg:58.51ms
step:2244/2330 train_time:131293ms step_avg:58.51ms
step:2245/2330 train_time:131350ms step_avg:58.51ms
step:2246/2330 train_time:131412ms step_avg:58.51ms
step:2247/2330 train_time:131470ms step_avg:58.51ms
step:2248/2330 train_time:131530ms step_avg:58.51ms
step:2249/2330 train_time:131587ms step_avg:58.51ms
step:2250/2330 train_time:131648ms step_avg:58.51ms
step:2250/2330 val_loss:3.7107 train_time:131730ms step_avg:58.55ms
step:2251/2330 train_time:131749ms step_avg:58.53ms
step:2252/2330 train_time:131770ms step_avg:58.51ms
step:2253/2330 train_time:131828ms step_avg:58.51ms
step:2254/2330 train_time:131893ms step_avg:58.52ms
step:2255/2330 train_time:131952ms step_avg:58.52ms
step:2256/2330 train_time:132014ms step_avg:58.52ms
step:2257/2330 train_time:132071ms step_avg:58.52ms
step:2258/2330 train_time:132131ms step_avg:58.52ms
step:2259/2330 train_time:132188ms step_avg:58.52ms
step:2260/2330 train_time:132248ms step_avg:58.52ms
step:2261/2330 train_time:132305ms step_avg:58.52ms
step:2262/2330 train_time:132365ms step_avg:58.52ms
step:2263/2330 train_time:132422ms step_avg:58.52ms
step:2264/2330 train_time:132481ms step_avg:58.52ms
step:2265/2330 train_time:132537ms step_avg:58.52ms
step:2266/2330 train_time:132597ms step_avg:58.52ms
step:2267/2330 train_time:132654ms step_avg:58.52ms
step:2268/2330 train_time:132717ms step_avg:58.52ms
step:2269/2330 train_time:132776ms step_avg:58.52ms
step:2270/2330 train_time:132839ms step_avg:58.52ms
step:2271/2330 train_time:132897ms step_avg:58.52ms
step:2272/2330 train_time:132960ms step_avg:58.52ms
step:2273/2330 train_time:133016ms step_avg:58.52ms
step:2274/2330 train_time:133078ms step_avg:58.52ms
step:2275/2330 train_time:133135ms step_avg:58.52ms
step:2276/2330 train_time:133197ms step_avg:58.52ms
step:2277/2330 train_time:133253ms step_avg:58.52ms
step:2278/2330 train_time:133314ms step_avg:58.52ms
step:2279/2330 train_time:133371ms step_avg:58.52ms
step:2280/2330 train_time:133432ms step_avg:58.52ms
step:2281/2330 train_time:133489ms step_avg:58.52ms
step:2282/2330 train_time:133550ms step_avg:58.52ms
step:2283/2330 train_time:133607ms step_avg:58.52ms
step:2284/2330 train_time:133668ms step_avg:58.52ms
step:2285/2330 train_time:133725ms step_avg:58.52ms
step:2286/2330 train_time:133787ms step_avg:58.52ms
step:2287/2330 train_time:133845ms step_avg:58.52ms
step:2288/2330 train_time:133906ms step_avg:58.53ms
step:2289/2330 train_time:133965ms step_avg:58.53ms
step:2290/2330 train_time:134025ms step_avg:58.53ms
step:2291/2330 train_time:134082ms step_avg:58.53ms
step:2292/2330 train_time:134143ms step_avg:58.53ms
step:2293/2330 train_time:134199ms step_avg:58.53ms
step:2294/2330 train_time:134260ms step_avg:58.53ms
step:2295/2330 train_time:134317ms step_avg:58.53ms
step:2296/2330 train_time:134378ms step_avg:58.53ms
step:2297/2330 train_time:134435ms step_avg:58.53ms
step:2298/2330 train_time:134496ms step_avg:58.53ms
step:2299/2330 train_time:134554ms step_avg:58.53ms
step:2300/2330 train_time:134614ms step_avg:58.53ms
step:2301/2330 train_time:134672ms step_avg:58.53ms
step:2302/2330 train_time:134734ms step_avg:58.53ms
step:2303/2330 train_time:134791ms step_avg:58.53ms
step:2304/2330 train_time:134853ms step_avg:58.53ms
step:2305/2330 train_time:134911ms step_avg:58.53ms
step:2306/2330 train_time:134973ms step_avg:58.53ms
step:2307/2330 train_time:135031ms step_avg:58.53ms
step:2308/2330 train_time:135092ms step_avg:58.53ms
step:2309/2330 train_time:135150ms step_avg:58.53ms
step:2310/2330 train_time:135211ms step_avg:58.53ms
step:2311/2330 train_time:135268ms step_avg:58.53ms
step:2312/2330 train_time:135329ms step_avg:58.53ms
step:2313/2330 train_time:135386ms step_avg:58.53ms
step:2314/2330 train_time:135446ms step_avg:58.53ms
step:2315/2330 train_time:135503ms step_avg:58.53ms
step:2316/2330 train_time:135563ms step_avg:58.53ms
step:2317/2330 train_time:135620ms step_avg:58.53ms
step:2318/2330 train_time:135681ms step_avg:58.53ms
step:2319/2330 train_time:135738ms step_avg:58.53ms
step:2320/2330 train_time:135800ms step_avg:58.53ms
step:2321/2330 train_time:135858ms step_avg:58.53ms
step:2322/2330 train_time:135920ms step_avg:58.54ms
step:2323/2330 train_time:135976ms step_avg:58.53ms
step:2324/2330 train_time:136041ms step_avg:58.54ms
step:2325/2330 train_time:136098ms step_avg:58.54ms
step:2326/2330 train_time:136160ms step_avg:58.54ms
step:2327/2330 train_time:136217ms step_avg:58.54ms
step:2328/2330 train_time:136278ms step_avg:58.54ms
step:2329/2330 train_time:136335ms step_avg:58.54ms
step:2330/2330 train_time:136397ms step_avg:58.54ms
step:2330/2330 val_loss:3.6955 train_time:136479ms step_avg:58.57ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
