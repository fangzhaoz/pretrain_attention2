import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--beta1', type=str, required=True)
parser.add_argument('--beta2', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_beta_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=6e-4,  betas=(float(args2.beta1), float(args2.beta2)), eps=1e-10, weight_decay=0.0, fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt", exist_ok=True)
            torch.save(log, f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 05:11:13 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   27C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   26C    P0             115W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   24C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   27C    P0             114W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   25C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   26C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:82ms step_avg:81.80ms
step:2/2330 train_time:182ms step_avg:91.11ms
step:3/2330 train_time:201ms step_avg:67.06ms
step:4/2330 train_time:220ms step_avg:55.11ms
step:5/2330 train_time:273ms step_avg:54.65ms
step:6/2330 train_time:331ms step_avg:55.19ms
step:7/2330 train_time:386ms step_avg:55.11ms
step:8/2330 train_time:444ms step_avg:55.56ms
step:9/2330 train_time:500ms step_avg:55.54ms
step:10/2330 train_time:558ms step_avg:55.82ms
step:11/2330 train_time:613ms step_avg:55.76ms
step:12/2330 train_time:672ms step_avg:56.02ms
step:13/2330 train_time:728ms step_avg:55.98ms
step:14/2330 train_time:786ms step_avg:56.13ms
step:15/2330 train_time:841ms step_avg:56.05ms
step:16/2330 train_time:899ms step_avg:56.21ms
step:17/2330 train_time:955ms step_avg:56.18ms
step:18/2330 train_time:1014ms step_avg:56.31ms
step:19/2330 train_time:1071ms step_avg:56.35ms
step:20/2330 train_time:1133ms step_avg:56.66ms
step:21/2330 train_time:1191ms step_avg:56.73ms
step:22/2330 train_time:1252ms step_avg:56.89ms
step:23/2330 train_time:1308ms step_avg:56.89ms
step:24/2330 train_time:1367ms step_avg:56.97ms
step:25/2330 train_time:1423ms step_avg:56.90ms
step:26/2330 train_time:1481ms step_avg:56.98ms
step:27/2330 train_time:1537ms step_avg:56.93ms
step:28/2330 train_time:1595ms step_avg:56.97ms
step:29/2330 train_time:1652ms step_avg:56.95ms
step:30/2330 train_time:1711ms step_avg:57.02ms
step:31/2330 train_time:1766ms step_avg:56.98ms
step:32/2330 train_time:1825ms step_avg:57.02ms
step:33/2330 train_time:1880ms step_avg:56.97ms
step:34/2330 train_time:1939ms step_avg:57.02ms
step:35/2330 train_time:1994ms step_avg:56.98ms
step:36/2330 train_time:2055ms step_avg:57.07ms
step:37/2330 train_time:2111ms step_avg:57.06ms
step:38/2330 train_time:2172ms step_avg:57.16ms
step:39/2330 train_time:2230ms step_avg:57.17ms
step:40/2330 train_time:2290ms step_avg:57.25ms
step:41/2330 train_time:2347ms step_avg:57.23ms
step:42/2330 train_time:2405ms step_avg:57.27ms
step:43/2330 train_time:2461ms step_avg:57.23ms
step:44/2330 train_time:2520ms step_avg:57.27ms
step:45/2330 train_time:2575ms step_avg:57.23ms
step:46/2330 train_time:2635ms step_avg:57.28ms
step:47/2330 train_time:2691ms step_avg:57.24ms
step:48/2330 train_time:2749ms step_avg:57.28ms
step:49/2330 train_time:2805ms step_avg:57.24ms
step:50/2330 train_time:2864ms step_avg:57.27ms
step:51/2330 train_time:2919ms step_avg:57.24ms
step:52/2330 train_time:2977ms step_avg:57.26ms
step:53/2330 train_time:3034ms step_avg:57.25ms
step:54/2330 train_time:3093ms step_avg:57.28ms
step:55/2330 train_time:3150ms step_avg:57.27ms
step:56/2330 train_time:3211ms step_avg:57.34ms
step:57/2330 train_time:3267ms step_avg:57.32ms
step:58/2330 train_time:3329ms step_avg:57.39ms
step:59/2330 train_time:3385ms step_avg:57.38ms
step:60/2330 train_time:3444ms step_avg:57.40ms
step:61/2330 train_time:3500ms step_avg:57.37ms
step:62/2330 train_time:3559ms step_avg:57.41ms
step:63/2330 train_time:3615ms step_avg:57.38ms
step:64/2330 train_time:3674ms step_avg:57.41ms
step:65/2330 train_time:3730ms step_avg:57.39ms
step:66/2330 train_time:3790ms step_avg:57.42ms
step:67/2330 train_time:3845ms step_avg:57.39ms
step:68/2330 train_time:3904ms step_avg:57.42ms
step:69/2330 train_time:3961ms step_avg:57.40ms
step:70/2330 train_time:4019ms step_avg:57.42ms
step:71/2330 train_time:4075ms step_avg:57.39ms
step:72/2330 train_time:4135ms step_avg:57.43ms
step:73/2330 train_time:4191ms step_avg:57.41ms
step:74/2330 train_time:4251ms step_avg:57.45ms
step:75/2330 train_time:4308ms step_avg:57.44ms
step:76/2330 train_time:4368ms step_avg:57.47ms
step:77/2330 train_time:4425ms step_avg:57.46ms
step:78/2330 train_time:4483ms step_avg:57.48ms
step:79/2330 train_time:4540ms step_avg:57.46ms
step:80/2330 train_time:4598ms step_avg:57.48ms
step:81/2330 train_time:4654ms step_avg:57.46ms
step:82/2330 train_time:4712ms step_avg:57.47ms
step:83/2330 train_time:4768ms step_avg:57.44ms
step:84/2330 train_time:4828ms step_avg:57.48ms
step:85/2330 train_time:4884ms step_avg:57.45ms
step:86/2330 train_time:4942ms step_avg:57.47ms
step:87/2330 train_time:4998ms step_avg:57.44ms
step:88/2330 train_time:5057ms step_avg:57.47ms
step:89/2330 train_time:5113ms step_avg:57.45ms
step:90/2330 train_time:5173ms step_avg:57.48ms
step:91/2330 train_time:5229ms step_avg:57.46ms
step:92/2330 train_time:5289ms step_avg:57.49ms
step:93/2330 train_time:5346ms step_avg:57.48ms
step:94/2330 train_time:5406ms step_avg:57.51ms
step:95/2330 train_time:5462ms step_avg:57.49ms
step:96/2330 train_time:5521ms step_avg:57.51ms
step:97/2330 train_time:5576ms step_avg:57.49ms
step:98/2330 train_time:5635ms step_avg:57.50ms
step:99/2330 train_time:5691ms step_avg:57.48ms
step:100/2330 train_time:5750ms step_avg:57.50ms
step:101/2330 train_time:5806ms step_avg:57.49ms
step:102/2330 train_time:5865ms step_avg:57.50ms
step:103/2330 train_time:5921ms step_avg:57.49ms
step:104/2330 train_time:5980ms step_avg:57.50ms
step:105/2330 train_time:6036ms step_avg:57.49ms
step:106/2330 train_time:6095ms step_avg:57.50ms
step:107/2330 train_time:6152ms step_avg:57.50ms
step:108/2330 train_time:6211ms step_avg:57.51ms
step:109/2330 train_time:6267ms step_avg:57.50ms
step:110/2330 train_time:6328ms step_avg:57.52ms
step:111/2330 train_time:6383ms step_avg:57.51ms
step:112/2330 train_time:6443ms step_avg:57.53ms
step:113/2330 train_time:6499ms step_avg:57.52ms
step:114/2330 train_time:6559ms step_avg:57.54ms
step:115/2330 train_time:6614ms step_avg:57.52ms
step:116/2330 train_time:6674ms step_avg:57.53ms
step:117/2330 train_time:6730ms step_avg:57.52ms
step:118/2330 train_time:6790ms step_avg:57.54ms
step:119/2330 train_time:6846ms step_avg:57.53ms
step:120/2330 train_time:6905ms step_avg:57.54ms
step:121/2330 train_time:6961ms step_avg:57.53ms
step:122/2330 train_time:7019ms step_avg:57.54ms
step:123/2330 train_time:7075ms step_avg:57.52ms
step:124/2330 train_time:7135ms step_avg:57.54ms
step:125/2330 train_time:7190ms step_avg:57.52ms
step:126/2330 train_time:7251ms step_avg:57.55ms
step:127/2330 train_time:7308ms step_avg:57.54ms
step:128/2330 train_time:7367ms step_avg:57.55ms
step:129/2330 train_time:7423ms step_avg:57.55ms
step:130/2330 train_time:7483ms step_avg:57.56ms
step:131/2330 train_time:7538ms step_avg:57.54ms
step:132/2330 train_time:7597ms step_avg:57.56ms
step:133/2330 train_time:7653ms step_avg:57.54ms
step:134/2330 train_time:7713ms step_avg:57.56ms
step:135/2330 train_time:7769ms step_avg:57.55ms
step:136/2330 train_time:7828ms step_avg:57.56ms
step:137/2330 train_time:7884ms step_avg:57.55ms
step:138/2330 train_time:7943ms step_avg:57.56ms
step:139/2330 train_time:7999ms step_avg:57.55ms
step:140/2330 train_time:8058ms step_avg:57.56ms
step:141/2330 train_time:8114ms step_avg:57.54ms
step:142/2330 train_time:8174ms step_avg:57.56ms
step:143/2330 train_time:8230ms step_avg:57.55ms
step:144/2330 train_time:8291ms step_avg:57.57ms
step:145/2330 train_time:8347ms step_avg:57.57ms
step:146/2330 train_time:8406ms step_avg:57.58ms
step:147/2330 train_time:8462ms step_avg:57.57ms
step:148/2330 train_time:8521ms step_avg:57.58ms
step:149/2330 train_time:8577ms step_avg:57.56ms
step:150/2330 train_time:8636ms step_avg:57.57ms
step:151/2330 train_time:8692ms step_avg:57.56ms
step:152/2330 train_time:8751ms step_avg:57.57ms
step:153/2330 train_time:8808ms step_avg:57.57ms
step:154/2330 train_time:8867ms step_avg:57.58ms
step:155/2330 train_time:8923ms step_avg:57.57ms
step:156/2330 train_time:8982ms step_avg:57.58ms
step:157/2330 train_time:9038ms step_avg:57.57ms
step:158/2330 train_time:9097ms step_avg:57.57ms
step:159/2330 train_time:9153ms step_avg:57.56ms
step:160/2330 train_time:9212ms step_avg:57.58ms
step:161/2330 train_time:9269ms step_avg:57.57ms
step:162/2330 train_time:9329ms step_avg:57.59ms
step:163/2330 train_time:9385ms step_avg:57.58ms
step:164/2330 train_time:9444ms step_avg:57.59ms
step:165/2330 train_time:9500ms step_avg:57.58ms
step:166/2330 train_time:9559ms step_avg:57.58ms
step:167/2330 train_time:9614ms step_avg:57.57ms
step:168/2330 train_time:9674ms step_avg:57.59ms
step:169/2330 train_time:9730ms step_avg:57.58ms
step:170/2330 train_time:9791ms step_avg:57.59ms
step:171/2330 train_time:9847ms step_avg:57.58ms
step:172/2330 train_time:9907ms step_avg:57.60ms
step:173/2330 train_time:9963ms step_avg:57.59ms
step:174/2330 train_time:10022ms step_avg:57.60ms
step:175/2330 train_time:10077ms step_avg:57.59ms
step:176/2330 train_time:10136ms step_avg:57.59ms
step:177/2330 train_time:10193ms step_avg:57.59ms
step:178/2330 train_time:10254ms step_avg:57.61ms
step:179/2330 train_time:10310ms step_avg:57.60ms
step:180/2330 train_time:10370ms step_avg:57.61ms
step:181/2330 train_time:10426ms step_avg:57.60ms
step:182/2330 train_time:10485ms step_avg:57.61ms
step:183/2330 train_time:10541ms step_avg:57.60ms
step:184/2330 train_time:10600ms step_avg:57.61ms
step:185/2330 train_time:10655ms step_avg:57.59ms
step:186/2330 train_time:10716ms step_avg:57.61ms
step:187/2330 train_time:10771ms step_avg:57.60ms
step:188/2330 train_time:10831ms step_avg:57.61ms
step:189/2330 train_time:10886ms step_avg:57.60ms
step:190/2330 train_time:10946ms step_avg:57.61ms
step:191/2330 train_time:11002ms step_avg:57.60ms
step:192/2330 train_time:11060ms step_avg:57.61ms
step:193/2330 train_time:11116ms step_avg:57.60ms
step:194/2330 train_time:11175ms step_avg:57.60ms
step:195/2330 train_time:11231ms step_avg:57.60ms
step:196/2330 train_time:11290ms step_avg:57.60ms
step:197/2330 train_time:11346ms step_avg:57.60ms
step:198/2330 train_time:11406ms step_avg:57.61ms
step:199/2330 train_time:11463ms step_avg:57.60ms
step:200/2330 train_time:11522ms step_avg:57.61ms
step:201/2330 train_time:11577ms step_avg:57.60ms
step:202/2330 train_time:11636ms step_avg:57.60ms
step:203/2330 train_time:11692ms step_avg:57.60ms
step:204/2330 train_time:11752ms step_avg:57.61ms
step:205/2330 train_time:11808ms step_avg:57.60ms
step:206/2330 train_time:11867ms step_avg:57.61ms
step:207/2330 train_time:11923ms step_avg:57.60ms
step:208/2330 train_time:11982ms step_avg:57.60ms
step:209/2330 train_time:12037ms step_avg:57.59ms
step:210/2330 train_time:12096ms step_avg:57.60ms
step:211/2330 train_time:12152ms step_avg:57.59ms
step:212/2330 train_time:12213ms step_avg:57.61ms
step:213/2330 train_time:12269ms step_avg:57.60ms
step:214/2330 train_time:12328ms step_avg:57.61ms
step:215/2330 train_time:12384ms step_avg:57.60ms
step:216/2330 train_time:12443ms step_avg:57.61ms
step:217/2330 train_time:12499ms step_avg:57.60ms
step:218/2330 train_time:12558ms step_avg:57.61ms
step:219/2330 train_time:12614ms step_avg:57.60ms
step:220/2330 train_time:12674ms step_avg:57.61ms
step:221/2330 train_time:12730ms step_avg:57.60ms
step:222/2330 train_time:12789ms step_avg:57.61ms
step:223/2330 train_time:12845ms step_avg:57.60ms
step:224/2330 train_time:12904ms step_avg:57.61ms
step:225/2330 train_time:12960ms step_avg:57.60ms
step:226/2330 train_time:13019ms step_avg:57.61ms
step:227/2330 train_time:13076ms step_avg:57.60ms
step:228/2330 train_time:13135ms step_avg:57.61ms
step:229/2330 train_time:13191ms step_avg:57.60ms
step:230/2330 train_time:13250ms step_avg:57.61ms
step:231/2330 train_time:13306ms step_avg:57.60ms
step:232/2330 train_time:13365ms step_avg:57.61ms
step:233/2330 train_time:13421ms step_avg:57.60ms
step:234/2330 train_time:13480ms step_avg:57.61ms
step:235/2330 train_time:13536ms step_avg:57.60ms
step:236/2330 train_time:13595ms step_avg:57.61ms
step:237/2330 train_time:13652ms step_avg:57.60ms
step:238/2330 train_time:13711ms step_avg:57.61ms
step:239/2330 train_time:13767ms step_avg:57.60ms
step:240/2330 train_time:13827ms step_avg:57.61ms
step:241/2330 train_time:13883ms step_avg:57.60ms
step:242/2330 train_time:13941ms step_avg:57.61ms
step:243/2330 train_time:13996ms step_avg:57.60ms
step:244/2330 train_time:14055ms step_avg:57.60ms
step:245/2330 train_time:14112ms step_avg:57.60ms
step:246/2330 train_time:14171ms step_avg:57.61ms
step:247/2330 train_time:14227ms step_avg:57.60ms
step:248/2330 train_time:14286ms step_avg:57.61ms
step:249/2330 train_time:14343ms step_avg:57.60ms
step:250/2330 train_time:14401ms step_avg:57.60ms
step:250/2330 val_loss:4.9639 train_time:14480ms step_avg:57.92ms
step:251/2330 train_time:14499ms step_avg:57.76ms
step:252/2330 train_time:14518ms step_avg:57.61ms
step:253/2330 train_time:14572ms step_avg:57.60ms
step:254/2330 train_time:14638ms step_avg:57.63ms
step:255/2330 train_time:14694ms step_avg:57.62ms
step:256/2330 train_time:14762ms step_avg:57.67ms
step:257/2330 train_time:14818ms step_avg:57.66ms
step:258/2330 train_time:14878ms step_avg:57.67ms
step:259/2330 train_time:14934ms step_avg:57.66ms
step:260/2330 train_time:14993ms step_avg:57.67ms
step:261/2330 train_time:15049ms step_avg:57.66ms
step:262/2330 train_time:15107ms step_avg:57.66ms
step:263/2330 train_time:15162ms step_avg:57.65ms
step:264/2330 train_time:15221ms step_avg:57.65ms
step:265/2330 train_time:15276ms step_avg:57.65ms
step:266/2330 train_time:15335ms step_avg:57.65ms
step:267/2330 train_time:15390ms step_avg:57.64ms
step:268/2330 train_time:15449ms step_avg:57.65ms
step:269/2330 train_time:15507ms step_avg:57.65ms
step:270/2330 train_time:15567ms step_avg:57.66ms
step:271/2330 train_time:15625ms step_avg:57.66ms
step:272/2330 train_time:15687ms step_avg:57.67ms
step:273/2330 train_time:15744ms step_avg:57.67ms
step:274/2330 train_time:15804ms step_avg:57.68ms
step:275/2330 train_time:15860ms step_avg:57.67ms
step:276/2330 train_time:15918ms step_avg:57.67ms
step:277/2330 train_time:15974ms step_avg:57.67ms
step:278/2330 train_time:16033ms step_avg:57.67ms
step:279/2330 train_time:16089ms step_avg:57.67ms
step:280/2330 train_time:16147ms step_avg:57.67ms
step:281/2330 train_time:16203ms step_avg:57.66ms
step:282/2330 train_time:16262ms step_avg:57.67ms
step:283/2330 train_time:16317ms step_avg:57.66ms
step:284/2330 train_time:16376ms step_avg:57.66ms
step:285/2330 train_time:16432ms step_avg:57.66ms
step:286/2330 train_time:16491ms step_avg:57.66ms
step:287/2330 train_time:16548ms step_avg:57.66ms
step:288/2330 train_time:16609ms step_avg:57.67ms
step:289/2330 train_time:16667ms step_avg:57.67ms
step:290/2330 train_time:16728ms step_avg:57.68ms
step:291/2330 train_time:16785ms step_avg:57.68ms
step:292/2330 train_time:16844ms step_avg:57.69ms
step:293/2330 train_time:16901ms step_avg:57.68ms
step:294/2330 train_time:16959ms step_avg:57.68ms
step:295/2330 train_time:17014ms step_avg:57.68ms
step:296/2330 train_time:17073ms step_avg:57.68ms
step:297/2330 train_time:17128ms step_avg:57.67ms
step:298/2330 train_time:17188ms step_avg:57.68ms
step:299/2330 train_time:17245ms step_avg:57.67ms
step:300/2330 train_time:17303ms step_avg:57.68ms
step:301/2330 train_time:17359ms step_avg:57.67ms
step:302/2330 train_time:17417ms step_avg:57.67ms
step:303/2330 train_time:17473ms step_avg:57.67ms
step:304/2330 train_time:17532ms step_avg:57.67ms
step:305/2330 train_time:17589ms step_avg:57.67ms
step:306/2330 train_time:17649ms step_avg:57.68ms
step:307/2330 train_time:17705ms step_avg:57.67ms
step:308/2330 train_time:17766ms step_avg:57.68ms
step:309/2330 train_time:17823ms step_avg:57.68ms
step:310/2330 train_time:17882ms step_avg:57.69ms
step:311/2330 train_time:17938ms step_avg:57.68ms
step:312/2330 train_time:17997ms step_avg:57.68ms
step:313/2330 train_time:18053ms step_avg:57.68ms
step:314/2330 train_time:18111ms step_avg:57.68ms
step:315/2330 train_time:18167ms step_avg:57.67ms
step:316/2330 train_time:18226ms step_avg:57.68ms
step:317/2330 train_time:18283ms step_avg:57.67ms
step:318/2330 train_time:18341ms step_avg:57.68ms
step:319/2330 train_time:18397ms step_avg:57.67ms
step:320/2330 train_time:18455ms step_avg:57.67ms
step:321/2330 train_time:18511ms step_avg:57.67ms
step:322/2330 train_time:18571ms step_avg:57.67ms
step:323/2330 train_time:18627ms step_avg:57.67ms
step:324/2330 train_time:18687ms step_avg:57.68ms
step:325/2330 train_time:18744ms step_avg:57.67ms
step:326/2330 train_time:18803ms step_avg:57.68ms
step:327/2330 train_time:18859ms step_avg:57.67ms
step:328/2330 train_time:18919ms step_avg:57.68ms
step:329/2330 train_time:18975ms step_avg:57.67ms
step:330/2330 train_time:19034ms step_avg:57.68ms
step:331/2330 train_time:19089ms step_avg:57.67ms
step:332/2330 train_time:19150ms step_avg:57.68ms
step:333/2330 train_time:19205ms step_avg:57.67ms
step:334/2330 train_time:19265ms step_avg:57.68ms
step:335/2330 train_time:19321ms step_avg:57.67ms
step:336/2330 train_time:19380ms step_avg:57.68ms
step:337/2330 train_time:19435ms step_avg:57.67ms
step:338/2330 train_time:19494ms step_avg:57.68ms
step:339/2330 train_time:19550ms step_avg:57.67ms
step:340/2330 train_time:19610ms step_avg:57.68ms
step:341/2330 train_time:19666ms step_avg:57.67ms
step:342/2330 train_time:19725ms step_avg:57.68ms
step:343/2330 train_time:19782ms step_avg:57.67ms
step:344/2330 train_time:19841ms step_avg:57.68ms
step:345/2330 train_time:19896ms step_avg:57.67ms
step:346/2330 train_time:19956ms step_avg:57.68ms
step:347/2330 train_time:20011ms step_avg:57.67ms
step:348/2330 train_time:20071ms step_avg:57.67ms
step:349/2330 train_time:20126ms step_avg:57.67ms
step:350/2330 train_time:20186ms step_avg:57.67ms
step:351/2330 train_time:20241ms step_avg:57.67ms
step:352/2330 train_time:20301ms step_avg:57.67ms
step:353/2330 train_time:20357ms step_avg:57.67ms
step:354/2330 train_time:20416ms step_avg:57.67ms
step:355/2330 train_time:20471ms step_avg:57.67ms
step:356/2330 train_time:20531ms step_avg:57.67ms
step:357/2330 train_time:20586ms step_avg:57.66ms
step:358/2330 train_time:20646ms step_avg:57.67ms
step:359/2330 train_time:20702ms step_avg:57.67ms
step:360/2330 train_time:20762ms step_avg:57.67ms
step:361/2330 train_time:20819ms step_avg:57.67ms
step:362/2330 train_time:20878ms step_avg:57.67ms
step:363/2330 train_time:20934ms step_avg:57.67ms
step:364/2330 train_time:20993ms step_avg:57.67ms
step:365/2330 train_time:21048ms step_avg:57.67ms
step:366/2330 train_time:21108ms step_avg:57.67ms
step:367/2330 train_time:21164ms step_avg:57.67ms
step:368/2330 train_time:21224ms step_avg:57.67ms
step:369/2330 train_time:21280ms step_avg:57.67ms
step:370/2330 train_time:21339ms step_avg:57.67ms
step:371/2330 train_time:21395ms step_avg:57.67ms
step:372/2330 train_time:21453ms step_avg:57.67ms
step:373/2330 train_time:21509ms step_avg:57.67ms
step:374/2330 train_time:21569ms step_avg:57.67ms
step:375/2330 train_time:21625ms step_avg:57.67ms
step:376/2330 train_time:21685ms step_avg:57.67ms
step:377/2330 train_time:21741ms step_avg:57.67ms
step:378/2330 train_time:21802ms step_avg:57.68ms
step:379/2330 train_time:21857ms step_avg:57.67ms
step:380/2330 train_time:21917ms step_avg:57.68ms
step:381/2330 train_time:21973ms step_avg:57.67ms
step:382/2330 train_time:22032ms step_avg:57.68ms
step:383/2330 train_time:22088ms step_avg:57.67ms
step:384/2330 train_time:22148ms step_avg:57.68ms
step:385/2330 train_time:22204ms step_avg:57.67ms
step:386/2330 train_time:22264ms step_avg:57.68ms
step:387/2330 train_time:22320ms step_avg:57.68ms
step:388/2330 train_time:22379ms step_avg:57.68ms
step:389/2330 train_time:22435ms step_avg:57.67ms
step:390/2330 train_time:22493ms step_avg:57.68ms
step:391/2330 train_time:22549ms step_avg:57.67ms
step:392/2330 train_time:22609ms step_avg:57.68ms
step:393/2330 train_time:22665ms step_avg:57.67ms
step:394/2330 train_time:22725ms step_avg:57.68ms
step:395/2330 train_time:22782ms step_avg:57.68ms
step:396/2330 train_time:22841ms step_avg:57.68ms
step:397/2330 train_time:22898ms step_avg:57.68ms
step:398/2330 train_time:22957ms step_avg:57.68ms
step:399/2330 train_time:23012ms step_avg:57.68ms
step:400/2330 train_time:23072ms step_avg:57.68ms
step:401/2330 train_time:23128ms step_avg:57.67ms
step:402/2330 train_time:23187ms step_avg:57.68ms
step:403/2330 train_time:23243ms step_avg:57.68ms
step:404/2330 train_time:23304ms step_avg:57.68ms
step:405/2330 train_time:23359ms step_avg:57.68ms
step:406/2330 train_time:23418ms step_avg:57.68ms
step:407/2330 train_time:23474ms step_avg:57.68ms
step:408/2330 train_time:23533ms step_avg:57.68ms
step:409/2330 train_time:23589ms step_avg:57.67ms
step:410/2330 train_time:23648ms step_avg:57.68ms
step:411/2330 train_time:23705ms step_avg:57.68ms
step:412/2330 train_time:23765ms step_avg:57.68ms
step:413/2330 train_time:23821ms step_avg:57.68ms
step:414/2330 train_time:23880ms step_avg:57.68ms
step:415/2330 train_time:23936ms step_avg:57.68ms
step:416/2330 train_time:23995ms step_avg:57.68ms
step:417/2330 train_time:24050ms step_avg:57.67ms
step:418/2330 train_time:24110ms step_avg:57.68ms
step:419/2330 train_time:24166ms step_avg:57.68ms
step:420/2330 train_time:24226ms step_avg:57.68ms
step:421/2330 train_time:24283ms step_avg:57.68ms
step:422/2330 train_time:24342ms step_avg:57.68ms
step:423/2330 train_time:24398ms step_avg:57.68ms
step:424/2330 train_time:24456ms step_avg:57.68ms
step:425/2330 train_time:24512ms step_avg:57.67ms
step:426/2330 train_time:24571ms step_avg:57.68ms
step:427/2330 train_time:24627ms step_avg:57.68ms
step:428/2330 train_time:24687ms step_avg:57.68ms
step:429/2330 train_time:24743ms step_avg:57.68ms
step:430/2330 train_time:24803ms step_avg:57.68ms
step:431/2330 train_time:24859ms step_avg:57.68ms
step:432/2330 train_time:24918ms step_avg:57.68ms
step:433/2330 train_time:24974ms step_avg:57.68ms
step:434/2330 train_time:25032ms step_avg:57.68ms
step:435/2330 train_time:25089ms step_avg:57.68ms
step:436/2330 train_time:25148ms step_avg:57.68ms
step:437/2330 train_time:25205ms step_avg:57.68ms
step:438/2330 train_time:25264ms step_avg:57.68ms
step:439/2330 train_time:25320ms step_avg:57.68ms
step:440/2330 train_time:25379ms step_avg:57.68ms
step:441/2330 train_time:25435ms step_avg:57.68ms
step:442/2330 train_time:25494ms step_avg:57.68ms
step:443/2330 train_time:25550ms step_avg:57.68ms
step:444/2330 train_time:25610ms step_avg:57.68ms
step:445/2330 train_time:25666ms step_avg:57.68ms
step:446/2330 train_time:25726ms step_avg:57.68ms
step:447/2330 train_time:25783ms step_avg:57.68ms
step:448/2330 train_time:25842ms step_avg:57.68ms
step:449/2330 train_time:25898ms step_avg:57.68ms
step:450/2330 train_time:25956ms step_avg:57.68ms
step:451/2330 train_time:26012ms step_avg:57.68ms
step:452/2330 train_time:26072ms step_avg:57.68ms
step:453/2330 train_time:26128ms step_avg:57.68ms
step:454/2330 train_time:26187ms step_avg:57.68ms
step:455/2330 train_time:26243ms step_avg:57.68ms
step:456/2330 train_time:26303ms step_avg:57.68ms
step:457/2330 train_time:26359ms step_avg:57.68ms
step:458/2330 train_time:26418ms step_avg:57.68ms
step:459/2330 train_time:26474ms step_avg:57.68ms
step:460/2330 train_time:26532ms step_avg:57.68ms
step:461/2330 train_time:26588ms step_avg:57.67ms
step:462/2330 train_time:26648ms step_avg:57.68ms
step:463/2330 train_time:26705ms step_avg:57.68ms
step:464/2330 train_time:26764ms step_avg:57.68ms
step:465/2330 train_time:26821ms step_avg:57.68ms
step:466/2330 train_time:26880ms step_avg:57.68ms
step:467/2330 train_time:26937ms step_avg:57.68ms
step:468/2330 train_time:26995ms step_avg:57.68ms
step:469/2330 train_time:27051ms step_avg:57.68ms
step:470/2330 train_time:27111ms step_avg:57.68ms
step:471/2330 train_time:27168ms step_avg:57.68ms
step:472/2330 train_time:27227ms step_avg:57.68ms
step:473/2330 train_time:27283ms step_avg:57.68ms
step:474/2330 train_time:27343ms step_avg:57.69ms
step:475/2330 train_time:27399ms step_avg:57.68ms
step:476/2330 train_time:27458ms step_avg:57.68ms
step:477/2330 train_time:27514ms step_avg:57.68ms
step:478/2330 train_time:27573ms step_avg:57.68ms
step:479/2330 train_time:27629ms step_avg:57.68ms
step:480/2330 train_time:27690ms step_avg:57.69ms
step:481/2330 train_time:27746ms step_avg:57.68ms
step:482/2330 train_time:27807ms step_avg:57.69ms
step:483/2330 train_time:27863ms step_avg:57.69ms
step:484/2330 train_time:27923ms step_avg:57.69ms
step:485/2330 train_time:27979ms step_avg:57.69ms
step:486/2330 train_time:28038ms step_avg:57.69ms
step:487/2330 train_time:28094ms step_avg:57.69ms
step:488/2330 train_time:28153ms step_avg:57.69ms
step:489/2330 train_time:28209ms step_avg:57.69ms
step:490/2330 train_time:28269ms step_avg:57.69ms
step:491/2330 train_time:28325ms step_avg:57.69ms
step:492/2330 train_time:28385ms step_avg:57.69ms
step:493/2330 train_time:28442ms step_avg:57.69ms
step:494/2330 train_time:28501ms step_avg:57.69ms
step:495/2330 train_time:28556ms step_avg:57.69ms
step:496/2330 train_time:28615ms step_avg:57.69ms
step:497/2330 train_time:28671ms step_avg:57.69ms
step:498/2330 train_time:28731ms step_avg:57.69ms
step:499/2330 train_time:28787ms step_avg:57.69ms
step:500/2330 train_time:28847ms step_avg:57.69ms
step:500/2330 val_loss:4.4779 train_time:28927ms step_avg:57.85ms
step:501/2330 train_time:28946ms step_avg:57.78ms
step:502/2330 train_time:28966ms step_avg:57.70ms
step:503/2330 train_time:29021ms step_avg:57.70ms
step:504/2330 train_time:29086ms step_avg:57.71ms
step:505/2330 train_time:29143ms step_avg:57.71ms
step:506/2330 train_time:29202ms step_avg:57.71ms
step:507/2330 train_time:29258ms step_avg:57.71ms
step:508/2330 train_time:29318ms step_avg:57.71ms
step:509/2330 train_time:29374ms step_avg:57.71ms
step:510/2330 train_time:29433ms step_avg:57.71ms
step:511/2330 train_time:29488ms step_avg:57.71ms
step:512/2330 train_time:29546ms step_avg:57.71ms
step:513/2330 train_time:29602ms step_avg:57.70ms
step:514/2330 train_time:29660ms step_avg:57.70ms
step:515/2330 train_time:29715ms step_avg:57.70ms
step:516/2330 train_time:29775ms step_avg:57.70ms
step:517/2330 train_time:29831ms step_avg:57.70ms
step:518/2330 train_time:29890ms step_avg:57.70ms
step:519/2330 train_time:29947ms step_avg:57.70ms
step:520/2330 train_time:30008ms step_avg:57.71ms
step:521/2330 train_time:30065ms step_avg:57.71ms
step:522/2330 train_time:30125ms step_avg:57.71ms
step:523/2330 train_time:30182ms step_avg:57.71ms
step:524/2330 train_time:30240ms step_avg:57.71ms
step:525/2330 train_time:30296ms step_avg:57.71ms
step:526/2330 train_time:30356ms step_avg:57.71ms
step:527/2330 train_time:30412ms step_avg:57.71ms
step:528/2330 train_time:30471ms step_avg:57.71ms
step:529/2330 train_time:30526ms step_avg:57.71ms
step:530/2330 train_time:30585ms step_avg:57.71ms
step:531/2330 train_time:30641ms step_avg:57.70ms
step:532/2330 train_time:30699ms step_avg:57.70ms
step:533/2330 train_time:30755ms step_avg:57.70ms
step:534/2330 train_time:30814ms step_avg:57.70ms
step:535/2330 train_time:30871ms step_avg:57.70ms
step:536/2330 train_time:30931ms step_avg:57.71ms
step:537/2330 train_time:30988ms step_avg:57.71ms
step:538/2330 train_time:31048ms step_avg:57.71ms
step:539/2330 train_time:31104ms step_avg:57.71ms
step:540/2330 train_time:31164ms step_avg:57.71ms
step:541/2330 train_time:31221ms step_avg:57.71ms
step:542/2330 train_time:31279ms step_avg:57.71ms
step:543/2330 train_time:31336ms step_avg:57.71ms
step:544/2330 train_time:31395ms step_avg:57.71ms
step:545/2330 train_time:31452ms step_avg:57.71ms
step:546/2330 train_time:31511ms step_avg:57.71ms
step:547/2330 train_time:31567ms step_avg:57.71ms
step:548/2330 train_time:31625ms step_avg:57.71ms
step:549/2330 train_time:31681ms step_avg:57.71ms
step:550/2330 train_time:31739ms step_avg:57.71ms
step:551/2330 train_time:31795ms step_avg:57.70ms
step:552/2330 train_time:31855ms step_avg:57.71ms
step:553/2330 train_time:31912ms step_avg:57.71ms
step:554/2330 train_time:31971ms step_avg:57.71ms
step:555/2330 train_time:32029ms step_avg:57.71ms
step:556/2330 train_time:32088ms step_avg:57.71ms
step:557/2330 train_time:32144ms step_avg:57.71ms
step:558/2330 train_time:32203ms step_avg:57.71ms
step:559/2330 train_time:32259ms step_avg:57.71ms
step:560/2330 train_time:32318ms step_avg:57.71ms
step:561/2330 train_time:32374ms step_avg:57.71ms
step:562/2330 train_time:32434ms step_avg:57.71ms
step:563/2330 train_time:32490ms step_avg:57.71ms
step:564/2330 train_time:32549ms step_avg:57.71ms
step:565/2330 train_time:32605ms step_avg:57.71ms
step:566/2330 train_time:32664ms step_avg:57.71ms
step:567/2330 train_time:32719ms step_avg:57.71ms
step:568/2330 train_time:32778ms step_avg:57.71ms
step:569/2330 train_time:32834ms step_avg:57.70ms
step:570/2330 train_time:32893ms step_avg:57.71ms
step:571/2330 train_time:32950ms step_avg:57.71ms
step:572/2330 train_time:33010ms step_avg:57.71ms
step:573/2330 train_time:33067ms step_avg:57.71ms
step:574/2330 train_time:33126ms step_avg:57.71ms
step:575/2330 train_time:33182ms step_avg:57.71ms
step:576/2330 train_time:33241ms step_avg:57.71ms
step:577/2330 train_time:33297ms step_avg:57.71ms
step:578/2330 train_time:33357ms step_avg:57.71ms
step:579/2330 train_time:33412ms step_avg:57.71ms
step:580/2330 train_time:33472ms step_avg:57.71ms
step:581/2330 train_time:33528ms step_avg:57.71ms
step:582/2330 train_time:33586ms step_avg:57.71ms
step:583/2330 train_time:33643ms step_avg:57.71ms
step:584/2330 train_time:33701ms step_avg:57.71ms
step:585/2330 train_time:33757ms step_avg:57.70ms
step:586/2330 train_time:33816ms step_avg:57.71ms
step:587/2330 train_time:33873ms step_avg:57.70ms
step:588/2330 train_time:33932ms step_avg:57.71ms
step:589/2330 train_time:33988ms step_avg:57.71ms
step:590/2330 train_time:34048ms step_avg:57.71ms
step:591/2330 train_time:34104ms step_avg:57.71ms
step:592/2330 train_time:34163ms step_avg:57.71ms
step:593/2330 train_time:34219ms step_avg:57.71ms
step:594/2330 train_time:34278ms step_avg:57.71ms
step:595/2330 train_time:34334ms step_avg:57.70ms
step:596/2330 train_time:34394ms step_avg:57.71ms
step:597/2330 train_time:34451ms step_avg:57.71ms
step:598/2330 train_time:34510ms step_avg:57.71ms
step:599/2330 train_time:34566ms step_avg:57.71ms
step:600/2330 train_time:34625ms step_avg:57.71ms
step:601/2330 train_time:34681ms step_avg:57.71ms
step:602/2330 train_time:34740ms step_avg:57.71ms
step:603/2330 train_time:34796ms step_avg:57.70ms
step:604/2330 train_time:34855ms step_avg:57.71ms
step:605/2330 train_time:34911ms step_avg:57.70ms
step:606/2330 train_time:34971ms step_avg:57.71ms
step:607/2330 train_time:35027ms step_avg:57.70ms
step:608/2330 train_time:35086ms step_avg:57.71ms
step:609/2330 train_time:35143ms step_avg:57.71ms
step:610/2330 train_time:35201ms step_avg:57.71ms
step:611/2330 train_time:35257ms step_avg:57.70ms
step:612/2330 train_time:35316ms step_avg:57.71ms
step:613/2330 train_time:35372ms step_avg:57.70ms
step:614/2330 train_time:35433ms step_avg:57.71ms
step:615/2330 train_time:35489ms step_avg:57.71ms
step:616/2330 train_time:35548ms step_avg:57.71ms
step:617/2330 train_time:35604ms step_avg:57.71ms
step:618/2330 train_time:35663ms step_avg:57.71ms
step:619/2330 train_time:35719ms step_avg:57.70ms
step:620/2330 train_time:35778ms step_avg:57.71ms
step:621/2330 train_time:35834ms step_avg:57.70ms
step:622/2330 train_time:35894ms step_avg:57.71ms
step:623/2330 train_time:35950ms step_avg:57.70ms
step:624/2330 train_time:36010ms step_avg:57.71ms
step:625/2330 train_time:36067ms step_avg:57.71ms
step:626/2330 train_time:36125ms step_avg:57.71ms
step:627/2330 train_time:36181ms step_avg:57.71ms
step:628/2330 train_time:36240ms step_avg:57.71ms
step:629/2330 train_time:36296ms step_avg:57.70ms
step:630/2330 train_time:36356ms step_avg:57.71ms
step:631/2330 train_time:36413ms step_avg:57.71ms
step:632/2330 train_time:36472ms step_avg:57.71ms
step:633/2330 train_time:36528ms step_avg:57.71ms
step:634/2330 train_time:36588ms step_avg:57.71ms
step:635/2330 train_time:36644ms step_avg:57.71ms
step:636/2330 train_time:36703ms step_avg:57.71ms
step:637/2330 train_time:36758ms step_avg:57.71ms
step:638/2330 train_time:36817ms step_avg:57.71ms
step:639/2330 train_time:36873ms step_avg:57.70ms
step:640/2330 train_time:36932ms step_avg:57.71ms
step:641/2330 train_time:36988ms step_avg:57.70ms
step:642/2330 train_time:37048ms step_avg:57.71ms
step:643/2330 train_time:37104ms step_avg:57.70ms
step:644/2330 train_time:37163ms step_avg:57.71ms
step:645/2330 train_time:37219ms step_avg:57.70ms
step:646/2330 train_time:37278ms step_avg:57.71ms
step:647/2330 train_time:37334ms step_avg:57.70ms
step:648/2330 train_time:37395ms step_avg:57.71ms
step:649/2330 train_time:37451ms step_avg:57.71ms
step:650/2330 train_time:37511ms step_avg:57.71ms
step:651/2330 train_time:37568ms step_avg:57.71ms
step:652/2330 train_time:37627ms step_avg:57.71ms
step:653/2330 train_time:37683ms step_avg:57.71ms
step:654/2330 train_time:37742ms step_avg:57.71ms
step:655/2330 train_time:37798ms step_avg:57.71ms
step:656/2330 train_time:37856ms step_avg:57.71ms
step:657/2330 train_time:37912ms step_avg:57.71ms
step:658/2330 train_time:37973ms step_avg:57.71ms
step:659/2330 train_time:38029ms step_avg:57.71ms
step:660/2330 train_time:38088ms step_avg:57.71ms
step:661/2330 train_time:38145ms step_avg:57.71ms
step:662/2330 train_time:38203ms step_avg:57.71ms
step:663/2330 train_time:38259ms step_avg:57.71ms
step:664/2330 train_time:38319ms step_avg:57.71ms
step:665/2330 train_time:38375ms step_avg:57.71ms
step:666/2330 train_time:38435ms step_avg:57.71ms
step:667/2330 train_time:38491ms step_avg:57.71ms
step:668/2330 train_time:38552ms step_avg:57.71ms
step:669/2330 train_time:38609ms step_avg:57.71ms
step:670/2330 train_time:38668ms step_avg:57.71ms
step:671/2330 train_time:38724ms step_avg:57.71ms
step:672/2330 train_time:38783ms step_avg:57.71ms
step:673/2330 train_time:38839ms step_avg:57.71ms
step:674/2330 train_time:38897ms step_avg:57.71ms
step:675/2330 train_time:38954ms step_avg:57.71ms
step:676/2330 train_time:39013ms step_avg:57.71ms
step:677/2330 train_time:39070ms step_avg:57.71ms
step:678/2330 train_time:39129ms step_avg:57.71ms
step:679/2330 train_time:39186ms step_avg:57.71ms
step:680/2330 train_time:39245ms step_avg:57.71ms
step:681/2330 train_time:39300ms step_avg:57.71ms
step:682/2330 train_time:39359ms step_avg:57.71ms
step:683/2330 train_time:39415ms step_avg:57.71ms
step:684/2330 train_time:39475ms step_avg:57.71ms
step:685/2330 train_time:39531ms step_avg:57.71ms
step:686/2330 train_time:39591ms step_avg:57.71ms
step:687/2330 train_time:39647ms step_avg:57.71ms
step:688/2330 train_time:39706ms step_avg:57.71ms
step:689/2330 train_time:39762ms step_avg:57.71ms
step:690/2330 train_time:39820ms step_avg:57.71ms
step:691/2330 train_time:39876ms step_avg:57.71ms
step:692/2330 train_time:39935ms step_avg:57.71ms
step:693/2330 train_time:39992ms step_avg:57.71ms
step:694/2330 train_time:40051ms step_avg:57.71ms
step:695/2330 train_time:40108ms step_avg:57.71ms
step:696/2330 train_time:40166ms step_avg:57.71ms
step:697/2330 train_time:40222ms step_avg:57.71ms
step:698/2330 train_time:40281ms step_avg:57.71ms
step:699/2330 train_time:40337ms step_avg:57.71ms
step:700/2330 train_time:40397ms step_avg:57.71ms
step:701/2330 train_time:40453ms step_avg:57.71ms
step:702/2330 train_time:40513ms step_avg:57.71ms
step:703/2330 train_time:40570ms step_avg:57.71ms
step:704/2330 train_time:40629ms step_avg:57.71ms
step:705/2330 train_time:40685ms step_avg:57.71ms
step:706/2330 train_time:40744ms step_avg:57.71ms
step:707/2330 train_time:40799ms step_avg:57.71ms
step:708/2330 train_time:40858ms step_avg:57.71ms
step:709/2330 train_time:40913ms step_avg:57.71ms
step:710/2330 train_time:40973ms step_avg:57.71ms
step:711/2330 train_time:41030ms step_avg:57.71ms
step:712/2330 train_time:41089ms step_avg:57.71ms
step:713/2330 train_time:41145ms step_avg:57.71ms
step:714/2330 train_time:41203ms step_avg:57.71ms
step:715/2330 train_time:41260ms step_avg:57.71ms
step:716/2330 train_time:41318ms step_avg:57.71ms
step:717/2330 train_time:41374ms step_avg:57.70ms
step:718/2330 train_time:41434ms step_avg:57.71ms
step:719/2330 train_time:41491ms step_avg:57.71ms
step:720/2330 train_time:41550ms step_avg:57.71ms
step:721/2330 train_time:41606ms step_avg:57.71ms
step:722/2330 train_time:41665ms step_avg:57.71ms
step:723/2330 train_time:41721ms step_avg:57.71ms
step:724/2330 train_time:41780ms step_avg:57.71ms
step:725/2330 train_time:41835ms step_avg:57.70ms
step:726/2330 train_time:41895ms step_avg:57.71ms
step:727/2330 train_time:41951ms step_avg:57.70ms
step:728/2330 train_time:42010ms step_avg:57.71ms
step:729/2330 train_time:42067ms step_avg:57.70ms
step:730/2330 train_time:42126ms step_avg:57.71ms
step:731/2330 train_time:42182ms step_avg:57.70ms
step:732/2330 train_time:42240ms step_avg:57.71ms
step:733/2330 train_time:42296ms step_avg:57.70ms
step:734/2330 train_time:42355ms step_avg:57.70ms
step:735/2330 train_time:42411ms step_avg:57.70ms
step:736/2330 train_time:42471ms step_avg:57.70ms
step:737/2330 train_time:42527ms step_avg:57.70ms
step:738/2330 train_time:42586ms step_avg:57.70ms
step:739/2330 train_time:42642ms step_avg:57.70ms
step:740/2330 train_time:42700ms step_avg:57.70ms
step:741/2330 train_time:42756ms step_avg:57.70ms
step:742/2330 train_time:42816ms step_avg:57.70ms
step:743/2330 train_time:42872ms step_avg:57.70ms
step:744/2330 train_time:42931ms step_avg:57.70ms
step:745/2330 train_time:42987ms step_avg:57.70ms
step:746/2330 train_time:43046ms step_avg:57.70ms
step:747/2330 train_time:43103ms step_avg:57.70ms
step:748/2330 train_time:43161ms step_avg:57.70ms
step:749/2330 train_time:43217ms step_avg:57.70ms
step:750/2330 train_time:43277ms step_avg:57.70ms
step:750/2330 val_loss:4.2582 train_time:43356ms step_avg:57.81ms
step:751/2330 train_time:43374ms step_avg:57.76ms
step:752/2330 train_time:43394ms step_avg:57.70ms
step:753/2330 train_time:43449ms step_avg:57.70ms
step:754/2330 train_time:43513ms step_avg:57.71ms
step:755/2330 train_time:43570ms step_avg:57.71ms
step:756/2330 train_time:43629ms step_avg:57.71ms
step:757/2330 train_time:43685ms step_avg:57.71ms
step:758/2330 train_time:43744ms step_avg:57.71ms
step:759/2330 train_time:43800ms step_avg:57.71ms
step:760/2330 train_time:43858ms step_avg:57.71ms
step:761/2330 train_time:43913ms step_avg:57.70ms
step:762/2330 train_time:43972ms step_avg:57.71ms
step:763/2330 train_time:44027ms step_avg:57.70ms
step:764/2330 train_time:44086ms step_avg:57.70ms
step:765/2330 train_time:44142ms step_avg:57.70ms
step:766/2330 train_time:44201ms step_avg:57.70ms
step:767/2330 train_time:44257ms step_avg:57.70ms
step:768/2330 train_time:44318ms step_avg:57.71ms
step:769/2330 train_time:44376ms step_avg:57.71ms
step:770/2330 train_time:44437ms step_avg:57.71ms
step:771/2330 train_time:44496ms step_avg:57.71ms
step:772/2330 train_time:44557ms step_avg:57.72ms
step:773/2330 train_time:44615ms step_avg:57.72ms
step:774/2330 train_time:44674ms step_avg:57.72ms
step:775/2330 train_time:44731ms step_avg:57.72ms
step:776/2330 train_time:44789ms step_avg:57.72ms
step:777/2330 train_time:44846ms step_avg:57.72ms
step:778/2330 train_time:44905ms step_avg:57.72ms
step:779/2330 train_time:44962ms step_avg:57.72ms
step:780/2330 train_time:45022ms step_avg:57.72ms
step:781/2330 train_time:45078ms step_avg:57.72ms
step:782/2330 train_time:45138ms step_avg:57.72ms
step:783/2330 train_time:45194ms step_avg:57.72ms
step:784/2330 train_time:45253ms step_avg:57.72ms
step:785/2330 train_time:45310ms step_avg:57.72ms
step:786/2330 train_time:45371ms step_avg:57.72ms
step:787/2330 train_time:45428ms step_avg:57.72ms
step:788/2330 train_time:45488ms step_avg:57.73ms
step:789/2330 train_time:45545ms step_avg:57.72ms
step:790/2330 train_time:45607ms step_avg:57.73ms
step:791/2330 train_time:45664ms step_avg:57.73ms
step:792/2330 train_time:45725ms step_avg:57.73ms
step:793/2330 train_time:45782ms step_avg:57.73ms
step:794/2330 train_time:45842ms step_avg:57.73ms
step:795/2330 train_time:45898ms step_avg:57.73ms
step:796/2330 train_time:45959ms step_avg:57.74ms
step:797/2330 train_time:46015ms step_avg:57.74ms
step:798/2330 train_time:46074ms step_avg:57.74ms
step:799/2330 train_time:46130ms step_avg:57.73ms
step:800/2330 train_time:46190ms step_avg:57.74ms
step:801/2330 train_time:46246ms step_avg:57.74ms
step:802/2330 train_time:46307ms step_avg:57.74ms
step:803/2330 train_time:46363ms step_avg:57.74ms
step:804/2330 train_time:46424ms step_avg:57.74ms
step:805/2330 train_time:46482ms step_avg:57.74ms
step:806/2330 train_time:46543ms step_avg:57.75ms
step:807/2330 train_time:46601ms step_avg:57.75ms
step:808/2330 train_time:46662ms step_avg:57.75ms
step:809/2330 train_time:46719ms step_avg:57.75ms
step:810/2330 train_time:46779ms step_avg:57.75ms
step:811/2330 train_time:46835ms step_avg:57.75ms
step:812/2330 train_time:46894ms step_avg:57.75ms
step:813/2330 train_time:46951ms step_avg:57.75ms
step:814/2330 train_time:47010ms step_avg:57.75ms
step:815/2330 train_time:47066ms step_avg:57.75ms
step:816/2330 train_time:47126ms step_avg:57.75ms
step:817/2330 train_time:47183ms step_avg:57.75ms
step:818/2330 train_time:47242ms step_avg:57.75ms
step:819/2330 train_time:47299ms step_avg:57.75ms
step:820/2330 train_time:47360ms step_avg:57.76ms
step:821/2330 train_time:47416ms step_avg:57.75ms
step:822/2330 train_time:47476ms step_avg:57.76ms
step:823/2330 train_time:47533ms step_avg:57.76ms
step:824/2330 train_time:47594ms step_avg:57.76ms
step:825/2330 train_time:47651ms step_avg:57.76ms
step:826/2330 train_time:47711ms step_avg:57.76ms
step:827/2330 train_time:47767ms step_avg:57.76ms
step:828/2330 train_time:47828ms step_avg:57.76ms
step:829/2330 train_time:47884ms step_avg:57.76ms
step:830/2330 train_time:47945ms step_avg:57.77ms
step:831/2330 train_time:48002ms step_avg:57.76ms
step:832/2330 train_time:48062ms step_avg:57.77ms
step:833/2330 train_time:48118ms step_avg:57.77ms
step:834/2330 train_time:48178ms step_avg:57.77ms
step:835/2330 train_time:48234ms step_avg:57.77ms
step:836/2330 train_time:48294ms step_avg:57.77ms
step:837/2330 train_time:48350ms step_avg:57.77ms
step:838/2330 train_time:48410ms step_avg:57.77ms
step:839/2330 train_time:48466ms step_avg:57.77ms
step:840/2330 train_time:48527ms step_avg:57.77ms
step:841/2330 train_time:48584ms step_avg:57.77ms
step:842/2330 train_time:48645ms step_avg:57.77ms
step:843/2330 train_time:48702ms step_avg:57.77ms
step:844/2330 train_time:48763ms step_avg:57.78ms
step:845/2330 train_time:48820ms step_avg:57.77ms
step:846/2330 train_time:48880ms step_avg:57.78ms
step:847/2330 train_time:48937ms step_avg:57.78ms
step:848/2330 train_time:48997ms step_avg:57.78ms
step:849/2330 train_time:49053ms step_avg:57.78ms
step:850/2330 train_time:49113ms step_avg:57.78ms
step:851/2330 train_time:49170ms step_avg:57.78ms
step:852/2330 train_time:49229ms step_avg:57.78ms
step:853/2330 train_time:49286ms step_avg:57.78ms
step:854/2330 train_time:49346ms step_avg:57.78ms
step:855/2330 train_time:49403ms step_avg:57.78ms
step:856/2330 train_time:49462ms step_avg:57.78ms
step:857/2330 train_time:49519ms step_avg:57.78ms
step:858/2330 train_time:49581ms step_avg:57.79ms
step:859/2330 train_time:49638ms step_avg:57.79ms
step:860/2330 train_time:49698ms step_avg:57.79ms
step:861/2330 train_time:49755ms step_avg:57.79ms
step:862/2330 train_time:49815ms step_avg:57.79ms
step:863/2330 train_time:49871ms step_avg:57.79ms
step:864/2330 train_time:49931ms step_avg:57.79ms
step:865/2330 train_time:49988ms step_avg:57.79ms
step:866/2330 train_time:50047ms step_avg:57.79ms
step:867/2330 train_time:50103ms step_avg:57.79ms
step:868/2330 train_time:50164ms step_avg:57.79ms
step:869/2330 train_time:50220ms step_avg:57.79ms
step:870/2330 train_time:50280ms step_avg:57.79ms
step:871/2330 train_time:50337ms step_avg:57.79ms
step:872/2330 train_time:50397ms step_avg:57.79ms
step:873/2330 train_time:50453ms step_avg:57.79ms
step:874/2330 train_time:50513ms step_avg:57.80ms
step:875/2330 train_time:50570ms step_avg:57.79ms
step:876/2330 train_time:50629ms step_avg:57.80ms
step:877/2330 train_time:50686ms step_avg:57.79ms
step:878/2330 train_time:50747ms step_avg:57.80ms
step:879/2330 train_time:50803ms step_avg:57.80ms
step:880/2330 train_time:50864ms step_avg:57.80ms
step:881/2330 train_time:50922ms step_avg:57.80ms
step:882/2330 train_time:50981ms step_avg:57.80ms
step:883/2330 train_time:51038ms step_avg:57.80ms
step:884/2330 train_time:51098ms step_avg:57.80ms
step:885/2330 train_time:51155ms step_avg:57.80ms
step:886/2330 train_time:51214ms step_avg:57.80ms
step:887/2330 train_time:51271ms step_avg:57.80ms
step:888/2330 train_time:51330ms step_avg:57.80ms
step:889/2330 train_time:51387ms step_avg:57.80ms
step:890/2330 train_time:51446ms step_avg:57.80ms
step:891/2330 train_time:51503ms step_avg:57.80ms
step:892/2330 train_time:51563ms step_avg:57.81ms
step:893/2330 train_time:51620ms step_avg:57.80ms
step:894/2330 train_time:51681ms step_avg:57.81ms
step:895/2330 train_time:51739ms step_avg:57.81ms
step:896/2330 train_time:51799ms step_avg:57.81ms
step:897/2330 train_time:51857ms step_avg:57.81ms
step:898/2330 train_time:51916ms step_avg:57.81ms
step:899/2330 train_time:51973ms step_avg:57.81ms
step:900/2330 train_time:52032ms step_avg:57.81ms
step:901/2330 train_time:52088ms step_avg:57.81ms
step:902/2330 train_time:52148ms step_avg:57.81ms
step:903/2330 train_time:52205ms step_avg:57.81ms
step:904/2330 train_time:52265ms step_avg:57.82ms
step:905/2330 train_time:52322ms step_avg:57.81ms
step:906/2330 train_time:52382ms step_avg:57.82ms
step:907/2330 train_time:52440ms step_avg:57.82ms
step:908/2330 train_time:52499ms step_avg:57.82ms
step:909/2330 train_time:52557ms step_avg:57.82ms
step:910/2330 train_time:52616ms step_avg:57.82ms
step:911/2330 train_time:52672ms step_avg:57.82ms
step:912/2330 train_time:52733ms step_avg:57.82ms
step:913/2330 train_time:52789ms step_avg:57.82ms
step:914/2330 train_time:52849ms step_avg:57.82ms
step:915/2330 train_time:52905ms step_avg:57.82ms
step:916/2330 train_time:52966ms step_avg:57.82ms
step:917/2330 train_time:53023ms step_avg:57.82ms
step:918/2330 train_time:53083ms step_avg:57.82ms
step:919/2330 train_time:53140ms step_avg:57.82ms
step:920/2330 train_time:53200ms step_avg:57.83ms
step:921/2330 train_time:53256ms step_avg:57.82ms
step:922/2330 train_time:53317ms step_avg:57.83ms
step:923/2330 train_time:53373ms step_avg:57.83ms
step:924/2330 train_time:53433ms step_avg:57.83ms
step:925/2330 train_time:53490ms step_avg:57.83ms
step:926/2330 train_time:53549ms step_avg:57.83ms
step:927/2330 train_time:53605ms step_avg:57.83ms
step:928/2330 train_time:53666ms step_avg:57.83ms
step:929/2330 train_time:53723ms step_avg:57.83ms
step:930/2330 train_time:53783ms step_avg:57.83ms
step:931/2330 train_time:53840ms step_avg:57.83ms
step:932/2330 train_time:53900ms step_avg:57.83ms
step:933/2330 train_time:53956ms step_avg:57.83ms
step:934/2330 train_time:54016ms step_avg:57.83ms
step:935/2330 train_time:54073ms step_avg:57.83ms
step:936/2330 train_time:54132ms step_avg:57.83ms
step:937/2330 train_time:54189ms step_avg:57.83ms
step:938/2330 train_time:54248ms step_avg:57.83ms
step:939/2330 train_time:54305ms step_avg:57.83ms
step:940/2330 train_time:54365ms step_avg:57.84ms
step:941/2330 train_time:54422ms step_avg:57.83ms
step:942/2330 train_time:54483ms step_avg:57.84ms
step:943/2330 train_time:54540ms step_avg:57.84ms
step:944/2330 train_time:54600ms step_avg:57.84ms
step:945/2330 train_time:54657ms step_avg:57.84ms
step:946/2330 train_time:54717ms step_avg:57.84ms
step:947/2330 train_time:54774ms step_avg:57.84ms
step:948/2330 train_time:54833ms step_avg:57.84ms
step:949/2330 train_time:54890ms step_avg:57.84ms
step:950/2330 train_time:54949ms step_avg:57.84ms
step:951/2330 train_time:55006ms step_avg:57.84ms
step:952/2330 train_time:55066ms step_avg:57.84ms
step:953/2330 train_time:55123ms step_avg:57.84ms
step:954/2330 train_time:55184ms step_avg:57.84ms
step:955/2330 train_time:55240ms step_avg:57.84ms
step:956/2330 train_time:55300ms step_avg:57.85ms
step:957/2330 train_time:55357ms step_avg:57.84ms
step:958/2330 train_time:55417ms step_avg:57.85ms
step:959/2330 train_time:55475ms step_avg:57.85ms
step:960/2330 train_time:55535ms step_avg:57.85ms
step:961/2330 train_time:55591ms step_avg:57.85ms
step:962/2330 train_time:55651ms step_avg:57.85ms
step:963/2330 train_time:55708ms step_avg:57.85ms
step:964/2330 train_time:55768ms step_avg:57.85ms
step:965/2330 train_time:55825ms step_avg:57.85ms
step:966/2330 train_time:55884ms step_avg:57.85ms
step:967/2330 train_time:55942ms step_avg:57.85ms
step:968/2330 train_time:56001ms step_avg:57.85ms
step:969/2330 train_time:56058ms step_avg:57.85ms
step:970/2330 train_time:56118ms step_avg:57.85ms
step:971/2330 train_time:56174ms step_avg:57.85ms
step:972/2330 train_time:56235ms step_avg:57.85ms
step:973/2330 train_time:56291ms step_avg:57.85ms
step:974/2330 train_time:56351ms step_avg:57.86ms
step:975/2330 train_time:56407ms step_avg:57.85ms
step:976/2330 train_time:56467ms step_avg:57.86ms
step:977/2330 train_time:56524ms step_avg:57.85ms
step:978/2330 train_time:56584ms step_avg:57.86ms
step:979/2330 train_time:56640ms step_avg:57.86ms
step:980/2330 train_time:56701ms step_avg:57.86ms
step:981/2330 train_time:56758ms step_avg:57.86ms
step:982/2330 train_time:56817ms step_avg:57.86ms
step:983/2330 train_time:56874ms step_avg:57.86ms
step:984/2330 train_time:56933ms step_avg:57.86ms
step:985/2330 train_time:56990ms step_avg:57.86ms
step:986/2330 train_time:57050ms step_avg:57.86ms
step:987/2330 train_time:57106ms step_avg:57.86ms
step:988/2330 train_time:57167ms step_avg:57.86ms
step:989/2330 train_time:57225ms step_avg:57.86ms
step:990/2330 train_time:57284ms step_avg:57.86ms
step:991/2330 train_time:57341ms step_avg:57.86ms
step:992/2330 train_time:57401ms step_avg:57.86ms
step:993/2330 train_time:57459ms step_avg:57.86ms
step:994/2330 train_time:57519ms step_avg:57.87ms
step:995/2330 train_time:57576ms step_avg:57.86ms
step:996/2330 train_time:57635ms step_avg:57.87ms
step:997/2330 train_time:57692ms step_avg:57.87ms
step:998/2330 train_time:57752ms step_avg:57.87ms
step:999/2330 train_time:57809ms step_avg:57.87ms
step:1000/2330 train_time:57869ms step_avg:57.87ms
step:1000/2330 val_loss:4.1097 train_time:57949ms step_avg:57.95ms
step:1001/2330 train_time:57968ms step_avg:57.91ms
step:1002/2330 train_time:57988ms step_avg:57.87ms
step:1003/2330 train_time:58042ms step_avg:57.87ms
step:1004/2330 train_time:58104ms step_avg:57.87ms
step:1005/2330 train_time:58160ms step_avg:57.87ms
step:1006/2330 train_time:58224ms step_avg:57.88ms
step:1007/2330 train_time:58280ms step_avg:57.88ms
step:1008/2330 train_time:58339ms step_avg:57.88ms
step:1009/2330 train_time:58395ms step_avg:57.87ms
step:1010/2330 train_time:58455ms step_avg:57.88ms
step:1011/2330 train_time:58511ms step_avg:57.87ms
step:1012/2330 train_time:58570ms step_avg:57.88ms
step:1013/2330 train_time:58626ms step_avg:57.87ms
step:1014/2330 train_time:58685ms step_avg:57.87ms
step:1015/2330 train_time:58741ms step_avg:57.87ms
step:1016/2330 train_time:58800ms step_avg:57.87ms
step:1017/2330 train_time:58861ms step_avg:57.88ms
step:1018/2330 train_time:58922ms step_avg:57.88ms
step:1019/2330 train_time:58981ms step_avg:57.88ms
step:1020/2330 train_time:59042ms step_avg:57.88ms
step:1021/2330 train_time:59098ms step_avg:57.88ms
step:1022/2330 train_time:59159ms step_avg:57.89ms
step:1023/2330 train_time:59215ms step_avg:57.88ms
step:1024/2330 train_time:59275ms step_avg:57.89ms
step:1025/2330 train_time:59332ms step_avg:57.88ms
step:1026/2330 train_time:59392ms step_avg:57.89ms
step:1027/2330 train_time:59449ms step_avg:57.89ms
step:1028/2330 train_time:59508ms step_avg:57.89ms
step:1029/2330 train_time:59564ms step_avg:57.89ms
step:1030/2330 train_time:59623ms step_avg:57.89ms
step:1031/2330 train_time:59680ms step_avg:57.89ms
step:1032/2330 train_time:59739ms step_avg:57.89ms
step:1033/2330 train_time:59796ms step_avg:57.89ms
step:1034/2330 train_time:59857ms step_avg:57.89ms
step:1035/2330 train_time:59915ms step_avg:57.89ms
step:1036/2330 train_time:59977ms step_avg:57.89ms
step:1037/2330 train_time:60034ms step_avg:57.89ms
step:1038/2330 train_time:60095ms step_avg:57.89ms
step:1039/2330 train_time:60151ms step_avg:57.89ms
step:1040/2330 train_time:60212ms step_avg:57.90ms
step:1041/2330 train_time:60269ms step_avg:57.89ms
step:1042/2330 train_time:60329ms step_avg:57.90ms
step:1043/2330 train_time:60386ms step_avg:57.90ms
step:1044/2330 train_time:60444ms step_avg:57.90ms
step:1045/2330 train_time:60501ms step_avg:57.90ms
step:1046/2330 train_time:60560ms step_avg:57.90ms
step:1047/2330 train_time:60617ms step_avg:57.90ms
step:1048/2330 train_time:60676ms step_avg:57.90ms
step:1049/2330 train_time:60733ms step_avg:57.90ms
step:1050/2330 train_time:60793ms step_avg:57.90ms
step:1051/2330 train_time:60852ms step_avg:57.90ms
step:1052/2330 train_time:60912ms step_avg:57.90ms
step:1053/2330 train_time:60971ms step_avg:57.90ms
step:1054/2330 train_time:61030ms step_avg:57.90ms
step:1055/2330 train_time:61088ms step_avg:57.90ms
step:1056/2330 train_time:61148ms step_avg:57.90ms
step:1057/2330 train_time:61205ms step_avg:57.90ms
step:1058/2330 train_time:61264ms step_avg:57.91ms
step:1059/2330 train_time:61321ms step_avg:57.90ms
step:1060/2330 train_time:61380ms step_avg:57.91ms
step:1061/2330 train_time:61436ms step_avg:57.90ms
step:1062/2330 train_time:61497ms step_avg:57.91ms
step:1063/2330 train_time:61553ms step_avg:57.91ms
step:1064/2330 train_time:61613ms step_avg:57.91ms
step:1065/2330 train_time:61670ms step_avg:57.91ms
step:1066/2330 train_time:61730ms step_avg:57.91ms
step:1067/2330 train_time:61787ms step_avg:57.91ms
step:1068/2330 train_time:61847ms step_avg:57.91ms
step:1069/2330 train_time:61904ms step_avg:57.91ms
step:1070/2330 train_time:61964ms step_avg:57.91ms
step:1071/2330 train_time:62022ms step_avg:57.91ms
step:1072/2330 train_time:62081ms step_avg:57.91ms
step:1073/2330 train_time:62138ms step_avg:57.91ms
step:1074/2330 train_time:62199ms step_avg:57.91ms
step:1075/2330 train_time:62256ms step_avg:57.91ms
step:1076/2330 train_time:62316ms step_avg:57.91ms
step:1077/2330 train_time:62373ms step_avg:57.91ms
step:1078/2330 train_time:62433ms step_avg:57.92ms
step:1079/2330 train_time:62489ms step_avg:57.91ms
step:1080/2330 train_time:62549ms step_avg:57.92ms
step:1081/2330 train_time:62606ms step_avg:57.91ms
step:1082/2330 train_time:62665ms step_avg:57.92ms
step:1083/2330 train_time:62722ms step_avg:57.91ms
step:1084/2330 train_time:62782ms step_avg:57.92ms
step:1085/2330 train_time:62838ms step_avg:57.92ms
step:1086/2330 train_time:62899ms step_avg:57.92ms
step:1087/2330 train_time:62955ms step_avg:57.92ms
step:1088/2330 train_time:63017ms step_avg:57.92ms
step:1089/2330 train_time:63075ms step_avg:57.92ms
step:1090/2330 train_time:63135ms step_avg:57.92ms
step:1091/2330 train_time:63193ms step_avg:57.92ms
step:1092/2330 train_time:63253ms step_avg:57.92ms
step:1093/2330 train_time:63310ms step_avg:57.92ms
step:1094/2330 train_time:63370ms step_avg:57.92ms
step:1095/2330 train_time:63427ms step_avg:57.92ms
step:1096/2330 train_time:63487ms step_avg:57.93ms
step:1097/2330 train_time:63544ms step_avg:57.93ms
step:1098/2330 train_time:63603ms step_avg:57.93ms
step:1099/2330 train_time:63660ms step_avg:57.93ms
step:1100/2330 train_time:63719ms step_avg:57.93ms
step:1101/2330 train_time:63776ms step_avg:57.93ms
step:1102/2330 train_time:63836ms step_avg:57.93ms
step:1103/2330 train_time:63892ms step_avg:57.93ms
step:1104/2330 train_time:63953ms step_avg:57.93ms
step:1105/2330 train_time:64010ms step_avg:57.93ms
step:1106/2330 train_time:64071ms step_avg:57.93ms
step:1107/2330 train_time:64129ms step_avg:57.93ms
step:1108/2330 train_time:64189ms step_avg:57.93ms
step:1109/2330 train_time:64247ms step_avg:57.93ms
step:1110/2330 train_time:64307ms step_avg:57.93ms
step:1111/2330 train_time:64364ms step_avg:57.93ms
step:1112/2330 train_time:64423ms step_avg:57.93ms
step:1113/2330 train_time:64481ms step_avg:57.93ms
step:1114/2330 train_time:64540ms step_avg:57.93ms
step:1115/2330 train_time:64596ms step_avg:57.93ms
step:1116/2330 train_time:64657ms step_avg:57.94ms
step:1117/2330 train_time:64714ms step_avg:57.94ms
step:1118/2330 train_time:64774ms step_avg:57.94ms
step:1119/2330 train_time:64830ms step_avg:57.94ms
step:1120/2330 train_time:64890ms step_avg:57.94ms
step:1121/2330 train_time:64946ms step_avg:57.94ms
step:1122/2330 train_time:65007ms step_avg:57.94ms
step:1123/2330 train_time:65064ms step_avg:57.94ms
step:1124/2330 train_time:65124ms step_avg:57.94ms
step:1125/2330 train_time:65181ms step_avg:57.94ms
step:1126/2330 train_time:65240ms step_avg:57.94ms
step:1127/2330 train_time:65297ms step_avg:57.94ms
step:1128/2330 train_time:65358ms step_avg:57.94ms
step:1129/2330 train_time:65414ms step_avg:57.94ms
step:1130/2330 train_time:65474ms step_avg:57.94ms
step:1131/2330 train_time:65531ms step_avg:57.94ms
step:1132/2330 train_time:65591ms step_avg:57.94ms
step:1133/2330 train_time:65648ms step_avg:57.94ms
step:1134/2330 train_time:65708ms step_avg:57.94ms
step:1135/2330 train_time:65766ms step_avg:57.94ms
step:1136/2330 train_time:65826ms step_avg:57.95ms
step:1137/2330 train_time:65884ms step_avg:57.95ms
step:1138/2330 train_time:65943ms step_avg:57.95ms
step:1139/2330 train_time:66000ms step_avg:57.95ms
step:1140/2330 train_time:66060ms step_avg:57.95ms
step:1141/2330 train_time:66117ms step_avg:57.95ms
step:1142/2330 train_time:66178ms step_avg:57.95ms
step:1143/2330 train_time:66234ms step_avg:57.95ms
step:1144/2330 train_time:66294ms step_avg:57.95ms
step:1145/2330 train_time:66352ms step_avg:57.95ms
step:1146/2330 train_time:66411ms step_avg:57.95ms
step:1147/2330 train_time:66468ms step_avg:57.95ms
step:1148/2330 train_time:66528ms step_avg:57.95ms
step:1149/2330 train_time:66585ms step_avg:57.95ms
step:1150/2330 train_time:66645ms step_avg:57.95ms
step:1151/2330 train_time:66701ms step_avg:57.95ms
step:1152/2330 train_time:66761ms step_avg:57.95ms
step:1153/2330 train_time:66817ms step_avg:57.95ms
step:1154/2330 train_time:66878ms step_avg:57.95ms
step:1155/2330 train_time:66935ms step_avg:57.95ms
step:1156/2330 train_time:66995ms step_avg:57.95ms
step:1157/2330 train_time:67052ms step_avg:57.95ms
step:1158/2330 train_time:67113ms step_avg:57.96ms
step:1159/2330 train_time:67170ms step_avg:57.96ms
step:1160/2330 train_time:67230ms step_avg:57.96ms
step:1161/2330 train_time:67287ms step_avg:57.96ms
step:1162/2330 train_time:67347ms step_avg:57.96ms
step:1163/2330 train_time:67403ms step_avg:57.96ms
step:1164/2330 train_time:67463ms step_avg:57.96ms
step:1165/2330 train_time:67520ms step_avg:57.96ms
step:1166/2330 train_time:67580ms step_avg:57.96ms
step:1167/2330 train_time:67637ms step_avg:57.96ms
step:1168/2330 train_time:67697ms step_avg:57.96ms
step:1169/2330 train_time:67753ms step_avg:57.96ms
step:1170/2330 train_time:67814ms step_avg:57.96ms
step:1171/2330 train_time:67871ms step_avg:57.96ms
step:1172/2330 train_time:67931ms step_avg:57.96ms
step:1173/2330 train_time:67989ms step_avg:57.96ms
step:1174/2330 train_time:68049ms step_avg:57.96ms
step:1175/2330 train_time:68106ms step_avg:57.96ms
step:1176/2330 train_time:68166ms step_avg:57.96ms
step:1177/2330 train_time:68222ms step_avg:57.96ms
step:1178/2330 train_time:68282ms step_avg:57.96ms
step:1179/2330 train_time:68339ms step_avg:57.96ms
step:1180/2330 train_time:68399ms step_avg:57.97ms
step:1181/2330 train_time:68456ms step_avg:57.96ms
step:1182/2330 train_time:68517ms step_avg:57.97ms
step:1183/2330 train_time:68573ms step_avg:57.97ms
step:1184/2330 train_time:68634ms step_avg:57.97ms
step:1185/2330 train_time:68690ms step_avg:57.97ms
step:1186/2330 train_time:68751ms step_avg:57.97ms
step:1187/2330 train_time:68808ms step_avg:57.97ms
step:1188/2330 train_time:68868ms step_avg:57.97ms
step:1189/2330 train_time:68925ms step_avg:57.97ms
step:1190/2330 train_time:68986ms step_avg:57.97ms
step:1191/2330 train_time:69043ms step_avg:57.97ms
step:1192/2330 train_time:69102ms step_avg:57.97ms
step:1193/2330 train_time:69159ms step_avg:57.97ms
step:1194/2330 train_time:69219ms step_avg:57.97ms
step:1195/2330 train_time:69276ms step_avg:57.97ms
step:1196/2330 train_time:69336ms step_avg:57.97ms
step:1197/2330 train_time:69393ms step_avg:57.97ms
step:1198/2330 train_time:69453ms step_avg:57.97ms
step:1199/2330 train_time:69510ms step_avg:57.97ms
step:1200/2330 train_time:69571ms step_avg:57.98ms
step:1201/2330 train_time:69627ms step_avg:57.97ms
step:1202/2330 train_time:69687ms step_avg:57.98ms
step:1203/2330 train_time:69744ms step_avg:57.98ms
step:1204/2330 train_time:69803ms step_avg:57.98ms
step:1205/2330 train_time:69860ms step_avg:57.98ms
step:1206/2330 train_time:69920ms step_avg:57.98ms
step:1207/2330 train_time:69976ms step_avg:57.98ms
step:1208/2330 train_time:70036ms step_avg:57.98ms
step:1209/2330 train_time:70093ms step_avg:57.98ms
step:1210/2330 train_time:70154ms step_avg:57.98ms
step:1211/2330 train_time:70212ms step_avg:57.98ms
step:1212/2330 train_time:70271ms step_avg:57.98ms
step:1213/2330 train_time:70329ms step_avg:57.98ms
step:1214/2330 train_time:70389ms step_avg:57.98ms
step:1215/2330 train_time:70445ms step_avg:57.98ms
step:1216/2330 train_time:70505ms step_avg:57.98ms
step:1217/2330 train_time:70562ms step_avg:57.98ms
step:1218/2330 train_time:70622ms step_avg:57.98ms
step:1219/2330 train_time:70678ms step_avg:57.98ms
step:1220/2330 train_time:70739ms step_avg:57.98ms
step:1221/2330 train_time:70795ms step_avg:57.98ms
step:1222/2330 train_time:70856ms step_avg:57.98ms
step:1223/2330 train_time:70913ms step_avg:57.98ms
step:1224/2330 train_time:70974ms step_avg:57.99ms
step:1225/2330 train_time:71031ms step_avg:57.98ms
step:1226/2330 train_time:71091ms step_avg:57.99ms
step:1227/2330 train_time:71149ms step_avg:57.99ms
step:1228/2330 train_time:71209ms step_avg:57.99ms
step:1229/2330 train_time:71265ms step_avg:57.99ms
step:1230/2330 train_time:71325ms step_avg:57.99ms
step:1231/2330 train_time:71382ms step_avg:57.99ms
step:1232/2330 train_time:71441ms step_avg:57.99ms
step:1233/2330 train_time:71498ms step_avg:57.99ms
step:1234/2330 train_time:71558ms step_avg:57.99ms
step:1235/2330 train_time:71615ms step_avg:57.99ms
step:1236/2330 train_time:71676ms step_avg:57.99ms
step:1237/2330 train_time:71732ms step_avg:57.99ms
step:1238/2330 train_time:71792ms step_avg:57.99ms
step:1239/2330 train_time:71849ms step_avg:57.99ms
step:1240/2330 train_time:71909ms step_avg:57.99ms
step:1241/2330 train_time:71966ms step_avg:57.99ms
step:1242/2330 train_time:72026ms step_avg:57.99ms
step:1243/2330 train_time:72083ms step_avg:57.99ms
step:1244/2330 train_time:72142ms step_avg:57.99ms
step:1245/2330 train_time:72199ms step_avg:57.99ms
step:1246/2330 train_time:72259ms step_avg:57.99ms
step:1247/2330 train_time:72316ms step_avg:57.99ms
step:1248/2330 train_time:72376ms step_avg:57.99ms
step:1249/2330 train_time:72433ms step_avg:57.99ms
step:1250/2330 train_time:72494ms step_avg:58.00ms
step:1250/2330 val_loss:4.0286 train_time:72575ms step_avg:58.06ms
step:1251/2330 train_time:72594ms step_avg:58.03ms
step:1252/2330 train_time:72614ms step_avg:58.00ms
step:1253/2330 train_time:72673ms step_avg:58.00ms
step:1254/2330 train_time:72736ms step_avg:58.00ms
step:1255/2330 train_time:72793ms step_avg:58.00ms
step:1256/2330 train_time:72853ms step_avg:58.00ms
step:1257/2330 train_time:72909ms step_avg:58.00ms
step:1258/2330 train_time:72969ms step_avg:58.00ms
step:1259/2330 train_time:73026ms step_avg:58.00ms
step:1260/2330 train_time:73086ms step_avg:58.00ms
step:1261/2330 train_time:73142ms step_avg:58.00ms
step:1262/2330 train_time:73202ms step_avg:58.00ms
step:1263/2330 train_time:73258ms step_avg:58.00ms
step:1264/2330 train_time:73317ms step_avg:58.00ms
step:1265/2330 train_time:73373ms step_avg:58.00ms
step:1266/2330 train_time:73432ms step_avg:58.00ms
step:1267/2330 train_time:73489ms step_avg:58.00ms
step:1268/2330 train_time:73550ms step_avg:58.00ms
step:1269/2330 train_time:73609ms step_avg:58.01ms
step:1270/2330 train_time:73670ms step_avg:58.01ms
step:1271/2330 train_time:73729ms step_avg:58.01ms
step:1272/2330 train_time:73790ms step_avg:58.01ms
step:1273/2330 train_time:73847ms step_avg:58.01ms
step:1274/2330 train_time:73907ms step_avg:58.01ms
step:1275/2330 train_time:73964ms step_avg:58.01ms
step:1276/2330 train_time:74024ms step_avg:58.01ms
step:1277/2330 train_time:74080ms step_avg:58.01ms
step:1278/2330 train_time:74140ms step_avg:58.01ms
step:1279/2330 train_time:74196ms step_avg:58.01ms
step:1280/2330 train_time:74256ms step_avg:58.01ms
step:1281/2330 train_time:74312ms step_avg:58.01ms
step:1282/2330 train_time:74372ms step_avg:58.01ms
step:1283/2330 train_time:74428ms step_avg:58.01ms
step:1284/2330 train_time:74488ms step_avg:58.01ms
step:1285/2330 train_time:74545ms step_avg:58.01ms
step:1286/2330 train_time:74606ms step_avg:58.01ms
step:1287/2330 train_time:74663ms step_avg:58.01ms
step:1288/2330 train_time:74724ms step_avg:58.02ms
step:1289/2330 train_time:74781ms step_avg:58.01ms
step:1290/2330 train_time:74841ms step_avg:58.02ms
step:1291/2330 train_time:74898ms step_avg:58.02ms
step:1292/2330 train_time:75518ms step_avg:58.45ms
step:1293/2330 train_time:76006ms step_avg:58.78ms
step:1294/2330 train_time:76027ms step_avg:58.75ms
step:1295/2330 train_time:76068ms step_avg:58.74ms
step:1296/2330 train_time:76126ms step_avg:58.74ms
step:1297/2330 train_time:76182ms step_avg:58.74ms
step:1298/2330 train_time:76241ms step_avg:58.74ms
step:1299/2330 train_time:76297ms step_avg:58.74ms
step:1300/2330 train_time:76356ms step_avg:58.74ms
step:1301/2330 train_time:76412ms step_avg:58.73ms
step:1302/2330 train_time:76471ms step_avg:58.73ms
step:1303/2330 train_time:76527ms step_avg:58.73ms
step:1304/2330 train_time:76586ms step_avg:58.73ms
step:1305/2330 train_time:76642ms step_avg:58.73ms
step:1306/2330 train_time:76701ms step_avg:58.73ms
step:1307/2330 train_time:76758ms step_avg:58.73ms
step:1308/2330 train_time:76816ms step_avg:58.73ms
step:1309/2330 train_time:76874ms step_avg:58.73ms
step:1310/2330 train_time:76938ms step_avg:58.73ms
step:1311/2330 train_time:76997ms step_avg:58.73ms
step:1312/2330 train_time:77059ms step_avg:58.73ms
step:1313/2330 train_time:77116ms step_avg:58.73ms
step:1314/2330 train_time:77179ms step_avg:58.74ms
step:1315/2330 train_time:77235ms step_avg:58.73ms
step:1316/2330 train_time:77296ms step_avg:58.74ms
step:1317/2330 train_time:77352ms step_avg:58.73ms
step:1318/2330 train_time:77411ms step_avg:58.73ms
step:1319/2330 train_time:77467ms step_avg:58.73ms
step:1320/2330 train_time:77528ms step_avg:58.73ms
step:1321/2330 train_time:77584ms step_avg:58.73ms
step:1322/2330 train_time:77644ms step_avg:58.73ms
step:1323/2330 train_time:77700ms step_avg:58.73ms
step:1324/2330 train_time:77759ms step_avg:58.73ms
step:1325/2330 train_time:77816ms step_avg:58.73ms
step:1326/2330 train_time:77876ms step_avg:58.73ms
step:1327/2330 train_time:77934ms step_avg:58.73ms
step:1328/2330 train_time:77995ms step_avg:58.73ms
step:1329/2330 train_time:78053ms step_avg:58.73ms
step:1330/2330 train_time:78115ms step_avg:58.73ms
step:1331/2330 train_time:78172ms step_avg:58.73ms
step:1332/2330 train_time:78233ms step_avg:58.73ms
step:1333/2330 train_time:78290ms step_avg:58.73ms
step:1334/2330 train_time:78350ms step_avg:58.73ms
step:1335/2330 train_time:78407ms step_avg:58.73ms
step:1336/2330 train_time:78467ms step_avg:58.73ms
step:1337/2330 train_time:78524ms step_avg:58.73ms
step:1338/2330 train_time:78583ms step_avg:58.73ms
step:1339/2330 train_time:78639ms step_avg:58.73ms
step:1340/2330 train_time:78698ms step_avg:58.73ms
step:1341/2330 train_time:78755ms step_avg:58.73ms
step:1342/2330 train_time:78815ms step_avg:58.73ms
step:1343/2330 train_time:78872ms step_avg:58.73ms
step:1344/2330 train_time:78932ms step_avg:58.73ms
step:1345/2330 train_time:78990ms step_avg:58.73ms
step:1346/2330 train_time:79050ms step_avg:58.73ms
step:1347/2330 train_time:79109ms step_avg:58.73ms
step:1348/2330 train_time:79169ms step_avg:58.73ms
step:1349/2330 train_time:79226ms step_avg:58.73ms
step:1350/2330 train_time:79287ms step_avg:58.73ms
step:1351/2330 train_time:79344ms step_avg:58.73ms
step:1352/2330 train_time:79404ms step_avg:58.73ms
step:1353/2330 train_time:79460ms step_avg:58.73ms
step:1354/2330 train_time:79520ms step_avg:58.73ms
step:1355/2330 train_time:79577ms step_avg:58.73ms
step:1356/2330 train_time:79636ms step_avg:58.73ms
step:1357/2330 train_time:79692ms step_avg:58.73ms
step:1358/2330 train_time:79752ms step_avg:58.73ms
step:1359/2330 train_time:79809ms step_avg:58.73ms
step:1360/2330 train_time:79868ms step_avg:58.73ms
step:1361/2330 train_time:79926ms step_avg:58.73ms
step:1362/2330 train_time:79986ms step_avg:58.73ms
step:1363/2330 train_time:80043ms step_avg:58.73ms
step:1364/2330 train_time:80103ms step_avg:58.73ms
step:1365/2330 train_time:80159ms step_avg:58.72ms
step:1366/2330 train_time:80220ms step_avg:58.73ms
step:1367/2330 train_time:80277ms step_avg:58.72ms
step:1368/2330 train_time:80336ms step_avg:58.73ms
step:1369/2330 train_time:80393ms step_avg:58.72ms
step:1370/2330 train_time:80454ms step_avg:58.73ms
step:1371/2330 train_time:80511ms step_avg:58.72ms
step:1372/2330 train_time:80572ms step_avg:58.73ms
step:1373/2330 train_time:80629ms step_avg:58.72ms
step:1374/2330 train_time:80688ms step_avg:58.72ms
step:1375/2330 train_time:80745ms step_avg:58.72ms
step:1376/2330 train_time:80804ms step_avg:58.72ms
step:1377/2330 train_time:80861ms step_avg:58.72ms
step:1378/2330 train_time:80921ms step_avg:58.72ms
step:1379/2330 train_time:80978ms step_avg:58.72ms
step:1380/2330 train_time:81037ms step_avg:58.72ms
step:1381/2330 train_time:81094ms step_avg:58.72ms
step:1382/2330 train_time:81156ms step_avg:58.72ms
step:1383/2330 train_time:81213ms step_avg:58.72ms
step:1384/2330 train_time:81273ms step_avg:58.72ms
step:1385/2330 train_time:81330ms step_avg:58.72ms
step:1386/2330 train_time:81390ms step_avg:58.72ms
step:1387/2330 train_time:81448ms step_avg:58.72ms
step:1388/2330 train_time:81508ms step_avg:58.72ms
step:1389/2330 train_time:81565ms step_avg:58.72ms
step:1390/2330 train_time:81624ms step_avg:58.72ms
step:1391/2330 train_time:81681ms step_avg:58.72ms
step:1392/2330 train_time:81740ms step_avg:58.72ms
step:1393/2330 train_time:81796ms step_avg:58.72ms
step:1394/2330 train_time:81857ms step_avg:58.72ms
step:1395/2330 train_time:81914ms step_avg:58.72ms
step:1396/2330 train_time:81973ms step_avg:58.72ms
step:1397/2330 train_time:82031ms step_avg:58.72ms
step:1398/2330 train_time:82091ms step_avg:58.72ms
step:1399/2330 train_time:82149ms step_avg:58.72ms
step:1400/2330 train_time:82208ms step_avg:58.72ms
step:1401/2330 train_time:82266ms step_avg:58.72ms
step:1402/2330 train_time:82326ms step_avg:58.72ms
step:1403/2330 train_time:82383ms step_avg:58.72ms
step:1404/2330 train_time:82442ms step_avg:58.72ms
step:1405/2330 train_time:82499ms step_avg:58.72ms
step:1406/2330 train_time:82559ms step_avg:58.72ms
step:1407/2330 train_time:82616ms step_avg:58.72ms
step:1408/2330 train_time:82675ms step_avg:58.72ms
step:1409/2330 train_time:82731ms step_avg:58.72ms
step:1410/2330 train_time:82792ms step_avg:58.72ms
step:1411/2330 train_time:82849ms step_avg:58.72ms
step:1412/2330 train_time:82909ms step_avg:58.72ms
step:1413/2330 train_time:82966ms step_avg:58.72ms
step:1414/2330 train_time:83026ms step_avg:58.72ms
step:1415/2330 train_time:83084ms step_avg:58.72ms
step:1416/2330 train_time:83143ms step_avg:58.72ms
step:1417/2330 train_time:83201ms step_avg:58.72ms
step:1418/2330 train_time:83260ms step_avg:58.72ms
step:1419/2330 train_time:83317ms step_avg:58.72ms
step:1420/2330 train_time:83377ms step_avg:58.72ms
step:1421/2330 train_time:83434ms step_avg:58.71ms
step:1422/2330 train_time:83494ms step_avg:58.72ms
step:1423/2330 train_time:83551ms step_avg:58.71ms
step:1424/2330 train_time:83611ms step_avg:58.72ms
step:1425/2330 train_time:83669ms step_avg:58.71ms
step:1426/2330 train_time:83728ms step_avg:58.72ms
step:1427/2330 train_time:83785ms step_avg:58.71ms
step:1428/2330 train_time:83845ms step_avg:58.71ms
step:1429/2330 train_time:83902ms step_avg:58.71ms
step:1430/2330 train_time:83961ms step_avg:58.71ms
step:1431/2330 train_time:84019ms step_avg:58.71ms
step:1432/2330 train_time:84078ms step_avg:58.71ms
step:1433/2330 train_time:84134ms step_avg:58.71ms
step:1434/2330 train_time:84195ms step_avg:58.71ms
step:1435/2330 train_time:84252ms step_avg:58.71ms
step:1436/2330 train_time:84313ms step_avg:58.71ms
step:1437/2330 train_time:84370ms step_avg:58.71ms
step:1438/2330 train_time:84429ms step_avg:58.71ms
step:1439/2330 train_time:84487ms step_avg:58.71ms
step:1440/2330 train_time:84546ms step_avg:58.71ms
step:1441/2330 train_time:84603ms step_avg:58.71ms
step:1442/2330 train_time:84663ms step_avg:58.71ms
step:1443/2330 train_time:84721ms step_avg:58.71ms
step:1444/2330 train_time:84779ms step_avg:58.71ms
step:1445/2330 train_time:84836ms step_avg:58.71ms
step:1446/2330 train_time:84896ms step_avg:58.71ms
step:1447/2330 train_time:84952ms step_avg:58.71ms
step:1448/2330 train_time:85013ms step_avg:58.71ms
step:1449/2330 train_time:85069ms step_avg:58.71ms
step:1450/2330 train_time:85130ms step_avg:58.71ms
step:1451/2330 train_time:85187ms step_avg:58.71ms
step:1452/2330 train_time:85247ms step_avg:58.71ms
step:1453/2330 train_time:85305ms step_avg:58.71ms
step:1454/2330 train_time:85365ms step_avg:58.71ms
step:1455/2330 train_time:85422ms step_avg:58.71ms
step:1456/2330 train_time:85482ms step_avg:58.71ms
step:1457/2330 train_time:85538ms step_avg:58.71ms
step:1458/2330 train_time:85598ms step_avg:58.71ms
step:1459/2330 train_time:85655ms step_avg:58.71ms
step:1460/2330 train_time:85715ms step_avg:58.71ms
step:1461/2330 train_time:85772ms step_avg:58.71ms
step:1462/2330 train_time:85832ms step_avg:58.71ms
step:1463/2330 train_time:85889ms step_avg:58.71ms
step:1464/2330 train_time:85949ms step_avg:58.71ms
step:1465/2330 train_time:86007ms step_avg:58.71ms
step:1466/2330 train_time:86067ms step_avg:58.71ms
step:1467/2330 train_time:86125ms step_avg:58.71ms
step:1468/2330 train_time:86184ms step_avg:58.71ms
step:1469/2330 train_time:86241ms step_avg:58.71ms
step:1470/2330 train_time:86301ms step_avg:58.71ms
step:1471/2330 train_time:86357ms step_avg:58.71ms
step:1472/2330 train_time:86417ms step_avg:58.71ms
step:1473/2330 train_time:86474ms step_avg:58.71ms
step:1474/2330 train_time:86534ms step_avg:58.71ms
step:1475/2330 train_time:86591ms step_avg:58.71ms
step:1476/2330 train_time:86651ms step_avg:58.71ms
step:1477/2330 train_time:86709ms step_avg:58.71ms
step:1478/2330 train_time:86769ms step_avg:58.71ms
step:1479/2330 train_time:86826ms step_avg:58.71ms
step:1480/2330 train_time:86885ms step_avg:58.71ms
step:1481/2330 train_time:86943ms step_avg:58.71ms
step:1482/2330 train_time:87002ms step_avg:58.71ms
step:1483/2330 train_time:87059ms step_avg:58.70ms
step:1484/2330 train_time:87118ms step_avg:58.70ms
step:1485/2330 train_time:87174ms step_avg:58.70ms
step:1486/2330 train_time:87235ms step_avg:58.70ms
step:1487/2330 train_time:87292ms step_avg:58.70ms
step:1488/2330 train_time:87352ms step_avg:58.70ms
step:1489/2330 train_time:87410ms step_avg:58.70ms
step:1490/2330 train_time:87470ms step_avg:58.70ms
step:1491/2330 train_time:87528ms step_avg:58.70ms
step:1492/2330 train_time:87587ms step_avg:58.70ms
step:1493/2330 train_time:87644ms step_avg:58.70ms
step:1494/2330 train_time:87704ms step_avg:58.70ms
step:1495/2330 train_time:87761ms step_avg:58.70ms
step:1496/2330 train_time:87819ms step_avg:58.70ms
step:1497/2330 train_time:87876ms step_avg:58.70ms
step:1498/2330 train_time:87936ms step_avg:58.70ms
step:1499/2330 train_time:87993ms step_avg:58.70ms
step:1500/2330 train_time:88053ms step_avg:58.70ms
step:1500/2330 val_loss:3.9407 train_time:88133ms step_avg:58.76ms
step:1501/2330 train_time:88154ms step_avg:58.73ms
step:1502/2330 train_time:88176ms step_avg:58.71ms
step:1503/2330 train_time:88234ms step_avg:58.71ms
step:1504/2330 train_time:88295ms step_avg:58.71ms
step:1505/2330 train_time:88352ms step_avg:58.71ms
step:1506/2330 train_time:88413ms step_avg:58.71ms
step:1507/2330 train_time:88470ms step_avg:58.71ms
step:1508/2330 train_time:88529ms step_avg:58.71ms
step:1509/2330 train_time:88585ms step_avg:58.70ms
step:1510/2330 train_time:88645ms step_avg:58.71ms
step:1511/2330 train_time:88701ms step_avg:58.70ms
step:1512/2330 train_time:88761ms step_avg:58.70ms
step:1513/2330 train_time:88817ms step_avg:58.70ms
step:1514/2330 train_time:88876ms step_avg:58.70ms
step:1515/2330 train_time:88932ms step_avg:58.70ms
step:1516/2330 train_time:88991ms step_avg:58.70ms
step:1517/2330 train_time:89048ms step_avg:58.70ms
step:1518/2330 train_time:89109ms step_avg:58.70ms
step:1519/2330 train_time:89168ms step_avg:58.70ms
step:1520/2330 train_time:89229ms step_avg:58.70ms
step:1521/2330 train_time:89288ms step_avg:58.70ms
step:1522/2330 train_time:89349ms step_avg:58.71ms
step:1523/2330 train_time:89406ms step_avg:58.70ms
step:1524/2330 train_time:89466ms step_avg:58.70ms
step:1525/2330 train_time:89523ms step_avg:58.70ms
step:1526/2330 train_time:89582ms step_avg:58.70ms
step:1527/2330 train_time:89639ms step_avg:58.70ms
step:1528/2330 train_time:89698ms step_avg:58.70ms
step:1529/2330 train_time:89756ms step_avg:58.70ms
step:1530/2330 train_time:89814ms step_avg:58.70ms
step:1531/2330 train_time:89871ms step_avg:58.70ms
step:1532/2330 train_time:89930ms step_avg:58.70ms
step:1533/2330 train_time:89987ms step_avg:58.70ms
step:1534/2330 train_time:90048ms step_avg:58.70ms
step:1535/2330 train_time:90105ms step_avg:58.70ms
step:1536/2330 train_time:90166ms step_avg:58.70ms
step:1537/2330 train_time:90225ms step_avg:58.70ms
step:1538/2330 train_time:90287ms step_avg:58.70ms
step:1539/2330 train_time:90345ms step_avg:58.70ms
step:1540/2330 train_time:90406ms step_avg:58.70ms
step:1541/2330 train_time:90463ms step_avg:58.70ms
step:1542/2330 train_time:90524ms step_avg:58.71ms
step:1543/2330 train_time:90581ms step_avg:58.70ms
step:1544/2330 train_time:90642ms step_avg:58.71ms
step:1545/2330 train_time:90699ms step_avg:58.71ms
step:1546/2330 train_time:90761ms step_avg:58.71ms
step:1547/2330 train_time:90818ms step_avg:58.71ms
step:1548/2330 train_time:90877ms step_avg:58.71ms
step:1549/2330 train_time:90934ms step_avg:58.71ms
step:1550/2330 train_time:90994ms step_avg:58.71ms
step:1551/2330 train_time:91053ms step_avg:58.71ms
step:1552/2330 train_time:91112ms step_avg:58.71ms
step:1553/2330 train_time:91170ms step_avg:58.71ms
step:1554/2330 train_time:91232ms step_avg:58.71ms
step:1555/2330 train_time:91290ms step_avg:58.71ms
step:1556/2330 train_time:91351ms step_avg:58.71ms
step:1557/2330 train_time:91408ms step_avg:58.71ms
step:1558/2330 train_time:91469ms step_avg:58.71ms
step:1559/2330 train_time:91526ms step_avg:58.71ms
step:1560/2330 train_time:91588ms step_avg:58.71ms
step:1561/2330 train_time:91645ms step_avg:58.71ms
step:1562/2330 train_time:91706ms step_avg:58.71ms
step:1563/2330 train_time:91763ms step_avg:58.71ms
step:1564/2330 train_time:91824ms step_avg:58.71ms
step:1565/2330 train_time:91881ms step_avg:58.71ms
step:1566/2330 train_time:91942ms step_avg:58.71ms
step:1567/2330 train_time:92000ms step_avg:58.71ms
step:1568/2330 train_time:92061ms step_avg:58.71ms
step:1569/2330 train_time:92119ms step_avg:58.71ms
step:1570/2330 train_time:92179ms step_avg:58.71ms
step:1571/2330 train_time:92238ms step_avg:58.71ms
step:1572/2330 train_time:92299ms step_avg:58.71ms
step:1573/2330 train_time:92356ms step_avg:58.71ms
step:1574/2330 train_time:92416ms step_avg:58.71ms
step:1575/2330 train_time:92473ms step_avg:58.71ms
step:1576/2330 train_time:92534ms step_avg:58.71ms
step:1577/2330 train_time:92591ms step_avg:58.71ms
step:1578/2330 train_time:92652ms step_avg:58.71ms
step:1579/2330 train_time:92709ms step_avg:58.71ms
step:1580/2330 train_time:92769ms step_avg:58.71ms
step:1581/2330 train_time:92826ms step_avg:58.71ms
step:1582/2330 train_time:92888ms step_avg:58.72ms
step:1583/2330 train_time:92946ms step_avg:58.71ms
step:1584/2330 train_time:93007ms step_avg:58.72ms
step:1585/2330 train_time:93064ms step_avg:58.72ms
step:1586/2330 train_time:93125ms step_avg:58.72ms
step:1587/2330 train_time:93181ms step_avg:58.72ms
step:1588/2330 train_time:93243ms step_avg:58.72ms
step:1589/2330 train_time:93301ms step_avg:58.72ms
step:1590/2330 train_time:93363ms step_avg:58.72ms
step:1591/2330 train_time:93421ms step_avg:58.72ms
step:1592/2330 train_time:93482ms step_avg:58.72ms
step:1593/2330 train_time:93540ms step_avg:58.72ms
step:1594/2330 train_time:93601ms step_avg:58.72ms
step:1595/2330 train_time:93660ms step_avg:58.72ms
step:1596/2330 train_time:93720ms step_avg:58.72ms
step:1597/2330 train_time:93778ms step_avg:58.72ms
step:1598/2330 train_time:93837ms step_avg:58.72ms
step:1599/2330 train_time:93894ms step_avg:58.72ms
step:1600/2330 train_time:93955ms step_avg:58.72ms
step:1601/2330 train_time:94012ms step_avg:58.72ms
step:1602/2330 train_time:94071ms step_avg:58.72ms
step:1603/2330 train_time:94128ms step_avg:58.72ms
step:1604/2330 train_time:94189ms step_avg:58.72ms
step:1605/2330 train_time:94247ms step_avg:58.72ms
step:1606/2330 train_time:94307ms step_avg:58.72ms
step:1607/2330 train_time:94365ms step_avg:58.72ms
step:1608/2330 train_time:94426ms step_avg:58.72ms
step:1609/2330 train_time:94484ms step_avg:58.72ms
step:1610/2330 train_time:94545ms step_avg:58.72ms
step:1611/2330 train_time:94603ms step_avg:58.72ms
step:1612/2330 train_time:94662ms step_avg:58.72ms
step:1613/2330 train_time:94720ms step_avg:58.72ms
step:1614/2330 train_time:94782ms step_avg:58.72ms
step:1615/2330 train_time:94840ms step_avg:58.72ms
step:1616/2330 train_time:94900ms step_avg:58.73ms
step:1617/2330 train_time:94959ms step_avg:58.73ms
step:1618/2330 train_time:95019ms step_avg:58.73ms
step:1619/2330 train_time:95076ms step_avg:58.73ms
step:1620/2330 train_time:95135ms step_avg:58.73ms
step:1621/2330 train_time:95193ms step_avg:58.72ms
step:1622/2330 train_time:95254ms step_avg:58.73ms
step:1623/2330 train_time:95311ms step_avg:58.73ms
step:1624/2330 train_time:95371ms step_avg:58.73ms
step:1625/2330 train_time:95428ms step_avg:58.72ms
step:1626/2330 train_time:95488ms step_avg:58.73ms
step:1627/2330 train_time:95546ms step_avg:58.73ms
step:1628/2330 train_time:95608ms step_avg:58.73ms
step:1629/2330 train_time:95665ms step_avg:58.73ms
step:1630/2330 train_time:95726ms step_avg:58.73ms
step:1631/2330 train_time:95784ms step_avg:58.73ms
step:1632/2330 train_time:95846ms step_avg:58.73ms
step:1633/2330 train_time:95904ms step_avg:58.73ms
step:1634/2330 train_time:95965ms step_avg:58.73ms
step:1635/2330 train_time:96022ms step_avg:58.73ms
step:1636/2330 train_time:96082ms step_avg:58.73ms
step:1637/2330 train_time:96141ms step_avg:58.73ms
step:1638/2330 train_time:96202ms step_avg:58.73ms
step:1639/2330 train_time:96261ms step_avg:58.73ms
step:1640/2330 train_time:96321ms step_avg:58.73ms
step:1641/2330 train_time:96378ms step_avg:58.73ms
step:1642/2330 train_time:96438ms step_avg:58.73ms
step:1643/2330 train_time:96495ms step_avg:58.73ms
step:1644/2330 train_time:96556ms step_avg:58.73ms
step:1645/2330 train_time:96614ms step_avg:58.73ms
step:1646/2330 train_time:96673ms step_avg:58.73ms
step:1647/2330 train_time:96731ms step_avg:58.73ms
step:1648/2330 train_time:96791ms step_avg:58.73ms
step:1649/2330 train_time:96849ms step_avg:58.73ms
step:1650/2330 train_time:96910ms step_avg:58.73ms
step:1651/2330 train_time:96967ms step_avg:58.73ms
step:1652/2330 train_time:97028ms step_avg:58.73ms
step:1653/2330 train_time:97085ms step_avg:58.73ms
step:1654/2330 train_time:97146ms step_avg:58.73ms
step:1655/2330 train_time:97204ms step_avg:58.73ms
step:1656/2330 train_time:97264ms step_avg:58.73ms
step:1657/2330 train_time:97322ms step_avg:58.73ms
step:1658/2330 train_time:97383ms step_avg:58.74ms
step:1659/2330 train_time:97440ms step_avg:58.73ms
step:1660/2330 train_time:97503ms step_avg:58.74ms
step:1661/2330 train_time:97561ms step_avg:58.74ms
step:1662/2330 train_time:97622ms step_avg:58.74ms
step:1663/2330 train_time:97680ms step_avg:58.74ms
step:1664/2330 train_time:97740ms step_avg:58.74ms
step:1665/2330 train_time:97798ms step_avg:58.74ms
step:1666/2330 train_time:97858ms step_avg:58.74ms
step:1667/2330 train_time:97915ms step_avg:58.74ms
step:1668/2330 train_time:97975ms step_avg:58.74ms
step:1669/2330 train_time:98033ms step_avg:58.74ms
step:1670/2330 train_time:98093ms step_avg:58.74ms
step:1671/2330 train_time:98150ms step_avg:58.74ms
step:1672/2330 train_time:98211ms step_avg:58.74ms
step:1673/2330 train_time:98268ms step_avg:58.74ms
step:1674/2330 train_time:98329ms step_avg:58.74ms
step:1675/2330 train_time:98386ms step_avg:58.74ms
step:1676/2330 train_time:98448ms step_avg:58.74ms
step:1677/2330 train_time:98504ms step_avg:58.74ms
step:1678/2330 train_time:98566ms step_avg:58.74ms
step:1679/2330 train_time:98623ms step_avg:58.74ms
step:1680/2330 train_time:98685ms step_avg:58.74ms
step:1681/2330 train_time:98743ms step_avg:58.74ms
step:1682/2330 train_time:98804ms step_avg:58.74ms
step:1683/2330 train_time:98861ms step_avg:58.74ms
step:1684/2330 train_time:98922ms step_avg:58.74ms
step:1685/2330 train_time:98981ms step_avg:58.74ms
step:1686/2330 train_time:99041ms step_avg:58.74ms
step:1687/2330 train_time:99100ms step_avg:58.74ms
step:1688/2330 train_time:99160ms step_avg:58.74ms
step:1689/2330 train_time:99218ms step_avg:58.74ms
step:1690/2330 train_time:99278ms step_avg:58.74ms
step:1691/2330 train_time:99335ms step_avg:58.74ms
step:1692/2330 train_time:99396ms step_avg:58.74ms
step:1693/2330 train_time:99452ms step_avg:58.74ms
step:1694/2330 train_time:99513ms step_avg:58.74ms
step:1695/2330 train_time:99570ms step_avg:58.74ms
step:1696/2330 train_time:99632ms step_avg:58.75ms
step:1697/2330 train_time:99689ms step_avg:58.74ms
step:1698/2330 train_time:99750ms step_avg:58.75ms
step:1699/2330 train_time:99807ms step_avg:58.74ms
step:1700/2330 train_time:99869ms step_avg:58.75ms
step:1701/2330 train_time:99926ms step_avg:58.75ms
step:1702/2330 train_time:99986ms step_avg:58.75ms
step:1703/2330 train_time:100044ms step_avg:58.75ms
step:1704/2330 train_time:100104ms step_avg:58.75ms
step:1705/2330 train_time:100162ms step_avg:58.75ms
step:1706/2330 train_time:100223ms step_avg:58.75ms
step:1707/2330 train_time:100281ms step_avg:58.75ms
step:1708/2330 train_time:100343ms step_avg:58.75ms
step:1709/2330 train_time:100401ms step_avg:58.75ms
step:1710/2330 train_time:100461ms step_avg:58.75ms
step:1711/2330 train_time:100518ms step_avg:58.75ms
step:1712/2330 train_time:100579ms step_avg:58.75ms
step:1713/2330 train_time:100635ms step_avg:58.75ms
step:1714/2330 train_time:100696ms step_avg:58.75ms
step:1715/2330 train_time:100753ms step_avg:58.75ms
step:1716/2330 train_time:100814ms step_avg:58.75ms
step:1717/2330 train_time:100871ms step_avg:58.75ms
step:1718/2330 train_time:100932ms step_avg:58.75ms
step:1719/2330 train_time:100988ms step_avg:58.75ms
step:1720/2330 train_time:101050ms step_avg:58.75ms
step:1721/2330 train_time:101107ms step_avg:58.75ms
step:1722/2330 train_time:101168ms step_avg:58.75ms
step:1723/2330 train_time:101225ms step_avg:58.75ms
step:1724/2330 train_time:101287ms step_avg:58.75ms
step:1725/2330 train_time:101345ms step_avg:58.75ms
step:1726/2330 train_time:101406ms step_avg:58.75ms
step:1727/2330 train_time:101465ms step_avg:58.75ms
step:1728/2330 train_time:101525ms step_avg:58.75ms
step:1729/2330 train_time:101583ms step_avg:58.75ms
step:1730/2330 train_time:101644ms step_avg:58.75ms
step:1731/2330 train_time:101702ms step_avg:58.75ms
step:1732/2330 train_time:101763ms step_avg:58.75ms
step:1733/2330 train_time:101820ms step_avg:58.75ms
step:1734/2330 train_time:101881ms step_avg:58.75ms
step:1735/2330 train_time:101938ms step_avg:58.75ms
step:1736/2330 train_time:101999ms step_avg:58.76ms
step:1737/2330 train_time:102056ms step_avg:58.75ms
step:1738/2330 train_time:102116ms step_avg:58.75ms
step:1739/2330 train_time:102173ms step_avg:58.75ms
step:1740/2330 train_time:102235ms step_avg:58.76ms
step:1741/2330 train_time:102292ms step_avg:58.75ms
step:1742/2330 train_time:102353ms step_avg:58.76ms
step:1743/2330 train_time:102410ms step_avg:58.75ms
step:1744/2330 train_time:102470ms step_avg:58.76ms
step:1745/2330 train_time:102527ms step_avg:58.75ms
step:1746/2330 train_time:102588ms step_avg:58.76ms
step:1747/2330 train_time:102646ms step_avg:58.76ms
step:1748/2330 train_time:102707ms step_avg:58.76ms
step:1749/2330 train_time:102765ms step_avg:58.76ms
step:1750/2330 train_time:102826ms step_avg:58.76ms
step:1750/2330 val_loss:3.8529 train_time:102907ms step_avg:58.80ms
step:1751/2330 train_time:102927ms step_avg:58.78ms
step:1752/2330 train_time:102948ms step_avg:58.76ms
step:1753/2330 train_time:103006ms step_avg:58.76ms
step:1754/2330 train_time:103069ms step_avg:58.76ms
step:1755/2330 train_time:103127ms step_avg:58.76ms
step:1756/2330 train_time:103187ms step_avg:58.76ms
step:1757/2330 train_time:103245ms step_avg:58.76ms
step:1758/2330 train_time:103304ms step_avg:58.76ms
step:1759/2330 train_time:103361ms step_avg:58.76ms
step:1760/2330 train_time:103421ms step_avg:58.76ms
step:1761/2330 train_time:103478ms step_avg:58.76ms
step:1762/2330 train_time:103536ms step_avg:58.76ms
step:1763/2330 train_time:103593ms step_avg:58.76ms
step:1764/2330 train_time:103653ms step_avg:58.76ms
step:1765/2330 train_time:103709ms step_avg:58.76ms
step:1766/2330 train_time:103769ms step_avg:58.76ms
step:1767/2330 train_time:103826ms step_avg:58.76ms
step:1768/2330 train_time:103894ms step_avg:58.76ms
step:1769/2330 train_time:103952ms step_avg:58.76ms
step:1770/2330 train_time:104014ms step_avg:58.77ms
step:1771/2330 train_time:104072ms step_avg:58.76ms
step:1772/2330 train_time:104132ms step_avg:58.77ms
step:1773/2330 train_time:104190ms step_avg:58.76ms
step:1774/2330 train_time:104250ms step_avg:58.77ms
step:1775/2330 train_time:104307ms step_avg:58.76ms
step:1776/2330 train_time:104368ms step_avg:58.77ms
step:1777/2330 train_time:104425ms step_avg:58.76ms
step:1778/2330 train_time:104486ms step_avg:58.77ms
step:1779/2330 train_time:104544ms step_avg:58.77ms
step:1780/2330 train_time:104604ms step_avg:58.77ms
step:1781/2330 train_time:104661ms step_avg:58.77ms
step:1782/2330 train_time:104721ms step_avg:58.77ms
step:1783/2330 train_time:104778ms step_avg:58.76ms
step:1784/2330 train_time:104841ms step_avg:58.77ms
step:1785/2330 train_time:104900ms step_avg:58.77ms
step:1786/2330 train_time:104961ms step_avg:58.77ms
step:1787/2330 train_time:105018ms step_avg:58.77ms
step:1788/2330 train_time:105079ms step_avg:58.77ms
step:1789/2330 train_time:105137ms step_avg:58.77ms
step:1790/2330 train_time:105197ms step_avg:58.77ms
step:1791/2330 train_time:105254ms step_avg:58.77ms
step:1792/2330 train_time:105314ms step_avg:58.77ms
step:1793/2330 train_time:105371ms step_avg:58.77ms
step:1794/2330 train_time:105432ms step_avg:58.77ms
step:1795/2330 train_time:105489ms step_avg:58.77ms
step:1796/2330 train_time:105549ms step_avg:58.77ms
step:1797/2330 train_time:105606ms step_avg:58.77ms
step:1798/2330 train_time:105667ms step_avg:58.77ms
step:1799/2330 train_time:105723ms step_avg:58.77ms
step:1800/2330 train_time:105786ms step_avg:58.77ms
step:1801/2330 train_time:105844ms step_avg:58.77ms
step:1802/2330 train_time:105906ms step_avg:58.77ms
step:1803/2330 train_time:105965ms step_avg:58.77ms
step:1804/2330 train_time:106025ms step_avg:58.77ms
step:1805/2330 train_time:106083ms step_avg:58.77ms
step:1806/2330 train_time:106144ms step_avg:58.77ms
step:1807/2330 train_time:106202ms step_avg:58.77ms
step:1808/2330 train_time:106263ms step_avg:58.77ms
step:1809/2330 train_time:106321ms step_avg:58.77ms
step:1810/2330 train_time:106382ms step_avg:58.77ms
step:1811/2330 train_time:106439ms step_avg:58.77ms
step:1812/2330 train_time:106498ms step_avg:58.77ms
step:1813/2330 train_time:106555ms step_avg:58.77ms
step:1814/2330 train_time:106616ms step_avg:58.77ms
step:1815/2330 train_time:106672ms step_avg:58.77ms
step:1816/2330 train_time:106733ms step_avg:58.77ms
step:1817/2330 train_time:106790ms step_avg:58.77ms
step:1818/2330 train_time:106853ms step_avg:58.77ms
step:1819/2330 train_time:106910ms step_avg:58.77ms
step:1820/2330 train_time:106972ms step_avg:58.78ms
step:1821/2330 train_time:107029ms step_avg:58.77ms
step:1822/2330 train_time:107093ms step_avg:58.78ms
step:1823/2330 train_time:107150ms step_avg:58.78ms
step:1824/2330 train_time:107211ms step_avg:58.78ms
step:1825/2330 train_time:107267ms step_avg:58.78ms
step:1826/2330 train_time:107330ms step_avg:58.78ms
step:1827/2330 train_time:107386ms step_avg:58.78ms
step:1828/2330 train_time:107448ms step_avg:58.78ms
step:1829/2330 train_time:107505ms step_avg:58.78ms
step:1830/2330 train_time:107565ms step_avg:58.78ms
step:1831/2330 train_time:107622ms step_avg:58.78ms
step:1832/2330 train_time:107683ms step_avg:58.78ms
step:1833/2330 train_time:107741ms step_avg:58.78ms
step:1834/2330 train_time:107802ms step_avg:58.78ms
step:1835/2330 train_time:107860ms step_avg:58.78ms
step:1836/2330 train_time:107922ms step_avg:58.78ms
step:1837/2330 train_time:107979ms step_avg:58.78ms
step:1838/2330 train_time:108040ms step_avg:58.78ms
step:1839/2330 train_time:108097ms step_avg:58.78ms
step:1840/2330 train_time:108159ms step_avg:58.78ms
step:1841/2330 train_time:108216ms step_avg:58.78ms
step:1842/2330 train_time:108276ms step_avg:58.78ms
step:1843/2330 train_time:108333ms step_avg:58.78ms
step:1844/2330 train_time:108394ms step_avg:58.78ms
step:1845/2330 train_time:108451ms step_avg:58.78ms
step:1846/2330 train_time:108511ms step_avg:58.78ms
step:1847/2330 train_time:108567ms step_avg:58.78ms
step:1848/2330 train_time:108630ms step_avg:58.78ms
step:1849/2330 train_time:108686ms step_avg:58.78ms
step:1850/2330 train_time:108749ms step_avg:58.78ms
step:1851/2330 train_time:108806ms step_avg:58.78ms
step:1852/2330 train_time:108868ms step_avg:58.78ms
step:1853/2330 train_time:108926ms step_avg:58.78ms
step:1854/2330 train_time:108988ms step_avg:58.79ms
step:1855/2330 train_time:109045ms step_avg:58.78ms
step:1856/2330 train_time:109108ms step_avg:58.79ms
step:1857/2330 train_time:109167ms step_avg:58.79ms
step:1858/2330 train_time:109227ms step_avg:58.79ms
step:1859/2330 train_time:109285ms step_avg:58.79ms
step:1860/2330 train_time:109346ms step_avg:58.79ms
step:1861/2330 train_time:109404ms step_avg:58.79ms
step:1862/2330 train_time:109464ms step_avg:58.79ms
step:1863/2330 train_time:109522ms step_avg:58.79ms
step:1864/2330 train_time:109582ms step_avg:58.79ms
step:1865/2330 train_time:109639ms step_avg:58.79ms
step:1866/2330 train_time:109699ms step_avg:58.79ms
step:1867/2330 train_time:109756ms step_avg:58.79ms
step:1868/2330 train_time:109816ms step_avg:58.79ms
step:1869/2330 train_time:109873ms step_avg:58.79ms
step:1870/2330 train_time:109934ms step_avg:58.79ms
step:1871/2330 train_time:109991ms step_avg:58.79ms
step:1872/2330 train_time:110052ms step_avg:58.79ms
step:1873/2330 train_time:110109ms step_avg:58.79ms
step:1874/2330 train_time:110170ms step_avg:58.79ms
step:1875/2330 train_time:110227ms step_avg:58.79ms
step:1876/2330 train_time:110289ms step_avg:58.79ms
step:1877/2330 train_time:110346ms step_avg:58.79ms
step:1878/2330 train_time:110408ms step_avg:58.79ms
step:1879/2330 train_time:110465ms step_avg:58.79ms
step:1880/2330 train_time:110526ms step_avg:58.79ms
step:1881/2330 train_time:110583ms step_avg:58.79ms
step:1882/2330 train_time:110644ms step_avg:58.79ms
step:1883/2330 train_time:110702ms step_avg:58.79ms
step:1884/2330 train_time:110763ms step_avg:58.79ms
step:1885/2330 train_time:110821ms step_avg:58.79ms
step:1886/2330 train_time:110881ms step_avg:58.79ms
step:1887/2330 train_time:110939ms step_avg:58.79ms
step:1888/2330 train_time:110999ms step_avg:58.79ms
step:1889/2330 train_time:111056ms step_avg:58.79ms
step:1890/2330 train_time:111117ms step_avg:58.79ms
step:1891/2330 train_time:111174ms step_avg:58.79ms
step:1892/2330 train_time:111234ms step_avg:58.79ms
step:1893/2330 train_time:111292ms step_avg:58.79ms
step:1894/2330 train_time:111352ms step_avg:58.79ms
step:1895/2330 train_time:111409ms step_avg:58.79ms
step:1896/2330 train_time:111469ms step_avg:58.79ms
step:1897/2330 train_time:111526ms step_avg:58.79ms
step:1898/2330 train_time:111588ms step_avg:58.79ms
step:1899/2330 train_time:111645ms step_avg:58.79ms
step:1900/2330 train_time:111707ms step_avg:58.79ms
step:1901/2330 train_time:111765ms step_avg:58.79ms
step:1902/2330 train_time:111826ms step_avg:58.79ms
step:1903/2330 train_time:111883ms step_avg:58.79ms
step:1904/2330 train_time:111945ms step_avg:58.79ms
step:1905/2330 train_time:112003ms step_avg:58.79ms
step:1906/2330 train_time:112063ms step_avg:58.80ms
step:1907/2330 train_time:112121ms step_avg:58.79ms
step:1908/2330 train_time:112182ms step_avg:58.80ms
step:1909/2330 train_time:112240ms step_avg:58.80ms
step:1910/2330 train_time:112300ms step_avg:58.80ms
step:1911/2330 train_time:112357ms step_avg:58.79ms
step:1912/2330 train_time:112417ms step_avg:58.80ms
step:1913/2330 train_time:112473ms step_avg:58.79ms
step:1914/2330 train_time:112535ms step_avg:58.80ms
step:1915/2330 train_time:112592ms step_avg:58.79ms
step:1916/2330 train_time:112653ms step_avg:58.80ms
step:1917/2330 train_time:112710ms step_avg:58.79ms
step:1918/2330 train_time:112770ms step_avg:58.80ms
step:1919/2330 train_time:112828ms step_avg:58.80ms
step:1920/2330 train_time:112889ms step_avg:58.80ms
step:1921/2330 train_time:112946ms step_avg:58.80ms
step:1922/2330 train_time:113007ms step_avg:58.80ms
step:1923/2330 train_time:113064ms step_avg:58.80ms
step:1924/2330 train_time:113127ms step_avg:58.80ms
step:1925/2330 train_time:113184ms step_avg:58.80ms
step:1926/2330 train_time:113246ms step_avg:58.80ms
step:1927/2330 train_time:113304ms step_avg:58.80ms
step:1928/2330 train_time:113364ms step_avg:58.80ms
step:1929/2330 train_time:113423ms step_avg:58.80ms
step:1930/2330 train_time:113483ms step_avg:58.80ms
step:1931/2330 train_time:113542ms step_avg:58.80ms
step:1932/2330 train_time:113602ms step_avg:58.80ms
step:1933/2330 train_time:113660ms step_avg:58.80ms
step:1934/2330 train_time:113720ms step_avg:58.80ms
step:1935/2330 train_time:113776ms step_avg:58.80ms
step:1936/2330 train_time:113836ms step_avg:58.80ms
step:1937/2330 train_time:113894ms step_avg:58.80ms
step:1938/2330 train_time:113955ms step_avg:58.80ms
step:1939/2330 train_time:114012ms step_avg:58.80ms
step:1940/2330 train_time:114072ms step_avg:58.80ms
step:1941/2330 train_time:114128ms step_avg:58.80ms
step:1942/2330 train_time:114190ms step_avg:58.80ms
step:1943/2330 train_time:114248ms step_avg:58.80ms
step:1944/2330 train_time:114309ms step_avg:58.80ms
step:1945/2330 train_time:114366ms step_avg:58.80ms
step:1946/2330 train_time:114427ms step_avg:58.80ms
step:1947/2330 train_time:114484ms step_avg:58.80ms
step:1948/2330 train_time:114546ms step_avg:58.80ms
step:1949/2330 train_time:114604ms step_avg:58.80ms
step:1950/2330 train_time:114665ms step_avg:58.80ms
step:1951/2330 train_time:114723ms step_avg:58.80ms
step:1952/2330 train_time:114784ms step_avg:58.80ms
step:1953/2330 train_time:114843ms step_avg:58.80ms
step:1954/2330 train_time:114903ms step_avg:58.80ms
step:1955/2330 train_time:114960ms step_avg:58.80ms
step:1956/2330 train_time:115020ms step_avg:58.80ms
step:1957/2330 train_time:115077ms step_avg:58.80ms
step:1958/2330 train_time:115137ms step_avg:58.80ms
step:1959/2330 train_time:115195ms step_avg:58.80ms
step:1960/2330 train_time:115255ms step_avg:58.80ms
step:1961/2330 train_time:115311ms step_avg:58.80ms
step:1962/2330 train_time:115372ms step_avg:58.80ms
step:1963/2330 train_time:115429ms step_avg:58.80ms
step:1964/2330 train_time:115491ms step_avg:58.80ms
step:1965/2330 train_time:115548ms step_avg:58.80ms
step:1966/2330 train_time:115609ms step_avg:58.80ms
step:1967/2330 train_time:115667ms step_avg:58.80ms
step:1968/2330 train_time:115728ms step_avg:58.80ms
step:1969/2330 train_time:115785ms step_avg:58.80ms
step:1970/2330 train_time:115847ms step_avg:58.81ms
step:1971/2330 train_time:115905ms step_avg:58.80ms
step:1972/2330 train_time:115966ms step_avg:58.81ms
step:1973/2330 train_time:116023ms step_avg:58.81ms
step:1974/2330 train_time:116084ms step_avg:58.81ms
step:1975/2330 train_time:116142ms step_avg:58.81ms
step:1976/2330 train_time:116203ms step_avg:58.81ms
step:1977/2330 train_time:116261ms step_avg:58.81ms
step:1978/2330 train_time:116322ms step_avg:58.81ms
step:1979/2330 train_time:116379ms step_avg:58.81ms
step:1980/2330 train_time:116440ms step_avg:58.81ms
step:1981/2330 train_time:116498ms step_avg:58.81ms
step:1982/2330 train_time:116557ms step_avg:58.81ms
step:1983/2330 train_time:116615ms step_avg:58.81ms
step:1984/2330 train_time:116674ms step_avg:58.81ms
step:1985/2330 train_time:116732ms step_avg:58.81ms
step:1986/2330 train_time:116792ms step_avg:58.81ms
step:1987/2330 train_time:116849ms step_avg:58.81ms
step:1988/2330 train_time:116911ms step_avg:58.81ms
step:1989/2330 train_time:116966ms step_avg:58.81ms
step:1990/2330 train_time:117029ms step_avg:58.81ms
step:1991/2330 train_time:117086ms step_avg:58.81ms
step:1992/2330 train_time:117148ms step_avg:58.81ms
step:1993/2330 train_time:117206ms step_avg:58.81ms
step:1994/2330 train_time:117267ms step_avg:58.81ms
step:1995/2330 train_time:117324ms step_avg:58.81ms
step:1996/2330 train_time:117386ms step_avg:58.81ms
step:1997/2330 train_time:117445ms step_avg:58.81ms
step:1998/2330 train_time:117506ms step_avg:58.81ms
step:1999/2330 train_time:117564ms step_avg:58.81ms
step:2000/2330 train_time:117624ms step_avg:58.81ms
step:2000/2330 val_loss:3.7884 train_time:117705ms step_avg:58.85ms
step:2001/2330 train_time:117725ms step_avg:58.83ms
step:2002/2330 train_time:117744ms step_avg:58.81ms
step:2003/2330 train_time:117803ms step_avg:58.81ms
step:2004/2330 train_time:117871ms step_avg:58.82ms
step:2005/2330 train_time:117929ms step_avg:58.82ms
step:2006/2330 train_time:117991ms step_avg:58.82ms
step:2007/2330 train_time:118049ms step_avg:58.82ms
step:2008/2330 train_time:118109ms step_avg:58.82ms
step:2009/2330 train_time:118165ms step_avg:58.82ms
step:2010/2330 train_time:118227ms step_avg:58.82ms
step:2011/2330 train_time:118283ms step_avg:58.82ms
step:2012/2330 train_time:118343ms step_avg:58.82ms
step:2013/2330 train_time:118399ms step_avg:58.82ms
step:2014/2330 train_time:118460ms step_avg:58.82ms
step:2015/2330 train_time:118516ms step_avg:58.82ms
step:2016/2330 train_time:118577ms step_avg:58.82ms
step:2017/2330 train_time:118633ms step_avg:58.82ms
step:2018/2330 train_time:118694ms step_avg:58.82ms
step:2019/2330 train_time:118752ms step_avg:58.82ms
step:2020/2330 train_time:118815ms step_avg:58.82ms
step:2021/2330 train_time:118872ms step_avg:58.82ms
step:2022/2330 train_time:118936ms step_avg:58.82ms
step:2023/2330 train_time:118994ms step_avg:58.82ms
step:2024/2330 train_time:119054ms step_avg:58.82ms
step:2025/2330 train_time:119111ms step_avg:58.82ms
step:2026/2330 train_time:119171ms step_avg:58.82ms
step:2027/2330 train_time:119228ms step_avg:58.82ms
step:2028/2330 train_time:119287ms step_avg:58.82ms
step:2029/2330 train_time:119344ms step_avg:58.82ms
step:2030/2330 train_time:119404ms step_avg:58.82ms
step:2031/2330 train_time:119461ms step_avg:58.82ms
step:2032/2330 train_time:119522ms step_avg:58.82ms
step:2033/2330 train_time:119579ms step_avg:58.82ms
step:2034/2330 train_time:119640ms step_avg:58.82ms
step:2035/2330 train_time:119697ms step_avg:58.82ms
step:2036/2330 train_time:119758ms step_avg:58.82ms
step:2037/2330 train_time:119817ms step_avg:58.82ms
step:2038/2330 train_time:119879ms step_avg:58.82ms
step:2039/2330 train_time:119936ms step_avg:58.82ms
step:2040/2330 train_time:119999ms step_avg:58.82ms
step:2041/2330 train_time:120057ms step_avg:58.82ms
step:2042/2330 train_time:120118ms step_avg:58.82ms
step:2043/2330 train_time:120175ms step_avg:58.82ms
step:2044/2330 train_time:120235ms step_avg:58.82ms
step:2045/2330 train_time:120291ms step_avg:58.82ms
step:2046/2330 train_time:120352ms step_avg:58.82ms
step:2047/2330 train_time:120409ms step_avg:58.82ms
step:2048/2330 train_time:120469ms step_avg:58.82ms
step:2049/2330 train_time:120525ms step_avg:58.82ms
step:2050/2330 train_time:120585ms step_avg:58.82ms
step:2051/2330 train_time:120643ms step_avg:58.82ms
step:2052/2330 train_time:120705ms step_avg:58.82ms
step:2053/2330 train_time:120762ms step_avg:58.82ms
step:2054/2330 train_time:120826ms step_avg:58.82ms
step:2055/2330 train_time:120884ms step_avg:58.82ms
step:2056/2330 train_time:120946ms step_avg:58.83ms
step:2057/2330 train_time:121004ms step_avg:58.83ms
step:2058/2330 train_time:121066ms step_avg:58.83ms
step:2059/2330 train_time:121124ms step_avg:58.83ms
step:2060/2330 train_time:121185ms step_avg:58.83ms
step:2061/2330 train_time:121242ms step_avg:58.83ms
step:2062/2330 train_time:121302ms step_avg:58.83ms
step:2063/2330 train_time:121360ms step_avg:58.83ms
step:2064/2330 train_time:121421ms step_avg:58.83ms
step:2065/2330 train_time:121477ms step_avg:58.83ms
step:2066/2330 train_time:121536ms step_avg:58.83ms
step:2067/2330 train_time:121593ms step_avg:58.83ms
step:2068/2330 train_time:121654ms step_avg:58.83ms
step:2069/2330 train_time:121712ms step_avg:58.83ms
step:2070/2330 train_time:121772ms step_avg:58.83ms
step:2071/2330 train_time:121830ms step_avg:58.83ms
step:2072/2330 train_time:121891ms step_avg:58.83ms
step:2073/2330 train_time:121948ms step_avg:58.83ms
step:2074/2330 train_time:122012ms step_avg:58.83ms
step:2075/2330 train_time:122069ms step_avg:58.83ms
step:2076/2330 train_time:122130ms step_avg:58.83ms
step:2077/2330 train_time:122187ms step_avg:58.83ms
step:2078/2330 train_time:122248ms step_avg:58.83ms
step:2079/2330 train_time:122305ms step_avg:58.83ms
step:2080/2330 train_time:122366ms step_avg:58.83ms
step:2081/2330 train_time:122423ms step_avg:58.83ms
step:2082/2330 train_time:122485ms step_avg:58.83ms
step:2083/2330 train_time:122542ms step_avg:58.83ms
step:2084/2330 train_time:122603ms step_avg:58.83ms
step:2085/2330 train_time:122661ms step_avg:58.83ms
step:2086/2330 train_time:122721ms step_avg:58.83ms
step:2087/2330 train_time:122780ms step_avg:58.83ms
step:2088/2330 train_time:122840ms step_avg:58.83ms
step:2089/2330 train_time:122898ms step_avg:58.83ms
step:2090/2330 train_time:122959ms step_avg:58.83ms
step:2091/2330 train_time:123016ms step_avg:58.83ms
step:2092/2330 train_time:123077ms step_avg:58.83ms
step:2093/2330 train_time:123134ms step_avg:58.83ms
step:2094/2330 train_time:123194ms step_avg:58.83ms
step:2095/2330 train_time:123251ms step_avg:58.83ms
step:2096/2330 train_time:123312ms step_avg:58.83ms
step:2097/2330 train_time:123369ms step_avg:58.83ms
step:2098/2330 train_time:123430ms step_avg:58.83ms
step:2099/2330 train_time:123486ms step_avg:58.83ms
step:2100/2330 train_time:123549ms step_avg:58.83ms
step:2101/2330 train_time:123606ms step_avg:58.83ms
step:2102/2330 train_time:123668ms step_avg:58.83ms
step:2103/2330 train_time:123725ms step_avg:58.83ms
step:2104/2330 train_time:123787ms step_avg:58.83ms
step:2105/2330 train_time:123845ms step_avg:58.83ms
step:2106/2330 train_time:123906ms step_avg:58.83ms
step:2107/2330 train_time:123963ms step_avg:58.83ms
step:2108/2330 train_time:124024ms step_avg:58.83ms
step:2109/2330 train_time:124082ms step_avg:58.83ms
step:2110/2330 train_time:124142ms step_avg:58.84ms
step:2111/2330 train_time:124201ms step_avg:58.83ms
step:2112/2330 train_time:124261ms step_avg:58.84ms
step:2113/2330 train_time:124320ms step_avg:58.84ms
step:2114/2330 train_time:124380ms step_avg:58.84ms
step:2115/2330 train_time:124436ms step_avg:58.84ms
step:2116/2330 train_time:124497ms step_avg:58.84ms
step:2117/2330 train_time:124554ms step_avg:58.84ms
step:2118/2330 train_time:124614ms step_avg:58.84ms
step:2119/2330 train_time:124671ms step_avg:58.83ms
step:2120/2330 train_time:124732ms step_avg:58.84ms
step:2121/2330 train_time:124788ms step_avg:58.83ms
step:2122/2330 train_time:124851ms step_avg:58.84ms
step:2123/2330 train_time:124908ms step_avg:58.84ms
step:2124/2330 train_time:124970ms step_avg:58.84ms
step:2125/2330 train_time:125027ms step_avg:58.84ms
step:2126/2330 train_time:125088ms step_avg:58.84ms
step:2127/2330 train_time:125145ms step_avg:58.84ms
step:2128/2330 train_time:125208ms step_avg:58.84ms
step:2129/2330 train_time:125266ms step_avg:58.84ms
step:2130/2330 train_time:125328ms step_avg:58.84ms
step:2131/2330 train_time:125385ms step_avg:58.84ms
step:2132/2330 train_time:125447ms step_avg:58.84ms
step:2133/2330 train_time:125504ms step_avg:58.84ms
step:2134/2330 train_time:125566ms step_avg:58.84ms
step:2135/2330 train_time:125623ms step_avg:58.84ms
step:2136/2330 train_time:125684ms step_avg:58.84ms
step:2137/2330 train_time:125741ms step_avg:58.84ms
step:2138/2330 train_time:125802ms step_avg:58.84ms
step:2139/2330 train_time:125860ms step_avg:58.84ms
step:2140/2330 train_time:125920ms step_avg:58.84ms
step:2141/2330 train_time:125977ms step_avg:58.84ms
step:2142/2330 train_time:126038ms step_avg:58.84ms
step:2143/2330 train_time:126096ms step_avg:58.84ms
step:2144/2330 train_time:126156ms step_avg:58.84ms
step:2145/2330 train_time:126213ms step_avg:58.84ms
step:2146/2330 train_time:126275ms step_avg:58.84ms
step:2147/2330 train_time:126332ms step_avg:58.84ms
step:2148/2330 train_time:126392ms step_avg:58.84ms
step:2149/2330 train_time:126449ms step_avg:58.84ms
step:2150/2330 train_time:126510ms step_avg:58.84ms
step:2151/2330 train_time:126567ms step_avg:58.84ms
step:2152/2330 train_time:126628ms step_avg:58.84ms
step:2153/2330 train_time:126685ms step_avg:58.84ms
step:2154/2330 train_time:126746ms step_avg:58.84ms
step:2155/2330 train_time:126803ms step_avg:58.84ms
step:2156/2330 train_time:126865ms step_avg:58.84ms
step:2157/2330 train_time:126923ms step_avg:58.84ms
step:2158/2330 train_time:126985ms step_avg:58.84ms
step:2159/2330 train_time:127042ms step_avg:58.84ms
step:2160/2330 train_time:127105ms step_avg:58.84ms
step:2161/2330 train_time:127163ms step_avg:58.84ms
step:2162/2330 train_time:127224ms step_avg:58.85ms
step:2163/2330 train_time:127282ms step_avg:58.85ms
step:2164/2330 train_time:127342ms step_avg:58.85ms
step:2165/2330 train_time:127400ms step_avg:58.85ms
step:2166/2330 train_time:127461ms step_avg:58.85ms
step:2167/2330 train_time:127519ms step_avg:58.85ms
step:2168/2330 train_time:127579ms step_avg:58.85ms
step:2169/2330 train_time:127636ms step_avg:58.85ms
step:2170/2330 train_time:127696ms step_avg:58.85ms
step:2171/2330 train_time:127754ms step_avg:58.85ms
step:2172/2330 train_time:127814ms step_avg:58.85ms
step:2173/2330 train_time:127871ms step_avg:58.85ms
step:2174/2330 train_time:127931ms step_avg:58.85ms
step:2175/2330 train_time:127988ms step_avg:58.85ms
step:2176/2330 train_time:128050ms step_avg:58.85ms
step:2177/2330 train_time:128106ms step_avg:58.85ms
step:2178/2330 train_time:128168ms step_avg:58.85ms
step:2179/2330 train_time:128225ms step_avg:58.85ms
step:2180/2330 train_time:128287ms step_avg:58.85ms
step:2181/2330 train_time:128345ms step_avg:58.85ms
step:2182/2330 train_time:128407ms step_avg:58.85ms
step:2183/2330 train_time:128466ms step_avg:58.85ms
step:2184/2330 train_time:128526ms step_avg:58.85ms
step:2185/2330 train_time:128585ms step_avg:58.85ms
step:2186/2330 train_time:128645ms step_avg:58.85ms
step:2187/2330 train_time:128703ms step_avg:58.85ms
step:2188/2330 train_time:128763ms step_avg:58.85ms
step:2189/2330 train_time:128820ms step_avg:58.85ms
step:2190/2330 train_time:128881ms step_avg:58.85ms
step:2191/2330 train_time:128939ms step_avg:58.85ms
step:2192/2330 train_time:129000ms step_avg:58.85ms
step:2193/2330 train_time:129057ms step_avg:58.85ms
step:2194/2330 train_time:129119ms step_avg:58.85ms
step:2195/2330 train_time:129175ms step_avg:58.85ms
step:2196/2330 train_time:129236ms step_avg:58.85ms
step:2197/2330 train_time:129293ms step_avg:58.85ms
step:2198/2330 train_time:129355ms step_avg:58.85ms
step:2199/2330 train_time:129412ms step_avg:58.85ms
step:2200/2330 train_time:129473ms step_avg:58.85ms
step:2201/2330 train_time:129529ms step_avg:58.85ms
step:2202/2330 train_time:129590ms step_avg:58.85ms
step:2203/2330 train_time:129648ms step_avg:58.85ms
step:2204/2330 train_time:129709ms step_avg:58.85ms
step:2205/2330 train_time:129766ms step_avg:58.85ms
step:2206/2330 train_time:129826ms step_avg:58.85ms
step:2207/2330 train_time:129883ms step_avg:58.85ms
step:2208/2330 train_time:129945ms step_avg:58.85ms
step:2209/2330 train_time:130002ms step_avg:58.85ms
step:2210/2330 train_time:130065ms step_avg:58.85ms
step:2211/2330 train_time:130124ms step_avg:58.85ms
step:2212/2330 train_time:130184ms step_avg:58.85ms
step:2213/2330 train_time:130241ms step_avg:58.85ms
step:2214/2330 train_time:130302ms step_avg:58.85ms
step:2215/2330 train_time:130359ms step_avg:58.85ms
step:2216/2330 train_time:130421ms step_avg:58.85ms
step:2217/2330 train_time:130480ms step_avg:58.85ms
step:2218/2330 train_time:130540ms step_avg:58.85ms
step:2219/2330 train_time:130597ms step_avg:58.85ms
step:2220/2330 train_time:130657ms step_avg:58.85ms
step:2221/2330 train_time:130714ms step_avg:58.85ms
step:2222/2330 train_time:130774ms step_avg:58.85ms
step:2223/2330 train_time:130831ms step_avg:58.85ms
step:2224/2330 train_time:130891ms step_avg:58.85ms
step:2225/2330 train_time:130948ms step_avg:58.85ms
step:2226/2330 train_time:131009ms step_avg:58.85ms
step:2227/2330 train_time:131066ms step_avg:58.85ms
step:2228/2330 train_time:131128ms step_avg:58.85ms
step:2229/2330 train_time:131185ms step_avg:58.85ms
step:2230/2330 train_time:131247ms step_avg:58.86ms
step:2231/2330 train_time:131304ms step_avg:58.85ms
step:2232/2330 train_time:131366ms step_avg:58.86ms
step:2233/2330 train_time:131423ms step_avg:58.86ms
step:2234/2330 train_time:131484ms step_avg:58.86ms
step:2235/2330 train_time:131542ms step_avg:58.86ms
step:2236/2330 train_time:131603ms step_avg:58.86ms
step:2237/2330 train_time:131661ms step_avg:58.86ms
step:2238/2330 train_time:131721ms step_avg:58.86ms
step:2239/2330 train_time:131779ms step_avg:58.86ms
step:2240/2330 train_time:131840ms step_avg:58.86ms
step:2241/2330 train_time:131897ms step_avg:58.86ms
step:2242/2330 train_time:131957ms step_avg:58.86ms
step:2243/2330 train_time:132015ms step_avg:58.86ms
step:2244/2330 train_time:132075ms step_avg:58.86ms
step:2245/2330 train_time:132132ms step_avg:58.86ms
step:2246/2330 train_time:132193ms step_avg:58.86ms
step:2247/2330 train_time:132251ms step_avg:58.86ms
step:2248/2330 train_time:132312ms step_avg:58.86ms
step:2249/2330 train_time:132369ms step_avg:58.86ms
step:2250/2330 train_time:132429ms step_avg:58.86ms
step:2250/2330 val_loss:3.7378 train_time:132512ms step_avg:58.89ms
step:2251/2330 train_time:132532ms step_avg:58.88ms
step:2252/2330 train_time:132552ms step_avg:58.86ms
step:2253/2330 train_time:132611ms step_avg:58.86ms
step:2254/2330 train_time:132674ms step_avg:58.86ms
step:2255/2330 train_time:132733ms step_avg:58.86ms
step:2256/2330 train_time:132794ms step_avg:58.86ms
step:2257/2330 train_time:132851ms step_avg:58.86ms
step:2258/2330 train_time:132911ms step_avg:58.86ms
step:2259/2330 train_time:132968ms step_avg:58.86ms
step:2260/2330 train_time:133028ms step_avg:58.86ms
step:2261/2330 train_time:133085ms step_avg:58.86ms
step:2262/2330 train_time:133144ms step_avg:58.86ms
step:2263/2330 train_time:133201ms step_avg:58.86ms
step:2264/2330 train_time:133261ms step_avg:58.86ms
step:2265/2330 train_time:133317ms step_avg:58.86ms
step:2266/2330 train_time:133378ms step_avg:58.86ms
step:2267/2330 train_time:133434ms step_avg:58.86ms
step:2268/2330 train_time:133496ms step_avg:58.86ms
step:2269/2330 train_time:133555ms step_avg:58.86ms
step:2270/2330 train_time:133618ms step_avg:58.86ms
step:2271/2330 train_time:133677ms step_avg:58.86ms
step:2272/2330 train_time:133739ms step_avg:58.86ms
step:2273/2330 train_time:133796ms step_avg:58.86ms
step:2274/2330 train_time:133858ms step_avg:58.86ms
step:2275/2330 train_time:133914ms step_avg:58.86ms
step:2276/2330 train_time:133974ms step_avg:58.86ms
step:2277/2330 train_time:134032ms step_avg:58.86ms
step:2278/2330 train_time:134093ms step_avg:58.86ms
step:2279/2330 train_time:134150ms step_avg:58.86ms
step:2280/2330 train_time:134210ms step_avg:58.86ms
step:2281/2330 train_time:134268ms step_avg:58.86ms
step:2282/2330 train_time:134328ms step_avg:58.86ms
step:2283/2330 train_time:134386ms step_avg:58.86ms
step:2284/2330 train_time:134445ms step_avg:58.86ms
step:2285/2330 train_time:134503ms step_avg:58.86ms
step:2286/2330 train_time:134565ms step_avg:58.86ms
step:2287/2330 train_time:134623ms step_avg:58.86ms
step:2288/2330 train_time:134683ms step_avg:58.87ms
step:2289/2330 train_time:134741ms step_avg:58.86ms
step:2290/2330 train_time:134802ms step_avg:58.87ms
step:2291/2330 train_time:134860ms step_avg:58.86ms
step:2292/2330 train_time:134919ms step_avg:58.87ms
step:2293/2330 train_time:134976ms step_avg:58.86ms
step:2294/2330 train_time:135036ms step_avg:58.86ms
step:2295/2330 train_time:135094ms step_avg:58.86ms
step:2296/2330 train_time:135154ms step_avg:58.87ms
step:2297/2330 train_time:135211ms step_avg:58.86ms
step:2298/2330 train_time:135271ms step_avg:58.86ms
step:2299/2330 train_time:135329ms step_avg:58.86ms
step:2300/2330 train_time:135390ms step_avg:58.87ms
step:2301/2330 train_time:135448ms step_avg:58.86ms
step:2302/2330 train_time:135508ms step_avg:58.87ms
step:2303/2330 train_time:135567ms step_avg:58.87ms
step:2304/2330 train_time:135629ms step_avg:58.87ms
step:2305/2330 train_time:135686ms step_avg:58.87ms
step:2306/2330 train_time:135748ms step_avg:58.87ms
step:2307/2330 train_time:135806ms step_avg:58.87ms
step:2308/2330 train_time:135867ms step_avg:58.87ms
step:2309/2330 train_time:135924ms step_avg:58.87ms
step:2310/2330 train_time:135985ms step_avg:58.87ms
step:2311/2330 train_time:136042ms step_avg:58.87ms
step:2312/2330 train_time:136101ms step_avg:58.87ms
step:2313/2330 train_time:136158ms step_avg:58.87ms
step:2314/2330 train_time:136219ms step_avg:58.87ms
step:2315/2330 train_time:136276ms step_avg:58.87ms
step:2316/2330 train_time:136337ms step_avg:58.87ms
step:2317/2330 train_time:136394ms step_avg:58.87ms
step:2318/2330 train_time:136455ms step_avg:58.87ms
step:2319/2330 train_time:136512ms step_avg:58.87ms
step:2320/2330 train_time:136574ms step_avg:58.87ms
step:2321/2330 train_time:136633ms step_avg:58.87ms
step:2322/2330 train_time:136694ms step_avg:58.87ms
step:2323/2330 train_time:136752ms step_avg:58.87ms
step:2324/2330 train_time:136813ms step_avg:58.87ms
step:2325/2330 train_time:136871ms step_avg:58.87ms
step:2326/2330 train_time:136931ms step_avg:58.87ms
step:2327/2330 train_time:136989ms step_avg:58.87ms
step:2328/2330 train_time:137049ms step_avg:58.87ms
step:2329/2330 train_time:137107ms step_avg:58.87ms
step:2330/2330 train_time:137168ms step_avg:58.87ms
step:2330/2330 val_loss:3.7225 train_time:137249ms step_avg:58.91ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
