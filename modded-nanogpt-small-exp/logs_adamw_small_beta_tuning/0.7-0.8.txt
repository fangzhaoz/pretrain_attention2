import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--beta1', type=str, required=True)
parser.add_argument('--beta2', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_beta_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=6e-4,  betas=(float(args2.beta1), float(args2.beta2)), eps=1e-10, weight_decay=0.0, fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt", exist_ok=True)
            torch.save(log, f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 05:31:14 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   27C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   26C    P0             115W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   24C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   27C    P0             113W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   25C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   26C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:85ms step_avg:85.27ms
step:2/2330 train_time:176ms step_avg:88.14ms
step:3/2330 train_time:195ms step_avg:65.08ms
step:4/2330 train_time:214ms step_avg:53.50ms
step:5/2330 train_time:267ms step_avg:53.49ms
step:6/2330 train_time:325ms step_avg:54.20ms
step:7/2330 train_time:380ms step_avg:54.27ms
step:8/2330 train_time:438ms step_avg:54.80ms
step:9/2330 train_time:494ms step_avg:54.86ms
step:10/2330 train_time:552ms step_avg:55.18ms
step:11/2330 train_time:607ms step_avg:55.15ms
step:12/2330 train_time:665ms step_avg:55.40ms
step:13/2330 train_time:720ms step_avg:55.37ms
step:14/2330 train_time:778ms step_avg:55.57ms
step:15/2330 train_time:833ms step_avg:55.57ms
step:16/2330 train_time:892ms step_avg:55.74ms
step:17/2330 train_time:947ms step_avg:55.73ms
step:18/2330 train_time:1007ms step_avg:55.92ms
step:19/2330 train_time:1065ms step_avg:56.06ms
step:20/2330 train_time:1125ms step_avg:56.23ms
step:21/2330 train_time:1182ms step_avg:56.27ms
step:22/2330 train_time:1244ms step_avg:56.56ms
step:23/2330 train_time:1300ms step_avg:56.53ms
step:24/2330 train_time:1359ms step_avg:56.62ms
step:25/2330 train_time:1414ms step_avg:56.56ms
step:26/2330 train_time:1473ms step_avg:56.66ms
step:27/2330 train_time:1529ms step_avg:56.64ms
step:28/2330 train_time:1588ms step_avg:56.70ms
step:29/2330 train_time:1643ms step_avg:56.64ms
step:30/2330 train_time:1702ms step_avg:56.73ms
step:31/2330 train_time:1757ms step_avg:56.68ms
step:32/2330 train_time:1816ms step_avg:56.75ms
step:33/2330 train_time:1871ms step_avg:56.71ms
step:34/2330 train_time:1930ms step_avg:56.76ms
step:35/2330 train_time:1986ms step_avg:56.76ms
step:36/2330 train_time:2046ms step_avg:56.83ms
step:37/2330 train_time:2103ms step_avg:56.85ms
step:38/2330 train_time:2163ms step_avg:56.92ms
step:39/2330 train_time:2219ms step_avg:56.90ms
step:40/2330 train_time:2280ms step_avg:56.99ms
step:41/2330 train_time:2335ms step_avg:56.96ms
step:42/2330 train_time:2395ms step_avg:57.02ms
step:43/2330 train_time:2451ms step_avg:57.00ms
step:44/2330 train_time:2509ms step_avg:57.03ms
step:45/2330 train_time:2565ms step_avg:57.00ms
step:46/2330 train_time:2624ms step_avg:57.03ms
step:47/2330 train_time:2679ms step_avg:56.99ms
step:48/2330 train_time:2739ms step_avg:57.06ms
step:49/2330 train_time:2794ms step_avg:57.03ms
step:50/2330 train_time:2852ms step_avg:57.05ms
step:51/2330 train_time:2908ms step_avg:57.02ms
step:52/2330 train_time:2967ms step_avg:57.05ms
step:53/2330 train_time:3022ms step_avg:57.03ms
step:54/2330 train_time:3082ms step_avg:57.08ms
step:55/2330 train_time:3139ms step_avg:57.07ms
step:56/2330 train_time:3198ms step_avg:57.12ms
step:57/2330 train_time:3255ms step_avg:57.10ms
step:58/2330 train_time:3314ms step_avg:57.14ms
step:59/2330 train_time:3370ms step_avg:57.12ms
step:60/2330 train_time:3429ms step_avg:57.15ms
step:61/2330 train_time:3485ms step_avg:57.13ms
step:62/2330 train_time:3544ms step_avg:57.16ms
step:63/2330 train_time:3599ms step_avg:57.13ms
step:64/2330 train_time:3658ms step_avg:57.15ms
step:65/2330 train_time:3713ms step_avg:57.12ms
step:66/2330 train_time:3773ms step_avg:57.16ms
step:67/2330 train_time:3828ms step_avg:57.14ms
step:68/2330 train_time:3887ms step_avg:57.16ms
step:69/2330 train_time:3943ms step_avg:57.15ms
step:70/2330 train_time:4002ms step_avg:57.18ms
step:71/2330 train_time:4059ms step_avg:57.17ms
step:72/2330 train_time:4118ms step_avg:57.19ms
step:73/2330 train_time:4174ms step_avg:57.18ms
step:74/2330 train_time:4233ms step_avg:57.20ms
step:75/2330 train_time:4290ms step_avg:57.20ms
step:76/2330 train_time:4349ms step_avg:57.22ms
step:77/2330 train_time:4405ms step_avg:57.20ms
step:78/2330 train_time:4464ms step_avg:57.23ms
step:79/2330 train_time:4520ms step_avg:57.21ms
step:80/2330 train_time:4578ms step_avg:57.22ms
step:81/2330 train_time:4633ms step_avg:57.20ms
step:82/2330 train_time:4693ms step_avg:57.23ms
step:83/2330 train_time:4749ms step_avg:57.22ms
step:84/2330 train_time:4808ms step_avg:57.24ms
step:85/2330 train_time:4864ms step_avg:57.22ms
step:86/2330 train_time:4923ms step_avg:57.24ms
step:87/2330 train_time:4979ms step_avg:57.24ms
step:88/2330 train_time:5038ms step_avg:57.25ms
step:89/2330 train_time:5094ms step_avg:57.23ms
step:90/2330 train_time:5152ms step_avg:57.25ms
step:91/2330 train_time:5209ms step_avg:57.24ms
step:92/2330 train_time:5268ms step_avg:57.26ms
step:93/2330 train_time:5324ms step_avg:57.25ms
step:94/2330 train_time:5383ms step_avg:57.27ms
step:95/2330 train_time:5439ms step_avg:57.26ms
step:96/2330 train_time:5499ms step_avg:57.28ms
step:97/2330 train_time:5554ms step_avg:57.26ms
step:98/2330 train_time:5613ms step_avg:57.28ms
step:99/2330 train_time:5669ms step_avg:57.26ms
step:100/2330 train_time:5728ms step_avg:57.28ms
step:101/2330 train_time:5784ms step_avg:57.26ms
step:102/2330 train_time:5842ms step_avg:57.28ms
step:103/2330 train_time:5899ms step_avg:57.27ms
step:104/2330 train_time:5957ms step_avg:57.28ms
step:105/2330 train_time:6013ms step_avg:57.27ms
step:106/2330 train_time:6072ms step_avg:57.28ms
step:107/2330 train_time:6128ms step_avg:57.27ms
step:108/2330 train_time:6187ms step_avg:57.29ms
step:109/2330 train_time:6243ms step_avg:57.27ms
step:110/2330 train_time:6302ms step_avg:57.29ms
step:111/2330 train_time:6358ms step_avg:57.28ms
step:112/2330 train_time:6417ms step_avg:57.29ms
step:113/2330 train_time:6472ms step_avg:57.28ms
step:114/2330 train_time:6531ms step_avg:57.29ms
step:115/2330 train_time:6587ms step_avg:57.28ms
step:116/2330 train_time:6646ms step_avg:57.29ms
step:117/2330 train_time:6702ms step_avg:57.28ms
step:118/2330 train_time:6760ms step_avg:57.29ms
step:119/2330 train_time:6815ms step_avg:57.27ms
step:120/2330 train_time:6875ms step_avg:57.29ms
step:121/2330 train_time:6931ms step_avg:57.28ms
step:122/2330 train_time:6990ms step_avg:57.30ms
step:123/2330 train_time:7047ms step_avg:57.29ms
step:124/2330 train_time:7106ms step_avg:57.31ms
step:125/2330 train_time:7161ms step_avg:57.29ms
step:126/2330 train_time:7221ms step_avg:57.31ms
step:127/2330 train_time:7277ms step_avg:57.30ms
step:128/2330 train_time:7336ms step_avg:57.32ms
step:129/2330 train_time:7393ms step_avg:57.31ms
step:130/2330 train_time:7451ms step_avg:57.32ms
step:131/2330 train_time:7508ms step_avg:57.31ms
step:132/2330 train_time:7566ms step_avg:57.32ms
step:133/2330 train_time:7622ms step_avg:57.31ms
step:134/2330 train_time:7681ms step_avg:57.32ms
step:135/2330 train_time:7737ms step_avg:57.31ms
step:136/2330 train_time:7796ms step_avg:57.32ms
step:137/2330 train_time:7852ms step_avg:57.32ms
step:138/2330 train_time:7911ms step_avg:57.32ms
step:139/2330 train_time:7966ms step_avg:57.31ms
step:140/2330 train_time:8025ms step_avg:57.32ms
step:141/2330 train_time:8082ms step_avg:57.32ms
step:142/2330 train_time:8141ms step_avg:57.33ms
step:143/2330 train_time:8196ms step_avg:57.31ms
step:144/2330 train_time:8256ms step_avg:57.33ms
step:145/2330 train_time:8312ms step_avg:57.32ms
step:146/2330 train_time:8370ms step_avg:57.33ms
step:147/2330 train_time:8426ms step_avg:57.32ms
step:148/2330 train_time:8485ms step_avg:57.33ms
step:149/2330 train_time:8541ms step_avg:57.32ms
step:150/2330 train_time:8601ms step_avg:57.34ms
step:151/2330 train_time:8656ms step_avg:57.33ms
step:152/2330 train_time:8715ms step_avg:57.34ms
step:153/2330 train_time:8771ms step_avg:57.32ms
step:154/2330 train_time:8830ms step_avg:57.34ms
step:155/2330 train_time:8886ms step_avg:57.33ms
step:156/2330 train_time:8945ms step_avg:57.34ms
step:157/2330 train_time:9001ms step_avg:57.33ms
step:158/2330 train_time:9060ms step_avg:57.34ms
step:159/2330 train_time:9115ms step_avg:57.33ms
step:160/2330 train_time:9175ms step_avg:57.34ms
step:161/2330 train_time:9231ms step_avg:57.34ms
step:162/2330 train_time:9290ms step_avg:57.35ms
step:163/2330 train_time:9347ms step_avg:57.34ms
step:164/2330 train_time:9406ms step_avg:57.35ms
step:165/2330 train_time:9462ms step_avg:57.34ms
step:166/2330 train_time:9520ms step_avg:57.35ms
step:167/2330 train_time:9576ms step_avg:57.34ms
step:168/2330 train_time:9635ms step_avg:57.35ms
step:169/2330 train_time:9691ms step_avg:57.34ms
step:170/2330 train_time:9750ms step_avg:57.35ms
step:171/2330 train_time:9806ms step_avg:57.34ms
step:172/2330 train_time:9865ms step_avg:57.35ms
step:173/2330 train_time:9921ms step_avg:57.35ms
step:174/2330 train_time:9979ms step_avg:57.35ms
step:175/2330 train_time:10035ms step_avg:57.34ms
step:176/2330 train_time:10095ms step_avg:57.36ms
step:177/2330 train_time:10151ms step_avg:57.35ms
step:178/2330 train_time:10209ms step_avg:57.36ms
step:179/2330 train_time:10266ms step_avg:57.35ms
step:180/2330 train_time:10325ms step_avg:57.36ms
step:181/2330 train_time:10381ms step_avg:57.35ms
step:182/2330 train_time:10440ms step_avg:57.36ms
step:183/2330 train_time:10496ms step_avg:57.36ms
step:184/2330 train_time:10556ms step_avg:57.37ms
step:185/2330 train_time:10611ms step_avg:57.36ms
step:186/2330 train_time:10670ms step_avg:57.37ms
step:187/2330 train_time:10726ms step_avg:57.36ms
step:188/2330 train_time:10784ms step_avg:57.36ms
step:189/2330 train_time:10840ms step_avg:57.36ms
step:190/2330 train_time:10900ms step_avg:57.37ms
step:191/2330 train_time:10956ms step_avg:57.36ms
step:192/2330 train_time:11014ms step_avg:57.37ms
step:193/2330 train_time:11070ms step_avg:57.36ms
step:194/2330 train_time:11129ms step_avg:57.37ms
step:195/2330 train_time:11185ms step_avg:57.36ms
step:196/2330 train_time:11245ms step_avg:57.37ms
step:197/2330 train_time:11301ms step_avg:57.36ms
step:198/2330 train_time:11359ms step_avg:57.37ms
step:199/2330 train_time:11415ms step_avg:57.36ms
step:200/2330 train_time:11474ms step_avg:57.37ms
step:201/2330 train_time:11530ms step_avg:57.36ms
step:202/2330 train_time:11589ms step_avg:57.37ms
step:203/2330 train_time:11645ms step_avg:57.36ms
step:204/2330 train_time:11704ms step_avg:57.37ms
step:205/2330 train_time:11759ms step_avg:57.36ms
step:206/2330 train_time:11818ms step_avg:57.37ms
step:207/2330 train_time:11874ms step_avg:57.36ms
step:208/2330 train_time:11934ms step_avg:57.37ms
step:209/2330 train_time:11990ms step_avg:57.37ms
step:210/2330 train_time:12048ms step_avg:57.37ms
step:211/2330 train_time:12104ms step_avg:57.36ms
step:212/2330 train_time:12163ms step_avg:57.37ms
step:213/2330 train_time:12219ms step_avg:57.36ms
step:214/2330 train_time:12278ms step_avg:57.37ms
step:215/2330 train_time:12333ms step_avg:57.36ms
step:216/2330 train_time:12393ms step_avg:57.37ms
step:217/2330 train_time:12449ms step_avg:57.37ms
step:218/2330 train_time:12508ms step_avg:57.38ms
step:219/2330 train_time:12564ms step_avg:57.37ms
step:220/2330 train_time:12623ms step_avg:57.38ms
step:221/2330 train_time:12679ms step_avg:57.37ms
step:222/2330 train_time:12738ms step_avg:57.38ms
step:223/2330 train_time:12794ms step_avg:57.37ms
step:224/2330 train_time:12853ms step_avg:57.38ms
step:225/2330 train_time:12909ms step_avg:57.37ms
step:226/2330 train_time:12968ms step_avg:57.38ms
step:227/2330 train_time:13024ms step_avg:57.37ms
step:228/2330 train_time:13083ms step_avg:57.38ms
step:229/2330 train_time:13138ms step_avg:57.37ms
step:230/2330 train_time:13197ms step_avg:57.38ms
step:231/2330 train_time:13252ms step_avg:57.37ms
step:232/2330 train_time:13312ms step_avg:57.38ms
step:233/2330 train_time:13368ms step_avg:57.37ms
step:234/2330 train_time:13426ms step_avg:57.38ms
step:235/2330 train_time:13483ms step_avg:57.37ms
step:236/2330 train_time:13541ms step_avg:57.38ms
step:237/2330 train_time:13598ms step_avg:57.38ms
step:238/2330 train_time:13656ms step_avg:57.38ms
step:239/2330 train_time:13712ms step_avg:57.37ms
step:240/2330 train_time:13771ms step_avg:57.38ms
step:241/2330 train_time:13827ms step_avg:57.37ms
step:242/2330 train_time:13886ms step_avg:57.38ms
step:243/2330 train_time:13942ms step_avg:57.37ms
step:244/2330 train_time:14002ms step_avg:57.39ms
step:245/2330 train_time:14058ms step_avg:57.38ms
step:246/2330 train_time:14117ms step_avg:57.39ms
step:247/2330 train_time:14173ms step_avg:57.38ms
step:248/2330 train_time:14232ms step_avg:57.39ms
step:249/2330 train_time:14288ms step_avg:57.38ms
step:250/2330 train_time:14347ms step_avg:57.39ms
step:250/2330 val_loss:4.8987 train_time:14426ms step_avg:57.70ms
step:251/2330 train_time:14446ms step_avg:57.55ms
step:252/2330 train_time:14466ms step_avg:57.40ms
step:253/2330 train_time:14520ms step_avg:57.39ms
step:254/2330 train_time:14586ms step_avg:57.42ms
step:255/2330 train_time:14642ms step_avg:57.42ms
step:256/2330 train_time:14704ms step_avg:57.44ms
step:257/2330 train_time:14760ms step_avg:57.43ms
step:258/2330 train_time:14820ms step_avg:57.44ms
step:259/2330 train_time:14875ms step_avg:57.43ms
step:260/2330 train_time:14935ms step_avg:57.44ms
step:261/2330 train_time:14990ms step_avg:57.43ms
step:262/2330 train_time:15048ms step_avg:57.44ms
step:263/2330 train_time:15103ms step_avg:57.43ms
step:264/2330 train_time:15162ms step_avg:57.43ms
step:265/2330 train_time:15217ms step_avg:57.42ms
step:266/2330 train_time:15276ms step_avg:57.43ms
step:267/2330 train_time:15332ms step_avg:57.42ms
step:268/2330 train_time:15392ms step_avg:57.43ms
step:269/2330 train_time:15449ms step_avg:57.43ms
step:270/2330 train_time:15507ms step_avg:57.43ms
step:271/2330 train_time:15564ms step_avg:57.43ms
step:272/2330 train_time:15624ms step_avg:57.44ms
step:273/2330 train_time:15680ms step_avg:57.44ms
step:274/2330 train_time:15742ms step_avg:57.45ms
step:275/2330 train_time:15798ms step_avg:57.45ms
step:276/2330 train_time:15858ms step_avg:57.46ms
step:277/2330 train_time:15914ms step_avg:57.45ms
step:278/2330 train_time:15973ms step_avg:57.46ms
step:279/2330 train_time:16029ms step_avg:57.45ms
step:280/2330 train_time:16087ms step_avg:57.45ms
step:281/2330 train_time:16142ms step_avg:57.45ms
step:282/2330 train_time:16201ms step_avg:57.45ms
step:283/2330 train_time:16256ms step_avg:57.44ms
step:284/2330 train_time:16316ms step_avg:57.45ms
step:285/2330 train_time:16372ms step_avg:57.44ms
step:286/2330 train_time:16430ms step_avg:57.45ms
step:287/2330 train_time:16487ms step_avg:57.45ms
step:288/2330 train_time:16546ms step_avg:57.45ms
step:289/2330 train_time:16602ms step_avg:57.45ms
step:290/2330 train_time:16663ms step_avg:57.46ms
step:291/2330 train_time:16719ms step_avg:57.45ms
step:292/2330 train_time:16778ms step_avg:57.46ms
step:293/2330 train_time:16835ms step_avg:57.46ms
step:294/2330 train_time:16893ms step_avg:57.46ms
step:295/2330 train_time:16949ms step_avg:57.45ms
step:296/2330 train_time:17008ms step_avg:57.46ms
step:297/2330 train_time:17064ms step_avg:57.45ms
step:298/2330 train_time:17123ms step_avg:57.46ms
step:299/2330 train_time:17178ms step_avg:57.45ms
step:300/2330 train_time:17237ms step_avg:57.46ms
step:301/2330 train_time:17293ms step_avg:57.45ms
step:302/2330 train_time:17352ms step_avg:57.46ms
step:303/2330 train_time:17408ms step_avg:57.45ms
step:304/2330 train_time:17467ms step_avg:57.46ms
step:305/2330 train_time:17524ms step_avg:57.46ms
step:306/2330 train_time:17583ms step_avg:57.46ms
step:307/2330 train_time:17639ms step_avg:57.46ms
step:308/2330 train_time:17698ms step_avg:57.46ms
step:309/2330 train_time:17754ms step_avg:57.46ms
step:310/2330 train_time:17815ms step_avg:57.47ms
step:311/2330 train_time:17870ms step_avg:57.46ms
step:312/2330 train_time:17930ms step_avg:57.47ms
step:313/2330 train_time:17986ms step_avg:57.46ms
step:314/2330 train_time:18044ms step_avg:57.47ms
step:315/2330 train_time:18099ms step_avg:57.46ms
step:316/2330 train_time:18159ms step_avg:57.47ms
step:317/2330 train_time:18215ms step_avg:57.46ms
step:318/2330 train_time:18274ms step_avg:57.46ms
step:319/2330 train_time:18329ms step_avg:57.46ms
step:320/2330 train_time:18388ms step_avg:57.46ms
step:321/2330 train_time:18444ms step_avg:57.46ms
step:322/2330 train_time:18503ms step_avg:57.46ms
step:323/2330 train_time:18559ms step_avg:57.46ms
step:324/2330 train_time:18618ms step_avg:57.46ms
step:325/2330 train_time:18674ms step_avg:57.46ms
step:326/2330 train_time:18733ms step_avg:57.46ms
step:327/2330 train_time:18789ms step_avg:57.46ms
step:328/2330 train_time:18848ms step_avg:57.46ms
step:329/2330 train_time:18904ms step_avg:57.46ms
step:330/2330 train_time:18964ms step_avg:57.47ms
step:331/2330 train_time:19019ms step_avg:57.46ms
step:332/2330 train_time:19079ms step_avg:57.47ms
step:333/2330 train_time:19134ms step_avg:57.46ms
step:334/2330 train_time:19193ms step_avg:57.46ms
step:335/2330 train_time:19249ms step_avg:57.46ms
step:336/2330 train_time:19308ms step_avg:57.46ms
step:337/2330 train_time:19364ms step_avg:57.46ms
step:338/2330 train_time:19422ms step_avg:57.46ms
step:339/2330 train_time:19479ms step_avg:57.46ms
step:340/2330 train_time:19538ms step_avg:57.46ms
step:341/2330 train_time:19594ms step_avg:57.46ms
step:342/2330 train_time:19654ms step_avg:57.47ms
step:343/2330 train_time:19710ms step_avg:57.46ms
step:344/2330 train_time:19769ms step_avg:57.47ms
step:345/2330 train_time:19825ms step_avg:57.46ms
step:346/2330 train_time:19884ms step_avg:57.47ms
step:347/2330 train_time:19940ms step_avg:57.46ms
step:348/2330 train_time:19999ms step_avg:57.47ms
step:349/2330 train_time:20055ms step_avg:57.46ms
step:350/2330 train_time:20114ms step_avg:57.47ms
step:351/2330 train_time:20169ms step_avg:57.46ms
step:352/2330 train_time:20229ms step_avg:57.47ms
step:353/2330 train_time:20285ms step_avg:57.47ms
step:354/2330 train_time:20344ms step_avg:57.47ms
step:355/2330 train_time:20400ms step_avg:57.46ms
step:356/2330 train_time:20459ms step_avg:57.47ms
step:357/2330 train_time:20515ms step_avg:57.46ms
step:358/2330 train_time:20574ms step_avg:57.47ms
step:359/2330 train_time:20629ms step_avg:57.46ms
step:360/2330 train_time:20689ms step_avg:57.47ms
step:361/2330 train_time:20745ms step_avg:57.47ms
step:362/2330 train_time:20804ms step_avg:57.47ms
step:363/2330 train_time:20860ms step_avg:57.47ms
step:364/2330 train_time:20920ms step_avg:57.47ms
step:365/2330 train_time:20976ms step_avg:57.47ms
step:366/2330 train_time:21036ms step_avg:57.48ms
step:367/2330 train_time:21091ms step_avg:57.47ms
step:368/2330 train_time:21151ms step_avg:57.47ms
step:369/2330 train_time:21207ms step_avg:57.47ms
step:370/2330 train_time:21265ms step_avg:57.47ms
step:371/2330 train_time:21321ms step_avg:57.47ms
step:372/2330 train_time:21381ms step_avg:57.48ms
step:373/2330 train_time:21437ms step_avg:57.47ms
step:374/2330 train_time:21495ms step_avg:57.47ms
step:375/2330 train_time:21552ms step_avg:57.47ms
step:376/2330 train_time:21611ms step_avg:57.48ms
step:377/2330 train_time:21666ms step_avg:57.47ms
step:378/2330 train_time:21725ms step_avg:57.47ms
step:379/2330 train_time:21782ms step_avg:57.47ms
step:380/2330 train_time:21841ms step_avg:57.48ms
step:381/2330 train_time:21897ms step_avg:57.47ms
step:382/2330 train_time:21957ms step_avg:57.48ms
step:383/2330 train_time:22013ms step_avg:57.48ms
step:384/2330 train_time:22072ms step_avg:57.48ms
step:385/2330 train_time:22128ms step_avg:57.48ms
step:386/2330 train_time:22187ms step_avg:57.48ms
step:387/2330 train_time:22243ms step_avg:57.48ms
step:388/2330 train_time:22301ms step_avg:57.48ms
step:389/2330 train_time:22357ms step_avg:57.47ms
step:390/2330 train_time:22416ms step_avg:57.48ms
step:391/2330 train_time:22472ms step_avg:57.47ms
step:392/2330 train_time:22531ms step_avg:57.48ms
step:393/2330 train_time:22587ms step_avg:57.47ms
step:394/2330 train_time:22646ms step_avg:57.48ms
step:395/2330 train_time:22702ms step_avg:57.47ms
step:396/2330 train_time:22762ms step_avg:57.48ms
step:397/2330 train_time:22818ms step_avg:57.48ms
step:398/2330 train_time:22877ms step_avg:57.48ms
step:399/2330 train_time:22932ms step_avg:57.47ms
step:400/2330 train_time:22992ms step_avg:57.48ms
step:401/2330 train_time:23049ms step_avg:57.48ms
step:402/2330 train_time:23107ms step_avg:57.48ms
step:403/2330 train_time:23163ms step_avg:57.48ms
step:404/2330 train_time:23222ms step_avg:57.48ms
step:405/2330 train_time:23278ms step_avg:57.48ms
step:406/2330 train_time:23337ms step_avg:57.48ms
step:407/2330 train_time:23393ms step_avg:57.48ms
step:408/2330 train_time:23452ms step_avg:57.48ms
step:409/2330 train_time:23507ms step_avg:57.47ms
step:410/2330 train_time:23566ms step_avg:57.48ms
step:411/2330 train_time:23622ms step_avg:57.47ms
step:412/2330 train_time:23682ms step_avg:57.48ms
step:413/2330 train_time:23738ms step_avg:57.48ms
step:414/2330 train_time:23797ms step_avg:57.48ms
step:415/2330 train_time:23853ms step_avg:57.48ms
step:416/2330 train_time:23912ms step_avg:57.48ms
step:417/2330 train_time:23968ms step_avg:57.48ms
step:418/2330 train_time:24027ms step_avg:57.48ms
step:419/2330 train_time:24083ms step_avg:57.48ms
step:420/2330 train_time:24143ms step_avg:57.48ms
step:421/2330 train_time:24198ms step_avg:57.48ms
step:422/2330 train_time:24258ms step_avg:57.48ms
step:423/2330 train_time:24313ms step_avg:57.48ms
step:424/2330 train_time:24374ms step_avg:57.48ms
step:425/2330 train_time:24430ms step_avg:57.48ms
step:426/2330 train_time:24489ms step_avg:57.49ms
step:427/2330 train_time:24546ms step_avg:57.48ms
step:428/2330 train_time:24604ms step_avg:57.49ms
step:429/2330 train_time:24660ms step_avg:57.48ms
step:430/2330 train_time:24719ms step_avg:57.49ms
step:431/2330 train_time:24775ms step_avg:57.48ms
step:432/2330 train_time:24835ms step_avg:57.49ms
step:433/2330 train_time:24892ms step_avg:57.49ms
step:434/2330 train_time:24951ms step_avg:57.49ms
step:435/2330 train_time:25007ms step_avg:57.49ms
step:436/2330 train_time:25066ms step_avg:57.49ms
step:437/2330 train_time:25123ms step_avg:57.49ms
step:438/2330 train_time:25181ms step_avg:57.49ms
step:439/2330 train_time:25237ms step_avg:57.49ms
step:440/2330 train_time:25297ms step_avg:57.49ms
step:441/2330 train_time:25354ms step_avg:57.49ms
step:442/2330 train_time:25413ms step_avg:57.50ms
step:443/2330 train_time:25469ms step_avg:57.49ms
step:444/2330 train_time:25528ms step_avg:57.49ms
step:445/2330 train_time:25583ms step_avg:57.49ms
step:446/2330 train_time:25644ms step_avg:57.50ms
step:447/2330 train_time:25699ms step_avg:57.49ms
step:448/2330 train_time:25758ms step_avg:57.50ms
step:449/2330 train_time:25815ms step_avg:57.49ms
step:450/2330 train_time:25875ms step_avg:57.50ms
step:451/2330 train_time:25930ms step_avg:57.50ms
step:452/2330 train_time:25989ms step_avg:57.50ms
step:453/2330 train_time:26045ms step_avg:57.49ms
step:454/2330 train_time:26105ms step_avg:57.50ms
step:455/2330 train_time:26161ms step_avg:57.50ms
step:456/2330 train_time:26220ms step_avg:57.50ms
step:457/2330 train_time:26276ms step_avg:57.50ms
step:458/2330 train_time:26336ms step_avg:57.50ms
step:459/2330 train_time:26392ms step_avg:57.50ms
step:460/2330 train_time:26453ms step_avg:57.51ms
step:461/2330 train_time:26508ms step_avg:57.50ms
step:462/2330 train_time:26568ms step_avg:57.51ms
step:463/2330 train_time:26624ms step_avg:57.50ms
step:464/2330 train_time:26684ms step_avg:57.51ms
step:465/2330 train_time:26739ms step_avg:57.50ms
step:466/2330 train_time:26799ms step_avg:57.51ms
step:467/2330 train_time:26856ms step_avg:57.51ms
step:468/2330 train_time:26915ms step_avg:57.51ms
step:469/2330 train_time:26971ms step_avg:57.51ms
step:470/2330 train_time:27029ms step_avg:57.51ms
step:471/2330 train_time:27085ms step_avg:57.51ms
step:472/2330 train_time:27144ms step_avg:57.51ms
step:473/2330 train_time:27199ms step_avg:57.50ms
step:474/2330 train_time:27259ms step_avg:57.51ms
step:475/2330 train_time:27315ms step_avg:57.50ms
step:476/2330 train_time:27374ms step_avg:57.51ms
step:477/2330 train_time:27430ms step_avg:57.51ms
step:478/2330 train_time:27490ms step_avg:57.51ms
step:479/2330 train_time:27545ms step_avg:57.51ms
step:480/2330 train_time:27604ms step_avg:57.51ms
step:481/2330 train_time:27660ms step_avg:57.51ms
step:482/2330 train_time:27719ms step_avg:57.51ms
step:483/2330 train_time:27776ms step_avg:57.51ms
step:484/2330 train_time:27835ms step_avg:57.51ms
step:485/2330 train_time:27892ms step_avg:57.51ms
step:486/2330 train_time:27951ms step_avg:57.51ms
step:487/2330 train_time:28006ms step_avg:57.51ms
step:488/2330 train_time:28065ms step_avg:57.51ms
step:489/2330 train_time:28121ms step_avg:57.51ms
step:490/2330 train_time:28181ms step_avg:57.51ms
step:491/2330 train_time:28237ms step_avg:57.51ms
step:492/2330 train_time:28296ms step_avg:57.51ms
step:493/2330 train_time:28352ms step_avg:57.51ms
step:494/2330 train_time:28411ms step_avg:57.51ms
step:495/2330 train_time:28467ms step_avg:57.51ms
step:496/2330 train_time:28526ms step_avg:57.51ms
step:497/2330 train_time:28582ms step_avg:57.51ms
step:498/2330 train_time:28641ms step_avg:57.51ms
step:499/2330 train_time:28698ms step_avg:57.51ms
step:500/2330 train_time:28757ms step_avg:57.51ms
step:500/2330 val_loss:4.4100 train_time:28836ms step_avg:57.67ms
step:501/2330 train_time:28856ms step_avg:57.60ms
step:502/2330 train_time:28875ms step_avg:57.52ms
step:503/2330 train_time:28933ms step_avg:57.52ms
step:504/2330 train_time:28994ms step_avg:57.53ms
step:505/2330 train_time:29050ms step_avg:57.53ms
step:506/2330 train_time:29111ms step_avg:57.53ms
step:507/2330 train_time:29166ms step_avg:57.53ms
step:508/2330 train_time:29228ms step_avg:57.53ms
step:509/2330 train_time:29284ms step_avg:57.53ms
step:510/2330 train_time:29343ms step_avg:57.53ms
step:511/2330 train_time:29398ms step_avg:57.53ms
step:512/2330 train_time:29456ms step_avg:57.53ms
step:513/2330 train_time:29512ms step_avg:57.53ms
step:514/2330 train_time:29570ms step_avg:57.53ms
step:515/2330 train_time:29627ms step_avg:57.53ms
step:516/2330 train_time:29685ms step_avg:57.53ms
step:517/2330 train_time:29742ms step_avg:57.53ms
step:518/2330 train_time:29800ms step_avg:57.53ms
step:519/2330 train_time:29857ms step_avg:57.53ms
step:520/2330 train_time:29917ms step_avg:57.53ms
step:521/2330 train_time:29974ms step_avg:57.53ms
step:522/2330 train_time:30034ms step_avg:57.54ms
step:523/2330 train_time:30091ms step_avg:57.53ms
step:524/2330 train_time:30151ms step_avg:57.54ms
step:525/2330 train_time:30207ms step_avg:57.54ms
step:526/2330 train_time:30266ms step_avg:57.54ms
step:527/2330 train_time:30322ms step_avg:57.54ms
step:528/2330 train_time:30381ms step_avg:57.54ms
step:529/2330 train_time:30437ms step_avg:57.54ms
step:530/2330 train_time:30496ms step_avg:57.54ms
step:531/2330 train_time:30551ms step_avg:57.54ms
step:532/2330 train_time:30610ms step_avg:57.54ms
step:533/2330 train_time:30666ms step_avg:57.54ms
step:534/2330 train_time:30725ms step_avg:57.54ms
step:535/2330 train_time:30781ms step_avg:57.54ms
step:536/2330 train_time:30841ms step_avg:57.54ms
step:537/2330 train_time:30898ms step_avg:57.54ms
step:538/2330 train_time:30957ms step_avg:57.54ms
step:539/2330 train_time:31013ms step_avg:57.54ms
step:540/2330 train_time:31073ms step_avg:57.54ms
step:541/2330 train_time:31130ms step_avg:57.54ms
step:542/2330 train_time:31189ms step_avg:57.54ms
step:543/2330 train_time:31245ms step_avg:57.54ms
step:544/2330 train_time:31304ms step_avg:57.54ms
step:545/2330 train_time:31359ms step_avg:57.54ms
step:546/2330 train_time:31419ms step_avg:57.54ms
step:547/2330 train_time:31474ms step_avg:57.54ms
step:548/2330 train_time:31533ms step_avg:57.54ms
step:549/2330 train_time:31589ms step_avg:57.54ms
step:550/2330 train_time:31647ms step_avg:57.54ms
step:551/2330 train_time:31703ms step_avg:57.54ms
step:552/2330 train_time:31763ms step_avg:57.54ms
step:553/2330 train_time:31819ms step_avg:57.54ms
step:554/2330 train_time:31878ms step_avg:57.54ms
step:555/2330 train_time:31934ms step_avg:57.54ms
step:556/2330 train_time:31994ms step_avg:57.54ms
step:557/2330 train_time:32050ms step_avg:57.54ms
step:558/2330 train_time:32110ms step_avg:57.54ms
step:559/2330 train_time:32166ms step_avg:57.54ms
step:560/2330 train_time:32226ms step_avg:57.55ms
step:561/2330 train_time:32282ms step_avg:57.54ms
step:562/2330 train_time:32341ms step_avg:57.55ms
step:563/2330 train_time:32397ms step_avg:57.54ms
step:564/2330 train_time:32456ms step_avg:57.55ms
step:565/2330 train_time:32511ms step_avg:57.54ms
step:566/2330 train_time:32570ms step_avg:57.54ms
step:567/2330 train_time:32626ms step_avg:57.54ms
step:568/2330 train_time:32685ms step_avg:57.54ms
step:569/2330 train_time:32742ms step_avg:57.54ms
step:570/2330 train_time:32801ms step_avg:57.55ms
step:571/2330 train_time:32857ms step_avg:57.54ms
step:572/2330 train_time:32917ms step_avg:57.55ms
step:573/2330 train_time:32973ms step_avg:57.54ms
step:574/2330 train_time:33032ms step_avg:57.55ms
step:575/2330 train_time:33089ms step_avg:57.55ms
step:576/2330 train_time:33148ms step_avg:57.55ms
step:577/2330 train_time:33204ms step_avg:57.55ms
step:578/2330 train_time:33263ms step_avg:57.55ms
step:579/2330 train_time:33320ms step_avg:57.55ms
step:580/2330 train_time:33378ms step_avg:57.55ms
step:581/2330 train_time:33434ms step_avg:57.55ms
step:582/2330 train_time:33493ms step_avg:57.55ms
step:583/2330 train_time:33548ms step_avg:57.54ms
step:584/2330 train_time:33607ms step_avg:57.55ms
step:585/2330 train_time:33663ms step_avg:57.54ms
step:586/2330 train_time:33722ms step_avg:57.55ms
step:587/2330 train_time:33778ms step_avg:57.54ms
step:588/2330 train_time:33838ms step_avg:57.55ms
step:589/2330 train_time:33894ms step_avg:57.54ms
step:590/2330 train_time:33953ms step_avg:57.55ms
step:591/2330 train_time:34009ms step_avg:57.54ms
step:592/2330 train_time:34069ms step_avg:57.55ms
step:593/2330 train_time:34125ms step_avg:57.55ms
step:594/2330 train_time:34184ms step_avg:57.55ms
step:595/2330 train_time:34240ms step_avg:57.55ms
step:596/2330 train_time:34299ms step_avg:57.55ms
step:597/2330 train_time:34355ms step_avg:57.55ms
step:598/2330 train_time:34414ms step_avg:57.55ms
step:599/2330 train_time:34470ms step_avg:57.55ms
step:600/2330 train_time:34529ms step_avg:57.55ms
step:601/2330 train_time:34585ms step_avg:57.55ms
step:602/2330 train_time:34644ms step_avg:57.55ms
step:603/2330 train_time:34700ms step_avg:57.55ms
step:604/2330 train_time:34759ms step_avg:57.55ms
step:605/2330 train_time:34815ms step_avg:57.55ms
step:606/2330 train_time:34874ms step_avg:57.55ms
step:607/2330 train_time:34930ms step_avg:57.55ms
step:608/2330 train_time:34990ms step_avg:57.55ms
step:609/2330 train_time:35046ms step_avg:57.55ms
step:610/2330 train_time:35106ms step_avg:57.55ms
step:611/2330 train_time:35162ms step_avg:57.55ms
step:612/2330 train_time:35222ms step_avg:57.55ms
step:613/2330 train_time:35279ms step_avg:57.55ms
step:614/2330 train_time:35338ms step_avg:57.55ms
step:615/2330 train_time:35394ms step_avg:57.55ms
step:616/2330 train_time:35453ms step_avg:57.55ms
step:617/2330 train_time:35509ms step_avg:57.55ms
step:618/2330 train_time:35568ms step_avg:57.55ms
step:619/2330 train_time:35625ms step_avg:57.55ms
step:620/2330 train_time:35684ms step_avg:57.55ms
step:621/2330 train_time:35740ms step_avg:57.55ms
step:622/2330 train_time:35799ms step_avg:57.55ms
step:623/2330 train_time:35855ms step_avg:57.55ms
step:624/2330 train_time:35915ms step_avg:57.56ms
step:625/2330 train_time:35970ms step_avg:57.55ms
step:626/2330 train_time:36030ms step_avg:57.56ms
step:627/2330 train_time:36087ms step_avg:57.55ms
step:628/2330 train_time:36146ms step_avg:57.56ms
step:629/2330 train_time:36202ms step_avg:57.56ms
step:630/2330 train_time:36261ms step_avg:57.56ms
step:631/2330 train_time:36318ms step_avg:57.56ms
step:632/2330 train_time:36377ms step_avg:57.56ms
step:633/2330 train_time:36433ms step_avg:57.56ms
step:634/2330 train_time:36494ms step_avg:57.56ms
step:635/2330 train_time:36549ms step_avg:57.56ms
step:636/2330 train_time:36609ms step_avg:57.56ms
step:637/2330 train_time:36665ms step_avg:57.56ms
step:638/2330 train_time:36725ms step_avg:57.56ms
step:639/2330 train_time:36782ms step_avg:57.56ms
step:640/2330 train_time:36841ms step_avg:57.56ms
step:641/2330 train_time:36897ms step_avg:57.56ms
step:642/2330 train_time:36955ms step_avg:57.56ms
step:643/2330 train_time:37011ms step_avg:57.56ms
step:644/2330 train_time:37072ms step_avg:57.56ms
step:645/2330 train_time:37128ms step_avg:57.56ms
step:646/2330 train_time:37187ms step_avg:57.56ms
step:647/2330 train_time:37243ms step_avg:57.56ms
step:648/2330 train_time:37303ms step_avg:57.57ms
step:649/2330 train_time:37359ms step_avg:57.56ms
step:650/2330 train_time:37418ms step_avg:57.57ms
step:651/2330 train_time:37474ms step_avg:57.56ms
step:652/2330 train_time:37533ms step_avg:57.57ms
step:653/2330 train_time:37588ms step_avg:57.56ms
step:654/2330 train_time:37648ms step_avg:57.57ms
step:655/2330 train_time:37704ms step_avg:57.56ms
step:656/2330 train_time:37764ms step_avg:57.57ms
step:657/2330 train_time:37820ms step_avg:57.56ms
step:658/2330 train_time:37879ms step_avg:57.57ms
step:659/2330 train_time:37936ms step_avg:57.57ms
step:660/2330 train_time:37994ms step_avg:57.57ms
step:661/2330 train_time:38050ms step_avg:57.56ms
step:662/2330 train_time:38110ms step_avg:57.57ms
step:663/2330 train_time:38166ms step_avg:57.57ms
step:664/2330 train_time:38226ms step_avg:57.57ms
step:665/2330 train_time:38283ms step_avg:57.57ms
step:666/2330 train_time:38342ms step_avg:57.57ms
step:667/2330 train_time:38398ms step_avg:57.57ms
step:668/2330 train_time:38457ms step_avg:57.57ms
step:669/2330 train_time:38513ms step_avg:57.57ms
step:670/2330 train_time:38572ms step_avg:57.57ms
step:671/2330 train_time:38628ms step_avg:57.57ms
step:672/2330 train_time:38687ms step_avg:57.57ms
step:673/2330 train_time:38742ms step_avg:57.57ms
step:674/2330 train_time:38803ms step_avg:57.57ms
step:675/2330 train_time:38859ms step_avg:57.57ms
step:676/2330 train_time:38919ms step_avg:57.57ms
step:677/2330 train_time:38975ms step_avg:57.57ms
step:678/2330 train_time:39034ms step_avg:57.57ms
step:679/2330 train_time:39089ms step_avg:57.57ms
step:680/2330 train_time:39149ms step_avg:57.57ms
step:681/2330 train_time:39206ms step_avg:57.57ms
step:682/2330 train_time:39265ms step_avg:57.57ms
step:683/2330 train_time:39322ms step_avg:57.57ms
step:684/2330 train_time:39381ms step_avg:57.57ms
step:685/2330 train_time:39437ms step_avg:57.57ms
step:686/2330 train_time:39496ms step_avg:57.57ms
step:687/2330 train_time:39552ms step_avg:57.57ms
step:688/2330 train_time:39611ms step_avg:57.57ms
step:689/2330 train_time:39668ms step_avg:57.57ms
step:690/2330 train_time:39726ms step_avg:57.57ms
step:691/2330 train_time:39783ms step_avg:57.57ms
step:692/2330 train_time:39843ms step_avg:57.58ms
step:693/2330 train_time:39899ms step_avg:57.57ms
step:694/2330 train_time:39958ms step_avg:57.58ms
step:695/2330 train_time:40014ms step_avg:57.57ms
step:696/2330 train_time:40074ms step_avg:57.58ms
step:697/2330 train_time:40129ms step_avg:57.57ms
step:698/2330 train_time:40190ms step_avg:57.58ms
step:699/2330 train_time:40246ms step_avg:57.58ms
step:700/2330 train_time:40307ms step_avg:57.58ms
step:701/2330 train_time:40363ms step_avg:57.58ms
step:702/2330 train_time:40423ms step_avg:57.58ms
step:703/2330 train_time:40479ms step_avg:57.58ms
step:704/2330 train_time:40538ms step_avg:57.58ms
step:705/2330 train_time:40594ms step_avg:57.58ms
step:706/2330 train_time:40653ms step_avg:57.58ms
step:707/2330 train_time:40708ms step_avg:57.58ms
step:708/2330 train_time:40768ms step_avg:57.58ms
step:709/2330 train_time:40826ms step_avg:57.58ms
step:710/2330 train_time:40885ms step_avg:57.58ms
step:711/2330 train_time:40942ms step_avg:57.58ms
step:712/2330 train_time:41001ms step_avg:57.59ms
step:713/2330 train_time:41057ms step_avg:57.58ms
step:714/2330 train_time:41115ms step_avg:57.58ms
step:715/2330 train_time:41171ms step_avg:57.58ms
step:716/2330 train_time:41231ms step_avg:57.58ms
step:717/2330 train_time:41288ms step_avg:57.58ms
step:718/2330 train_time:41347ms step_avg:57.59ms
step:719/2330 train_time:41404ms step_avg:57.59ms
step:720/2330 train_time:41463ms step_avg:57.59ms
step:721/2330 train_time:41519ms step_avg:57.58ms
step:722/2330 train_time:41578ms step_avg:57.59ms
step:723/2330 train_time:41634ms step_avg:57.59ms
step:724/2330 train_time:41694ms step_avg:57.59ms
step:725/2330 train_time:41749ms step_avg:57.59ms
step:726/2330 train_time:41810ms step_avg:57.59ms
step:727/2330 train_time:41866ms step_avg:57.59ms
step:728/2330 train_time:41925ms step_avg:57.59ms
step:729/2330 train_time:41981ms step_avg:57.59ms
step:730/2330 train_time:42041ms step_avg:57.59ms
step:731/2330 train_time:42097ms step_avg:57.59ms
step:732/2330 train_time:42155ms step_avg:57.59ms
step:733/2330 train_time:42212ms step_avg:57.59ms
step:734/2330 train_time:42272ms step_avg:57.59ms
step:735/2330 train_time:42329ms step_avg:57.59ms
step:736/2330 train_time:42388ms step_avg:57.59ms
step:737/2330 train_time:42444ms step_avg:57.59ms
step:738/2330 train_time:42503ms step_avg:57.59ms
step:739/2330 train_time:42559ms step_avg:57.59ms
step:740/2330 train_time:42618ms step_avg:57.59ms
step:741/2330 train_time:42673ms step_avg:57.59ms
step:742/2330 train_time:42733ms step_avg:57.59ms
step:743/2330 train_time:42790ms step_avg:57.59ms
step:744/2330 train_time:42849ms step_avg:57.59ms
step:745/2330 train_time:42905ms step_avg:57.59ms
step:746/2330 train_time:42964ms step_avg:57.59ms
step:747/2330 train_time:43021ms step_avg:57.59ms
step:748/2330 train_time:43079ms step_avg:57.59ms
step:749/2330 train_time:43135ms step_avg:57.59ms
step:750/2330 train_time:43195ms step_avg:57.59ms
step:750/2330 val_loss:4.2096 train_time:43274ms step_avg:57.70ms
step:751/2330 train_time:43294ms step_avg:57.65ms
step:752/2330 train_time:43313ms step_avg:57.60ms
step:753/2330 train_time:43371ms step_avg:57.60ms
step:754/2330 train_time:43434ms step_avg:57.61ms
step:755/2330 train_time:43492ms step_avg:57.60ms
step:756/2330 train_time:43553ms step_avg:57.61ms
step:757/2330 train_time:43609ms step_avg:57.61ms
step:758/2330 train_time:43669ms step_avg:57.61ms
step:759/2330 train_time:43725ms step_avg:57.61ms
step:760/2330 train_time:43783ms step_avg:57.61ms
step:761/2330 train_time:43839ms step_avg:57.61ms
step:762/2330 train_time:43897ms step_avg:57.61ms
step:763/2330 train_time:43953ms step_avg:57.61ms
step:764/2330 train_time:44011ms step_avg:57.61ms
step:765/2330 train_time:44069ms step_avg:57.61ms
step:766/2330 train_time:44126ms step_avg:57.61ms
step:767/2330 train_time:44183ms step_avg:57.60ms
step:768/2330 train_time:44242ms step_avg:57.61ms
step:769/2330 train_time:44300ms step_avg:57.61ms
step:770/2330 train_time:44360ms step_avg:57.61ms
step:771/2330 train_time:44418ms step_avg:57.61ms
step:772/2330 train_time:44479ms step_avg:57.61ms
step:773/2330 train_time:44537ms step_avg:57.62ms
step:774/2330 train_time:44597ms step_avg:57.62ms
step:775/2330 train_time:44654ms step_avg:57.62ms
step:776/2330 train_time:44714ms step_avg:57.62ms
step:777/2330 train_time:44771ms step_avg:57.62ms
step:778/2330 train_time:44831ms step_avg:57.62ms
step:779/2330 train_time:44888ms step_avg:57.62ms
step:780/2330 train_time:44947ms step_avg:57.62ms
step:781/2330 train_time:45003ms step_avg:57.62ms
step:782/2330 train_time:45062ms step_avg:57.62ms
step:783/2330 train_time:45119ms step_avg:57.62ms
step:784/2330 train_time:45178ms step_avg:57.62ms
step:785/2330 train_time:45235ms step_avg:57.62ms
step:786/2330 train_time:45294ms step_avg:57.63ms
step:787/2330 train_time:45352ms step_avg:57.63ms
step:788/2330 train_time:45413ms step_avg:57.63ms
step:789/2330 train_time:45471ms step_avg:57.63ms
step:790/2330 train_time:45532ms step_avg:57.64ms
step:791/2330 train_time:45590ms step_avg:57.64ms
step:792/2330 train_time:45650ms step_avg:57.64ms
step:793/2330 train_time:45706ms step_avg:57.64ms
step:794/2330 train_time:45766ms step_avg:57.64ms
step:795/2330 train_time:45823ms step_avg:57.64ms
step:796/2330 train_time:45883ms step_avg:57.64ms
step:797/2330 train_time:45940ms step_avg:57.64ms
step:798/2330 train_time:45999ms step_avg:57.64ms
step:799/2330 train_time:46056ms step_avg:57.64ms
step:800/2330 train_time:46115ms step_avg:57.64ms
step:801/2330 train_time:46172ms step_avg:57.64ms
step:802/2330 train_time:46231ms step_avg:57.64ms
step:803/2330 train_time:46288ms step_avg:57.64ms
step:804/2330 train_time:46348ms step_avg:57.65ms
step:805/2330 train_time:46405ms step_avg:57.65ms
step:806/2330 train_time:46465ms step_avg:57.65ms
step:807/2330 train_time:46524ms step_avg:57.65ms
step:808/2330 train_time:46585ms step_avg:57.65ms
step:809/2330 train_time:46642ms step_avg:57.65ms
step:810/2330 train_time:46703ms step_avg:57.66ms
step:811/2330 train_time:46760ms step_avg:57.66ms
step:812/2330 train_time:46819ms step_avg:57.66ms
step:813/2330 train_time:46876ms step_avg:57.66ms
step:814/2330 train_time:46935ms step_avg:57.66ms
step:815/2330 train_time:46992ms step_avg:57.66ms
step:816/2330 train_time:47051ms step_avg:57.66ms
step:817/2330 train_time:47109ms step_avg:57.66ms
step:818/2330 train_time:47168ms step_avg:57.66ms
step:819/2330 train_time:47226ms step_avg:57.66ms
step:820/2330 train_time:47286ms step_avg:57.67ms
step:821/2330 train_time:47344ms step_avg:57.67ms
step:822/2330 train_time:47403ms step_avg:57.67ms
step:823/2330 train_time:47461ms step_avg:57.67ms
step:824/2330 train_time:47521ms step_avg:57.67ms
step:825/2330 train_time:47579ms step_avg:57.67ms
step:826/2330 train_time:47638ms step_avg:57.67ms
step:827/2330 train_time:47696ms step_avg:57.67ms
step:828/2330 train_time:47756ms step_avg:57.68ms
step:829/2330 train_time:47813ms step_avg:57.68ms
step:830/2330 train_time:47873ms step_avg:57.68ms
step:831/2330 train_time:47929ms step_avg:57.68ms
step:832/2330 train_time:47989ms step_avg:57.68ms
step:833/2330 train_time:48046ms step_avg:57.68ms
step:834/2330 train_time:48106ms step_avg:57.68ms
step:835/2330 train_time:48163ms step_avg:57.68ms
step:836/2330 train_time:48223ms step_avg:57.68ms
step:837/2330 train_time:48280ms step_avg:57.68ms
step:838/2330 train_time:48340ms step_avg:57.68ms
step:839/2330 train_time:48397ms step_avg:57.68ms
step:840/2330 train_time:48456ms step_avg:57.69ms
step:841/2330 train_time:48514ms step_avg:57.69ms
step:842/2330 train_time:48573ms step_avg:57.69ms
step:843/2330 train_time:48631ms step_avg:57.69ms
step:844/2330 train_time:48691ms step_avg:57.69ms
step:845/2330 train_time:48749ms step_avg:57.69ms
step:846/2330 train_time:48809ms step_avg:57.69ms
step:847/2330 train_time:48866ms step_avg:57.69ms
step:848/2330 train_time:48926ms step_avg:57.70ms
step:849/2330 train_time:48983ms step_avg:57.69ms
step:850/2330 train_time:49042ms step_avg:57.70ms
step:851/2330 train_time:49098ms step_avg:57.69ms
step:852/2330 train_time:49158ms step_avg:57.70ms
step:853/2330 train_time:49215ms step_avg:57.70ms
step:854/2330 train_time:49275ms step_avg:57.70ms
step:855/2330 train_time:49331ms step_avg:57.70ms
step:856/2330 train_time:49391ms step_avg:57.70ms
step:857/2330 train_time:49449ms step_avg:57.70ms
step:858/2330 train_time:49508ms step_avg:57.70ms
step:859/2330 train_time:49566ms step_avg:57.70ms
step:860/2330 train_time:49626ms step_avg:57.70ms
step:861/2330 train_time:49684ms step_avg:57.70ms
step:862/2330 train_time:49744ms step_avg:57.71ms
step:863/2330 train_time:49801ms step_avg:57.71ms
step:864/2330 train_time:49861ms step_avg:57.71ms
step:865/2330 train_time:49919ms step_avg:57.71ms
step:866/2330 train_time:49978ms step_avg:57.71ms
step:867/2330 train_time:50035ms step_avg:57.71ms
step:868/2330 train_time:50094ms step_avg:57.71ms
step:869/2330 train_time:50151ms step_avg:57.71ms
step:870/2330 train_time:50211ms step_avg:57.71ms
step:871/2330 train_time:50268ms step_avg:57.71ms
step:872/2330 train_time:50328ms step_avg:57.72ms
step:873/2330 train_time:50385ms step_avg:57.72ms
step:874/2330 train_time:50445ms step_avg:57.72ms
step:875/2330 train_time:50503ms step_avg:57.72ms
step:876/2330 train_time:50562ms step_avg:57.72ms
step:877/2330 train_time:50618ms step_avg:57.72ms
step:878/2330 train_time:50680ms step_avg:57.72ms
step:879/2330 train_time:50736ms step_avg:57.72ms
step:880/2330 train_time:50797ms step_avg:57.72ms
step:881/2330 train_time:50854ms step_avg:57.72ms
step:882/2330 train_time:50914ms step_avg:57.73ms
step:883/2330 train_time:50971ms step_avg:57.72ms
step:884/2330 train_time:51030ms step_avg:57.73ms
step:885/2330 train_time:51088ms step_avg:57.73ms
step:886/2330 train_time:51147ms step_avg:57.73ms
step:887/2330 train_time:51204ms step_avg:57.73ms
step:888/2330 train_time:51263ms step_avg:57.73ms
step:889/2330 train_time:51321ms step_avg:57.73ms
step:890/2330 train_time:51379ms step_avg:57.73ms
step:891/2330 train_time:51436ms step_avg:57.73ms
step:892/2330 train_time:51496ms step_avg:57.73ms
step:893/2330 train_time:51554ms step_avg:57.73ms
step:894/2330 train_time:51613ms step_avg:57.73ms
step:895/2330 train_time:51671ms step_avg:57.73ms
step:896/2330 train_time:51731ms step_avg:57.74ms
step:897/2330 train_time:51788ms step_avg:57.73ms
step:898/2330 train_time:51848ms step_avg:57.74ms
step:899/2330 train_time:51906ms step_avg:57.74ms
step:900/2330 train_time:51965ms step_avg:57.74ms
step:901/2330 train_time:52023ms step_avg:57.74ms
step:902/2330 train_time:52083ms step_avg:57.74ms
step:903/2330 train_time:52140ms step_avg:57.74ms
step:904/2330 train_time:52199ms step_avg:57.74ms
step:905/2330 train_time:52255ms step_avg:57.74ms
step:906/2330 train_time:52316ms step_avg:57.74ms
step:907/2330 train_time:52373ms step_avg:57.74ms
step:908/2330 train_time:52432ms step_avg:57.74ms
step:909/2330 train_time:52489ms step_avg:57.74ms
step:910/2330 train_time:52548ms step_avg:57.75ms
step:911/2330 train_time:52605ms step_avg:57.74ms
step:912/2330 train_time:52666ms step_avg:57.75ms
step:913/2330 train_time:52723ms step_avg:57.75ms
step:914/2330 train_time:52783ms step_avg:57.75ms
step:915/2330 train_time:52839ms step_avg:57.75ms
step:916/2330 train_time:52900ms step_avg:57.75ms
step:917/2330 train_time:52957ms step_avg:57.75ms
step:918/2330 train_time:53017ms step_avg:57.75ms
step:919/2330 train_time:53074ms step_avg:57.75ms
step:920/2330 train_time:53134ms step_avg:57.75ms
step:921/2330 train_time:53190ms step_avg:57.75ms
step:922/2330 train_time:53251ms step_avg:57.76ms
step:923/2330 train_time:53308ms step_avg:57.76ms
step:924/2330 train_time:53368ms step_avg:57.76ms
step:925/2330 train_time:53425ms step_avg:57.76ms
step:926/2330 train_time:53485ms step_avg:57.76ms
step:927/2330 train_time:53542ms step_avg:57.76ms
step:928/2330 train_time:53602ms step_avg:57.76ms
step:929/2330 train_time:53659ms step_avg:57.76ms
step:930/2330 train_time:53719ms step_avg:57.76ms
step:931/2330 train_time:53776ms step_avg:57.76ms
step:932/2330 train_time:53835ms step_avg:57.76ms
step:933/2330 train_time:53892ms step_avg:57.76ms
step:934/2330 train_time:53952ms step_avg:57.76ms
step:935/2330 train_time:54010ms step_avg:57.76ms
step:936/2330 train_time:54070ms step_avg:57.77ms
step:937/2330 train_time:54128ms step_avg:57.77ms
step:938/2330 train_time:54187ms step_avg:57.77ms
step:939/2330 train_time:54244ms step_avg:57.77ms
step:940/2330 train_time:54304ms step_avg:57.77ms
step:941/2330 train_time:54360ms step_avg:57.77ms
step:942/2330 train_time:54420ms step_avg:57.77ms
step:943/2330 train_time:54477ms step_avg:57.77ms
step:944/2330 train_time:54537ms step_avg:57.77ms
step:945/2330 train_time:54593ms step_avg:57.77ms
step:946/2330 train_time:54653ms step_avg:57.77ms
step:947/2330 train_time:54710ms step_avg:57.77ms
step:948/2330 train_time:54770ms step_avg:57.77ms
step:949/2330 train_time:54826ms step_avg:57.77ms
step:950/2330 train_time:54887ms step_avg:57.78ms
step:951/2330 train_time:54945ms step_avg:57.78ms
step:952/2330 train_time:55005ms step_avg:57.78ms
step:953/2330 train_time:55063ms step_avg:57.78ms
step:954/2330 train_time:55123ms step_avg:57.78ms
step:955/2330 train_time:55180ms step_avg:57.78ms
step:956/2330 train_time:55240ms step_avg:57.78ms
step:957/2330 train_time:55296ms step_avg:57.78ms
step:958/2330 train_time:55356ms step_avg:57.78ms
step:959/2330 train_time:55413ms step_avg:57.78ms
step:960/2330 train_time:55473ms step_avg:57.78ms
step:961/2330 train_time:55529ms step_avg:57.78ms
step:962/2330 train_time:55589ms step_avg:57.78ms
step:963/2330 train_time:55647ms step_avg:57.78ms
step:964/2330 train_time:55707ms step_avg:57.79ms
step:965/2330 train_time:55764ms step_avg:57.79ms
step:966/2330 train_time:55823ms step_avg:57.79ms
step:967/2330 train_time:55880ms step_avg:57.79ms
step:968/2330 train_time:55941ms step_avg:57.79ms
step:969/2330 train_time:55998ms step_avg:57.79ms
step:970/2330 train_time:56058ms step_avg:57.79ms
step:971/2330 train_time:56115ms step_avg:57.79ms
step:972/2330 train_time:56175ms step_avg:57.79ms
step:973/2330 train_time:56232ms step_avg:57.79ms
step:974/2330 train_time:56292ms step_avg:57.79ms
step:975/2330 train_time:56349ms step_avg:57.79ms
step:976/2330 train_time:56409ms step_avg:57.80ms
step:977/2330 train_time:56467ms step_avg:57.80ms
step:978/2330 train_time:56527ms step_avg:57.80ms
step:979/2330 train_time:56583ms step_avg:57.80ms
step:980/2330 train_time:56643ms step_avg:57.80ms
step:981/2330 train_time:56700ms step_avg:57.80ms
step:982/2330 train_time:56759ms step_avg:57.80ms
step:983/2330 train_time:56816ms step_avg:57.80ms
step:984/2330 train_time:56877ms step_avg:57.80ms
step:985/2330 train_time:56933ms step_avg:57.80ms
step:986/2330 train_time:56994ms step_avg:57.80ms
step:987/2330 train_time:57050ms step_avg:57.80ms
step:988/2330 train_time:57111ms step_avg:57.80ms
step:989/2330 train_time:57167ms step_avg:57.80ms
step:990/2330 train_time:57228ms step_avg:57.81ms
step:991/2330 train_time:57285ms step_avg:57.81ms
step:992/2330 train_time:57344ms step_avg:57.81ms
step:993/2330 train_time:57401ms step_avg:57.81ms
step:994/2330 train_time:57461ms step_avg:57.81ms
step:995/2330 train_time:57517ms step_avg:57.81ms
step:996/2330 train_time:57578ms step_avg:57.81ms
step:997/2330 train_time:57635ms step_avg:57.81ms
step:998/2330 train_time:57695ms step_avg:57.81ms
step:999/2330 train_time:57752ms step_avg:57.81ms
step:1000/2330 train_time:57812ms step_avg:57.81ms
step:1000/2330 val_loss:4.0723 train_time:57893ms step_avg:57.89ms
step:1001/2330 train_time:57913ms step_avg:57.86ms
step:1002/2330 train_time:57935ms step_avg:57.82ms
step:1003/2330 train_time:57989ms step_avg:57.82ms
step:1004/2330 train_time:58053ms step_avg:57.82ms
step:1005/2330 train_time:58111ms step_avg:57.82ms
step:1006/2330 train_time:58171ms step_avg:57.82ms
step:1007/2330 train_time:58228ms step_avg:57.82ms
step:1008/2330 train_time:58287ms step_avg:57.82ms
step:1009/2330 train_time:58342ms step_avg:57.82ms
step:1010/2330 train_time:58402ms step_avg:57.82ms
step:1011/2330 train_time:58458ms step_avg:57.82ms
step:1012/2330 train_time:58517ms step_avg:57.82ms
step:1013/2330 train_time:58573ms step_avg:57.82ms
step:1014/2330 train_time:58632ms step_avg:57.82ms
step:1015/2330 train_time:58687ms step_avg:57.82ms
step:1016/2330 train_time:58746ms step_avg:57.82ms
step:1017/2330 train_time:58802ms step_avg:57.82ms
step:1018/2330 train_time:58868ms step_avg:57.83ms
step:1019/2330 train_time:58924ms step_avg:57.83ms
step:1020/2330 train_time:58991ms step_avg:57.83ms
step:1021/2330 train_time:59049ms step_avg:57.83ms
step:1022/2330 train_time:59108ms step_avg:57.84ms
step:1023/2330 train_time:59165ms step_avg:57.83ms
step:1024/2330 train_time:59225ms step_avg:57.84ms
step:1025/2330 train_time:59282ms step_avg:57.84ms
step:1026/2330 train_time:59341ms step_avg:57.84ms
step:1027/2330 train_time:59398ms step_avg:57.84ms
step:1028/2330 train_time:59457ms step_avg:57.84ms
step:1029/2330 train_time:59513ms step_avg:57.84ms
step:1030/2330 train_time:59572ms step_avg:57.84ms
step:1031/2330 train_time:59629ms step_avg:57.84ms
step:1032/2330 train_time:59687ms step_avg:57.84ms
step:1033/2330 train_time:59743ms step_avg:57.83ms
step:1034/2330 train_time:59805ms step_avg:57.84ms
step:1035/2330 train_time:59862ms step_avg:57.84ms
step:1036/2330 train_time:59926ms step_avg:57.84ms
step:1037/2330 train_time:59982ms step_avg:57.84ms
step:1038/2330 train_time:60047ms step_avg:57.85ms
step:1039/2330 train_time:60104ms step_avg:57.85ms
step:1040/2330 train_time:60165ms step_avg:57.85ms
step:1041/2330 train_time:60222ms step_avg:57.85ms
step:1042/2330 train_time:60281ms step_avg:57.85ms
step:1043/2330 train_time:60338ms step_avg:57.85ms
step:1044/2330 train_time:60398ms step_avg:57.85ms
step:1045/2330 train_time:60455ms step_avg:57.85ms
step:1046/2330 train_time:60514ms step_avg:57.85ms
step:1047/2330 train_time:60570ms step_avg:57.85ms
step:1048/2330 train_time:60630ms step_avg:57.85ms
step:1049/2330 train_time:60686ms step_avg:57.85ms
step:1050/2330 train_time:60745ms step_avg:57.85ms
step:1051/2330 train_time:60803ms step_avg:57.85ms
step:1052/2330 train_time:60863ms step_avg:57.85ms
step:1053/2330 train_time:60921ms step_avg:57.85ms
step:1054/2330 train_time:60983ms step_avg:57.86ms
step:1055/2330 train_time:61040ms step_avg:57.86ms
step:1056/2330 train_time:61102ms step_avg:57.86ms
step:1057/2330 train_time:61159ms step_avg:57.86ms
step:1058/2330 train_time:61220ms step_avg:57.86ms
step:1059/2330 train_time:61276ms step_avg:57.86ms
step:1060/2330 train_time:61337ms step_avg:57.86ms
step:1061/2330 train_time:61393ms step_avg:57.86ms
step:1062/2330 train_time:61453ms step_avg:57.87ms
step:1063/2330 train_time:61509ms step_avg:57.86ms
step:1064/2330 train_time:61569ms step_avg:57.87ms
step:1065/2330 train_time:61625ms step_avg:57.86ms
step:1066/2330 train_time:61684ms step_avg:57.87ms
step:1067/2330 train_time:61741ms step_avg:57.86ms
step:1068/2330 train_time:61802ms step_avg:57.87ms
step:1069/2330 train_time:61859ms step_avg:57.87ms
step:1070/2330 train_time:61920ms step_avg:57.87ms
step:1071/2330 train_time:61977ms step_avg:57.87ms
step:1072/2330 train_time:62039ms step_avg:57.87ms
step:1073/2330 train_time:62095ms step_avg:57.87ms
step:1074/2330 train_time:62156ms step_avg:57.87ms
step:1075/2330 train_time:62213ms step_avg:57.87ms
step:1076/2330 train_time:62273ms step_avg:57.87ms
step:1077/2330 train_time:62329ms step_avg:57.87ms
step:1078/2330 train_time:62389ms step_avg:57.87ms
step:1079/2330 train_time:62445ms step_avg:57.87ms
step:1080/2330 train_time:62506ms step_avg:57.88ms
step:1081/2330 train_time:62562ms step_avg:57.87ms
step:1082/2330 train_time:62622ms step_avg:57.88ms
step:1083/2330 train_time:62679ms step_avg:57.87ms
step:1084/2330 train_time:62738ms step_avg:57.88ms
step:1085/2330 train_time:62796ms step_avg:57.88ms
step:1086/2330 train_time:62855ms step_avg:57.88ms
step:1087/2330 train_time:62912ms step_avg:57.88ms
step:1088/2330 train_time:62972ms step_avg:57.88ms
step:1089/2330 train_time:63029ms step_avg:57.88ms
step:1090/2330 train_time:63090ms step_avg:57.88ms
step:1091/2330 train_time:63147ms step_avg:57.88ms
step:1092/2330 train_time:63207ms step_avg:57.88ms
step:1093/2330 train_time:63264ms step_avg:57.88ms
step:1094/2330 train_time:63325ms step_avg:57.88ms
step:1095/2330 train_time:63381ms step_avg:57.88ms
step:1096/2330 train_time:63443ms step_avg:57.89ms
step:1097/2330 train_time:63500ms step_avg:57.89ms
step:1098/2330 train_time:63560ms step_avg:57.89ms
step:1099/2330 train_time:63617ms step_avg:57.89ms
step:1100/2330 train_time:63677ms step_avg:57.89ms
step:1101/2330 train_time:63734ms step_avg:57.89ms
step:1102/2330 train_time:63793ms step_avg:57.89ms
step:1103/2330 train_time:63850ms step_avg:57.89ms
step:1104/2330 train_time:63910ms step_avg:57.89ms
step:1105/2330 train_time:63967ms step_avg:57.89ms
step:1106/2330 train_time:64028ms step_avg:57.89ms
step:1107/2330 train_time:64084ms step_avg:57.89ms
step:1108/2330 train_time:64146ms step_avg:57.89ms
step:1109/2330 train_time:64203ms step_avg:57.89ms
step:1110/2330 train_time:64264ms step_avg:57.90ms
step:1111/2330 train_time:64321ms step_avg:57.89ms
step:1112/2330 train_time:64380ms step_avg:57.90ms
step:1113/2330 train_time:64436ms step_avg:57.89ms
step:1114/2330 train_time:64497ms step_avg:57.90ms
step:1115/2330 train_time:64554ms step_avg:57.90ms
step:1116/2330 train_time:64614ms step_avg:57.90ms
step:1117/2330 train_time:64671ms step_avg:57.90ms
step:1118/2330 train_time:64730ms step_avg:57.90ms
step:1119/2330 train_time:64787ms step_avg:57.90ms
step:1120/2330 train_time:64848ms step_avg:57.90ms
step:1121/2330 train_time:64905ms step_avg:57.90ms
step:1122/2330 train_time:64966ms step_avg:57.90ms
step:1123/2330 train_time:65023ms step_avg:57.90ms
step:1124/2330 train_time:65083ms step_avg:57.90ms
step:1125/2330 train_time:65140ms step_avg:57.90ms
step:1126/2330 train_time:65201ms step_avg:57.90ms
step:1127/2330 train_time:65258ms step_avg:57.90ms
step:1128/2330 train_time:65318ms step_avg:57.91ms
step:1129/2330 train_time:65376ms step_avg:57.91ms
step:1130/2330 train_time:65435ms step_avg:57.91ms
step:1131/2330 train_time:65491ms step_avg:57.91ms
step:1132/2330 train_time:65551ms step_avg:57.91ms
step:1133/2330 train_time:65608ms step_avg:57.91ms
step:1134/2330 train_time:65668ms step_avg:57.91ms
step:1135/2330 train_time:65724ms step_avg:57.91ms
step:1136/2330 train_time:65784ms step_avg:57.91ms
step:1137/2330 train_time:65840ms step_avg:57.91ms
step:1138/2330 train_time:65902ms step_avg:57.91ms
step:1139/2330 train_time:65959ms step_avg:57.91ms
step:1140/2330 train_time:66019ms step_avg:57.91ms
step:1141/2330 train_time:66077ms step_avg:57.91ms
step:1142/2330 train_time:66137ms step_avg:57.91ms
step:1143/2330 train_time:66194ms step_avg:57.91ms
step:1144/2330 train_time:66254ms step_avg:57.91ms
step:1145/2330 train_time:66311ms step_avg:57.91ms
step:1146/2330 train_time:66371ms step_avg:57.92ms
step:1147/2330 train_time:66427ms step_avg:57.91ms
step:1148/2330 train_time:66488ms step_avg:57.92ms
step:1149/2330 train_time:66545ms step_avg:57.92ms
step:1150/2330 train_time:66606ms step_avg:57.92ms
step:1151/2330 train_time:66662ms step_avg:57.92ms
step:1152/2330 train_time:66722ms step_avg:57.92ms
step:1153/2330 train_time:66778ms step_avg:57.92ms
step:1154/2330 train_time:66839ms step_avg:57.92ms
step:1155/2330 train_time:66896ms step_avg:57.92ms
step:1156/2330 train_time:66956ms step_avg:57.92ms
step:1157/2330 train_time:67013ms step_avg:57.92ms
step:1158/2330 train_time:67073ms step_avg:57.92ms
step:1159/2330 train_time:67130ms step_avg:57.92ms
step:1160/2330 train_time:67189ms step_avg:57.92ms
step:1161/2330 train_time:67247ms step_avg:57.92ms
step:1162/2330 train_time:67307ms step_avg:57.92ms
step:1163/2330 train_time:67363ms step_avg:57.92ms
step:1164/2330 train_time:67424ms step_avg:57.92ms
step:1165/2330 train_time:67480ms step_avg:57.92ms
step:1166/2330 train_time:67541ms step_avg:57.93ms
step:1167/2330 train_time:67597ms step_avg:57.92ms
step:1168/2330 train_time:67657ms step_avg:57.93ms
step:1169/2330 train_time:67714ms step_avg:57.92ms
step:1170/2330 train_time:67774ms step_avg:57.93ms
step:1171/2330 train_time:67830ms step_avg:57.92ms
step:1172/2330 train_time:67890ms step_avg:57.93ms
step:1173/2330 train_time:67946ms step_avg:57.93ms
step:1174/2330 train_time:68006ms step_avg:57.93ms
step:1175/2330 train_time:68064ms step_avg:57.93ms
step:1176/2330 train_time:68124ms step_avg:57.93ms
step:1177/2330 train_time:68181ms step_avg:57.93ms
step:1178/2330 train_time:68241ms step_avg:57.93ms
step:1179/2330 train_time:68298ms step_avg:57.93ms
step:1180/2330 train_time:68358ms step_avg:57.93ms
step:1181/2330 train_time:68415ms step_avg:57.93ms
step:1182/2330 train_time:68475ms step_avg:57.93ms
step:1183/2330 train_time:68532ms step_avg:57.93ms
step:1184/2330 train_time:68593ms step_avg:57.93ms
step:1185/2330 train_time:68649ms step_avg:57.93ms
step:1186/2330 train_time:68709ms step_avg:57.93ms
step:1187/2330 train_time:68766ms step_avg:57.93ms
step:1188/2330 train_time:68826ms step_avg:57.93ms
step:1189/2330 train_time:68882ms step_avg:57.93ms
step:1190/2330 train_time:68943ms step_avg:57.94ms
step:1191/2330 train_time:68999ms step_avg:57.93ms
step:1192/2330 train_time:69060ms step_avg:57.94ms
step:1193/2330 train_time:69118ms step_avg:57.94ms
step:1194/2330 train_time:69178ms step_avg:57.94ms
step:1195/2330 train_time:69234ms step_avg:57.94ms
step:1196/2330 train_time:69294ms step_avg:57.94ms
step:1197/2330 train_time:69351ms step_avg:57.94ms
step:1198/2330 train_time:69412ms step_avg:57.94ms
step:1199/2330 train_time:69469ms step_avg:57.94ms
step:1200/2330 train_time:69529ms step_avg:57.94ms
step:1201/2330 train_time:69585ms step_avg:57.94ms
step:1202/2330 train_time:69646ms step_avg:57.94ms
step:1203/2330 train_time:69703ms step_avg:57.94ms
step:1204/2330 train_time:69762ms step_avg:57.94ms
step:1205/2330 train_time:69819ms step_avg:57.94ms
step:1206/2330 train_time:69880ms step_avg:57.94ms
step:1207/2330 train_time:69936ms step_avg:57.94ms
step:1208/2330 train_time:69996ms step_avg:57.94ms
step:1209/2330 train_time:70052ms step_avg:57.94ms
step:1210/2330 train_time:70113ms step_avg:57.94ms
step:1211/2330 train_time:70170ms step_avg:57.94ms
step:1212/2330 train_time:70230ms step_avg:57.95ms
step:1213/2330 train_time:70286ms step_avg:57.94ms
step:1214/2330 train_time:70347ms step_avg:57.95ms
step:1215/2330 train_time:70403ms step_avg:57.94ms
step:1216/2330 train_time:70463ms step_avg:57.95ms
step:1217/2330 train_time:70520ms step_avg:57.95ms
step:1218/2330 train_time:70581ms step_avg:57.95ms
step:1219/2330 train_time:70638ms step_avg:57.95ms
step:1220/2330 train_time:70698ms step_avg:57.95ms
step:1221/2330 train_time:70755ms step_avg:57.95ms
step:1222/2330 train_time:70814ms step_avg:57.95ms
step:1223/2330 train_time:70870ms step_avg:57.95ms
step:1224/2330 train_time:70930ms step_avg:57.95ms
step:1225/2330 train_time:70986ms step_avg:57.95ms
step:1226/2330 train_time:71048ms step_avg:57.95ms
step:1227/2330 train_time:71104ms step_avg:57.95ms
step:1228/2330 train_time:71165ms step_avg:57.95ms
step:1229/2330 train_time:71222ms step_avg:57.95ms
step:1230/2330 train_time:71282ms step_avg:57.95ms
step:1231/2330 train_time:71338ms step_avg:57.95ms
step:1232/2330 train_time:71400ms step_avg:57.95ms
step:1233/2330 train_time:71457ms step_avg:57.95ms
step:1234/2330 train_time:71517ms step_avg:57.96ms
step:1235/2330 train_time:71574ms step_avg:57.95ms
step:1236/2330 train_time:71633ms step_avg:57.96ms
step:1237/2330 train_time:71691ms step_avg:57.96ms
step:1238/2330 train_time:71750ms step_avg:57.96ms
step:1239/2330 train_time:71807ms step_avg:57.96ms
step:1240/2330 train_time:71866ms step_avg:57.96ms
step:1241/2330 train_time:71923ms step_avg:57.96ms
step:1242/2330 train_time:71983ms step_avg:57.96ms
step:1243/2330 train_time:72040ms step_avg:57.96ms
step:1244/2330 train_time:72099ms step_avg:57.96ms
step:1245/2330 train_time:72156ms step_avg:57.96ms
step:1246/2330 train_time:72216ms step_avg:57.96ms
step:1247/2330 train_time:72272ms step_avg:57.96ms
step:1248/2330 train_time:72332ms step_avg:57.96ms
step:1249/2330 train_time:72389ms step_avg:57.96ms
step:1250/2330 train_time:72449ms step_avg:57.96ms
step:1250/2330 val_loss:3.9892 train_time:72530ms step_avg:58.02ms
step:1251/2330 train_time:72550ms step_avg:57.99ms
step:1252/2330 train_time:72571ms step_avg:57.96ms
step:1253/2330 train_time:72629ms step_avg:57.96ms
step:1254/2330 train_time:72694ms step_avg:57.97ms
step:1255/2330 train_time:72751ms step_avg:57.97ms
step:1256/2330 train_time:72813ms step_avg:57.97ms
step:1257/2330 train_time:72869ms step_avg:57.97ms
step:1258/2330 train_time:72928ms step_avg:57.97ms
step:1259/2330 train_time:72985ms step_avg:57.97ms
step:1260/2330 train_time:73044ms step_avg:57.97ms
step:1261/2330 train_time:73100ms step_avg:57.97ms
step:1262/2330 train_time:73159ms step_avg:57.97ms
step:1263/2330 train_time:73215ms step_avg:57.97ms
step:1264/2330 train_time:73274ms step_avg:57.97ms
step:1265/2330 train_time:73330ms step_avg:57.97ms
step:1266/2330 train_time:73390ms step_avg:57.97ms
step:1267/2330 train_time:73446ms step_avg:57.97ms
step:1268/2330 train_time:73507ms step_avg:57.97ms
step:1269/2330 train_time:73565ms step_avg:57.97ms
step:1270/2330 train_time:73628ms step_avg:57.97ms
step:1271/2330 train_time:73686ms step_avg:57.97ms
step:1272/2330 train_time:73747ms step_avg:57.98ms
step:1273/2330 train_time:73805ms step_avg:57.98ms
step:1274/2330 train_time:73865ms step_avg:57.98ms
step:1275/2330 train_time:73921ms step_avg:57.98ms
step:1276/2330 train_time:73982ms step_avg:57.98ms
step:1277/2330 train_time:74038ms step_avg:57.98ms
step:1278/2330 train_time:74098ms step_avg:57.98ms
step:1279/2330 train_time:74153ms step_avg:57.98ms
step:1280/2330 train_time:74213ms step_avg:57.98ms
step:1281/2330 train_time:74270ms step_avg:57.98ms
step:1282/2330 train_time:74330ms step_avg:57.98ms
step:1283/2330 train_time:74386ms step_avg:57.98ms
step:1284/2330 train_time:74446ms step_avg:57.98ms
step:1285/2330 train_time:74503ms step_avg:57.98ms
step:1286/2330 train_time:74563ms step_avg:57.98ms
step:1287/2330 train_time:74620ms step_avg:57.98ms
step:1288/2330 train_time:74683ms step_avg:57.98ms
step:1289/2330 train_time:74740ms step_avg:57.98ms
step:1290/2330 train_time:74801ms step_avg:57.99ms
step:1291/2330 train_time:74858ms step_avg:57.98ms
step:1292/2330 train_time:75293ms step_avg:58.28ms
step:1293/2330 train_time:75349ms step_avg:58.27ms
step:1294/2330 train_time:75408ms step_avg:58.28ms
step:1295/2330 train_time:75464ms step_avg:58.27ms
step:1296/2330 train_time:75523ms step_avg:58.27ms
step:1297/2330 train_time:75579ms step_avg:58.27ms
step:1298/2330 train_time:75638ms step_avg:58.27ms
step:1299/2330 train_time:75694ms step_avg:58.27ms
step:1300/2330 train_time:75753ms step_avg:58.27ms
step:1301/2330 train_time:75810ms step_avg:58.27ms
step:1302/2330 train_time:75869ms step_avg:58.27ms
step:1303/2330 train_time:75925ms step_avg:58.27ms
step:1304/2330 train_time:75984ms step_avg:58.27ms
step:1305/2330 train_time:76040ms step_avg:58.27ms
step:1306/2330 train_time:76100ms step_avg:58.27ms
step:1307/2330 train_time:76161ms step_avg:58.27ms
step:1308/2330 train_time:76227ms step_avg:58.28ms
step:1309/2330 train_time:76284ms step_avg:58.28ms
step:1310/2330 train_time:76347ms step_avg:58.28ms
step:1311/2330 train_time:76404ms step_avg:58.28ms
step:1312/2330 train_time:76464ms step_avg:58.28ms
step:1313/2330 train_time:76520ms step_avg:58.28ms
step:1314/2330 train_time:76580ms step_avg:58.28ms
step:1315/2330 train_time:76636ms step_avg:58.28ms
step:1316/2330 train_time:76696ms step_avg:58.28ms
step:1317/2330 train_time:76751ms step_avg:58.28ms
step:1318/2330 train_time:76811ms step_avg:58.28ms
step:1319/2330 train_time:76867ms step_avg:58.28ms
step:1320/2330 train_time:76927ms step_avg:58.28ms
step:1321/2330 train_time:76983ms step_avg:58.28ms
step:1322/2330 train_time:77042ms step_avg:58.28ms
step:1323/2330 train_time:77100ms step_avg:58.28ms
step:1324/2330 train_time:77163ms step_avg:58.28ms
step:1325/2330 train_time:77219ms step_avg:58.28ms
step:1326/2330 train_time:77282ms step_avg:58.28ms
step:1327/2330 train_time:77340ms step_avg:58.28ms
step:1328/2330 train_time:77400ms step_avg:58.28ms
step:1329/2330 train_time:77457ms step_avg:58.28ms
step:1330/2330 train_time:77516ms step_avg:58.28ms
step:1331/2330 train_time:77573ms step_avg:58.28ms
step:1332/2330 train_time:77633ms step_avg:58.28ms
step:1333/2330 train_time:77689ms step_avg:58.28ms
step:1334/2330 train_time:77749ms step_avg:58.28ms
step:1335/2330 train_time:77805ms step_avg:58.28ms
step:1336/2330 train_time:77864ms step_avg:58.28ms
step:1337/2330 train_time:77921ms step_avg:58.28ms
step:1338/2330 train_time:77980ms step_avg:58.28ms
step:1339/2330 train_time:78037ms step_avg:58.28ms
step:1340/2330 train_time:78098ms step_avg:58.28ms
step:1341/2330 train_time:78155ms step_avg:58.28ms
step:1342/2330 train_time:78216ms step_avg:58.28ms
step:1343/2330 train_time:78275ms step_avg:58.28ms
step:1344/2330 train_time:78334ms step_avg:58.28ms
step:1345/2330 train_time:78392ms step_avg:58.28ms
step:1346/2330 train_time:78452ms step_avg:58.29ms
step:1347/2330 train_time:78509ms step_avg:58.28ms
step:1348/2330 train_time:78570ms step_avg:58.29ms
step:1349/2330 train_time:78626ms step_avg:58.28ms
step:1350/2330 train_time:78686ms step_avg:58.29ms
step:1351/2330 train_time:78742ms step_avg:58.28ms
step:1352/2330 train_time:78802ms step_avg:58.29ms
step:1353/2330 train_time:78858ms step_avg:58.28ms
step:1354/2330 train_time:78918ms step_avg:58.28ms
step:1355/2330 train_time:78975ms step_avg:58.28ms
step:1356/2330 train_time:79035ms step_avg:58.29ms
step:1357/2330 train_time:79092ms step_avg:58.28ms
step:1358/2330 train_time:79152ms step_avg:58.29ms
step:1359/2330 train_time:79211ms step_avg:58.29ms
step:1360/2330 train_time:79272ms step_avg:58.29ms
step:1361/2330 train_time:79329ms step_avg:58.29ms
step:1362/2330 train_time:79390ms step_avg:58.29ms
step:1363/2330 train_time:79448ms step_avg:58.29ms
step:1364/2330 train_time:79508ms step_avg:58.29ms
step:1365/2330 train_time:79565ms step_avg:58.29ms
step:1366/2330 train_time:79625ms step_avg:58.29ms
step:1367/2330 train_time:79681ms step_avg:58.29ms
step:1368/2330 train_time:79742ms step_avg:58.29ms
step:1369/2330 train_time:79798ms step_avg:58.29ms
step:1370/2330 train_time:79857ms step_avg:58.29ms
step:1371/2330 train_time:79914ms step_avg:58.29ms
step:1372/2330 train_time:79974ms step_avg:58.29ms
step:1373/2330 train_time:80030ms step_avg:58.29ms
step:1374/2330 train_time:80090ms step_avg:58.29ms
step:1375/2330 train_time:80147ms step_avg:58.29ms
step:1376/2330 train_time:80208ms step_avg:58.29ms
step:1377/2330 train_time:80265ms step_avg:58.29ms
step:1378/2330 train_time:80326ms step_avg:58.29ms
step:1379/2330 train_time:80383ms step_avg:58.29ms
step:1380/2330 train_time:80444ms step_avg:58.29ms
step:1381/2330 train_time:80502ms step_avg:58.29ms
step:1382/2330 train_time:80561ms step_avg:58.29ms
step:1383/2330 train_time:80618ms step_avg:58.29ms
step:1384/2330 train_time:80677ms step_avg:58.29ms
step:1385/2330 train_time:80734ms step_avg:58.29ms
step:1386/2330 train_time:80795ms step_avg:58.29ms
step:1387/2330 train_time:80851ms step_avg:58.29ms
step:1388/2330 train_time:80911ms step_avg:58.29ms
step:1389/2330 train_time:80968ms step_avg:58.29ms
step:1390/2330 train_time:81028ms step_avg:58.29ms
step:1391/2330 train_time:81085ms step_avg:58.29ms
step:1392/2330 train_time:81145ms step_avg:58.29ms
step:1393/2330 train_time:81201ms step_avg:58.29ms
step:1394/2330 train_time:81263ms step_avg:58.29ms
step:1395/2330 train_time:81319ms step_avg:58.29ms
step:1396/2330 train_time:81379ms step_avg:58.29ms
step:1397/2330 train_time:81437ms step_avg:58.29ms
step:1398/2330 train_time:81497ms step_avg:58.30ms
step:1399/2330 train_time:81554ms step_avg:58.29ms
step:1400/2330 train_time:81614ms step_avg:58.30ms
step:1401/2330 train_time:81671ms step_avg:58.30ms
step:1402/2330 train_time:81731ms step_avg:58.30ms
step:1403/2330 train_time:81789ms step_avg:58.30ms
step:1404/2330 train_time:81849ms step_avg:58.30ms
step:1405/2330 train_time:81906ms step_avg:58.30ms
step:1406/2330 train_time:81966ms step_avg:58.30ms
step:1407/2330 train_time:82023ms step_avg:58.30ms
step:1408/2330 train_time:82082ms step_avg:58.30ms
step:1409/2330 train_time:82138ms step_avg:58.30ms
step:1410/2330 train_time:82199ms step_avg:58.30ms
step:1411/2330 train_time:82256ms step_avg:58.30ms
step:1412/2330 train_time:82317ms step_avg:58.30ms
step:1413/2330 train_time:82374ms step_avg:58.30ms
step:1414/2330 train_time:82434ms step_avg:58.30ms
step:1415/2330 train_time:82491ms step_avg:58.30ms
step:1416/2330 train_time:82552ms step_avg:58.30ms
step:1417/2330 train_time:82609ms step_avg:58.30ms
step:1418/2330 train_time:82669ms step_avg:58.30ms
step:1419/2330 train_time:82726ms step_avg:58.30ms
step:1420/2330 train_time:82785ms step_avg:58.30ms
step:1421/2330 train_time:82842ms step_avg:58.30ms
step:1422/2330 train_time:82902ms step_avg:58.30ms
step:1423/2330 train_time:82959ms step_avg:58.30ms
step:1424/2330 train_time:83018ms step_avg:58.30ms
step:1425/2330 train_time:83074ms step_avg:58.30ms
step:1426/2330 train_time:83136ms step_avg:58.30ms
step:1427/2330 train_time:83193ms step_avg:58.30ms
step:1428/2330 train_time:83252ms step_avg:58.30ms
step:1429/2330 train_time:83310ms step_avg:58.30ms
step:1430/2330 train_time:83370ms step_avg:58.30ms
step:1431/2330 train_time:83427ms step_avg:58.30ms
step:1432/2330 train_time:83487ms step_avg:58.30ms
step:1433/2330 train_time:83545ms step_avg:58.30ms
step:1434/2330 train_time:83604ms step_avg:58.30ms
step:1435/2330 train_time:83661ms step_avg:58.30ms
step:1436/2330 train_time:83721ms step_avg:58.30ms
step:1437/2330 train_time:83778ms step_avg:58.30ms
step:1438/2330 train_time:83838ms step_avg:58.30ms
step:1439/2330 train_time:83895ms step_avg:58.30ms
step:1440/2330 train_time:83955ms step_avg:58.30ms
step:1441/2330 train_time:84013ms step_avg:58.30ms
step:1442/2330 train_time:84073ms step_avg:58.30ms
step:1443/2330 train_time:84130ms step_avg:58.30ms
step:1444/2330 train_time:84190ms step_avg:58.30ms
step:1445/2330 train_time:84247ms step_avg:58.30ms
step:1446/2330 train_time:84307ms step_avg:58.30ms
step:1447/2330 train_time:84364ms step_avg:58.30ms
step:1448/2330 train_time:84424ms step_avg:58.30ms
step:1449/2330 train_time:84481ms step_avg:58.30ms
step:1450/2330 train_time:84542ms step_avg:58.30ms
step:1451/2330 train_time:84599ms step_avg:58.30ms
step:1452/2330 train_time:84659ms step_avg:58.30ms
step:1453/2330 train_time:84715ms step_avg:58.30ms
step:1454/2330 train_time:84776ms step_avg:58.31ms
step:1455/2330 train_time:84833ms step_avg:58.30ms
step:1456/2330 train_time:84893ms step_avg:58.31ms
step:1457/2330 train_time:84950ms step_avg:58.30ms
step:1458/2330 train_time:85009ms step_avg:58.31ms
step:1459/2330 train_time:85066ms step_avg:58.30ms
step:1460/2330 train_time:85126ms step_avg:58.31ms
step:1461/2330 train_time:85183ms step_avg:58.30ms
step:1462/2330 train_time:85244ms step_avg:58.31ms
step:1463/2330 train_time:85301ms step_avg:58.31ms
step:1464/2330 train_time:85361ms step_avg:58.31ms
step:1465/2330 train_time:85418ms step_avg:58.31ms
step:1466/2330 train_time:85478ms step_avg:58.31ms
step:1467/2330 train_time:85535ms step_avg:58.31ms
step:1468/2330 train_time:85595ms step_avg:58.31ms
step:1469/2330 train_time:85652ms step_avg:58.31ms
step:1470/2330 train_time:85711ms step_avg:58.31ms
step:1471/2330 train_time:85768ms step_avg:58.31ms
step:1472/2330 train_time:85828ms step_avg:58.31ms
step:1473/2330 train_time:85886ms step_avg:58.31ms
step:1474/2330 train_time:85945ms step_avg:58.31ms
step:1475/2330 train_time:86002ms step_avg:58.31ms
step:1476/2330 train_time:86062ms step_avg:58.31ms
step:1477/2330 train_time:86119ms step_avg:58.31ms
step:1478/2330 train_time:86180ms step_avg:58.31ms
step:1479/2330 train_time:86237ms step_avg:58.31ms
step:1480/2330 train_time:86297ms step_avg:58.31ms
step:1481/2330 train_time:86354ms step_avg:58.31ms
step:1482/2330 train_time:86414ms step_avg:58.31ms
step:1483/2330 train_time:86471ms step_avg:58.31ms
step:1484/2330 train_time:86530ms step_avg:58.31ms
step:1485/2330 train_time:86588ms step_avg:58.31ms
step:1486/2330 train_time:86648ms step_avg:58.31ms
step:1487/2330 train_time:86705ms step_avg:58.31ms
step:1488/2330 train_time:86765ms step_avg:58.31ms
step:1489/2330 train_time:86821ms step_avg:58.31ms
step:1490/2330 train_time:86881ms step_avg:58.31ms
step:1491/2330 train_time:86938ms step_avg:58.31ms
step:1492/2330 train_time:86998ms step_avg:58.31ms
step:1493/2330 train_time:87054ms step_avg:58.31ms
step:1494/2330 train_time:87115ms step_avg:58.31ms
step:1495/2330 train_time:87172ms step_avg:58.31ms
step:1496/2330 train_time:87232ms step_avg:58.31ms
step:1497/2330 train_time:87290ms step_avg:58.31ms
step:1498/2330 train_time:87350ms step_avg:58.31ms
step:1499/2330 train_time:87409ms step_avg:58.31ms
step:1500/2330 train_time:87468ms step_avg:58.31ms
step:1500/2330 val_loss:3.9044 train_time:87548ms step_avg:58.37ms
step:1501/2330 train_time:87568ms step_avg:58.34ms
step:1502/2330 train_time:87587ms step_avg:58.31ms
step:1503/2330 train_time:87647ms step_avg:58.31ms
step:1504/2330 train_time:87710ms step_avg:58.32ms
step:1505/2330 train_time:87766ms step_avg:58.32ms
step:1506/2330 train_time:87829ms step_avg:58.32ms
step:1507/2330 train_time:87886ms step_avg:58.32ms
step:1508/2330 train_time:87945ms step_avg:58.32ms
step:1509/2330 train_time:88001ms step_avg:58.32ms
step:1510/2330 train_time:88060ms step_avg:58.32ms
step:1511/2330 train_time:88116ms step_avg:58.32ms
step:1512/2330 train_time:88176ms step_avg:58.32ms
step:1513/2330 train_time:88232ms step_avg:58.32ms
step:1514/2330 train_time:88291ms step_avg:58.32ms
step:1515/2330 train_time:88347ms step_avg:58.31ms
step:1516/2330 train_time:88407ms step_avg:58.32ms
step:1517/2330 train_time:88464ms step_avg:58.31ms
step:1518/2330 train_time:88526ms step_avg:58.32ms
step:1519/2330 train_time:88585ms step_avg:58.32ms
step:1520/2330 train_time:88646ms step_avg:58.32ms
step:1521/2330 train_time:88703ms step_avg:58.32ms
step:1522/2330 train_time:88764ms step_avg:58.32ms
step:1523/2330 train_time:88822ms step_avg:58.32ms
step:1524/2330 train_time:88882ms step_avg:58.32ms
step:1525/2330 train_time:88939ms step_avg:58.32ms
step:1526/2330 train_time:88997ms step_avg:58.32ms
step:1527/2330 train_time:89053ms step_avg:58.32ms
step:1528/2330 train_time:89113ms step_avg:58.32ms
step:1529/2330 train_time:89171ms step_avg:58.32ms
step:1530/2330 train_time:89230ms step_avg:58.32ms
step:1531/2330 train_time:89286ms step_avg:58.32ms
step:1532/2330 train_time:89346ms step_avg:58.32ms
step:1533/2330 train_time:89403ms step_avg:58.32ms
step:1534/2330 train_time:89464ms step_avg:58.32ms
step:1535/2330 train_time:89522ms step_avg:58.32ms
step:1536/2330 train_time:89584ms step_avg:58.32ms
step:1537/2330 train_time:89642ms step_avg:58.32ms
step:1538/2330 train_time:89704ms step_avg:58.32ms
step:1539/2330 train_time:89762ms step_avg:58.32ms
step:1540/2330 train_time:89823ms step_avg:58.33ms
step:1541/2330 train_time:89881ms step_avg:58.33ms
step:1542/2330 train_time:89941ms step_avg:58.33ms
step:1543/2330 train_time:89999ms step_avg:58.33ms
step:1544/2330 train_time:90059ms step_avg:58.33ms
step:1545/2330 train_time:90116ms step_avg:58.33ms
step:1546/2330 train_time:90176ms step_avg:58.33ms
step:1547/2330 train_time:90233ms step_avg:58.33ms
step:1548/2330 train_time:90293ms step_avg:58.33ms
step:1549/2330 train_time:90350ms step_avg:58.33ms
step:1550/2330 train_time:90411ms step_avg:58.33ms
step:1551/2330 train_time:90468ms step_avg:58.33ms
step:1552/2330 train_time:90530ms step_avg:58.33ms
step:1553/2330 train_time:90588ms step_avg:58.33ms
step:1554/2330 train_time:90648ms step_avg:58.33ms
step:1555/2330 train_time:90706ms step_avg:58.33ms
step:1556/2330 train_time:90767ms step_avg:58.33ms
step:1557/2330 train_time:90825ms step_avg:58.33ms
step:1558/2330 train_time:90887ms step_avg:58.34ms
step:1559/2330 train_time:90944ms step_avg:58.33ms
step:1560/2330 train_time:91005ms step_avg:58.34ms
step:1561/2330 train_time:91063ms step_avg:58.34ms
step:1562/2330 train_time:91124ms step_avg:58.34ms
step:1563/2330 train_time:91183ms step_avg:58.34ms
step:1564/2330 train_time:91243ms step_avg:58.34ms
step:1565/2330 train_time:91300ms step_avg:58.34ms
step:1566/2330 train_time:91360ms step_avg:58.34ms
step:1567/2330 train_time:91417ms step_avg:58.34ms
step:1568/2330 train_time:91478ms step_avg:58.34ms
step:1569/2330 train_time:91535ms step_avg:58.34ms
step:1570/2330 train_time:91596ms step_avg:58.34ms
step:1571/2330 train_time:91653ms step_avg:58.34ms
step:1572/2330 train_time:91715ms step_avg:58.34ms
step:1573/2330 train_time:91772ms step_avg:58.34ms
step:1574/2330 train_time:91834ms step_avg:58.34ms
step:1575/2330 train_time:91890ms step_avg:58.34ms
step:1576/2330 train_time:91953ms step_avg:58.35ms
step:1577/2330 train_time:92010ms step_avg:58.35ms
step:1578/2330 train_time:92071ms step_avg:58.35ms
step:1579/2330 train_time:92128ms step_avg:58.35ms
step:1580/2330 train_time:92189ms step_avg:58.35ms
step:1581/2330 train_time:92246ms step_avg:58.35ms
step:1582/2330 train_time:92307ms step_avg:58.35ms
step:1583/2330 train_time:92364ms step_avg:58.35ms
step:1584/2330 train_time:92426ms step_avg:58.35ms
step:1585/2330 train_time:92483ms step_avg:58.35ms
step:1586/2330 train_time:92544ms step_avg:58.35ms
step:1587/2330 train_time:92601ms step_avg:58.35ms
step:1588/2330 train_time:92662ms step_avg:58.35ms
step:1589/2330 train_time:92721ms step_avg:58.35ms
step:1590/2330 train_time:92782ms step_avg:58.35ms
step:1591/2330 train_time:92841ms step_avg:58.35ms
step:1592/2330 train_time:92901ms step_avg:58.35ms
step:1593/2330 train_time:92959ms step_avg:58.35ms
step:1594/2330 train_time:93019ms step_avg:58.36ms
step:1595/2330 train_time:93076ms step_avg:58.36ms
step:1596/2330 train_time:93138ms step_avg:58.36ms
step:1597/2330 train_time:93194ms step_avg:58.36ms
step:1598/2330 train_time:93255ms step_avg:58.36ms
step:1599/2330 train_time:93311ms step_avg:58.36ms
step:1600/2330 train_time:93373ms step_avg:58.36ms
step:1601/2330 train_time:93429ms step_avg:58.36ms
step:1602/2330 train_time:93491ms step_avg:58.36ms
step:1603/2330 train_time:93547ms step_avg:58.36ms
step:1604/2330 train_time:93609ms step_avg:58.36ms
step:1605/2330 train_time:93666ms step_avg:58.36ms
step:1606/2330 train_time:93727ms step_avg:58.36ms
step:1607/2330 train_time:93785ms step_avg:58.36ms
step:1608/2330 train_time:93845ms step_avg:58.36ms
step:1609/2330 train_time:93903ms step_avg:58.36ms
step:1610/2330 train_time:93964ms step_avg:58.36ms
step:1611/2330 train_time:94022ms step_avg:58.36ms
step:1612/2330 train_time:94082ms step_avg:58.36ms
step:1613/2330 train_time:94139ms step_avg:58.36ms
step:1614/2330 train_time:94201ms step_avg:58.36ms
step:1615/2330 train_time:94258ms step_avg:58.36ms
step:1616/2330 train_time:94320ms step_avg:58.37ms
step:1617/2330 train_time:94377ms step_avg:58.37ms
step:1618/2330 train_time:94439ms step_avg:58.37ms
step:1619/2330 train_time:94495ms step_avg:58.37ms
step:1620/2330 train_time:94557ms step_avg:58.37ms
step:1621/2330 train_time:94613ms step_avg:58.37ms
step:1622/2330 train_time:94675ms step_avg:58.37ms
step:1623/2330 train_time:94732ms step_avg:58.37ms
step:1624/2330 train_time:94793ms step_avg:58.37ms
step:1625/2330 train_time:94850ms step_avg:58.37ms
step:1626/2330 train_time:94912ms step_avg:58.37ms
step:1627/2330 train_time:94969ms step_avg:58.37ms
step:1628/2330 train_time:95031ms step_avg:58.37ms
step:1629/2330 train_time:95088ms step_avg:58.37ms
step:1630/2330 train_time:95149ms step_avg:58.37ms
step:1631/2330 train_time:95206ms step_avg:58.37ms
step:1632/2330 train_time:95267ms step_avg:58.37ms
step:1633/2330 train_time:95326ms step_avg:58.37ms
step:1634/2330 train_time:95386ms step_avg:58.38ms
step:1635/2330 train_time:95444ms step_avg:58.38ms
step:1636/2330 train_time:95504ms step_avg:58.38ms
step:1637/2330 train_time:95562ms step_avg:58.38ms
step:1638/2330 train_time:95624ms step_avg:58.38ms
step:1639/2330 train_time:95682ms step_avg:58.38ms
step:1640/2330 train_time:95743ms step_avg:58.38ms
step:1641/2330 train_time:95801ms step_avg:58.38ms
step:1642/2330 train_time:95861ms step_avg:58.38ms
step:1643/2330 train_time:95919ms step_avg:58.38ms
step:1644/2330 train_time:95979ms step_avg:58.38ms
step:1645/2330 train_time:96036ms step_avg:58.38ms
step:1646/2330 train_time:96097ms step_avg:58.38ms
step:1647/2330 train_time:96153ms step_avg:58.38ms
step:1648/2330 train_time:96214ms step_avg:58.38ms
step:1649/2330 train_time:96272ms step_avg:58.38ms
step:1650/2330 train_time:96333ms step_avg:58.38ms
step:1651/2330 train_time:96390ms step_avg:58.38ms
step:1652/2330 train_time:96451ms step_avg:58.38ms
step:1653/2330 train_time:96508ms step_avg:58.38ms
step:1654/2330 train_time:96570ms step_avg:58.39ms
step:1655/2330 train_time:96626ms step_avg:58.38ms
step:1656/2330 train_time:96688ms step_avg:58.39ms
step:1657/2330 train_time:96745ms step_avg:58.39ms
step:1658/2330 train_time:96807ms step_avg:58.39ms
step:1659/2330 train_time:96864ms step_avg:58.39ms
step:1660/2330 train_time:96926ms step_avg:58.39ms
step:1661/2330 train_time:96984ms step_avg:58.39ms
step:1662/2330 train_time:97045ms step_avg:58.39ms
step:1663/2330 train_time:97104ms step_avg:58.39ms
step:1664/2330 train_time:97164ms step_avg:58.39ms
step:1665/2330 train_time:97222ms step_avg:58.39ms
step:1666/2330 train_time:97282ms step_avg:58.39ms
step:1667/2330 train_time:97340ms step_avg:58.39ms
step:1668/2330 train_time:97402ms step_avg:58.39ms
step:1669/2330 train_time:97460ms step_avg:58.39ms
step:1670/2330 train_time:97519ms step_avg:58.39ms
step:1671/2330 train_time:97576ms step_avg:58.39ms
step:1672/2330 train_time:97639ms step_avg:58.40ms
step:1673/2330 train_time:97696ms step_avg:58.40ms
step:1674/2330 train_time:97756ms step_avg:58.40ms
step:1675/2330 train_time:97812ms step_avg:58.40ms
step:1676/2330 train_time:97874ms step_avg:58.40ms
step:1677/2330 train_time:97931ms step_avg:58.40ms
step:1678/2330 train_time:97992ms step_avg:58.40ms
step:1679/2330 train_time:98048ms step_avg:58.40ms
step:1680/2330 train_time:98111ms step_avg:58.40ms
step:1681/2330 train_time:98169ms step_avg:58.40ms
step:1682/2330 train_time:98229ms step_avg:58.40ms
step:1683/2330 train_time:98286ms step_avg:58.40ms
step:1684/2330 train_time:98347ms step_avg:58.40ms
step:1685/2330 train_time:98405ms step_avg:58.40ms
step:1686/2330 train_time:98466ms step_avg:58.40ms
step:1687/2330 train_time:98524ms step_avg:58.40ms
step:1688/2330 train_time:98584ms step_avg:58.40ms
step:1689/2330 train_time:98642ms step_avg:58.40ms
step:1690/2330 train_time:98703ms step_avg:58.40ms
step:1691/2330 train_time:98760ms step_avg:58.40ms
step:1692/2330 train_time:98821ms step_avg:58.40ms
step:1693/2330 train_time:98879ms step_avg:58.40ms
step:1694/2330 train_time:98939ms step_avg:58.41ms
step:1695/2330 train_time:98997ms step_avg:58.41ms
step:1696/2330 train_time:99057ms step_avg:58.41ms
step:1697/2330 train_time:99114ms step_avg:58.41ms
step:1698/2330 train_time:99176ms step_avg:58.41ms
step:1699/2330 train_time:99233ms step_avg:58.41ms
step:1700/2330 train_time:99294ms step_avg:58.41ms
step:1701/2330 train_time:99350ms step_avg:58.41ms
step:1702/2330 train_time:99413ms step_avg:58.41ms
step:1703/2330 train_time:99470ms step_avg:58.41ms
step:1704/2330 train_time:99531ms step_avg:58.41ms
step:1705/2330 train_time:99588ms step_avg:58.41ms
step:1706/2330 train_time:99650ms step_avg:58.41ms
step:1707/2330 train_time:99706ms step_avg:58.41ms
step:1708/2330 train_time:99769ms step_avg:58.41ms
step:1709/2330 train_time:99827ms step_avg:58.41ms
step:1710/2330 train_time:99887ms step_avg:58.41ms
step:1711/2330 train_time:99945ms step_avg:58.41ms
step:1712/2330 train_time:100005ms step_avg:58.41ms
step:1713/2330 train_time:100063ms step_avg:58.41ms
step:1714/2330 train_time:100125ms step_avg:58.42ms
step:1715/2330 train_time:100183ms step_avg:58.42ms
step:1716/2330 train_time:100243ms step_avg:58.42ms
step:1717/2330 train_time:100302ms step_avg:58.42ms
step:1718/2330 train_time:100362ms step_avg:58.42ms
step:1719/2330 train_time:100420ms step_avg:58.42ms
step:1720/2330 train_time:100480ms step_avg:58.42ms
step:1721/2330 train_time:100537ms step_avg:58.42ms
step:1722/2330 train_time:100598ms step_avg:58.42ms
step:1723/2330 train_time:100654ms step_avg:58.42ms
step:1724/2330 train_time:100716ms step_avg:58.42ms
step:1725/2330 train_time:100773ms step_avg:58.42ms
step:1726/2330 train_time:100835ms step_avg:58.42ms
step:1727/2330 train_time:100892ms step_avg:58.42ms
step:1728/2330 train_time:100953ms step_avg:58.42ms
step:1729/2330 train_time:101010ms step_avg:58.42ms
step:1730/2330 train_time:101071ms step_avg:58.42ms
step:1731/2330 train_time:101128ms step_avg:58.42ms
step:1732/2330 train_time:101190ms step_avg:58.42ms
step:1733/2330 train_time:101247ms step_avg:58.42ms
step:1734/2330 train_time:101308ms step_avg:58.42ms
step:1735/2330 train_time:101366ms step_avg:58.42ms
step:1736/2330 train_time:101427ms step_avg:58.43ms
step:1737/2330 train_time:101485ms step_avg:58.43ms
step:1738/2330 train_time:101545ms step_avg:58.43ms
step:1739/2330 train_time:101603ms step_avg:58.43ms
step:1740/2330 train_time:101663ms step_avg:58.43ms
step:1741/2330 train_time:101720ms step_avg:58.43ms
step:1742/2330 train_time:101782ms step_avg:58.43ms
step:1743/2330 train_time:101839ms step_avg:58.43ms
step:1744/2330 train_time:101900ms step_avg:58.43ms
step:1745/2330 train_time:101957ms step_avg:58.43ms
step:1746/2330 train_time:102018ms step_avg:58.43ms
step:1747/2330 train_time:102075ms step_avg:58.43ms
step:1748/2330 train_time:102136ms step_avg:58.43ms
step:1749/2330 train_time:102194ms step_avg:58.43ms
step:1750/2330 train_time:102254ms step_avg:58.43ms
step:1750/2330 val_loss:3.8231 train_time:102337ms step_avg:58.48ms
step:1751/2330 train_time:102358ms step_avg:58.46ms
step:1752/2330 train_time:102378ms step_avg:58.43ms
step:1753/2330 train_time:102431ms step_avg:58.43ms
step:1754/2330 train_time:102501ms step_avg:58.44ms
step:1755/2330 train_time:102558ms step_avg:58.44ms
step:1756/2330 train_time:102623ms step_avg:58.44ms
step:1757/2330 train_time:102679ms step_avg:58.44ms
step:1758/2330 train_time:102740ms step_avg:58.44ms
step:1759/2330 train_time:102797ms step_avg:58.44ms
step:1760/2330 train_time:102858ms step_avg:58.44ms
step:1761/2330 train_time:102914ms step_avg:58.44ms
step:1762/2330 train_time:102976ms step_avg:58.44ms
step:1763/2330 train_time:103033ms step_avg:58.44ms
step:1764/2330 train_time:103092ms step_avg:58.44ms
step:1765/2330 train_time:103149ms step_avg:58.44ms
step:1766/2330 train_time:103209ms step_avg:58.44ms
step:1767/2330 train_time:103270ms step_avg:58.44ms
step:1768/2330 train_time:103331ms step_avg:58.45ms
step:1769/2330 train_time:103391ms step_avg:58.45ms
step:1770/2330 train_time:103452ms step_avg:58.45ms
step:1771/2330 train_time:103510ms step_avg:58.45ms
step:1772/2330 train_time:103572ms step_avg:58.45ms
step:1773/2330 train_time:103629ms step_avg:58.45ms
step:1774/2330 train_time:103690ms step_avg:58.45ms
step:1775/2330 train_time:103747ms step_avg:58.45ms
step:1776/2330 train_time:103808ms step_avg:58.45ms
step:1777/2330 train_time:103864ms step_avg:58.45ms
step:1778/2330 train_time:103926ms step_avg:58.45ms
step:1779/2330 train_time:103983ms step_avg:58.45ms
step:1780/2330 train_time:104042ms step_avg:58.45ms
step:1781/2330 train_time:104099ms step_avg:58.45ms
step:1782/2330 train_time:104158ms step_avg:58.45ms
step:1783/2330 train_time:104217ms step_avg:58.45ms
step:1784/2330 train_time:104278ms step_avg:58.45ms
step:1785/2330 train_time:104336ms step_avg:58.45ms
step:1786/2330 train_time:104398ms step_avg:58.45ms
step:1787/2330 train_time:104457ms step_avg:58.45ms
step:1788/2330 train_time:104519ms step_avg:58.46ms
step:1789/2330 train_time:104577ms step_avg:58.46ms
step:1790/2330 train_time:104638ms step_avg:58.46ms
step:1791/2330 train_time:104696ms step_avg:58.46ms
step:1792/2330 train_time:104757ms step_avg:58.46ms
step:1793/2330 train_time:104814ms step_avg:58.46ms
step:1794/2330 train_time:104875ms step_avg:58.46ms
step:1795/2330 train_time:104932ms step_avg:58.46ms
step:1796/2330 train_time:104992ms step_avg:58.46ms
step:1797/2330 train_time:105049ms step_avg:58.46ms
step:1798/2330 train_time:105110ms step_avg:58.46ms
step:1799/2330 train_time:105166ms step_avg:58.46ms
step:1800/2330 train_time:105226ms step_avg:58.46ms
step:1801/2330 train_time:105283ms step_avg:58.46ms
step:1802/2330 train_time:105345ms step_avg:58.46ms
step:1803/2330 train_time:105403ms step_avg:58.46ms
step:1804/2330 train_time:105465ms step_avg:58.46ms
step:1805/2330 train_time:105521ms step_avg:58.46ms
step:1806/2330 train_time:105584ms step_avg:58.46ms
step:1807/2330 train_time:105642ms step_avg:58.46ms
step:1808/2330 train_time:105703ms step_avg:58.46ms
step:1809/2330 train_time:105759ms step_avg:58.46ms
step:1810/2330 train_time:105820ms step_avg:58.46ms
step:1811/2330 train_time:105878ms step_avg:58.46ms
step:1812/2330 train_time:105939ms step_avg:58.46ms
step:1813/2330 train_time:105997ms step_avg:58.46ms
step:1814/2330 train_time:106057ms step_avg:58.47ms
step:1815/2330 train_time:106114ms step_avg:58.47ms
step:1816/2330 train_time:106175ms step_avg:58.47ms
step:1817/2330 train_time:106233ms step_avg:58.47ms
step:1818/2330 train_time:106294ms step_avg:58.47ms
step:1819/2330 train_time:106353ms step_avg:58.47ms
step:1820/2330 train_time:106413ms step_avg:58.47ms
step:1821/2330 train_time:106471ms step_avg:58.47ms
step:1822/2330 train_time:106532ms step_avg:58.47ms
step:1823/2330 train_time:106591ms step_avg:58.47ms
step:1824/2330 train_time:106651ms step_avg:58.47ms
step:1825/2330 train_time:106709ms step_avg:58.47ms
step:1826/2330 train_time:106769ms step_avg:58.47ms
step:1827/2330 train_time:106826ms step_avg:58.47ms
step:1828/2330 train_time:106887ms step_avg:58.47ms
step:1829/2330 train_time:106945ms step_avg:58.47ms
step:1830/2330 train_time:107005ms step_avg:58.47ms
step:1831/2330 train_time:107061ms step_avg:58.47ms
step:1832/2330 train_time:107123ms step_avg:58.47ms
step:1833/2330 train_time:107180ms step_avg:58.47ms
step:1834/2330 train_time:107241ms step_avg:58.47ms
step:1835/2330 train_time:107299ms step_avg:58.47ms
step:1836/2330 train_time:107360ms step_avg:58.47ms
step:1837/2330 train_time:107418ms step_avg:58.47ms
step:1838/2330 train_time:107479ms step_avg:58.48ms
step:1839/2330 train_time:107536ms step_avg:58.48ms
step:1840/2330 train_time:107598ms step_avg:58.48ms
step:1841/2330 train_time:107657ms step_avg:58.48ms
step:1842/2330 train_time:107718ms step_avg:58.48ms
step:1843/2330 train_time:107775ms step_avg:58.48ms
step:1844/2330 train_time:107837ms step_avg:58.48ms
step:1845/2330 train_time:107895ms step_avg:58.48ms
step:1846/2330 train_time:107955ms step_avg:58.48ms
step:1847/2330 train_time:108012ms step_avg:58.48ms
step:1848/2330 train_time:108073ms step_avg:58.48ms
step:1849/2330 train_time:108130ms step_avg:58.48ms
step:1850/2330 train_time:108190ms step_avg:58.48ms
step:1851/2330 train_time:108248ms step_avg:58.48ms
step:1852/2330 train_time:108308ms step_avg:58.48ms
step:1853/2330 train_time:108364ms step_avg:58.48ms
step:1854/2330 train_time:108426ms step_avg:58.48ms
step:1855/2330 train_time:108483ms step_avg:58.48ms
step:1856/2330 train_time:108544ms step_avg:58.48ms
step:1857/2330 train_time:108601ms step_avg:58.48ms
step:1858/2330 train_time:108663ms step_avg:58.48ms
step:1859/2330 train_time:108720ms step_avg:58.48ms
step:1860/2330 train_time:108782ms step_avg:58.48ms
step:1861/2330 train_time:108840ms step_avg:58.48ms
step:1862/2330 train_time:108901ms step_avg:58.49ms
step:1863/2330 train_time:108958ms step_avg:58.49ms
step:1864/2330 train_time:109018ms step_avg:58.49ms
step:1865/2330 train_time:109076ms step_avg:58.49ms
step:1866/2330 train_time:109136ms step_avg:58.49ms
step:1867/2330 train_time:109194ms step_avg:58.49ms
step:1868/2330 train_time:109255ms step_avg:58.49ms
step:1869/2330 train_time:109313ms step_avg:58.49ms
step:1870/2330 train_time:109373ms step_avg:58.49ms
step:1871/2330 train_time:109431ms step_avg:58.49ms
step:1872/2330 train_time:109491ms step_avg:58.49ms
step:1873/2330 train_time:109550ms step_avg:58.49ms
step:1874/2330 train_time:109611ms step_avg:58.49ms
step:1875/2330 train_time:109668ms step_avg:58.49ms
step:1876/2330 train_time:109729ms step_avg:58.49ms
step:1877/2330 train_time:109787ms step_avg:58.49ms
step:1878/2330 train_time:109849ms step_avg:58.49ms
step:1879/2330 train_time:109906ms step_avg:58.49ms
step:1880/2330 train_time:109967ms step_avg:58.49ms
step:1881/2330 train_time:110023ms step_avg:58.49ms
step:1882/2330 train_time:110085ms step_avg:58.49ms
step:1883/2330 train_time:110142ms step_avg:58.49ms
step:1884/2330 train_time:110203ms step_avg:58.49ms
step:1885/2330 train_time:110260ms step_avg:58.49ms
step:1886/2330 train_time:110321ms step_avg:58.49ms
step:1887/2330 train_time:110379ms step_avg:58.49ms
step:1888/2330 train_time:110439ms step_avg:58.50ms
step:1889/2330 train_time:110498ms step_avg:58.50ms
step:1890/2330 train_time:110558ms step_avg:58.50ms
step:1891/2330 train_time:110616ms step_avg:58.50ms
step:1892/2330 train_time:110678ms step_avg:58.50ms
step:1893/2330 train_time:110736ms step_avg:58.50ms
step:1894/2330 train_time:110796ms step_avg:58.50ms
step:1895/2330 train_time:110854ms step_avg:58.50ms
step:1896/2330 train_time:110916ms step_avg:58.50ms
step:1897/2330 train_time:110973ms step_avg:58.50ms
step:1898/2330 train_time:111033ms step_avg:58.50ms
step:1899/2330 train_time:111090ms step_avg:58.50ms
step:1900/2330 train_time:111151ms step_avg:58.50ms
step:1901/2330 train_time:111209ms step_avg:58.50ms
step:1902/2330 train_time:111268ms step_avg:58.50ms
step:1903/2330 train_time:111325ms step_avg:58.50ms
step:1904/2330 train_time:111387ms step_avg:58.50ms
step:1905/2330 train_time:111444ms step_avg:58.50ms
step:1906/2330 train_time:111505ms step_avg:58.50ms
step:1907/2330 train_time:111562ms step_avg:58.50ms
step:1908/2330 train_time:111623ms step_avg:58.50ms
step:1909/2330 train_time:111680ms step_avg:58.50ms
step:1910/2330 train_time:111742ms step_avg:58.50ms
step:1911/2330 train_time:111800ms step_avg:58.50ms
step:1912/2330 train_time:111861ms step_avg:58.50ms
step:1913/2330 train_time:111918ms step_avg:58.50ms
step:1914/2330 train_time:111979ms step_avg:58.51ms
step:1915/2330 train_time:112037ms step_avg:58.50ms
step:1916/2330 train_time:112097ms step_avg:58.51ms
step:1917/2330 train_time:112156ms step_avg:58.51ms
step:1918/2330 train_time:112217ms step_avg:58.51ms
step:1919/2330 train_time:112275ms step_avg:58.51ms
step:1920/2330 train_time:112335ms step_avg:58.51ms
step:1921/2330 train_time:112394ms step_avg:58.51ms
step:1922/2330 train_time:112455ms step_avg:58.51ms
step:1923/2330 train_time:112513ms step_avg:58.51ms
step:1924/2330 train_time:112574ms step_avg:58.51ms
step:1925/2330 train_time:112633ms step_avg:58.51ms
step:1926/2330 train_time:112693ms step_avg:58.51ms
step:1927/2330 train_time:112751ms step_avg:58.51ms
step:1928/2330 train_time:112812ms step_avg:58.51ms
step:1929/2330 train_time:112870ms step_avg:58.51ms
step:1930/2330 train_time:112930ms step_avg:58.51ms
step:1931/2330 train_time:112987ms step_avg:58.51ms
step:1932/2330 train_time:113047ms step_avg:58.51ms
step:1933/2330 train_time:113104ms step_avg:58.51ms
step:1934/2330 train_time:113165ms step_avg:58.51ms
step:1935/2330 train_time:113222ms step_avg:58.51ms
step:1936/2330 train_time:113284ms step_avg:58.51ms
step:1937/2330 train_time:113341ms step_avg:58.51ms
step:1938/2330 train_time:113402ms step_avg:58.51ms
step:1939/2330 train_time:113459ms step_avg:58.51ms
step:1940/2330 train_time:113520ms step_avg:58.52ms
step:1941/2330 train_time:113577ms step_avg:58.51ms
step:1942/2330 train_time:113638ms step_avg:58.52ms
step:1943/2330 train_time:113697ms step_avg:58.52ms
step:1944/2330 train_time:113757ms step_avg:58.52ms
step:1945/2330 train_time:113815ms step_avg:58.52ms
step:1946/2330 train_time:113876ms step_avg:58.52ms
step:1947/2330 train_time:113934ms step_avg:58.52ms
step:1948/2330 train_time:113994ms step_avg:58.52ms
step:1949/2330 train_time:114053ms step_avg:58.52ms
step:1950/2330 train_time:114114ms step_avg:58.52ms
step:1951/2330 train_time:114172ms step_avg:58.52ms
step:1952/2330 train_time:114232ms step_avg:58.52ms
step:1953/2330 train_time:114289ms step_avg:58.52ms
step:1954/2330 train_time:114349ms step_avg:58.52ms
step:1955/2330 train_time:114405ms step_avg:58.52ms
step:1956/2330 train_time:114467ms step_avg:58.52ms
step:1957/2330 train_time:114524ms step_avg:58.52ms
step:1958/2330 train_time:114585ms step_avg:58.52ms
step:1959/2330 train_time:114642ms step_avg:58.52ms
step:1960/2330 train_time:114704ms step_avg:58.52ms
step:1961/2330 train_time:114761ms step_avg:58.52ms
step:1962/2330 train_time:114823ms step_avg:58.52ms
step:1963/2330 train_time:114879ms step_avg:58.52ms
step:1964/2330 train_time:114942ms step_avg:58.52ms
step:1965/2330 train_time:114999ms step_avg:58.52ms
step:1966/2330 train_time:115060ms step_avg:58.52ms
step:1967/2330 train_time:115118ms step_avg:58.52ms
step:1968/2330 train_time:115178ms step_avg:58.53ms
step:1969/2330 train_time:115236ms step_avg:58.53ms
step:1970/2330 train_time:115297ms step_avg:58.53ms
step:1971/2330 train_time:115355ms step_avg:58.53ms
step:1972/2330 train_time:115415ms step_avg:58.53ms
step:1973/2330 train_time:115472ms step_avg:58.53ms
step:1974/2330 train_time:115533ms step_avg:58.53ms
step:1975/2330 train_time:115591ms step_avg:58.53ms
step:1976/2330 train_time:115652ms step_avg:58.53ms
step:1977/2330 train_time:115711ms step_avg:58.53ms
step:1978/2330 train_time:115770ms step_avg:58.53ms
step:1979/2330 train_time:115828ms step_avg:58.53ms
step:1980/2330 train_time:115888ms step_avg:58.53ms
step:1981/2330 train_time:115945ms step_avg:58.53ms
step:1982/2330 train_time:116008ms step_avg:58.53ms
step:1983/2330 train_time:116065ms step_avg:58.53ms
step:1984/2330 train_time:116126ms step_avg:58.53ms
step:1985/2330 train_time:116182ms step_avg:58.53ms
step:1986/2330 train_time:116245ms step_avg:58.53ms
step:1987/2330 train_time:116302ms step_avg:58.53ms
step:1988/2330 train_time:116362ms step_avg:58.53ms
step:1989/2330 train_time:116419ms step_avg:58.53ms
step:1990/2330 train_time:116480ms step_avg:58.53ms
step:1991/2330 train_time:116537ms step_avg:58.53ms
step:1992/2330 train_time:116599ms step_avg:58.53ms
step:1993/2330 train_time:116657ms step_avg:58.53ms
step:1994/2330 train_time:116719ms step_avg:58.53ms
step:1995/2330 train_time:116776ms step_avg:58.53ms
step:1996/2330 train_time:116837ms step_avg:58.54ms
step:1997/2330 train_time:116895ms step_avg:58.54ms
step:1998/2330 train_time:116956ms step_avg:58.54ms
step:1999/2330 train_time:117013ms step_avg:58.54ms
step:2000/2330 train_time:117075ms step_avg:58.54ms
step:2000/2330 val_loss:3.7592 train_time:117156ms step_avg:58.58ms
step:2001/2330 train_time:117176ms step_avg:58.56ms
step:2002/2330 train_time:117196ms step_avg:58.54ms
step:2003/2330 train_time:117256ms step_avg:58.54ms
step:2004/2330 train_time:117322ms step_avg:58.54ms
step:2005/2330 train_time:117380ms step_avg:58.54ms
step:2006/2330 train_time:117442ms step_avg:58.55ms
step:2007/2330 train_time:117500ms step_avg:58.54ms
step:2008/2330 train_time:117559ms step_avg:58.55ms
step:2009/2330 train_time:117616ms step_avg:58.54ms
step:2010/2330 train_time:117676ms step_avg:58.55ms
step:2011/2330 train_time:117734ms step_avg:58.54ms
step:2012/2330 train_time:117793ms step_avg:58.55ms
step:2013/2330 train_time:117850ms step_avg:58.54ms
step:2014/2330 train_time:117910ms step_avg:58.54ms
step:2015/2330 train_time:117966ms step_avg:58.54ms
step:2016/2330 train_time:118026ms step_avg:58.54ms
step:2017/2330 train_time:118083ms step_avg:58.54ms
step:2018/2330 train_time:118144ms step_avg:58.54ms
step:2019/2330 train_time:118203ms step_avg:58.55ms
step:2020/2330 train_time:118265ms step_avg:58.55ms
step:2021/2330 train_time:118324ms step_avg:58.55ms
step:2022/2330 train_time:118386ms step_avg:58.55ms
step:2023/2330 train_time:118444ms step_avg:58.55ms
step:2024/2330 train_time:118504ms step_avg:58.55ms
step:2025/2330 train_time:118562ms step_avg:58.55ms
step:2026/2330 train_time:118622ms step_avg:58.55ms
step:2027/2330 train_time:118680ms step_avg:58.55ms
step:2028/2330 train_time:118740ms step_avg:58.55ms
step:2029/2330 train_time:118798ms step_avg:58.55ms
step:2030/2330 train_time:118858ms step_avg:58.55ms
step:2031/2330 train_time:118917ms step_avg:58.55ms
step:2032/2330 train_time:118976ms step_avg:58.55ms
step:2033/2330 train_time:119034ms step_avg:58.55ms
step:2034/2330 train_time:119093ms step_avg:58.55ms
step:2035/2330 train_time:119151ms step_avg:58.55ms
step:2036/2330 train_time:119211ms step_avg:58.55ms
step:2037/2330 train_time:119269ms step_avg:58.55ms
step:2038/2330 train_time:119331ms step_avg:58.55ms
step:2039/2330 train_time:119389ms step_avg:58.55ms
step:2040/2330 train_time:119451ms step_avg:58.55ms
step:2041/2330 train_time:119508ms step_avg:58.55ms
step:2042/2330 train_time:119571ms step_avg:58.56ms
step:2043/2330 train_time:119628ms step_avg:58.55ms
step:2044/2330 train_time:119689ms step_avg:58.56ms
step:2045/2330 train_time:119745ms step_avg:58.56ms
step:2046/2330 train_time:119808ms step_avg:58.56ms
step:2047/2330 train_time:119865ms step_avg:58.56ms
step:2048/2330 train_time:119926ms step_avg:58.56ms
step:2049/2330 train_time:119983ms step_avg:58.56ms
step:2050/2330 train_time:120043ms step_avg:58.56ms
step:2051/2330 train_time:120101ms step_avg:58.56ms
step:2052/2330 train_time:120162ms step_avg:58.56ms
step:2053/2330 train_time:120220ms step_avg:58.56ms
step:2054/2330 train_time:120281ms step_avg:58.56ms
step:2055/2330 train_time:120339ms step_avg:58.56ms
step:2056/2330 train_time:120401ms step_avg:58.56ms
step:2057/2330 train_time:120459ms step_avg:58.56ms
step:2058/2330 train_time:120521ms step_avg:58.56ms
step:2059/2330 train_time:120580ms step_avg:58.56ms
step:2060/2330 train_time:120639ms step_avg:58.56ms
step:2061/2330 train_time:120697ms step_avg:58.56ms
step:2062/2330 train_time:120757ms step_avg:58.56ms
step:2063/2330 train_time:120815ms step_avg:58.56ms
step:2064/2330 train_time:120875ms step_avg:58.56ms
step:2065/2330 train_time:120933ms step_avg:58.56ms
step:2066/2330 train_time:120992ms step_avg:58.56ms
step:2067/2330 train_time:121049ms step_avg:58.56ms
step:2068/2330 train_time:121110ms step_avg:58.56ms
step:2069/2330 train_time:121167ms step_avg:58.56ms
step:2070/2330 train_time:121228ms step_avg:58.56ms
step:2071/2330 train_time:121286ms step_avg:58.56ms
step:2072/2330 train_time:121347ms step_avg:58.56ms
step:2073/2330 train_time:121404ms step_avg:58.56ms
step:2074/2330 train_time:121465ms step_avg:58.57ms
step:2075/2330 train_time:121523ms step_avg:58.57ms
step:2076/2330 train_time:121584ms step_avg:58.57ms
step:2077/2330 train_time:121641ms step_avg:58.57ms
step:2078/2330 train_time:121702ms step_avg:58.57ms
step:2079/2330 train_time:121759ms step_avg:58.57ms
step:2080/2330 train_time:121821ms step_avg:58.57ms
step:2081/2330 train_time:121878ms step_avg:58.57ms
step:2082/2330 train_time:121939ms step_avg:58.57ms
step:2083/2330 train_time:121997ms step_avg:58.57ms
step:2084/2330 train_time:122058ms step_avg:58.57ms
step:2085/2330 train_time:122115ms step_avg:58.57ms
step:2086/2330 train_time:122176ms step_avg:58.57ms
step:2087/2330 train_time:122235ms step_avg:58.57ms
step:2088/2330 train_time:122296ms step_avg:58.57ms
step:2089/2330 train_time:122353ms step_avg:58.57ms
step:2090/2330 train_time:122415ms step_avg:58.57ms
step:2091/2330 train_time:122473ms step_avg:58.57ms
step:2092/2330 train_time:122534ms step_avg:58.57ms
step:2093/2330 train_time:122592ms step_avg:58.57ms
step:2094/2330 train_time:122652ms step_avg:58.57ms
step:2095/2330 train_time:122709ms step_avg:58.57ms
step:2096/2330 train_time:122770ms step_avg:58.57ms
step:2097/2330 train_time:122826ms step_avg:58.57ms
step:2098/2330 train_time:122888ms step_avg:58.57ms
step:2099/2330 train_time:122944ms step_avg:58.57ms
step:2100/2330 train_time:123005ms step_avg:58.57ms
step:2101/2330 train_time:123062ms step_avg:58.57ms
step:2102/2330 train_time:123123ms step_avg:58.57ms
step:2103/2330 train_time:123180ms step_avg:58.57ms
step:2104/2330 train_time:123242ms step_avg:58.57ms
step:2105/2330 train_time:123300ms step_avg:58.57ms
step:2106/2330 train_time:123361ms step_avg:58.58ms
step:2107/2330 train_time:123419ms step_avg:58.58ms
step:2108/2330 train_time:123480ms step_avg:58.58ms
step:2109/2330 train_time:123538ms step_avg:58.58ms
step:2110/2330 train_time:123600ms step_avg:58.58ms
step:2111/2330 train_time:123658ms step_avg:58.58ms
step:2112/2330 train_time:123719ms step_avg:58.58ms
step:2113/2330 train_time:123777ms step_avg:58.58ms
step:2114/2330 train_time:123838ms step_avg:58.58ms
step:2115/2330 train_time:123895ms step_avg:58.58ms
step:2116/2330 train_time:123955ms step_avg:58.58ms
step:2117/2330 train_time:124013ms step_avg:58.58ms
step:2118/2330 train_time:124072ms step_avg:58.58ms
step:2119/2330 train_time:124131ms step_avg:58.58ms
step:2120/2330 train_time:124191ms step_avg:58.58ms
step:2121/2330 train_time:124248ms step_avg:58.58ms
step:2122/2330 train_time:124309ms step_avg:58.58ms
step:2123/2330 train_time:124367ms step_avg:58.58ms
step:2124/2330 train_time:124428ms step_avg:58.58ms
step:2125/2330 train_time:124485ms step_avg:58.58ms
step:2126/2330 train_time:124546ms step_avg:58.58ms
step:2127/2330 train_time:124603ms step_avg:58.58ms
step:2128/2330 train_time:124664ms step_avg:58.58ms
step:2129/2330 train_time:124722ms step_avg:58.58ms
step:2130/2330 train_time:124783ms step_avg:58.58ms
step:2131/2330 train_time:124841ms step_avg:58.58ms
step:2132/2330 train_time:124902ms step_avg:58.58ms
step:2133/2330 train_time:124960ms step_avg:58.58ms
step:2134/2330 train_time:125020ms step_avg:58.58ms
step:2135/2330 train_time:125078ms step_avg:58.58ms
step:2136/2330 train_time:125139ms step_avg:58.59ms
step:2137/2330 train_time:125197ms step_avg:58.59ms
step:2138/2330 train_time:125258ms step_avg:58.59ms
step:2139/2330 train_time:125316ms step_avg:58.59ms
step:2140/2330 train_time:125376ms step_avg:58.59ms
step:2141/2330 train_time:125434ms step_avg:58.59ms
step:2142/2330 train_time:125494ms step_avg:58.59ms
step:2143/2330 train_time:125551ms step_avg:58.59ms
step:2144/2330 train_time:125612ms step_avg:58.59ms
step:2145/2330 train_time:125669ms step_avg:58.59ms
step:2146/2330 train_time:125730ms step_avg:58.59ms
step:2147/2330 train_time:125788ms step_avg:58.59ms
step:2148/2330 train_time:125848ms step_avg:58.59ms
step:2149/2330 train_time:125905ms step_avg:58.59ms
step:2150/2330 train_time:125968ms step_avg:58.59ms
step:2151/2330 train_time:126025ms step_avg:58.59ms
step:2152/2330 train_time:126085ms step_avg:58.59ms
step:2153/2330 train_time:126143ms step_avg:58.59ms
step:2154/2330 train_time:126203ms step_avg:58.59ms
step:2155/2330 train_time:126260ms step_avg:58.59ms
step:2156/2330 train_time:126321ms step_avg:58.59ms
step:2157/2330 train_time:126378ms step_avg:58.59ms
step:2158/2330 train_time:126440ms step_avg:58.59ms
step:2159/2330 train_time:126498ms step_avg:58.59ms
step:2160/2330 train_time:126559ms step_avg:58.59ms
step:2161/2330 train_time:126617ms step_avg:58.59ms
step:2162/2330 train_time:126678ms step_avg:58.59ms
step:2163/2330 train_time:126736ms step_avg:58.59ms
step:2164/2330 train_time:126797ms step_avg:58.59ms
step:2165/2330 train_time:126855ms step_avg:58.59ms
step:2166/2330 train_time:126916ms step_avg:58.59ms
step:2167/2330 train_time:126973ms step_avg:58.59ms
step:2168/2330 train_time:127033ms step_avg:58.59ms
step:2169/2330 train_time:127090ms step_avg:58.59ms
step:2170/2330 train_time:127150ms step_avg:58.59ms
step:2171/2330 train_time:127208ms step_avg:58.59ms
step:2172/2330 train_time:127268ms step_avg:58.59ms
step:2173/2330 train_time:127325ms step_avg:58.59ms
step:2174/2330 train_time:127386ms step_avg:58.60ms
step:2175/2330 train_time:127443ms step_avg:58.59ms
step:2176/2330 train_time:127505ms step_avg:58.60ms
step:2177/2330 train_time:127562ms step_avg:58.60ms
step:2178/2330 train_time:127623ms step_avg:58.60ms
step:2179/2330 train_time:127681ms step_avg:58.60ms
step:2180/2330 train_time:127742ms step_avg:58.60ms
step:2181/2330 train_time:127801ms step_avg:58.60ms
step:2182/2330 train_time:127862ms step_avg:58.60ms
step:2183/2330 train_time:127921ms step_avg:58.60ms
step:2184/2330 train_time:127982ms step_avg:58.60ms
step:2185/2330 train_time:128039ms step_avg:58.60ms
step:2186/2330 train_time:128100ms step_avg:58.60ms
step:2187/2330 train_time:128157ms step_avg:58.60ms
step:2188/2330 train_time:128219ms step_avg:58.60ms
step:2189/2330 train_time:128277ms step_avg:58.60ms
step:2190/2330 train_time:128337ms step_avg:58.60ms
step:2191/2330 train_time:128395ms step_avg:58.60ms
step:2192/2330 train_time:128455ms step_avg:58.60ms
step:2193/2330 train_time:128512ms step_avg:58.60ms
step:2194/2330 train_time:128574ms step_avg:58.60ms
step:2195/2330 train_time:128630ms step_avg:58.60ms
step:2196/2330 train_time:128691ms step_avg:58.60ms
step:2197/2330 train_time:128747ms step_avg:58.60ms
step:2198/2330 train_time:128809ms step_avg:58.60ms
step:2199/2330 train_time:128867ms step_avg:58.60ms
step:2200/2330 train_time:128929ms step_avg:58.60ms
step:2201/2330 train_time:128986ms step_avg:58.60ms
step:2202/2330 train_time:129046ms step_avg:58.60ms
step:2203/2330 train_time:129103ms step_avg:58.60ms
step:2204/2330 train_time:129164ms step_avg:58.60ms
step:2205/2330 train_time:129221ms step_avg:58.60ms
step:2206/2330 train_time:129282ms step_avg:58.60ms
step:2207/2330 train_time:129339ms step_avg:58.60ms
step:2208/2330 train_time:129401ms step_avg:58.61ms
step:2209/2330 train_time:129459ms step_avg:58.61ms
step:2210/2330 train_time:129521ms step_avg:58.61ms
step:2211/2330 train_time:129579ms step_avg:58.61ms
step:2212/2330 train_time:129639ms step_avg:58.61ms
step:2213/2330 train_time:129698ms step_avg:58.61ms
step:2214/2330 train_time:129759ms step_avg:58.61ms
step:2215/2330 train_time:129817ms step_avg:58.61ms
step:2216/2330 train_time:129878ms step_avg:58.61ms
step:2217/2330 train_time:129936ms step_avg:58.61ms
step:2218/2330 train_time:129996ms step_avg:58.61ms
step:2219/2330 train_time:130052ms step_avg:58.61ms
step:2220/2330 train_time:130113ms step_avg:58.61ms
step:2221/2330 train_time:130170ms step_avg:58.61ms
step:2222/2330 train_time:130233ms step_avg:58.61ms
step:2223/2330 train_time:130289ms step_avg:58.61ms
step:2224/2330 train_time:130351ms step_avg:58.61ms
step:2225/2330 train_time:130407ms step_avg:58.61ms
step:2226/2330 train_time:130469ms step_avg:58.61ms
step:2227/2330 train_time:130526ms step_avg:58.61ms
step:2228/2330 train_time:130588ms step_avg:58.61ms
step:2229/2330 train_time:130645ms step_avg:58.61ms
step:2230/2330 train_time:130707ms step_avg:58.61ms
step:2231/2330 train_time:130764ms step_avg:58.61ms
step:2232/2330 train_time:130826ms step_avg:58.61ms
step:2233/2330 train_time:130883ms step_avg:58.61ms
step:2234/2330 train_time:130943ms step_avg:58.61ms
step:2235/2330 train_time:131001ms step_avg:58.61ms
step:2236/2330 train_time:131062ms step_avg:58.61ms
step:2237/2330 train_time:131119ms step_avg:58.61ms
step:2238/2330 train_time:131180ms step_avg:58.61ms
step:2239/2330 train_time:131238ms step_avg:58.61ms
step:2240/2330 train_time:131299ms step_avg:58.62ms
step:2241/2330 train_time:131356ms step_avg:58.61ms
step:2242/2330 train_time:131416ms step_avg:58.62ms
step:2243/2330 train_time:131474ms step_avg:58.62ms
step:2244/2330 train_time:131535ms step_avg:58.62ms
step:2245/2330 train_time:131593ms step_avg:58.62ms
step:2246/2330 train_time:131653ms step_avg:58.62ms
step:2247/2330 train_time:131710ms step_avg:58.62ms
step:2248/2330 train_time:131771ms step_avg:58.62ms
step:2249/2330 train_time:131829ms step_avg:58.62ms
step:2250/2330 train_time:131889ms step_avg:58.62ms
step:2250/2330 val_loss:3.7109 train_time:131970ms step_avg:58.65ms
step:2251/2330 train_time:131990ms step_avg:58.64ms
step:2252/2330 train_time:132011ms step_avg:58.62ms
step:2253/2330 train_time:132072ms step_avg:58.62ms
step:2254/2330 train_time:132134ms step_avg:58.62ms
step:2255/2330 train_time:132192ms step_avg:58.62ms
step:2256/2330 train_time:132253ms step_avg:58.62ms
step:2257/2330 train_time:132311ms step_avg:58.62ms
step:2258/2330 train_time:132370ms step_avg:58.62ms
step:2259/2330 train_time:132428ms step_avg:58.62ms
step:2260/2330 train_time:132487ms step_avg:58.62ms
step:2261/2330 train_time:132545ms step_avg:58.62ms
step:2262/2330 train_time:132605ms step_avg:58.62ms
step:2263/2330 train_time:132662ms step_avg:58.62ms
step:2264/2330 train_time:132722ms step_avg:58.62ms
step:2265/2330 train_time:132778ms step_avg:58.62ms
step:2266/2330 train_time:132838ms step_avg:58.62ms
step:2267/2330 train_time:132894ms step_avg:58.62ms
step:2268/2330 train_time:132957ms step_avg:58.62ms
step:2269/2330 train_time:133016ms step_avg:58.62ms
step:2270/2330 train_time:133077ms step_avg:58.62ms
step:2271/2330 train_time:133135ms step_avg:58.62ms
step:2272/2330 train_time:133198ms step_avg:58.63ms
step:2273/2330 train_time:133255ms step_avg:58.63ms
step:2274/2330 train_time:133316ms step_avg:58.63ms
step:2275/2330 train_time:133372ms step_avg:58.63ms
step:2276/2330 train_time:133433ms step_avg:58.63ms
step:2277/2330 train_time:133491ms step_avg:58.63ms
step:2278/2330 train_time:133551ms step_avg:58.63ms
step:2279/2330 train_time:133608ms step_avg:58.63ms
step:2280/2330 train_time:133668ms step_avg:58.63ms
step:2281/2330 train_time:133725ms step_avg:58.63ms
step:2282/2330 train_time:133786ms step_avg:58.63ms
step:2283/2330 train_time:133843ms step_avg:58.63ms
step:2284/2330 train_time:133904ms step_avg:58.63ms
step:2285/2330 train_time:133961ms step_avg:58.63ms
step:2286/2330 train_time:134024ms step_avg:58.63ms
step:2287/2330 train_time:134083ms step_avg:58.63ms
step:2288/2330 train_time:134146ms step_avg:58.63ms
step:2289/2330 train_time:134205ms step_avg:58.63ms
step:2290/2330 train_time:134265ms step_avg:58.63ms
step:2291/2330 train_time:134322ms step_avg:58.63ms
step:2292/2330 train_time:134384ms step_avg:58.63ms
step:2293/2330 train_time:134442ms step_avg:58.63ms
step:2294/2330 train_time:134503ms step_avg:58.63ms
step:2295/2330 train_time:134560ms step_avg:58.63ms
step:2296/2330 train_time:134620ms step_avg:58.63ms
step:2297/2330 train_time:134677ms step_avg:58.63ms
step:2298/2330 train_time:134737ms step_avg:58.63ms
step:2299/2330 train_time:134794ms step_avg:58.63ms
step:2300/2330 train_time:134855ms step_avg:58.63ms
step:2301/2330 train_time:134912ms step_avg:58.63ms
step:2302/2330 train_time:134973ms step_avg:58.63ms
step:2303/2330 train_time:135031ms step_avg:58.63ms
step:2304/2330 train_time:135092ms step_avg:58.63ms
step:2305/2330 train_time:135150ms step_avg:58.63ms
step:2306/2330 train_time:135212ms step_avg:58.64ms
step:2307/2330 train_time:135269ms step_avg:58.63ms
step:2308/2330 train_time:135333ms step_avg:58.64ms
step:2309/2330 train_time:135390ms step_avg:58.64ms
step:2310/2330 train_time:135451ms step_avg:58.64ms
step:2311/2330 train_time:135508ms step_avg:58.64ms
step:2312/2330 train_time:135568ms step_avg:58.64ms
step:2313/2330 train_time:135626ms step_avg:58.64ms
step:2314/2330 train_time:135686ms step_avg:58.64ms
step:2315/2330 train_time:135743ms step_avg:58.64ms
step:2316/2330 train_time:135803ms step_avg:58.64ms
step:2317/2330 train_time:135861ms step_avg:58.64ms
step:2318/2330 train_time:135921ms step_avg:58.64ms
step:2319/2330 train_time:135979ms step_avg:58.64ms
step:2320/2330 train_time:136039ms step_avg:58.64ms
step:2321/2330 train_time:136098ms step_avg:58.64ms
step:2322/2330 train_time:136158ms step_avg:58.64ms
step:2323/2330 train_time:136216ms step_avg:58.64ms
step:2324/2330 train_time:136277ms step_avg:58.64ms
step:2325/2330 train_time:136334ms step_avg:58.64ms
step:2326/2330 train_time:136395ms step_avg:58.64ms
step:2327/2330 train_time:136452ms step_avg:58.64ms
step:2328/2330 train_time:136514ms step_avg:58.64ms
step:2329/2330 train_time:136570ms step_avg:58.64ms
step:2330/2330 train_time:136632ms step_avg:58.64ms
step:2330/2330 val_loss:3.6952 train_time:136714ms step_avg:58.68ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
