import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--beta1', type=str, required=True)
parser.add_argument('--beta2', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_beta_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=6e-4,  betas=(float(args2.beta1), float(args2.beta2)), eps=1e-10, weight_decay=0.0, fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt", exist_ok=True)
            torch.save(log, f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 04:36:14 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   27C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   26C    P0             115W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   24C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   27C    P0             113W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   25C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   26C    P0             109W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:94ms step_avg:94.28ms
step:2/2330 train_time:177ms step_avg:88.35ms
step:3/2330 train_time:195ms step_avg:64.92ms
step:4/2330 train_time:214ms step_avg:53.46ms
step:5/2330 train_time:268ms step_avg:53.53ms
step:6/2330 train_time:326ms step_avg:54.27ms
step:7/2330 train_time:381ms step_avg:54.43ms
step:8/2330 train_time:439ms step_avg:54.91ms
step:9/2330 train_time:494ms step_avg:54.93ms
step:10/2330 train_time:553ms step_avg:55.26ms
step:11/2330 train_time:607ms step_avg:55.22ms
step:12/2330 train_time:666ms step_avg:55.52ms
step:13/2330 train_time:722ms step_avg:55.53ms
step:14/2330 train_time:780ms step_avg:55.71ms
step:15/2330 train_time:835ms step_avg:55.66ms
step:16/2330 train_time:893ms step_avg:55.82ms
step:17/2330 train_time:948ms step_avg:55.79ms
step:18/2330 train_time:1007ms step_avg:55.96ms
step:19/2330 train_time:1066ms step_avg:56.08ms
step:20/2330 train_time:1128ms step_avg:56.38ms
step:21/2330 train_time:1186ms step_avg:56.45ms
step:22/2330 train_time:1247ms step_avg:56.67ms
step:23/2330 train_time:1303ms step_avg:56.65ms
step:24/2330 train_time:1362ms step_avg:56.74ms
step:25/2330 train_time:1418ms step_avg:56.71ms
step:26/2330 train_time:1476ms step_avg:56.78ms
step:27/2330 train_time:1532ms step_avg:56.73ms
step:28/2330 train_time:1590ms step_avg:56.77ms
step:29/2330 train_time:1645ms step_avg:56.73ms
step:30/2330 train_time:1703ms step_avg:56.78ms
step:31/2330 train_time:1760ms step_avg:56.77ms
step:32/2330 train_time:1818ms step_avg:56.81ms
step:33/2330 train_time:1874ms step_avg:56.78ms
step:34/2330 train_time:1931ms step_avg:56.81ms
step:35/2330 train_time:1987ms step_avg:56.78ms
step:36/2330 train_time:2048ms step_avg:56.88ms
step:37/2330 train_time:2104ms step_avg:56.87ms
step:38/2330 train_time:2166ms step_avg:56.99ms
step:39/2330 train_time:2223ms step_avg:56.99ms
step:40/2330 train_time:2283ms step_avg:57.06ms
step:41/2330 train_time:2339ms step_avg:57.06ms
step:42/2330 train_time:2398ms step_avg:57.09ms
step:43/2330 train_time:2453ms step_avg:57.05ms
step:44/2330 train_time:2512ms step_avg:57.10ms
step:45/2330 train_time:2567ms step_avg:57.05ms
step:46/2330 train_time:2627ms step_avg:57.10ms
step:47/2330 train_time:2682ms step_avg:57.07ms
step:48/2330 train_time:2741ms step_avg:57.10ms
step:49/2330 train_time:2796ms step_avg:57.06ms
step:50/2330 train_time:2855ms step_avg:57.10ms
step:51/2330 train_time:2911ms step_avg:57.08ms
step:52/2330 train_time:2970ms step_avg:57.12ms
step:53/2330 train_time:3027ms step_avg:57.11ms
step:54/2330 train_time:3086ms step_avg:57.14ms
step:55/2330 train_time:3143ms step_avg:57.14ms
step:56/2330 train_time:3203ms step_avg:57.19ms
step:57/2330 train_time:3259ms step_avg:57.18ms
step:58/2330 train_time:3319ms step_avg:57.22ms
step:59/2330 train_time:3376ms step_avg:57.21ms
step:60/2330 train_time:3434ms step_avg:57.24ms
step:61/2330 train_time:3490ms step_avg:57.21ms
step:62/2330 train_time:3549ms step_avg:57.24ms
step:63/2330 train_time:3605ms step_avg:57.22ms
step:64/2330 train_time:3663ms step_avg:57.24ms
step:65/2330 train_time:3719ms step_avg:57.21ms
step:66/2330 train_time:3777ms step_avg:57.23ms
step:67/2330 train_time:3833ms step_avg:57.21ms
step:68/2330 train_time:3892ms step_avg:57.23ms
step:69/2330 train_time:3948ms step_avg:57.22ms
step:70/2330 train_time:4007ms step_avg:57.24ms
step:71/2330 train_time:4064ms step_avg:57.24ms
step:72/2330 train_time:4123ms step_avg:57.26ms
step:73/2330 train_time:4179ms step_avg:57.24ms
step:74/2330 train_time:4238ms step_avg:57.27ms
step:75/2330 train_time:4295ms step_avg:57.26ms
step:76/2330 train_time:4354ms step_avg:57.29ms
step:77/2330 train_time:4409ms step_avg:57.26ms
step:78/2330 train_time:4469ms step_avg:57.30ms
step:79/2330 train_time:4525ms step_avg:57.28ms
step:80/2330 train_time:4584ms step_avg:57.29ms
step:81/2330 train_time:4639ms step_avg:57.27ms
step:82/2330 train_time:4698ms step_avg:57.29ms
step:83/2330 train_time:4753ms step_avg:57.27ms
step:84/2330 train_time:4812ms step_avg:57.29ms
step:85/2330 train_time:4868ms step_avg:57.27ms
step:86/2330 train_time:4927ms step_avg:57.29ms
step:87/2330 train_time:4982ms step_avg:57.26ms
step:88/2330 train_time:5042ms step_avg:57.29ms
step:89/2330 train_time:5097ms step_avg:57.27ms
step:90/2330 train_time:5158ms step_avg:57.31ms
step:91/2330 train_time:5214ms step_avg:57.30ms
step:92/2330 train_time:5273ms step_avg:57.32ms
step:93/2330 train_time:5328ms step_avg:57.29ms
step:94/2330 train_time:5388ms step_avg:57.32ms
step:95/2330 train_time:5445ms step_avg:57.31ms
step:96/2330 train_time:5504ms step_avg:57.33ms
step:97/2330 train_time:5559ms step_avg:57.31ms
step:98/2330 train_time:5619ms step_avg:57.34ms
step:99/2330 train_time:5675ms step_avg:57.32ms
step:100/2330 train_time:5735ms step_avg:57.35ms
step:101/2330 train_time:5790ms step_avg:57.33ms
step:102/2330 train_time:5850ms step_avg:57.36ms
step:103/2330 train_time:5906ms step_avg:57.34ms
step:104/2330 train_time:5965ms step_avg:57.36ms
step:105/2330 train_time:6022ms step_avg:57.35ms
step:106/2330 train_time:6080ms step_avg:57.36ms
step:107/2330 train_time:6136ms step_avg:57.34ms
step:108/2330 train_time:6194ms step_avg:57.36ms
step:109/2330 train_time:6250ms step_avg:57.34ms
step:110/2330 train_time:6310ms step_avg:57.37ms
step:111/2330 train_time:6367ms step_avg:57.36ms
step:112/2330 train_time:6425ms step_avg:57.37ms
step:113/2330 train_time:6481ms step_avg:57.35ms
step:114/2330 train_time:6540ms step_avg:57.37ms
step:115/2330 train_time:6596ms step_avg:57.35ms
step:116/2330 train_time:6654ms step_avg:57.37ms
step:117/2330 train_time:6710ms step_avg:57.35ms
step:118/2330 train_time:6770ms step_avg:57.38ms
step:119/2330 train_time:6826ms step_avg:57.36ms
step:120/2330 train_time:6884ms step_avg:57.37ms
step:121/2330 train_time:6940ms step_avg:57.36ms
step:122/2330 train_time:6999ms step_avg:57.37ms
step:123/2330 train_time:7055ms step_avg:57.36ms
step:124/2330 train_time:7114ms step_avg:57.37ms
step:125/2330 train_time:7170ms step_avg:57.36ms
step:126/2330 train_time:7230ms step_avg:57.38ms
step:127/2330 train_time:7286ms step_avg:57.37ms
step:128/2330 train_time:7345ms step_avg:57.38ms
step:129/2330 train_time:7400ms step_avg:57.37ms
step:130/2330 train_time:7459ms step_avg:57.38ms
step:131/2330 train_time:7515ms step_avg:57.37ms
step:132/2330 train_time:7574ms step_avg:57.38ms
step:133/2330 train_time:7629ms step_avg:57.36ms
step:134/2330 train_time:7689ms step_avg:57.38ms
step:135/2330 train_time:7744ms step_avg:57.37ms
step:136/2330 train_time:7803ms step_avg:57.38ms
step:137/2330 train_time:7859ms step_avg:57.37ms
step:138/2330 train_time:7918ms step_avg:57.38ms
step:139/2330 train_time:7975ms step_avg:57.37ms
step:140/2330 train_time:8033ms step_avg:57.38ms
step:141/2330 train_time:8088ms step_avg:57.36ms
step:142/2330 train_time:8148ms step_avg:57.38ms
step:143/2330 train_time:8204ms step_avg:57.37ms
step:144/2330 train_time:8263ms step_avg:57.38ms
step:145/2330 train_time:8319ms step_avg:57.37ms
step:146/2330 train_time:8378ms step_avg:57.38ms
step:147/2330 train_time:8434ms step_avg:57.37ms
step:148/2330 train_time:8492ms step_avg:57.38ms
step:149/2330 train_time:8548ms step_avg:57.37ms
step:150/2330 train_time:8608ms step_avg:57.39ms
step:151/2330 train_time:8664ms step_avg:57.38ms
step:152/2330 train_time:8722ms step_avg:57.38ms
step:153/2330 train_time:8778ms step_avg:57.37ms
step:154/2330 train_time:8837ms step_avg:57.38ms
step:155/2330 train_time:8892ms step_avg:57.37ms
step:156/2330 train_time:8952ms step_avg:57.38ms
step:157/2330 train_time:9007ms step_avg:57.37ms
step:158/2330 train_time:9066ms step_avg:57.38ms
step:159/2330 train_time:9122ms step_avg:57.37ms
step:160/2330 train_time:9181ms step_avg:57.38ms
step:161/2330 train_time:9237ms step_avg:57.37ms
step:162/2330 train_time:9296ms step_avg:57.38ms
step:163/2330 train_time:9352ms step_avg:57.38ms
step:164/2330 train_time:9411ms step_avg:57.39ms
step:165/2330 train_time:9467ms step_avg:57.38ms
step:166/2330 train_time:9527ms step_avg:57.39ms
step:167/2330 train_time:9582ms step_avg:57.38ms
step:168/2330 train_time:9641ms step_avg:57.39ms
step:169/2330 train_time:9697ms step_avg:57.38ms
step:170/2330 train_time:9757ms step_avg:57.39ms
step:171/2330 train_time:9812ms step_avg:57.38ms
step:172/2330 train_time:9871ms step_avg:57.39ms
step:173/2330 train_time:9927ms step_avg:57.38ms
step:174/2330 train_time:9986ms step_avg:57.39ms
step:175/2330 train_time:10042ms step_avg:57.39ms
step:176/2330 train_time:10101ms step_avg:57.39ms
step:177/2330 train_time:10157ms step_avg:57.38ms
step:178/2330 train_time:10217ms step_avg:57.40ms
step:179/2330 train_time:10273ms step_avg:57.39ms
step:180/2330 train_time:10332ms step_avg:57.40ms
step:181/2330 train_time:10387ms step_avg:57.39ms
step:182/2330 train_time:10447ms step_avg:57.40ms
step:183/2330 train_time:10504ms step_avg:57.40ms
step:184/2330 train_time:10563ms step_avg:57.41ms
step:185/2330 train_time:10619ms step_avg:57.40ms
step:186/2330 train_time:10678ms step_avg:57.41ms
step:187/2330 train_time:10733ms step_avg:57.40ms
step:188/2330 train_time:10791ms step_avg:57.40ms
step:189/2330 train_time:10847ms step_avg:57.39ms
step:190/2330 train_time:10907ms step_avg:57.40ms
step:191/2330 train_time:10963ms step_avg:57.40ms
step:192/2330 train_time:11022ms step_avg:57.40ms
step:193/2330 train_time:11078ms step_avg:57.40ms
step:194/2330 train_time:11137ms step_avg:57.41ms
step:195/2330 train_time:11193ms step_avg:57.40ms
step:196/2330 train_time:11251ms step_avg:57.41ms
step:197/2330 train_time:11307ms step_avg:57.40ms
step:198/2330 train_time:11368ms step_avg:57.42ms
step:199/2330 train_time:11424ms step_avg:57.41ms
step:200/2330 train_time:11482ms step_avg:57.41ms
step:201/2330 train_time:11538ms step_avg:57.40ms
step:202/2330 train_time:11597ms step_avg:57.41ms
step:203/2330 train_time:11653ms step_avg:57.40ms
step:204/2330 train_time:11712ms step_avg:57.41ms
step:205/2330 train_time:11767ms step_avg:57.40ms
step:206/2330 train_time:11826ms step_avg:57.41ms
step:207/2330 train_time:11882ms step_avg:57.40ms
step:208/2330 train_time:11941ms step_avg:57.41ms
step:209/2330 train_time:11997ms step_avg:57.40ms
step:210/2330 train_time:12055ms step_avg:57.41ms
step:211/2330 train_time:12112ms step_avg:57.40ms
step:212/2330 train_time:12171ms step_avg:57.41ms
step:213/2330 train_time:12227ms step_avg:57.40ms
step:214/2330 train_time:12286ms step_avg:57.41ms
step:215/2330 train_time:12342ms step_avg:57.41ms
step:216/2330 train_time:12402ms step_avg:57.42ms
step:217/2330 train_time:12457ms step_avg:57.41ms
step:218/2330 train_time:12516ms step_avg:57.41ms
step:219/2330 train_time:12572ms step_avg:57.41ms
step:220/2330 train_time:12631ms step_avg:57.41ms
step:221/2330 train_time:12687ms step_avg:57.41ms
step:222/2330 train_time:12746ms step_avg:57.41ms
step:223/2330 train_time:12801ms step_avg:57.40ms
step:224/2330 train_time:12860ms step_avg:57.41ms
step:225/2330 train_time:12917ms step_avg:57.41ms
step:226/2330 train_time:12975ms step_avg:57.41ms
step:227/2330 train_time:13030ms step_avg:57.40ms
step:228/2330 train_time:13090ms step_avg:57.41ms
step:229/2330 train_time:13146ms step_avg:57.40ms
step:230/2330 train_time:13205ms step_avg:57.41ms
step:231/2330 train_time:13261ms step_avg:57.41ms
step:232/2330 train_time:13320ms step_avg:57.41ms
step:233/2330 train_time:13376ms step_avg:57.41ms
step:234/2330 train_time:13434ms step_avg:57.41ms
step:235/2330 train_time:13490ms step_avg:57.40ms
step:236/2330 train_time:13550ms step_avg:57.41ms
step:237/2330 train_time:13606ms step_avg:57.41ms
step:238/2330 train_time:13665ms step_avg:57.41ms
step:239/2330 train_time:13720ms step_avg:57.41ms
step:240/2330 train_time:13779ms step_avg:57.41ms
step:241/2330 train_time:13834ms step_avg:57.40ms
step:242/2330 train_time:13893ms step_avg:57.41ms
step:243/2330 train_time:13949ms step_avg:57.40ms
step:244/2330 train_time:14008ms step_avg:57.41ms
step:245/2330 train_time:14064ms step_avg:57.40ms
step:246/2330 train_time:14123ms step_avg:57.41ms
step:247/2330 train_time:14179ms step_avg:57.40ms
step:248/2330 train_time:14239ms step_avg:57.42ms
step:249/2330 train_time:14295ms step_avg:57.41ms
step:250/2330 train_time:14354ms step_avg:57.42ms
step:250/2330 val_loss:4.9091 train_time:14434ms step_avg:57.73ms
step:251/2330 train_time:14452ms step_avg:57.58ms
step:252/2330 train_time:14472ms step_avg:57.43ms
step:253/2330 train_time:14528ms step_avg:57.42ms
step:254/2330 train_time:14593ms step_avg:57.45ms
step:255/2330 train_time:14649ms step_avg:57.45ms
step:256/2330 train_time:14712ms step_avg:57.47ms
step:257/2330 train_time:14767ms step_avg:57.46ms
step:258/2330 train_time:14827ms step_avg:57.47ms
step:259/2330 train_time:14882ms step_avg:57.46ms
step:260/2330 train_time:14941ms step_avg:57.47ms
step:261/2330 train_time:14997ms step_avg:57.46ms
step:262/2330 train_time:15055ms step_avg:57.46ms
step:263/2330 train_time:15110ms step_avg:57.45ms
step:264/2330 train_time:15168ms step_avg:57.46ms
step:265/2330 train_time:15223ms step_avg:57.45ms
step:266/2330 train_time:15282ms step_avg:57.45ms
step:267/2330 train_time:15338ms step_avg:57.45ms
step:268/2330 train_time:15396ms step_avg:57.45ms
step:269/2330 train_time:15453ms step_avg:57.44ms
step:270/2330 train_time:15513ms step_avg:57.46ms
step:271/2330 train_time:15570ms step_avg:57.45ms
step:272/2330 train_time:15630ms step_avg:57.46ms
step:273/2330 train_time:15686ms step_avg:57.46ms
step:274/2330 train_time:15746ms step_avg:57.47ms
step:275/2330 train_time:15803ms step_avg:57.46ms
step:276/2330 train_time:15861ms step_avg:57.47ms
step:277/2330 train_time:15917ms step_avg:57.46ms
step:278/2330 train_time:15976ms step_avg:57.47ms
step:279/2330 train_time:16031ms step_avg:57.46ms
step:280/2330 train_time:16089ms step_avg:57.46ms
step:281/2330 train_time:16145ms step_avg:57.46ms
step:282/2330 train_time:16203ms step_avg:57.46ms
step:283/2330 train_time:16259ms step_avg:57.45ms
step:284/2330 train_time:16317ms step_avg:57.45ms
step:285/2330 train_time:16373ms step_avg:57.45ms
step:286/2330 train_time:16431ms step_avg:57.45ms
step:287/2330 train_time:16487ms step_avg:57.45ms
step:288/2330 train_time:16547ms step_avg:57.45ms
step:289/2330 train_time:16603ms step_avg:57.45ms
step:290/2330 train_time:16663ms step_avg:57.46ms
step:291/2330 train_time:16720ms step_avg:57.46ms
step:292/2330 train_time:16779ms step_avg:57.46ms
step:293/2330 train_time:16835ms step_avg:57.46ms
step:294/2330 train_time:16895ms step_avg:57.47ms
step:295/2330 train_time:16952ms step_avg:57.46ms
step:296/2330 train_time:17010ms step_avg:57.47ms
step:297/2330 train_time:17065ms step_avg:57.46ms
step:298/2330 train_time:17124ms step_avg:57.46ms
step:299/2330 train_time:17180ms step_avg:57.46ms
step:300/2330 train_time:17238ms step_avg:57.46ms
step:301/2330 train_time:17294ms step_avg:57.46ms
step:302/2330 train_time:17353ms step_avg:57.46ms
step:303/2330 train_time:17408ms step_avg:57.45ms
step:304/2330 train_time:17467ms step_avg:57.46ms
step:305/2330 train_time:17523ms step_avg:57.45ms
step:306/2330 train_time:17583ms step_avg:57.46ms
step:307/2330 train_time:17639ms step_avg:57.46ms
step:308/2330 train_time:17699ms step_avg:57.46ms
step:309/2330 train_time:17754ms step_avg:57.46ms
step:310/2330 train_time:17815ms step_avg:57.47ms
step:311/2330 train_time:17870ms step_avg:57.46ms
step:312/2330 train_time:17930ms step_avg:57.47ms
step:313/2330 train_time:17986ms step_avg:57.46ms
step:314/2330 train_time:18045ms step_avg:57.47ms
step:315/2330 train_time:18101ms step_avg:57.46ms
step:316/2330 train_time:18159ms step_avg:57.47ms
step:317/2330 train_time:18215ms step_avg:57.46ms
step:318/2330 train_time:18274ms step_avg:57.46ms
step:319/2330 train_time:18329ms step_avg:57.46ms
step:320/2330 train_time:18388ms step_avg:57.46ms
step:321/2330 train_time:18444ms step_avg:57.46ms
step:322/2330 train_time:18503ms step_avg:57.46ms
step:323/2330 train_time:18559ms step_avg:57.46ms
step:324/2330 train_time:18619ms step_avg:57.47ms
step:325/2330 train_time:18675ms step_avg:57.46ms
step:326/2330 train_time:18735ms step_avg:57.47ms
step:327/2330 train_time:18791ms step_avg:57.46ms
step:328/2330 train_time:18849ms step_avg:57.47ms
step:329/2330 train_time:18905ms step_avg:57.46ms
step:330/2330 train_time:18965ms step_avg:57.47ms
step:331/2330 train_time:19021ms step_avg:57.46ms
step:332/2330 train_time:19079ms step_avg:57.47ms
step:333/2330 train_time:19135ms step_avg:57.46ms
step:334/2330 train_time:19194ms step_avg:57.47ms
step:335/2330 train_time:19251ms step_avg:57.46ms
step:336/2330 train_time:19309ms step_avg:57.47ms
step:337/2330 train_time:19364ms step_avg:57.46ms
step:338/2330 train_time:19423ms step_avg:57.47ms
step:339/2330 train_time:19479ms step_avg:57.46ms
step:340/2330 train_time:19538ms step_avg:57.46ms
step:341/2330 train_time:19594ms step_avg:57.46ms
step:342/2330 train_time:19653ms step_avg:57.47ms
step:343/2330 train_time:19709ms step_avg:57.46ms
step:344/2330 train_time:19769ms step_avg:57.47ms
step:345/2330 train_time:19824ms step_avg:57.46ms
step:346/2330 train_time:19885ms step_avg:57.47ms
step:347/2330 train_time:19942ms step_avg:57.47ms
step:348/2330 train_time:20000ms step_avg:57.47ms
step:349/2330 train_time:20057ms step_avg:57.47ms
step:350/2330 train_time:20115ms step_avg:57.47ms
step:351/2330 train_time:20171ms step_avg:57.47ms
step:352/2330 train_time:20230ms step_avg:57.47ms
step:353/2330 train_time:20286ms step_avg:57.47ms
step:354/2330 train_time:20345ms step_avg:57.47ms
step:355/2330 train_time:20402ms step_avg:57.47ms
step:356/2330 train_time:20461ms step_avg:57.47ms
step:357/2330 train_time:20517ms step_avg:57.47ms
step:358/2330 train_time:20575ms step_avg:57.47ms
step:359/2330 train_time:20632ms step_avg:57.47ms
step:360/2330 train_time:20691ms step_avg:57.47ms
step:361/2330 train_time:20746ms step_avg:57.47ms
step:362/2330 train_time:20806ms step_avg:57.47ms
step:363/2330 train_time:20862ms step_avg:57.47ms
step:364/2330 train_time:20921ms step_avg:57.48ms
step:365/2330 train_time:20978ms step_avg:57.47ms
step:366/2330 train_time:21036ms step_avg:57.48ms
step:367/2330 train_time:21092ms step_avg:57.47ms
step:368/2330 train_time:21151ms step_avg:57.48ms
step:369/2330 train_time:21208ms step_avg:57.47ms
step:370/2330 train_time:21266ms step_avg:57.48ms
step:371/2330 train_time:21323ms step_avg:57.47ms
step:372/2330 train_time:21382ms step_avg:57.48ms
step:373/2330 train_time:21438ms step_avg:57.47ms
step:374/2330 train_time:21497ms step_avg:57.48ms
step:375/2330 train_time:21553ms step_avg:57.47ms
step:376/2330 train_time:21611ms step_avg:57.48ms
step:377/2330 train_time:21667ms step_avg:57.47ms
step:378/2330 train_time:21727ms step_avg:57.48ms
step:379/2330 train_time:21783ms step_avg:57.47ms
step:380/2330 train_time:21842ms step_avg:57.48ms
step:381/2330 train_time:21898ms step_avg:57.48ms
step:382/2330 train_time:21957ms step_avg:57.48ms
step:383/2330 train_time:22013ms step_avg:57.47ms
step:384/2330 train_time:22072ms step_avg:57.48ms
step:385/2330 train_time:22129ms step_avg:57.48ms
step:386/2330 train_time:22187ms step_avg:57.48ms
step:387/2330 train_time:22243ms step_avg:57.47ms
step:388/2330 train_time:22302ms step_avg:57.48ms
step:389/2330 train_time:22358ms step_avg:57.48ms
step:390/2330 train_time:22417ms step_avg:57.48ms
step:391/2330 train_time:22473ms step_avg:57.48ms
step:392/2330 train_time:22531ms step_avg:57.48ms
step:393/2330 train_time:22588ms step_avg:57.47ms
step:394/2330 train_time:22647ms step_avg:57.48ms
step:395/2330 train_time:22702ms step_avg:57.47ms
step:396/2330 train_time:22762ms step_avg:57.48ms
step:397/2330 train_time:22819ms step_avg:57.48ms
step:398/2330 train_time:22878ms step_avg:57.48ms
step:399/2330 train_time:22933ms step_avg:57.48ms
step:400/2330 train_time:22993ms step_avg:57.48ms
step:401/2330 train_time:23049ms step_avg:57.48ms
step:402/2330 train_time:23107ms step_avg:57.48ms
step:403/2330 train_time:23163ms step_avg:57.48ms
step:404/2330 train_time:23222ms step_avg:57.48ms
step:405/2330 train_time:23278ms step_avg:57.48ms
step:406/2330 train_time:23337ms step_avg:57.48ms
step:407/2330 train_time:23393ms step_avg:57.48ms
step:408/2330 train_time:23453ms step_avg:57.48ms
step:409/2330 train_time:23508ms step_avg:57.48ms
step:410/2330 train_time:23568ms step_avg:57.48ms
step:411/2330 train_time:23623ms step_avg:57.48ms
step:412/2330 train_time:23684ms step_avg:57.49ms
step:413/2330 train_time:23741ms step_avg:57.48ms
step:414/2330 train_time:23800ms step_avg:57.49ms
step:415/2330 train_time:23857ms step_avg:57.49ms
step:416/2330 train_time:23916ms step_avg:57.49ms
step:417/2330 train_time:23972ms step_avg:57.49ms
step:418/2330 train_time:24030ms step_avg:57.49ms
step:419/2330 train_time:24086ms step_avg:57.48ms
step:420/2330 train_time:24145ms step_avg:57.49ms
step:421/2330 train_time:24201ms step_avg:57.49ms
step:422/2330 train_time:24261ms step_avg:57.49ms
step:423/2330 train_time:24317ms step_avg:57.49ms
step:424/2330 train_time:24376ms step_avg:57.49ms
step:425/2330 train_time:24432ms step_avg:57.49ms
step:426/2330 train_time:24491ms step_avg:57.49ms
step:427/2330 train_time:24548ms step_avg:57.49ms
step:428/2330 train_time:24606ms step_avg:57.49ms
step:429/2330 train_time:24662ms step_avg:57.49ms
step:430/2330 train_time:24722ms step_avg:57.49ms
step:431/2330 train_time:24778ms step_avg:57.49ms
step:432/2330 train_time:24837ms step_avg:57.49ms
step:433/2330 train_time:24893ms step_avg:57.49ms
step:434/2330 train_time:24953ms step_avg:57.49ms
step:435/2330 train_time:25008ms step_avg:57.49ms
step:436/2330 train_time:25067ms step_avg:57.49ms
step:437/2330 train_time:25123ms step_avg:57.49ms
step:438/2330 train_time:25182ms step_avg:57.49ms
step:439/2330 train_time:25239ms step_avg:57.49ms
step:440/2330 train_time:25298ms step_avg:57.50ms
step:441/2330 train_time:25354ms step_avg:57.49ms
step:442/2330 train_time:25413ms step_avg:57.50ms
step:443/2330 train_time:25469ms step_avg:57.49ms
step:444/2330 train_time:25527ms step_avg:57.49ms
step:445/2330 train_time:25583ms step_avg:57.49ms
step:446/2330 train_time:25642ms step_avg:57.49ms
step:447/2330 train_time:25699ms step_avg:57.49ms
step:448/2330 train_time:25757ms step_avg:57.49ms
step:449/2330 train_time:25813ms step_avg:57.49ms
step:450/2330 train_time:25874ms step_avg:57.50ms
step:451/2330 train_time:25930ms step_avg:57.49ms
step:452/2330 train_time:25989ms step_avg:57.50ms
step:453/2330 train_time:26045ms step_avg:57.50ms
step:454/2330 train_time:26106ms step_avg:57.50ms
step:455/2330 train_time:26162ms step_avg:57.50ms
step:456/2330 train_time:26220ms step_avg:57.50ms
step:457/2330 train_time:26276ms step_avg:57.50ms
step:458/2330 train_time:26335ms step_avg:57.50ms
step:459/2330 train_time:26391ms step_avg:57.50ms
step:460/2330 train_time:26450ms step_avg:57.50ms
step:461/2330 train_time:26506ms step_avg:57.50ms
step:462/2330 train_time:26564ms step_avg:57.50ms
step:463/2330 train_time:26621ms step_avg:57.50ms
step:464/2330 train_time:26680ms step_avg:57.50ms
step:465/2330 train_time:26736ms step_avg:57.50ms
step:466/2330 train_time:26795ms step_avg:57.50ms
step:467/2330 train_time:26851ms step_avg:57.50ms
step:468/2330 train_time:26910ms step_avg:57.50ms
step:469/2330 train_time:26965ms step_avg:57.50ms
step:470/2330 train_time:27025ms step_avg:57.50ms
step:471/2330 train_time:27080ms step_avg:57.50ms
step:472/2330 train_time:27139ms step_avg:57.50ms
step:473/2330 train_time:27195ms step_avg:57.49ms
step:474/2330 train_time:27254ms step_avg:57.50ms
step:475/2330 train_time:27310ms step_avg:57.49ms
step:476/2330 train_time:27369ms step_avg:57.50ms
step:477/2330 train_time:27424ms step_avg:57.49ms
step:478/2330 train_time:27484ms step_avg:57.50ms
step:479/2330 train_time:27541ms step_avg:57.50ms
step:480/2330 train_time:27600ms step_avg:57.50ms
step:481/2330 train_time:27656ms step_avg:57.50ms
step:482/2330 train_time:27715ms step_avg:57.50ms
step:483/2330 train_time:27771ms step_avg:57.50ms
step:484/2330 train_time:27829ms step_avg:57.50ms
step:485/2330 train_time:27885ms step_avg:57.49ms
step:486/2330 train_time:27944ms step_avg:57.50ms
step:487/2330 train_time:28000ms step_avg:57.49ms
step:488/2330 train_time:28059ms step_avg:57.50ms
step:489/2330 train_time:28116ms step_avg:57.50ms
step:490/2330 train_time:28175ms step_avg:57.50ms
step:491/2330 train_time:28231ms step_avg:57.50ms
step:492/2330 train_time:28290ms step_avg:57.50ms
step:493/2330 train_time:28346ms step_avg:57.50ms
step:494/2330 train_time:28406ms step_avg:57.50ms
step:495/2330 train_time:28461ms step_avg:57.50ms
step:496/2330 train_time:28521ms step_avg:57.50ms
step:497/2330 train_time:28576ms step_avg:57.50ms
step:498/2330 train_time:28635ms step_avg:57.50ms
step:499/2330 train_time:28691ms step_avg:57.50ms
step:500/2330 train_time:28750ms step_avg:57.50ms
step:500/2330 val_loss:4.4203 train_time:28829ms step_avg:57.66ms
step:501/2330 train_time:28847ms step_avg:57.58ms
step:502/2330 train_time:28867ms step_avg:57.50ms
step:503/2330 train_time:28927ms step_avg:57.51ms
step:504/2330 train_time:28991ms step_avg:57.52ms
step:505/2330 train_time:29048ms step_avg:57.52ms
step:506/2330 train_time:29109ms step_avg:57.53ms
step:507/2330 train_time:29164ms step_avg:57.52ms
step:508/2330 train_time:29224ms step_avg:57.53ms
step:509/2330 train_time:29280ms step_avg:57.52ms
step:510/2330 train_time:29339ms step_avg:57.53ms
step:511/2330 train_time:29394ms step_avg:57.52ms
step:512/2330 train_time:29453ms step_avg:57.53ms
step:513/2330 train_time:29508ms step_avg:57.52ms
step:514/2330 train_time:29567ms step_avg:57.52ms
step:515/2330 train_time:29622ms step_avg:57.52ms
step:516/2330 train_time:29682ms step_avg:57.52ms
step:517/2330 train_time:29738ms step_avg:57.52ms
step:518/2330 train_time:29796ms step_avg:57.52ms
step:519/2330 train_time:29853ms step_avg:57.52ms
step:520/2330 train_time:29913ms step_avg:57.53ms
step:521/2330 train_time:29970ms step_avg:57.52ms
step:522/2330 train_time:30031ms step_avg:57.53ms
step:523/2330 train_time:30087ms step_avg:57.53ms
step:524/2330 train_time:30148ms step_avg:57.53ms
step:525/2330 train_time:30204ms step_avg:57.53ms
step:526/2330 train_time:30264ms step_avg:57.54ms
step:527/2330 train_time:30320ms step_avg:57.53ms
step:528/2330 train_time:30379ms step_avg:57.54ms
step:529/2330 train_time:30434ms step_avg:57.53ms
step:530/2330 train_time:30493ms step_avg:57.53ms
step:531/2330 train_time:30548ms step_avg:57.53ms
step:532/2330 train_time:30607ms step_avg:57.53ms
step:533/2330 train_time:30662ms step_avg:57.53ms
step:534/2330 train_time:30722ms step_avg:57.53ms
step:535/2330 train_time:30778ms step_avg:57.53ms
step:536/2330 train_time:30838ms step_avg:57.53ms
step:537/2330 train_time:30895ms step_avg:57.53ms
step:538/2330 train_time:30953ms step_avg:57.53ms
step:539/2330 train_time:31010ms step_avg:57.53ms
step:540/2330 train_time:31071ms step_avg:57.54ms
step:541/2330 train_time:31127ms step_avg:57.54ms
step:542/2330 train_time:31187ms step_avg:57.54ms
step:543/2330 train_time:31243ms step_avg:57.54ms
step:544/2330 train_time:31303ms step_avg:57.54ms
step:545/2330 train_time:31359ms step_avg:57.54ms
step:546/2330 train_time:31418ms step_avg:57.54ms
step:547/2330 train_time:31474ms step_avg:57.54ms
step:548/2330 train_time:31533ms step_avg:57.54ms
step:549/2330 train_time:31588ms step_avg:57.54ms
step:550/2330 train_time:31648ms step_avg:57.54ms
step:551/2330 train_time:31703ms step_avg:57.54ms
step:552/2330 train_time:31764ms step_avg:57.54ms
step:553/2330 train_time:31820ms step_avg:57.54ms
step:554/2330 train_time:31880ms step_avg:57.55ms
step:555/2330 train_time:31937ms step_avg:57.54ms
step:556/2330 train_time:31997ms step_avg:57.55ms
step:557/2330 train_time:32053ms step_avg:57.55ms
step:558/2330 train_time:32112ms step_avg:57.55ms
step:559/2330 train_time:32169ms step_avg:57.55ms
step:560/2330 train_time:32228ms step_avg:57.55ms
step:561/2330 train_time:32284ms step_avg:57.55ms
step:562/2330 train_time:32344ms step_avg:57.55ms
step:563/2330 train_time:32400ms step_avg:57.55ms
step:564/2330 train_time:32459ms step_avg:57.55ms
step:565/2330 train_time:32515ms step_avg:57.55ms
step:566/2330 train_time:32573ms step_avg:57.55ms
step:567/2330 train_time:32629ms step_avg:57.55ms
step:568/2330 train_time:32689ms step_avg:57.55ms
step:569/2330 train_time:32744ms step_avg:57.55ms
step:570/2330 train_time:32804ms step_avg:57.55ms
step:571/2330 train_time:32860ms step_avg:57.55ms
step:572/2330 train_time:32919ms step_avg:57.55ms
step:573/2330 train_time:32977ms step_avg:57.55ms
step:574/2330 train_time:33036ms step_avg:57.55ms
step:575/2330 train_time:33092ms step_avg:57.55ms
step:576/2330 train_time:33152ms step_avg:57.56ms
step:577/2330 train_time:33208ms step_avg:57.55ms
step:578/2330 train_time:33267ms step_avg:57.56ms
step:579/2330 train_time:33323ms step_avg:57.55ms
step:580/2330 train_time:33384ms step_avg:57.56ms
step:581/2330 train_time:33440ms step_avg:57.56ms
step:582/2330 train_time:33499ms step_avg:57.56ms
step:583/2330 train_time:33554ms step_avg:57.55ms
step:584/2330 train_time:33614ms step_avg:57.56ms
step:585/2330 train_time:33669ms step_avg:57.55ms
step:586/2330 train_time:33728ms step_avg:57.56ms
step:587/2330 train_time:33784ms step_avg:57.55ms
step:588/2330 train_time:33845ms step_avg:57.56ms
step:589/2330 train_time:33901ms step_avg:57.56ms
step:590/2330 train_time:33961ms step_avg:57.56ms
step:591/2330 train_time:34019ms step_avg:57.56ms
step:592/2330 train_time:34079ms step_avg:57.57ms
step:593/2330 train_time:34135ms step_avg:57.56ms
step:594/2330 train_time:34194ms step_avg:57.57ms
step:595/2330 train_time:34250ms step_avg:57.56ms
step:596/2330 train_time:34309ms step_avg:57.57ms
step:597/2330 train_time:34366ms step_avg:57.56ms
step:598/2330 train_time:34425ms step_avg:57.57ms
step:599/2330 train_time:34481ms step_avg:57.56ms
step:600/2330 train_time:34540ms step_avg:57.57ms
step:601/2330 train_time:34596ms step_avg:57.56ms
step:602/2330 train_time:34655ms step_avg:57.57ms
step:603/2330 train_time:34710ms step_avg:57.56ms
step:604/2330 train_time:34770ms step_avg:57.57ms
step:605/2330 train_time:34825ms step_avg:57.56ms
step:606/2330 train_time:34885ms step_avg:57.57ms
step:607/2330 train_time:34941ms step_avg:57.56ms
step:608/2330 train_time:35001ms step_avg:57.57ms
step:609/2330 train_time:35057ms step_avg:57.56ms
step:610/2330 train_time:35116ms step_avg:57.57ms
step:611/2330 train_time:35172ms step_avg:57.56ms
step:612/2330 train_time:35231ms step_avg:57.57ms
step:613/2330 train_time:35288ms step_avg:57.57ms
step:614/2330 train_time:35347ms step_avg:57.57ms
step:615/2330 train_time:35403ms step_avg:57.57ms
step:616/2330 train_time:35463ms step_avg:57.57ms
step:617/2330 train_time:35518ms step_avg:57.57ms
step:618/2330 train_time:35578ms step_avg:57.57ms
step:619/2330 train_time:35634ms step_avg:57.57ms
step:620/2330 train_time:35692ms step_avg:57.57ms
step:621/2330 train_time:35748ms step_avg:57.57ms
step:622/2330 train_time:35808ms step_avg:57.57ms
step:623/2330 train_time:35864ms step_avg:57.57ms
step:624/2330 train_time:35924ms step_avg:57.57ms
step:625/2330 train_time:35980ms step_avg:57.57ms
step:626/2330 train_time:36040ms step_avg:57.57ms
step:627/2330 train_time:36097ms step_avg:57.57ms
step:628/2330 train_time:36156ms step_avg:57.57ms
step:629/2330 train_time:36212ms step_avg:57.57ms
step:630/2330 train_time:36271ms step_avg:57.57ms
step:631/2330 train_time:36327ms step_avg:57.57ms
step:632/2330 train_time:36386ms step_avg:57.57ms
step:633/2330 train_time:36442ms step_avg:57.57ms
step:634/2330 train_time:36502ms step_avg:57.57ms
step:635/2330 train_time:36558ms step_avg:57.57ms
step:636/2330 train_time:36617ms step_avg:57.57ms
step:637/2330 train_time:36673ms step_avg:57.57ms
step:638/2330 train_time:36733ms step_avg:57.58ms
step:639/2330 train_time:36790ms step_avg:57.57ms
step:640/2330 train_time:36849ms step_avg:57.58ms
step:641/2330 train_time:36905ms step_avg:57.57ms
step:642/2330 train_time:36965ms step_avg:57.58ms
step:643/2330 train_time:37021ms step_avg:57.58ms
step:644/2330 train_time:37081ms step_avg:57.58ms
step:645/2330 train_time:37136ms step_avg:57.58ms
step:646/2330 train_time:37196ms step_avg:57.58ms
step:647/2330 train_time:37252ms step_avg:57.58ms
step:648/2330 train_time:37311ms step_avg:57.58ms
step:649/2330 train_time:37367ms step_avg:57.58ms
step:650/2330 train_time:37427ms step_avg:57.58ms
step:651/2330 train_time:37482ms step_avg:57.58ms
step:652/2330 train_time:37543ms step_avg:57.58ms
step:653/2330 train_time:37599ms step_avg:57.58ms
step:654/2330 train_time:37658ms step_avg:57.58ms
step:655/2330 train_time:37714ms step_avg:57.58ms
step:656/2330 train_time:37774ms step_avg:57.58ms
step:657/2330 train_time:37830ms step_avg:57.58ms
step:658/2330 train_time:37889ms step_avg:57.58ms
step:659/2330 train_time:37945ms step_avg:57.58ms
step:660/2330 train_time:38006ms step_avg:57.58ms
step:661/2330 train_time:38062ms step_avg:57.58ms
step:662/2330 train_time:38121ms step_avg:57.59ms
step:663/2330 train_time:38177ms step_avg:57.58ms
step:664/2330 train_time:38236ms step_avg:57.58ms
step:665/2330 train_time:38292ms step_avg:57.58ms
step:666/2330 train_time:38351ms step_avg:57.58ms
step:667/2330 train_time:38407ms step_avg:57.58ms
step:668/2330 train_time:38467ms step_avg:57.59ms
step:669/2330 train_time:38523ms step_avg:57.58ms
step:670/2330 train_time:38583ms step_avg:57.59ms
step:671/2330 train_time:38640ms step_avg:57.58ms
step:672/2330 train_time:38699ms step_avg:57.59ms
step:673/2330 train_time:38755ms step_avg:57.59ms
step:674/2330 train_time:38813ms step_avg:57.59ms
step:675/2330 train_time:38869ms step_avg:57.58ms
step:676/2330 train_time:38928ms step_avg:57.59ms
step:677/2330 train_time:38984ms step_avg:57.58ms
step:678/2330 train_time:39044ms step_avg:57.59ms
step:679/2330 train_time:39101ms step_avg:57.59ms
step:680/2330 train_time:39160ms step_avg:57.59ms
step:681/2330 train_time:39216ms step_avg:57.59ms
step:682/2330 train_time:39275ms step_avg:57.59ms
step:683/2330 train_time:39330ms step_avg:57.58ms
step:684/2330 train_time:39390ms step_avg:57.59ms
step:685/2330 train_time:39446ms step_avg:57.59ms
step:686/2330 train_time:39507ms step_avg:57.59ms
step:687/2330 train_time:39562ms step_avg:57.59ms
step:688/2330 train_time:39622ms step_avg:57.59ms
step:689/2330 train_time:39679ms step_avg:57.59ms
step:690/2330 train_time:39738ms step_avg:57.59ms
step:691/2330 train_time:39795ms step_avg:57.59ms
step:692/2330 train_time:39853ms step_avg:57.59ms
step:693/2330 train_time:39909ms step_avg:57.59ms
step:694/2330 train_time:39968ms step_avg:57.59ms
step:695/2330 train_time:40024ms step_avg:57.59ms
step:696/2330 train_time:40084ms step_avg:57.59ms
step:697/2330 train_time:40140ms step_avg:57.59ms
step:698/2330 train_time:40199ms step_avg:57.59ms
step:699/2330 train_time:40255ms step_avg:57.59ms
step:700/2330 train_time:40314ms step_avg:57.59ms
step:701/2330 train_time:40370ms step_avg:57.59ms
step:702/2330 train_time:40429ms step_avg:57.59ms
step:703/2330 train_time:40485ms step_avg:57.59ms
step:704/2330 train_time:40545ms step_avg:57.59ms
step:705/2330 train_time:40601ms step_avg:57.59ms
step:706/2330 train_time:40661ms step_avg:57.59ms
step:707/2330 train_time:40718ms step_avg:57.59ms
step:708/2330 train_time:40777ms step_avg:57.59ms
step:709/2330 train_time:40833ms step_avg:57.59ms
step:710/2330 train_time:40892ms step_avg:57.59ms
step:711/2330 train_time:40948ms step_avg:57.59ms
step:712/2330 train_time:41008ms step_avg:57.60ms
step:713/2330 train_time:41063ms step_avg:57.59ms
step:714/2330 train_time:41125ms step_avg:57.60ms
step:715/2330 train_time:41181ms step_avg:57.60ms
step:716/2330 train_time:41240ms step_avg:57.60ms
step:717/2330 train_time:41297ms step_avg:57.60ms
step:718/2330 train_time:41355ms step_avg:57.60ms
step:719/2330 train_time:41411ms step_avg:57.60ms
step:720/2330 train_time:41471ms step_avg:57.60ms
step:721/2330 train_time:41527ms step_avg:57.60ms
step:722/2330 train_time:41587ms step_avg:57.60ms
step:723/2330 train_time:41642ms step_avg:57.60ms
step:724/2330 train_time:41703ms step_avg:57.60ms
step:725/2330 train_time:41759ms step_avg:57.60ms
step:726/2330 train_time:41818ms step_avg:57.60ms
step:727/2330 train_time:41874ms step_avg:57.60ms
step:728/2330 train_time:41934ms step_avg:57.60ms
step:729/2330 train_time:41990ms step_avg:57.60ms
step:730/2330 train_time:42050ms step_avg:57.60ms
step:731/2330 train_time:42105ms step_avg:57.60ms
step:732/2330 train_time:42166ms step_avg:57.60ms
step:733/2330 train_time:42222ms step_avg:57.60ms
step:734/2330 train_time:42281ms step_avg:57.60ms
step:735/2330 train_time:42337ms step_avg:57.60ms
step:736/2330 train_time:42397ms step_avg:57.60ms
step:737/2330 train_time:42453ms step_avg:57.60ms
step:738/2330 train_time:42512ms step_avg:57.60ms
step:739/2330 train_time:42567ms step_avg:57.60ms
step:740/2330 train_time:42627ms step_avg:57.60ms
step:741/2330 train_time:42683ms step_avg:57.60ms
step:742/2330 train_time:42743ms step_avg:57.61ms
step:743/2330 train_time:42799ms step_avg:57.60ms
step:744/2330 train_time:42859ms step_avg:57.61ms
step:745/2330 train_time:42915ms step_avg:57.60ms
step:746/2330 train_time:42974ms step_avg:57.61ms
step:747/2330 train_time:43030ms step_avg:57.60ms
step:748/2330 train_time:43090ms step_avg:57.61ms
step:749/2330 train_time:43145ms step_avg:57.60ms
step:750/2330 train_time:43205ms step_avg:57.61ms
step:750/2330 val_loss:4.2179 train_time:43285ms step_avg:57.71ms
step:751/2330 train_time:43303ms step_avg:57.66ms
step:752/2330 train_time:43323ms step_avg:57.61ms
step:753/2330 train_time:43380ms step_avg:57.61ms
step:754/2330 train_time:43444ms step_avg:57.62ms
step:755/2330 train_time:43501ms step_avg:57.62ms
step:756/2330 train_time:43560ms step_avg:57.62ms
step:757/2330 train_time:43617ms step_avg:57.62ms
step:758/2330 train_time:43675ms step_avg:57.62ms
step:759/2330 train_time:43731ms step_avg:57.62ms
step:760/2330 train_time:43790ms step_avg:57.62ms
step:761/2330 train_time:43845ms step_avg:57.61ms
step:762/2330 train_time:43903ms step_avg:57.62ms
step:763/2330 train_time:43959ms step_avg:57.61ms
step:764/2330 train_time:44017ms step_avg:57.61ms
step:765/2330 train_time:44074ms step_avg:57.61ms
step:766/2330 train_time:44131ms step_avg:57.61ms
step:767/2330 train_time:44187ms step_avg:57.61ms
step:768/2330 train_time:44248ms step_avg:57.61ms
step:769/2330 train_time:44305ms step_avg:57.61ms
step:770/2330 train_time:44367ms step_avg:57.62ms
step:771/2330 train_time:44424ms step_avg:57.62ms
step:772/2330 train_time:44486ms step_avg:57.62ms
step:773/2330 train_time:44543ms step_avg:57.62ms
step:774/2330 train_time:44605ms step_avg:57.63ms
step:775/2330 train_time:44662ms step_avg:57.63ms
step:776/2330 train_time:44722ms step_avg:57.63ms
step:777/2330 train_time:44780ms step_avg:57.63ms
step:778/2330 train_time:44840ms step_avg:57.63ms
step:779/2330 train_time:44896ms step_avg:57.63ms
step:780/2330 train_time:44955ms step_avg:57.63ms
step:781/2330 train_time:45012ms step_avg:57.63ms
step:782/2330 train_time:45071ms step_avg:57.63ms
step:783/2330 train_time:45127ms step_avg:57.63ms
step:784/2330 train_time:45186ms step_avg:57.64ms
step:785/2330 train_time:45243ms step_avg:57.63ms
step:786/2330 train_time:45303ms step_avg:57.64ms
step:787/2330 train_time:45361ms step_avg:57.64ms
step:788/2330 train_time:45422ms step_avg:57.64ms
step:789/2330 train_time:45479ms step_avg:57.64ms
step:790/2330 train_time:45540ms step_avg:57.65ms
step:791/2330 train_time:45598ms step_avg:57.65ms
step:792/2330 train_time:45657ms step_avg:57.65ms
step:793/2330 train_time:45714ms step_avg:57.65ms
step:794/2330 train_time:45774ms step_avg:57.65ms
step:795/2330 train_time:45831ms step_avg:57.65ms
step:796/2330 train_time:45890ms step_avg:57.65ms
step:797/2330 train_time:45947ms step_avg:57.65ms
step:798/2330 train_time:46007ms step_avg:57.65ms
step:799/2330 train_time:46064ms step_avg:57.65ms
step:800/2330 train_time:46123ms step_avg:57.65ms
step:801/2330 train_time:46180ms step_avg:57.65ms
step:802/2330 train_time:46240ms step_avg:57.66ms
step:803/2330 train_time:46298ms step_avg:57.66ms
step:804/2330 train_time:46357ms step_avg:57.66ms
step:805/2330 train_time:46415ms step_avg:57.66ms
step:806/2330 train_time:46474ms step_avg:57.66ms
step:807/2330 train_time:46532ms step_avg:57.66ms
step:808/2330 train_time:46593ms step_avg:57.66ms
step:809/2330 train_time:46650ms step_avg:57.66ms
step:810/2330 train_time:46710ms step_avg:57.67ms
step:811/2330 train_time:46766ms step_avg:57.67ms
step:812/2330 train_time:46827ms step_avg:57.67ms
step:813/2330 train_time:46884ms step_avg:57.67ms
step:814/2330 train_time:46944ms step_avg:57.67ms
step:815/2330 train_time:47001ms step_avg:57.67ms
step:816/2330 train_time:47060ms step_avg:57.67ms
step:817/2330 train_time:47116ms step_avg:57.67ms
step:818/2330 train_time:47176ms step_avg:57.67ms
step:819/2330 train_time:47233ms step_avg:57.67ms
step:820/2330 train_time:47293ms step_avg:57.67ms
step:821/2330 train_time:47349ms step_avg:57.67ms
step:822/2330 train_time:47410ms step_avg:57.68ms
step:823/2330 train_time:47467ms step_avg:57.68ms
step:824/2330 train_time:47528ms step_avg:57.68ms
step:825/2330 train_time:47585ms step_avg:57.68ms
step:826/2330 train_time:47646ms step_avg:57.68ms
step:827/2330 train_time:47703ms step_avg:57.68ms
step:828/2330 train_time:47763ms step_avg:57.68ms
step:829/2330 train_time:47820ms step_avg:57.68ms
step:830/2330 train_time:47880ms step_avg:57.69ms
step:831/2330 train_time:47937ms step_avg:57.69ms
step:832/2330 train_time:47996ms step_avg:57.69ms
step:833/2330 train_time:48053ms step_avg:57.69ms
step:834/2330 train_time:48112ms step_avg:57.69ms
step:835/2330 train_time:48168ms step_avg:57.69ms
step:836/2330 train_time:48229ms step_avg:57.69ms
step:837/2330 train_time:48286ms step_avg:57.69ms
step:838/2330 train_time:48345ms step_avg:57.69ms
step:839/2330 train_time:48402ms step_avg:57.69ms
step:840/2330 train_time:48463ms step_avg:57.69ms
step:841/2330 train_time:48520ms step_avg:57.69ms
step:842/2330 train_time:48580ms step_avg:57.70ms
step:843/2330 train_time:48637ms step_avg:57.69ms
step:844/2330 train_time:48697ms step_avg:57.70ms
step:845/2330 train_time:48754ms step_avg:57.70ms
step:846/2330 train_time:48813ms step_avg:57.70ms
step:847/2330 train_time:48870ms step_avg:57.70ms
step:848/2330 train_time:48931ms step_avg:57.70ms
step:849/2330 train_time:48987ms step_avg:57.70ms
step:850/2330 train_time:49048ms step_avg:57.70ms
step:851/2330 train_time:49105ms step_avg:57.70ms
step:852/2330 train_time:49164ms step_avg:57.70ms
step:853/2330 train_time:49221ms step_avg:57.70ms
step:854/2330 train_time:49281ms step_avg:57.71ms
step:855/2330 train_time:49338ms step_avg:57.71ms
step:856/2330 train_time:49399ms step_avg:57.71ms
step:857/2330 train_time:49455ms step_avg:57.71ms
step:858/2330 train_time:49516ms step_avg:57.71ms
step:859/2330 train_time:49572ms step_avg:57.71ms
step:860/2330 train_time:49632ms step_avg:57.71ms
step:861/2330 train_time:49690ms step_avg:57.71ms
step:862/2330 train_time:49751ms step_avg:57.72ms
step:863/2330 train_time:49808ms step_avg:57.71ms
step:864/2330 train_time:49867ms step_avg:57.72ms
step:865/2330 train_time:49925ms step_avg:57.72ms
step:866/2330 train_time:49984ms step_avg:57.72ms
step:867/2330 train_time:50042ms step_avg:57.72ms
step:868/2330 train_time:50101ms step_avg:57.72ms
step:869/2330 train_time:50158ms step_avg:57.72ms
step:870/2330 train_time:50218ms step_avg:57.72ms
step:871/2330 train_time:50274ms step_avg:57.72ms
step:872/2330 train_time:50334ms step_avg:57.72ms
step:873/2330 train_time:50391ms step_avg:57.72ms
step:874/2330 train_time:50451ms step_avg:57.72ms
step:875/2330 train_time:50508ms step_avg:57.72ms
step:876/2330 train_time:50568ms step_avg:57.73ms
step:877/2330 train_time:50625ms step_avg:57.73ms
step:878/2330 train_time:50685ms step_avg:57.73ms
step:879/2330 train_time:50742ms step_avg:57.73ms
step:880/2330 train_time:50803ms step_avg:57.73ms
step:881/2330 train_time:50860ms step_avg:57.73ms
step:882/2330 train_time:50919ms step_avg:57.73ms
step:883/2330 train_time:50975ms step_avg:57.73ms
step:884/2330 train_time:51036ms step_avg:57.73ms
step:885/2330 train_time:51092ms step_avg:57.73ms
step:886/2330 train_time:51151ms step_avg:57.73ms
step:887/2330 train_time:51208ms step_avg:57.73ms
step:888/2330 train_time:51268ms step_avg:57.73ms
step:889/2330 train_time:51327ms step_avg:57.74ms
step:890/2330 train_time:51386ms step_avg:57.74ms
step:891/2330 train_time:51444ms step_avg:57.74ms
step:892/2330 train_time:51504ms step_avg:57.74ms
step:893/2330 train_time:51561ms step_avg:57.74ms
step:894/2330 train_time:51622ms step_avg:57.74ms
step:895/2330 train_time:51680ms step_avg:57.74ms
step:896/2330 train_time:51739ms step_avg:57.74ms
step:897/2330 train_time:51796ms step_avg:57.74ms
step:898/2330 train_time:51856ms step_avg:57.75ms
step:899/2330 train_time:51913ms step_avg:57.74ms
step:900/2330 train_time:51972ms step_avg:57.75ms
step:901/2330 train_time:52029ms step_avg:57.75ms
step:902/2330 train_time:52089ms step_avg:57.75ms
step:903/2330 train_time:52145ms step_avg:57.75ms
step:904/2330 train_time:52205ms step_avg:57.75ms
step:905/2330 train_time:52262ms step_avg:57.75ms
step:906/2330 train_time:52321ms step_avg:57.75ms
step:907/2330 train_time:52379ms step_avg:57.75ms
step:908/2330 train_time:52438ms step_avg:57.75ms
step:909/2330 train_time:52495ms step_avg:57.75ms
step:910/2330 train_time:52555ms step_avg:57.75ms
step:911/2330 train_time:52612ms step_avg:57.75ms
step:912/2330 train_time:52672ms step_avg:57.75ms
step:913/2330 train_time:52729ms step_avg:57.75ms
step:914/2330 train_time:52789ms step_avg:57.76ms
step:915/2330 train_time:52846ms step_avg:57.75ms
step:916/2330 train_time:52906ms step_avg:57.76ms
step:917/2330 train_time:52962ms step_avg:57.76ms
step:918/2330 train_time:53022ms step_avg:57.76ms
step:919/2330 train_time:53079ms step_avg:57.76ms
step:920/2330 train_time:53139ms step_avg:57.76ms
step:921/2330 train_time:53195ms step_avg:57.76ms
step:922/2330 train_time:53256ms step_avg:57.76ms
step:923/2330 train_time:53313ms step_avg:57.76ms
step:924/2330 train_time:53372ms step_avg:57.76ms
step:925/2330 train_time:53429ms step_avg:57.76ms
step:926/2330 train_time:53490ms step_avg:57.76ms
step:927/2330 train_time:53546ms step_avg:57.76ms
step:928/2330 train_time:53608ms step_avg:57.77ms
step:929/2330 train_time:53664ms step_avg:57.77ms
step:930/2330 train_time:53724ms step_avg:57.77ms
step:931/2330 train_time:53782ms step_avg:57.77ms
step:932/2330 train_time:53842ms step_avg:57.77ms
step:933/2330 train_time:53899ms step_avg:57.77ms
step:934/2330 train_time:53959ms step_avg:57.77ms
step:935/2330 train_time:54016ms step_avg:57.77ms
step:936/2330 train_time:54075ms step_avg:57.77ms
step:937/2330 train_time:54132ms step_avg:57.77ms
step:938/2330 train_time:54191ms step_avg:57.77ms
step:939/2330 train_time:54247ms step_avg:57.77ms
step:940/2330 train_time:54308ms step_avg:57.77ms
step:941/2330 train_time:54365ms step_avg:57.77ms
step:942/2330 train_time:54424ms step_avg:57.77ms
step:943/2330 train_time:54480ms step_avg:57.77ms
step:944/2330 train_time:54541ms step_avg:57.78ms
step:945/2330 train_time:54598ms step_avg:57.78ms
step:946/2330 train_time:54657ms step_avg:57.78ms
step:947/2330 train_time:54714ms step_avg:57.78ms
step:948/2330 train_time:54774ms step_avg:57.78ms
step:949/2330 train_time:54832ms step_avg:57.78ms
step:950/2330 train_time:54892ms step_avg:57.78ms
step:951/2330 train_time:54949ms step_avg:57.78ms
step:952/2330 train_time:55009ms step_avg:57.78ms
step:953/2330 train_time:55066ms step_avg:57.78ms
step:954/2330 train_time:55126ms step_avg:57.78ms
step:955/2330 train_time:55183ms step_avg:57.78ms
step:956/2330 train_time:55243ms step_avg:57.79ms
step:957/2330 train_time:55300ms step_avg:57.79ms
step:958/2330 train_time:55361ms step_avg:57.79ms
step:959/2330 train_time:55417ms step_avg:57.79ms
step:960/2330 train_time:55478ms step_avg:57.79ms
step:961/2330 train_time:55536ms step_avg:57.79ms
step:962/2330 train_time:55594ms step_avg:57.79ms
step:963/2330 train_time:55651ms step_avg:57.79ms
step:964/2330 train_time:55711ms step_avg:57.79ms
step:965/2330 train_time:55768ms step_avg:57.79ms
step:966/2330 train_time:55829ms step_avg:57.79ms
step:967/2330 train_time:55886ms step_avg:57.79ms
step:968/2330 train_time:55946ms step_avg:57.80ms
step:969/2330 train_time:56002ms step_avg:57.79ms
step:970/2330 train_time:56062ms step_avg:57.80ms
step:971/2330 train_time:56119ms step_avg:57.80ms
step:972/2330 train_time:56179ms step_avg:57.80ms
step:973/2330 train_time:56236ms step_avg:57.80ms
step:974/2330 train_time:56295ms step_avg:57.80ms
step:975/2330 train_time:56352ms step_avg:57.80ms
step:976/2330 train_time:56412ms step_avg:57.80ms
step:977/2330 train_time:56469ms step_avg:57.80ms
step:978/2330 train_time:56530ms step_avg:57.80ms
step:979/2330 train_time:56586ms step_avg:57.80ms
step:980/2330 train_time:56646ms step_avg:57.80ms
step:981/2330 train_time:56702ms step_avg:57.80ms
step:982/2330 train_time:56764ms step_avg:57.80ms
step:983/2330 train_time:56820ms step_avg:57.80ms
step:984/2330 train_time:56881ms step_avg:57.81ms
step:985/2330 train_time:56938ms step_avg:57.81ms
step:986/2330 train_time:56997ms step_avg:57.81ms
step:987/2330 train_time:57053ms step_avg:57.80ms
step:988/2330 train_time:57114ms step_avg:57.81ms
step:989/2330 train_time:57171ms step_avg:57.81ms
step:990/2330 train_time:57231ms step_avg:57.81ms
step:991/2330 train_time:57287ms step_avg:57.81ms
step:992/2330 train_time:57348ms step_avg:57.81ms
step:993/2330 train_time:57405ms step_avg:57.81ms
step:994/2330 train_time:57464ms step_avg:57.81ms
step:995/2330 train_time:57522ms step_avg:57.81ms
step:996/2330 train_time:57582ms step_avg:57.81ms
step:997/2330 train_time:57638ms step_avg:57.81ms
step:998/2330 train_time:57699ms step_avg:57.81ms
step:999/2330 train_time:57756ms step_avg:57.81ms
step:1000/2330 train_time:57815ms step_avg:57.82ms
step:1000/2330 val_loss:4.0727 train_time:57894ms step_avg:57.89ms
step:1001/2330 train_time:57913ms step_avg:57.86ms
step:1002/2330 train_time:57933ms step_avg:57.82ms
step:1003/2330 train_time:57987ms step_avg:57.81ms
step:1004/2330 train_time:58049ms step_avg:57.82ms
step:1005/2330 train_time:58105ms step_avg:57.82ms
step:1006/2330 train_time:58166ms step_avg:57.82ms
step:1007/2330 train_time:58222ms step_avg:57.82ms
step:1008/2330 train_time:58282ms step_avg:57.82ms
step:1009/2330 train_time:58339ms step_avg:57.82ms
step:1010/2330 train_time:58398ms step_avg:57.82ms
step:1011/2330 train_time:58454ms step_avg:57.82ms
step:1012/2330 train_time:58513ms step_avg:57.82ms
step:1013/2330 train_time:58568ms step_avg:57.82ms
step:1014/2330 train_time:58628ms step_avg:57.82ms
step:1015/2330 train_time:58684ms step_avg:57.82ms
step:1016/2330 train_time:58743ms step_avg:57.82ms
step:1017/2330 train_time:58805ms step_avg:57.82ms
step:1018/2330 train_time:58869ms step_avg:57.83ms
step:1019/2330 train_time:58928ms step_avg:57.83ms
step:1020/2330 train_time:58988ms step_avg:57.83ms
step:1021/2330 train_time:59045ms step_avg:57.83ms
step:1022/2330 train_time:59105ms step_avg:57.83ms
step:1023/2330 train_time:59162ms step_avg:57.83ms
step:1024/2330 train_time:59221ms step_avg:57.83ms
step:1025/2330 train_time:59278ms step_avg:57.83ms
step:1026/2330 train_time:59337ms step_avg:57.83ms
step:1027/2330 train_time:59394ms step_avg:57.83ms
step:1028/2330 train_time:59453ms step_avg:57.83ms
step:1029/2330 train_time:59509ms step_avg:57.83ms
step:1030/2330 train_time:59568ms step_avg:57.83ms
step:1031/2330 train_time:59624ms step_avg:57.83ms
step:1032/2330 train_time:59684ms step_avg:57.83ms
step:1033/2330 train_time:59742ms step_avg:57.83ms
step:1034/2330 train_time:59803ms step_avg:57.84ms
step:1035/2330 train_time:59860ms step_avg:57.84ms
step:1036/2330 train_time:59921ms step_avg:57.84ms
step:1037/2330 train_time:59978ms step_avg:57.84ms
step:1038/2330 train_time:60039ms step_avg:57.84ms
step:1039/2330 train_time:60096ms step_avg:57.84ms
step:1040/2330 train_time:60157ms step_avg:57.84ms
step:1041/2330 train_time:60214ms step_avg:57.84ms
step:1042/2330 train_time:60274ms step_avg:57.84ms
step:1043/2330 train_time:60330ms step_avg:57.84ms
step:1044/2330 train_time:60390ms step_avg:57.84ms
step:1045/2330 train_time:60446ms step_avg:57.84ms
step:1046/2330 train_time:60505ms step_avg:57.84ms
step:1047/2330 train_time:60562ms step_avg:57.84ms
step:1048/2330 train_time:60621ms step_avg:57.84ms
step:1049/2330 train_time:60679ms step_avg:57.84ms
step:1050/2330 train_time:60738ms step_avg:57.85ms
step:1051/2330 train_time:60796ms step_avg:57.85ms
step:1052/2330 train_time:60856ms step_avg:57.85ms
step:1053/2330 train_time:60913ms step_avg:57.85ms
step:1054/2330 train_time:60974ms step_avg:57.85ms
step:1055/2330 train_time:61031ms step_avg:57.85ms
step:1056/2330 train_time:61092ms step_avg:57.85ms
step:1057/2330 train_time:61149ms step_avg:57.85ms
step:1058/2330 train_time:61210ms step_avg:57.85ms
step:1059/2330 train_time:61266ms step_avg:57.85ms
step:1060/2330 train_time:61327ms step_avg:57.86ms
step:1061/2330 train_time:61383ms step_avg:57.85ms
step:1062/2330 train_time:61442ms step_avg:57.86ms
step:1063/2330 train_time:61498ms step_avg:57.85ms
step:1064/2330 train_time:61559ms step_avg:57.86ms
step:1065/2330 train_time:61615ms step_avg:57.85ms
step:1066/2330 train_time:61675ms step_avg:57.86ms
step:1067/2330 train_time:61732ms step_avg:57.86ms
step:1068/2330 train_time:61793ms step_avg:57.86ms
step:1069/2330 train_time:61850ms step_avg:57.86ms
step:1070/2330 train_time:61911ms step_avg:57.86ms
step:1071/2330 train_time:61968ms step_avg:57.86ms
step:1072/2330 train_time:62028ms step_avg:57.86ms
step:1073/2330 train_time:62085ms step_avg:57.86ms
step:1074/2330 train_time:62146ms step_avg:57.86ms
step:1075/2330 train_time:62204ms step_avg:57.86ms
step:1076/2330 train_time:62263ms step_avg:57.86ms
step:1077/2330 train_time:62319ms step_avg:57.86ms
step:1078/2330 train_time:62379ms step_avg:57.87ms
step:1079/2330 train_time:62435ms step_avg:57.86ms
step:1080/2330 train_time:62495ms step_avg:57.87ms
step:1081/2330 train_time:62551ms step_avg:57.86ms
step:1082/2330 train_time:62611ms step_avg:57.87ms
step:1083/2330 train_time:62668ms step_avg:57.86ms
step:1084/2330 train_time:62728ms step_avg:57.87ms
step:1085/2330 train_time:62785ms step_avg:57.87ms
step:1086/2330 train_time:62846ms step_avg:57.87ms
step:1087/2330 train_time:62903ms step_avg:57.87ms
step:1088/2330 train_time:62963ms step_avg:57.87ms
step:1089/2330 train_time:63021ms step_avg:57.87ms
step:1090/2330 train_time:63081ms step_avg:57.87ms
step:1091/2330 train_time:63138ms step_avg:57.87ms
step:1092/2330 train_time:63198ms step_avg:57.87ms
step:1093/2330 train_time:63254ms step_avg:57.87ms
step:1094/2330 train_time:63314ms step_avg:57.87ms
step:1095/2330 train_time:63370ms step_avg:57.87ms
step:1096/2330 train_time:63431ms step_avg:57.88ms
step:1097/2330 train_time:63488ms step_avg:57.87ms
step:1098/2330 train_time:63548ms step_avg:57.88ms
step:1099/2330 train_time:63605ms step_avg:57.87ms
step:1100/2330 train_time:63665ms step_avg:57.88ms
step:1101/2330 train_time:63722ms step_avg:57.88ms
step:1102/2330 train_time:63782ms step_avg:57.88ms
step:1103/2330 train_time:63839ms step_avg:57.88ms
step:1104/2330 train_time:63899ms step_avg:57.88ms
step:1105/2330 train_time:63956ms step_avg:57.88ms
step:1106/2330 train_time:64015ms step_avg:57.88ms
step:1107/2330 train_time:64073ms step_avg:57.88ms
step:1108/2330 train_time:64134ms step_avg:57.88ms
step:1109/2330 train_time:64191ms step_avg:57.88ms
step:1110/2330 train_time:64251ms step_avg:57.88ms
step:1111/2330 train_time:64307ms step_avg:57.88ms
step:1112/2330 train_time:64368ms step_avg:57.88ms
step:1113/2330 train_time:64424ms step_avg:57.88ms
step:1114/2330 train_time:64484ms step_avg:57.89ms
step:1115/2330 train_time:64540ms step_avg:57.88ms
step:1116/2330 train_time:64601ms step_avg:57.89ms
step:1117/2330 train_time:64658ms step_avg:57.89ms
step:1118/2330 train_time:64718ms step_avg:57.89ms
step:1119/2330 train_time:64774ms step_avg:57.89ms
step:1120/2330 train_time:64835ms step_avg:57.89ms
step:1121/2330 train_time:64892ms step_avg:57.89ms
step:1122/2330 train_time:64953ms step_avg:57.89ms
step:1123/2330 train_time:65009ms step_avg:57.89ms
step:1124/2330 train_time:65070ms step_avg:57.89ms
step:1125/2330 train_time:65127ms step_avg:57.89ms
step:1126/2330 train_time:65187ms step_avg:57.89ms
step:1127/2330 train_time:65244ms step_avg:57.89ms
step:1128/2330 train_time:65303ms step_avg:57.89ms
step:1129/2330 train_time:65361ms step_avg:57.89ms
step:1130/2330 train_time:65420ms step_avg:57.89ms
step:1131/2330 train_time:65477ms step_avg:57.89ms
step:1132/2330 train_time:65537ms step_avg:57.89ms
step:1133/2330 train_time:65593ms step_avg:57.89ms
step:1134/2330 train_time:65653ms step_avg:57.90ms
step:1135/2330 train_time:65710ms step_avg:57.89ms
step:1136/2330 train_time:65770ms step_avg:57.90ms
step:1137/2330 train_time:65828ms step_avg:57.90ms
step:1138/2330 train_time:65888ms step_avg:57.90ms
step:1139/2330 train_time:65945ms step_avg:57.90ms
step:1140/2330 train_time:66005ms step_avg:57.90ms
step:1141/2330 train_time:66063ms step_avg:57.90ms
step:1142/2330 train_time:66123ms step_avg:57.90ms
step:1143/2330 train_time:66179ms step_avg:57.90ms
step:1144/2330 train_time:66239ms step_avg:57.90ms
step:1145/2330 train_time:66295ms step_avg:57.90ms
step:1146/2330 train_time:66759ms step_avg:58.25ms
step:1147/2330 train_time:66814ms step_avg:58.25ms
step:1148/2330 train_time:66874ms step_avg:58.25ms
step:1149/2330 train_time:66930ms step_avg:58.25ms
step:1150/2330 train_time:66989ms step_avg:58.25ms
step:1151/2330 train_time:67045ms step_avg:58.25ms
step:1152/2330 train_time:67105ms step_avg:58.25ms
step:1153/2330 train_time:67161ms step_avg:58.25ms
step:1154/2330 train_time:67220ms step_avg:58.25ms
step:1155/2330 train_time:67276ms step_avg:58.25ms
step:1156/2330 train_time:67335ms step_avg:58.25ms
step:1157/2330 train_time:67391ms step_avg:58.25ms
step:1158/2330 train_time:67450ms step_avg:58.25ms
step:1159/2330 train_time:67507ms step_avg:58.25ms
step:1160/2330 train_time:67565ms step_avg:58.25ms
step:1161/2330 train_time:67627ms step_avg:58.25ms
step:1162/2330 train_time:67691ms step_avg:58.25ms
step:1163/2330 train_time:67750ms step_avg:58.25ms
step:1164/2330 train_time:67812ms step_avg:58.26ms
step:1165/2330 train_time:67867ms step_avg:58.26ms
step:1166/2330 train_time:67929ms step_avg:58.26ms
step:1167/2330 train_time:67986ms step_avg:58.26ms
step:1168/2330 train_time:68046ms step_avg:58.26ms
step:1169/2330 train_time:68103ms step_avg:58.26ms
step:1170/2330 train_time:68162ms step_avg:58.26ms
step:1171/2330 train_time:68219ms step_avg:58.26ms
step:1172/2330 train_time:68278ms step_avg:58.26ms
step:1173/2330 train_time:68334ms step_avg:58.26ms
step:1174/2330 train_time:68393ms step_avg:58.26ms
step:1175/2330 train_time:68450ms step_avg:58.26ms
step:1176/2330 train_time:68510ms step_avg:58.26ms
step:1177/2330 train_time:68568ms step_avg:58.26ms
step:1178/2330 train_time:68629ms step_avg:58.26ms
step:1179/2330 train_time:68688ms step_avg:58.26ms
step:1180/2330 train_time:68750ms step_avg:58.26ms
step:1181/2330 train_time:68807ms step_avg:58.26ms
step:1182/2330 train_time:68868ms step_avg:58.26ms
step:1183/2330 train_time:68925ms step_avg:58.26ms
step:1184/2330 train_time:68986ms step_avg:58.27ms
step:1185/2330 train_time:69043ms step_avg:58.26ms
step:1186/2330 train_time:69102ms step_avg:58.27ms
step:1187/2330 train_time:69159ms step_avg:58.26ms
step:1188/2330 train_time:69219ms step_avg:58.26ms
step:1189/2330 train_time:69275ms step_avg:58.26ms
step:1190/2330 train_time:69333ms step_avg:58.26ms
step:1191/2330 train_time:69390ms step_avg:58.26ms
step:1192/2330 train_time:69450ms step_avg:58.26ms
step:1193/2330 train_time:69506ms step_avg:58.26ms
step:1194/2330 train_time:69567ms step_avg:58.26ms
step:1195/2330 train_time:69625ms step_avg:58.26ms
step:1196/2330 train_time:69685ms step_avg:58.27ms
step:1197/2330 train_time:69744ms step_avg:58.27ms
step:1198/2330 train_time:69804ms step_avg:58.27ms
step:1199/2330 train_time:69861ms step_avg:58.27ms
step:1200/2330 train_time:69921ms step_avg:58.27ms
step:1201/2330 train_time:69977ms step_avg:58.27ms
step:1202/2330 train_time:70039ms step_avg:58.27ms
step:1203/2330 train_time:70095ms step_avg:58.27ms
step:1204/2330 train_time:70155ms step_avg:58.27ms
step:1205/2330 train_time:70211ms step_avg:58.27ms
step:1206/2330 train_time:70270ms step_avg:58.27ms
step:1207/2330 train_time:70326ms step_avg:58.27ms
step:1208/2330 train_time:70386ms step_avg:58.27ms
step:1209/2330 train_time:70443ms step_avg:58.27ms
step:1210/2330 train_time:70503ms step_avg:58.27ms
step:1211/2330 train_time:70561ms step_avg:58.27ms
step:1212/2330 train_time:70621ms step_avg:58.27ms
step:1213/2330 train_time:70678ms step_avg:58.27ms
step:1214/2330 train_time:70739ms step_avg:58.27ms
step:1215/2330 train_time:70796ms step_avg:58.27ms
step:1216/2330 train_time:70857ms step_avg:58.27ms
step:1217/2330 train_time:70914ms step_avg:58.27ms
step:1218/2330 train_time:70974ms step_avg:58.27ms
step:1219/2330 train_time:71031ms step_avg:58.27ms
step:1220/2330 train_time:71093ms step_avg:58.27ms
step:1221/2330 train_time:71149ms step_avg:58.27ms
step:1222/2330 train_time:71209ms step_avg:58.27ms
step:1223/2330 train_time:71266ms step_avg:58.27ms
step:1224/2330 train_time:71326ms step_avg:58.27ms
step:1225/2330 train_time:71382ms step_avg:58.27ms
step:1226/2330 train_time:71442ms step_avg:58.27ms
step:1227/2330 train_time:71499ms step_avg:58.27ms
step:1228/2330 train_time:71559ms step_avg:58.27ms
step:1229/2330 train_time:71616ms step_avg:58.27ms
step:1230/2330 train_time:71676ms step_avg:58.27ms
step:1231/2330 train_time:71733ms step_avg:58.27ms
step:1232/2330 train_time:71794ms step_avg:58.27ms
step:1233/2330 train_time:71851ms step_avg:58.27ms
step:1234/2330 train_time:71912ms step_avg:58.28ms
step:1235/2330 train_time:71969ms step_avg:58.27ms
step:1236/2330 train_time:72031ms step_avg:58.28ms
step:1237/2330 train_time:72087ms step_avg:58.28ms
step:1238/2330 train_time:72148ms step_avg:58.28ms
step:1239/2330 train_time:72205ms step_avg:58.28ms
step:1240/2330 train_time:72264ms step_avg:58.28ms
step:1241/2330 train_time:72320ms step_avg:58.28ms
step:1242/2330 train_time:72380ms step_avg:58.28ms
step:1243/2330 train_time:72437ms step_avg:58.28ms
step:1244/2330 train_time:72497ms step_avg:58.28ms
step:1245/2330 train_time:72553ms step_avg:58.28ms
step:1246/2330 train_time:72614ms step_avg:58.28ms
step:1247/2330 train_time:72671ms step_avg:58.28ms
step:1248/2330 train_time:72731ms step_avg:58.28ms
step:1249/2330 train_time:72789ms step_avg:58.28ms
step:1250/2330 train_time:72850ms step_avg:58.28ms
step:1250/2330 val_loss:3.9891 train_time:72931ms step_avg:58.34ms
step:1251/2330 train_time:72950ms step_avg:58.31ms
step:1252/2330 train_time:72969ms step_avg:58.28ms
step:1253/2330 train_time:73031ms step_avg:58.29ms
step:1254/2330 train_time:73097ms step_avg:58.29ms
step:1255/2330 train_time:73155ms step_avg:58.29ms
step:1256/2330 train_time:73215ms step_avg:58.29ms
step:1257/2330 train_time:73272ms step_avg:58.29ms
step:1258/2330 train_time:73331ms step_avg:58.29ms
step:1259/2330 train_time:73388ms step_avg:58.29ms
step:1260/2330 train_time:73448ms step_avg:58.29ms
step:1261/2330 train_time:73504ms step_avg:58.29ms
step:1262/2330 train_time:73563ms step_avg:58.29ms
step:1263/2330 train_time:73619ms step_avg:58.29ms
step:1264/2330 train_time:73678ms step_avg:58.29ms
step:1265/2330 train_time:73735ms step_avg:58.29ms
step:1266/2330 train_time:73794ms step_avg:58.29ms
step:1267/2330 train_time:73850ms step_avg:58.29ms
step:1268/2330 train_time:73911ms step_avg:58.29ms
step:1269/2330 train_time:73970ms step_avg:58.29ms
step:1270/2330 train_time:74030ms step_avg:58.29ms
step:1271/2330 train_time:74089ms step_avg:58.29ms
step:1272/2330 train_time:74150ms step_avg:58.29ms
step:1273/2330 train_time:74208ms step_avg:58.29ms
step:1274/2330 train_time:74268ms step_avg:58.29ms
step:1275/2330 train_time:74324ms step_avg:58.29ms
step:1276/2330 train_time:74383ms step_avg:58.29ms
step:1277/2330 train_time:74439ms step_avg:58.29ms
step:1278/2330 train_time:74499ms step_avg:58.29ms
step:1279/2330 train_time:74555ms step_avg:58.29ms
step:1280/2330 train_time:74615ms step_avg:58.29ms
step:1281/2330 train_time:74672ms step_avg:58.29ms
step:1282/2330 train_time:74731ms step_avg:58.29ms
step:1283/2330 train_time:74788ms step_avg:58.29ms
step:1284/2330 train_time:74847ms step_avg:58.29ms
step:1285/2330 train_time:74904ms step_avg:58.29ms
step:1286/2330 train_time:74964ms step_avg:58.29ms
step:1287/2330 train_time:75022ms step_avg:58.29ms
step:1288/2330 train_time:75084ms step_avg:58.30ms
step:1289/2330 train_time:75141ms step_avg:58.29ms
step:1290/2330 train_time:75202ms step_avg:58.30ms
step:1291/2330 train_time:75260ms step_avg:58.30ms
step:1292/2330 train_time:75320ms step_avg:58.30ms
step:1293/2330 train_time:75377ms step_avg:58.30ms
step:1294/2330 train_time:75436ms step_avg:58.30ms
step:1295/2330 train_time:75492ms step_avg:58.30ms
step:1296/2330 train_time:75552ms step_avg:58.30ms
step:1297/2330 train_time:75609ms step_avg:58.29ms
step:1298/2330 train_time:75668ms step_avg:58.30ms
step:1299/2330 train_time:75724ms step_avg:58.29ms
step:1300/2330 train_time:75784ms step_avg:58.30ms
step:1301/2330 train_time:75840ms step_avg:58.29ms
step:1302/2330 train_time:75899ms step_avg:58.29ms
step:1303/2330 train_time:75956ms step_avg:58.29ms
step:1304/2330 train_time:76018ms step_avg:58.30ms
step:1305/2330 train_time:76076ms step_avg:58.30ms
step:1306/2330 train_time:76137ms step_avg:58.30ms
step:1307/2330 train_time:76194ms step_avg:58.30ms
step:1308/2330 train_time:76255ms step_avg:58.30ms
step:1309/2330 train_time:76312ms step_avg:58.30ms
step:1310/2330 train_time:76373ms step_avg:58.30ms
step:1311/2330 train_time:76430ms step_avg:58.30ms
step:1312/2330 train_time:76490ms step_avg:58.30ms
step:1313/2330 train_time:76546ms step_avg:58.30ms
step:1314/2330 train_time:76605ms step_avg:58.30ms
step:1315/2330 train_time:76662ms step_avg:58.30ms
step:1316/2330 train_time:76721ms step_avg:58.30ms
step:1317/2330 train_time:76778ms step_avg:58.30ms
step:1318/2330 train_time:76837ms step_avg:58.30ms
step:1319/2330 train_time:76893ms step_avg:58.30ms
step:1320/2330 train_time:76954ms step_avg:58.30ms
step:1321/2330 train_time:77012ms step_avg:58.30ms
step:1322/2330 train_time:77072ms step_avg:58.30ms
step:1323/2330 train_time:77130ms step_avg:58.30ms
step:1324/2330 train_time:77190ms step_avg:58.30ms
step:1325/2330 train_time:77247ms step_avg:58.30ms
step:1326/2330 train_time:77308ms step_avg:58.30ms
step:1327/2330 train_time:77365ms step_avg:58.30ms
step:1328/2330 train_time:77424ms step_avg:58.30ms
step:1329/2330 train_time:77481ms step_avg:58.30ms
step:1330/2330 train_time:77541ms step_avg:58.30ms
step:1331/2330 train_time:77597ms step_avg:58.30ms
step:1332/2330 train_time:77657ms step_avg:58.30ms
step:1333/2330 train_time:77713ms step_avg:58.30ms
step:1334/2330 train_time:77773ms step_avg:58.30ms
step:1335/2330 train_time:77830ms step_avg:58.30ms
step:1336/2330 train_time:77890ms step_avg:58.30ms
step:1337/2330 train_time:77947ms step_avg:58.30ms
step:1338/2330 train_time:78007ms step_avg:58.30ms
step:1339/2330 train_time:78064ms step_avg:58.30ms
step:1340/2330 train_time:78124ms step_avg:58.30ms
step:1341/2330 train_time:78182ms step_avg:58.30ms
step:1342/2330 train_time:78242ms step_avg:58.30ms
step:1343/2330 train_time:78298ms step_avg:58.30ms
step:1344/2330 train_time:78359ms step_avg:58.30ms
step:1345/2330 train_time:78416ms step_avg:58.30ms
step:1346/2330 train_time:78476ms step_avg:58.30ms
step:1347/2330 train_time:78533ms step_avg:58.30ms
step:1348/2330 train_time:78593ms step_avg:58.30ms
step:1349/2330 train_time:78649ms step_avg:58.30ms
step:1350/2330 train_time:78710ms step_avg:58.30ms
step:1351/2330 train_time:78767ms step_avg:58.30ms
step:1352/2330 train_time:78827ms step_avg:58.30ms
step:1353/2330 train_time:78884ms step_avg:58.30ms
step:1354/2330 train_time:78943ms step_avg:58.30ms
step:1355/2330 train_time:79000ms step_avg:58.30ms
step:1356/2330 train_time:79060ms step_avg:58.30ms
step:1357/2330 train_time:79116ms step_avg:58.30ms
step:1358/2330 train_time:79177ms step_avg:58.30ms
step:1359/2330 train_time:79234ms step_avg:58.30ms
step:1360/2330 train_time:79295ms step_avg:58.30ms
step:1361/2330 train_time:79352ms step_avg:58.30ms
step:1362/2330 train_time:79412ms step_avg:58.31ms
step:1363/2330 train_time:79468ms step_avg:58.30ms
step:1364/2330 train_time:79529ms step_avg:58.31ms
step:1365/2330 train_time:79586ms step_avg:58.30ms
step:1366/2330 train_time:79645ms step_avg:58.31ms
step:1367/2330 train_time:79701ms step_avg:58.30ms
step:1368/2330 train_time:79762ms step_avg:58.31ms
step:1369/2330 train_time:79818ms step_avg:58.30ms
step:1370/2330 train_time:79879ms step_avg:58.31ms
step:1371/2330 train_time:79936ms step_avg:58.30ms
step:1372/2330 train_time:79996ms step_avg:58.31ms
step:1373/2330 train_time:80053ms step_avg:58.31ms
step:1374/2330 train_time:80114ms step_avg:58.31ms
step:1375/2330 train_time:80171ms step_avg:58.31ms
step:1376/2330 train_time:80231ms step_avg:58.31ms
step:1377/2330 train_time:80288ms step_avg:58.31ms
step:1378/2330 train_time:80348ms step_avg:58.31ms
step:1379/2330 train_time:80406ms step_avg:58.31ms
step:1380/2330 train_time:80466ms step_avg:58.31ms
step:1381/2330 train_time:80522ms step_avg:58.31ms
step:1382/2330 train_time:80582ms step_avg:58.31ms
step:1383/2330 train_time:80640ms step_avg:58.31ms
step:1384/2330 train_time:80699ms step_avg:58.31ms
step:1385/2330 train_time:80756ms step_avg:58.31ms
step:1386/2330 train_time:80816ms step_avg:58.31ms
step:1387/2330 train_time:80872ms step_avg:58.31ms
step:1388/2330 train_time:80932ms step_avg:58.31ms
step:1389/2330 train_time:80990ms step_avg:58.31ms
step:1390/2330 train_time:81050ms step_avg:58.31ms
step:1391/2330 train_time:81107ms step_avg:58.31ms
step:1392/2330 train_time:81167ms step_avg:58.31ms
step:1393/2330 train_time:81224ms step_avg:58.31ms
step:1394/2330 train_time:81284ms step_avg:58.31ms
step:1395/2330 train_time:81340ms step_avg:58.31ms
step:1396/2330 train_time:81400ms step_avg:58.31ms
step:1397/2330 train_time:81457ms step_avg:58.31ms
step:1398/2330 train_time:81517ms step_avg:58.31ms
step:1399/2330 train_time:81574ms step_avg:58.31ms
step:1400/2330 train_time:81634ms step_avg:58.31ms
step:1401/2330 train_time:81692ms step_avg:58.31ms
step:1402/2330 train_time:81751ms step_avg:58.31ms
step:1403/2330 train_time:81809ms step_avg:58.31ms
step:1404/2330 train_time:81868ms step_avg:58.31ms
step:1405/2330 train_time:81925ms step_avg:58.31ms
step:1406/2330 train_time:81984ms step_avg:58.31ms
step:1407/2330 train_time:82041ms step_avg:58.31ms
step:1408/2330 train_time:82101ms step_avg:58.31ms
step:1409/2330 train_time:82157ms step_avg:58.31ms
step:1410/2330 train_time:82219ms step_avg:58.31ms
step:1411/2330 train_time:82276ms step_avg:58.31ms
step:1412/2330 train_time:82336ms step_avg:58.31ms
step:1413/2330 train_time:82393ms step_avg:58.31ms
step:1414/2330 train_time:82454ms step_avg:58.31ms
step:1415/2330 train_time:82511ms step_avg:58.31ms
step:1416/2330 train_time:82571ms step_avg:58.31ms
step:1417/2330 train_time:82629ms step_avg:58.31ms
step:1418/2330 train_time:82689ms step_avg:58.31ms
step:1419/2330 train_time:82746ms step_avg:58.31ms
step:1420/2330 train_time:82806ms step_avg:58.31ms
step:1421/2330 train_time:82863ms step_avg:58.31ms
step:1422/2330 train_time:82922ms step_avg:58.31ms
step:1423/2330 train_time:82980ms step_avg:58.31ms
step:1424/2330 train_time:83039ms step_avg:58.31ms
step:1425/2330 train_time:83095ms step_avg:58.31ms
step:1426/2330 train_time:83156ms step_avg:58.31ms
step:1427/2330 train_time:83213ms step_avg:58.31ms
step:1428/2330 train_time:83273ms step_avg:58.31ms
step:1429/2330 train_time:83331ms step_avg:58.31ms
step:1430/2330 train_time:83391ms step_avg:58.32ms
step:1431/2330 train_time:83449ms step_avg:58.32ms
step:1432/2330 train_time:83508ms step_avg:58.32ms
step:1433/2330 train_time:83565ms step_avg:58.32ms
step:1434/2330 train_time:83625ms step_avg:58.32ms
step:1435/2330 train_time:83681ms step_avg:58.31ms
step:1436/2330 train_time:83742ms step_avg:58.32ms
step:1437/2330 train_time:83799ms step_avg:58.32ms
step:1438/2330 train_time:83858ms step_avg:58.32ms
step:1439/2330 train_time:83916ms step_avg:58.32ms
step:1440/2330 train_time:83976ms step_avg:58.32ms
step:1441/2330 train_time:84032ms step_avg:58.32ms
step:1442/2330 train_time:84092ms step_avg:58.32ms
step:1443/2330 train_time:84149ms step_avg:58.32ms
step:1444/2330 train_time:84210ms step_avg:58.32ms
step:1445/2330 train_time:84268ms step_avg:58.32ms
step:1446/2330 train_time:84327ms step_avg:58.32ms
step:1447/2330 train_time:84385ms step_avg:58.32ms
step:1448/2330 train_time:84444ms step_avg:58.32ms
step:1449/2330 train_time:84501ms step_avg:58.32ms
step:1450/2330 train_time:84561ms step_avg:58.32ms
step:1451/2330 train_time:84618ms step_avg:58.32ms
step:1452/2330 train_time:84677ms step_avg:58.32ms
step:1453/2330 train_time:84734ms step_avg:58.32ms
step:1454/2330 train_time:84795ms step_avg:58.32ms
step:1455/2330 train_time:84851ms step_avg:58.32ms
step:1456/2330 train_time:84913ms step_avg:58.32ms
step:1457/2330 train_time:84970ms step_avg:58.32ms
step:1458/2330 train_time:85030ms step_avg:58.32ms
step:1459/2330 train_time:85088ms step_avg:58.32ms
step:1460/2330 train_time:85148ms step_avg:58.32ms
step:1461/2330 train_time:85204ms step_avg:58.32ms
step:1462/2330 train_time:85265ms step_avg:58.32ms
step:1463/2330 train_time:85322ms step_avg:58.32ms
step:1464/2330 train_time:85382ms step_avg:58.32ms
step:1465/2330 train_time:85440ms step_avg:58.32ms
step:1466/2330 train_time:85499ms step_avg:58.32ms
step:1467/2330 train_time:85556ms step_avg:58.32ms
step:1468/2330 train_time:85615ms step_avg:58.32ms
step:1469/2330 train_time:85673ms step_avg:58.32ms
step:1470/2330 train_time:85733ms step_avg:58.32ms
step:1471/2330 train_time:85789ms step_avg:58.32ms
step:1472/2330 train_time:85850ms step_avg:58.32ms
step:1473/2330 train_time:85907ms step_avg:58.32ms
step:1474/2330 train_time:85967ms step_avg:58.32ms
step:1475/2330 train_time:86024ms step_avg:58.32ms
step:1476/2330 train_time:86084ms step_avg:58.32ms
step:1477/2330 train_time:86141ms step_avg:58.32ms
step:1478/2330 train_time:86200ms step_avg:58.32ms
step:1479/2330 train_time:86257ms step_avg:58.32ms
step:1480/2330 train_time:86317ms step_avg:58.32ms
step:1481/2330 train_time:86375ms step_avg:58.32ms
step:1482/2330 train_time:86435ms step_avg:58.32ms
step:1483/2330 train_time:86492ms step_avg:58.32ms
step:1484/2330 train_time:86552ms step_avg:58.32ms
step:1485/2330 train_time:86609ms step_avg:58.32ms
step:1486/2330 train_time:86668ms step_avg:58.32ms
step:1487/2330 train_time:86725ms step_avg:58.32ms
step:1488/2330 train_time:86785ms step_avg:58.32ms
step:1489/2330 train_time:86842ms step_avg:58.32ms
step:1490/2330 train_time:86902ms step_avg:58.32ms
step:1491/2330 train_time:86958ms step_avg:58.32ms
step:1492/2330 train_time:87019ms step_avg:58.32ms
step:1493/2330 train_time:87076ms step_avg:58.32ms
step:1494/2330 train_time:87137ms step_avg:58.32ms
step:1495/2330 train_time:87193ms step_avg:58.32ms
step:1496/2330 train_time:87254ms step_avg:58.32ms
step:1497/2330 train_time:87311ms step_avg:58.32ms
step:1498/2330 train_time:87371ms step_avg:58.33ms
step:1499/2330 train_time:87428ms step_avg:58.32ms
step:1500/2330 train_time:87488ms step_avg:58.33ms
step:1500/2330 val_loss:3.9051 train_time:87568ms step_avg:58.38ms
step:1501/2330 train_time:87586ms step_avg:58.35ms
step:1502/2330 train_time:87608ms step_avg:58.33ms
step:1503/2330 train_time:87666ms step_avg:58.33ms
step:1504/2330 train_time:87732ms step_avg:58.33ms
step:1505/2330 train_time:87789ms step_avg:58.33ms
step:1506/2330 train_time:87854ms step_avg:58.34ms
step:1507/2330 train_time:87910ms step_avg:58.33ms
step:1508/2330 train_time:87970ms step_avg:58.34ms
step:1509/2330 train_time:88026ms step_avg:58.33ms
step:1510/2330 train_time:88086ms step_avg:58.34ms
step:1511/2330 train_time:88142ms step_avg:58.33ms
step:1512/2330 train_time:88202ms step_avg:58.33ms
step:1513/2330 train_time:88258ms step_avg:58.33ms
step:1514/2330 train_time:88317ms step_avg:58.33ms
step:1515/2330 train_time:88374ms step_avg:58.33ms
step:1516/2330 train_time:88432ms step_avg:58.33ms
step:1517/2330 train_time:88489ms step_avg:58.33ms
step:1518/2330 train_time:88549ms step_avg:58.33ms
step:1519/2330 train_time:88610ms step_avg:58.33ms
step:1520/2330 train_time:88670ms step_avg:58.34ms
step:1521/2330 train_time:88728ms step_avg:58.34ms
step:1522/2330 train_time:88790ms step_avg:58.34ms
step:1523/2330 train_time:88847ms step_avg:58.34ms
step:1524/2330 train_time:88907ms step_avg:58.34ms
step:1525/2330 train_time:88964ms step_avg:58.34ms
step:1526/2330 train_time:89024ms step_avg:58.34ms
step:1527/2330 train_time:89081ms step_avg:58.34ms
step:1528/2330 train_time:89141ms step_avg:58.34ms
step:1529/2330 train_time:89198ms step_avg:58.34ms
step:1530/2330 train_time:89258ms step_avg:58.34ms
step:1531/2330 train_time:89314ms step_avg:58.34ms
step:1532/2330 train_time:89376ms step_avg:58.34ms
step:1533/2330 train_time:89432ms step_avg:58.34ms
step:1534/2330 train_time:89492ms step_avg:58.34ms
step:1535/2330 train_time:89550ms step_avg:58.34ms
step:1536/2330 train_time:89611ms step_avg:58.34ms
step:1537/2330 train_time:89670ms step_avg:58.34ms
step:1538/2330 train_time:89731ms step_avg:58.34ms
step:1539/2330 train_time:89789ms step_avg:58.34ms
step:1540/2330 train_time:89850ms step_avg:58.34ms
step:1541/2330 train_time:89908ms step_avg:58.34ms
step:1542/2330 train_time:89968ms step_avg:58.34ms
step:1543/2330 train_time:90025ms step_avg:58.34ms
step:1544/2330 train_time:90086ms step_avg:58.35ms
step:1545/2330 train_time:90143ms step_avg:58.35ms
step:1546/2330 train_time:90204ms step_avg:58.35ms
step:1547/2330 train_time:90261ms step_avg:58.35ms
step:1548/2330 train_time:90321ms step_avg:58.35ms
step:1549/2330 train_time:90377ms step_avg:58.35ms
step:1550/2330 train_time:90439ms step_avg:58.35ms
step:1551/2330 train_time:90496ms step_avg:58.35ms
step:1552/2330 train_time:90558ms step_avg:58.35ms
step:1553/2330 train_time:90615ms step_avg:58.35ms
step:1554/2330 train_time:90677ms step_avg:58.35ms
step:1555/2330 train_time:90735ms step_avg:58.35ms
step:1556/2330 train_time:90798ms step_avg:58.35ms
step:1557/2330 train_time:90855ms step_avg:58.35ms
step:1558/2330 train_time:90917ms step_avg:58.36ms
step:1559/2330 train_time:90975ms step_avg:58.35ms
step:1560/2330 train_time:91036ms step_avg:58.36ms
step:1561/2330 train_time:91093ms step_avg:58.36ms
step:1562/2330 train_time:91153ms step_avg:58.36ms
step:1563/2330 train_time:91211ms step_avg:58.36ms
step:1564/2330 train_time:91271ms step_avg:58.36ms
step:1565/2330 train_time:91328ms step_avg:58.36ms
step:1566/2330 train_time:91389ms step_avg:58.36ms
step:1567/2330 train_time:91446ms step_avg:58.36ms
step:1568/2330 train_time:91508ms step_avg:58.36ms
step:1569/2330 train_time:91564ms step_avg:58.36ms
step:1570/2330 train_time:91626ms step_avg:58.36ms
step:1571/2330 train_time:91683ms step_avg:58.36ms
step:1572/2330 train_time:91745ms step_avg:58.36ms
step:1573/2330 train_time:91802ms step_avg:58.36ms
step:1574/2330 train_time:91864ms step_avg:58.36ms
step:1575/2330 train_time:91920ms step_avg:58.36ms
step:1576/2330 train_time:91983ms step_avg:58.37ms
step:1577/2330 train_time:92040ms step_avg:58.36ms
step:1578/2330 train_time:92101ms step_avg:58.37ms
step:1579/2330 train_time:92158ms step_avg:58.36ms
step:1580/2330 train_time:92220ms step_avg:58.37ms
step:1581/2330 train_time:92276ms step_avg:58.37ms
step:1582/2330 train_time:92338ms step_avg:58.37ms
step:1583/2330 train_time:92395ms step_avg:58.37ms
step:1584/2330 train_time:92456ms step_avg:58.37ms
step:1585/2330 train_time:92514ms step_avg:58.37ms
step:1586/2330 train_time:92575ms step_avg:58.37ms
step:1587/2330 train_time:92632ms step_avg:58.37ms
step:1588/2330 train_time:92692ms step_avg:58.37ms
step:1589/2330 train_time:92750ms step_avg:58.37ms
step:1590/2330 train_time:92811ms step_avg:58.37ms
step:1591/2330 train_time:92869ms step_avg:58.37ms
step:1592/2330 train_time:92929ms step_avg:58.37ms
step:1593/2330 train_time:92986ms step_avg:58.37ms
step:1594/2330 train_time:93047ms step_avg:58.37ms
step:1595/2330 train_time:93105ms step_avg:58.37ms
step:1596/2330 train_time:93166ms step_avg:58.37ms
step:1597/2330 train_time:93223ms step_avg:58.37ms
step:1598/2330 train_time:93284ms step_avg:58.38ms
step:1599/2330 train_time:93340ms step_avg:58.37ms
step:1600/2330 train_time:93403ms step_avg:58.38ms
step:1601/2330 train_time:93460ms step_avg:58.38ms
step:1602/2330 train_time:93520ms step_avg:58.38ms
step:1603/2330 train_time:93576ms step_avg:58.38ms
step:1604/2330 train_time:93639ms step_avg:58.38ms
step:1605/2330 train_time:93696ms step_avg:58.38ms
step:1606/2330 train_time:93757ms step_avg:58.38ms
step:1607/2330 train_time:93814ms step_avg:58.38ms
step:1608/2330 train_time:93876ms step_avg:58.38ms
step:1609/2330 train_time:93934ms step_avg:58.38ms
step:1610/2330 train_time:93996ms step_avg:58.38ms
step:1611/2330 train_time:94053ms step_avg:58.38ms
step:1612/2330 train_time:94114ms step_avg:58.38ms
step:1613/2330 train_time:94171ms step_avg:58.38ms
step:1614/2330 train_time:94232ms step_avg:58.38ms
step:1615/2330 train_time:94290ms step_avg:58.38ms
step:1616/2330 train_time:94350ms step_avg:58.39ms
step:1617/2330 train_time:94408ms step_avg:58.38ms
step:1618/2330 train_time:94468ms step_avg:58.39ms
step:1619/2330 train_time:94525ms step_avg:58.38ms
step:1620/2330 train_time:94586ms step_avg:58.39ms
step:1621/2330 train_time:94642ms step_avg:58.39ms
step:1622/2330 train_time:94704ms step_avg:58.39ms
step:1623/2330 train_time:94761ms step_avg:58.39ms
step:1624/2330 train_time:94821ms step_avg:58.39ms
step:1625/2330 train_time:94879ms step_avg:58.39ms
step:1626/2330 train_time:94941ms step_avg:58.39ms
step:1627/2330 train_time:94997ms step_avg:58.39ms
step:1628/2330 train_time:95061ms step_avg:58.39ms
step:1629/2330 train_time:95117ms step_avg:58.39ms
step:1630/2330 train_time:95179ms step_avg:58.39ms
step:1631/2330 train_time:95236ms step_avg:58.39ms
step:1632/2330 train_time:95299ms step_avg:58.39ms
step:1633/2330 train_time:95356ms step_avg:58.39ms
step:1634/2330 train_time:95418ms step_avg:58.40ms
step:1635/2330 train_time:95475ms step_avg:58.39ms
step:1636/2330 train_time:95536ms step_avg:58.40ms
step:1637/2330 train_time:95594ms step_avg:58.40ms
step:1638/2330 train_time:95655ms step_avg:58.40ms
step:1639/2330 train_time:95712ms step_avg:58.40ms
step:1640/2330 train_time:95773ms step_avg:58.40ms
step:1641/2330 train_time:95830ms step_avg:58.40ms
step:1642/2330 train_time:95891ms step_avg:58.40ms
step:1643/2330 train_time:95948ms step_avg:58.40ms
step:1644/2330 train_time:96009ms step_avg:58.40ms
step:1645/2330 train_time:96066ms step_avg:58.40ms
step:1646/2330 train_time:96127ms step_avg:58.40ms
step:1647/2330 train_time:96184ms step_avg:58.40ms
step:1648/2330 train_time:96246ms step_avg:58.40ms
step:1649/2330 train_time:96303ms step_avg:58.40ms
step:1650/2330 train_time:96363ms step_avg:58.40ms
step:1651/2330 train_time:96419ms step_avg:58.40ms
step:1652/2330 train_time:96481ms step_avg:58.40ms
step:1653/2330 train_time:96538ms step_avg:58.40ms
step:1654/2330 train_time:96601ms step_avg:58.40ms
step:1655/2330 train_time:96657ms step_avg:58.40ms
step:1656/2330 train_time:96720ms step_avg:58.41ms
step:1657/2330 train_time:96777ms step_avg:58.40ms
step:1658/2330 train_time:96840ms step_avg:58.41ms
step:1659/2330 train_time:96896ms step_avg:58.41ms
step:1660/2330 train_time:96958ms step_avg:58.41ms
step:1661/2330 train_time:97015ms step_avg:58.41ms
step:1662/2330 train_time:97078ms step_avg:58.41ms
step:1663/2330 train_time:97135ms step_avg:58.41ms
step:1664/2330 train_time:97195ms step_avg:58.41ms
step:1665/2330 train_time:97254ms step_avg:58.41ms
step:1666/2330 train_time:97314ms step_avg:58.41ms
step:1667/2330 train_time:97372ms step_avg:58.41ms
step:1668/2330 train_time:97431ms step_avg:58.41ms
step:1669/2330 train_time:97488ms step_avg:58.41ms
step:1670/2330 train_time:97550ms step_avg:58.41ms
step:1671/2330 train_time:97608ms step_avg:58.41ms
step:1672/2330 train_time:97669ms step_avg:58.41ms
step:1673/2330 train_time:97725ms step_avg:58.41ms
step:1674/2330 train_time:97787ms step_avg:58.42ms
step:1675/2330 train_time:97843ms step_avg:58.41ms
step:1676/2330 train_time:97905ms step_avg:58.42ms
step:1677/2330 train_time:97961ms step_avg:58.41ms
step:1678/2330 train_time:98023ms step_avg:58.42ms
step:1679/2330 train_time:98079ms step_avg:58.42ms
step:1680/2330 train_time:98141ms step_avg:58.42ms
step:1681/2330 train_time:98199ms step_avg:58.42ms
step:1682/2330 train_time:98261ms step_avg:58.42ms
step:1683/2330 train_time:98318ms step_avg:58.42ms
step:1684/2330 train_time:98382ms step_avg:58.42ms
step:1685/2330 train_time:98438ms step_avg:58.42ms
step:1686/2330 train_time:98500ms step_avg:58.42ms
step:1687/2330 train_time:98557ms step_avg:58.42ms
step:1688/2330 train_time:98619ms step_avg:58.42ms
step:1689/2330 train_time:98677ms step_avg:58.42ms
step:1690/2330 train_time:98738ms step_avg:58.42ms
step:1691/2330 train_time:98795ms step_avg:58.42ms
step:1692/2330 train_time:98858ms step_avg:58.43ms
step:1693/2330 train_time:98915ms step_avg:58.43ms
step:1694/2330 train_time:98975ms step_avg:58.43ms
step:1695/2330 train_time:99032ms step_avg:58.43ms
step:1696/2330 train_time:99093ms step_avg:58.43ms
step:1697/2330 train_time:99151ms step_avg:58.43ms
step:1698/2330 train_time:99212ms step_avg:58.43ms
step:1699/2330 train_time:99270ms step_avg:58.43ms
step:1700/2330 train_time:99331ms step_avg:58.43ms
step:1701/2330 train_time:99388ms step_avg:58.43ms
step:1702/2330 train_time:99450ms step_avg:58.43ms
step:1703/2330 train_time:99507ms step_avg:58.43ms
step:1704/2330 train_time:99568ms step_avg:58.43ms
step:1705/2330 train_time:99624ms step_avg:58.43ms
step:1706/2330 train_time:99686ms step_avg:58.43ms
step:1707/2330 train_time:99743ms step_avg:58.43ms
step:1708/2330 train_time:99804ms step_avg:58.43ms
step:1709/2330 train_time:99861ms step_avg:58.43ms
step:1710/2330 train_time:99922ms step_avg:58.43ms
step:1711/2330 train_time:99978ms step_avg:58.43ms
step:1712/2330 train_time:100041ms step_avg:58.44ms
step:1713/2330 train_time:100098ms step_avg:58.43ms
step:1714/2330 train_time:100162ms step_avg:58.44ms
step:1715/2330 train_time:100218ms step_avg:58.44ms
step:1716/2330 train_time:100282ms step_avg:58.44ms
step:1717/2330 train_time:100338ms step_avg:58.44ms
step:1718/2330 train_time:100401ms step_avg:58.44ms
step:1719/2330 train_time:100458ms step_avg:58.44ms
step:1720/2330 train_time:100519ms step_avg:58.44ms
step:1721/2330 train_time:100576ms step_avg:58.44ms
step:1722/2330 train_time:100638ms step_avg:58.44ms
step:1723/2330 train_time:100695ms step_avg:58.44ms
step:1724/2330 train_time:100757ms step_avg:58.44ms
step:1725/2330 train_time:100814ms step_avg:58.44ms
step:1726/2330 train_time:100875ms step_avg:58.44ms
step:1727/2330 train_time:100932ms step_avg:58.44ms
step:1728/2330 train_time:100992ms step_avg:58.44ms
step:1729/2330 train_time:101051ms step_avg:58.44ms
step:1730/2330 train_time:101111ms step_avg:58.45ms
step:1731/2330 train_time:101169ms step_avg:58.45ms
step:1732/2330 train_time:101229ms step_avg:58.45ms
step:1733/2330 train_time:101286ms step_avg:58.45ms
step:1734/2330 train_time:101348ms step_avg:58.45ms
step:1735/2330 train_time:101405ms step_avg:58.45ms
step:1736/2330 train_time:101467ms step_avg:58.45ms
step:1737/2330 train_time:101524ms step_avg:58.45ms
step:1738/2330 train_time:101584ms step_avg:58.45ms
step:1739/2330 train_time:101641ms step_avg:58.45ms
step:1740/2330 train_time:101703ms step_avg:58.45ms
step:1741/2330 train_time:101759ms step_avg:58.45ms
step:1742/2330 train_time:101821ms step_avg:58.45ms
step:1743/2330 train_time:101877ms step_avg:58.45ms
step:1744/2330 train_time:101939ms step_avg:58.45ms
step:1745/2330 train_time:101996ms step_avg:58.45ms
step:1746/2330 train_time:102058ms step_avg:58.45ms
step:1747/2330 train_time:102115ms step_avg:58.45ms
step:1748/2330 train_time:102178ms step_avg:58.45ms
step:1749/2330 train_time:102235ms step_avg:58.45ms
step:1750/2330 train_time:102297ms step_avg:58.46ms
step:1750/2330 val_loss:3.8203 train_time:102380ms step_avg:58.50ms
step:1751/2330 train_time:102398ms step_avg:58.48ms
step:1752/2330 train_time:102419ms step_avg:58.46ms
step:1753/2330 train_time:102478ms step_avg:58.46ms
step:1754/2330 train_time:102541ms step_avg:58.46ms
step:1755/2330 train_time:102598ms step_avg:58.46ms
step:1756/2330 train_time:102662ms step_avg:58.46ms
step:1757/2330 train_time:102718ms step_avg:58.46ms
step:1758/2330 train_time:102780ms step_avg:58.46ms
step:1759/2330 train_time:102837ms step_avg:58.46ms
step:1760/2330 train_time:102897ms step_avg:58.46ms
step:1761/2330 train_time:102954ms step_avg:58.46ms
step:1762/2330 train_time:103014ms step_avg:58.46ms
step:1763/2330 train_time:103071ms step_avg:58.46ms
step:1764/2330 train_time:103130ms step_avg:58.46ms
step:1765/2330 train_time:103187ms step_avg:58.46ms
step:1766/2330 train_time:103247ms step_avg:58.46ms
step:1767/2330 train_time:103304ms step_avg:58.46ms
step:1768/2330 train_time:103368ms step_avg:58.47ms
step:1769/2330 train_time:103427ms step_avg:58.47ms
step:1770/2330 train_time:103490ms step_avg:58.47ms
step:1771/2330 train_time:103549ms step_avg:58.47ms
step:1772/2330 train_time:103610ms step_avg:58.47ms
step:1773/2330 train_time:103667ms step_avg:58.47ms
step:1774/2330 train_time:103728ms step_avg:58.47ms
step:1775/2330 train_time:103787ms step_avg:58.47ms
step:1776/2330 train_time:103847ms step_avg:58.47ms
step:1777/2330 train_time:103905ms step_avg:58.47ms
step:1778/2330 train_time:103966ms step_avg:58.47ms
step:1779/2330 train_time:104023ms step_avg:58.47ms
step:1780/2330 train_time:104083ms step_avg:58.47ms
step:1781/2330 train_time:104140ms step_avg:58.47ms
step:1782/2330 train_time:104200ms step_avg:58.47ms
step:1783/2330 train_time:104257ms step_avg:58.47ms
step:1784/2330 train_time:104318ms step_avg:58.47ms
step:1785/2330 train_time:104375ms step_avg:58.47ms
step:1786/2330 train_time:104438ms step_avg:58.48ms
step:1787/2330 train_time:104495ms step_avg:58.48ms
step:1788/2330 train_time:104558ms step_avg:58.48ms
step:1789/2330 train_time:104615ms step_avg:58.48ms
step:1790/2330 train_time:104678ms step_avg:58.48ms
step:1791/2330 train_time:104735ms step_avg:58.48ms
step:1792/2330 train_time:104796ms step_avg:58.48ms
step:1793/2330 train_time:104853ms step_avg:58.48ms
step:1794/2330 train_time:104914ms step_avg:58.48ms
step:1795/2330 train_time:104971ms step_avg:58.48ms
step:1796/2330 train_time:105033ms step_avg:58.48ms
step:1797/2330 train_time:105090ms step_avg:58.48ms
step:1798/2330 train_time:105151ms step_avg:58.48ms
step:1799/2330 train_time:105208ms step_avg:58.48ms
step:1800/2330 train_time:105268ms step_avg:58.48ms
step:1801/2330 train_time:105326ms step_avg:58.48ms
step:1802/2330 train_time:105387ms step_avg:58.48ms
step:1803/2330 train_time:105445ms step_avg:58.48ms
step:1804/2330 train_time:105506ms step_avg:58.48ms
step:1805/2330 train_time:105564ms step_avg:58.48ms
step:1806/2330 train_time:105626ms step_avg:58.49ms
step:1807/2330 train_time:105683ms step_avg:58.49ms
step:1808/2330 train_time:105743ms step_avg:58.49ms
step:1809/2330 train_time:105800ms step_avg:58.49ms
step:1810/2330 train_time:105861ms step_avg:58.49ms
step:1811/2330 train_time:105918ms step_avg:58.49ms
step:1812/2330 train_time:105978ms step_avg:58.49ms
step:1813/2330 train_time:106034ms step_avg:58.49ms
step:1814/2330 train_time:106096ms step_avg:58.49ms
step:1815/2330 train_time:106153ms step_avg:58.49ms
step:1816/2330 train_time:106214ms step_avg:58.49ms
step:1817/2330 train_time:106270ms step_avg:58.49ms
step:1818/2330 train_time:106333ms step_avg:58.49ms
step:1819/2330 train_time:106391ms step_avg:58.49ms
step:1820/2330 train_time:106451ms step_avg:58.49ms
step:1821/2330 train_time:106509ms step_avg:58.49ms
step:1822/2330 train_time:106570ms step_avg:58.49ms
step:1823/2330 train_time:106628ms step_avg:58.49ms
step:1824/2330 train_time:106689ms step_avg:58.49ms
step:1825/2330 train_time:106747ms step_avg:58.49ms
step:1826/2330 train_time:106807ms step_avg:58.49ms
step:1827/2330 train_time:106864ms step_avg:58.49ms
step:1828/2330 train_time:106926ms step_avg:58.49ms
step:1829/2330 train_time:106983ms step_avg:58.49ms
step:1830/2330 train_time:107043ms step_avg:58.49ms
step:1831/2330 train_time:107101ms step_avg:58.49ms
step:1832/2330 train_time:107160ms step_avg:58.49ms
step:1833/2330 train_time:107217ms step_avg:58.49ms
step:1834/2330 train_time:107279ms step_avg:58.49ms
step:1835/2330 train_time:107335ms step_avg:58.49ms
step:1836/2330 train_time:107398ms step_avg:58.50ms
step:1837/2330 train_time:107455ms step_avg:58.49ms
step:1838/2330 train_time:107516ms step_avg:58.50ms
step:1839/2330 train_time:107572ms step_avg:58.50ms
step:1840/2330 train_time:107636ms step_avg:58.50ms
step:1841/2330 train_time:107694ms step_avg:58.50ms
step:1842/2330 train_time:107756ms step_avg:58.50ms
step:1843/2330 train_time:107812ms step_avg:58.50ms
step:1844/2330 train_time:107874ms step_avg:58.50ms
step:1845/2330 train_time:107931ms step_avg:58.50ms
step:1846/2330 train_time:107993ms step_avg:58.50ms
step:1847/2330 train_time:108050ms step_avg:58.50ms
step:1848/2330 train_time:108111ms step_avg:58.50ms
step:1849/2330 train_time:108169ms step_avg:58.50ms
step:1850/2330 train_time:108228ms step_avg:58.50ms
step:1851/2330 train_time:108286ms step_avg:58.50ms
step:1852/2330 train_time:108347ms step_avg:58.50ms
step:1853/2330 train_time:108404ms step_avg:58.50ms
step:1854/2330 train_time:108464ms step_avg:58.50ms
step:1855/2330 train_time:108521ms step_avg:58.50ms
step:1856/2330 train_time:108583ms step_avg:58.50ms
step:1857/2330 train_time:108640ms step_avg:58.50ms
step:1858/2330 train_time:108701ms step_avg:58.50ms
step:1859/2330 train_time:108758ms step_avg:58.50ms
step:1860/2330 train_time:108819ms step_avg:58.50ms
step:1861/2330 train_time:108876ms step_avg:58.50ms
step:1862/2330 train_time:108938ms step_avg:58.51ms
step:1863/2330 train_time:108995ms step_avg:58.51ms
step:1864/2330 train_time:109056ms step_avg:58.51ms
step:1865/2330 train_time:109113ms step_avg:58.51ms
step:1866/2330 train_time:109175ms step_avg:58.51ms
step:1867/2330 train_time:109231ms step_avg:58.51ms
step:1868/2330 train_time:109293ms step_avg:58.51ms
step:1869/2330 train_time:109350ms step_avg:58.51ms
step:1870/2330 train_time:109412ms step_avg:58.51ms
step:1871/2330 train_time:109469ms step_avg:58.51ms
step:1872/2330 train_time:109530ms step_avg:58.51ms
step:1873/2330 train_time:109589ms step_avg:58.51ms
step:1874/2330 train_time:109649ms step_avg:58.51ms
step:1875/2330 train_time:109707ms step_avg:58.51ms
step:1876/2330 train_time:109768ms step_avg:58.51ms
step:1877/2330 train_time:109825ms step_avg:58.51ms
step:1878/2330 train_time:109885ms step_avg:58.51ms
step:1879/2330 train_time:109942ms step_avg:58.51ms
step:1880/2330 train_time:110002ms step_avg:58.51ms
step:1881/2330 train_time:110059ms step_avg:58.51ms
step:1882/2330 train_time:110121ms step_avg:58.51ms
step:1883/2330 train_time:110178ms step_avg:58.51ms
step:1884/2330 train_time:110238ms step_avg:58.51ms
step:1885/2330 train_time:110296ms step_avg:58.51ms
step:1886/2330 train_time:110357ms step_avg:58.51ms
step:1887/2330 train_time:110413ms step_avg:58.51ms
step:1888/2330 train_time:110475ms step_avg:58.51ms
step:1889/2330 train_time:110531ms step_avg:58.51ms
step:1890/2330 train_time:110594ms step_avg:58.52ms
step:1891/2330 train_time:110651ms step_avg:58.51ms
step:1892/2330 train_time:110713ms step_avg:58.52ms
step:1893/2330 train_time:110771ms step_avg:58.52ms
step:1894/2330 train_time:110833ms step_avg:58.52ms
step:1895/2330 train_time:110890ms step_avg:58.52ms
step:1896/2330 train_time:110950ms step_avg:58.52ms
step:1897/2330 train_time:111007ms step_avg:58.52ms
step:1898/2330 train_time:111068ms step_avg:58.52ms
step:1899/2330 train_time:111126ms step_avg:58.52ms
step:1900/2330 train_time:111186ms step_avg:58.52ms
step:1901/2330 train_time:111243ms step_avg:58.52ms
step:1902/2330 train_time:111304ms step_avg:58.52ms
step:1903/2330 train_time:111361ms step_avg:58.52ms
step:1904/2330 train_time:111421ms step_avg:58.52ms
step:1905/2330 train_time:111478ms step_avg:58.52ms
step:1906/2330 train_time:111540ms step_avg:58.52ms
step:1907/2330 train_time:111597ms step_avg:58.52ms
step:1908/2330 train_time:111658ms step_avg:58.52ms
step:1909/2330 train_time:111714ms step_avg:58.52ms
step:1910/2330 train_time:111777ms step_avg:58.52ms
step:1911/2330 train_time:111834ms step_avg:58.52ms
step:1912/2330 train_time:111895ms step_avg:58.52ms
step:1913/2330 train_time:111952ms step_avg:58.52ms
step:1914/2330 train_time:112014ms step_avg:58.52ms
step:1915/2330 train_time:112071ms step_avg:58.52ms
step:1916/2330 train_time:112132ms step_avg:58.52ms
step:1917/2330 train_time:112190ms step_avg:58.52ms
step:1918/2330 train_time:112251ms step_avg:58.52ms
step:1919/2330 train_time:112308ms step_avg:58.52ms
step:1920/2330 train_time:112368ms step_avg:58.53ms
step:1921/2330 train_time:112427ms step_avg:58.53ms
step:1922/2330 train_time:112487ms step_avg:58.53ms
step:1923/2330 train_time:112546ms step_avg:58.53ms
step:1924/2330 train_time:112607ms step_avg:58.53ms
step:1925/2330 train_time:112665ms step_avg:58.53ms
step:1926/2330 train_time:112725ms step_avg:58.53ms
step:1927/2330 train_time:112783ms step_avg:58.53ms
step:1928/2330 train_time:112843ms step_avg:58.53ms
step:1929/2330 train_time:112900ms step_avg:58.53ms
step:1930/2330 train_time:112962ms step_avg:58.53ms
step:1931/2330 train_time:113019ms step_avg:58.53ms
step:1932/2330 train_time:113080ms step_avg:58.53ms
step:1933/2330 train_time:113136ms step_avg:58.53ms
step:1934/2330 train_time:113198ms step_avg:58.53ms
step:1935/2330 train_time:113255ms step_avg:58.53ms
step:1936/2330 train_time:113317ms step_avg:58.53ms
step:1937/2330 train_time:113374ms step_avg:58.53ms
step:1938/2330 train_time:113435ms step_avg:58.53ms
step:1939/2330 train_time:113492ms step_avg:58.53ms
step:1940/2330 train_time:113553ms step_avg:58.53ms
step:1941/2330 train_time:113610ms step_avg:58.53ms
step:1942/2330 train_time:113672ms step_avg:58.53ms
step:1943/2330 train_time:113729ms step_avg:58.53ms
step:1944/2330 train_time:113790ms step_avg:58.53ms
step:1945/2330 train_time:113848ms step_avg:58.53ms
step:1946/2330 train_time:113908ms step_avg:58.53ms
step:1947/2330 train_time:113967ms step_avg:58.53ms
step:1948/2330 train_time:114027ms step_avg:58.54ms
step:1949/2330 train_time:114086ms step_avg:58.54ms
step:1950/2330 train_time:114146ms step_avg:58.54ms
step:1951/2330 train_time:114205ms step_avg:58.54ms
step:1952/2330 train_time:114265ms step_avg:58.54ms
step:1953/2330 train_time:114323ms step_avg:58.54ms
step:1954/2330 train_time:114383ms step_avg:58.54ms
step:1955/2330 train_time:114441ms step_avg:58.54ms
step:1956/2330 train_time:114502ms step_avg:58.54ms
step:1957/2330 train_time:114559ms step_avg:58.54ms
step:1958/2330 train_time:114619ms step_avg:58.54ms
step:1959/2330 train_time:114676ms step_avg:58.54ms
step:1960/2330 train_time:114738ms step_avg:58.54ms
step:1961/2330 train_time:114794ms step_avg:58.54ms
step:1962/2330 train_time:114856ms step_avg:58.54ms
step:1963/2330 train_time:114912ms step_avg:58.54ms
step:1964/2330 train_time:114975ms step_avg:58.54ms
step:1965/2330 train_time:115032ms step_avg:58.54ms
step:1966/2330 train_time:115094ms step_avg:58.54ms
step:1967/2330 train_time:115151ms step_avg:58.54ms
step:1968/2330 train_time:115213ms step_avg:58.54ms
step:1969/2330 train_time:115271ms step_avg:58.54ms
step:1970/2330 train_time:115331ms step_avg:58.54ms
step:1971/2330 train_time:115389ms step_avg:58.54ms
step:1972/2330 train_time:115450ms step_avg:58.54ms
step:1973/2330 train_time:115508ms step_avg:58.54ms
step:1974/2330 train_time:115568ms step_avg:58.54ms
step:1975/2330 train_time:115625ms step_avg:58.54ms
step:1976/2330 train_time:115685ms step_avg:58.54ms
step:1977/2330 train_time:115742ms step_avg:58.54ms
step:1978/2330 train_time:115804ms step_avg:58.55ms
step:1979/2330 train_time:115861ms step_avg:58.55ms
step:1980/2330 train_time:115923ms step_avg:58.55ms
step:1981/2330 train_time:115979ms step_avg:58.55ms
step:1982/2330 train_time:116042ms step_avg:58.55ms
step:1983/2330 train_time:116099ms step_avg:58.55ms
step:1984/2330 train_time:116160ms step_avg:58.55ms
step:1985/2330 train_time:116216ms step_avg:58.55ms
step:1986/2330 train_time:116278ms step_avg:58.55ms
step:1987/2330 train_time:116334ms step_avg:58.55ms
step:1988/2330 train_time:116396ms step_avg:58.55ms
step:1989/2330 train_time:116453ms step_avg:58.55ms
step:1990/2330 train_time:116514ms step_avg:58.55ms
step:1991/2330 train_time:116571ms step_avg:58.55ms
step:1992/2330 train_time:116633ms step_avg:58.55ms
step:1993/2330 train_time:116691ms step_avg:58.55ms
step:1994/2330 train_time:116752ms step_avg:58.55ms
step:1995/2330 train_time:116810ms step_avg:58.55ms
step:1996/2330 train_time:116871ms step_avg:58.55ms
step:1997/2330 train_time:116928ms step_avg:58.55ms
step:1998/2330 train_time:116990ms step_avg:58.55ms
step:1999/2330 train_time:117047ms step_avg:58.55ms
step:2000/2330 train_time:117107ms step_avg:58.55ms
step:2000/2330 val_loss:3.7574 train_time:117189ms step_avg:58.59ms
step:2001/2330 train_time:117207ms step_avg:58.57ms
step:2002/2330 train_time:117230ms step_avg:58.56ms
step:2003/2330 train_time:117291ms step_avg:58.56ms
step:2004/2330 train_time:117356ms step_avg:58.56ms
step:2005/2330 train_time:117415ms step_avg:58.56ms
step:2006/2330 train_time:117476ms step_avg:58.56ms
step:2007/2330 train_time:117533ms step_avg:58.56ms
step:2008/2330 train_time:117593ms step_avg:58.56ms
step:2009/2330 train_time:117650ms step_avg:58.56ms
step:2010/2330 train_time:117710ms step_avg:58.56ms
step:2011/2330 train_time:117766ms step_avg:58.56ms
step:2012/2330 train_time:117826ms step_avg:58.56ms
step:2013/2330 train_time:117883ms step_avg:58.56ms
step:2014/2330 train_time:117942ms step_avg:58.56ms
step:2015/2330 train_time:117999ms step_avg:58.56ms
step:2016/2330 train_time:118060ms step_avg:58.56ms
step:2017/2330 train_time:118116ms step_avg:58.56ms
step:2018/2330 train_time:118179ms step_avg:58.56ms
step:2019/2330 train_time:118237ms step_avg:58.56ms
step:2020/2330 train_time:118302ms step_avg:58.57ms
step:2021/2330 train_time:118360ms step_avg:58.56ms
step:2022/2330 train_time:118424ms step_avg:58.57ms
step:2023/2330 train_time:118481ms step_avg:58.57ms
step:2024/2330 train_time:118541ms step_avg:58.57ms
step:2025/2330 train_time:118598ms step_avg:58.57ms
step:2026/2330 train_time:118660ms step_avg:58.57ms
step:2027/2330 train_time:118716ms step_avg:58.57ms
step:2028/2330 train_time:118777ms step_avg:58.57ms
step:2029/2330 train_time:118834ms step_avg:58.57ms
step:2030/2330 train_time:118894ms step_avg:58.57ms
step:2031/2330 train_time:118951ms step_avg:58.57ms
step:2032/2330 train_time:119011ms step_avg:58.57ms
step:2033/2330 train_time:119068ms step_avg:58.57ms
step:2034/2330 train_time:119128ms step_avg:58.57ms
step:2035/2330 train_time:119187ms step_avg:58.57ms
step:2036/2330 train_time:119249ms step_avg:58.57ms
step:2037/2330 train_time:119308ms step_avg:58.57ms
step:2038/2330 train_time:119370ms step_avg:58.57ms
step:2039/2330 train_time:119428ms step_avg:58.57ms
step:2040/2330 train_time:119489ms step_avg:58.57ms
step:2041/2330 train_time:119547ms step_avg:58.57ms
step:2042/2330 train_time:119608ms step_avg:58.57ms
step:2043/2330 train_time:119665ms step_avg:58.57ms
step:2044/2330 train_time:119725ms step_avg:58.57ms
step:2045/2330 train_time:119782ms step_avg:58.57ms
step:2046/2330 train_time:119842ms step_avg:58.57ms
step:2047/2330 train_time:119899ms step_avg:58.57ms
step:2048/2330 train_time:119959ms step_avg:58.57ms
step:2049/2330 train_time:120016ms step_avg:58.57ms
step:2050/2330 train_time:120077ms step_avg:58.57ms
step:2051/2330 train_time:120134ms step_avg:58.57ms
step:2052/2330 train_time:120197ms step_avg:58.58ms
step:2053/2330 train_time:120254ms step_avg:58.57ms
step:2054/2330 train_time:120317ms step_avg:58.58ms
step:2055/2330 train_time:120375ms step_avg:58.58ms
step:2056/2330 train_time:120437ms step_avg:58.58ms
step:2057/2330 train_time:120496ms step_avg:58.58ms
step:2058/2330 train_time:120557ms step_avg:58.58ms
step:2059/2330 train_time:120616ms step_avg:58.58ms
step:2060/2330 train_time:120676ms step_avg:58.58ms
step:2061/2330 train_time:120734ms step_avg:58.58ms
step:2062/2330 train_time:120794ms step_avg:58.58ms
step:2063/2330 train_time:120851ms step_avg:58.58ms
step:2064/2330 train_time:120911ms step_avg:58.58ms
step:2065/2330 train_time:120969ms step_avg:58.58ms
step:2066/2330 train_time:121029ms step_avg:58.58ms
step:2067/2330 train_time:121086ms step_avg:58.58ms
step:2068/2330 train_time:121147ms step_avg:58.58ms
step:2069/2330 train_time:121204ms step_avg:58.58ms
step:2070/2330 train_time:121266ms step_avg:58.58ms
step:2071/2330 train_time:121324ms step_avg:58.58ms
step:2072/2330 train_time:121384ms step_avg:58.58ms
step:2073/2330 train_time:121441ms step_avg:58.58ms
step:2074/2330 train_time:121503ms step_avg:58.58ms
step:2075/2330 train_time:121561ms step_avg:58.58ms
step:2076/2330 train_time:121621ms step_avg:58.58ms
step:2077/2330 train_time:121679ms step_avg:58.58ms
step:2078/2330 train_time:121740ms step_avg:58.59ms
step:2079/2330 train_time:121796ms step_avg:58.58ms
step:2080/2330 train_time:121858ms step_avg:58.59ms
step:2081/2330 train_time:121915ms step_avg:58.58ms
step:2082/2330 train_time:121976ms step_avg:58.59ms
step:2083/2330 train_time:122033ms step_avg:58.59ms
step:2084/2330 train_time:122095ms step_avg:58.59ms
step:2085/2330 train_time:122152ms step_avg:58.59ms
step:2086/2330 train_time:122213ms step_avg:58.59ms
step:2087/2330 train_time:122271ms step_avg:58.59ms
step:2088/2330 train_time:122331ms step_avg:58.59ms
step:2089/2330 train_time:122390ms step_avg:58.59ms
step:2090/2330 train_time:122451ms step_avg:58.59ms
step:2091/2330 train_time:122508ms step_avg:58.59ms
step:2092/2330 train_time:122569ms step_avg:58.59ms
step:2093/2330 train_time:122628ms step_avg:58.59ms
step:2094/2330 train_time:122688ms step_avg:58.59ms
step:2095/2330 train_time:122746ms step_avg:58.59ms
step:2096/2330 train_time:122806ms step_avg:58.59ms
step:2097/2330 train_time:122863ms step_avg:58.59ms
step:2098/2330 train_time:122923ms step_avg:58.59ms
step:2099/2330 train_time:122980ms step_avg:58.59ms
step:2100/2330 train_time:123041ms step_avg:58.59ms
step:2101/2330 train_time:123097ms step_avg:58.59ms
step:2102/2330 train_time:123159ms step_avg:58.59ms
step:2103/2330 train_time:123216ms step_avg:58.59ms
step:2104/2330 train_time:123278ms step_avg:58.59ms
step:2105/2330 train_time:123336ms step_avg:58.59ms
step:2106/2330 train_time:123398ms step_avg:58.59ms
step:2107/2330 train_time:123455ms step_avg:58.59ms
step:2108/2330 train_time:123516ms step_avg:58.59ms
step:2109/2330 train_time:123574ms step_avg:58.59ms
step:2110/2330 train_time:123636ms step_avg:58.60ms
step:2111/2330 train_time:123693ms step_avg:58.59ms
step:2112/2330 train_time:123754ms step_avg:58.60ms
step:2113/2330 train_time:123813ms step_avg:58.60ms
step:2114/2330 train_time:123874ms step_avg:58.60ms
step:2115/2330 train_time:123932ms step_avg:58.60ms
step:2116/2330 train_time:123992ms step_avg:58.60ms
step:2117/2330 train_time:124050ms step_avg:58.60ms
step:2118/2330 train_time:124110ms step_avg:58.60ms
step:2119/2330 train_time:124168ms step_avg:58.60ms
step:2120/2330 train_time:124228ms step_avg:58.60ms
step:2121/2330 train_time:124286ms step_avg:58.60ms
step:2122/2330 train_time:124346ms step_avg:58.60ms
step:2123/2330 train_time:124403ms step_avg:58.60ms
step:2124/2330 train_time:124464ms step_avg:58.60ms
step:2125/2330 train_time:124522ms step_avg:58.60ms
step:2126/2330 train_time:124582ms step_avg:58.60ms
step:2127/2330 train_time:124638ms step_avg:58.60ms
step:2128/2330 train_time:124702ms step_avg:58.60ms
step:2129/2330 train_time:124759ms step_avg:58.60ms
step:2130/2330 train_time:124821ms step_avg:58.60ms
step:2131/2330 train_time:124878ms step_avg:58.60ms
step:2132/2330 train_time:124939ms step_avg:58.60ms
step:2133/2330 train_time:124996ms step_avg:58.60ms
step:2134/2330 train_time:125058ms step_avg:58.60ms
step:2135/2330 train_time:125115ms step_avg:58.60ms
step:2136/2330 train_time:125176ms step_avg:58.60ms
step:2137/2330 train_time:125234ms step_avg:58.60ms
step:2138/2330 train_time:125294ms step_avg:58.60ms
step:2139/2330 train_time:125352ms step_avg:58.60ms
step:2140/2330 train_time:125413ms step_avg:58.60ms
step:2141/2330 train_time:125472ms step_avg:58.60ms
step:2142/2330 train_time:125532ms step_avg:58.61ms
step:2143/2330 train_time:125591ms step_avg:58.61ms
step:2144/2330 train_time:125651ms step_avg:58.61ms
step:2145/2330 train_time:125710ms step_avg:58.61ms
step:2146/2330 train_time:125770ms step_avg:58.61ms
step:2147/2330 train_time:125829ms step_avg:58.61ms
step:2148/2330 train_time:125889ms step_avg:58.61ms
step:2149/2330 train_time:125946ms step_avg:58.61ms
step:2150/2330 train_time:126007ms step_avg:58.61ms
step:2151/2330 train_time:126064ms step_avg:58.61ms
step:2152/2330 train_time:126125ms step_avg:58.61ms
step:2153/2330 train_time:126182ms step_avg:58.61ms
step:2154/2330 train_time:126243ms step_avg:58.61ms
step:2155/2330 train_time:126300ms step_avg:58.61ms
step:2156/2330 train_time:126361ms step_avg:58.61ms
step:2157/2330 train_time:126418ms step_avg:58.61ms
step:2158/2330 train_time:126480ms step_avg:58.61ms
step:2159/2330 train_time:126537ms step_avg:58.61ms
step:2160/2330 train_time:126599ms step_avg:58.61ms
step:2161/2330 train_time:126657ms step_avg:58.61ms
step:2162/2330 train_time:126718ms step_avg:58.61ms
step:2163/2330 train_time:126775ms step_avg:58.61ms
step:2164/2330 train_time:126836ms step_avg:58.61ms
step:2165/2330 train_time:126894ms step_avg:58.61ms
step:2166/2330 train_time:126955ms step_avg:58.61ms
step:2167/2330 train_time:127012ms step_avg:58.61ms
step:2168/2330 train_time:127073ms step_avg:58.61ms
step:2169/2330 train_time:127131ms step_avg:58.61ms
step:2170/2330 train_time:127191ms step_avg:58.61ms
step:2171/2330 train_time:127249ms step_avg:58.61ms
step:2172/2330 train_time:127309ms step_avg:58.61ms
step:2173/2330 train_time:127366ms step_avg:58.61ms
step:2174/2330 train_time:127427ms step_avg:58.61ms
step:2175/2330 train_time:127485ms step_avg:58.61ms
step:2176/2330 train_time:127545ms step_avg:58.61ms
step:2177/2330 train_time:127602ms step_avg:58.61ms
step:2178/2330 train_time:127664ms step_avg:58.62ms
step:2179/2330 train_time:127720ms step_avg:58.61ms
step:2180/2330 train_time:127782ms step_avg:58.62ms
step:2181/2330 train_time:127838ms step_avg:58.61ms
step:2182/2330 train_time:127900ms step_avg:58.62ms
step:2183/2330 train_time:127957ms step_avg:58.62ms
step:2184/2330 train_time:128019ms step_avg:58.62ms
step:2185/2330 train_time:128076ms step_avg:58.62ms
step:2186/2330 train_time:128137ms step_avg:58.62ms
step:2187/2330 train_time:128194ms step_avg:58.62ms
step:2188/2330 train_time:128256ms step_avg:58.62ms
step:2189/2330 train_time:128313ms step_avg:58.62ms
step:2190/2330 train_time:128376ms step_avg:58.62ms
step:2191/2330 train_time:128434ms step_avg:58.62ms
step:2192/2330 train_time:128494ms step_avg:58.62ms
step:2193/2330 train_time:128552ms step_avg:58.62ms
step:2194/2330 train_time:128612ms step_avg:58.62ms
step:2195/2330 train_time:128671ms step_avg:58.62ms
step:2196/2330 train_time:128731ms step_avg:58.62ms
step:2197/2330 train_time:128789ms step_avg:58.62ms
step:2198/2330 train_time:128849ms step_avg:58.62ms
step:2199/2330 train_time:128906ms step_avg:58.62ms
step:2200/2330 train_time:128967ms step_avg:58.62ms
step:2201/2330 train_time:129024ms step_avg:58.62ms
step:2202/2330 train_time:129084ms step_avg:58.62ms
step:2203/2330 train_time:129142ms step_avg:58.62ms
step:2204/2330 train_time:129204ms step_avg:58.62ms
step:2205/2330 train_time:129261ms step_avg:58.62ms
step:2206/2330 train_time:129321ms step_avg:58.62ms
step:2207/2330 train_time:129378ms step_avg:58.62ms
step:2208/2330 train_time:129440ms step_avg:58.62ms
step:2209/2330 train_time:129497ms step_avg:58.62ms
step:2210/2330 train_time:129559ms step_avg:58.62ms
step:2211/2330 train_time:129617ms step_avg:58.62ms
step:2212/2330 train_time:129678ms step_avg:58.62ms
step:2213/2330 train_time:129735ms step_avg:58.62ms
step:2214/2330 train_time:129797ms step_avg:58.63ms
step:2215/2330 train_time:129854ms step_avg:58.62ms
step:2216/2330 train_time:129916ms step_avg:58.63ms
step:2217/2330 train_time:129974ms step_avg:58.63ms
step:2218/2330 train_time:130035ms step_avg:58.63ms
step:2219/2330 train_time:130093ms step_avg:58.63ms
step:2220/2330 train_time:130154ms step_avg:58.63ms
step:2221/2330 train_time:130212ms step_avg:58.63ms
step:2222/2330 train_time:130272ms step_avg:58.63ms
step:2223/2330 train_time:130330ms step_avg:58.63ms
step:2224/2330 train_time:130390ms step_avg:58.63ms
step:2225/2330 train_time:130447ms step_avg:58.63ms
step:2226/2330 train_time:130507ms step_avg:58.63ms
step:2227/2330 train_time:130564ms step_avg:58.63ms
step:2228/2330 train_time:130625ms step_avg:58.63ms
step:2229/2330 train_time:130683ms step_avg:58.63ms
step:2230/2330 train_time:130743ms step_avg:58.63ms
step:2231/2330 train_time:130799ms step_avg:58.63ms
step:2232/2330 train_time:130861ms step_avg:58.63ms
step:2233/2330 train_time:130918ms step_avg:58.63ms
step:2234/2330 train_time:130979ms step_avg:58.63ms
step:2235/2330 train_time:131036ms step_avg:58.63ms
step:2236/2330 train_time:131097ms step_avg:58.63ms
step:2237/2330 train_time:131154ms step_avg:58.63ms
step:2238/2330 train_time:131216ms step_avg:58.63ms
step:2239/2330 train_time:131274ms step_avg:58.63ms
step:2240/2330 train_time:131334ms step_avg:58.63ms
step:2241/2330 train_time:131392ms step_avg:58.63ms
step:2242/2330 train_time:131452ms step_avg:58.63ms
step:2243/2330 train_time:131510ms step_avg:58.63ms
step:2244/2330 train_time:131571ms step_avg:58.63ms
step:2245/2330 train_time:131630ms step_avg:58.63ms
step:2246/2330 train_time:131691ms step_avg:58.63ms
step:2247/2330 train_time:131748ms step_avg:58.63ms
step:2248/2330 train_time:131809ms step_avg:58.63ms
step:2249/2330 train_time:131866ms step_avg:58.63ms
step:2250/2330 train_time:131927ms step_avg:58.63ms
step:2250/2330 val_loss:3.7093 train_time:132009ms step_avg:58.67ms
step:2251/2330 train_time:132027ms step_avg:58.65ms
step:2252/2330 train_time:132049ms step_avg:58.64ms
step:2253/2330 train_time:132111ms step_avg:58.64ms
step:2254/2330 train_time:132176ms step_avg:58.64ms
step:2255/2330 train_time:132234ms step_avg:58.64ms
step:2256/2330 train_time:132298ms step_avg:58.64ms
step:2257/2330 train_time:132354ms step_avg:58.64ms
step:2258/2330 train_time:132415ms step_avg:58.64ms
step:2259/2330 train_time:132472ms step_avg:58.64ms
step:2260/2330 train_time:132532ms step_avg:58.64ms
step:2261/2330 train_time:132589ms step_avg:58.64ms
step:2262/2330 train_time:132649ms step_avg:58.64ms
step:2263/2330 train_time:132706ms step_avg:58.64ms
step:2264/2330 train_time:132766ms step_avg:58.64ms
step:2265/2330 train_time:132823ms step_avg:58.64ms
step:2266/2330 train_time:132882ms step_avg:58.64ms
step:2267/2330 train_time:132939ms step_avg:58.64ms
step:2268/2330 train_time:133000ms step_avg:58.64ms
step:2269/2330 train_time:133060ms step_avg:58.64ms
step:2270/2330 train_time:133122ms step_avg:58.64ms
step:2271/2330 train_time:133181ms step_avg:58.64ms
step:2272/2330 train_time:133243ms step_avg:58.65ms
step:2273/2330 train_time:133301ms step_avg:58.65ms
step:2274/2330 train_time:133361ms step_avg:58.65ms
step:2275/2330 train_time:133419ms step_avg:58.65ms
step:2276/2330 train_time:133479ms step_avg:58.65ms
step:2277/2330 train_time:133535ms step_avg:58.65ms
step:2278/2330 train_time:133596ms step_avg:58.65ms
step:2279/2330 train_time:133652ms step_avg:58.65ms
step:2280/2330 train_time:133714ms step_avg:58.65ms
step:2281/2330 train_time:133770ms step_avg:58.65ms
step:2282/2330 train_time:133831ms step_avg:58.65ms
step:2283/2330 train_time:133888ms step_avg:58.65ms
step:2284/2330 train_time:133948ms step_avg:58.65ms
step:2285/2330 train_time:134007ms step_avg:58.65ms
step:2286/2330 train_time:134068ms step_avg:58.65ms
step:2287/2330 train_time:134126ms step_avg:58.65ms
step:2288/2330 train_time:134188ms step_avg:58.65ms
step:2289/2330 train_time:134246ms step_avg:58.65ms
step:2290/2330 train_time:134307ms step_avg:58.65ms
step:2291/2330 train_time:134366ms step_avg:58.65ms
step:2292/2330 train_time:134426ms step_avg:58.65ms
step:2293/2330 train_time:134485ms step_avg:58.65ms
step:2294/2330 train_time:134545ms step_avg:58.65ms
step:2295/2330 train_time:134602ms step_avg:58.65ms
step:2296/2330 train_time:134662ms step_avg:58.65ms
step:2297/2330 train_time:134719ms step_avg:58.65ms
step:2298/2330 train_time:134779ms step_avg:58.65ms
step:2299/2330 train_time:134836ms step_avg:58.65ms
step:2300/2330 train_time:134896ms step_avg:58.65ms
step:2301/2330 train_time:134953ms step_avg:58.65ms
step:2302/2330 train_time:135016ms step_avg:58.65ms
step:2303/2330 train_time:135074ms step_avg:58.65ms
step:2304/2330 train_time:135136ms step_avg:58.65ms
step:2305/2330 train_time:135193ms step_avg:58.65ms
step:2306/2330 train_time:135256ms step_avg:58.65ms
step:2307/2330 train_time:135313ms step_avg:58.65ms
step:2308/2330 train_time:135375ms step_avg:58.65ms
step:2309/2330 train_time:135433ms step_avg:58.65ms
step:2310/2330 train_time:135494ms step_avg:58.66ms
step:2311/2330 train_time:135551ms step_avg:58.65ms
step:2312/2330 train_time:135612ms step_avg:58.66ms
step:2313/2330 train_time:135670ms step_avg:58.66ms
step:2314/2330 train_time:135730ms step_avg:58.66ms
step:2315/2330 train_time:135788ms step_avg:58.66ms
step:2316/2330 train_time:135848ms step_avg:58.66ms
step:2317/2330 train_time:135905ms step_avg:58.66ms
step:2318/2330 train_time:135965ms step_avg:58.66ms
step:2319/2330 train_time:136023ms step_avg:58.66ms
step:2320/2330 train_time:136084ms step_avg:58.66ms
step:2321/2330 train_time:136142ms step_avg:58.66ms
step:2322/2330 train_time:136202ms step_avg:58.66ms
step:2323/2330 train_time:136260ms step_avg:58.66ms
step:2324/2330 train_time:136321ms step_avg:58.66ms
step:2325/2330 train_time:136378ms step_avg:58.66ms
step:2326/2330 train_time:136440ms step_avg:58.66ms
step:2327/2330 train_time:136497ms step_avg:58.66ms
step:2328/2330 train_time:136558ms step_avg:58.66ms
step:2329/2330 train_time:136615ms step_avg:58.66ms
step:2330/2330 train_time:136677ms step_avg:58.66ms
step:2330/2330 val_loss:3.6939 train_time:136759ms step_avg:58.69ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
