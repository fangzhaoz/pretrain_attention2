import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--beta1', type=str, required=True)
parser.add_argument('--beta2', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_beta_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=6e-4,  betas=(float(args2.beta1), float(args2.beta2)), eps=1e-10, weight_decay=0.0, fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt", exist_ok=True)
            torch.save(log, f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 04:09:15 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   36C    P0             117W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   35C    P0             118W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             119W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             115W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   35C    P0             119W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   34C    P0             115W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:78ms step_avg:77.99ms
step:2/2330 train_time:175ms step_avg:87.62ms
step:3/2330 train_time:194ms step_avg:64.69ms
step:4/2330 train_time:213ms step_avg:53.23ms
step:5/2330 train_time:266ms step_avg:53.30ms
step:6/2330 train_time:325ms step_avg:54.09ms
step:7/2330 train_time:380ms step_avg:54.29ms
step:8/2330 train_time:439ms step_avg:54.81ms
step:9/2330 train_time:494ms step_avg:54.90ms
step:10/2330 train_time:552ms step_avg:55.21ms
step:11/2330 train_time:607ms step_avg:55.23ms
step:12/2330 train_time:666ms step_avg:55.52ms
step:13/2330 train_time:722ms step_avg:55.51ms
step:14/2330 train_time:780ms step_avg:55.73ms
step:15/2330 train_time:836ms step_avg:55.73ms
step:16/2330 train_time:894ms step_avg:55.89ms
step:17/2330 train_time:950ms step_avg:55.86ms
step:18/2330 train_time:1009ms step_avg:56.03ms
step:19/2330 train_time:1067ms step_avg:56.15ms
step:20/2330 train_time:1130ms step_avg:56.49ms
step:21/2330 train_time:1187ms step_avg:56.55ms
step:22/2330 train_time:1247ms step_avg:56.69ms
step:23/2330 train_time:1304ms step_avg:56.70ms
step:24/2330 train_time:1363ms step_avg:56.77ms
step:25/2330 train_time:1418ms step_avg:56.73ms
step:26/2330 train_time:1476ms step_avg:56.78ms
step:27/2330 train_time:1532ms step_avg:56.74ms
step:28/2330 train_time:1590ms step_avg:56.78ms
step:29/2330 train_time:1645ms step_avg:56.73ms
step:30/2330 train_time:1704ms step_avg:56.79ms
step:31/2330 train_time:1759ms step_avg:56.75ms
step:32/2330 train_time:1818ms step_avg:56.80ms
step:33/2330 train_time:1873ms step_avg:56.75ms
step:34/2330 train_time:1932ms step_avg:56.82ms
step:35/2330 train_time:1988ms step_avg:56.80ms
step:36/2330 train_time:2048ms step_avg:56.88ms
step:37/2330 train_time:2105ms step_avg:56.90ms
step:38/2330 train_time:2165ms step_avg:56.97ms
step:39/2330 train_time:2222ms step_avg:56.98ms
step:40/2330 train_time:2281ms step_avg:57.03ms
step:41/2330 train_time:2337ms step_avg:57.00ms
step:42/2330 train_time:2396ms step_avg:57.05ms
step:43/2330 train_time:2452ms step_avg:57.01ms
step:44/2330 train_time:2511ms step_avg:57.06ms
step:45/2330 train_time:2566ms step_avg:57.02ms
step:46/2330 train_time:2625ms step_avg:57.07ms
step:47/2330 train_time:2681ms step_avg:57.04ms
step:48/2330 train_time:2739ms step_avg:57.07ms
step:49/2330 train_time:2795ms step_avg:57.04ms
step:50/2330 train_time:2853ms step_avg:57.06ms
step:51/2330 train_time:2909ms step_avg:57.04ms
step:52/2330 train_time:2968ms step_avg:57.07ms
step:53/2330 train_time:3024ms step_avg:57.06ms
step:54/2330 train_time:3084ms step_avg:57.10ms
step:55/2330 train_time:3140ms step_avg:57.10ms
step:56/2330 train_time:3200ms step_avg:57.14ms
step:57/2330 train_time:3257ms step_avg:57.13ms
step:58/2330 train_time:3316ms step_avg:57.17ms
step:59/2330 train_time:3372ms step_avg:57.16ms
step:60/2330 train_time:3431ms step_avg:57.19ms
step:61/2330 train_time:3487ms step_avg:57.16ms
step:62/2330 train_time:3546ms step_avg:57.20ms
step:63/2330 train_time:3602ms step_avg:57.18ms
step:64/2330 train_time:3660ms step_avg:57.19ms
step:65/2330 train_time:3716ms step_avg:57.17ms
step:66/2330 train_time:3775ms step_avg:57.19ms
step:67/2330 train_time:3830ms step_avg:57.16ms
step:68/2330 train_time:3889ms step_avg:57.20ms
step:69/2330 train_time:3945ms step_avg:57.18ms
step:70/2330 train_time:4004ms step_avg:57.20ms
step:71/2330 train_time:4061ms step_avg:57.20ms
step:72/2330 train_time:4121ms step_avg:57.23ms
step:73/2330 train_time:4177ms step_avg:57.22ms
step:74/2330 train_time:4236ms step_avg:57.24ms
step:75/2330 train_time:4292ms step_avg:57.23ms
step:76/2330 train_time:4352ms step_avg:57.26ms
step:77/2330 train_time:4408ms step_avg:57.25ms
step:78/2330 train_time:4467ms step_avg:57.27ms
step:79/2330 train_time:4523ms step_avg:57.25ms
step:80/2330 train_time:4583ms step_avg:57.29ms
step:81/2330 train_time:4639ms step_avg:57.28ms
step:82/2330 train_time:4698ms step_avg:57.29ms
step:83/2330 train_time:4753ms step_avg:57.27ms
step:84/2330 train_time:4812ms step_avg:57.28ms
step:85/2330 train_time:4867ms step_avg:57.26ms
step:86/2330 train_time:4928ms step_avg:57.30ms
step:87/2330 train_time:4983ms step_avg:57.28ms
step:88/2330 train_time:5042ms step_avg:57.30ms
step:89/2330 train_time:5099ms step_avg:57.29ms
step:90/2330 train_time:5158ms step_avg:57.31ms
step:91/2330 train_time:5214ms step_avg:57.30ms
step:92/2330 train_time:5273ms step_avg:57.31ms
step:93/2330 train_time:5329ms step_avg:57.30ms
step:94/2330 train_time:5389ms step_avg:57.33ms
step:95/2330 train_time:5444ms step_avg:57.31ms
step:96/2330 train_time:5504ms step_avg:57.33ms
step:97/2330 train_time:5560ms step_avg:57.32ms
step:98/2330 train_time:5619ms step_avg:57.33ms
step:99/2330 train_time:5675ms step_avg:57.32ms
step:100/2330 train_time:5733ms step_avg:57.33ms
step:101/2330 train_time:5789ms step_avg:57.32ms
step:102/2330 train_time:5848ms step_avg:57.34ms
step:103/2330 train_time:5904ms step_avg:57.32ms
step:104/2330 train_time:5963ms step_avg:57.34ms
step:105/2330 train_time:6019ms step_avg:57.32ms
step:106/2330 train_time:6078ms step_avg:57.34ms
step:107/2330 train_time:6134ms step_avg:57.33ms
step:108/2330 train_time:6193ms step_avg:57.34ms
step:109/2330 train_time:6250ms step_avg:57.34ms
step:110/2330 train_time:6309ms step_avg:57.35ms
step:111/2330 train_time:6365ms step_avg:57.34ms
step:112/2330 train_time:6424ms step_avg:57.36ms
step:113/2330 train_time:6481ms step_avg:57.35ms
step:114/2330 train_time:6540ms step_avg:57.37ms
step:115/2330 train_time:6596ms step_avg:57.36ms
step:116/2330 train_time:6655ms step_avg:57.37ms
step:117/2330 train_time:6710ms step_avg:57.35ms
step:118/2330 train_time:6769ms step_avg:57.37ms
step:119/2330 train_time:6826ms step_avg:57.36ms
step:120/2330 train_time:6884ms step_avg:57.37ms
step:121/2330 train_time:6940ms step_avg:57.36ms
step:122/2330 train_time:6999ms step_avg:57.37ms
step:123/2330 train_time:7056ms step_avg:57.36ms
step:124/2330 train_time:7114ms step_avg:57.37ms
step:125/2330 train_time:7170ms step_avg:57.36ms
step:126/2330 train_time:7229ms step_avg:57.37ms
step:127/2330 train_time:7285ms step_avg:57.36ms
step:128/2330 train_time:7344ms step_avg:57.38ms
step:129/2330 train_time:7401ms step_avg:57.37ms
step:130/2330 train_time:7460ms step_avg:57.38ms
step:131/2330 train_time:7516ms step_avg:57.37ms
step:132/2330 train_time:7574ms step_avg:57.38ms
step:133/2330 train_time:7630ms step_avg:57.37ms
step:134/2330 train_time:7690ms step_avg:57.38ms
step:135/2330 train_time:7745ms step_avg:57.37ms
step:136/2330 train_time:7804ms step_avg:57.38ms
step:137/2330 train_time:7860ms step_avg:57.37ms
step:138/2330 train_time:7919ms step_avg:57.39ms
step:139/2330 train_time:7975ms step_avg:57.38ms
step:140/2330 train_time:8034ms step_avg:57.38ms
step:141/2330 train_time:8089ms step_avg:57.37ms
step:142/2330 train_time:8149ms step_avg:57.39ms
step:143/2330 train_time:8205ms step_avg:57.38ms
step:144/2330 train_time:8264ms step_avg:57.39ms
step:145/2330 train_time:8320ms step_avg:57.38ms
step:146/2330 train_time:8379ms step_avg:57.39ms
step:147/2330 train_time:8435ms step_avg:57.38ms
step:148/2330 train_time:8494ms step_avg:57.39ms
step:149/2330 train_time:8550ms step_avg:57.38ms
step:150/2330 train_time:8609ms step_avg:57.39ms
step:151/2330 train_time:8665ms step_avg:57.38ms
step:152/2330 train_time:8724ms step_avg:57.39ms
step:153/2330 train_time:8779ms step_avg:57.38ms
step:154/2330 train_time:8838ms step_avg:57.39ms
step:155/2330 train_time:8894ms step_avg:57.38ms
step:156/2330 train_time:8953ms step_avg:57.39ms
step:157/2330 train_time:9009ms step_avg:57.38ms
step:158/2330 train_time:9068ms step_avg:57.39ms
step:159/2330 train_time:9125ms step_avg:57.39ms
step:160/2330 train_time:9183ms step_avg:57.40ms
step:161/2330 train_time:9239ms step_avg:57.39ms
step:162/2330 train_time:9298ms step_avg:57.39ms
step:163/2330 train_time:9354ms step_avg:57.39ms
step:164/2330 train_time:9413ms step_avg:57.40ms
step:165/2330 train_time:9469ms step_avg:57.39ms
step:166/2330 train_time:9528ms step_avg:57.40ms
step:167/2330 train_time:9584ms step_avg:57.39ms
step:168/2330 train_time:9642ms step_avg:57.40ms
step:169/2330 train_time:9699ms step_avg:57.39ms
step:170/2330 train_time:9757ms step_avg:57.39ms
step:171/2330 train_time:9814ms step_avg:57.39ms
step:172/2330 train_time:9872ms step_avg:57.40ms
step:173/2330 train_time:9929ms step_avg:57.39ms
step:174/2330 train_time:9989ms step_avg:57.41ms
step:175/2330 train_time:10045ms step_avg:57.40ms
step:176/2330 train_time:10104ms step_avg:57.41ms
step:177/2330 train_time:10160ms step_avg:57.40ms
step:178/2330 train_time:10219ms step_avg:57.41ms
step:179/2330 train_time:10275ms step_avg:57.40ms
step:180/2330 train_time:10333ms step_avg:57.41ms
step:181/2330 train_time:10389ms step_avg:57.40ms
step:182/2330 train_time:10449ms step_avg:57.41ms
step:183/2330 train_time:10504ms step_avg:57.40ms
step:184/2330 train_time:10564ms step_avg:57.41ms
step:185/2330 train_time:10620ms step_avg:57.40ms
step:186/2330 train_time:10679ms step_avg:57.41ms
step:187/2330 train_time:10734ms step_avg:57.40ms
step:188/2330 train_time:10793ms step_avg:57.41ms
step:189/2330 train_time:10849ms step_avg:57.40ms
step:190/2330 train_time:10908ms step_avg:57.41ms
step:191/2330 train_time:10964ms step_avg:57.40ms
step:192/2330 train_time:11024ms step_avg:57.42ms
step:193/2330 train_time:11080ms step_avg:57.41ms
step:194/2330 train_time:11139ms step_avg:57.42ms
step:195/2330 train_time:11195ms step_avg:57.41ms
step:196/2330 train_time:11253ms step_avg:57.41ms
step:197/2330 train_time:11309ms step_avg:57.41ms
step:198/2330 train_time:11369ms step_avg:57.42ms
step:199/2330 train_time:11426ms step_avg:57.42ms
step:200/2330 train_time:11484ms step_avg:57.42ms
step:201/2330 train_time:11540ms step_avg:57.41ms
step:202/2330 train_time:11599ms step_avg:57.42ms
step:203/2330 train_time:11655ms step_avg:57.41ms
step:204/2330 train_time:11713ms step_avg:57.42ms
step:205/2330 train_time:11770ms step_avg:57.41ms
step:206/2330 train_time:11828ms step_avg:57.42ms
step:207/2330 train_time:11884ms step_avg:57.41ms
step:208/2330 train_time:11942ms step_avg:57.42ms
step:209/2330 train_time:11999ms step_avg:57.41ms
step:210/2330 train_time:12058ms step_avg:57.42ms
step:211/2330 train_time:12114ms step_avg:57.41ms
step:212/2330 train_time:12173ms step_avg:57.42ms
step:213/2330 train_time:12229ms step_avg:57.41ms
step:214/2330 train_time:12287ms step_avg:57.42ms
step:215/2330 train_time:12343ms step_avg:57.41ms
step:216/2330 train_time:12403ms step_avg:57.42ms
step:217/2330 train_time:12459ms step_avg:57.41ms
step:218/2330 train_time:12518ms step_avg:57.42ms
step:219/2330 train_time:12573ms step_avg:57.41ms
step:220/2330 train_time:12633ms step_avg:57.42ms
step:221/2330 train_time:12689ms step_avg:57.41ms
step:222/2330 train_time:12749ms step_avg:57.43ms
step:223/2330 train_time:12805ms step_avg:57.42ms
step:224/2330 train_time:12864ms step_avg:57.43ms
step:225/2330 train_time:12920ms step_avg:57.42ms
step:226/2330 train_time:12979ms step_avg:57.43ms
step:227/2330 train_time:13036ms step_avg:57.43ms
step:228/2330 train_time:13094ms step_avg:57.43ms
step:229/2330 train_time:13150ms step_avg:57.42ms
step:230/2330 train_time:13209ms step_avg:57.43ms
step:231/2330 train_time:13265ms step_avg:57.42ms
step:232/2330 train_time:13325ms step_avg:57.44ms
step:233/2330 train_time:13382ms step_avg:57.43ms
step:234/2330 train_time:13441ms step_avg:57.44ms
step:235/2330 train_time:13496ms step_avg:57.43ms
step:236/2330 train_time:13555ms step_avg:57.43ms
step:237/2330 train_time:13611ms step_avg:57.43ms
step:238/2330 train_time:13670ms step_avg:57.44ms
step:239/2330 train_time:13725ms step_avg:57.43ms
step:240/2330 train_time:13785ms step_avg:57.44ms
step:241/2330 train_time:13841ms step_avg:57.43ms
step:242/2330 train_time:13900ms step_avg:57.44ms
step:243/2330 train_time:13956ms step_avg:57.43ms
step:244/2330 train_time:14015ms step_avg:57.44ms
step:245/2330 train_time:14072ms step_avg:57.43ms
step:246/2330 train_time:14130ms step_avg:57.44ms
step:247/2330 train_time:14186ms step_avg:57.43ms
step:248/2330 train_time:14246ms step_avg:57.44ms
step:249/2330 train_time:14302ms step_avg:57.44ms
step:250/2330 train_time:14361ms step_avg:57.44ms
step:250/2330 val_loss:4.8981 train_time:14440ms step_avg:57.76ms
step:251/2330 train_time:14460ms step_avg:57.61ms
step:252/2330 train_time:14480ms step_avg:57.46ms
step:253/2330 train_time:14532ms step_avg:57.44ms
step:254/2330 train_time:14597ms step_avg:57.47ms
step:255/2330 train_time:14652ms step_avg:57.46ms
step:256/2330 train_time:14717ms step_avg:57.49ms
step:257/2330 train_time:14772ms step_avg:57.48ms
step:258/2330 train_time:14832ms step_avg:57.49ms
step:259/2330 train_time:14888ms step_avg:57.48ms
step:260/2330 train_time:14948ms step_avg:57.49ms
step:261/2330 train_time:15003ms step_avg:57.48ms
step:262/2330 train_time:15061ms step_avg:57.49ms
step:263/2330 train_time:15117ms step_avg:57.48ms
step:264/2330 train_time:15176ms step_avg:57.48ms
step:265/2330 train_time:15231ms step_avg:57.47ms
step:266/2330 train_time:15289ms step_avg:57.48ms
step:267/2330 train_time:15345ms step_avg:57.47ms
step:268/2330 train_time:15405ms step_avg:57.48ms
step:269/2330 train_time:15462ms step_avg:57.48ms
step:270/2330 train_time:15522ms step_avg:57.49ms
step:271/2330 train_time:15579ms step_avg:57.49ms
step:272/2330 train_time:15638ms step_avg:57.49ms
step:273/2330 train_time:15695ms step_avg:57.49ms
step:274/2330 train_time:15754ms step_avg:57.50ms
step:275/2330 train_time:15811ms step_avg:57.49ms
step:276/2330 train_time:15870ms step_avg:57.50ms
step:277/2330 train_time:15926ms step_avg:57.49ms
step:278/2330 train_time:15986ms step_avg:57.50ms
step:279/2330 train_time:16041ms step_avg:57.50ms
step:280/2330 train_time:16100ms step_avg:57.50ms
step:281/2330 train_time:16156ms step_avg:57.49ms
step:282/2330 train_time:16214ms step_avg:57.50ms
step:283/2330 train_time:16269ms step_avg:57.49ms
step:284/2330 train_time:16328ms step_avg:57.49ms
step:285/2330 train_time:16384ms step_avg:57.49ms
step:286/2330 train_time:16443ms step_avg:57.49ms
step:287/2330 train_time:16500ms step_avg:57.49ms
step:288/2330 train_time:16559ms step_avg:57.50ms
step:289/2330 train_time:16616ms step_avg:57.49ms
step:290/2330 train_time:16675ms step_avg:57.50ms
step:291/2330 train_time:16731ms step_avg:57.50ms
step:292/2330 train_time:16790ms step_avg:57.50ms
step:293/2330 train_time:16847ms step_avg:57.50ms
step:294/2330 train_time:16906ms step_avg:57.50ms
step:295/2330 train_time:16962ms step_avg:57.50ms
step:296/2330 train_time:17020ms step_avg:57.50ms
step:297/2330 train_time:17076ms step_avg:57.49ms
step:298/2330 train_time:17135ms step_avg:57.50ms
step:299/2330 train_time:17190ms step_avg:57.49ms
step:300/2330 train_time:17249ms step_avg:57.50ms
step:301/2330 train_time:17305ms step_avg:57.49ms
step:302/2330 train_time:17364ms step_avg:57.50ms
step:303/2330 train_time:17420ms step_avg:57.49ms
step:304/2330 train_time:17479ms step_avg:57.50ms
step:305/2330 train_time:17535ms step_avg:57.49ms
step:306/2330 train_time:17594ms step_avg:57.50ms
step:307/2330 train_time:17651ms step_avg:57.50ms
step:308/2330 train_time:17711ms step_avg:57.50ms
step:309/2330 train_time:17767ms step_avg:57.50ms
step:310/2330 train_time:17827ms step_avg:57.51ms
step:311/2330 train_time:17882ms step_avg:57.50ms
step:312/2330 train_time:17941ms step_avg:57.50ms
step:313/2330 train_time:17998ms step_avg:57.50ms
step:314/2330 train_time:18057ms step_avg:57.51ms
step:315/2330 train_time:18113ms step_avg:57.50ms
step:316/2330 train_time:18171ms step_avg:57.50ms
step:317/2330 train_time:18227ms step_avg:57.50ms
step:318/2330 train_time:18286ms step_avg:57.50ms
step:319/2330 train_time:18341ms step_avg:57.50ms
step:320/2330 train_time:18401ms step_avg:57.50ms
step:321/2330 train_time:18458ms step_avg:57.50ms
step:322/2330 train_time:18517ms step_avg:57.51ms
step:323/2330 train_time:18572ms step_avg:57.50ms
step:324/2330 train_time:18632ms step_avg:57.51ms
step:325/2330 train_time:18687ms step_avg:57.50ms
step:326/2330 train_time:18747ms step_avg:57.51ms
step:327/2330 train_time:18803ms step_avg:57.50ms
step:328/2330 train_time:18864ms step_avg:57.51ms
step:329/2330 train_time:18920ms step_avg:57.51ms
step:330/2330 train_time:18979ms step_avg:57.51ms
step:331/2330 train_time:19036ms step_avg:57.51ms
step:332/2330 train_time:19096ms step_avg:57.52ms
step:333/2330 train_time:19151ms step_avg:57.51ms
step:334/2330 train_time:19210ms step_avg:57.51ms
step:335/2330 train_time:19266ms step_avg:57.51ms
step:336/2330 train_time:19325ms step_avg:57.51ms
step:337/2330 train_time:19381ms step_avg:57.51ms
step:338/2330 train_time:19441ms step_avg:57.52ms
step:339/2330 train_time:19497ms step_avg:57.51ms
step:340/2330 train_time:19556ms step_avg:57.52ms
step:341/2330 train_time:19613ms step_avg:57.51ms
step:342/2330 train_time:19672ms step_avg:57.52ms
step:343/2330 train_time:19727ms step_avg:57.51ms
step:344/2330 train_time:19787ms step_avg:57.52ms
step:345/2330 train_time:19843ms step_avg:57.51ms
step:346/2330 train_time:19903ms step_avg:57.52ms
step:347/2330 train_time:19958ms step_avg:57.52ms
step:348/2330 train_time:20019ms step_avg:57.53ms
step:349/2330 train_time:20075ms step_avg:57.52ms
step:350/2330 train_time:20134ms step_avg:57.53ms
step:351/2330 train_time:20191ms step_avg:57.52ms
step:352/2330 train_time:20249ms step_avg:57.53ms
step:353/2330 train_time:20305ms step_avg:57.52ms
step:354/2330 train_time:20364ms step_avg:57.53ms
step:355/2330 train_time:20420ms step_avg:57.52ms
step:356/2330 train_time:20479ms step_avg:57.53ms
step:357/2330 train_time:20535ms step_avg:57.52ms
step:358/2330 train_time:20595ms step_avg:57.53ms
step:359/2330 train_time:20650ms step_avg:57.52ms
step:360/2330 train_time:20709ms step_avg:57.53ms
step:361/2330 train_time:20765ms step_avg:57.52ms
step:362/2330 train_time:20825ms step_avg:57.53ms
step:363/2330 train_time:20882ms step_avg:57.53ms
step:364/2330 train_time:20941ms step_avg:57.53ms
step:365/2330 train_time:20997ms step_avg:57.53ms
step:366/2330 train_time:21056ms step_avg:57.53ms
step:367/2330 train_time:21112ms step_avg:57.53ms
step:368/2330 train_time:21171ms step_avg:57.53ms
step:369/2330 train_time:21226ms step_avg:57.52ms
step:370/2330 train_time:21286ms step_avg:57.53ms
step:371/2330 train_time:21342ms step_avg:57.53ms
step:372/2330 train_time:21401ms step_avg:57.53ms
step:373/2330 train_time:21457ms step_avg:57.53ms
step:374/2330 train_time:21516ms step_avg:57.53ms
step:375/2330 train_time:21572ms step_avg:57.53ms
step:376/2330 train_time:21631ms step_avg:57.53ms
step:377/2330 train_time:21687ms step_avg:57.52ms
step:378/2330 train_time:21748ms step_avg:57.53ms
step:379/2330 train_time:21803ms step_avg:57.53ms
step:380/2330 train_time:21865ms step_avg:57.54ms
step:381/2330 train_time:21921ms step_avg:57.54ms
step:382/2330 train_time:21980ms step_avg:57.54ms
step:383/2330 train_time:22037ms step_avg:57.54ms
step:384/2330 train_time:22096ms step_avg:57.54ms
step:385/2330 train_time:22152ms step_avg:57.54ms
step:386/2330 train_time:22210ms step_avg:57.54ms
step:387/2330 train_time:22266ms step_avg:57.53ms
step:388/2330 train_time:22325ms step_avg:57.54ms
step:389/2330 train_time:22381ms step_avg:57.53ms
step:390/2330 train_time:22440ms step_avg:57.54ms
step:391/2330 train_time:22496ms step_avg:57.54ms
step:392/2330 train_time:22555ms step_avg:57.54ms
step:393/2330 train_time:22612ms step_avg:57.54ms
step:394/2330 train_time:22671ms step_avg:57.54ms
step:395/2330 train_time:22727ms step_avg:57.54ms
step:396/2330 train_time:22786ms step_avg:57.54ms
step:397/2330 train_time:22842ms step_avg:57.54ms
step:398/2330 train_time:22902ms step_avg:57.54ms
step:399/2330 train_time:22959ms step_avg:57.54ms
step:400/2330 train_time:23019ms step_avg:57.55ms
step:401/2330 train_time:23076ms step_avg:57.55ms
step:402/2330 train_time:23135ms step_avg:57.55ms
step:403/2330 train_time:23190ms step_avg:57.54ms
step:404/2330 train_time:23249ms step_avg:57.55ms
step:405/2330 train_time:23305ms step_avg:57.54ms
step:406/2330 train_time:23367ms step_avg:57.55ms
step:407/2330 train_time:23422ms step_avg:57.55ms
step:408/2330 train_time:23482ms step_avg:57.55ms
step:409/2330 train_time:23540ms step_avg:57.55ms
step:410/2330 train_time:23598ms step_avg:57.56ms
step:411/2330 train_time:23654ms step_avg:57.55ms
step:412/2330 train_time:23713ms step_avg:57.56ms
step:413/2330 train_time:23769ms step_avg:57.55ms
step:414/2330 train_time:23830ms step_avg:57.56ms
step:415/2330 train_time:23885ms step_avg:57.55ms
step:416/2330 train_time:23947ms step_avg:57.56ms
step:417/2330 train_time:24002ms step_avg:57.56ms
step:418/2330 train_time:24061ms step_avg:57.56ms
step:419/2330 train_time:24118ms step_avg:57.56ms
step:420/2330 train_time:24177ms step_avg:57.56ms
step:421/2330 train_time:24232ms step_avg:57.56ms
step:422/2330 train_time:24291ms step_avg:57.56ms
step:423/2330 train_time:24347ms step_avg:57.56ms
step:424/2330 train_time:24407ms step_avg:57.56ms
step:425/2330 train_time:24463ms step_avg:57.56ms
step:426/2330 train_time:24523ms step_avg:57.57ms
step:427/2330 train_time:24579ms step_avg:57.56ms
step:428/2330 train_time:24638ms step_avg:57.57ms
step:429/2330 train_time:24696ms step_avg:57.57ms
step:430/2330 train_time:24754ms step_avg:57.57ms
step:431/2330 train_time:24810ms step_avg:57.56ms
step:432/2330 train_time:24870ms step_avg:57.57ms
step:433/2330 train_time:24925ms step_avg:57.56ms
step:434/2330 train_time:24985ms step_avg:57.57ms
step:435/2330 train_time:25041ms step_avg:57.57ms
step:436/2330 train_time:25100ms step_avg:57.57ms
step:437/2330 train_time:25156ms step_avg:57.57ms
step:438/2330 train_time:25215ms step_avg:57.57ms
step:439/2330 train_time:25271ms step_avg:57.57ms
step:440/2330 train_time:25330ms step_avg:57.57ms
step:441/2330 train_time:25386ms step_avg:57.56ms
step:442/2330 train_time:25446ms step_avg:57.57ms
step:443/2330 train_time:25501ms step_avg:57.56ms
step:444/2330 train_time:25561ms step_avg:57.57ms
step:445/2330 train_time:25617ms step_avg:57.57ms
step:446/2330 train_time:25676ms step_avg:57.57ms
step:447/2330 train_time:25733ms step_avg:57.57ms
step:448/2330 train_time:25792ms step_avg:57.57ms
step:449/2330 train_time:25849ms step_avg:57.57ms
step:450/2330 train_time:25907ms step_avg:57.57ms
step:451/2330 train_time:25963ms step_avg:57.57ms
step:452/2330 train_time:26023ms step_avg:57.57ms
step:453/2330 train_time:26080ms step_avg:57.57ms
step:454/2330 train_time:26139ms step_avg:57.57ms
step:455/2330 train_time:26194ms step_avg:57.57ms
step:456/2330 train_time:26253ms step_avg:57.57ms
step:457/2330 train_time:26309ms step_avg:57.57ms
step:458/2330 train_time:26368ms step_avg:57.57ms
step:459/2330 train_time:26424ms step_avg:57.57ms
step:460/2330 train_time:26486ms step_avg:57.58ms
step:461/2330 train_time:26541ms step_avg:57.57ms
step:462/2330 train_time:26600ms step_avg:57.58ms
step:463/2330 train_time:26656ms step_avg:57.57ms
step:464/2330 train_time:26715ms step_avg:57.58ms
step:465/2330 train_time:26771ms step_avg:57.57ms
step:466/2330 train_time:26830ms step_avg:57.58ms
step:467/2330 train_time:26887ms step_avg:57.57ms
step:468/2330 train_time:26946ms step_avg:57.58ms
step:469/2330 train_time:27002ms step_avg:57.57ms
step:470/2330 train_time:27063ms step_avg:57.58ms
step:471/2330 train_time:27119ms step_avg:57.58ms
step:472/2330 train_time:27178ms step_avg:57.58ms
step:473/2330 train_time:27234ms step_avg:57.58ms
step:474/2330 train_time:27293ms step_avg:57.58ms
step:475/2330 train_time:27349ms step_avg:57.58ms
step:476/2330 train_time:27409ms step_avg:57.58ms
step:477/2330 train_time:27464ms step_avg:57.58ms
step:478/2330 train_time:27524ms step_avg:57.58ms
step:479/2330 train_time:27580ms step_avg:57.58ms
step:480/2330 train_time:27640ms step_avg:57.58ms
step:481/2330 train_time:27696ms step_avg:57.58ms
step:482/2330 train_time:27754ms step_avg:57.58ms
step:483/2330 train_time:27810ms step_avg:57.58ms
step:484/2330 train_time:27871ms step_avg:57.58ms
step:485/2330 train_time:27927ms step_avg:57.58ms
step:486/2330 train_time:27987ms step_avg:57.59ms
step:487/2330 train_time:28042ms step_avg:57.58ms
step:488/2330 train_time:28103ms step_avg:57.59ms
step:489/2330 train_time:28160ms step_avg:57.59ms
step:490/2330 train_time:28219ms step_avg:57.59ms
step:491/2330 train_time:28275ms step_avg:57.59ms
step:492/2330 train_time:28333ms step_avg:57.59ms
step:493/2330 train_time:28389ms step_avg:57.58ms
step:494/2330 train_time:28448ms step_avg:57.59ms
step:495/2330 train_time:28503ms step_avg:57.58ms
step:496/2330 train_time:28564ms step_avg:57.59ms
step:497/2330 train_time:28620ms step_avg:57.59ms
step:498/2330 train_time:28679ms step_avg:57.59ms
step:499/2330 train_time:28736ms step_avg:57.59ms
step:500/2330 train_time:28795ms step_avg:57.59ms
step:500/2330 val_loss:4.4123 train_time:28874ms step_avg:57.75ms
step:501/2330 train_time:28894ms step_avg:57.67ms
step:502/2330 train_time:28914ms step_avg:57.60ms
step:503/2330 train_time:28971ms step_avg:57.60ms
step:504/2330 train_time:29034ms step_avg:57.61ms
step:505/2330 train_time:29090ms step_avg:57.60ms
step:506/2330 train_time:29152ms step_avg:57.61ms
step:507/2330 train_time:29208ms step_avg:57.61ms
step:508/2330 train_time:29269ms step_avg:57.62ms
step:509/2330 train_time:29324ms step_avg:57.61ms
step:510/2330 train_time:29383ms step_avg:57.61ms
step:511/2330 train_time:29439ms step_avg:57.61ms
step:512/2330 train_time:29498ms step_avg:57.61ms
step:513/2330 train_time:29553ms step_avg:57.61ms
step:514/2330 train_time:29611ms step_avg:57.61ms
step:515/2330 train_time:29667ms step_avg:57.61ms
step:516/2330 train_time:29726ms step_avg:57.61ms
step:517/2330 train_time:29782ms step_avg:57.61ms
step:518/2330 train_time:29842ms step_avg:57.61ms
step:519/2330 train_time:29899ms step_avg:57.61ms
step:520/2330 train_time:29959ms step_avg:57.61ms
step:521/2330 train_time:30017ms step_avg:57.61ms
step:522/2330 train_time:30076ms step_avg:57.62ms
step:523/2330 train_time:30133ms step_avg:57.62ms
step:524/2330 train_time:30193ms step_avg:57.62ms
step:525/2330 train_time:30248ms step_avg:57.62ms
step:526/2330 train_time:30309ms step_avg:57.62ms
step:527/2330 train_time:30364ms step_avg:57.62ms
step:528/2330 train_time:30424ms step_avg:57.62ms
step:529/2330 train_time:30480ms step_avg:57.62ms
step:530/2330 train_time:30538ms step_avg:57.62ms
step:531/2330 train_time:30593ms step_avg:57.61ms
step:532/2330 train_time:30651ms step_avg:57.62ms
step:533/2330 train_time:30707ms step_avg:57.61ms
step:534/2330 train_time:30767ms step_avg:57.62ms
step:535/2330 train_time:30823ms step_avg:57.61ms
step:536/2330 train_time:30882ms step_avg:57.62ms
step:537/2330 train_time:30939ms step_avg:57.61ms
step:538/2330 train_time:30999ms step_avg:57.62ms
step:539/2330 train_time:31056ms step_avg:57.62ms
step:540/2330 train_time:31115ms step_avg:57.62ms
step:541/2330 train_time:31172ms step_avg:57.62ms
step:542/2330 train_time:31232ms step_avg:57.62ms
step:543/2330 train_time:31287ms step_avg:57.62ms
step:544/2330 train_time:31347ms step_avg:57.62ms
step:545/2330 train_time:31402ms step_avg:57.62ms
step:546/2330 train_time:31462ms step_avg:57.62ms
step:547/2330 train_time:31518ms step_avg:57.62ms
step:548/2330 train_time:31577ms step_avg:57.62ms
step:549/2330 train_time:31633ms step_avg:57.62ms
step:550/2330 train_time:31691ms step_avg:57.62ms
step:551/2330 train_time:31747ms step_avg:57.62ms
step:552/2330 train_time:31808ms step_avg:57.62ms
step:553/2330 train_time:31863ms step_avg:57.62ms
step:554/2330 train_time:31924ms step_avg:57.62ms
step:555/2330 train_time:31980ms step_avg:57.62ms
step:556/2330 train_time:32039ms step_avg:57.62ms
step:557/2330 train_time:32097ms step_avg:57.62ms
step:558/2330 train_time:32156ms step_avg:57.63ms
step:559/2330 train_time:32213ms step_avg:57.63ms
step:560/2330 train_time:32273ms step_avg:57.63ms
step:561/2330 train_time:32329ms step_avg:57.63ms
step:562/2330 train_time:32389ms step_avg:57.63ms
step:563/2330 train_time:32444ms step_avg:57.63ms
step:564/2330 train_time:32504ms step_avg:57.63ms
step:565/2330 train_time:32560ms step_avg:57.63ms
step:566/2330 train_time:32619ms step_avg:57.63ms
step:567/2330 train_time:32675ms step_avg:57.63ms
step:568/2330 train_time:32733ms step_avg:57.63ms
step:569/2330 train_time:32789ms step_avg:57.63ms
step:570/2330 train_time:32850ms step_avg:57.63ms
step:571/2330 train_time:32905ms step_avg:57.63ms
step:572/2330 train_time:32967ms step_avg:57.64ms
step:573/2330 train_time:33023ms step_avg:57.63ms
step:574/2330 train_time:33083ms step_avg:57.64ms
step:575/2330 train_time:33140ms step_avg:57.63ms
step:576/2330 train_time:33199ms step_avg:57.64ms
step:577/2330 train_time:33256ms step_avg:57.64ms
step:578/2330 train_time:33315ms step_avg:57.64ms
step:579/2330 train_time:33370ms step_avg:57.63ms
step:580/2330 train_time:33430ms step_avg:57.64ms
step:581/2330 train_time:33486ms step_avg:57.64ms
step:582/2330 train_time:33547ms step_avg:57.64ms
step:583/2330 train_time:33603ms step_avg:57.64ms
step:584/2330 train_time:33661ms step_avg:57.64ms
step:585/2330 train_time:33717ms step_avg:57.64ms
step:586/2330 train_time:33777ms step_avg:57.64ms
step:587/2330 train_time:33832ms step_avg:57.64ms
step:588/2330 train_time:33893ms step_avg:57.64ms
step:589/2330 train_time:33948ms step_avg:57.64ms
step:590/2330 train_time:34009ms step_avg:57.64ms
step:591/2330 train_time:34065ms step_avg:57.64ms
step:592/2330 train_time:34126ms step_avg:57.65ms
step:593/2330 train_time:34183ms step_avg:57.64ms
step:594/2330 train_time:34242ms step_avg:57.65ms
step:595/2330 train_time:34298ms step_avg:57.64ms
step:596/2330 train_time:34357ms step_avg:57.65ms
step:597/2330 train_time:34414ms step_avg:57.64ms
step:598/2330 train_time:34473ms step_avg:57.65ms
step:599/2330 train_time:34528ms step_avg:57.64ms
step:600/2330 train_time:34588ms step_avg:57.65ms
step:601/2330 train_time:34643ms step_avg:57.64ms
step:602/2330 train_time:34704ms step_avg:57.65ms
step:603/2330 train_time:34760ms step_avg:57.65ms
step:604/2330 train_time:34819ms step_avg:57.65ms
step:605/2330 train_time:34876ms step_avg:57.65ms
step:606/2330 train_time:34935ms step_avg:57.65ms
step:607/2330 train_time:34991ms step_avg:57.65ms
step:608/2330 train_time:35051ms step_avg:57.65ms
step:609/2330 train_time:35107ms step_avg:57.65ms
step:610/2330 train_time:35168ms step_avg:57.65ms
step:611/2330 train_time:35223ms step_avg:57.65ms
step:612/2330 train_time:35283ms step_avg:57.65ms
step:613/2330 train_time:35339ms step_avg:57.65ms
step:614/2330 train_time:35399ms step_avg:57.65ms
step:615/2330 train_time:35454ms step_avg:57.65ms
step:616/2330 train_time:35513ms step_avg:57.65ms
step:617/2330 train_time:35569ms step_avg:57.65ms
step:618/2330 train_time:35629ms step_avg:57.65ms
step:619/2330 train_time:35684ms step_avg:57.65ms
step:620/2330 train_time:35745ms step_avg:57.65ms
step:621/2330 train_time:35802ms step_avg:57.65ms
step:622/2330 train_time:35861ms step_avg:57.65ms
step:623/2330 train_time:35917ms step_avg:57.65ms
step:624/2330 train_time:35976ms step_avg:57.65ms
step:625/2330 train_time:36032ms step_avg:57.65ms
step:626/2330 train_time:36091ms step_avg:57.65ms
step:627/2330 train_time:36147ms step_avg:57.65ms
step:628/2330 train_time:36208ms step_avg:57.66ms
step:629/2330 train_time:36263ms step_avg:57.65ms
step:630/2330 train_time:36324ms step_avg:57.66ms
step:631/2330 train_time:36380ms step_avg:57.65ms
step:632/2330 train_time:36439ms step_avg:57.66ms
step:633/2330 train_time:36496ms step_avg:57.65ms
step:634/2330 train_time:36554ms step_avg:57.66ms
step:635/2330 train_time:36610ms step_avg:57.65ms
step:636/2330 train_time:36670ms step_avg:57.66ms
step:637/2330 train_time:36726ms step_avg:57.65ms
step:638/2330 train_time:36786ms step_avg:57.66ms
step:639/2330 train_time:36842ms step_avg:57.66ms
step:640/2330 train_time:36902ms step_avg:57.66ms
step:641/2330 train_time:36958ms step_avg:57.66ms
step:642/2330 train_time:37018ms step_avg:57.66ms
step:643/2330 train_time:37074ms step_avg:57.66ms
step:644/2330 train_time:37133ms step_avg:57.66ms
step:645/2330 train_time:37190ms step_avg:57.66ms
step:646/2330 train_time:37250ms step_avg:57.66ms
step:647/2330 train_time:37305ms step_avg:57.66ms
step:648/2330 train_time:37366ms step_avg:57.66ms
step:649/2330 train_time:37422ms step_avg:57.66ms
step:650/2330 train_time:37481ms step_avg:57.66ms
step:651/2330 train_time:37538ms step_avg:57.66ms
step:652/2330 train_time:37597ms step_avg:57.66ms
step:653/2330 train_time:37653ms step_avg:57.66ms
step:654/2330 train_time:37712ms step_avg:57.66ms
step:655/2330 train_time:37768ms step_avg:57.66ms
step:656/2330 train_time:37828ms step_avg:57.67ms
step:657/2330 train_time:37883ms step_avg:57.66ms
step:658/2330 train_time:37944ms step_avg:57.67ms
step:659/2330 train_time:38000ms step_avg:57.66ms
step:660/2330 train_time:38060ms step_avg:57.67ms
step:661/2330 train_time:38116ms step_avg:57.66ms
step:662/2330 train_time:38176ms step_avg:57.67ms
step:663/2330 train_time:38232ms step_avg:57.66ms
step:664/2330 train_time:38291ms step_avg:57.67ms
step:665/2330 train_time:38346ms step_avg:57.66ms
step:666/2330 train_time:38407ms step_avg:57.67ms
step:667/2330 train_time:38463ms step_avg:57.67ms
step:668/2330 train_time:38523ms step_avg:57.67ms
step:669/2330 train_time:38578ms step_avg:57.67ms
step:670/2330 train_time:38638ms step_avg:57.67ms
step:671/2330 train_time:38693ms step_avg:57.67ms
step:672/2330 train_time:38752ms step_avg:57.67ms
step:673/2330 train_time:38808ms step_avg:57.66ms
step:674/2330 train_time:38869ms step_avg:57.67ms
step:675/2330 train_time:38924ms step_avg:57.67ms
step:676/2330 train_time:38986ms step_avg:57.67ms
step:677/2330 train_time:39041ms step_avg:57.67ms
step:678/2330 train_time:39101ms step_avg:57.67ms
step:679/2330 train_time:39157ms step_avg:57.67ms
step:680/2330 train_time:39216ms step_avg:57.67ms
step:681/2330 train_time:39271ms step_avg:57.67ms
step:682/2330 train_time:39331ms step_avg:57.67ms
step:683/2330 train_time:39386ms step_avg:57.67ms
step:684/2330 train_time:39447ms step_avg:57.67ms
step:685/2330 train_time:39502ms step_avg:57.67ms
step:686/2330 train_time:39562ms step_avg:57.67ms
step:687/2330 train_time:39618ms step_avg:57.67ms
step:688/2330 train_time:39677ms step_avg:57.67ms
step:689/2330 train_time:39733ms step_avg:57.67ms
step:690/2330 train_time:39792ms step_avg:57.67ms
step:691/2330 train_time:39847ms step_avg:57.67ms
step:692/2330 train_time:39909ms step_avg:57.67ms
step:693/2330 train_time:39964ms step_avg:57.67ms
step:694/2330 train_time:40026ms step_avg:57.67ms
step:695/2330 train_time:40081ms step_avg:57.67ms
step:696/2330 train_time:40140ms step_avg:57.67ms
step:697/2330 train_time:40196ms step_avg:57.67ms
step:698/2330 train_time:40255ms step_avg:57.67ms
step:699/2330 train_time:40312ms step_avg:57.67ms
step:700/2330 train_time:40370ms step_avg:57.67ms
step:701/2330 train_time:40427ms step_avg:57.67ms
step:702/2330 train_time:40487ms step_avg:57.67ms
step:703/2330 train_time:40542ms step_avg:57.67ms
step:704/2330 train_time:40602ms step_avg:57.67ms
step:705/2330 train_time:40659ms step_avg:57.67ms
step:706/2330 train_time:40718ms step_avg:57.67ms
step:707/2330 train_time:40774ms step_avg:57.67ms
step:708/2330 train_time:40833ms step_avg:57.67ms
step:709/2330 train_time:40890ms step_avg:57.67ms
step:710/2330 train_time:40949ms step_avg:57.67ms
step:711/2330 train_time:41004ms step_avg:57.67ms
step:712/2330 train_time:41066ms step_avg:57.68ms
step:713/2330 train_time:41122ms step_avg:57.67ms
step:714/2330 train_time:41181ms step_avg:57.68ms
step:715/2330 train_time:41237ms step_avg:57.67ms
step:716/2330 train_time:41296ms step_avg:57.68ms
step:717/2330 train_time:41352ms step_avg:57.67ms
step:718/2330 train_time:41411ms step_avg:57.68ms
step:719/2330 train_time:41467ms step_avg:57.67ms
step:720/2330 train_time:41527ms step_avg:57.68ms
step:721/2330 train_time:41583ms step_avg:57.67ms
step:722/2330 train_time:41643ms step_avg:57.68ms
step:723/2330 train_time:41699ms step_avg:57.67ms
step:724/2330 train_time:41758ms step_avg:57.68ms
step:725/2330 train_time:41815ms step_avg:57.68ms
step:726/2330 train_time:41874ms step_avg:57.68ms
step:727/2330 train_time:41930ms step_avg:57.68ms
step:728/2330 train_time:41991ms step_avg:57.68ms
step:729/2330 train_time:42047ms step_avg:57.68ms
step:730/2330 train_time:42107ms step_avg:57.68ms
step:731/2330 train_time:42163ms step_avg:57.68ms
step:732/2330 train_time:42223ms step_avg:57.68ms
step:733/2330 train_time:42279ms step_avg:57.68ms
step:734/2330 train_time:42338ms step_avg:57.68ms
step:735/2330 train_time:42394ms step_avg:57.68ms
step:736/2330 train_time:42452ms step_avg:57.68ms
step:737/2330 train_time:42508ms step_avg:57.68ms
step:738/2330 train_time:42568ms step_avg:57.68ms
step:739/2330 train_time:42624ms step_avg:57.68ms
step:740/2330 train_time:42685ms step_avg:57.68ms
step:741/2330 train_time:42741ms step_avg:57.68ms
step:742/2330 train_time:42801ms step_avg:57.68ms
step:743/2330 train_time:42857ms step_avg:57.68ms
step:744/2330 train_time:42916ms step_avg:57.68ms
step:745/2330 train_time:42972ms step_avg:57.68ms
step:746/2330 train_time:43031ms step_avg:57.68ms
step:747/2330 train_time:43087ms step_avg:57.68ms
step:748/2330 train_time:43147ms step_avg:57.68ms
step:749/2330 train_time:43204ms step_avg:57.68ms
step:750/2330 train_time:43263ms step_avg:57.68ms
step:750/2330 val_loss:4.2125 train_time:43342ms step_avg:57.79ms
step:751/2330 train_time:43362ms step_avg:57.74ms
step:752/2330 train_time:43381ms step_avg:57.69ms
step:753/2330 train_time:43439ms step_avg:57.69ms
step:754/2330 train_time:43503ms step_avg:57.70ms
step:755/2330 train_time:43560ms step_avg:57.69ms
step:756/2330 train_time:43619ms step_avg:57.70ms
step:757/2330 train_time:43676ms step_avg:57.70ms
step:758/2330 train_time:43735ms step_avg:57.70ms
step:759/2330 train_time:43791ms step_avg:57.70ms
step:760/2330 train_time:43849ms step_avg:57.70ms
step:761/2330 train_time:43905ms step_avg:57.69ms
step:762/2330 train_time:43964ms step_avg:57.69ms
step:763/2330 train_time:44019ms step_avg:57.69ms
step:764/2330 train_time:44077ms step_avg:57.69ms
step:765/2330 train_time:44134ms step_avg:57.69ms
step:766/2330 train_time:44192ms step_avg:57.69ms
step:767/2330 train_time:44249ms step_avg:57.69ms
step:768/2330 train_time:44309ms step_avg:57.69ms
step:769/2330 train_time:44367ms step_avg:57.69ms
step:770/2330 train_time:44430ms step_avg:57.70ms
step:771/2330 train_time:44487ms step_avg:57.70ms
step:772/2330 train_time:44550ms step_avg:57.71ms
step:773/2330 train_time:44607ms step_avg:57.71ms
step:774/2330 train_time:44669ms step_avg:57.71ms
step:775/2330 train_time:44725ms step_avg:57.71ms
step:776/2330 train_time:44786ms step_avg:57.71ms
step:777/2330 train_time:44843ms step_avg:57.71ms
step:778/2330 train_time:44903ms step_avg:57.72ms
step:779/2330 train_time:44960ms step_avg:57.71ms
step:780/2330 train_time:45019ms step_avg:57.72ms
step:781/2330 train_time:45076ms step_avg:57.72ms
step:782/2330 train_time:45135ms step_avg:57.72ms
step:783/2330 train_time:45192ms step_avg:57.72ms
step:784/2330 train_time:45251ms step_avg:57.72ms
step:785/2330 train_time:45309ms step_avg:57.72ms
step:786/2330 train_time:45369ms step_avg:57.72ms
step:787/2330 train_time:45427ms step_avg:57.72ms
step:788/2330 train_time:45487ms step_avg:57.72ms
step:789/2330 train_time:45545ms step_avg:57.72ms
step:790/2330 train_time:45606ms step_avg:57.73ms
step:791/2330 train_time:45664ms step_avg:57.73ms
step:792/2330 train_time:45724ms step_avg:57.73ms
step:793/2330 train_time:45781ms step_avg:57.73ms
step:794/2330 train_time:45842ms step_avg:57.74ms
step:795/2330 train_time:45898ms step_avg:57.73ms
step:796/2330 train_time:45958ms step_avg:57.74ms
step:797/2330 train_time:46015ms step_avg:57.74ms
step:798/2330 train_time:46074ms step_avg:57.74ms
step:799/2330 train_time:46130ms step_avg:57.74ms
step:800/2330 train_time:46190ms step_avg:57.74ms
step:801/2330 train_time:46248ms step_avg:57.74ms
step:802/2330 train_time:46307ms step_avg:57.74ms
step:803/2330 train_time:46365ms step_avg:57.74ms
step:804/2330 train_time:46425ms step_avg:57.74ms
step:805/2330 train_time:46482ms step_avg:57.74ms
step:806/2330 train_time:46545ms step_avg:57.75ms
step:807/2330 train_time:46602ms step_avg:57.75ms
step:808/2330 train_time:46662ms step_avg:57.75ms
step:809/2330 train_time:46720ms step_avg:57.75ms
step:810/2330 train_time:46779ms step_avg:57.75ms
step:811/2330 train_time:46836ms step_avg:57.75ms
step:812/2330 train_time:46895ms step_avg:57.75ms
step:813/2330 train_time:46952ms step_avg:57.75ms
step:814/2330 train_time:47011ms step_avg:57.75ms
step:815/2330 train_time:47067ms step_avg:57.75ms
step:816/2330 train_time:47129ms step_avg:57.76ms
step:817/2330 train_time:47185ms step_avg:57.75ms
step:818/2330 train_time:47245ms step_avg:57.76ms
step:819/2330 train_time:47302ms step_avg:57.76ms
step:820/2330 train_time:47363ms step_avg:57.76ms
step:821/2330 train_time:47420ms step_avg:57.76ms
step:822/2330 train_time:47481ms step_avg:57.76ms
step:823/2330 train_time:47538ms step_avg:57.76ms
step:824/2330 train_time:47598ms step_avg:57.76ms
step:825/2330 train_time:47655ms step_avg:57.76ms
step:826/2330 train_time:47715ms step_avg:57.77ms
step:827/2330 train_time:47772ms step_avg:57.77ms
step:828/2330 train_time:47831ms step_avg:57.77ms
step:829/2330 train_time:47888ms step_avg:57.77ms
step:830/2330 train_time:47949ms step_avg:57.77ms
step:831/2330 train_time:48006ms step_avg:57.77ms
step:832/2330 train_time:48065ms step_avg:57.77ms
step:833/2330 train_time:48122ms step_avg:57.77ms
step:834/2330 train_time:48183ms step_avg:57.77ms
step:835/2330 train_time:48240ms step_avg:57.77ms
step:836/2330 train_time:48300ms step_avg:57.77ms
step:837/2330 train_time:48357ms step_avg:57.77ms
step:838/2330 train_time:48416ms step_avg:57.78ms
step:839/2330 train_time:48474ms step_avg:57.78ms
step:840/2330 train_time:48533ms step_avg:57.78ms
step:841/2330 train_time:48591ms step_avg:57.78ms
step:842/2330 train_time:48651ms step_avg:57.78ms
step:843/2330 train_time:48708ms step_avg:57.78ms
step:844/2330 train_time:48769ms step_avg:57.78ms
step:845/2330 train_time:48826ms step_avg:57.78ms
step:846/2330 train_time:48886ms step_avg:57.79ms
step:847/2330 train_time:48943ms step_avg:57.78ms
step:848/2330 train_time:49004ms step_avg:57.79ms
step:849/2330 train_time:49061ms step_avg:57.79ms
step:850/2330 train_time:49121ms step_avg:57.79ms
step:851/2330 train_time:49177ms step_avg:57.79ms
step:852/2330 train_time:49237ms step_avg:57.79ms
step:853/2330 train_time:49295ms step_avg:57.79ms
step:854/2330 train_time:49354ms step_avg:57.79ms
step:855/2330 train_time:49411ms step_avg:57.79ms
step:856/2330 train_time:49471ms step_avg:57.79ms
step:857/2330 train_time:49529ms step_avg:57.79ms
step:858/2330 train_time:49588ms step_avg:57.80ms
step:859/2330 train_time:49646ms step_avg:57.79ms
step:860/2330 train_time:49706ms step_avg:57.80ms
step:861/2330 train_time:49763ms step_avg:57.80ms
step:862/2330 train_time:49825ms step_avg:57.80ms
step:863/2330 train_time:49882ms step_avg:57.80ms
step:864/2330 train_time:49942ms step_avg:57.80ms
step:865/2330 train_time:49999ms step_avg:57.80ms
step:866/2330 train_time:50059ms step_avg:57.80ms
step:867/2330 train_time:50116ms step_avg:57.80ms
step:868/2330 train_time:50176ms step_avg:57.81ms
step:869/2330 train_time:50233ms step_avg:57.81ms
step:870/2330 train_time:50293ms step_avg:57.81ms
step:871/2330 train_time:50350ms step_avg:57.81ms
step:872/2330 train_time:50410ms step_avg:57.81ms
step:873/2330 train_time:50467ms step_avg:57.81ms
step:874/2330 train_time:50528ms step_avg:57.81ms
step:875/2330 train_time:50585ms step_avg:57.81ms
step:876/2330 train_time:50646ms step_avg:57.81ms
step:877/2330 train_time:50703ms step_avg:57.81ms
step:878/2330 train_time:50764ms step_avg:57.82ms
step:879/2330 train_time:50821ms step_avg:57.82ms
step:880/2330 train_time:50881ms step_avg:57.82ms
step:881/2330 train_time:50938ms step_avg:57.82ms
step:882/2330 train_time:50998ms step_avg:57.82ms
step:883/2330 train_time:51054ms step_avg:57.82ms
step:884/2330 train_time:51114ms step_avg:57.82ms
step:885/2330 train_time:51171ms step_avg:57.82ms
step:886/2330 train_time:51230ms step_avg:57.82ms
step:887/2330 train_time:51287ms step_avg:57.82ms
step:888/2330 train_time:51347ms step_avg:57.82ms
step:889/2330 train_time:51405ms step_avg:57.82ms
step:890/2330 train_time:51465ms step_avg:57.83ms
step:891/2330 train_time:51522ms step_avg:57.82ms
step:892/2330 train_time:51582ms step_avg:57.83ms
step:893/2330 train_time:51639ms step_avg:57.83ms
step:894/2330 train_time:51699ms step_avg:57.83ms
step:895/2330 train_time:51756ms step_avg:57.83ms
step:896/2330 train_time:51816ms step_avg:57.83ms
step:897/2330 train_time:51874ms step_avg:57.83ms
step:898/2330 train_time:51933ms step_avg:57.83ms
step:899/2330 train_time:51990ms step_avg:57.83ms
step:900/2330 train_time:52052ms step_avg:57.84ms
step:901/2330 train_time:52109ms step_avg:57.83ms
step:902/2330 train_time:52169ms step_avg:57.84ms
step:903/2330 train_time:52225ms step_avg:57.84ms
step:904/2330 train_time:52286ms step_avg:57.84ms
step:905/2330 train_time:52342ms step_avg:57.84ms
step:906/2330 train_time:52404ms step_avg:57.84ms
step:907/2330 train_time:52461ms step_avg:57.84ms
step:908/2330 train_time:52522ms step_avg:57.84ms
step:909/2330 train_time:52579ms step_avg:57.84ms
step:910/2330 train_time:52639ms step_avg:57.84ms
step:911/2330 train_time:52696ms step_avg:57.84ms
step:912/2330 train_time:52756ms step_avg:57.85ms
step:913/2330 train_time:52814ms step_avg:57.85ms
step:914/2330 train_time:52874ms step_avg:57.85ms
step:915/2330 train_time:52933ms step_avg:57.85ms
step:916/2330 train_time:52993ms step_avg:57.85ms
step:917/2330 train_time:53049ms step_avg:57.85ms
step:918/2330 train_time:53110ms step_avg:57.85ms
step:919/2330 train_time:53167ms step_avg:57.85ms
step:920/2330 train_time:53226ms step_avg:57.85ms
step:921/2330 train_time:53282ms step_avg:57.85ms
step:922/2330 train_time:53344ms step_avg:57.86ms
step:923/2330 train_time:53400ms step_avg:57.86ms
step:924/2330 train_time:53460ms step_avg:57.86ms
step:925/2330 train_time:53517ms step_avg:57.86ms
step:926/2330 train_time:53577ms step_avg:57.86ms
step:927/2330 train_time:53634ms step_avg:57.86ms
step:928/2330 train_time:53693ms step_avg:57.86ms
step:929/2330 train_time:53749ms step_avg:57.86ms
step:930/2330 train_time:53812ms step_avg:57.86ms
step:931/2330 train_time:53869ms step_avg:57.86ms
step:932/2330 train_time:53929ms step_avg:57.86ms
step:933/2330 train_time:53986ms step_avg:57.86ms
step:934/2330 train_time:54046ms step_avg:57.87ms
step:935/2330 train_time:54103ms step_avg:57.86ms
step:936/2330 train_time:54164ms step_avg:57.87ms
step:937/2330 train_time:54221ms step_avg:57.87ms
step:938/2330 train_time:54280ms step_avg:57.87ms
step:939/2330 train_time:54337ms step_avg:57.87ms
step:940/2330 train_time:54397ms step_avg:57.87ms
step:941/2330 train_time:54454ms step_avg:57.87ms
step:942/2330 train_time:54513ms step_avg:57.87ms
step:943/2330 train_time:54570ms step_avg:57.87ms
step:944/2330 train_time:54630ms step_avg:57.87ms
step:945/2330 train_time:54687ms step_avg:57.87ms
step:946/2330 train_time:54748ms step_avg:57.87ms
step:947/2330 train_time:54805ms step_avg:57.87ms
step:948/2330 train_time:54865ms step_avg:57.87ms
step:949/2330 train_time:54922ms step_avg:57.87ms
step:950/2330 train_time:54982ms step_avg:57.88ms
step:951/2330 train_time:55040ms step_avg:57.88ms
step:952/2330 train_time:55099ms step_avg:57.88ms
step:953/2330 train_time:55157ms step_avg:57.88ms
step:954/2330 train_time:55217ms step_avg:57.88ms
step:955/2330 train_time:55273ms step_avg:57.88ms
step:956/2330 train_time:55333ms step_avg:57.88ms
step:957/2330 train_time:55390ms step_avg:57.88ms
step:958/2330 train_time:55451ms step_avg:57.88ms
step:959/2330 train_time:55507ms step_avg:57.88ms
step:960/2330 train_time:55568ms step_avg:57.88ms
step:961/2330 train_time:55625ms step_avg:57.88ms
step:962/2330 train_time:55686ms step_avg:57.89ms
step:963/2330 train_time:55742ms step_avg:57.88ms
step:964/2330 train_time:55803ms step_avg:57.89ms
step:965/2330 train_time:55861ms step_avg:57.89ms
step:966/2330 train_time:55921ms step_avg:57.89ms
step:967/2330 train_time:55978ms step_avg:57.89ms
step:968/2330 train_time:56038ms step_avg:57.89ms
step:969/2330 train_time:56094ms step_avg:57.89ms
step:970/2330 train_time:56154ms step_avg:57.89ms
step:971/2330 train_time:56211ms step_avg:57.89ms
step:972/2330 train_time:56270ms step_avg:57.89ms
step:973/2330 train_time:56328ms step_avg:57.89ms
step:974/2330 train_time:56388ms step_avg:57.89ms
step:975/2330 train_time:56446ms step_avg:57.89ms
step:976/2330 train_time:56506ms step_avg:57.90ms
step:977/2330 train_time:56563ms step_avg:57.89ms
step:978/2330 train_time:56624ms step_avg:57.90ms
step:979/2330 train_time:56681ms step_avg:57.90ms
step:980/2330 train_time:56741ms step_avg:57.90ms
step:981/2330 train_time:56797ms step_avg:57.90ms
step:982/2330 train_time:56859ms step_avg:57.90ms
step:983/2330 train_time:56916ms step_avg:57.90ms
step:984/2330 train_time:56976ms step_avg:57.90ms
step:985/2330 train_time:57033ms step_avg:57.90ms
step:986/2330 train_time:57093ms step_avg:57.90ms
step:987/2330 train_time:57149ms step_avg:57.90ms
step:988/2330 train_time:57210ms step_avg:57.90ms
step:989/2330 train_time:57267ms step_avg:57.90ms
step:990/2330 train_time:57327ms step_avg:57.91ms
step:991/2330 train_time:57384ms step_avg:57.91ms
step:992/2330 train_time:57444ms step_avg:57.91ms
step:993/2330 train_time:57501ms step_avg:57.91ms
step:994/2330 train_time:57563ms step_avg:57.91ms
step:995/2330 train_time:57620ms step_avg:57.91ms
step:996/2330 train_time:57680ms step_avg:57.91ms
step:997/2330 train_time:57737ms step_avg:57.91ms
step:998/2330 train_time:57797ms step_avg:57.91ms
step:999/2330 train_time:57854ms step_avg:57.91ms
step:1000/2330 train_time:57914ms step_avg:57.91ms
step:1000/2330 val_loss:4.0719 train_time:57994ms step_avg:57.99ms
step:1001/2330 train_time:58014ms step_avg:57.96ms
step:1002/2330 train_time:58034ms step_avg:57.92ms
step:1003/2330 train_time:58094ms step_avg:57.92ms
step:1004/2330 train_time:58158ms step_avg:57.93ms
step:1005/2330 train_time:58215ms step_avg:57.93ms
step:1006/2330 train_time:58275ms step_avg:57.93ms
step:1007/2330 train_time:58331ms step_avg:57.93ms
step:1008/2330 train_time:58390ms step_avg:57.93ms
step:1009/2330 train_time:58446ms step_avg:57.93ms
step:1010/2330 train_time:58506ms step_avg:57.93ms
step:1011/2330 train_time:58562ms step_avg:57.92ms
step:1012/2330 train_time:58621ms step_avg:57.93ms
step:1013/2330 train_time:58677ms step_avg:57.92ms
step:1014/2330 train_time:58735ms step_avg:57.92ms
step:1015/2330 train_time:58791ms step_avg:57.92ms
step:1016/2330 train_time:58851ms step_avg:57.92ms
step:1017/2330 train_time:58907ms step_avg:57.92ms
step:1018/2330 train_time:58974ms step_avg:57.93ms
step:1019/2330 train_time:59031ms step_avg:57.93ms
step:1020/2330 train_time:59095ms step_avg:57.94ms
step:1021/2330 train_time:59152ms step_avg:57.94ms
step:1022/2330 train_time:59212ms step_avg:57.94ms
step:1023/2330 train_time:59270ms step_avg:57.94ms
step:1024/2330 train_time:59329ms step_avg:57.94ms
step:1025/2330 train_time:59385ms step_avg:57.94ms
step:1026/2330 train_time:59446ms step_avg:57.94ms
step:1027/2330 train_time:59502ms step_avg:57.94ms
step:1028/2330 train_time:59562ms step_avg:57.94ms
step:1029/2330 train_time:59618ms step_avg:57.94ms
step:1030/2330 train_time:59677ms step_avg:57.94ms
step:1031/2330 train_time:59734ms step_avg:57.94ms
step:1032/2330 train_time:59793ms step_avg:57.94ms
step:1033/2330 train_time:59850ms step_avg:57.94ms
step:1034/2330 train_time:59911ms step_avg:57.94ms
step:1035/2330 train_time:59969ms step_avg:57.94ms
step:1036/2330 train_time:60031ms step_avg:57.95ms
step:1037/2330 train_time:60089ms step_avg:57.94ms
step:1038/2330 train_time:60151ms step_avg:57.95ms
step:1039/2330 train_time:60207ms step_avg:57.95ms
step:1040/2330 train_time:60268ms step_avg:57.95ms
step:1041/2330 train_time:60324ms step_avg:57.95ms
step:1042/2330 train_time:60386ms step_avg:57.95ms
step:1043/2330 train_time:60443ms step_avg:57.95ms
step:1044/2330 train_time:60501ms step_avg:57.95ms
step:1045/2330 train_time:60558ms step_avg:57.95ms
step:1046/2330 train_time:60617ms step_avg:57.95ms
step:1047/2330 train_time:60674ms step_avg:57.95ms
step:1048/2330 train_time:60734ms step_avg:57.95ms
step:1049/2330 train_time:60791ms step_avg:57.95ms
step:1050/2330 train_time:60850ms step_avg:57.95ms
step:1051/2330 train_time:60907ms step_avg:57.95ms
step:1052/2330 train_time:60969ms step_avg:57.96ms
step:1053/2330 train_time:61025ms step_avg:57.95ms
step:1054/2330 train_time:61088ms step_avg:57.96ms
step:1055/2330 train_time:61145ms step_avg:57.96ms
step:1056/2330 train_time:61205ms step_avg:57.96ms
step:1057/2330 train_time:61262ms step_avg:57.96ms
step:1058/2330 train_time:61322ms step_avg:57.96ms
step:1059/2330 train_time:61379ms step_avg:57.96ms
step:1060/2330 train_time:61439ms step_avg:57.96ms
step:1061/2330 train_time:61495ms step_avg:57.96ms
step:1062/2330 train_time:61555ms step_avg:57.96ms
step:1063/2330 train_time:61611ms step_avg:57.96ms
step:1064/2330 train_time:61671ms step_avg:57.96ms
step:1065/2330 train_time:61727ms step_avg:57.96ms
step:1066/2330 train_time:61788ms step_avg:57.96ms
step:1067/2330 train_time:61845ms step_avg:57.96ms
step:1068/2330 train_time:61905ms step_avg:57.96ms
step:1069/2330 train_time:61962ms step_avg:57.96ms
step:1070/2330 train_time:62023ms step_avg:57.97ms
step:1071/2330 train_time:62080ms step_avg:57.96ms
step:1072/2330 train_time:62139ms step_avg:57.97ms
step:1073/2330 train_time:62197ms step_avg:57.97ms
step:1074/2330 train_time:62257ms step_avg:57.97ms
step:1075/2330 train_time:62313ms step_avg:57.97ms
step:1076/2330 train_time:62375ms step_avg:57.97ms
step:1077/2330 train_time:62431ms step_avg:57.97ms
step:1078/2330 train_time:62492ms step_avg:57.97ms
step:1079/2330 train_time:62548ms step_avg:57.97ms
step:1080/2330 train_time:62609ms step_avg:57.97ms
step:1081/2330 train_time:62665ms step_avg:57.97ms
step:1082/2330 train_time:62725ms step_avg:57.97ms
step:1083/2330 train_time:62782ms step_avg:57.97ms
step:1084/2330 train_time:62842ms step_avg:57.97ms
step:1085/2330 train_time:62899ms step_avg:57.97ms
step:1086/2330 train_time:62959ms step_avg:57.97ms
step:1087/2330 train_time:63015ms step_avg:57.97ms
step:1088/2330 train_time:63076ms step_avg:57.97ms
step:1089/2330 train_time:63133ms step_avg:57.97ms
step:1090/2330 train_time:63194ms step_avg:57.98ms
step:1091/2330 train_time:63251ms step_avg:57.97ms
step:1092/2330 train_time:63311ms step_avg:57.98ms
step:1093/2330 train_time:63367ms step_avg:57.98ms
step:1094/2330 train_time:63430ms step_avg:57.98ms
step:1095/2330 train_time:63486ms step_avg:57.98ms
step:1096/2330 train_time:63548ms step_avg:57.98ms
step:1097/2330 train_time:63604ms step_avg:57.98ms
step:1098/2330 train_time:63664ms step_avg:57.98ms
step:1099/2330 train_time:63720ms step_avg:57.98ms
step:1100/2330 train_time:63781ms step_avg:57.98ms
step:1101/2330 train_time:63837ms step_avg:57.98ms
step:1102/2330 train_time:63898ms step_avg:57.98ms
step:1103/2330 train_time:63954ms step_avg:57.98ms
step:1104/2330 train_time:64015ms step_avg:57.98ms
step:1105/2330 train_time:64072ms step_avg:57.98ms
step:1106/2330 train_time:64133ms step_avg:57.99ms
step:1107/2330 train_time:64190ms step_avg:57.99ms
step:1108/2330 train_time:64251ms step_avg:57.99ms
step:1109/2330 train_time:64307ms step_avg:57.99ms
step:1110/2330 train_time:64369ms step_avg:57.99ms
step:1111/2330 train_time:64425ms step_avg:57.99ms
step:1112/2330 train_time:64486ms step_avg:57.99ms
step:1113/2330 train_time:64542ms step_avg:57.99ms
step:1114/2330 train_time:64602ms step_avg:57.99ms
step:1115/2330 train_time:64659ms step_avg:57.99ms
step:1116/2330 train_time:64719ms step_avg:57.99ms
step:1117/2330 train_time:64776ms step_avg:57.99ms
step:1118/2330 train_time:64836ms step_avg:57.99ms
step:1119/2330 train_time:64893ms step_avg:57.99ms
step:1120/2330 train_time:64953ms step_avg:57.99ms
step:1121/2330 train_time:65009ms step_avg:57.99ms
step:1122/2330 train_time:65070ms step_avg:57.99ms
step:1123/2330 train_time:65127ms step_avg:57.99ms
step:1124/2330 train_time:65188ms step_avg:58.00ms
step:1125/2330 train_time:65245ms step_avg:58.00ms
step:1126/2330 train_time:65305ms step_avg:58.00ms
step:1127/2330 train_time:65362ms step_avg:58.00ms
step:1128/2330 train_time:65422ms step_avg:58.00ms
step:1129/2330 train_time:65479ms step_avg:58.00ms
step:1130/2330 train_time:65539ms step_avg:58.00ms
step:1131/2330 train_time:65595ms step_avg:58.00ms
step:1132/2330 train_time:65656ms step_avg:58.00ms
step:1133/2330 train_time:65712ms step_avg:58.00ms
step:1134/2330 train_time:65773ms step_avg:58.00ms
step:1135/2330 train_time:65830ms step_avg:58.00ms
step:1136/2330 train_time:65890ms step_avg:58.00ms
step:1137/2330 train_time:65946ms step_avg:58.00ms
step:1138/2330 train_time:66007ms step_avg:58.00ms
step:1139/2330 train_time:66064ms step_avg:58.00ms
step:1140/2330 train_time:66125ms step_avg:58.00ms
step:1141/2330 train_time:66182ms step_avg:58.00ms
step:1142/2330 train_time:66242ms step_avg:58.00ms
step:1143/2330 train_time:66298ms step_avg:58.00ms
step:1144/2330 train_time:66359ms step_avg:58.01ms
step:1145/2330 train_time:66415ms step_avg:58.00ms
step:1146/2330 train_time:66476ms step_avg:58.01ms
step:1147/2330 train_time:66532ms step_avg:58.01ms
step:1148/2330 train_time:66594ms step_avg:58.01ms
step:1149/2330 train_time:66650ms step_avg:58.01ms
step:1150/2330 train_time:66710ms step_avg:58.01ms
step:1151/2330 train_time:66766ms step_avg:58.01ms
step:1152/2330 train_time:66828ms step_avg:58.01ms
step:1153/2330 train_time:66885ms step_avg:58.01ms
step:1154/2330 train_time:66945ms step_avg:58.01ms
step:1155/2330 train_time:67001ms step_avg:58.01ms
step:1156/2330 train_time:67062ms step_avg:58.01ms
step:1157/2330 train_time:67119ms step_avg:58.01ms
step:1158/2330 train_time:67178ms step_avg:58.01ms
step:1159/2330 train_time:67235ms step_avg:58.01ms
step:1160/2330 train_time:67296ms step_avg:58.01ms
step:1161/2330 train_time:67352ms step_avg:58.01ms
step:1162/2330 train_time:67413ms step_avg:58.01ms
step:1163/2330 train_time:67470ms step_avg:58.01ms
step:1164/2330 train_time:67531ms step_avg:58.02ms
step:1165/2330 train_time:67587ms step_avg:58.01ms
step:1166/2330 train_time:67648ms step_avg:58.02ms
step:1167/2330 train_time:67705ms step_avg:58.02ms
step:1168/2330 train_time:67765ms step_avg:58.02ms
step:1169/2330 train_time:67821ms step_avg:58.02ms
step:1170/2330 train_time:67882ms step_avg:58.02ms
step:1171/2330 train_time:67939ms step_avg:58.02ms
step:1172/2330 train_time:67999ms step_avg:58.02ms
step:1173/2330 train_time:68055ms step_avg:58.02ms
step:1174/2330 train_time:68115ms step_avg:58.02ms
step:1175/2330 train_time:68172ms step_avg:58.02ms
step:1176/2330 train_time:68233ms step_avg:58.02ms
step:1177/2330 train_time:68290ms step_avg:58.02ms
step:1178/2330 train_time:68351ms step_avg:58.02ms
step:1179/2330 train_time:68407ms step_avg:58.02ms
step:1180/2330 train_time:68469ms step_avg:58.02ms
step:1181/2330 train_time:68525ms step_avg:58.02ms
step:1182/2330 train_time:68587ms step_avg:58.03ms
step:1183/2330 train_time:68643ms step_avg:58.02ms
step:1184/2330 train_time:68703ms step_avg:58.03ms
step:1185/2330 train_time:68760ms step_avg:58.03ms
step:1186/2330 train_time:68820ms step_avg:58.03ms
step:1187/2330 train_time:68876ms step_avg:58.03ms
step:1188/2330 train_time:68936ms step_avg:58.03ms
step:1189/2330 train_time:68993ms step_avg:58.03ms
step:1190/2330 train_time:69054ms step_avg:58.03ms
step:1191/2330 train_time:69110ms step_avg:58.03ms
step:1192/2330 train_time:69172ms step_avg:58.03ms
step:1193/2330 train_time:69229ms step_avg:58.03ms
step:1194/2330 train_time:69290ms step_avg:58.03ms
step:1195/2330 train_time:69346ms step_avg:58.03ms
step:1196/2330 train_time:69407ms step_avg:58.03ms
step:1197/2330 train_time:69463ms step_avg:58.03ms
step:1198/2330 train_time:69523ms step_avg:58.03ms
step:1199/2330 train_time:69580ms step_avg:58.03ms
step:1200/2330 train_time:69640ms step_avg:58.03ms
step:1201/2330 train_time:69696ms step_avg:58.03ms
step:1202/2330 train_time:69758ms step_avg:58.04ms
step:1203/2330 train_time:69815ms step_avg:58.03ms
step:1204/2330 train_time:69875ms step_avg:58.04ms
step:1205/2330 train_time:69931ms step_avg:58.03ms
step:1206/2330 train_time:69992ms step_avg:58.04ms
step:1207/2330 train_time:70048ms step_avg:58.03ms
step:1208/2330 train_time:70109ms step_avg:58.04ms
step:1209/2330 train_time:70166ms step_avg:58.04ms
step:1210/2330 train_time:70227ms step_avg:58.04ms
step:1211/2330 train_time:70284ms step_avg:58.04ms
step:1212/2330 train_time:70344ms step_avg:58.04ms
step:1213/2330 train_time:70401ms step_avg:58.04ms
step:1214/2330 train_time:70462ms step_avg:58.04ms
step:1215/2330 train_time:70519ms step_avg:58.04ms
step:1216/2330 train_time:70578ms step_avg:58.04ms
step:1217/2330 train_time:70634ms step_avg:58.04ms
step:1218/2330 train_time:70696ms step_avg:58.04ms
step:1219/2330 train_time:70753ms step_avg:58.04ms
step:1220/2330 train_time:70812ms step_avg:58.04ms
step:1221/2330 train_time:70869ms step_avg:58.04ms
step:1222/2330 train_time:70930ms step_avg:58.04ms
step:1223/2330 train_time:70986ms step_avg:58.04ms
step:1224/2330 train_time:71047ms step_avg:58.05ms
step:1225/2330 train_time:71103ms step_avg:58.04ms
step:1226/2330 train_time:71164ms step_avg:58.05ms
step:1227/2330 train_time:71221ms step_avg:58.04ms
step:1228/2330 train_time:71281ms step_avg:58.05ms
step:1229/2330 train_time:71338ms step_avg:58.05ms
step:1230/2330 train_time:71398ms step_avg:58.05ms
step:1231/2330 train_time:71454ms step_avg:58.05ms
step:1232/2330 train_time:71515ms step_avg:58.05ms
step:1233/2330 train_time:71572ms step_avg:58.05ms
step:1234/2330 train_time:71632ms step_avg:58.05ms
step:1235/2330 train_time:71690ms step_avg:58.05ms
step:1236/2330 train_time:71750ms step_avg:58.05ms
step:1237/2330 train_time:71806ms step_avg:58.05ms
step:1238/2330 train_time:71868ms step_avg:58.05ms
step:1239/2330 train_time:71924ms step_avg:58.05ms
step:1240/2330 train_time:71984ms step_avg:58.05ms
step:1241/2330 train_time:72041ms step_avg:58.05ms
step:1242/2330 train_time:72101ms step_avg:58.05ms
step:1243/2330 train_time:72157ms step_avg:58.05ms
step:1244/2330 train_time:72217ms step_avg:58.05ms
step:1245/2330 train_time:72274ms step_avg:58.05ms
step:1246/2330 train_time:72336ms step_avg:58.05ms
step:1247/2330 train_time:72393ms step_avg:58.05ms
step:1248/2330 train_time:72453ms step_avg:58.06ms
step:1249/2330 train_time:72509ms step_avg:58.05ms
step:1250/2330 train_time:72571ms step_avg:58.06ms
step:1250/2330 val_loss:3.9897 train_time:72652ms step_avg:58.12ms
step:1251/2330 train_time:72672ms step_avg:58.09ms
step:1252/2330 train_time:72693ms step_avg:58.06ms
step:1253/2330 train_time:72750ms step_avg:58.06ms
step:1254/2330 train_time:72817ms step_avg:58.07ms
step:1255/2330 train_time:72875ms step_avg:58.07ms
step:1256/2330 train_time:72935ms step_avg:58.07ms
step:1257/2330 train_time:72993ms step_avg:58.07ms
step:1258/2330 train_time:73052ms step_avg:58.07ms
step:1259/2330 train_time:73109ms step_avg:58.07ms
step:1260/2330 train_time:73168ms step_avg:58.07ms
step:1261/2330 train_time:73224ms step_avg:58.07ms
step:1262/2330 train_time:73283ms step_avg:58.07ms
step:1263/2330 train_time:73339ms step_avg:58.07ms
step:1264/2330 train_time:73399ms step_avg:58.07ms
step:1265/2330 train_time:73455ms step_avg:58.07ms
step:1266/2330 train_time:73515ms step_avg:58.07ms
step:1267/2330 train_time:73571ms step_avg:58.07ms
step:1268/2330 train_time:73631ms step_avg:58.07ms
step:1269/2330 train_time:73691ms step_avg:58.07ms
step:1270/2330 train_time:73753ms step_avg:58.07ms
step:1271/2330 train_time:73811ms step_avg:58.07ms
step:1272/2330 train_time:73873ms step_avg:58.08ms
step:1273/2330 train_time:73930ms step_avg:58.08ms
step:1274/2330 train_time:73991ms step_avg:58.08ms
step:1275/2330 train_time:74048ms step_avg:58.08ms
step:1276/2330 train_time:74107ms step_avg:58.08ms
step:1277/2330 train_time:74164ms step_avg:58.08ms
step:1278/2330 train_time:74224ms step_avg:58.08ms
step:1279/2330 train_time:74280ms step_avg:58.08ms
step:1280/2330 train_time:74340ms step_avg:58.08ms
step:1281/2330 train_time:74396ms step_avg:58.08ms
step:1282/2330 train_time:74457ms step_avg:58.08ms
step:1283/2330 train_time:74513ms step_avg:58.08ms
step:1284/2330 train_time:74573ms step_avg:58.08ms
step:1285/2330 train_time:74629ms step_avg:58.08ms
step:1286/2330 train_time:74691ms step_avg:58.08ms
step:1287/2330 train_time:74748ms step_avg:58.08ms
step:1288/2330 train_time:74810ms step_avg:58.08ms
step:1289/2330 train_time:74867ms step_avg:58.08ms
step:1290/2330 train_time:74929ms step_avg:58.08ms
step:1291/2330 train_time:74986ms step_avg:58.08ms
step:1292/2330 train_time:75046ms step_avg:58.09ms
step:1293/2330 train_time:75104ms step_avg:58.08ms
step:1294/2330 train_time:75611ms step_avg:58.43ms
step:1295/2330 train_time:75667ms step_avg:58.43ms
step:1296/2330 train_time:75726ms step_avg:58.43ms
step:1297/2330 train_time:75783ms step_avg:58.43ms
step:1298/2330 train_time:75841ms step_avg:58.43ms
step:1299/2330 train_time:75897ms step_avg:58.43ms
step:1300/2330 train_time:75957ms step_avg:58.43ms
step:1301/2330 train_time:76014ms step_avg:58.43ms
step:1302/2330 train_time:76073ms step_avg:58.43ms
step:1303/2330 train_time:76129ms step_avg:58.43ms
step:1304/2330 train_time:76188ms step_avg:58.43ms
step:1305/2330 train_time:76243ms step_avg:58.42ms
step:1306/2330 train_time:76303ms step_avg:58.43ms
step:1307/2330 train_time:76360ms step_avg:58.42ms
step:1308/2330 train_time:76419ms step_avg:58.42ms
step:1309/2330 train_time:76477ms step_avg:58.42ms
step:1310/2330 train_time:76546ms step_avg:58.43ms
step:1311/2330 train_time:76603ms step_avg:58.43ms
step:1312/2330 train_time:76667ms step_avg:58.44ms
step:1313/2330 train_time:76724ms step_avg:58.43ms
step:1314/2330 train_time:76785ms step_avg:58.44ms
step:1315/2330 train_time:76841ms step_avg:58.43ms
step:1316/2330 train_time:76901ms step_avg:58.44ms
step:1317/2330 train_time:76957ms step_avg:58.43ms
step:1318/2330 train_time:77017ms step_avg:58.43ms
step:1319/2330 train_time:77073ms step_avg:58.43ms
step:1320/2330 train_time:77133ms step_avg:58.43ms
step:1321/2330 train_time:77189ms step_avg:58.43ms
step:1322/2330 train_time:77248ms step_avg:58.43ms
step:1323/2330 train_time:77305ms step_avg:58.43ms
step:1324/2330 train_time:77364ms step_avg:58.43ms
step:1325/2330 train_time:77421ms step_avg:58.43ms
step:1326/2330 train_time:77484ms step_avg:58.43ms
step:1327/2330 train_time:77542ms step_avg:58.43ms
step:1328/2330 train_time:77605ms step_avg:58.44ms
step:1329/2330 train_time:77663ms step_avg:58.44ms
step:1330/2330 train_time:77723ms step_avg:58.44ms
step:1331/2330 train_time:77780ms step_avg:58.44ms
step:1332/2330 train_time:77840ms step_avg:58.44ms
step:1333/2330 train_time:77896ms step_avg:58.44ms
step:1334/2330 train_time:77957ms step_avg:58.44ms
step:1335/2330 train_time:78014ms step_avg:58.44ms
step:1336/2330 train_time:78074ms step_avg:58.44ms
step:1337/2330 train_time:78131ms step_avg:58.44ms
step:1338/2330 train_time:78190ms step_avg:58.44ms
step:1339/2330 train_time:78246ms step_avg:58.44ms
step:1340/2330 train_time:78305ms step_avg:58.44ms
step:1341/2330 train_time:78362ms step_avg:58.44ms
step:1342/2330 train_time:78423ms step_avg:58.44ms
step:1343/2330 train_time:78480ms step_avg:58.44ms
step:1344/2330 train_time:78542ms step_avg:58.44ms
step:1345/2330 train_time:78599ms step_avg:58.44ms
step:1346/2330 train_time:78662ms step_avg:58.44ms
step:1347/2330 train_time:78719ms step_avg:58.44ms
step:1348/2330 train_time:78780ms step_avg:58.44ms
step:1349/2330 train_time:78836ms step_avg:58.44ms
step:1350/2330 train_time:78898ms step_avg:58.44ms
step:1351/2330 train_time:78955ms step_avg:58.44ms
step:1352/2330 train_time:79015ms step_avg:58.44ms
step:1353/2330 train_time:79072ms step_avg:58.44ms
step:1354/2330 train_time:79132ms step_avg:58.44ms
step:1355/2330 train_time:79188ms step_avg:58.44ms
step:1356/2330 train_time:79248ms step_avg:58.44ms
step:1357/2330 train_time:79305ms step_avg:58.44ms
step:1358/2330 train_time:79364ms step_avg:58.44ms
step:1359/2330 train_time:79422ms step_avg:58.44ms
step:1360/2330 train_time:79482ms step_avg:58.44ms
step:1361/2330 train_time:79539ms step_avg:58.44ms
step:1362/2330 train_time:79600ms step_avg:58.44ms
step:1363/2330 train_time:79657ms step_avg:58.44ms
step:1364/2330 train_time:79718ms step_avg:58.44ms
step:1365/2330 train_time:79775ms step_avg:58.44ms
step:1366/2330 train_time:79835ms step_avg:58.44ms
step:1367/2330 train_time:79893ms step_avg:58.44ms
step:1368/2330 train_time:79953ms step_avg:58.45ms
step:1369/2330 train_time:80010ms step_avg:58.44ms
step:1370/2330 train_time:80070ms step_avg:58.45ms
step:1371/2330 train_time:80127ms step_avg:58.44ms
step:1372/2330 train_time:80186ms step_avg:58.44ms
step:1373/2330 train_time:80243ms step_avg:58.44ms
step:1374/2330 train_time:80302ms step_avg:58.44ms
step:1375/2330 train_time:80360ms step_avg:58.44ms
step:1376/2330 train_time:80420ms step_avg:58.44ms
step:1377/2330 train_time:80477ms step_avg:58.44ms
step:1378/2330 train_time:80536ms step_avg:58.44ms
step:1379/2330 train_time:80595ms step_avg:58.44ms
step:1380/2330 train_time:80655ms step_avg:58.45ms
step:1381/2330 train_time:80712ms step_avg:58.44ms
step:1382/2330 train_time:80772ms step_avg:58.45ms
step:1383/2330 train_time:80830ms step_avg:58.45ms
step:1384/2330 train_time:80890ms step_avg:58.45ms
step:1385/2330 train_time:80947ms step_avg:58.45ms
step:1386/2330 train_time:81007ms step_avg:58.45ms
step:1387/2330 train_time:81063ms step_avg:58.45ms
step:1388/2330 train_time:81124ms step_avg:58.45ms
step:1389/2330 train_time:81181ms step_avg:58.45ms
step:1390/2330 train_time:81240ms step_avg:58.45ms
step:1391/2330 train_time:81297ms step_avg:58.45ms
step:1392/2330 train_time:81358ms step_avg:58.45ms
step:1393/2330 train_time:81416ms step_avg:58.45ms
step:1394/2330 train_time:81476ms step_avg:58.45ms
step:1395/2330 train_time:81532ms step_avg:58.45ms
step:1396/2330 train_time:81593ms step_avg:58.45ms
step:1397/2330 train_time:81650ms step_avg:58.45ms
step:1398/2330 train_time:81710ms step_avg:58.45ms
step:1399/2330 train_time:81767ms step_avg:58.45ms
step:1400/2330 train_time:81827ms step_avg:58.45ms
step:1401/2330 train_time:81884ms step_avg:58.45ms
step:1402/2330 train_time:81944ms step_avg:58.45ms
step:1403/2330 train_time:82002ms step_avg:58.45ms
step:1404/2330 train_time:82062ms step_avg:58.45ms
step:1405/2330 train_time:82119ms step_avg:58.45ms
step:1406/2330 train_time:82179ms step_avg:58.45ms
step:1407/2330 train_time:82235ms step_avg:58.45ms
step:1408/2330 train_time:82296ms step_avg:58.45ms
step:1409/2330 train_time:82352ms step_avg:58.45ms
step:1410/2330 train_time:82412ms step_avg:58.45ms
step:1411/2330 train_time:82470ms step_avg:58.45ms
step:1412/2330 train_time:82530ms step_avg:58.45ms
step:1413/2330 train_time:82587ms step_avg:58.45ms
step:1414/2330 train_time:82647ms step_avg:58.45ms
step:1415/2330 train_time:82703ms step_avg:58.45ms
step:1416/2330 train_time:82764ms step_avg:58.45ms
step:1417/2330 train_time:82820ms step_avg:58.45ms
step:1418/2330 train_time:82881ms step_avg:58.45ms
step:1419/2330 train_time:82938ms step_avg:58.45ms
step:1420/2330 train_time:82999ms step_avg:58.45ms
step:1421/2330 train_time:83056ms step_avg:58.45ms
step:1422/2330 train_time:83117ms step_avg:58.45ms
step:1423/2330 train_time:83174ms step_avg:58.45ms
step:1424/2330 train_time:83233ms step_avg:58.45ms
step:1425/2330 train_time:83290ms step_avg:58.45ms
step:1426/2330 train_time:83350ms step_avg:58.45ms
step:1427/2330 train_time:83408ms step_avg:58.45ms
step:1428/2330 train_time:83467ms step_avg:58.45ms
step:1429/2330 train_time:83524ms step_avg:58.45ms
step:1430/2330 train_time:83585ms step_avg:58.45ms
step:1431/2330 train_time:83641ms step_avg:58.45ms
step:1432/2330 train_time:83702ms step_avg:58.45ms
step:1433/2330 train_time:83759ms step_avg:58.45ms
step:1434/2330 train_time:83820ms step_avg:58.45ms
step:1435/2330 train_time:83877ms step_avg:58.45ms
step:1436/2330 train_time:83938ms step_avg:58.45ms
step:1437/2330 train_time:83996ms step_avg:58.45ms
step:1438/2330 train_time:84055ms step_avg:58.45ms
step:1439/2330 train_time:84112ms step_avg:58.45ms
step:1440/2330 train_time:84172ms step_avg:58.45ms
step:1441/2330 train_time:84230ms step_avg:58.45ms
step:1442/2330 train_time:84290ms step_avg:58.45ms
step:1443/2330 train_time:84347ms step_avg:58.45ms
step:1444/2330 train_time:84407ms step_avg:58.45ms
step:1445/2330 train_time:84464ms step_avg:58.45ms
step:1446/2330 train_time:84524ms step_avg:58.45ms
step:1447/2330 train_time:84582ms step_avg:58.45ms
step:1448/2330 train_time:84641ms step_avg:58.45ms
step:1449/2330 train_time:84698ms step_avg:58.45ms
step:1450/2330 train_time:84758ms step_avg:58.45ms
step:1451/2330 train_time:84815ms step_avg:58.45ms
step:1452/2330 train_time:84875ms step_avg:58.45ms
step:1453/2330 train_time:84933ms step_avg:58.45ms
step:1454/2330 train_time:84993ms step_avg:58.45ms
step:1455/2330 train_time:85050ms step_avg:58.45ms
step:1456/2330 train_time:85110ms step_avg:58.45ms
step:1457/2330 train_time:85166ms step_avg:58.45ms
step:1458/2330 train_time:85226ms step_avg:58.45ms
step:1459/2330 train_time:85283ms step_avg:58.45ms
step:1460/2330 train_time:85344ms step_avg:58.45ms
step:1461/2330 train_time:85401ms step_avg:58.45ms
step:1462/2330 train_time:85461ms step_avg:58.46ms
step:1463/2330 train_time:85519ms step_avg:58.45ms
step:1464/2330 train_time:85579ms step_avg:58.46ms
step:1465/2330 train_time:85636ms step_avg:58.45ms
step:1466/2330 train_time:85697ms step_avg:58.46ms
step:1467/2330 train_time:85753ms step_avg:58.45ms
step:1468/2330 train_time:85814ms step_avg:58.46ms
step:1469/2330 train_time:85870ms step_avg:58.46ms
step:1470/2330 train_time:85931ms step_avg:58.46ms
step:1471/2330 train_time:85987ms step_avg:58.45ms
step:1472/2330 train_time:86047ms step_avg:58.46ms
step:1473/2330 train_time:86104ms step_avg:58.46ms
step:1474/2330 train_time:86165ms step_avg:58.46ms
step:1475/2330 train_time:86222ms step_avg:58.46ms
step:1476/2330 train_time:86283ms step_avg:58.46ms
step:1477/2330 train_time:86339ms step_avg:58.46ms
step:1478/2330 train_time:86400ms step_avg:58.46ms
step:1479/2330 train_time:86457ms step_avg:58.46ms
step:1480/2330 train_time:86519ms step_avg:58.46ms
step:1481/2330 train_time:86575ms step_avg:58.46ms
step:1482/2330 train_time:86635ms step_avg:58.46ms
step:1483/2330 train_time:86692ms step_avg:58.46ms
step:1484/2330 train_time:86752ms step_avg:58.46ms
step:1485/2330 train_time:86809ms step_avg:58.46ms
step:1486/2330 train_time:86869ms step_avg:58.46ms
step:1487/2330 train_time:86926ms step_avg:58.46ms
step:1488/2330 train_time:86986ms step_avg:58.46ms
step:1489/2330 train_time:87043ms step_avg:58.46ms
step:1490/2330 train_time:87103ms step_avg:58.46ms
step:1491/2330 train_time:87161ms step_avg:58.46ms
step:1492/2330 train_time:87221ms step_avg:58.46ms
step:1493/2330 train_time:87278ms step_avg:58.46ms
step:1494/2330 train_time:87338ms step_avg:58.46ms
step:1495/2330 train_time:87396ms step_avg:58.46ms
step:1496/2330 train_time:87456ms step_avg:58.46ms
step:1497/2330 train_time:87514ms step_avg:58.46ms
step:1498/2330 train_time:87574ms step_avg:58.46ms
step:1499/2330 train_time:87631ms step_avg:58.46ms
step:1500/2330 train_time:87691ms step_avg:58.46ms
step:1500/2330 val_loss:3.9074 train_time:87771ms step_avg:58.51ms
step:1501/2330 train_time:87791ms step_avg:58.49ms
step:1502/2330 train_time:87812ms step_avg:58.46ms
step:1503/2330 train_time:87867ms step_avg:58.46ms
step:1504/2330 train_time:87934ms step_avg:58.47ms
step:1505/2330 train_time:87991ms step_avg:58.47ms
step:1506/2330 train_time:88052ms step_avg:58.47ms
step:1507/2330 train_time:88108ms step_avg:58.47ms
step:1508/2330 train_time:88168ms step_avg:58.47ms
step:1509/2330 train_time:88224ms step_avg:58.47ms
step:1510/2330 train_time:88284ms step_avg:58.47ms
step:1511/2330 train_time:88340ms step_avg:58.46ms
step:1512/2330 train_time:88400ms step_avg:58.47ms
step:1513/2330 train_time:88456ms step_avg:58.46ms
step:1514/2330 train_time:88515ms step_avg:58.46ms
step:1515/2330 train_time:88571ms step_avg:58.46ms
step:1516/2330 train_time:88632ms step_avg:58.46ms
step:1517/2330 train_time:88688ms step_avg:58.46ms
step:1518/2330 train_time:88749ms step_avg:58.46ms
step:1519/2330 train_time:88806ms step_avg:58.46ms
step:1520/2330 train_time:88870ms step_avg:58.47ms
step:1521/2330 train_time:88927ms step_avg:58.47ms
step:1522/2330 train_time:88991ms step_avg:58.47ms
step:1523/2330 train_time:89048ms step_avg:58.47ms
step:1524/2330 train_time:89110ms step_avg:58.47ms
step:1525/2330 train_time:89166ms step_avg:58.47ms
step:1526/2330 train_time:89226ms step_avg:58.47ms
step:1527/2330 train_time:89282ms step_avg:58.47ms
step:1528/2330 train_time:89343ms step_avg:58.47ms
step:1529/2330 train_time:89401ms step_avg:58.47ms
step:1530/2330 train_time:89459ms step_avg:58.47ms
step:1531/2330 train_time:89516ms step_avg:58.47ms
step:1532/2330 train_time:89576ms step_avg:58.47ms
step:1533/2330 train_time:89633ms step_avg:58.47ms
step:1534/2330 train_time:89693ms step_avg:58.47ms
step:1535/2330 train_time:89751ms step_avg:58.47ms
step:1536/2330 train_time:89814ms step_avg:58.47ms
step:1537/2330 train_time:89871ms step_avg:58.47ms
step:1538/2330 train_time:89934ms step_avg:58.47ms
step:1539/2330 train_time:89991ms step_avg:58.47ms
step:1540/2330 train_time:90053ms step_avg:58.48ms
step:1541/2330 train_time:90110ms step_avg:58.48ms
step:1542/2330 train_time:90171ms step_avg:58.48ms
step:1543/2330 train_time:90227ms step_avg:58.48ms
step:1544/2330 train_time:90289ms step_avg:58.48ms
step:1545/2330 train_time:90345ms step_avg:58.48ms
step:1546/2330 train_time:90408ms step_avg:58.48ms
step:1547/2330 train_time:90464ms step_avg:58.48ms
step:1548/2330 train_time:90526ms step_avg:58.48ms
step:1549/2330 train_time:90582ms step_avg:58.48ms
step:1550/2330 train_time:90644ms step_avg:58.48ms
step:1551/2330 train_time:90701ms step_avg:58.48ms
step:1552/2330 train_time:90762ms step_avg:58.48ms
step:1553/2330 train_time:90820ms step_avg:58.48ms
step:1554/2330 train_time:90881ms step_avg:58.48ms
step:1555/2330 train_time:90940ms step_avg:58.48ms
step:1556/2330 train_time:91001ms step_avg:58.48ms
step:1557/2330 train_time:91060ms step_avg:58.48ms
step:1558/2330 train_time:91120ms step_avg:58.49ms
step:1559/2330 train_time:91177ms step_avg:58.48ms
step:1560/2330 train_time:91237ms step_avg:58.49ms
step:1561/2330 train_time:91295ms step_avg:58.48ms
step:1562/2330 train_time:91355ms step_avg:58.49ms
step:1563/2330 train_time:91412ms step_avg:58.48ms
step:1564/2330 train_time:91471ms step_avg:58.49ms
step:1565/2330 train_time:91528ms step_avg:58.48ms
step:1566/2330 train_time:91589ms step_avg:58.49ms
step:1567/2330 train_time:91646ms step_avg:58.49ms
step:1568/2330 train_time:91708ms step_avg:58.49ms
step:1569/2330 train_time:91764ms step_avg:58.49ms
step:1570/2330 train_time:91828ms step_avg:58.49ms
step:1571/2330 train_time:91885ms step_avg:58.49ms
step:1572/2330 train_time:91948ms step_avg:58.49ms
step:1573/2330 train_time:92005ms step_avg:58.49ms
step:1574/2330 train_time:92067ms step_avg:58.49ms
step:1575/2330 train_time:92124ms step_avg:58.49ms
step:1576/2330 train_time:92186ms step_avg:58.49ms
step:1577/2330 train_time:92243ms step_avg:58.49ms
step:1578/2330 train_time:92304ms step_avg:58.49ms
step:1579/2330 train_time:92362ms step_avg:58.49ms
step:1580/2330 train_time:92421ms step_avg:58.49ms
step:1581/2330 train_time:92479ms step_avg:58.49ms
step:1582/2330 train_time:92539ms step_avg:58.49ms
step:1583/2330 train_time:92597ms step_avg:58.49ms
step:1584/2330 train_time:92657ms step_avg:58.50ms
step:1585/2330 train_time:92714ms step_avg:58.49ms
step:1586/2330 train_time:92774ms step_avg:58.50ms
step:1587/2330 train_time:92831ms step_avg:58.49ms
step:1588/2330 train_time:92893ms step_avg:58.50ms
step:1589/2330 train_time:92950ms step_avg:58.50ms
step:1590/2330 train_time:93012ms step_avg:58.50ms
step:1591/2330 train_time:93070ms step_avg:58.50ms
step:1592/2330 train_time:93131ms step_avg:58.50ms
step:1593/2330 train_time:93188ms step_avg:58.50ms
step:1594/2330 train_time:93250ms step_avg:58.50ms
step:1595/2330 train_time:93307ms step_avg:58.50ms
step:1596/2330 train_time:93369ms step_avg:58.50ms
step:1597/2330 train_time:93425ms step_avg:58.50ms
step:1598/2330 train_time:93487ms step_avg:58.50ms
step:1599/2330 train_time:93544ms step_avg:58.50ms
step:1600/2330 train_time:93606ms step_avg:58.50ms
step:1601/2330 train_time:93662ms step_avg:58.50ms
step:1602/2330 train_time:93724ms step_avg:58.50ms
step:1603/2330 train_time:93781ms step_avg:58.50ms
step:1604/2330 train_time:93842ms step_avg:58.50ms
step:1605/2330 train_time:93900ms step_avg:58.50ms
step:1606/2330 train_time:93961ms step_avg:58.51ms
step:1607/2330 train_time:94019ms step_avg:58.51ms
step:1608/2330 train_time:94079ms step_avg:58.51ms
step:1609/2330 train_time:94137ms step_avg:58.51ms
step:1610/2330 train_time:94198ms step_avg:58.51ms
step:1611/2330 train_time:94256ms step_avg:58.51ms
step:1612/2330 train_time:94315ms step_avg:58.51ms
step:1613/2330 train_time:94373ms step_avg:58.51ms
step:1614/2330 train_time:94433ms step_avg:58.51ms
step:1615/2330 train_time:94490ms step_avg:58.51ms
step:1616/2330 train_time:94550ms step_avg:58.51ms
step:1617/2330 train_time:94607ms step_avg:58.51ms
step:1618/2330 train_time:94669ms step_avg:58.51ms
step:1619/2330 train_time:94726ms step_avg:58.51ms
step:1620/2330 train_time:94788ms step_avg:58.51ms
step:1621/2330 train_time:94844ms step_avg:58.51ms
step:1622/2330 train_time:94908ms step_avg:58.51ms
step:1623/2330 train_time:94965ms step_avg:58.51ms
step:1624/2330 train_time:95028ms step_avg:58.51ms
step:1625/2330 train_time:95084ms step_avg:58.51ms
step:1626/2330 train_time:95148ms step_avg:58.52ms
step:1627/2330 train_time:95205ms step_avg:58.52ms
step:1628/2330 train_time:95267ms step_avg:58.52ms
step:1629/2330 train_time:95323ms step_avg:58.52ms
step:1630/2330 train_time:95386ms step_avg:58.52ms
step:1631/2330 train_time:95443ms step_avg:58.52ms
step:1632/2330 train_time:95504ms step_avg:58.52ms
step:1633/2330 train_time:95562ms step_avg:58.52ms
step:1634/2330 train_time:95622ms step_avg:58.52ms
step:1635/2330 train_time:95679ms step_avg:58.52ms
step:1636/2330 train_time:95740ms step_avg:58.52ms
step:1637/2330 train_time:95799ms step_avg:58.52ms
step:1638/2330 train_time:95859ms step_avg:58.52ms
step:1639/2330 train_time:95916ms step_avg:58.52ms
step:1640/2330 train_time:95977ms step_avg:58.52ms
step:1641/2330 train_time:96036ms step_avg:58.52ms
step:1642/2330 train_time:96096ms step_avg:58.52ms
step:1643/2330 train_time:96153ms step_avg:58.52ms
step:1644/2330 train_time:96214ms step_avg:58.52ms
step:1645/2330 train_time:96270ms step_avg:58.52ms
step:1646/2330 train_time:96332ms step_avg:58.52ms
step:1647/2330 train_time:96388ms step_avg:58.52ms
step:1648/2330 train_time:96452ms step_avg:58.53ms
step:1649/2330 train_time:96509ms step_avg:58.53ms
step:1650/2330 train_time:96570ms step_avg:58.53ms
step:1651/2330 train_time:96627ms step_avg:58.53ms
step:1652/2330 train_time:96688ms step_avg:58.53ms
step:1653/2330 train_time:96745ms step_avg:58.53ms
step:1654/2330 train_time:96807ms step_avg:58.53ms
step:1655/2330 train_time:96863ms step_avg:58.53ms
step:1656/2330 train_time:96927ms step_avg:58.53ms
step:1657/2330 train_time:96984ms step_avg:58.53ms
step:1658/2330 train_time:97047ms step_avg:58.53ms
step:1659/2330 train_time:97105ms step_avg:58.53ms
step:1660/2330 train_time:97166ms step_avg:58.53ms
step:1661/2330 train_time:97223ms step_avg:58.53ms
step:1662/2330 train_time:97285ms step_avg:58.53ms
step:1663/2330 train_time:97341ms step_avg:58.53ms
step:1664/2330 train_time:97403ms step_avg:58.54ms
step:1665/2330 train_time:97460ms step_avg:58.53ms
step:1666/2330 train_time:97521ms step_avg:58.54ms
step:1667/2330 train_time:97578ms step_avg:58.54ms
step:1668/2330 train_time:97639ms step_avg:58.54ms
step:1669/2330 train_time:97697ms step_avg:58.54ms
step:1670/2330 train_time:97757ms step_avg:58.54ms
step:1671/2330 train_time:97814ms step_avg:58.54ms
step:1672/2330 train_time:97875ms step_avg:58.54ms
step:1673/2330 train_time:97933ms step_avg:58.54ms
step:1674/2330 train_time:97993ms step_avg:58.54ms
step:1675/2330 train_time:98051ms step_avg:58.54ms
step:1676/2330 train_time:98111ms step_avg:58.54ms
step:1677/2330 train_time:98168ms step_avg:58.54ms
step:1678/2330 train_time:98230ms step_avg:58.54ms
step:1679/2330 train_time:98287ms step_avg:58.54ms
step:1680/2330 train_time:98349ms step_avg:58.54ms
step:1681/2330 train_time:98407ms step_avg:58.54ms
step:1682/2330 train_time:98468ms step_avg:58.54ms
step:1683/2330 train_time:98525ms step_avg:58.54ms
step:1684/2330 train_time:98587ms step_avg:58.54ms
step:1685/2330 train_time:98644ms step_avg:58.54ms
step:1686/2330 train_time:98706ms step_avg:58.54ms
step:1687/2330 train_time:98762ms step_avg:58.54ms
step:1688/2330 train_time:98825ms step_avg:58.55ms
step:1689/2330 train_time:98883ms step_avg:58.55ms
step:1690/2330 train_time:98944ms step_avg:58.55ms
step:1691/2330 train_time:99003ms step_avg:58.55ms
step:1692/2330 train_time:99063ms step_avg:58.55ms
step:1693/2330 train_time:99121ms step_avg:58.55ms
step:1694/2330 train_time:99182ms step_avg:58.55ms
step:1695/2330 train_time:99240ms step_avg:58.55ms
step:1696/2330 train_time:99301ms step_avg:58.55ms
step:1697/2330 train_time:99359ms step_avg:58.55ms
step:1698/2330 train_time:99420ms step_avg:58.55ms
step:1699/2330 train_time:99477ms step_avg:58.55ms
step:1700/2330 train_time:99537ms step_avg:58.55ms
step:1701/2330 train_time:99594ms step_avg:58.55ms
step:1702/2330 train_time:99655ms step_avg:58.55ms
step:1703/2330 train_time:99711ms step_avg:58.55ms
step:1704/2330 train_time:99772ms step_avg:58.55ms
step:1705/2330 train_time:99829ms step_avg:58.55ms
step:1706/2330 train_time:99891ms step_avg:58.55ms
step:1707/2330 train_time:99948ms step_avg:58.55ms
step:1708/2330 train_time:100010ms step_avg:58.55ms
step:1709/2330 train_time:100066ms step_avg:58.55ms
step:1710/2330 train_time:100129ms step_avg:58.55ms
step:1711/2330 train_time:100186ms step_avg:58.55ms
step:1712/2330 train_time:100248ms step_avg:58.56ms
step:1713/2330 train_time:100305ms step_avg:58.56ms
step:1714/2330 train_time:100368ms step_avg:58.56ms
step:1715/2330 train_time:100425ms step_avg:58.56ms
step:1716/2330 train_time:100487ms step_avg:58.56ms
step:1717/2330 train_time:100544ms step_avg:58.56ms
step:1718/2330 train_time:100606ms step_avg:58.56ms
step:1719/2330 train_time:100663ms step_avg:58.56ms
step:1720/2330 train_time:100724ms step_avg:58.56ms
step:1721/2330 train_time:100782ms step_avg:58.56ms
step:1722/2330 train_time:100843ms step_avg:58.56ms
step:1723/2330 train_time:100901ms step_avg:58.56ms
step:1724/2330 train_time:100962ms step_avg:58.56ms
step:1725/2330 train_time:101020ms step_avg:58.56ms
step:1726/2330 train_time:101080ms step_avg:58.56ms
step:1727/2330 train_time:101138ms step_avg:58.56ms
step:1728/2330 train_time:101199ms step_avg:58.56ms
step:1729/2330 train_time:101258ms step_avg:58.56ms
step:1730/2330 train_time:101319ms step_avg:58.57ms
step:1731/2330 train_time:101376ms step_avg:58.57ms
step:1732/2330 train_time:101437ms step_avg:58.57ms
step:1733/2330 train_time:101495ms step_avg:58.57ms
step:1734/2330 train_time:101555ms step_avg:58.57ms
step:1735/2330 train_time:101611ms step_avg:58.57ms
step:1736/2330 train_time:101672ms step_avg:58.57ms
step:1737/2330 train_time:101728ms step_avg:58.57ms
step:1738/2330 train_time:101791ms step_avg:58.57ms
step:1739/2330 train_time:101847ms step_avg:58.57ms
step:1740/2330 train_time:101909ms step_avg:58.57ms
step:1741/2330 train_time:101966ms step_avg:58.57ms
step:1742/2330 train_time:102028ms step_avg:58.57ms
step:1743/2330 train_time:102085ms step_avg:58.57ms
step:1744/2330 train_time:102148ms step_avg:58.57ms
step:1745/2330 train_time:102206ms step_avg:58.57ms
step:1746/2330 train_time:102266ms step_avg:58.57ms
step:1747/2330 train_time:102323ms step_avg:58.57ms
step:1748/2330 train_time:102386ms step_avg:58.57ms
step:1749/2330 train_time:102443ms step_avg:58.57ms
step:1750/2330 train_time:102505ms step_avg:58.57ms
step:1750/2330 val_loss:3.8235 train_time:102586ms step_avg:58.62ms
step:1751/2330 train_time:102606ms step_avg:58.60ms
step:1752/2330 train_time:102627ms step_avg:58.58ms
step:1753/2330 train_time:102686ms step_avg:58.58ms
step:1754/2330 train_time:102754ms step_avg:58.58ms
step:1755/2330 train_time:102811ms step_avg:58.58ms
step:1756/2330 train_time:102871ms step_avg:58.58ms
step:1757/2330 train_time:102929ms step_avg:58.58ms
step:1758/2330 train_time:102988ms step_avg:58.58ms
step:1759/2330 train_time:103046ms step_avg:58.58ms
step:1760/2330 train_time:103106ms step_avg:58.58ms
step:1761/2330 train_time:103163ms step_avg:58.58ms
step:1762/2330 train_time:103222ms step_avg:58.58ms
step:1763/2330 train_time:103279ms step_avg:58.58ms
step:1764/2330 train_time:103339ms step_avg:58.58ms
step:1765/2330 train_time:103395ms step_avg:58.58ms
step:1766/2330 train_time:103455ms step_avg:58.58ms
step:1767/2330 train_time:103513ms step_avg:58.58ms
step:1768/2330 train_time:103577ms step_avg:58.58ms
step:1769/2330 train_time:103637ms step_avg:58.58ms
step:1770/2330 train_time:103699ms step_avg:58.59ms
step:1771/2330 train_time:103757ms step_avg:58.59ms
step:1772/2330 train_time:103819ms step_avg:58.59ms
step:1773/2330 train_time:103875ms step_avg:58.59ms
step:1774/2330 train_time:103938ms step_avg:58.59ms
step:1775/2330 train_time:103995ms step_avg:58.59ms
step:1776/2330 train_time:104056ms step_avg:58.59ms
step:1777/2330 train_time:104113ms step_avg:58.59ms
step:1778/2330 train_time:104174ms step_avg:58.59ms
step:1779/2330 train_time:104231ms step_avg:58.59ms
step:1780/2330 train_time:104291ms step_avg:58.59ms
step:1781/2330 train_time:104349ms step_avg:58.59ms
step:1782/2330 train_time:104408ms step_avg:58.59ms
step:1783/2330 train_time:104466ms step_avg:58.59ms
step:1784/2330 train_time:104526ms step_avg:58.59ms
step:1785/2330 train_time:104585ms step_avg:58.59ms
step:1786/2330 train_time:104645ms step_avg:58.59ms
step:1787/2330 train_time:104704ms step_avg:58.59ms
step:1788/2330 train_time:104765ms step_avg:58.59ms
step:1789/2330 train_time:104822ms step_avg:58.59ms
step:1790/2330 train_time:104883ms step_avg:58.59ms
step:1791/2330 train_time:104941ms step_avg:58.59ms
step:1792/2330 train_time:105003ms step_avg:58.60ms
step:1793/2330 train_time:105060ms step_avg:58.59ms
step:1794/2330 train_time:105121ms step_avg:58.60ms
step:1795/2330 train_time:105177ms step_avg:58.59ms
step:1796/2330 train_time:105239ms step_avg:58.60ms
step:1797/2330 train_time:105295ms step_avg:58.60ms
step:1798/2330 train_time:105357ms step_avg:58.60ms
step:1799/2330 train_time:105413ms step_avg:58.60ms
step:1800/2330 train_time:105475ms step_avg:58.60ms
step:1801/2330 train_time:105532ms step_avg:58.60ms
step:1802/2330 train_time:105594ms step_avg:58.60ms
step:1803/2330 train_time:105653ms step_avg:58.60ms
step:1804/2330 train_time:105713ms step_avg:58.60ms
step:1805/2330 train_time:105772ms step_avg:58.60ms
step:1806/2330 train_time:105833ms step_avg:58.60ms
step:1807/2330 train_time:105890ms step_avg:58.60ms
step:1808/2330 train_time:105951ms step_avg:58.60ms
step:1809/2330 train_time:106009ms step_avg:58.60ms
step:1810/2330 train_time:106069ms step_avg:58.60ms
step:1811/2330 train_time:106128ms step_avg:58.60ms
step:1812/2330 train_time:106188ms step_avg:58.60ms
step:1813/2330 train_time:106245ms step_avg:58.60ms
step:1814/2330 train_time:106306ms step_avg:58.60ms
step:1815/2330 train_time:106363ms step_avg:58.60ms
step:1816/2330 train_time:106423ms step_avg:58.60ms
step:1817/2330 train_time:106480ms step_avg:58.60ms
step:1818/2330 train_time:106541ms step_avg:58.60ms
step:1819/2330 train_time:106598ms step_avg:58.60ms
step:1820/2330 train_time:106660ms step_avg:58.60ms
step:1821/2330 train_time:106717ms step_avg:58.60ms
step:1822/2330 train_time:106780ms step_avg:58.61ms
step:1823/2330 train_time:106837ms step_avg:58.61ms
step:1824/2330 train_time:106899ms step_avg:58.61ms
step:1825/2330 train_time:106956ms step_avg:58.61ms
step:1826/2330 train_time:107018ms step_avg:58.61ms
step:1827/2330 train_time:107074ms step_avg:58.61ms
step:1828/2330 train_time:107136ms step_avg:58.61ms
step:1829/2330 train_time:107193ms step_avg:58.61ms
step:1830/2330 train_time:107254ms step_avg:58.61ms
step:1831/2330 train_time:107311ms step_avg:58.61ms
step:1832/2330 train_time:107372ms step_avg:58.61ms
step:1833/2330 train_time:107429ms step_avg:58.61ms
step:1834/2330 train_time:107489ms step_avg:58.61ms
step:1835/2330 train_time:107547ms step_avg:58.61ms
step:1836/2330 train_time:107608ms step_avg:58.61ms
step:1837/2330 train_time:107667ms step_avg:58.61ms
step:1838/2330 train_time:107727ms step_avg:58.61ms
step:1839/2330 train_time:107786ms step_avg:58.61ms
step:1840/2330 train_time:107846ms step_avg:58.61ms
step:1841/2330 train_time:107904ms step_avg:58.61ms
step:1842/2330 train_time:107965ms step_avg:58.61ms
step:1843/2330 train_time:108023ms step_avg:58.61ms
step:1844/2330 train_time:108083ms step_avg:58.61ms
step:1845/2330 train_time:108140ms step_avg:58.61ms
step:1846/2330 train_time:108201ms step_avg:58.61ms
step:1847/2330 train_time:108258ms step_avg:58.61ms
step:1848/2330 train_time:108320ms step_avg:58.61ms
step:1849/2330 train_time:108376ms step_avg:58.61ms
step:1850/2330 train_time:108439ms step_avg:58.62ms
step:1851/2330 train_time:108495ms step_avg:58.61ms
step:1852/2330 train_time:108557ms step_avg:58.62ms
step:1853/2330 train_time:108614ms step_avg:58.62ms
step:1854/2330 train_time:108677ms step_avg:58.62ms
step:1855/2330 train_time:108734ms step_avg:58.62ms
step:1856/2330 train_time:108797ms step_avg:58.62ms
step:1857/2330 train_time:108855ms step_avg:58.62ms
step:1858/2330 train_time:108915ms step_avg:58.62ms
step:1859/2330 train_time:108973ms step_avg:58.62ms
step:1860/2330 train_time:109034ms step_avg:58.62ms
step:1861/2330 train_time:109092ms step_avg:58.62ms
step:1862/2330 train_time:109153ms step_avg:58.62ms
step:1863/2330 train_time:109211ms step_avg:58.62ms
step:1864/2330 train_time:109271ms step_avg:58.62ms
step:1865/2330 train_time:109329ms step_avg:58.62ms
step:1866/2330 train_time:109389ms step_avg:58.62ms
step:1867/2330 train_time:109446ms step_avg:58.62ms
step:1868/2330 train_time:109506ms step_avg:58.62ms
step:1869/2330 train_time:109564ms step_avg:58.62ms
step:1870/2330 train_time:109624ms step_avg:58.62ms
step:1871/2330 train_time:109681ms step_avg:58.62ms
step:1872/2330 train_time:109743ms step_avg:58.62ms
step:1873/2330 train_time:109801ms step_avg:58.62ms
step:1874/2330 train_time:109861ms step_avg:58.62ms
step:1875/2330 train_time:109918ms step_avg:58.62ms
step:1876/2330 train_time:109980ms step_avg:58.62ms
step:1877/2330 train_time:110037ms step_avg:58.62ms
step:1878/2330 train_time:110099ms step_avg:58.63ms
step:1879/2330 train_time:110156ms step_avg:58.62ms
step:1880/2330 train_time:110220ms step_avg:58.63ms
step:1881/2330 train_time:110277ms step_avg:58.63ms
step:1882/2330 train_time:110338ms step_avg:58.63ms
step:1883/2330 train_time:110395ms step_avg:58.63ms
step:1884/2330 train_time:110457ms step_avg:58.63ms
step:1885/2330 train_time:110514ms step_avg:58.63ms
step:1886/2330 train_time:110575ms step_avg:58.63ms
step:1887/2330 train_time:110632ms step_avg:58.63ms
step:1888/2330 train_time:110693ms step_avg:58.63ms
step:1889/2330 train_time:110751ms step_avg:58.63ms
step:1890/2330 train_time:110811ms step_avg:58.63ms
step:1891/2330 train_time:110869ms step_avg:58.63ms
step:1892/2330 train_time:110930ms step_avg:58.63ms
step:1893/2330 train_time:110988ms step_avg:58.63ms
step:1894/2330 train_time:111048ms step_avg:58.63ms
step:1895/2330 train_time:111107ms step_avg:58.63ms
step:1896/2330 train_time:111167ms step_avg:58.63ms
step:1897/2330 train_time:111223ms step_avg:58.63ms
step:1898/2330 train_time:111285ms step_avg:58.63ms
step:1899/2330 train_time:111342ms step_avg:58.63ms
step:1900/2330 train_time:111403ms step_avg:58.63ms
step:1901/2330 train_time:111460ms step_avg:58.63ms
step:1902/2330 train_time:111522ms step_avg:58.63ms
step:1903/2330 train_time:111578ms step_avg:58.63ms
step:1904/2330 train_time:111640ms step_avg:58.63ms
step:1905/2330 train_time:111697ms step_avg:58.63ms
step:1906/2330 train_time:111759ms step_avg:58.64ms
step:1907/2330 train_time:111816ms step_avg:58.63ms
step:1908/2330 train_time:111878ms step_avg:58.64ms
step:1909/2330 train_time:111935ms step_avg:58.64ms
step:1910/2330 train_time:111997ms step_avg:58.64ms
step:1911/2330 train_time:112054ms step_avg:58.64ms
step:1912/2330 train_time:112116ms step_avg:58.64ms
step:1913/2330 train_time:112173ms step_avg:58.64ms
step:1914/2330 train_time:112236ms step_avg:58.64ms
step:1915/2330 train_time:112293ms step_avg:58.64ms
step:1916/2330 train_time:112354ms step_avg:58.64ms
step:1917/2330 train_time:112413ms step_avg:58.64ms
step:1918/2330 train_time:112473ms step_avg:58.64ms
step:1919/2330 train_time:112531ms step_avg:58.64ms
step:1920/2330 train_time:112592ms step_avg:58.64ms
step:1921/2330 train_time:112649ms step_avg:58.64ms
step:1922/2330 train_time:112709ms step_avg:58.64ms
step:1923/2330 train_time:112766ms step_avg:58.64ms
step:1924/2330 train_time:112827ms step_avg:58.64ms
step:1925/2330 train_time:112885ms step_avg:58.64ms
step:1926/2330 train_time:112945ms step_avg:58.64ms
step:1927/2330 train_time:113003ms step_avg:58.64ms
step:1928/2330 train_time:113064ms step_avg:58.64ms
step:1929/2330 train_time:113122ms step_avg:58.64ms
step:1930/2330 train_time:113183ms step_avg:58.64ms
step:1931/2330 train_time:113240ms step_avg:58.64ms
step:1932/2330 train_time:113302ms step_avg:58.64ms
step:1933/2330 train_time:113359ms step_avg:58.64ms
step:1934/2330 train_time:113420ms step_avg:58.65ms
step:1935/2330 train_time:113477ms step_avg:58.64ms
step:1936/2330 train_time:113540ms step_avg:58.65ms
step:1937/2330 train_time:113597ms step_avg:58.65ms
step:1938/2330 train_time:113658ms step_avg:58.65ms
step:1939/2330 train_time:113715ms step_avg:58.65ms
step:1940/2330 train_time:113777ms step_avg:58.65ms
step:1941/2330 train_time:113834ms step_avg:58.65ms
step:1942/2330 train_time:113896ms step_avg:58.65ms
step:1943/2330 train_time:113953ms step_avg:58.65ms
step:1944/2330 train_time:114014ms step_avg:58.65ms
step:1945/2330 train_time:114072ms step_avg:58.65ms
step:1946/2330 train_time:114132ms step_avg:58.65ms
step:1947/2330 train_time:114190ms step_avg:58.65ms
step:1948/2330 train_time:114250ms step_avg:58.65ms
step:1949/2330 train_time:114309ms step_avg:58.65ms
step:1950/2330 train_time:114369ms step_avg:58.65ms
step:1951/2330 train_time:114427ms step_avg:58.65ms
step:1952/2330 train_time:114488ms step_avg:58.65ms
step:1953/2330 train_time:114546ms step_avg:58.65ms
step:1954/2330 train_time:114606ms step_avg:58.65ms
step:1955/2330 train_time:114663ms step_avg:58.65ms
step:1956/2330 train_time:114724ms step_avg:58.65ms
step:1957/2330 train_time:114781ms step_avg:58.65ms
step:1958/2330 train_time:114842ms step_avg:58.65ms
step:1959/2330 train_time:114899ms step_avg:58.65ms
step:1960/2330 train_time:114962ms step_avg:58.65ms
step:1961/2330 train_time:115017ms step_avg:58.65ms
step:1962/2330 train_time:115080ms step_avg:58.65ms
step:1963/2330 train_time:115137ms step_avg:58.65ms
step:1964/2330 train_time:115200ms step_avg:58.66ms
step:1965/2330 train_time:115257ms step_avg:58.65ms
step:1966/2330 train_time:115320ms step_avg:58.66ms
step:1967/2330 train_time:115376ms step_avg:58.66ms
step:1968/2330 train_time:115437ms step_avg:58.66ms
step:1969/2330 train_time:115494ms step_avg:58.66ms
step:1970/2330 train_time:115557ms step_avg:58.66ms
step:1971/2330 train_time:115614ms step_avg:58.66ms
step:1972/2330 train_time:115675ms step_avg:58.66ms
step:1973/2330 train_time:115733ms step_avg:58.66ms
step:1974/2330 train_time:115794ms step_avg:58.66ms
step:1975/2330 train_time:115852ms step_avg:58.66ms
step:1976/2330 train_time:115912ms step_avg:58.66ms
step:1977/2330 train_time:115969ms step_avg:58.66ms
step:1978/2330 train_time:116030ms step_avg:58.66ms
step:1979/2330 train_time:116088ms step_avg:58.66ms
step:1980/2330 train_time:116149ms step_avg:58.66ms
step:1981/2330 train_time:116207ms step_avg:58.66ms
step:1982/2330 train_time:116268ms step_avg:58.66ms
step:1983/2330 train_time:116326ms step_avg:58.66ms
step:1984/2330 train_time:116386ms step_avg:58.66ms
step:1985/2330 train_time:116443ms step_avg:58.66ms
step:1986/2330 train_time:116504ms step_avg:58.66ms
step:1987/2330 train_time:116561ms step_avg:58.66ms
step:1988/2330 train_time:116621ms step_avg:58.66ms
step:1989/2330 train_time:116678ms step_avg:58.66ms
step:1990/2330 train_time:116739ms step_avg:58.66ms
step:1991/2330 train_time:116796ms step_avg:58.66ms
step:1992/2330 train_time:116858ms step_avg:58.66ms
step:1993/2330 train_time:116916ms step_avg:58.66ms
step:1994/2330 train_time:116978ms step_avg:58.67ms
step:1995/2330 train_time:117035ms step_avg:58.66ms
step:1996/2330 train_time:117097ms step_avg:58.67ms
step:1997/2330 train_time:117154ms step_avg:58.67ms
step:1998/2330 train_time:117217ms step_avg:58.67ms
step:1999/2330 train_time:117273ms step_avg:58.67ms
step:2000/2330 train_time:117336ms step_avg:58.67ms
step:2000/2330 val_loss:3.7601 train_time:117418ms step_avg:58.71ms
step:2001/2330 train_time:117437ms step_avg:58.69ms
step:2002/2330 train_time:117458ms step_avg:58.67ms
step:2003/2330 train_time:117517ms step_avg:58.67ms
step:2004/2330 train_time:117584ms step_avg:58.67ms
step:2005/2330 train_time:117641ms step_avg:58.67ms
step:2006/2330 train_time:117703ms step_avg:58.68ms
step:2007/2330 train_time:117761ms step_avg:58.68ms
step:2008/2330 train_time:117820ms step_avg:58.68ms
step:2009/2330 train_time:117878ms step_avg:58.67ms
step:2010/2330 train_time:117937ms step_avg:58.68ms
step:2011/2330 train_time:117993ms step_avg:58.67ms
step:2012/2330 train_time:118054ms step_avg:58.67ms
step:2013/2330 train_time:118110ms step_avg:58.67ms
step:2014/2330 train_time:118171ms step_avg:58.67ms
step:2015/2330 train_time:118227ms step_avg:58.67ms
step:2016/2330 train_time:118287ms step_avg:58.67ms
step:2017/2330 train_time:118345ms step_avg:58.67ms
step:2018/2330 train_time:118406ms step_avg:58.68ms
step:2019/2330 train_time:118465ms step_avg:58.68ms
step:2020/2330 train_time:118528ms step_avg:58.68ms
step:2021/2330 train_time:118586ms step_avg:58.68ms
step:2022/2330 train_time:118648ms step_avg:58.68ms
step:2023/2330 train_time:118705ms step_avg:58.68ms
step:2024/2330 train_time:118766ms step_avg:58.68ms
step:2025/2330 train_time:118824ms step_avg:58.68ms
step:2026/2330 train_time:118884ms step_avg:58.68ms
step:2027/2330 train_time:118943ms step_avg:58.68ms
step:2028/2330 train_time:119003ms step_avg:58.68ms
step:2029/2330 train_time:119060ms step_avg:58.68ms
step:2030/2330 train_time:119120ms step_avg:58.68ms
step:2031/2330 train_time:119177ms step_avg:58.68ms
step:2032/2330 train_time:119237ms step_avg:58.68ms
step:2033/2330 train_time:119293ms step_avg:58.68ms
step:2034/2330 train_time:119354ms step_avg:58.68ms
step:2035/2330 train_time:119412ms step_avg:58.68ms
step:2036/2330 train_time:119476ms step_avg:58.68ms
step:2037/2330 train_time:119534ms step_avg:58.68ms
step:2038/2330 train_time:119597ms step_avg:58.68ms
step:2039/2330 train_time:119654ms step_avg:58.68ms
step:2040/2330 train_time:119716ms step_avg:58.68ms
step:2041/2330 train_time:119773ms step_avg:58.68ms
step:2042/2330 train_time:119836ms step_avg:58.69ms
step:2043/2330 train_time:119892ms step_avg:58.68ms
step:2044/2330 train_time:119955ms step_avg:58.69ms
step:2045/2330 train_time:120011ms step_avg:58.69ms
step:2046/2330 train_time:120072ms step_avg:58.69ms
step:2047/2330 train_time:120129ms step_avg:58.69ms
step:2048/2330 train_time:120189ms step_avg:58.69ms
step:2049/2330 train_time:120246ms step_avg:58.69ms
step:2050/2330 train_time:120307ms step_avg:58.69ms
step:2051/2330 train_time:120365ms step_avg:58.69ms
step:2052/2330 train_time:120426ms step_avg:58.69ms
step:2053/2330 train_time:120484ms step_avg:58.69ms
step:2054/2330 train_time:120545ms step_avg:58.69ms
step:2055/2330 train_time:120603ms step_avg:58.69ms
step:2056/2330 train_time:120664ms step_avg:58.69ms
step:2057/2330 train_time:120723ms step_avg:58.69ms
step:2058/2330 train_time:120783ms step_avg:58.69ms
step:2059/2330 train_time:120841ms step_avg:58.69ms
step:2060/2330 train_time:120901ms step_avg:58.69ms
step:2061/2330 train_time:120958ms step_avg:58.69ms
step:2062/2330 train_time:121018ms step_avg:58.69ms
step:2063/2330 train_time:121076ms step_avg:58.69ms
step:2064/2330 train_time:121136ms step_avg:58.69ms
step:2065/2330 train_time:121192ms step_avg:58.69ms
step:2066/2330 train_time:121254ms step_avg:58.69ms
step:2067/2330 train_time:121311ms step_avg:58.69ms
step:2068/2330 train_time:121374ms step_avg:58.69ms
step:2069/2330 train_time:121431ms step_avg:58.69ms
step:2070/2330 train_time:121494ms step_avg:58.69ms
step:2071/2330 train_time:121550ms step_avg:58.69ms
step:2072/2330 train_time:121614ms step_avg:58.69ms
step:2073/2330 train_time:121673ms step_avg:58.69ms
step:2074/2330 train_time:121733ms step_avg:58.69ms
step:2075/2330 train_time:121790ms step_avg:58.69ms
step:2076/2330 train_time:121853ms step_avg:58.70ms
step:2077/2330 train_time:121910ms step_avg:58.70ms
step:2078/2330 train_time:121971ms step_avg:58.70ms
step:2079/2330 train_time:122028ms step_avg:58.70ms
step:2080/2330 train_time:122088ms step_avg:58.70ms
step:2081/2330 train_time:122146ms step_avg:58.70ms
step:2082/2330 train_time:122206ms step_avg:58.70ms
step:2083/2330 train_time:122263ms step_avg:58.70ms
step:2084/2330 train_time:122324ms step_avg:58.70ms
step:2085/2330 train_time:122381ms step_avg:58.70ms
step:2086/2330 train_time:122442ms step_avg:58.70ms
step:2087/2330 train_time:122500ms step_avg:58.70ms
step:2088/2330 train_time:122560ms step_avg:58.70ms
step:2089/2330 train_time:122618ms step_avg:58.70ms
step:2090/2330 train_time:122679ms step_avg:58.70ms
step:2091/2330 train_time:122737ms step_avg:58.70ms
step:2092/2330 train_time:122798ms step_avg:58.70ms
step:2093/2330 train_time:122854ms step_avg:58.70ms
step:2094/2330 train_time:122917ms step_avg:58.70ms
step:2095/2330 train_time:122973ms step_avg:58.70ms
step:2096/2330 train_time:123034ms step_avg:58.70ms
step:2097/2330 train_time:123091ms step_avg:58.70ms
step:2098/2330 train_time:123153ms step_avg:58.70ms
step:2099/2330 train_time:123210ms step_avg:58.70ms
step:2100/2330 train_time:123272ms step_avg:58.70ms
step:2101/2330 train_time:123329ms step_avg:58.70ms
step:2102/2330 train_time:123391ms step_avg:58.70ms
step:2103/2330 train_time:123448ms step_avg:58.70ms
step:2104/2330 train_time:123510ms step_avg:58.70ms
step:2105/2330 train_time:123568ms step_avg:58.70ms
step:2106/2330 train_time:123628ms step_avg:58.70ms
step:2107/2330 train_time:123687ms step_avg:58.70ms
step:2108/2330 train_time:123747ms step_avg:58.70ms
step:2109/2330 train_time:123806ms step_avg:58.70ms
step:2110/2330 train_time:123867ms step_avg:58.70ms
step:2111/2330 train_time:123925ms step_avg:58.70ms
step:2112/2330 train_time:123986ms step_avg:58.71ms
step:2113/2330 train_time:124044ms step_avg:58.70ms
step:2114/2330 train_time:124104ms step_avg:58.71ms
step:2115/2330 train_time:124161ms step_avg:58.71ms
step:2116/2330 train_time:124221ms step_avg:58.71ms
step:2117/2330 train_time:124279ms step_avg:58.71ms
step:2118/2330 train_time:124338ms step_avg:58.71ms
step:2119/2330 train_time:124395ms step_avg:58.70ms
step:2120/2330 train_time:124457ms step_avg:58.71ms
step:2121/2330 train_time:124514ms step_avg:58.71ms
step:2122/2330 train_time:124577ms step_avg:58.71ms
step:2123/2330 train_time:124634ms step_avg:58.71ms
step:2124/2330 train_time:124696ms step_avg:58.71ms
step:2125/2330 train_time:124753ms step_avg:58.71ms
step:2126/2330 train_time:124816ms step_avg:58.71ms
step:2127/2330 train_time:124873ms step_avg:58.71ms
step:2128/2330 train_time:124935ms step_avg:58.71ms
step:2129/2330 train_time:124992ms step_avg:58.71ms
step:2130/2330 train_time:125055ms step_avg:58.71ms
step:2131/2330 train_time:125112ms step_avg:58.71ms
step:2132/2330 train_time:125173ms step_avg:58.71ms
step:2133/2330 train_time:125230ms step_avg:58.71ms
step:2134/2330 train_time:125291ms step_avg:58.71ms
step:2135/2330 train_time:125349ms step_avg:58.71ms
step:2136/2330 train_time:125411ms step_avg:58.71ms
step:2137/2330 train_time:125468ms step_avg:58.71ms
step:2138/2330 train_time:125530ms step_avg:58.71ms
step:2139/2330 train_time:125587ms step_avg:58.71ms
step:2140/2330 train_time:125648ms step_avg:58.71ms
step:2141/2330 train_time:125707ms step_avg:58.71ms
step:2142/2330 train_time:125767ms step_avg:58.71ms
step:2143/2330 train_time:125825ms step_avg:58.71ms
step:2144/2330 train_time:125886ms step_avg:58.72ms
step:2145/2330 train_time:125944ms step_avg:58.72ms
step:2146/2330 train_time:126005ms step_avg:58.72ms
step:2147/2330 train_time:126062ms step_avg:58.72ms
step:2148/2330 train_time:126122ms step_avg:58.72ms
step:2149/2330 train_time:126180ms step_avg:58.72ms
step:2150/2330 train_time:126240ms step_avg:58.72ms
step:2151/2330 train_time:126297ms step_avg:58.72ms
step:2152/2330 train_time:126357ms step_avg:58.72ms
step:2153/2330 train_time:126415ms step_avg:58.72ms
step:2154/2330 train_time:126476ms step_avg:58.72ms
step:2155/2330 train_time:126533ms step_avg:58.72ms
step:2156/2330 train_time:126596ms step_avg:58.72ms
step:2157/2330 train_time:126653ms step_avg:58.72ms
step:2158/2330 train_time:126715ms step_avg:58.72ms
step:2159/2330 train_time:126772ms step_avg:58.72ms
step:2160/2330 train_time:126836ms step_avg:58.72ms
step:2161/2330 train_time:126892ms step_avg:58.72ms
step:2162/2330 train_time:126955ms step_avg:58.72ms
step:2163/2330 train_time:127012ms step_avg:58.72ms
step:2164/2330 train_time:127073ms step_avg:58.72ms
step:2165/2330 train_time:127130ms step_avg:58.72ms
step:2166/2330 train_time:127191ms step_avg:58.72ms
step:2167/2330 train_time:127248ms step_avg:58.72ms
step:2168/2330 train_time:127308ms step_avg:58.72ms
step:2169/2330 train_time:127366ms step_avg:58.72ms
step:2170/2330 train_time:127426ms step_avg:58.72ms
step:2171/2330 train_time:127483ms step_avg:58.72ms
step:2172/2330 train_time:127544ms step_avg:58.72ms
step:2173/2330 train_time:127602ms step_avg:58.72ms
step:2174/2330 train_time:127662ms step_avg:58.72ms
step:2175/2330 train_time:127720ms step_avg:58.72ms
step:2176/2330 train_time:127781ms step_avg:58.72ms
step:2177/2330 train_time:127839ms step_avg:58.72ms
step:2178/2330 train_time:127898ms step_avg:58.72ms
step:2179/2330 train_time:127956ms step_avg:58.72ms
step:2180/2330 train_time:128018ms step_avg:58.72ms
step:2181/2330 train_time:128075ms step_avg:58.72ms
step:2182/2330 train_time:128136ms step_avg:58.72ms
step:2183/2330 train_time:128193ms step_avg:58.72ms
step:2184/2330 train_time:128255ms step_avg:58.72ms
step:2185/2330 train_time:128311ms step_avg:58.72ms
step:2186/2330 train_time:128374ms step_avg:58.73ms
step:2187/2330 train_time:128430ms step_avg:58.72ms
step:2188/2330 train_time:128493ms step_avg:58.73ms
step:2189/2330 train_time:128550ms step_avg:58.73ms
step:2190/2330 train_time:128613ms step_avg:58.73ms
step:2191/2330 train_time:128670ms step_avg:58.73ms
step:2192/2330 train_time:128733ms step_avg:58.73ms
step:2193/2330 train_time:128789ms step_avg:58.73ms
step:2194/2330 train_time:128852ms step_avg:58.73ms
step:2195/2330 train_time:128909ms step_avg:58.73ms
step:2196/2330 train_time:128971ms step_avg:58.73ms
step:2197/2330 train_time:129029ms step_avg:58.73ms
step:2198/2330 train_time:129089ms step_avg:58.73ms
step:2199/2330 train_time:129147ms step_avg:58.73ms
step:2200/2330 train_time:129208ms step_avg:58.73ms
step:2201/2330 train_time:129266ms step_avg:58.73ms
step:2202/2330 train_time:129326ms step_avg:58.73ms
step:2203/2330 train_time:129383ms step_avg:58.73ms
step:2204/2330 train_time:129444ms step_avg:58.73ms
step:2205/2330 train_time:129501ms step_avg:58.73ms
step:2206/2330 train_time:129562ms step_avg:58.73ms
step:2207/2330 train_time:129620ms step_avg:58.73ms
step:2208/2330 train_time:129680ms step_avg:58.73ms
step:2209/2330 train_time:129737ms step_avg:58.73ms
step:2210/2330 train_time:129799ms step_avg:58.73ms
step:2211/2330 train_time:129856ms step_avg:58.73ms
step:2212/2330 train_time:129920ms step_avg:58.73ms
step:2213/2330 train_time:129977ms step_avg:58.73ms
step:2214/2330 train_time:130039ms step_avg:58.73ms
step:2215/2330 train_time:130095ms step_avg:58.73ms
step:2216/2330 train_time:130158ms step_avg:58.74ms
step:2217/2330 train_time:130215ms step_avg:58.73ms
step:2218/2330 train_time:130278ms step_avg:58.74ms
step:2219/2330 train_time:130334ms step_avg:58.74ms
step:2220/2330 train_time:130396ms step_avg:58.74ms
step:2221/2330 train_time:130452ms step_avg:58.74ms
step:2222/2330 train_time:130514ms step_avg:58.74ms
step:2223/2330 train_time:130571ms step_avg:58.74ms
step:2224/2330 train_time:130633ms step_avg:58.74ms
step:2225/2330 train_time:130690ms step_avg:58.74ms
step:2226/2330 train_time:130752ms step_avg:58.74ms
step:2227/2330 train_time:130810ms step_avg:58.74ms
step:2228/2330 train_time:130871ms step_avg:58.74ms
step:2229/2330 train_time:130929ms step_avg:58.74ms
step:2230/2330 train_time:130989ms step_avg:58.74ms
step:2231/2330 train_time:131046ms step_avg:58.74ms
step:2232/2330 train_time:131107ms step_avg:58.74ms
step:2233/2330 train_time:131165ms step_avg:58.74ms
step:2234/2330 train_time:131226ms step_avg:58.74ms
step:2235/2330 train_time:131283ms step_avg:58.74ms
step:2236/2330 train_time:131344ms step_avg:58.74ms
step:2237/2330 train_time:131402ms step_avg:58.74ms
step:2238/2330 train_time:131462ms step_avg:58.74ms
step:2239/2330 train_time:131520ms step_avg:58.74ms
step:2240/2330 train_time:131580ms step_avg:58.74ms
step:2241/2330 train_time:131637ms step_avg:58.74ms
step:2242/2330 train_time:131697ms step_avg:58.74ms
step:2243/2330 train_time:131754ms step_avg:58.74ms
step:2244/2330 train_time:131816ms step_avg:58.74ms
step:2245/2330 train_time:131874ms step_avg:58.74ms
step:2246/2330 train_time:131936ms step_avg:58.74ms
step:2247/2330 train_time:131992ms step_avg:58.74ms
step:2248/2330 train_time:132055ms step_avg:58.74ms
step:2249/2330 train_time:132112ms step_avg:58.74ms
step:2250/2330 train_time:132175ms step_avg:58.74ms
step:2250/2330 val_loss:3.7120 train_time:132257ms step_avg:58.78ms
step:2251/2330 train_time:132277ms step_avg:58.76ms
step:2252/2330 train_time:132297ms step_avg:58.75ms
step:2253/2330 train_time:132357ms step_avg:58.75ms
step:2254/2330 train_time:132423ms step_avg:58.75ms
step:2255/2330 train_time:132480ms step_avg:58.75ms
step:2256/2330 train_time:132543ms step_avg:58.75ms
step:2257/2330 train_time:132600ms step_avg:58.75ms
step:2258/2330 train_time:132661ms step_avg:58.75ms
step:2259/2330 train_time:132718ms step_avg:58.75ms
step:2260/2330 train_time:132778ms step_avg:58.75ms
step:2261/2330 train_time:132835ms step_avg:58.75ms
step:2262/2330 train_time:132895ms step_avg:58.75ms
step:2263/2330 train_time:132951ms step_avg:58.75ms
step:2264/2330 train_time:133012ms step_avg:58.75ms
step:2265/2330 train_time:133069ms step_avg:58.75ms
step:2266/2330 train_time:133128ms step_avg:58.75ms
step:2267/2330 train_time:133185ms step_avg:58.75ms
step:2268/2330 train_time:133246ms step_avg:58.75ms
step:2269/2330 train_time:133306ms step_avg:58.75ms
step:2270/2330 train_time:133368ms step_avg:58.75ms
step:2271/2330 train_time:133427ms step_avg:58.75ms
step:2272/2330 train_time:133487ms step_avg:58.75ms
step:2273/2330 train_time:133546ms step_avg:58.75ms
step:2274/2330 train_time:133606ms step_avg:58.75ms
step:2275/2330 train_time:133663ms step_avg:58.75ms
step:2276/2330 train_time:133722ms step_avg:58.75ms
step:2277/2330 train_time:133780ms step_avg:58.75ms
step:2278/2330 train_time:133839ms step_avg:58.75ms
step:2279/2330 train_time:133896ms step_avg:58.75ms
step:2280/2330 train_time:133956ms step_avg:58.75ms
step:2281/2330 train_time:134012ms step_avg:58.75ms
step:2282/2330 train_time:134073ms step_avg:58.75ms
step:2283/2330 train_time:134129ms step_avg:58.75ms
step:2284/2330 train_time:134191ms step_avg:58.75ms
step:2285/2330 train_time:134249ms step_avg:58.75ms
step:2286/2330 train_time:134309ms step_avg:58.75ms
step:2287/2330 train_time:134367ms step_avg:58.75ms
step:2288/2330 train_time:134428ms step_avg:58.75ms
step:2289/2330 train_time:134486ms step_avg:58.75ms
step:2290/2330 train_time:134547ms step_avg:58.75ms
step:2291/2330 train_time:134605ms step_avg:58.75ms
step:2292/2330 train_time:134666ms step_avg:58.75ms
step:2293/2330 train_time:134724ms step_avg:58.75ms
step:2294/2330 train_time:134785ms step_avg:58.76ms
step:2295/2330 train_time:134844ms step_avg:58.76ms
step:2296/2330 train_time:134904ms step_avg:58.76ms
step:2297/2330 train_time:134961ms step_avg:58.76ms
step:2298/2330 train_time:135021ms step_avg:58.76ms
step:2299/2330 train_time:135078ms step_avg:58.76ms
step:2300/2330 train_time:135138ms step_avg:58.76ms
step:2301/2330 train_time:135195ms step_avg:58.75ms
step:2302/2330 train_time:135256ms step_avg:58.76ms
step:2303/2330 train_time:135313ms step_avg:58.76ms
step:2304/2330 train_time:135377ms step_avg:58.76ms
step:2305/2330 train_time:135434ms step_avg:58.76ms
step:2306/2330 train_time:135497ms step_avg:58.76ms
step:2307/2330 train_time:135553ms step_avg:58.76ms
step:2308/2330 train_time:135616ms step_avg:58.76ms
step:2309/2330 train_time:135673ms step_avg:58.76ms
step:2310/2330 train_time:135734ms step_avg:58.76ms
step:2311/2330 train_time:135792ms step_avg:58.76ms
step:2312/2330 train_time:135853ms step_avg:58.76ms
step:2313/2330 train_time:135910ms step_avg:58.76ms
step:2314/2330 train_time:135971ms step_avg:58.76ms
step:2315/2330 train_time:136028ms step_avg:58.76ms
step:2316/2330 train_time:136088ms step_avg:58.76ms
step:2317/2330 train_time:136146ms step_avg:58.76ms
step:2318/2330 train_time:136205ms step_avg:58.76ms
step:2319/2330 train_time:136263ms step_avg:58.76ms
step:2320/2330 train_time:136324ms step_avg:58.76ms
step:2321/2330 train_time:136382ms step_avg:58.76ms
step:2322/2330 train_time:136443ms step_avg:58.76ms
step:2323/2330 train_time:136500ms step_avg:58.76ms
step:2324/2330 train_time:136561ms step_avg:58.76ms
step:2325/2330 train_time:136618ms step_avg:58.76ms
step:2326/2330 train_time:136679ms step_avg:58.76ms
step:2327/2330 train_time:136737ms step_avg:58.76ms
step:2328/2330 train_time:136796ms step_avg:58.76ms
step:2329/2330 train_time:136853ms step_avg:58.76ms
step:2330/2330 train_time:136915ms step_avg:58.76ms
step:2330/2330 val_loss:3.6971 train_time:136998ms step_avg:58.80ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
