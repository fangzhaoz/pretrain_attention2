import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--beta1', type=str, required=True)
parser.add_argument('--beta2', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_beta_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=6e-4,  betas=(float(args2.beta1), float(args2.beta2)), eps=1e-10, weight_decay=0.0, fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt", exist_ok=True)
            torch.save(log, f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 04:31:13 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   27C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   25C    P0             115W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   24C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   27C    P0             113W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   25C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   26C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:83ms step_avg:83.23ms
step:2/2330 train_time:178ms step_avg:88.85ms
step:3/2330 train_time:195ms step_avg:65.01ms
step:4/2330 train_time:214ms step_avg:53.60ms
step:5/2330 train_time:268ms step_avg:53.64ms
step:6/2330 train_time:327ms step_avg:54.54ms
step:7/2330 train_time:382ms step_avg:54.64ms
step:8/2330 train_time:442ms step_avg:55.22ms
step:9/2330 train_time:497ms step_avg:55.22ms
step:10/2330 train_time:555ms step_avg:55.54ms
step:11/2330 train_time:610ms step_avg:55.47ms
step:12/2330 train_time:669ms step_avg:55.76ms
step:13/2330 train_time:724ms step_avg:55.71ms
step:14/2330 train_time:783ms step_avg:55.94ms
step:15/2330 train_time:839ms step_avg:55.90ms
step:16/2330 train_time:897ms step_avg:56.05ms
step:17/2330 train_time:952ms step_avg:55.99ms
step:18/2330 train_time:1011ms step_avg:56.15ms
step:19/2330 train_time:1067ms step_avg:56.17ms
step:20/2330 train_time:1130ms step_avg:56.49ms
step:21/2330 train_time:1187ms step_avg:56.52ms
step:22/2330 train_time:1248ms step_avg:56.75ms
step:23/2330 train_time:1304ms step_avg:56.71ms
step:24/2330 train_time:1364ms step_avg:56.84ms
step:25/2330 train_time:1420ms step_avg:56.80ms
step:26/2330 train_time:1479ms step_avg:56.89ms
step:27/2330 train_time:1535ms step_avg:56.85ms
step:28/2330 train_time:1593ms step_avg:56.89ms
step:29/2330 train_time:1649ms step_avg:56.86ms
step:30/2330 train_time:1707ms step_avg:56.91ms
step:31/2330 train_time:1763ms step_avg:56.86ms
step:32/2330 train_time:1821ms step_avg:56.92ms
step:33/2330 train_time:1877ms step_avg:56.86ms
step:34/2330 train_time:1936ms step_avg:56.94ms
step:35/2330 train_time:1991ms step_avg:56.89ms
step:36/2330 train_time:2051ms step_avg:56.98ms
step:37/2330 train_time:2108ms step_avg:56.97ms
step:38/2330 train_time:2169ms step_avg:57.07ms
step:39/2330 train_time:2225ms step_avg:57.05ms
step:40/2330 train_time:2284ms step_avg:57.10ms
step:41/2330 train_time:2340ms step_avg:57.06ms
step:42/2330 train_time:2400ms step_avg:57.14ms
step:43/2330 train_time:2455ms step_avg:57.10ms
step:44/2330 train_time:2515ms step_avg:57.15ms
step:45/2330 train_time:2570ms step_avg:57.11ms
step:46/2330 train_time:2629ms step_avg:57.15ms
step:47/2330 train_time:2684ms step_avg:57.12ms
step:48/2330 train_time:2743ms step_avg:57.15ms
step:49/2330 train_time:2799ms step_avg:57.12ms
step:50/2330 train_time:2858ms step_avg:57.16ms
step:51/2330 train_time:2914ms step_avg:57.13ms
step:52/2330 train_time:2973ms step_avg:57.17ms
step:53/2330 train_time:3028ms step_avg:57.14ms
step:54/2330 train_time:3088ms step_avg:57.18ms
step:55/2330 train_time:3145ms step_avg:57.17ms
step:56/2330 train_time:3204ms step_avg:57.22ms
step:57/2330 train_time:3260ms step_avg:57.19ms
step:58/2330 train_time:3320ms step_avg:57.24ms
step:59/2330 train_time:3376ms step_avg:57.21ms
step:60/2330 train_time:3435ms step_avg:57.26ms
step:61/2330 train_time:3491ms step_avg:57.23ms
step:62/2330 train_time:3550ms step_avg:57.25ms
step:63/2330 train_time:3606ms step_avg:57.24ms
step:64/2330 train_time:3665ms step_avg:57.26ms
step:65/2330 train_time:3722ms step_avg:57.26ms
step:66/2330 train_time:3781ms step_avg:57.28ms
step:67/2330 train_time:3836ms step_avg:57.26ms
step:68/2330 train_time:3895ms step_avg:57.27ms
step:69/2330 train_time:3951ms step_avg:57.26ms
step:70/2330 train_time:4009ms step_avg:57.28ms
step:71/2330 train_time:4065ms step_avg:57.26ms
step:72/2330 train_time:4125ms step_avg:57.29ms
step:73/2330 train_time:4181ms step_avg:57.28ms
step:74/2330 train_time:4242ms step_avg:57.32ms
step:75/2330 train_time:4298ms step_avg:57.30ms
step:76/2330 train_time:4356ms step_avg:57.32ms
step:77/2330 train_time:4412ms step_avg:57.30ms
step:78/2330 train_time:4471ms step_avg:57.33ms
step:79/2330 train_time:4527ms step_avg:57.30ms
step:80/2330 train_time:4587ms step_avg:57.34ms
step:81/2330 train_time:4643ms step_avg:57.32ms
step:82/2330 train_time:4702ms step_avg:57.34ms
step:83/2330 train_time:4758ms step_avg:57.33ms
step:84/2330 train_time:4816ms step_avg:57.34ms
step:85/2330 train_time:4872ms step_avg:57.32ms
step:86/2330 train_time:4931ms step_avg:57.33ms
step:87/2330 train_time:4986ms step_avg:57.31ms
step:88/2330 train_time:5047ms step_avg:57.35ms
step:89/2330 train_time:5103ms step_avg:57.34ms
step:90/2330 train_time:5162ms step_avg:57.35ms
step:91/2330 train_time:5219ms step_avg:57.35ms
step:92/2330 train_time:5278ms step_avg:57.37ms
step:93/2330 train_time:5334ms step_avg:57.36ms
step:94/2330 train_time:5392ms step_avg:57.36ms
step:95/2330 train_time:5448ms step_avg:57.35ms
step:96/2330 train_time:5508ms step_avg:57.38ms
step:97/2330 train_time:5565ms step_avg:57.37ms
step:98/2330 train_time:5624ms step_avg:57.38ms
step:99/2330 train_time:5679ms step_avg:57.37ms
step:100/2330 train_time:5739ms step_avg:57.39ms
step:101/2330 train_time:5794ms step_avg:57.37ms
step:102/2330 train_time:5854ms step_avg:57.39ms
step:103/2330 train_time:5910ms step_avg:57.38ms
step:104/2330 train_time:5969ms step_avg:57.39ms
step:105/2330 train_time:6025ms step_avg:57.38ms
step:106/2330 train_time:6084ms step_avg:57.40ms
step:107/2330 train_time:6142ms step_avg:57.40ms
step:108/2330 train_time:6201ms step_avg:57.42ms
step:109/2330 train_time:6257ms step_avg:57.40ms
step:110/2330 train_time:6316ms step_avg:57.42ms
step:111/2330 train_time:6372ms step_avg:57.40ms
step:112/2330 train_time:6430ms step_avg:57.41ms
step:113/2330 train_time:6486ms step_avg:57.40ms
step:114/2330 train_time:6545ms step_avg:57.42ms
step:115/2330 train_time:6602ms step_avg:57.41ms
step:116/2330 train_time:6660ms step_avg:57.42ms
step:117/2330 train_time:6716ms step_avg:57.40ms
step:118/2330 train_time:6775ms step_avg:57.41ms
step:119/2330 train_time:6830ms step_avg:57.40ms
step:120/2330 train_time:6889ms step_avg:57.41ms
step:121/2330 train_time:6944ms step_avg:57.39ms
step:122/2330 train_time:7004ms step_avg:57.41ms
step:123/2330 train_time:7060ms step_avg:57.40ms
step:124/2330 train_time:7119ms step_avg:57.41ms
step:125/2330 train_time:7175ms step_avg:57.40ms
step:126/2330 train_time:7235ms step_avg:57.42ms
step:127/2330 train_time:7290ms step_avg:57.40ms
step:128/2330 train_time:7350ms step_avg:57.42ms
step:129/2330 train_time:7405ms step_avg:57.41ms
step:130/2330 train_time:7466ms step_avg:57.43ms
step:131/2330 train_time:7522ms step_avg:57.42ms
step:132/2330 train_time:7581ms step_avg:57.43ms
step:133/2330 train_time:7637ms step_avg:57.42ms
step:134/2330 train_time:7697ms step_avg:57.44ms
step:135/2330 train_time:7753ms step_avg:57.43ms
step:136/2330 train_time:7812ms step_avg:57.44ms
step:137/2330 train_time:7868ms step_avg:57.43ms
step:138/2330 train_time:7926ms step_avg:57.44ms
step:139/2330 train_time:7982ms step_avg:57.42ms
step:140/2330 train_time:8041ms step_avg:57.44ms
step:141/2330 train_time:8096ms step_avg:57.42ms
step:142/2330 train_time:8157ms step_avg:57.44ms
step:143/2330 train_time:8212ms step_avg:57.43ms
step:144/2330 train_time:8272ms step_avg:57.44ms
step:145/2330 train_time:8327ms step_avg:57.43ms
step:146/2330 train_time:8387ms step_avg:57.44ms
step:147/2330 train_time:8443ms step_avg:57.43ms
step:148/2330 train_time:8502ms step_avg:57.45ms
step:149/2330 train_time:8559ms step_avg:57.44ms
step:150/2330 train_time:8617ms step_avg:57.45ms
step:151/2330 train_time:8673ms step_avg:57.44ms
step:152/2330 train_time:8732ms step_avg:57.45ms
step:153/2330 train_time:8787ms step_avg:57.43ms
step:154/2330 train_time:8847ms step_avg:57.45ms
step:155/2330 train_time:8903ms step_avg:57.44ms
step:156/2330 train_time:8962ms step_avg:57.45ms
step:157/2330 train_time:9018ms step_avg:57.44ms
step:158/2330 train_time:9077ms step_avg:57.45ms
step:159/2330 train_time:9133ms step_avg:57.44ms
step:160/2330 train_time:9191ms step_avg:57.45ms
step:161/2330 train_time:9247ms step_avg:57.43ms
step:162/2330 train_time:9308ms step_avg:57.45ms
step:163/2330 train_time:9363ms step_avg:57.44ms
step:164/2330 train_time:9422ms step_avg:57.45ms
step:165/2330 train_time:9479ms step_avg:57.45ms
step:166/2330 train_time:9537ms step_avg:57.45ms
step:167/2330 train_time:9593ms step_avg:57.44ms
step:168/2330 train_time:9652ms step_avg:57.45ms
step:169/2330 train_time:9708ms step_avg:57.44ms
step:170/2330 train_time:9768ms step_avg:57.46ms
step:171/2330 train_time:9824ms step_avg:57.45ms
step:172/2330 train_time:9884ms step_avg:57.46ms
step:173/2330 train_time:9940ms step_avg:57.46ms
step:174/2330 train_time:9999ms step_avg:57.46ms
step:175/2330 train_time:10055ms step_avg:57.46ms
step:176/2330 train_time:10114ms step_avg:57.46ms
step:177/2330 train_time:10170ms step_avg:57.45ms
step:178/2330 train_time:10228ms step_avg:57.46ms
step:179/2330 train_time:10284ms step_avg:57.45ms
step:180/2330 train_time:10345ms step_avg:57.47ms
step:181/2330 train_time:10401ms step_avg:57.46ms
step:182/2330 train_time:10460ms step_avg:57.47ms
step:183/2330 train_time:10516ms step_avg:57.47ms
step:184/2330 train_time:10576ms step_avg:57.48ms
step:185/2330 train_time:10631ms step_avg:57.47ms
step:186/2330 train_time:10690ms step_avg:57.47ms
step:187/2330 train_time:10746ms step_avg:57.46ms
step:188/2330 train_time:10805ms step_avg:57.47ms
step:189/2330 train_time:10861ms step_avg:57.47ms
step:190/2330 train_time:10920ms step_avg:57.47ms
step:191/2330 train_time:10976ms step_avg:57.46ms
step:192/2330 train_time:11035ms step_avg:57.48ms
step:193/2330 train_time:11091ms step_avg:57.47ms
step:194/2330 train_time:11150ms step_avg:57.48ms
step:195/2330 train_time:11206ms step_avg:57.47ms
step:196/2330 train_time:11266ms step_avg:57.48ms
step:197/2330 train_time:11321ms step_avg:57.47ms
step:198/2330 train_time:11381ms step_avg:57.48ms
step:199/2330 train_time:11437ms step_avg:57.47ms
step:200/2330 train_time:11496ms step_avg:57.48ms
step:201/2330 train_time:11552ms step_avg:57.47ms
step:202/2330 train_time:11611ms step_avg:57.48ms
step:203/2330 train_time:11668ms step_avg:57.48ms
step:204/2330 train_time:11726ms step_avg:57.48ms
step:205/2330 train_time:11782ms step_avg:57.47ms
step:206/2330 train_time:11842ms step_avg:57.49ms
step:207/2330 train_time:11898ms step_avg:57.48ms
step:208/2330 train_time:11957ms step_avg:57.49ms
step:209/2330 train_time:12012ms step_avg:57.48ms
step:210/2330 train_time:12072ms step_avg:57.48ms
step:211/2330 train_time:12127ms step_avg:57.48ms
step:212/2330 train_time:12187ms step_avg:57.48ms
step:213/2330 train_time:12243ms step_avg:57.48ms
step:214/2330 train_time:12303ms step_avg:57.49ms
step:215/2330 train_time:12358ms step_avg:57.48ms
step:216/2330 train_time:12418ms step_avg:57.49ms
step:217/2330 train_time:12474ms step_avg:57.48ms
step:218/2330 train_time:12532ms step_avg:57.49ms
step:219/2330 train_time:12588ms step_avg:57.48ms
step:220/2330 train_time:12648ms step_avg:57.49ms
step:221/2330 train_time:12704ms step_avg:57.48ms
step:222/2330 train_time:12762ms step_avg:57.49ms
step:223/2330 train_time:12818ms step_avg:57.48ms
step:224/2330 train_time:12877ms step_avg:57.49ms
step:225/2330 train_time:12933ms step_avg:57.48ms
step:226/2330 train_time:12992ms step_avg:57.48ms
step:227/2330 train_time:13047ms step_avg:57.48ms
step:228/2330 train_time:13107ms step_avg:57.49ms
step:229/2330 train_time:13163ms step_avg:57.48ms
step:230/2330 train_time:13223ms step_avg:57.49ms
step:231/2330 train_time:13278ms step_avg:57.48ms
step:232/2330 train_time:13339ms step_avg:57.50ms
step:233/2330 train_time:13395ms step_avg:57.49ms
step:234/2330 train_time:13454ms step_avg:57.49ms
step:235/2330 train_time:13510ms step_avg:57.49ms
step:236/2330 train_time:13569ms step_avg:57.49ms
step:237/2330 train_time:13625ms step_avg:57.49ms
step:238/2330 train_time:13685ms step_avg:57.50ms
step:239/2330 train_time:13742ms step_avg:57.50ms
step:240/2330 train_time:13801ms step_avg:57.50ms
step:241/2330 train_time:13857ms step_avg:57.50ms
step:242/2330 train_time:13917ms step_avg:57.51ms
step:243/2330 train_time:13973ms step_avg:57.50ms
step:244/2330 train_time:14031ms step_avg:57.51ms
step:245/2330 train_time:14087ms step_avg:57.50ms
step:246/2330 train_time:14147ms step_avg:57.51ms
step:247/2330 train_time:14202ms step_avg:57.50ms
step:248/2330 train_time:14262ms step_avg:57.51ms
step:249/2330 train_time:14318ms step_avg:57.50ms
step:250/2330 train_time:14376ms step_avg:57.51ms
step:250/2330 val_loss:4.9157 train_time:14455ms step_avg:57.82ms
step:251/2330 train_time:14472ms step_avg:57.66ms
step:252/2330 train_time:14494ms step_avg:57.51ms
step:253/2330 train_time:14549ms step_avg:57.51ms
step:254/2330 train_time:14611ms step_avg:57.53ms
step:255/2330 train_time:14668ms step_avg:57.52ms
step:256/2330 train_time:14730ms step_avg:57.54ms
step:257/2330 train_time:14786ms step_avg:57.53ms
step:258/2330 train_time:14846ms step_avg:57.54ms
step:259/2330 train_time:14901ms step_avg:57.53ms
step:260/2330 train_time:14960ms step_avg:57.54ms
step:261/2330 train_time:15016ms step_avg:57.53ms
step:262/2330 train_time:15074ms step_avg:57.53ms
step:263/2330 train_time:15130ms step_avg:57.53ms
step:264/2330 train_time:15188ms step_avg:57.53ms
step:265/2330 train_time:15243ms step_avg:57.52ms
step:266/2330 train_time:15302ms step_avg:57.53ms
step:267/2330 train_time:15358ms step_avg:57.52ms
step:268/2330 train_time:15417ms step_avg:57.53ms
step:269/2330 train_time:15474ms step_avg:57.52ms
step:270/2330 train_time:15532ms step_avg:57.53ms
step:271/2330 train_time:15589ms step_avg:57.52ms
step:272/2330 train_time:15649ms step_avg:57.53ms
step:273/2330 train_time:15704ms step_avg:57.53ms
step:274/2330 train_time:15766ms step_avg:57.54ms
step:275/2330 train_time:15822ms step_avg:57.53ms
step:276/2330 train_time:15882ms step_avg:57.54ms
step:277/2330 train_time:15938ms step_avg:57.54ms
step:278/2330 train_time:15998ms step_avg:57.55ms
step:279/2330 train_time:16054ms step_avg:57.54ms
step:280/2330 train_time:16112ms step_avg:57.54ms
step:281/2330 train_time:16168ms step_avg:57.54ms
step:282/2330 train_time:16226ms step_avg:57.54ms
step:283/2330 train_time:16281ms step_avg:57.53ms
step:284/2330 train_time:16340ms step_avg:57.54ms
step:285/2330 train_time:16397ms step_avg:57.53ms
step:286/2330 train_time:16457ms step_avg:57.54ms
step:287/2330 train_time:16513ms step_avg:57.54ms
step:288/2330 train_time:16572ms step_avg:57.54ms
step:289/2330 train_time:16628ms step_avg:57.54ms
step:290/2330 train_time:16689ms step_avg:57.55ms
step:291/2330 train_time:16744ms step_avg:57.54ms
step:292/2330 train_time:16805ms step_avg:57.55ms
step:293/2330 train_time:16861ms step_avg:57.54ms
step:294/2330 train_time:16921ms step_avg:57.55ms
step:295/2330 train_time:16977ms step_avg:57.55ms
step:296/2330 train_time:17036ms step_avg:57.55ms
step:297/2330 train_time:17092ms step_avg:57.55ms
step:298/2330 train_time:17150ms step_avg:57.55ms
step:299/2330 train_time:17206ms step_avg:57.54ms
step:300/2330 train_time:17265ms step_avg:57.55ms
step:301/2330 train_time:17321ms step_avg:57.54ms
step:302/2330 train_time:17380ms step_avg:57.55ms
step:303/2330 train_time:17437ms step_avg:57.55ms
step:304/2330 train_time:17496ms step_avg:57.55ms
step:305/2330 train_time:17552ms step_avg:57.55ms
step:306/2330 train_time:17611ms step_avg:57.55ms
step:307/2330 train_time:17667ms step_avg:57.55ms
step:308/2330 train_time:17727ms step_avg:57.55ms
step:309/2330 train_time:17783ms step_avg:57.55ms
step:310/2330 train_time:17843ms step_avg:57.56ms
step:311/2330 train_time:17899ms step_avg:57.55ms
step:312/2330 train_time:17958ms step_avg:57.56ms
step:313/2330 train_time:18013ms step_avg:57.55ms
step:314/2330 train_time:18072ms step_avg:57.56ms
step:315/2330 train_time:18128ms step_avg:57.55ms
step:316/2330 train_time:18187ms step_avg:57.55ms
step:317/2330 train_time:18243ms step_avg:57.55ms
step:318/2330 train_time:18303ms step_avg:57.56ms
step:319/2330 train_time:18358ms step_avg:57.55ms
step:320/2330 train_time:18418ms step_avg:57.56ms
step:321/2330 train_time:18474ms step_avg:57.55ms
step:322/2330 train_time:18533ms step_avg:57.56ms
step:323/2330 train_time:18590ms step_avg:57.55ms
step:324/2330 train_time:18649ms step_avg:57.56ms
step:325/2330 train_time:18705ms step_avg:57.55ms
step:326/2330 train_time:18764ms step_avg:57.56ms
step:327/2330 train_time:18820ms step_avg:57.55ms
step:328/2330 train_time:18880ms step_avg:57.56ms
step:329/2330 train_time:18936ms step_avg:57.56ms
step:330/2330 train_time:18996ms step_avg:57.56ms
step:331/2330 train_time:19052ms step_avg:57.56ms
step:332/2330 train_time:19111ms step_avg:57.56ms
step:333/2330 train_time:19166ms step_avg:57.56ms
step:334/2330 train_time:19225ms step_avg:57.56ms
step:335/2330 train_time:19281ms step_avg:57.56ms
step:336/2330 train_time:19340ms step_avg:57.56ms
step:337/2330 train_time:19397ms step_avg:57.56ms
step:338/2330 train_time:19456ms step_avg:57.56ms
step:339/2330 train_time:19511ms step_avg:57.56ms
step:340/2330 train_time:19571ms step_avg:57.56ms
step:341/2330 train_time:19627ms step_avg:57.56ms
step:342/2330 train_time:19686ms step_avg:57.56ms
step:343/2330 train_time:19743ms step_avg:57.56ms
step:344/2330 train_time:19802ms step_avg:57.56ms
step:345/2330 train_time:19858ms step_avg:57.56ms
step:346/2330 train_time:19918ms step_avg:57.57ms
step:347/2330 train_time:19974ms step_avg:57.56ms
step:348/2330 train_time:20033ms step_avg:57.56ms
step:349/2330 train_time:20088ms step_avg:57.56ms
step:350/2330 train_time:20147ms step_avg:57.56ms
step:351/2330 train_time:20203ms step_avg:57.56ms
step:352/2330 train_time:20263ms step_avg:57.57ms
step:353/2330 train_time:20319ms step_avg:57.56ms
step:354/2330 train_time:20378ms step_avg:57.56ms
step:355/2330 train_time:20434ms step_avg:57.56ms
step:356/2330 train_time:20493ms step_avg:57.57ms
step:357/2330 train_time:20550ms step_avg:57.56ms
step:358/2330 train_time:20608ms step_avg:57.56ms
step:359/2330 train_time:20664ms step_avg:57.56ms
step:360/2330 train_time:20724ms step_avg:57.57ms
step:361/2330 train_time:20780ms step_avg:57.56ms
step:362/2330 train_time:20840ms step_avg:57.57ms
step:363/2330 train_time:20896ms step_avg:57.57ms
step:364/2330 train_time:20955ms step_avg:57.57ms
step:365/2330 train_time:21011ms step_avg:57.56ms
step:366/2330 train_time:21070ms step_avg:57.57ms
step:367/2330 train_time:21125ms step_avg:57.56ms
step:368/2330 train_time:21185ms step_avg:57.57ms
step:369/2330 train_time:21241ms step_avg:57.56ms
step:370/2330 train_time:21300ms step_avg:57.57ms
step:371/2330 train_time:21357ms step_avg:57.56ms
step:372/2330 train_time:21416ms step_avg:57.57ms
step:373/2330 train_time:21472ms step_avg:57.57ms
step:374/2330 train_time:21531ms step_avg:57.57ms
step:375/2330 train_time:21587ms step_avg:57.56ms
step:376/2330 train_time:21646ms step_avg:57.57ms
step:377/2330 train_time:21701ms step_avg:57.56ms
step:378/2330 train_time:21761ms step_avg:57.57ms
step:379/2330 train_time:21818ms step_avg:57.57ms
step:380/2330 train_time:21877ms step_avg:57.57ms
step:381/2330 train_time:21933ms step_avg:57.57ms
step:382/2330 train_time:21992ms step_avg:57.57ms
step:383/2330 train_time:22048ms step_avg:57.57ms
step:384/2330 train_time:22107ms step_avg:57.57ms
step:385/2330 train_time:22163ms step_avg:57.57ms
step:386/2330 train_time:22222ms step_avg:57.57ms
step:387/2330 train_time:22278ms step_avg:57.57ms
step:388/2330 train_time:22337ms step_avg:57.57ms
step:389/2330 train_time:22393ms step_avg:57.57ms
step:390/2330 train_time:22452ms step_avg:57.57ms
step:391/2330 train_time:22508ms step_avg:57.56ms
step:392/2330 train_time:22567ms step_avg:57.57ms
step:393/2330 train_time:22623ms step_avg:57.56ms
step:394/2330 train_time:22681ms step_avg:57.57ms
step:395/2330 train_time:22738ms step_avg:57.56ms
step:396/2330 train_time:22797ms step_avg:57.57ms
step:397/2330 train_time:22854ms step_avg:57.57ms
step:398/2330 train_time:22912ms step_avg:57.57ms
step:399/2330 train_time:22968ms step_avg:57.56ms
step:400/2330 train_time:23027ms step_avg:57.57ms
step:401/2330 train_time:23083ms step_avg:57.56ms
step:402/2330 train_time:23143ms step_avg:57.57ms
step:403/2330 train_time:23199ms step_avg:57.57ms
step:404/2330 train_time:23258ms step_avg:57.57ms
step:405/2330 train_time:23314ms step_avg:57.57ms
step:406/2330 train_time:23373ms step_avg:57.57ms
step:407/2330 train_time:23429ms step_avg:57.56ms
step:408/2330 train_time:23488ms step_avg:57.57ms
step:409/2330 train_time:23544ms step_avg:57.56ms
step:410/2330 train_time:23604ms step_avg:57.57ms
step:411/2330 train_time:23659ms step_avg:57.57ms
step:412/2330 train_time:23720ms step_avg:57.57ms
step:413/2330 train_time:23776ms step_avg:57.57ms
step:414/2330 train_time:23836ms step_avg:57.57ms
step:415/2330 train_time:23892ms step_avg:57.57ms
step:416/2330 train_time:23951ms step_avg:57.57ms
step:417/2330 train_time:24007ms step_avg:57.57ms
step:418/2330 train_time:24066ms step_avg:57.57ms
step:419/2330 train_time:24121ms step_avg:57.57ms
step:420/2330 train_time:24182ms step_avg:57.58ms
step:421/2330 train_time:24238ms step_avg:57.57ms
step:422/2330 train_time:24297ms step_avg:57.58ms
step:423/2330 train_time:24353ms step_avg:57.57ms
step:424/2330 train_time:24412ms step_avg:57.58ms
step:425/2330 train_time:24468ms step_avg:57.57ms
step:426/2330 train_time:24527ms step_avg:57.58ms
step:427/2330 train_time:24583ms step_avg:57.57ms
step:428/2330 train_time:24643ms step_avg:57.58ms
step:429/2330 train_time:24699ms step_avg:57.57ms
step:430/2330 train_time:24757ms step_avg:57.58ms
step:431/2330 train_time:24813ms step_avg:57.57ms
step:432/2330 train_time:24872ms step_avg:57.57ms
step:433/2330 train_time:24928ms step_avg:57.57ms
step:434/2330 train_time:24987ms step_avg:57.57ms
step:435/2330 train_time:25043ms step_avg:57.57ms
step:436/2330 train_time:25103ms step_avg:57.58ms
step:437/2330 train_time:25158ms step_avg:57.57ms
step:438/2330 train_time:25218ms step_avg:57.58ms
step:439/2330 train_time:25274ms step_avg:57.57ms
step:440/2330 train_time:25333ms step_avg:57.58ms
step:441/2330 train_time:25389ms step_avg:57.57ms
step:442/2330 train_time:25448ms step_avg:57.57ms
step:443/2330 train_time:25504ms step_avg:57.57ms
step:444/2330 train_time:25565ms step_avg:57.58ms
step:445/2330 train_time:25621ms step_avg:57.57ms
step:446/2330 train_time:25680ms step_avg:57.58ms
step:447/2330 train_time:25736ms step_avg:57.57ms
step:448/2330 train_time:25795ms step_avg:57.58ms
step:449/2330 train_time:25851ms step_avg:57.58ms
step:450/2330 train_time:25911ms step_avg:57.58ms
step:451/2330 train_time:25967ms step_avg:57.58ms
step:452/2330 train_time:26026ms step_avg:57.58ms
step:453/2330 train_time:26081ms step_avg:57.57ms
step:454/2330 train_time:26142ms step_avg:57.58ms
step:455/2330 train_time:26198ms step_avg:57.58ms
step:456/2330 train_time:26258ms step_avg:57.58ms
step:457/2330 train_time:26314ms step_avg:57.58ms
step:458/2330 train_time:26373ms step_avg:57.58ms
step:459/2330 train_time:26429ms step_avg:57.58ms
step:460/2330 train_time:26488ms step_avg:57.58ms
step:461/2330 train_time:26543ms step_avg:57.58ms
step:462/2330 train_time:26604ms step_avg:57.58ms
step:463/2330 train_time:26660ms step_avg:57.58ms
step:464/2330 train_time:26719ms step_avg:57.58ms
step:465/2330 train_time:26775ms step_avg:57.58ms
step:466/2330 train_time:26834ms step_avg:57.58ms
step:467/2330 train_time:26890ms step_avg:57.58ms
step:468/2330 train_time:26949ms step_avg:57.58ms
step:469/2330 train_time:27005ms step_avg:57.58ms
step:470/2330 train_time:27064ms step_avg:57.58ms
step:471/2330 train_time:27120ms step_avg:57.58ms
step:472/2330 train_time:27180ms step_avg:57.59ms
step:473/2330 train_time:27237ms step_avg:57.58ms
step:474/2330 train_time:27295ms step_avg:57.58ms
step:475/2330 train_time:27351ms step_avg:57.58ms
step:476/2330 train_time:27410ms step_avg:57.58ms
step:477/2330 train_time:27465ms step_avg:57.58ms
step:478/2330 train_time:27526ms step_avg:57.59ms
step:479/2330 train_time:27582ms step_avg:57.58ms
step:480/2330 train_time:27643ms step_avg:57.59ms
step:481/2330 train_time:27699ms step_avg:57.59ms
step:482/2330 train_time:27758ms step_avg:57.59ms
step:483/2330 train_time:27813ms step_avg:57.58ms
step:484/2330 train_time:27872ms step_avg:57.59ms
step:485/2330 train_time:27928ms step_avg:57.58ms
step:486/2330 train_time:27988ms step_avg:57.59ms
step:487/2330 train_time:28043ms step_avg:57.58ms
step:488/2330 train_time:28104ms step_avg:57.59ms
step:489/2330 train_time:28160ms step_avg:57.59ms
step:490/2330 train_time:28221ms step_avg:57.59ms
step:491/2330 train_time:28277ms step_avg:57.59ms
step:492/2330 train_time:28336ms step_avg:57.59ms
step:493/2330 train_time:28392ms step_avg:57.59ms
step:494/2330 train_time:28451ms step_avg:57.59ms
step:495/2330 train_time:28507ms step_avg:57.59ms
step:496/2330 train_time:28567ms step_avg:57.59ms
step:497/2330 train_time:28622ms step_avg:57.59ms
step:498/2330 train_time:28682ms step_avg:57.59ms
step:499/2330 train_time:28739ms step_avg:57.59ms
step:500/2330 train_time:28798ms step_avg:57.60ms
step:500/2330 val_loss:4.4132 train_time:28877ms step_avg:57.75ms
step:501/2330 train_time:28895ms step_avg:57.67ms
step:502/2330 train_time:28917ms step_avg:57.60ms
step:503/2330 train_time:28974ms step_avg:57.60ms
step:504/2330 train_time:29036ms step_avg:57.61ms
step:505/2330 train_time:29094ms step_avg:57.61ms
step:506/2330 train_time:29153ms step_avg:57.61ms
step:507/2330 train_time:29209ms step_avg:57.61ms
step:508/2330 train_time:29268ms step_avg:57.61ms
step:509/2330 train_time:29323ms step_avg:57.61ms
step:510/2330 train_time:29382ms step_avg:57.61ms
step:511/2330 train_time:29438ms step_avg:57.61ms
step:512/2330 train_time:29496ms step_avg:57.61ms
step:513/2330 train_time:29551ms step_avg:57.61ms
step:514/2330 train_time:29610ms step_avg:57.61ms
step:515/2330 train_time:29666ms step_avg:57.60ms
step:516/2330 train_time:29725ms step_avg:57.61ms
step:517/2330 train_time:29780ms step_avg:57.60ms
step:518/2330 train_time:29840ms step_avg:57.61ms
step:519/2330 train_time:29897ms step_avg:57.60ms
step:520/2330 train_time:29958ms step_avg:57.61ms
step:521/2330 train_time:30016ms step_avg:57.61ms
step:522/2330 train_time:30076ms step_avg:57.62ms
step:523/2330 train_time:30133ms step_avg:57.62ms
step:524/2330 train_time:30192ms step_avg:57.62ms
step:525/2330 train_time:30248ms step_avg:57.62ms
step:526/2330 train_time:30307ms step_avg:57.62ms
step:527/2330 train_time:30363ms step_avg:57.61ms
step:528/2330 train_time:30422ms step_avg:57.62ms
step:529/2330 train_time:30477ms step_avg:57.61ms
step:530/2330 train_time:30536ms step_avg:57.62ms
step:531/2330 train_time:30591ms step_avg:57.61ms
step:532/2330 train_time:30650ms step_avg:57.61ms
step:533/2330 train_time:30705ms step_avg:57.61ms
step:534/2330 train_time:30764ms step_avg:57.61ms
step:535/2330 train_time:30820ms step_avg:57.61ms
step:536/2330 train_time:30881ms step_avg:57.61ms
step:537/2330 train_time:30937ms step_avg:57.61ms
step:538/2330 train_time:30998ms step_avg:57.62ms
step:539/2330 train_time:31054ms step_avg:57.61ms
step:540/2330 train_time:31115ms step_avg:57.62ms
step:541/2330 train_time:31172ms step_avg:57.62ms
step:542/2330 train_time:31230ms step_avg:57.62ms
step:543/2330 train_time:31286ms step_avg:57.62ms
step:544/2330 train_time:31345ms step_avg:57.62ms
step:545/2330 train_time:31401ms step_avg:57.62ms
step:546/2330 train_time:31460ms step_avg:57.62ms
step:547/2330 train_time:31515ms step_avg:57.61ms
step:548/2330 train_time:31575ms step_avg:57.62ms
step:549/2330 train_time:31630ms step_avg:57.61ms
step:550/2330 train_time:31690ms step_avg:57.62ms
step:551/2330 train_time:31745ms step_avg:57.61ms
step:552/2330 train_time:31805ms step_avg:57.62ms
step:553/2330 train_time:31861ms step_avg:57.61ms
step:554/2330 train_time:31922ms step_avg:57.62ms
step:555/2330 train_time:31979ms step_avg:57.62ms
step:556/2330 train_time:32038ms step_avg:57.62ms
step:557/2330 train_time:32095ms step_avg:57.62ms
step:558/2330 train_time:32154ms step_avg:57.62ms
step:559/2330 train_time:32210ms step_avg:57.62ms
step:560/2330 train_time:32269ms step_avg:57.62ms
step:561/2330 train_time:32325ms step_avg:57.62ms
step:562/2330 train_time:32384ms step_avg:57.62ms
step:563/2330 train_time:32440ms step_avg:57.62ms
step:564/2330 train_time:32499ms step_avg:57.62ms
step:565/2330 train_time:32555ms step_avg:57.62ms
step:566/2330 train_time:32613ms step_avg:57.62ms
step:567/2330 train_time:32669ms step_avg:57.62ms
step:568/2330 train_time:32728ms step_avg:57.62ms
step:569/2330 train_time:32783ms step_avg:57.61ms
step:570/2330 train_time:32843ms step_avg:57.62ms
step:571/2330 train_time:32899ms step_avg:57.62ms
step:572/2330 train_time:32960ms step_avg:57.62ms
step:573/2330 train_time:33017ms step_avg:57.62ms
step:574/2330 train_time:33076ms step_avg:57.62ms
step:575/2330 train_time:33133ms step_avg:57.62ms
step:576/2330 train_time:33192ms step_avg:57.63ms
step:577/2330 train_time:33249ms step_avg:57.62ms
step:578/2330 train_time:33308ms step_avg:57.63ms
step:579/2330 train_time:33363ms step_avg:57.62ms
step:580/2330 train_time:33424ms step_avg:57.63ms
step:581/2330 train_time:33480ms step_avg:57.63ms
step:582/2330 train_time:33539ms step_avg:57.63ms
step:583/2330 train_time:33595ms step_avg:57.62ms
step:584/2330 train_time:33653ms step_avg:57.63ms
step:585/2330 train_time:33709ms step_avg:57.62ms
step:586/2330 train_time:33768ms step_avg:57.62ms
step:587/2330 train_time:33823ms step_avg:57.62ms
step:588/2330 train_time:33884ms step_avg:57.63ms
step:589/2330 train_time:33940ms step_avg:57.62ms
step:590/2330 train_time:34000ms step_avg:57.63ms
step:591/2330 train_time:34056ms step_avg:57.62ms
step:592/2330 train_time:34116ms step_avg:57.63ms
step:593/2330 train_time:34172ms step_avg:57.63ms
step:594/2330 train_time:34231ms step_avg:57.63ms
step:595/2330 train_time:34288ms step_avg:57.63ms
step:596/2330 train_time:34347ms step_avg:57.63ms
step:597/2330 train_time:34402ms step_avg:57.63ms
step:598/2330 train_time:34462ms step_avg:57.63ms
step:599/2330 train_time:34519ms step_avg:57.63ms
step:600/2330 train_time:34578ms step_avg:57.63ms
step:601/2330 train_time:34634ms step_avg:57.63ms
step:602/2330 train_time:34693ms step_avg:57.63ms
step:603/2330 train_time:34749ms step_avg:57.63ms
step:604/2330 train_time:34807ms step_avg:57.63ms
step:605/2330 train_time:34862ms step_avg:57.62ms
step:606/2330 train_time:34925ms step_avg:57.63ms
step:607/2330 train_time:34980ms step_avg:57.63ms
step:608/2330 train_time:35041ms step_avg:57.63ms
step:609/2330 train_time:35097ms step_avg:57.63ms
step:610/2330 train_time:35157ms step_avg:57.63ms
step:611/2330 train_time:35213ms step_avg:57.63ms
step:612/2330 train_time:35272ms step_avg:57.63ms
step:613/2330 train_time:35328ms step_avg:57.63ms
step:614/2330 train_time:35387ms step_avg:57.63ms
step:615/2330 train_time:35442ms step_avg:57.63ms
step:616/2330 train_time:35503ms step_avg:57.63ms
step:617/2330 train_time:35559ms step_avg:57.63ms
step:618/2330 train_time:35618ms step_avg:57.63ms
step:619/2330 train_time:35675ms step_avg:57.63ms
step:620/2330 train_time:35734ms step_avg:57.63ms
step:621/2330 train_time:35790ms step_avg:57.63ms
step:622/2330 train_time:35848ms step_avg:57.63ms
step:623/2330 train_time:35904ms step_avg:57.63ms
step:624/2330 train_time:35965ms step_avg:57.64ms
step:625/2330 train_time:36021ms step_avg:57.63ms
step:626/2330 train_time:36080ms step_avg:57.64ms
step:627/2330 train_time:36137ms step_avg:57.63ms
step:628/2330 train_time:36196ms step_avg:57.64ms
step:629/2330 train_time:36252ms step_avg:57.63ms
step:630/2330 train_time:36312ms step_avg:57.64ms
step:631/2330 train_time:36367ms step_avg:57.63ms
step:632/2330 train_time:36427ms step_avg:57.64ms
step:633/2330 train_time:36483ms step_avg:57.63ms
step:634/2330 train_time:36543ms step_avg:57.64ms
step:635/2330 train_time:36599ms step_avg:57.64ms
step:636/2330 train_time:36659ms step_avg:57.64ms
step:637/2330 train_time:36715ms step_avg:57.64ms
step:638/2330 train_time:36775ms step_avg:57.64ms
step:639/2330 train_time:36831ms step_avg:57.64ms
step:640/2330 train_time:36889ms step_avg:57.64ms
step:641/2330 train_time:36945ms step_avg:57.64ms
step:642/2330 train_time:37005ms step_avg:57.64ms
step:643/2330 train_time:37061ms step_avg:57.64ms
step:644/2330 train_time:37121ms step_avg:57.64ms
step:645/2330 train_time:37178ms step_avg:57.64ms
step:646/2330 train_time:37237ms step_avg:57.64ms
step:647/2330 train_time:37294ms step_avg:57.64ms
step:648/2330 train_time:37353ms step_avg:57.64ms
step:649/2330 train_time:37409ms step_avg:57.64ms
step:650/2330 train_time:37468ms step_avg:57.64ms
step:651/2330 train_time:37524ms step_avg:57.64ms
step:652/2330 train_time:37584ms step_avg:57.64ms
step:653/2330 train_time:37640ms step_avg:57.64ms
step:654/2330 train_time:37699ms step_avg:57.64ms
step:655/2330 train_time:37756ms step_avg:57.64ms
step:656/2330 train_time:37815ms step_avg:57.64ms
step:657/2330 train_time:37870ms step_avg:57.64ms
step:658/2330 train_time:37930ms step_avg:57.64ms
step:659/2330 train_time:37986ms step_avg:57.64ms
step:660/2330 train_time:38047ms step_avg:57.65ms
step:661/2330 train_time:38102ms step_avg:57.64ms
step:662/2330 train_time:38163ms step_avg:57.65ms
step:663/2330 train_time:38218ms step_avg:57.64ms
step:664/2330 train_time:38278ms step_avg:57.65ms
step:665/2330 train_time:38335ms step_avg:57.65ms
step:666/2330 train_time:38394ms step_avg:57.65ms
step:667/2330 train_time:38450ms step_avg:57.65ms
step:668/2330 train_time:38508ms step_avg:57.65ms
step:669/2330 train_time:38564ms step_avg:57.64ms
step:670/2330 train_time:38624ms step_avg:57.65ms
step:671/2330 train_time:38681ms step_avg:57.65ms
step:672/2330 train_time:38741ms step_avg:57.65ms
step:673/2330 train_time:38797ms step_avg:57.65ms
step:674/2330 train_time:38856ms step_avg:57.65ms
step:675/2330 train_time:38912ms step_avg:57.65ms
step:676/2330 train_time:38971ms step_avg:57.65ms
step:677/2330 train_time:39027ms step_avg:57.65ms
step:678/2330 train_time:39087ms step_avg:57.65ms
step:679/2330 train_time:39143ms step_avg:57.65ms
step:680/2330 train_time:39204ms step_avg:57.65ms
step:681/2330 train_time:39260ms step_avg:57.65ms
step:682/2330 train_time:39319ms step_avg:57.65ms
step:683/2330 train_time:39375ms step_avg:57.65ms
step:684/2330 train_time:39435ms step_avg:57.65ms
step:685/2330 train_time:39490ms step_avg:57.65ms
step:686/2330 train_time:39550ms step_avg:57.65ms
step:687/2330 train_time:39605ms step_avg:57.65ms
step:688/2330 train_time:39665ms step_avg:57.65ms
step:689/2330 train_time:39721ms step_avg:57.65ms
step:690/2330 train_time:39781ms step_avg:57.65ms
step:691/2330 train_time:39837ms step_avg:57.65ms
step:692/2330 train_time:39897ms step_avg:57.65ms
step:693/2330 train_time:39953ms step_avg:57.65ms
step:694/2330 train_time:40012ms step_avg:57.65ms
step:695/2330 train_time:40068ms step_avg:57.65ms
step:696/2330 train_time:40128ms step_avg:57.65ms
step:697/2330 train_time:40184ms step_avg:57.65ms
step:698/2330 train_time:40244ms step_avg:57.66ms
step:699/2330 train_time:40301ms step_avg:57.65ms
step:700/2330 train_time:40362ms step_avg:57.66ms
step:701/2330 train_time:40418ms step_avg:57.66ms
step:702/2330 train_time:40477ms step_avg:57.66ms
step:703/2330 train_time:40533ms step_avg:57.66ms
step:704/2330 train_time:40591ms step_avg:57.66ms
step:705/2330 train_time:40647ms step_avg:57.66ms
step:706/2330 train_time:40708ms step_avg:57.66ms
step:707/2330 train_time:40763ms step_avg:57.66ms
step:708/2330 train_time:40824ms step_avg:57.66ms
step:709/2330 train_time:40880ms step_avg:57.66ms
step:710/2330 train_time:40940ms step_avg:57.66ms
step:711/2330 train_time:40996ms step_avg:57.66ms
step:712/2330 train_time:41056ms step_avg:57.66ms
step:713/2330 train_time:41112ms step_avg:57.66ms
step:714/2330 train_time:41171ms step_avg:57.66ms
step:715/2330 train_time:41227ms step_avg:57.66ms
step:716/2330 train_time:41286ms step_avg:57.66ms
step:717/2330 train_time:41342ms step_avg:57.66ms
step:718/2330 train_time:41402ms step_avg:57.66ms
step:719/2330 train_time:41458ms step_avg:57.66ms
step:720/2330 train_time:41517ms step_avg:57.66ms
step:721/2330 train_time:41573ms step_avg:57.66ms
step:722/2330 train_time:41632ms step_avg:57.66ms
step:723/2330 train_time:41688ms step_avg:57.66ms
step:724/2330 train_time:41747ms step_avg:57.66ms
step:725/2330 train_time:41803ms step_avg:57.66ms
step:726/2330 train_time:41864ms step_avg:57.66ms
step:727/2330 train_time:41920ms step_avg:57.66ms
step:728/2330 train_time:41980ms step_avg:57.66ms
step:729/2330 train_time:42036ms step_avg:57.66ms
step:730/2330 train_time:42096ms step_avg:57.67ms
step:731/2330 train_time:42152ms step_avg:57.66ms
step:732/2330 train_time:42211ms step_avg:57.67ms
step:733/2330 train_time:42267ms step_avg:57.66ms
step:734/2330 train_time:42326ms step_avg:57.67ms
step:735/2330 train_time:42382ms step_avg:57.66ms
step:736/2330 train_time:42443ms step_avg:57.67ms
step:737/2330 train_time:42498ms step_avg:57.66ms
step:738/2330 train_time:42558ms step_avg:57.67ms
step:739/2330 train_time:42614ms step_avg:57.67ms
step:740/2330 train_time:42673ms step_avg:57.67ms
step:741/2330 train_time:42729ms step_avg:57.66ms
step:742/2330 train_time:42789ms step_avg:57.67ms
step:743/2330 train_time:42845ms step_avg:57.66ms
step:744/2330 train_time:42905ms step_avg:57.67ms
step:745/2330 train_time:42960ms step_avg:57.66ms
step:746/2330 train_time:43021ms step_avg:57.67ms
step:747/2330 train_time:43077ms step_avg:57.67ms
step:748/2330 train_time:43137ms step_avg:57.67ms
step:749/2330 train_time:43193ms step_avg:57.67ms
step:750/2330 train_time:43252ms step_avg:57.67ms
step:750/2330 val_loss:4.2077 train_time:43332ms step_avg:57.78ms
step:751/2330 train_time:43349ms step_avg:57.72ms
step:752/2330 train_time:43370ms step_avg:57.67ms
step:753/2330 train_time:43427ms step_avg:57.67ms
step:754/2330 train_time:43493ms step_avg:57.68ms
step:755/2330 train_time:43550ms step_avg:57.68ms
step:756/2330 train_time:43611ms step_avg:57.69ms
step:757/2330 train_time:43667ms step_avg:57.68ms
step:758/2330 train_time:43727ms step_avg:57.69ms
step:759/2330 train_time:43783ms step_avg:57.68ms
step:760/2330 train_time:43842ms step_avg:57.69ms
step:761/2330 train_time:43898ms step_avg:57.68ms
step:762/2330 train_time:43956ms step_avg:57.69ms
step:763/2330 train_time:44012ms step_avg:57.68ms
step:764/2330 train_time:44070ms step_avg:57.68ms
step:765/2330 train_time:44127ms step_avg:57.68ms
step:766/2330 train_time:44185ms step_avg:57.68ms
step:767/2330 train_time:44241ms step_avg:57.68ms
step:768/2330 train_time:44300ms step_avg:57.68ms
step:769/2330 train_time:44358ms step_avg:57.68ms
step:770/2330 train_time:44419ms step_avg:57.69ms
step:771/2330 train_time:44478ms step_avg:57.69ms
step:772/2330 train_time:44539ms step_avg:57.69ms
step:773/2330 train_time:44598ms step_avg:57.69ms
step:774/2330 train_time:44657ms step_avg:57.70ms
step:775/2330 train_time:44714ms step_avg:57.70ms
step:776/2330 train_time:44774ms step_avg:57.70ms
step:777/2330 train_time:44831ms step_avg:57.70ms
step:778/2330 train_time:44890ms step_avg:57.70ms
step:779/2330 train_time:44946ms step_avg:57.70ms
step:780/2330 train_time:45006ms step_avg:57.70ms
step:781/2330 train_time:45063ms step_avg:57.70ms
step:782/2330 train_time:45122ms step_avg:57.70ms
step:783/2330 train_time:45179ms step_avg:57.70ms
step:784/2330 train_time:45238ms step_avg:57.70ms
step:785/2330 train_time:45295ms step_avg:57.70ms
step:786/2330 train_time:45355ms step_avg:57.70ms
step:787/2330 train_time:45413ms step_avg:57.70ms
step:788/2330 train_time:45473ms step_avg:57.71ms
step:789/2330 train_time:45532ms step_avg:57.71ms
step:790/2330 train_time:45591ms step_avg:57.71ms
step:791/2330 train_time:45648ms step_avg:57.71ms
step:792/2330 train_time:45709ms step_avg:57.71ms
step:793/2330 train_time:45766ms step_avg:57.71ms
step:794/2330 train_time:45827ms step_avg:57.72ms
step:795/2330 train_time:45884ms step_avg:57.72ms
step:796/2330 train_time:45943ms step_avg:57.72ms
step:797/2330 train_time:45999ms step_avg:57.72ms
step:798/2330 train_time:46059ms step_avg:57.72ms
step:799/2330 train_time:46115ms step_avg:57.72ms
step:800/2330 train_time:46176ms step_avg:57.72ms
step:801/2330 train_time:46232ms step_avg:57.72ms
step:802/2330 train_time:46293ms step_avg:57.72ms
step:803/2330 train_time:46350ms step_avg:57.72ms
step:804/2330 train_time:46410ms step_avg:57.72ms
step:805/2330 train_time:46468ms step_avg:57.72ms
step:806/2330 train_time:46529ms step_avg:57.73ms
step:807/2330 train_time:46587ms step_avg:57.73ms
step:808/2330 train_time:46647ms step_avg:57.73ms
step:809/2330 train_time:46705ms step_avg:57.73ms
step:810/2330 train_time:46765ms step_avg:57.73ms
step:811/2330 train_time:46822ms step_avg:57.73ms
step:812/2330 train_time:46882ms step_avg:57.74ms
step:813/2330 train_time:46939ms step_avg:57.74ms
step:814/2330 train_time:46999ms step_avg:57.74ms
step:815/2330 train_time:47055ms step_avg:57.74ms
step:816/2330 train_time:47114ms step_avg:57.74ms
step:817/2330 train_time:47171ms step_avg:57.74ms
step:818/2330 train_time:47230ms step_avg:57.74ms
step:819/2330 train_time:47286ms step_avg:57.74ms
step:820/2330 train_time:47347ms step_avg:57.74ms
step:821/2330 train_time:47405ms step_avg:57.74ms
step:822/2330 train_time:47465ms step_avg:57.74ms
step:823/2330 train_time:47523ms step_avg:57.74ms
step:824/2330 train_time:47582ms step_avg:57.75ms
step:825/2330 train_time:47640ms step_avg:57.75ms
step:826/2330 train_time:47700ms step_avg:57.75ms
step:827/2330 train_time:47757ms step_avg:57.75ms
step:828/2330 train_time:47817ms step_avg:57.75ms
step:829/2330 train_time:47874ms step_avg:57.75ms
step:830/2330 train_time:47935ms step_avg:57.75ms
step:831/2330 train_time:47991ms step_avg:57.75ms
step:832/2330 train_time:48052ms step_avg:57.75ms
step:833/2330 train_time:48108ms step_avg:57.75ms
step:834/2330 train_time:48168ms step_avg:57.76ms
step:835/2330 train_time:48225ms step_avg:57.75ms
step:836/2330 train_time:48285ms step_avg:57.76ms
step:837/2330 train_time:48342ms step_avg:57.76ms
step:838/2330 train_time:48401ms step_avg:57.76ms
step:839/2330 train_time:48458ms step_avg:57.76ms
step:840/2330 train_time:48519ms step_avg:57.76ms
step:841/2330 train_time:48576ms step_avg:57.76ms
step:842/2330 train_time:48637ms step_avg:57.76ms
step:843/2330 train_time:48694ms step_avg:57.76ms
step:844/2330 train_time:48753ms step_avg:57.76ms
step:845/2330 train_time:48811ms step_avg:57.76ms
step:846/2330 train_time:48870ms step_avg:57.77ms
step:847/2330 train_time:48927ms step_avg:57.77ms
step:848/2330 train_time:48989ms step_avg:57.77ms
step:849/2330 train_time:49045ms step_avg:57.77ms
step:850/2330 train_time:49106ms step_avg:57.77ms
step:851/2330 train_time:49162ms step_avg:57.77ms
step:852/2330 train_time:49222ms step_avg:57.77ms
step:853/2330 train_time:49280ms step_avg:57.77ms
step:854/2330 train_time:49339ms step_avg:57.77ms
step:855/2330 train_time:49395ms step_avg:57.77ms
step:856/2330 train_time:49455ms step_avg:57.77ms
step:857/2330 train_time:49512ms step_avg:57.77ms
step:858/2330 train_time:49574ms step_avg:57.78ms
step:859/2330 train_time:49630ms step_avg:57.78ms
step:860/2330 train_time:49691ms step_avg:57.78ms
step:861/2330 train_time:49747ms step_avg:57.78ms
step:862/2330 train_time:49809ms step_avg:57.78ms
step:863/2330 train_time:49866ms step_avg:57.78ms
step:864/2330 train_time:49927ms step_avg:57.79ms
step:865/2330 train_time:49984ms step_avg:57.79ms
step:866/2330 train_time:50044ms step_avg:57.79ms
step:867/2330 train_time:50101ms step_avg:57.79ms
step:868/2330 train_time:50160ms step_avg:57.79ms
step:869/2330 train_time:50217ms step_avg:57.79ms
step:870/2330 train_time:50277ms step_avg:57.79ms
step:871/2330 train_time:50334ms step_avg:57.79ms
step:872/2330 train_time:50393ms step_avg:57.79ms
step:873/2330 train_time:50450ms step_avg:57.79ms
step:874/2330 train_time:50511ms step_avg:57.79ms
step:875/2330 train_time:50568ms step_avg:57.79ms
step:876/2330 train_time:50628ms step_avg:57.79ms
step:877/2330 train_time:50685ms step_avg:57.79ms
step:878/2330 train_time:50745ms step_avg:57.80ms
step:879/2330 train_time:50802ms step_avg:57.79ms
step:880/2330 train_time:50862ms step_avg:57.80ms
step:881/2330 train_time:50920ms step_avg:57.80ms
step:882/2330 train_time:50980ms step_avg:57.80ms
step:883/2330 train_time:51037ms step_avg:57.80ms
step:884/2330 train_time:51096ms step_avg:57.80ms
step:885/2330 train_time:51152ms step_avg:57.80ms
step:886/2330 train_time:51213ms step_avg:57.80ms
step:887/2330 train_time:51270ms step_avg:57.80ms
step:888/2330 train_time:51330ms step_avg:57.80ms
step:889/2330 train_time:51387ms step_avg:57.80ms
step:890/2330 train_time:51447ms step_avg:57.81ms
step:891/2330 train_time:51503ms step_avg:57.80ms
step:892/2330 train_time:51564ms step_avg:57.81ms
step:893/2330 train_time:51621ms step_avg:57.81ms
step:894/2330 train_time:51681ms step_avg:57.81ms
step:895/2330 train_time:51739ms step_avg:57.81ms
step:896/2330 train_time:51799ms step_avg:57.81ms
step:897/2330 train_time:51855ms step_avg:57.81ms
step:898/2330 train_time:51915ms step_avg:57.81ms
step:899/2330 train_time:51972ms step_avg:57.81ms
step:900/2330 train_time:52032ms step_avg:57.81ms
step:901/2330 train_time:52089ms step_avg:57.81ms
step:902/2330 train_time:52150ms step_avg:57.82ms
step:903/2330 train_time:52207ms step_avg:57.82ms
step:904/2330 train_time:52268ms step_avg:57.82ms
step:905/2330 train_time:52325ms step_avg:57.82ms
step:906/2330 train_time:52385ms step_avg:57.82ms
step:907/2330 train_time:52443ms step_avg:57.82ms
step:908/2330 train_time:52502ms step_avg:57.82ms
step:909/2330 train_time:52559ms step_avg:57.82ms
step:910/2330 train_time:52619ms step_avg:57.82ms
step:911/2330 train_time:52675ms step_avg:57.82ms
step:912/2330 train_time:52736ms step_avg:57.82ms
step:913/2330 train_time:52793ms step_avg:57.82ms
step:914/2330 train_time:52852ms step_avg:57.83ms
step:915/2330 train_time:52909ms step_avg:57.82ms
step:916/2330 train_time:52970ms step_avg:57.83ms
step:917/2330 train_time:53027ms step_avg:57.83ms
step:918/2330 train_time:53088ms step_avg:57.83ms
step:919/2330 train_time:53145ms step_avg:57.83ms
step:920/2330 train_time:53206ms step_avg:57.83ms
step:921/2330 train_time:53262ms step_avg:57.83ms
step:922/2330 train_time:53323ms step_avg:57.83ms
step:923/2330 train_time:53381ms step_avg:57.83ms
step:924/2330 train_time:53440ms step_avg:57.84ms
step:925/2330 train_time:53497ms step_avg:57.83ms
step:926/2330 train_time:53557ms step_avg:57.84ms
step:927/2330 train_time:53613ms step_avg:57.83ms
step:928/2330 train_time:53674ms step_avg:57.84ms
step:929/2330 train_time:53730ms step_avg:57.84ms
step:930/2330 train_time:53791ms step_avg:57.84ms
step:931/2330 train_time:53848ms step_avg:57.84ms
step:932/2330 train_time:53908ms step_avg:57.84ms
step:933/2330 train_time:53966ms step_avg:57.84ms
step:934/2330 train_time:54026ms step_avg:57.84ms
step:935/2330 train_time:54084ms step_avg:57.84ms
step:936/2330 train_time:54143ms step_avg:57.85ms
step:937/2330 train_time:54200ms step_avg:57.84ms
step:938/2330 train_time:54260ms step_avg:57.85ms
step:939/2330 train_time:54317ms step_avg:57.85ms
step:940/2330 train_time:54377ms step_avg:57.85ms
step:941/2330 train_time:54433ms step_avg:57.85ms
step:942/2330 train_time:54493ms step_avg:57.85ms
step:943/2330 train_time:54550ms step_avg:57.85ms
step:944/2330 train_time:54610ms step_avg:57.85ms
step:945/2330 train_time:54667ms step_avg:57.85ms
step:946/2330 train_time:54727ms step_avg:57.85ms
step:947/2330 train_time:54784ms step_avg:57.85ms
step:948/2330 train_time:54843ms step_avg:57.85ms
step:949/2330 train_time:54901ms step_avg:57.85ms
step:950/2330 train_time:54960ms step_avg:57.85ms
step:951/2330 train_time:55018ms step_avg:57.85ms
step:952/2330 train_time:55077ms step_avg:57.85ms
step:953/2330 train_time:55134ms step_avg:57.85ms
step:954/2330 train_time:55194ms step_avg:57.86ms
step:955/2330 train_time:55250ms step_avg:57.85ms
step:956/2330 train_time:55312ms step_avg:57.86ms
step:957/2330 train_time:55369ms step_avg:57.86ms
step:958/2330 train_time:55431ms step_avg:57.86ms
step:959/2330 train_time:55487ms step_avg:57.86ms
step:960/2330 train_time:55548ms step_avg:57.86ms
step:961/2330 train_time:55605ms step_avg:57.86ms
step:962/2330 train_time:55665ms step_avg:57.86ms
step:963/2330 train_time:55722ms step_avg:57.86ms
step:964/2330 train_time:55781ms step_avg:57.86ms
step:965/2330 train_time:55839ms step_avg:57.86ms
step:966/2330 train_time:55898ms step_avg:57.87ms
step:967/2330 train_time:55955ms step_avg:57.86ms
step:968/2330 train_time:56014ms step_avg:57.87ms
step:969/2330 train_time:56072ms step_avg:57.87ms
step:970/2330 train_time:56132ms step_avg:57.87ms
step:971/2330 train_time:56189ms step_avg:57.87ms
step:972/2330 train_time:56249ms step_avg:57.87ms
step:973/2330 train_time:56306ms step_avg:57.87ms
step:974/2330 train_time:56366ms step_avg:57.87ms
step:975/2330 train_time:56423ms step_avg:57.87ms
step:976/2330 train_time:56484ms step_avg:57.87ms
step:977/2330 train_time:56541ms step_avg:57.87ms
step:978/2330 train_time:56600ms step_avg:57.87ms
step:979/2330 train_time:56657ms step_avg:57.87ms
step:980/2330 train_time:56717ms step_avg:57.87ms
step:981/2330 train_time:56774ms step_avg:57.87ms
step:982/2330 train_time:56834ms step_avg:57.88ms
step:983/2330 train_time:56890ms step_avg:57.87ms
step:984/2330 train_time:56952ms step_avg:57.88ms
step:985/2330 train_time:57009ms step_avg:57.88ms
step:986/2330 train_time:57070ms step_avg:57.88ms
step:987/2330 train_time:57126ms step_avg:57.88ms
step:988/2330 train_time:57188ms step_avg:57.88ms
step:989/2330 train_time:57245ms step_avg:57.88ms
step:990/2330 train_time:57305ms step_avg:57.88ms
step:991/2330 train_time:57362ms step_avg:57.88ms
step:992/2330 train_time:57422ms step_avg:57.89ms
step:993/2330 train_time:57480ms step_avg:57.88ms
step:994/2330 train_time:57539ms step_avg:57.89ms
step:995/2330 train_time:57597ms step_avg:57.89ms
step:996/2330 train_time:57656ms step_avg:57.89ms
step:997/2330 train_time:57712ms step_avg:57.89ms
step:998/2330 train_time:57773ms step_avg:57.89ms
step:999/2330 train_time:57829ms step_avg:57.89ms
step:1000/2330 train_time:57890ms step_avg:57.89ms
step:1000/2330 val_loss:4.0662 train_time:57970ms step_avg:57.97ms
step:1001/2330 train_time:57988ms step_avg:57.93ms
step:1002/2330 train_time:58007ms step_avg:57.89ms
step:1003/2330 train_time:58062ms step_avg:57.89ms
step:1004/2330 train_time:58132ms step_avg:57.90ms
step:1005/2330 train_time:58188ms step_avg:57.90ms
step:1006/2330 train_time:58252ms step_avg:57.90ms
step:1007/2330 train_time:58308ms step_avg:57.90ms
step:1008/2330 train_time:58368ms step_avg:57.91ms
step:1009/2330 train_time:58424ms step_avg:57.90ms
step:1010/2330 train_time:58484ms step_avg:57.91ms
step:1011/2330 train_time:58540ms step_avg:57.90ms
step:1012/2330 train_time:58599ms step_avg:57.90ms
step:1013/2330 train_time:58655ms step_avg:57.90ms
step:1014/2330 train_time:58714ms step_avg:57.90ms
step:1015/2330 train_time:58771ms step_avg:57.90ms
step:1016/2330 train_time:58830ms step_avg:57.90ms
step:1017/2330 train_time:58888ms step_avg:57.90ms
step:1018/2330 train_time:58950ms step_avg:57.91ms
step:1019/2330 train_time:59009ms step_avg:57.91ms
step:1020/2330 train_time:59070ms step_avg:57.91ms
step:1021/2330 train_time:59127ms step_avg:57.91ms
step:1022/2330 train_time:59190ms step_avg:57.92ms
step:1023/2330 train_time:59246ms step_avg:57.91ms
step:1024/2330 train_time:59309ms step_avg:57.92ms
step:1025/2330 train_time:59365ms step_avg:57.92ms
step:1026/2330 train_time:59425ms step_avg:57.92ms
step:1027/2330 train_time:59482ms step_avg:57.92ms
step:1028/2330 train_time:59542ms step_avg:57.92ms
step:1029/2330 train_time:59599ms step_avg:57.92ms
step:1030/2330 train_time:59658ms step_avg:57.92ms
step:1031/2330 train_time:59714ms step_avg:57.92ms
step:1032/2330 train_time:59773ms step_avg:57.92ms
step:1033/2330 train_time:59830ms step_avg:57.92ms
step:1034/2330 train_time:59890ms step_avg:57.92ms
step:1035/2330 train_time:59947ms step_avg:57.92ms
step:1036/2330 train_time:60008ms step_avg:57.92ms
step:1037/2330 train_time:60066ms step_avg:57.92ms
step:1038/2330 train_time:60127ms step_avg:57.93ms
step:1039/2330 train_time:60185ms step_avg:57.93ms
step:1040/2330 train_time:60245ms step_avg:57.93ms
step:1041/2330 train_time:60303ms step_avg:57.93ms
step:1042/2330 train_time:60362ms step_avg:57.93ms
step:1043/2330 train_time:60419ms step_avg:57.93ms
step:1044/2330 train_time:60478ms step_avg:57.93ms
step:1045/2330 train_time:60535ms step_avg:57.93ms
step:1046/2330 train_time:60595ms step_avg:57.93ms
step:1047/2330 train_time:60651ms step_avg:57.93ms
step:1048/2330 train_time:60711ms step_avg:57.93ms
step:1049/2330 train_time:60767ms step_avg:57.93ms
step:1050/2330 train_time:60827ms step_avg:57.93ms
step:1051/2330 train_time:60883ms step_avg:57.93ms
step:1052/2330 train_time:60944ms step_avg:57.93ms
step:1053/2330 train_time:61002ms step_avg:57.93ms
step:1054/2330 train_time:61062ms step_avg:57.93ms
step:1055/2330 train_time:61119ms step_avg:57.93ms
step:1056/2330 train_time:61180ms step_avg:57.94ms
step:1057/2330 train_time:61237ms step_avg:57.93ms
step:1058/2330 train_time:61296ms step_avg:57.94ms
step:1059/2330 train_time:61352ms step_avg:57.93ms
step:1060/2330 train_time:61414ms step_avg:57.94ms
step:1061/2330 train_time:61470ms step_avg:57.94ms
step:1062/2330 train_time:61532ms step_avg:57.94ms
step:1063/2330 train_time:61588ms step_avg:57.94ms
step:1064/2330 train_time:61648ms step_avg:57.94ms
step:1065/2330 train_time:61704ms step_avg:57.94ms
step:1066/2330 train_time:61764ms step_avg:57.94ms
step:1067/2330 train_time:61820ms step_avg:57.94ms
step:1068/2330 train_time:61881ms step_avg:57.94ms
step:1069/2330 train_time:61937ms step_avg:57.94ms
step:1070/2330 train_time:61997ms step_avg:57.94ms
step:1071/2330 train_time:62054ms step_avg:57.94ms
step:1072/2330 train_time:62114ms step_avg:57.94ms
step:1073/2330 train_time:62170ms step_avg:57.94ms
step:1074/2330 train_time:62233ms step_avg:57.94ms
step:1075/2330 train_time:62289ms step_avg:57.94ms
step:1076/2330 train_time:62350ms step_avg:57.95ms
step:1077/2330 train_time:62407ms step_avg:57.94ms
step:1078/2330 train_time:62467ms step_avg:57.95ms
step:1079/2330 train_time:62523ms step_avg:57.95ms
step:1080/2330 train_time:62584ms step_avg:57.95ms
step:1081/2330 train_time:62640ms step_avg:57.95ms
step:1082/2330 train_time:62699ms step_avg:57.95ms
step:1083/2330 train_time:62756ms step_avg:57.95ms
step:1084/2330 train_time:62815ms step_avg:57.95ms
step:1085/2330 train_time:62872ms step_avg:57.95ms
step:1086/2330 train_time:62933ms step_avg:57.95ms
step:1087/2330 train_time:62991ms step_avg:57.95ms
step:1088/2330 train_time:63051ms step_avg:57.95ms
step:1089/2330 train_time:63108ms step_avg:57.95ms
step:1090/2330 train_time:63169ms step_avg:57.95ms
step:1091/2330 train_time:63225ms step_avg:57.95ms
step:1092/2330 train_time:63288ms step_avg:57.96ms
step:1093/2330 train_time:63346ms step_avg:57.96ms
step:1094/2330 train_time:63406ms step_avg:57.96ms
step:1095/2330 train_time:63463ms step_avg:57.96ms
step:1096/2330 train_time:63524ms step_avg:57.96ms
step:1097/2330 train_time:63580ms step_avg:57.96ms
step:1098/2330 train_time:63640ms step_avg:57.96ms
step:1099/2330 train_time:63697ms step_avg:57.96ms
step:1100/2330 train_time:63757ms step_avg:57.96ms
step:1101/2330 train_time:63813ms step_avg:57.96ms
step:1102/2330 train_time:63872ms step_avg:57.96ms
step:1103/2330 train_time:63930ms step_avg:57.96ms
step:1104/2330 train_time:63990ms step_avg:57.96ms
step:1105/2330 train_time:64047ms step_avg:57.96ms
step:1106/2330 train_time:64107ms step_avg:57.96ms
step:1107/2330 train_time:64163ms step_avg:57.96ms
step:1108/2330 train_time:64224ms step_avg:57.96ms
step:1109/2330 train_time:64281ms step_avg:57.96ms
step:1110/2330 train_time:64341ms step_avg:57.96ms
step:1111/2330 train_time:64399ms step_avg:57.96ms
step:1112/2330 train_time:64459ms step_avg:57.97ms
step:1113/2330 train_time:64515ms step_avg:57.97ms
step:1114/2330 train_time:64576ms step_avg:57.97ms
step:1115/2330 train_time:64632ms step_avg:57.97ms
step:1116/2330 train_time:64692ms step_avg:57.97ms
step:1117/2330 train_time:64750ms step_avg:57.97ms
step:1118/2330 train_time:64810ms step_avg:57.97ms
step:1119/2330 train_time:64866ms step_avg:57.97ms
step:1120/2330 train_time:64927ms step_avg:57.97ms
step:1121/2330 train_time:64984ms step_avg:57.97ms
step:1122/2330 train_time:65044ms step_avg:57.97ms
step:1123/2330 train_time:65101ms step_avg:57.97ms
step:1124/2330 train_time:65161ms step_avg:57.97ms
step:1125/2330 train_time:65219ms step_avg:57.97ms
step:1126/2330 train_time:65278ms step_avg:57.97ms
step:1127/2330 train_time:65336ms step_avg:57.97ms
step:1128/2330 train_time:65395ms step_avg:57.97ms
step:1129/2330 train_time:65453ms step_avg:57.97ms
step:1130/2330 train_time:65512ms step_avg:57.98ms
step:1131/2330 train_time:65569ms step_avg:57.97ms
step:1132/2330 train_time:65630ms step_avg:57.98ms
step:1133/2330 train_time:65686ms step_avg:57.97ms
step:1134/2330 train_time:65748ms step_avg:57.98ms
step:1135/2330 train_time:65805ms step_avg:57.98ms
step:1136/2330 train_time:65865ms step_avg:57.98ms
step:1137/2330 train_time:65922ms step_avg:57.98ms
step:1138/2330 train_time:65981ms step_avg:57.98ms
step:1139/2330 train_time:66038ms step_avg:57.98ms
step:1140/2330 train_time:66098ms step_avg:57.98ms
step:1141/2330 train_time:66156ms step_avg:57.98ms
step:1142/2330 train_time:66215ms step_avg:57.98ms
step:1143/2330 train_time:66271ms step_avg:57.98ms
step:1144/2330 train_time:66332ms step_avg:57.98ms
step:1145/2330 train_time:66389ms step_avg:57.98ms
step:1146/2330 train_time:66881ms step_avg:58.36ms
step:1147/2330 train_time:66936ms step_avg:58.36ms
step:1148/2330 train_time:66996ms step_avg:58.36ms
step:1149/2330 train_time:67052ms step_avg:58.36ms
step:1150/2330 train_time:67111ms step_avg:58.36ms
step:1151/2330 train_time:67167ms step_avg:58.36ms
step:1152/2330 train_time:67227ms step_avg:58.36ms
step:1153/2330 train_time:67283ms step_avg:58.35ms
step:1154/2330 train_time:67342ms step_avg:58.36ms
step:1155/2330 train_time:67398ms step_avg:58.35ms
step:1156/2330 train_time:67457ms step_avg:58.35ms
step:1157/2330 train_time:67513ms step_avg:58.35ms
step:1158/2330 train_time:67572ms step_avg:58.35ms
step:1159/2330 train_time:67628ms step_avg:58.35ms
step:1160/2330 train_time:67688ms step_avg:58.35ms
step:1161/2330 train_time:67750ms step_avg:58.35ms
step:1162/2330 train_time:67815ms step_avg:58.36ms
step:1163/2330 train_time:67873ms step_avg:58.36ms
step:1164/2330 train_time:67936ms step_avg:58.36ms
step:1165/2330 train_time:67992ms step_avg:58.36ms
step:1166/2330 train_time:68052ms step_avg:58.36ms
step:1167/2330 train_time:68108ms step_avg:58.36ms
step:1168/2330 train_time:68168ms step_avg:58.36ms
step:1169/2330 train_time:68224ms step_avg:58.36ms
step:1170/2330 train_time:68284ms step_avg:58.36ms
step:1171/2330 train_time:68341ms step_avg:58.36ms
step:1172/2330 train_time:68400ms step_avg:58.36ms
step:1173/2330 train_time:68457ms step_avg:58.36ms
step:1174/2330 train_time:68516ms step_avg:58.36ms
step:1175/2330 train_time:68572ms step_avg:58.36ms
step:1176/2330 train_time:68631ms step_avg:58.36ms
step:1177/2330 train_time:68689ms step_avg:58.36ms
step:1178/2330 train_time:68750ms step_avg:58.36ms
step:1179/2330 train_time:68809ms step_avg:58.36ms
step:1180/2330 train_time:68871ms step_avg:58.37ms
step:1181/2330 train_time:68928ms step_avg:58.36ms
step:1182/2330 train_time:68989ms step_avg:58.37ms
step:1183/2330 train_time:69046ms step_avg:58.36ms
step:1184/2330 train_time:69106ms step_avg:58.37ms
step:1185/2330 train_time:69163ms step_avg:58.37ms
step:1186/2330 train_time:69222ms step_avg:58.37ms
step:1187/2330 train_time:69279ms step_avg:58.36ms
step:1188/2330 train_time:69338ms step_avg:58.37ms
step:1189/2330 train_time:69394ms step_avg:58.36ms
step:1190/2330 train_time:69454ms step_avg:58.36ms
step:1191/2330 train_time:69510ms step_avg:58.36ms
step:1192/2330 train_time:69570ms step_avg:58.36ms
step:1193/2330 train_time:69627ms step_avg:58.36ms
step:1194/2330 train_time:69688ms step_avg:58.36ms
step:1195/2330 train_time:69745ms step_avg:58.36ms
step:1196/2330 train_time:69807ms step_avg:58.37ms
step:1197/2330 train_time:69865ms step_avg:58.37ms
step:1198/2330 train_time:69925ms step_avg:58.37ms
step:1199/2330 train_time:69983ms step_avg:58.37ms
step:1200/2330 train_time:70043ms step_avg:58.37ms
step:1201/2330 train_time:70101ms step_avg:58.37ms
step:1202/2330 train_time:70161ms step_avg:58.37ms
step:1203/2330 train_time:70218ms step_avg:58.37ms
step:1204/2330 train_time:70278ms step_avg:58.37ms
step:1205/2330 train_time:70334ms step_avg:58.37ms
step:1206/2330 train_time:70393ms step_avg:58.37ms
step:1207/2330 train_time:70450ms step_avg:58.37ms
step:1208/2330 train_time:70510ms step_avg:58.37ms
step:1209/2330 train_time:70566ms step_avg:58.37ms
step:1210/2330 train_time:70626ms step_avg:58.37ms
step:1211/2330 train_time:70682ms step_avg:58.37ms
step:1212/2330 train_time:70743ms step_avg:58.37ms
step:1213/2330 train_time:70800ms step_avg:58.37ms
step:1214/2330 train_time:70861ms step_avg:58.37ms
step:1215/2330 train_time:70918ms step_avg:58.37ms
step:1216/2330 train_time:70979ms step_avg:58.37ms
step:1217/2330 train_time:71036ms step_avg:58.37ms
step:1218/2330 train_time:71096ms step_avg:58.37ms
step:1219/2330 train_time:71153ms step_avg:58.37ms
step:1220/2330 train_time:71214ms step_avg:58.37ms
step:1221/2330 train_time:71270ms step_avg:58.37ms
step:1222/2330 train_time:71330ms step_avg:58.37ms
step:1223/2330 train_time:71387ms step_avg:58.37ms
step:1224/2330 train_time:71446ms step_avg:58.37ms
step:1225/2330 train_time:71503ms step_avg:58.37ms
step:1226/2330 train_time:71563ms step_avg:58.37ms
step:1227/2330 train_time:71620ms step_avg:58.37ms
step:1228/2330 train_time:71679ms step_avg:58.37ms
step:1229/2330 train_time:71737ms step_avg:58.37ms
step:1230/2330 train_time:71796ms step_avg:58.37ms
step:1231/2330 train_time:71854ms step_avg:58.37ms
step:1232/2330 train_time:71915ms step_avg:58.37ms
step:1233/2330 train_time:71971ms step_avg:58.37ms
step:1234/2330 train_time:72032ms step_avg:58.37ms
step:1235/2330 train_time:72089ms step_avg:58.37ms
step:1236/2330 train_time:72149ms step_avg:58.37ms
step:1237/2330 train_time:72205ms step_avg:58.37ms
step:1238/2330 train_time:72267ms step_avg:58.37ms
step:1239/2330 train_time:72323ms step_avg:58.37ms
step:1240/2330 train_time:72383ms step_avg:58.37ms
step:1241/2330 train_time:72440ms step_avg:58.37ms
step:1242/2330 train_time:72500ms step_avg:58.37ms
step:1243/2330 train_time:72558ms step_avg:58.37ms
step:1244/2330 train_time:72617ms step_avg:58.37ms
step:1245/2330 train_time:72674ms step_avg:58.37ms
step:1246/2330 train_time:72733ms step_avg:58.37ms
step:1247/2330 train_time:72791ms step_avg:58.37ms
step:1248/2330 train_time:72852ms step_avg:58.37ms
step:1249/2330 train_time:72908ms step_avg:58.37ms
step:1250/2330 train_time:72969ms step_avg:58.38ms
step:1250/2330 val_loss:3.9893 train_time:73051ms step_avg:58.44ms
step:1251/2330 train_time:73069ms step_avg:58.41ms
step:1252/2330 train_time:73091ms step_avg:58.38ms
step:1253/2330 train_time:73150ms step_avg:58.38ms
step:1254/2330 train_time:73216ms step_avg:58.39ms
step:1255/2330 train_time:73275ms step_avg:58.39ms
step:1256/2330 train_time:73334ms step_avg:58.39ms
step:1257/2330 train_time:73390ms step_avg:58.39ms
step:1258/2330 train_time:73452ms step_avg:58.39ms
step:1259/2330 train_time:73508ms step_avg:58.39ms
step:1260/2330 train_time:73568ms step_avg:58.39ms
step:1261/2330 train_time:73624ms step_avg:58.39ms
step:1262/2330 train_time:73683ms step_avg:58.39ms
step:1263/2330 train_time:73740ms step_avg:58.38ms
step:1264/2330 train_time:73798ms step_avg:58.38ms
step:1265/2330 train_time:73855ms step_avg:58.38ms
step:1266/2330 train_time:73914ms step_avg:58.38ms
step:1267/2330 train_time:73970ms step_avg:58.38ms
step:1268/2330 train_time:74032ms step_avg:58.38ms
step:1269/2330 train_time:74090ms step_avg:58.38ms
step:1270/2330 train_time:74154ms step_avg:58.39ms
step:1271/2330 train_time:74211ms step_avg:58.39ms
step:1272/2330 train_time:74273ms step_avg:58.39ms
step:1273/2330 train_time:74331ms step_avg:58.39ms
step:1274/2330 train_time:74392ms step_avg:58.39ms
step:1275/2330 train_time:74449ms step_avg:58.39ms
step:1276/2330 train_time:74509ms step_avg:58.39ms
step:1277/2330 train_time:74565ms step_avg:58.39ms
step:1278/2330 train_time:74624ms step_avg:58.39ms
step:1279/2330 train_time:74681ms step_avg:58.39ms
step:1280/2330 train_time:74740ms step_avg:58.39ms
step:1281/2330 train_time:74797ms step_avg:58.39ms
step:1282/2330 train_time:74856ms step_avg:58.39ms
step:1283/2330 train_time:74912ms step_avg:58.39ms
step:1284/2330 train_time:74973ms step_avg:58.39ms
step:1285/2330 train_time:75030ms step_avg:58.39ms
step:1286/2330 train_time:75090ms step_avg:58.39ms
step:1287/2330 train_time:75149ms step_avg:58.39ms
step:1288/2330 train_time:75210ms step_avg:58.39ms
step:1289/2330 train_time:75267ms step_avg:58.39ms
step:1290/2330 train_time:75328ms step_avg:58.39ms
step:1291/2330 train_time:75386ms step_avg:58.39ms
step:1292/2330 train_time:75447ms step_avg:58.40ms
step:1293/2330 train_time:75503ms step_avg:58.39ms
step:1294/2330 train_time:75922ms step_avg:58.67ms
step:1295/2330 train_time:75977ms step_avg:58.67ms
step:1296/2330 train_time:76036ms step_avg:58.67ms
step:1297/2330 train_time:76092ms step_avg:58.67ms
step:1298/2330 train_time:76151ms step_avg:58.67ms
step:1299/2330 train_time:76207ms step_avg:58.67ms
step:1300/2330 train_time:76266ms step_avg:58.67ms
step:1301/2330 train_time:76322ms step_avg:58.66ms
step:1302/2330 train_time:76381ms step_avg:58.66ms
step:1303/2330 train_time:76437ms step_avg:58.66ms
step:1304/2330 train_time:76496ms step_avg:58.66ms
step:1305/2330 train_time:76552ms step_avg:58.66ms
step:1306/2330 train_time:76612ms step_avg:58.66ms
step:1307/2330 train_time:76668ms step_avg:58.66ms
step:1308/2330 train_time:76727ms step_avg:58.66ms
step:1309/2330 train_time:76789ms step_avg:58.66ms
step:1310/2330 train_time:76854ms step_avg:58.67ms
step:1311/2330 train_time:76912ms step_avg:58.67ms
step:1312/2330 train_time:76973ms step_avg:58.67ms
step:1313/2330 train_time:77031ms step_avg:58.67ms
step:1314/2330 train_time:77090ms step_avg:58.67ms
step:1315/2330 train_time:77146ms step_avg:58.67ms
step:1316/2330 train_time:77207ms step_avg:58.67ms
step:1317/2330 train_time:77263ms step_avg:58.67ms
step:1318/2330 train_time:77322ms step_avg:58.67ms
step:1319/2330 train_time:77378ms step_avg:58.66ms
step:1320/2330 train_time:77437ms step_avg:58.66ms
step:1321/2330 train_time:77493ms step_avg:58.66ms
step:1322/2330 train_time:77553ms step_avg:58.66ms
step:1323/2330 train_time:77610ms step_avg:58.66ms
step:1324/2330 train_time:77669ms step_avg:58.66ms
step:1325/2330 train_time:77727ms step_avg:58.66ms
step:1326/2330 train_time:77788ms step_avg:58.66ms
step:1327/2330 train_time:77847ms step_avg:58.66ms
step:1328/2330 train_time:77908ms step_avg:58.67ms
step:1329/2330 train_time:77966ms step_avg:58.67ms
step:1330/2330 train_time:78027ms step_avg:58.67ms
step:1331/2330 train_time:78084ms step_avg:58.67ms
step:1332/2330 train_time:78144ms step_avg:58.67ms
step:1333/2330 train_time:78200ms step_avg:58.66ms
step:1334/2330 train_time:78260ms step_avg:58.67ms
step:1335/2330 train_time:78316ms step_avg:58.66ms
step:1336/2330 train_time:78376ms step_avg:58.66ms
step:1337/2330 train_time:78432ms step_avg:58.66ms
step:1338/2330 train_time:78492ms step_avg:58.66ms
step:1339/2330 train_time:78548ms step_avg:58.66ms
step:1340/2330 train_time:78608ms step_avg:58.66ms
step:1341/2330 train_time:78665ms step_avg:58.66ms
step:1342/2330 train_time:78726ms step_avg:58.66ms
step:1343/2330 train_time:78784ms step_avg:58.66ms
step:1344/2330 train_time:78845ms step_avg:58.66ms
step:1345/2330 train_time:78902ms step_avg:58.66ms
step:1346/2330 train_time:78964ms step_avg:58.67ms
step:1347/2330 train_time:79020ms step_avg:58.66ms
step:1348/2330 train_time:79082ms step_avg:58.67ms
step:1349/2330 train_time:79138ms step_avg:58.66ms
step:1350/2330 train_time:79200ms step_avg:58.67ms
step:1351/2330 train_time:79256ms step_avg:58.66ms
step:1352/2330 train_time:79315ms step_avg:58.67ms
step:1353/2330 train_time:79371ms step_avg:58.66ms
step:1354/2330 train_time:79432ms step_avg:58.66ms
step:1355/2330 train_time:79487ms step_avg:58.66ms
step:1356/2330 train_time:79548ms step_avg:58.66ms
step:1357/2330 train_time:79604ms step_avg:58.66ms
step:1358/2330 train_time:79664ms step_avg:58.66ms
step:1359/2330 train_time:79722ms step_avg:58.66ms
step:1360/2330 train_time:79782ms step_avg:58.66ms
step:1361/2330 train_time:79838ms step_avg:58.66ms
step:1362/2330 train_time:79900ms step_avg:58.66ms
step:1363/2330 train_time:79957ms step_avg:58.66ms
step:1364/2330 train_time:80019ms step_avg:58.66ms
step:1365/2330 train_time:80076ms step_avg:58.66ms
step:1366/2330 train_time:80136ms step_avg:58.66ms
step:1367/2330 train_time:80193ms step_avg:58.66ms
step:1368/2330 train_time:80254ms step_avg:58.66ms
step:1369/2330 train_time:80311ms step_avg:58.66ms
step:1370/2330 train_time:80371ms step_avg:58.66ms
step:1371/2330 train_time:80427ms step_avg:58.66ms
step:1372/2330 train_time:80486ms step_avg:58.66ms
step:1373/2330 train_time:80543ms step_avg:58.66ms
step:1374/2330 train_time:80603ms step_avg:58.66ms
step:1375/2330 train_time:80660ms step_avg:58.66ms
step:1376/2330 train_time:80720ms step_avg:58.66ms
step:1377/2330 train_time:80777ms step_avg:58.66ms
step:1378/2330 train_time:80838ms step_avg:58.66ms
step:1379/2330 train_time:80894ms step_avg:58.66ms
step:1380/2330 train_time:80957ms step_avg:58.66ms
step:1381/2330 train_time:81014ms step_avg:58.66ms
step:1382/2330 train_time:81076ms step_avg:58.67ms
step:1383/2330 train_time:81132ms step_avg:58.66ms
step:1384/2330 train_time:81193ms step_avg:58.67ms
step:1385/2330 train_time:81250ms step_avg:58.66ms
step:1386/2330 train_time:81310ms step_avg:58.67ms
step:1387/2330 train_time:81367ms step_avg:58.66ms
step:1388/2330 train_time:81426ms step_avg:58.66ms
step:1389/2330 train_time:81483ms step_avg:58.66ms
step:1390/2330 train_time:81542ms step_avg:58.66ms
step:1391/2330 train_time:81598ms step_avg:58.66ms
step:1392/2330 train_time:81658ms step_avg:58.66ms
step:1393/2330 train_time:81715ms step_avg:58.66ms
step:1394/2330 train_time:81777ms step_avg:58.66ms
step:1395/2330 train_time:81834ms step_avg:58.66ms
step:1396/2330 train_time:81895ms step_avg:58.66ms
step:1397/2330 train_time:81952ms step_avg:58.66ms
step:1398/2330 train_time:82014ms step_avg:58.67ms
step:1399/2330 train_time:82071ms step_avg:58.66ms
step:1400/2330 train_time:82132ms step_avg:58.67ms
step:1401/2330 train_time:82189ms step_avg:58.66ms
step:1402/2330 train_time:82249ms step_avg:58.67ms
step:1403/2330 train_time:82305ms step_avg:58.66ms
step:1404/2330 train_time:82365ms step_avg:58.66ms
step:1405/2330 train_time:82422ms step_avg:58.66ms
step:1406/2330 train_time:82482ms step_avg:58.66ms
step:1407/2330 train_time:82538ms step_avg:58.66ms
step:1408/2330 train_time:82598ms step_avg:58.66ms
step:1409/2330 train_time:82655ms step_avg:58.66ms
step:1410/2330 train_time:82717ms step_avg:58.66ms
step:1411/2330 train_time:82774ms step_avg:58.66ms
step:1412/2330 train_time:82834ms step_avg:58.66ms
step:1413/2330 train_time:82891ms step_avg:58.66ms
step:1414/2330 train_time:82952ms step_avg:58.66ms
step:1415/2330 train_time:83009ms step_avg:58.66ms
step:1416/2330 train_time:83069ms step_avg:58.66ms
step:1417/2330 train_time:83126ms step_avg:58.66ms
step:1418/2330 train_time:83186ms step_avg:58.66ms
step:1419/2330 train_time:83242ms step_avg:58.66ms
step:1420/2330 train_time:83302ms step_avg:58.66ms
step:1421/2330 train_time:83358ms step_avg:58.66ms
step:1422/2330 train_time:83418ms step_avg:58.66ms
step:1423/2330 train_time:83475ms step_avg:58.66ms
step:1424/2330 train_time:83535ms step_avg:58.66ms
step:1425/2330 train_time:83592ms step_avg:58.66ms
step:1426/2330 train_time:83653ms step_avg:58.66ms
step:1427/2330 train_time:83710ms step_avg:58.66ms
step:1428/2330 train_time:83770ms step_avg:58.66ms
step:1429/2330 train_time:83827ms step_avg:58.66ms
step:1430/2330 train_time:83888ms step_avg:58.66ms
step:1431/2330 train_time:83945ms step_avg:58.66ms
step:1432/2330 train_time:84005ms step_avg:58.66ms
step:1433/2330 train_time:84062ms step_avg:58.66ms
step:1434/2330 train_time:84122ms step_avg:58.66ms
step:1435/2330 train_time:84178ms step_avg:58.66ms
step:1436/2330 train_time:84240ms step_avg:58.66ms
step:1437/2330 train_time:84296ms step_avg:58.66ms
step:1438/2330 train_time:84357ms step_avg:58.66ms
step:1439/2330 train_time:84413ms step_avg:58.66ms
step:1440/2330 train_time:84473ms step_avg:58.66ms
step:1441/2330 train_time:84530ms step_avg:58.66ms
step:1442/2330 train_time:84590ms step_avg:58.66ms
step:1443/2330 train_time:84648ms step_avg:58.66ms
step:1444/2330 train_time:84708ms step_avg:58.66ms
step:1445/2330 train_time:84764ms step_avg:58.66ms
step:1446/2330 train_time:84825ms step_avg:58.66ms
step:1447/2330 train_time:84882ms step_avg:58.66ms
step:1448/2330 train_time:84942ms step_avg:58.66ms
step:1449/2330 train_time:84999ms step_avg:58.66ms
step:1450/2330 train_time:85059ms step_avg:58.66ms
step:1451/2330 train_time:85117ms step_avg:58.66ms
step:1452/2330 train_time:85177ms step_avg:58.66ms
step:1453/2330 train_time:85234ms step_avg:58.66ms
step:1454/2330 train_time:85294ms step_avg:58.66ms
step:1455/2330 train_time:85350ms step_avg:58.66ms
step:1456/2330 train_time:85411ms step_avg:58.66ms
step:1457/2330 train_time:85468ms step_avg:58.66ms
step:1458/2330 train_time:85528ms step_avg:58.66ms
step:1459/2330 train_time:85584ms step_avg:58.66ms
step:1460/2330 train_time:85645ms step_avg:58.66ms
step:1461/2330 train_time:85702ms step_avg:58.66ms
step:1462/2330 train_time:85762ms step_avg:58.66ms
step:1463/2330 train_time:85819ms step_avg:58.66ms
step:1464/2330 train_time:85879ms step_avg:58.66ms
step:1465/2330 train_time:85936ms step_avg:58.66ms
step:1466/2330 train_time:85997ms step_avg:58.66ms
step:1467/2330 train_time:86054ms step_avg:58.66ms
step:1468/2330 train_time:86114ms step_avg:58.66ms
step:1469/2330 train_time:86171ms step_avg:58.66ms
step:1470/2330 train_time:86231ms step_avg:58.66ms
step:1471/2330 train_time:86288ms step_avg:58.66ms
step:1472/2330 train_time:86348ms step_avg:58.66ms
step:1473/2330 train_time:86404ms step_avg:58.66ms
step:1474/2330 train_time:86465ms step_avg:58.66ms
step:1475/2330 train_time:86520ms step_avg:58.66ms
step:1476/2330 train_time:86582ms step_avg:58.66ms
step:1477/2330 train_time:86638ms step_avg:58.66ms
step:1478/2330 train_time:86697ms step_avg:58.66ms
step:1479/2330 train_time:86754ms step_avg:58.66ms
step:1480/2330 train_time:86816ms step_avg:58.66ms
step:1481/2330 train_time:86873ms step_avg:58.66ms
step:1482/2330 train_time:86933ms step_avg:58.66ms
step:1483/2330 train_time:86990ms step_avg:58.66ms
step:1484/2330 train_time:87051ms step_avg:58.66ms
step:1485/2330 train_time:87108ms step_avg:58.66ms
step:1486/2330 train_time:87168ms step_avg:58.66ms
step:1487/2330 train_time:87224ms step_avg:58.66ms
step:1488/2330 train_time:87284ms step_avg:58.66ms
step:1489/2330 train_time:87341ms step_avg:58.66ms
step:1490/2330 train_time:87401ms step_avg:58.66ms
step:1491/2330 train_time:87458ms step_avg:58.66ms
step:1492/2330 train_time:87518ms step_avg:58.66ms
step:1493/2330 train_time:87574ms step_avg:58.66ms
step:1494/2330 train_time:87634ms step_avg:58.66ms
step:1495/2330 train_time:87690ms step_avg:58.66ms
step:1496/2330 train_time:87753ms step_avg:58.66ms
step:1497/2330 train_time:87810ms step_avg:58.66ms
step:1498/2330 train_time:87871ms step_avg:58.66ms
step:1499/2330 train_time:87929ms step_avg:58.66ms
step:1500/2330 train_time:87989ms step_avg:58.66ms
step:1500/2330 val_loss:3.9077 train_time:88069ms step_avg:58.71ms
step:1501/2330 train_time:88088ms step_avg:58.69ms
step:1502/2330 train_time:88108ms step_avg:58.66ms
step:1503/2330 train_time:88168ms step_avg:58.66ms
step:1504/2330 train_time:88233ms step_avg:58.67ms
step:1505/2330 train_time:88290ms step_avg:58.66ms
step:1506/2330 train_time:88351ms step_avg:58.67ms
step:1507/2330 train_time:88407ms step_avg:58.66ms
step:1508/2330 train_time:88467ms step_avg:58.67ms
step:1509/2330 train_time:88523ms step_avg:58.66ms
step:1510/2330 train_time:88584ms step_avg:58.66ms
step:1511/2330 train_time:88640ms step_avg:58.66ms
step:1512/2330 train_time:88700ms step_avg:58.66ms
step:1513/2330 train_time:88755ms step_avg:58.66ms
step:1514/2330 train_time:88814ms step_avg:58.66ms
step:1515/2330 train_time:88870ms step_avg:58.66ms
step:1516/2330 train_time:88930ms step_avg:58.66ms
step:1517/2330 train_time:88987ms step_avg:58.66ms
step:1518/2330 train_time:89048ms step_avg:58.66ms
step:1519/2330 train_time:89107ms step_avg:58.66ms
step:1520/2330 train_time:89168ms step_avg:58.66ms
step:1521/2330 train_time:89226ms step_avg:58.66ms
step:1522/2330 train_time:89288ms step_avg:58.66ms
step:1523/2330 train_time:89345ms step_avg:58.66ms
step:1524/2330 train_time:89405ms step_avg:58.66ms
step:1525/2330 train_time:89462ms step_avg:58.66ms
step:1526/2330 train_time:89522ms step_avg:58.66ms
step:1527/2330 train_time:89578ms step_avg:58.66ms
step:1528/2330 train_time:89638ms step_avg:58.66ms
step:1529/2330 train_time:89695ms step_avg:58.66ms
step:1530/2330 train_time:89753ms step_avg:58.66ms
step:1531/2330 train_time:89809ms step_avg:58.66ms
step:1532/2330 train_time:89870ms step_avg:58.66ms
step:1533/2330 train_time:89927ms step_avg:58.66ms
step:1534/2330 train_time:89988ms step_avg:58.66ms
step:1535/2330 train_time:90046ms step_avg:58.66ms
step:1536/2330 train_time:90107ms step_avg:58.66ms
step:1537/2330 train_time:90165ms step_avg:58.66ms
step:1538/2330 train_time:90227ms step_avg:58.67ms
step:1539/2330 train_time:90286ms step_avg:58.67ms
step:1540/2330 train_time:90347ms step_avg:58.67ms
step:1541/2330 train_time:90405ms step_avg:58.67ms
step:1542/2330 train_time:90465ms step_avg:58.67ms
step:1543/2330 train_time:90522ms step_avg:58.67ms
step:1544/2330 train_time:90583ms step_avg:58.67ms
step:1545/2330 train_time:90640ms step_avg:58.67ms
step:1546/2330 train_time:90701ms step_avg:58.67ms
step:1547/2330 train_time:90758ms step_avg:58.67ms
step:1548/2330 train_time:90819ms step_avg:58.67ms
step:1549/2330 train_time:90876ms step_avg:58.67ms
step:1550/2330 train_time:90937ms step_avg:58.67ms
step:1551/2330 train_time:90994ms step_avg:58.67ms
step:1552/2330 train_time:91054ms step_avg:58.67ms
step:1553/2330 train_time:91110ms step_avg:58.67ms
step:1554/2330 train_time:91174ms step_avg:58.67ms
step:1555/2330 train_time:91230ms step_avg:58.67ms
step:1556/2330 train_time:91294ms step_avg:58.67ms
step:1557/2330 train_time:91351ms step_avg:58.67ms
step:1558/2330 train_time:91414ms step_avg:58.67ms
step:1559/2330 train_time:91470ms step_avg:58.67ms
step:1560/2330 train_time:91533ms step_avg:58.67ms
step:1561/2330 train_time:91589ms step_avg:58.67ms
step:1562/2330 train_time:91651ms step_avg:58.68ms
step:1563/2330 train_time:91708ms step_avg:58.67ms
step:1564/2330 train_time:91768ms step_avg:58.68ms
step:1565/2330 train_time:91825ms step_avg:58.67ms
step:1566/2330 train_time:91887ms step_avg:58.68ms
step:1567/2330 train_time:91944ms step_avg:58.68ms
step:1568/2330 train_time:92004ms step_avg:58.68ms
step:1569/2330 train_time:92061ms step_avg:58.67ms
step:1570/2330 train_time:92123ms step_avg:58.68ms
step:1571/2330 train_time:92181ms step_avg:58.68ms
step:1572/2330 train_time:92242ms step_avg:58.68ms
step:1573/2330 train_time:92301ms step_avg:58.68ms
step:1574/2330 train_time:92361ms step_avg:58.68ms
step:1575/2330 train_time:92419ms step_avg:58.68ms
step:1576/2330 train_time:92480ms step_avg:58.68ms
step:1577/2330 train_time:92536ms step_avg:58.68ms
step:1578/2330 train_time:92598ms step_avg:58.68ms
step:1579/2330 train_time:92654ms step_avg:58.68ms
step:1580/2330 train_time:92715ms step_avg:58.68ms
step:1581/2330 train_time:92772ms step_avg:58.68ms
step:1582/2330 train_time:92834ms step_avg:58.68ms
step:1583/2330 train_time:92891ms step_avg:58.68ms
step:1584/2330 train_time:92953ms step_avg:58.68ms
step:1585/2330 train_time:93009ms step_avg:58.68ms
step:1586/2330 train_time:93073ms step_avg:58.68ms
step:1587/2330 train_time:93129ms step_avg:58.68ms
step:1588/2330 train_time:93192ms step_avg:58.68ms
step:1589/2330 train_time:93249ms step_avg:58.68ms
step:1590/2330 train_time:93313ms step_avg:58.69ms
step:1591/2330 train_time:93370ms step_avg:58.69ms
step:1592/2330 train_time:93433ms step_avg:58.69ms
step:1593/2330 train_time:93490ms step_avg:58.69ms
step:1594/2330 train_time:93552ms step_avg:58.69ms
step:1595/2330 train_time:93610ms step_avg:58.69ms
step:1596/2330 train_time:93670ms step_avg:58.69ms
step:1597/2330 train_time:93727ms step_avg:58.69ms
step:1598/2330 train_time:93788ms step_avg:58.69ms
step:1599/2330 train_time:93844ms step_avg:58.69ms
step:1600/2330 train_time:93905ms step_avg:58.69ms
step:1601/2330 train_time:93962ms step_avg:58.69ms
step:1602/2330 train_time:94023ms step_avg:58.69ms
step:1603/2330 train_time:94079ms step_avg:58.69ms
step:1604/2330 train_time:94140ms step_avg:58.69ms
step:1605/2330 train_time:94197ms step_avg:58.69ms
step:1606/2330 train_time:94258ms step_avg:58.69ms
step:1607/2330 train_time:94315ms step_avg:58.69ms
step:1608/2330 train_time:94377ms step_avg:58.69ms
step:1609/2330 train_time:94434ms step_avg:58.69ms
step:1610/2330 train_time:94496ms step_avg:58.69ms
step:1611/2330 train_time:94551ms step_avg:58.69ms
step:1612/2330 train_time:94614ms step_avg:58.69ms
step:1613/2330 train_time:94670ms step_avg:58.69ms
step:1614/2330 train_time:94732ms step_avg:58.69ms
step:1615/2330 train_time:94789ms step_avg:58.69ms
step:1616/2330 train_time:94852ms step_avg:58.70ms
step:1617/2330 train_time:94908ms step_avg:58.69ms
step:1618/2330 train_time:94971ms step_avg:58.70ms
step:1619/2330 train_time:95028ms step_avg:58.70ms
step:1620/2330 train_time:95090ms step_avg:58.70ms
step:1621/2330 train_time:95147ms step_avg:58.70ms
step:1622/2330 train_time:95208ms step_avg:58.70ms
step:1623/2330 train_time:95265ms step_avg:58.70ms
step:1624/2330 train_time:95327ms step_avg:58.70ms
step:1625/2330 train_time:95386ms step_avg:58.70ms
step:1626/2330 train_time:95446ms step_avg:58.70ms
step:1627/2330 train_time:95504ms step_avg:58.70ms
step:1628/2330 train_time:95564ms step_avg:58.70ms
step:1629/2330 train_time:95622ms step_avg:58.70ms
step:1630/2330 train_time:95682ms step_avg:58.70ms
step:1631/2330 train_time:95738ms step_avg:58.70ms
step:1632/2330 train_time:95800ms step_avg:58.70ms
step:1633/2330 train_time:95857ms step_avg:58.70ms
step:1634/2330 train_time:95917ms step_avg:58.70ms
step:1635/2330 train_time:95974ms step_avg:58.70ms
step:1636/2330 train_time:96036ms step_avg:58.70ms
step:1637/2330 train_time:96093ms step_avg:58.70ms
step:1638/2330 train_time:96155ms step_avg:58.70ms
step:1639/2330 train_time:96211ms step_avg:58.70ms
step:1640/2330 train_time:96274ms step_avg:58.70ms
step:1641/2330 train_time:96331ms step_avg:58.70ms
step:1642/2330 train_time:96393ms step_avg:58.70ms
step:1643/2330 train_time:96449ms step_avg:58.70ms
step:1644/2330 train_time:96512ms step_avg:58.71ms
step:1645/2330 train_time:96568ms step_avg:58.70ms
step:1646/2330 train_time:96631ms step_avg:58.71ms
step:1647/2330 train_time:96688ms step_avg:58.71ms
step:1648/2330 train_time:96750ms step_avg:58.71ms
step:1649/2330 train_time:96808ms step_avg:58.71ms
step:1650/2330 train_time:96868ms step_avg:58.71ms
step:1651/2330 train_time:96925ms step_avg:58.71ms
step:1652/2330 train_time:96985ms step_avg:58.71ms
step:1653/2330 train_time:97044ms step_avg:58.71ms
step:1654/2330 train_time:97104ms step_avg:58.71ms
step:1655/2330 train_time:97161ms step_avg:58.71ms
step:1656/2330 train_time:97222ms step_avg:58.71ms
step:1657/2330 train_time:97279ms step_avg:58.71ms
step:1658/2330 train_time:97341ms step_avg:58.71ms
step:1659/2330 train_time:97397ms step_avg:58.71ms
step:1660/2330 train_time:97459ms step_avg:58.71ms
step:1661/2330 train_time:97515ms step_avg:58.71ms
step:1662/2330 train_time:97577ms step_avg:58.71ms
step:1663/2330 train_time:97633ms step_avg:58.71ms
step:1664/2330 train_time:97695ms step_avg:58.71ms
step:1665/2330 train_time:97752ms step_avg:58.71ms
step:1666/2330 train_time:97814ms step_avg:58.71ms
step:1667/2330 train_time:97871ms step_avg:58.71ms
step:1668/2330 train_time:97933ms step_avg:58.71ms
step:1669/2330 train_time:97990ms step_avg:58.71ms
step:1670/2330 train_time:98053ms step_avg:58.71ms
step:1671/2330 train_time:98110ms step_avg:58.71ms
step:1672/2330 train_time:98172ms step_avg:58.72ms
step:1673/2330 train_time:98229ms step_avg:58.71ms
step:1674/2330 train_time:98290ms step_avg:58.72ms
step:1675/2330 train_time:98347ms step_avg:58.71ms
step:1676/2330 train_time:98409ms step_avg:58.72ms
step:1677/2330 train_time:98466ms step_avg:58.72ms
step:1678/2330 train_time:98528ms step_avg:58.72ms
step:1679/2330 train_time:98585ms step_avg:58.72ms
step:1680/2330 train_time:98646ms step_avg:58.72ms
step:1681/2330 train_time:98703ms step_avg:58.72ms
step:1682/2330 train_time:98765ms step_avg:58.72ms
step:1683/2330 train_time:98822ms step_avg:58.72ms
step:1684/2330 train_time:98883ms step_avg:58.72ms
step:1685/2330 train_time:98940ms step_avg:58.72ms
step:1686/2330 train_time:99000ms step_avg:58.72ms
step:1687/2330 train_time:99057ms step_avg:58.72ms
step:1688/2330 train_time:99118ms step_avg:58.72ms
step:1689/2330 train_time:99175ms step_avg:58.72ms
step:1690/2330 train_time:99237ms step_avg:58.72ms
step:1691/2330 train_time:99294ms step_avg:58.72ms
step:1692/2330 train_time:99356ms step_avg:58.72ms
step:1693/2330 train_time:99412ms step_avg:58.72ms
step:1694/2330 train_time:99474ms step_avg:58.72ms
step:1695/2330 train_time:99531ms step_avg:58.72ms
step:1696/2330 train_time:99594ms step_avg:58.72ms
step:1697/2330 train_time:99650ms step_avg:58.72ms
step:1698/2330 train_time:99714ms step_avg:58.72ms
step:1699/2330 train_time:99770ms step_avg:58.72ms
step:1700/2330 train_time:99832ms step_avg:58.72ms
step:1701/2330 train_time:99889ms step_avg:58.72ms
step:1702/2330 train_time:99951ms step_avg:58.73ms
step:1703/2330 train_time:100008ms step_avg:58.72ms
step:1704/2330 train_time:100069ms step_avg:58.73ms
step:1705/2330 train_time:100126ms step_avg:58.73ms
step:1706/2330 train_time:100187ms step_avg:58.73ms
step:1707/2330 train_time:100245ms step_avg:58.73ms
step:1708/2330 train_time:100307ms step_avg:58.73ms
step:1709/2330 train_time:100365ms step_avg:58.73ms
step:1710/2330 train_time:100426ms step_avg:58.73ms
step:1711/2330 train_time:100483ms step_avg:58.73ms
step:1712/2330 train_time:100544ms step_avg:58.73ms
step:1713/2330 train_time:100602ms step_avg:58.73ms
step:1714/2330 train_time:100662ms step_avg:58.73ms
step:1715/2330 train_time:100719ms step_avg:58.73ms
step:1716/2330 train_time:100781ms step_avg:58.73ms
step:1717/2330 train_time:100837ms step_avg:58.73ms
step:1718/2330 train_time:100898ms step_avg:58.73ms
step:1719/2330 train_time:100954ms step_avg:58.73ms
step:1720/2330 train_time:101015ms step_avg:58.73ms
step:1721/2330 train_time:101071ms step_avg:58.73ms
step:1722/2330 train_time:101134ms step_avg:58.73ms
step:1723/2330 train_time:101191ms step_avg:58.73ms
step:1724/2330 train_time:101253ms step_avg:58.73ms
step:1725/2330 train_time:101309ms step_avg:58.73ms
step:1726/2330 train_time:101373ms step_avg:58.73ms
step:1727/2330 train_time:101430ms step_avg:58.73ms
step:1728/2330 train_time:101492ms step_avg:58.73ms
step:1729/2330 train_time:101549ms step_avg:58.73ms
step:1730/2330 train_time:101611ms step_avg:58.73ms
step:1731/2330 train_time:101668ms step_avg:58.73ms
step:1732/2330 train_time:101729ms step_avg:58.73ms
step:1733/2330 train_time:101786ms step_avg:58.73ms
step:1734/2330 train_time:101847ms step_avg:58.74ms
step:1735/2330 train_time:101905ms step_avg:58.73ms
step:1736/2330 train_time:101965ms step_avg:58.74ms
step:1737/2330 train_time:102023ms step_avg:58.73ms
step:1738/2330 train_time:102083ms step_avg:58.74ms
step:1739/2330 train_time:102142ms step_avg:58.74ms
step:1740/2330 train_time:102202ms step_avg:58.74ms
step:1741/2330 train_time:102260ms step_avg:58.74ms
step:1742/2330 train_time:102320ms step_avg:58.74ms
step:1743/2330 train_time:102377ms step_avg:58.74ms
step:1744/2330 train_time:102438ms step_avg:58.74ms
step:1745/2330 train_time:102495ms step_avg:58.74ms
step:1746/2330 train_time:102556ms step_avg:58.74ms
step:1747/2330 train_time:102613ms step_avg:58.74ms
step:1748/2330 train_time:102674ms step_avg:58.74ms
step:1749/2330 train_time:102731ms step_avg:58.74ms
step:1750/2330 train_time:102792ms step_avg:58.74ms
step:1750/2330 val_loss:3.8233 train_time:102876ms step_avg:58.79ms
step:1751/2330 train_time:102894ms step_avg:58.76ms
step:1752/2330 train_time:102914ms step_avg:58.74ms
step:1753/2330 train_time:102972ms step_avg:58.74ms
step:1754/2330 train_time:103040ms step_avg:58.75ms
step:1755/2330 train_time:103096ms step_avg:58.74ms
step:1756/2330 train_time:103158ms step_avg:58.75ms
step:1757/2330 train_time:103215ms step_avg:58.74ms
step:1758/2330 train_time:103276ms step_avg:58.75ms
step:1759/2330 train_time:103332ms step_avg:58.74ms
step:1760/2330 train_time:103393ms step_avg:58.75ms
step:1761/2330 train_time:103450ms step_avg:58.74ms
step:1762/2330 train_time:103510ms step_avg:58.75ms
step:1763/2330 train_time:103567ms step_avg:58.74ms
step:1764/2330 train_time:103626ms step_avg:58.75ms
step:1765/2330 train_time:103683ms step_avg:58.74ms
step:1766/2330 train_time:103742ms step_avg:58.74ms
step:1767/2330 train_time:103799ms step_avg:58.74ms
step:1768/2330 train_time:103863ms step_avg:58.75ms
step:1769/2330 train_time:103921ms step_avg:58.75ms
step:1770/2330 train_time:103984ms step_avg:58.75ms
step:1771/2330 train_time:104043ms step_avg:58.75ms
step:1772/2330 train_time:104103ms step_avg:58.75ms
step:1773/2330 train_time:104160ms step_avg:58.75ms
step:1774/2330 train_time:104222ms step_avg:58.75ms
step:1775/2330 train_time:104278ms step_avg:58.75ms
step:1776/2330 train_time:104339ms step_avg:58.75ms
step:1777/2330 train_time:104396ms step_avg:58.75ms
step:1778/2330 train_time:104458ms step_avg:58.75ms
step:1779/2330 train_time:104515ms step_avg:58.75ms
step:1780/2330 train_time:104576ms step_avg:58.75ms
step:1781/2330 train_time:104632ms step_avg:58.75ms
step:1782/2330 train_time:104693ms step_avg:58.75ms
step:1783/2330 train_time:104749ms step_avg:58.75ms
step:1784/2330 train_time:104813ms step_avg:58.75ms
step:1785/2330 train_time:104871ms step_avg:58.75ms
step:1786/2330 train_time:104932ms step_avg:58.75ms
step:1787/2330 train_time:104991ms step_avg:58.75ms
step:1788/2330 train_time:105052ms step_avg:58.75ms
step:1789/2330 train_time:105110ms step_avg:58.75ms
step:1790/2330 train_time:105171ms step_avg:58.75ms
step:1791/2330 train_time:105229ms step_avg:58.75ms
step:1792/2330 train_time:105290ms step_avg:58.76ms
step:1793/2330 train_time:105347ms step_avg:58.75ms
step:1794/2330 train_time:105408ms step_avg:58.76ms
step:1795/2330 train_time:105464ms step_avg:58.75ms
step:1796/2330 train_time:105525ms step_avg:58.76ms
step:1797/2330 train_time:105582ms step_avg:58.75ms
step:1798/2330 train_time:105642ms step_avg:58.76ms
step:1799/2330 train_time:105699ms step_avg:58.75ms
step:1800/2330 train_time:105761ms step_avg:58.76ms
step:1801/2330 train_time:105817ms step_avg:58.75ms
step:1802/2330 train_time:105879ms step_avg:58.76ms
step:1803/2330 train_time:105936ms step_avg:58.76ms
step:1804/2330 train_time:105998ms step_avg:58.76ms
step:1805/2330 train_time:106056ms step_avg:58.76ms
step:1806/2330 train_time:106117ms step_avg:58.76ms
step:1807/2330 train_time:106174ms step_avg:58.76ms
step:1808/2330 train_time:106235ms step_avg:58.76ms
step:1809/2330 train_time:106293ms step_avg:58.76ms
step:1810/2330 train_time:106354ms step_avg:58.76ms
step:1811/2330 train_time:106413ms step_avg:58.76ms
step:1812/2330 train_time:106473ms step_avg:58.76ms
step:1813/2330 train_time:106530ms step_avg:58.76ms
step:1814/2330 train_time:106591ms step_avg:58.76ms
step:1815/2330 train_time:106649ms step_avg:58.76ms
step:1816/2330 train_time:106709ms step_avg:58.76ms
step:1817/2330 train_time:106767ms step_avg:58.76ms
step:1818/2330 train_time:106827ms step_avg:58.76ms
step:1819/2330 train_time:106884ms step_avg:58.76ms
step:1820/2330 train_time:106945ms step_avg:58.76ms
step:1821/2330 train_time:107002ms step_avg:58.76ms
step:1822/2330 train_time:107063ms step_avg:58.76ms
step:1823/2330 train_time:107120ms step_avg:58.76ms
step:1824/2330 train_time:107182ms step_avg:58.76ms
step:1825/2330 train_time:107238ms step_avg:58.76ms
step:1826/2330 train_time:107301ms step_avg:58.76ms
step:1827/2330 train_time:107357ms step_avg:58.76ms
step:1828/2330 train_time:107419ms step_avg:58.76ms
step:1829/2330 train_time:107476ms step_avg:58.76ms
step:1830/2330 train_time:107537ms step_avg:58.76ms
step:1831/2330 train_time:107594ms step_avg:58.76ms
step:1832/2330 train_time:107656ms step_avg:58.76ms
step:1833/2330 train_time:107714ms step_avg:58.76ms
step:1834/2330 train_time:107775ms step_avg:58.76ms
step:1835/2330 train_time:107833ms step_avg:58.76ms
step:1836/2330 train_time:107893ms step_avg:58.77ms
step:1837/2330 train_time:107951ms step_avg:58.77ms
step:1838/2330 train_time:108012ms step_avg:58.77ms
step:1839/2330 train_time:108070ms step_avg:58.77ms
step:1840/2330 train_time:108130ms step_avg:58.77ms
step:1841/2330 train_time:108188ms step_avg:58.77ms
step:1842/2330 train_time:108250ms step_avg:58.77ms
step:1843/2330 train_time:108307ms step_avg:58.77ms
step:1844/2330 train_time:108368ms step_avg:58.77ms
step:1845/2330 train_time:108426ms step_avg:58.77ms
step:1846/2330 train_time:108487ms step_avg:58.77ms
step:1847/2330 train_time:108543ms step_avg:58.77ms
step:1848/2330 train_time:108604ms step_avg:58.77ms
step:1849/2330 train_time:108661ms step_avg:58.77ms
step:1850/2330 train_time:108722ms step_avg:58.77ms
step:1851/2330 train_time:108779ms step_avg:58.77ms
step:1852/2330 train_time:108840ms step_avg:58.77ms
step:1853/2330 train_time:108897ms step_avg:58.77ms
step:1854/2330 train_time:108958ms step_avg:58.77ms
step:1855/2330 train_time:109015ms step_avg:58.77ms
step:1856/2330 train_time:109076ms step_avg:58.77ms
step:1857/2330 train_time:109134ms step_avg:58.77ms
step:1858/2330 train_time:109196ms step_avg:58.77ms
step:1859/2330 train_time:109253ms step_avg:58.77ms
step:1860/2330 train_time:109315ms step_avg:58.77ms
step:1861/2330 train_time:109373ms step_avg:58.77ms
step:1862/2330 train_time:109434ms step_avg:58.77ms
step:1863/2330 train_time:109492ms step_avg:58.77ms
step:1864/2330 train_time:109552ms step_avg:58.77ms
step:1865/2330 train_time:109610ms step_avg:58.77ms
step:1866/2330 train_time:109671ms step_avg:58.77ms
step:1867/2330 train_time:109729ms step_avg:58.77ms
step:1868/2330 train_time:109790ms step_avg:58.77ms
step:1869/2330 train_time:109847ms step_avg:58.77ms
step:1870/2330 train_time:109908ms step_avg:58.77ms
step:1871/2330 train_time:109965ms step_avg:58.77ms
step:1872/2330 train_time:110026ms step_avg:58.77ms
step:1873/2330 train_time:110083ms step_avg:58.77ms
step:1874/2330 train_time:110144ms step_avg:58.77ms
step:1875/2330 train_time:110202ms step_avg:58.77ms
step:1876/2330 train_time:110263ms step_avg:58.78ms
step:1877/2330 train_time:110319ms step_avg:58.77ms
step:1878/2330 train_time:110380ms step_avg:58.78ms
step:1879/2330 train_time:110437ms step_avg:58.77ms
step:1880/2330 train_time:110499ms step_avg:58.78ms
step:1881/2330 train_time:110556ms step_avg:58.78ms
step:1882/2330 train_time:110618ms step_avg:58.78ms
step:1883/2330 train_time:110675ms step_avg:58.78ms
step:1884/2330 train_time:110737ms step_avg:58.78ms
step:1885/2330 train_time:110794ms step_avg:58.78ms
step:1886/2330 train_time:110856ms step_avg:58.78ms
step:1887/2330 train_time:110913ms step_avg:58.78ms
step:1888/2330 train_time:110975ms step_avg:58.78ms
step:1889/2330 train_time:111033ms step_avg:58.78ms
step:1890/2330 train_time:111093ms step_avg:58.78ms
step:1891/2330 train_time:111151ms step_avg:58.78ms
step:1892/2330 train_time:111212ms step_avg:58.78ms
step:1893/2330 train_time:111270ms step_avg:58.78ms
step:1894/2330 train_time:111331ms step_avg:58.78ms
step:1895/2330 train_time:111388ms step_avg:58.78ms
step:1896/2330 train_time:111448ms step_avg:58.78ms
step:1897/2330 train_time:111506ms step_avg:58.78ms
step:1898/2330 train_time:111566ms step_avg:58.78ms
step:1899/2330 train_time:111624ms step_avg:58.78ms
step:1900/2330 train_time:111684ms step_avg:58.78ms
step:1901/2330 train_time:111741ms step_avg:58.78ms
step:1902/2330 train_time:111802ms step_avg:58.78ms
step:1903/2330 train_time:111859ms step_avg:58.78ms
step:1904/2330 train_time:111920ms step_avg:58.78ms
step:1905/2330 train_time:111977ms step_avg:58.78ms
step:1906/2330 train_time:112040ms step_avg:58.78ms
step:1907/2330 train_time:112096ms step_avg:58.78ms
step:1908/2330 train_time:112158ms step_avg:58.78ms
step:1909/2330 train_time:112215ms step_avg:58.78ms
step:1910/2330 train_time:112278ms step_avg:58.78ms
step:1911/2330 train_time:112334ms step_avg:58.78ms
step:1912/2330 train_time:112396ms step_avg:58.78ms
step:1913/2330 train_time:112453ms step_avg:58.78ms
step:1914/2330 train_time:112515ms step_avg:58.79ms
step:1915/2330 train_time:112573ms step_avg:58.78ms
step:1916/2330 train_time:112633ms step_avg:58.79ms
step:1917/2330 train_time:112691ms step_avg:58.79ms
step:1918/2330 train_time:112752ms step_avg:58.79ms
step:1919/2330 train_time:112809ms step_avg:58.79ms
step:1920/2330 train_time:112869ms step_avg:58.79ms
step:1921/2330 train_time:112928ms step_avg:58.79ms
step:1922/2330 train_time:112989ms step_avg:58.79ms
step:1923/2330 train_time:113046ms step_avg:58.79ms
step:1924/2330 train_time:113106ms step_avg:58.79ms
step:1925/2330 train_time:113163ms step_avg:58.79ms
step:1926/2330 train_time:113224ms step_avg:58.79ms
step:1927/2330 train_time:113280ms step_avg:58.79ms
step:1928/2330 train_time:113342ms step_avg:58.79ms
step:1929/2330 train_time:113399ms step_avg:58.79ms
step:1930/2330 train_time:113460ms step_avg:58.79ms
step:1931/2330 train_time:113518ms step_avg:58.79ms
step:1932/2330 train_time:113578ms step_avg:58.79ms
step:1933/2330 train_time:113635ms step_avg:58.79ms
step:1934/2330 train_time:113698ms step_avg:58.79ms
step:1935/2330 train_time:113755ms step_avg:58.79ms
step:1936/2330 train_time:113817ms step_avg:58.79ms
step:1937/2330 train_time:113874ms step_avg:58.79ms
step:1938/2330 train_time:113934ms step_avg:58.79ms
step:1939/2330 train_time:113991ms step_avg:58.79ms
step:1940/2330 train_time:114052ms step_avg:58.79ms
step:1941/2330 train_time:114111ms step_avg:58.79ms
step:1942/2330 train_time:114171ms step_avg:58.79ms
step:1943/2330 train_time:114229ms step_avg:58.79ms
step:1944/2330 train_time:114290ms step_avg:58.79ms
step:1945/2330 train_time:114347ms step_avg:58.79ms
step:1946/2330 train_time:114408ms step_avg:58.79ms
step:1947/2330 train_time:114465ms step_avg:58.79ms
step:1948/2330 train_time:114526ms step_avg:58.79ms
step:1949/2330 train_time:114584ms step_avg:58.79ms
step:1950/2330 train_time:114643ms step_avg:58.79ms
step:1951/2330 train_time:114699ms step_avg:58.79ms
step:1952/2330 train_time:114761ms step_avg:58.79ms
step:1953/2330 train_time:114819ms step_avg:58.79ms
step:1954/2330 train_time:114879ms step_avg:58.79ms
step:1955/2330 train_time:114935ms step_avg:58.79ms
step:1956/2330 train_time:114999ms step_avg:58.79ms
step:1957/2330 train_time:115056ms step_avg:58.79ms
step:1958/2330 train_time:115118ms step_avg:58.79ms
step:1959/2330 train_time:115175ms step_avg:58.79ms
step:1960/2330 train_time:115237ms step_avg:58.79ms
step:1961/2330 train_time:115294ms step_avg:58.79ms
step:1962/2330 train_time:115355ms step_avg:58.79ms
step:1963/2330 train_time:115412ms step_avg:58.79ms
step:1964/2330 train_time:115473ms step_avg:58.79ms
step:1965/2330 train_time:115531ms step_avg:58.79ms
step:1966/2330 train_time:115592ms step_avg:58.80ms
step:1967/2330 train_time:115650ms step_avg:58.80ms
step:1968/2330 train_time:115711ms step_avg:58.80ms
step:1969/2330 train_time:115768ms step_avg:58.80ms
step:1970/2330 train_time:115829ms step_avg:58.80ms
step:1971/2330 train_time:115887ms step_avg:58.80ms
step:1972/2330 train_time:115947ms step_avg:58.80ms
step:1973/2330 train_time:116005ms step_avg:58.80ms
step:1974/2330 train_time:116066ms step_avg:58.80ms
step:1975/2330 train_time:116123ms step_avg:58.80ms
step:1976/2330 train_time:116183ms step_avg:58.80ms
step:1977/2330 train_time:116240ms step_avg:58.80ms
step:1978/2330 train_time:116302ms step_avg:58.80ms
step:1979/2330 train_time:116359ms step_avg:58.80ms
step:1980/2330 train_time:116420ms step_avg:58.80ms
step:1981/2330 train_time:116477ms step_avg:58.80ms
step:1982/2330 train_time:116539ms step_avg:58.80ms
step:1983/2330 train_time:116596ms step_avg:58.80ms
step:1984/2330 train_time:116658ms step_avg:58.80ms
step:1985/2330 train_time:116714ms step_avg:58.80ms
step:1986/2330 train_time:116777ms step_avg:58.80ms
step:1987/2330 train_time:116834ms step_avg:58.80ms
step:1988/2330 train_time:116896ms step_avg:58.80ms
step:1989/2330 train_time:116953ms step_avg:58.80ms
step:1990/2330 train_time:117014ms step_avg:58.80ms
step:1991/2330 train_time:117072ms step_avg:58.80ms
step:1992/2330 train_time:117132ms step_avg:58.80ms
step:1993/2330 train_time:117190ms step_avg:58.80ms
step:1994/2330 train_time:117251ms step_avg:58.80ms
step:1995/2330 train_time:117309ms step_avg:58.80ms
step:1996/2330 train_time:117369ms step_avg:58.80ms
step:1997/2330 train_time:117427ms step_avg:58.80ms
step:1998/2330 train_time:117487ms step_avg:58.80ms
step:1999/2330 train_time:117544ms step_avg:58.80ms
step:2000/2330 train_time:117605ms step_avg:58.80ms
step:2000/2330 val_loss:3.7586 train_time:117687ms step_avg:58.84ms
step:2001/2330 train_time:117705ms step_avg:58.82ms
step:2002/2330 train_time:117726ms step_avg:58.80ms
step:2003/2330 train_time:117788ms step_avg:58.81ms
step:2004/2330 train_time:117853ms step_avg:58.81ms
step:2005/2330 train_time:117911ms step_avg:58.81ms
step:2006/2330 train_time:117972ms step_avg:58.81ms
step:2007/2330 train_time:118030ms step_avg:58.81ms
step:2008/2330 train_time:118089ms step_avg:58.81ms
step:2009/2330 train_time:118145ms step_avg:58.81ms
step:2010/2330 train_time:118206ms step_avg:58.81ms
step:2011/2330 train_time:118262ms step_avg:58.81ms
step:2012/2330 train_time:118321ms step_avg:58.81ms
step:2013/2330 train_time:118377ms step_avg:58.81ms
step:2014/2330 train_time:118438ms step_avg:58.81ms
step:2015/2330 train_time:118495ms step_avg:58.81ms
step:2016/2330 train_time:118556ms step_avg:58.81ms
step:2017/2330 train_time:118613ms step_avg:58.81ms
step:2018/2330 train_time:118675ms step_avg:58.81ms
step:2019/2330 train_time:118734ms step_avg:58.81ms
step:2020/2330 train_time:118799ms step_avg:58.81ms
step:2021/2330 train_time:118856ms step_avg:58.81ms
step:2022/2330 train_time:118919ms step_avg:58.81ms
step:2023/2330 train_time:118976ms step_avg:58.81ms
step:2024/2330 train_time:119040ms step_avg:58.81ms
step:2025/2330 train_time:119097ms step_avg:58.81ms
step:2026/2330 train_time:119157ms step_avg:58.81ms
step:2027/2330 train_time:119214ms step_avg:58.81ms
step:2028/2330 train_time:119274ms step_avg:58.81ms
step:2029/2330 train_time:119331ms step_avg:58.81ms
step:2030/2330 train_time:119392ms step_avg:58.81ms
step:2031/2330 train_time:119449ms step_avg:58.81ms
step:2032/2330 train_time:119508ms step_avg:58.81ms
step:2033/2330 train_time:119565ms step_avg:58.81ms
step:2034/2330 train_time:119626ms step_avg:58.81ms
step:2035/2330 train_time:119686ms step_avg:58.81ms
step:2036/2330 train_time:119747ms step_avg:58.81ms
step:2037/2330 train_time:119806ms step_avg:58.82ms
step:2038/2330 train_time:119867ms step_avg:58.82ms
step:2039/2330 train_time:119927ms step_avg:58.82ms
step:2040/2330 train_time:119987ms step_avg:58.82ms
step:2041/2330 train_time:120045ms step_avg:58.82ms
step:2042/2330 train_time:120106ms step_avg:58.82ms
step:2043/2330 train_time:120163ms step_avg:58.82ms
step:2044/2330 train_time:120223ms step_avg:58.82ms
step:2045/2330 train_time:120279ms step_avg:58.82ms
step:2046/2330 train_time:120341ms step_avg:58.82ms
step:2047/2330 train_time:120398ms step_avg:58.82ms
step:2048/2330 train_time:120457ms step_avg:58.82ms
step:2049/2330 train_time:120514ms step_avg:58.82ms
step:2050/2330 train_time:120575ms step_avg:58.82ms
step:2051/2330 train_time:120633ms step_avg:58.82ms
step:2052/2330 train_time:120695ms step_avg:58.82ms
step:2053/2330 train_time:120752ms step_avg:58.82ms
step:2054/2330 train_time:120816ms step_avg:58.82ms
step:2055/2330 train_time:120874ms step_avg:58.82ms
step:2056/2330 train_time:120937ms step_avg:58.82ms
step:2057/2330 train_time:120994ms step_avg:58.82ms
step:2058/2330 train_time:121056ms step_avg:58.82ms
step:2059/2330 train_time:121112ms step_avg:58.82ms
step:2060/2330 train_time:121174ms step_avg:58.82ms
step:2061/2330 train_time:121231ms step_avg:58.82ms
step:2062/2330 train_time:121291ms step_avg:58.82ms
step:2063/2330 train_time:121349ms step_avg:58.82ms
step:2064/2330 train_time:121409ms step_avg:58.82ms
step:2065/2330 train_time:121466ms step_avg:58.82ms
step:2066/2330 train_time:121527ms step_avg:58.82ms
step:2067/2330 train_time:121584ms step_avg:58.82ms
step:2068/2330 train_time:121645ms step_avg:58.82ms
step:2069/2330 train_time:121703ms step_avg:58.82ms
step:2070/2330 train_time:121764ms step_avg:58.82ms
step:2071/2330 train_time:121821ms step_avg:58.82ms
step:2072/2330 train_time:121884ms step_avg:58.82ms
step:2073/2330 train_time:121941ms step_avg:58.82ms
step:2074/2330 train_time:122002ms step_avg:58.82ms
step:2075/2330 train_time:122059ms step_avg:58.82ms
step:2076/2330 train_time:122120ms step_avg:58.82ms
step:2077/2330 train_time:122177ms step_avg:58.82ms
step:2078/2330 train_time:122238ms step_avg:58.83ms
step:2079/2330 train_time:122296ms step_avg:58.82ms
step:2080/2330 train_time:122356ms step_avg:58.83ms
step:2081/2330 train_time:122413ms step_avg:58.82ms
step:2082/2330 train_time:122474ms step_avg:58.83ms
step:2083/2330 train_time:122530ms step_avg:58.82ms
step:2084/2330 train_time:122592ms step_avg:58.83ms
step:2085/2330 train_time:122649ms step_avg:58.82ms
step:2086/2330 train_time:122711ms step_avg:58.83ms
step:2087/2330 train_time:122769ms step_avg:58.83ms
step:2088/2330 train_time:122830ms step_avg:58.83ms
step:2089/2330 train_time:122888ms step_avg:58.83ms
step:2090/2330 train_time:122949ms step_avg:58.83ms
step:2091/2330 train_time:123007ms step_avg:58.83ms
step:2092/2330 train_time:123068ms step_avg:58.83ms
step:2093/2330 train_time:123126ms step_avg:58.83ms
step:2094/2330 train_time:123187ms step_avg:58.83ms
step:2095/2330 train_time:123245ms step_avg:58.83ms
step:2096/2330 train_time:123305ms step_avg:58.83ms
step:2097/2330 train_time:123361ms step_avg:58.83ms
step:2098/2330 train_time:123423ms step_avg:58.83ms
step:2099/2330 train_time:123479ms step_avg:58.83ms
step:2100/2330 train_time:123540ms step_avg:58.83ms
step:2101/2330 train_time:123597ms step_avg:58.83ms
step:2102/2330 train_time:123658ms step_avg:58.83ms
step:2103/2330 train_time:123715ms step_avg:58.83ms
step:2104/2330 train_time:123778ms step_avg:58.83ms
step:2105/2330 train_time:123836ms step_avg:58.83ms
step:2106/2330 train_time:123897ms step_avg:58.83ms
step:2107/2330 train_time:123954ms step_avg:58.83ms
step:2108/2330 train_time:124016ms step_avg:58.83ms
step:2109/2330 train_time:124074ms step_avg:58.83ms
step:2110/2330 train_time:124135ms step_avg:58.83ms
step:2111/2330 train_time:124192ms step_avg:58.83ms
step:2112/2330 train_time:124255ms step_avg:58.83ms
step:2113/2330 train_time:124312ms step_avg:58.83ms
step:2114/2330 train_time:124372ms step_avg:58.83ms
step:2115/2330 train_time:124430ms step_avg:58.83ms
step:2116/2330 train_time:124490ms step_avg:58.83ms
step:2117/2330 train_time:124548ms step_avg:58.83ms
step:2118/2330 train_time:124607ms step_avg:58.83ms
step:2119/2330 train_time:124664ms step_avg:58.83ms
step:2120/2330 train_time:124725ms step_avg:58.83ms
step:2121/2330 train_time:124783ms step_avg:58.83ms
step:2122/2330 train_time:124844ms step_avg:58.83ms
step:2123/2330 train_time:124901ms step_avg:58.83ms
step:2124/2330 train_time:124962ms step_avg:58.83ms
step:2125/2330 train_time:125020ms step_avg:58.83ms
step:2126/2330 train_time:125080ms step_avg:58.83ms
step:2127/2330 train_time:125137ms step_avg:58.83ms
step:2128/2330 train_time:125198ms step_avg:58.83ms
step:2129/2330 train_time:125255ms step_avg:58.83ms
step:2130/2330 train_time:125317ms step_avg:58.83ms
step:2131/2330 train_time:125374ms step_avg:58.83ms
step:2132/2330 train_time:125435ms step_avg:58.83ms
step:2133/2330 train_time:125492ms step_avg:58.83ms
step:2134/2330 train_time:125553ms step_avg:58.83ms
step:2135/2330 train_time:125610ms step_avg:58.83ms
step:2136/2330 train_time:125672ms step_avg:58.84ms
step:2137/2330 train_time:125729ms step_avg:58.83ms
step:2138/2330 train_time:125791ms step_avg:58.84ms
step:2139/2330 train_time:125849ms step_avg:58.84ms
step:2140/2330 train_time:125910ms step_avg:58.84ms
step:2141/2330 train_time:125968ms step_avg:58.84ms
step:2142/2330 train_time:126029ms step_avg:58.84ms
step:2143/2330 train_time:126088ms step_avg:58.84ms
step:2144/2330 train_time:126148ms step_avg:58.84ms
step:2145/2330 train_time:126207ms step_avg:58.84ms
step:2146/2330 train_time:126267ms step_avg:58.84ms
step:2147/2330 train_time:126324ms step_avg:58.84ms
step:2148/2330 train_time:126384ms step_avg:58.84ms
step:2149/2330 train_time:126442ms step_avg:58.84ms
step:2150/2330 train_time:126503ms step_avg:58.84ms
step:2151/2330 train_time:126560ms step_avg:58.84ms
step:2152/2330 train_time:126621ms step_avg:58.84ms
step:2153/2330 train_time:126678ms step_avg:58.84ms
step:2154/2330 train_time:126739ms step_avg:58.84ms
step:2155/2330 train_time:126795ms step_avg:58.84ms
step:2156/2330 train_time:126858ms step_avg:58.84ms
step:2157/2330 train_time:126915ms step_avg:58.84ms
step:2158/2330 train_time:126977ms step_avg:58.84ms
step:2159/2330 train_time:127034ms step_avg:58.84ms
step:2160/2330 train_time:127097ms step_avg:58.84ms
step:2161/2330 train_time:127153ms step_avg:58.84ms
step:2162/2330 train_time:127216ms step_avg:58.84ms
step:2163/2330 train_time:127273ms step_avg:58.84ms
step:2164/2330 train_time:127335ms step_avg:58.84ms
step:2165/2330 train_time:127392ms step_avg:58.84ms
step:2166/2330 train_time:127452ms step_avg:58.84ms
step:2167/2330 train_time:127510ms step_avg:58.84ms
step:2168/2330 train_time:127570ms step_avg:58.84ms
step:2169/2330 train_time:127627ms step_avg:58.84ms
step:2170/2330 train_time:127689ms step_avg:58.84ms
step:2171/2330 train_time:127747ms step_avg:58.84ms
step:2172/2330 train_time:127807ms step_avg:58.84ms
step:2173/2330 train_time:127864ms step_avg:58.84ms
step:2174/2330 train_time:127925ms step_avg:58.84ms
step:2175/2330 train_time:127982ms step_avg:58.84ms
step:2176/2330 train_time:128044ms step_avg:58.84ms
step:2177/2330 train_time:128101ms step_avg:58.84ms
step:2178/2330 train_time:128161ms step_avg:58.84ms
step:2179/2330 train_time:128218ms step_avg:58.84ms
step:2180/2330 train_time:128280ms step_avg:58.84ms
step:2181/2330 train_time:128337ms step_avg:58.84ms
step:2182/2330 train_time:128398ms step_avg:58.84ms
step:2183/2330 train_time:128455ms step_avg:58.84ms
step:2184/2330 train_time:128517ms step_avg:58.84ms
step:2185/2330 train_time:128574ms step_avg:58.84ms
step:2186/2330 train_time:128636ms step_avg:58.85ms
step:2187/2330 train_time:128693ms step_avg:58.84ms
step:2188/2330 train_time:128756ms step_avg:58.85ms
step:2189/2330 train_time:128813ms step_avg:58.85ms
step:2190/2330 train_time:128874ms step_avg:58.85ms
step:2191/2330 train_time:128931ms step_avg:58.85ms
step:2192/2330 train_time:128993ms step_avg:58.85ms
step:2193/2330 train_time:129050ms step_avg:58.85ms
step:2194/2330 train_time:129111ms step_avg:58.85ms
step:2195/2330 train_time:129168ms step_avg:58.85ms
step:2196/2330 train_time:129229ms step_avg:58.85ms
step:2197/2330 train_time:129287ms step_avg:58.85ms
step:2198/2330 train_time:129347ms step_avg:58.85ms
step:2199/2330 train_time:129406ms step_avg:58.85ms
step:2200/2330 train_time:129466ms step_avg:58.85ms
step:2201/2330 train_time:129524ms step_avg:58.85ms
step:2202/2330 train_time:129584ms step_avg:58.85ms
step:2203/2330 train_time:129641ms step_avg:58.85ms
step:2204/2330 train_time:129703ms step_avg:58.85ms
step:2205/2330 train_time:129760ms step_avg:58.85ms
step:2206/2330 train_time:129820ms step_avg:58.85ms
step:2207/2330 train_time:129876ms step_avg:58.85ms
step:2208/2330 train_time:129938ms step_avg:58.85ms
step:2209/2330 train_time:129995ms step_avg:58.85ms
step:2210/2330 train_time:130057ms step_avg:58.85ms
step:2211/2330 train_time:130114ms step_avg:58.85ms
step:2212/2330 train_time:130175ms step_avg:58.85ms
step:2213/2330 train_time:130232ms step_avg:58.85ms
step:2214/2330 train_time:130295ms step_avg:58.85ms
step:2215/2330 train_time:130352ms step_avg:58.85ms
step:2216/2330 train_time:130415ms step_avg:58.85ms
step:2217/2330 train_time:130473ms step_avg:58.85ms
step:2218/2330 train_time:130534ms step_avg:58.85ms
step:2219/2330 train_time:130590ms step_avg:58.85ms
step:2220/2330 train_time:130652ms step_avg:58.85ms
step:2221/2330 train_time:130709ms step_avg:58.85ms
step:2222/2330 train_time:130770ms step_avg:58.85ms
step:2223/2330 train_time:130828ms step_avg:58.85ms
step:2224/2330 train_time:130889ms step_avg:58.85ms
step:2225/2330 train_time:130947ms step_avg:58.85ms
step:2226/2330 train_time:131007ms step_avg:58.85ms
step:2227/2330 train_time:131065ms step_avg:58.85ms
step:2228/2330 train_time:131126ms step_avg:58.85ms
step:2229/2330 train_time:131183ms step_avg:58.85ms
step:2230/2330 train_time:131244ms step_avg:58.85ms
step:2231/2330 train_time:131302ms step_avg:58.85ms
step:2232/2330 train_time:131362ms step_avg:58.85ms
step:2233/2330 train_time:131419ms step_avg:58.85ms
step:2234/2330 train_time:131480ms step_avg:58.85ms
step:2235/2330 train_time:131538ms step_avg:58.85ms
step:2236/2330 train_time:131598ms step_avg:58.85ms
step:2237/2330 train_time:131654ms step_avg:58.85ms
step:2238/2330 train_time:131716ms step_avg:58.85ms
step:2239/2330 train_time:131772ms step_avg:58.85ms
step:2240/2330 train_time:131835ms step_avg:58.85ms
step:2241/2330 train_time:131892ms step_avg:58.85ms
step:2242/2330 train_time:131953ms step_avg:58.85ms
step:2243/2330 train_time:132010ms step_avg:58.85ms
step:2244/2330 train_time:132072ms step_avg:58.86ms
step:2245/2330 train_time:132130ms step_avg:58.86ms
step:2246/2330 train_time:132191ms step_avg:58.86ms
step:2247/2330 train_time:132250ms step_avg:58.86ms
step:2248/2330 train_time:132310ms step_avg:58.86ms
step:2249/2330 train_time:132367ms step_avg:58.86ms
step:2250/2330 train_time:132428ms step_avg:58.86ms
step:2250/2330 val_loss:3.7106 train_time:132510ms step_avg:58.89ms
step:2251/2330 train_time:132529ms step_avg:58.88ms
step:2252/2330 train_time:132549ms step_avg:58.86ms
step:2253/2330 train_time:132610ms step_avg:58.86ms
step:2254/2330 train_time:132675ms step_avg:58.86ms
step:2255/2330 train_time:132732ms step_avg:58.86ms
step:2256/2330 train_time:132794ms step_avg:58.86ms
step:2257/2330 train_time:132850ms step_avg:58.86ms
step:2258/2330 train_time:132912ms step_avg:58.86ms
step:2259/2330 train_time:132969ms step_avg:58.86ms
step:2260/2330 train_time:133029ms step_avg:58.86ms
step:2261/2330 train_time:133086ms step_avg:58.86ms
step:2262/2330 train_time:133146ms step_avg:58.86ms
step:2263/2330 train_time:133202ms step_avg:58.86ms
step:2264/2330 train_time:133262ms step_avg:58.86ms
step:2265/2330 train_time:133319ms step_avg:58.86ms
step:2266/2330 train_time:133379ms step_avg:58.86ms
step:2267/2330 train_time:133436ms step_avg:58.86ms
step:2268/2330 train_time:133497ms step_avg:58.86ms
step:2269/2330 train_time:133556ms step_avg:58.86ms
step:2270/2330 train_time:133621ms step_avg:58.86ms
step:2271/2330 train_time:133678ms step_avg:58.86ms
step:2272/2330 train_time:133740ms step_avg:58.86ms
step:2273/2330 train_time:133797ms step_avg:58.86ms
step:2274/2330 train_time:133860ms step_avg:58.87ms
step:2275/2330 train_time:133916ms step_avg:58.86ms
step:2276/2330 train_time:133977ms step_avg:58.87ms
step:2277/2330 train_time:134034ms step_avg:58.86ms
step:2278/2330 train_time:134096ms step_avg:58.87ms
step:2279/2330 train_time:134152ms step_avg:58.86ms
step:2280/2330 train_time:134213ms step_avg:58.87ms
step:2281/2330 train_time:134270ms step_avg:58.86ms
step:2282/2330 train_time:134330ms step_avg:58.87ms
step:2283/2330 train_time:134387ms step_avg:58.86ms
step:2284/2330 train_time:134449ms step_avg:58.87ms
step:2285/2330 train_time:134508ms step_avg:58.87ms
step:2286/2330 train_time:134569ms step_avg:58.87ms
step:2287/2330 train_time:134628ms step_avg:58.87ms
step:2288/2330 train_time:134689ms step_avg:58.87ms
step:2289/2330 train_time:134748ms step_avg:58.87ms
step:2290/2330 train_time:134809ms step_avg:58.87ms
step:2291/2330 train_time:134868ms step_avg:58.87ms
step:2292/2330 train_time:134928ms step_avg:58.87ms
step:2293/2330 train_time:134985ms step_avg:58.87ms
step:2294/2330 train_time:135046ms step_avg:58.87ms
step:2295/2330 train_time:135103ms step_avg:58.87ms
step:2296/2330 train_time:135163ms step_avg:58.87ms
step:2297/2330 train_time:135220ms step_avg:58.87ms
step:2298/2330 train_time:135280ms step_avg:58.87ms
step:2299/2330 train_time:135337ms step_avg:58.87ms
step:2300/2330 train_time:135398ms step_avg:58.87ms
step:2301/2330 train_time:135456ms step_avg:58.87ms
step:2302/2330 train_time:135516ms step_avg:58.87ms
step:2303/2330 train_time:135573ms step_avg:58.87ms
step:2304/2330 train_time:135636ms step_avg:58.87ms
step:2305/2330 train_time:135693ms step_avg:58.87ms
step:2306/2330 train_time:135756ms step_avg:58.87ms
step:2307/2330 train_time:135814ms step_avg:58.87ms
step:2308/2330 train_time:135875ms step_avg:58.87ms
step:2309/2330 train_time:135933ms step_avg:58.87ms
step:2310/2330 train_time:135995ms step_avg:58.87ms
step:2311/2330 train_time:136052ms step_avg:58.87ms
step:2312/2330 train_time:136113ms step_avg:58.87ms
step:2313/2330 train_time:136170ms step_avg:58.87ms
step:2314/2330 train_time:136231ms step_avg:58.87ms
step:2315/2330 train_time:136289ms step_avg:58.87ms
step:2316/2330 train_time:136349ms step_avg:58.87ms
step:2317/2330 train_time:136407ms step_avg:58.87ms
step:2318/2330 train_time:136466ms step_avg:58.87ms
step:2319/2330 train_time:136525ms step_avg:58.87ms
step:2320/2330 train_time:136585ms step_avg:58.87ms
step:2321/2330 train_time:136644ms step_avg:58.87ms
step:2322/2330 train_time:136704ms step_avg:58.87ms
step:2323/2330 train_time:136763ms step_avg:58.87ms
step:2324/2330 train_time:136824ms step_avg:58.87ms
step:2325/2330 train_time:136882ms step_avg:58.87ms
step:2326/2330 train_time:136942ms step_avg:58.87ms
step:2327/2330 train_time:136999ms step_avg:58.87ms
step:2328/2330 train_time:137060ms step_avg:58.87ms
step:2329/2330 train_time:137117ms step_avg:58.87ms
step:2330/2330 train_time:137177ms step_avg:58.87ms
step:2330/2330 val_loss:3.6954 train_time:137259ms step_avg:58.91ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
