import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--beta1', type=str, required=True)
parser.add_argument('--beta2', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_beta_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=6e-4,  betas=(float(args2.beta1), float(args2.beta2)), eps=1e-10, weight_decay=0.0, fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt", exist_ok=True)
            torch.save(log, f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 03:50:31 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   36C    P0             117W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   35C    P0             118W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             118W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             115W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   34C    P0             118W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             114W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:90ms step_avg:89.96ms
step:2/2330 train_time:179ms step_avg:89.27ms
step:3/2330 train_time:197ms step_avg:65.62ms
step:4/2330 train_time:216ms step_avg:54.01ms
step:5/2330 train_time:270ms step_avg:53.92ms
step:6/2330 train_time:328ms step_avg:54.64ms
step:7/2330 train_time:383ms step_avg:54.65ms
step:8/2330 train_time:441ms step_avg:55.14ms
step:9/2330 train_time:496ms step_avg:55.10ms
step:10/2330 train_time:555ms step_avg:55.50ms
step:11/2330 train_time:610ms step_avg:55.49ms
step:12/2330 train_time:669ms step_avg:55.77ms
step:13/2330 train_time:725ms step_avg:55.74ms
step:14/2330 train_time:783ms step_avg:55.92ms
step:15/2330 train_time:838ms step_avg:55.89ms
step:16/2330 train_time:897ms step_avg:56.07ms
step:17/2330 train_time:952ms step_avg:55.98ms
step:18/2330 train_time:1011ms step_avg:56.19ms
step:19/2330 train_time:1069ms step_avg:56.26ms
step:20/2330 train_time:1131ms step_avg:56.56ms
step:21/2330 train_time:1188ms step_avg:56.55ms
step:22/2330 train_time:1249ms step_avg:56.78ms
step:23/2330 train_time:1304ms step_avg:56.71ms
step:24/2330 train_time:1364ms step_avg:56.84ms
step:25/2330 train_time:1419ms step_avg:56.77ms
step:26/2330 train_time:1478ms step_avg:56.85ms
step:27/2330 train_time:1533ms step_avg:56.79ms
step:28/2330 train_time:1592ms step_avg:56.86ms
step:29/2330 train_time:1647ms step_avg:56.79ms
step:30/2330 train_time:1705ms step_avg:56.85ms
step:31/2330 train_time:1760ms step_avg:56.79ms
step:32/2330 train_time:1820ms step_avg:56.88ms
step:33/2330 train_time:1875ms step_avg:56.83ms
step:34/2330 train_time:1935ms step_avg:56.90ms
step:35/2330 train_time:1990ms step_avg:56.86ms
step:36/2330 train_time:2051ms step_avg:56.97ms
step:37/2330 train_time:2108ms step_avg:56.96ms
step:38/2330 train_time:2168ms step_avg:57.05ms
step:39/2330 train_time:2225ms step_avg:57.05ms
step:40/2330 train_time:2284ms step_avg:57.11ms
step:41/2330 train_time:2340ms step_avg:57.08ms
step:42/2330 train_time:2399ms step_avg:57.12ms
step:43/2330 train_time:2455ms step_avg:57.09ms
step:44/2330 train_time:2513ms step_avg:57.12ms
step:45/2330 train_time:2569ms step_avg:57.08ms
step:46/2330 train_time:2629ms step_avg:57.14ms
step:47/2330 train_time:2684ms step_avg:57.11ms
step:48/2330 train_time:2742ms step_avg:57.13ms
step:49/2330 train_time:2797ms step_avg:57.09ms
step:50/2330 train_time:2856ms step_avg:57.12ms
step:51/2330 train_time:2911ms step_avg:57.08ms
step:52/2330 train_time:2970ms step_avg:57.12ms
step:53/2330 train_time:3026ms step_avg:57.10ms
step:54/2330 train_time:3087ms step_avg:57.17ms
step:55/2330 train_time:3143ms step_avg:57.14ms
step:56/2330 train_time:3203ms step_avg:57.19ms
step:57/2330 train_time:3259ms step_avg:57.17ms
step:58/2330 train_time:3318ms step_avg:57.21ms
step:59/2330 train_time:3374ms step_avg:57.19ms
step:60/2330 train_time:3433ms step_avg:57.22ms
step:61/2330 train_time:3489ms step_avg:57.20ms
step:62/2330 train_time:3548ms step_avg:57.23ms
step:63/2330 train_time:3604ms step_avg:57.20ms
step:64/2330 train_time:3663ms step_avg:57.23ms
step:65/2330 train_time:3718ms step_avg:57.20ms
step:66/2330 train_time:3777ms step_avg:57.22ms
step:67/2330 train_time:3833ms step_avg:57.20ms
step:68/2330 train_time:3891ms step_avg:57.22ms
step:69/2330 train_time:3947ms step_avg:57.20ms
step:70/2330 train_time:4006ms step_avg:57.23ms
step:71/2330 train_time:4062ms step_avg:57.21ms
step:72/2330 train_time:4121ms step_avg:57.24ms
step:73/2330 train_time:4178ms step_avg:57.23ms
step:74/2330 train_time:4237ms step_avg:57.25ms
step:75/2330 train_time:4293ms step_avg:57.24ms
step:76/2330 train_time:4352ms step_avg:57.26ms
step:77/2330 train_time:4407ms step_avg:57.24ms
step:78/2330 train_time:4467ms step_avg:57.26ms
step:79/2330 train_time:4522ms step_avg:57.24ms
step:80/2330 train_time:4581ms step_avg:57.27ms
step:81/2330 train_time:4637ms step_avg:57.25ms
step:82/2330 train_time:4695ms step_avg:57.26ms
step:83/2330 train_time:4751ms step_avg:57.24ms
step:84/2330 train_time:4810ms step_avg:57.26ms
step:85/2330 train_time:4865ms step_avg:57.24ms
step:86/2330 train_time:4925ms step_avg:57.26ms
step:87/2330 train_time:4980ms step_avg:57.24ms
step:88/2330 train_time:5039ms step_avg:57.26ms
step:89/2330 train_time:5095ms step_avg:57.25ms
step:90/2330 train_time:5154ms step_avg:57.27ms
step:91/2330 train_time:5210ms step_avg:57.25ms
step:92/2330 train_time:5271ms step_avg:57.29ms
step:93/2330 train_time:5327ms step_avg:57.28ms
step:94/2330 train_time:5386ms step_avg:57.30ms
step:95/2330 train_time:5441ms step_avg:57.28ms
step:96/2330 train_time:5501ms step_avg:57.30ms
step:97/2330 train_time:5557ms step_avg:57.29ms
step:98/2330 train_time:5615ms step_avg:57.30ms
step:99/2330 train_time:5671ms step_avg:57.28ms
step:100/2330 train_time:5730ms step_avg:57.30ms
step:101/2330 train_time:5786ms step_avg:57.28ms
step:102/2330 train_time:5845ms step_avg:57.31ms
step:103/2330 train_time:5901ms step_avg:57.29ms
step:104/2330 train_time:5960ms step_avg:57.30ms
step:105/2330 train_time:6016ms step_avg:57.29ms
step:106/2330 train_time:6074ms step_avg:57.31ms
step:107/2330 train_time:6131ms step_avg:57.30ms
step:108/2330 train_time:6190ms step_avg:57.31ms
step:109/2330 train_time:6245ms step_avg:57.30ms
step:110/2330 train_time:6305ms step_avg:57.32ms
step:111/2330 train_time:6362ms step_avg:57.31ms
step:112/2330 train_time:6420ms step_avg:57.32ms
step:113/2330 train_time:6477ms step_avg:57.32ms
step:114/2330 train_time:6536ms step_avg:57.33ms
step:115/2330 train_time:6591ms step_avg:57.31ms
step:116/2330 train_time:6649ms step_avg:57.32ms
step:117/2330 train_time:6705ms step_avg:57.31ms
step:118/2330 train_time:6764ms step_avg:57.32ms
step:119/2330 train_time:6819ms step_avg:57.30ms
step:120/2330 train_time:6878ms step_avg:57.32ms
step:121/2330 train_time:6935ms step_avg:57.32ms
step:122/2330 train_time:6994ms step_avg:57.33ms
step:123/2330 train_time:7049ms step_avg:57.31ms
step:124/2330 train_time:7108ms step_avg:57.32ms
step:125/2330 train_time:7164ms step_avg:57.31ms
step:126/2330 train_time:7224ms step_avg:57.33ms
step:127/2330 train_time:7280ms step_avg:57.32ms
step:128/2330 train_time:7339ms step_avg:57.34ms
step:129/2330 train_time:7396ms step_avg:57.33ms
step:130/2330 train_time:7455ms step_avg:57.34ms
step:131/2330 train_time:7511ms step_avg:57.33ms
step:132/2330 train_time:7569ms step_avg:57.34ms
step:133/2330 train_time:7625ms step_avg:57.33ms
step:134/2330 train_time:7684ms step_avg:57.34ms
step:135/2330 train_time:7740ms step_avg:57.33ms
step:136/2330 train_time:7799ms step_avg:57.35ms
step:137/2330 train_time:7854ms step_avg:57.33ms
step:138/2330 train_time:7914ms step_avg:57.35ms
step:139/2330 train_time:7970ms step_avg:57.34ms
step:140/2330 train_time:8029ms step_avg:57.35ms
step:141/2330 train_time:8084ms step_avg:57.33ms
step:142/2330 train_time:8143ms step_avg:57.34ms
step:143/2330 train_time:8199ms step_avg:57.34ms
step:144/2330 train_time:8258ms step_avg:57.35ms
step:145/2330 train_time:8314ms step_avg:57.33ms
step:146/2330 train_time:8373ms step_avg:57.35ms
step:147/2330 train_time:8429ms step_avg:57.34ms
step:148/2330 train_time:8488ms step_avg:57.35ms
step:149/2330 train_time:8543ms step_avg:57.34ms
step:150/2330 train_time:8603ms step_avg:57.35ms
step:151/2330 train_time:8659ms step_avg:57.34ms
step:152/2330 train_time:8717ms step_avg:57.35ms
step:153/2330 train_time:8773ms step_avg:57.34ms
step:154/2330 train_time:8832ms step_avg:57.35ms
step:155/2330 train_time:8888ms step_avg:57.34ms
step:156/2330 train_time:8946ms step_avg:57.35ms
step:157/2330 train_time:9002ms step_avg:57.34ms
step:158/2330 train_time:9061ms step_avg:57.35ms
step:159/2330 train_time:9117ms step_avg:57.34ms
step:160/2330 train_time:9176ms step_avg:57.35ms
step:161/2330 train_time:9231ms step_avg:57.34ms
step:162/2330 train_time:9291ms step_avg:57.35ms
step:163/2330 train_time:9347ms step_avg:57.34ms
step:164/2330 train_time:9407ms step_avg:57.36ms
step:165/2330 train_time:9462ms step_avg:57.35ms
step:166/2330 train_time:9522ms step_avg:57.36ms
step:167/2330 train_time:9577ms step_avg:57.35ms
step:168/2330 train_time:9636ms step_avg:57.36ms
step:169/2330 train_time:9692ms step_avg:57.35ms
step:170/2330 train_time:9752ms step_avg:57.36ms
step:171/2330 train_time:9807ms step_avg:57.35ms
step:172/2330 train_time:9867ms step_avg:57.37ms
step:173/2330 train_time:9922ms step_avg:57.35ms
step:174/2330 train_time:9981ms step_avg:57.36ms
step:175/2330 train_time:10038ms step_avg:57.36ms
step:176/2330 train_time:10096ms step_avg:57.37ms
step:177/2330 train_time:10152ms step_avg:57.36ms
step:178/2330 train_time:10212ms step_avg:57.37ms
step:179/2330 train_time:10267ms step_avg:57.36ms
step:180/2330 train_time:10327ms step_avg:57.37ms
step:181/2330 train_time:10382ms step_avg:57.36ms
step:182/2330 train_time:10441ms step_avg:57.37ms
step:183/2330 train_time:10496ms step_avg:57.36ms
step:184/2330 train_time:10556ms step_avg:57.37ms
step:185/2330 train_time:10611ms step_avg:57.36ms
step:186/2330 train_time:10671ms step_avg:57.37ms
step:187/2330 train_time:10727ms step_avg:57.36ms
step:188/2330 train_time:10786ms step_avg:57.37ms
step:189/2330 train_time:10842ms step_avg:57.36ms
step:190/2330 train_time:10902ms step_avg:57.38ms
step:191/2330 train_time:10957ms step_avg:57.37ms
step:192/2330 train_time:11016ms step_avg:57.37ms
step:193/2330 train_time:11071ms step_avg:57.36ms
step:194/2330 train_time:11131ms step_avg:57.38ms
step:195/2330 train_time:11187ms step_avg:57.37ms
step:196/2330 train_time:11247ms step_avg:57.38ms
step:197/2330 train_time:11303ms step_avg:57.37ms
step:198/2330 train_time:11361ms step_avg:57.38ms
step:199/2330 train_time:11417ms step_avg:57.37ms
step:200/2330 train_time:11476ms step_avg:57.38ms
step:201/2330 train_time:11532ms step_avg:57.37ms
step:202/2330 train_time:11590ms step_avg:57.38ms
step:203/2330 train_time:11646ms step_avg:57.37ms
step:204/2330 train_time:11705ms step_avg:57.38ms
step:205/2330 train_time:11761ms step_avg:57.37ms
step:206/2330 train_time:11820ms step_avg:57.38ms
step:207/2330 train_time:11875ms step_avg:57.37ms
step:208/2330 train_time:11935ms step_avg:57.38ms
step:209/2330 train_time:11991ms step_avg:57.37ms
step:210/2330 train_time:12050ms step_avg:57.38ms
step:211/2330 train_time:12106ms step_avg:57.37ms
step:212/2330 train_time:12165ms step_avg:57.38ms
step:213/2330 train_time:12221ms step_avg:57.38ms
step:214/2330 train_time:12280ms step_avg:57.38ms
step:215/2330 train_time:12336ms step_avg:57.38ms
step:216/2330 train_time:12395ms step_avg:57.38ms
step:217/2330 train_time:12450ms step_avg:57.38ms
step:218/2330 train_time:12510ms step_avg:57.39ms
step:219/2330 train_time:12567ms step_avg:57.38ms
step:220/2330 train_time:12625ms step_avg:57.39ms
step:221/2330 train_time:12682ms step_avg:57.38ms
step:222/2330 train_time:12740ms step_avg:57.39ms
step:223/2330 train_time:12795ms step_avg:57.38ms
step:224/2330 train_time:12854ms step_avg:57.38ms
step:225/2330 train_time:12910ms step_avg:57.38ms
step:226/2330 train_time:12970ms step_avg:57.39ms
step:227/2330 train_time:13026ms step_avg:57.38ms
step:228/2330 train_time:13084ms step_avg:57.39ms
step:229/2330 train_time:13141ms step_avg:57.38ms
step:230/2330 train_time:13199ms step_avg:57.39ms
step:231/2330 train_time:13255ms step_avg:57.38ms
step:232/2330 train_time:13314ms step_avg:57.39ms
step:233/2330 train_time:13370ms step_avg:57.38ms
step:234/2330 train_time:13430ms step_avg:57.39ms
step:235/2330 train_time:13486ms step_avg:57.39ms
step:236/2330 train_time:13544ms step_avg:57.39ms
step:237/2330 train_time:13601ms step_avg:57.39ms
step:238/2330 train_time:13660ms step_avg:57.39ms
step:239/2330 train_time:13716ms step_avg:57.39ms
step:240/2330 train_time:13775ms step_avg:57.39ms
step:241/2330 train_time:13830ms step_avg:57.39ms
step:242/2330 train_time:13889ms step_avg:57.39ms
step:243/2330 train_time:13945ms step_avg:57.39ms
step:244/2330 train_time:14005ms step_avg:57.40ms
step:245/2330 train_time:14061ms step_avg:57.39ms
step:246/2330 train_time:14120ms step_avg:57.40ms
step:247/2330 train_time:14176ms step_avg:57.39ms
step:248/2330 train_time:14235ms step_avg:57.40ms
step:249/2330 train_time:14291ms step_avg:57.39ms
step:250/2330 train_time:14351ms step_avg:57.40ms
step:250/2330 val_loss:4.9157 train_time:14431ms step_avg:57.72ms
step:251/2330 train_time:14449ms step_avg:57.57ms
step:252/2330 train_time:14469ms step_avg:57.42ms
step:253/2330 train_time:14526ms step_avg:57.41ms
step:254/2330 train_time:14590ms step_avg:57.44ms
step:255/2330 train_time:14647ms step_avg:57.44ms
step:256/2330 train_time:14710ms step_avg:57.46ms
step:257/2330 train_time:14766ms step_avg:57.46ms
step:258/2330 train_time:14825ms step_avg:57.46ms
step:259/2330 train_time:14880ms step_avg:57.45ms
step:260/2330 train_time:14939ms step_avg:57.46ms
step:261/2330 train_time:14994ms step_avg:57.45ms
step:262/2330 train_time:15052ms step_avg:57.45ms
step:263/2330 train_time:15108ms step_avg:57.44ms
step:264/2330 train_time:15166ms step_avg:57.45ms
step:265/2330 train_time:15222ms step_avg:57.44ms
step:266/2330 train_time:15280ms step_avg:57.44ms
step:267/2330 train_time:15335ms step_avg:57.44ms
step:268/2330 train_time:15394ms step_avg:57.44ms
step:269/2330 train_time:15451ms step_avg:57.44ms
step:270/2330 train_time:15510ms step_avg:57.44ms
step:271/2330 train_time:15567ms step_avg:57.44ms
step:272/2330 train_time:15627ms step_avg:57.45ms
step:273/2330 train_time:15684ms step_avg:57.45ms
step:274/2330 train_time:15743ms step_avg:57.46ms
step:275/2330 train_time:15800ms step_avg:57.45ms
step:276/2330 train_time:15859ms step_avg:57.46ms
step:277/2330 train_time:15914ms step_avg:57.45ms
step:278/2330 train_time:15973ms step_avg:57.46ms
step:279/2330 train_time:16028ms step_avg:57.45ms
step:280/2330 train_time:16087ms step_avg:57.45ms
step:281/2330 train_time:16142ms step_avg:57.45ms
step:282/2330 train_time:16202ms step_avg:57.45ms
step:283/2330 train_time:16257ms step_avg:57.45ms
step:284/2330 train_time:16316ms step_avg:57.45ms
step:285/2330 train_time:16372ms step_avg:57.44ms
step:286/2330 train_time:16430ms step_avg:57.45ms
step:287/2330 train_time:16486ms step_avg:57.44ms
step:288/2330 train_time:16546ms step_avg:57.45ms
step:289/2330 train_time:16603ms step_avg:57.45ms
step:290/2330 train_time:16663ms step_avg:57.46ms
step:291/2330 train_time:16720ms step_avg:57.46ms
step:292/2330 train_time:16780ms step_avg:57.47ms
step:293/2330 train_time:16836ms step_avg:57.46ms
step:294/2330 train_time:16895ms step_avg:57.47ms
step:295/2330 train_time:16951ms step_avg:57.46ms
step:296/2330 train_time:17010ms step_avg:57.46ms
step:297/2330 train_time:17065ms step_avg:57.46ms
step:298/2330 train_time:17124ms step_avg:57.46ms
step:299/2330 train_time:17180ms step_avg:57.46ms
step:300/2330 train_time:17238ms step_avg:57.46ms
step:301/2330 train_time:17294ms step_avg:57.46ms
step:302/2330 train_time:17352ms step_avg:57.46ms
step:303/2330 train_time:17408ms step_avg:57.45ms
step:304/2330 train_time:17467ms step_avg:57.46ms
step:305/2330 train_time:17523ms step_avg:57.45ms
step:306/2330 train_time:17583ms step_avg:57.46ms
step:307/2330 train_time:17640ms step_avg:57.46ms
step:308/2330 train_time:17700ms step_avg:57.47ms
step:309/2330 train_time:17757ms step_avg:57.47ms
step:310/2330 train_time:17816ms step_avg:57.47ms
step:311/2330 train_time:17873ms step_avg:57.47ms
step:312/2330 train_time:17931ms step_avg:57.47ms
step:313/2330 train_time:17987ms step_avg:57.47ms
step:314/2330 train_time:18047ms step_avg:57.47ms
step:315/2330 train_time:18102ms step_avg:57.47ms
step:316/2330 train_time:18161ms step_avg:57.47ms
step:317/2330 train_time:18217ms step_avg:57.47ms
step:318/2330 train_time:18276ms step_avg:57.47ms
step:319/2330 train_time:18332ms step_avg:57.47ms
step:320/2330 train_time:18391ms step_avg:57.47ms
step:321/2330 train_time:18447ms step_avg:57.47ms
step:322/2330 train_time:18506ms step_avg:57.47ms
step:323/2330 train_time:18562ms step_avg:57.47ms
step:324/2330 train_time:18622ms step_avg:57.47ms
step:325/2330 train_time:18678ms step_avg:57.47ms
step:326/2330 train_time:18737ms step_avg:57.48ms
step:327/2330 train_time:18794ms step_avg:57.48ms
step:328/2330 train_time:18853ms step_avg:57.48ms
step:329/2330 train_time:18909ms step_avg:57.47ms
step:330/2330 train_time:18969ms step_avg:57.48ms
step:331/2330 train_time:19024ms step_avg:57.47ms
step:332/2330 train_time:19084ms step_avg:57.48ms
step:333/2330 train_time:19140ms step_avg:57.48ms
step:334/2330 train_time:19198ms step_avg:57.48ms
step:335/2330 train_time:19254ms step_avg:57.48ms
step:336/2330 train_time:19314ms step_avg:57.48ms
step:337/2330 train_time:19369ms step_avg:57.48ms
step:338/2330 train_time:19429ms step_avg:57.48ms
step:339/2330 train_time:19485ms step_avg:57.48ms
step:340/2330 train_time:19545ms step_avg:57.49ms
step:341/2330 train_time:19601ms step_avg:57.48ms
step:342/2330 train_time:19660ms step_avg:57.49ms
step:343/2330 train_time:19716ms step_avg:57.48ms
step:344/2330 train_time:19775ms step_avg:57.49ms
step:345/2330 train_time:19831ms step_avg:57.48ms
step:346/2330 train_time:19892ms step_avg:57.49ms
step:347/2330 train_time:19948ms step_avg:57.49ms
step:348/2330 train_time:20006ms step_avg:57.49ms
step:349/2330 train_time:20062ms step_avg:57.48ms
step:350/2330 train_time:20121ms step_avg:57.49ms
step:351/2330 train_time:20177ms step_avg:57.48ms
step:352/2330 train_time:20236ms step_avg:57.49ms
step:353/2330 train_time:20292ms step_avg:57.48ms
step:354/2330 train_time:20351ms step_avg:57.49ms
step:355/2330 train_time:20406ms step_avg:57.48ms
step:356/2330 train_time:20467ms step_avg:57.49ms
step:357/2330 train_time:20523ms step_avg:57.49ms
step:358/2330 train_time:20583ms step_avg:57.49ms
step:359/2330 train_time:20638ms step_avg:57.49ms
step:360/2330 train_time:20697ms step_avg:57.49ms
step:361/2330 train_time:20754ms step_avg:57.49ms
step:362/2330 train_time:20813ms step_avg:57.49ms
step:363/2330 train_time:20870ms step_avg:57.49ms
step:364/2330 train_time:20928ms step_avg:57.49ms
step:365/2330 train_time:20984ms step_avg:57.49ms
step:366/2330 train_time:21043ms step_avg:57.50ms
step:367/2330 train_time:21100ms step_avg:57.49ms
step:368/2330 train_time:21159ms step_avg:57.50ms
step:369/2330 train_time:21215ms step_avg:57.49ms
step:370/2330 train_time:21274ms step_avg:57.50ms
step:371/2330 train_time:21329ms step_avg:57.49ms
step:372/2330 train_time:21389ms step_avg:57.50ms
step:373/2330 train_time:21445ms step_avg:57.49ms
step:374/2330 train_time:21505ms step_avg:57.50ms
step:375/2330 train_time:21560ms step_avg:57.49ms
step:376/2330 train_time:21620ms step_avg:57.50ms
step:377/2330 train_time:21676ms step_avg:57.50ms
step:378/2330 train_time:21735ms step_avg:57.50ms
step:379/2330 train_time:21791ms step_avg:57.50ms
step:380/2330 train_time:21850ms step_avg:57.50ms
step:381/2330 train_time:21906ms step_avg:57.50ms
step:382/2330 train_time:21966ms step_avg:57.50ms
step:383/2330 train_time:22022ms step_avg:57.50ms
step:384/2330 train_time:22081ms step_avg:57.50ms
step:385/2330 train_time:22137ms step_avg:57.50ms
step:386/2330 train_time:22195ms step_avg:57.50ms
step:387/2330 train_time:22251ms step_avg:57.50ms
step:388/2330 train_time:22311ms step_avg:57.50ms
step:389/2330 train_time:22368ms step_avg:57.50ms
step:390/2330 train_time:22426ms step_avg:57.50ms
step:391/2330 train_time:22482ms step_avg:57.50ms
step:392/2330 train_time:22541ms step_avg:57.50ms
step:393/2330 train_time:22598ms step_avg:57.50ms
step:394/2330 train_time:22656ms step_avg:57.50ms
step:395/2330 train_time:22713ms step_avg:57.50ms
step:396/2330 train_time:22772ms step_avg:57.51ms
step:397/2330 train_time:22829ms step_avg:57.50ms
step:398/2330 train_time:22888ms step_avg:57.51ms
step:399/2330 train_time:22944ms step_avg:57.50ms
step:400/2330 train_time:23003ms step_avg:57.51ms
step:401/2330 train_time:23060ms step_avg:57.51ms
step:402/2330 train_time:23120ms step_avg:57.51ms
step:403/2330 train_time:23176ms step_avg:57.51ms
step:404/2330 train_time:23235ms step_avg:57.51ms
step:405/2330 train_time:23292ms step_avg:57.51ms
step:406/2330 train_time:23350ms step_avg:57.51ms
step:407/2330 train_time:23407ms step_avg:57.51ms
step:408/2330 train_time:23466ms step_avg:57.52ms
step:409/2330 train_time:23522ms step_avg:57.51ms
step:410/2330 train_time:23581ms step_avg:57.51ms
step:411/2330 train_time:23637ms step_avg:57.51ms
step:412/2330 train_time:23696ms step_avg:57.52ms
step:413/2330 train_time:23752ms step_avg:57.51ms
step:414/2330 train_time:23812ms step_avg:57.52ms
step:415/2330 train_time:23868ms step_avg:57.51ms
step:416/2330 train_time:23928ms step_avg:57.52ms
step:417/2330 train_time:23983ms step_avg:57.51ms
step:418/2330 train_time:24043ms step_avg:57.52ms
step:419/2330 train_time:24100ms step_avg:57.52ms
step:420/2330 train_time:24159ms step_avg:57.52ms
step:421/2330 train_time:24216ms step_avg:57.52ms
step:422/2330 train_time:24274ms step_avg:57.52ms
step:423/2330 train_time:24329ms step_avg:57.52ms
step:424/2330 train_time:24389ms step_avg:57.52ms
step:425/2330 train_time:24444ms step_avg:57.52ms
step:426/2330 train_time:24505ms step_avg:57.52ms
step:427/2330 train_time:24561ms step_avg:57.52ms
step:428/2330 train_time:24619ms step_avg:57.52ms
step:429/2330 train_time:24675ms step_avg:57.52ms
step:430/2330 train_time:24734ms step_avg:57.52ms
step:431/2330 train_time:24790ms step_avg:57.52ms
step:432/2330 train_time:24849ms step_avg:57.52ms
step:433/2330 train_time:24905ms step_avg:57.52ms
step:434/2330 train_time:24965ms step_avg:57.52ms
step:435/2330 train_time:25021ms step_avg:57.52ms
step:436/2330 train_time:25080ms step_avg:57.52ms
step:437/2330 train_time:25136ms step_avg:57.52ms
step:438/2330 train_time:25195ms step_avg:57.52ms
step:439/2330 train_time:25251ms step_avg:57.52ms
step:440/2330 train_time:25310ms step_avg:57.52ms
step:441/2330 train_time:25366ms step_avg:57.52ms
step:442/2330 train_time:25427ms step_avg:57.53ms
step:443/2330 train_time:25483ms step_avg:57.52ms
step:444/2330 train_time:25542ms step_avg:57.53ms
step:445/2330 train_time:25598ms step_avg:57.52ms
step:446/2330 train_time:25657ms step_avg:57.53ms
step:447/2330 train_time:25713ms step_avg:57.52ms
step:448/2330 train_time:25773ms step_avg:57.53ms
step:449/2330 train_time:25829ms step_avg:57.53ms
step:450/2330 train_time:25889ms step_avg:57.53ms
step:451/2330 train_time:25944ms step_avg:57.53ms
step:452/2330 train_time:26004ms step_avg:57.53ms
step:453/2330 train_time:26060ms step_avg:57.53ms
step:454/2330 train_time:26119ms step_avg:57.53ms
step:455/2330 train_time:26176ms step_avg:57.53ms
step:456/2330 train_time:26234ms step_avg:57.53ms
step:457/2330 train_time:26290ms step_avg:57.53ms
step:458/2330 train_time:26349ms step_avg:57.53ms
step:459/2330 train_time:26405ms step_avg:57.53ms
step:460/2330 train_time:26465ms step_avg:57.53ms
step:461/2330 train_time:26521ms step_avg:57.53ms
step:462/2330 train_time:26580ms step_avg:57.53ms
step:463/2330 train_time:26636ms step_avg:57.53ms
step:464/2330 train_time:26695ms step_avg:57.53ms
step:465/2330 train_time:26751ms step_avg:57.53ms
step:466/2330 train_time:26810ms step_avg:57.53ms
step:467/2330 train_time:26866ms step_avg:57.53ms
step:468/2330 train_time:26925ms step_avg:57.53ms
step:469/2330 train_time:26981ms step_avg:57.53ms
step:470/2330 train_time:27040ms step_avg:57.53ms
step:471/2330 train_time:27096ms step_avg:57.53ms
step:472/2330 train_time:27155ms step_avg:57.53ms
step:473/2330 train_time:27211ms step_avg:57.53ms
step:474/2330 train_time:27270ms step_avg:57.53ms
step:475/2330 train_time:27326ms step_avg:57.53ms
step:476/2330 train_time:27385ms step_avg:57.53ms
step:477/2330 train_time:27441ms step_avg:57.53ms
step:478/2330 train_time:27500ms step_avg:57.53ms
step:479/2330 train_time:27556ms step_avg:57.53ms
step:480/2330 train_time:27615ms step_avg:57.53ms
step:481/2330 train_time:27672ms step_avg:57.53ms
step:482/2330 train_time:27730ms step_avg:57.53ms
step:483/2330 train_time:27786ms step_avg:57.53ms
step:484/2330 train_time:27846ms step_avg:57.53ms
step:485/2330 train_time:27903ms step_avg:57.53ms
step:486/2330 train_time:27961ms step_avg:57.53ms
step:487/2330 train_time:28017ms step_avg:57.53ms
step:488/2330 train_time:28076ms step_avg:57.53ms
step:489/2330 train_time:28132ms step_avg:57.53ms
step:490/2330 train_time:28191ms step_avg:57.53ms
step:491/2330 train_time:28247ms step_avg:57.53ms
step:492/2330 train_time:28307ms step_avg:57.54ms
step:493/2330 train_time:28363ms step_avg:57.53ms
step:494/2330 train_time:28422ms step_avg:57.54ms
step:495/2330 train_time:28479ms step_avg:57.53ms
step:496/2330 train_time:28537ms step_avg:57.54ms
step:497/2330 train_time:28594ms step_avg:57.53ms
step:498/2330 train_time:28653ms step_avg:57.54ms
step:499/2330 train_time:28709ms step_avg:57.53ms
step:500/2330 train_time:28768ms step_avg:57.54ms
step:500/2330 val_loss:4.4308 train_time:28847ms step_avg:57.69ms
step:501/2330 train_time:28866ms step_avg:57.62ms
step:502/2330 train_time:28886ms step_avg:57.54ms
step:503/2330 train_time:28942ms step_avg:57.54ms
step:504/2330 train_time:29009ms step_avg:57.56ms
step:505/2330 train_time:29065ms step_avg:57.56ms
step:506/2330 train_time:29126ms step_avg:57.56ms
step:507/2330 train_time:29183ms step_avg:57.56ms
step:508/2330 train_time:29242ms step_avg:57.56ms
step:509/2330 train_time:29298ms step_avg:57.56ms
step:510/2330 train_time:29357ms step_avg:57.56ms
step:511/2330 train_time:29412ms step_avg:57.56ms
step:512/2330 train_time:29471ms step_avg:57.56ms
step:513/2330 train_time:29526ms step_avg:57.56ms
step:514/2330 train_time:29585ms step_avg:57.56ms
step:515/2330 train_time:29640ms step_avg:57.55ms
step:516/2330 train_time:29699ms step_avg:57.56ms
step:517/2330 train_time:29755ms step_avg:57.55ms
step:518/2330 train_time:29813ms step_avg:57.55ms
step:519/2330 train_time:29868ms step_avg:57.55ms
step:520/2330 train_time:29931ms step_avg:57.56ms
step:521/2330 train_time:29988ms step_avg:57.56ms
step:522/2330 train_time:30051ms step_avg:57.57ms
step:523/2330 train_time:30107ms step_avg:57.57ms
step:524/2330 train_time:30168ms step_avg:57.57ms
step:525/2330 train_time:30223ms step_avg:57.57ms
step:526/2330 train_time:30284ms step_avg:57.57ms
step:527/2330 train_time:30339ms step_avg:57.57ms
step:528/2330 train_time:30398ms step_avg:57.57ms
step:529/2330 train_time:30453ms step_avg:57.57ms
step:530/2330 train_time:30512ms step_avg:57.57ms
step:531/2330 train_time:30567ms step_avg:57.56ms
step:532/2330 train_time:30627ms step_avg:57.57ms
step:533/2330 train_time:30682ms step_avg:57.57ms
step:534/2330 train_time:30741ms step_avg:57.57ms
step:535/2330 train_time:30797ms step_avg:57.56ms
step:536/2330 train_time:30856ms step_avg:57.57ms
step:537/2330 train_time:30913ms step_avg:57.57ms
step:538/2330 train_time:30972ms step_avg:57.57ms
step:539/2330 train_time:31029ms step_avg:57.57ms
step:540/2330 train_time:31090ms step_avg:57.57ms
step:541/2330 train_time:31146ms step_avg:57.57ms
step:542/2330 train_time:31207ms step_avg:57.58ms
step:543/2330 train_time:31263ms step_avg:57.57ms
step:544/2330 train_time:31323ms step_avg:57.58ms
step:545/2330 train_time:31378ms step_avg:57.57ms
step:546/2330 train_time:31437ms step_avg:57.58ms
step:547/2330 train_time:31493ms step_avg:57.57ms
step:548/2330 train_time:31552ms step_avg:57.58ms
step:549/2330 train_time:31608ms step_avg:57.57ms
step:550/2330 train_time:31667ms step_avg:57.58ms
step:551/2330 train_time:31723ms step_avg:57.57ms
step:552/2330 train_time:31782ms step_avg:57.58ms
step:553/2330 train_time:31839ms step_avg:57.57ms
step:554/2330 train_time:31898ms step_avg:57.58ms
step:555/2330 train_time:31955ms step_avg:57.58ms
step:556/2330 train_time:32013ms step_avg:57.58ms
step:557/2330 train_time:32070ms step_avg:57.58ms
step:558/2330 train_time:32130ms step_avg:57.58ms
step:559/2330 train_time:32185ms step_avg:57.58ms
step:560/2330 train_time:32246ms step_avg:57.58ms
step:561/2330 train_time:32302ms step_avg:57.58ms
step:562/2330 train_time:32361ms step_avg:57.58ms
step:563/2330 train_time:32417ms step_avg:57.58ms
step:564/2330 train_time:32476ms step_avg:57.58ms
step:565/2330 train_time:32532ms step_avg:57.58ms
step:566/2330 train_time:32592ms step_avg:57.58ms
step:567/2330 train_time:32647ms step_avg:57.58ms
step:568/2330 train_time:32707ms step_avg:57.58ms
step:569/2330 train_time:32762ms step_avg:57.58ms
step:570/2330 train_time:32823ms step_avg:57.58ms
step:571/2330 train_time:32879ms step_avg:57.58ms
step:572/2330 train_time:32938ms step_avg:57.58ms
step:573/2330 train_time:32995ms step_avg:57.58ms
step:574/2330 train_time:33055ms step_avg:57.59ms
step:575/2330 train_time:33112ms step_avg:57.59ms
step:576/2330 train_time:33171ms step_avg:57.59ms
step:577/2330 train_time:33227ms step_avg:57.59ms
step:578/2330 train_time:33287ms step_avg:57.59ms
step:579/2330 train_time:33342ms step_avg:57.59ms
step:580/2330 train_time:33402ms step_avg:57.59ms
step:581/2330 train_time:33458ms step_avg:57.59ms
step:582/2330 train_time:33517ms step_avg:57.59ms
step:583/2330 train_time:33573ms step_avg:57.59ms
step:584/2330 train_time:33633ms step_avg:57.59ms
step:585/2330 train_time:33688ms step_avg:57.59ms
step:586/2330 train_time:33747ms step_avg:57.59ms
step:587/2330 train_time:33802ms step_avg:57.58ms
step:588/2330 train_time:33863ms step_avg:57.59ms
step:589/2330 train_time:33920ms step_avg:57.59ms
step:590/2330 train_time:33979ms step_avg:57.59ms
step:591/2330 train_time:34035ms step_avg:57.59ms
step:592/2330 train_time:34094ms step_avg:57.59ms
step:593/2330 train_time:34150ms step_avg:57.59ms
step:594/2330 train_time:34210ms step_avg:57.59ms
step:595/2330 train_time:34265ms step_avg:57.59ms
step:596/2330 train_time:34326ms step_avg:57.59ms
step:597/2330 train_time:34381ms step_avg:57.59ms
step:598/2330 train_time:34440ms step_avg:57.59ms
step:599/2330 train_time:34497ms step_avg:57.59ms
step:600/2330 train_time:34556ms step_avg:57.59ms
step:601/2330 train_time:34612ms step_avg:57.59ms
step:602/2330 train_time:34671ms step_avg:57.59ms
step:603/2330 train_time:34727ms step_avg:57.59ms
step:604/2330 train_time:34787ms step_avg:57.59ms
step:605/2330 train_time:34843ms step_avg:57.59ms
step:606/2330 train_time:34902ms step_avg:57.59ms
step:607/2330 train_time:34958ms step_avg:57.59ms
step:608/2330 train_time:35017ms step_avg:57.59ms
step:609/2330 train_time:35073ms step_avg:57.59ms
step:610/2330 train_time:35133ms step_avg:57.59ms
step:611/2330 train_time:35189ms step_avg:57.59ms
step:612/2330 train_time:35250ms step_avg:57.60ms
step:613/2330 train_time:35305ms step_avg:57.59ms
step:614/2330 train_time:35365ms step_avg:57.60ms
step:615/2330 train_time:35421ms step_avg:57.60ms
step:616/2330 train_time:35480ms step_avg:57.60ms
step:617/2330 train_time:35536ms step_avg:57.59ms
step:618/2330 train_time:35595ms step_avg:57.60ms
step:619/2330 train_time:35651ms step_avg:57.59ms
step:620/2330 train_time:35710ms step_avg:57.60ms
step:621/2330 train_time:35765ms step_avg:57.59ms
step:622/2330 train_time:35826ms step_avg:57.60ms
step:623/2330 train_time:35882ms step_avg:57.59ms
step:624/2330 train_time:35942ms step_avg:57.60ms
step:625/2330 train_time:35999ms step_avg:57.60ms
step:626/2330 train_time:36058ms step_avg:57.60ms
step:627/2330 train_time:36114ms step_avg:57.60ms
step:628/2330 train_time:36173ms step_avg:57.60ms
step:629/2330 train_time:36228ms step_avg:57.60ms
step:630/2330 train_time:36289ms step_avg:57.60ms
step:631/2330 train_time:36345ms step_avg:57.60ms
step:632/2330 train_time:36405ms step_avg:57.60ms
step:633/2330 train_time:36461ms step_avg:57.60ms
step:634/2330 train_time:36521ms step_avg:57.60ms
step:635/2330 train_time:36577ms step_avg:57.60ms
step:636/2330 train_time:36636ms step_avg:57.60ms
step:637/2330 train_time:36691ms step_avg:57.60ms
step:638/2330 train_time:36751ms step_avg:57.60ms
step:639/2330 train_time:36807ms step_avg:57.60ms
step:640/2330 train_time:36866ms step_avg:57.60ms
step:641/2330 train_time:36922ms step_avg:57.60ms
step:642/2330 train_time:36982ms step_avg:57.61ms
step:643/2330 train_time:37039ms step_avg:57.60ms
step:644/2330 train_time:37098ms step_avg:57.61ms
step:645/2330 train_time:37155ms step_avg:57.60ms
step:646/2330 train_time:37214ms step_avg:57.61ms
step:647/2330 train_time:37270ms step_avg:57.60ms
step:648/2330 train_time:37329ms step_avg:57.61ms
step:649/2330 train_time:37386ms step_avg:57.61ms
step:650/2330 train_time:37446ms step_avg:57.61ms
step:651/2330 train_time:37501ms step_avg:57.61ms
step:652/2330 train_time:37561ms step_avg:57.61ms
step:653/2330 train_time:37617ms step_avg:57.61ms
step:654/2330 train_time:37676ms step_avg:57.61ms
step:655/2330 train_time:37732ms step_avg:57.61ms
step:656/2330 train_time:37791ms step_avg:57.61ms
step:657/2330 train_time:37847ms step_avg:57.61ms
step:658/2330 train_time:37907ms step_avg:57.61ms
step:659/2330 train_time:37962ms step_avg:57.61ms
step:660/2330 train_time:38022ms step_avg:57.61ms
step:661/2330 train_time:38078ms step_avg:57.61ms
step:662/2330 train_time:38137ms step_avg:57.61ms
step:663/2330 train_time:38194ms step_avg:57.61ms
step:664/2330 train_time:38253ms step_avg:57.61ms
step:665/2330 train_time:38309ms step_avg:57.61ms
step:666/2330 train_time:38369ms step_avg:57.61ms
step:667/2330 train_time:38425ms step_avg:57.61ms
step:668/2330 train_time:38486ms step_avg:57.61ms
step:669/2330 train_time:38541ms step_avg:57.61ms
step:670/2330 train_time:38600ms step_avg:57.61ms
step:671/2330 train_time:38656ms step_avg:57.61ms
step:672/2330 train_time:38715ms step_avg:57.61ms
step:673/2330 train_time:38771ms step_avg:57.61ms
step:674/2330 train_time:38831ms step_avg:57.61ms
step:675/2330 train_time:38886ms step_avg:57.61ms
step:676/2330 train_time:38948ms step_avg:57.61ms
step:677/2330 train_time:39003ms step_avg:57.61ms
step:678/2330 train_time:39062ms step_avg:57.61ms
step:679/2330 train_time:39119ms step_avg:57.61ms
step:680/2330 train_time:39178ms step_avg:57.62ms
step:681/2330 train_time:39235ms step_avg:57.61ms
step:682/2330 train_time:39294ms step_avg:57.62ms
step:683/2330 train_time:39350ms step_avg:57.61ms
step:684/2330 train_time:39410ms step_avg:57.62ms
step:685/2330 train_time:39466ms step_avg:57.61ms
step:686/2330 train_time:39527ms step_avg:57.62ms
step:687/2330 train_time:39583ms step_avg:57.62ms
step:688/2330 train_time:39643ms step_avg:57.62ms
step:689/2330 train_time:39700ms step_avg:57.62ms
step:690/2330 train_time:39759ms step_avg:57.62ms
step:691/2330 train_time:39815ms step_avg:57.62ms
step:692/2330 train_time:39874ms step_avg:57.62ms
step:693/2330 train_time:39929ms step_avg:57.62ms
step:694/2330 train_time:39989ms step_avg:57.62ms
step:695/2330 train_time:40045ms step_avg:57.62ms
step:696/2330 train_time:40105ms step_avg:57.62ms
step:697/2330 train_time:40162ms step_avg:57.62ms
step:698/2330 train_time:40221ms step_avg:57.62ms
step:699/2330 train_time:40278ms step_avg:57.62ms
step:700/2330 train_time:40337ms step_avg:57.62ms
step:701/2330 train_time:40393ms step_avg:57.62ms
step:702/2330 train_time:40453ms step_avg:57.62ms
step:703/2330 train_time:40509ms step_avg:57.62ms
step:704/2330 train_time:40569ms step_avg:57.63ms
step:705/2330 train_time:40624ms step_avg:57.62ms
step:706/2330 train_time:40684ms step_avg:57.63ms
step:707/2330 train_time:40740ms step_avg:57.62ms
step:708/2330 train_time:40800ms step_avg:57.63ms
step:709/2330 train_time:40856ms step_avg:57.63ms
step:710/2330 train_time:40915ms step_avg:57.63ms
step:711/2330 train_time:40971ms step_avg:57.62ms
step:712/2330 train_time:41030ms step_avg:57.63ms
step:713/2330 train_time:41086ms step_avg:57.62ms
step:714/2330 train_time:41147ms step_avg:57.63ms
step:715/2330 train_time:41202ms step_avg:57.63ms
step:716/2330 train_time:41261ms step_avg:57.63ms
step:717/2330 train_time:41317ms step_avg:57.62ms
step:718/2330 train_time:41376ms step_avg:57.63ms
step:719/2330 train_time:41434ms step_avg:57.63ms
step:720/2330 train_time:41492ms step_avg:57.63ms
step:721/2330 train_time:41548ms step_avg:57.63ms
step:722/2330 train_time:41608ms step_avg:57.63ms
step:723/2330 train_time:41663ms step_avg:57.63ms
step:724/2330 train_time:41724ms step_avg:57.63ms
step:725/2330 train_time:41780ms step_avg:57.63ms
step:726/2330 train_time:41840ms step_avg:57.63ms
step:727/2330 train_time:41896ms step_avg:57.63ms
step:728/2330 train_time:41955ms step_avg:57.63ms
step:729/2330 train_time:42011ms step_avg:57.63ms
step:730/2330 train_time:42071ms step_avg:57.63ms
step:731/2330 train_time:42127ms step_avg:57.63ms
step:732/2330 train_time:42188ms step_avg:57.63ms
step:733/2330 train_time:42243ms step_avg:57.63ms
step:734/2330 train_time:42303ms step_avg:57.63ms
step:735/2330 train_time:42360ms step_avg:57.63ms
step:736/2330 train_time:42419ms step_avg:57.64ms
step:737/2330 train_time:42476ms step_avg:57.63ms
step:738/2330 train_time:42535ms step_avg:57.63ms
step:739/2330 train_time:42590ms step_avg:57.63ms
step:740/2330 train_time:42651ms step_avg:57.64ms
step:741/2330 train_time:42707ms step_avg:57.63ms
step:742/2330 train_time:42767ms step_avg:57.64ms
step:743/2330 train_time:42823ms step_avg:57.63ms
step:744/2330 train_time:42882ms step_avg:57.64ms
step:745/2330 train_time:42939ms step_avg:57.64ms
step:746/2330 train_time:42998ms step_avg:57.64ms
step:747/2330 train_time:43054ms step_avg:57.64ms
step:748/2330 train_time:43113ms step_avg:57.64ms
step:749/2330 train_time:43169ms step_avg:57.64ms
step:750/2330 train_time:43229ms step_avg:57.64ms
step:750/2330 val_loss:4.2192 train_time:43309ms step_avg:57.74ms
step:751/2330 train_time:43326ms step_avg:57.69ms
step:752/2330 train_time:43347ms step_avg:57.64ms
step:753/2330 train_time:43405ms step_avg:57.64ms
step:754/2330 train_time:43468ms step_avg:57.65ms
step:755/2330 train_time:43524ms step_avg:57.65ms
step:756/2330 train_time:43584ms step_avg:57.65ms
step:757/2330 train_time:43640ms step_avg:57.65ms
step:758/2330 train_time:43699ms step_avg:57.65ms
step:759/2330 train_time:43755ms step_avg:57.65ms
step:760/2330 train_time:43813ms step_avg:57.65ms
step:761/2330 train_time:43868ms step_avg:57.65ms
step:762/2330 train_time:43927ms step_avg:57.65ms
step:763/2330 train_time:43983ms step_avg:57.64ms
step:764/2330 train_time:44042ms step_avg:57.65ms
step:765/2330 train_time:44098ms step_avg:57.64ms
step:766/2330 train_time:44157ms step_avg:57.65ms
step:767/2330 train_time:44212ms step_avg:57.64ms
step:768/2330 train_time:44273ms step_avg:57.65ms
step:769/2330 train_time:44331ms step_avg:57.65ms
step:770/2330 train_time:44394ms step_avg:57.65ms
step:771/2330 train_time:44451ms step_avg:57.65ms
step:772/2330 train_time:44513ms step_avg:57.66ms
step:773/2330 train_time:44569ms step_avg:57.66ms
step:774/2330 train_time:44631ms step_avg:57.66ms
step:775/2330 train_time:44687ms step_avg:57.66ms
step:776/2330 train_time:44748ms step_avg:57.66ms
step:777/2330 train_time:44804ms step_avg:57.66ms
step:778/2330 train_time:44864ms step_avg:57.67ms
step:779/2330 train_time:44920ms step_avg:57.66ms
step:780/2330 train_time:44980ms step_avg:57.67ms
step:781/2330 train_time:45036ms step_avg:57.66ms
step:782/2330 train_time:45096ms step_avg:57.67ms
step:783/2330 train_time:45152ms step_avg:57.67ms
step:784/2330 train_time:45212ms step_avg:57.67ms
step:785/2330 train_time:45269ms step_avg:57.67ms
step:786/2330 train_time:45330ms step_avg:57.67ms
step:787/2330 train_time:45386ms step_avg:57.67ms
step:788/2330 train_time:45450ms step_avg:57.68ms
step:789/2330 train_time:45505ms step_avg:57.67ms
step:790/2330 train_time:45568ms step_avg:57.68ms
step:791/2330 train_time:45625ms step_avg:57.68ms
step:792/2330 train_time:45686ms step_avg:57.68ms
step:793/2330 train_time:45743ms step_avg:57.68ms
step:794/2330 train_time:45802ms step_avg:57.68ms
step:795/2330 train_time:45858ms step_avg:57.68ms
step:796/2330 train_time:45918ms step_avg:57.69ms
step:797/2330 train_time:45975ms step_avg:57.69ms
step:798/2330 train_time:46034ms step_avg:57.69ms
step:799/2330 train_time:46090ms step_avg:57.69ms
step:800/2330 train_time:46150ms step_avg:57.69ms
step:801/2330 train_time:46206ms step_avg:57.69ms
step:802/2330 train_time:46267ms step_avg:57.69ms
step:803/2330 train_time:46324ms step_avg:57.69ms
step:804/2330 train_time:46384ms step_avg:57.69ms
step:805/2330 train_time:46441ms step_avg:57.69ms
step:806/2330 train_time:46502ms step_avg:57.69ms
step:807/2330 train_time:46559ms step_avg:57.69ms
step:808/2330 train_time:46619ms step_avg:57.70ms
step:809/2330 train_time:46676ms step_avg:57.70ms
step:810/2330 train_time:46737ms step_avg:57.70ms
step:811/2330 train_time:46793ms step_avg:57.70ms
step:812/2330 train_time:46854ms step_avg:57.70ms
step:813/2330 train_time:46910ms step_avg:57.70ms
step:814/2330 train_time:46972ms step_avg:57.70ms
step:815/2330 train_time:47029ms step_avg:57.70ms
step:816/2330 train_time:47088ms step_avg:57.71ms
step:817/2330 train_time:47144ms step_avg:57.70ms
step:818/2330 train_time:47206ms step_avg:57.71ms
step:819/2330 train_time:47262ms step_avg:57.71ms
step:820/2330 train_time:47323ms step_avg:57.71ms
step:821/2330 train_time:47380ms step_avg:57.71ms
step:822/2330 train_time:47440ms step_avg:57.71ms
step:823/2330 train_time:47497ms step_avg:57.71ms
step:824/2330 train_time:47558ms step_avg:57.72ms
step:825/2330 train_time:47614ms step_avg:57.71ms
step:826/2330 train_time:47676ms step_avg:57.72ms
step:827/2330 train_time:47733ms step_avg:57.72ms
step:828/2330 train_time:47793ms step_avg:57.72ms
step:829/2330 train_time:47850ms step_avg:57.72ms
step:830/2330 train_time:47911ms step_avg:57.72ms
step:831/2330 train_time:47968ms step_avg:57.72ms
step:832/2330 train_time:48028ms step_avg:57.73ms
step:833/2330 train_time:48084ms step_avg:57.72ms
step:834/2330 train_time:48144ms step_avg:57.73ms
step:835/2330 train_time:48200ms step_avg:57.72ms
step:836/2330 train_time:48261ms step_avg:57.73ms
step:837/2330 train_time:48317ms step_avg:57.73ms
step:838/2330 train_time:48377ms step_avg:57.73ms
step:839/2330 train_time:48434ms step_avg:57.73ms
step:840/2330 train_time:48495ms step_avg:57.73ms
step:841/2330 train_time:48552ms step_avg:57.73ms
step:842/2330 train_time:48613ms step_avg:57.73ms
step:843/2330 train_time:48669ms step_avg:57.73ms
step:844/2330 train_time:48731ms step_avg:57.74ms
step:845/2330 train_time:48787ms step_avg:57.74ms
step:846/2330 train_time:48847ms step_avg:57.74ms
step:847/2330 train_time:48904ms step_avg:57.74ms
step:848/2330 train_time:48964ms step_avg:57.74ms
step:849/2330 train_time:49020ms step_avg:57.74ms
step:850/2330 train_time:49080ms step_avg:57.74ms
step:851/2330 train_time:49137ms step_avg:57.74ms
step:852/2330 train_time:49197ms step_avg:57.74ms
step:853/2330 train_time:49254ms step_avg:57.74ms
step:854/2330 train_time:49314ms step_avg:57.74ms
step:855/2330 train_time:49370ms step_avg:57.74ms
step:856/2330 train_time:49431ms step_avg:57.75ms
step:857/2330 train_time:49488ms step_avg:57.75ms
step:858/2330 train_time:49550ms step_avg:57.75ms
step:859/2330 train_time:49606ms step_avg:57.75ms
step:860/2330 train_time:49667ms step_avg:57.75ms
step:861/2330 train_time:49724ms step_avg:57.75ms
step:862/2330 train_time:49783ms step_avg:57.75ms
step:863/2330 train_time:49840ms step_avg:57.75ms
step:864/2330 train_time:49900ms step_avg:57.75ms
step:865/2330 train_time:49956ms step_avg:57.75ms
step:866/2330 train_time:50016ms step_avg:57.76ms
step:867/2330 train_time:50073ms step_avg:57.75ms
step:868/2330 train_time:50133ms step_avg:57.76ms
step:869/2330 train_time:50190ms step_avg:57.76ms
step:870/2330 train_time:50251ms step_avg:57.76ms
step:871/2330 train_time:50308ms step_avg:57.76ms
step:872/2330 train_time:50369ms step_avg:57.76ms
step:873/2330 train_time:50426ms step_avg:57.76ms
step:874/2330 train_time:50486ms step_avg:57.76ms
step:875/2330 train_time:50543ms step_avg:57.76ms
step:876/2330 train_time:50603ms step_avg:57.77ms
step:877/2330 train_time:50660ms step_avg:57.77ms
step:878/2330 train_time:50720ms step_avg:57.77ms
step:879/2330 train_time:50777ms step_avg:57.77ms
step:880/2330 train_time:50836ms step_avg:57.77ms
step:881/2330 train_time:50893ms step_avg:57.77ms
step:882/2330 train_time:50954ms step_avg:57.77ms
step:883/2330 train_time:51010ms step_avg:57.77ms
step:884/2330 train_time:51071ms step_avg:57.77ms
step:885/2330 train_time:51128ms step_avg:57.77ms
step:886/2330 train_time:51189ms step_avg:57.78ms
step:887/2330 train_time:51245ms step_avg:57.77ms
step:888/2330 train_time:51306ms step_avg:57.78ms
step:889/2330 train_time:51363ms step_avg:57.78ms
step:890/2330 train_time:51423ms step_avg:57.78ms
step:891/2330 train_time:51480ms step_avg:57.78ms
step:892/2330 train_time:51540ms step_avg:57.78ms
step:893/2330 train_time:51597ms step_avg:57.78ms
step:894/2330 train_time:51657ms step_avg:57.78ms
step:895/2330 train_time:51714ms step_avg:57.78ms
step:896/2330 train_time:51774ms step_avg:57.78ms
step:897/2330 train_time:51831ms step_avg:57.78ms
step:898/2330 train_time:51891ms step_avg:57.79ms
step:899/2330 train_time:51948ms step_avg:57.78ms
step:900/2330 train_time:52008ms step_avg:57.79ms
step:901/2330 train_time:52064ms step_avg:57.78ms
step:902/2330 train_time:52125ms step_avg:57.79ms
step:903/2330 train_time:52181ms step_avg:57.79ms
step:904/2330 train_time:52241ms step_avg:57.79ms
step:905/2330 train_time:52298ms step_avg:57.79ms
step:906/2330 train_time:52359ms step_avg:57.79ms
step:907/2330 train_time:52416ms step_avg:57.79ms
step:908/2330 train_time:52476ms step_avg:57.79ms
step:909/2330 train_time:52533ms step_avg:57.79ms
step:910/2330 train_time:52593ms step_avg:57.79ms
step:911/2330 train_time:52649ms step_avg:57.79ms
step:912/2330 train_time:52710ms step_avg:57.80ms
step:913/2330 train_time:52767ms step_avg:57.80ms
step:914/2330 train_time:52829ms step_avg:57.80ms
step:915/2330 train_time:52886ms step_avg:57.80ms
step:916/2330 train_time:52946ms step_avg:57.80ms
step:917/2330 train_time:53003ms step_avg:57.80ms
step:918/2330 train_time:53063ms step_avg:57.80ms
step:919/2330 train_time:53119ms step_avg:57.80ms
step:920/2330 train_time:53179ms step_avg:57.80ms
step:921/2330 train_time:53236ms step_avg:57.80ms
step:922/2330 train_time:53297ms step_avg:57.81ms
step:923/2330 train_time:53354ms step_avg:57.80ms
step:924/2330 train_time:53414ms step_avg:57.81ms
step:925/2330 train_time:53471ms step_avg:57.81ms
step:926/2330 train_time:53532ms step_avg:57.81ms
step:927/2330 train_time:53588ms step_avg:57.81ms
step:928/2330 train_time:53649ms step_avg:57.81ms
step:929/2330 train_time:53706ms step_avg:57.81ms
step:930/2330 train_time:53766ms step_avg:57.81ms
step:931/2330 train_time:53823ms step_avg:57.81ms
step:932/2330 train_time:53882ms step_avg:57.81ms
step:933/2330 train_time:53939ms step_avg:57.81ms
step:934/2330 train_time:53998ms step_avg:57.81ms
step:935/2330 train_time:54055ms step_avg:57.81ms
step:936/2330 train_time:54115ms step_avg:57.82ms
step:937/2330 train_time:54172ms step_avg:57.81ms
step:938/2330 train_time:54232ms step_avg:57.82ms
step:939/2330 train_time:54289ms step_avg:57.82ms
step:940/2330 train_time:54350ms step_avg:57.82ms
step:941/2330 train_time:54405ms step_avg:57.82ms
step:942/2330 train_time:54466ms step_avg:57.82ms
step:943/2330 train_time:54522ms step_avg:57.82ms
step:944/2330 train_time:54583ms step_avg:57.82ms
step:945/2330 train_time:54641ms step_avg:57.82ms
step:946/2330 train_time:54701ms step_avg:57.82ms
step:947/2330 train_time:54758ms step_avg:57.82ms
step:948/2330 train_time:54817ms step_avg:57.82ms
step:949/2330 train_time:54873ms step_avg:57.82ms
step:950/2330 train_time:54934ms step_avg:57.83ms
step:951/2330 train_time:54990ms step_avg:57.82ms
step:952/2330 train_time:55052ms step_avg:57.83ms
step:953/2330 train_time:55109ms step_avg:57.83ms
step:954/2330 train_time:55169ms step_avg:57.83ms
step:955/2330 train_time:55225ms step_avg:57.83ms
step:956/2330 train_time:55286ms step_avg:57.83ms
step:957/2330 train_time:55343ms step_avg:57.83ms
step:958/2330 train_time:55403ms step_avg:57.83ms
step:959/2330 train_time:55460ms step_avg:57.83ms
step:960/2330 train_time:55520ms step_avg:57.83ms
step:961/2330 train_time:55578ms step_avg:57.83ms
step:962/2330 train_time:55637ms step_avg:57.83ms
step:963/2330 train_time:55694ms step_avg:57.83ms
step:964/2330 train_time:55754ms step_avg:57.84ms
step:965/2330 train_time:55811ms step_avg:57.84ms
step:966/2330 train_time:55872ms step_avg:57.84ms
step:967/2330 train_time:55928ms step_avg:57.84ms
step:968/2330 train_time:55990ms step_avg:57.84ms
step:969/2330 train_time:56046ms step_avg:57.84ms
step:970/2330 train_time:56106ms step_avg:57.84ms
step:971/2330 train_time:56163ms step_avg:57.84ms
step:972/2330 train_time:56223ms step_avg:57.84ms
step:973/2330 train_time:56280ms step_avg:57.84ms
step:974/2330 train_time:56340ms step_avg:57.84ms
step:975/2330 train_time:56397ms step_avg:57.84ms
step:976/2330 train_time:56457ms step_avg:57.85ms
step:977/2330 train_time:56514ms step_avg:57.84ms
step:978/2330 train_time:56575ms step_avg:57.85ms
step:979/2330 train_time:56631ms step_avg:57.85ms
step:980/2330 train_time:56691ms step_avg:57.85ms
step:981/2330 train_time:56748ms step_avg:57.85ms
step:982/2330 train_time:56809ms step_avg:57.85ms
step:983/2330 train_time:56866ms step_avg:57.85ms
step:984/2330 train_time:56927ms step_avg:57.85ms
step:985/2330 train_time:56983ms step_avg:57.85ms
step:986/2330 train_time:57043ms step_avg:57.85ms
step:987/2330 train_time:57100ms step_avg:57.85ms
step:988/2330 train_time:57161ms step_avg:57.86ms
step:989/2330 train_time:57218ms step_avg:57.85ms
step:990/2330 train_time:57277ms step_avg:57.86ms
step:991/2330 train_time:57334ms step_avg:57.85ms
step:992/2330 train_time:57395ms step_avg:57.86ms
step:993/2330 train_time:57451ms step_avg:57.86ms
step:994/2330 train_time:57512ms step_avg:57.86ms
step:995/2330 train_time:57569ms step_avg:57.86ms
step:996/2330 train_time:57629ms step_avg:57.86ms
step:997/2330 train_time:57685ms step_avg:57.86ms
step:998/2330 train_time:57746ms step_avg:57.86ms
step:999/2330 train_time:57803ms step_avg:57.86ms
step:1000/2330 train_time:57863ms step_avg:57.86ms
step:1000/2330 val_loss:4.0741 train_time:57944ms step_avg:57.94ms
step:1001/2330 train_time:57962ms step_avg:57.90ms
step:1002/2330 train_time:57981ms step_avg:57.87ms
step:1003/2330 train_time:58037ms step_avg:57.86ms
step:1004/2330 train_time:58102ms step_avg:57.87ms
step:1005/2330 train_time:58158ms step_avg:57.87ms
step:1006/2330 train_time:58222ms step_avg:57.87ms
step:1007/2330 train_time:58278ms step_avg:57.87ms
step:1008/2330 train_time:58338ms step_avg:57.88ms
step:1009/2330 train_time:58394ms step_avg:57.87ms
step:1010/2330 train_time:58454ms step_avg:57.87ms
step:1011/2330 train_time:58510ms step_avg:57.87ms
step:1012/2330 train_time:58569ms step_avg:57.87ms
step:1013/2330 train_time:58625ms step_avg:57.87ms
step:1014/2330 train_time:58685ms step_avg:57.87ms
step:1015/2330 train_time:58741ms step_avg:57.87ms
step:1016/2330 train_time:58800ms step_avg:57.87ms
step:1017/2330 train_time:58859ms step_avg:57.87ms
step:1018/2330 train_time:58922ms step_avg:57.88ms
step:1019/2330 train_time:58980ms step_avg:57.88ms
step:1020/2330 train_time:59040ms step_avg:57.88ms
step:1021/2330 train_time:59097ms step_avg:57.88ms
step:1022/2330 train_time:59157ms step_avg:57.88ms
step:1023/2330 train_time:59213ms step_avg:57.88ms
step:1024/2330 train_time:59275ms step_avg:57.89ms
step:1025/2330 train_time:59332ms step_avg:57.89ms
step:1026/2330 train_time:59392ms step_avg:57.89ms
step:1027/2330 train_time:59449ms step_avg:57.89ms
step:1028/2330 train_time:59508ms step_avg:57.89ms
step:1029/2330 train_time:59565ms step_avg:57.89ms
step:1030/2330 train_time:59624ms step_avg:57.89ms
step:1031/2330 train_time:59680ms step_avg:57.89ms
step:1032/2330 train_time:59740ms step_avg:57.89ms
step:1033/2330 train_time:59797ms step_avg:57.89ms
step:1034/2330 train_time:59857ms step_avg:57.89ms
step:1035/2330 train_time:59915ms step_avg:57.89ms
step:1036/2330 train_time:59977ms step_avg:57.89ms
step:1037/2330 train_time:60035ms step_avg:57.89ms
step:1038/2330 train_time:60094ms step_avg:57.89ms
step:1039/2330 train_time:60153ms step_avg:57.89ms
step:1040/2330 train_time:60213ms step_avg:57.90ms
step:1041/2330 train_time:60270ms step_avg:57.90ms
step:1042/2330 train_time:60329ms step_avg:57.90ms
step:1043/2330 train_time:60386ms step_avg:57.90ms
step:1044/2330 train_time:60446ms step_avg:57.90ms
step:1045/2330 train_time:60502ms step_avg:57.90ms
step:1046/2330 train_time:60562ms step_avg:57.90ms
step:1047/2330 train_time:60619ms step_avg:57.90ms
step:1048/2330 train_time:60679ms step_avg:57.90ms
step:1049/2330 train_time:60736ms step_avg:57.90ms
step:1050/2330 train_time:60795ms step_avg:57.90ms
step:1051/2330 train_time:60853ms step_avg:57.90ms
step:1052/2330 train_time:60913ms step_avg:57.90ms
step:1053/2330 train_time:60972ms step_avg:57.90ms
step:1054/2330 train_time:61032ms step_avg:57.90ms
step:1055/2330 train_time:61089ms step_avg:57.90ms
step:1056/2330 train_time:61149ms step_avg:57.91ms
step:1057/2330 train_time:61205ms step_avg:57.90ms
step:1058/2330 train_time:61267ms step_avg:57.91ms
step:1059/2330 train_time:61323ms step_avg:57.91ms
step:1060/2330 train_time:61384ms step_avg:57.91ms
step:1061/2330 train_time:61440ms step_avg:57.91ms
step:1062/2330 train_time:61500ms step_avg:57.91ms
step:1063/2330 train_time:61557ms step_avg:57.91ms
step:1064/2330 train_time:61616ms step_avg:57.91ms
step:1065/2330 train_time:61672ms step_avg:57.91ms
step:1066/2330 train_time:61732ms step_avg:57.91ms
step:1067/2330 train_time:61789ms step_avg:57.91ms
step:1068/2330 train_time:61849ms step_avg:57.91ms
step:1069/2330 train_time:61906ms step_avg:57.91ms
step:1070/2330 train_time:61967ms step_avg:57.91ms
step:1071/2330 train_time:62025ms step_avg:57.91ms
step:1072/2330 train_time:62085ms step_avg:57.92ms
step:1073/2330 train_time:62142ms step_avg:57.91ms
step:1074/2330 train_time:62202ms step_avg:57.92ms
step:1075/2330 train_time:62259ms step_avg:57.92ms
step:1076/2330 train_time:62320ms step_avg:57.92ms
step:1077/2330 train_time:62377ms step_avg:57.92ms
step:1078/2330 train_time:62437ms step_avg:57.92ms
step:1079/2330 train_time:62493ms step_avg:57.92ms
step:1080/2330 train_time:62553ms step_avg:57.92ms
step:1081/2330 train_time:62610ms step_avg:57.92ms
step:1082/2330 train_time:62669ms step_avg:57.92ms
step:1083/2330 train_time:62726ms step_avg:57.92ms
step:1084/2330 train_time:62787ms step_avg:57.92ms
step:1085/2330 train_time:62844ms step_avg:57.92ms
step:1086/2330 train_time:62905ms step_avg:57.92ms
step:1087/2330 train_time:62962ms step_avg:57.92ms
step:1088/2330 train_time:63022ms step_avg:57.93ms
step:1089/2330 train_time:63079ms step_avg:57.92ms
step:1090/2330 train_time:63139ms step_avg:57.93ms
step:1091/2330 train_time:63196ms step_avg:57.92ms
step:1092/2330 train_time:63256ms step_avg:57.93ms
step:1093/2330 train_time:63312ms step_avg:57.93ms
step:1094/2330 train_time:63374ms step_avg:57.93ms
step:1095/2330 train_time:63431ms step_avg:57.93ms
step:1096/2330 train_time:63491ms step_avg:57.93ms
step:1097/2330 train_time:63548ms step_avg:57.93ms
step:1098/2330 train_time:63608ms step_avg:57.93ms
step:1099/2330 train_time:63665ms step_avg:57.93ms
step:1100/2330 train_time:63725ms step_avg:57.93ms
step:1101/2330 train_time:63782ms step_avg:57.93ms
step:1102/2330 train_time:63842ms step_avg:57.93ms
step:1103/2330 train_time:63898ms step_avg:57.93ms
step:1104/2330 train_time:63960ms step_avg:57.93ms
step:1105/2330 train_time:64016ms step_avg:57.93ms
step:1106/2330 train_time:64076ms step_avg:57.94ms
step:1107/2330 train_time:64133ms step_avg:57.93ms
step:1108/2330 train_time:64194ms step_avg:57.94ms
step:1109/2330 train_time:64250ms step_avg:57.94ms
step:1110/2330 train_time:64310ms step_avg:57.94ms
step:1111/2330 train_time:64367ms step_avg:57.94ms
step:1112/2330 train_time:64427ms step_avg:57.94ms
step:1113/2330 train_time:64484ms step_avg:57.94ms
step:1114/2330 train_time:64545ms step_avg:57.94ms
step:1115/2330 train_time:64602ms step_avg:57.94ms
step:1116/2330 train_time:64662ms step_avg:57.94ms
step:1117/2330 train_time:64719ms step_avg:57.94ms
step:1118/2330 train_time:64779ms step_avg:57.94ms
step:1119/2330 train_time:64836ms step_avg:57.94ms
step:1120/2330 train_time:64895ms step_avg:57.94ms
step:1121/2330 train_time:64953ms step_avg:57.94ms
step:1122/2330 train_time:65013ms step_avg:57.94ms
step:1123/2330 train_time:65070ms step_avg:57.94ms
step:1124/2330 train_time:65130ms step_avg:57.95ms
step:1125/2330 train_time:65188ms step_avg:57.94ms
step:1126/2330 train_time:65248ms step_avg:57.95ms
step:1127/2330 train_time:65305ms step_avg:57.95ms
step:1128/2330 train_time:65366ms step_avg:57.95ms
step:1129/2330 train_time:65423ms step_avg:57.95ms
step:1130/2330 train_time:65484ms step_avg:57.95ms
step:1131/2330 train_time:65541ms step_avg:57.95ms
step:1132/2330 train_time:65601ms step_avg:57.95ms
step:1133/2330 train_time:65658ms step_avg:57.95ms
step:1134/2330 train_time:65719ms step_avg:57.95ms
step:1135/2330 train_time:65776ms step_avg:57.95ms
step:1136/2330 train_time:65836ms step_avg:57.95ms
step:1137/2330 train_time:65893ms step_avg:57.95ms
step:1138/2330 train_time:65953ms step_avg:57.95ms
step:1139/2330 train_time:66010ms step_avg:57.95ms
step:1140/2330 train_time:66070ms step_avg:57.96ms
step:1141/2330 train_time:66128ms step_avg:57.96ms
step:1142/2330 train_time:66187ms step_avg:57.96ms
step:1143/2330 train_time:66244ms step_avg:57.96ms
step:1144/2330 train_time:66304ms step_avg:57.96ms
step:1145/2330 train_time:66361ms step_avg:57.96ms
step:1146/2330 train_time:66422ms step_avg:57.96ms
step:1147/2330 train_time:66479ms step_avg:57.96ms
step:1148/2330 train_time:66539ms step_avg:57.96ms
step:1149/2330 train_time:66596ms step_avg:57.96ms
step:1150/2330 train_time:66656ms step_avg:57.96ms
step:1151/2330 train_time:66713ms step_avg:57.96ms
step:1152/2330 train_time:66773ms step_avg:57.96ms
step:1153/2330 train_time:66831ms step_avg:57.96ms
step:1154/2330 train_time:66890ms step_avg:57.96ms
step:1155/2330 train_time:66947ms step_avg:57.96ms
step:1156/2330 train_time:67007ms step_avg:57.96ms
step:1157/2330 train_time:67064ms step_avg:57.96ms
step:1158/2330 train_time:67124ms step_avg:57.97ms
step:1159/2330 train_time:67181ms step_avg:57.96ms
step:1160/2330 train_time:67241ms step_avg:57.97ms
step:1161/2330 train_time:67298ms step_avg:57.97ms
step:1162/2330 train_time:67358ms step_avg:57.97ms
step:1163/2330 train_time:67416ms step_avg:57.97ms
step:1164/2330 train_time:67476ms step_avg:57.97ms
step:1165/2330 train_time:67534ms step_avg:57.97ms
step:1166/2330 train_time:67593ms step_avg:57.97ms
step:1167/2330 train_time:67650ms step_avg:57.97ms
step:1168/2330 train_time:67710ms step_avg:57.97ms
step:1169/2330 train_time:67767ms step_avg:57.97ms
step:1170/2330 train_time:67827ms step_avg:57.97ms
step:1171/2330 train_time:67884ms step_avg:57.97ms
step:1172/2330 train_time:67944ms step_avg:57.97ms
step:1173/2330 train_time:68001ms step_avg:57.97ms
step:1174/2330 train_time:68062ms step_avg:57.97ms
step:1175/2330 train_time:68118ms step_avg:57.97ms
step:1176/2330 train_time:68179ms step_avg:57.98ms
step:1177/2330 train_time:68236ms step_avg:57.97ms
step:1178/2330 train_time:68296ms step_avg:57.98ms
step:1179/2330 train_time:68353ms step_avg:57.98ms
step:1180/2330 train_time:68413ms step_avg:57.98ms
step:1181/2330 train_time:68470ms step_avg:57.98ms
step:1182/2330 train_time:68529ms step_avg:57.98ms
step:1183/2330 train_time:68586ms step_avg:57.98ms
step:1184/2330 train_time:68647ms step_avg:57.98ms
step:1185/2330 train_time:68703ms step_avg:57.98ms
step:1186/2330 train_time:68766ms step_avg:57.98ms
step:1187/2330 train_time:68822ms step_avg:57.98ms
step:1188/2330 train_time:68883ms step_avg:57.98ms
step:1189/2330 train_time:68939ms step_avg:57.98ms
step:1190/2330 train_time:69000ms step_avg:57.98ms
step:1191/2330 train_time:69057ms step_avg:57.98ms
step:1192/2330 train_time:69116ms step_avg:57.98ms
step:1193/2330 train_time:69173ms step_avg:57.98ms
step:1194/2330 train_time:69234ms step_avg:57.98ms
step:1195/2330 train_time:69290ms step_avg:57.98ms
step:1196/2330 train_time:69351ms step_avg:57.99ms
step:1197/2330 train_time:69407ms step_avg:57.98ms
step:1198/2330 train_time:69467ms step_avg:57.99ms
step:1199/2330 train_time:69525ms step_avg:57.99ms
step:1200/2330 train_time:69585ms step_avg:57.99ms
step:1201/2330 train_time:69641ms step_avg:57.99ms
step:1202/2330 train_time:69703ms step_avg:57.99ms
step:1203/2330 train_time:69760ms step_avg:57.99ms
step:1204/2330 train_time:69820ms step_avg:57.99ms
step:1205/2330 train_time:69877ms step_avg:57.99ms
step:1206/2330 train_time:69937ms step_avg:57.99ms
step:1207/2330 train_time:69994ms step_avg:57.99ms
step:1208/2330 train_time:70054ms step_avg:57.99ms
step:1209/2330 train_time:70111ms step_avg:57.99ms
step:1210/2330 train_time:70171ms step_avg:57.99ms
step:1211/2330 train_time:70228ms step_avg:57.99ms
step:1212/2330 train_time:70289ms step_avg:57.99ms
step:1213/2330 train_time:70345ms step_avg:57.99ms
step:1214/2330 train_time:70406ms step_avg:57.99ms
step:1215/2330 train_time:70463ms step_avg:57.99ms
step:1216/2330 train_time:70523ms step_avg:58.00ms
step:1217/2330 train_time:70580ms step_avg:58.00ms
step:1218/2330 train_time:70642ms step_avg:58.00ms
step:1219/2330 train_time:70698ms step_avg:58.00ms
step:1220/2330 train_time:70758ms step_avg:58.00ms
step:1221/2330 train_time:70815ms step_avg:58.00ms
step:1222/2330 train_time:70875ms step_avg:58.00ms
step:1223/2330 train_time:70932ms step_avg:58.00ms
step:1224/2330 train_time:70992ms step_avg:58.00ms
step:1225/2330 train_time:71049ms step_avg:58.00ms
step:1226/2330 train_time:71108ms step_avg:58.00ms
step:1227/2330 train_time:71165ms step_avg:58.00ms
step:1228/2330 train_time:71226ms step_avg:58.00ms
step:1229/2330 train_time:71283ms step_avg:58.00ms
step:1230/2330 train_time:71343ms step_avg:58.00ms
step:1231/2330 train_time:71400ms step_avg:58.00ms
step:1232/2330 train_time:71460ms step_avg:58.00ms
step:1233/2330 train_time:71517ms step_avg:58.00ms
step:1234/2330 train_time:71577ms step_avg:58.00ms
step:1235/2330 train_time:71634ms step_avg:58.00ms
step:1236/2330 train_time:71694ms step_avg:58.00ms
step:1237/2330 train_time:71751ms step_avg:58.00ms
step:1238/2330 train_time:71811ms step_avg:58.01ms
step:1239/2330 train_time:71867ms step_avg:58.00ms
step:1240/2330 train_time:71929ms step_avg:58.01ms
step:1241/2330 train_time:71985ms step_avg:58.01ms
step:1242/2330 train_time:72047ms step_avg:58.01ms
step:1243/2330 train_time:72104ms step_avg:58.01ms
step:1244/2330 train_time:72164ms step_avg:58.01ms
step:1245/2330 train_time:72221ms step_avg:58.01ms
step:1246/2330 train_time:72282ms step_avg:58.01ms
step:1247/2330 train_time:72338ms step_avg:58.01ms
step:1248/2330 train_time:72399ms step_avg:58.01ms
step:1249/2330 train_time:72456ms step_avg:58.01ms
step:1250/2330 train_time:72516ms step_avg:58.01ms
step:1250/2330 val_loss:3.9899 train_time:72596ms step_avg:58.08ms
step:1251/2330 train_time:72615ms step_avg:58.05ms
step:1252/2330 train_time:72635ms step_avg:58.02ms
step:1253/2330 train_time:72693ms step_avg:58.02ms
step:1254/2330 train_time:72759ms step_avg:58.02ms
step:1255/2330 train_time:72815ms step_avg:58.02ms
step:1256/2330 train_time:72880ms step_avg:58.03ms
step:1257/2330 train_time:72936ms step_avg:58.02ms
step:1258/2330 train_time:72995ms step_avg:58.03ms
step:1259/2330 train_time:73052ms step_avg:58.02ms
step:1260/2330 train_time:73112ms step_avg:58.03ms
step:1261/2330 train_time:73168ms step_avg:58.02ms
step:1262/2330 train_time:73228ms step_avg:58.03ms
step:1263/2330 train_time:73284ms step_avg:58.02ms
step:1264/2330 train_time:73344ms step_avg:58.02ms
step:1265/2330 train_time:73400ms step_avg:58.02ms
step:1266/2330 train_time:73459ms step_avg:58.02ms
step:1267/2330 train_time:73516ms step_avg:58.02ms
step:1268/2330 train_time:73577ms step_avg:58.03ms
step:1269/2330 train_time:73635ms step_avg:58.03ms
step:1270/2330 train_time:73698ms step_avg:58.03ms
step:1271/2330 train_time:73757ms step_avg:58.03ms
step:1272/2330 train_time:73819ms step_avg:58.03ms
step:1273/2330 train_time:73875ms step_avg:58.03ms
step:1274/2330 train_time:73936ms step_avg:58.03ms
step:1275/2330 train_time:73992ms step_avg:58.03ms
step:1276/2330 train_time:74053ms step_avg:58.04ms
step:1277/2330 train_time:74109ms step_avg:58.03ms
step:1278/2330 train_time:74170ms step_avg:58.04ms
step:1279/2330 train_time:74226ms step_avg:58.03ms
step:1280/2330 train_time:74286ms step_avg:58.04ms
step:1281/2330 train_time:74342ms step_avg:58.03ms
step:1282/2330 train_time:74402ms step_avg:58.04ms
step:1283/2330 train_time:74458ms step_avg:58.03ms
step:1284/2330 train_time:74518ms step_avg:58.04ms
step:1285/2330 train_time:74575ms step_avg:58.04ms
step:1286/2330 train_time:74636ms step_avg:58.04ms
step:1287/2330 train_time:74694ms step_avg:58.04ms
step:1288/2330 train_time:74755ms step_avg:58.04ms
step:1289/2330 train_time:74812ms step_avg:58.04ms
step:1290/2330 train_time:74874ms step_avg:58.04ms
step:1291/2330 train_time:74931ms step_avg:58.04ms
step:1292/2330 train_time:74992ms step_avg:58.04ms
step:1293/2330 train_time:75049ms step_avg:58.04ms
step:1294/2330 train_time:75109ms step_avg:58.04ms
step:1295/2330 train_time:75166ms step_avg:58.04ms
step:1296/2330 train_time:75225ms step_avg:58.04ms
step:1297/2330 train_time:75282ms step_avg:58.04ms
step:1298/2330 train_time:75341ms step_avg:58.04ms
step:1299/2330 train_time:75397ms step_avg:58.04ms
step:1300/2330 train_time:75457ms step_avg:58.04ms
step:1301/2330 train_time:75515ms step_avg:58.04ms
step:1302/2330 train_time:75574ms step_avg:58.04ms
step:1303/2330 train_time:75631ms step_avg:58.04ms
step:1304/2330 train_time:75693ms step_avg:58.05ms
step:1305/2330 train_time:75750ms step_avg:58.05ms
step:1306/2330 train_time:75812ms step_avg:58.05ms
step:1307/2330 train_time:75869ms step_avg:58.05ms
step:1308/2330 train_time:75930ms step_avg:58.05ms
step:1309/2330 train_time:75987ms step_avg:58.05ms
step:1310/2330 train_time:76049ms step_avg:58.05ms
step:1311/2330 train_time:76106ms step_avg:58.05ms
step:1312/2330 train_time:76165ms step_avg:58.05ms
step:1313/2330 train_time:76222ms step_avg:58.05ms
step:1314/2330 train_time:76281ms step_avg:58.05ms
step:1315/2330 train_time:76338ms step_avg:58.05ms
step:1316/2330 train_time:76398ms step_avg:58.05ms
step:1317/2330 train_time:76455ms step_avg:58.05ms
step:1318/2330 train_time:76515ms step_avg:58.05ms
step:1319/2330 train_time:76572ms step_avg:58.05ms
step:1320/2330 train_time:76632ms step_avg:58.05ms
step:1321/2330 train_time:76689ms step_avg:58.05ms
step:1322/2330 train_time:76750ms step_avg:58.06ms
step:1323/2330 train_time:76808ms step_avg:58.06ms
step:1324/2330 train_time:76868ms step_avg:58.06ms
step:1325/2330 train_time:76925ms step_avg:58.06ms
step:1326/2330 train_time:76986ms step_avg:58.06ms
step:1327/2330 train_time:77043ms step_avg:58.06ms
step:1328/2330 train_time:77103ms step_avg:58.06ms
step:1329/2330 train_time:77161ms step_avg:58.06ms
step:1330/2330 train_time:77220ms step_avg:58.06ms
step:1331/2330 train_time:77277ms step_avg:58.06ms
step:1332/2330 train_time:77337ms step_avg:58.06ms
step:1333/2330 train_time:77393ms step_avg:58.06ms
step:1334/2330 train_time:77453ms step_avg:58.06ms
step:1335/2330 train_time:77510ms step_avg:58.06ms
step:1336/2330 train_time:77570ms step_avg:58.06ms
step:1337/2330 train_time:77627ms step_avg:58.06ms
step:1338/2330 train_time:77687ms step_avg:58.06ms
step:1339/2330 train_time:77744ms step_avg:58.06ms
step:1340/2330 train_time:77805ms step_avg:58.06ms
step:1341/2330 train_time:77862ms step_avg:58.06ms
step:1342/2330 train_time:77922ms step_avg:58.06ms
step:1343/2330 train_time:77979ms step_avg:58.06ms
step:1344/2330 train_time:78040ms step_avg:58.07ms
step:1345/2330 train_time:78096ms step_avg:58.06ms
step:1346/2330 train_time:78157ms step_avg:58.07ms
step:1347/2330 train_time:78214ms step_avg:58.07ms
step:1348/2330 train_time:78274ms step_avg:58.07ms
step:1349/2330 train_time:78331ms step_avg:58.07ms
step:1350/2330 train_time:78391ms step_avg:58.07ms
step:1351/2330 train_time:78447ms step_avg:58.07ms
step:1352/2330 train_time:78508ms step_avg:58.07ms
step:1353/2330 train_time:78565ms step_avg:58.07ms
step:1354/2330 train_time:78624ms step_avg:58.07ms
step:1355/2330 train_time:78682ms step_avg:58.07ms
step:1356/2330 train_time:78742ms step_avg:58.07ms
step:1357/2330 train_time:78800ms step_avg:58.07ms
step:1358/2330 train_time:78860ms step_avg:58.07ms
step:1359/2330 train_time:78918ms step_avg:58.07ms
step:1360/2330 train_time:78978ms step_avg:58.07ms
step:1361/2330 train_time:79034ms step_avg:58.07ms
step:1362/2330 train_time:79096ms step_avg:58.07ms
step:1363/2330 train_time:79152ms step_avg:58.07ms
step:1364/2330 train_time:79213ms step_avg:58.07ms
step:1365/2330 train_time:79269ms step_avg:58.07ms
step:1366/2330 train_time:79330ms step_avg:58.07ms
step:1367/2330 train_time:79387ms step_avg:58.07ms
step:1368/2330 train_time:79447ms step_avg:58.08ms
step:1369/2330 train_time:79504ms step_avg:58.07ms
step:1370/2330 train_time:79564ms step_avg:58.08ms
step:1371/2330 train_time:79621ms step_avg:58.08ms
step:1372/2330 train_time:79682ms step_avg:58.08ms
step:1373/2330 train_time:79739ms step_avg:58.08ms
step:1374/2330 train_time:79799ms step_avg:58.08ms
step:1375/2330 train_time:79856ms step_avg:58.08ms
step:1376/2330 train_time:79916ms step_avg:58.08ms
step:1377/2330 train_time:79973ms step_avg:58.08ms
step:1378/2330 train_time:80034ms step_avg:58.08ms
step:1379/2330 train_time:80091ms step_avg:58.08ms
step:1380/2330 train_time:80152ms step_avg:58.08ms
step:1381/2330 train_time:80208ms step_avg:58.08ms
step:1382/2330 train_time:80270ms step_avg:58.08ms
step:1383/2330 train_time:80326ms step_avg:58.08ms
step:1384/2330 train_time:80386ms step_avg:58.08ms
step:1385/2330 train_time:80443ms step_avg:58.08ms
step:1386/2330 train_time:80502ms step_avg:58.08ms
step:1387/2330 train_time:80559ms step_avg:58.08ms
step:1388/2330 train_time:80619ms step_avg:58.08ms
step:1389/2330 train_time:80676ms step_avg:58.08ms
step:1390/2330 train_time:80737ms step_avg:58.08ms
step:1391/2330 train_time:80794ms step_avg:58.08ms
step:1392/2330 train_time:80855ms step_avg:58.09ms
step:1393/2330 train_time:80913ms step_avg:58.09ms
step:1394/2330 train_time:80972ms step_avg:58.09ms
step:1395/2330 train_time:81028ms step_avg:58.08ms
step:1396/2330 train_time:81090ms step_avg:58.09ms
step:1397/2330 train_time:81147ms step_avg:58.09ms
step:1398/2330 train_time:81207ms step_avg:58.09ms
step:1399/2330 train_time:81264ms step_avg:58.09ms
step:1400/2330 train_time:81324ms step_avg:58.09ms
step:1401/2330 train_time:81380ms step_avg:58.09ms
step:1402/2330 train_time:81440ms step_avg:58.09ms
step:1403/2330 train_time:81497ms step_avg:58.09ms
step:1404/2330 train_time:81557ms step_avg:58.09ms
step:1405/2330 train_time:81615ms step_avg:58.09ms
step:1406/2330 train_time:81675ms step_avg:58.09ms
step:1407/2330 train_time:81732ms step_avg:58.09ms
step:1408/2330 train_time:81792ms step_avg:58.09ms
step:1409/2330 train_time:81849ms step_avg:58.09ms
step:1410/2330 train_time:81910ms step_avg:58.09ms
step:1411/2330 train_time:81967ms step_avg:58.09ms
step:1412/2330 train_time:82027ms step_avg:58.09ms
step:1413/2330 train_time:82085ms step_avg:58.09ms
step:1414/2330 train_time:82145ms step_avg:58.09ms
step:1415/2330 train_time:82202ms step_avg:58.09ms
step:1416/2330 train_time:82263ms step_avg:58.09ms
step:1417/2330 train_time:82320ms step_avg:58.09ms
step:1418/2330 train_time:82379ms step_avg:58.10ms
step:1419/2330 train_time:82436ms step_avg:58.09ms
step:1420/2330 train_time:82497ms step_avg:58.10ms
step:1421/2330 train_time:82553ms step_avg:58.10ms
step:1422/2330 train_time:82614ms step_avg:58.10ms
step:1423/2330 train_time:82671ms step_avg:58.10ms
step:1424/2330 train_time:82732ms step_avg:58.10ms
step:1425/2330 train_time:82788ms step_avg:58.10ms
step:1426/2330 train_time:82849ms step_avg:58.10ms
step:1427/2330 train_time:82906ms step_avg:58.10ms
step:1428/2330 train_time:82967ms step_avg:58.10ms
step:1429/2330 train_time:83023ms step_avg:58.10ms
step:1430/2330 train_time:83084ms step_avg:58.10ms
step:1431/2330 train_time:83141ms step_avg:58.10ms
step:1432/2330 train_time:83202ms step_avg:58.10ms
step:1433/2330 train_time:83259ms step_avg:58.10ms
step:1434/2330 train_time:83319ms step_avg:58.10ms
step:1435/2330 train_time:83375ms step_avg:58.10ms
step:1436/2330 train_time:83436ms step_avg:58.10ms
step:1437/2330 train_time:83494ms step_avg:58.10ms
step:1438/2330 train_time:83553ms step_avg:58.10ms
step:1439/2330 train_time:83609ms step_avg:58.10ms
step:1440/2330 train_time:83670ms step_avg:58.10ms
step:1441/2330 train_time:83727ms step_avg:58.10ms
step:1442/2330 train_time:83787ms step_avg:58.10ms
step:1443/2330 train_time:83844ms step_avg:58.10ms
step:1444/2330 train_time:83904ms step_avg:58.11ms
step:1445/2330 train_time:83961ms step_avg:58.10ms
step:1446/2330 train_time:84021ms step_avg:58.11ms
step:1447/2330 train_time:84078ms step_avg:58.11ms
step:1448/2330 train_time:84139ms step_avg:58.11ms
step:1449/2330 train_time:84196ms step_avg:58.11ms
step:1450/2330 train_time:84257ms step_avg:58.11ms
step:1451/2330 train_time:84314ms step_avg:58.11ms
step:1452/2330 train_time:84373ms step_avg:58.11ms
step:1453/2330 train_time:84429ms step_avg:58.11ms
step:1454/2330 train_time:84491ms step_avg:58.11ms
step:1455/2330 train_time:84548ms step_avg:58.11ms
step:1456/2330 train_time:84608ms step_avg:58.11ms
step:1457/2330 train_time:84665ms step_avg:58.11ms
step:1458/2330 train_time:84725ms step_avg:58.11ms
step:1459/2330 train_time:84782ms step_avg:58.11ms
step:1460/2330 train_time:84843ms step_avg:58.11ms
step:1461/2330 train_time:84900ms step_avg:58.11ms
step:1462/2330 train_time:84960ms step_avg:58.11ms
step:1463/2330 train_time:85017ms step_avg:58.11ms
step:1464/2330 train_time:85077ms step_avg:58.11ms
step:1465/2330 train_time:85134ms step_avg:58.11ms
step:1466/2330 train_time:85195ms step_avg:58.11ms
step:1467/2330 train_time:85251ms step_avg:58.11ms
step:1468/2330 train_time:85313ms step_avg:58.11ms
step:1469/2330 train_time:85370ms step_avg:58.11ms
step:1470/2330 train_time:85430ms step_avg:58.12ms
step:1471/2330 train_time:85486ms step_avg:58.11ms
step:1472/2330 train_time:85546ms step_avg:58.12ms
step:1473/2330 train_time:85603ms step_avg:58.11ms
step:1474/2330 train_time:85663ms step_avg:58.12ms
step:1475/2330 train_time:85720ms step_avg:58.12ms
step:1476/2330 train_time:85781ms step_avg:58.12ms
step:1477/2330 train_time:85837ms step_avg:58.12ms
step:1478/2330 train_time:85898ms step_avg:58.12ms
step:1479/2330 train_time:85955ms step_avg:58.12ms
step:1480/2330 train_time:86016ms step_avg:58.12ms
step:1481/2330 train_time:86073ms step_avg:58.12ms
step:1482/2330 train_time:86134ms step_avg:58.12ms
step:1483/2330 train_time:86191ms step_avg:58.12ms
step:1484/2330 train_time:86252ms step_avg:58.12ms
step:1485/2330 train_time:86308ms step_avg:58.12ms
step:1486/2330 train_time:86369ms step_avg:58.12ms
step:1487/2330 train_time:86426ms step_avg:58.12ms
step:1488/2330 train_time:86486ms step_avg:58.12ms
step:1489/2330 train_time:86543ms step_avg:58.12ms
step:1490/2330 train_time:86603ms step_avg:58.12ms
step:1491/2330 train_time:86660ms step_avg:58.12ms
step:1492/2330 train_time:86720ms step_avg:58.12ms
step:1493/2330 train_time:86778ms step_avg:58.12ms
step:1494/2330 train_time:86838ms step_avg:58.12ms
step:1495/2330 train_time:86895ms step_avg:58.12ms
step:1496/2330 train_time:86955ms step_avg:58.13ms
step:1497/2330 train_time:87012ms step_avg:58.12ms
step:1498/2330 train_time:87073ms step_avg:58.13ms
step:1499/2330 train_time:87130ms step_avg:58.13ms
step:1500/2330 train_time:87190ms step_avg:58.13ms
step:1500/2330 val_loss:3.9078 train_time:87271ms step_avg:58.18ms
step:1501/2330 train_time:87289ms step_avg:58.15ms
step:1502/2330 train_time:87309ms step_avg:58.13ms
step:1503/2330 train_time:87366ms step_avg:58.13ms
step:1504/2330 train_time:87433ms step_avg:58.13ms
step:1505/2330 train_time:87491ms step_avg:58.13ms
step:1506/2330 train_time:87551ms step_avg:58.13ms
step:1507/2330 train_time:87608ms step_avg:58.13ms
step:1508/2330 train_time:87667ms step_avg:58.13ms
step:1509/2330 train_time:87724ms step_avg:58.13ms
step:1510/2330 train_time:87784ms step_avg:58.13ms
step:1511/2330 train_time:87840ms step_avg:58.13ms
step:1512/2330 train_time:87900ms step_avg:58.13ms
step:1513/2330 train_time:87956ms step_avg:58.13ms
step:1514/2330 train_time:88016ms step_avg:58.13ms
step:1515/2330 train_time:88072ms step_avg:58.13ms
step:1516/2330 train_time:88133ms step_avg:58.14ms
step:1517/2330 train_time:88189ms step_avg:58.13ms
step:1518/2330 train_time:88250ms step_avg:58.14ms
step:1519/2330 train_time:88309ms step_avg:58.14ms
step:1520/2330 train_time:88371ms step_avg:58.14ms
step:1521/2330 train_time:88429ms step_avg:58.14ms
step:1522/2330 train_time:88490ms step_avg:58.14ms
step:1523/2330 train_time:88548ms step_avg:58.14ms
step:1524/2330 train_time:88608ms step_avg:58.14ms
step:1525/2330 train_time:88665ms step_avg:58.14ms
step:1526/2330 train_time:88725ms step_avg:58.14ms
step:1527/2330 train_time:88782ms step_avg:58.14ms
step:1528/2330 train_time:88842ms step_avg:58.14ms
step:1529/2330 train_time:88900ms step_avg:58.14ms
step:1530/2330 train_time:88958ms step_avg:58.14ms
step:1531/2330 train_time:89015ms step_avg:58.14ms
step:1532/2330 train_time:89076ms step_avg:58.14ms
step:1533/2330 train_time:89133ms step_avg:58.14ms
step:1534/2330 train_time:89194ms step_avg:58.14ms
step:1535/2330 train_time:89250ms step_avg:58.14ms
step:1536/2330 train_time:89312ms step_avg:58.15ms
step:1537/2330 train_time:89370ms step_avg:58.15ms
step:1538/2330 train_time:89432ms step_avg:58.15ms
step:1539/2330 train_time:89490ms step_avg:58.15ms
step:1540/2330 train_time:89550ms step_avg:58.15ms
step:1541/2330 train_time:89608ms step_avg:58.15ms
step:1542/2330 train_time:89669ms step_avg:58.15ms
step:1543/2330 train_time:89726ms step_avg:58.15ms
step:1544/2330 train_time:89787ms step_avg:58.15ms
step:1545/2330 train_time:89844ms step_avg:58.15ms
step:1546/2330 train_time:89907ms step_avg:58.15ms
step:1547/2330 train_time:89963ms step_avg:58.15ms
step:1548/2330 train_time:90025ms step_avg:58.16ms
step:1549/2330 train_time:90082ms step_avg:58.15ms
step:1550/2330 train_time:90143ms step_avg:58.16ms
step:1551/2330 train_time:90200ms step_avg:58.16ms
step:1552/2330 train_time:90262ms step_avg:58.16ms
step:1553/2330 train_time:90318ms step_avg:58.16ms
step:1554/2330 train_time:90381ms step_avg:58.16ms
step:1555/2330 train_time:90438ms step_avg:58.16ms
step:1556/2330 train_time:90501ms step_avg:58.16ms
step:1557/2330 train_time:90558ms step_avg:58.16ms
step:1558/2330 train_time:90622ms step_avg:58.17ms
step:1559/2330 train_time:90678ms step_avg:58.16ms
step:1560/2330 train_time:90740ms step_avg:58.17ms
step:1561/2330 train_time:90797ms step_avg:58.17ms
step:1562/2330 train_time:90859ms step_avg:58.17ms
step:1563/2330 train_time:90915ms step_avg:58.17ms
step:1564/2330 train_time:90977ms step_avg:58.17ms
step:1565/2330 train_time:91034ms step_avg:58.17ms
step:1566/2330 train_time:91095ms step_avg:58.17ms
step:1567/2330 train_time:91153ms step_avg:58.17ms
step:1568/2330 train_time:91213ms step_avg:58.17ms
step:1569/2330 train_time:91270ms step_avg:58.17ms
step:1570/2330 train_time:91332ms step_avg:58.17ms
step:1571/2330 train_time:91389ms step_avg:58.17ms
step:1572/2330 train_time:91451ms step_avg:58.17ms
step:1573/2330 train_time:91509ms step_avg:58.18ms
step:1574/2330 train_time:91570ms step_avg:58.18ms
step:1575/2330 train_time:91628ms step_avg:58.18ms
step:1576/2330 train_time:91689ms step_avg:58.18ms
step:1577/2330 train_time:91748ms step_avg:58.18ms
step:1578/2330 train_time:91808ms step_avg:58.18ms
step:1579/2330 train_time:91866ms step_avg:58.18ms
step:1580/2330 train_time:91926ms step_avg:58.18ms
step:1581/2330 train_time:91984ms step_avg:58.18ms
step:1582/2330 train_time:92045ms step_avg:58.18ms
step:1583/2330 train_time:92101ms step_avg:58.18ms
step:1584/2330 train_time:92164ms step_avg:58.18ms
step:1585/2330 train_time:92220ms step_avg:58.18ms
step:1586/2330 train_time:92283ms step_avg:58.19ms
step:1587/2330 train_time:92340ms step_avg:58.18ms
step:1588/2330 train_time:92402ms step_avg:58.19ms
step:1589/2330 train_time:92458ms step_avg:58.19ms
step:1590/2330 train_time:92521ms step_avg:58.19ms
step:1591/2330 train_time:92578ms step_avg:58.19ms
step:1592/2330 train_time:92640ms step_avg:58.19ms
step:1593/2330 train_time:92697ms step_avg:58.19ms
step:1594/2330 train_time:92760ms step_avg:58.19ms
step:1595/2330 train_time:92817ms step_avg:58.19ms
step:1596/2330 train_time:92879ms step_avg:58.19ms
step:1597/2330 train_time:92936ms step_avg:58.19ms
step:1598/2330 train_time:92996ms step_avg:58.20ms
step:1599/2330 train_time:93053ms step_avg:58.19ms
step:1600/2330 train_time:93115ms step_avg:58.20ms
step:1601/2330 train_time:93173ms step_avg:58.20ms
step:1602/2330 train_time:93233ms step_avg:58.20ms
step:1603/2330 train_time:93291ms step_avg:58.20ms
step:1604/2330 train_time:93351ms step_avg:58.20ms
step:1605/2330 train_time:93409ms step_avg:58.20ms
step:1606/2330 train_time:93469ms step_avg:58.20ms
step:1607/2330 train_time:93527ms step_avg:58.20ms
step:1608/2330 train_time:93588ms step_avg:58.20ms
step:1609/2330 train_time:93645ms step_avg:58.20ms
step:1610/2330 train_time:93706ms step_avg:58.20ms
step:1611/2330 train_time:93764ms step_avg:58.20ms
step:1612/2330 train_time:93826ms step_avg:58.20ms
step:1613/2330 train_time:93883ms step_avg:58.20ms
step:1614/2330 train_time:93944ms step_avg:58.21ms
step:1615/2330 train_time:94001ms step_avg:58.21ms
step:1616/2330 train_time:94064ms step_avg:58.21ms
step:1617/2330 train_time:94121ms step_avg:58.21ms
step:1618/2330 train_time:94182ms step_avg:58.21ms
step:1619/2330 train_time:94239ms step_avg:58.21ms
step:1620/2330 train_time:94301ms step_avg:58.21ms
step:1621/2330 train_time:94358ms step_avg:58.21ms
step:1622/2330 train_time:94421ms step_avg:58.21ms
step:1623/2330 train_time:94478ms step_avg:58.21ms
step:1624/2330 train_time:94539ms step_avg:58.21ms
step:1625/2330 train_time:94596ms step_avg:58.21ms
step:1626/2330 train_time:94657ms step_avg:58.21ms
step:1627/2330 train_time:94714ms step_avg:58.21ms
step:1628/2330 train_time:94774ms step_avg:58.21ms
step:1629/2330 train_time:94832ms step_avg:58.21ms
step:1630/2330 train_time:94893ms step_avg:58.22ms
step:1631/2330 train_time:94951ms step_avg:58.22ms
step:1632/2330 train_time:95011ms step_avg:58.22ms
step:1633/2330 train_time:95069ms step_avg:58.22ms
step:1634/2330 train_time:95129ms step_avg:58.22ms
step:1635/2330 train_time:95186ms step_avg:58.22ms
step:1636/2330 train_time:95247ms step_avg:58.22ms
step:1637/2330 train_time:95304ms step_avg:58.22ms
step:1638/2330 train_time:95366ms step_avg:58.22ms
step:1639/2330 train_time:95423ms step_avg:58.22ms
step:1640/2330 train_time:95486ms step_avg:58.22ms
step:1641/2330 train_time:95542ms step_avg:58.22ms
step:1642/2330 train_time:95605ms step_avg:58.22ms
step:1643/2330 train_time:95660ms step_avg:58.22ms
step:1644/2330 train_time:95724ms step_avg:58.23ms
step:1645/2330 train_time:95780ms step_avg:58.22ms
step:1646/2330 train_time:95843ms step_avg:58.23ms
step:1647/2330 train_time:95900ms step_avg:58.23ms
step:1648/2330 train_time:95962ms step_avg:58.23ms
step:1649/2330 train_time:96019ms step_avg:58.23ms
step:1650/2330 train_time:96081ms step_avg:58.23ms
step:1651/2330 train_time:96138ms step_avg:58.23ms
step:1652/2330 train_time:96200ms step_avg:58.23ms
step:1653/2330 train_time:96257ms step_avg:58.23ms
step:1654/2330 train_time:96319ms step_avg:58.23ms
step:1655/2330 train_time:96376ms step_avg:58.23ms
step:1656/2330 train_time:96437ms step_avg:58.24ms
step:1657/2330 train_time:96494ms step_avg:58.23ms
step:1658/2330 train_time:96556ms step_avg:58.24ms
step:1659/2330 train_time:96614ms step_avg:58.24ms
step:1660/2330 train_time:96674ms step_avg:58.24ms
step:1661/2330 train_time:96731ms step_avg:58.24ms
step:1662/2330 train_time:96792ms step_avg:58.24ms
step:1663/2330 train_time:96850ms step_avg:58.24ms
step:1664/2330 train_time:96910ms step_avg:58.24ms
step:1665/2330 train_time:96968ms step_avg:58.24ms
step:1666/2330 train_time:97029ms step_avg:58.24ms
step:1667/2330 train_time:97088ms step_avg:58.24ms
step:1668/2330 train_time:97149ms step_avg:58.24ms
step:1669/2330 train_time:97207ms step_avg:58.24ms
step:1670/2330 train_time:97267ms step_avg:58.24ms
step:1671/2330 train_time:97325ms step_avg:58.24ms
step:1672/2330 train_time:97386ms step_avg:58.25ms
step:1673/2330 train_time:97443ms step_avg:58.24ms
step:1674/2330 train_time:97504ms step_avg:58.25ms
step:1675/2330 train_time:97561ms step_avg:58.25ms
step:1676/2330 train_time:97623ms step_avg:58.25ms
step:1677/2330 train_time:97680ms step_avg:58.25ms
step:1678/2330 train_time:97743ms step_avg:58.25ms
step:1679/2330 train_time:97799ms step_avg:58.25ms
step:1680/2330 train_time:97862ms step_avg:58.25ms
step:1681/2330 train_time:97919ms step_avg:58.25ms
step:1682/2330 train_time:97980ms step_avg:58.25ms
step:1683/2330 train_time:98037ms step_avg:58.25ms
step:1684/2330 train_time:98100ms step_avg:58.25ms
step:1685/2330 train_time:98156ms step_avg:58.25ms
step:1686/2330 train_time:98219ms step_avg:58.26ms
step:1687/2330 train_time:98276ms step_avg:58.25ms
step:1688/2330 train_time:98337ms step_avg:58.26ms
step:1689/2330 train_time:98394ms step_avg:58.26ms
step:1690/2330 train_time:98455ms step_avg:58.26ms
step:1691/2330 train_time:98513ms step_avg:58.26ms
step:1692/2330 train_time:98573ms step_avg:58.26ms
step:1693/2330 train_time:98630ms step_avg:58.26ms
step:1694/2330 train_time:98691ms step_avg:58.26ms
step:1695/2330 train_time:98750ms step_avg:58.26ms
step:1696/2330 train_time:98810ms step_avg:58.26ms
step:1697/2330 train_time:98868ms step_avg:58.26ms
step:1698/2330 train_time:98929ms step_avg:58.26ms
step:1699/2330 train_time:98988ms step_avg:58.26ms
step:1700/2330 train_time:99048ms step_avg:58.26ms
step:1701/2330 train_time:99104ms step_avg:58.26ms
step:1702/2330 train_time:99167ms step_avg:58.27ms
step:1703/2330 train_time:99225ms step_avg:58.26ms
step:1704/2330 train_time:99286ms step_avg:58.27ms
step:1705/2330 train_time:99342ms step_avg:58.27ms
step:1706/2330 train_time:99405ms step_avg:58.27ms
step:1707/2330 train_time:99462ms step_avg:58.27ms
step:1708/2330 train_time:99525ms step_avg:58.27ms
step:1709/2330 train_time:99582ms step_avg:58.27ms
step:1710/2330 train_time:99643ms step_avg:58.27ms
step:1711/2330 train_time:99700ms step_avg:58.27ms
step:1712/2330 train_time:99762ms step_avg:58.27ms
step:1713/2330 train_time:99818ms step_avg:58.27ms
step:1714/2330 train_time:99881ms step_avg:58.27ms
step:1715/2330 train_time:99937ms step_avg:58.27ms
step:1716/2330 train_time:99999ms step_avg:58.27ms
step:1717/2330 train_time:100055ms step_avg:58.27ms
step:1718/2330 train_time:100118ms step_avg:58.28ms
step:1719/2330 train_time:100175ms step_avg:58.28ms
step:1720/2330 train_time:100236ms step_avg:58.28ms
step:1721/2330 train_time:100293ms step_avg:58.28ms
step:1722/2330 train_time:100355ms step_avg:58.28ms
step:1723/2330 train_time:100412ms step_avg:58.28ms
step:1724/2330 train_time:100472ms step_avg:58.28ms
step:1725/2330 train_time:100529ms step_avg:58.28ms
step:1726/2330 train_time:100591ms step_avg:58.28ms
step:1727/2330 train_time:100650ms step_avg:58.28ms
step:1728/2330 train_time:100710ms step_avg:58.28ms
step:1729/2330 train_time:100767ms step_avg:58.28ms
step:1730/2330 train_time:100828ms step_avg:58.28ms
step:1731/2330 train_time:100887ms step_avg:58.28ms
step:1732/2330 train_time:100947ms step_avg:58.28ms
step:1733/2330 train_time:101004ms step_avg:58.28ms
step:1734/2330 train_time:101066ms step_avg:58.28ms
step:1735/2330 train_time:101122ms step_avg:58.28ms
step:1736/2330 train_time:101184ms step_avg:58.29ms
step:1737/2330 train_time:101241ms step_avg:58.29ms
step:1738/2330 train_time:101303ms step_avg:58.29ms
step:1739/2330 train_time:101359ms step_avg:58.29ms
step:1740/2330 train_time:101422ms step_avg:58.29ms
step:1741/2330 train_time:101479ms step_avg:58.29ms
step:1742/2330 train_time:101541ms step_avg:58.29ms
step:1743/2330 train_time:101597ms step_avg:58.29ms
step:1744/2330 train_time:101660ms step_avg:58.29ms
step:1745/2330 train_time:101717ms step_avg:58.29ms
step:1746/2330 train_time:101779ms step_avg:58.29ms
step:1747/2330 train_time:101836ms step_avg:58.29ms
step:1748/2330 train_time:101897ms step_avg:58.29ms
step:1749/2330 train_time:101954ms step_avg:58.29ms
step:1750/2330 train_time:102015ms step_avg:58.29ms
step:1750/2330 val_loss:3.8244 train_time:102096ms step_avg:58.34ms
step:1751/2330 train_time:102114ms step_avg:58.32ms
step:1752/2330 train_time:102134ms step_avg:58.30ms
step:1753/2330 train_time:102189ms step_avg:58.29ms
step:1754/2330 train_time:102257ms step_avg:58.30ms
step:1755/2330 train_time:102313ms step_avg:58.30ms
step:1756/2330 train_time:102384ms step_avg:58.31ms
step:1757/2330 train_time:102440ms step_avg:58.30ms
step:1758/2330 train_time:102502ms step_avg:58.31ms
step:1759/2330 train_time:102559ms step_avg:58.31ms
step:1760/2330 train_time:102619ms step_avg:58.31ms
step:1761/2330 train_time:102676ms step_avg:58.31ms
step:1762/2330 train_time:102735ms step_avg:58.31ms
step:1763/2330 train_time:102792ms step_avg:58.31ms
step:1764/2330 train_time:102853ms step_avg:58.31ms
step:1765/2330 train_time:102909ms step_avg:58.31ms
step:1766/2330 train_time:102969ms step_avg:58.31ms
step:1767/2330 train_time:103027ms step_avg:58.31ms
step:1768/2330 train_time:103092ms step_avg:58.31ms
step:1769/2330 train_time:103150ms step_avg:58.31ms
step:1770/2330 train_time:103212ms step_avg:58.31ms
step:1771/2330 train_time:103270ms step_avg:58.31ms
step:1772/2330 train_time:103332ms step_avg:58.31ms
step:1773/2330 train_time:103389ms step_avg:58.31ms
step:1774/2330 train_time:103451ms step_avg:58.32ms
step:1775/2330 train_time:103508ms step_avg:58.31ms
step:1776/2330 train_time:103569ms step_avg:58.32ms
step:1777/2330 train_time:103626ms step_avg:58.31ms
step:1778/2330 train_time:103687ms step_avg:58.32ms
step:1779/2330 train_time:103744ms step_avg:58.32ms
step:1780/2330 train_time:103804ms step_avg:58.32ms
step:1781/2330 train_time:103861ms step_avg:58.32ms
step:1782/2330 train_time:103920ms step_avg:58.32ms
step:1783/2330 train_time:103979ms step_avg:58.32ms
step:1784/2330 train_time:104039ms step_avg:58.32ms
step:1785/2330 train_time:104098ms step_avg:58.32ms
step:1786/2330 train_time:104159ms step_avg:58.32ms
step:1787/2330 train_time:104218ms step_avg:58.32ms
step:1788/2330 train_time:104279ms step_avg:58.32ms
step:1789/2330 train_time:104336ms step_avg:58.32ms
step:1790/2330 train_time:104397ms step_avg:58.32ms
step:1791/2330 train_time:104455ms step_avg:58.32ms
step:1792/2330 train_time:104516ms step_avg:58.32ms
step:1793/2330 train_time:104573ms step_avg:58.32ms
step:1794/2330 train_time:104634ms step_avg:58.32ms
step:1795/2330 train_time:104690ms step_avg:58.32ms
step:1796/2330 train_time:104752ms step_avg:58.33ms
step:1797/2330 train_time:104808ms step_avg:58.32ms
step:1798/2330 train_time:104870ms step_avg:58.33ms
step:1799/2330 train_time:104926ms step_avg:58.32ms
step:1800/2330 train_time:104988ms step_avg:58.33ms
step:1801/2330 train_time:105045ms step_avg:58.33ms
step:1802/2330 train_time:105107ms step_avg:58.33ms
step:1803/2330 train_time:105165ms step_avg:58.33ms
step:1804/2330 train_time:105226ms step_avg:58.33ms
step:1805/2330 train_time:105285ms step_avg:58.33ms
step:1806/2330 train_time:105344ms step_avg:58.33ms
step:1807/2330 train_time:105402ms step_avg:58.33ms
step:1808/2330 train_time:105463ms step_avg:58.33ms
step:1809/2330 train_time:105520ms step_avg:58.33ms
step:1810/2330 train_time:105581ms step_avg:58.33ms
step:1811/2330 train_time:105638ms step_avg:58.33ms
step:1812/2330 train_time:105700ms step_avg:58.33ms
step:1813/2330 train_time:105758ms step_avg:58.33ms
step:1814/2330 train_time:105818ms step_avg:58.33ms
step:1815/2330 train_time:105875ms step_avg:58.33ms
step:1816/2330 train_time:105935ms step_avg:58.33ms
step:1817/2330 train_time:105993ms step_avg:58.33ms
step:1818/2330 train_time:106055ms step_avg:58.34ms
step:1819/2330 train_time:106112ms step_avg:58.34ms
step:1820/2330 train_time:106175ms step_avg:58.34ms
step:1821/2330 train_time:106232ms step_avg:58.34ms
step:1822/2330 train_time:106295ms step_avg:58.34ms
step:1823/2330 train_time:106352ms step_avg:58.34ms
step:1824/2330 train_time:106413ms step_avg:58.34ms
step:1825/2330 train_time:106470ms step_avg:58.34ms
step:1826/2330 train_time:106532ms step_avg:58.34ms
step:1827/2330 train_time:106588ms step_avg:58.34ms
step:1828/2330 train_time:106650ms step_avg:58.34ms
step:1829/2330 train_time:106707ms step_avg:58.34ms
step:1830/2330 train_time:106768ms step_avg:58.34ms
step:1831/2330 train_time:106825ms step_avg:58.34ms
step:1832/2330 train_time:106886ms step_avg:58.34ms
step:1833/2330 train_time:106943ms step_avg:58.34ms
step:1834/2330 train_time:107004ms step_avg:58.34ms
step:1835/2330 train_time:107063ms step_avg:58.34ms
step:1836/2330 train_time:107123ms step_avg:58.35ms
step:1837/2330 train_time:107182ms step_avg:58.35ms
step:1838/2330 train_time:107242ms step_avg:58.35ms
step:1839/2330 train_time:107301ms step_avg:58.35ms
step:1840/2330 train_time:107361ms step_avg:58.35ms
step:1841/2330 train_time:107419ms step_avg:58.35ms
step:1842/2330 train_time:107480ms step_avg:58.35ms
step:1843/2330 train_time:107539ms step_avg:58.35ms
step:1844/2330 train_time:107599ms step_avg:58.35ms
step:1845/2330 train_time:107657ms step_avg:58.35ms
step:1846/2330 train_time:107717ms step_avg:58.35ms
step:1847/2330 train_time:107774ms step_avg:58.35ms
step:1848/2330 train_time:107836ms step_avg:58.35ms
step:1849/2330 train_time:107892ms step_avg:58.35ms
step:1850/2330 train_time:107953ms step_avg:58.35ms
step:1851/2330 train_time:108011ms step_avg:58.35ms
step:1852/2330 train_time:108073ms step_avg:58.35ms
step:1853/2330 train_time:108129ms step_avg:58.35ms
step:1854/2330 train_time:108192ms step_avg:58.36ms
step:1855/2330 train_time:108249ms step_avg:58.35ms
step:1856/2330 train_time:108311ms step_avg:58.36ms
step:1857/2330 train_time:108368ms step_avg:58.36ms
step:1858/2330 train_time:108430ms step_avg:58.36ms
step:1859/2330 train_time:108486ms step_avg:58.36ms
step:1860/2330 train_time:108548ms step_avg:58.36ms
step:1861/2330 train_time:108606ms step_avg:58.36ms
step:1862/2330 train_time:108666ms step_avg:58.36ms
step:1863/2330 train_time:108724ms step_avg:58.36ms
step:1864/2330 train_time:108785ms step_avg:58.36ms
step:1865/2330 train_time:108842ms step_avg:58.36ms
step:1866/2330 train_time:108902ms step_avg:58.36ms
step:1867/2330 train_time:108959ms step_avg:58.36ms
step:1868/2330 train_time:109020ms step_avg:58.36ms
step:1869/2330 train_time:109078ms step_avg:58.36ms
step:1870/2330 train_time:109139ms step_avg:58.36ms
step:1871/2330 train_time:109197ms step_avg:58.36ms
step:1872/2330 train_time:109257ms step_avg:58.36ms
step:1873/2330 train_time:109316ms step_avg:58.36ms
step:1874/2330 train_time:109376ms step_avg:58.37ms
step:1875/2330 train_time:109433ms step_avg:58.36ms
step:1876/2330 train_time:109494ms step_avg:58.37ms
step:1877/2330 train_time:109551ms step_avg:58.37ms
step:1878/2330 train_time:109613ms step_avg:58.37ms
step:1879/2330 train_time:109670ms step_avg:58.37ms
step:1880/2330 train_time:109732ms step_avg:58.37ms
step:1881/2330 train_time:109789ms step_avg:58.37ms
step:1882/2330 train_time:109851ms step_avg:58.37ms
step:1883/2330 train_time:109907ms step_avg:58.37ms
step:1884/2330 train_time:109969ms step_avg:58.37ms
step:1885/2330 train_time:110026ms step_avg:58.37ms
step:1886/2330 train_time:110088ms step_avg:58.37ms
step:1887/2330 train_time:110146ms step_avg:58.37ms
step:1888/2330 train_time:110206ms step_avg:58.37ms
step:1889/2330 train_time:110263ms step_avg:58.37ms
step:1890/2330 train_time:110324ms step_avg:58.37ms
step:1891/2330 train_time:110383ms step_avg:58.37ms
step:1892/2330 train_time:110443ms step_avg:58.37ms
step:1893/2330 train_time:110502ms step_avg:58.37ms
step:1894/2330 train_time:110562ms step_avg:58.37ms
step:1895/2330 train_time:110621ms step_avg:58.37ms
step:1896/2330 train_time:110681ms step_avg:58.38ms
step:1897/2330 train_time:110739ms step_avg:58.38ms
step:1898/2330 train_time:110799ms step_avg:58.38ms
step:1899/2330 train_time:110856ms step_avg:58.38ms
step:1900/2330 train_time:110917ms step_avg:58.38ms
step:1901/2330 train_time:110975ms step_avg:58.38ms
step:1902/2330 train_time:111036ms step_avg:58.38ms
step:1903/2330 train_time:111092ms step_avg:58.38ms
step:1904/2330 train_time:111154ms step_avg:58.38ms
step:1905/2330 train_time:111211ms step_avg:58.38ms
step:1906/2330 train_time:111273ms step_avg:58.38ms
step:1907/2330 train_time:111329ms step_avg:58.38ms
step:1908/2330 train_time:111392ms step_avg:58.38ms
step:1909/2330 train_time:111449ms step_avg:58.38ms
step:1910/2330 train_time:111510ms step_avg:58.38ms
step:1911/2330 train_time:111567ms step_avg:58.38ms
step:1912/2330 train_time:111629ms step_avg:58.38ms
step:1913/2330 train_time:111685ms step_avg:58.38ms
step:1914/2330 train_time:111747ms step_avg:58.38ms
step:1915/2330 train_time:111804ms step_avg:58.38ms
step:1916/2330 train_time:111865ms step_avg:58.38ms
step:1917/2330 train_time:111924ms step_avg:58.38ms
step:1918/2330 train_time:111984ms step_avg:58.39ms
step:1919/2330 train_time:112041ms step_avg:58.39ms
step:1920/2330 train_time:112101ms step_avg:58.39ms
step:1921/2330 train_time:112160ms step_avg:58.39ms
step:1922/2330 train_time:112220ms step_avg:58.39ms
step:1923/2330 train_time:112279ms step_avg:58.39ms
step:1924/2330 train_time:112339ms step_avg:58.39ms
step:1925/2330 train_time:112398ms step_avg:58.39ms
step:1926/2330 train_time:112458ms step_avg:58.39ms
step:1927/2330 train_time:112516ms step_avg:58.39ms
step:1928/2330 train_time:112577ms step_avg:58.39ms
step:1929/2330 train_time:112634ms step_avg:58.39ms
step:1930/2330 train_time:112696ms step_avg:58.39ms
step:1931/2330 train_time:112753ms step_avg:58.39ms
step:1932/2330 train_time:112815ms step_avg:58.39ms
step:1933/2330 train_time:112872ms step_avg:58.39ms
step:1934/2330 train_time:112933ms step_avg:58.39ms
step:1935/2330 train_time:112990ms step_avg:58.39ms
step:1936/2330 train_time:113052ms step_avg:58.39ms
step:1937/2330 train_time:113109ms step_avg:58.39ms
step:1938/2330 train_time:113170ms step_avg:58.40ms
step:1939/2330 train_time:113226ms step_avg:58.39ms
step:1940/2330 train_time:113288ms step_avg:58.40ms
step:1941/2330 train_time:113345ms step_avg:58.40ms
step:1942/2330 train_time:113406ms step_avg:58.40ms
step:1943/2330 train_time:113464ms step_avg:58.40ms
step:1944/2330 train_time:113524ms step_avg:58.40ms
step:1945/2330 train_time:113581ms step_avg:58.40ms
step:1946/2330 train_time:113642ms step_avg:58.40ms
step:1947/2330 train_time:113700ms step_avg:58.40ms
step:1948/2330 train_time:113761ms step_avg:58.40ms
step:1949/2330 train_time:113820ms step_avg:58.40ms
step:1950/2330 train_time:113880ms step_avg:58.40ms
step:1951/2330 train_time:113937ms step_avg:58.40ms
step:1952/2330 train_time:113999ms step_avg:58.40ms
step:1953/2330 train_time:114057ms step_avg:58.40ms
step:1954/2330 train_time:114118ms step_avg:58.40ms
step:1955/2330 train_time:114175ms step_avg:58.40ms
step:1956/2330 train_time:114235ms step_avg:58.40ms
step:1957/2330 train_time:114293ms step_avg:58.40ms
step:1958/2330 train_time:114354ms step_avg:58.40ms
step:1959/2330 train_time:114411ms step_avg:58.40ms
step:1960/2330 train_time:114474ms step_avg:58.41ms
step:1961/2330 train_time:114530ms step_avg:58.40ms
step:1962/2330 train_time:114593ms step_avg:58.41ms
step:1963/2330 train_time:114649ms step_avg:58.41ms
step:1964/2330 train_time:114711ms step_avg:58.41ms
step:1965/2330 train_time:114769ms step_avg:58.41ms
step:1966/2330 train_time:114831ms step_avg:58.41ms
step:1967/2330 train_time:114888ms step_avg:58.41ms
step:1968/2330 train_time:114950ms step_avg:58.41ms
step:1969/2330 train_time:115007ms step_avg:58.41ms
step:1970/2330 train_time:115070ms step_avg:58.41ms
step:1971/2330 train_time:115127ms step_avg:58.41ms
step:1972/2330 train_time:115188ms step_avg:58.41ms
step:1973/2330 train_time:115246ms step_avg:58.41ms
step:1974/2330 train_time:115306ms step_avg:58.41ms
step:1975/2330 train_time:115364ms step_avg:58.41ms
step:1976/2330 train_time:115424ms step_avg:58.41ms
step:1977/2330 train_time:115482ms step_avg:58.41ms
step:1978/2330 train_time:115542ms step_avg:58.41ms
step:1979/2330 train_time:115600ms step_avg:58.41ms
step:1980/2330 train_time:115661ms step_avg:58.41ms
step:1981/2330 train_time:115719ms step_avg:58.41ms
step:1982/2330 train_time:115779ms step_avg:58.42ms
step:1983/2330 train_time:115837ms step_avg:58.42ms
step:1984/2330 train_time:115898ms step_avg:58.42ms
step:1985/2330 train_time:115956ms step_avg:58.42ms
step:1986/2330 train_time:116017ms step_avg:58.42ms
step:1987/2330 train_time:116074ms step_avg:58.42ms
step:1988/2330 train_time:116136ms step_avg:58.42ms
step:1989/2330 train_time:116194ms step_avg:58.42ms
step:1990/2330 train_time:116255ms step_avg:58.42ms
step:1991/2330 train_time:116312ms step_avg:58.42ms
step:1992/2330 train_time:116374ms step_avg:58.42ms
step:1993/2330 train_time:116432ms step_avg:58.42ms
step:1994/2330 train_time:116494ms step_avg:58.42ms
step:1995/2330 train_time:116551ms step_avg:58.42ms
step:1996/2330 train_time:116612ms step_avg:58.42ms
step:1997/2330 train_time:116669ms step_avg:58.42ms
step:1998/2330 train_time:116730ms step_avg:58.42ms
step:1999/2330 train_time:116786ms step_avg:58.42ms
step:2000/2330 train_time:116848ms step_avg:58.42ms
step:2000/2330 val_loss:3.7633 train_time:116929ms step_avg:58.46ms
step:2001/2330 train_time:116948ms step_avg:58.44ms
step:2002/2330 train_time:116968ms step_avg:58.43ms
step:2003/2330 train_time:117030ms step_avg:58.43ms
step:2004/2330 train_time:117093ms step_avg:58.43ms
step:2005/2330 train_time:117151ms step_avg:58.43ms
step:2006/2330 train_time:117211ms step_avg:58.43ms
step:2007/2330 train_time:117269ms step_avg:58.43ms
step:2008/2330 train_time:117329ms step_avg:58.43ms
step:2009/2330 train_time:117386ms step_avg:58.43ms
step:2010/2330 train_time:117445ms step_avg:58.43ms
step:2011/2330 train_time:117502ms step_avg:58.43ms
step:2012/2330 train_time:117562ms step_avg:58.43ms
step:2013/2330 train_time:117618ms step_avg:58.43ms
step:2014/2330 train_time:117678ms step_avg:58.43ms
step:2015/2330 train_time:117734ms step_avg:58.43ms
step:2016/2330 train_time:117795ms step_avg:58.43ms
step:2017/2330 train_time:117851ms step_avg:58.43ms
step:2018/2330 train_time:117913ms step_avg:58.43ms
step:2019/2330 train_time:117972ms step_avg:58.43ms
step:2020/2330 train_time:118035ms step_avg:58.43ms
step:2021/2330 train_time:118093ms step_avg:58.43ms
step:2022/2330 train_time:118156ms step_avg:58.44ms
step:2023/2330 train_time:118214ms step_avg:58.43ms
step:2024/2330 train_time:118277ms step_avg:58.44ms
step:2025/2330 train_time:118333ms step_avg:58.44ms
step:2026/2330 train_time:118395ms step_avg:58.44ms
step:2027/2330 train_time:118451ms step_avg:58.44ms
step:2028/2330 train_time:118512ms step_avg:58.44ms
step:2029/2330 train_time:118569ms step_avg:58.44ms
step:2030/2330 train_time:118629ms step_avg:58.44ms
step:2031/2330 train_time:118686ms step_avg:58.44ms
step:2032/2330 train_time:118746ms step_avg:58.44ms
step:2033/2330 train_time:118803ms step_avg:58.44ms
step:2034/2330 train_time:118863ms step_avg:58.44ms
step:2035/2330 train_time:118922ms step_avg:58.44ms
step:2036/2330 train_time:118983ms step_avg:58.44ms
step:2037/2330 train_time:119043ms step_avg:58.44ms
step:2038/2330 train_time:119103ms step_avg:58.44ms
step:2039/2330 train_time:119161ms step_avg:58.44ms
step:2040/2330 train_time:119222ms step_avg:58.44ms
step:2041/2330 train_time:119281ms step_avg:58.44ms
step:2042/2330 train_time:119341ms step_avg:58.44ms
step:2043/2330 train_time:119398ms step_avg:58.44ms
step:2044/2330 train_time:119459ms step_avg:58.44ms
step:2045/2330 train_time:119516ms step_avg:58.44ms
step:2046/2330 train_time:119577ms step_avg:58.44ms
step:2047/2330 train_time:119634ms step_avg:58.44ms
step:2048/2330 train_time:119695ms step_avg:58.44ms
step:2049/2330 train_time:119751ms step_avg:58.44ms
step:2050/2330 train_time:119813ms step_avg:58.45ms
step:2051/2330 train_time:119870ms step_avg:58.44ms
step:2052/2330 train_time:119932ms step_avg:58.45ms
step:2053/2330 train_time:119989ms step_avg:58.45ms
step:2054/2330 train_time:120051ms step_avg:58.45ms
step:2055/2330 train_time:120108ms step_avg:58.45ms
step:2056/2330 train_time:120170ms step_avg:58.45ms
step:2057/2330 train_time:120228ms step_avg:58.45ms
step:2058/2330 train_time:120289ms step_avg:58.45ms
step:2059/2330 train_time:120346ms step_avg:58.45ms
step:2060/2330 train_time:120406ms step_avg:58.45ms
step:2061/2330 train_time:120463ms step_avg:58.45ms
step:2062/2330 train_time:120524ms step_avg:58.45ms
step:2063/2330 train_time:120582ms step_avg:58.45ms
step:2064/2330 train_time:120642ms step_avg:58.45ms
step:2065/2330 train_time:120698ms step_avg:58.45ms
step:2066/2330 train_time:120759ms step_avg:58.45ms
step:2067/2330 train_time:120816ms step_avg:58.45ms
step:2068/2330 train_time:120879ms step_avg:58.45ms
step:2069/2330 train_time:120936ms step_avg:58.45ms
step:2070/2330 train_time:120997ms step_avg:58.45ms
step:2071/2330 train_time:121054ms step_avg:58.45ms
step:2072/2330 train_time:121118ms step_avg:58.45ms
step:2073/2330 train_time:121174ms step_avg:58.45ms
step:2074/2330 train_time:121236ms step_avg:58.46ms
step:2075/2330 train_time:121293ms step_avg:58.45ms
step:2076/2330 train_time:121355ms step_avg:58.46ms
step:2077/2330 train_time:121412ms step_avg:58.46ms
step:2078/2330 train_time:121473ms step_avg:58.46ms
step:2079/2330 train_time:121530ms step_avg:58.46ms
step:2080/2330 train_time:121591ms step_avg:58.46ms
step:2081/2330 train_time:121648ms step_avg:58.46ms
step:2082/2330 train_time:121708ms step_avg:58.46ms
step:2083/2330 train_time:121765ms step_avg:58.46ms
step:2084/2330 train_time:121826ms step_avg:58.46ms
step:2085/2330 train_time:121885ms step_avg:58.46ms
step:2086/2330 train_time:121945ms step_avg:58.46ms
step:2087/2330 train_time:122003ms step_avg:58.46ms
step:2088/2330 train_time:122063ms step_avg:58.46ms
step:2089/2330 train_time:122122ms step_avg:58.46ms
step:2090/2330 train_time:122182ms step_avg:58.46ms
step:2091/2330 train_time:122240ms step_avg:58.46ms
step:2092/2330 train_time:122301ms step_avg:58.46ms
step:2093/2330 train_time:122358ms step_avg:58.46ms
step:2094/2330 train_time:122419ms step_avg:58.46ms
step:2095/2330 train_time:122476ms step_avg:58.46ms
step:2096/2330 train_time:122538ms step_avg:58.46ms
step:2097/2330 train_time:122595ms step_avg:58.46ms
step:2098/2330 train_time:122656ms step_avg:58.46ms
step:2099/2330 train_time:122712ms step_avg:58.46ms
step:2100/2330 train_time:122774ms step_avg:58.46ms
step:2101/2330 train_time:122831ms step_avg:58.46ms
step:2102/2330 train_time:122892ms step_avg:58.46ms
step:2103/2330 train_time:122949ms step_avg:58.46ms
step:2104/2330 train_time:123010ms step_avg:58.47ms
step:2105/2330 train_time:123068ms step_avg:58.46ms
step:2106/2330 train_time:123128ms step_avg:58.47ms
step:2107/2330 train_time:123185ms step_avg:58.46ms
step:2108/2330 train_time:123246ms step_avg:58.47ms
step:2109/2330 train_time:123305ms step_avg:58.47ms
step:2110/2330 train_time:123365ms step_avg:58.47ms
step:2111/2330 train_time:123423ms step_avg:58.47ms
step:2112/2330 train_time:123484ms step_avg:58.47ms
step:2113/2330 train_time:123543ms step_avg:58.47ms
step:2114/2330 train_time:123604ms step_avg:58.47ms
step:2115/2330 train_time:123662ms step_avg:58.47ms
step:2116/2330 train_time:123722ms step_avg:58.47ms
step:2117/2330 train_time:123780ms step_avg:58.47ms
step:2118/2330 train_time:123841ms step_avg:58.47ms
step:2119/2330 train_time:123898ms step_avg:58.47ms
step:2120/2330 train_time:123960ms step_avg:58.47ms
step:2121/2330 train_time:124017ms step_avg:58.47ms
step:2122/2330 train_time:124079ms step_avg:58.47ms
step:2123/2330 train_time:124136ms step_avg:58.47ms
step:2124/2330 train_time:124199ms step_avg:58.47ms
step:2125/2330 train_time:124255ms step_avg:58.47ms
step:2126/2330 train_time:124317ms step_avg:58.47ms
step:2127/2330 train_time:124373ms step_avg:58.47ms
step:2128/2330 train_time:124436ms step_avg:58.48ms
step:2129/2330 train_time:124493ms step_avg:58.47ms
step:2130/2330 train_time:124554ms step_avg:58.48ms
step:2131/2330 train_time:124611ms step_avg:58.48ms
step:2132/2330 train_time:124673ms step_avg:58.48ms
step:2133/2330 train_time:124731ms step_avg:58.48ms
step:2134/2330 train_time:124790ms step_avg:58.48ms
step:2135/2330 train_time:124848ms step_avg:58.48ms
step:2136/2330 train_time:124908ms step_avg:58.48ms
step:2137/2330 train_time:124967ms step_avg:58.48ms
step:2138/2330 train_time:125027ms step_avg:58.48ms
step:2139/2330 train_time:125086ms step_avg:58.48ms
step:2140/2330 train_time:125146ms step_avg:58.48ms
step:2141/2330 train_time:125203ms step_avg:58.48ms
step:2142/2330 train_time:125263ms step_avg:58.48ms
step:2143/2330 train_time:125321ms step_avg:58.48ms
step:2144/2330 train_time:125381ms step_avg:58.48ms
step:2145/2330 train_time:125439ms step_avg:58.48ms
step:2146/2330 train_time:125500ms step_avg:58.48ms
step:2147/2330 train_time:125557ms step_avg:58.48ms
step:2148/2330 train_time:125618ms step_avg:58.48ms
step:2149/2330 train_time:125675ms step_avg:58.48ms
step:2150/2330 train_time:125738ms step_avg:58.48ms
step:2151/2330 train_time:125794ms step_avg:58.48ms
step:2152/2330 train_time:125857ms step_avg:58.48ms
step:2153/2330 train_time:125914ms step_avg:58.48ms
step:2154/2330 train_time:125976ms step_avg:58.48ms
step:2155/2330 train_time:126033ms step_avg:58.48ms
step:2156/2330 train_time:126096ms step_avg:58.49ms
step:2157/2330 train_time:126152ms step_avg:58.49ms
step:2158/2330 train_time:126215ms step_avg:58.49ms
step:2159/2330 train_time:126272ms step_avg:58.49ms
step:2160/2330 train_time:126334ms step_avg:58.49ms
step:2161/2330 train_time:126391ms step_avg:58.49ms
step:2162/2330 train_time:126452ms step_avg:58.49ms
step:2163/2330 train_time:126509ms step_avg:58.49ms
step:2164/2330 train_time:126569ms step_avg:58.49ms
step:2165/2330 train_time:126627ms step_avg:58.49ms
step:2166/2330 train_time:126687ms step_avg:58.49ms
step:2167/2330 train_time:126745ms step_avg:58.49ms
step:2168/2330 train_time:126805ms step_avg:58.49ms
step:2169/2330 train_time:126863ms step_avg:58.49ms
step:2170/2330 train_time:126924ms step_avg:58.49ms
step:2171/2330 train_time:126981ms step_avg:58.49ms
step:2172/2330 train_time:127042ms step_avg:58.49ms
step:2173/2330 train_time:127099ms step_avg:58.49ms
step:2174/2330 train_time:127161ms step_avg:58.49ms
step:2175/2330 train_time:127218ms step_avg:58.49ms
step:2176/2330 train_time:127280ms step_avg:58.49ms
step:2177/2330 train_time:127337ms step_avg:58.49ms
step:2178/2330 train_time:127399ms step_avg:58.49ms
step:2179/2330 train_time:127455ms step_avg:58.49ms
step:2180/2330 train_time:127518ms step_avg:58.49ms
step:2181/2330 train_time:127575ms step_avg:58.49ms
step:2182/2330 train_time:127636ms step_avg:58.49ms
step:2183/2330 train_time:127693ms step_avg:58.49ms
step:2184/2330 train_time:127754ms step_avg:58.50ms
step:2185/2330 train_time:127810ms step_avg:58.49ms
step:2186/2330 train_time:127872ms step_avg:58.50ms
step:2187/2330 train_time:127929ms step_avg:58.50ms
step:2188/2330 train_time:127990ms step_avg:58.50ms
step:2189/2330 train_time:128048ms step_avg:58.50ms
step:2190/2330 train_time:128108ms step_avg:58.50ms
step:2191/2330 train_time:128166ms step_avg:58.50ms
step:2192/2330 train_time:128227ms step_avg:58.50ms
step:2193/2330 train_time:128285ms step_avg:58.50ms
step:2194/2330 train_time:128345ms step_avg:58.50ms
step:2195/2330 train_time:128404ms step_avg:58.50ms
step:2196/2330 train_time:128464ms step_avg:58.50ms
step:2197/2330 train_time:128522ms step_avg:58.50ms
step:2198/2330 train_time:128582ms step_avg:58.50ms
step:2199/2330 train_time:128640ms step_avg:58.50ms
step:2200/2330 train_time:128701ms step_avg:58.50ms
step:2201/2330 train_time:128757ms step_avg:58.50ms
step:2202/2330 train_time:128820ms step_avg:58.50ms
step:2203/2330 train_time:128876ms step_avg:58.50ms
step:2204/2330 train_time:128939ms step_avg:58.50ms
step:2205/2330 train_time:128996ms step_avg:58.50ms
step:2206/2330 train_time:129058ms step_avg:58.50ms
step:2207/2330 train_time:129114ms step_avg:58.50ms
step:2208/2330 train_time:129176ms step_avg:58.50ms
step:2209/2330 train_time:129233ms step_avg:58.50ms
step:2210/2330 train_time:129294ms step_avg:58.50ms
step:2211/2330 train_time:129351ms step_avg:58.50ms
step:2212/2330 train_time:129414ms step_avg:58.51ms
step:2213/2330 train_time:129471ms step_avg:58.50ms
step:2214/2330 train_time:129532ms step_avg:58.51ms
step:2215/2330 train_time:129589ms step_avg:58.51ms
step:2216/2330 train_time:129649ms step_avg:58.51ms
step:2217/2330 train_time:129707ms step_avg:58.51ms
step:2218/2330 train_time:129767ms step_avg:58.51ms
step:2219/2330 train_time:129824ms step_avg:58.51ms
step:2220/2330 train_time:129884ms step_avg:58.51ms
step:2221/2330 train_time:129942ms step_avg:58.51ms
step:2222/2330 train_time:130002ms step_avg:58.51ms
step:2223/2330 train_time:130059ms step_avg:58.51ms
step:2224/2330 train_time:130121ms step_avg:58.51ms
step:2225/2330 train_time:130178ms step_avg:58.51ms
step:2226/2330 train_time:130239ms step_avg:58.51ms
step:2227/2330 train_time:130296ms step_avg:58.51ms
step:2228/2330 train_time:130359ms step_avg:58.51ms
step:2229/2330 train_time:130416ms step_avg:58.51ms
step:2230/2330 train_time:130478ms step_avg:58.51ms
step:2231/2330 train_time:130534ms step_avg:58.51ms
step:2232/2330 train_time:130596ms step_avg:58.51ms
step:2233/2330 train_time:130652ms step_avg:58.51ms
step:2234/2330 train_time:130715ms step_avg:58.51ms
step:2235/2330 train_time:130771ms step_avg:58.51ms
step:2236/2330 train_time:130834ms step_avg:58.51ms
step:2237/2330 train_time:130890ms step_avg:58.51ms
step:2238/2330 train_time:130953ms step_avg:58.51ms
step:2239/2330 train_time:131010ms step_avg:58.51ms
step:2240/2330 train_time:131070ms step_avg:58.51ms
step:2241/2330 train_time:131128ms step_avg:58.51ms
step:2242/2330 train_time:131188ms step_avg:58.51ms
step:2243/2330 train_time:131246ms step_avg:58.51ms
step:2244/2330 train_time:131307ms step_avg:58.51ms
step:2245/2330 train_time:131365ms step_avg:58.51ms
step:2246/2330 train_time:131425ms step_avg:58.52ms
step:2247/2330 train_time:131484ms step_avg:58.52ms
step:2248/2330 train_time:131544ms step_avg:58.52ms
step:2249/2330 train_time:131603ms step_avg:58.52ms
step:2250/2330 train_time:131663ms step_avg:58.52ms
step:2250/2330 val_loss:3.7162 train_time:131744ms step_avg:58.55ms
step:2251/2330 train_time:131763ms step_avg:58.54ms
step:2252/2330 train_time:131785ms step_avg:58.52ms
step:2253/2330 train_time:131844ms step_avg:58.52ms
step:2254/2330 train_time:131910ms step_avg:58.52ms
step:2255/2330 train_time:131968ms step_avg:58.52ms
step:2256/2330 train_time:132030ms step_avg:58.52ms
step:2257/2330 train_time:132087ms step_avg:58.52ms
step:2258/2330 train_time:132148ms step_avg:58.52ms
step:2259/2330 train_time:132205ms step_avg:58.52ms
step:2260/2330 train_time:132266ms step_avg:58.52ms
step:2261/2330 train_time:132323ms step_avg:58.52ms
step:2262/2330 train_time:132383ms step_avg:58.52ms
step:2263/2330 train_time:132439ms step_avg:58.52ms
step:2264/2330 train_time:132501ms step_avg:58.53ms
step:2265/2330 train_time:132557ms step_avg:58.52ms
step:2266/2330 train_time:132619ms step_avg:58.53ms
step:2267/2330 train_time:132675ms step_avg:58.52ms
step:2268/2330 train_time:132737ms step_avg:58.53ms
step:2269/2330 train_time:132796ms step_avg:58.53ms
step:2270/2330 train_time:132859ms step_avg:58.53ms
step:2271/2330 train_time:132916ms step_avg:58.53ms
step:2272/2330 train_time:132978ms step_avg:58.53ms
step:2273/2330 train_time:133036ms step_avg:58.53ms
step:2274/2330 train_time:133097ms step_avg:58.53ms
step:2275/2330 train_time:133154ms step_avg:58.53ms
step:2276/2330 train_time:133214ms step_avg:58.53ms
step:2277/2330 train_time:133272ms step_avg:58.53ms
step:2278/2330 train_time:133332ms step_avg:58.53ms
step:2279/2330 train_time:133389ms step_avg:58.53ms
step:2280/2330 train_time:133450ms step_avg:58.53ms
step:2281/2330 train_time:133506ms step_avg:58.53ms
step:2282/2330 train_time:133568ms step_avg:58.53ms
step:2283/2330 train_time:133624ms step_avg:58.53ms
step:2284/2330 train_time:133686ms step_avg:58.53ms
step:2285/2330 train_time:133743ms step_avg:58.53ms
step:2286/2330 train_time:133807ms step_avg:58.53ms
step:2287/2330 train_time:133864ms step_avg:58.53ms
step:2288/2330 train_time:133928ms step_avg:58.53ms
step:2289/2330 train_time:133984ms step_avg:58.53ms
step:2290/2330 train_time:134047ms step_avg:58.54ms
step:2291/2330 train_time:134104ms step_avg:58.54ms
step:2292/2330 train_time:134166ms step_avg:58.54ms
step:2293/2330 train_time:134221ms step_avg:58.54ms
step:2294/2330 train_time:134285ms step_avg:58.54ms
step:2295/2330 train_time:134342ms step_avg:58.54ms
step:2296/2330 train_time:134402ms step_avg:58.54ms
step:2297/2330 train_time:134459ms step_avg:58.54ms
step:2298/2330 train_time:134519ms step_avg:58.54ms
step:2299/2330 train_time:134576ms step_avg:58.54ms
step:2300/2330 train_time:134636ms step_avg:58.54ms
step:2301/2330 train_time:134694ms step_avg:58.54ms
step:2302/2330 train_time:134755ms step_avg:58.54ms
step:2303/2330 train_time:134813ms step_avg:58.54ms
step:2304/2330 train_time:134873ms step_avg:58.54ms
step:2305/2330 train_time:134931ms step_avg:58.54ms
step:2306/2330 train_time:134992ms step_avg:58.54ms
step:2307/2330 train_time:135050ms step_avg:58.54ms
step:2308/2330 train_time:135111ms step_avg:58.54ms
step:2309/2330 train_time:135169ms step_avg:58.54ms
step:2310/2330 train_time:135230ms step_avg:58.54ms
step:2311/2330 train_time:135287ms step_avg:58.54ms
step:2312/2330 train_time:135348ms step_avg:58.54ms
step:2313/2330 train_time:135404ms step_avg:58.54ms
step:2314/2330 train_time:135466ms step_avg:58.54ms
step:2315/2330 train_time:135522ms step_avg:58.54ms
step:2316/2330 train_time:135583ms step_avg:58.54ms
step:2317/2330 train_time:135640ms step_avg:58.54ms
step:2318/2330 train_time:135702ms step_avg:58.54ms
step:2319/2330 train_time:135759ms step_avg:58.54ms
step:2320/2330 train_time:135822ms step_avg:58.54ms
step:2321/2330 train_time:135880ms step_avg:58.54ms
step:2322/2330 train_time:135943ms step_avg:58.55ms
step:2323/2330 train_time:136000ms step_avg:58.54ms
step:2324/2330 train_time:136061ms step_avg:58.55ms
step:2325/2330 train_time:136118ms step_avg:58.55ms
step:2326/2330 train_time:136181ms step_avg:58.55ms
step:2327/2330 train_time:136238ms step_avg:58.55ms
step:2328/2330 train_time:136297ms step_avg:58.55ms
step:2329/2330 train_time:136355ms step_avg:58.55ms
step:2330/2330 train_time:136415ms step_avg:58.55ms
step:2330/2330 val_loss:3.7005 train_time:136496ms step_avg:58.58ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
