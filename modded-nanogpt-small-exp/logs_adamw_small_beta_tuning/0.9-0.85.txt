import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--beta1', type=str, required=True)
parser.add_argument('--beta2', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_beta_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=6e-4,  betas=(float(args2.beta1), float(args2.beta2)), eps=1e-10, weight_decay=0.0, fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt", exist_ok=True)
            torch.save(log, f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 03:55:14 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   29C    P0             114W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   26C    P0             115W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   25C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   27C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:86ms step_avg:86.11ms
step:2/2330 train_time:197ms step_avg:98.33ms
step:3/2330 train_time:214ms step_avg:71.47ms
step:4/2330 train_time:234ms step_avg:58.41ms
step:5/2330 train_time:288ms step_avg:57.51ms
step:6/2330 train_time:345ms step_avg:57.58ms
step:7/2330 train_time:401ms step_avg:57.22ms
step:8/2330 train_time:459ms step_avg:57.39ms
step:9/2330 train_time:514ms step_avg:57.16ms
step:10/2330 train_time:573ms step_avg:57.26ms
step:11/2330 train_time:628ms step_avg:57.07ms
step:12/2330 train_time:687ms step_avg:57.23ms
step:13/2330 train_time:742ms step_avg:57.07ms
step:14/2330 train_time:800ms step_avg:57.16ms
step:15/2330 train_time:855ms step_avg:57.03ms
step:16/2330 train_time:914ms step_avg:57.10ms
step:17/2330 train_time:968ms step_avg:56.96ms
step:18/2330 train_time:1027ms step_avg:57.08ms
step:19/2330 train_time:1084ms step_avg:57.03ms
step:20/2330 train_time:1147ms step_avg:57.34ms
step:21/2330 train_time:1204ms step_avg:57.34ms
step:22/2330 train_time:1264ms step_avg:57.44ms
step:23/2330 train_time:1319ms step_avg:57.36ms
step:24/2330 train_time:1377ms step_avg:57.39ms
step:25/2330 train_time:1433ms step_avg:57.30ms
step:26/2330 train_time:1491ms step_avg:57.35ms
step:27/2330 train_time:1546ms step_avg:57.26ms
step:28/2330 train_time:1605ms step_avg:57.32ms
step:29/2330 train_time:1660ms step_avg:57.24ms
step:30/2330 train_time:1719ms step_avg:57.29ms
step:31/2330 train_time:1774ms step_avg:57.21ms
step:32/2330 train_time:1833ms step_avg:57.27ms
step:33/2330 train_time:1888ms step_avg:57.20ms
step:34/2330 train_time:1946ms step_avg:57.24ms
step:35/2330 train_time:2001ms step_avg:57.18ms
step:36/2330 train_time:2061ms step_avg:57.26ms
step:37/2330 train_time:2118ms step_avg:57.23ms
step:38/2330 train_time:2178ms step_avg:57.32ms
step:39/2330 train_time:2235ms step_avg:57.32ms
step:40/2330 train_time:2295ms step_avg:57.36ms
step:41/2330 train_time:2351ms step_avg:57.34ms
step:42/2330 train_time:2410ms step_avg:57.39ms
step:43/2330 train_time:2466ms step_avg:57.35ms
step:44/2330 train_time:2525ms step_avg:57.39ms
step:45/2330 train_time:2580ms step_avg:57.34ms
step:46/2330 train_time:2638ms step_avg:57.36ms
step:47/2330 train_time:2694ms step_avg:57.32ms
step:48/2330 train_time:2752ms step_avg:57.34ms
step:49/2330 train_time:2808ms step_avg:57.31ms
step:50/2330 train_time:2866ms step_avg:57.33ms
step:51/2330 train_time:2921ms step_avg:57.28ms
step:52/2330 train_time:2980ms step_avg:57.31ms
step:53/2330 train_time:3036ms step_avg:57.28ms
step:54/2330 train_time:3095ms step_avg:57.31ms
step:55/2330 train_time:3151ms step_avg:57.29ms
step:56/2330 train_time:3211ms step_avg:57.34ms
step:57/2330 train_time:3266ms step_avg:57.30ms
step:58/2330 train_time:3326ms step_avg:57.35ms
step:59/2330 train_time:3382ms step_avg:57.32ms
step:60/2330 train_time:3442ms step_avg:57.37ms
step:61/2330 train_time:3498ms step_avg:57.35ms
step:62/2330 train_time:3556ms step_avg:57.36ms
step:63/2330 train_time:3612ms step_avg:57.33ms
step:64/2330 train_time:3671ms step_avg:57.36ms
step:65/2330 train_time:3726ms step_avg:57.33ms
step:66/2330 train_time:3785ms step_avg:57.35ms
step:67/2330 train_time:3841ms step_avg:57.32ms
step:68/2330 train_time:3899ms step_avg:57.34ms
step:69/2330 train_time:3954ms step_avg:57.31ms
step:70/2330 train_time:4013ms step_avg:57.33ms
step:71/2330 train_time:4068ms step_avg:57.30ms
step:72/2330 train_time:4129ms step_avg:57.34ms
step:73/2330 train_time:4184ms step_avg:57.32ms
step:74/2330 train_time:4244ms step_avg:57.35ms
step:75/2330 train_time:4300ms step_avg:57.33ms
step:76/2330 train_time:4358ms step_avg:57.35ms
step:77/2330 train_time:4414ms step_avg:57.32ms
step:78/2330 train_time:4474ms step_avg:57.36ms
step:79/2330 train_time:4530ms step_avg:57.34ms
step:80/2330 train_time:4589ms step_avg:57.37ms
step:81/2330 train_time:4645ms step_avg:57.35ms
step:82/2330 train_time:4704ms step_avg:57.36ms
step:83/2330 train_time:4760ms step_avg:57.34ms
step:84/2330 train_time:4818ms step_avg:57.36ms
step:85/2330 train_time:4874ms step_avg:57.34ms
step:86/2330 train_time:4932ms step_avg:57.35ms
step:87/2330 train_time:4988ms step_avg:57.34ms
step:88/2330 train_time:5046ms step_avg:57.35ms
step:89/2330 train_time:5102ms step_avg:57.33ms
step:90/2330 train_time:5160ms step_avg:57.34ms
step:91/2330 train_time:5217ms step_avg:57.33ms
step:92/2330 train_time:5275ms step_avg:57.34ms
step:93/2330 train_time:5331ms step_avg:57.32ms
step:94/2330 train_time:5390ms step_avg:57.34ms
step:95/2330 train_time:5447ms step_avg:57.33ms
step:96/2330 train_time:5505ms step_avg:57.35ms
step:97/2330 train_time:5561ms step_avg:57.33ms
step:98/2330 train_time:5620ms step_avg:57.35ms
step:99/2330 train_time:5676ms step_avg:57.33ms
step:100/2330 train_time:5734ms step_avg:57.34ms
step:101/2330 train_time:5790ms step_avg:57.33ms
step:102/2330 train_time:5849ms step_avg:57.34ms
step:103/2330 train_time:5904ms step_avg:57.32ms
step:104/2330 train_time:5963ms step_avg:57.34ms
step:105/2330 train_time:6019ms step_avg:57.33ms
step:106/2330 train_time:6078ms step_avg:57.34ms
step:107/2330 train_time:6134ms step_avg:57.33ms
step:108/2330 train_time:6192ms step_avg:57.34ms
step:109/2330 train_time:6248ms step_avg:57.32ms
step:110/2330 train_time:6308ms step_avg:57.34ms
step:111/2330 train_time:6363ms step_avg:57.33ms
step:112/2330 train_time:6423ms step_avg:57.35ms
step:113/2330 train_time:6479ms step_avg:57.33ms
step:114/2330 train_time:6538ms step_avg:57.35ms
step:115/2330 train_time:6593ms step_avg:57.33ms
step:116/2330 train_time:6652ms step_avg:57.34ms
step:117/2330 train_time:6708ms step_avg:57.33ms
step:118/2330 train_time:6768ms step_avg:57.36ms
step:119/2330 train_time:6824ms step_avg:57.34ms
step:120/2330 train_time:6883ms step_avg:57.35ms
step:121/2330 train_time:6938ms step_avg:57.34ms
step:122/2330 train_time:6998ms step_avg:57.36ms
step:123/2330 train_time:7054ms step_avg:57.35ms
step:124/2330 train_time:7112ms step_avg:57.35ms
step:125/2330 train_time:7167ms step_avg:57.34ms
step:126/2330 train_time:7227ms step_avg:57.36ms
step:127/2330 train_time:7282ms step_avg:57.34ms
step:128/2330 train_time:7342ms step_avg:57.36ms
step:129/2330 train_time:7397ms step_avg:57.34ms
step:130/2330 train_time:7456ms step_avg:57.36ms
step:131/2330 train_time:7513ms step_avg:57.35ms
step:132/2330 train_time:7571ms step_avg:57.36ms
step:133/2330 train_time:7626ms step_avg:57.34ms
step:134/2330 train_time:7686ms step_avg:57.36ms
step:135/2330 train_time:7741ms step_avg:57.34ms
step:136/2330 train_time:7801ms step_avg:57.36ms
step:137/2330 train_time:7857ms step_avg:57.35ms
step:138/2330 train_time:7916ms step_avg:57.36ms
step:139/2330 train_time:7972ms step_avg:57.35ms
step:140/2330 train_time:8031ms step_avg:57.36ms
step:141/2330 train_time:8086ms step_avg:57.35ms
step:142/2330 train_time:8146ms step_avg:57.37ms
step:143/2330 train_time:8202ms step_avg:57.35ms
step:144/2330 train_time:8261ms step_avg:57.37ms
step:145/2330 train_time:8317ms step_avg:57.36ms
step:146/2330 train_time:8374ms step_avg:57.36ms
step:147/2330 train_time:8430ms step_avg:57.35ms
step:148/2330 train_time:8489ms step_avg:57.36ms
step:149/2330 train_time:8545ms step_avg:57.35ms
step:150/2330 train_time:8604ms step_avg:57.36ms
step:151/2330 train_time:8660ms step_avg:57.35ms
step:152/2330 train_time:8719ms step_avg:57.36ms
step:153/2330 train_time:8774ms step_avg:57.35ms
step:154/2330 train_time:8833ms step_avg:57.36ms
step:155/2330 train_time:8889ms step_avg:57.35ms
step:156/2330 train_time:8949ms step_avg:57.36ms
step:157/2330 train_time:9004ms step_avg:57.35ms
step:158/2330 train_time:9062ms step_avg:57.36ms
step:159/2330 train_time:9118ms step_avg:57.35ms
step:160/2330 train_time:9177ms step_avg:57.36ms
step:161/2330 train_time:9233ms step_avg:57.35ms
step:162/2330 train_time:9292ms step_avg:57.36ms
step:163/2330 train_time:9347ms step_avg:57.35ms
step:164/2330 train_time:9407ms step_avg:57.36ms
step:165/2330 train_time:9462ms step_avg:57.35ms
step:166/2330 train_time:9522ms step_avg:57.36ms
step:167/2330 train_time:9577ms step_avg:57.35ms
step:168/2330 train_time:9636ms step_avg:57.36ms
step:169/2330 train_time:9692ms step_avg:57.35ms
step:170/2330 train_time:9752ms step_avg:57.36ms
step:171/2330 train_time:9807ms step_avg:57.35ms
step:172/2330 train_time:9867ms step_avg:57.36ms
step:173/2330 train_time:9923ms step_avg:57.36ms
step:174/2330 train_time:9982ms step_avg:57.37ms
step:175/2330 train_time:10038ms step_avg:57.36ms
step:176/2330 train_time:10096ms step_avg:57.37ms
step:177/2330 train_time:10152ms step_avg:57.36ms
step:178/2330 train_time:10211ms step_avg:57.37ms
step:179/2330 train_time:10267ms step_avg:57.36ms
step:180/2330 train_time:10327ms step_avg:57.37ms
step:181/2330 train_time:10382ms step_avg:57.36ms
step:182/2330 train_time:10442ms step_avg:57.37ms
step:183/2330 train_time:10498ms step_avg:57.37ms
step:184/2330 train_time:10557ms step_avg:57.37ms
step:185/2330 train_time:10612ms step_avg:57.36ms
step:186/2330 train_time:10671ms step_avg:57.37ms
step:187/2330 train_time:10727ms step_avg:57.36ms
step:188/2330 train_time:10786ms step_avg:57.37ms
step:189/2330 train_time:10842ms step_avg:57.36ms
step:190/2330 train_time:10901ms step_avg:57.38ms
step:191/2330 train_time:10957ms step_avg:57.37ms
step:192/2330 train_time:11017ms step_avg:57.38ms
step:193/2330 train_time:11072ms step_avg:57.37ms
step:194/2330 train_time:11132ms step_avg:57.38ms
step:195/2330 train_time:11187ms step_avg:57.37ms
step:196/2330 train_time:11247ms step_avg:57.38ms
step:197/2330 train_time:11302ms step_avg:57.37ms
step:198/2330 train_time:11362ms step_avg:57.39ms
step:199/2330 train_time:11418ms step_avg:57.38ms
step:200/2330 train_time:11477ms step_avg:57.39ms
step:201/2330 train_time:11533ms step_avg:57.38ms
step:202/2330 train_time:11592ms step_avg:57.39ms
step:203/2330 train_time:11648ms step_avg:57.38ms
step:204/2330 train_time:11707ms step_avg:57.39ms
step:205/2330 train_time:11762ms step_avg:57.37ms
step:206/2330 train_time:11822ms step_avg:57.39ms
step:207/2330 train_time:11878ms step_avg:57.38ms
step:208/2330 train_time:11936ms step_avg:57.39ms
step:209/2330 train_time:11992ms step_avg:57.38ms
step:210/2330 train_time:12051ms step_avg:57.39ms
step:211/2330 train_time:12107ms step_avg:57.38ms
step:212/2330 train_time:12167ms step_avg:57.39ms
step:213/2330 train_time:12222ms step_avg:57.38ms
step:214/2330 train_time:12282ms step_avg:57.39ms
step:215/2330 train_time:12338ms step_avg:57.39ms
step:216/2330 train_time:12398ms step_avg:57.40ms
step:217/2330 train_time:12454ms step_avg:57.39ms
step:218/2330 train_time:12512ms step_avg:57.40ms
step:219/2330 train_time:12568ms step_avg:57.39ms
step:220/2330 train_time:12628ms step_avg:57.40ms
step:221/2330 train_time:12683ms step_avg:57.39ms
step:222/2330 train_time:12743ms step_avg:57.40ms
step:223/2330 train_time:12798ms step_avg:57.39ms
step:224/2330 train_time:12857ms step_avg:57.40ms
step:225/2330 train_time:12912ms step_avg:57.39ms
step:226/2330 train_time:12972ms step_avg:57.40ms
step:227/2330 train_time:13028ms step_avg:57.39ms
step:228/2330 train_time:13087ms step_avg:57.40ms
step:229/2330 train_time:13143ms step_avg:57.39ms
step:230/2330 train_time:13202ms step_avg:57.40ms
step:231/2330 train_time:13258ms step_avg:57.39ms
step:232/2330 train_time:13317ms step_avg:57.40ms
step:233/2330 train_time:13372ms step_avg:57.39ms
step:234/2330 train_time:13431ms step_avg:57.40ms
step:235/2330 train_time:13487ms step_avg:57.39ms
step:236/2330 train_time:13546ms step_avg:57.40ms
step:237/2330 train_time:13602ms step_avg:57.39ms
step:238/2330 train_time:13661ms step_avg:57.40ms
step:239/2330 train_time:13717ms step_avg:57.39ms
step:240/2330 train_time:13776ms step_avg:57.40ms
step:241/2330 train_time:13831ms step_avg:57.39ms
step:242/2330 train_time:13891ms step_avg:57.40ms
step:243/2330 train_time:13946ms step_avg:57.39ms
step:244/2330 train_time:14006ms step_avg:57.40ms
step:245/2330 train_time:14062ms step_avg:57.39ms
step:246/2330 train_time:14121ms step_avg:57.40ms
step:247/2330 train_time:14177ms step_avg:57.40ms
step:248/2330 train_time:14236ms step_avg:57.40ms
step:249/2330 train_time:14291ms step_avg:57.39ms
step:250/2330 train_time:14351ms step_avg:57.40ms
step:250/2330 val_loss:4.9256 train_time:14430ms step_avg:57.72ms
step:251/2330 train_time:14449ms step_avg:57.56ms
step:252/2330 train_time:14468ms step_avg:57.41ms
step:253/2330 train_time:14522ms step_avg:57.40ms
step:254/2330 train_time:14586ms step_avg:57.43ms
step:255/2330 train_time:14641ms step_avg:57.42ms
step:256/2330 train_time:14706ms step_avg:57.44ms
step:257/2330 train_time:14761ms step_avg:57.44ms
step:258/2330 train_time:14820ms step_avg:57.44ms
step:259/2330 train_time:14875ms step_avg:57.43ms
step:260/2330 train_time:14935ms step_avg:57.44ms
step:261/2330 train_time:14991ms step_avg:57.44ms
step:262/2330 train_time:15049ms step_avg:57.44ms
step:263/2330 train_time:15104ms step_avg:57.43ms
step:264/2330 train_time:15162ms step_avg:57.43ms
step:265/2330 train_time:15217ms step_avg:57.42ms
step:266/2330 train_time:15276ms step_avg:57.43ms
step:267/2330 train_time:15333ms step_avg:57.43ms
step:268/2330 train_time:15392ms step_avg:57.43ms
step:269/2330 train_time:15450ms step_avg:57.43ms
step:270/2330 train_time:15509ms step_avg:57.44ms
step:271/2330 train_time:15566ms step_avg:57.44ms
step:272/2330 train_time:15625ms step_avg:57.45ms
step:273/2330 train_time:15681ms step_avg:57.44ms
step:274/2330 train_time:15740ms step_avg:57.45ms
step:275/2330 train_time:15796ms step_avg:57.44ms
step:276/2330 train_time:15855ms step_avg:57.44ms
step:277/2330 train_time:15910ms step_avg:57.44ms
step:278/2330 train_time:15970ms step_avg:57.44ms
step:279/2330 train_time:16025ms step_avg:57.44ms
step:280/2330 train_time:16083ms step_avg:57.44ms
step:281/2330 train_time:16139ms step_avg:57.43ms
step:282/2330 train_time:16197ms step_avg:57.44ms
step:283/2330 train_time:16252ms step_avg:57.43ms
step:284/2330 train_time:16312ms step_avg:57.44ms
step:285/2330 train_time:16367ms step_avg:57.43ms
step:286/2330 train_time:16427ms step_avg:57.44ms
step:287/2330 train_time:16484ms step_avg:57.43ms
step:288/2330 train_time:16543ms step_avg:57.44ms
step:289/2330 train_time:16599ms step_avg:57.44ms
step:290/2330 train_time:16660ms step_avg:57.45ms
step:291/2330 train_time:16716ms step_avg:57.44ms
step:292/2330 train_time:16775ms step_avg:57.45ms
step:293/2330 train_time:16830ms step_avg:57.44ms
step:294/2330 train_time:16890ms step_avg:57.45ms
step:295/2330 train_time:16947ms step_avg:57.45ms
step:296/2330 train_time:17005ms step_avg:57.45ms
step:297/2330 train_time:17061ms step_avg:57.44ms
step:298/2330 train_time:17120ms step_avg:57.45ms
step:299/2330 train_time:17175ms step_avg:57.44ms
step:300/2330 train_time:17234ms step_avg:57.45ms
step:301/2330 train_time:17290ms step_avg:57.44ms
step:302/2330 train_time:17349ms step_avg:57.45ms
step:303/2330 train_time:17406ms step_avg:57.44ms
step:304/2330 train_time:17464ms step_avg:57.45ms
step:305/2330 train_time:17520ms step_avg:57.44ms
step:306/2330 train_time:17579ms step_avg:57.45ms
step:307/2330 train_time:17635ms step_avg:57.44ms
step:308/2330 train_time:17696ms step_avg:57.45ms
step:309/2330 train_time:17751ms step_avg:57.45ms
step:310/2330 train_time:17812ms step_avg:57.46ms
step:311/2330 train_time:17868ms step_avg:57.45ms
step:312/2330 train_time:17926ms step_avg:57.46ms
step:313/2330 train_time:17981ms step_avg:57.45ms
step:314/2330 train_time:18041ms step_avg:57.45ms
step:315/2330 train_time:18097ms step_avg:57.45ms
step:316/2330 train_time:18155ms step_avg:57.45ms
step:317/2330 train_time:18211ms step_avg:57.45ms
step:318/2330 train_time:18270ms step_avg:57.45ms
step:319/2330 train_time:18326ms step_avg:57.45ms
step:320/2330 train_time:18385ms step_avg:57.45ms
step:321/2330 train_time:18441ms step_avg:57.45ms
step:322/2330 train_time:18501ms step_avg:57.46ms
step:323/2330 train_time:18557ms step_avg:57.45ms
step:324/2330 train_time:18616ms step_avg:57.46ms
step:325/2330 train_time:18672ms step_avg:57.45ms
step:326/2330 train_time:18732ms step_avg:57.46ms
step:327/2330 train_time:18788ms step_avg:57.46ms
step:328/2330 train_time:18847ms step_avg:57.46ms
step:329/2330 train_time:18903ms step_avg:57.46ms
step:330/2330 train_time:18962ms step_avg:57.46ms
step:331/2330 train_time:19018ms step_avg:57.46ms
step:332/2330 train_time:19077ms step_avg:57.46ms
step:333/2330 train_time:19132ms step_avg:57.45ms
step:334/2330 train_time:19192ms step_avg:57.46ms
step:335/2330 train_time:19248ms step_avg:57.46ms
step:336/2330 train_time:19306ms step_avg:57.46ms
step:337/2330 train_time:19362ms step_avg:57.45ms
step:338/2330 train_time:19421ms step_avg:57.46ms
step:339/2330 train_time:19477ms step_avg:57.45ms
step:340/2330 train_time:19536ms step_avg:57.46ms
step:341/2330 train_time:19592ms step_avg:57.45ms
step:342/2330 train_time:19651ms step_avg:57.46ms
step:343/2330 train_time:19708ms step_avg:57.46ms
step:344/2330 train_time:19766ms step_avg:57.46ms
step:345/2330 train_time:19822ms step_avg:57.46ms
step:346/2330 train_time:19881ms step_avg:57.46ms
step:347/2330 train_time:19937ms step_avg:57.46ms
step:348/2330 train_time:19997ms step_avg:57.46ms
step:349/2330 train_time:20052ms step_avg:57.46ms
step:350/2330 train_time:20112ms step_avg:57.46ms
step:351/2330 train_time:20167ms step_avg:57.46ms
step:352/2330 train_time:20225ms step_avg:57.46ms
step:353/2330 train_time:20281ms step_avg:57.45ms
step:354/2330 train_time:20341ms step_avg:57.46ms
step:355/2330 train_time:20397ms step_avg:57.46ms
step:356/2330 train_time:20457ms step_avg:57.46ms
step:357/2330 train_time:20512ms step_avg:57.46ms
step:358/2330 train_time:20571ms step_avg:57.46ms
step:359/2330 train_time:20627ms step_avg:57.46ms
step:360/2330 train_time:20687ms step_avg:57.46ms
step:361/2330 train_time:20744ms step_avg:57.46ms
step:362/2330 train_time:20803ms step_avg:57.47ms
step:363/2330 train_time:20859ms step_avg:57.46ms
step:364/2330 train_time:20917ms step_avg:57.47ms
step:365/2330 train_time:20973ms step_avg:57.46ms
step:366/2330 train_time:21033ms step_avg:57.47ms
step:367/2330 train_time:21088ms step_avg:57.46ms
step:368/2330 train_time:21147ms step_avg:57.46ms
step:369/2330 train_time:21202ms step_avg:57.46ms
step:370/2330 train_time:21261ms step_avg:57.46ms
step:371/2330 train_time:21317ms step_avg:57.46ms
step:372/2330 train_time:21376ms step_avg:57.46ms
step:373/2330 train_time:21431ms step_avg:57.46ms
step:374/2330 train_time:21491ms step_avg:57.46ms
step:375/2330 train_time:21547ms step_avg:57.46ms
step:376/2330 train_time:21606ms step_avg:57.46ms
step:377/2330 train_time:21662ms step_avg:57.46ms
step:378/2330 train_time:21721ms step_avg:57.46ms
step:379/2330 train_time:21777ms step_avg:57.46ms
step:380/2330 train_time:21836ms step_avg:57.46ms
step:381/2330 train_time:21892ms step_avg:57.46ms
step:382/2330 train_time:21951ms step_avg:57.46ms
step:383/2330 train_time:22008ms step_avg:57.46ms
step:384/2330 train_time:22067ms step_avg:57.47ms
step:385/2330 train_time:22123ms step_avg:57.46ms
step:386/2330 train_time:22182ms step_avg:57.47ms
step:387/2330 train_time:22238ms step_avg:57.46ms
step:388/2330 train_time:22297ms step_avg:57.47ms
step:389/2330 train_time:22353ms step_avg:57.46ms
step:390/2330 train_time:22411ms step_avg:57.47ms
step:391/2330 train_time:22467ms step_avg:57.46ms
step:392/2330 train_time:22526ms step_avg:57.47ms
step:393/2330 train_time:22582ms step_avg:57.46ms
step:394/2330 train_time:22642ms step_avg:57.47ms
step:395/2330 train_time:22697ms step_avg:57.46ms
step:396/2330 train_time:22758ms step_avg:57.47ms
step:397/2330 train_time:22813ms step_avg:57.46ms
step:398/2330 train_time:22873ms step_avg:57.47ms
step:399/2330 train_time:22928ms step_avg:57.46ms
step:400/2330 train_time:22987ms step_avg:57.47ms
step:401/2330 train_time:23043ms step_avg:57.46ms
step:402/2330 train_time:23103ms step_avg:57.47ms
step:403/2330 train_time:23159ms step_avg:57.47ms
step:404/2330 train_time:23219ms step_avg:57.47ms
step:405/2330 train_time:23275ms step_avg:57.47ms
step:406/2330 train_time:23334ms step_avg:57.47ms
step:407/2330 train_time:23390ms step_avg:57.47ms
step:408/2330 train_time:23449ms step_avg:57.47ms
step:409/2330 train_time:23506ms step_avg:57.47ms
step:410/2330 train_time:23564ms step_avg:57.47ms
step:411/2330 train_time:23620ms step_avg:57.47ms
step:412/2330 train_time:23679ms step_avg:57.47ms
step:413/2330 train_time:23734ms step_avg:57.47ms
step:414/2330 train_time:23794ms step_avg:57.47ms
step:415/2330 train_time:23850ms step_avg:57.47ms
step:416/2330 train_time:23909ms step_avg:57.47ms
step:417/2330 train_time:23964ms step_avg:57.47ms
step:418/2330 train_time:24023ms step_avg:57.47ms
step:419/2330 train_time:24079ms step_avg:57.47ms
step:420/2330 train_time:24138ms step_avg:57.47ms
step:421/2330 train_time:24194ms step_avg:57.47ms
step:422/2330 train_time:24254ms step_avg:57.47ms
step:423/2330 train_time:24310ms step_avg:57.47ms
step:424/2330 train_time:24369ms step_avg:57.48ms
step:425/2330 train_time:24425ms step_avg:57.47ms
step:426/2330 train_time:24484ms step_avg:57.47ms
step:427/2330 train_time:24540ms step_avg:57.47ms
step:428/2330 train_time:24599ms step_avg:57.47ms
step:429/2330 train_time:24656ms step_avg:57.47ms
step:430/2330 train_time:24715ms step_avg:57.48ms
step:431/2330 train_time:24770ms step_avg:57.47ms
step:432/2330 train_time:24830ms step_avg:57.48ms
step:433/2330 train_time:24886ms step_avg:57.47ms
step:434/2330 train_time:24944ms step_avg:57.48ms
step:435/2330 train_time:25000ms step_avg:57.47ms
step:436/2330 train_time:25059ms step_avg:57.47ms
step:437/2330 train_time:25115ms step_avg:57.47ms
step:438/2330 train_time:25174ms step_avg:57.48ms
step:439/2330 train_time:25230ms step_avg:57.47ms
step:440/2330 train_time:25290ms step_avg:57.48ms
step:441/2330 train_time:25345ms step_avg:57.47ms
step:442/2330 train_time:25404ms step_avg:57.48ms
step:443/2330 train_time:25460ms step_avg:57.47ms
step:444/2330 train_time:25520ms step_avg:57.48ms
step:445/2330 train_time:25576ms step_avg:57.47ms
step:446/2330 train_time:25635ms step_avg:57.48ms
step:447/2330 train_time:25690ms step_avg:57.47ms
step:448/2330 train_time:25750ms step_avg:57.48ms
step:449/2330 train_time:25806ms step_avg:57.47ms
step:450/2330 train_time:25865ms step_avg:57.48ms
step:451/2330 train_time:25920ms step_avg:57.47ms
step:452/2330 train_time:25979ms step_avg:57.48ms
step:453/2330 train_time:26035ms step_avg:57.47ms
step:454/2330 train_time:26096ms step_avg:57.48ms
step:455/2330 train_time:26152ms step_avg:57.48ms
step:456/2330 train_time:26211ms step_avg:57.48ms
step:457/2330 train_time:26267ms step_avg:57.48ms
step:458/2330 train_time:26325ms step_avg:57.48ms
step:459/2330 train_time:26381ms step_avg:57.48ms
step:460/2330 train_time:26440ms step_avg:57.48ms
step:461/2330 train_time:26496ms step_avg:57.48ms
step:462/2330 train_time:26555ms step_avg:57.48ms
step:463/2330 train_time:26610ms step_avg:57.47ms
step:464/2330 train_time:26670ms step_avg:57.48ms
step:465/2330 train_time:26726ms step_avg:57.48ms
step:466/2330 train_time:26785ms step_avg:57.48ms
step:467/2330 train_time:26841ms step_avg:57.48ms
step:468/2330 train_time:26901ms step_avg:57.48ms
step:469/2330 train_time:26956ms step_avg:57.48ms
step:470/2330 train_time:27016ms step_avg:57.48ms
step:471/2330 train_time:27072ms step_avg:57.48ms
step:472/2330 train_time:27133ms step_avg:57.48ms
step:473/2330 train_time:27188ms step_avg:57.48ms
step:474/2330 train_time:27248ms step_avg:57.48ms
step:475/2330 train_time:27304ms step_avg:57.48ms
step:476/2330 train_time:27362ms step_avg:57.48ms
step:477/2330 train_time:27418ms step_avg:57.48ms
step:478/2330 train_time:27477ms step_avg:57.48ms
step:479/2330 train_time:27533ms step_avg:57.48ms
step:480/2330 train_time:27593ms step_avg:57.48ms
step:481/2330 train_time:27648ms step_avg:57.48ms
step:482/2330 train_time:27708ms step_avg:57.49ms
step:483/2330 train_time:27765ms step_avg:57.48ms
step:484/2330 train_time:27824ms step_avg:57.49ms
step:485/2330 train_time:27881ms step_avg:57.49ms
step:486/2330 train_time:27939ms step_avg:57.49ms
step:487/2330 train_time:27995ms step_avg:57.49ms
step:488/2330 train_time:28055ms step_avg:57.49ms
step:489/2330 train_time:28110ms step_avg:57.49ms
step:490/2330 train_time:28170ms step_avg:57.49ms
step:491/2330 train_time:28226ms step_avg:57.49ms
step:492/2330 train_time:28285ms step_avg:57.49ms
step:493/2330 train_time:28341ms step_avg:57.49ms
step:494/2330 train_time:28400ms step_avg:57.49ms
step:495/2330 train_time:28456ms step_avg:57.49ms
step:496/2330 train_time:28515ms step_avg:57.49ms
step:497/2330 train_time:28570ms step_avg:57.49ms
step:498/2330 train_time:28632ms step_avg:57.49ms
step:499/2330 train_time:28687ms step_avg:57.49ms
step:500/2330 train_time:28746ms step_avg:57.49ms
step:500/2330 val_loss:4.4349 train_time:28825ms step_avg:57.65ms
step:501/2330 train_time:28843ms step_avg:57.57ms
step:502/2330 train_time:28863ms step_avg:57.50ms
step:503/2330 train_time:28920ms step_avg:57.50ms
step:504/2330 train_time:28982ms step_avg:57.50ms
step:505/2330 train_time:29038ms step_avg:57.50ms
step:506/2330 train_time:29102ms step_avg:57.51ms
step:507/2330 train_time:29158ms step_avg:57.51ms
step:508/2330 train_time:29217ms step_avg:57.51ms
step:509/2330 train_time:29272ms step_avg:57.51ms
step:510/2330 train_time:29331ms step_avg:57.51ms
step:511/2330 train_time:29387ms step_avg:57.51ms
step:512/2330 train_time:29445ms step_avg:57.51ms
step:513/2330 train_time:29500ms step_avg:57.51ms
step:514/2330 train_time:29559ms step_avg:57.51ms
step:515/2330 train_time:29614ms step_avg:57.50ms
step:516/2330 train_time:29673ms step_avg:57.51ms
step:517/2330 train_time:29729ms step_avg:57.50ms
step:518/2330 train_time:29788ms step_avg:57.51ms
step:519/2330 train_time:29845ms step_avg:57.51ms
step:520/2330 train_time:29905ms step_avg:57.51ms
step:521/2330 train_time:29962ms step_avg:57.51ms
step:522/2330 train_time:30022ms step_avg:57.51ms
step:523/2330 train_time:30079ms step_avg:57.51ms
step:524/2330 train_time:30138ms step_avg:57.51ms
step:525/2330 train_time:30194ms step_avg:57.51ms
step:526/2330 train_time:30252ms step_avg:57.51ms
step:527/2330 train_time:30308ms step_avg:57.51ms
step:528/2330 train_time:30367ms step_avg:57.51ms
step:529/2330 train_time:30423ms step_avg:57.51ms
step:530/2330 train_time:30481ms step_avg:57.51ms
step:531/2330 train_time:30537ms step_avg:57.51ms
step:532/2330 train_time:30596ms step_avg:57.51ms
step:533/2330 train_time:30651ms step_avg:57.51ms
step:534/2330 train_time:30710ms step_avg:57.51ms
step:535/2330 train_time:30766ms step_avg:57.51ms
step:536/2330 train_time:30825ms step_avg:57.51ms
step:537/2330 train_time:30882ms step_avg:57.51ms
step:538/2330 train_time:30941ms step_avg:57.51ms
step:539/2330 train_time:30998ms step_avg:57.51ms
step:540/2330 train_time:31058ms step_avg:57.51ms
step:541/2330 train_time:31114ms step_avg:57.51ms
step:542/2330 train_time:31173ms step_avg:57.52ms
step:543/2330 train_time:31229ms step_avg:57.51ms
step:544/2330 train_time:31288ms step_avg:57.52ms
step:545/2330 train_time:31344ms step_avg:57.51ms
step:546/2330 train_time:31403ms step_avg:57.52ms
step:547/2330 train_time:31459ms step_avg:57.51ms
step:548/2330 train_time:31518ms step_avg:57.51ms
step:549/2330 train_time:31573ms step_avg:57.51ms
step:550/2330 train_time:31633ms step_avg:57.51ms
step:551/2330 train_time:31690ms step_avg:57.51ms
step:552/2330 train_time:31749ms step_avg:57.52ms
step:553/2330 train_time:31805ms step_avg:57.51ms
step:554/2330 train_time:31863ms step_avg:57.52ms
step:555/2330 train_time:31919ms step_avg:57.51ms
step:556/2330 train_time:31978ms step_avg:57.52ms
step:557/2330 train_time:32034ms step_avg:57.51ms
step:558/2330 train_time:32095ms step_avg:57.52ms
step:559/2330 train_time:32152ms step_avg:57.52ms
step:560/2330 train_time:32210ms step_avg:57.52ms
step:561/2330 train_time:32267ms step_avg:57.52ms
step:562/2330 train_time:32326ms step_avg:57.52ms
step:563/2330 train_time:32382ms step_avg:57.52ms
step:564/2330 train_time:32440ms step_avg:57.52ms
step:565/2330 train_time:32497ms step_avg:57.52ms
step:566/2330 train_time:32555ms step_avg:57.52ms
step:567/2330 train_time:32611ms step_avg:57.51ms
step:568/2330 train_time:32670ms step_avg:57.52ms
step:569/2330 train_time:32726ms step_avg:57.51ms
step:570/2330 train_time:32784ms step_avg:57.52ms
step:571/2330 train_time:32841ms step_avg:57.51ms
step:572/2330 train_time:32899ms step_avg:57.52ms
step:573/2330 train_time:32955ms step_avg:57.51ms
step:574/2330 train_time:33015ms step_avg:57.52ms
step:575/2330 train_time:33071ms step_avg:57.51ms
step:576/2330 train_time:33130ms step_avg:57.52ms
step:577/2330 train_time:33186ms step_avg:57.52ms
step:578/2330 train_time:33246ms step_avg:57.52ms
step:579/2330 train_time:33302ms step_avg:57.52ms
step:580/2330 train_time:33361ms step_avg:57.52ms
step:581/2330 train_time:33417ms step_avg:57.52ms
step:582/2330 train_time:33477ms step_avg:57.52ms
step:583/2330 train_time:33532ms step_avg:57.52ms
step:584/2330 train_time:33592ms step_avg:57.52ms
step:585/2330 train_time:33648ms step_avg:57.52ms
step:586/2330 train_time:33707ms step_avg:57.52ms
step:587/2330 train_time:33763ms step_avg:57.52ms
step:588/2330 train_time:33822ms step_avg:57.52ms
step:589/2330 train_time:33878ms step_avg:57.52ms
step:590/2330 train_time:33938ms step_avg:57.52ms
step:591/2330 train_time:33994ms step_avg:57.52ms
step:592/2330 train_time:34053ms step_avg:57.52ms
step:593/2330 train_time:34109ms step_avg:57.52ms
step:594/2330 train_time:34168ms step_avg:57.52ms
step:595/2330 train_time:34224ms step_avg:57.52ms
step:596/2330 train_time:34283ms step_avg:57.52ms
step:597/2330 train_time:34339ms step_avg:57.52ms
step:598/2330 train_time:34398ms step_avg:57.52ms
step:599/2330 train_time:34454ms step_avg:57.52ms
step:600/2330 train_time:34514ms step_avg:57.52ms
step:601/2330 train_time:34569ms step_avg:57.52ms
step:602/2330 train_time:34629ms step_avg:57.52ms
step:603/2330 train_time:34685ms step_avg:57.52ms
step:604/2330 train_time:34744ms step_avg:57.52ms
step:605/2330 train_time:34800ms step_avg:57.52ms
step:606/2330 train_time:34860ms step_avg:57.52ms
step:607/2330 train_time:34915ms step_avg:57.52ms
step:608/2330 train_time:34976ms step_avg:57.53ms
step:609/2330 train_time:35032ms step_avg:57.52ms
step:610/2330 train_time:35091ms step_avg:57.53ms
step:611/2330 train_time:35147ms step_avg:57.52ms
step:612/2330 train_time:35206ms step_avg:57.53ms
step:613/2330 train_time:35262ms step_avg:57.52ms
step:614/2330 train_time:35321ms step_avg:57.53ms
step:615/2330 train_time:35377ms step_avg:57.52ms
step:616/2330 train_time:35436ms step_avg:57.53ms
step:617/2330 train_time:35493ms step_avg:57.52ms
step:618/2330 train_time:35552ms step_avg:57.53ms
step:619/2330 train_time:35607ms step_avg:57.52ms
step:620/2330 train_time:35666ms step_avg:57.53ms
step:621/2330 train_time:35722ms step_avg:57.52ms
step:622/2330 train_time:35781ms step_avg:57.53ms
step:623/2330 train_time:35837ms step_avg:57.52ms
step:624/2330 train_time:35897ms step_avg:57.53ms
step:625/2330 train_time:35953ms step_avg:57.52ms
step:626/2330 train_time:36013ms step_avg:57.53ms
step:627/2330 train_time:36069ms step_avg:57.53ms
step:628/2330 train_time:36128ms step_avg:57.53ms
step:629/2330 train_time:36184ms step_avg:57.53ms
step:630/2330 train_time:36242ms step_avg:57.53ms
step:631/2330 train_time:36299ms step_avg:57.53ms
step:632/2330 train_time:36358ms step_avg:57.53ms
step:633/2330 train_time:36413ms step_avg:57.52ms
step:634/2330 train_time:36473ms step_avg:57.53ms
step:635/2330 train_time:36529ms step_avg:57.53ms
step:636/2330 train_time:36587ms step_avg:57.53ms
step:637/2330 train_time:36643ms step_avg:57.52ms
step:638/2330 train_time:36703ms step_avg:57.53ms
step:639/2330 train_time:36758ms step_avg:57.53ms
step:640/2330 train_time:36817ms step_avg:57.53ms
step:641/2330 train_time:36873ms step_avg:57.52ms
step:642/2330 train_time:36932ms step_avg:57.53ms
step:643/2330 train_time:36987ms step_avg:57.52ms
step:644/2330 train_time:37047ms step_avg:57.53ms
step:645/2330 train_time:37103ms step_avg:57.52ms
step:646/2330 train_time:37163ms step_avg:57.53ms
step:647/2330 train_time:37218ms step_avg:57.52ms
step:648/2330 train_time:37279ms step_avg:57.53ms
step:649/2330 train_time:37334ms step_avg:57.53ms
step:650/2330 train_time:37394ms step_avg:57.53ms
step:651/2330 train_time:37450ms step_avg:57.53ms
step:652/2330 train_time:37508ms step_avg:57.53ms
step:653/2330 train_time:37564ms step_avg:57.52ms
step:654/2330 train_time:37624ms step_avg:57.53ms
step:655/2330 train_time:37680ms step_avg:57.53ms
step:656/2330 train_time:37739ms step_avg:57.53ms
step:657/2330 train_time:37795ms step_avg:57.53ms
step:658/2330 train_time:37854ms step_avg:57.53ms
step:659/2330 train_time:37910ms step_avg:57.53ms
step:660/2330 train_time:37969ms step_avg:57.53ms
step:661/2330 train_time:38025ms step_avg:57.53ms
step:662/2330 train_time:38084ms step_avg:57.53ms
step:663/2330 train_time:38141ms step_avg:57.53ms
step:664/2330 train_time:38200ms step_avg:57.53ms
step:665/2330 train_time:38255ms step_avg:57.53ms
step:666/2330 train_time:38316ms step_avg:57.53ms
step:667/2330 train_time:38372ms step_avg:57.53ms
step:668/2330 train_time:38431ms step_avg:57.53ms
step:669/2330 train_time:38487ms step_avg:57.53ms
step:670/2330 train_time:38545ms step_avg:57.53ms
step:671/2330 train_time:38601ms step_avg:57.53ms
step:672/2330 train_time:38660ms step_avg:57.53ms
step:673/2330 train_time:38716ms step_avg:57.53ms
step:674/2330 train_time:38776ms step_avg:57.53ms
step:675/2330 train_time:38832ms step_avg:57.53ms
step:676/2330 train_time:38891ms step_avg:57.53ms
step:677/2330 train_time:38947ms step_avg:57.53ms
step:678/2330 train_time:39005ms step_avg:57.53ms
step:679/2330 train_time:39061ms step_avg:57.53ms
step:680/2330 train_time:39121ms step_avg:57.53ms
step:681/2330 train_time:39176ms step_avg:57.53ms
step:682/2330 train_time:39236ms step_avg:57.53ms
step:683/2330 train_time:39292ms step_avg:57.53ms
step:684/2330 train_time:39351ms step_avg:57.53ms
step:685/2330 train_time:39408ms step_avg:57.53ms
step:686/2330 train_time:39467ms step_avg:57.53ms
step:687/2330 train_time:39522ms step_avg:57.53ms
step:688/2330 train_time:39582ms step_avg:57.53ms
step:689/2330 train_time:39638ms step_avg:57.53ms
step:690/2330 train_time:39697ms step_avg:57.53ms
step:691/2330 train_time:39753ms step_avg:57.53ms
step:692/2330 train_time:39812ms step_avg:57.53ms
step:693/2330 train_time:39868ms step_avg:57.53ms
step:694/2330 train_time:39927ms step_avg:57.53ms
step:695/2330 train_time:39983ms step_avg:57.53ms
step:696/2330 train_time:40042ms step_avg:57.53ms
step:697/2330 train_time:40099ms step_avg:57.53ms
step:698/2330 train_time:40158ms step_avg:57.53ms
step:699/2330 train_time:40213ms step_avg:57.53ms
step:700/2330 train_time:40273ms step_avg:57.53ms
step:701/2330 train_time:40330ms step_avg:57.53ms
step:702/2330 train_time:40388ms step_avg:57.53ms
step:703/2330 train_time:40444ms step_avg:57.53ms
step:704/2330 train_time:40503ms step_avg:57.53ms
step:705/2330 train_time:40559ms step_avg:57.53ms
step:706/2330 train_time:40619ms step_avg:57.53ms
step:707/2330 train_time:40674ms step_avg:57.53ms
step:708/2330 train_time:40734ms step_avg:57.53ms
step:709/2330 train_time:40790ms step_avg:57.53ms
step:710/2330 train_time:40849ms step_avg:57.53ms
step:711/2330 train_time:40905ms step_avg:57.53ms
step:712/2330 train_time:40964ms step_avg:57.53ms
step:713/2330 train_time:41020ms step_avg:57.53ms
step:714/2330 train_time:41080ms step_avg:57.53ms
step:715/2330 train_time:41135ms step_avg:57.53ms
step:716/2330 train_time:41194ms step_avg:57.53ms
step:717/2330 train_time:41250ms step_avg:57.53ms
step:718/2330 train_time:41309ms step_avg:57.53ms
step:719/2330 train_time:41365ms step_avg:57.53ms
step:720/2330 train_time:41424ms step_avg:57.53ms
step:721/2330 train_time:41481ms step_avg:57.53ms
step:722/2330 train_time:41539ms step_avg:57.53ms
step:723/2330 train_time:41595ms step_avg:57.53ms
step:724/2330 train_time:41655ms step_avg:57.53ms
step:725/2330 train_time:41711ms step_avg:57.53ms
step:726/2330 train_time:41769ms step_avg:57.53ms
step:727/2330 train_time:41825ms step_avg:57.53ms
step:728/2330 train_time:41884ms step_avg:57.53ms
step:729/2330 train_time:41940ms step_avg:57.53ms
step:730/2330 train_time:42000ms step_avg:57.53ms
step:731/2330 train_time:42056ms step_avg:57.53ms
step:732/2330 train_time:42115ms step_avg:57.53ms
step:733/2330 train_time:42172ms step_avg:57.53ms
step:734/2330 train_time:42230ms step_avg:57.53ms
step:735/2330 train_time:42286ms step_avg:57.53ms
step:736/2330 train_time:42345ms step_avg:57.53ms
step:737/2330 train_time:42401ms step_avg:57.53ms
step:738/2330 train_time:42461ms step_avg:57.54ms
step:739/2330 train_time:42517ms step_avg:57.53ms
step:740/2330 train_time:42577ms step_avg:57.54ms
step:741/2330 train_time:42633ms step_avg:57.53ms
step:742/2330 train_time:42692ms step_avg:57.54ms
step:743/2330 train_time:42749ms step_avg:57.54ms
step:744/2330 train_time:42808ms step_avg:57.54ms
step:745/2330 train_time:42864ms step_avg:57.53ms
step:746/2330 train_time:42923ms step_avg:57.54ms
step:747/2330 train_time:42978ms step_avg:57.53ms
step:748/2330 train_time:43038ms step_avg:57.54ms
step:749/2330 train_time:43094ms step_avg:57.54ms
step:750/2330 train_time:43153ms step_avg:57.54ms
step:750/2330 val_loss:4.2321 train_time:43233ms step_avg:57.64ms
step:751/2330 train_time:43251ms step_avg:57.59ms
step:752/2330 train_time:43273ms step_avg:57.54ms
step:753/2330 train_time:43329ms step_avg:57.54ms
step:754/2330 train_time:43390ms step_avg:57.55ms
step:755/2330 train_time:43446ms step_avg:57.54ms
step:756/2330 train_time:43509ms step_avg:57.55ms
step:757/2330 train_time:43564ms step_avg:57.55ms
step:758/2330 train_time:43625ms step_avg:57.55ms
step:759/2330 train_time:43681ms step_avg:57.55ms
step:760/2330 train_time:43740ms step_avg:57.55ms
step:761/2330 train_time:43796ms step_avg:57.55ms
step:762/2330 train_time:43854ms step_avg:57.55ms
step:763/2330 train_time:43909ms step_avg:57.55ms
step:764/2330 train_time:43968ms step_avg:57.55ms
step:765/2330 train_time:44025ms step_avg:57.55ms
step:766/2330 train_time:44083ms step_avg:57.55ms
step:767/2330 train_time:44139ms step_avg:57.55ms
step:768/2330 train_time:44199ms step_avg:57.55ms
step:769/2330 train_time:44256ms step_avg:57.55ms
step:770/2330 train_time:44317ms step_avg:57.55ms
step:771/2330 train_time:44375ms step_avg:57.56ms
step:772/2330 train_time:44437ms step_avg:57.56ms
step:773/2330 train_time:44494ms step_avg:57.56ms
step:774/2330 train_time:44554ms step_avg:57.56ms
step:775/2330 train_time:44611ms step_avg:57.56ms
step:776/2330 train_time:44671ms step_avg:57.57ms
step:777/2330 train_time:44728ms step_avg:57.56ms
step:778/2330 train_time:44788ms step_avg:57.57ms
step:779/2330 train_time:44845ms step_avg:57.57ms
step:780/2330 train_time:44904ms step_avg:57.57ms
step:781/2330 train_time:44960ms step_avg:57.57ms
step:782/2330 train_time:45020ms step_avg:57.57ms
step:783/2330 train_time:45077ms step_avg:57.57ms
step:784/2330 train_time:45136ms step_avg:57.57ms
step:785/2330 train_time:45193ms step_avg:57.57ms
step:786/2330 train_time:45253ms step_avg:57.57ms
step:787/2330 train_time:45310ms step_avg:57.57ms
step:788/2330 train_time:45370ms step_avg:57.58ms
step:789/2330 train_time:45427ms step_avg:57.58ms
step:790/2330 train_time:45488ms step_avg:57.58ms
step:791/2330 train_time:45545ms step_avg:57.58ms
step:792/2330 train_time:45606ms step_avg:57.58ms
step:793/2330 train_time:45662ms step_avg:57.58ms
step:794/2330 train_time:45723ms step_avg:57.59ms
step:795/2330 train_time:45780ms step_avg:57.58ms
step:796/2330 train_time:45840ms step_avg:57.59ms
step:797/2330 train_time:45897ms step_avg:57.59ms
step:798/2330 train_time:45956ms step_avg:57.59ms
step:799/2330 train_time:46013ms step_avg:57.59ms
step:800/2330 train_time:46073ms step_avg:57.59ms
step:801/2330 train_time:46130ms step_avg:57.59ms
step:802/2330 train_time:46190ms step_avg:57.59ms
step:803/2330 train_time:46247ms step_avg:57.59ms
step:804/2330 train_time:46307ms step_avg:57.60ms
step:805/2330 train_time:46364ms step_avg:57.59ms
step:806/2330 train_time:46425ms step_avg:57.60ms
step:807/2330 train_time:46481ms step_avg:57.60ms
step:808/2330 train_time:46543ms step_avg:57.60ms
step:809/2330 train_time:46600ms step_avg:57.60ms
step:810/2330 train_time:46661ms step_avg:57.61ms
step:811/2330 train_time:46717ms step_avg:57.60ms
step:812/2330 train_time:46777ms step_avg:57.61ms
step:813/2330 train_time:46834ms step_avg:57.61ms
step:814/2330 train_time:46894ms step_avg:57.61ms
step:815/2330 train_time:46950ms step_avg:57.61ms
step:816/2330 train_time:47009ms step_avg:57.61ms
step:817/2330 train_time:47065ms step_avg:57.61ms
step:818/2330 train_time:47125ms step_avg:57.61ms
step:819/2330 train_time:47182ms step_avg:57.61ms
step:820/2330 train_time:47243ms step_avg:57.61ms
step:821/2330 train_time:47301ms step_avg:57.61ms
step:822/2330 train_time:47360ms step_avg:57.62ms
step:823/2330 train_time:47418ms step_avg:57.62ms
step:824/2330 train_time:47477ms step_avg:57.62ms
step:825/2330 train_time:47535ms step_avg:57.62ms
step:826/2330 train_time:47595ms step_avg:57.62ms
step:827/2330 train_time:47653ms step_avg:57.62ms
step:828/2330 train_time:47712ms step_avg:57.62ms
step:829/2330 train_time:47769ms step_avg:57.62ms
step:830/2330 train_time:47829ms step_avg:57.63ms
step:831/2330 train_time:47885ms step_avg:57.62ms
step:832/2330 train_time:47945ms step_avg:57.63ms
step:833/2330 train_time:48001ms step_avg:57.62ms
step:834/2330 train_time:48061ms step_avg:57.63ms
step:835/2330 train_time:48118ms step_avg:57.63ms
step:836/2330 train_time:48178ms step_avg:57.63ms
step:837/2330 train_time:48235ms step_avg:57.63ms
step:838/2330 train_time:48295ms step_avg:57.63ms
step:839/2330 train_time:48352ms step_avg:57.63ms
step:840/2330 train_time:48412ms step_avg:57.63ms
step:841/2330 train_time:48468ms step_avg:57.63ms
step:842/2330 train_time:48529ms step_avg:57.64ms
step:843/2330 train_time:48586ms step_avg:57.63ms
step:844/2330 train_time:48647ms step_avg:57.64ms
step:845/2330 train_time:48703ms step_avg:57.64ms
step:846/2330 train_time:48765ms step_avg:57.64ms
step:847/2330 train_time:48822ms step_avg:57.64ms
step:848/2330 train_time:48882ms step_avg:57.64ms
step:849/2330 train_time:48939ms step_avg:57.64ms
step:850/2330 train_time:48999ms step_avg:57.65ms
step:851/2330 train_time:49055ms step_avg:57.64ms
step:852/2330 train_time:49115ms step_avg:57.65ms
step:853/2330 train_time:49172ms step_avg:57.65ms
step:854/2330 train_time:49231ms step_avg:57.65ms
step:855/2330 train_time:49287ms step_avg:57.65ms
step:856/2330 train_time:49348ms step_avg:57.65ms
step:857/2330 train_time:49405ms step_avg:57.65ms
step:858/2330 train_time:49466ms step_avg:57.65ms
step:859/2330 train_time:49523ms step_avg:57.65ms
step:860/2330 train_time:49584ms step_avg:57.66ms
step:861/2330 train_time:49640ms step_avg:57.65ms
step:862/2330 train_time:49701ms step_avg:57.66ms
step:863/2330 train_time:49758ms step_avg:57.66ms
step:864/2330 train_time:49817ms step_avg:57.66ms
step:865/2330 train_time:49874ms step_avg:57.66ms
step:866/2330 train_time:49934ms step_avg:57.66ms
step:867/2330 train_time:49991ms step_avg:57.66ms
step:868/2330 train_time:50051ms step_avg:57.66ms
step:869/2330 train_time:50107ms step_avg:57.66ms
step:870/2330 train_time:50168ms step_avg:57.66ms
step:871/2330 train_time:50225ms step_avg:57.66ms
step:872/2330 train_time:50286ms step_avg:57.67ms
step:873/2330 train_time:50343ms step_avg:57.67ms
step:874/2330 train_time:50404ms step_avg:57.67ms
step:875/2330 train_time:50460ms step_avg:57.67ms
step:876/2330 train_time:50522ms step_avg:57.67ms
step:877/2330 train_time:50579ms step_avg:57.67ms
step:878/2330 train_time:50638ms step_avg:57.67ms
step:879/2330 train_time:50696ms step_avg:57.67ms
step:880/2330 train_time:50756ms step_avg:57.68ms
step:881/2330 train_time:50813ms step_avg:57.68ms
step:882/2330 train_time:50872ms step_avg:57.68ms
step:883/2330 train_time:50929ms step_avg:57.68ms
step:884/2330 train_time:50988ms step_avg:57.68ms
step:885/2330 train_time:51046ms step_avg:57.68ms
step:886/2330 train_time:51106ms step_avg:57.68ms
step:887/2330 train_time:51162ms step_avg:57.68ms
step:888/2330 train_time:51223ms step_avg:57.68ms
step:889/2330 train_time:51280ms step_avg:57.68ms
step:890/2330 train_time:51339ms step_avg:57.68ms
step:891/2330 train_time:51397ms step_avg:57.68ms
step:892/2330 train_time:51457ms step_avg:57.69ms
step:893/2330 train_time:51514ms step_avg:57.69ms
step:894/2330 train_time:51574ms step_avg:57.69ms
step:895/2330 train_time:51631ms step_avg:57.69ms
step:896/2330 train_time:51691ms step_avg:57.69ms
step:897/2330 train_time:51748ms step_avg:57.69ms
step:898/2330 train_time:51808ms step_avg:57.69ms
step:899/2330 train_time:51865ms step_avg:57.69ms
step:900/2330 train_time:51925ms step_avg:57.69ms
step:901/2330 train_time:51982ms step_avg:57.69ms
step:902/2330 train_time:52043ms step_avg:57.70ms
step:903/2330 train_time:52100ms step_avg:57.70ms
step:904/2330 train_time:52159ms step_avg:57.70ms
step:905/2330 train_time:52216ms step_avg:57.70ms
step:906/2330 train_time:52276ms step_avg:57.70ms
step:907/2330 train_time:52332ms step_avg:57.70ms
step:908/2330 train_time:52392ms step_avg:57.70ms
step:909/2330 train_time:52449ms step_avg:57.70ms
step:910/2330 train_time:52509ms step_avg:57.70ms
step:911/2330 train_time:52565ms step_avg:57.70ms
step:912/2330 train_time:52627ms step_avg:57.71ms
step:913/2330 train_time:52683ms step_avg:57.70ms
step:914/2330 train_time:52745ms step_avg:57.71ms
step:915/2330 train_time:52801ms step_avg:57.71ms
step:916/2330 train_time:52862ms step_avg:57.71ms
step:917/2330 train_time:52919ms step_avg:57.71ms
step:918/2330 train_time:52980ms step_avg:57.71ms
step:919/2330 train_time:53037ms step_avg:57.71ms
step:920/2330 train_time:53096ms step_avg:57.71ms
step:921/2330 train_time:53154ms step_avg:57.71ms
step:922/2330 train_time:53213ms step_avg:57.72ms
step:923/2330 train_time:53270ms step_avg:57.71ms
step:924/2330 train_time:53331ms step_avg:57.72ms
step:925/2330 train_time:53388ms step_avg:57.72ms
step:926/2330 train_time:53449ms step_avg:57.72ms
step:927/2330 train_time:53505ms step_avg:57.72ms
step:928/2330 train_time:53565ms step_avg:57.72ms
step:929/2330 train_time:53622ms step_avg:57.72ms
step:930/2330 train_time:53683ms step_avg:57.72ms
step:931/2330 train_time:53739ms step_avg:57.72ms
step:932/2330 train_time:53800ms step_avg:57.73ms
step:933/2330 train_time:53857ms step_avg:57.72ms
step:934/2330 train_time:53917ms step_avg:57.73ms
step:935/2330 train_time:53974ms step_avg:57.73ms
step:936/2330 train_time:54033ms step_avg:57.73ms
step:937/2330 train_time:54090ms step_avg:57.73ms
step:938/2330 train_time:54149ms step_avg:57.73ms
step:939/2330 train_time:54206ms step_avg:57.73ms
step:940/2330 train_time:54266ms step_avg:57.73ms
step:941/2330 train_time:54324ms step_avg:57.73ms
step:942/2330 train_time:54383ms step_avg:57.73ms
step:943/2330 train_time:54441ms step_avg:57.73ms
step:944/2330 train_time:54501ms step_avg:57.73ms
step:945/2330 train_time:54558ms step_avg:57.73ms
step:946/2330 train_time:54618ms step_avg:57.74ms
step:947/2330 train_time:54675ms step_avg:57.73ms
step:948/2330 train_time:54735ms step_avg:57.74ms
step:949/2330 train_time:54792ms step_avg:57.74ms
step:950/2330 train_time:54852ms step_avg:57.74ms
step:951/2330 train_time:54908ms step_avg:57.74ms
step:952/2330 train_time:54968ms step_avg:57.74ms
step:953/2330 train_time:55025ms step_avg:57.74ms
step:954/2330 train_time:55086ms step_avg:57.74ms
step:955/2330 train_time:55142ms step_avg:57.74ms
step:956/2330 train_time:55203ms step_avg:57.74ms
step:957/2330 train_time:55259ms step_avg:57.74ms
step:958/2330 train_time:55320ms step_avg:57.75ms
step:959/2330 train_time:55377ms step_avg:57.74ms
step:960/2330 train_time:55438ms step_avg:57.75ms
step:961/2330 train_time:55495ms step_avg:57.75ms
step:962/2330 train_time:55555ms step_avg:57.75ms
step:963/2330 train_time:55612ms step_avg:57.75ms
step:964/2330 train_time:55672ms step_avg:57.75ms
step:965/2330 train_time:55728ms step_avg:57.75ms
step:966/2330 train_time:55788ms step_avg:57.75ms
step:967/2330 train_time:55845ms step_avg:57.75ms
step:968/2330 train_time:55906ms step_avg:57.75ms
step:969/2330 train_time:55962ms step_avg:57.75ms
step:970/2330 train_time:56023ms step_avg:57.76ms
step:971/2330 train_time:56080ms step_avg:57.76ms
step:972/2330 train_time:56140ms step_avg:57.76ms
step:973/2330 train_time:56197ms step_avg:57.76ms
step:974/2330 train_time:56257ms step_avg:57.76ms
step:975/2330 train_time:56314ms step_avg:57.76ms
step:976/2330 train_time:56375ms step_avg:57.76ms
step:977/2330 train_time:56432ms step_avg:57.76ms
step:978/2330 train_time:56492ms step_avg:57.76ms
step:979/2330 train_time:56549ms step_avg:57.76ms
step:980/2330 train_time:56610ms step_avg:57.77ms
step:981/2330 train_time:56666ms step_avg:57.76ms
step:982/2330 train_time:56727ms step_avg:57.77ms
step:983/2330 train_time:56784ms step_avg:57.77ms
step:984/2330 train_time:56845ms step_avg:57.77ms
step:985/2330 train_time:56901ms step_avg:57.77ms
step:986/2330 train_time:56962ms step_avg:57.77ms
step:987/2330 train_time:57019ms step_avg:57.77ms
step:988/2330 train_time:57079ms step_avg:57.77ms
step:989/2330 train_time:57137ms step_avg:57.77ms
step:990/2330 train_time:57196ms step_avg:57.77ms
step:991/2330 train_time:57253ms step_avg:57.77ms
step:992/2330 train_time:57313ms step_avg:57.78ms
step:993/2330 train_time:57370ms step_avg:57.77ms
step:994/2330 train_time:57430ms step_avg:57.78ms
step:995/2330 train_time:57487ms step_avg:57.78ms
step:996/2330 train_time:57547ms step_avg:57.78ms
step:997/2330 train_time:57604ms step_avg:57.78ms
step:998/2330 train_time:57665ms step_avg:57.78ms
step:999/2330 train_time:57721ms step_avg:57.78ms
step:1000/2330 train_time:57782ms step_avg:57.78ms
step:1000/2330 val_loss:4.0798 train_time:57862ms step_avg:57.86ms
step:1001/2330 train_time:57881ms step_avg:57.82ms
step:1002/2330 train_time:57901ms step_avg:57.79ms
step:1003/2330 train_time:57955ms step_avg:57.78ms
step:1004/2330 train_time:58020ms step_avg:57.79ms
step:1005/2330 train_time:58076ms step_avg:57.79ms
step:1006/2330 train_time:58138ms step_avg:57.79ms
step:1007/2330 train_time:58194ms step_avg:57.79ms
step:1008/2330 train_time:58255ms step_avg:57.79ms
step:1009/2330 train_time:58311ms step_avg:57.79ms
step:1010/2330 train_time:58371ms step_avg:57.79ms
step:1011/2330 train_time:58427ms step_avg:57.79ms
step:1012/2330 train_time:58486ms step_avg:57.79ms
step:1013/2330 train_time:58542ms step_avg:57.79ms
step:1014/2330 train_time:58601ms step_avg:57.79ms
step:1015/2330 train_time:58657ms step_avg:57.79ms
step:1016/2330 train_time:58717ms step_avg:57.79ms
step:1017/2330 train_time:58775ms step_avg:57.79ms
step:1018/2330 train_time:58837ms step_avg:57.80ms
step:1019/2330 train_time:58895ms step_avg:57.80ms
step:1020/2330 train_time:58957ms step_avg:57.80ms
step:1021/2330 train_time:59014ms step_avg:57.80ms
step:1022/2330 train_time:59075ms step_avg:57.80ms
step:1023/2330 train_time:59131ms step_avg:57.80ms
step:1024/2330 train_time:59193ms step_avg:57.81ms
step:1025/2330 train_time:59249ms step_avg:57.80ms
step:1026/2330 train_time:59310ms step_avg:57.81ms
step:1027/2330 train_time:59367ms step_avg:57.81ms
step:1028/2330 train_time:59426ms step_avg:57.81ms
step:1029/2330 train_time:59483ms step_avg:57.81ms
step:1030/2330 train_time:59542ms step_avg:57.81ms
step:1031/2330 train_time:59598ms step_avg:57.81ms
step:1032/2330 train_time:59659ms step_avg:57.81ms
step:1033/2330 train_time:59716ms step_avg:57.81ms
step:1034/2330 train_time:59777ms step_avg:57.81ms
step:1035/2330 train_time:59833ms step_avg:57.81ms
step:1036/2330 train_time:59897ms step_avg:57.82ms
step:1037/2330 train_time:59954ms step_avg:57.81ms
step:1038/2330 train_time:60016ms step_avg:57.82ms
step:1039/2330 train_time:60072ms step_avg:57.82ms
step:1040/2330 train_time:60134ms step_avg:57.82ms
step:1041/2330 train_time:60190ms step_avg:57.82ms
step:1042/2330 train_time:60250ms step_avg:57.82ms
step:1043/2330 train_time:60307ms step_avg:57.82ms
step:1044/2330 train_time:60367ms step_avg:57.82ms
step:1045/2330 train_time:60425ms step_avg:57.82ms
step:1046/2330 train_time:60484ms step_avg:57.82ms
step:1047/2330 train_time:60540ms step_avg:57.82ms
step:1048/2330 train_time:60600ms step_avg:57.82ms
step:1049/2330 train_time:60657ms step_avg:57.82ms
step:1050/2330 train_time:60716ms step_avg:57.82ms
step:1051/2330 train_time:60773ms step_avg:57.82ms
step:1052/2330 train_time:60834ms step_avg:57.83ms
step:1053/2330 train_time:60892ms step_avg:57.83ms
step:1054/2330 train_time:60952ms step_avg:57.83ms
step:1055/2330 train_time:61009ms step_avg:57.83ms
step:1056/2330 train_time:61071ms step_avg:57.83ms
step:1057/2330 train_time:61127ms step_avg:57.83ms
step:1058/2330 train_time:61187ms step_avg:57.83ms
step:1059/2330 train_time:61244ms step_avg:57.83ms
step:1060/2330 train_time:61304ms step_avg:57.83ms
step:1061/2330 train_time:61360ms step_avg:57.83ms
step:1062/2330 train_time:61421ms step_avg:57.84ms
step:1063/2330 train_time:61477ms step_avg:57.83ms
step:1064/2330 train_time:61537ms step_avg:57.84ms
step:1065/2330 train_time:61594ms step_avg:57.83ms
step:1066/2330 train_time:61654ms step_avg:57.84ms
step:1067/2330 train_time:61711ms step_avg:57.84ms
step:1068/2330 train_time:61771ms step_avg:57.84ms
step:1069/2330 train_time:61828ms step_avg:57.84ms
step:1070/2330 train_time:61889ms step_avg:57.84ms
step:1071/2330 train_time:61946ms step_avg:57.84ms
step:1072/2330 train_time:62007ms step_avg:57.84ms
step:1073/2330 train_time:62064ms step_avg:57.84ms
step:1074/2330 train_time:62124ms step_avg:57.84ms
step:1075/2330 train_time:62181ms step_avg:57.84ms
step:1076/2330 train_time:62241ms step_avg:57.84ms
step:1077/2330 train_time:62297ms step_avg:57.84ms
step:1078/2330 train_time:62358ms step_avg:57.85ms
step:1079/2330 train_time:62414ms step_avg:57.84ms
step:1080/2330 train_time:62476ms step_avg:57.85ms
step:1081/2330 train_time:62532ms step_avg:57.85ms
step:1082/2330 train_time:62593ms step_avg:57.85ms
step:1083/2330 train_time:62649ms step_avg:57.85ms
step:1084/2330 train_time:62709ms step_avg:57.85ms
step:1085/2330 train_time:62766ms step_avg:57.85ms
step:1086/2330 train_time:62826ms step_avg:57.85ms
step:1087/2330 train_time:62883ms step_avg:57.85ms
step:1088/2330 train_time:62942ms step_avg:57.85ms
step:1089/2330 train_time:62999ms step_avg:57.85ms
step:1090/2330 train_time:63061ms step_avg:57.85ms
step:1091/2330 train_time:63118ms step_avg:57.85ms
step:1092/2330 train_time:63179ms step_avg:57.86ms
step:1093/2330 train_time:63235ms step_avg:57.85ms
step:1094/2330 train_time:63296ms step_avg:57.86ms
step:1095/2330 train_time:63352ms step_avg:57.86ms
step:1096/2330 train_time:63414ms step_avg:57.86ms
step:1097/2330 train_time:63470ms step_avg:57.86ms
step:1098/2330 train_time:63530ms step_avg:57.86ms
step:1099/2330 train_time:63587ms step_avg:57.86ms
step:1100/2330 train_time:63646ms step_avg:57.86ms
step:1101/2330 train_time:63703ms step_avg:57.86ms
step:1102/2330 train_time:63763ms step_avg:57.86ms
step:1103/2330 train_time:63820ms step_avg:57.86ms
step:1104/2330 train_time:63880ms step_avg:57.86ms
step:1105/2330 train_time:63937ms step_avg:57.86ms
step:1106/2330 train_time:63998ms step_avg:57.86ms
step:1107/2330 train_time:64055ms step_avg:57.86ms
step:1108/2330 train_time:64116ms step_avg:57.87ms
step:1109/2330 train_time:64172ms step_avg:57.86ms
step:1110/2330 train_time:64233ms step_avg:57.87ms
step:1111/2330 train_time:64290ms step_avg:57.87ms
step:1112/2330 train_time:64350ms step_avg:57.87ms
step:1113/2330 train_time:64406ms step_avg:57.87ms
step:1114/2330 train_time:64466ms step_avg:57.87ms
step:1115/2330 train_time:64523ms step_avg:57.87ms
step:1116/2330 train_time:64583ms step_avg:57.87ms
step:1117/2330 train_time:64639ms step_avg:57.87ms
step:1118/2330 train_time:64701ms step_avg:57.87ms
step:1119/2330 train_time:64757ms step_avg:57.87ms
step:1120/2330 train_time:64817ms step_avg:57.87ms
step:1121/2330 train_time:64874ms step_avg:57.87ms
step:1122/2330 train_time:64935ms step_avg:57.87ms
step:1123/2330 train_time:64992ms step_avg:57.87ms
step:1124/2330 train_time:65052ms step_avg:57.88ms
step:1125/2330 train_time:65110ms step_avg:57.88ms
step:1126/2330 train_time:65169ms step_avg:57.88ms
step:1127/2330 train_time:65226ms step_avg:57.88ms
step:1128/2330 train_time:65286ms step_avg:57.88ms
step:1129/2330 train_time:65343ms step_avg:57.88ms
step:1130/2330 train_time:65403ms step_avg:57.88ms
step:1131/2330 train_time:65459ms step_avg:57.88ms
step:1132/2330 train_time:65520ms step_avg:57.88ms
step:1133/2330 train_time:65577ms step_avg:57.88ms
step:1134/2330 train_time:65637ms step_avg:57.88ms
step:1135/2330 train_time:65693ms step_avg:57.88ms
step:1136/2330 train_time:65755ms step_avg:57.88ms
step:1137/2330 train_time:65812ms step_avg:57.88ms
step:1138/2330 train_time:65871ms step_avg:57.88ms
step:1139/2330 train_time:65927ms step_avg:57.88ms
step:1140/2330 train_time:65988ms step_avg:57.88ms
step:1141/2330 train_time:66046ms step_avg:57.88ms
step:1142/2330 train_time:66106ms step_avg:57.89ms
step:1143/2330 train_time:66162ms step_avg:57.88ms
step:1144/2330 train_time:66223ms step_avg:57.89ms
step:1145/2330 train_time:66279ms step_avg:57.89ms
step:1146/2330 train_time:66832ms step_avg:58.32ms
step:1147/2330 train_time:66887ms step_avg:58.32ms
step:1148/2330 train_time:66947ms step_avg:58.32ms
step:1149/2330 train_time:67002ms step_avg:58.31ms
step:1150/2330 train_time:67062ms step_avg:58.31ms
step:1151/2330 train_time:67117ms step_avg:58.31ms
step:1152/2330 train_time:67177ms step_avg:58.31ms
step:1153/2330 train_time:67233ms step_avg:58.31ms
step:1154/2330 train_time:67292ms step_avg:58.31ms
step:1155/2330 train_time:67348ms step_avg:58.31ms
step:1156/2330 train_time:67408ms step_avg:58.31ms
step:1157/2330 train_time:67464ms step_avg:58.31ms
step:1158/2330 train_time:67523ms step_avg:58.31ms
step:1159/2330 train_time:67579ms step_avg:58.31ms
step:1160/2330 train_time:67638ms step_avg:58.31ms
step:1161/2330 train_time:67699ms step_avg:58.31ms
step:1162/2330 train_time:67765ms step_avg:58.32ms
step:1163/2330 train_time:67822ms step_avg:58.32ms
step:1164/2330 train_time:67885ms step_avg:58.32ms
step:1165/2330 train_time:67942ms step_avg:58.32ms
step:1166/2330 train_time:68003ms step_avg:58.32ms
step:1167/2330 train_time:68059ms step_avg:58.32ms
step:1168/2330 train_time:68120ms step_avg:58.32ms
step:1169/2330 train_time:68176ms step_avg:58.32ms
step:1170/2330 train_time:68236ms step_avg:58.32ms
step:1171/2330 train_time:68292ms step_avg:58.32ms
step:1172/2330 train_time:68352ms step_avg:58.32ms
step:1173/2330 train_time:68408ms step_avg:58.32ms
step:1174/2330 train_time:68468ms step_avg:58.32ms
step:1175/2330 train_time:68524ms step_avg:58.32ms
step:1176/2330 train_time:68583ms step_avg:58.32ms
step:1177/2330 train_time:68641ms step_avg:58.32ms
step:1178/2330 train_time:68703ms step_avg:58.32ms
step:1179/2330 train_time:68761ms step_avg:58.32ms
step:1180/2330 train_time:68822ms step_avg:58.32ms
step:1181/2330 train_time:68880ms step_avg:58.32ms
step:1182/2330 train_time:68942ms step_avg:58.33ms
step:1183/2330 train_time:68998ms step_avg:58.32ms
step:1184/2330 train_time:69059ms step_avg:58.33ms
step:1185/2330 train_time:69115ms step_avg:58.32ms
step:1186/2330 train_time:69176ms step_avg:58.33ms
step:1187/2330 train_time:69231ms step_avg:58.32ms
step:1188/2330 train_time:69293ms step_avg:58.33ms
step:1189/2330 train_time:69348ms step_avg:58.32ms
step:1190/2330 train_time:69408ms step_avg:58.33ms
step:1191/2330 train_time:69464ms step_avg:58.32ms
step:1192/2330 train_time:69524ms step_avg:58.33ms
step:1193/2330 train_time:69582ms step_avg:58.32ms
step:1194/2330 train_time:69641ms step_avg:58.33ms
step:1195/2330 train_time:69700ms step_avg:58.33ms
step:1196/2330 train_time:69760ms step_avg:58.33ms
step:1197/2330 train_time:69818ms step_avg:58.33ms
step:1198/2330 train_time:69878ms step_avg:58.33ms
step:1199/2330 train_time:69934ms step_avg:58.33ms
step:1200/2330 train_time:69997ms step_avg:58.33ms
step:1201/2330 train_time:70054ms step_avg:58.33ms
step:1202/2330 train_time:70114ms step_avg:58.33ms
step:1203/2330 train_time:70171ms step_avg:58.33ms
step:1204/2330 train_time:70231ms step_avg:58.33ms
step:1205/2330 train_time:70287ms step_avg:58.33ms
step:1206/2330 train_time:70347ms step_avg:58.33ms
step:1207/2330 train_time:70403ms step_avg:58.33ms
step:1208/2330 train_time:70463ms step_avg:58.33ms
step:1209/2330 train_time:70519ms step_avg:58.33ms
step:1210/2330 train_time:70580ms step_avg:58.33ms
step:1211/2330 train_time:70636ms step_avg:58.33ms
step:1212/2330 train_time:70697ms step_avg:58.33ms
step:1213/2330 train_time:70755ms step_avg:58.33ms
step:1214/2330 train_time:70816ms step_avg:58.33ms
step:1215/2330 train_time:70873ms step_avg:58.33ms
step:1216/2330 train_time:70933ms step_avg:58.33ms
step:1217/2330 train_time:70989ms step_avg:58.33ms
step:1218/2330 train_time:71049ms step_avg:58.33ms
step:1219/2330 train_time:71107ms step_avg:58.33ms
step:1220/2330 train_time:71167ms step_avg:58.33ms
step:1221/2330 train_time:71224ms step_avg:58.33ms
step:1222/2330 train_time:71284ms step_avg:58.33ms
step:1223/2330 train_time:71340ms step_avg:58.33ms
step:1224/2330 train_time:71400ms step_avg:58.33ms
step:1225/2330 train_time:71456ms step_avg:58.33ms
step:1226/2330 train_time:71517ms step_avg:58.33ms
step:1227/2330 train_time:71573ms step_avg:58.33ms
step:1228/2330 train_time:71634ms step_avg:58.33ms
step:1229/2330 train_time:71690ms step_avg:58.33ms
step:1230/2330 train_time:71751ms step_avg:58.33ms
step:1231/2330 train_time:71808ms step_avg:58.33ms
step:1232/2330 train_time:71869ms step_avg:58.34ms
step:1233/2330 train_time:71927ms step_avg:58.33ms
step:1234/2330 train_time:71986ms step_avg:58.34ms
step:1235/2330 train_time:72043ms step_avg:58.33ms
step:1236/2330 train_time:72104ms step_avg:58.34ms
step:1237/2330 train_time:72160ms step_avg:58.33ms
step:1238/2330 train_time:72221ms step_avg:58.34ms
step:1239/2330 train_time:72277ms step_avg:58.34ms
step:1240/2330 train_time:72339ms step_avg:58.34ms
step:1241/2330 train_time:72395ms step_avg:58.34ms
step:1242/2330 train_time:72455ms step_avg:58.34ms
step:1243/2330 train_time:72512ms step_avg:58.34ms
step:1244/2330 train_time:72571ms step_avg:58.34ms
step:1245/2330 train_time:72628ms step_avg:58.34ms
step:1246/2330 train_time:72688ms step_avg:58.34ms
step:1247/2330 train_time:72745ms step_avg:58.34ms
step:1248/2330 train_time:72806ms step_avg:58.34ms
step:1249/2330 train_time:72863ms step_avg:58.34ms
step:1250/2330 train_time:72924ms step_avg:58.34ms
step:1250/2330 val_loss:3.9970 train_time:73005ms step_avg:58.40ms
step:1251/2330 train_time:73023ms step_avg:58.37ms
step:1252/2330 train_time:73044ms step_avg:58.34ms
step:1253/2330 train_time:73103ms step_avg:58.34ms
step:1254/2330 train_time:73167ms step_avg:58.35ms
step:1255/2330 train_time:73224ms step_avg:58.35ms
step:1256/2330 train_time:73284ms step_avg:58.35ms
step:1257/2330 train_time:73341ms step_avg:58.35ms
step:1258/2330 train_time:73401ms step_avg:58.35ms
step:1259/2330 train_time:73457ms step_avg:58.35ms
step:1260/2330 train_time:73517ms step_avg:58.35ms
step:1261/2330 train_time:73573ms step_avg:58.34ms
step:1262/2330 train_time:73632ms step_avg:58.35ms
step:1263/2330 train_time:73689ms step_avg:58.34ms
step:1264/2330 train_time:73749ms step_avg:58.35ms
step:1265/2330 train_time:73804ms step_avg:58.34ms
step:1266/2330 train_time:73864ms step_avg:58.34ms
step:1267/2330 train_time:73921ms step_avg:58.34ms
step:1268/2330 train_time:73983ms step_avg:58.35ms
step:1269/2330 train_time:74043ms step_avg:58.35ms
step:1270/2330 train_time:74104ms step_avg:58.35ms
step:1271/2330 train_time:74162ms step_avg:58.35ms
step:1272/2330 train_time:74222ms step_avg:58.35ms
step:1273/2330 train_time:74280ms step_avg:58.35ms
step:1274/2330 train_time:74339ms step_avg:58.35ms
step:1275/2330 train_time:74396ms step_avg:58.35ms
step:1276/2330 train_time:74455ms step_avg:58.35ms
step:1277/2330 train_time:74511ms step_avg:58.35ms
step:1278/2330 train_time:74572ms step_avg:58.35ms
step:1279/2330 train_time:74629ms step_avg:58.35ms
step:1280/2330 train_time:74689ms step_avg:58.35ms
step:1281/2330 train_time:74745ms step_avg:58.35ms
step:1282/2330 train_time:74805ms step_avg:58.35ms
step:1283/2330 train_time:74862ms step_avg:58.35ms
step:1284/2330 train_time:74921ms step_avg:58.35ms
step:1285/2330 train_time:74979ms step_avg:58.35ms
step:1286/2330 train_time:75040ms step_avg:58.35ms
step:1287/2330 train_time:75098ms step_avg:58.35ms
step:1288/2330 train_time:75159ms step_avg:58.35ms
step:1289/2330 train_time:75217ms step_avg:58.35ms
step:1290/2330 train_time:75276ms step_avg:58.35ms
step:1291/2330 train_time:75333ms step_avg:58.35ms
step:1292/2330 train_time:75394ms step_avg:58.35ms
step:1293/2330 train_time:75451ms step_avg:58.35ms
step:1294/2330 train_time:75511ms step_avg:58.35ms
step:1295/2330 train_time:75567ms step_avg:58.35ms
step:1296/2330 train_time:75627ms step_avg:58.35ms
step:1297/2330 train_time:75683ms step_avg:58.35ms
step:1298/2330 train_time:75743ms step_avg:58.35ms
step:1299/2330 train_time:75799ms step_avg:58.35ms
step:1300/2330 train_time:75859ms step_avg:58.35ms
step:1301/2330 train_time:75916ms step_avg:58.35ms
step:1302/2330 train_time:75975ms step_avg:58.35ms
step:1303/2330 train_time:76032ms step_avg:58.35ms
step:1304/2330 train_time:76094ms step_avg:58.35ms
step:1305/2330 train_time:76151ms step_avg:58.35ms
step:1306/2330 train_time:76213ms step_avg:58.36ms
step:1307/2330 train_time:76270ms step_avg:58.35ms
step:1308/2330 train_time:76331ms step_avg:58.36ms
step:1309/2330 train_time:76388ms step_avg:58.36ms
step:1310/2330 train_time:76448ms step_avg:58.36ms
step:1311/2330 train_time:76504ms step_avg:58.36ms
step:1312/2330 train_time:76564ms step_avg:58.36ms
step:1313/2330 train_time:76621ms step_avg:58.36ms
step:1314/2330 train_time:76680ms step_avg:58.36ms
step:1315/2330 train_time:76737ms step_avg:58.36ms
step:1316/2330 train_time:76796ms step_avg:58.36ms
step:1317/2330 train_time:76853ms step_avg:58.35ms
step:1318/2330 train_time:76914ms step_avg:58.36ms
step:1319/2330 train_time:76970ms step_avg:58.35ms
step:1320/2330 train_time:77031ms step_avg:58.36ms
step:1321/2330 train_time:77088ms step_avg:58.36ms
step:1322/2330 train_time:77150ms step_avg:58.36ms
step:1323/2330 train_time:77207ms step_avg:58.36ms
step:1324/2330 train_time:77268ms step_avg:58.36ms
step:1325/2330 train_time:77325ms step_avg:58.36ms
step:1326/2330 train_time:77385ms step_avg:58.36ms
step:1327/2330 train_time:77443ms step_avg:58.36ms
step:1328/2330 train_time:77503ms step_avg:58.36ms
step:1329/2330 train_time:77560ms step_avg:58.36ms
step:1330/2330 train_time:77619ms step_avg:58.36ms
step:1331/2330 train_time:77676ms step_avg:58.36ms
step:1332/2330 train_time:77736ms step_avg:58.36ms
step:1333/2330 train_time:77792ms step_avg:58.36ms
step:1334/2330 train_time:77853ms step_avg:58.36ms
step:1335/2330 train_time:77910ms step_avg:58.36ms
step:1336/2330 train_time:77970ms step_avg:58.36ms
step:1337/2330 train_time:78028ms step_avg:58.36ms
step:1338/2330 train_time:78088ms step_avg:58.36ms
step:1339/2330 train_time:78145ms step_avg:58.36ms
step:1340/2330 train_time:78206ms step_avg:58.36ms
step:1341/2330 train_time:78263ms step_avg:58.36ms
step:1342/2330 train_time:78323ms step_avg:58.36ms
step:1343/2330 train_time:78381ms step_avg:58.36ms
step:1344/2330 train_time:78440ms step_avg:58.36ms
step:1345/2330 train_time:78497ms step_avg:58.36ms
step:1346/2330 train_time:78557ms step_avg:58.36ms
step:1347/2330 train_time:78614ms step_avg:58.36ms
step:1348/2330 train_time:78673ms step_avg:58.36ms
step:1349/2330 train_time:78729ms step_avg:58.36ms
step:1350/2330 train_time:78791ms step_avg:58.36ms
step:1351/2330 train_time:78847ms step_avg:58.36ms
step:1352/2330 train_time:78908ms step_avg:58.36ms
step:1353/2330 train_time:78965ms step_avg:58.36ms
step:1354/2330 train_time:79026ms step_avg:58.36ms
step:1355/2330 train_time:79083ms step_avg:58.36ms
step:1356/2330 train_time:79143ms step_avg:58.36ms
step:1357/2330 train_time:79200ms step_avg:58.36ms
step:1358/2330 train_time:79260ms step_avg:58.37ms
step:1359/2330 train_time:79318ms step_avg:58.36ms
step:1360/2330 train_time:79378ms step_avg:58.37ms
step:1361/2330 train_time:79435ms step_avg:58.36ms
step:1362/2330 train_time:79495ms step_avg:58.37ms
step:1363/2330 train_time:79552ms step_avg:58.37ms
step:1364/2330 train_time:79612ms step_avg:58.37ms
step:1365/2330 train_time:79668ms step_avg:58.36ms
step:1366/2330 train_time:79729ms step_avg:58.37ms
step:1367/2330 train_time:79786ms step_avg:58.37ms
step:1368/2330 train_time:79846ms step_avg:58.37ms
step:1369/2330 train_time:79902ms step_avg:58.37ms
step:1370/2330 train_time:79962ms step_avg:58.37ms
step:1371/2330 train_time:80020ms step_avg:58.37ms
step:1372/2330 train_time:80080ms step_avg:58.37ms
step:1373/2330 train_time:80136ms step_avg:58.37ms
step:1374/2330 train_time:80196ms step_avg:58.37ms
step:1375/2330 train_time:80253ms step_avg:58.37ms
step:1376/2330 train_time:80314ms step_avg:58.37ms
step:1377/2330 train_time:80371ms step_avg:58.37ms
step:1378/2330 train_time:80431ms step_avg:58.37ms
step:1379/2330 train_time:80488ms step_avg:58.37ms
step:1380/2330 train_time:80549ms step_avg:58.37ms
step:1381/2330 train_time:80605ms step_avg:58.37ms
step:1382/2330 train_time:80666ms step_avg:58.37ms
step:1383/2330 train_time:80723ms step_avg:58.37ms
step:1384/2330 train_time:80783ms step_avg:58.37ms
step:1385/2330 train_time:80840ms step_avg:58.37ms
step:1386/2330 train_time:80899ms step_avg:58.37ms
step:1387/2330 train_time:80956ms step_avg:58.37ms
step:1388/2330 train_time:81017ms step_avg:58.37ms
step:1389/2330 train_time:81073ms step_avg:58.37ms
step:1390/2330 train_time:81134ms step_avg:58.37ms
step:1391/2330 train_time:81192ms step_avg:58.37ms
step:1392/2330 train_time:81252ms step_avg:58.37ms
step:1393/2330 train_time:81309ms step_avg:58.37ms
step:1394/2330 train_time:81370ms step_avg:58.37ms
step:1395/2330 train_time:81426ms step_avg:58.37ms
step:1396/2330 train_time:81487ms step_avg:58.37ms
step:1397/2330 train_time:81543ms step_avg:58.37ms
step:1398/2330 train_time:81604ms step_avg:58.37ms
step:1399/2330 train_time:81661ms step_avg:58.37ms
step:1400/2330 train_time:81721ms step_avg:58.37ms
step:1401/2330 train_time:81778ms step_avg:58.37ms
step:1402/2330 train_time:81837ms step_avg:58.37ms
step:1403/2330 train_time:81894ms step_avg:58.37ms
step:1404/2330 train_time:81955ms step_avg:58.37ms
step:1405/2330 train_time:82012ms step_avg:58.37ms
step:1406/2330 train_time:82071ms step_avg:58.37ms
step:1407/2330 train_time:82128ms step_avg:58.37ms
step:1408/2330 train_time:82188ms step_avg:58.37ms
step:1409/2330 train_time:82245ms step_avg:58.37ms
step:1410/2330 train_time:82305ms step_avg:58.37ms
step:1411/2330 train_time:82362ms step_avg:58.37ms
step:1412/2330 train_time:82422ms step_avg:58.37ms
step:1413/2330 train_time:82479ms step_avg:58.37ms
step:1414/2330 train_time:82539ms step_avg:58.37ms
step:1415/2330 train_time:82596ms step_avg:58.37ms
step:1416/2330 train_time:82656ms step_avg:58.37ms
step:1417/2330 train_time:82713ms step_avg:58.37ms
step:1418/2330 train_time:82774ms step_avg:58.37ms
step:1419/2330 train_time:82830ms step_avg:58.37ms
step:1420/2330 train_time:82892ms step_avg:58.37ms
step:1421/2330 train_time:82948ms step_avg:58.37ms
step:1422/2330 train_time:83009ms step_avg:58.37ms
step:1423/2330 train_time:83066ms step_avg:58.37ms
step:1424/2330 train_time:83125ms step_avg:58.37ms
step:1425/2330 train_time:83181ms step_avg:58.37ms
step:1426/2330 train_time:83242ms step_avg:58.37ms
step:1427/2330 train_time:83299ms step_avg:58.37ms
step:1428/2330 train_time:83359ms step_avg:58.37ms
step:1429/2330 train_time:83416ms step_avg:58.37ms
step:1430/2330 train_time:83476ms step_avg:58.37ms
step:1431/2330 train_time:83532ms step_avg:58.37ms
step:1432/2330 train_time:83593ms step_avg:58.37ms
step:1433/2330 train_time:83650ms step_avg:58.37ms
step:1434/2330 train_time:83710ms step_avg:58.38ms
step:1435/2330 train_time:83766ms step_avg:58.37ms
step:1436/2330 train_time:83828ms step_avg:58.38ms
step:1437/2330 train_time:83885ms step_avg:58.38ms
step:1438/2330 train_time:83945ms step_avg:58.38ms
step:1439/2330 train_time:84002ms step_avg:58.38ms
step:1440/2330 train_time:84062ms step_avg:58.38ms
step:1441/2330 train_time:84119ms step_avg:58.38ms
step:1442/2330 train_time:84179ms step_avg:58.38ms
step:1443/2330 train_time:84235ms step_avg:58.38ms
step:1444/2330 train_time:84296ms step_avg:58.38ms
step:1445/2330 train_time:84353ms step_avg:58.38ms
step:1446/2330 train_time:84414ms step_avg:58.38ms
step:1447/2330 train_time:84471ms step_avg:58.38ms
step:1448/2330 train_time:84532ms step_avg:58.38ms
step:1449/2330 train_time:84589ms step_avg:58.38ms
step:1450/2330 train_time:84649ms step_avg:58.38ms
step:1451/2330 train_time:84707ms step_avg:58.38ms
step:1452/2330 train_time:84766ms step_avg:58.38ms
step:1453/2330 train_time:84822ms step_avg:58.38ms
step:1454/2330 train_time:84883ms step_avg:58.38ms
step:1455/2330 train_time:84940ms step_avg:58.38ms
step:1456/2330 train_time:85001ms step_avg:58.38ms
step:1457/2330 train_time:85059ms step_avg:58.38ms
step:1458/2330 train_time:85118ms step_avg:58.38ms
step:1459/2330 train_time:85176ms step_avg:58.38ms
step:1460/2330 train_time:85236ms step_avg:58.38ms
step:1461/2330 train_time:85293ms step_avg:58.38ms
step:1462/2330 train_time:85354ms step_avg:58.38ms
step:1463/2330 train_time:85410ms step_avg:58.38ms
step:1464/2330 train_time:85471ms step_avg:58.38ms
step:1465/2330 train_time:85528ms step_avg:58.38ms
step:1466/2330 train_time:85589ms step_avg:58.38ms
step:1467/2330 train_time:85646ms step_avg:58.38ms
step:1468/2330 train_time:85706ms step_avg:58.38ms
step:1469/2330 train_time:85763ms step_avg:58.38ms
step:1470/2330 train_time:85824ms step_avg:58.38ms
step:1471/2330 train_time:85880ms step_avg:58.38ms
step:1472/2330 train_time:85941ms step_avg:58.38ms
step:1473/2330 train_time:85998ms step_avg:58.38ms
step:1474/2330 train_time:86059ms step_avg:58.38ms
step:1475/2330 train_time:86115ms step_avg:58.38ms
step:1476/2330 train_time:86176ms step_avg:58.38ms
step:1477/2330 train_time:86233ms step_avg:58.38ms
step:1478/2330 train_time:86292ms step_avg:58.38ms
step:1479/2330 train_time:86349ms step_avg:58.38ms
step:1480/2330 train_time:86410ms step_avg:58.38ms
step:1481/2330 train_time:86466ms step_avg:58.38ms
step:1482/2330 train_time:86527ms step_avg:58.39ms
step:1483/2330 train_time:86584ms step_avg:58.38ms
step:1484/2330 train_time:86644ms step_avg:58.39ms
step:1485/2330 train_time:86701ms step_avg:58.38ms
step:1486/2330 train_time:86761ms step_avg:58.39ms
step:1487/2330 train_time:86817ms step_avg:58.38ms
step:1488/2330 train_time:86878ms step_avg:58.39ms
step:1489/2330 train_time:86934ms step_avg:58.38ms
step:1490/2330 train_time:86995ms step_avg:58.39ms
step:1491/2330 train_time:87052ms step_avg:58.38ms
step:1492/2330 train_time:87113ms step_avg:58.39ms
step:1493/2330 train_time:87170ms step_avg:58.39ms
step:1494/2330 train_time:87230ms step_avg:58.39ms
step:1495/2330 train_time:87287ms step_avg:58.39ms
step:1496/2330 train_time:87348ms step_avg:58.39ms
step:1497/2330 train_time:87404ms step_avg:58.39ms
step:1498/2330 train_time:87464ms step_avg:58.39ms
step:1499/2330 train_time:87522ms step_avg:58.39ms
step:1500/2330 train_time:87581ms step_avg:58.39ms
step:1500/2330 val_loss:3.9165 train_time:87661ms step_avg:58.44ms
step:1501/2330 train_time:87680ms step_avg:58.41ms
step:1502/2330 train_time:87700ms step_avg:58.39ms
step:1503/2330 train_time:87761ms step_avg:58.39ms
step:1504/2330 train_time:87826ms step_avg:58.39ms
step:1505/2330 train_time:87884ms step_avg:58.39ms
step:1506/2330 train_time:87946ms step_avg:58.40ms
step:1507/2330 train_time:88002ms step_avg:58.40ms
step:1508/2330 train_time:88064ms step_avg:58.40ms
step:1509/2330 train_time:88120ms step_avg:58.40ms
step:1510/2330 train_time:88181ms step_avg:58.40ms
step:1511/2330 train_time:88237ms step_avg:58.40ms
step:1512/2330 train_time:88298ms step_avg:58.40ms
step:1513/2330 train_time:88354ms step_avg:58.40ms
step:1514/2330 train_time:88414ms step_avg:58.40ms
step:1515/2330 train_time:88470ms step_avg:58.40ms
step:1516/2330 train_time:88530ms step_avg:58.40ms
step:1517/2330 train_time:88587ms step_avg:58.40ms
step:1518/2330 train_time:88646ms step_avg:58.40ms
step:1519/2330 train_time:88704ms step_avg:58.40ms
step:1520/2330 train_time:88765ms step_avg:58.40ms
step:1521/2330 train_time:88824ms step_avg:58.40ms
step:1522/2330 train_time:88885ms step_avg:58.40ms
step:1523/2330 train_time:88942ms step_avg:58.40ms
step:1524/2330 train_time:89004ms step_avg:58.40ms
step:1525/2330 train_time:89061ms step_avg:58.40ms
step:1526/2330 train_time:89121ms step_avg:58.40ms
step:1527/2330 train_time:89176ms step_avg:58.40ms
step:1528/2330 train_time:89238ms step_avg:58.40ms
step:1529/2330 train_time:89296ms step_avg:58.40ms
step:1530/2330 train_time:89356ms step_avg:58.40ms
step:1531/2330 train_time:89412ms step_avg:58.40ms
step:1532/2330 train_time:89473ms step_avg:58.40ms
step:1533/2330 train_time:89530ms step_avg:58.40ms
step:1534/2330 train_time:89589ms step_avg:58.40ms
step:1535/2330 train_time:89646ms step_avg:58.40ms
step:1536/2330 train_time:89707ms step_avg:58.40ms
step:1537/2330 train_time:89765ms step_avg:58.40ms
step:1538/2330 train_time:89827ms step_avg:58.40ms
step:1539/2330 train_time:89885ms step_avg:58.40ms
step:1540/2330 train_time:89946ms step_avg:58.41ms
step:1541/2330 train_time:90004ms step_avg:58.41ms
step:1542/2330 train_time:90064ms step_avg:58.41ms
step:1543/2330 train_time:90123ms step_avg:58.41ms
step:1544/2330 train_time:90183ms step_avg:58.41ms
step:1545/2330 train_time:90240ms step_avg:58.41ms
step:1546/2330 train_time:90302ms step_avg:58.41ms
step:1547/2330 train_time:90358ms step_avg:58.41ms
step:1548/2330 train_time:90421ms step_avg:58.41ms
step:1549/2330 train_time:90478ms step_avg:58.41ms
step:1550/2330 train_time:90539ms step_avg:58.41ms
step:1551/2330 train_time:90595ms step_avg:58.41ms
step:1552/2330 train_time:90657ms step_avg:58.41ms
step:1553/2330 train_time:90713ms step_avg:58.41ms
step:1554/2330 train_time:90777ms step_avg:58.41ms
step:1555/2330 train_time:90832ms step_avg:58.41ms
step:1556/2330 train_time:90896ms step_avg:58.42ms
step:1557/2330 train_time:90952ms step_avg:58.41ms
step:1558/2330 train_time:91015ms step_avg:58.42ms
step:1559/2330 train_time:91073ms step_avg:58.42ms
step:1560/2330 train_time:91133ms step_avg:58.42ms
step:1561/2330 train_time:91190ms step_avg:58.42ms
step:1562/2330 train_time:91251ms step_avg:58.42ms
step:1563/2330 train_time:91308ms step_avg:58.42ms
step:1564/2330 train_time:91369ms step_avg:58.42ms
step:1565/2330 train_time:91426ms step_avg:58.42ms
step:1566/2330 train_time:91486ms step_avg:58.42ms
step:1567/2330 train_time:91544ms step_avg:58.42ms
step:1568/2330 train_time:91604ms step_avg:58.42ms
step:1569/2330 train_time:91661ms step_avg:58.42ms
step:1570/2330 train_time:91723ms step_avg:58.42ms
step:1571/2330 train_time:91780ms step_avg:58.42ms
step:1572/2330 train_time:91841ms step_avg:58.42ms
step:1573/2330 train_time:91897ms step_avg:58.42ms
step:1574/2330 train_time:91960ms step_avg:58.42ms
step:1575/2330 train_time:92017ms step_avg:58.42ms
step:1576/2330 train_time:92080ms step_avg:58.43ms
step:1577/2330 train_time:92136ms step_avg:58.42ms
step:1578/2330 train_time:92200ms step_avg:58.43ms
step:1579/2330 train_time:92256ms step_avg:58.43ms
step:1580/2330 train_time:92318ms step_avg:58.43ms
step:1581/2330 train_time:92375ms step_avg:58.43ms
step:1582/2330 train_time:92437ms step_avg:58.43ms
step:1583/2330 train_time:92493ms step_avg:58.43ms
step:1584/2330 train_time:92555ms step_avg:58.43ms
step:1585/2330 train_time:92612ms step_avg:58.43ms
step:1586/2330 train_time:92673ms step_avg:58.43ms
step:1587/2330 train_time:92730ms step_avg:58.43ms
step:1588/2330 train_time:92791ms step_avg:58.43ms
step:1589/2330 train_time:92849ms step_avg:58.43ms
step:1590/2330 train_time:92909ms step_avg:58.43ms
step:1591/2330 train_time:92966ms step_avg:58.43ms
step:1592/2330 train_time:93027ms step_avg:58.43ms
step:1593/2330 train_time:93086ms step_avg:58.43ms
step:1594/2330 train_time:93147ms step_avg:58.44ms
step:1595/2330 train_time:93205ms step_avg:58.44ms
step:1596/2330 train_time:93265ms step_avg:58.44ms
step:1597/2330 train_time:93322ms step_avg:58.44ms
step:1598/2330 train_time:93383ms step_avg:58.44ms
step:1599/2330 train_time:93439ms step_avg:58.44ms
step:1600/2330 train_time:93503ms step_avg:58.44ms
step:1601/2330 train_time:93559ms step_avg:58.44ms
step:1602/2330 train_time:93621ms step_avg:58.44ms
step:1603/2330 train_time:93677ms step_avg:58.44ms
step:1604/2330 train_time:93740ms step_avg:58.44ms
step:1605/2330 train_time:93796ms step_avg:58.44ms
step:1606/2330 train_time:93859ms step_avg:58.44ms
step:1607/2330 train_time:93916ms step_avg:58.44ms
step:1608/2330 train_time:93978ms step_avg:58.44ms
step:1609/2330 train_time:94034ms step_avg:58.44ms
step:1610/2330 train_time:94098ms step_avg:58.45ms
step:1611/2330 train_time:94154ms step_avg:58.44ms
step:1612/2330 train_time:94217ms step_avg:58.45ms
step:1613/2330 train_time:94274ms step_avg:58.45ms
step:1614/2330 train_time:94337ms step_avg:58.45ms
step:1615/2330 train_time:94394ms step_avg:58.45ms
step:1616/2330 train_time:94455ms step_avg:58.45ms
step:1617/2330 train_time:94512ms step_avg:58.45ms
step:1618/2330 train_time:94573ms step_avg:58.45ms
step:1619/2330 train_time:94630ms step_avg:58.45ms
step:1620/2330 train_time:94690ms step_avg:58.45ms
step:1621/2330 train_time:94747ms step_avg:58.45ms
step:1622/2330 train_time:94807ms step_avg:58.45ms
step:1623/2330 train_time:94865ms step_avg:58.45ms
step:1624/2330 train_time:94925ms step_avg:58.45ms
step:1625/2330 train_time:94982ms step_avg:58.45ms
step:1626/2330 train_time:95044ms step_avg:58.45ms
step:1627/2330 train_time:95101ms step_avg:58.45ms
step:1628/2330 train_time:95163ms step_avg:58.45ms
step:1629/2330 train_time:95219ms step_avg:58.45ms
step:1630/2330 train_time:95282ms step_avg:58.45ms
step:1631/2330 train_time:95338ms step_avg:58.45ms
step:1632/2330 train_time:95400ms step_avg:58.46ms
step:1633/2330 train_time:95457ms step_avg:58.45ms
step:1634/2330 train_time:95520ms step_avg:58.46ms
step:1635/2330 train_time:95576ms step_avg:58.46ms
step:1636/2330 train_time:95638ms step_avg:58.46ms
step:1637/2330 train_time:95694ms step_avg:58.46ms
step:1638/2330 train_time:95756ms step_avg:58.46ms
step:1639/2330 train_time:95813ms step_avg:58.46ms
step:1640/2330 train_time:95875ms step_avg:58.46ms
step:1641/2330 train_time:95932ms step_avg:58.46ms
step:1642/2330 train_time:95993ms step_avg:58.46ms
step:1643/2330 train_time:96050ms step_avg:58.46ms
step:1644/2330 train_time:96112ms step_avg:58.46ms
step:1645/2330 train_time:96169ms step_avg:58.46ms
step:1646/2330 train_time:96230ms step_avg:58.46ms
step:1647/2330 train_time:96288ms step_avg:58.46ms
step:1648/2330 train_time:96349ms step_avg:58.46ms
step:1649/2330 train_time:96407ms step_avg:58.46ms
step:1650/2330 train_time:96468ms step_avg:58.47ms
step:1651/2330 train_time:96526ms step_avg:58.47ms
step:1652/2330 train_time:96586ms step_avg:58.47ms
step:1653/2330 train_time:96643ms step_avg:58.47ms
step:1654/2330 train_time:96704ms step_avg:58.47ms
step:1655/2330 train_time:96760ms step_avg:58.47ms
step:1656/2330 train_time:96822ms step_avg:58.47ms
step:1657/2330 train_time:96879ms step_avg:58.47ms
step:1658/2330 train_time:96941ms step_avg:58.47ms
step:1659/2330 train_time:96997ms step_avg:58.47ms
step:1660/2330 train_time:97060ms step_avg:58.47ms
step:1661/2330 train_time:97117ms step_avg:58.47ms
step:1662/2330 train_time:97179ms step_avg:58.47ms
step:1663/2330 train_time:97235ms step_avg:58.47ms
step:1664/2330 train_time:97298ms step_avg:58.47ms
step:1665/2330 train_time:97354ms step_avg:58.47ms
step:1666/2330 train_time:97418ms step_avg:58.47ms
step:1667/2330 train_time:97475ms step_avg:58.47ms
step:1668/2330 train_time:97537ms step_avg:58.48ms
step:1669/2330 train_time:97594ms step_avg:58.47ms
step:1670/2330 train_time:97655ms step_avg:58.48ms
step:1671/2330 train_time:97713ms step_avg:58.48ms
step:1672/2330 train_time:97773ms step_avg:58.48ms
step:1673/2330 train_time:97831ms step_avg:58.48ms
step:1674/2330 train_time:97891ms step_avg:58.48ms
step:1675/2330 train_time:97949ms step_avg:58.48ms
step:1676/2330 train_time:98009ms step_avg:58.48ms
step:1677/2330 train_time:98066ms step_avg:58.48ms
step:1678/2330 train_time:98127ms step_avg:58.48ms
step:1679/2330 train_time:98184ms step_avg:58.48ms
step:1680/2330 train_time:98245ms step_avg:58.48ms
step:1681/2330 train_time:98302ms step_avg:58.48ms
step:1682/2330 train_time:98364ms step_avg:58.48ms
step:1683/2330 train_time:98421ms step_avg:58.48ms
step:1684/2330 train_time:98482ms step_avg:58.48ms
step:1685/2330 train_time:98539ms step_avg:58.48ms
step:1686/2330 train_time:98601ms step_avg:58.48ms
step:1687/2330 train_time:98657ms step_avg:58.48ms
step:1688/2330 train_time:98721ms step_avg:58.48ms
step:1689/2330 train_time:98777ms step_avg:58.48ms
step:1690/2330 train_time:98840ms step_avg:58.49ms
step:1691/2330 train_time:98896ms step_avg:58.48ms
step:1692/2330 train_time:98958ms step_avg:58.49ms
step:1693/2330 train_time:99015ms step_avg:58.48ms
step:1694/2330 train_time:99076ms step_avg:58.49ms
step:1695/2330 train_time:99133ms step_avg:58.49ms
step:1696/2330 train_time:99195ms step_avg:58.49ms
step:1697/2330 train_time:99252ms step_avg:58.49ms
step:1698/2330 train_time:99314ms step_avg:58.49ms
step:1699/2330 train_time:99371ms step_avg:58.49ms
step:1700/2330 train_time:99431ms step_avg:58.49ms
step:1701/2330 train_time:99489ms step_avg:58.49ms
step:1702/2330 train_time:99550ms step_avg:58.49ms
step:1703/2330 train_time:99608ms step_avg:58.49ms
step:1704/2330 train_time:99667ms step_avg:58.49ms
step:1705/2330 train_time:99725ms step_avg:58.49ms
step:1706/2330 train_time:99785ms step_avg:58.49ms
step:1707/2330 train_time:99842ms step_avg:58.49ms
step:1708/2330 train_time:99903ms step_avg:58.49ms
step:1709/2330 train_time:99960ms step_avg:58.49ms
step:1710/2330 train_time:100021ms step_avg:58.49ms
step:1711/2330 train_time:100077ms step_avg:58.49ms
step:1712/2330 train_time:100139ms step_avg:58.49ms
step:1713/2330 train_time:100196ms step_avg:58.49ms
step:1714/2330 train_time:100258ms step_avg:58.49ms
step:1715/2330 train_time:100315ms step_avg:58.49ms
step:1716/2330 train_time:100379ms step_avg:58.50ms
step:1717/2330 train_time:100435ms step_avg:58.49ms
step:1718/2330 train_time:100497ms step_avg:58.50ms
step:1719/2330 train_time:100553ms step_avg:58.50ms
step:1720/2330 train_time:100616ms step_avg:58.50ms
step:1721/2330 train_time:100673ms step_avg:58.50ms
step:1722/2330 train_time:100735ms step_avg:58.50ms
step:1723/2330 train_time:100792ms step_avg:58.50ms
step:1724/2330 train_time:100853ms step_avg:58.50ms
step:1725/2330 train_time:100910ms step_avg:58.50ms
step:1726/2330 train_time:100971ms step_avg:58.50ms
step:1727/2330 train_time:101029ms step_avg:58.50ms
step:1728/2330 train_time:101089ms step_avg:58.50ms
step:1729/2330 train_time:101146ms step_avg:58.50ms
step:1730/2330 train_time:101207ms step_avg:58.50ms
step:1731/2330 train_time:101264ms step_avg:58.50ms
step:1732/2330 train_time:101326ms step_avg:58.50ms
step:1733/2330 train_time:101382ms step_avg:58.50ms
step:1734/2330 train_time:101444ms step_avg:58.50ms
step:1735/2330 train_time:101501ms step_avg:58.50ms
step:1736/2330 train_time:101562ms step_avg:58.50ms
step:1737/2330 train_time:101620ms step_avg:58.50ms
step:1738/2330 train_time:101681ms step_avg:58.50ms
step:1739/2330 train_time:101737ms step_avg:58.50ms
step:1740/2330 train_time:101801ms step_avg:58.51ms
step:1741/2330 train_time:101857ms step_avg:58.50ms
step:1742/2330 train_time:101919ms step_avg:58.51ms
step:1743/2330 train_time:101975ms step_avg:58.51ms
step:1744/2330 train_time:102038ms step_avg:58.51ms
step:1745/2330 train_time:102094ms step_avg:58.51ms
step:1746/2330 train_time:102157ms step_avg:58.51ms
step:1747/2330 train_time:102213ms step_avg:58.51ms
step:1748/2330 train_time:102277ms step_avg:58.51ms
step:1749/2330 train_time:102335ms step_avg:58.51ms
step:1750/2330 train_time:102395ms step_avg:58.51ms
step:1750/2330 val_loss:3.8327 train_time:102476ms step_avg:58.56ms
step:1751/2330 train_time:102494ms step_avg:58.53ms
step:1752/2330 train_time:102514ms step_avg:58.51ms
step:1753/2330 train_time:102570ms step_avg:58.51ms
step:1754/2330 train_time:102637ms step_avg:58.52ms
step:1755/2330 train_time:102694ms step_avg:58.52ms
step:1756/2330 train_time:102759ms step_avg:58.52ms
step:1757/2330 train_time:102815ms step_avg:58.52ms
step:1758/2330 train_time:102877ms step_avg:58.52ms
step:1759/2330 train_time:102933ms step_avg:58.52ms
step:1760/2330 train_time:102994ms step_avg:58.52ms
step:1761/2330 train_time:103051ms step_avg:58.52ms
step:1762/2330 train_time:103110ms step_avg:58.52ms
step:1763/2330 train_time:103166ms step_avg:58.52ms
step:1764/2330 train_time:103227ms step_avg:58.52ms
step:1765/2330 train_time:103283ms step_avg:58.52ms
step:1766/2330 train_time:103344ms step_avg:58.52ms
step:1767/2330 train_time:103403ms step_avg:58.52ms
step:1768/2330 train_time:103464ms step_avg:58.52ms
step:1769/2330 train_time:103521ms step_avg:58.52ms
step:1770/2330 train_time:103583ms step_avg:58.52ms
step:1771/2330 train_time:103640ms step_avg:58.52ms
step:1772/2330 train_time:103705ms step_avg:58.52ms
step:1773/2330 train_time:103761ms step_avg:58.52ms
step:1774/2330 train_time:103825ms step_avg:58.53ms
step:1775/2330 train_time:103881ms step_avg:58.52ms
step:1776/2330 train_time:103943ms step_avg:58.53ms
step:1777/2330 train_time:104000ms step_avg:58.53ms
step:1778/2330 train_time:104062ms step_avg:58.53ms
step:1779/2330 train_time:104118ms step_avg:58.53ms
step:1780/2330 train_time:104179ms step_avg:58.53ms
step:1781/2330 train_time:104236ms step_avg:58.53ms
step:1782/2330 train_time:104296ms step_avg:58.53ms
step:1783/2330 train_time:104353ms step_avg:58.53ms
step:1784/2330 train_time:104414ms step_avg:58.53ms
step:1785/2330 train_time:104473ms step_avg:58.53ms
step:1786/2330 train_time:104533ms step_avg:58.53ms
step:1787/2330 train_time:104590ms step_avg:58.53ms
step:1788/2330 train_time:104652ms step_avg:58.53ms
step:1789/2330 train_time:104709ms step_avg:58.53ms
step:1790/2330 train_time:104771ms step_avg:58.53ms
step:1791/2330 train_time:104827ms step_avg:58.53ms
step:1792/2330 train_time:104889ms step_avg:58.53ms
step:1793/2330 train_time:104946ms step_avg:58.53ms
step:1794/2330 train_time:105008ms step_avg:58.53ms
step:1795/2330 train_time:105064ms step_avg:58.53ms
step:1796/2330 train_time:105125ms step_avg:58.53ms
step:1797/2330 train_time:105181ms step_avg:58.53ms
step:1798/2330 train_time:105244ms step_avg:58.53ms
step:1799/2330 train_time:105300ms step_avg:58.53ms
step:1800/2330 train_time:105361ms step_avg:58.53ms
step:1801/2330 train_time:105418ms step_avg:58.53ms
step:1802/2330 train_time:105480ms step_avg:58.53ms
step:1803/2330 train_time:105537ms step_avg:58.53ms
step:1804/2330 train_time:105598ms step_avg:58.54ms
step:1805/2330 train_time:105656ms step_avg:58.54ms
step:1806/2330 train_time:105717ms step_avg:58.54ms
step:1807/2330 train_time:105776ms step_avg:58.54ms
step:1808/2330 train_time:105836ms step_avg:58.54ms
step:1809/2330 train_time:105894ms step_avg:58.54ms
step:1810/2330 train_time:105954ms step_avg:58.54ms
step:1811/2330 train_time:106012ms step_avg:58.54ms
step:1812/2330 train_time:106072ms step_avg:58.54ms
step:1813/2330 train_time:106129ms step_avg:58.54ms
step:1814/2330 train_time:106190ms step_avg:58.54ms
step:1815/2330 train_time:106247ms step_avg:58.54ms
step:1816/2330 train_time:106308ms step_avg:58.54ms
step:1817/2330 train_time:106365ms step_avg:58.54ms
step:1818/2330 train_time:106426ms step_avg:58.54ms
step:1819/2330 train_time:106482ms step_avg:58.54ms
step:1820/2330 train_time:106545ms step_avg:58.54ms
step:1821/2330 train_time:106602ms step_avg:58.54ms
step:1822/2330 train_time:106666ms step_avg:58.54ms
step:1823/2330 train_time:106722ms step_avg:58.54ms
step:1824/2330 train_time:106785ms step_avg:58.54ms
step:1825/2330 train_time:106842ms step_avg:58.54ms
step:1826/2330 train_time:106904ms step_avg:58.55ms
step:1827/2330 train_time:106961ms step_avg:58.54ms
step:1828/2330 train_time:107023ms step_avg:58.55ms
step:1829/2330 train_time:107079ms step_avg:58.55ms
step:1830/2330 train_time:107141ms step_avg:58.55ms
step:1831/2330 train_time:107198ms step_avg:58.55ms
step:1832/2330 train_time:107260ms step_avg:58.55ms
step:1833/2330 train_time:107317ms step_avg:58.55ms
step:1834/2330 train_time:107378ms step_avg:58.55ms
step:1835/2330 train_time:107435ms step_avg:58.55ms
step:1836/2330 train_time:107496ms step_avg:58.55ms
step:1837/2330 train_time:107554ms step_avg:58.55ms
step:1838/2330 train_time:107614ms step_avg:58.55ms
step:1839/2330 train_time:107671ms step_avg:58.55ms
step:1840/2330 train_time:107732ms step_avg:58.55ms
step:1841/2330 train_time:107791ms step_avg:58.55ms
step:1842/2330 train_time:107851ms step_avg:58.55ms
step:1843/2330 train_time:107910ms step_avg:58.55ms
step:1844/2330 train_time:107970ms step_avg:58.55ms
step:1845/2330 train_time:108027ms step_avg:58.55ms
step:1846/2330 train_time:108088ms step_avg:58.55ms
step:1847/2330 train_time:108145ms step_avg:58.55ms
step:1848/2330 train_time:108206ms step_avg:58.55ms
step:1849/2330 train_time:108263ms step_avg:58.55ms
step:1850/2330 train_time:108325ms step_avg:58.55ms
step:1851/2330 train_time:108382ms step_avg:58.55ms
step:1852/2330 train_time:108444ms step_avg:58.56ms
step:1853/2330 train_time:108500ms step_avg:58.55ms
step:1854/2330 train_time:108563ms step_avg:58.56ms
step:1855/2330 train_time:108619ms step_avg:58.55ms
step:1856/2330 train_time:108682ms step_avg:58.56ms
step:1857/2330 train_time:108739ms step_avg:58.56ms
step:1858/2330 train_time:108801ms step_avg:58.56ms
step:1859/2330 train_time:108858ms step_avg:58.56ms
step:1860/2330 train_time:108920ms step_avg:58.56ms
step:1861/2330 train_time:108976ms step_avg:58.56ms
step:1862/2330 train_time:109038ms step_avg:58.56ms
step:1863/2330 train_time:109096ms step_avg:58.56ms
step:1864/2330 train_time:109156ms step_avg:58.56ms
step:1865/2330 train_time:109215ms step_avg:58.56ms
step:1866/2330 train_time:109274ms step_avg:58.56ms
step:1867/2330 train_time:109332ms step_avg:58.56ms
step:1868/2330 train_time:109392ms step_avg:58.56ms
step:1869/2330 train_time:109450ms step_avg:58.56ms
step:1870/2330 train_time:109510ms step_avg:58.56ms
step:1871/2330 train_time:109568ms step_avg:58.56ms
step:1872/2330 train_time:109628ms step_avg:58.56ms
step:1873/2330 train_time:109684ms step_avg:58.56ms
step:1874/2330 train_time:109747ms step_avg:58.56ms
step:1875/2330 train_time:109804ms step_avg:58.56ms
step:1876/2330 train_time:109866ms step_avg:58.56ms
step:1877/2330 train_time:109922ms step_avg:58.56ms
step:1878/2330 train_time:109985ms step_avg:58.56ms
step:1879/2330 train_time:110041ms step_avg:58.56ms
step:1880/2330 train_time:110103ms step_avg:58.57ms
step:1881/2330 train_time:110159ms step_avg:58.56ms
step:1882/2330 train_time:110223ms step_avg:58.57ms
step:1883/2330 train_time:110279ms step_avg:58.57ms
step:1884/2330 train_time:110342ms step_avg:58.57ms
step:1885/2330 train_time:110399ms step_avg:58.57ms
step:1886/2330 train_time:110460ms step_avg:58.57ms
step:1887/2330 train_time:110517ms step_avg:58.57ms
step:1888/2330 train_time:110578ms step_avg:58.57ms
step:1889/2330 train_time:110636ms step_avg:58.57ms
step:1890/2330 train_time:110696ms step_avg:58.57ms
step:1891/2330 train_time:110754ms step_avg:58.57ms
step:1892/2330 train_time:110815ms step_avg:58.57ms
step:1893/2330 train_time:110873ms step_avg:58.57ms
step:1894/2330 train_time:110933ms step_avg:58.57ms
step:1895/2330 train_time:110991ms step_avg:58.57ms
step:1896/2330 train_time:111051ms step_avg:58.57ms
step:1897/2330 train_time:111109ms step_avg:58.57ms
step:1898/2330 train_time:111169ms step_avg:58.57ms
step:1899/2330 train_time:111226ms step_avg:58.57ms
step:1900/2330 train_time:111288ms step_avg:58.57ms
step:1901/2330 train_time:111345ms step_avg:58.57ms
step:1902/2330 train_time:111406ms step_avg:58.57ms
step:1903/2330 train_time:111462ms step_avg:58.57ms
step:1904/2330 train_time:111525ms step_avg:58.57ms
step:1905/2330 train_time:111581ms step_avg:58.57ms
step:1906/2330 train_time:111645ms step_avg:58.58ms
step:1907/2330 train_time:111702ms step_avg:58.57ms
step:1908/2330 train_time:111763ms step_avg:58.58ms
step:1909/2330 train_time:111819ms step_avg:58.57ms
step:1910/2330 train_time:111883ms step_avg:58.58ms
step:1911/2330 train_time:111939ms step_avg:58.58ms
step:1912/2330 train_time:112002ms step_avg:58.58ms
step:1913/2330 train_time:112058ms step_avg:58.58ms
step:1914/2330 train_time:112120ms step_avg:58.58ms
step:1915/2330 train_time:112176ms step_avg:58.58ms
step:1916/2330 train_time:112238ms step_avg:58.58ms
step:1917/2330 train_time:112297ms step_avg:58.58ms
step:1918/2330 train_time:112357ms step_avg:58.58ms
step:1919/2330 train_time:112415ms step_avg:58.58ms
step:1920/2330 train_time:112475ms step_avg:58.58ms
step:1921/2330 train_time:112533ms step_avg:58.58ms
step:1922/2330 train_time:112593ms step_avg:58.58ms
step:1923/2330 train_time:112651ms step_avg:58.58ms
step:1924/2330 train_time:112710ms step_avg:58.58ms
step:1925/2330 train_time:112767ms step_avg:58.58ms
step:1926/2330 train_time:112829ms step_avg:58.58ms
step:1927/2330 train_time:112886ms step_avg:58.58ms
step:1928/2330 train_time:112948ms step_avg:58.58ms
step:1929/2330 train_time:113004ms step_avg:58.58ms
step:1930/2330 train_time:113066ms step_avg:58.58ms
step:1931/2330 train_time:113123ms step_avg:58.58ms
step:1932/2330 train_time:113186ms step_avg:58.58ms
step:1933/2330 train_time:113242ms step_avg:58.58ms
step:1934/2330 train_time:113304ms step_avg:58.59ms
step:1935/2330 train_time:113360ms step_avg:58.58ms
step:1936/2330 train_time:113424ms step_avg:58.59ms
step:1937/2330 train_time:113480ms step_avg:58.59ms
step:1938/2330 train_time:113543ms step_avg:58.59ms
step:1939/2330 train_time:113599ms step_avg:58.59ms
step:1940/2330 train_time:113662ms step_avg:58.59ms
step:1941/2330 train_time:113718ms step_avg:58.59ms
step:1942/2330 train_time:113781ms step_avg:58.59ms
step:1943/2330 train_time:113838ms step_avg:58.59ms
step:1944/2330 train_time:113899ms step_avg:58.59ms
step:1945/2330 train_time:113956ms step_avg:58.59ms
step:1946/2330 train_time:114017ms step_avg:58.59ms
step:1947/2330 train_time:114074ms step_avg:58.59ms
step:1948/2330 train_time:114134ms step_avg:58.59ms
step:1949/2330 train_time:114193ms step_avg:58.59ms
step:1950/2330 train_time:114253ms step_avg:58.59ms
step:1951/2330 train_time:114311ms step_avg:58.59ms
step:1952/2330 train_time:114372ms step_avg:58.59ms
step:1953/2330 train_time:114429ms step_avg:58.59ms
step:1954/2330 train_time:114488ms step_avg:58.59ms
step:1955/2330 train_time:114546ms step_avg:58.59ms
step:1956/2330 train_time:114608ms step_avg:58.59ms
step:1957/2330 train_time:114665ms step_avg:58.59ms
step:1958/2330 train_time:114725ms step_avg:58.59ms
step:1959/2330 train_time:114782ms step_avg:58.59ms
step:1960/2330 train_time:114844ms step_avg:58.59ms
step:1961/2330 train_time:114901ms step_avg:58.59ms
step:1962/2330 train_time:114963ms step_avg:58.59ms
step:1963/2330 train_time:115020ms step_avg:58.59ms
step:1964/2330 train_time:115083ms step_avg:58.60ms
step:1965/2330 train_time:115140ms step_avg:58.60ms
step:1966/2330 train_time:115202ms step_avg:58.60ms
step:1967/2330 train_time:115259ms step_avg:58.60ms
step:1968/2330 train_time:115322ms step_avg:58.60ms
step:1969/2330 train_time:115378ms step_avg:58.60ms
step:1970/2330 train_time:115440ms step_avg:58.60ms
step:1971/2330 train_time:115497ms step_avg:58.60ms
step:1972/2330 train_time:115558ms step_avg:58.60ms
step:1973/2330 train_time:115615ms step_avg:58.60ms
step:1974/2330 train_time:115675ms step_avg:58.60ms
step:1975/2330 train_time:115733ms step_avg:58.60ms
step:1976/2330 train_time:115792ms step_avg:58.60ms
step:1977/2330 train_time:115850ms step_avg:58.60ms
step:1978/2330 train_time:115910ms step_avg:58.60ms
step:1979/2330 train_time:115967ms step_avg:58.60ms
step:1980/2330 train_time:116029ms step_avg:58.60ms
step:1981/2330 train_time:116086ms step_avg:58.60ms
step:1982/2330 train_time:116148ms step_avg:58.60ms
step:1983/2330 train_time:116206ms step_avg:58.60ms
step:1984/2330 train_time:116267ms step_avg:58.60ms
step:1985/2330 train_time:116323ms step_avg:58.60ms
step:1986/2330 train_time:116385ms step_avg:58.60ms
step:1987/2330 train_time:116442ms step_avg:58.60ms
step:1988/2330 train_time:116504ms step_avg:58.60ms
step:1989/2330 train_time:116560ms step_avg:58.60ms
step:1990/2330 train_time:116622ms step_avg:58.60ms
step:1991/2330 train_time:116679ms step_avg:58.60ms
step:1992/2330 train_time:116742ms step_avg:58.61ms
step:1993/2330 train_time:116799ms step_avg:58.60ms
step:1994/2330 train_time:116860ms step_avg:58.61ms
step:1995/2330 train_time:116916ms step_avg:58.60ms
step:1996/2330 train_time:116978ms step_avg:58.61ms
step:1997/2330 train_time:117035ms step_avg:58.61ms
step:1998/2330 train_time:117096ms step_avg:58.61ms
step:1999/2330 train_time:117154ms step_avg:58.61ms
step:2000/2330 train_time:117214ms step_avg:58.61ms
step:2000/2330 val_loss:3.7713 train_time:117296ms step_avg:58.65ms
step:2001/2330 train_time:117314ms step_avg:58.63ms
step:2002/2330 train_time:117335ms step_avg:58.61ms
step:2003/2330 train_time:117394ms step_avg:58.61ms
step:2004/2330 train_time:117460ms step_avg:58.61ms
step:2005/2330 train_time:117519ms step_avg:58.61ms
step:2006/2330 train_time:117582ms step_avg:58.62ms
step:2007/2330 train_time:117639ms step_avg:58.61ms
step:2008/2330 train_time:117699ms step_avg:58.62ms
step:2009/2330 train_time:117756ms step_avg:58.61ms
step:2010/2330 train_time:117817ms step_avg:58.62ms
step:2011/2330 train_time:117873ms step_avg:58.61ms
step:2012/2330 train_time:117933ms step_avg:58.61ms
step:2013/2330 train_time:117989ms step_avg:58.61ms
step:2014/2330 train_time:118050ms step_avg:58.61ms
step:2015/2330 train_time:118106ms step_avg:58.61ms
step:2016/2330 train_time:118166ms step_avg:58.61ms
step:2017/2330 train_time:118222ms step_avg:58.61ms
step:2018/2330 train_time:118286ms step_avg:58.62ms
step:2019/2330 train_time:118343ms step_avg:58.61ms
step:2020/2330 train_time:118407ms step_avg:58.62ms
step:2021/2330 train_time:118465ms step_avg:58.62ms
step:2022/2330 train_time:118527ms step_avg:58.62ms
step:2023/2330 train_time:118584ms step_avg:58.62ms
step:2024/2330 train_time:118645ms step_avg:58.62ms
step:2025/2330 train_time:118703ms step_avg:58.62ms
step:2026/2330 train_time:118763ms step_avg:58.62ms
step:2027/2330 train_time:118821ms step_avg:58.62ms
step:2028/2330 train_time:118880ms step_avg:58.62ms
step:2029/2330 train_time:118937ms step_avg:58.62ms
step:2030/2330 train_time:118997ms step_avg:58.62ms
step:2031/2330 train_time:119054ms step_avg:58.62ms
step:2032/2330 train_time:119114ms step_avg:58.62ms
step:2033/2330 train_time:119170ms step_avg:58.62ms
step:2034/2330 train_time:119232ms step_avg:58.62ms
step:2035/2330 train_time:119289ms step_avg:58.62ms
step:2036/2330 train_time:119352ms step_avg:58.62ms
step:2037/2330 train_time:119409ms step_avg:58.62ms
step:2038/2330 train_time:119474ms step_avg:58.62ms
step:2039/2330 train_time:119530ms step_avg:58.62ms
step:2040/2330 train_time:119593ms step_avg:58.62ms
step:2041/2330 train_time:119649ms step_avg:58.62ms
step:2042/2330 train_time:119713ms step_avg:58.63ms
step:2043/2330 train_time:119769ms step_avg:58.62ms
step:2044/2330 train_time:119831ms step_avg:58.63ms
step:2045/2330 train_time:119887ms step_avg:58.62ms
step:2046/2330 train_time:119949ms step_avg:58.63ms
step:2047/2330 train_time:120005ms step_avg:58.63ms
step:2048/2330 train_time:120066ms step_avg:58.63ms
step:2049/2330 train_time:120123ms step_avg:58.63ms
step:2050/2330 train_time:120183ms step_avg:58.63ms
step:2051/2330 train_time:120240ms step_avg:58.63ms
step:2052/2330 train_time:120300ms step_avg:58.63ms
step:2053/2330 train_time:120360ms step_avg:58.63ms
step:2054/2330 train_time:120420ms step_avg:58.63ms
step:2055/2330 train_time:120479ms step_avg:58.63ms
step:2056/2330 train_time:120539ms step_avg:58.63ms
step:2057/2330 train_time:120597ms step_avg:58.63ms
step:2058/2330 train_time:120658ms step_avg:58.63ms
step:2059/2330 train_time:120716ms step_avg:58.63ms
step:2060/2330 train_time:120776ms step_avg:58.63ms
step:2061/2330 train_time:120832ms step_avg:58.63ms
step:2062/2330 train_time:120895ms step_avg:58.63ms
step:2063/2330 train_time:120951ms step_avg:58.63ms
step:2064/2330 train_time:121012ms step_avg:58.63ms
step:2065/2330 train_time:121069ms step_avg:58.63ms
step:2066/2330 train_time:121130ms step_avg:58.63ms
step:2067/2330 train_time:121186ms step_avg:58.63ms
step:2068/2330 train_time:121249ms step_avg:58.63ms
step:2069/2330 train_time:121305ms step_avg:58.63ms
step:2070/2330 train_time:121368ms step_avg:58.63ms
step:2071/2330 train_time:121425ms step_avg:58.63ms
step:2072/2330 train_time:121487ms step_avg:58.63ms
step:2073/2330 train_time:121544ms step_avg:58.63ms
step:2074/2330 train_time:121607ms step_avg:58.63ms
step:2075/2330 train_time:121665ms step_avg:58.63ms
step:2076/2330 train_time:121725ms step_avg:58.63ms
step:2077/2330 train_time:121784ms step_avg:58.63ms
step:2078/2330 train_time:121844ms step_avg:58.64ms
step:2079/2330 train_time:121901ms step_avg:58.63ms
step:2080/2330 train_time:121961ms step_avg:58.64ms
step:2081/2330 train_time:122019ms step_avg:58.63ms
step:2082/2330 train_time:122079ms step_avg:58.64ms
step:2083/2330 train_time:122136ms step_avg:58.63ms
step:2084/2330 train_time:122196ms step_avg:58.64ms
step:2085/2330 train_time:122252ms step_avg:58.63ms
step:2086/2330 train_time:122314ms step_avg:58.64ms
step:2087/2330 train_time:122371ms step_avg:58.63ms
step:2088/2330 train_time:122434ms step_avg:58.64ms
step:2089/2330 train_time:122490ms step_avg:58.64ms
step:2090/2330 train_time:122553ms step_avg:58.64ms
step:2091/2330 train_time:122610ms step_avg:58.64ms
step:2092/2330 train_time:122673ms step_avg:58.64ms
step:2093/2330 train_time:122729ms step_avg:58.64ms
step:2094/2330 train_time:122792ms step_avg:58.64ms
step:2095/2330 train_time:122848ms step_avg:58.64ms
step:2096/2330 train_time:122910ms step_avg:58.64ms
step:2097/2330 train_time:122967ms step_avg:58.64ms
step:2098/2330 train_time:123029ms step_avg:58.64ms
step:2099/2330 train_time:123085ms step_avg:58.64ms
step:2100/2330 train_time:123146ms step_avg:58.64ms
step:2101/2330 train_time:123204ms step_avg:58.64ms
step:2102/2330 train_time:123264ms step_avg:58.64ms
step:2103/2330 train_time:123323ms step_avg:58.64ms
step:2104/2330 train_time:123383ms step_avg:58.64ms
step:2105/2330 train_time:123441ms step_avg:58.64ms
step:2106/2330 train_time:123501ms step_avg:58.64ms
step:2107/2330 train_time:123558ms step_avg:58.64ms
step:2108/2330 train_time:123619ms step_avg:58.64ms
step:2109/2330 train_time:123677ms step_avg:58.64ms
step:2110/2330 train_time:123738ms step_avg:58.64ms
step:2111/2330 train_time:123795ms step_avg:58.64ms
step:2112/2330 train_time:123856ms step_avg:58.64ms
step:2113/2330 train_time:123913ms step_avg:58.64ms
step:2114/2330 train_time:123975ms step_avg:58.64ms
step:2115/2330 train_time:124031ms step_avg:58.64ms
step:2116/2330 train_time:124094ms step_avg:58.65ms
step:2117/2330 train_time:124150ms step_avg:58.64ms
step:2118/2330 train_time:124212ms step_avg:58.65ms
step:2119/2330 train_time:124268ms step_avg:58.64ms
step:2120/2330 train_time:124330ms step_avg:58.65ms
step:2121/2330 train_time:124387ms step_avg:58.65ms
step:2122/2330 train_time:124449ms step_avg:58.65ms
step:2123/2330 train_time:124505ms step_avg:58.65ms
step:2124/2330 train_time:124568ms step_avg:58.65ms
step:2125/2330 train_time:124625ms step_avg:58.65ms
step:2126/2330 train_time:124686ms step_avg:58.65ms
step:2127/2330 train_time:124743ms step_avg:58.65ms
step:2128/2330 train_time:124805ms step_avg:58.65ms
step:2129/2330 train_time:124863ms step_avg:58.65ms
step:2130/2330 train_time:124923ms step_avg:58.65ms
step:2131/2330 train_time:124980ms step_avg:58.65ms
step:2132/2330 train_time:125041ms step_avg:58.65ms
step:2133/2330 train_time:125100ms step_avg:58.65ms
step:2134/2330 train_time:125161ms step_avg:58.65ms
step:2135/2330 train_time:125219ms step_avg:58.65ms
step:2136/2330 train_time:125279ms step_avg:58.65ms
step:2137/2330 train_time:125336ms step_avg:58.65ms
step:2138/2330 train_time:125397ms step_avg:58.65ms
step:2139/2330 train_time:125454ms step_avg:58.65ms
step:2140/2330 train_time:125516ms step_avg:58.65ms
step:2141/2330 train_time:125573ms step_avg:58.65ms
step:2142/2330 train_time:125636ms step_avg:58.65ms
step:2143/2330 train_time:125692ms step_avg:58.65ms
step:2144/2330 train_time:125755ms step_avg:58.65ms
step:2145/2330 train_time:125811ms step_avg:58.65ms
step:2146/2330 train_time:125873ms step_avg:58.65ms
step:2147/2330 train_time:125929ms step_avg:58.65ms
step:2148/2330 train_time:125992ms step_avg:58.66ms
step:2149/2330 train_time:126049ms step_avg:58.65ms
step:2150/2330 train_time:126111ms step_avg:58.66ms
step:2151/2330 train_time:126168ms step_avg:58.66ms
step:2152/2330 train_time:126229ms step_avg:58.66ms
step:2153/2330 train_time:126285ms step_avg:58.66ms
step:2154/2330 train_time:126347ms step_avg:58.66ms
step:2155/2330 train_time:126404ms step_avg:58.66ms
step:2156/2330 train_time:126465ms step_avg:58.66ms
step:2157/2330 train_time:126523ms step_avg:58.66ms
step:2158/2330 train_time:126583ms step_avg:58.66ms
step:2159/2330 train_time:126640ms step_avg:58.66ms
step:2160/2330 train_time:126702ms step_avg:58.66ms
step:2161/2330 train_time:126760ms step_avg:58.66ms
step:2162/2330 train_time:126821ms step_avg:58.66ms
step:2163/2330 train_time:126879ms step_avg:58.66ms
step:2164/2330 train_time:126939ms step_avg:58.66ms
step:2165/2330 train_time:126998ms step_avg:58.66ms
step:2166/2330 train_time:127058ms step_avg:58.66ms
step:2167/2330 train_time:127115ms step_avg:58.66ms
step:2168/2330 train_time:127177ms step_avg:58.66ms
step:2169/2330 train_time:127233ms step_avg:58.66ms
step:2170/2330 train_time:127295ms step_avg:58.66ms
step:2171/2330 train_time:127351ms step_avg:58.66ms
step:2172/2330 train_time:127414ms step_avg:58.66ms
step:2173/2330 train_time:127471ms step_avg:58.66ms
step:2174/2330 train_time:127533ms step_avg:58.66ms
step:2175/2330 train_time:127590ms step_avg:58.66ms
step:2176/2330 train_time:127653ms step_avg:58.66ms
step:2177/2330 train_time:127710ms step_avg:58.66ms
step:2178/2330 train_time:127772ms step_avg:58.66ms
step:2179/2330 train_time:127828ms step_avg:58.66ms
step:2180/2330 train_time:127891ms step_avg:58.67ms
step:2181/2330 train_time:127948ms step_avg:58.66ms
step:2182/2330 train_time:128011ms step_avg:58.67ms
step:2183/2330 train_time:128068ms step_avg:58.67ms
step:2184/2330 train_time:128128ms step_avg:58.67ms
step:2185/2330 train_time:128185ms step_avg:58.67ms
step:2186/2330 train_time:128246ms step_avg:58.67ms
step:2187/2330 train_time:128304ms step_avg:58.67ms
step:2188/2330 train_time:128365ms step_avg:58.67ms
step:2189/2330 train_time:128423ms step_avg:58.67ms
step:2190/2330 train_time:128483ms step_avg:58.67ms
step:2191/2330 train_time:128540ms step_avg:58.67ms
step:2192/2330 train_time:128601ms step_avg:58.67ms
step:2193/2330 train_time:128659ms step_avg:58.67ms
step:2194/2330 train_time:128719ms step_avg:58.67ms
step:2195/2330 train_time:128777ms step_avg:58.67ms
step:2196/2330 train_time:128836ms step_avg:58.67ms
step:2197/2330 train_time:128892ms step_avg:58.67ms
step:2198/2330 train_time:128955ms step_avg:58.67ms
step:2199/2330 train_time:129012ms step_avg:58.67ms
step:2200/2330 train_time:129073ms step_avg:58.67ms
step:2201/2330 train_time:129130ms step_avg:58.67ms
step:2202/2330 train_time:129192ms step_avg:58.67ms
step:2203/2330 train_time:129248ms step_avg:58.67ms
step:2204/2330 train_time:129311ms step_avg:58.67ms
step:2205/2330 train_time:129367ms step_avg:58.67ms
step:2206/2330 train_time:129430ms step_avg:58.67ms
step:2207/2330 train_time:129486ms step_avg:58.67ms
step:2208/2330 train_time:129550ms step_avg:58.67ms
step:2209/2330 train_time:129607ms step_avg:58.67ms
step:2210/2330 train_time:129671ms step_avg:58.67ms
step:2211/2330 train_time:129727ms step_avg:58.67ms
step:2212/2330 train_time:129789ms step_avg:58.68ms
step:2213/2330 train_time:129846ms step_avg:58.67ms
step:2214/2330 train_time:129908ms step_avg:58.68ms
step:2215/2330 train_time:129965ms step_avg:58.67ms
step:2216/2330 train_time:130026ms step_avg:58.68ms
step:2217/2330 train_time:130083ms step_avg:58.68ms
step:2218/2330 train_time:130144ms step_avg:58.68ms
step:2219/2330 train_time:130201ms step_avg:58.68ms
step:2220/2330 train_time:130261ms step_avg:58.68ms
step:2221/2330 train_time:130319ms step_avg:58.68ms
step:2222/2330 train_time:130379ms step_avg:58.68ms
step:2223/2330 train_time:130436ms step_avg:58.68ms
step:2224/2330 train_time:130498ms step_avg:58.68ms
step:2225/2330 train_time:130555ms step_avg:58.68ms
step:2226/2330 train_time:130616ms step_avg:58.68ms
step:2227/2330 train_time:130673ms step_avg:58.68ms
step:2228/2330 train_time:130735ms step_avg:58.68ms
step:2229/2330 train_time:130791ms step_avg:58.68ms
step:2230/2330 train_time:130853ms step_avg:58.68ms
step:2231/2330 train_time:130910ms step_avg:58.68ms
step:2232/2330 train_time:130972ms step_avg:58.68ms
step:2233/2330 train_time:131029ms step_avg:58.68ms
step:2234/2330 train_time:131090ms step_avg:58.68ms
step:2235/2330 train_time:131147ms step_avg:58.68ms
step:2236/2330 train_time:131209ms step_avg:58.68ms
step:2237/2330 train_time:131266ms step_avg:58.68ms
step:2238/2330 train_time:131328ms step_avg:58.68ms
step:2239/2330 train_time:131385ms step_avg:58.68ms
step:2240/2330 train_time:131446ms step_avg:58.68ms
step:2241/2330 train_time:131503ms step_avg:58.68ms
step:2242/2330 train_time:131564ms step_avg:58.68ms
step:2243/2330 train_time:131621ms step_avg:58.68ms
step:2244/2330 train_time:131682ms step_avg:58.68ms
step:2245/2330 train_time:131740ms step_avg:58.68ms
step:2246/2330 train_time:131800ms step_avg:58.68ms
step:2247/2330 train_time:131859ms step_avg:58.68ms
step:2248/2330 train_time:131920ms step_avg:58.68ms
step:2249/2330 train_time:131978ms step_avg:58.68ms
step:2250/2330 train_time:132038ms step_avg:58.68ms
step:2250/2330 val_loss:3.7244 train_time:132120ms step_avg:58.72ms
step:2251/2330 train_time:132138ms step_avg:58.70ms
step:2252/2330 train_time:132159ms step_avg:58.69ms
step:2253/2330 train_time:132221ms step_avg:58.69ms
step:2254/2330 train_time:132284ms step_avg:58.69ms
step:2255/2330 train_time:132342ms step_avg:58.69ms
step:2256/2330 train_time:132403ms step_avg:58.69ms
step:2257/2330 train_time:132461ms step_avg:58.69ms
step:2258/2330 train_time:132520ms step_avg:58.69ms
step:2259/2330 train_time:132577ms step_avg:58.69ms
step:2260/2330 train_time:132636ms step_avg:58.69ms
step:2261/2330 train_time:132692ms step_avg:58.69ms
step:2262/2330 train_time:132753ms step_avg:58.69ms
step:2263/2330 train_time:132809ms step_avg:58.69ms
step:2264/2330 train_time:132870ms step_avg:58.69ms
step:2265/2330 train_time:132927ms step_avg:58.69ms
step:2266/2330 train_time:132988ms step_avg:58.69ms
step:2267/2330 train_time:133044ms step_avg:58.69ms
step:2268/2330 train_time:133107ms step_avg:58.69ms
step:2269/2330 train_time:133165ms step_avg:58.69ms
step:2270/2330 train_time:133231ms step_avg:58.69ms
step:2271/2330 train_time:133288ms step_avg:58.69ms
step:2272/2330 train_time:133352ms step_avg:58.69ms
step:2273/2330 train_time:133408ms step_avg:58.69ms
step:2274/2330 train_time:133472ms step_avg:58.69ms
step:2275/2330 train_time:133529ms step_avg:58.69ms
step:2276/2330 train_time:133590ms step_avg:58.69ms
step:2277/2330 train_time:133646ms step_avg:58.69ms
step:2278/2330 train_time:133707ms step_avg:58.69ms
step:2279/2330 train_time:133763ms step_avg:58.69ms
step:2280/2330 train_time:133824ms step_avg:58.69ms
step:2281/2330 train_time:133881ms step_avg:58.69ms
step:2282/2330 train_time:133940ms step_avg:58.69ms
step:2283/2330 train_time:133997ms step_avg:58.69ms
step:2284/2330 train_time:134057ms step_avg:58.69ms
step:2285/2330 train_time:134116ms step_avg:58.69ms
step:2286/2330 train_time:134177ms step_avg:58.69ms
step:2287/2330 train_time:134234ms step_avg:58.69ms
step:2288/2330 train_time:134297ms step_avg:58.70ms
step:2289/2330 train_time:134355ms step_avg:58.70ms
step:2290/2330 train_time:134417ms step_avg:58.70ms
step:2291/2330 train_time:134476ms step_avg:58.70ms
step:2292/2330 train_time:134535ms step_avg:58.70ms
step:2293/2330 train_time:134592ms step_avg:58.70ms
step:2294/2330 train_time:134653ms step_avg:58.70ms
step:2295/2330 train_time:134711ms step_avg:58.70ms
step:2296/2330 train_time:134771ms step_avg:58.70ms
step:2297/2330 train_time:134828ms step_avg:58.70ms
step:2298/2330 train_time:134888ms step_avg:58.70ms
step:2299/2330 train_time:134945ms step_avg:58.70ms
step:2300/2330 train_time:135006ms step_avg:58.70ms
step:2301/2330 train_time:135062ms step_avg:58.70ms
step:2302/2330 train_time:135125ms step_avg:58.70ms
step:2303/2330 train_time:135182ms step_avg:58.70ms
step:2304/2330 train_time:135243ms step_avg:58.70ms
step:2305/2330 train_time:135302ms step_avg:58.70ms
step:2306/2330 train_time:135363ms step_avg:58.70ms
step:2307/2330 train_time:135421ms step_avg:58.70ms
step:2308/2330 train_time:135482ms step_avg:58.70ms
step:2309/2330 train_time:135540ms step_avg:58.70ms
step:2310/2330 train_time:135600ms step_avg:58.70ms
step:2311/2330 train_time:135657ms step_avg:58.70ms
step:2312/2330 train_time:135717ms step_avg:58.70ms
step:2313/2330 train_time:135774ms step_avg:58.70ms
step:2314/2330 train_time:135835ms step_avg:58.70ms
step:2315/2330 train_time:135891ms step_avg:58.70ms
step:2316/2330 train_time:135952ms step_avg:58.70ms
step:2317/2330 train_time:136009ms step_avg:58.70ms
step:2318/2330 train_time:136071ms step_avg:58.70ms
step:2319/2330 train_time:136127ms step_avg:58.70ms
step:2320/2330 train_time:136190ms step_avg:58.70ms
step:2321/2330 train_time:136246ms step_avg:58.70ms
step:2322/2330 train_time:136311ms step_avg:58.70ms
step:2323/2330 train_time:136368ms step_avg:58.70ms
step:2324/2330 train_time:136430ms step_avg:58.70ms
step:2325/2330 train_time:136487ms step_avg:58.70ms
step:2326/2330 train_time:136549ms step_avg:58.71ms
step:2327/2330 train_time:136606ms step_avg:58.70ms
step:2328/2330 train_time:136668ms step_avg:58.71ms
step:2329/2330 train_time:136725ms step_avg:58.71ms
step:2330/2330 train_time:136786ms step_avg:58.71ms
step:2330/2330 val_loss:3.7092 train_time:136867ms step_avg:58.74ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
