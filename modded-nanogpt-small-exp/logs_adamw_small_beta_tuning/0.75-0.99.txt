import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--beta1', type=str, required=True)
parser.add_argument('--beta2', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_beta_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=6e-4,  betas=(float(args2.beta1), float(args2.beta2)), eps=1e-10, weight_decay=0.0, fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt", exist_ok=True)
            torch.save(log, f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 04:46:14 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   27C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   26C    P0             115W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   25C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   27C    P0             114W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   25C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   26C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:81ms step_avg:80.77ms
step:2/2330 train_time:189ms step_avg:94.33ms
step:3/2330 train_time:207ms step_avg:69.15ms
step:4/2330 train_time:227ms step_avg:56.68ms
step:5/2330 train_time:279ms step_avg:55.85ms
step:6/2330 train_time:337ms step_avg:56.23ms
step:7/2330 train_time:393ms step_avg:56.10ms
step:8/2330 train_time:451ms step_avg:56.32ms
step:9/2330 train_time:506ms step_avg:56.23ms
step:10/2330 train_time:564ms step_avg:56.42ms
step:11/2330 train_time:620ms step_avg:56.35ms
step:12/2330 train_time:678ms step_avg:56.48ms
step:13/2330 train_time:733ms step_avg:56.37ms
step:14/2330 train_time:791ms step_avg:56.51ms
step:15/2330 train_time:846ms step_avg:56.42ms
step:16/2330 train_time:904ms step_avg:56.52ms
step:17/2330 train_time:960ms step_avg:56.45ms
step:18/2330 train_time:1018ms step_avg:56.58ms
step:19/2330 train_time:1075ms step_avg:56.60ms
step:20/2330 train_time:1138ms step_avg:56.89ms
step:21/2330 train_time:1196ms step_avg:56.94ms
step:22/2330 train_time:1257ms step_avg:57.12ms
step:23/2330 train_time:1313ms step_avg:57.08ms
step:24/2330 train_time:1373ms step_avg:57.22ms
step:25/2330 train_time:1429ms step_avg:57.15ms
step:26/2330 train_time:1489ms step_avg:57.27ms
step:27/2330 train_time:1545ms step_avg:57.20ms
step:28/2330 train_time:1603ms step_avg:57.25ms
step:29/2330 train_time:1659ms step_avg:57.20ms
step:30/2330 train_time:1717ms step_avg:57.22ms
step:31/2330 train_time:1772ms step_avg:57.17ms
step:32/2330 train_time:1831ms step_avg:57.21ms
step:33/2330 train_time:1886ms step_avg:57.16ms
step:34/2330 train_time:1944ms step_avg:57.19ms
step:35/2330 train_time:2001ms step_avg:57.18ms
step:36/2330 train_time:2061ms step_avg:57.24ms
step:37/2330 train_time:2118ms step_avg:57.24ms
step:38/2330 train_time:2178ms step_avg:57.30ms
step:39/2330 train_time:2234ms step_avg:57.27ms
step:40/2330 train_time:2295ms step_avg:57.37ms
step:41/2330 train_time:2351ms step_avg:57.33ms
step:42/2330 train_time:2410ms step_avg:57.39ms
step:43/2330 train_time:2466ms step_avg:57.35ms
step:44/2330 train_time:2526ms step_avg:57.40ms
step:45/2330 train_time:2581ms step_avg:57.36ms
step:46/2330 train_time:2640ms step_avg:57.38ms
step:47/2330 train_time:2695ms step_avg:57.34ms
step:48/2330 train_time:2754ms step_avg:57.38ms
step:49/2330 train_time:2809ms step_avg:57.33ms
step:50/2330 train_time:2869ms step_avg:57.37ms
step:51/2330 train_time:2925ms step_avg:57.34ms
step:52/2330 train_time:2983ms step_avg:57.36ms
step:53/2330 train_time:3039ms step_avg:57.34ms
step:54/2330 train_time:3099ms step_avg:57.39ms
step:55/2330 train_time:3156ms step_avg:57.38ms
step:56/2330 train_time:3215ms step_avg:57.41ms
step:57/2330 train_time:3271ms step_avg:57.39ms
step:58/2330 train_time:3331ms step_avg:57.44ms
step:59/2330 train_time:3388ms step_avg:57.42ms
step:60/2330 train_time:3449ms step_avg:57.48ms
step:61/2330 train_time:3505ms step_avg:57.45ms
step:62/2330 train_time:3564ms step_avg:57.48ms
step:63/2330 train_time:3619ms step_avg:57.45ms
step:64/2330 train_time:3679ms step_avg:57.48ms
step:65/2330 train_time:3734ms step_avg:57.45ms
step:66/2330 train_time:3793ms step_avg:57.47ms
step:67/2330 train_time:3848ms step_avg:57.44ms
step:68/2330 train_time:3909ms step_avg:57.48ms
step:69/2330 train_time:3965ms step_avg:57.46ms
step:70/2330 train_time:4024ms step_avg:57.49ms
step:71/2330 train_time:4081ms step_avg:57.48ms
step:72/2330 train_time:4139ms step_avg:57.49ms
step:73/2330 train_time:4195ms step_avg:57.47ms
step:74/2330 train_time:4256ms step_avg:57.51ms
step:75/2330 train_time:4312ms step_avg:57.49ms
step:76/2330 train_time:4373ms step_avg:57.54ms
step:77/2330 train_time:4428ms step_avg:57.51ms
step:78/2330 train_time:4489ms step_avg:57.55ms
step:79/2330 train_time:4545ms step_avg:57.53ms
step:80/2330 train_time:4605ms step_avg:57.56ms
step:81/2330 train_time:4661ms step_avg:57.54ms
step:82/2330 train_time:4719ms step_avg:57.55ms
step:83/2330 train_time:4775ms step_avg:57.54ms
step:84/2330 train_time:4834ms step_avg:57.55ms
step:85/2330 train_time:4889ms step_avg:57.52ms
step:86/2330 train_time:4949ms step_avg:57.55ms
step:87/2330 train_time:5006ms step_avg:57.53ms
step:88/2330 train_time:5065ms step_avg:57.55ms
step:89/2330 train_time:5121ms step_avg:57.54ms
step:90/2330 train_time:5180ms step_avg:57.56ms
step:91/2330 train_time:5237ms step_avg:57.54ms
step:92/2330 train_time:5295ms step_avg:57.55ms
step:93/2330 train_time:5351ms step_avg:57.54ms
step:94/2330 train_time:5411ms step_avg:57.57ms
step:95/2330 train_time:5467ms step_avg:57.55ms
step:96/2330 train_time:5526ms step_avg:57.57ms
step:97/2330 train_time:5582ms step_avg:57.55ms
step:98/2330 train_time:5642ms step_avg:57.57ms
step:99/2330 train_time:5698ms step_avg:57.56ms
step:100/2330 train_time:5756ms step_avg:57.56ms
step:101/2330 train_time:5812ms step_avg:57.55ms
step:102/2330 train_time:5871ms step_avg:57.56ms
step:103/2330 train_time:5926ms step_avg:57.54ms
step:104/2330 train_time:5986ms step_avg:57.56ms
step:105/2330 train_time:6042ms step_avg:57.54ms
step:106/2330 train_time:6101ms step_avg:57.56ms
step:107/2330 train_time:6157ms step_avg:57.55ms
step:108/2330 train_time:6217ms step_avg:57.56ms
step:109/2330 train_time:6273ms step_avg:57.55ms
step:110/2330 train_time:6332ms step_avg:57.57ms
step:111/2330 train_time:6389ms step_avg:57.55ms
step:112/2330 train_time:6449ms step_avg:57.58ms
step:113/2330 train_time:6505ms step_avg:57.56ms
step:114/2330 train_time:6564ms step_avg:57.58ms
step:115/2330 train_time:6620ms step_avg:57.57ms
step:116/2330 train_time:6679ms step_avg:57.57ms
step:117/2330 train_time:6735ms step_avg:57.56ms
step:118/2330 train_time:6793ms step_avg:57.57ms
step:119/2330 train_time:6849ms step_avg:57.56ms
step:120/2330 train_time:6908ms step_avg:57.57ms
step:121/2330 train_time:6965ms step_avg:57.56ms
step:122/2330 train_time:7024ms step_avg:57.57ms
step:123/2330 train_time:7080ms step_avg:57.56ms
step:124/2330 train_time:7139ms step_avg:57.57ms
step:125/2330 train_time:7195ms step_avg:57.56ms
step:126/2330 train_time:7254ms step_avg:57.57ms
step:127/2330 train_time:7310ms step_avg:57.56ms
step:128/2330 train_time:7371ms step_avg:57.58ms
step:129/2330 train_time:7426ms step_avg:57.57ms
step:130/2330 train_time:7487ms step_avg:57.59ms
step:131/2330 train_time:7542ms step_avg:57.57ms
step:132/2330 train_time:7602ms step_avg:57.59ms
step:133/2330 train_time:7658ms step_avg:57.58ms
step:134/2330 train_time:7717ms step_avg:57.59ms
step:135/2330 train_time:7772ms step_avg:57.57ms
step:136/2330 train_time:7831ms step_avg:57.58ms
step:137/2330 train_time:7887ms step_avg:57.57ms
step:138/2330 train_time:7946ms step_avg:57.58ms
step:139/2330 train_time:8002ms step_avg:57.57ms
step:140/2330 train_time:8062ms step_avg:57.58ms
step:141/2330 train_time:8118ms step_avg:57.57ms
step:142/2330 train_time:8177ms step_avg:57.58ms
step:143/2330 train_time:8233ms step_avg:57.57ms
step:144/2330 train_time:8292ms step_avg:57.58ms
step:145/2330 train_time:8347ms step_avg:57.57ms
step:146/2330 train_time:8408ms step_avg:57.59ms
step:147/2330 train_time:8464ms step_avg:57.58ms
step:148/2330 train_time:8523ms step_avg:57.59ms
step:149/2330 train_time:8579ms step_avg:57.58ms
step:150/2330 train_time:8638ms step_avg:57.58ms
step:151/2330 train_time:8693ms step_avg:57.57ms
step:152/2330 train_time:8753ms step_avg:57.58ms
step:153/2330 train_time:8809ms step_avg:57.57ms
step:154/2330 train_time:8868ms step_avg:57.59ms
step:155/2330 train_time:8924ms step_avg:57.58ms
step:156/2330 train_time:8983ms step_avg:57.58ms
step:157/2330 train_time:9039ms step_avg:57.57ms
step:158/2330 train_time:9098ms step_avg:57.58ms
step:159/2330 train_time:9154ms step_avg:57.57ms
step:160/2330 train_time:9213ms step_avg:57.58ms
step:161/2330 train_time:9269ms step_avg:57.57ms
step:162/2330 train_time:9329ms step_avg:57.59ms
step:163/2330 train_time:9385ms step_avg:57.58ms
step:164/2330 train_time:9444ms step_avg:57.59ms
step:165/2330 train_time:9500ms step_avg:57.57ms
step:166/2330 train_time:9559ms step_avg:57.59ms
step:167/2330 train_time:9616ms step_avg:57.58ms
step:168/2330 train_time:9674ms step_avg:57.58ms
step:169/2330 train_time:9729ms step_avg:57.57ms
step:170/2330 train_time:9789ms step_avg:57.58ms
step:171/2330 train_time:9845ms step_avg:57.57ms
step:172/2330 train_time:9905ms step_avg:57.59ms
step:173/2330 train_time:9962ms step_avg:57.58ms
step:174/2330 train_time:10021ms step_avg:57.59ms
step:175/2330 train_time:10078ms step_avg:57.59ms
step:176/2330 train_time:10136ms step_avg:57.59ms
step:177/2330 train_time:10193ms step_avg:57.59ms
step:178/2330 train_time:10251ms step_avg:57.59ms
step:179/2330 train_time:10307ms step_avg:57.58ms
step:180/2330 train_time:10366ms step_avg:57.59ms
step:181/2330 train_time:10422ms step_avg:57.58ms
step:182/2330 train_time:10481ms step_avg:57.59ms
step:183/2330 train_time:10537ms step_avg:57.58ms
step:184/2330 train_time:10597ms step_avg:57.59ms
step:185/2330 train_time:10653ms step_avg:57.58ms
step:186/2330 train_time:10713ms step_avg:57.60ms
step:187/2330 train_time:10768ms step_avg:57.58ms
step:188/2330 train_time:10828ms step_avg:57.59ms
step:189/2330 train_time:10883ms step_avg:57.58ms
step:190/2330 train_time:10943ms step_avg:57.59ms
step:191/2330 train_time:10999ms step_avg:57.59ms
step:192/2330 train_time:11059ms step_avg:57.60ms
step:193/2330 train_time:11115ms step_avg:57.59ms
step:194/2330 train_time:11174ms step_avg:57.60ms
step:195/2330 train_time:11229ms step_avg:57.59ms
step:196/2330 train_time:11289ms step_avg:57.60ms
step:197/2330 train_time:11345ms step_avg:57.59ms
step:198/2330 train_time:11404ms step_avg:57.60ms
step:199/2330 train_time:11460ms step_avg:57.59ms
step:200/2330 train_time:11519ms step_avg:57.59ms
step:201/2330 train_time:11574ms step_avg:57.58ms
step:202/2330 train_time:11633ms step_avg:57.59ms
step:203/2330 train_time:11689ms step_avg:57.58ms
step:204/2330 train_time:11749ms step_avg:57.59ms
step:205/2330 train_time:11805ms step_avg:57.59ms
step:206/2330 train_time:11864ms step_avg:57.59ms
step:207/2330 train_time:11920ms step_avg:57.58ms
step:208/2330 train_time:11979ms step_avg:57.59ms
step:209/2330 train_time:12035ms step_avg:57.58ms
step:210/2330 train_time:12094ms step_avg:57.59ms
step:211/2330 train_time:12150ms step_avg:57.58ms
step:212/2330 train_time:12209ms step_avg:57.59ms
step:213/2330 train_time:12266ms step_avg:57.59ms
step:214/2330 train_time:12324ms step_avg:57.59ms
step:215/2330 train_time:12380ms step_avg:57.58ms
step:216/2330 train_time:12439ms step_avg:57.59ms
step:217/2330 train_time:12495ms step_avg:57.58ms
step:218/2330 train_time:12554ms step_avg:57.59ms
step:219/2330 train_time:12610ms step_avg:57.58ms
step:220/2330 train_time:12670ms step_avg:57.59ms
step:221/2330 train_time:12725ms step_avg:57.58ms
step:222/2330 train_time:12785ms step_avg:57.59ms
step:223/2330 train_time:12841ms step_avg:57.58ms
step:224/2330 train_time:12901ms step_avg:57.59ms
step:225/2330 train_time:12957ms step_avg:57.59ms
step:226/2330 train_time:13016ms step_avg:57.59ms
step:227/2330 train_time:13072ms step_avg:57.58ms
step:228/2330 train_time:13131ms step_avg:57.59ms
step:229/2330 train_time:13187ms step_avg:57.59ms
step:230/2330 train_time:13247ms step_avg:57.60ms
step:231/2330 train_time:13303ms step_avg:57.59ms
step:232/2330 train_time:13362ms step_avg:57.59ms
step:233/2330 train_time:13419ms step_avg:57.59ms
step:234/2330 train_time:13477ms step_avg:57.59ms
step:235/2330 train_time:13533ms step_avg:57.59ms
step:236/2330 train_time:13592ms step_avg:57.60ms
step:237/2330 train_time:13648ms step_avg:57.59ms
step:238/2330 train_time:13708ms step_avg:57.60ms
step:239/2330 train_time:13764ms step_avg:57.59ms
step:240/2330 train_time:13823ms step_avg:57.59ms
step:241/2330 train_time:13879ms step_avg:57.59ms
step:242/2330 train_time:13937ms step_avg:57.59ms
step:243/2330 train_time:13994ms step_avg:57.59ms
step:244/2330 train_time:14053ms step_avg:57.59ms
step:245/2330 train_time:14108ms step_avg:57.58ms
step:246/2330 train_time:14169ms step_avg:57.60ms
step:247/2330 train_time:14224ms step_avg:57.59ms
step:248/2330 train_time:14284ms step_avg:57.60ms
step:249/2330 train_time:14339ms step_avg:57.59ms
step:250/2330 train_time:14398ms step_avg:57.59ms
step:250/2330 val_loss:4.9842 train_time:14477ms step_avg:57.91ms
step:251/2330 train_time:14496ms step_avg:57.75ms
step:252/2330 train_time:14515ms step_avg:57.60ms
step:253/2330 train_time:14573ms step_avg:57.60ms
step:254/2330 train_time:14639ms step_avg:57.64ms
step:255/2330 train_time:14696ms step_avg:57.63ms
step:256/2330 train_time:14757ms step_avg:57.64ms
step:257/2330 train_time:14812ms step_avg:57.63ms
step:258/2330 train_time:14872ms step_avg:57.64ms
step:259/2330 train_time:14927ms step_avg:57.63ms
step:260/2330 train_time:14987ms step_avg:57.64ms
step:261/2330 train_time:15042ms step_avg:57.63ms
step:262/2330 train_time:15101ms step_avg:57.64ms
step:263/2330 train_time:15156ms step_avg:57.63ms
step:264/2330 train_time:15214ms step_avg:57.63ms
step:265/2330 train_time:15269ms step_avg:57.62ms
step:266/2330 train_time:15328ms step_avg:57.62ms
step:267/2330 train_time:15383ms step_avg:57.62ms
step:268/2330 train_time:15442ms step_avg:57.62ms
step:269/2330 train_time:15499ms step_avg:57.62ms
step:270/2330 train_time:15559ms step_avg:57.63ms
step:271/2330 train_time:15617ms step_avg:57.63ms
step:272/2330 train_time:15677ms step_avg:57.63ms
step:273/2330 train_time:15733ms step_avg:57.63ms
step:274/2330 train_time:15794ms step_avg:57.64ms
step:275/2330 train_time:15850ms step_avg:57.64ms
step:276/2330 train_time:15909ms step_avg:57.64ms
step:277/2330 train_time:15965ms step_avg:57.64ms
step:278/2330 train_time:16025ms step_avg:57.64ms
step:279/2330 train_time:16080ms step_avg:57.63ms
step:280/2330 train_time:16138ms step_avg:57.64ms
step:281/2330 train_time:16194ms step_avg:57.63ms
step:282/2330 train_time:16252ms step_avg:57.63ms
step:283/2330 train_time:16307ms step_avg:57.62ms
step:284/2330 train_time:16366ms step_avg:57.63ms
step:285/2330 train_time:16422ms step_avg:57.62ms
step:286/2330 train_time:16481ms step_avg:57.63ms
step:287/2330 train_time:16539ms step_avg:57.63ms
step:288/2330 train_time:16598ms step_avg:57.63ms
step:289/2330 train_time:16655ms step_avg:57.63ms
step:290/2330 train_time:16715ms step_avg:57.64ms
step:291/2330 train_time:16772ms step_avg:57.63ms
step:292/2330 train_time:16833ms step_avg:57.65ms
step:293/2330 train_time:16889ms step_avg:57.64ms
step:294/2330 train_time:16949ms step_avg:57.65ms
step:295/2330 train_time:17004ms step_avg:57.64ms
step:296/2330 train_time:17066ms step_avg:57.65ms
step:297/2330 train_time:17121ms step_avg:57.65ms
step:298/2330 train_time:17180ms step_avg:57.65ms
step:299/2330 train_time:17235ms step_avg:57.64ms
step:300/2330 train_time:17293ms step_avg:57.64ms
step:301/2330 train_time:17349ms step_avg:57.64ms
step:302/2330 train_time:17408ms step_avg:57.64ms
step:303/2330 train_time:17464ms step_avg:57.64ms
step:304/2330 train_time:17523ms step_avg:57.64ms
step:305/2330 train_time:17580ms step_avg:57.64ms
step:306/2330 train_time:17639ms step_avg:57.64ms
step:307/2330 train_time:17696ms step_avg:57.64ms
step:308/2330 train_time:17756ms step_avg:57.65ms
step:309/2330 train_time:17812ms step_avg:57.64ms
step:310/2330 train_time:17872ms step_avg:57.65ms
step:311/2330 train_time:17927ms step_avg:57.64ms
step:312/2330 train_time:17988ms step_avg:57.65ms
step:313/2330 train_time:18044ms step_avg:57.65ms
step:314/2330 train_time:18103ms step_avg:57.65ms
step:315/2330 train_time:18160ms step_avg:57.65ms
step:316/2330 train_time:18218ms step_avg:57.65ms
step:317/2330 train_time:18274ms step_avg:57.65ms
step:318/2330 train_time:18334ms step_avg:57.65ms
step:319/2330 train_time:18389ms step_avg:57.65ms
step:320/2330 train_time:18449ms step_avg:57.65ms
step:321/2330 train_time:18504ms step_avg:57.65ms
step:322/2330 train_time:18565ms step_avg:57.66ms
step:323/2330 train_time:18621ms step_avg:57.65ms
step:324/2330 train_time:18680ms step_avg:57.66ms
step:325/2330 train_time:18737ms step_avg:57.65ms
step:326/2330 train_time:18796ms step_avg:57.66ms
step:327/2330 train_time:18852ms step_avg:57.65ms
step:328/2330 train_time:18912ms step_avg:57.66ms
step:329/2330 train_time:18968ms step_avg:57.65ms
step:330/2330 train_time:19029ms step_avg:57.66ms
step:331/2330 train_time:19085ms step_avg:57.66ms
step:332/2330 train_time:19144ms step_avg:57.66ms
step:333/2330 train_time:19200ms step_avg:57.66ms
step:334/2330 train_time:19259ms step_avg:57.66ms
step:335/2330 train_time:19315ms step_avg:57.66ms
step:336/2330 train_time:19375ms step_avg:57.66ms
step:337/2330 train_time:19431ms step_avg:57.66ms
step:338/2330 train_time:19489ms step_avg:57.66ms
step:339/2330 train_time:19545ms step_avg:57.66ms
step:340/2330 train_time:19605ms step_avg:57.66ms
step:341/2330 train_time:19662ms step_avg:57.66ms
step:342/2330 train_time:19720ms step_avg:57.66ms
step:343/2330 train_time:19777ms step_avg:57.66ms
step:344/2330 train_time:19835ms step_avg:57.66ms
step:345/2330 train_time:19891ms step_avg:57.66ms
step:346/2330 train_time:19951ms step_avg:57.66ms
step:347/2330 train_time:20006ms step_avg:57.65ms
step:348/2330 train_time:20067ms step_avg:57.67ms
step:349/2330 train_time:20123ms step_avg:57.66ms
step:350/2330 train_time:20183ms step_avg:57.67ms
step:351/2330 train_time:20239ms step_avg:57.66ms
step:352/2330 train_time:20298ms step_avg:57.67ms
step:353/2330 train_time:20354ms step_avg:57.66ms
step:354/2330 train_time:20413ms step_avg:57.66ms
step:355/2330 train_time:20468ms step_avg:57.66ms
step:356/2330 train_time:20529ms step_avg:57.67ms
step:357/2330 train_time:20585ms step_avg:57.66ms
step:358/2330 train_time:20645ms step_avg:57.67ms
step:359/2330 train_time:20701ms step_avg:57.66ms
step:360/2330 train_time:20760ms step_avg:57.67ms
step:361/2330 train_time:20816ms step_avg:57.66ms
step:362/2330 train_time:20876ms step_avg:57.67ms
step:363/2330 train_time:20931ms step_avg:57.66ms
step:364/2330 train_time:20991ms step_avg:57.67ms
step:365/2330 train_time:21047ms step_avg:57.66ms
step:366/2330 train_time:21108ms step_avg:57.67ms
step:367/2330 train_time:21164ms step_avg:57.67ms
step:368/2330 train_time:21223ms step_avg:57.67ms
step:369/2330 train_time:21278ms step_avg:57.66ms
step:370/2330 train_time:21338ms step_avg:57.67ms
step:371/2330 train_time:21393ms step_avg:57.66ms
step:372/2330 train_time:21452ms step_avg:57.67ms
step:373/2330 train_time:21507ms step_avg:57.66ms
step:374/2330 train_time:21569ms step_avg:57.67ms
step:375/2330 train_time:21624ms step_avg:57.66ms
step:376/2330 train_time:21685ms step_avg:57.67ms
step:377/2330 train_time:21740ms step_avg:57.67ms
step:378/2330 train_time:21799ms step_avg:57.67ms
step:379/2330 train_time:21855ms step_avg:57.66ms
step:380/2330 train_time:21914ms step_avg:57.67ms
step:381/2330 train_time:21969ms step_avg:57.66ms
step:382/2330 train_time:22030ms step_avg:57.67ms
step:383/2330 train_time:22086ms step_avg:57.67ms
step:384/2330 train_time:22146ms step_avg:57.67ms
step:385/2330 train_time:22202ms step_avg:57.67ms
step:386/2330 train_time:22262ms step_avg:57.67ms
step:387/2330 train_time:22318ms step_avg:57.67ms
step:388/2330 train_time:22377ms step_avg:57.67ms
step:389/2330 train_time:22432ms step_avg:57.67ms
step:390/2330 train_time:22492ms step_avg:57.67ms
step:391/2330 train_time:22547ms step_avg:57.67ms
step:392/2330 train_time:22607ms step_avg:57.67ms
step:393/2330 train_time:22664ms step_avg:57.67ms
step:394/2330 train_time:22723ms step_avg:57.67ms
step:395/2330 train_time:22779ms step_avg:57.67ms
step:396/2330 train_time:22838ms step_avg:57.67ms
step:397/2330 train_time:22894ms step_avg:57.67ms
step:398/2330 train_time:22953ms step_avg:57.67ms
step:399/2330 train_time:23009ms step_avg:57.67ms
step:400/2330 train_time:23068ms step_avg:57.67ms
step:401/2330 train_time:23125ms step_avg:57.67ms
step:402/2330 train_time:23184ms step_avg:57.67ms
step:403/2330 train_time:23240ms step_avg:57.67ms
step:404/2330 train_time:23300ms step_avg:57.67ms
step:405/2330 train_time:23356ms step_avg:57.67ms
step:406/2330 train_time:23415ms step_avg:57.67ms
step:407/2330 train_time:23471ms step_avg:57.67ms
step:408/2330 train_time:23531ms step_avg:57.67ms
step:409/2330 train_time:23586ms step_avg:57.67ms
step:410/2330 train_time:23646ms step_avg:57.67ms
step:411/2330 train_time:23702ms step_avg:57.67ms
step:412/2330 train_time:23762ms step_avg:57.67ms
step:413/2330 train_time:23818ms step_avg:57.67ms
step:414/2330 train_time:23876ms step_avg:57.67ms
step:415/2330 train_time:23932ms step_avg:57.67ms
step:416/2330 train_time:23992ms step_avg:57.67ms
step:417/2330 train_time:24047ms step_avg:57.67ms
step:418/2330 train_time:24107ms step_avg:57.67ms
step:419/2330 train_time:24163ms step_avg:57.67ms
step:420/2330 train_time:24222ms step_avg:57.67ms
step:421/2330 train_time:24278ms step_avg:57.67ms
step:422/2330 train_time:24337ms step_avg:57.67ms
step:423/2330 train_time:24393ms step_avg:57.67ms
step:424/2330 train_time:24452ms step_avg:57.67ms
step:425/2330 train_time:24508ms step_avg:57.66ms
step:426/2330 train_time:24569ms step_avg:57.67ms
step:427/2330 train_time:24625ms step_avg:57.67ms
step:428/2330 train_time:24684ms step_avg:57.67ms
step:429/2330 train_time:24741ms step_avg:57.67ms
step:430/2330 train_time:24801ms step_avg:57.68ms
step:431/2330 train_time:24857ms step_avg:57.67ms
step:432/2330 train_time:24916ms step_avg:57.68ms
step:433/2330 train_time:24971ms step_avg:57.67ms
step:434/2330 train_time:25031ms step_avg:57.67ms
step:435/2330 train_time:25087ms step_avg:57.67ms
step:436/2330 train_time:25146ms step_avg:57.67ms
step:437/2330 train_time:25202ms step_avg:57.67ms
step:438/2330 train_time:25261ms step_avg:57.67ms
step:439/2330 train_time:25318ms step_avg:57.67ms
step:440/2330 train_time:25376ms step_avg:57.67ms
step:441/2330 train_time:25431ms step_avg:57.67ms
step:442/2330 train_time:25492ms step_avg:57.68ms
step:443/2330 train_time:25548ms step_avg:57.67ms
step:444/2330 train_time:25608ms step_avg:57.68ms
step:445/2330 train_time:25664ms step_avg:57.67ms
step:446/2330 train_time:25725ms step_avg:57.68ms
step:447/2330 train_time:25781ms step_avg:57.68ms
step:448/2330 train_time:25841ms step_avg:57.68ms
step:449/2330 train_time:25897ms step_avg:57.68ms
step:450/2330 train_time:25956ms step_avg:57.68ms
step:451/2330 train_time:26011ms step_avg:57.67ms
step:452/2330 train_time:26071ms step_avg:57.68ms
step:453/2330 train_time:26127ms step_avg:57.67ms
step:454/2330 train_time:26187ms step_avg:57.68ms
step:455/2330 train_time:26243ms step_avg:57.68ms
step:456/2330 train_time:26302ms step_avg:57.68ms
step:457/2330 train_time:26358ms step_avg:57.68ms
step:458/2330 train_time:26417ms step_avg:57.68ms
step:459/2330 train_time:26473ms step_avg:57.68ms
step:460/2330 train_time:26532ms step_avg:57.68ms
step:461/2330 train_time:26588ms step_avg:57.67ms
step:462/2330 train_time:26649ms step_avg:57.68ms
step:463/2330 train_time:26704ms step_avg:57.68ms
step:464/2330 train_time:26765ms step_avg:57.68ms
step:465/2330 train_time:26821ms step_avg:57.68ms
step:466/2330 train_time:26880ms step_avg:57.68ms
step:467/2330 train_time:26936ms step_avg:57.68ms
step:468/2330 train_time:26995ms step_avg:57.68ms
step:469/2330 train_time:27050ms step_avg:57.68ms
step:470/2330 train_time:27111ms step_avg:57.68ms
step:471/2330 train_time:27166ms step_avg:57.68ms
step:472/2330 train_time:27227ms step_avg:57.68ms
step:473/2330 train_time:27283ms step_avg:57.68ms
step:474/2330 train_time:27341ms step_avg:57.68ms
step:475/2330 train_time:27398ms step_avg:57.68ms
step:476/2330 train_time:27457ms step_avg:57.68ms
step:477/2330 train_time:27513ms step_avg:57.68ms
step:478/2330 train_time:27572ms step_avg:57.68ms
step:479/2330 train_time:27628ms step_avg:57.68ms
step:480/2330 train_time:27688ms step_avg:57.68ms
step:481/2330 train_time:27744ms step_avg:57.68ms
step:482/2330 train_time:27804ms step_avg:57.68ms
step:483/2330 train_time:27860ms step_avg:57.68ms
step:484/2330 train_time:27919ms step_avg:57.68ms
step:485/2330 train_time:27975ms step_avg:57.68ms
step:486/2330 train_time:28033ms step_avg:57.68ms
step:487/2330 train_time:28089ms step_avg:57.68ms
step:488/2330 train_time:28149ms step_avg:57.68ms
step:489/2330 train_time:28205ms step_avg:57.68ms
step:490/2330 train_time:28265ms step_avg:57.68ms
step:491/2330 train_time:28321ms step_avg:57.68ms
step:492/2330 train_time:28379ms step_avg:57.68ms
step:493/2330 train_time:28436ms step_avg:57.68ms
step:494/2330 train_time:28495ms step_avg:57.68ms
step:495/2330 train_time:28550ms step_avg:57.68ms
step:496/2330 train_time:28610ms step_avg:57.68ms
step:497/2330 train_time:28666ms step_avg:57.68ms
step:498/2330 train_time:28725ms step_avg:57.68ms
step:499/2330 train_time:28782ms step_avg:57.68ms
step:500/2330 train_time:28841ms step_avg:57.68ms
step:500/2330 val_loss:4.5029 train_time:28919ms step_avg:57.84ms
step:501/2330 train_time:28937ms step_avg:57.76ms
step:502/2330 train_time:28957ms step_avg:57.68ms
step:503/2330 train_time:29014ms step_avg:57.68ms
step:504/2330 train_time:29077ms step_avg:57.69ms
step:505/2330 train_time:29134ms step_avg:57.69ms
step:506/2330 train_time:29196ms step_avg:57.70ms
step:507/2330 train_time:29251ms step_avg:57.69ms
step:508/2330 train_time:29311ms step_avg:57.70ms
step:509/2330 train_time:29366ms step_avg:57.69ms
step:510/2330 train_time:29425ms step_avg:57.70ms
step:511/2330 train_time:29480ms step_avg:57.69ms
step:512/2330 train_time:29538ms step_avg:57.69ms
step:513/2330 train_time:29594ms step_avg:57.69ms
step:514/2330 train_time:29652ms step_avg:57.69ms
step:515/2330 train_time:29708ms step_avg:57.68ms
step:516/2330 train_time:29768ms step_avg:57.69ms
step:517/2330 train_time:29823ms step_avg:57.69ms
step:518/2330 train_time:29883ms step_avg:57.69ms
step:519/2330 train_time:29940ms step_avg:57.69ms
step:520/2330 train_time:30002ms step_avg:57.70ms
step:521/2330 train_time:30059ms step_avg:57.69ms
step:522/2330 train_time:30119ms step_avg:57.70ms
step:523/2330 train_time:30176ms step_avg:57.70ms
step:524/2330 train_time:30235ms step_avg:57.70ms
step:525/2330 train_time:30292ms step_avg:57.70ms
step:526/2330 train_time:30350ms step_avg:57.70ms
step:527/2330 train_time:30406ms step_avg:57.70ms
step:528/2330 train_time:30465ms step_avg:57.70ms
step:529/2330 train_time:30520ms step_avg:57.69ms
step:530/2330 train_time:30579ms step_avg:57.70ms
step:531/2330 train_time:30635ms step_avg:57.69ms
step:532/2330 train_time:30693ms step_avg:57.69ms
step:533/2330 train_time:30749ms step_avg:57.69ms
step:534/2330 train_time:30808ms step_avg:57.69ms
step:535/2330 train_time:30864ms step_avg:57.69ms
step:536/2330 train_time:30925ms step_avg:57.70ms
step:537/2330 train_time:30982ms step_avg:57.69ms
step:538/2330 train_time:31042ms step_avg:57.70ms
step:539/2330 train_time:31099ms step_avg:57.70ms
step:540/2330 train_time:31160ms step_avg:57.70ms
step:541/2330 train_time:31218ms step_avg:57.70ms
step:542/2330 train_time:31277ms step_avg:57.71ms
step:543/2330 train_time:31333ms step_avg:57.70ms
step:544/2330 train_time:31391ms step_avg:57.70ms
step:545/2330 train_time:31446ms step_avg:57.70ms
step:546/2330 train_time:31506ms step_avg:57.70ms
step:547/2330 train_time:31561ms step_avg:57.70ms
step:548/2330 train_time:31620ms step_avg:57.70ms
step:549/2330 train_time:31677ms step_avg:57.70ms
step:550/2330 train_time:31735ms step_avg:57.70ms
step:551/2330 train_time:31790ms step_avg:57.70ms
step:552/2330 train_time:31850ms step_avg:57.70ms
step:553/2330 train_time:31906ms step_avg:57.70ms
step:554/2330 train_time:31967ms step_avg:57.70ms
step:555/2330 train_time:32023ms step_avg:57.70ms
step:556/2330 train_time:32085ms step_avg:57.71ms
step:557/2330 train_time:32140ms step_avg:57.70ms
step:558/2330 train_time:32201ms step_avg:57.71ms
step:559/2330 train_time:32257ms step_avg:57.71ms
step:560/2330 train_time:32318ms step_avg:57.71ms
step:561/2330 train_time:32374ms step_avg:57.71ms
step:562/2330 train_time:32432ms step_avg:57.71ms
step:563/2330 train_time:32488ms step_avg:57.71ms
step:564/2330 train_time:32547ms step_avg:57.71ms
step:565/2330 train_time:32603ms step_avg:57.70ms
step:566/2330 train_time:32663ms step_avg:57.71ms
step:567/2330 train_time:32719ms step_avg:57.71ms
step:568/2330 train_time:32778ms step_avg:57.71ms
step:569/2330 train_time:32834ms step_avg:57.70ms
step:570/2330 train_time:32892ms step_avg:57.71ms
step:571/2330 train_time:32948ms step_avg:57.70ms
step:572/2330 train_time:33009ms step_avg:57.71ms
step:573/2330 train_time:33065ms step_avg:57.70ms
step:574/2330 train_time:33125ms step_avg:57.71ms
step:575/2330 train_time:33180ms step_avg:57.70ms
step:576/2330 train_time:33242ms step_avg:57.71ms
step:577/2330 train_time:33299ms step_avg:57.71ms
step:578/2330 train_time:33358ms step_avg:57.71ms
step:579/2330 train_time:33415ms step_avg:57.71ms
step:580/2330 train_time:33473ms step_avg:57.71ms
step:581/2330 train_time:33529ms step_avg:57.71ms
step:582/2330 train_time:33588ms step_avg:57.71ms
step:583/2330 train_time:33643ms step_avg:57.71ms
step:584/2330 train_time:33703ms step_avg:57.71ms
step:585/2330 train_time:33758ms step_avg:57.71ms
step:586/2330 train_time:33817ms step_avg:57.71ms
step:587/2330 train_time:33873ms step_avg:57.70ms
step:588/2330 train_time:33932ms step_avg:57.71ms
step:589/2330 train_time:33988ms step_avg:57.70ms
step:590/2330 train_time:34047ms step_avg:57.71ms
step:591/2330 train_time:34103ms step_avg:57.70ms
step:592/2330 train_time:34165ms step_avg:57.71ms
step:593/2330 train_time:34221ms step_avg:57.71ms
step:594/2330 train_time:34281ms step_avg:57.71ms
step:595/2330 train_time:34337ms step_avg:57.71ms
step:596/2330 train_time:34398ms step_avg:57.71ms
step:597/2330 train_time:34454ms step_avg:57.71ms
step:598/2330 train_time:34512ms step_avg:57.71ms
step:599/2330 train_time:34568ms step_avg:57.71ms
step:600/2330 train_time:34628ms step_avg:57.71ms
step:601/2330 train_time:34683ms step_avg:57.71ms
step:602/2330 train_time:34743ms step_avg:57.71ms
step:603/2330 train_time:34800ms step_avg:57.71ms
step:604/2330 train_time:34858ms step_avg:57.71ms
step:605/2330 train_time:34915ms step_avg:57.71ms
step:606/2330 train_time:34973ms step_avg:57.71ms
step:607/2330 train_time:35030ms step_avg:57.71ms
step:608/2330 train_time:35089ms step_avg:57.71ms
step:609/2330 train_time:35144ms step_avg:57.71ms
step:610/2330 train_time:35206ms step_avg:57.71ms
step:611/2330 train_time:35261ms step_avg:57.71ms
step:612/2330 train_time:35322ms step_avg:57.72ms
step:613/2330 train_time:35378ms step_avg:57.71ms
step:614/2330 train_time:35437ms step_avg:57.72ms
step:615/2330 train_time:35493ms step_avg:57.71ms
step:616/2330 train_time:35552ms step_avg:57.71ms
step:617/2330 train_time:35608ms step_avg:57.71ms
step:618/2330 train_time:35667ms step_avg:57.71ms
step:619/2330 train_time:35722ms step_avg:57.71ms
step:620/2330 train_time:35782ms step_avg:57.71ms
step:621/2330 train_time:35838ms step_avg:57.71ms
step:622/2330 train_time:35897ms step_avg:57.71ms
step:623/2330 train_time:35954ms step_avg:57.71ms
step:624/2330 train_time:36012ms step_avg:57.71ms
step:625/2330 train_time:36068ms step_avg:57.71ms
step:626/2330 train_time:36128ms step_avg:57.71ms
step:627/2330 train_time:36183ms step_avg:57.71ms
step:628/2330 train_time:36244ms step_avg:57.71ms
step:629/2330 train_time:36300ms step_avg:57.71ms
step:630/2330 train_time:36360ms step_avg:57.71ms
step:631/2330 train_time:36416ms step_avg:57.71ms
step:632/2330 train_time:36475ms step_avg:57.71ms
step:633/2330 train_time:36531ms step_avg:57.71ms
step:634/2330 train_time:36591ms step_avg:57.71ms
step:635/2330 train_time:36646ms step_avg:57.71ms
step:636/2330 train_time:36706ms step_avg:57.71ms
step:637/2330 train_time:36761ms step_avg:57.71ms
step:638/2330 train_time:36822ms step_avg:57.71ms
step:639/2330 train_time:36878ms step_avg:57.71ms
step:640/2330 train_time:36937ms step_avg:57.71ms
step:641/2330 train_time:36993ms step_avg:57.71ms
step:642/2330 train_time:37052ms step_avg:57.71ms
step:643/2330 train_time:37107ms step_avg:57.71ms
step:644/2330 train_time:37167ms step_avg:57.71ms
step:645/2330 train_time:37222ms step_avg:57.71ms
step:646/2330 train_time:37282ms step_avg:57.71ms
step:647/2330 train_time:37338ms step_avg:57.71ms
step:648/2330 train_time:37397ms step_avg:57.71ms
step:649/2330 train_time:37454ms step_avg:57.71ms
step:650/2330 train_time:37512ms step_avg:57.71ms
step:651/2330 train_time:37568ms step_avg:57.71ms
step:652/2330 train_time:37628ms step_avg:57.71ms
step:653/2330 train_time:37683ms step_avg:57.71ms
step:654/2330 train_time:37743ms step_avg:57.71ms
step:655/2330 train_time:37798ms step_avg:57.71ms
step:656/2330 train_time:37858ms step_avg:57.71ms
step:657/2330 train_time:37914ms step_avg:57.71ms
step:658/2330 train_time:37974ms step_avg:57.71ms
step:659/2330 train_time:38030ms step_avg:57.71ms
step:660/2330 train_time:38090ms step_avg:57.71ms
step:661/2330 train_time:38145ms step_avg:57.71ms
step:662/2330 train_time:38205ms step_avg:57.71ms
step:663/2330 train_time:38261ms step_avg:57.71ms
step:664/2330 train_time:38321ms step_avg:57.71ms
step:665/2330 train_time:38377ms step_avg:57.71ms
step:666/2330 train_time:38437ms step_avg:57.71ms
step:667/2330 train_time:38493ms step_avg:57.71ms
step:668/2330 train_time:38551ms step_avg:57.71ms
step:669/2330 train_time:38607ms step_avg:57.71ms
step:670/2330 train_time:38666ms step_avg:57.71ms
step:671/2330 train_time:38721ms step_avg:57.71ms
step:672/2330 train_time:38782ms step_avg:57.71ms
step:673/2330 train_time:38838ms step_avg:57.71ms
step:674/2330 train_time:38898ms step_avg:57.71ms
step:675/2330 train_time:38953ms step_avg:57.71ms
step:676/2330 train_time:39013ms step_avg:57.71ms
step:677/2330 train_time:39069ms step_avg:57.71ms
step:678/2330 train_time:39129ms step_avg:57.71ms
step:679/2330 train_time:39184ms step_avg:57.71ms
step:680/2330 train_time:39245ms step_avg:57.71ms
step:681/2330 train_time:39301ms step_avg:57.71ms
step:682/2330 train_time:39361ms step_avg:57.71ms
step:683/2330 train_time:39417ms step_avg:57.71ms
step:684/2330 train_time:39476ms step_avg:57.71ms
step:685/2330 train_time:39533ms step_avg:57.71ms
step:686/2330 train_time:39591ms step_avg:57.71ms
step:687/2330 train_time:39647ms step_avg:57.71ms
step:688/2330 train_time:39707ms step_avg:57.71ms
step:689/2330 train_time:39764ms step_avg:57.71ms
step:690/2330 train_time:39823ms step_avg:57.71ms
step:691/2330 train_time:39880ms step_avg:57.71ms
step:692/2330 train_time:39939ms step_avg:57.72ms
step:693/2330 train_time:39996ms step_avg:57.71ms
step:694/2330 train_time:40054ms step_avg:57.72ms
step:695/2330 train_time:40110ms step_avg:57.71ms
step:696/2330 train_time:40170ms step_avg:57.72ms
step:697/2330 train_time:40225ms step_avg:57.71ms
step:698/2330 train_time:40286ms step_avg:57.72ms
step:699/2330 train_time:40342ms step_avg:57.71ms
step:700/2330 train_time:40401ms step_avg:57.72ms
step:701/2330 train_time:40458ms step_avg:57.71ms
step:702/2330 train_time:40517ms step_avg:57.72ms
step:703/2330 train_time:40573ms step_avg:57.71ms
step:704/2330 train_time:40632ms step_avg:57.72ms
step:705/2330 train_time:40687ms step_avg:57.71ms
step:706/2330 train_time:40747ms step_avg:57.72ms
step:707/2330 train_time:40803ms step_avg:57.71ms
step:708/2330 train_time:40863ms step_avg:57.72ms
step:709/2330 train_time:40919ms step_avg:57.71ms
step:710/2330 train_time:40979ms step_avg:57.72ms
step:711/2330 train_time:41036ms step_avg:57.72ms
step:712/2330 train_time:41094ms step_avg:57.72ms
step:713/2330 train_time:41150ms step_avg:57.71ms
step:714/2330 train_time:41209ms step_avg:57.72ms
step:715/2330 train_time:41265ms step_avg:57.71ms
step:716/2330 train_time:41325ms step_avg:57.72ms
step:717/2330 train_time:41381ms step_avg:57.71ms
step:718/2330 train_time:41441ms step_avg:57.72ms
step:719/2330 train_time:41497ms step_avg:57.71ms
step:720/2330 train_time:41556ms step_avg:57.72ms
step:721/2330 train_time:41611ms step_avg:57.71ms
step:722/2330 train_time:41671ms step_avg:57.72ms
step:723/2330 train_time:41727ms step_avg:57.71ms
step:724/2330 train_time:41787ms step_avg:57.72ms
step:725/2330 train_time:41842ms step_avg:57.71ms
step:726/2330 train_time:41903ms step_avg:57.72ms
step:727/2330 train_time:41959ms step_avg:57.71ms
step:728/2330 train_time:42018ms step_avg:57.72ms
step:729/2330 train_time:42074ms step_avg:57.71ms
step:730/2330 train_time:42133ms step_avg:57.72ms
step:731/2330 train_time:42189ms step_avg:57.71ms
step:732/2330 train_time:42249ms step_avg:57.72ms
step:733/2330 train_time:42305ms step_avg:57.71ms
step:734/2330 train_time:42365ms step_avg:57.72ms
step:735/2330 train_time:42421ms step_avg:57.72ms
step:736/2330 train_time:42481ms step_avg:57.72ms
step:737/2330 train_time:42537ms step_avg:57.72ms
step:738/2330 train_time:42597ms step_avg:57.72ms
step:739/2330 train_time:42653ms step_avg:57.72ms
step:740/2330 train_time:42711ms step_avg:57.72ms
step:741/2330 train_time:42768ms step_avg:57.72ms
step:742/2330 train_time:42828ms step_avg:57.72ms
step:743/2330 train_time:42883ms step_avg:57.72ms
step:744/2330 train_time:42944ms step_avg:57.72ms
step:745/2330 train_time:43000ms step_avg:57.72ms
step:746/2330 train_time:43060ms step_avg:57.72ms
step:747/2330 train_time:43117ms step_avg:57.72ms
step:748/2330 train_time:43176ms step_avg:57.72ms
step:749/2330 train_time:43233ms step_avg:57.72ms
step:750/2330 train_time:43292ms step_avg:57.72ms
step:750/2330 val_loss:4.2631 train_time:43371ms step_avg:57.83ms
step:751/2330 train_time:43390ms step_avg:57.78ms
step:752/2330 train_time:43410ms step_avg:57.73ms
step:753/2330 train_time:43468ms step_avg:57.73ms
step:754/2330 train_time:43531ms step_avg:57.73ms
step:755/2330 train_time:43588ms step_avg:57.73ms
step:756/2330 train_time:43647ms step_avg:57.73ms
step:757/2330 train_time:43703ms step_avg:57.73ms
step:758/2330 train_time:43761ms step_avg:57.73ms
step:759/2330 train_time:43817ms step_avg:57.73ms
step:760/2330 train_time:43876ms step_avg:57.73ms
step:761/2330 train_time:43931ms step_avg:57.73ms
step:762/2330 train_time:43991ms step_avg:57.73ms
step:763/2330 train_time:44047ms step_avg:57.73ms
step:764/2330 train_time:44105ms step_avg:57.73ms
step:765/2330 train_time:44161ms step_avg:57.73ms
step:766/2330 train_time:44219ms step_avg:57.73ms
step:767/2330 train_time:44275ms step_avg:57.72ms
step:768/2330 train_time:44335ms step_avg:57.73ms
step:769/2330 train_time:44392ms step_avg:57.73ms
step:770/2330 train_time:44456ms step_avg:57.73ms
step:771/2330 train_time:44513ms step_avg:57.73ms
step:772/2330 train_time:44575ms step_avg:57.74ms
step:773/2330 train_time:44632ms step_avg:57.74ms
step:774/2330 train_time:44694ms step_avg:57.74ms
step:775/2330 train_time:44750ms step_avg:57.74ms
step:776/2330 train_time:44810ms step_avg:57.74ms
step:777/2330 train_time:44866ms step_avg:57.74ms
step:778/2330 train_time:44925ms step_avg:57.74ms
step:779/2330 train_time:44982ms step_avg:57.74ms
step:780/2330 train_time:45041ms step_avg:57.75ms
step:781/2330 train_time:45097ms step_avg:57.74ms
step:782/2330 train_time:45157ms step_avg:57.75ms
step:783/2330 train_time:45213ms step_avg:57.74ms
step:784/2330 train_time:45273ms step_avg:57.75ms
step:785/2330 train_time:45330ms step_avg:57.74ms
step:786/2330 train_time:45391ms step_avg:57.75ms
step:787/2330 train_time:45449ms step_avg:57.75ms
step:788/2330 train_time:45509ms step_avg:57.75ms
step:789/2330 train_time:45566ms step_avg:57.75ms
step:790/2330 train_time:45627ms step_avg:57.76ms
step:791/2330 train_time:45684ms step_avg:57.76ms
step:792/2330 train_time:45744ms step_avg:57.76ms
step:793/2330 train_time:45800ms step_avg:57.76ms
step:794/2330 train_time:45859ms step_avg:57.76ms
step:795/2330 train_time:45916ms step_avg:57.76ms
step:796/2330 train_time:45975ms step_avg:57.76ms
step:797/2330 train_time:46032ms step_avg:57.76ms
step:798/2330 train_time:46093ms step_avg:57.76ms
step:799/2330 train_time:46148ms step_avg:57.76ms
step:800/2330 train_time:46209ms step_avg:57.76ms
step:801/2330 train_time:46265ms step_avg:57.76ms
step:802/2330 train_time:46326ms step_avg:57.76ms
step:803/2330 train_time:46383ms step_avg:57.76ms
step:804/2330 train_time:46443ms step_avg:57.76ms
step:805/2330 train_time:46500ms step_avg:57.76ms
step:806/2330 train_time:46561ms step_avg:57.77ms
step:807/2330 train_time:46617ms step_avg:57.77ms
step:808/2330 train_time:46678ms step_avg:57.77ms
step:809/2330 train_time:46735ms step_avg:57.77ms
step:810/2330 train_time:46795ms step_avg:57.77ms
step:811/2330 train_time:46852ms step_avg:57.77ms
step:812/2330 train_time:46912ms step_avg:57.77ms
step:813/2330 train_time:46969ms step_avg:57.77ms
step:814/2330 train_time:47030ms step_avg:57.78ms
step:815/2330 train_time:47086ms step_avg:57.77ms
step:816/2330 train_time:47146ms step_avg:57.78ms
step:817/2330 train_time:47202ms step_avg:57.78ms
step:818/2330 train_time:47261ms step_avg:57.78ms
step:819/2330 train_time:47318ms step_avg:57.78ms
step:820/2330 train_time:47379ms step_avg:57.78ms
step:821/2330 train_time:47435ms step_avg:57.78ms
step:822/2330 train_time:47496ms step_avg:57.78ms
step:823/2330 train_time:47553ms step_avg:57.78ms
step:824/2330 train_time:47614ms step_avg:57.78ms
step:825/2330 train_time:47671ms step_avg:57.78ms
step:826/2330 train_time:47731ms step_avg:57.79ms
step:827/2330 train_time:47788ms step_avg:57.78ms
step:828/2330 train_time:47849ms step_avg:57.79ms
step:829/2330 train_time:47906ms step_avg:57.79ms
step:830/2330 train_time:47966ms step_avg:57.79ms
step:831/2330 train_time:48022ms step_avg:57.79ms
step:832/2330 train_time:48081ms step_avg:57.79ms
step:833/2330 train_time:48137ms step_avg:57.79ms
step:834/2330 train_time:48197ms step_avg:57.79ms
step:835/2330 train_time:48254ms step_avg:57.79ms
step:836/2330 train_time:48314ms step_avg:57.79ms
step:837/2330 train_time:48371ms step_avg:57.79ms
step:838/2330 train_time:48431ms step_avg:57.79ms
step:839/2330 train_time:48489ms step_avg:57.79ms
step:840/2330 train_time:48548ms step_avg:57.80ms
step:841/2330 train_time:48605ms step_avg:57.79ms
step:842/2330 train_time:48665ms step_avg:57.80ms
step:843/2330 train_time:48722ms step_avg:57.80ms
step:844/2330 train_time:48782ms step_avg:57.80ms
step:845/2330 train_time:48838ms step_avg:57.80ms
step:846/2330 train_time:48898ms step_avg:57.80ms
step:847/2330 train_time:48955ms step_avg:57.80ms
step:848/2330 train_time:49015ms step_avg:57.80ms
step:849/2330 train_time:49071ms step_avg:57.80ms
step:850/2330 train_time:49132ms step_avg:57.80ms
step:851/2330 train_time:49189ms step_avg:57.80ms
step:852/2330 train_time:49249ms step_avg:57.80ms
step:853/2330 train_time:49305ms step_avg:57.80ms
step:854/2330 train_time:49366ms step_avg:57.81ms
step:855/2330 train_time:49424ms step_avg:57.81ms
step:856/2330 train_time:49482ms step_avg:57.81ms
step:857/2330 train_time:49538ms step_avg:57.80ms
step:858/2330 train_time:49600ms step_avg:57.81ms
step:859/2330 train_time:49656ms step_avg:57.81ms
step:860/2330 train_time:49718ms step_avg:57.81ms
step:861/2330 train_time:49774ms step_avg:57.81ms
step:862/2330 train_time:49836ms step_avg:57.81ms
step:863/2330 train_time:49892ms step_avg:57.81ms
step:864/2330 train_time:49953ms step_avg:57.82ms
step:865/2330 train_time:50010ms step_avg:57.81ms
step:866/2330 train_time:50071ms step_avg:57.82ms
step:867/2330 train_time:50128ms step_avg:57.82ms
step:868/2330 train_time:50187ms step_avg:57.82ms
step:869/2330 train_time:50244ms step_avg:57.82ms
step:870/2330 train_time:50304ms step_avg:57.82ms
step:871/2330 train_time:50360ms step_avg:57.82ms
step:872/2330 train_time:50420ms step_avg:57.82ms
step:873/2330 train_time:50478ms step_avg:57.82ms
step:874/2330 train_time:50538ms step_avg:57.82ms
step:875/2330 train_time:50594ms step_avg:57.82ms
step:876/2330 train_time:50654ms step_avg:57.82ms
step:877/2330 train_time:50711ms step_avg:57.82ms
step:878/2330 train_time:50771ms step_avg:57.83ms
step:879/2330 train_time:50828ms step_avg:57.83ms
step:880/2330 train_time:50888ms step_avg:57.83ms
step:881/2330 train_time:50945ms step_avg:57.83ms
step:882/2330 train_time:51006ms step_avg:57.83ms
step:883/2330 train_time:51062ms step_avg:57.83ms
step:884/2330 train_time:51122ms step_avg:57.83ms
step:885/2330 train_time:51178ms step_avg:57.83ms
step:886/2330 train_time:51241ms step_avg:57.83ms
step:887/2330 train_time:51296ms step_avg:57.83ms
step:888/2330 train_time:51357ms step_avg:57.83ms
step:889/2330 train_time:51415ms step_avg:57.83ms
step:890/2330 train_time:51474ms step_avg:57.84ms
step:891/2330 train_time:51531ms step_avg:57.84ms
step:892/2330 train_time:51593ms step_avg:57.84ms
step:893/2330 train_time:51650ms step_avg:57.84ms
step:894/2330 train_time:51710ms step_avg:57.84ms
step:895/2330 train_time:51767ms step_avg:57.84ms
step:896/2330 train_time:51827ms step_avg:57.84ms
step:897/2330 train_time:51883ms step_avg:57.84ms
step:898/2330 train_time:51943ms step_avg:57.84ms
step:899/2330 train_time:52000ms step_avg:57.84ms
step:900/2330 train_time:52060ms step_avg:57.84ms
step:901/2330 train_time:52117ms step_avg:57.84ms
step:902/2330 train_time:52178ms step_avg:57.85ms
step:903/2330 train_time:52234ms step_avg:57.84ms
step:904/2330 train_time:52295ms step_avg:57.85ms
step:905/2330 train_time:52351ms step_avg:57.85ms
step:906/2330 train_time:52411ms step_avg:57.85ms
step:907/2330 train_time:52468ms step_avg:57.85ms
step:908/2330 train_time:52529ms step_avg:57.85ms
step:909/2330 train_time:52585ms step_avg:57.85ms
step:910/2330 train_time:52646ms step_avg:57.85ms
step:911/2330 train_time:52702ms step_avg:57.85ms
step:912/2330 train_time:52762ms step_avg:57.85ms
step:913/2330 train_time:52818ms step_avg:57.85ms
step:914/2330 train_time:52878ms step_avg:57.85ms
step:915/2330 train_time:52935ms step_avg:57.85ms
step:916/2330 train_time:52996ms step_avg:57.86ms
step:917/2330 train_time:53052ms step_avg:57.85ms
step:918/2330 train_time:53114ms step_avg:57.86ms
step:919/2330 train_time:53170ms step_avg:57.86ms
step:920/2330 train_time:53231ms step_avg:57.86ms
step:921/2330 train_time:53288ms step_avg:57.86ms
step:922/2330 train_time:53348ms step_avg:57.86ms
step:923/2330 train_time:53404ms step_avg:57.86ms
step:924/2330 train_time:53464ms step_avg:57.86ms
step:925/2330 train_time:53520ms step_avg:57.86ms
step:926/2330 train_time:53581ms step_avg:57.86ms
step:927/2330 train_time:53637ms step_avg:57.86ms
step:928/2330 train_time:53697ms step_avg:57.86ms
step:929/2330 train_time:53753ms step_avg:57.86ms
step:930/2330 train_time:53815ms step_avg:57.87ms
step:931/2330 train_time:53871ms step_avg:57.86ms
step:932/2330 train_time:53932ms step_avg:57.87ms
step:933/2330 train_time:53988ms step_avg:57.86ms
step:934/2330 train_time:54048ms step_avg:57.87ms
step:935/2330 train_time:54106ms step_avg:57.87ms
step:936/2330 train_time:54165ms step_avg:57.87ms
step:937/2330 train_time:54223ms step_avg:57.87ms
step:938/2330 train_time:54282ms step_avg:57.87ms
step:939/2330 train_time:54338ms step_avg:57.87ms
step:940/2330 train_time:54399ms step_avg:57.87ms
step:941/2330 train_time:54455ms step_avg:57.87ms
step:942/2330 train_time:54515ms step_avg:57.87ms
step:943/2330 train_time:54571ms step_avg:57.87ms
step:944/2330 train_time:54633ms step_avg:57.87ms
step:945/2330 train_time:54689ms step_avg:57.87ms
step:946/2330 train_time:54751ms step_avg:57.88ms
step:947/2330 train_time:54807ms step_avg:57.87ms
step:948/2330 train_time:54868ms step_avg:57.88ms
step:949/2330 train_time:54925ms step_avg:57.88ms
step:950/2330 train_time:54984ms step_avg:57.88ms
step:951/2330 train_time:55041ms step_avg:57.88ms
step:952/2330 train_time:55101ms step_avg:57.88ms
step:953/2330 train_time:55158ms step_avg:57.88ms
step:954/2330 train_time:55218ms step_avg:57.88ms
step:955/2330 train_time:55274ms step_avg:57.88ms
step:956/2330 train_time:55335ms step_avg:57.88ms
step:957/2330 train_time:55391ms step_avg:57.88ms
step:958/2330 train_time:55452ms step_avg:57.88ms
step:959/2330 train_time:55508ms step_avg:57.88ms
step:960/2330 train_time:55570ms step_avg:57.89ms
step:961/2330 train_time:55626ms step_avg:57.88ms
step:962/2330 train_time:55686ms step_avg:57.89ms
step:963/2330 train_time:55744ms step_avg:57.89ms
step:964/2330 train_time:55803ms step_avg:57.89ms
step:965/2330 train_time:55860ms step_avg:57.89ms
step:966/2330 train_time:55919ms step_avg:57.89ms
step:967/2330 train_time:55976ms step_avg:57.89ms
step:968/2330 train_time:56037ms step_avg:57.89ms
step:969/2330 train_time:56093ms step_avg:57.89ms
step:970/2330 train_time:56154ms step_avg:57.89ms
step:971/2330 train_time:56210ms step_avg:57.89ms
step:972/2330 train_time:56271ms step_avg:57.89ms
step:973/2330 train_time:56328ms step_avg:57.89ms
step:974/2330 train_time:56389ms step_avg:57.89ms
step:975/2330 train_time:56445ms step_avg:57.89ms
step:976/2330 train_time:56505ms step_avg:57.89ms
step:977/2330 train_time:56561ms step_avg:57.89ms
step:978/2330 train_time:56621ms step_avg:57.89ms
step:979/2330 train_time:56678ms step_avg:57.89ms
step:980/2330 train_time:56739ms step_avg:57.90ms
step:981/2330 train_time:56795ms step_avg:57.89ms
step:982/2330 train_time:56855ms step_avg:57.90ms
step:983/2330 train_time:56912ms step_avg:57.90ms
step:984/2330 train_time:56972ms step_avg:57.90ms
step:985/2330 train_time:57029ms step_avg:57.90ms
step:986/2330 train_time:57090ms step_avg:57.90ms
step:987/2330 train_time:57146ms step_avg:57.90ms
step:988/2330 train_time:57206ms step_avg:57.90ms
step:989/2330 train_time:57263ms step_avg:57.90ms
step:990/2330 train_time:57323ms step_avg:57.90ms
step:991/2330 train_time:57380ms step_avg:57.90ms
step:992/2330 train_time:57440ms step_avg:57.90ms
step:993/2330 train_time:57496ms step_avg:57.90ms
step:994/2330 train_time:57556ms step_avg:57.90ms
step:995/2330 train_time:57612ms step_avg:57.90ms
step:996/2330 train_time:57673ms step_avg:57.90ms
step:997/2330 train_time:57730ms step_avg:57.90ms
step:998/2330 train_time:57790ms step_avg:57.91ms
step:999/2330 train_time:57847ms step_avg:57.91ms
step:1000/2330 train_time:57906ms step_avg:57.91ms
step:1000/2330 val_loss:4.1096 train_time:57986ms step_avg:57.99ms
step:1001/2330 train_time:58006ms step_avg:57.95ms
step:1002/2330 train_time:58027ms step_avg:57.91ms
step:1003/2330 train_time:58079ms step_avg:57.91ms
step:1004/2330 train_time:58142ms step_avg:57.91ms
step:1005/2330 train_time:58199ms step_avg:57.91ms
step:1006/2330 train_time:58265ms step_avg:57.92ms
step:1007/2330 train_time:58322ms step_avg:57.92ms
step:1008/2330 train_time:58382ms step_avg:57.92ms
step:1009/2330 train_time:58437ms step_avg:57.92ms
step:1010/2330 train_time:58499ms step_avg:57.92ms
step:1011/2330 train_time:58555ms step_avg:57.92ms
step:1012/2330 train_time:58614ms step_avg:57.92ms
step:1013/2330 train_time:58670ms step_avg:57.92ms
step:1014/2330 train_time:58729ms step_avg:57.92ms
step:1015/2330 train_time:58785ms step_avg:57.92ms
step:1016/2330 train_time:58844ms step_avg:57.92ms
step:1017/2330 train_time:58902ms step_avg:57.92ms
step:1018/2330 train_time:58966ms step_avg:57.92ms
step:1019/2330 train_time:59024ms step_avg:57.92ms
step:1020/2330 train_time:59084ms step_avg:57.93ms
step:1021/2330 train_time:59140ms step_avg:57.92ms
step:1022/2330 train_time:59205ms step_avg:57.93ms
step:1023/2330 train_time:59261ms step_avg:57.93ms
step:1024/2330 train_time:59322ms step_avg:57.93ms
step:1025/2330 train_time:59377ms step_avg:57.93ms
step:1026/2330 train_time:59438ms step_avg:57.93ms
step:1027/2330 train_time:59494ms step_avg:57.93ms
step:1028/2330 train_time:59553ms step_avg:57.93ms
step:1029/2330 train_time:59610ms step_avg:57.93ms
step:1030/2330 train_time:59668ms step_avg:57.93ms
step:1031/2330 train_time:59725ms step_avg:57.93ms
step:1032/2330 train_time:59784ms step_avg:57.93ms
step:1033/2330 train_time:59840ms step_avg:57.93ms
step:1034/2330 train_time:59902ms step_avg:57.93ms
step:1035/2330 train_time:59958ms step_avg:57.93ms
step:1036/2330 train_time:60021ms step_avg:57.94ms
step:1037/2330 train_time:60078ms step_avg:57.93ms
step:1038/2330 train_time:60140ms step_avg:57.94ms
step:1039/2330 train_time:60197ms step_avg:57.94ms
step:1040/2330 train_time:60257ms step_avg:57.94ms
step:1041/2330 train_time:60314ms step_avg:57.94ms
step:1042/2330 train_time:60373ms step_avg:57.94ms
step:1043/2330 train_time:60430ms step_avg:57.94ms
step:1044/2330 train_time:60490ms step_avg:57.94ms
step:1045/2330 train_time:60546ms step_avg:57.94ms
step:1046/2330 train_time:60606ms step_avg:57.94ms
step:1047/2330 train_time:60663ms step_avg:57.94ms
step:1048/2330 train_time:60722ms step_avg:57.94ms
step:1049/2330 train_time:60779ms step_avg:57.94ms
step:1050/2330 train_time:60839ms step_avg:57.94ms
step:1051/2330 train_time:60896ms step_avg:57.94ms
step:1052/2330 train_time:60957ms step_avg:57.94ms
step:1053/2330 train_time:61015ms step_avg:57.94ms
step:1054/2330 train_time:61076ms step_avg:57.95ms
step:1055/2330 train_time:61134ms step_avg:57.95ms
step:1056/2330 train_time:61194ms step_avg:57.95ms
step:1057/2330 train_time:61251ms step_avg:57.95ms
step:1058/2330 train_time:61310ms step_avg:57.95ms
step:1059/2330 train_time:61366ms step_avg:57.95ms
step:1060/2330 train_time:61426ms step_avg:57.95ms
step:1061/2330 train_time:61483ms step_avg:57.95ms
step:1062/2330 train_time:61542ms step_avg:57.95ms
step:1063/2330 train_time:61599ms step_avg:57.95ms
step:1064/2330 train_time:61659ms step_avg:57.95ms
step:1065/2330 train_time:61716ms step_avg:57.95ms
step:1066/2330 train_time:61775ms step_avg:57.95ms
step:1067/2330 train_time:61832ms step_avg:57.95ms
step:1068/2330 train_time:61892ms step_avg:57.95ms
step:1069/2330 train_time:61949ms step_avg:57.95ms
step:1070/2330 train_time:62009ms step_avg:57.95ms
step:1071/2330 train_time:62066ms step_avg:57.95ms
step:1072/2330 train_time:62126ms step_avg:57.95ms
step:1073/2330 train_time:62182ms step_avg:57.95ms
step:1074/2330 train_time:62243ms step_avg:57.95ms
step:1075/2330 train_time:62300ms step_avg:57.95ms
step:1076/2330 train_time:62361ms step_avg:57.96ms
step:1077/2330 train_time:62418ms step_avg:57.96ms
step:1078/2330 train_time:62478ms step_avg:57.96ms
step:1079/2330 train_time:62535ms step_avg:57.96ms
step:1080/2330 train_time:62595ms step_avg:57.96ms
step:1081/2330 train_time:62651ms step_avg:57.96ms
step:1082/2330 train_time:62711ms step_avg:57.96ms
step:1083/2330 train_time:62768ms step_avg:57.96ms
step:1084/2330 train_time:62827ms step_avg:57.96ms
step:1085/2330 train_time:62884ms step_avg:57.96ms
step:1086/2330 train_time:62944ms step_avg:57.96ms
step:1087/2330 train_time:63000ms step_avg:57.96ms
step:1088/2330 train_time:63062ms step_avg:57.96ms
step:1089/2330 train_time:63118ms step_avg:57.96ms
step:1090/2330 train_time:63179ms step_avg:57.96ms
step:1091/2330 train_time:63237ms step_avg:57.96ms
step:1092/2330 train_time:63297ms step_avg:57.96ms
step:1093/2330 train_time:63354ms step_avg:57.96ms
step:1094/2330 train_time:63414ms step_avg:57.97ms
step:1095/2330 train_time:63471ms step_avg:57.96ms
step:1096/2330 train_time:63531ms step_avg:57.97ms
step:1097/2330 train_time:63588ms step_avg:57.96ms
step:1098/2330 train_time:63646ms step_avg:57.97ms
step:1099/2330 train_time:63703ms step_avg:57.96ms
step:1100/2330 train_time:63763ms step_avg:57.97ms
step:1101/2330 train_time:63820ms step_avg:57.97ms
step:1102/2330 train_time:63881ms step_avg:57.97ms
step:1103/2330 train_time:63937ms step_avg:57.97ms
step:1104/2330 train_time:63998ms step_avg:57.97ms
step:1105/2330 train_time:64054ms step_avg:57.97ms
step:1106/2330 train_time:64114ms step_avg:57.97ms
step:1107/2330 train_time:64172ms step_avg:57.97ms
step:1108/2330 train_time:64232ms step_avg:57.97ms
step:1109/2330 train_time:64288ms step_avg:57.97ms
step:1110/2330 train_time:64349ms step_avg:57.97ms
step:1111/2330 train_time:64406ms step_avg:57.97ms
step:1112/2330 train_time:64466ms step_avg:57.97ms
step:1113/2330 train_time:64522ms step_avg:57.97ms
step:1114/2330 train_time:64582ms step_avg:57.97ms
step:1115/2330 train_time:64638ms step_avg:57.97ms
step:1116/2330 train_time:64700ms step_avg:57.97ms
step:1117/2330 train_time:64756ms step_avg:57.97ms
step:1118/2330 train_time:64817ms step_avg:57.98ms
step:1119/2330 train_time:64874ms step_avg:57.98ms
step:1120/2330 train_time:64934ms step_avg:57.98ms
step:1121/2330 train_time:64990ms step_avg:57.98ms
step:1122/2330 train_time:65050ms step_avg:57.98ms
step:1123/2330 train_time:65107ms step_avg:57.98ms
step:1124/2330 train_time:65167ms step_avg:57.98ms
step:1125/2330 train_time:65223ms step_avg:57.98ms
step:1126/2330 train_time:65284ms step_avg:57.98ms
step:1127/2330 train_time:65340ms step_avg:57.98ms
step:1128/2330 train_time:65400ms step_avg:57.98ms
step:1129/2330 train_time:65457ms step_avg:57.98ms
step:1130/2330 train_time:65517ms step_avg:57.98ms
step:1131/2330 train_time:65574ms step_avg:57.98ms
step:1132/2330 train_time:65634ms step_avg:57.98ms
step:1133/2330 train_time:65691ms step_avg:57.98ms
step:1134/2330 train_time:65750ms step_avg:57.98ms
step:1135/2330 train_time:65807ms step_avg:57.98ms
step:1136/2330 train_time:65867ms step_avg:57.98ms
step:1137/2330 train_time:65924ms step_avg:57.98ms
step:1138/2330 train_time:65984ms step_avg:57.98ms
step:1139/2330 train_time:66040ms step_avg:57.98ms
step:1140/2330 train_time:66102ms step_avg:57.98ms
step:1141/2330 train_time:66158ms step_avg:57.98ms
step:1142/2330 train_time:66219ms step_avg:57.99ms
step:1143/2330 train_time:66277ms step_avg:57.98ms
step:1144/2330 train_time:66337ms step_avg:57.99ms
step:1145/2330 train_time:66394ms step_avg:57.99ms
step:1146/2330 train_time:66453ms step_avg:57.99ms
step:1147/2330 train_time:66510ms step_avg:57.99ms
step:1148/2330 train_time:66570ms step_avg:57.99ms
step:1149/2330 train_time:66627ms step_avg:57.99ms
step:1150/2330 train_time:66688ms step_avg:57.99ms
step:1151/2330 train_time:66745ms step_avg:57.99ms
step:1152/2330 train_time:66805ms step_avg:57.99ms
step:1153/2330 train_time:66861ms step_avg:57.99ms
step:1154/2330 train_time:66921ms step_avg:57.99ms
step:1155/2330 train_time:66977ms step_avg:57.99ms
step:1156/2330 train_time:67038ms step_avg:57.99ms
step:1157/2330 train_time:67094ms step_avg:57.99ms
step:1158/2330 train_time:67155ms step_avg:57.99ms
step:1159/2330 train_time:67212ms step_avg:57.99ms
step:1160/2330 train_time:67272ms step_avg:57.99ms
step:1161/2330 train_time:67330ms step_avg:57.99ms
step:1162/2330 train_time:67389ms step_avg:57.99ms
step:1163/2330 train_time:67446ms step_avg:57.99ms
step:1164/2330 train_time:67506ms step_avg:58.00ms
step:1165/2330 train_time:67563ms step_avg:57.99ms
step:1166/2330 train_time:67623ms step_avg:58.00ms
step:1167/2330 train_time:67680ms step_avg:57.99ms
step:1168/2330 train_time:67740ms step_avg:58.00ms
step:1169/2330 train_time:67797ms step_avg:58.00ms
step:1170/2330 train_time:67857ms step_avg:58.00ms
step:1171/2330 train_time:67914ms step_avg:58.00ms
step:1172/2330 train_time:67974ms step_avg:58.00ms
step:1173/2330 train_time:68030ms step_avg:58.00ms
step:1174/2330 train_time:68090ms step_avg:58.00ms
step:1175/2330 train_time:68147ms step_avg:58.00ms
step:1176/2330 train_time:68207ms step_avg:58.00ms
step:1177/2330 train_time:68263ms step_avg:58.00ms
step:1178/2330 train_time:68324ms step_avg:58.00ms
step:1179/2330 train_time:68381ms step_avg:58.00ms
step:1180/2330 train_time:68440ms step_avg:58.00ms
step:1181/2330 train_time:68497ms step_avg:58.00ms
step:1182/2330 train_time:68558ms step_avg:58.00ms
step:1183/2330 train_time:68616ms step_avg:58.00ms
step:1184/2330 train_time:68676ms step_avg:58.00ms
step:1185/2330 train_time:68733ms step_avg:58.00ms
step:1186/2330 train_time:68792ms step_avg:58.00ms
step:1187/2330 train_time:68848ms step_avg:58.00ms
step:1188/2330 train_time:68908ms step_avg:58.00ms
step:1189/2330 train_time:68965ms step_avg:58.00ms
step:1190/2330 train_time:69024ms step_avg:58.00ms
step:1191/2330 train_time:69081ms step_avg:58.00ms
step:1192/2330 train_time:69141ms step_avg:58.00ms
step:1193/2330 train_time:69197ms step_avg:58.00ms
step:1194/2330 train_time:69258ms step_avg:58.01ms
step:1195/2330 train_time:69315ms step_avg:58.00ms
step:1196/2330 train_time:69376ms step_avg:58.01ms
step:1197/2330 train_time:69432ms step_avg:58.01ms
step:1198/2330 train_time:69492ms step_avg:58.01ms
step:1199/2330 train_time:69549ms step_avg:58.01ms
step:1200/2330 train_time:69609ms step_avg:58.01ms
step:1201/2330 train_time:69665ms step_avg:58.01ms
step:1202/2330 train_time:69725ms step_avg:58.01ms
step:1203/2330 train_time:69782ms step_avg:58.01ms
step:1204/2330 train_time:69842ms step_avg:58.01ms
step:1205/2330 train_time:69898ms step_avg:58.01ms
step:1206/2330 train_time:69958ms step_avg:58.01ms
step:1207/2330 train_time:70014ms step_avg:58.01ms
step:1208/2330 train_time:70075ms step_avg:58.01ms
step:1209/2330 train_time:70131ms step_avg:58.01ms
step:1210/2330 train_time:70191ms step_avg:58.01ms
step:1211/2330 train_time:70247ms step_avg:58.01ms
step:1212/2330 train_time:70308ms step_avg:58.01ms
step:1213/2330 train_time:70365ms step_avg:58.01ms
step:1214/2330 train_time:70426ms step_avg:58.01ms
step:1215/2330 train_time:70482ms step_avg:58.01ms
step:1216/2330 train_time:70542ms step_avg:58.01ms
step:1217/2330 train_time:70599ms step_avg:58.01ms
step:1218/2330 train_time:70660ms step_avg:58.01ms
step:1219/2330 train_time:70717ms step_avg:58.01ms
step:1220/2330 train_time:70777ms step_avg:58.01ms
step:1221/2330 train_time:70835ms step_avg:58.01ms
step:1222/2330 train_time:70895ms step_avg:58.02ms
step:1223/2330 train_time:70951ms step_avg:58.01ms
step:1224/2330 train_time:71011ms step_avg:58.02ms
step:1225/2330 train_time:71067ms step_avg:58.01ms
step:1226/2330 train_time:71127ms step_avg:58.02ms
step:1227/2330 train_time:71184ms step_avg:58.01ms
step:1228/2330 train_time:71243ms step_avg:58.02ms
step:1229/2330 train_time:71300ms step_avg:58.01ms
step:1230/2330 train_time:71361ms step_avg:58.02ms
step:1231/2330 train_time:71418ms step_avg:58.02ms
step:1232/2330 train_time:71478ms step_avg:58.02ms
step:1233/2330 train_time:71535ms step_avg:58.02ms
step:1234/2330 train_time:71595ms step_avg:58.02ms
step:1235/2330 train_time:71651ms step_avg:58.02ms
step:1236/2330 train_time:71711ms step_avg:58.02ms
step:1237/2330 train_time:71768ms step_avg:58.02ms
step:1238/2330 train_time:71828ms step_avg:58.02ms
step:1239/2330 train_time:71885ms step_avg:58.02ms
step:1240/2330 train_time:71945ms step_avg:58.02ms
step:1241/2330 train_time:72001ms step_avg:58.02ms
step:1242/2330 train_time:72061ms step_avg:58.02ms
step:1243/2330 train_time:72118ms step_avg:58.02ms
step:1244/2330 train_time:72178ms step_avg:58.02ms
step:1245/2330 train_time:72234ms step_avg:58.02ms
step:1246/2330 train_time:72295ms step_avg:58.02ms
step:1247/2330 train_time:72351ms step_avg:58.02ms
step:1248/2330 train_time:72410ms step_avg:58.02ms
step:1249/2330 train_time:72467ms step_avg:58.02ms
step:1250/2330 train_time:72528ms step_avg:58.02ms
step:1250/2330 val_loss:4.0217 train_time:72609ms step_avg:58.09ms
step:1251/2330 train_time:72628ms step_avg:58.06ms
step:1252/2330 train_time:72648ms step_avg:58.03ms
step:1253/2330 train_time:72708ms step_avg:58.03ms
step:1254/2330 train_time:72773ms step_avg:58.03ms
step:1255/2330 train_time:72831ms step_avg:58.03ms
step:1256/2330 train_time:72892ms step_avg:58.04ms
step:1257/2330 train_time:72949ms step_avg:58.03ms
step:1258/2330 train_time:73008ms step_avg:58.03ms
step:1259/2330 train_time:73064ms step_avg:58.03ms
step:1260/2330 train_time:73123ms step_avg:58.03ms
step:1261/2330 train_time:73179ms step_avg:58.03ms
step:1262/2330 train_time:73238ms step_avg:58.03ms
step:1263/2330 train_time:73294ms step_avg:58.03ms
step:1264/2330 train_time:73354ms step_avg:58.03ms
step:1265/2330 train_time:73411ms step_avg:58.03ms
step:1266/2330 train_time:73469ms step_avg:58.03ms
step:1267/2330 train_time:73526ms step_avg:58.03ms
step:1268/2330 train_time:73586ms step_avg:58.03ms
step:1269/2330 train_time:73645ms step_avg:58.03ms
step:1270/2330 train_time:73707ms step_avg:58.04ms
step:1271/2330 train_time:73765ms step_avg:58.04ms
step:1272/2330 train_time:73826ms step_avg:58.04ms
step:1273/2330 train_time:73883ms step_avg:58.04ms
step:1274/2330 train_time:73943ms step_avg:58.04ms
step:1275/2330 train_time:74000ms step_avg:58.04ms
step:1276/2330 train_time:74060ms step_avg:58.04ms
step:1277/2330 train_time:74116ms step_avg:58.04ms
step:1278/2330 train_time:74176ms step_avg:58.04ms
step:1279/2330 train_time:74232ms step_avg:58.04ms
step:1280/2330 train_time:74291ms step_avg:58.04ms
step:1281/2330 train_time:74348ms step_avg:58.04ms
step:1282/2330 train_time:74407ms step_avg:58.04ms
step:1283/2330 train_time:74464ms step_avg:58.04ms
step:1284/2330 train_time:74523ms step_avg:58.04ms
step:1285/2330 train_time:74579ms step_avg:58.04ms
step:1286/2330 train_time:74640ms step_avg:58.04ms
step:1287/2330 train_time:74696ms step_avg:58.04ms
step:1288/2330 train_time:74759ms step_avg:58.04ms
step:1289/2330 train_time:74815ms step_avg:58.04ms
step:1290/2330 train_time:74877ms step_avg:58.04ms
step:1291/2330 train_time:74934ms step_avg:58.04ms
step:1292/2330 train_time:74996ms step_avg:58.05ms
step:1293/2330 train_time:75052ms step_avg:58.04ms
step:1294/2330 train_time:75112ms step_avg:58.05ms
step:1295/2330 train_time:75168ms step_avg:58.04ms
step:1296/2330 train_time:75656ms step_avg:58.38ms
step:1297/2330 train_time:75711ms step_avg:58.37ms
step:1298/2330 train_time:75770ms step_avg:58.37ms
step:1299/2330 train_time:75826ms step_avg:58.37ms
step:1300/2330 train_time:75885ms step_avg:58.37ms
step:1301/2330 train_time:75941ms step_avg:58.37ms
step:1302/2330 train_time:75999ms step_avg:58.37ms
step:1303/2330 train_time:76055ms step_avg:58.37ms
step:1304/2330 train_time:76115ms step_avg:58.37ms
step:1305/2330 train_time:76171ms step_avg:58.37ms
step:1306/2330 train_time:76231ms step_avg:58.37ms
step:1307/2330 train_time:76287ms step_avg:58.37ms
step:1308/2330 train_time:76346ms step_avg:58.37ms
step:1309/2330 train_time:76402ms step_avg:58.37ms
step:1310/2330 train_time:76460ms step_avg:58.37ms
step:1311/2330 train_time:76522ms step_avg:58.37ms
step:1312/2330 train_time:76585ms step_avg:58.37ms
step:1313/2330 train_time:76643ms step_avg:58.37ms
step:1314/2330 train_time:76706ms step_avg:58.38ms
step:1315/2330 train_time:76762ms step_avg:58.37ms
step:1316/2330 train_time:76822ms step_avg:58.38ms
step:1317/2330 train_time:76878ms step_avg:58.37ms
step:1318/2330 train_time:76939ms step_avg:58.38ms
step:1319/2330 train_time:76995ms step_avg:58.37ms
step:1320/2330 train_time:77054ms step_avg:58.37ms
step:1321/2330 train_time:77111ms step_avg:58.37ms
step:1322/2330 train_time:77170ms step_avg:58.37ms
step:1323/2330 train_time:77226ms step_avg:58.37ms
step:1324/2330 train_time:77285ms step_avg:58.37ms
step:1325/2330 train_time:77341ms step_avg:58.37ms
step:1326/2330 train_time:77400ms step_avg:58.37ms
step:1327/2330 train_time:77458ms step_avg:58.37ms
step:1328/2330 train_time:77520ms step_avg:58.37ms
step:1329/2330 train_time:77578ms step_avg:58.37ms
step:1330/2330 train_time:77640ms step_avg:58.38ms
step:1331/2330 train_time:77697ms step_avg:58.38ms
step:1332/2330 train_time:77758ms step_avg:58.38ms
step:1333/2330 train_time:77815ms step_avg:58.38ms
step:1334/2330 train_time:77876ms step_avg:58.38ms
step:1335/2330 train_time:77932ms step_avg:58.38ms
step:1336/2330 train_time:77992ms step_avg:58.38ms
step:1337/2330 train_time:78048ms step_avg:58.38ms
step:1338/2330 train_time:78107ms step_avg:58.38ms
step:1339/2330 train_time:78163ms step_avg:58.37ms
step:1340/2330 train_time:78223ms step_avg:58.38ms
step:1341/2330 train_time:78279ms step_avg:58.37ms
step:1342/2330 train_time:78338ms step_avg:58.37ms
step:1343/2330 train_time:78395ms step_avg:58.37ms
step:1344/2330 train_time:78455ms step_avg:58.37ms
step:1345/2330 train_time:78512ms step_avg:58.37ms
step:1346/2330 train_time:78574ms step_avg:58.38ms
step:1347/2330 train_time:78632ms step_avg:58.38ms
step:1348/2330 train_time:78693ms step_avg:58.38ms
step:1349/2330 train_time:78750ms step_avg:58.38ms
step:1350/2330 train_time:78810ms step_avg:58.38ms
step:1351/2330 train_time:78867ms step_avg:58.38ms
step:1352/2330 train_time:78926ms step_avg:58.38ms
step:1353/2330 train_time:78982ms step_avg:58.38ms
step:1354/2330 train_time:79043ms step_avg:58.38ms
step:1355/2330 train_time:79099ms step_avg:58.38ms
step:1356/2330 train_time:79158ms step_avg:58.38ms
step:1357/2330 train_time:79214ms step_avg:58.37ms
step:1358/2330 train_time:79274ms step_avg:58.38ms
step:1359/2330 train_time:79331ms step_avg:58.37ms
step:1360/2330 train_time:79390ms step_avg:58.38ms
step:1361/2330 train_time:79448ms step_avg:58.37ms
step:1362/2330 train_time:79508ms step_avg:58.38ms
step:1363/2330 train_time:79565ms step_avg:58.37ms
step:1364/2330 train_time:79625ms step_avg:58.38ms
step:1365/2330 train_time:79683ms step_avg:58.38ms
step:1366/2330 train_time:79744ms step_avg:58.38ms
step:1367/2330 train_time:79800ms step_avg:58.38ms
step:1368/2330 train_time:79861ms step_avg:58.38ms
step:1369/2330 train_time:79918ms step_avg:58.38ms
step:1370/2330 train_time:79978ms step_avg:58.38ms
step:1371/2330 train_time:80035ms step_avg:58.38ms
step:1372/2330 train_time:80095ms step_avg:58.38ms
step:1373/2330 train_time:80151ms step_avg:58.38ms
step:1374/2330 train_time:80211ms step_avg:58.38ms
step:1375/2330 train_time:80268ms step_avg:58.38ms
step:1376/2330 train_time:80327ms step_avg:58.38ms
step:1377/2330 train_time:80384ms step_avg:58.38ms
step:1378/2330 train_time:80443ms step_avg:58.38ms
step:1379/2330 train_time:80500ms step_avg:58.38ms
step:1380/2330 train_time:80560ms step_avg:58.38ms
step:1381/2330 train_time:80617ms step_avg:58.38ms
step:1382/2330 train_time:80679ms step_avg:58.38ms
step:1383/2330 train_time:80736ms step_avg:58.38ms
step:1384/2330 train_time:80798ms step_avg:58.38ms
step:1385/2330 train_time:80855ms step_avg:58.38ms
step:1386/2330 train_time:80916ms step_avg:58.38ms
step:1387/2330 train_time:80972ms step_avg:58.38ms
step:1388/2330 train_time:81033ms step_avg:58.38ms
step:1389/2330 train_time:81090ms step_avg:58.38ms
step:1390/2330 train_time:81150ms step_avg:58.38ms
step:1391/2330 train_time:81207ms step_avg:58.38ms
step:1392/2330 train_time:81266ms step_avg:58.38ms
step:1393/2330 train_time:81323ms step_avg:58.38ms
step:1394/2330 train_time:81382ms step_avg:58.38ms
step:1395/2330 train_time:81439ms step_avg:58.38ms
step:1396/2330 train_time:81499ms step_avg:58.38ms
step:1397/2330 train_time:81556ms step_avg:58.38ms
step:1398/2330 train_time:81617ms step_avg:58.38ms
step:1399/2330 train_time:81674ms step_avg:58.38ms
step:1400/2330 train_time:81735ms step_avg:58.38ms
step:1401/2330 train_time:81792ms step_avg:58.38ms
step:1402/2330 train_time:81852ms step_avg:58.38ms
step:1403/2330 train_time:81910ms step_avg:58.38ms
step:1404/2330 train_time:81969ms step_avg:58.38ms
step:1405/2330 train_time:82026ms step_avg:58.38ms
step:1406/2330 train_time:82085ms step_avg:58.38ms
step:1407/2330 train_time:82142ms step_avg:58.38ms
step:1408/2330 train_time:82201ms step_avg:58.38ms
step:1409/2330 train_time:82257ms step_avg:58.38ms
step:1410/2330 train_time:82318ms step_avg:58.38ms
step:1411/2330 train_time:82374ms step_avg:58.38ms
step:1412/2330 train_time:82434ms step_avg:58.38ms
step:1413/2330 train_time:82492ms step_avg:58.38ms
step:1414/2330 train_time:82551ms step_avg:58.38ms
step:1415/2330 train_time:82609ms step_avg:58.38ms
step:1416/2330 train_time:82670ms step_avg:58.38ms
step:1417/2330 train_time:82728ms step_avg:58.38ms
step:1418/2330 train_time:82788ms step_avg:58.38ms
step:1419/2330 train_time:82845ms step_avg:58.38ms
step:1420/2330 train_time:82904ms step_avg:58.38ms
step:1421/2330 train_time:82962ms step_avg:58.38ms
step:1422/2330 train_time:83021ms step_avg:58.38ms
step:1423/2330 train_time:83078ms step_avg:58.38ms
step:1424/2330 train_time:83137ms step_avg:58.38ms
step:1425/2330 train_time:83193ms step_avg:58.38ms
step:1426/2330 train_time:83254ms step_avg:58.38ms
step:1427/2330 train_time:83311ms step_avg:58.38ms
step:1428/2330 train_time:83370ms step_avg:58.38ms
step:1429/2330 train_time:83427ms step_avg:58.38ms
step:1430/2330 train_time:83487ms step_avg:58.38ms
step:1431/2330 train_time:83543ms step_avg:58.38ms
step:1432/2330 train_time:83603ms step_avg:58.38ms
step:1433/2330 train_time:83661ms step_avg:58.38ms
step:1434/2330 train_time:83722ms step_avg:58.38ms
step:1435/2330 train_time:83778ms step_avg:58.38ms
step:1436/2330 train_time:83839ms step_avg:58.38ms
step:1437/2330 train_time:83896ms step_avg:58.38ms
step:1438/2330 train_time:83956ms step_avg:58.38ms
step:1439/2330 train_time:84012ms step_avg:58.38ms
step:1440/2330 train_time:84073ms step_avg:58.38ms
step:1441/2330 train_time:84129ms step_avg:58.38ms
step:1442/2330 train_time:84189ms step_avg:58.38ms
step:1443/2330 train_time:84247ms step_avg:58.38ms
step:1444/2330 train_time:84307ms step_avg:58.38ms
step:1445/2330 train_time:84364ms step_avg:58.38ms
step:1446/2330 train_time:84423ms step_avg:58.38ms
step:1447/2330 train_time:84480ms step_avg:58.38ms
step:1448/2330 train_time:84540ms step_avg:58.38ms
step:1449/2330 train_time:84596ms step_avg:58.38ms
step:1450/2330 train_time:84657ms step_avg:58.38ms
step:1451/2330 train_time:84714ms step_avg:58.38ms
step:1452/2330 train_time:84774ms step_avg:58.38ms
step:1453/2330 train_time:84831ms step_avg:58.38ms
step:1454/2330 train_time:84891ms step_avg:58.38ms
step:1455/2330 train_time:84948ms step_avg:58.38ms
step:1456/2330 train_time:85009ms step_avg:58.38ms
step:1457/2330 train_time:85066ms step_avg:58.38ms
step:1458/2330 train_time:85125ms step_avg:58.39ms
step:1459/2330 train_time:85182ms step_avg:58.38ms
step:1460/2330 train_time:85241ms step_avg:58.38ms
step:1461/2330 train_time:85298ms step_avg:58.38ms
step:1462/2330 train_time:85358ms step_avg:58.38ms
step:1463/2330 train_time:85414ms step_avg:58.38ms
step:1464/2330 train_time:85475ms step_avg:58.38ms
step:1465/2330 train_time:85532ms step_avg:58.38ms
step:1466/2330 train_time:85592ms step_avg:58.38ms
step:1467/2330 train_time:85650ms step_avg:58.38ms
step:1468/2330 train_time:85710ms step_avg:58.39ms
step:1469/2330 train_time:85767ms step_avg:58.38ms
step:1470/2330 train_time:85827ms step_avg:58.39ms
step:1471/2330 train_time:85884ms step_avg:58.38ms
step:1472/2330 train_time:85943ms step_avg:58.39ms
step:1473/2330 train_time:86000ms step_avg:58.38ms
step:1474/2330 train_time:86060ms step_avg:58.39ms
step:1475/2330 train_time:86117ms step_avg:58.38ms
step:1476/2330 train_time:86176ms step_avg:58.38ms
step:1477/2330 train_time:86233ms step_avg:58.38ms
step:1478/2330 train_time:86293ms step_avg:58.39ms
step:1479/2330 train_time:86350ms step_avg:58.38ms
step:1480/2330 train_time:86410ms step_avg:58.39ms
step:1481/2330 train_time:86467ms step_avg:58.38ms
step:1482/2330 train_time:86527ms step_avg:58.39ms
step:1483/2330 train_time:86584ms step_avg:58.38ms
step:1484/2330 train_time:86644ms step_avg:58.39ms
step:1485/2330 train_time:86701ms step_avg:58.38ms
step:1486/2330 train_time:86761ms step_avg:58.39ms
step:1487/2330 train_time:86818ms step_avg:58.38ms
step:1488/2330 train_time:86878ms step_avg:58.39ms
step:1489/2330 train_time:86934ms step_avg:58.38ms
step:1490/2330 train_time:86995ms step_avg:58.39ms
step:1491/2330 train_time:87051ms step_avg:58.38ms
step:1492/2330 train_time:87112ms step_avg:58.39ms
step:1493/2330 train_time:87168ms step_avg:58.38ms
step:1494/2330 train_time:87230ms step_avg:58.39ms
step:1495/2330 train_time:87287ms step_avg:58.39ms
step:1496/2330 train_time:87347ms step_avg:58.39ms
step:1497/2330 train_time:87403ms step_avg:58.39ms
step:1498/2330 train_time:87463ms step_avg:58.39ms
step:1499/2330 train_time:87519ms step_avg:58.39ms
step:1500/2330 train_time:87580ms step_avg:58.39ms
step:1500/2330 val_loss:3.9348 train_time:87661ms step_avg:58.44ms
step:1501/2330 train_time:87681ms step_avg:58.41ms
step:1502/2330 train_time:87701ms step_avg:58.39ms
step:1503/2330 train_time:87761ms step_avg:58.39ms
step:1504/2330 train_time:87825ms step_avg:58.39ms
step:1505/2330 train_time:87883ms step_avg:58.39ms
step:1506/2330 train_time:87944ms step_avg:58.40ms
step:1507/2330 train_time:88000ms step_avg:58.39ms
step:1508/2330 train_time:88059ms step_avg:58.39ms
step:1509/2330 train_time:88117ms step_avg:58.39ms
step:1510/2330 train_time:88176ms step_avg:58.40ms
step:1511/2330 train_time:88233ms step_avg:58.39ms
step:1512/2330 train_time:88292ms step_avg:58.39ms
step:1513/2330 train_time:88348ms step_avg:58.39ms
step:1514/2330 train_time:88407ms step_avg:58.39ms
step:1515/2330 train_time:88463ms step_avg:58.39ms
step:1516/2330 train_time:88522ms step_avg:58.39ms
step:1517/2330 train_time:88579ms step_avg:58.39ms
step:1518/2330 train_time:88639ms step_avg:58.39ms
step:1519/2330 train_time:88697ms step_avg:58.39ms
step:1520/2330 train_time:88758ms step_avg:58.39ms
step:1521/2330 train_time:88816ms step_avg:58.39ms
step:1522/2330 train_time:88878ms step_avg:58.40ms
step:1523/2330 train_time:88934ms step_avg:58.39ms
step:1524/2330 train_time:88995ms step_avg:58.40ms
step:1525/2330 train_time:89051ms step_avg:58.39ms
step:1526/2330 train_time:89111ms step_avg:58.40ms
step:1527/2330 train_time:89167ms step_avg:58.39ms
step:1528/2330 train_time:89228ms step_avg:58.40ms
step:1529/2330 train_time:89286ms step_avg:58.40ms
step:1530/2330 train_time:89345ms step_avg:58.40ms
step:1531/2330 train_time:89402ms step_avg:58.39ms
step:1532/2330 train_time:89461ms step_avg:58.40ms
step:1533/2330 train_time:89518ms step_avg:58.39ms
step:1534/2330 train_time:89578ms step_avg:58.40ms
step:1535/2330 train_time:89636ms step_avg:58.40ms
step:1536/2330 train_time:89696ms step_avg:58.40ms
step:1537/2330 train_time:89754ms step_avg:58.40ms
step:1538/2330 train_time:89815ms step_avg:58.40ms
step:1539/2330 train_time:89872ms step_avg:58.40ms
step:1540/2330 train_time:89935ms step_avg:58.40ms
step:1541/2330 train_time:89994ms step_avg:58.40ms
step:1542/2330 train_time:90053ms step_avg:58.40ms
step:1543/2330 train_time:90110ms step_avg:58.40ms
step:1544/2330 train_time:90170ms step_avg:58.40ms
step:1545/2330 train_time:90227ms step_avg:58.40ms
step:1546/2330 train_time:90287ms step_avg:58.40ms
step:1547/2330 train_time:90344ms step_avg:58.40ms
step:1548/2330 train_time:90404ms step_avg:58.40ms
step:1549/2330 train_time:90461ms step_avg:58.40ms
step:1550/2330 train_time:90522ms step_avg:58.40ms
step:1551/2330 train_time:90580ms step_avg:58.40ms
step:1552/2330 train_time:90640ms step_avg:58.40ms
step:1553/2330 train_time:90698ms step_avg:58.40ms
step:1554/2330 train_time:90760ms step_avg:58.40ms
step:1555/2330 train_time:90818ms step_avg:58.40ms
step:1556/2330 train_time:90879ms step_avg:58.41ms
step:1557/2330 train_time:90937ms step_avg:58.41ms
step:1558/2330 train_time:90998ms step_avg:58.41ms
step:1559/2330 train_time:91057ms step_avg:58.41ms
step:1560/2330 train_time:91117ms step_avg:58.41ms
step:1561/2330 train_time:91175ms step_avg:58.41ms
step:1562/2330 train_time:91236ms step_avg:58.41ms
step:1563/2330 train_time:91293ms step_avg:58.41ms
step:1564/2330 train_time:91353ms step_avg:58.41ms
step:1565/2330 train_time:91410ms step_avg:58.41ms
step:1566/2330 train_time:91471ms step_avg:58.41ms
step:1567/2330 train_time:91527ms step_avg:58.41ms
step:1568/2330 train_time:91589ms step_avg:58.41ms
step:1569/2330 train_time:91646ms step_avg:58.41ms
step:1570/2330 train_time:91707ms step_avg:58.41ms
step:1571/2330 train_time:91764ms step_avg:58.41ms
step:1572/2330 train_time:91827ms step_avg:58.41ms
step:1573/2330 train_time:91885ms step_avg:58.41ms
step:1574/2330 train_time:91946ms step_avg:58.42ms
step:1575/2330 train_time:92003ms step_avg:58.41ms
step:1576/2330 train_time:92065ms step_avg:58.42ms
step:1577/2330 train_time:92122ms step_avg:58.42ms
step:1578/2330 train_time:92184ms step_avg:58.42ms
step:1579/2330 train_time:92242ms step_avg:58.42ms
step:1580/2330 train_time:92301ms step_avg:58.42ms
step:1581/2330 train_time:92359ms step_avg:58.42ms
step:1582/2330 train_time:92419ms step_avg:58.42ms
step:1583/2330 train_time:92476ms step_avg:58.42ms
step:1584/2330 train_time:92537ms step_avg:58.42ms
step:1585/2330 train_time:92595ms step_avg:58.42ms
step:1586/2330 train_time:92655ms step_avg:58.42ms
step:1587/2330 train_time:92711ms step_avg:58.42ms
step:1588/2330 train_time:92773ms step_avg:58.42ms
step:1589/2330 train_time:92830ms step_avg:58.42ms
step:1590/2330 train_time:92891ms step_avg:58.42ms
step:1591/2330 train_time:92948ms step_avg:58.42ms
step:1592/2330 train_time:93010ms step_avg:58.42ms
step:1593/2330 train_time:93067ms step_avg:58.42ms
step:1594/2330 train_time:93129ms step_avg:58.42ms
step:1595/2330 train_time:93186ms step_avg:58.42ms
step:1596/2330 train_time:93247ms step_avg:58.43ms
step:1597/2330 train_time:93303ms step_avg:58.42ms
step:1598/2330 train_time:93366ms step_avg:58.43ms
step:1599/2330 train_time:93422ms step_avg:58.43ms
step:1600/2330 train_time:93485ms step_avg:58.43ms
step:1601/2330 train_time:93541ms step_avg:58.43ms
step:1602/2330 train_time:93602ms step_avg:58.43ms
step:1603/2330 train_time:93660ms step_avg:58.43ms
step:1604/2330 train_time:93720ms step_avg:58.43ms
step:1605/2330 train_time:93778ms step_avg:58.43ms
step:1606/2330 train_time:93838ms step_avg:58.43ms
step:1607/2330 train_time:93896ms step_avg:58.43ms
step:1608/2330 train_time:93955ms step_avg:58.43ms
step:1609/2330 train_time:94012ms step_avg:58.43ms
step:1610/2330 train_time:94073ms step_avg:58.43ms
step:1611/2330 train_time:94130ms step_avg:58.43ms
step:1612/2330 train_time:94191ms step_avg:58.43ms
step:1613/2330 train_time:94247ms step_avg:58.43ms
step:1614/2330 train_time:94310ms step_avg:58.43ms
step:1615/2330 train_time:94366ms step_avg:58.43ms
step:1616/2330 train_time:94428ms step_avg:58.43ms
step:1617/2330 train_time:94484ms step_avg:58.43ms
step:1618/2330 train_time:94546ms step_avg:58.43ms
step:1619/2330 train_time:94603ms step_avg:58.43ms
step:1620/2330 train_time:94665ms step_avg:58.44ms
step:1621/2330 train_time:94722ms step_avg:58.43ms
step:1622/2330 train_time:94785ms step_avg:58.44ms
step:1623/2330 train_time:94842ms step_avg:58.44ms
step:1624/2330 train_time:94904ms step_avg:58.44ms
step:1625/2330 train_time:94962ms step_avg:58.44ms
step:1626/2330 train_time:95022ms step_avg:58.44ms
step:1627/2330 train_time:95081ms step_avg:58.44ms
step:1628/2330 train_time:95141ms step_avg:58.44ms
step:1629/2330 train_time:95200ms step_avg:58.44ms
step:1630/2330 train_time:95260ms step_avg:58.44ms
step:1631/2330 train_time:95317ms step_avg:58.44ms
step:1632/2330 train_time:95377ms step_avg:58.44ms
step:1633/2330 train_time:95435ms step_avg:58.44ms
step:1634/2330 train_time:95496ms step_avg:58.44ms
step:1635/2330 train_time:95552ms step_avg:58.44ms
step:1636/2330 train_time:95613ms step_avg:58.44ms
step:1637/2330 train_time:95670ms step_avg:58.44ms
step:1638/2330 train_time:95732ms step_avg:58.44ms
step:1639/2330 train_time:95788ms step_avg:58.44ms
step:1640/2330 train_time:95850ms step_avg:58.45ms
step:1641/2330 train_time:95906ms step_avg:58.44ms
step:1642/2330 train_time:95968ms step_avg:58.45ms
step:1643/2330 train_time:96025ms step_avg:58.44ms
step:1644/2330 train_time:96087ms step_avg:58.45ms
step:1645/2330 train_time:96144ms step_avg:58.45ms
step:1646/2330 train_time:96206ms step_avg:58.45ms
step:1647/2330 train_time:96263ms step_avg:58.45ms
step:1648/2330 train_time:96325ms step_avg:58.45ms
step:1649/2330 train_time:96382ms step_avg:58.45ms
step:1650/2330 train_time:96443ms step_avg:58.45ms
step:1651/2330 train_time:96501ms step_avg:58.45ms
step:1652/2330 train_time:96562ms step_avg:58.45ms
step:1653/2330 train_time:96620ms step_avg:58.45ms
step:1654/2330 train_time:96680ms step_avg:58.45ms
step:1655/2330 train_time:96739ms step_avg:58.45ms
step:1656/2330 train_time:96799ms step_avg:58.45ms
step:1657/2330 train_time:96857ms step_avg:58.45ms
step:1658/2330 train_time:96918ms step_avg:58.45ms
step:1659/2330 train_time:96976ms step_avg:58.45ms
step:1660/2330 train_time:97036ms step_avg:58.46ms
step:1661/2330 train_time:97093ms step_avg:58.45ms
step:1662/2330 train_time:97153ms step_avg:58.46ms
step:1663/2330 train_time:97210ms step_avg:58.45ms
step:1664/2330 train_time:97271ms step_avg:58.46ms
step:1665/2330 train_time:97328ms step_avg:58.46ms
step:1666/2330 train_time:97390ms step_avg:58.46ms
step:1667/2330 train_time:97446ms step_avg:58.46ms
step:1668/2330 train_time:97508ms step_avg:58.46ms
step:1669/2330 train_time:97564ms step_avg:58.46ms
step:1670/2330 train_time:97627ms step_avg:58.46ms
step:1671/2330 train_time:97683ms step_avg:58.46ms
step:1672/2330 train_time:97746ms step_avg:58.46ms
step:1673/2330 train_time:97803ms step_avg:58.46ms
step:1674/2330 train_time:97865ms step_avg:58.46ms
step:1675/2330 train_time:97922ms step_avg:58.46ms
step:1676/2330 train_time:97984ms step_avg:58.46ms
step:1677/2330 train_time:98041ms step_avg:58.46ms
step:1678/2330 train_time:98104ms step_avg:58.46ms
step:1679/2330 train_time:98162ms step_avg:58.46ms
step:1680/2330 train_time:98222ms step_avg:58.47ms
step:1681/2330 train_time:98280ms step_avg:58.47ms
step:1682/2330 train_time:98340ms step_avg:58.47ms
step:1683/2330 train_time:98398ms step_avg:58.47ms
step:1684/2330 train_time:98458ms step_avg:58.47ms
step:1685/2330 train_time:98515ms step_avg:58.47ms
step:1686/2330 train_time:98575ms step_avg:58.47ms
step:1687/2330 train_time:98632ms step_avg:58.47ms
step:1688/2330 train_time:98694ms step_avg:58.47ms
step:1689/2330 train_time:98751ms step_avg:58.47ms
step:1690/2330 train_time:98811ms step_avg:58.47ms
step:1691/2330 train_time:98868ms step_avg:58.47ms
step:1692/2330 train_time:98930ms step_avg:58.47ms
step:1693/2330 train_time:98986ms step_avg:58.47ms
step:1694/2330 train_time:99049ms step_avg:58.47ms
step:1695/2330 train_time:99105ms step_avg:58.47ms
step:1696/2330 train_time:99168ms step_avg:58.47ms
step:1697/2330 train_time:99224ms step_avg:58.47ms
step:1698/2330 train_time:99286ms step_avg:58.47ms
step:1699/2330 train_time:99342ms step_avg:58.47ms
step:1700/2330 train_time:99405ms step_avg:58.47ms
step:1701/2330 train_time:99462ms step_avg:58.47ms
step:1702/2330 train_time:99524ms step_avg:58.47ms
step:1703/2330 train_time:99581ms step_avg:58.47ms
step:1704/2330 train_time:99642ms step_avg:58.48ms
step:1705/2330 train_time:99700ms step_avg:58.48ms
step:1706/2330 train_time:99760ms step_avg:58.48ms
step:1707/2330 train_time:99818ms step_avg:58.48ms
step:1708/2330 train_time:99878ms step_avg:58.48ms
step:1709/2330 train_time:99935ms step_avg:58.48ms
step:1710/2330 train_time:99996ms step_avg:58.48ms
step:1711/2330 train_time:100054ms step_avg:58.48ms
step:1712/2330 train_time:100114ms step_avg:58.48ms
step:1713/2330 train_time:100172ms step_avg:58.48ms
step:1714/2330 train_time:100232ms step_avg:58.48ms
step:1715/2330 train_time:100289ms step_avg:58.48ms
step:1716/2330 train_time:100351ms step_avg:58.48ms
step:1717/2330 train_time:100407ms step_avg:58.48ms
step:1718/2330 train_time:100469ms step_avg:58.48ms
step:1719/2330 train_time:100525ms step_avg:58.48ms
step:1720/2330 train_time:100588ms step_avg:58.48ms
step:1721/2330 train_time:100644ms step_avg:58.48ms
step:1722/2330 train_time:100706ms step_avg:58.48ms
step:1723/2330 train_time:100763ms step_avg:58.48ms
step:1724/2330 train_time:100825ms step_avg:58.48ms
step:1725/2330 train_time:100882ms step_avg:58.48ms
step:1726/2330 train_time:100944ms step_avg:58.48ms
step:1727/2330 train_time:101001ms step_avg:58.48ms
step:1728/2330 train_time:101062ms step_avg:58.49ms
step:1729/2330 train_time:101119ms step_avg:58.48ms
step:1730/2330 train_time:101180ms step_avg:58.49ms
step:1731/2330 train_time:101239ms step_avg:58.49ms
step:1732/2330 train_time:101299ms step_avg:58.49ms
step:1733/2330 train_time:101356ms step_avg:58.49ms
step:1734/2330 train_time:101416ms step_avg:58.49ms
step:1735/2330 train_time:101473ms step_avg:58.49ms
step:1736/2330 train_time:101533ms step_avg:58.49ms
step:1737/2330 train_time:101591ms step_avg:58.49ms
step:1738/2330 train_time:101651ms step_avg:58.49ms
step:1739/2330 train_time:101707ms step_avg:58.49ms
step:1740/2330 train_time:101769ms step_avg:58.49ms
step:1741/2330 train_time:101826ms step_avg:58.49ms
step:1742/2330 train_time:101888ms step_avg:58.49ms
step:1743/2330 train_time:101944ms step_avg:58.49ms
step:1744/2330 train_time:102006ms step_avg:58.49ms
step:1745/2330 train_time:102063ms step_avg:58.49ms
step:1746/2330 train_time:102125ms step_avg:58.49ms
step:1747/2330 train_time:102183ms step_avg:58.49ms
step:1748/2330 train_time:102244ms step_avg:58.49ms
step:1749/2330 train_time:102302ms step_avg:58.49ms
step:1750/2330 train_time:102362ms step_avg:58.49ms
step:1750/2330 val_loss:3.8499 train_time:102444ms step_avg:58.54ms
step:1751/2330 train_time:102462ms step_avg:58.52ms
step:1752/2330 train_time:102482ms step_avg:58.49ms
step:1753/2330 train_time:102542ms step_avg:58.50ms
step:1754/2330 train_time:102607ms step_avg:58.50ms
step:1755/2330 train_time:102665ms step_avg:58.50ms
step:1756/2330 train_time:102726ms step_avg:58.50ms
step:1757/2330 train_time:102784ms step_avg:58.50ms
step:1758/2330 train_time:102844ms step_avg:58.50ms
step:1759/2330 train_time:102901ms step_avg:58.50ms
step:1760/2330 train_time:102961ms step_avg:58.50ms
step:1761/2330 train_time:103017ms step_avg:58.50ms
step:1762/2330 train_time:103077ms step_avg:58.50ms
step:1763/2330 train_time:103133ms step_avg:58.50ms
step:1764/2330 train_time:103193ms step_avg:58.50ms
step:1765/2330 train_time:103249ms step_avg:58.50ms
step:1766/2330 train_time:103309ms step_avg:58.50ms
step:1767/2330 train_time:103366ms step_avg:58.50ms
step:1768/2330 train_time:103428ms step_avg:58.50ms
step:1769/2330 train_time:103487ms step_avg:58.50ms
step:1770/2330 train_time:103551ms step_avg:58.50ms
step:1771/2330 train_time:103609ms step_avg:58.50ms
step:1772/2330 train_time:103672ms step_avg:58.51ms
step:1773/2330 train_time:103728ms step_avg:58.50ms
step:1774/2330 train_time:103791ms step_avg:58.51ms
step:1775/2330 train_time:103847ms step_avg:58.51ms
step:1776/2330 train_time:103909ms step_avg:58.51ms
step:1777/2330 train_time:103966ms step_avg:58.51ms
step:1778/2330 train_time:104026ms step_avg:58.51ms
step:1779/2330 train_time:104084ms step_avg:58.51ms
step:1780/2330 train_time:104144ms step_avg:58.51ms
step:1781/2330 train_time:104201ms step_avg:58.51ms
step:1782/2330 train_time:104260ms step_avg:58.51ms
step:1783/2330 train_time:104318ms step_avg:58.51ms
step:1784/2330 train_time:104377ms step_avg:58.51ms
step:1785/2330 train_time:104435ms step_avg:58.51ms
step:1786/2330 train_time:104496ms step_avg:58.51ms
step:1787/2330 train_time:104554ms step_avg:58.51ms
step:1788/2330 train_time:104616ms step_avg:58.51ms
step:1789/2330 train_time:104673ms step_avg:58.51ms
step:1790/2330 train_time:104734ms step_avg:58.51ms
step:1791/2330 train_time:104792ms step_avg:58.51ms
step:1792/2330 train_time:104853ms step_avg:58.51ms
step:1793/2330 train_time:104911ms step_avg:58.51ms
step:1794/2330 train_time:104972ms step_avg:58.51ms
step:1795/2330 train_time:105028ms step_avg:58.51ms
step:1796/2330 train_time:105090ms step_avg:58.51ms
step:1797/2330 train_time:105146ms step_avg:58.51ms
step:1798/2330 train_time:105208ms step_avg:58.51ms
step:1799/2330 train_time:105264ms step_avg:58.51ms
step:1800/2330 train_time:105325ms step_avg:58.51ms
step:1801/2330 train_time:105383ms step_avg:58.51ms
step:1802/2330 train_time:105444ms step_avg:58.52ms
step:1803/2330 train_time:105503ms step_avg:58.52ms
step:1804/2330 train_time:105564ms step_avg:58.52ms
step:1805/2330 train_time:105622ms step_avg:58.52ms
step:1806/2330 train_time:105682ms step_avg:58.52ms
step:1807/2330 train_time:105740ms step_avg:58.52ms
step:1808/2330 train_time:105801ms step_avg:58.52ms
step:1809/2330 train_time:105859ms step_avg:58.52ms
step:1810/2330 train_time:105920ms step_avg:58.52ms
step:1811/2330 train_time:105979ms step_avg:58.52ms
step:1812/2330 train_time:106038ms step_avg:58.52ms
step:1813/2330 train_time:106095ms step_avg:58.52ms
step:1814/2330 train_time:106156ms step_avg:58.52ms
step:1815/2330 train_time:106213ms step_avg:58.52ms
step:1816/2330 train_time:106274ms step_avg:58.52ms
step:1817/2330 train_time:106330ms step_avg:58.52ms
step:1818/2330 train_time:106392ms step_avg:58.52ms
step:1819/2330 train_time:106449ms step_avg:58.52ms
step:1820/2330 train_time:106511ms step_avg:58.52ms
step:1821/2330 train_time:106568ms step_avg:58.52ms
step:1822/2330 train_time:106630ms step_avg:58.52ms
step:1823/2330 train_time:106688ms step_avg:58.52ms
step:1824/2330 train_time:106749ms step_avg:58.52ms
step:1825/2330 train_time:106806ms step_avg:58.52ms
step:1826/2330 train_time:106868ms step_avg:58.53ms
step:1827/2330 train_time:106925ms step_avg:58.53ms
step:1828/2330 train_time:106988ms step_avg:58.53ms
step:1829/2330 train_time:107045ms step_avg:58.53ms
step:1830/2330 train_time:107106ms step_avg:58.53ms
step:1831/2330 train_time:107164ms step_avg:58.53ms
step:1832/2330 train_time:107224ms step_avg:58.53ms
step:1833/2330 train_time:107282ms step_avg:58.53ms
step:1834/2330 train_time:107341ms step_avg:58.53ms
step:1835/2330 train_time:107399ms step_avg:58.53ms
step:1836/2330 train_time:107460ms step_avg:58.53ms
step:1837/2330 train_time:107517ms step_avg:58.53ms
step:1838/2330 train_time:107577ms step_avg:58.53ms
step:1839/2330 train_time:107635ms step_avg:58.53ms
step:1840/2330 train_time:107696ms step_avg:58.53ms
step:1841/2330 train_time:107754ms step_avg:58.53ms
step:1842/2330 train_time:107814ms step_avg:58.53ms
step:1843/2330 train_time:107872ms step_avg:58.53ms
step:1844/2330 train_time:107932ms step_avg:58.53ms
step:1845/2330 train_time:107989ms step_avg:58.53ms
step:1846/2330 train_time:108051ms step_avg:58.53ms
step:1847/2330 train_time:108108ms step_avg:58.53ms
step:1848/2330 train_time:108169ms step_avg:58.53ms
step:1849/2330 train_time:108225ms step_avg:58.53ms
step:1850/2330 train_time:108287ms step_avg:58.53ms
step:1851/2330 train_time:108345ms step_avg:58.53ms
step:1852/2330 train_time:108406ms step_avg:58.53ms
step:1853/2330 train_time:108464ms step_avg:58.53ms
step:1854/2330 train_time:108525ms step_avg:58.54ms
step:1855/2330 train_time:108582ms step_avg:58.53ms
step:1856/2330 train_time:108643ms step_avg:58.54ms
step:1857/2330 train_time:108702ms step_avg:58.54ms
step:1858/2330 train_time:108762ms step_avg:58.54ms
step:1859/2330 train_time:108820ms step_avg:58.54ms
step:1860/2330 train_time:108879ms step_avg:58.54ms
step:1861/2330 train_time:108936ms step_avg:58.54ms
step:1862/2330 train_time:108997ms step_avg:58.54ms
step:1863/2330 train_time:109054ms step_avg:58.54ms
step:1864/2330 train_time:109114ms step_avg:58.54ms
step:1865/2330 train_time:109170ms step_avg:58.54ms
step:1866/2330 train_time:109233ms step_avg:58.54ms
step:1867/2330 train_time:109289ms step_avg:58.54ms
step:1868/2330 train_time:109351ms step_avg:58.54ms
step:1869/2330 train_time:109408ms step_avg:58.54ms
step:1870/2330 train_time:109469ms step_avg:58.54ms
step:1871/2330 train_time:109526ms step_avg:58.54ms
step:1872/2330 train_time:109588ms step_avg:58.54ms
step:1873/2330 train_time:109646ms step_avg:58.54ms
step:1874/2330 train_time:109708ms step_avg:58.54ms
step:1875/2330 train_time:109765ms step_avg:58.54ms
step:1876/2330 train_time:109827ms step_avg:58.54ms
step:1877/2330 train_time:109884ms step_avg:58.54ms
step:1878/2330 train_time:109945ms step_avg:58.54ms
step:1879/2330 train_time:110004ms step_avg:58.54ms
step:1880/2330 train_time:110065ms step_avg:58.55ms
step:1881/2330 train_time:110122ms step_avg:58.54ms
step:1882/2330 train_time:110182ms step_avg:58.54ms
step:1883/2330 train_time:110239ms step_avg:58.54ms
step:1884/2330 train_time:110299ms step_avg:58.55ms
step:1885/2330 train_time:110357ms step_avg:58.54ms
step:1886/2330 train_time:110416ms step_avg:58.55ms
step:1887/2330 train_time:110474ms step_avg:58.54ms
step:1888/2330 train_time:110536ms step_avg:58.55ms
step:1889/2330 train_time:110593ms step_avg:58.55ms
step:1890/2330 train_time:110655ms step_avg:58.55ms
step:1891/2330 train_time:110712ms step_avg:58.55ms
step:1892/2330 train_time:110774ms step_avg:58.55ms
step:1893/2330 train_time:110830ms step_avg:58.55ms
step:1894/2330 train_time:110894ms step_avg:58.55ms
step:1895/2330 train_time:110950ms step_avg:58.55ms
step:1896/2330 train_time:111013ms step_avg:58.55ms
step:1897/2330 train_time:111069ms step_avg:58.55ms
step:1898/2330 train_time:111131ms step_avg:58.55ms
step:1899/2330 train_time:111187ms step_avg:58.55ms
step:1900/2330 train_time:111249ms step_avg:58.55ms
step:1901/2330 train_time:111306ms step_avg:58.55ms
step:1902/2330 train_time:111368ms step_avg:58.55ms
step:1903/2330 train_time:111425ms step_avg:58.55ms
step:1904/2330 train_time:111486ms step_avg:58.55ms
step:1905/2330 train_time:111544ms step_avg:58.55ms
step:1906/2330 train_time:111605ms step_avg:58.55ms
step:1907/2330 train_time:111663ms step_avg:58.55ms
step:1908/2330 train_time:111724ms step_avg:58.56ms
step:1909/2330 train_time:111782ms step_avg:58.56ms
step:1910/2330 train_time:111842ms step_avg:58.56ms
step:1911/2330 train_time:111900ms step_avg:58.56ms
step:1912/2330 train_time:111960ms step_avg:58.56ms
step:1913/2330 train_time:112017ms step_avg:58.56ms
step:1914/2330 train_time:112078ms step_avg:58.56ms
step:1915/2330 train_time:112135ms step_avg:58.56ms
step:1916/2330 train_time:112196ms step_avg:58.56ms
step:1917/2330 train_time:112253ms step_avg:58.56ms
step:1918/2330 train_time:112313ms step_avg:58.56ms
step:1919/2330 train_time:112370ms step_avg:58.56ms
step:1920/2330 train_time:112432ms step_avg:58.56ms
step:1921/2330 train_time:112489ms step_avg:58.56ms
step:1922/2330 train_time:112550ms step_avg:58.56ms
step:1923/2330 train_time:112608ms step_avg:58.56ms
step:1924/2330 train_time:112669ms step_avg:58.56ms
step:1925/2330 train_time:112726ms step_avg:58.56ms
step:1926/2330 train_time:112789ms step_avg:58.56ms
step:1927/2330 train_time:112846ms step_avg:58.56ms
step:1928/2330 train_time:112908ms step_avg:58.56ms
step:1929/2330 train_time:112966ms step_avg:58.56ms
step:1930/2330 train_time:113027ms step_avg:58.56ms
step:1931/2330 train_time:113086ms step_avg:58.56ms
step:1932/2330 train_time:113146ms step_avg:58.56ms
step:1933/2330 train_time:113203ms step_avg:58.56ms
step:1934/2330 train_time:113264ms step_avg:58.56ms
step:1935/2330 train_time:113321ms step_avg:58.56ms
step:1936/2330 train_time:113381ms step_avg:58.56ms
step:1937/2330 train_time:113439ms step_avg:58.56ms
step:1938/2330 train_time:113499ms step_avg:58.56ms
step:1939/2330 train_time:113557ms step_avg:58.56ms
step:1940/2330 train_time:113617ms step_avg:58.57ms
step:1941/2330 train_time:113674ms step_avg:58.56ms
step:1942/2330 train_time:113735ms step_avg:58.57ms
step:1943/2330 train_time:113792ms step_avg:58.57ms
step:1944/2330 train_time:113854ms step_avg:58.57ms
step:1945/2330 train_time:113910ms step_avg:58.57ms
step:1946/2330 train_time:113972ms step_avg:58.57ms
step:1947/2330 train_time:114029ms step_avg:58.57ms
step:1948/2330 train_time:114090ms step_avg:58.57ms
step:1949/2330 train_time:114147ms step_avg:58.57ms
step:1950/2330 train_time:114208ms step_avg:58.57ms
step:1951/2330 train_time:114265ms step_avg:58.57ms
step:1952/2330 train_time:114327ms step_avg:58.57ms
step:1953/2330 train_time:114383ms step_avg:58.57ms
step:1954/2330 train_time:114445ms step_avg:58.57ms
step:1955/2330 train_time:114502ms step_avg:58.57ms
step:1956/2330 train_time:114563ms step_avg:58.57ms
step:1957/2330 train_time:114620ms step_avg:58.57ms
step:1958/2330 train_time:114681ms step_avg:58.57ms
step:1959/2330 train_time:114739ms step_avg:58.57ms
step:1960/2330 train_time:114800ms step_avg:58.57ms
step:1961/2330 train_time:114857ms step_avg:58.57ms
step:1962/2330 train_time:114917ms step_avg:58.57ms
step:1963/2330 train_time:114974ms step_avg:58.57ms
step:1964/2330 train_time:115035ms step_avg:58.57ms
step:1965/2330 train_time:115091ms step_avg:58.57ms
step:1966/2330 train_time:115153ms step_avg:58.57ms
step:1967/2330 train_time:115210ms step_avg:58.57ms
step:1968/2330 train_time:115272ms step_avg:58.57ms
step:1969/2330 train_time:115328ms step_avg:58.57ms
step:1970/2330 train_time:115391ms step_avg:58.57ms
step:1971/2330 train_time:115448ms step_avg:58.57ms
step:1972/2330 train_time:115510ms step_avg:58.58ms
step:1973/2330 train_time:115567ms step_avg:58.57ms
step:1974/2330 train_time:115629ms step_avg:58.58ms
step:1975/2330 train_time:115686ms step_avg:58.58ms
step:1976/2330 train_time:115749ms step_avg:58.58ms
step:1977/2330 train_time:115807ms step_avg:58.58ms
step:1978/2330 train_time:115868ms step_avg:58.58ms
step:1979/2330 train_time:115926ms step_avg:58.58ms
step:1980/2330 train_time:115986ms step_avg:58.58ms
step:1981/2330 train_time:116043ms step_avg:58.58ms
step:1982/2330 train_time:116104ms step_avg:58.58ms
step:1983/2330 train_time:116162ms step_avg:58.58ms
step:1984/2330 train_time:116222ms step_avg:58.58ms
step:1985/2330 train_time:116280ms step_avg:58.58ms
step:1986/2330 train_time:116340ms step_avg:58.58ms
step:1987/2330 train_time:116398ms step_avg:58.58ms
step:1988/2330 train_time:116458ms step_avg:58.58ms
step:1989/2330 train_time:116515ms step_avg:58.58ms
step:1990/2330 train_time:116575ms step_avg:58.58ms
step:1991/2330 train_time:116632ms step_avg:58.58ms
step:1992/2330 train_time:116695ms step_avg:58.58ms
step:1993/2330 train_time:116751ms step_avg:58.58ms
step:1994/2330 train_time:116814ms step_avg:58.58ms
step:1995/2330 train_time:116870ms step_avg:58.58ms
step:1996/2330 train_time:116933ms step_avg:58.58ms
step:1997/2330 train_time:116990ms step_avg:58.58ms
step:1998/2330 train_time:117051ms step_avg:58.58ms
step:1999/2330 train_time:117108ms step_avg:58.58ms
step:2000/2330 train_time:117170ms step_avg:58.59ms
step:2000/2330 val_loss:3.7833 train_time:117254ms step_avg:58.63ms
step:2001/2330 train_time:117273ms step_avg:58.61ms
step:2002/2330 train_time:117294ms step_avg:58.59ms
step:2003/2330 train_time:117352ms step_avg:58.59ms
step:2004/2330 train_time:117419ms step_avg:58.59ms
step:2005/2330 train_time:117477ms step_avg:58.59ms
step:2006/2330 train_time:117538ms step_avg:58.59ms
step:2007/2330 train_time:117595ms step_avg:58.59ms
step:2008/2330 train_time:117655ms step_avg:58.59ms
step:2009/2330 train_time:117711ms step_avg:58.59ms
step:2010/2330 train_time:117771ms step_avg:58.59ms
step:2011/2330 train_time:117828ms step_avg:58.59ms
step:2012/2330 train_time:117888ms step_avg:58.59ms
step:2013/2330 train_time:117945ms step_avg:58.59ms
step:2014/2330 train_time:118005ms step_avg:58.59ms
step:2015/2330 train_time:118061ms step_avg:58.59ms
step:2016/2330 train_time:118120ms step_avg:58.59ms
step:2017/2330 train_time:118177ms step_avg:58.59ms
step:2018/2330 train_time:118239ms step_avg:58.59ms
step:2019/2330 train_time:118298ms step_avg:58.59ms
step:2020/2330 train_time:118360ms step_avg:58.59ms
step:2021/2330 train_time:118419ms step_avg:58.59ms
step:2022/2330 train_time:118480ms step_avg:58.60ms
step:2023/2330 train_time:118537ms step_avg:58.59ms
step:2024/2330 train_time:118599ms step_avg:58.60ms
step:2025/2330 train_time:118656ms step_avg:58.60ms
step:2026/2330 train_time:118716ms step_avg:58.60ms
step:2027/2330 train_time:118773ms step_avg:58.60ms
step:2028/2330 train_time:118832ms step_avg:58.60ms
step:2029/2330 train_time:118889ms step_avg:58.59ms
step:2030/2330 train_time:118949ms step_avg:58.60ms
step:2031/2330 train_time:119005ms step_avg:58.59ms
step:2032/2330 train_time:119065ms step_avg:58.60ms
step:2033/2330 train_time:119123ms step_avg:58.59ms
step:2034/2330 train_time:119183ms step_avg:58.60ms
step:2035/2330 train_time:119241ms step_avg:58.60ms
step:2036/2330 train_time:119302ms step_avg:58.60ms
step:2037/2330 train_time:119360ms step_avg:58.60ms
step:2038/2330 train_time:119420ms step_avg:58.60ms
step:2039/2330 train_time:119479ms step_avg:58.60ms
step:2040/2330 train_time:119540ms step_avg:58.60ms
step:2041/2330 train_time:119598ms step_avg:58.60ms
step:2042/2330 train_time:119658ms step_avg:58.60ms
step:2043/2330 train_time:119716ms step_avg:58.60ms
step:2044/2330 train_time:119776ms step_avg:58.60ms
step:2045/2330 train_time:119833ms step_avg:58.60ms
step:2046/2330 train_time:119895ms step_avg:58.60ms
step:2047/2330 train_time:119951ms step_avg:58.60ms
step:2048/2330 train_time:120013ms step_avg:58.60ms
step:2049/2330 train_time:120069ms step_avg:58.60ms
step:2050/2330 train_time:120130ms step_avg:58.60ms
step:2051/2330 train_time:120187ms step_avg:58.60ms
step:2052/2330 train_time:120249ms step_avg:58.60ms
step:2053/2330 train_time:120307ms step_avg:58.60ms
step:2054/2330 train_time:120369ms step_avg:58.60ms
step:2055/2330 train_time:120427ms step_avg:58.60ms
step:2056/2330 train_time:120490ms step_avg:58.60ms
step:2057/2330 train_time:120547ms step_avg:58.60ms
step:2058/2330 train_time:120608ms step_avg:58.60ms
step:2059/2330 train_time:120665ms step_avg:58.60ms
step:2060/2330 train_time:120726ms step_avg:58.60ms
step:2061/2330 train_time:120783ms step_avg:58.60ms
step:2062/2330 train_time:120844ms step_avg:58.61ms
step:2063/2330 train_time:120901ms step_avg:58.60ms
step:2064/2330 train_time:120962ms step_avg:58.61ms
step:2065/2330 train_time:121019ms step_avg:58.61ms
step:2066/2330 train_time:121079ms step_avg:58.61ms
step:2067/2330 train_time:121136ms step_avg:58.60ms
step:2068/2330 train_time:121197ms step_avg:58.61ms
step:2069/2330 train_time:121254ms step_avg:58.61ms
step:2070/2330 train_time:121316ms step_avg:58.61ms
step:2071/2330 train_time:121372ms step_avg:58.61ms
step:2072/2330 train_time:121435ms step_avg:58.61ms
step:2073/2330 train_time:121492ms step_avg:58.61ms
step:2074/2330 train_time:121554ms step_avg:58.61ms
step:2075/2330 train_time:121611ms step_avg:58.61ms
step:2076/2330 train_time:121673ms step_avg:58.61ms
step:2077/2330 train_time:121730ms step_avg:58.61ms
step:2078/2330 train_time:121791ms step_avg:58.61ms
step:2079/2330 train_time:121847ms step_avg:58.61ms
step:2080/2330 train_time:121909ms step_avg:58.61ms
step:2081/2330 train_time:121966ms step_avg:58.61ms
step:2082/2330 train_time:122027ms step_avg:58.61ms
step:2083/2330 train_time:122085ms step_avg:58.61ms
step:2084/2330 train_time:122145ms step_avg:58.61ms
step:2085/2330 train_time:122203ms step_avg:58.61ms
step:2086/2330 train_time:122264ms step_avg:58.61ms
step:2087/2330 train_time:122323ms step_avg:58.61ms
step:2088/2330 train_time:122383ms step_avg:58.61ms
step:2089/2330 train_time:122442ms step_avg:58.61ms
step:2090/2330 train_time:122502ms step_avg:58.61ms
step:2091/2330 train_time:122560ms step_avg:58.61ms
step:2092/2330 train_time:122620ms step_avg:58.61ms
step:2093/2330 train_time:122677ms step_avg:58.61ms
step:2094/2330 train_time:122738ms step_avg:58.61ms
step:2095/2330 train_time:122794ms step_avg:58.61ms
step:2096/2330 train_time:122855ms step_avg:58.61ms
step:2097/2330 train_time:122912ms step_avg:58.61ms
step:2098/2330 train_time:122973ms step_avg:58.61ms
step:2099/2330 train_time:123030ms step_avg:58.61ms
step:2100/2330 train_time:123091ms step_avg:58.61ms
step:2101/2330 train_time:123148ms step_avg:58.61ms
step:2102/2330 train_time:123210ms step_avg:58.62ms
step:2103/2330 train_time:123268ms step_avg:58.62ms
step:2104/2330 train_time:123329ms step_avg:58.62ms
step:2105/2330 train_time:123387ms step_avg:58.62ms
step:2106/2330 train_time:123448ms step_avg:58.62ms
step:2107/2330 train_time:123505ms step_avg:58.62ms
step:2108/2330 train_time:123567ms step_avg:58.62ms
step:2109/2330 train_time:123625ms step_avg:58.62ms
step:2110/2330 train_time:123685ms step_avg:58.62ms
step:2111/2330 train_time:123743ms step_avg:58.62ms
step:2112/2330 train_time:123803ms step_avg:58.62ms
step:2113/2330 train_time:123862ms step_avg:58.62ms
step:2114/2330 train_time:123922ms step_avg:58.62ms
step:2115/2330 train_time:123980ms step_avg:58.62ms
step:2116/2330 train_time:124041ms step_avg:58.62ms
step:2117/2330 train_time:124098ms step_avg:58.62ms
step:2118/2330 train_time:124158ms step_avg:58.62ms
step:2119/2330 train_time:124215ms step_avg:58.62ms
step:2120/2330 train_time:124276ms step_avg:58.62ms
step:2121/2330 train_time:124334ms step_avg:58.62ms
step:2122/2330 train_time:124395ms step_avg:58.62ms
step:2123/2330 train_time:124452ms step_avg:58.62ms
step:2124/2330 train_time:124514ms step_avg:58.62ms
step:2125/2330 train_time:124571ms step_avg:58.62ms
step:2126/2330 train_time:124632ms step_avg:58.62ms
step:2127/2330 train_time:124688ms step_avg:58.62ms
step:2128/2330 train_time:124751ms step_avg:58.62ms
step:2129/2330 train_time:124808ms step_avg:58.62ms
step:2130/2330 train_time:124869ms step_avg:58.62ms
step:2131/2330 train_time:124927ms step_avg:58.62ms
step:2132/2330 train_time:124988ms step_avg:58.62ms
step:2133/2330 train_time:125045ms step_avg:58.62ms
step:2134/2330 train_time:125107ms step_avg:58.63ms
step:2135/2330 train_time:125164ms step_avg:58.62ms
step:2136/2330 train_time:125225ms step_avg:58.63ms
step:2137/2330 train_time:125284ms step_avg:58.63ms
step:2138/2330 train_time:125344ms step_avg:58.63ms
step:2139/2330 train_time:125401ms step_avg:58.63ms
step:2140/2330 train_time:125462ms step_avg:58.63ms
step:2141/2330 train_time:125520ms step_avg:58.63ms
step:2142/2330 train_time:125580ms step_avg:58.63ms
step:2143/2330 train_time:125637ms step_avg:58.63ms
step:2144/2330 train_time:125697ms step_avg:58.63ms
step:2145/2330 train_time:125754ms step_avg:58.63ms
step:2146/2330 train_time:125815ms step_avg:58.63ms
step:2147/2330 train_time:125872ms step_avg:58.63ms
step:2148/2330 train_time:125932ms step_avg:58.63ms
step:2149/2330 train_time:125989ms step_avg:58.63ms
step:2150/2330 train_time:126051ms step_avg:58.63ms
step:2151/2330 train_time:126108ms step_avg:58.63ms
step:2152/2330 train_time:126170ms step_avg:58.63ms
step:2153/2330 train_time:126228ms step_avg:58.63ms
step:2154/2330 train_time:126289ms step_avg:58.63ms
step:2155/2330 train_time:126347ms step_avg:58.63ms
step:2156/2330 train_time:126408ms step_avg:58.63ms
step:2157/2330 train_time:126465ms step_avg:58.63ms
step:2158/2330 train_time:126527ms step_avg:58.63ms
step:2159/2330 train_time:126584ms step_avg:58.63ms
step:2160/2330 train_time:126645ms step_avg:58.63ms
step:2161/2330 train_time:126703ms step_avg:58.63ms
step:2162/2330 train_time:126764ms step_avg:58.63ms
step:2163/2330 train_time:126821ms step_avg:58.63ms
step:2164/2330 train_time:126881ms step_avg:58.63ms
step:2165/2330 train_time:126939ms step_avg:58.63ms
step:2166/2330 train_time:126999ms step_avg:58.63ms
step:2167/2330 train_time:127057ms step_avg:58.63ms
step:2168/2330 train_time:127117ms step_avg:58.63ms
step:2169/2330 train_time:127175ms step_avg:58.63ms
step:2170/2330 train_time:127237ms step_avg:58.63ms
step:2171/2330 train_time:127294ms step_avg:58.63ms
step:2172/2330 train_time:127355ms step_avg:58.63ms
step:2173/2330 train_time:127411ms step_avg:58.63ms
step:2174/2330 train_time:127473ms step_avg:58.64ms
step:2175/2330 train_time:127530ms step_avg:58.63ms
step:2176/2330 train_time:127592ms step_avg:58.64ms
step:2177/2330 train_time:127648ms step_avg:58.63ms
step:2178/2330 train_time:127710ms step_avg:58.64ms
step:2179/2330 train_time:127767ms step_avg:58.64ms
step:2180/2330 train_time:127830ms step_avg:58.64ms
step:2181/2330 train_time:127887ms step_avg:58.64ms
step:2182/2330 train_time:127948ms step_avg:58.64ms
step:2183/2330 train_time:128005ms step_avg:58.64ms
step:2184/2330 train_time:128065ms step_avg:58.64ms
step:2185/2330 train_time:128123ms step_avg:58.64ms
step:2186/2330 train_time:128184ms step_avg:58.64ms
step:2187/2330 train_time:128243ms step_avg:58.64ms
step:2188/2330 train_time:128303ms step_avg:58.64ms
step:2189/2330 train_time:128360ms step_avg:58.64ms
step:2190/2330 train_time:128420ms step_avg:58.64ms
step:2191/2330 train_time:128478ms step_avg:58.64ms
step:2192/2330 train_time:128538ms step_avg:58.64ms
step:2193/2330 train_time:128595ms step_avg:58.64ms
step:2194/2330 train_time:128656ms step_avg:58.64ms
step:2195/2330 train_time:128713ms step_avg:58.64ms
step:2196/2330 train_time:128774ms step_avg:58.64ms
step:2197/2330 train_time:128831ms step_avg:58.64ms
step:2198/2330 train_time:128892ms step_avg:58.64ms
step:2199/2330 train_time:128948ms step_avg:58.64ms
step:2200/2330 train_time:129010ms step_avg:58.64ms
step:2201/2330 train_time:129067ms step_avg:58.64ms
step:2202/2330 train_time:129129ms step_avg:58.64ms
step:2203/2330 train_time:129186ms step_avg:58.64ms
step:2204/2330 train_time:129247ms step_avg:58.64ms
step:2205/2330 train_time:129305ms step_avg:58.64ms
step:2206/2330 train_time:129366ms step_avg:58.64ms
step:2207/2330 train_time:129424ms step_avg:58.64ms
step:2208/2330 train_time:129485ms step_avg:58.64ms
step:2209/2330 train_time:129543ms step_avg:58.64ms
step:2210/2330 train_time:129603ms step_avg:58.64ms
step:2211/2330 train_time:129662ms step_avg:58.64ms
step:2212/2330 train_time:129722ms step_avg:58.64ms
step:2213/2330 train_time:129780ms step_avg:58.64ms
step:2214/2330 train_time:129840ms step_avg:58.65ms
step:2215/2330 train_time:129898ms step_avg:58.64ms
step:2216/2330 train_time:129957ms step_avg:58.65ms
step:2217/2330 train_time:130014ms step_avg:58.64ms
step:2218/2330 train_time:130076ms step_avg:58.65ms
step:2219/2330 train_time:130132ms step_avg:58.64ms
step:2220/2330 train_time:130193ms step_avg:58.65ms
step:2221/2330 train_time:130249ms step_avg:58.64ms
step:2222/2330 train_time:130312ms step_avg:58.65ms
step:2223/2330 train_time:130369ms step_avg:58.65ms
step:2224/2330 train_time:130431ms step_avg:58.65ms
step:2225/2330 train_time:130488ms step_avg:58.65ms
step:2226/2330 train_time:130550ms step_avg:58.65ms
step:2227/2330 train_time:130607ms step_avg:58.65ms
step:2228/2330 train_time:130669ms step_avg:58.65ms
step:2229/2330 train_time:130727ms step_avg:58.65ms
step:2230/2330 train_time:130787ms step_avg:58.65ms
step:2231/2330 train_time:130846ms step_avg:58.65ms
step:2232/2330 train_time:130906ms step_avg:58.65ms
step:2233/2330 train_time:130963ms step_avg:58.65ms
step:2234/2330 train_time:131023ms step_avg:58.65ms
step:2235/2330 train_time:131080ms step_avg:58.65ms
step:2236/2330 train_time:131141ms step_avg:58.65ms
step:2237/2330 train_time:131199ms step_avg:58.65ms
step:2238/2330 train_time:131259ms step_avg:58.65ms
step:2239/2330 train_time:131316ms step_avg:58.65ms
step:2240/2330 train_time:131377ms step_avg:58.65ms
step:2241/2330 train_time:131433ms step_avg:58.65ms
step:2242/2330 train_time:131495ms step_avg:58.65ms
step:2243/2330 train_time:131552ms step_avg:58.65ms
step:2244/2330 train_time:131614ms step_avg:58.65ms
step:2245/2330 train_time:131670ms step_avg:58.65ms
step:2246/2330 train_time:131732ms step_avg:58.65ms
step:2247/2330 train_time:131788ms step_avg:58.65ms
step:2248/2330 train_time:131851ms step_avg:58.65ms
step:2249/2330 train_time:131907ms step_avg:58.65ms
step:2250/2330 train_time:131970ms step_avg:58.65ms
step:2250/2330 val_loss:3.7324 train_time:132052ms step_avg:58.69ms
step:2251/2330 train_time:132071ms step_avg:58.67ms
step:2252/2330 train_time:132092ms step_avg:58.66ms
step:2253/2330 train_time:132150ms step_avg:58.66ms
step:2254/2330 train_time:132216ms step_avg:58.66ms
step:2255/2330 train_time:132272ms step_avg:58.66ms
step:2256/2330 train_time:132336ms step_avg:58.66ms
step:2257/2330 train_time:132393ms step_avg:58.66ms
step:2258/2330 train_time:132453ms step_avg:58.66ms
step:2259/2330 train_time:132510ms step_avg:58.66ms
step:2260/2330 train_time:132570ms step_avg:58.66ms
step:2261/2330 train_time:132627ms step_avg:58.66ms
step:2262/2330 train_time:132688ms step_avg:58.66ms
step:2263/2330 train_time:132744ms step_avg:58.66ms
step:2264/2330 train_time:132805ms step_avg:58.66ms
step:2265/2330 train_time:132862ms step_avg:58.66ms
step:2266/2330 train_time:132921ms step_avg:58.66ms
step:2267/2330 train_time:132979ms step_avg:58.66ms
step:2268/2330 train_time:133041ms step_avg:58.66ms
step:2269/2330 train_time:133101ms step_avg:58.66ms
step:2270/2330 train_time:133162ms step_avg:58.66ms
step:2271/2330 train_time:133221ms step_avg:58.66ms
step:2272/2330 train_time:133283ms step_avg:58.66ms
step:2273/2330 train_time:133340ms step_avg:58.66ms
step:2274/2330 train_time:133401ms step_avg:58.66ms
step:2275/2330 train_time:133459ms step_avg:58.66ms
step:2276/2330 train_time:133518ms step_avg:58.66ms
step:2277/2330 train_time:133575ms step_avg:58.66ms
step:2278/2330 train_time:133635ms step_avg:58.66ms
step:2279/2330 train_time:133691ms step_avg:58.66ms
step:2280/2330 train_time:133752ms step_avg:58.66ms
step:2281/2330 train_time:133809ms step_avg:58.66ms
step:2282/2330 train_time:133870ms step_avg:58.66ms
step:2283/2330 train_time:133926ms step_avg:58.66ms
step:2284/2330 train_time:133988ms step_avg:58.66ms
step:2285/2330 train_time:134046ms step_avg:58.66ms
step:2286/2330 train_time:134108ms step_avg:58.66ms
step:2287/2330 train_time:134165ms step_avg:58.66ms
step:2288/2330 train_time:134229ms step_avg:58.67ms
step:2289/2330 train_time:134287ms step_avg:58.67ms
step:2290/2330 train_time:134349ms step_avg:58.67ms
step:2291/2330 train_time:134406ms step_avg:58.67ms
step:2292/2330 train_time:134467ms step_avg:58.67ms
step:2293/2330 train_time:134524ms step_avg:58.67ms
step:2294/2330 train_time:134585ms step_avg:58.67ms
step:2295/2330 train_time:134644ms step_avg:58.67ms
step:2296/2330 train_time:134704ms step_avg:58.67ms
step:2297/2330 train_time:134762ms step_avg:58.67ms
step:2298/2330 train_time:134821ms step_avg:58.67ms
step:2299/2330 train_time:134878ms step_avg:58.67ms
step:2300/2330 train_time:134938ms step_avg:58.67ms
step:2301/2330 train_time:134995ms step_avg:58.67ms
step:2302/2330 train_time:135056ms step_avg:58.67ms
step:2303/2330 train_time:135113ms step_avg:58.67ms
step:2304/2330 train_time:135175ms step_avg:58.67ms
step:2305/2330 train_time:135231ms step_avg:58.67ms
step:2306/2330 train_time:135294ms step_avg:58.67ms
step:2307/2330 train_time:135350ms step_avg:58.67ms
step:2308/2330 train_time:135412ms step_avg:58.67ms
step:2309/2330 train_time:135469ms step_avg:58.67ms
step:2310/2330 train_time:135532ms step_avg:58.67ms
step:2311/2330 train_time:135588ms step_avg:58.67ms
step:2312/2330 train_time:135650ms step_avg:58.67ms
step:2313/2330 train_time:135707ms step_avg:58.67ms
step:2314/2330 train_time:135769ms step_avg:58.67ms
step:2315/2330 train_time:135826ms step_avg:58.67ms
step:2316/2330 train_time:135887ms step_avg:58.67ms
step:2317/2330 train_time:135943ms step_avg:58.67ms
step:2318/2330 train_time:136005ms step_avg:58.67ms
step:2319/2330 train_time:136062ms step_avg:58.67ms
step:2320/2330 train_time:136124ms step_avg:58.67ms
step:2321/2330 train_time:136182ms step_avg:58.67ms
step:2322/2330 train_time:136242ms step_avg:58.67ms
step:2323/2330 train_time:136301ms step_avg:58.67ms
step:2324/2330 train_time:136362ms step_avg:58.68ms
step:2325/2330 train_time:136419ms step_avg:58.67ms
step:2326/2330 train_time:136479ms step_avg:58.68ms
step:2327/2330 train_time:136536ms step_avg:58.67ms
step:2328/2330 train_time:136596ms step_avg:58.68ms
step:2329/2330 train_time:136654ms step_avg:58.67ms
step:2330/2330 train_time:136713ms step_avg:58.68ms
step:2330/2330 val_loss:3.7171 train_time:136795ms step_avg:58.71ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
