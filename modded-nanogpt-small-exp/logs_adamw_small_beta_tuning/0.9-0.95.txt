import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--beta1', type=str, required=True)
parser.add_argument('--beta2', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_beta_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=6e-4,  betas=(float(args2.beta1), float(args2.beta2)), eps=1e-10, weight_decay=0.0, fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt", exist_ok=True)
            torch.save(log, f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 03:47:00 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   35C    P0             117W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   34C    P0             117W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             118W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   28C    P0             115W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   34C    P0             118W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             114W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:87ms step_avg:86.83ms
step:2/2330 train_time:192ms step_avg:96.08ms
step:3/2330 train_time:210ms step_avg:69.97ms
step:4/2330 train_time:229ms step_avg:57.27ms
step:5/2330 train_time:283ms step_avg:56.54ms
step:6/2330 train_time:341ms step_avg:56.84ms
step:7/2330 train_time:396ms step_avg:56.62ms
step:8/2330 train_time:455ms step_avg:56.91ms
step:9/2330 train_time:510ms step_avg:56.72ms
step:10/2330 train_time:569ms step_avg:56.89ms
step:11/2330 train_time:624ms step_avg:56.75ms
step:12/2330 train_time:683ms step_avg:56.89ms
step:13/2330 train_time:738ms step_avg:56.79ms
step:14/2330 train_time:797ms step_avg:56.91ms
step:15/2330 train_time:852ms step_avg:56.81ms
step:16/2330 train_time:910ms step_avg:56.88ms
step:17/2330 train_time:966ms step_avg:56.80ms
step:18/2330 train_time:1024ms step_avg:56.92ms
step:19/2330 train_time:1082ms step_avg:56.96ms
step:20/2330 train_time:1144ms step_avg:57.19ms
step:21/2330 train_time:1202ms step_avg:57.24ms
step:22/2330 train_time:1263ms step_avg:57.40ms
step:23/2330 train_time:1319ms step_avg:57.34ms
step:24/2330 train_time:1379ms step_avg:57.45ms
step:25/2330 train_time:1434ms step_avg:57.36ms
step:26/2330 train_time:1493ms step_avg:57.42ms
step:27/2330 train_time:1548ms step_avg:57.35ms
step:28/2330 train_time:1607ms step_avg:57.39ms
step:29/2330 train_time:1662ms step_avg:57.31ms
step:30/2330 train_time:1720ms step_avg:57.34ms
step:31/2330 train_time:1776ms step_avg:57.28ms
step:32/2330 train_time:1834ms step_avg:57.32ms
step:33/2330 train_time:1889ms step_avg:57.26ms
step:34/2330 train_time:1948ms step_avg:57.28ms
step:35/2330 train_time:2004ms step_avg:57.25ms
step:36/2330 train_time:2063ms step_avg:57.30ms
step:37/2330 train_time:2120ms step_avg:57.29ms
step:38/2330 train_time:2180ms step_avg:57.37ms
step:39/2330 train_time:2237ms step_avg:57.36ms
step:40/2330 train_time:2297ms step_avg:57.42ms
step:41/2330 train_time:2353ms step_avg:57.40ms
step:42/2330 train_time:2413ms step_avg:57.44ms
step:43/2330 train_time:2469ms step_avg:57.41ms
step:44/2330 train_time:2527ms step_avg:57.43ms
step:45/2330 train_time:2582ms step_avg:57.37ms
step:46/2330 train_time:2641ms step_avg:57.41ms
step:47/2330 train_time:2696ms step_avg:57.37ms
step:48/2330 train_time:2755ms step_avg:57.40ms
step:49/2330 train_time:2810ms step_avg:57.35ms
step:50/2330 train_time:2869ms step_avg:57.37ms
step:51/2330 train_time:2924ms step_avg:57.34ms
step:52/2330 train_time:2983ms step_avg:57.37ms
step:53/2330 train_time:3039ms step_avg:57.34ms
step:54/2330 train_time:3098ms step_avg:57.38ms
step:55/2330 train_time:3156ms step_avg:57.37ms
step:56/2330 train_time:3215ms step_avg:57.42ms
step:57/2330 train_time:3272ms step_avg:57.40ms
step:58/2330 train_time:3331ms step_avg:57.43ms
step:59/2330 train_time:3387ms step_avg:57.41ms
step:60/2330 train_time:3446ms step_avg:57.43ms
step:61/2330 train_time:3502ms step_avg:57.40ms
step:62/2330 train_time:3561ms step_avg:57.43ms
step:63/2330 train_time:3616ms step_avg:57.40ms
step:64/2330 train_time:3675ms step_avg:57.42ms
step:65/2330 train_time:3731ms step_avg:57.40ms
step:66/2330 train_time:3789ms step_avg:57.41ms
step:67/2330 train_time:3845ms step_avg:57.39ms
step:68/2330 train_time:3904ms step_avg:57.41ms
step:69/2330 train_time:3959ms step_avg:57.38ms
step:70/2330 train_time:4018ms step_avg:57.40ms
step:71/2330 train_time:4074ms step_avg:57.38ms
step:72/2330 train_time:4133ms step_avg:57.41ms
step:73/2330 train_time:4189ms step_avg:57.39ms
step:74/2330 train_time:4249ms step_avg:57.42ms
step:75/2330 train_time:4305ms step_avg:57.40ms
step:76/2330 train_time:4365ms step_avg:57.43ms
step:77/2330 train_time:4421ms step_avg:57.41ms
step:78/2330 train_time:4479ms step_avg:57.43ms
step:79/2330 train_time:4534ms step_avg:57.40ms
step:80/2330 train_time:4595ms step_avg:57.44ms
step:81/2330 train_time:4650ms step_avg:57.41ms
step:82/2330 train_time:4709ms step_avg:57.43ms
step:83/2330 train_time:4765ms step_avg:57.40ms
step:84/2330 train_time:4823ms step_avg:57.41ms
step:85/2330 train_time:4879ms step_avg:57.40ms
step:86/2330 train_time:4938ms step_avg:57.42ms
step:87/2330 train_time:4994ms step_avg:57.40ms
step:88/2330 train_time:5053ms step_avg:57.42ms
step:89/2330 train_time:5109ms step_avg:57.40ms
step:90/2330 train_time:5168ms step_avg:57.42ms
step:91/2330 train_time:5224ms step_avg:57.40ms
step:92/2330 train_time:5283ms step_avg:57.42ms
step:93/2330 train_time:5339ms step_avg:57.41ms
step:94/2330 train_time:5399ms step_avg:57.43ms
step:95/2330 train_time:5455ms step_avg:57.42ms
step:96/2330 train_time:5514ms step_avg:57.44ms
step:97/2330 train_time:5570ms step_avg:57.43ms
step:98/2330 train_time:5629ms step_avg:57.44ms
step:99/2330 train_time:5684ms step_avg:57.42ms
step:100/2330 train_time:5744ms step_avg:57.44ms
step:101/2330 train_time:5799ms step_avg:57.42ms
step:102/2330 train_time:5858ms step_avg:57.43ms
step:103/2330 train_time:5913ms step_avg:57.41ms
step:104/2330 train_time:5972ms step_avg:57.42ms
step:105/2330 train_time:6028ms step_avg:57.41ms
step:106/2330 train_time:6087ms step_avg:57.43ms
step:107/2330 train_time:6144ms step_avg:57.42ms
step:108/2330 train_time:6203ms step_avg:57.43ms
step:109/2330 train_time:6259ms step_avg:57.42ms
step:110/2330 train_time:6318ms step_avg:57.43ms
step:111/2330 train_time:6373ms step_avg:57.41ms
step:112/2330 train_time:6433ms step_avg:57.44ms
step:113/2330 train_time:6489ms step_avg:57.42ms
step:114/2330 train_time:6549ms step_avg:57.45ms
step:115/2330 train_time:6605ms step_avg:57.43ms
step:116/2330 train_time:6664ms step_avg:57.45ms
step:117/2330 train_time:6719ms step_avg:57.43ms
step:118/2330 train_time:6778ms step_avg:57.44ms
step:119/2330 train_time:6834ms step_avg:57.43ms
step:120/2330 train_time:6893ms step_avg:57.44ms
step:121/2330 train_time:6948ms step_avg:57.42ms
step:122/2330 train_time:7008ms step_avg:57.45ms
step:123/2330 train_time:7064ms step_avg:57.43ms
step:124/2330 train_time:7123ms step_avg:57.44ms
step:125/2330 train_time:7180ms step_avg:57.44ms
step:126/2330 train_time:7238ms step_avg:57.45ms
step:127/2330 train_time:7294ms step_avg:57.44ms
step:128/2330 train_time:7353ms step_avg:57.45ms
step:129/2330 train_time:7409ms step_avg:57.43ms
step:130/2330 train_time:7469ms step_avg:57.45ms
step:131/2330 train_time:7525ms step_avg:57.44ms
step:132/2330 train_time:7584ms step_avg:57.45ms
step:133/2330 train_time:7640ms step_avg:57.44ms
step:134/2330 train_time:7699ms step_avg:57.45ms
step:135/2330 train_time:7755ms step_avg:57.44ms
step:136/2330 train_time:7813ms step_avg:57.45ms
step:137/2330 train_time:7869ms step_avg:57.44ms
step:138/2330 train_time:7928ms step_avg:57.45ms
step:139/2330 train_time:7983ms step_avg:57.44ms
step:140/2330 train_time:8042ms step_avg:57.45ms
step:141/2330 train_time:8098ms step_avg:57.43ms
step:142/2330 train_time:8157ms step_avg:57.44ms
step:143/2330 train_time:8213ms step_avg:57.43ms
step:144/2330 train_time:8272ms step_avg:57.44ms
step:145/2330 train_time:8327ms step_avg:57.43ms
step:146/2330 train_time:8388ms step_avg:57.45ms
step:147/2330 train_time:8444ms step_avg:57.44ms
step:148/2330 train_time:8503ms step_avg:57.45ms
step:149/2330 train_time:8558ms step_avg:57.44ms
step:150/2330 train_time:8617ms step_avg:57.45ms
step:151/2330 train_time:8673ms step_avg:57.44ms
step:152/2330 train_time:8732ms step_avg:57.45ms
step:153/2330 train_time:8787ms step_avg:57.43ms
step:154/2330 train_time:8847ms step_avg:57.45ms
step:155/2330 train_time:8902ms step_avg:57.43ms
step:156/2330 train_time:8961ms step_avg:57.44ms
step:157/2330 train_time:9017ms step_avg:57.43ms
step:158/2330 train_time:9076ms step_avg:57.44ms
step:159/2330 train_time:9132ms step_avg:57.43ms
step:160/2330 train_time:9191ms step_avg:57.45ms
step:161/2330 train_time:9248ms step_avg:57.44ms
step:162/2330 train_time:9307ms step_avg:57.45ms
step:163/2330 train_time:9362ms step_avg:57.44ms
step:164/2330 train_time:9421ms step_avg:57.45ms
step:165/2330 train_time:9477ms step_avg:57.44ms
step:166/2330 train_time:9537ms step_avg:57.45ms
step:167/2330 train_time:9592ms step_avg:57.43ms
step:168/2330 train_time:9653ms step_avg:57.46ms
step:169/2330 train_time:9708ms step_avg:57.44ms
step:170/2330 train_time:9768ms step_avg:57.46ms
step:171/2330 train_time:9824ms step_avg:57.45ms
step:172/2330 train_time:9883ms step_avg:57.46ms
step:173/2330 train_time:9939ms step_avg:57.45ms
step:174/2330 train_time:9998ms step_avg:57.46ms
step:175/2330 train_time:10054ms step_avg:57.45ms
step:176/2330 train_time:10113ms step_avg:57.46ms
step:177/2330 train_time:10169ms step_avg:57.45ms
step:178/2330 train_time:10228ms step_avg:57.46ms
step:179/2330 train_time:10284ms step_avg:57.45ms
step:180/2330 train_time:10343ms step_avg:57.46ms
step:181/2330 train_time:10399ms step_avg:57.45ms
step:182/2330 train_time:10459ms step_avg:57.46ms
step:183/2330 train_time:10514ms step_avg:57.46ms
step:184/2330 train_time:10573ms step_avg:57.46ms
step:185/2330 train_time:10628ms step_avg:57.45ms
step:186/2330 train_time:10687ms step_avg:57.46ms
step:187/2330 train_time:10742ms step_avg:57.45ms
step:188/2330 train_time:10802ms step_avg:57.46ms
step:189/2330 train_time:10857ms step_avg:57.45ms
step:190/2330 train_time:10916ms step_avg:57.45ms
step:191/2330 train_time:10972ms step_avg:57.45ms
step:192/2330 train_time:11031ms step_avg:57.45ms
step:193/2330 train_time:11086ms step_avg:57.44ms
step:194/2330 train_time:11146ms step_avg:57.45ms
step:195/2330 train_time:11202ms step_avg:57.45ms
step:196/2330 train_time:11261ms step_avg:57.46ms
step:197/2330 train_time:11318ms step_avg:57.45ms
step:198/2330 train_time:11377ms step_avg:57.46ms
step:199/2330 train_time:11433ms step_avg:57.45ms
step:200/2330 train_time:11492ms step_avg:57.46ms
step:201/2330 train_time:11548ms step_avg:57.45ms
step:202/2330 train_time:11607ms step_avg:57.46ms
step:203/2330 train_time:11663ms step_avg:57.45ms
step:204/2330 train_time:11722ms step_avg:57.46ms
step:205/2330 train_time:11778ms step_avg:57.45ms
step:206/2330 train_time:11836ms step_avg:57.46ms
step:207/2330 train_time:11892ms step_avg:57.45ms
step:208/2330 train_time:11952ms step_avg:57.46ms
step:209/2330 train_time:12008ms step_avg:57.45ms
step:210/2330 train_time:12066ms step_avg:57.46ms
step:211/2330 train_time:12122ms step_avg:57.45ms
step:212/2330 train_time:12182ms step_avg:57.46ms
step:213/2330 train_time:12238ms step_avg:57.46ms
step:214/2330 train_time:12296ms step_avg:57.46ms
step:215/2330 train_time:12352ms step_avg:57.45ms
step:216/2330 train_time:12412ms step_avg:57.46ms
step:217/2330 train_time:12468ms step_avg:57.45ms
step:218/2330 train_time:12527ms step_avg:57.46ms
step:219/2330 train_time:12583ms step_avg:57.46ms
step:220/2330 train_time:12642ms step_avg:57.46ms
step:221/2330 train_time:12697ms step_avg:57.45ms
step:222/2330 train_time:12757ms step_avg:57.46ms
step:223/2330 train_time:12813ms step_avg:57.46ms
step:224/2330 train_time:12872ms step_avg:57.46ms
step:225/2330 train_time:12928ms step_avg:57.46ms
step:226/2330 train_time:12987ms step_avg:57.47ms
step:227/2330 train_time:13043ms step_avg:57.46ms
step:228/2330 train_time:13102ms step_avg:57.47ms
step:229/2330 train_time:13159ms step_avg:57.46ms
step:230/2330 train_time:13217ms step_avg:57.46ms
step:231/2330 train_time:13272ms step_avg:57.45ms
step:232/2330 train_time:13332ms step_avg:57.47ms
step:233/2330 train_time:13388ms step_avg:57.46ms
step:234/2330 train_time:13447ms step_avg:57.47ms
step:235/2330 train_time:13503ms step_avg:57.46ms
step:236/2330 train_time:13562ms step_avg:57.47ms
step:237/2330 train_time:13618ms step_avg:57.46ms
step:238/2330 train_time:13677ms step_avg:57.47ms
step:239/2330 train_time:13733ms step_avg:57.46ms
step:240/2330 train_time:13792ms step_avg:57.47ms
step:241/2330 train_time:13848ms step_avg:57.46ms
step:242/2330 train_time:13907ms step_avg:57.47ms
step:243/2330 train_time:13963ms step_avg:57.46ms
step:244/2330 train_time:14022ms step_avg:57.47ms
step:245/2330 train_time:14078ms step_avg:57.46ms
step:246/2330 train_time:14137ms step_avg:57.47ms
step:247/2330 train_time:14193ms step_avg:57.46ms
step:248/2330 train_time:14252ms step_avg:57.47ms
step:249/2330 train_time:14307ms step_avg:57.46ms
step:250/2330 train_time:14367ms step_avg:57.47ms
step:250/2330 val_loss:4.9221 train_time:14446ms step_avg:57.78ms
step:251/2330 train_time:14464ms step_avg:57.62ms
step:252/2330 train_time:14484ms step_avg:57.47ms
step:253/2330 train_time:14539ms step_avg:57.47ms
step:254/2330 train_time:14606ms step_avg:57.51ms
step:255/2330 train_time:14662ms step_avg:57.50ms
step:256/2330 train_time:14727ms step_avg:57.53ms
step:257/2330 train_time:14781ms step_avg:57.52ms
step:258/2330 train_time:14841ms step_avg:57.52ms
step:259/2330 train_time:14896ms step_avg:57.51ms
step:260/2330 train_time:14955ms step_avg:57.52ms
step:261/2330 train_time:15010ms step_avg:57.51ms
step:262/2330 train_time:15069ms step_avg:57.51ms
step:263/2330 train_time:15124ms step_avg:57.51ms
step:264/2330 train_time:15183ms step_avg:57.51ms
step:265/2330 train_time:15238ms step_avg:57.50ms
step:266/2330 train_time:15296ms step_avg:57.50ms
step:267/2330 train_time:15353ms step_avg:57.50ms
step:268/2330 train_time:15413ms step_avg:57.51ms
step:269/2330 train_time:15470ms step_avg:57.51ms
step:270/2330 train_time:15530ms step_avg:57.52ms
step:271/2330 train_time:15586ms step_avg:57.51ms
step:272/2330 train_time:15647ms step_avg:57.52ms
step:273/2330 train_time:15702ms step_avg:57.52ms
step:274/2330 train_time:15763ms step_avg:57.53ms
step:275/2330 train_time:15819ms step_avg:57.52ms
step:276/2330 train_time:15878ms step_avg:57.53ms
step:277/2330 train_time:15933ms step_avg:57.52ms
step:278/2330 train_time:15992ms step_avg:57.53ms
step:279/2330 train_time:16048ms step_avg:57.52ms
step:280/2330 train_time:16107ms step_avg:57.52ms
step:281/2330 train_time:16162ms step_avg:57.52ms
step:282/2330 train_time:16221ms step_avg:57.52ms
step:283/2330 train_time:16277ms step_avg:57.52ms
step:284/2330 train_time:16336ms step_avg:57.52ms
step:285/2330 train_time:16393ms step_avg:57.52ms
step:286/2330 train_time:16452ms step_avg:57.52ms
step:287/2330 train_time:16509ms step_avg:57.52ms
step:288/2330 train_time:16568ms step_avg:57.53ms
step:289/2330 train_time:16624ms step_avg:57.52ms
step:290/2330 train_time:16684ms step_avg:57.53ms
step:291/2330 train_time:16741ms step_avg:57.53ms
step:292/2330 train_time:16800ms step_avg:57.53ms
step:293/2330 train_time:16857ms step_avg:57.53ms
step:294/2330 train_time:16915ms step_avg:57.53ms
step:295/2330 train_time:16971ms step_avg:57.53ms
step:296/2330 train_time:17030ms step_avg:57.53ms
step:297/2330 train_time:17085ms step_avg:57.53ms
step:298/2330 train_time:17145ms step_avg:57.53ms
step:299/2330 train_time:17201ms step_avg:57.53ms
step:300/2330 train_time:17259ms step_avg:57.53ms
step:301/2330 train_time:17316ms step_avg:57.53ms
step:302/2330 train_time:17375ms step_avg:57.53ms
step:303/2330 train_time:17431ms step_avg:57.53ms
step:304/2330 train_time:17490ms step_avg:57.53ms
step:305/2330 train_time:17547ms step_avg:57.53ms
step:306/2330 train_time:17606ms step_avg:57.54ms
step:307/2330 train_time:17662ms step_avg:57.53ms
step:308/2330 train_time:17722ms step_avg:57.54ms
step:309/2330 train_time:17778ms step_avg:57.53ms
step:310/2330 train_time:17837ms step_avg:57.54ms
step:311/2330 train_time:17893ms step_avg:57.53ms
step:312/2330 train_time:17954ms step_avg:57.54ms
step:313/2330 train_time:18009ms step_avg:57.54ms
step:314/2330 train_time:18068ms step_avg:57.54ms
step:315/2330 train_time:18124ms step_avg:57.54ms
step:316/2330 train_time:18183ms step_avg:57.54ms
step:317/2330 train_time:18239ms step_avg:57.54ms
step:318/2330 train_time:18297ms step_avg:57.54ms
step:319/2330 train_time:18353ms step_avg:57.53ms
step:320/2330 train_time:18412ms step_avg:57.54ms
step:321/2330 train_time:18468ms step_avg:57.53ms
step:322/2330 train_time:18527ms step_avg:57.54ms
step:323/2330 train_time:18583ms step_avg:57.53ms
step:324/2330 train_time:18645ms step_avg:57.55ms
step:325/2330 train_time:18701ms step_avg:57.54ms
step:326/2330 train_time:18760ms step_avg:57.55ms
step:327/2330 train_time:18816ms step_avg:57.54ms
step:328/2330 train_time:18875ms step_avg:57.55ms
step:329/2330 train_time:18931ms step_avg:57.54ms
step:330/2330 train_time:18990ms step_avg:57.55ms
step:331/2330 train_time:19046ms step_avg:57.54ms
step:332/2330 train_time:19105ms step_avg:57.55ms
step:333/2330 train_time:19161ms step_avg:57.54ms
step:334/2330 train_time:19220ms step_avg:57.55ms
step:335/2330 train_time:19277ms step_avg:57.54ms
step:336/2330 train_time:19336ms step_avg:57.55ms
step:337/2330 train_time:19392ms step_avg:57.54ms
step:338/2330 train_time:19450ms step_avg:57.55ms
step:339/2330 train_time:19506ms step_avg:57.54ms
step:340/2330 train_time:19566ms step_avg:57.55ms
step:341/2330 train_time:19622ms step_avg:57.54ms
step:342/2330 train_time:19683ms step_avg:57.55ms
step:343/2330 train_time:19739ms step_avg:57.55ms
step:344/2330 train_time:19797ms step_avg:57.55ms
step:345/2330 train_time:19853ms step_avg:57.54ms
step:346/2330 train_time:19912ms step_avg:57.55ms
step:347/2330 train_time:19968ms step_avg:57.55ms
step:348/2330 train_time:20027ms step_avg:57.55ms
step:349/2330 train_time:20083ms step_avg:57.54ms
step:350/2330 train_time:20143ms step_avg:57.55ms
step:351/2330 train_time:20199ms step_avg:57.55ms
step:352/2330 train_time:20258ms step_avg:57.55ms
step:353/2330 train_time:20314ms step_avg:57.55ms
step:354/2330 train_time:20373ms step_avg:57.55ms
step:355/2330 train_time:20429ms step_avg:57.55ms
step:356/2330 train_time:20488ms step_avg:57.55ms
step:357/2330 train_time:20544ms step_avg:57.55ms
step:358/2330 train_time:20604ms step_avg:57.55ms
step:359/2330 train_time:20660ms step_avg:57.55ms
step:360/2330 train_time:20719ms step_avg:57.55ms
step:361/2330 train_time:20775ms step_avg:57.55ms
step:362/2330 train_time:20834ms step_avg:57.55ms
step:363/2330 train_time:20891ms step_avg:57.55ms
step:364/2330 train_time:20950ms step_avg:57.55ms
step:365/2330 train_time:21005ms step_avg:57.55ms
step:366/2330 train_time:21065ms step_avg:57.55ms
step:367/2330 train_time:21120ms step_avg:57.55ms
step:368/2330 train_time:21179ms step_avg:57.55ms
step:369/2330 train_time:21235ms step_avg:57.55ms
step:370/2330 train_time:21294ms step_avg:57.55ms
step:371/2330 train_time:21351ms step_avg:57.55ms
step:372/2330 train_time:21409ms step_avg:57.55ms
step:373/2330 train_time:21465ms step_avg:57.55ms
step:374/2330 train_time:21525ms step_avg:57.55ms
step:375/2330 train_time:21581ms step_avg:57.55ms
step:376/2330 train_time:21641ms step_avg:57.55ms
step:377/2330 train_time:21697ms step_avg:57.55ms
step:378/2330 train_time:21756ms step_avg:57.56ms
step:379/2330 train_time:21812ms step_avg:57.55ms
step:380/2330 train_time:21871ms step_avg:57.56ms
step:381/2330 train_time:21927ms step_avg:57.55ms
step:382/2330 train_time:21986ms step_avg:57.56ms
step:383/2330 train_time:22043ms step_avg:57.55ms
step:384/2330 train_time:22102ms step_avg:57.56ms
step:385/2330 train_time:22158ms step_avg:57.55ms
step:386/2330 train_time:22217ms step_avg:57.56ms
step:387/2330 train_time:22273ms step_avg:57.55ms
step:388/2330 train_time:22333ms step_avg:57.56ms
step:389/2330 train_time:22389ms step_avg:57.55ms
step:390/2330 train_time:22448ms step_avg:57.56ms
step:391/2330 train_time:22504ms step_avg:57.56ms
step:392/2330 train_time:22565ms step_avg:57.56ms
step:393/2330 train_time:22621ms step_avg:57.56ms
step:394/2330 train_time:22680ms step_avg:57.56ms
step:395/2330 train_time:22736ms step_avg:57.56ms
step:396/2330 train_time:22795ms step_avg:57.56ms
step:397/2330 train_time:22851ms step_avg:57.56ms
step:398/2330 train_time:22910ms step_avg:57.56ms
step:399/2330 train_time:22967ms step_avg:57.56ms
step:400/2330 train_time:23025ms step_avg:57.56ms
step:401/2330 train_time:23081ms step_avg:57.56ms
step:402/2330 train_time:23141ms step_avg:57.56ms
step:403/2330 train_time:23197ms step_avg:57.56ms
step:404/2330 train_time:23255ms step_avg:57.56ms
step:405/2330 train_time:23312ms step_avg:57.56ms
step:406/2330 train_time:23370ms step_avg:57.56ms
step:407/2330 train_time:23426ms step_avg:57.56ms
step:408/2330 train_time:23487ms step_avg:57.57ms
step:409/2330 train_time:23542ms step_avg:57.56ms
step:410/2330 train_time:23603ms step_avg:57.57ms
step:411/2330 train_time:23659ms step_avg:57.56ms
step:412/2330 train_time:23718ms step_avg:57.57ms
step:413/2330 train_time:23775ms step_avg:57.57ms
step:414/2330 train_time:23833ms step_avg:57.57ms
step:415/2330 train_time:23889ms step_avg:57.56ms
step:416/2330 train_time:23949ms step_avg:57.57ms
step:417/2330 train_time:24005ms step_avg:57.57ms
step:418/2330 train_time:24064ms step_avg:57.57ms
step:419/2330 train_time:24120ms step_avg:57.57ms
step:420/2330 train_time:24179ms step_avg:57.57ms
step:421/2330 train_time:24235ms step_avg:57.57ms
step:422/2330 train_time:24295ms step_avg:57.57ms
step:423/2330 train_time:24351ms step_avg:57.57ms
step:424/2330 train_time:24411ms step_avg:57.57ms
step:425/2330 train_time:24467ms step_avg:57.57ms
step:426/2330 train_time:24526ms step_avg:57.57ms
step:427/2330 train_time:24581ms step_avg:57.57ms
step:428/2330 train_time:24641ms step_avg:57.57ms
step:429/2330 train_time:24697ms step_avg:57.57ms
step:430/2330 train_time:24756ms step_avg:57.57ms
step:431/2330 train_time:24812ms step_avg:57.57ms
step:432/2330 train_time:24873ms step_avg:57.58ms
step:433/2330 train_time:24929ms step_avg:57.57ms
step:434/2330 train_time:24989ms step_avg:57.58ms
step:435/2330 train_time:25045ms step_avg:57.57ms
step:436/2330 train_time:25105ms step_avg:57.58ms
step:437/2330 train_time:25160ms step_avg:57.58ms
step:438/2330 train_time:25219ms step_avg:57.58ms
step:439/2330 train_time:25276ms step_avg:57.58ms
step:440/2330 train_time:25335ms step_avg:57.58ms
step:441/2330 train_time:25392ms step_avg:57.58ms
step:442/2330 train_time:25450ms step_avg:57.58ms
step:443/2330 train_time:25506ms step_avg:57.58ms
step:444/2330 train_time:25565ms step_avg:57.58ms
step:445/2330 train_time:25621ms step_avg:57.58ms
step:446/2330 train_time:25680ms step_avg:57.58ms
step:447/2330 train_time:25736ms step_avg:57.58ms
step:448/2330 train_time:25795ms step_avg:57.58ms
step:449/2330 train_time:25852ms step_avg:57.58ms
step:450/2330 train_time:25911ms step_avg:57.58ms
step:451/2330 train_time:25966ms step_avg:57.58ms
step:452/2330 train_time:26027ms step_avg:57.58ms
step:453/2330 train_time:26082ms step_avg:57.58ms
step:454/2330 train_time:26142ms step_avg:57.58ms
step:455/2330 train_time:26197ms step_avg:57.58ms
step:456/2330 train_time:26257ms step_avg:57.58ms
step:457/2330 train_time:26313ms step_avg:57.58ms
step:458/2330 train_time:26372ms step_avg:57.58ms
step:459/2330 train_time:26428ms step_avg:57.58ms
step:460/2330 train_time:26487ms step_avg:57.58ms
step:461/2330 train_time:26543ms step_avg:57.58ms
step:462/2330 train_time:26602ms step_avg:57.58ms
step:463/2330 train_time:26658ms step_avg:57.58ms
step:464/2330 train_time:26717ms step_avg:57.58ms
step:465/2330 train_time:26773ms step_avg:57.58ms
step:466/2330 train_time:26832ms step_avg:57.58ms
step:467/2330 train_time:26889ms step_avg:57.58ms
step:468/2330 train_time:26947ms step_avg:57.58ms
step:469/2330 train_time:27003ms step_avg:57.58ms
step:470/2330 train_time:27063ms step_avg:57.58ms
step:471/2330 train_time:27119ms step_avg:57.58ms
step:472/2330 train_time:27178ms step_avg:57.58ms
step:473/2330 train_time:27235ms step_avg:57.58ms
step:474/2330 train_time:27294ms step_avg:57.58ms
step:475/2330 train_time:27350ms step_avg:57.58ms
step:476/2330 train_time:27410ms step_avg:57.58ms
step:477/2330 train_time:27466ms step_avg:57.58ms
step:478/2330 train_time:27525ms step_avg:57.58ms
step:479/2330 train_time:27581ms step_avg:57.58ms
step:480/2330 train_time:27641ms step_avg:57.58ms
step:481/2330 train_time:27697ms step_avg:57.58ms
step:482/2330 train_time:27755ms step_avg:57.58ms
step:483/2330 train_time:27812ms step_avg:57.58ms
step:484/2330 train_time:27871ms step_avg:57.58ms
step:485/2330 train_time:27927ms step_avg:57.58ms
step:486/2330 train_time:27986ms step_avg:57.58ms
step:487/2330 train_time:28042ms step_avg:57.58ms
step:488/2330 train_time:28102ms step_avg:57.59ms
step:489/2330 train_time:28157ms step_avg:57.58ms
step:490/2330 train_time:28216ms step_avg:57.58ms
step:491/2330 train_time:28272ms step_avg:57.58ms
step:492/2330 train_time:28331ms step_avg:57.58ms
step:493/2330 train_time:28387ms step_avg:57.58ms
step:494/2330 train_time:28447ms step_avg:57.59ms
step:495/2330 train_time:28503ms step_avg:57.58ms
step:496/2330 train_time:28563ms step_avg:57.59ms
step:497/2330 train_time:28619ms step_avg:57.58ms
step:498/2330 train_time:28677ms step_avg:57.58ms
step:499/2330 train_time:28733ms step_avg:57.58ms
step:500/2330 train_time:28792ms step_avg:57.58ms
step:500/2330 val_loss:4.4289 train_time:28871ms step_avg:57.74ms
step:501/2330 train_time:28888ms step_avg:57.66ms
step:502/2330 train_time:28910ms step_avg:57.59ms
step:503/2330 train_time:28966ms step_avg:57.59ms
step:504/2330 train_time:29029ms step_avg:57.60ms
step:505/2330 train_time:29086ms step_avg:57.60ms
step:506/2330 train_time:29145ms step_avg:57.60ms
step:507/2330 train_time:29202ms step_avg:57.60ms
step:508/2330 train_time:29261ms step_avg:57.60ms
step:509/2330 train_time:29317ms step_avg:57.60ms
step:510/2330 train_time:29375ms step_avg:57.60ms
step:511/2330 train_time:29430ms step_avg:57.59ms
step:512/2330 train_time:29489ms step_avg:57.60ms
step:513/2330 train_time:29545ms step_avg:57.59ms
step:514/2330 train_time:29603ms step_avg:57.59ms
step:515/2330 train_time:29659ms step_avg:57.59ms
step:516/2330 train_time:29717ms step_avg:57.59ms
step:517/2330 train_time:29772ms step_avg:57.59ms
step:518/2330 train_time:29831ms step_avg:57.59ms
step:519/2330 train_time:29888ms step_avg:57.59ms
step:520/2330 train_time:29949ms step_avg:57.59ms
step:521/2330 train_time:30006ms step_avg:57.59ms
step:522/2330 train_time:30068ms step_avg:57.60ms
step:523/2330 train_time:30124ms step_avg:57.60ms
step:524/2330 train_time:30183ms step_avg:57.60ms
step:525/2330 train_time:30240ms step_avg:57.60ms
step:526/2330 train_time:30299ms step_avg:57.60ms
step:527/2330 train_time:30355ms step_avg:57.60ms
step:528/2330 train_time:30413ms step_avg:57.60ms
step:529/2330 train_time:30468ms step_avg:57.60ms
step:530/2330 train_time:30528ms step_avg:57.60ms
step:531/2330 train_time:30583ms step_avg:57.60ms
step:532/2330 train_time:30643ms step_avg:57.60ms
step:533/2330 train_time:30699ms step_avg:57.60ms
step:534/2330 train_time:30758ms step_avg:57.60ms
step:535/2330 train_time:30815ms step_avg:57.60ms
step:536/2330 train_time:30873ms step_avg:57.60ms
step:537/2330 train_time:30929ms step_avg:57.60ms
step:538/2330 train_time:30991ms step_avg:57.60ms
step:539/2330 train_time:31047ms step_avg:57.60ms
step:540/2330 train_time:31107ms step_avg:57.61ms
step:541/2330 train_time:31163ms step_avg:57.60ms
step:542/2330 train_time:31223ms step_avg:57.61ms
step:543/2330 train_time:31280ms step_avg:57.61ms
step:544/2330 train_time:31339ms step_avg:57.61ms
step:545/2330 train_time:31394ms step_avg:57.60ms
step:546/2330 train_time:31453ms step_avg:57.61ms
step:547/2330 train_time:31509ms step_avg:57.60ms
step:548/2330 train_time:31568ms step_avg:57.61ms
step:549/2330 train_time:31623ms step_avg:57.60ms
step:550/2330 train_time:31683ms step_avg:57.60ms
step:551/2330 train_time:31738ms step_avg:57.60ms
step:552/2330 train_time:31798ms step_avg:57.60ms
step:553/2330 train_time:31854ms step_avg:57.60ms
step:554/2330 train_time:31914ms step_avg:57.61ms
step:555/2330 train_time:31970ms step_avg:57.60ms
step:556/2330 train_time:32030ms step_avg:57.61ms
step:557/2330 train_time:32086ms step_avg:57.60ms
step:558/2330 train_time:32147ms step_avg:57.61ms
step:559/2330 train_time:32202ms step_avg:57.61ms
step:560/2330 train_time:32262ms step_avg:57.61ms
step:561/2330 train_time:32319ms step_avg:57.61ms
step:562/2330 train_time:32377ms step_avg:57.61ms
step:563/2330 train_time:32433ms step_avg:57.61ms
step:564/2330 train_time:32492ms step_avg:57.61ms
step:565/2330 train_time:32547ms step_avg:57.61ms
step:566/2330 train_time:32607ms step_avg:57.61ms
step:567/2330 train_time:32663ms step_avg:57.61ms
step:568/2330 train_time:32722ms step_avg:57.61ms
step:569/2330 train_time:32778ms step_avg:57.61ms
step:570/2330 train_time:32837ms step_avg:57.61ms
step:571/2330 train_time:32893ms step_avg:57.61ms
step:572/2330 train_time:32953ms step_avg:57.61ms
step:573/2330 train_time:33009ms step_avg:57.61ms
step:574/2330 train_time:33070ms step_avg:57.61ms
step:575/2330 train_time:33126ms step_avg:57.61ms
step:576/2330 train_time:33187ms step_avg:57.62ms
step:577/2330 train_time:33243ms step_avg:57.61ms
step:578/2330 train_time:33301ms step_avg:57.61ms
step:579/2330 train_time:33357ms step_avg:57.61ms
step:580/2330 train_time:33416ms step_avg:57.61ms
step:581/2330 train_time:33472ms step_avg:57.61ms
step:582/2330 train_time:33530ms step_avg:57.61ms
step:583/2330 train_time:33586ms step_avg:57.61ms
step:584/2330 train_time:33646ms step_avg:57.61ms
step:585/2330 train_time:33702ms step_avg:57.61ms
step:586/2330 train_time:33760ms step_avg:57.61ms
step:587/2330 train_time:33816ms step_avg:57.61ms
step:588/2330 train_time:33876ms step_avg:57.61ms
step:589/2330 train_time:33932ms step_avg:57.61ms
step:590/2330 train_time:33991ms step_avg:57.61ms
step:591/2330 train_time:34047ms step_avg:57.61ms
step:592/2330 train_time:34107ms step_avg:57.61ms
step:593/2330 train_time:34163ms step_avg:57.61ms
step:594/2330 train_time:34223ms step_avg:57.61ms
step:595/2330 train_time:34279ms step_avg:57.61ms
step:596/2330 train_time:34338ms step_avg:57.61ms
step:597/2330 train_time:34394ms step_avg:57.61ms
step:598/2330 train_time:34453ms step_avg:57.61ms
step:599/2330 train_time:34508ms step_avg:57.61ms
step:600/2330 train_time:34568ms step_avg:57.61ms
step:601/2330 train_time:34624ms step_avg:57.61ms
step:602/2330 train_time:34683ms step_avg:57.61ms
step:603/2330 train_time:34739ms step_avg:57.61ms
step:604/2330 train_time:34799ms step_avg:57.61ms
step:605/2330 train_time:34855ms step_avg:57.61ms
step:606/2330 train_time:34915ms step_avg:57.61ms
step:607/2330 train_time:34970ms step_avg:57.61ms
step:608/2330 train_time:35030ms step_avg:57.62ms
step:609/2330 train_time:35086ms step_avg:57.61ms
step:610/2330 train_time:35147ms step_avg:57.62ms
step:611/2330 train_time:35203ms step_avg:57.62ms
step:612/2330 train_time:35262ms step_avg:57.62ms
step:613/2330 train_time:35318ms step_avg:57.61ms
step:614/2330 train_time:35377ms step_avg:57.62ms
step:615/2330 train_time:35432ms step_avg:57.61ms
step:616/2330 train_time:35492ms step_avg:57.62ms
step:617/2330 train_time:35548ms step_avg:57.61ms
step:618/2330 train_time:35608ms step_avg:57.62ms
step:619/2330 train_time:35664ms step_avg:57.61ms
step:620/2330 train_time:35724ms step_avg:57.62ms
step:621/2330 train_time:35780ms step_avg:57.62ms
step:622/2330 train_time:35840ms step_avg:57.62ms
step:623/2330 train_time:35897ms step_avg:57.62ms
step:624/2330 train_time:35955ms step_avg:57.62ms
step:625/2330 train_time:36011ms step_avg:57.62ms
step:626/2330 train_time:36071ms step_avg:57.62ms
step:627/2330 train_time:36126ms step_avg:57.62ms
step:628/2330 train_time:36186ms step_avg:57.62ms
step:629/2330 train_time:36242ms step_avg:57.62ms
step:630/2330 train_time:36302ms step_avg:57.62ms
step:631/2330 train_time:36358ms step_avg:57.62ms
step:632/2330 train_time:36417ms step_avg:57.62ms
step:633/2330 train_time:36473ms step_avg:57.62ms
step:634/2330 train_time:36532ms step_avg:57.62ms
step:635/2330 train_time:36588ms step_avg:57.62ms
step:636/2330 train_time:36647ms step_avg:57.62ms
step:637/2330 train_time:36703ms step_avg:57.62ms
step:638/2330 train_time:36762ms step_avg:57.62ms
step:639/2330 train_time:36818ms step_avg:57.62ms
step:640/2330 train_time:36878ms step_avg:57.62ms
step:641/2330 train_time:36934ms step_avg:57.62ms
step:642/2330 train_time:36994ms step_avg:57.62ms
step:643/2330 train_time:37049ms step_avg:57.62ms
step:644/2330 train_time:37110ms step_avg:57.62ms
step:645/2330 train_time:37165ms step_avg:57.62ms
step:646/2330 train_time:37226ms step_avg:57.62ms
step:647/2330 train_time:37281ms step_avg:57.62ms
step:648/2330 train_time:37340ms step_avg:57.62ms
step:649/2330 train_time:37397ms step_avg:57.62ms
step:650/2330 train_time:37456ms step_avg:57.62ms
step:651/2330 train_time:37512ms step_avg:57.62ms
step:652/2330 train_time:37572ms step_avg:57.63ms
step:653/2330 train_time:37627ms step_avg:57.62ms
step:654/2330 train_time:37688ms step_avg:57.63ms
step:655/2330 train_time:37744ms step_avg:57.62ms
step:656/2330 train_time:37803ms step_avg:57.63ms
step:657/2330 train_time:37860ms step_avg:57.63ms
step:658/2330 train_time:37920ms step_avg:57.63ms
step:659/2330 train_time:37977ms step_avg:57.63ms
step:660/2330 train_time:38036ms step_avg:57.63ms
step:661/2330 train_time:38092ms step_avg:57.63ms
step:662/2330 train_time:38152ms step_avg:57.63ms
step:663/2330 train_time:38207ms step_avg:57.63ms
step:664/2330 train_time:38268ms step_avg:57.63ms
step:665/2330 train_time:38323ms step_avg:57.63ms
step:666/2330 train_time:38384ms step_avg:57.63ms
step:667/2330 train_time:38441ms step_avg:57.63ms
step:668/2330 train_time:38500ms step_avg:57.63ms
step:669/2330 train_time:38557ms step_avg:57.63ms
step:670/2330 train_time:38616ms step_avg:57.64ms
step:671/2330 train_time:38672ms step_avg:57.63ms
step:672/2330 train_time:38731ms step_avg:57.63ms
step:673/2330 train_time:38787ms step_avg:57.63ms
step:674/2330 train_time:38846ms step_avg:57.63ms
step:675/2330 train_time:38903ms step_avg:57.63ms
step:676/2330 train_time:38962ms step_avg:57.64ms
step:677/2330 train_time:39018ms step_avg:57.63ms
step:678/2330 train_time:39077ms step_avg:57.64ms
step:679/2330 train_time:39133ms step_avg:57.63ms
step:680/2330 train_time:39192ms step_avg:57.64ms
step:681/2330 train_time:39248ms step_avg:57.63ms
step:682/2330 train_time:39308ms step_avg:57.64ms
step:683/2330 train_time:39363ms step_avg:57.63ms
step:684/2330 train_time:39423ms step_avg:57.64ms
step:685/2330 train_time:39479ms step_avg:57.63ms
step:686/2330 train_time:39538ms step_avg:57.64ms
step:687/2330 train_time:39595ms step_avg:57.63ms
step:688/2330 train_time:39655ms step_avg:57.64ms
step:689/2330 train_time:39711ms step_avg:57.64ms
step:690/2330 train_time:39772ms step_avg:57.64ms
step:691/2330 train_time:39827ms step_avg:57.64ms
step:692/2330 train_time:39887ms step_avg:57.64ms
step:693/2330 train_time:39942ms step_avg:57.64ms
step:694/2330 train_time:40002ms step_avg:57.64ms
step:695/2330 train_time:40058ms step_avg:57.64ms
step:696/2330 train_time:40118ms step_avg:57.64ms
step:697/2330 train_time:40174ms step_avg:57.64ms
step:698/2330 train_time:40232ms step_avg:57.64ms
step:699/2330 train_time:40288ms step_avg:57.64ms
step:700/2330 train_time:40349ms step_avg:57.64ms
step:701/2330 train_time:40405ms step_avg:57.64ms
step:702/2330 train_time:40465ms step_avg:57.64ms
step:703/2330 train_time:40521ms step_avg:57.64ms
step:704/2330 train_time:40581ms step_avg:57.64ms
step:705/2330 train_time:40638ms step_avg:57.64ms
step:706/2330 train_time:40697ms step_avg:57.64ms
step:707/2330 train_time:40753ms step_avg:57.64ms
step:708/2330 train_time:40812ms step_avg:57.64ms
step:709/2330 train_time:40868ms step_avg:57.64ms
step:710/2330 train_time:40928ms step_avg:57.65ms
step:711/2330 train_time:40984ms step_avg:57.64ms
step:712/2330 train_time:41043ms step_avg:57.64ms
step:713/2330 train_time:41099ms step_avg:57.64ms
step:714/2330 train_time:41158ms step_avg:57.64ms
step:715/2330 train_time:41214ms step_avg:57.64ms
step:716/2330 train_time:41273ms step_avg:57.64ms
step:717/2330 train_time:41329ms step_avg:57.64ms
step:718/2330 train_time:41389ms step_avg:57.65ms
step:719/2330 train_time:41445ms step_avg:57.64ms
step:720/2330 train_time:41506ms step_avg:57.65ms
step:721/2330 train_time:41561ms step_avg:57.64ms
step:722/2330 train_time:41620ms step_avg:57.65ms
step:723/2330 train_time:41676ms step_avg:57.64ms
step:724/2330 train_time:41736ms step_avg:57.65ms
step:725/2330 train_time:41792ms step_avg:57.64ms
step:726/2330 train_time:41851ms step_avg:57.65ms
step:727/2330 train_time:41907ms step_avg:57.64ms
step:728/2330 train_time:41967ms step_avg:57.65ms
step:729/2330 train_time:42023ms step_avg:57.64ms
step:730/2330 train_time:42082ms step_avg:57.65ms
step:731/2330 train_time:42138ms step_avg:57.64ms
step:732/2330 train_time:42197ms step_avg:57.65ms
step:733/2330 train_time:42253ms step_avg:57.64ms
step:734/2330 train_time:42312ms step_avg:57.65ms
step:735/2330 train_time:42368ms step_avg:57.64ms
step:736/2330 train_time:42428ms step_avg:57.65ms
step:737/2330 train_time:42483ms step_avg:57.64ms
step:738/2330 train_time:42544ms step_avg:57.65ms
step:739/2330 train_time:42600ms step_avg:57.65ms
step:740/2330 train_time:42660ms step_avg:57.65ms
step:741/2330 train_time:42716ms step_avg:57.65ms
step:742/2330 train_time:42775ms step_avg:57.65ms
step:743/2330 train_time:42831ms step_avg:57.65ms
step:744/2330 train_time:42890ms step_avg:57.65ms
step:745/2330 train_time:42946ms step_avg:57.65ms
step:746/2330 train_time:43006ms step_avg:57.65ms
step:747/2330 train_time:43061ms step_avg:57.65ms
step:748/2330 train_time:43120ms step_avg:57.65ms
step:749/2330 train_time:43176ms step_avg:57.65ms
step:750/2330 train_time:43235ms step_avg:57.65ms
step:750/2330 val_loss:4.2263 train_time:43315ms step_avg:57.75ms
step:751/2330 train_time:43333ms step_avg:57.70ms
step:752/2330 train_time:43353ms step_avg:57.65ms
step:753/2330 train_time:43409ms step_avg:57.65ms
step:754/2330 train_time:43475ms step_avg:57.66ms
step:755/2330 train_time:43532ms step_avg:57.66ms
step:756/2330 train_time:43594ms step_avg:57.66ms
step:757/2330 train_time:43650ms step_avg:57.66ms
step:758/2330 train_time:43710ms step_avg:57.66ms
step:759/2330 train_time:43765ms step_avg:57.66ms
step:760/2330 train_time:43825ms step_avg:57.66ms
step:761/2330 train_time:43881ms step_avg:57.66ms
step:762/2330 train_time:43939ms step_avg:57.66ms
step:763/2330 train_time:43995ms step_avg:57.66ms
step:764/2330 train_time:44053ms step_avg:57.66ms
step:765/2330 train_time:44110ms step_avg:57.66ms
step:766/2330 train_time:44169ms step_avg:57.66ms
step:767/2330 train_time:44226ms step_avg:57.66ms
step:768/2330 train_time:44286ms step_avg:57.66ms
step:769/2330 train_time:44343ms step_avg:57.66ms
step:770/2330 train_time:44404ms step_avg:57.67ms
step:771/2330 train_time:44462ms step_avg:57.67ms
step:772/2330 train_time:44522ms step_avg:57.67ms
step:773/2330 train_time:44580ms step_avg:57.67ms
step:774/2330 train_time:44641ms step_avg:57.68ms
step:775/2330 train_time:44698ms step_avg:57.67ms
step:776/2330 train_time:44757ms step_avg:57.68ms
step:777/2330 train_time:44814ms step_avg:57.68ms
step:778/2330 train_time:44874ms step_avg:57.68ms
step:779/2330 train_time:44931ms step_avg:57.68ms
step:780/2330 train_time:44990ms step_avg:57.68ms
step:781/2330 train_time:45047ms step_avg:57.68ms
step:782/2330 train_time:45107ms step_avg:57.68ms
step:783/2330 train_time:45164ms step_avg:57.68ms
step:784/2330 train_time:45223ms step_avg:57.68ms
step:785/2330 train_time:45280ms step_avg:57.68ms
step:786/2330 train_time:45340ms step_avg:57.68ms
step:787/2330 train_time:45397ms step_avg:57.68ms
step:788/2330 train_time:45457ms step_avg:57.69ms
step:789/2330 train_time:45515ms step_avg:57.69ms
step:790/2330 train_time:45576ms step_avg:57.69ms
step:791/2330 train_time:45634ms step_avg:57.69ms
step:792/2330 train_time:45694ms step_avg:57.69ms
step:793/2330 train_time:45751ms step_avg:57.69ms
step:794/2330 train_time:45811ms step_avg:57.70ms
step:795/2330 train_time:45867ms step_avg:57.69ms
step:796/2330 train_time:45928ms step_avg:57.70ms
step:797/2330 train_time:45984ms step_avg:57.70ms
step:798/2330 train_time:46044ms step_avg:57.70ms
step:799/2330 train_time:46101ms step_avg:57.70ms
step:800/2330 train_time:46161ms step_avg:57.70ms
step:801/2330 train_time:46217ms step_avg:57.70ms
step:802/2330 train_time:46277ms step_avg:57.70ms
step:803/2330 train_time:46334ms step_avg:57.70ms
step:804/2330 train_time:46395ms step_avg:57.70ms
step:805/2330 train_time:46452ms step_avg:57.70ms
step:806/2330 train_time:46512ms step_avg:57.71ms
step:807/2330 train_time:46569ms step_avg:57.71ms
step:808/2330 train_time:46630ms step_avg:57.71ms
step:809/2330 train_time:46687ms step_avg:57.71ms
step:810/2330 train_time:46748ms step_avg:57.71ms
step:811/2330 train_time:46804ms step_avg:57.71ms
step:812/2330 train_time:46865ms step_avg:57.72ms
step:813/2330 train_time:46921ms step_avg:57.71ms
step:814/2330 train_time:46981ms step_avg:57.72ms
step:815/2330 train_time:47039ms step_avg:57.72ms
step:816/2330 train_time:47098ms step_avg:57.72ms
step:817/2330 train_time:47154ms step_avg:57.72ms
step:818/2330 train_time:47215ms step_avg:57.72ms
step:819/2330 train_time:47271ms step_avg:57.72ms
step:820/2330 train_time:47332ms step_avg:57.72ms
step:821/2330 train_time:47389ms step_avg:57.72ms
step:822/2330 train_time:47450ms step_avg:57.73ms
step:823/2330 train_time:47507ms step_avg:57.72ms
step:824/2330 train_time:47567ms step_avg:57.73ms
step:825/2330 train_time:47625ms step_avg:57.73ms
step:826/2330 train_time:47685ms step_avg:57.73ms
step:827/2330 train_time:47742ms step_avg:57.73ms
step:828/2330 train_time:47802ms step_avg:57.73ms
step:829/2330 train_time:47859ms step_avg:57.73ms
step:830/2330 train_time:47919ms step_avg:57.73ms
step:831/2330 train_time:47976ms step_avg:57.73ms
step:832/2330 train_time:48036ms step_avg:57.74ms
step:833/2330 train_time:48092ms step_avg:57.73ms
step:834/2330 train_time:48152ms step_avg:57.74ms
step:835/2330 train_time:48209ms step_avg:57.74ms
step:836/2330 train_time:48268ms step_avg:57.74ms
step:837/2330 train_time:48326ms step_avg:57.74ms
step:838/2330 train_time:48386ms step_avg:57.74ms
step:839/2330 train_time:48443ms step_avg:57.74ms
step:840/2330 train_time:48503ms step_avg:57.74ms
step:841/2330 train_time:48560ms step_avg:57.74ms
step:842/2330 train_time:48619ms step_avg:57.74ms
step:843/2330 train_time:48676ms step_avg:57.74ms
step:844/2330 train_time:48737ms step_avg:57.75ms
step:845/2330 train_time:48793ms step_avg:57.74ms
step:846/2330 train_time:48854ms step_avg:57.75ms
step:847/2330 train_time:48910ms step_avg:57.75ms
step:848/2330 train_time:48972ms step_avg:57.75ms
step:849/2330 train_time:49028ms step_avg:57.75ms
step:850/2330 train_time:49089ms step_avg:57.75ms
step:851/2330 train_time:49146ms step_avg:57.75ms
step:852/2330 train_time:49206ms step_avg:57.75ms
step:853/2330 train_time:49262ms step_avg:57.75ms
step:854/2330 train_time:49324ms step_avg:57.76ms
step:855/2330 train_time:49381ms step_avg:57.76ms
step:856/2330 train_time:49440ms step_avg:57.76ms
step:857/2330 train_time:49498ms step_avg:57.76ms
step:858/2330 train_time:49557ms step_avg:57.76ms
step:859/2330 train_time:49614ms step_avg:57.76ms
step:860/2330 train_time:49673ms step_avg:57.76ms
step:861/2330 train_time:49730ms step_avg:57.76ms
step:862/2330 train_time:49790ms step_avg:57.76ms
step:863/2330 train_time:49847ms step_avg:57.76ms
step:864/2330 train_time:49908ms step_avg:57.76ms
step:865/2330 train_time:49964ms step_avg:57.76ms
step:866/2330 train_time:50025ms step_avg:57.77ms
step:867/2330 train_time:50081ms step_avg:57.76ms
step:868/2330 train_time:50142ms step_avg:57.77ms
step:869/2330 train_time:50198ms step_avg:57.77ms
step:870/2330 train_time:50258ms step_avg:57.77ms
step:871/2330 train_time:50315ms step_avg:57.77ms
step:872/2330 train_time:50375ms step_avg:57.77ms
step:873/2330 train_time:50432ms step_avg:57.77ms
step:874/2330 train_time:50492ms step_avg:57.77ms
step:875/2330 train_time:50549ms step_avg:57.77ms
step:876/2330 train_time:50611ms step_avg:57.77ms
step:877/2330 train_time:50667ms step_avg:57.77ms
step:878/2330 train_time:50727ms step_avg:57.78ms
step:879/2330 train_time:50784ms step_avg:57.77ms
step:880/2330 train_time:50844ms step_avg:57.78ms
step:881/2330 train_time:50901ms step_avg:57.78ms
step:882/2330 train_time:50960ms step_avg:57.78ms
step:883/2330 train_time:51017ms step_avg:57.78ms
step:884/2330 train_time:51077ms step_avg:57.78ms
step:885/2330 train_time:51135ms step_avg:57.78ms
step:886/2330 train_time:51195ms step_avg:57.78ms
step:887/2330 train_time:51252ms step_avg:57.78ms
step:888/2330 train_time:51312ms step_avg:57.78ms
step:889/2330 train_time:51368ms step_avg:57.78ms
step:890/2330 train_time:51429ms step_avg:57.79ms
step:891/2330 train_time:51486ms step_avg:57.78ms
step:892/2330 train_time:51547ms step_avg:57.79ms
step:893/2330 train_time:51603ms step_avg:57.79ms
step:894/2330 train_time:51663ms step_avg:57.79ms
step:895/2330 train_time:51721ms step_avg:57.79ms
step:896/2330 train_time:51780ms step_avg:57.79ms
step:897/2330 train_time:51837ms step_avg:57.79ms
step:898/2330 train_time:51897ms step_avg:57.79ms
step:899/2330 train_time:51953ms step_avg:57.79ms
step:900/2330 train_time:52014ms step_avg:57.79ms
step:901/2330 train_time:52070ms step_avg:57.79ms
step:902/2330 train_time:52132ms step_avg:57.80ms
step:903/2330 train_time:52188ms step_avg:57.79ms
step:904/2330 train_time:52249ms step_avg:57.80ms
step:905/2330 train_time:52306ms step_avg:57.80ms
step:906/2330 train_time:52366ms step_avg:57.80ms
step:907/2330 train_time:52422ms step_avg:57.80ms
step:908/2330 train_time:52483ms step_avg:57.80ms
step:909/2330 train_time:52541ms step_avg:57.80ms
step:910/2330 train_time:52601ms step_avg:57.80ms
step:911/2330 train_time:52658ms step_avg:57.80ms
step:912/2330 train_time:52718ms step_avg:57.81ms
step:913/2330 train_time:52776ms step_avg:57.80ms
step:914/2330 train_time:52835ms step_avg:57.81ms
step:915/2330 train_time:52892ms step_avg:57.81ms
step:916/2330 train_time:52952ms step_avg:57.81ms
step:917/2330 train_time:53008ms step_avg:57.81ms
step:918/2330 train_time:53069ms step_avg:57.81ms
step:919/2330 train_time:53125ms step_avg:57.81ms
step:920/2330 train_time:53186ms step_avg:57.81ms
step:921/2330 train_time:53243ms step_avg:57.81ms
step:922/2330 train_time:53304ms step_avg:57.81ms
step:923/2330 train_time:53361ms step_avg:57.81ms
step:924/2330 train_time:53421ms step_avg:57.81ms
step:925/2330 train_time:53478ms step_avg:57.81ms
step:926/2330 train_time:53538ms step_avg:57.82ms
step:927/2330 train_time:53596ms step_avg:57.82ms
step:928/2330 train_time:53655ms step_avg:57.82ms
step:929/2330 train_time:53711ms step_avg:57.82ms
step:930/2330 train_time:53772ms step_avg:57.82ms
step:931/2330 train_time:53828ms step_avg:57.82ms
step:932/2330 train_time:53889ms step_avg:57.82ms
step:933/2330 train_time:53946ms step_avg:57.82ms
step:934/2330 train_time:54006ms step_avg:57.82ms
step:935/2330 train_time:54062ms step_avg:57.82ms
step:936/2330 train_time:54123ms step_avg:57.82ms
step:937/2330 train_time:54180ms step_avg:57.82ms
step:938/2330 train_time:54240ms step_avg:57.82ms
step:939/2330 train_time:54296ms step_avg:57.82ms
step:940/2330 train_time:54357ms step_avg:57.83ms
step:941/2330 train_time:54413ms step_avg:57.83ms
step:942/2330 train_time:54473ms step_avg:57.83ms
step:943/2330 train_time:54530ms step_avg:57.83ms
step:944/2330 train_time:54592ms step_avg:57.83ms
step:945/2330 train_time:54649ms step_avg:57.83ms
step:946/2330 train_time:54709ms step_avg:57.83ms
step:947/2330 train_time:54766ms step_avg:57.83ms
step:948/2330 train_time:54825ms step_avg:57.83ms
step:949/2330 train_time:54882ms step_avg:57.83ms
step:950/2330 train_time:54943ms step_avg:57.83ms
step:951/2330 train_time:55000ms step_avg:57.83ms
step:952/2330 train_time:55059ms step_avg:57.83ms
step:953/2330 train_time:55116ms step_avg:57.83ms
step:954/2330 train_time:55176ms step_avg:57.84ms
step:955/2330 train_time:55233ms step_avg:57.84ms
step:956/2330 train_time:55293ms step_avg:57.84ms
step:957/2330 train_time:55349ms step_avg:57.84ms
step:958/2330 train_time:55411ms step_avg:57.84ms
step:959/2330 train_time:55467ms step_avg:57.84ms
step:960/2330 train_time:55529ms step_avg:57.84ms
step:961/2330 train_time:55586ms step_avg:57.84ms
step:962/2330 train_time:55647ms step_avg:57.84ms
step:963/2330 train_time:55703ms step_avg:57.84ms
step:964/2330 train_time:55764ms step_avg:57.85ms
step:965/2330 train_time:55821ms step_avg:57.85ms
step:966/2330 train_time:55880ms step_avg:57.85ms
step:967/2330 train_time:55939ms step_avg:57.85ms
step:968/2330 train_time:55998ms step_avg:57.85ms
step:969/2330 train_time:56054ms step_avg:57.85ms
step:970/2330 train_time:56115ms step_avg:57.85ms
step:971/2330 train_time:56172ms step_avg:57.85ms
step:972/2330 train_time:56233ms step_avg:57.85ms
step:973/2330 train_time:56290ms step_avg:57.85ms
step:974/2330 train_time:56349ms step_avg:57.85ms
step:975/2330 train_time:56406ms step_avg:57.85ms
step:976/2330 train_time:56467ms step_avg:57.86ms
step:977/2330 train_time:56524ms step_avg:57.85ms
step:978/2330 train_time:56583ms step_avg:57.86ms
step:979/2330 train_time:56640ms step_avg:57.86ms
step:980/2330 train_time:56700ms step_avg:57.86ms
step:981/2330 train_time:56756ms step_avg:57.86ms
step:982/2330 train_time:56816ms step_avg:57.86ms
step:983/2330 train_time:56873ms step_avg:57.86ms
step:984/2330 train_time:56933ms step_avg:57.86ms
step:985/2330 train_time:56989ms step_avg:57.86ms
step:986/2330 train_time:57051ms step_avg:57.86ms
step:987/2330 train_time:57107ms step_avg:57.86ms
step:988/2330 train_time:57168ms step_avg:57.86ms
step:989/2330 train_time:57225ms step_avg:57.86ms
step:990/2330 train_time:57285ms step_avg:57.86ms
step:991/2330 train_time:57343ms step_avg:57.86ms
step:992/2330 train_time:57402ms step_avg:57.87ms
step:993/2330 train_time:57459ms step_avg:57.86ms
step:994/2330 train_time:57519ms step_avg:57.87ms
step:995/2330 train_time:57576ms step_avg:57.87ms
step:996/2330 train_time:57636ms step_avg:57.87ms
step:997/2330 train_time:57693ms step_avg:57.87ms
step:998/2330 train_time:57753ms step_avg:57.87ms
step:999/2330 train_time:57810ms step_avg:57.87ms
step:1000/2330 train_time:57869ms step_avg:57.87ms
step:1000/2330 val_loss:4.0802 train_time:57950ms step_avg:57.95ms
step:1001/2330 train_time:57969ms step_avg:57.91ms
step:1002/2330 train_time:57989ms step_avg:57.87ms
step:1003/2330 train_time:58042ms step_avg:57.87ms
step:1004/2330 train_time:58105ms step_avg:57.87ms
step:1005/2330 train_time:58162ms step_avg:57.87ms
step:1006/2330 train_time:58226ms step_avg:57.88ms
step:1007/2330 train_time:58282ms step_avg:57.88ms
step:1008/2330 train_time:58341ms step_avg:57.88ms
step:1009/2330 train_time:58397ms step_avg:57.88ms
step:1010/2330 train_time:58456ms step_avg:57.88ms
step:1011/2330 train_time:58512ms step_avg:57.88ms
step:1012/2330 train_time:58571ms step_avg:57.88ms
step:1013/2330 train_time:58627ms step_avg:57.87ms
step:1014/2330 train_time:58686ms step_avg:57.88ms
step:1015/2330 train_time:58742ms step_avg:57.87ms
step:1016/2330 train_time:58801ms step_avg:57.88ms
step:1017/2330 train_time:58862ms step_avg:57.88ms
step:1018/2330 train_time:58925ms step_avg:57.88ms
step:1019/2330 train_time:58984ms step_avg:57.88ms
step:1020/2330 train_time:59046ms step_avg:57.89ms
step:1021/2330 train_time:59103ms step_avg:57.89ms
step:1022/2330 train_time:59163ms step_avg:57.89ms
step:1023/2330 train_time:59220ms step_avg:57.89ms
step:1024/2330 train_time:59280ms step_avg:57.89ms
step:1025/2330 train_time:59336ms step_avg:57.89ms
step:1026/2330 train_time:59395ms step_avg:57.89ms
step:1027/2330 train_time:59452ms step_avg:57.89ms
step:1028/2330 train_time:59511ms step_avg:57.89ms
step:1029/2330 train_time:59567ms step_avg:57.89ms
step:1030/2330 train_time:59627ms step_avg:57.89ms
step:1031/2330 train_time:59684ms step_avg:57.89ms
step:1032/2330 train_time:59743ms step_avg:57.89ms
step:1033/2330 train_time:59801ms step_avg:57.89ms
step:1034/2330 train_time:59862ms step_avg:57.89ms
step:1035/2330 train_time:59920ms step_avg:57.89ms
step:1036/2330 train_time:59982ms step_avg:57.90ms
step:1037/2330 train_time:60039ms step_avg:57.90ms
step:1038/2330 train_time:60101ms step_avg:57.90ms
step:1039/2330 train_time:60158ms step_avg:57.90ms
step:1040/2330 train_time:60218ms step_avg:57.90ms
step:1041/2330 train_time:60275ms step_avg:57.90ms
step:1042/2330 train_time:60335ms step_avg:57.90ms
step:1043/2330 train_time:60391ms step_avg:57.90ms
step:1044/2330 train_time:60452ms step_avg:57.90ms
step:1045/2330 train_time:60508ms step_avg:57.90ms
step:1046/2330 train_time:60568ms step_avg:57.90ms
step:1047/2330 train_time:60624ms step_avg:57.90ms
step:1048/2330 train_time:60684ms step_avg:57.90ms
step:1049/2330 train_time:60741ms step_avg:57.90ms
step:1050/2330 train_time:60801ms step_avg:57.91ms
step:1051/2330 train_time:60858ms step_avg:57.91ms
step:1052/2330 train_time:60918ms step_avg:57.91ms
step:1053/2330 train_time:60976ms step_avg:57.91ms
step:1054/2330 train_time:61037ms step_avg:57.91ms
step:1055/2330 train_time:61094ms step_avg:57.91ms
step:1056/2330 train_time:61155ms step_avg:57.91ms
step:1057/2330 train_time:61212ms step_avg:57.91ms
step:1058/2330 train_time:61273ms step_avg:57.91ms
step:1059/2330 train_time:61330ms step_avg:57.91ms
step:1060/2330 train_time:61390ms step_avg:57.92ms
step:1061/2330 train_time:61447ms step_avg:57.91ms
step:1062/2330 train_time:61506ms step_avg:57.92ms
step:1063/2330 train_time:61562ms step_avg:57.91ms
step:1064/2330 train_time:61622ms step_avg:57.92ms
step:1065/2330 train_time:61678ms step_avg:57.91ms
step:1066/2330 train_time:61739ms step_avg:57.92ms
step:1067/2330 train_time:61795ms step_avg:57.92ms
step:1068/2330 train_time:61857ms step_avg:57.92ms
step:1069/2330 train_time:61913ms step_avg:57.92ms
step:1070/2330 train_time:61975ms step_avg:57.92ms
step:1071/2330 train_time:62031ms step_avg:57.92ms
step:1072/2330 train_time:62093ms step_avg:57.92ms
step:1073/2330 train_time:62149ms step_avg:57.92ms
step:1074/2330 train_time:62211ms step_avg:57.92ms
step:1075/2330 train_time:62267ms step_avg:57.92ms
step:1076/2330 train_time:62327ms step_avg:57.92ms
step:1077/2330 train_time:62384ms step_avg:57.92ms
step:1078/2330 train_time:62444ms step_avg:57.93ms
step:1079/2330 train_time:62500ms step_avg:57.92ms
step:1080/2330 train_time:62559ms step_avg:57.93ms
step:1081/2330 train_time:62616ms step_avg:57.92ms
step:1082/2330 train_time:62676ms step_avg:57.93ms
step:1083/2330 train_time:62733ms step_avg:57.92ms
step:1084/2330 train_time:62793ms step_avg:57.93ms
step:1085/2330 train_time:62849ms step_avg:57.93ms
step:1086/2330 train_time:62910ms step_avg:57.93ms
step:1087/2330 train_time:62967ms step_avg:57.93ms
step:1088/2330 train_time:63027ms step_avg:57.93ms
step:1089/2330 train_time:63085ms step_avg:57.93ms
step:1090/2330 train_time:63145ms step_avg:57.93ms
step:1091/2330 train_time:63201ms step_avg:57.93ms
step:1092/2330 train_time:63262ms step_avg:57.93ms
step:1093/2330 train_time:63318ms step_avg:57.93ms
step:1094/2330 train_time:63378ms step_avg:57.93ms
step:1095/2330 train_time:63435ms step_avg:57.93ms
step:1096/2330 train_time:63496ms step_avg:57.93ms
step:1097/2330 train_time:63553ms step_avg:57.93ms
step:1098/2330 train_time:63613ms step_avg:57.94ms
step:1099/2330 train_time:63669ms step_avg:57.93ms
step:1100/2330 train_time:63729ms step_avg:57.94ms
step:1101/2330 train_time:63786ms step_avg:57.94ms
step:1102/2330 train_time:63846ms step_avg:57.94ms
step:1103/2330 train_time:63903ms step_avg:57.94ms
step:1104/2330 train_time:63963ms step_avg:57.94ms
step:1105/2330 train_time:64020ms step_avg:57.94ms
step:1106/2330 train_time:64080ms step_avg:57.94ms
step:1107/2330 train_time:64137ms step_avg:57.94ms
step:1108/2330 train_time:64198ms step_avg:57.94ms
step:1109/2330 train_time:64255ms step_avg:57.94ms
step:1110/2330 train_time:64315ms step_avg:57.94ms
step:1111/2330 train_time:64371ms step_avg:57.94ms
step:1112/2330 train_time:64432ms step_avg:57.94ms
step:1113/2330 train_time:64489ms step_avg:57.94ms
step:1114/2330 train_time:64549ms step_avg:57.94ms
step:1115/2330 train_time:64606ms step_avg:57.94ms
step:1116/2330 train_time:64665ms step_avg:57.94ms
step:1117/2330 train_time:64722ms step_avg:57.94ms
step:1118/2330 train_time:64782ms step_avg:57.94ms
step:1119/2330 train_time:64839ms step_avg:57.94ms
step:1120/2330 train_time:64899ms step_avg:57.95ms
step:1121/2330 train_time:64956ms step_avg:57.94ms
step:1122/2330 train_time:65016ms step_avg:57.95ms
step:1123/2330 train_time:65073ms step_avg:57.95ms
step:1124/2330 train_time:65133ms step_avg:57.95ms
step:1125/2330 train_time:65191ms step_avg:57.95ms
step:1126/2330 train_time:65250ms step_avg:57.95ms
step:1127/2330 train_time:65308ms step_avg:57.95ms
step:1128/2330 train_time:65367ms step_avg:57.95ms
step:1129/2330 train_time:65424ms step_avg:57.95ms
step:1130/2330 train_time:65484ms step_avg:57.95ms
step:1131/2330 train_time:65542ms step_avg:57.95ms
step:1132/2330 train_time:65601ms step_avg:57.95ms
step:1133/2330 train_time:65657ms step_avg:57.95ms
step:1134/2330 train_time:65718ms step_avg:57.95ms
step:1135/2330 train_time:65774ms step_avg:57.95ms
step:1136/2330 train_time:65835ms step_avg:57.95ms
step:1137/2330 train_time:65892ms step_avg:57.95ms
step:1138/2330 train_time:65952ms step_avg:57.95ms
step:1139/2330 train_time:66009ms step_avg:57.95ms
step:1140/2330 train_time:66069ms step_avg:57.95ms
step:1141/2330 train_time:66126ms step_avg:57.95ms
step:1142/2330 train_time:66185ms step_avg:57.96ms
step:1143/2330 train_time:66242ms step_avg:57.95ms
step:1144/2330 train_time:66302ms step_avg:57.96ms
step:1145/2330 train_time:66359ms step_avg:57.96ms
step:1146/2330 train_time:66419ms step_avg:57.96ms
step:1147/2330 train_time:66475ms step_avg:57.96ms
step:1148/2330 train_time:66537ms step_avg:57.96ms
step:1149/2330 train_time:66593ms step_avg:57.96ms
step:1150/2330 train_time:66654ms step_avg:57.96ms
step:1151/2330 train_time:66711ms step_avg:57.96ms
step:1152/2330 train_time:66772ms step_avg:57.96ms
step:1153/2330 train_time:66829ms step_avg:57.96ms
step:1154/2330 train_time:66888ms step_avg:57.96ms
step:1155/2330 train_time:66945ms step_avg:57.96ms
step:1156/2330 train_time:67005ms step_avg:57.96ms
step:1157/2330 train_time:67062ms step_avg:57.96ms
step:1158/2330 train_time:67122ms step_avg:57.96ms
step:1159/2330 train_time:67179ms step_avg:57.96ms
step:1160/2330 train_time:67240ms step_avg:57.97ms
step:1161/2330 train_time:67296ms step_avg:57.96ms
step:1162/2330 train_time:67356ms step_avg:57.97ms
step:1163/2330 train_time:67412ms step_avg:57.96ms
step:1164/2330 train_time:67474ms step_avg:57.97ms
step:1165/2330 train_time:67531ms step_avg:57.97ms
step:1166/2330 train_time:67592ms step_avg:57.97ms
step:1167/2330 train_time:67649ms step_avg:57.97ms
step:1168/2330 train_time:67709ms step_avg:57.97ms
step:1169/2330 train_time:67765ms step_avg:57.97ms
step:1170/2330 train_time:67825ms step_avg:57.97ms
step:1171/2330 train_time:67882ms step_avg:57.97ms
step:1172/2330 train_time:67942ms step_avg:57.97ms
step:1173/2330 train_time:67999ms step_avg:57.97ms
step:1174/2330 train_time:68059ms step_avg:57.97ms
step:1175/2330 train_time:68116ms step_avg:57.97ms
step:1176/2330 train_time:68176ms step_avg:57.97ms
step:1177/2330 train_time:68232ms step_avg:57.97ms
step:1178/2330 train_time:68294ms step_avg:57.97ms
step:1179/2330 train_time:68350ms step_avg:57.97ms
step:1180/2330 train_time:68410ms step_avg:57.97ms
step:1181/2330 train_time:68467ms step_avg:57.97ms
step:1182/2330 train_time:68527ms step_avg:57.98ms
step:1183/2330 train_time:68584ms step_avg:57.97ms
step:1184/2330 train_time:68644ms step_avg:57.98ms
step:1185/2330 train_time:68701ms step_avg:57.98ms
step:1186/2330 train_time:68761ms step_avg:57.98ms
step:1187/2330 train_time:68817ms step_avg:57.98ms
step:1188/2330 train_time:68879ms step_avg:57.98ms
step:1189/2330 train_time:68935ms step_avg:57.98ms
step:1190/2330 train_time:68996ms step_avg:57.98ms
step:1191/2330 train_time:69052ms step_avg:57.98ms
step:1192/2330 train_time:69113ms step_avg:57.98ms
step:1193/2330 train_time:69170ms step_avg:57.98ms
step:1194/2330 train_time:69231ms step_avg:57.98ms
step:1195/2330 train_time:69287ms step_avg:57.98ms
step:1196/2330 train_time:69347ms step_avg:57.98ms
step:1197/2330 train_time:69403ms step_avg:57.98ms
step:1198/2330 train_time:69463ms step_avg:57.98ms
step:1199/2330 train_time:69520ms step_avg:57.98ms
step:1200/2330 train_time:69580ms step_avg:57.98ms
step:1201/2330 train_time:69637ms step_avg:57.98ms
step:1202/2330 train_time:69697ms step_avg:57.98ms
step:1203/2330 train_time:69754ms step_avg:57.98ms
step:1204/2330 train_time:69815ms step_avg:57.99ms
step:1205/2330 train_time:69872ms step_avg:57.98ms
step:1206/2330 train_time:69932ms step_avg:57.99ms
step:1207/2330 train_time:69989ms step_avg:57.99ms
step:1208/2330 train_time:70050ms step_avg:57.99ms
step:1209/2330 train_time:70107ms step_avg:57.99ms
step:1210/2330 train_time:70167ms step_avg:57.99ms
step:1211/2330 train_time:70224ms step_avg:57.99ms
step:1212/2330 train_time:70284ms step_avg:57.99ms
step:1213/2330 train_time:70342ms step_avg:57.99ms
step:1214/2330 train_time:70401ms step_avg:57.99ms
step:1215/2330 train_time:70458ms step_avg:57.99ms
step:1216/2330 train_time:70518ms step_avg:57.99ms
step:1217/2330 train_time:70575ms step_avg:57.99ms
step:1218/2330 train_time:70636ms step_avg:57.99ms
step:1219/2330 train_time:70693ms step_avg:57.99ms
step:1220/2330 train_time:70753ms step_avg:57.99ms
step:1221/2330 train_time:70809ms step_avg:57.99ms
step:1222/2330 train_time:70869ms step_avg:57.99ms
step:1223/2330 train_time:70926ms step_avg:57.99ms
step:1224/2330 train_time:70986ms step_avg:58.00ms
step:1225/2330 train_time:71043ms step_avg:57.99ms
step:1226/2330 train_time:71103ms step_avg:58.00ms
step:1227/2330 train_time:71160ms step_avg:57.99ms
step:1228/2330 train_time:71220ms step_avg:58.00ms
step:1229/2330 train_time:71277ms step_avg:58.00ms
step:1230/2330 train_time:71337ms step_avg:58.00ms
step:1231/2330 train_time:71394ms step_avg:58.00ms
step:1232/2330 train_time:71454ms step_avg:58.00ms
step:1233/2330 train_time:71511ms step_avg:58.00ms
step:1234/2330 train_time:71572ms step_avg:58.00ms
step:1235/2330 train_time:71629ms step_avg:58.00ms
step:1236/2330 train_time:71688ms step_avg:58.00ms
step:1237/2330 train_time:71746ms step_avg:58.00ms
step:1238/2330 train_time:71805ms step_avg:58.00ms
step:1239/2330 train_time:71862ms step_avg:58.00ms
step:1240/2330 train_time:71922ms step_avg:58.00ms
step:1241/2330 train_time:71979ms step_avg:58.00ms
step:1242/2330 train_time:72039ms step_avg:58.00ms
step:1243/2330 train_time:72096ms step_avg:58.00ms
step:1244/2330 train_time:72157ms step_avg:58.00ms
step:1245/2330 train_time:72213ms step_avg:58.00ms
step:1246/2330 train_time:72276ms step_avg:58.01ms
step:1247/2330 train_time:72333ms step_avg:58.01ms
step:1248/2330 train_time:72393ms step_avg:58.01ms
step:1249/2330 train_time:72449ms step_avg:58.01ms
step:1250/2330 train_time:72509ms step_avg:58.01ms
step:1250/2330 val_loss:4.0000 train_time:72590ms step_avg:58.07ms
step:1251/2330 train_time:72606ms step_avg:58.04ms
step:1252/2330 train_time:72629ms step_avg:58.01ms
step:1253/2330 train_time:72687ms step_avg:58.01ms
step:1254/2330 train_time:72756ms step_avg:58.02ms
step:1255/2330 train_time:72815ms step_avg:58.02ms
step:1256/2330 train_time:72874ms step_avg:58.02ms
step:1257/2330 train_time:72931ms step_avg:58.02ms
step:1258/2330 train_time:72990ms step_avg:58.02ms
step:1259/2330 train_time:73047ms step_avg:58.02ms
step:1260/2330 train_time:73106ms step_avg:58.02ms
step:1261/2330 train_time:73163ms step_avg:58.02ms
step:1262/2330 train_time:73222ms step_avg:58.02ms
step:1263/2330 train_time:73278ms step_avg:58.02ms
step:1264/2330 train_time:73338ms step_avg:58.02ms
step:1265/2330 train_time:73394ms step_avg:58.02ms
step:1266/2330 train_time:73453ms step_avg:58.02ms
step:1267/2330 train_time:73509ms step_avg:58.02ms
step:1268/2330 train_time:73568ms step_avg:58.02ms
step:1269/2330 train_time:73625ms step_avg:58.02ms
step:1270/2330 train_time:73688ms step_avg:58.02ms
step:1271/2330 train_time:73747ms step_avg:58.02ms
step:1272/2330 train_time:73809ms step_avg:58.03ms
step:1273/2330 train_time:73867ms step_avg:58.03ms
step:1274/2330 train_time:73928ms step_avg:58.03ms
step:1275/2330 train_time:73984ms step_avg:58.03ms
step:1276/2330 train_time:74045ms step_avg:58.03ms
step:1277/2330 train_time:74101ms step_avg:58.03ms
step:1278/2330 train_time:74161ms step_avg:58.03ms
step:1279/2330 train_time:74218ms step_avg:58.03ms
step:1280/2330 train_time:74277ms step_avg:58.03ms
step:1281/2330 train_time:74333ms step_avg:58.03ms
step:1282/2330 train_time:74393ms step_avg:58.03ms
step:1283/2330 train_time:74450ms step_avg:58.03ms
step:1284/2330 train_time:74509ms step_avg:58.03ms
step:1285/2330 train_time:74566ms step_avg:58.03ms
step:1286/2330 train_time:74626ms step_avg:58.03ms
step:1287/2330 train_time:74684ms step_avg:58.03ms
step:1288/2330 train_time:74744ms step_avg:58.03ms
step:1289/2330 train_time:74801ms step_avg:58.03ms
step:1290/2330 train_time:75323ms step_avg:58.39ms
step:1291/2330 train_time:75340ms step_avg:58.36ms
step:1292/2330 train_time:75537ms step_avg:58.46ms
step:1293/2330 train_time:75593ms step_avg:58.46ms
step:1294/2330 train_time:75651ms step_avg:58.46ms
step:1295/2330 train_time:75707ms step_avg:58.46ms
step:1296/2330 train_time:75766ms step_avg:58.46ms
step:1297/2330 train_time:75822ms step_avg:58.46ms
step:1298/2330 train_time:75881ms step_avg:58.46ms
step:1299/2330 train_time:75937ms step_avg:58.46ms
step:1300/2330 train_time:75996ms step_avg:58.46ms
step:1301/2330 train_time:76053ms step_avg:58.46ms
step:1302/2330 train_time:76112ms step_avg:58.46ms
step:1303/2330 train_time:76168ms step_avg:58.46ms
step:1304/2330 train_time:76227ms step_avg:58.46ms
step:1305/2330 train_time:76283ms step_avg:58.45ms
step:1306/2330 train_time:76342ms step_avg:58.46ms
step:1307/2330 train_time:76405ms step_avg:58.46ms
step:1308/2330 train_time:76469ms step_avg:58.46ms
step:1309/2330 train_time:76528ms step_avg:58.46ms
step:1310/2330 train_time:76589ms step_avg:58.46ms
step:1311/2330 train_time:76646ms step_avg:58.46ms
step:1312/2330 train_time:76706ms step_avg:58.46ms
step:1313/2330 train_time:76763ms step_avg:58.46ms
step:1314/2330 train_time:76822ms step_avg:58.46ms
step:1315/2330 train_time:76879ms step_avg:58.46ms
step:1316/2330 train_time:76938ms step_avg:58.46ms
step:1317/2330 train_time:76995ms step_avg:58.46ms
step:1318/2330 train_time:77054ms step_avg:58.46ms
step:1319/2330 train_time:77109ms step_avg:58.46ms
step:1320/2330 train_time:77169ms step_avg:58.46ms
step:1321/2330 train_time:77225ms step_avg:58.46ms
step:1322/2330 train_time:77285ms step_avg:58.46ms
step:1323/2330 train_time:77343ms step_avg:58.46ms
step:1324/2330 train_time:77405ms step_avg:58.46ms
step:1325/2330 train_time:77464ms step_avg:58.46ms
step:1326/2330 train_time:77525ms step_avg:58.47ms
step:1327/2330 train_time:77584ms step_avg:58.47ms
step:1328/2330 train_time:77644ms step_avg:58.47ms
step:1329/2330 train_time:77702ms step_avg:58.47ms
step:1330/2330 train_time:77762ms step_avg:58.47ms
step:1331/2330 train_time:77819ms step_avg:58.47ms
step:1332/2330 train_time:77878ms step_avg:58.47ms
step:1333/2330 train_time:77935ms step_avg:58.47ms
step:1334/2330 train_time:77994ms step_avg:58.47ms
step:1335/2330 train_time:78050ms step_avg:58.46ms
step:1336/2330 train_time:78110ms step_avg:58.47ms
step:1337/2330 train_time:78167ms step_avg:58.46ms
step:1338/2330 train_time:78226ms step_avg:58.46ms
step:1339/2330 train_time:78282ms step_avg:58.46ms
step:1340/2330 train_time:78342ms step_avg:58.46ms
step:1341/2330 train_time:78400ms step_avg:58.46ms
step:1342/2330 train_time:78461ms step_avg:58.47ms
step:1343/2330 train_time:78519ms step_avg:58.47ms
step:1344/2330 train_time:78580ms step_avg:58.47ms
step:1345/2330 train_time:78636ms step_avg:58.47ms
step:1346/2330 train_time:78697ms step_avg:58.47ms
step:1347/2330 train_time:78753ms step_avg:58.47ms
step:1348/2330 train_time:78814ms step_avg:58.47ms
step:1349/2330 train_time:78870ms step_avg:58.47ms
step:1350/2330 train_time:78930ms step_avg:58.47ms
step:1351/2330 train_time:78987ms step_avg:58.47ms
step:1352/2330 train_time:79046ms step_avg:58.47ms
step:1353/2330 train_time:79103ms step_avg:58.46ms
step:1354/2330 train_time:79162ms step_avg:58.46ms
step:1355/2330 train_time:79218ms step_avg:58.46ms
step:1356/2330 train_time:79278ms step_avg:58.46ms
step:1357/2330 train_time:79335ms step_avg:58.46ms
step:1358/2330 train_time:79396ms step_avg:58.47ms
step:1359/2330 train_time:79453ms step_avg:58.46ms
step:1360/2330 train_time:79514ms step_avg:58.47ms
step:1361/2330 train_time:79570ms step_avg:58.46ms
step:1362/2330 train_time:79631ms step_avg:58.47ms
step:1363/2330 train_time:79687ms step_avg:58.46ms
step:1364/2330 train_time:79747ms step_avg:58.47ms
step:1365/2330 train_time:79805ms step_avg:58.47ms
step:1366/2330 train_time:79864ms step_avg:58.47ms
step:1367/2330 train_time:79922ms step_avg:58.47ms
step:1368/2330 train_time:79981ms step_avg:58.47ms
step:1369/2330 train_time:80037ms step_avg:58.46ms
step:1370/2330 train_time:80098ms step_avg:58.47ms
step:1371/2330 train_time:80154ms step_avg:58.46ms
step:1372/2330 train_time:80215ms step_avg:58.47ms
step:1373/2330 train_time:80272ms step_avg:58.46ms
step:1374/2330 train_time:80331ms step_avg:58.47ms
step:1375/2330 train_time:80387ms step_avg:58.46ms
step:1376/2330 train_time:80448ms step_avg:58.47ms
step:1377/2330 train_time:80506ms step_avg:58.46ms
step:1378/2330 train_time:80566ms step_avg:58.47ms
step:1379/2330 train_time:80624ms step_avg:58.47ms
step:1380/2330 train_time:80684ms step_avg:58.47ms
step:1381/2330 train_time:80740ms step_avg:58.47ms
step:1382/2330 train_time:80801ms step_avg:58.47ms
step:1383/2330 train_time:80858ms step_avg:58.47ms
step:1384/2330 train_time:80919ms step_avg:58.47ms
step:1385/2330 train_time:80976ms step_avg:58.47ms
step:1386/2330 train_time:81035ms step_avg:58.47ms
step:1387/2330 train_time:81092ms step_avg:58.47ms
step:1388/2330 train_time:81152ms step_avg:58.47ms
step:1389/2330 train_time:81209ms step_avg:58.47ms
step:1390/2330 train_time:81268ms step_avg:58.47ms
step:1391/2330 train_time:81325ms step_avg:58.47ms
step:1392/2330 train_time:81385ms step_avg:58.47ms
step:1393/2330 train_time:81442ms step_avg:58.47ms
step:1394/2330 train_time:81502ms step_avg:58.47ms
step:1395/2330 train_time:81559ms step_avg:58.46ms
step:1396/2330 train_time:81619ms step_avg:58.47ms
step:1397/2330 train_time:81676ms step_avg:58.47ms
step:1398/2330 train_time:81737ms step_avg:58.47ms
step:1399/2330 train_time:81794ms step_avg:58.47ms
step:1400/2330 train_time:81853ms step_avg:58.47ms
step:1401/2330 train_time:81910ms step_avg:58.47ms
step:1402/2330 train_time:81970ms step_avg:58.47ms
step:1403/2330 train_time:82028ms step_avg:58.47ms
step:1404/2330 train_time:82087ms step_avg:58.47ms
step:1405/2330 train_time:82144ms step_avg:58.47ms
step:1406/2330 train_time:82205ms step_avg:58.47ms
step:1407/2330 train_time:82262ms step_avg:58.47ms
step:1408/2330 train_time:82321ms step_avg:58.47ms
step:1409/2330 train_time:82378ms step_avg:58.47ms
step:1410/2330 train_time:82438ms step_avg:58.47ms
step:1411/2330 train_time:82495ms step_avg:58.47ms
step:1412/2330 train_time:82555ms step_avg:58.47ms
step:1413/2330 train_time:82612ms step_avg:58.47ms
step:1414/2330 train_time:82672ms step_avg:58.47ms
step:1415/2330 train_time:82730ms step_avg:58.47ms
step:1416/2330 train_time:82790ms step_avg:58.47ms
step:1417/2330 train_time:82847ms step_avg:58.47ms
step:1418/2330 train_time:82907ms step_avg:58.47ms
step:1419/2330 train_time:82964ms step_avg:58.47ms
step:1420/2330 train_time:83024ms step_avg:58.47ms
step:1421/2330 train_time:83080ms step_avg:58.47ms
step:1422/2330 train_time:83140ms step_avg:58.47ms
step:1423/2330 train_time:83197ms step_avg:58.47ms
step:1424/2330 train_time:83257ms step_avg:58.47ms
step:1425/2330 train_time:83314ms step_avg:58.47ms
step:1426/2330 train_time:83373ms step_avg:58.47ms
step:1427/2330 train_time:83430ms step_avg:58.47ms
step:1428/2330 train_time:83490ms step_avg:58.47ms
step:1429/2330 train_time:83546ms step_avg:58.46ms
step:1430/2330 train_time:83606ms step_avg:58.47ms
step:1431/2330 train_time:83663ms step_avg:58.46ms
step:1432/2330 train_time:83724ms step_avg:58.47ms
step:1433/2330 train_time:83781ms step_avg:58.47ms
step:1434/2330 train_time:83840ms step_avg:58.47ms
step:1435/2330 train_time:83897ms step_avg:58.47ms
step:1436/2330 train_time:83958ms step_avg:58.47ms
step:1437/2330 train_time:84014ms step_avg:58.46ms
step:1438/2330 train_time:84075ms step_avg:58.47ms
step:1439/2330 train_time:84131ms step_avg:58.47ms
step:1440/2330 train_time:84191ms step_avg:58.47ms
step:1441/2330 train_time:84249ms step_avg:58.47ms
step:1442/2330 train_time:84309ms step_avg:58.47ms
step:1443/2330 train_time:84366ms step_avg:58.47ms
step:1444/2330 train_time:84427ms step_avg:58.47ms
step:1445/2330 train_time:84484ms step_avg:58.47ms
step:1446/2330 train_time:84543ms step_avg:58.47ms
step:1447/2330 train_time:84601ms step_avg:58.47ms
step:1448/2330 train_time:84661ms step_avg:58.47ms
step:1449/2330 train_time:84718ms step_avg:58.47ms
step:1450/2330 train_time:84778ms step_avg:58.47ms
step:1451/2330 train_time:84835ms step_avg:58.47ms
step:1452/2330 train_time:84896ms step_avg:58.47ms
step:1453/2330 train_time:84953ms step_avg:58.47ms
step:1454/2330 train_time:85013ms step_avg:58.47ms
step:1455/2330 train_time:85070ms step_avg:58.47ms
step:1456/2330 train_time:85130ms step_avg:58.47ms
step:1457/2330 train_time:85187ms step_avg:58.47ms
step:1458/2330 train_time:85247ms step_avg:58.47ms
step:1459/2330 train_time:85304ms step_avg:58.47ms
step:1460/2330 train_time:85364ms step_avg:58.47ms
step:1461/2330 train_time:85421ms step_avg:58.47ms
step:1462/2330 train_time:85481ms step_avg:58.47ms
step:1463/2330 train_time:85538ms step_avg:58.47ms
step:1464/2330 train_time:85597ms step_avg:58.47ms
step:1465/2330 train_time:85655ms step_avg:58.47ms
step:1466/2330 train_time:85715ms step_avg:58.47ms
step:1467/2330 train_time:85772ms step_avg:58.47ms
step:1468/2330 train_time:85832ms step_avg:58.47ms
step:1469/2330 train_time:85888ms step_avg:58.47ms
step:1470/2330 train_time:85948ms step_avg:58.47ms
step:1471/2330 train_time:86004ms step_avg:58.47ms
step:1472/2330 train_time:86065ms step_avg:58.47ms
step:1473/2330 train_time:86122ms step_avg:58.47ms
step:1474/2330 train_time:86182ms step_avg:58.47ms
step:1475/2330 train_time:86239ms step_avg:58.47ms
step:1476/2330 train_time:86299ms step_avg:58.47ms
step:1477/2330 train_time:86356ms step_avg:58.47ms
step:1478/2330 train_time:86416ms step_avg:58.47ms
step:1479/2330 train_time:86473ms step_avg:58.47ms
step:1480/2330 train_time:86532ms step_avg:58.47ms
step:1481/2330 train_time:86590ms step_avg:58.47ms
step:1482/2330 train_time:86649ms step_avg:58.47ms
step:1483/2330 train_time:86707ms step_avg:58.47ms
step:1484/2330 train_time:86768ms step_avg:58.47ms
step:1485/2330 train_time:86825ms step_avg:58.47ms
step:1486/2330 train_time:86885ms step_avg:58.47ms
step:1487/2330 train_time:86942ms step_avg:58.47ms
step:1488/2330 train_time:87002ms step_avg:58.47ms
step:1489/2330 train_time:87058ms step_avg:58.47ms
step:1490/2330 train_time:87119ms step_avg:58.47ms
step:1491/2330 train_time:87175ms step_avg:58.47ms
step:1492/2330 train_time:87235ms step_avg:58.47ms
step:1493/2330 train_time:87292ms step_avg:58.47ms
step:1494/2330 train_time:87353ms step_avg:58.47ms
step:1495/2330 train_time:87409ms step_avg:58.47ms
step:1496/2330 train_time:87470ms step_avg:58.47ms
step:1497/2330 train_time:87527ms step_avg:58.47ms
step:1498/2330 train_time:87587ms step_avg:58.47ms
step:1499/2330 train_time:87644ms step_avg:58.47ms
step:1500/2330 train_time:87704ms step_avg:58.47ms
step:1500/2330 val_loss:3.9186 train_time:87784ms step_avg:58.52ms
step:1501/2330 train_time:87802ms step_avg:58.50ms
step:1502/2330 train_time:87823ms step_avg:58.47ms
step:1503/2330 train_time:87881ms step_avg:58.47ms
step:1504/2330 train_time:87946ms step_avg:58.47ms
step:1505/2330 train_time:88003ms step_avg:58.47ms
step:1506/2330 train_time:88066ms step_avg:58.48ms
step:1507/2330 train_time:88122ms step_avg:58.48ms
step:1508/2330 train_time:88184ms step_avg:58.48ms
step:1509/2330 train_time:88239ms step_avg:58.48ms
step:1510/2330 train_time:88300ms step_avg:58.48ms
step:1511/2330 train_time:88356ms step_avg:58.48ms
step:1512/2330 train_time:88416ms step_avg:58.48ms
step:1513/2330 train_time:88472ms step_avg:58.47ms
step:1514/2330 train_time:88531ms step_avg:58.47ms
step:1515/2330 train_time:88587ms step_avg:58.47ms
step:1516/2330 train_time:88646ms step_avg:58.47ms
step:1517/2330 train_time:88702ms step_avg:58.47ms
step:1518/2330 train_time:88762ms step_avg:58.47ms
step:1519/2330 train_time:88821ms step_avg:58.47ms
step:1520/2330 train_time:88882ms step_avg:58.48ms
step:1521/2330 train_time:88940ms step_avg:58.47ms
step:1522/2330 train_time:89002ms step_avg:58.48ms
step:1523/2330 train_time:89058ms step_avg:58.48ms
step:1524/2330 train_time:89121ms step_avg:58.48ms
step:1525/2330 train_time:89178ms step_avg:58.48ms
step:1526/2330 train_time:89238ms step_avg:58.48ms
step:1527/2330 train_time:89294ms step_avg:58.48ms
step:1528/2330 train_time:89355ms step_avg:58.48ms
step:1529/2330 train_time:89413ms step_avg:58.48ms
step:1530/2330 train_time:89470ms step_avg:58.48ms
step:1531/2330 train_time:89527ms step_avg:58.48ms
step:1532/2330 train_time:89587ms step_avg:58.48ms
step:1533/2330 train_time:89644ms step_avg:58.48ms
step:1534/2330 train_time:89704ms step_avg:58.48ms
step:1535/2330 train_time:89761ms step_avg:58.48ms
step:1536/2330 train_time:89822ms step_avg:58.48ms
step:1537/2330 train_time:89879ms step_avg:58.48ms
step:1538/2330 train_time:89941ms step_avg:58.48ms
step:1539/2330 train_time:89999ms step_avg:58.48ms
step:1540/2330 train_time:90060ms step_avg:58.48ms
step:1541/2330 train_time:90117ms step_avg:58.48ms
step:1542/2330 train_time:90180ms step_avg:58.48ms
step:1543/2330 train_time:90237ms step_avg:58.48ms
step:1544/2330 train_time:90298ms step_avg:58.48ms
step:1545/2330 train_time:90354ms step_avg:58.48ms
step:1546/2330 train_time:90416ms step_avg:58.48ms
step:1547/2330 train_time:90472ms step_avg:58.48ms
step:1548/2330 train_time:90532ms step_avg:58.48ms
step:1549/2330 train_time:90590ms step_avg:58.48ms
step:1550/2330 train_time:90650ms step_avg:58.48ms
step:1551/2330 train_time:90708ms step_avg:58.48ms
step:1552/2330 train_time:90768ms step_avg:58.48ms
step:1553/2330 train_time:90826ms step_avg:58.48ms
step:1554/2330 train_time:90887ms step_avg:58.49ms
step:1555/2330 train_time:90945ms step_avg:58.49ms
step:1556/2330 train_time:91006ms step_avg:58.49ms
step:1557/2330 train_time:91064ms step_avg:58.49ms
step:1558/2330 train_time:91126ms step_avg:58.49ms
step:1559/2330 train_time:91183ms step_avg:58.49ms
step:1560/2330 train_time:91245ms step_avg:58.49ms
step:1561/2330 train_time:91301ms step_avg:58.49ms
step:1562/2330 train_time:91364ms step_avg:58.49ms
step:1563/2330 train_time:91420ms step_avg:58.49ms
step:1564/2330 train_time:91481ms step_avg:58.49ms
step:1565/2330 train_time:91538ms step_avg:58.49ms
step:1566/2330 train_time:91599ms step_avg:58.49ms
step:1567/2330 train_time:91655ms step_avg:58.49ms
step:1568/2330 train_time:91717ms step_avg:58.49ms
step:1569/2330 train_time:91774ms step_avg:58.49ms
step:1570/2330 train_time:91834ms step_avg:58.49ms
step:1571/2330 train_time:91892ms step_avg:58.49ms
step:1572/2330 train_time:91952ms step_avg:58.49ms
step:1573/2330 train_time:92012ms step_avg:58.49ms
step:1574/2330 train_time:92072ms step_avg:58.50ms
step:1575/2330 train_time:92130ms step_avg:58.49ms
step:1576/2330 train_time:92191ms step_avg:58.50ms
step:1577/2330 train_time:92249ms step_avg:58.50ms
step:1578/2330 train_time:92309ms step_avg:58.50ms
step:1579/2330 train_time:92366ms step_avg:58.50ms
step:1580/2330 train_time:92428ms step_avg:58.50ms
step:1581/2330 train_time:92485ms step_avg:58.50ms
step:1582/2330 train_time:92546ms step_avg:58.50ms
step:1583/2330 train_time:92602ms step_avg:58.50ms
step:1584/2330 train_time:92665ms step_avg:58.50ms
step:1585/2330 train_time:92721ms step_avg:58.50ms
step:1586/2330 train_time:92782ms step_avg:58.50ms
step:1587/2330 train_time:92839ms step_avg:58.50ms
step:1588/2330 train_time:92900ms step_avg:58.50ms
step:1589/2330 train_time:92957ms step_avg:58.50ms
step:1590/2330 train_time:93019ms step_avg:58.50ms
step:1591/2330 train_time:93075ms step_avg:58.50ms
step:1592/2330 train_time:93137ms step_avg:58.50ms
step:1593/2330 train_time:93195ms step_avg:58.50ms
step:1594/2330 train_time:93255ms step_avg:58.50ms
step:1595/2330 train_time:93314ms step_avg:58.50ms
step:1596/2330 train_time:93374ms step_avg:58.50ms
step:1597/2330 train_time:93431ms step_avg:58.50ms
step:1598/2330 train_time:93491ms step_avg:58.50ms
step:1599/2330 train_time:93549ms step_avg:58.50ms
step:1600/2330 train_time:93609ms step_avg:58.51ms
step:1601/2330 train_time:93667ms step_avg:58.51ms
step:1602/2330 train_time:93727ms step_avg:58.51ms
step:1603/2330 train_time:93785ms step_avg:58.51ms
step:1604/2330 train_time:93845ms step_avg:58.51ms
step:1605/2330 train_time:93902ms step_avg:58.51ms
step:1606/2330 train_time:93963ms step_avg:58.51ms
step:1607/2330 train_time:94020ms step_avg:58.51ms
step:1608/2330 train_time:94082ms step_avg:58.51ms
step:1609/2330 train_time:94139ms step_avg:58.51ms
step:1610/2330 train_time:94201ms step_avg:58.51ms
step:1611/2330 train_time:94257ms step_avg:58.51ms
step:1612/2330 train_time:94320ms step_avg:58.51ms
step:1613/2330 train_time:94377ms step_avg:58.51ms
step:1614/2330 train_time:94438ms step_avg:58.51ms
step:1615/2330 train_time:94495ms step_avg:58.51ms
step:1616/2330 train_time:94556ms step_avg:58.51ms
step:1617/2330 train_time:94613ms step_avg:58.51ms
step:1618/2330 train_time:94673ms step_avg:58.51ms
step:1619/2330 train_time:94731ms step_avg:58.51ms
step:1620/2330 train_time:94791ms step_avg:58.51ms
step:1621/2330 train_time:94849ms step_avg:58.51ms
step:1622/2330 train_time:94910ms step_avg:58.51ms
step:1623/2330 train_time:94967ms step_avg:58.51ms
step:1624/2330 train_time:95028ms step_avg:58.51ms
step:1625/2330 train_time:95085ms step_avg:58.51ms
step:1626/2330 train_time:95148ms step_avg:58.52ms
step:1627/2330 train_time:95205ms step_avg:58.52ms
step:1628/2330 train_time:95265ms step_avg:58.52ms
step:1629/2330 train_time:95322ms step_avg:58.52ms
step:1630/2330 train_time:95384ms step_avg:58.52ms
step:1631/2330 train_time:95440ms step_avg:58.52ms
step:1632/2330 train_time:95502ms step_avg:58.52ms
step:1633/2330 train_time:95558ms step_avg:58.52ms
step:1634/2330 train_time:95622ms step_avg:58.52ms
step:1635/2330 train_time:95679ms step_avg:58.52ms
step:1636/2330 train_time:95740ms step_avg:58.52ms
step:1637/2330 train_time:95796ms step_avg:58.52ms
step:1638/2330 train_time:95859ms step_avg:58.52ms
step:1639/2330 train_time:95916ms step_avg:58.52ms
step:1640/2330 train_time:95977ms step_avg:58.52ms
step:1641/2330 train_time:96035ms step_avg:58.52ms
step:1642/2330 train_time:96095ms step_avg:58.52ms
step:1643/2330 train_time:96152ms step_avg:58.52ms
step:1644/2330 train_time:96213ms step_avg:58.52ms
step:1645/2330 train_time:96271ms step_avg:58.52ms
step:1646/2330 train_time:96331ms step_avg:58.52ms
step:1647/2330 train_time:96388ms step_avg:58.52ms
step:1648/2330 train_time:96451ms step_avg:58.53ms
step:1649/2330 train_time:96509ms step_avg:58.53ms
step:1650/2330 train_time:96570ms step_avg:58.53ms
step:1651/2330 train_time:96627ms step_avg:58.53ms
step:1652/2330 train_time:96688ms step_avg:58.53ms
step:1653/2330 train_time:96745ms step_avg:58.53ms
step:1654/2330 train_time:96807ms step_avg:58.53ms
step:1655/2330 train_time:96864ms step_avg:58.53ms
step:1656/2330 train_time:96927ms step_avg:58.53ms
step:1657/2330 train_time:96983ms step_avg:58.53ms
step:1658/2330 train_time:97045ms step_avg:58.53ms
step:1659/2330 train_time:97102ms step_avg:58.53ms
step:1660/2330 train_time:97163ms step_avg:58.53ms
step:1661/2330 train_time:97220ms step_avg:58.53ms
step:1662/2330 train_time:97281ms step_avg:58.53ms
step:1663/2330 train_time:97337ms step_avg:58.53ms
step:1664/2330 train_time:97400ms step_avg:58.53ms
step:1665/2330 train_time:97456ms step_avg:58.53ms
step:1666/2330 train_time:97518ms step_avg:58.53ms
step:1667/2330 train_time:97575ms step_avg:58.53ms
step:1668/2330 train_time:97636ms step_avg:58.53ms
step:1669/2330 train_time:97694ms step_avg:58.53ms
step:1670/2330 train_time:97754ms step_avg:58.54ms
step:1671/2330 train_time:97812ms step_avg:58.54ms
step:1672/2330 train_time:97873ms step_avg:58.54ms
step:1673/2330 train_time:97931ms step_avg:58.54ms
step:1674/2330 train_time:97991ms step_avg:58.54ms
step:1675/2330 train_time:98048ms step_avg:58.54ms
step:1676/2330 train_time:98109ms step_avg:58.54ms
step:1677/2330 train_time:98166ms step_avg:58.54ms
step:1678/2330 train_time:98227ms step_avg:58.54ms
step:1679/2330 train_time:98284ms step_avg:58.54ms
step:1680/2330 train_time:98347ms step_avg:58.54ms
step:1681/2330 train_time:98403ms step_avg:58.54ms
step:1682/2330 train_time:98465ms step_avg:58.54ms
step:1683/2330 train_time:98521ms step_avg:58.54ms
step:1684/2330 train_time:98584ms step_avg:58.54ms
step:1685/2330 train_time:98640ms step_avg:58.54ms
step:1686/2330 train_time:98703ms step_avg:58.54ms
step:1687/2330 train_time:98759ms step_avg:58.54ms
step:1688/2330 train_time:98822ms step_avg:58.54ms
step:1689/2330 train_time:98879ms step_avg:58.54ms
step:1690/2330 train_time:98941ms step_avg:58.54ms
step:1691/2330 train_time:98997ms step_avg:58.54ms
step:1692/2330 train_time:99058ms step_avg:58.55ms
step:1693/2330 train_time:99115ms step_avg:58.54ms
step:1694/2330 train_time:99176ms step_avg:58.55ms
step:1695/2330 train_time:99234ms step_avg:58.54ms
step:1696/2330 train_time:99293ms step_avg:58.55ms
step:1697/2330 train_time:99351ms step_avg:58.55ms
step:1698/2330 train_time:99412ms step_avg:58.55ms
step:1699/2330 train_time:99470ms step_avg:58.55ms
step:1700/2330 train_time:99531ms step_avg:58.55ms
step:1701/2330 train_time:99588ms step_avg:58.55ms
step:1702/2330 train_time:99650ms step_avg:58.55ms
step:1703/2330 train_time:99707ms step_avg:58.55ms
step:1704/2330 train_time:99768ms step_avg:58.55ms
step:1705/2330 train_time:99825ms step_avg:58.55ms
step:1706/2330 train_time:99887ms step_avg:58.55ms
step:1707/2330 train_time:99945ms step_avg:58.55ms
step:1708/2330 train_time:100005ms step_avg:58.55ms
step:1709/2330 train_time:100061ms step_avg:58.55ms
step:1710/2330 train_time:100124ms step_avg:58.55ms
step:1711/2330 train_time:100180ms step_avg:58.55ms
step:1712/2330 train_time:100243ms step_avg:58.55ms
step:1713/2330 train_time:100299ms step_avg:58.55ms
step:1714/2330 train_time:100361ms step_avg:58.55ms
step:1715/2330 train_time:100418ms step_avg:58.55ms
step:1716/2330 train_time:100481ms step_avg:58.56ms
step:1717/2330 train_time:100538ms step_avg:58.55ms
step:1718/2330 train_time:100599ms step_avg:58.56ms
step:1719/2330 train_time:100656ms step_avg:58.55ms
step:1720/2330 train_time:100717ms step_avg:58.56ms
step:1721/2330 train_time:100774ms step_avg:58.56ms
step:1722/2330 train_time:100834ms step_avg:58.56ms
step:1723/2330 train_time:100892ms step_avg:58.56ms
step:1724/2330 train_time:100952ms step_avg:58.56ms
step:1725/2330 train_time:101011ms step_avg:58.56ms
step:1726/2330 train_time:101071ms step_avg:58.56ms
step:1727/2330 train_time:101129ms step_avg:58.56ms
step:1728/2330 train_time:101190ms step_avg:58.56ms
step:1729/2330 train_time:101247ms step_avg:58.56ms
step:1730/2330 train_time:101308ms step_avg:58.56ms
step:1731/2330 train_time:101365ms step_avg:58.56ms
step:1732/2330 train_time:101427ms step_avg:58.56ms
step:1733/2330 train_time:101483ms step_avg:58.56ms
step:1734/2330 train_time:101545ms step_avg:58.56ms
step:1735/2330 train_time:101602ms step_avg:58.56ms
step:1736/2330 train_time:101664ms step_avg:58.56ms
step:1737/2330 train_time:101721ms step_avg:58.56ms
step:1738/2330 train_time:101783ms step_avg:58.56ms
step:1739/2330 train_time:101839ms step_avg:58.56ms
step:1740/2330 train_time:101902ms step_avg:58.56ms
step:1741/2330 train_time:101958ms step_avg:58.56ms
step:1742/2330 train_time:102021ms step_avg:58.57ms
step:1743/2330 train_time:102078ms step_avg:58.56ms
step:1744/2330 train_time:102139ms step_avg:58.57ms
step:1745/2330 train_time:102196ms step_avg:58.57ms
step:1746/2330 train_time:102256ms step_avg:58.57ms
step:1747/2330 train_time:102315ms step_avg:58.57ms
step:1748/2330 train_time:102375ms step_avg:58.57ms
step:1749/2330 train_time:102433ms step_avg:58.57ms
step:1750/2330 train_time:102493ms step_avg:58.57ms
step:1750/2330 val_loss:3.8346 train_time:102575ms step_avg:58.61ms
step:1751/2330 train_time:102593ms step_avg:58.59ms
step:1752/2330 train_time:102614ms step_avg:58.57ms
step:1753/2330 train_time:102676ms step_avg:58.57ms
step:1754/2330 train_time:102741ms step_avg:58.58ms
step:1755/2330 train_time:102798ms step_avg:58.57ms
step:1756/2330 train_time:102860ms step_avg:58.58ms
step:1757/2330 train_time:102916ms step_avg:58.57ms
step:1758/2330 train_time:102976ms step_avg:58.58ms
step:1759/2330 train_time:103032ms step_avg:58.57ms
step:1760/2330 train_time:103093ms step_avg:58.58ms
step:1761/2330 train_time:103150ms step_avg:58.57ms
step:1762/2330 train_time:103209ms step_avg:58.57ms
step:1763/2330 train_time:103266ms step_avg:58.57ms
step:1764/2330 train_time:103325ms step_avg:58.57ms
step:1765/2330 train_time:103382ms step_avg:58.57ms
step:1766/2330 train_time:103442ms step_avg:58.57ms
step:1767/2330 train_time:103499ms step_avg:58.57ms
step:1768/2330 train_time:103562ms step_avg:58.58ms
step:1769/2330 train_time:103621ms step_avg:58.58ms
step:1770/2330 train_time:103684ms step_avg:58.58ms
step:1771/2330 train_time:103741ms step_avg:58.58ms
step:1772/2330 train_time:103804ms step_avg:58.58ms
step:1773/2330 train_time:103861ms step_avg:58.58ms
step:1774/2330 train_time:103922ms step_avg:58.58ms
step:1775/2330 train_time:103979ms step_avg:58.58ms
step:1776/2330 train_time:104040ms step_avg:58.58ms
step:1777/2330 train_time:104097ms step_avg:58.58ms
step:1778/2330 train_time:104158ms step_avg:58.58ms
step:1779/2330 train_time:104214ms step_avg:58.58ms
step:1780/2330 train_time:104276ms step_avg:58.58ms
step:1781/2330 train_time:104332ms step_avg:58.58ms
step:1782/2330 train_time:104392ms step_avg:58.58ms
step:1783/2330 train_time:104448ms step_avg:58.58ms
step:1784/2330 train_time:104510ms step_avg:58.58ms
step:1785/2330 train_time:104568ms step_avg:58.58ms
step:1786/2330 train_time:104629ms step_avg:58.58ms
step:1787/2330 train_time:104688ms step_avg:58.58ms
step:1788/2330 train_time:104748ms step_avg:58.58ms
step:1789/2330 train_time:104805ms step_avg:58.58ms
step:1790/2330 train_time:104867ms step_avg:58.59ms
step:1791/2330 train_time:104925ms step_avg:58.58ms
step:1792/2330 train_time:104986ms step_avg:58.59ms
step:1793/2330 train_time:105043ms step_avg:58.59ms
step:1794/2330 train_time:105105ms step_avg:58.59ms
step:1795/2330 train_time:105161ms step_avg:58.59ms
step:1796/2330 train_time:105224ms step_avg:58.59ms
step:1797/2330 train_time:105280ms step_avg:58.59ms
step:1798/2330 train_time:105342ms step_avg:58.59ms
step:1799/2330 train_time:105398ms step_avg:58.59ms
step:1800/2330 train_time:105460ms step_avg:58.59ms
step:1801/2330 train_time:105516ms step_avg:58.59ms
step:1802/2330 train_time:105578ms step_avg:58.59ms
step:1803/2330 train_time:105636ms step_avg:58.59ms
step:1804/2330 train_time:105697ms step_avg:58.59ms
step:1805/2330 train_time:105754ms step_avg:58.59ms
step:1806/2330 train_time:105816ms step_avg:58.59ms
step:1807/2330 train_time:105873ms step_avg:58.59ms
step:1808/2330 train_time:105934ms step_avg:58.59ms
step:1809/2330 train_time:105991ms step_avg:58.59ms
step:1810/2330 train_time:106053ms step_avg:58.59ms
step:1811/2330 train_time:106110ms step_avg:58.59ms
step:1812/2330 train_time:106170ms step_avg:58.59ms
step:1813/2330 train_time:106228ms step_avg:58.59ms
step:1814/2330 train_time:106287ms step_avg:58.59ms
step:1815/2330 train_time:106345ms step_avg:58.59ms
step:1816/2330 train_time:106405ms step_avg:58.59ms
step:1817/2330 train_time:106462ms step_avg:58.59ms
step:1818/2330 train_time:106523ms step_avg:58.59ms
step:1819/2330 train_time:106580ms step_avg:58.59ms
step:1820/2330 train_time:106640ms step_avg:58.59ms
step:1821/2330 train_time:106697ms step_avg:58.59ms
step:1822/2330 train_time:106759ms step_avg:58.59ms
step:1823/2330 train_time:106816ms step_avg:58.59ms
step:1824/2330 train_time:106878ms step_avg:58.60ms
step:1825/2330 train_time:106935ms step_avg:58.59ms
step:1826/2330 train_time:106998ms step_avg:58.60ms
step:1827/2330 train_time:107055ms step_avg:58.60ms
step:1828/2330 train_time:107116ms step_avg:58.60ms
step:1829/2330 train_time:107172ms step_avg:58.60ms
step:1830/2330 train_time:107234ms step_avg:58.60ms
step:1831/2330 train_time:107290ms step_avg:58.60ms
step:1832/2330 train_time:107352ms step_avg:58.60ms
step:1833/2330 train_time:107410ms step_avg:58.60ms
step:1834/2330 train_time:107469ms step_avg:58.60ms
step:1835/2330 train_time:107527ms step_avg:58.60ms
step:1836/2330 train_time:107588ms step_avg:58.60ms
step:1837/2330 train_time:107646ms step_avg:58.60ms
step:1838/2330 train_time:107706ms step_avg:58.60ms
step:1839/2330 train_time:107763ms step_avg:58.60ms
step:1840/2330 train_time:107824ms step_avg:58.60ms
step:1841/2330 train_time:107881ms step_avg:58.60ms
step:1842/2330 train_time:107943ms step_avg:58.60ms
step:1843/2330 train_time:108000ms step_avg:58.60ms
step:1844/2330 train_time:108062ms step_avg:58.60ms
step:1845/2330 train_time:108119ms step_avg:58.60ms
step:1846/2330 train_time:108181ms step_avg:58.60ms
step:1847/2330 train_time:108238ms step_avg:58.60ms
step:1848/2330 train_time:108299ms step_avg:58.60ms
step:1849/2330 train_time:108356ms step_avg:58.60ms
step:1850/2330 train_time:108417ms step_avg:58.60ms
step:1851/2330 train_time:108474ms step_avg:58.60ms
step:1852/2330 train_time:108535ms step_avg:58.60ms
step:1853/2330 train_time:108591ms step_avg:58.60ms
step:1854/2330 train_time:108652ms step_avg:58.60ms
step:1855/2330 train_time:108710ms step_avg:58.60ms
step:1856/2330 train_time:108770ms step_avg:58.60ms
step:1857/2330 train_time:108827ms step_avg:58.60ms
step:1858/2330 train_time:108888ms step_avg:58.61ms
step:1859/2330 train_time:108947ms step_avg:58.61ms
step:1860/2330 train_time:109007ms step_avg:58.61ms
step:1861/2330 train_time:109066ms step_avg:58.61ms
step:1862/2330 train_time:109126ms step_avg:58.61ms
step:1863/2330 train_time:109183ms step_avg:58.61ms
step:1864/2330 train_time:109244ms step_avg:58.61ms
step:1865/2330 train_time:109301ms step_avg:58.61ms
step:1866/2330 train_time:109362ms step_avg:58.61ms
step:1867/2330 train_time:109419ms step_avg:58.61ms
step:1868/2330 train_time:109480ms step_avg:58.61ms
step:1869/2330 train_time:109537ms step_avg:58.61ms
step:1870/2330 train_time:109599ms step_avg:58.61ms
step:1871/2330 train_time:109656ms step_avg:58.61ms
step:1872/2330 train_time:109717ms step_avg:58.61ms
step:1873/2330 train_time:109773ms step_avg:58.61ms
step:1874/2330 train_time:109835ms step_avg:58.61ms
step:1875/2330 train_time:109891ms step_avg:58.61ms
step:1876/2330 train_time:109952ms step_avg:58.61ms
step:1877/2330 train_time:110010ms step_avg:58.61ms
step:1878/2330 train_time:110070ms step_avg:58.61ms
step:1879/2330 train_time:110128ms step_avg:58.61ms
step:1880/2330 train_time:110189ms step_avg:58.61ms
step:1881/2330 train_time:110246ms step_avg:58.61ms
step:1882/2330 train_time:110308ms step_avg:58.61ms
step:1883/2330 train_time:110367ms step_avg:58.61ms
step:1884/2330 train_time:110426ms step_avg:58.61ms
step:1885/2330 train_time:110483ms step_avg:58.61ms
step:1886/2330 train_time:110545ms step_avg:58.61ms
step:1887/2330 train_time:110603ms step_avg:58.61ms
step:1888/2330 train_time:110663ms step_avg:58.61ms
step:1889/2330 train_time:110720ms step_avg:58.61ms
step:1890/2330 train_time:110782ms step_avg:58.61ms
step:1891/2330 train_time:110838ms step_avg:58.61ms
step:1892/2330 train_time:110901ms step_avg:58.62ms
step:1893/2330 train_time:110957ms step_avg:58.61ms
step:1894/2330 train_time:111020ms step_avg:58.62ms
step:1895/2330 train_time:111076ms step_avg:58.62ms
step:1896/2330 train_time:111138ms step_avg:58.62ms
step:1897/2330 train_time:111194ms step_avg:58.62ms
step:1898/2330 train_time:111257ms step_avg:58.62ms
step:1899/2330 train_time:111314ms step_avg:58.62ms
step:1900/2330 train_time:111376ms step_avg:58.62ms
step:1901/2330 train_time:111433ms step_avg:58.62ms
step:1902/2330 train_time:111493ms step_avg:58.62ms
step:1903/2330 train_time:111550ms step_avg:58.62ms
step:1904/2330 train_time:111610ms step_avg:58.62ms
step:1905/2330 train_time:111669ms step_avg:58.62ms
step:1906/2330 train_time:111729ms step_avg:58.62ms
step:1907/2330 train_time:111787ms step_avg:58.62ms
step:1908/2330 train_time:111847ms step_avg:58.62ms
step:1909/2330 train_time:111904ms step_avg:58.62ms
step:1910/2330 train_time:111966ms step_avg:58.62ms
step:1911/2330 train_time:112022ms step_avg:58.62ms
step:1912/2330 train_time:112084ms step_avg:58.62ms
step:1913/2330 train_time:112141ms step_avg:58.62ms
step:1914/2330 train_time:112203ms step_avg:58.62ms
step:1915/2330 train_time:112260ms step_avg:58.62ms
step:1916/2330 train_time:112322ms step_avg:58.62ms
step:1917/2330 train_time:112379ms step_avg:58.62ms
step:1918/2330 train_time:112441ms step_avg:58.62ms
step:1919/2330 train_time:112497ms step_avg:58.62ms
step:1920/2330 train_time:112560ms step_avg:58.62ms
step:1921/2330 train_time:112616ms step_avg:58.62ms
step:1922/2330 train_time:112679ms step_avg:58.63ms
step:1923/2330 train_time:112736ms step_avg:58.62ms
step:1924/2330 train_time:112798ms step_avg:58.63ms
step:1925/2330 train_time:112855ms step_avg:58.63ms
step:1926/2330 train_time:112917ms step_avg:58.63ms
step:1927/2330 train_time:112973ms step_avg:58.63ms
step:1928/2330 train_time:113035ms step_avg:58.63ms
step:1929/2330 train_time:113092ms step_avg:58.63ms
step:1930/2330 train_time:113153ms step_avg:58.63ms
step:1931/2330 train_time:113210ms step_avg:58.63ms
step:1932/2330 train_time:113270ms step_avg:58.63ms
step:1933/2330 train_time:113328ms step_avg:58.63ms
step:1934/2330 train_time:113389ms step_avg:58.63ms
step:1935/2330 train_time:113446ms step_avg:58.63ms
step:1936/2330 train_time:113507ms step_avg:58.63ms
step:1937/2330 train_time:113565ms step_avg:58.63ms
step:1938/2330 train_time:113626ms step_avg:58.63ms
step:1939/2330 train_time:113683ms step_avg:58.63ms
step:1940/2330 train_time:113745ms step_avg:58.63ms
step:1941/2330 train_time:113802ms step_avg:58.63ms
step:1942/2330 train_time:113863ms step_avg:58.63ms
step:1943/2330 train_time:113920ms step_avg:58.63ms
step:1944/2330 train_time:113981ms step_avg:58.63ms
step:1945/2330 train_time:114037ms step_avg:58.63ms
step:1946/2330 train_time:114100ms step_avg:58.63ms
step:1947/2330 train_time:114157ms step_avg:58.63ms
step:1948/2330 train_time:114219ms step_avg:58.63ms
step:1949/2330 train_time:114275ms step_avg:58.63ms
step:1950/2330 train_time:114337ms step_avg:58.63ms
step:1951/2330 train_time:114393ms step_avg:58.63ms
step:1952/2330 train_time:114454ms step_avg:58.63ms
step:1953/2330 train_time:114511ms step_avg:58.63ms
step:1954/2330 train_time:114572ms step_avg:58.63ms
step:1955/2330 train_time:114629ms step_avg:58.63ms
step:1956/2330 train_time:114689ms step_avg:58.63ms
step:1957/2330 train_time:114748ms step_avg:58.63ms
step:1958/2330 train_time:114809ms step_avg:58.64ms
step:1959/2330 train_time:114867ms step_avg:58.64ms
step:1960/2330 train_time:114927ms step_avg:58.64ms
step:1961/2330 train_time:114984ms step_avg:58.64ms
step:1962/2330 train_time:115045ms step_avg:58.64ms
step:1963/2330 train_time:115103ms step_avg:58.64ms
step:1964/2330 train_time:115163ms step_avg:58.64ms
step:1965/2330 train_time:115221ms step_avg:58.64ms
step:1966/2330 train_time:115282ms step_avg:58.64ms
step:1967/2330 train_time:115338ms step_avg:58.64ms
step:1968/2330 train_time:115401ms step_avg:58.64ms
step:1969/2330 train_time:115457ms step_avg:58.64ms
step:1970/2330 train_time:115520ms step_avg:58.64ms
step:1971/2330 train_time:115576ms step_avg:58.64ms
step:1972/2330 train_time:115640ms step_avg:58.64ms
step:1973/2330 train_time:115697ms step_avg:58.64ms
step:1974/2330 train_time:115758ms step_avg:58.64ms
step:1975/2330 train_time:115815ms step_avg:58.64ms
step:1976/2330 train_time:115876ms step_avg:58.64ms
step:1977/2330 train_time:115933ms step_avg:58.64ms
step:1978/2330 train_time:115995ms step_avg:58.64ms
step:1979/2330 train_time:116053ms step_avg:58.64ms
step:1980/2330 train_time:116114ms step_avg:58.64ms
step:1981/2330 train_time:116171ms step_avg:58.64ms
step:1982/2330 train_time:116232ms step_avg:58.64ms
step:1983/2330 train_time:116289ms step_avg:58.64ms
step:1984/2330 train_time:116349ms step_avg:58.64ms
step:1985/2330 train_time:116407ms step_avg:58.64ms
step:1986/2330 train_time:116467ms step_avg:58.64ms
step:1987/2330 train_time:116524ms step_avg:58.64ms
step:1988/2330 train_time:116586ms step_avg:58.64ms
step:1989/2330 train_time:116643ms step_avg:58.64ms
step:1990/2330 train_time:116704ms step_avg:58.65ms
step:1991/2330 train_time:116761ms step_avg:58.64ms
step:1992/2330 train_time:116822ms step_avg:58.65ms
step:1993/2330 train_time:116878ms step_avg:58.64ms
step:1994/2330 train_time:116941ms step_avg:58.65ms
step:1995/2330 train_time:116997ms step_avg:58.65ms
step:1996/2330 train_time:117060ms step_avg:58.65ms
step:1997/2330 train_time:117117ms step_avg:58.65ms
step:1998/2330 train_time:117179ms step_avg:58.65ms
step:1999/2330 train_time:117235ms step_avg:58.65ms
step:2000/2330 train_time:117298ms step_avg:58.65ms
step:2000/2330 val_loss:3.7732 train_time:117380ms step_avg:58.69ms
step:2001/2330 train_time:117398ms step_avg:58.67ms
step:2002/2330 train_time:117418ms step_avg:58.65ms
step:2003/2330 train_time:117477ms step_avg:58.65ms
step:2004/2330 train_time:117548ms step_avg:58.66ms
step:2005/2330 train_time:117606ms step_avg:58.66ms
step:2006/2330 train_time:117667ms step_avg:58.66ms
step:2007/2330 train_time:117724ms step_avg:58.66ms
step:2008/2330 train_time:117784ms step_avg:58.66ms
step:2009/2330 train_time:117840ms step_avg:58.66ms
step:2010/2330 train_time:117900ms step_avg:58.66ms
step:2011/2330 train_time:117957ms step_avg:58.66ms
step:2012/2330 train_time:118017ms step_avg:58.66ms
step:2013/2330 train_time:118074ms step_avg:58.66ms
step:2014/2330 train_time:118134ms step_avg:58.66ms
step:2015/2330 train_time:118190ms step_avg:58.66ms
step:2016/2330 train_time:118250ms step_avg:58.66ms
step:2017/2330 train_time:118306ms step_avg:58.65ms
step:2018/2330 train_time:118367ms step_avg:58.66ms
step:2019/2330 train_time:118425ms step_avg:58.66ms
step:2020/2330 train_time:118488ms step_avg:58.66ms
step:2021/2330 train_time:118545ms step_avg:58.66ms
step:2022/2330 train_time:118608ms step_avg:58.66ms
step:2023/2330 train_time:118666ms step_avg:58.66ms
step:2024/2330 train_time:118727ms step_avg:58.66ms
step:2025/2330 train_time:118783ms step_avg:58.66ms
step:2026/2330 train_time:118846ms step_avg:58.66ms
step:2027/2330 train_time:118903ms step_avg:58.66ms
step:2028/2330 train_time:118963ms step_avg:58.66ms
step:2029/2330 train_time:119021ms step_avg:58.66ms
step:2030/2330 train_time:119080ms step_avg:58.66ms
step:2031/2330 train_time:119137ms step_avg:58.66ms
step:2032/2330 train_time:119197ms step_avg:58.66ms
step:2033/2330 train_time:119253ms step_avg:58.66ms
step:2034/2330 train_time:119314ms step_avg:58.66ms
step:2035/2330 train_time:119372ms step_avg:58.66ms
step:2036/2330 train_time:119433ms step_avg:58.66ms
step:2037/2330 train_time:119490ms step_avg:58.66ms
step:2038/2330 train_time:119554ms step_avg:58.66ms
step:2039/2330 train_time:119611ms step_avg:58.66ms
step:2040/2330 train_time:119673ms step_avg:58.66ms
step:2041/2330 train_time:119730ms step_avg:58.66ms
step:2042/2330 train_time:119792ms step_avg:58.66ms
step:2043/2330 train_time:119849ms step_avg:58.66ms
step:2044/2330 train_time:119911ms step_avg:58.66ms
step:2045/2330 train_time:119967ms step_avg:58.66ms
step:2046/2330 train_time:120029ms step_avg:58.67ms
step:2047/2330 train_time:120086ms step_avg:58.66ms
step:2048/2330 train_time:120146ms step_avg:58.66ms
step:2049/2330 train_time:120202ms step_avg:58.66ms
step:2050/2330 train_time:120263ms step_avg:58.66ms
step:2051/2330 train_time:120320ms step_avg:58.66ms
step:2052/2330 train_time:120380ms step_avg:58.66ms
step:2053/2330 train_time:120438ms step_avg:58.66ms
step:2054/2330 train_time:120499ms step_avg:58.67ms
step:2055/2330 train_time:120558ms step_avg:58.67ms
step:2056/2330 train_time:120619ms step_avg:58.67ms
step:2057/2330 train_time:120678ms step_avg:58.67ms
step:2058/2330 train_time:120739ms step_avg:58.67ms
step:2059/2330 train_time:120796ms step_avg:58.67ms
step:2060/2330 train_time:120857ms step_avg:58.67ms
step:2061/2330 train_time:120915ms step_avg:58.67ms
step:2062/2330 train_time:120975ms step_avg:58.67ms
step:2063/2330 train_time:121032ms step_avg:58.67ms
step:2064/2330 train_time:121092ms step_avg:58.67ms
step:2065/2330 train_time:121149ms step_avg:58.67ms
step:2066/2330 train_time:121210ms step_avg:58.67ms
step:2067/2330 train_time:121266ms step_avg:58.67ms
step:2068/2330 train_time:121328ms step_avg:58.67ms
step:2069/2330 train_time:121384ms step_avg:58.67ms
step:2070/2330 train_time:121447ms step_avg:58.67ms
step:2071/2330 train_time:121504ms step_avg:58.67ms
step:2072/2330 train_time:121564ms step_avg:58.67ms
step:2073/2330 train_time:121623ms step_avg:58.67ms
step:2074/2330 train_time:121683ms step_avg:58.67ms
step:2075/2330 train_time:121741ms step_avg:58.67ms
step:2076/2330 train_time:121801ms step_avg:58.67ms
step:2077/2330 train_time:121859ms step_avg:58.67ms
step:2078/2330 train_time:121920ms step_avg:58.67ms
step:2079/2330 train_time:121978ms step_avg:58.67ms
step:2080/2330 train_time:122038ms step_avg:58.67ms
step:2081/2330 train_time:122097ms step_avg:58.67ms
step:2082/2330 train_time:122157ms step_avg:58.67ms
step:2083/2330 train_time:122214ms step_avg:58.67ms
step:2084/2330 train_time:122274ms step_avg:58.67ms
step:2085/2330 train_time:122331ms step_avg:58.67ms
step:2086/2330 train_time:122392ms step_avg:58.67ms
step:2087/2330 train_time:122448ms step_avg:58.67ms
step:2088/2330 train_time:122511ms step_avg:58.67ms
step:2089/2330 train_time:122567ms step_avg:58.67ms
step:2090/2330 train_time:122630ms step_avg:58.67ms
step:2091/2330 train_time:122686ms step_avg:58.67ms
step:2092/2330 train_time:122748ms step_avg:58.68ms
step:2093/2330 train_time:122805ms step_avg:58.67ms
step:2094/2330 train_time:122867ms step_avg:58.68ms
step:2095/2330 train_time:122924ms step_avg:58.67ms
step:2096/2330 train_time:122984ms step_avg:58.68ms
step:2097/2330 train_time:123041ms step_avg:58.68ms
step:2098/2330 train_time:123102ms step_avg:58.68ms
step:2099/2330 train_time:123161ms step_avg:58.68ms
step:2100/2330 train_time:123221ms step_avg:58.68ms
step:2101/2330 train_time:123280ms step_avg:58.68ms
step:2102/2330 train_time:123340ms step_avg:58.68ms
step:2103/2330 train_time:123398ms step_avg:58.68ms
step:2104/2330 train_time:123459ms step_avg:58.68ms
step:2105/2330 train_time:123518ms step_avg:58.68ms
step:2106/2330 train_time:123578ms step_avg:58.68ms
step:2107/2330 train_time:123636ms step_avg:58.68ms
step:2108/2330 train_time:123697ms step_avg:58.68ms
step:2109/2330 train_time:123755ms step_avg:58.68ms
step:2110/2330 train_time:123816ms step_avg:58.68ms
step:2111/2330 train_time:123874ms step_avg:58.68ms
step:2112/2330 train_time:123935ms step_avg:58.68ms
step:2113/2330 train_time:123991ms step_avg:58.68ms
step:2114/2330 train_time:124054ms step_avg:58.68ms
step:2115/2330 train_time:124111ms step_avg:58.68ms
step:2116/2330 train_time:124171ms step_avg:58.68ms
step:2117/2330 train_time:124228ms step_avg:58.68ms
step:2118/2330 train_time:124289ms step_avg:58.68ms
step:2119/2330 train_time:124346ms step_avg:58.68ms
step:2120/2330 train_time:124406ms step_avg:58.68ms
step:2121/2330 train_time:124463ms step_avg:58.68ms
step:2122/2330 train_time:124525ms step_avg:58.68ms
step:2123/2330 train_time:124582ms step_avg:58.68ms
step:2124/2330 train_time:124642ms step_avg:58.68ms
step:2125/2330 train_time:124700ms step_avg:58.68ms
step:2126/2330 train_time:124760ms step_avg:58.68ms
step:2127/2330 train_time:124818ms step_avg:58.68ms
step:2128/2330 train_time:124878ms step_avg:58.68ms
step:2129/2330 train_time:124937ms step_avg:58.68ms
step:2130/2330 train_time:124998ms step_avg:58.68ms
step:2131/2330 train_time:125056ms step_avg:58.68ms
step:2132/2330 train_time:125117ms step_avg:58.69ms
step:2133/2330 train_time:125175ms step_avg:58.68ms
step:2134/2330 train_time:125235ms step_avg:58.69ms
step:2135/2330 train_time:125292ms step_avg:58.68ms
step:2136/2330 train_time:125353ms step_avg:58.69ms
step:2137/2330 train_time:125410ms step_avg:58.69ms
step:2138/2330 train_time:125471ms step_avg:58.69ms
step:2139/2330 train_time:125527ms step_avg:58.69ms
step:2140/2330 train_time:125590ms step_avg:58.69ms
step:2141/2330 train_time:125647ms step_avg:58.69ms
step:2142/2330 train_time:125709ms step_avg:58.69ms
step:2143/2330 train_time:125766ms step_avg:58.69ms
step:2144/2330 train_time:125827ms step_avg:58.69ms
step:2145/2330 train_time:125883ms step_avg:58.69ms
step:2146/2330 train_time:125944ms step_avg:58.69ms
step:2147/2330 train_time:126001ms step_avg:58.69ms
step:2148/2330 train_time:126062ms step_avg:58.69ms
step:2149/2330 train_time:126120ms step_avg:58.69ms
step:2150/2330 train_time:126181ms step_avg:58.69ms
step:2151/2330 train_time:126239ms step_avg:58.69ms
step:2152/2330 train_time:126298ms step_avg:58.69ms
step:2153/2330 train_time:126357ms step_avg:58.69ms
step:2154/2330 train_time:126418ms step_avg:58.69ms
step:2155/2330 train_time:126476ms step_avg:58.69ms
step:2156/2330 train_time:126537ms step_avg:58.69ms
step:2157/2330 train_time:126595ms step_avg:58.69ms
step:2158/2330 train_time:126655ms step_avg:58.69ms
step:2159/2330 train_time:126712ms step_avg:58.69ms
step:2160/2330 train_time:126774ms step_avg:58.69ms
step:2161/2330 train_time:126830ms step_avg:58.69ms
step:2162/2330 train_time:126892ms step_avg:58.69ms
step:2163/2330 train_time:126949ms step_avg:58.69ms
step:2164/2330 train_time:127010ms step_avg:58.69ms
step:2165/2330 train_time:127067ms step_avg:58.69ms
step:2166/2330 train_time:127129ms step_avg:58.69ms
step:2167/2330 train_time:127185ms step_avg:58.69ms
step:2168/2330 train_time:127246ms step_avg:58.69ms
step:2169/2330 train_time:127303ms step_avg:58.69ms
step:2170/2330 train_time:127364ms step_avg:58.69ms
step:2171/2330 train_time:127421ms step_avg:58.69ms
step:2172/2330 train_time:127482ms step_avg:58.69ms
step:2173/2330 train_time:127540ms step_avg:58.69ms
step:2174/2330 train_time:127601ms step_avg:58.69ms
step:2175/2330 train_time:127658ms step_avg:58.69ms
step:2176/2330 train_time:127720ms step_avg:58.69ms
step:2177/2330 train_time:127777ms step_avg:58.69ms
step:2178/2330 train_time:127838ms step_avg:58.70ms
step:2179/2330 train_time:127896ms step_avg:58.69ms
step:2180/2330 train_time:127957ms step_avg:58.70ms
step:2181/2330 train_time:128015ms step_avg:58.70ms
step:2182/2330 train_time:128076ms step_avg:58.70ms
step:2183/2330 train_time:128133ms step_avg:58.70ms
step:2184/2330 train_time:128195ms step_avg:58.70ms
step:2185/2330 train_time:128252ms step_avg:58.70ms
step:2186/2330 train_time:128314ms step_avg:58.70ms
step:2187/2330 train_time:128370ms step_avg:58.70ms
step:2188/2330 train_time:128432ms step_avg:58.70ms
step:2189/2330 train_time:128489ms step_avg:58.70ms
step:2190/2330 train_time:128550ms step_avg:58.70ms
step:2191/2330 train_time:128607ms step_avg:58.70ms
step:2192/2330 train_time:128669ms step_avg:58.70ms
step:2193/2330 train_time:128725ms step_avg:58.70ms
step:2194/2330 train_time:128787ms step_avg:58.70ms
step:2195/2330 train_time:128843ms step_avg:58.70ms
step:2196/2330 train_time:128904ms step_avg:58.70ms
step:2197/2330 train_time:128961ms step_avg:58.70ms
step:2198/2330 train_time:129022ms step_avg:58.70ms
step:2199/2330 train_time:129080ms step_avg:58.70ms
step:2200/2330 train_time:129140ms step_avg:58.70ms
step:2201/2330 train_time:129198ms step_avg:58.70ms
step:2202/2330 train_time:129258ms step_avg:58.70ms
step:2203/2330 train_time:129316ms step_avg:58.70ms
step:2204/2330 train_time:129377ms step_avg:58.70ms
step:2205/2330 train_time:129434ms step_avg:58.70ms
step:2206/2330 train_time:129495ms step_avg:58.70ms
step:2207/2330 train_time:129551ms step_avg:58.70ms
step:2208/2330 train_time:129613ms step_avg:58.70ms
step:2209/2330 train_time:129670ms step_avg:58.70ms
step:2210/2330 train_time:129732ms step_avg:58.70ms
step:2211/2330 train_time:129788ms step_avg:58.70ms
step:2212/2330 train_time:129851ms step_avg:58.70ms
step:2213/2330 train_time:129907ms step_avg:58.70ms
step:2214/2330 train_time:129971ms step_avg:58.70ms
step:2215/2330 train_time:130028ms step_avg:58.70ms
step:2216/2330 train_time:130090ms step_avg:58.70ms
step:2217/2330 train_time:130147ms step_avg:58.70ms
step:2218/2330 train_time:130208ms step_avg:58.71ms
step:2219/2330 train_time:130264ms step_avg:58.70ms
step:2220/2330 train_time:130326ms step_avg:58.71ms
step:2221/2330 train_time:130382ms step_avg:58.70ms
step:2222/2330 train_time:130444ms step_avg:58.71ms
step:2223/2330 train_time:130501ms step_avg:58.70ms
step:2224/2330 train_time:130561ms step_avg:58.71ms
step:2225/2330 train_time:130618ms step_avg:58.70ms
step:2226/2330 train_time:130678ms step_avg:58.71ms
step:2227/2330 train_time:130736ms step_avg:58.70ms
step:2228/2330 train_time:130796ms step_avg:58.71ms
step:2229/2330 train_time:130855ms step_avg:58.71ms
step:2230/2330 train_time:130915ms step_avg:58.71ms
step:2231/2330 train_time:130974ms step_avg:58.71ms
step:2232/2330 train_time:131034ms step_avg:58.71ms
step:2233/2330 train_time:131090ms step_avg:58.71ms
step:2234/2330 train_time:131152ms step_avg:58.71ms
step:2235/2330 train_time:131209ms step_avg:58.71ms
step:2236/2330 train_time:131271ms step_avg:58.71ms
step:2237/2330 train_time:131327ms step_avg:58.71ms
step:2238/2330 train_time:131389ms step_avg:58.71ms
step:2239/2330 train_time:131445ms step_avg:58.71ms
step:2240/2330 train_time:131508ms step_avg:58.71ms
step:2241/2330 train_time:131565ms step_avg:58.71ms
step:2242/2330 train_time:131626ms step_avg:58.71ms
step:2243/2330 train_time:131682ms step_avg:58.71ms
step:2244/2330 train_time:131743ms step_avg:58.71ms
step:2245/2330 train_time:131800ms step_avg:58.71ms
step:2246/2330 train_time:131861ms step_avg:58.71ms
step:2247/2330 train_time:131919ms step_avg:58.71ms
step:2248/2330 train_time:131980ms step_avg:58.71ms
step:2249/2330 train_time:132039ms step_avg:58.71ms
step:2250/2330 train_time:132099ms step_avg:58.71ms
step:2250/2330 val_loss:3.7246 train_time:132181ms step_avg:58.75ms
step:2251/2330 train_time:132199ms step_avg:58.73ms
step:2252/2330 train_time:132221ms step_avg:58.71ms
step:2253/2330 train_time:132282ms step_avg:58.71ms
step:2254/2330 train_time:132346ms step_avg:58.72ms
step:2255/2330 train_time:132404ms step_avg:58.72ms
step:2256/2330 train_time:132467ms step_avg:58.72ms
step:2257/2330 train_time:132524ms step_avg:58.72ms
step:2258/2330 train_time:132585ms step_avg:58.72ms
step:2259/2330 train_time:132642ms step_avg:58.72ms
step:2260/2330 train_time:132702ms step_avg:58.72ms
step:2261/2330 train_time:132758ms step_avg:58.72ms
step:2262/2330 train_time:132819ms step_avg:58.72ms
step:2263/2330 train_time:132875ms step_avg:58.72ms
step:2264/2330 train_time:132934ms step_avg:58.72ms
step:2265/2330 train_time:132990ms step_avg:58.72ms
step:2266/2330 train_time:133050ms step_avg:58.72ms
step:2267/2330 train_time:133106ms step_avg:58.71ms
step:2268/2330 train_time:133168ms step_avg:58.72ms
step:2269/2330 train_time:133227ms step_avg:58.72ms
step:2270/2330 train_time:133290ms step_avg:58.72ms
step:2271/2330 train_time:133349ms step_avg:58.72ms
step:2272/2330 train_time:133412ms step_avg:58.72ms
step:2273/2330 train_time:133470ms step_avg:58.72ms
step:2274/2330 train_time:133531ms step_avg:58.72ms
step:2275/2330 train_time:133589ms step_avg:58.72ms
step:2276/2330 train_time:133650ms step_avg:58.72ms
step:2277/2330 train_time:133706ms step_avg:58.72ms
step:2278/2330 train_time:133768ms step_avg:58.72ms
step:2279/2330 train_time:133824ms step_avg:58.72ms
step:2280/2330 train_time:133885ms step_avg:58.72ms
step:2281/2330 train_time:133941ms step_avg:58.72ms
step:2282/2330 train_time:134002ms step_avg:58.72ms
step:2283/2330 train_time:134059ms step_avg:58.72ms
step:2284/2330 train_time:134119ms step_avg:58.72ms
step:2285/2330 train_time:134177ms step_avg:58.72ms
step:2286/2330 train_time:134238ms step_avg:58.72ms
step:2287/2330 train_time:134297ms step_avg:58.72ms
step:2288/2330 train_time:134357ms step_avg:58.72ms
step:2289/2330 train_time:134416ms step_avg:58.72ms
step:2290/2330 train_time:134478ms step_avg:58.72ms
step:2291/2330 train_time:134536ms step_avg:58.72ms
step:2292/2330 train_time:134597ms step_avg:58.72ms
step:2293/2330 train_time:134655ms step_avg:58.72ms
step:2294/2330 train_time:134716ms step_avg:58.73ms
step:2295/2330 train_time:134774ms step_avg:58.72ms
step:2296/2330 train_time:134834ms step_avg:58.73ms
step:2297/2330 train_time:134891ms step_avg:58.72ms
step:2298/2330 train_time:134951ms step_avg:58.73ms
step:2299/2330 train_time:135008ms step_avg:58.72ms
step:2300/2330 train_time:135068ms step_avg:58.73ms
step:2301/2330 train_time:135124ms step_avg:58.72ms
step:2302/2330 train_time:135187ms step_avg:58.73ms
step:2303/2330 train_time:135243ms step_avg:58.72ms
step:2304/2330 train_time:135306ms step_avg:58.73ms
step:2305/2330 train_time:135363ms step_avg:58.73ms
step:2306/2330 train_time:135427ms step_avg:58.73ms
step:2307/2330 train_time:135483ms step_avg:58.73ms
step:2308/2330 train_time:135546ms step_avg:58.73ms
step:2309/2330 train_time:135602ms step_avg:58.73ms
step:2310/2330 train_time:135664ms step_avg:58.73ms
step:2311/2330 train_time:135721ms step_avg:58.73ms
step:2312/2330 train_time:135782ms step_avg:58.73ms
step:2313/2330 train_time:135838ms step_avg:58.73ms
step:2314/2330 train_time:135900ms step_avg:58.73ms
step:2315/2330 train_time:135957ms step_avg:58.73ms
step:2316/2330 train_time:136018ms step_avg:58.73ms
step:2317/2330 train_time:136076ms step_avg:58.73ms
step:2318/2330 train_time:136136ms step_avg:58.73ms
step:2319/2330 train_time:136193ms step_avg:58.73ms
step:2320/2330 train_time:136253ms step_avg:58.73ms
step:2321/2330 train_time:136311ms step_avg:58.73ms
step:2322/2330 train_time:136373ms step_avg:58.73ms
step:2323/2330 train_time:136432ms step_avg:58.73ms
step:2324/2330 train_time:136492ms step_avg:58.73ms
step:2325/2330 train_time:136551ms step_avg:58.73ms
step:2326/2330 train_time:136612ms step_avg:58.73ms
step:2327/2330 train_time:136670ms step_avg:58.73ms
step:2328/2330 train_time:136730ms step_avg:58.73ms
step:2329/2330 train_time:136788ms step_avg:58.73ms
step:2330/2330 train_time:136848ms step_avg:58.73ms
step:2330/2330 val_loss:3.7088 train_time:136931ms step_avg:58.77ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
