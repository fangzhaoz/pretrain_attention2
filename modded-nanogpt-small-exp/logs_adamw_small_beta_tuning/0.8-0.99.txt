import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--beta1', type=str, required=True)
parser.add_argument('--beta2', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_beta_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=6e-4,  betas=(float(args2.beta1), float(args2.beta2)), eps=1e-10, weight_decay=0.0, fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt", exist_ok=True)
            torch.save(log, f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 04:20:49 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   35C    P0             118W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   35C    P0             117W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             118W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   28C    P0             115W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   34C    P0             118W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             114W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:90ms step_avg:89.68ms
step:2/2330 train_time:221ms step_avg:110.25ms
step:3/2330 train_time:239ms step_avg:79.82ms
step:4/2330 train_time:259ms step_avg:64.69ms
step:5/2330 train_time:311ms step_avg:62.23ms
step:6/2330 train_time:369ms step_avg:61.50ms
step:7/2330 train_time:425ms step_avg:60.68ms
step:8/2330 train_time:483ms step_avg:60.39ms
step:9/2330 train_time:539ms step_avg:59.86ms
step:10/2330 train_time:597ms step_avg:59.66ms
step:11/2330 train_time:651ms step_avg:59.23ms
step:12/2330 train_time:710ms step_avg:59.17ms
step:13/2330 train_time:765ms step_avg:58.86ms
step:14/2330 train_time:823ms step_avg:58.81ms
step:15/2330 train_time:879ms step_avg:58.59ms
step:16/2330 train_time:937ms step_avg:58.55ms
step:17/2330 train_time:992ms step_avg:58.35ms
step:18/2330 train_time:1050ms step_avg:58.36ms
step:19/2330 train_time:1108ms step_avg:58.30ms
step:20/2330 train_time:1169ms step_avg:58.45ms
step:21/2330 train_time:1227ms step_avg:58.42ms
step:22/2330 train_time:1287ms step_avg:58.51ms
step:23/2330 train_time:1344ms step_avg:58.43ms
step:24/2330 train_time:1403ms step_avg:58.44ms
step:25/2330 train_time:1458ms step_avg:58.34ms
step:26/2330 train_time:1517ms step_avg:58.36ms
step:27/2330 train_time:1573ms step_avg:58.24ms
step:28/2330 train_time:1631ms step_avg:58.27ms
step:29/2330 train_time:1687ms step_avg:58.18ms
step:30/2330 train_time:1745ms step_avg:58.17ms
step:31/2330 train_time:1801ms step_avg:58.08ms
step:32/2330 train_time:1859ms step_avg:58.09ms
step:33/2330 train_time:1914ms step_avg:58.01ms
step:34/2330 train_time:1973ms step_avg:58.04ms
step:35/2330 train_time:2029ms step_avg:57.98ms
step:36/2330 train_time:2089ms step_avg:58.02ms
step:37/2330 train_time:2145ms step_avg:57.98ms
step:38/2330 train_time:2205ms step_avg:58.02ms
step:39/2330 train_time:2261ms step_avg:57.98ms
step:40/2330 train_time:2321ms step_avg:58.03ms
step:41/2330 train_time:2378ms step_avg:58.00ms
step:42/2330 train_time:2437ms step_avg:58.02ms
step:43/2330 train_time:2492ms step_avg:57.96ms
step:44/2330 train_time:2551ms step_avg:57.98ms
step:45/2330 train_time:2607ms step_avg:57.94ms
step:46/2330 train_time:2666ms step_avg:57.95ms
step:47/2330 train_time:2721ms step_avg:57.90ms
step:48/2330 train_time:2780ms step_avg:57.92ms
step:49/2330 train_time:2836ms step_avg:57.88ms
step:50/2330 train_time:2895ms step_avg:57.89ms
step:51/2330 train_time:2951ms step_avg:57.87ms
step:52/2330 train_time:3011ms step_avg:57.90ms
step:53/2330 train_time:3067ms step_avg:57.87ms
step:54/2330 train_time:3126ms step_avg:57.88ms
step:55/2330 train_time:3183ms step_avg:57.86ms
step:56/2330 train_time:3242ms step_avg:57.89ms
step:57/2330 train_time:3297ms step_avg:57.85ms
step:58/2330 train_time:3358ms step_avg:57.89ms
step:59/2330 train_time:3413ms step_avg:57.85ms
step:60/2330 train_time:3474ms step_avg:57.89ms
step:61/2330 train_time:3531ms step_avg:57.88ms
step:62/2330 train_time:3590ms step_avg:57.90ms
step:63/2330 train_time:3646ms step_avg:57.87ms
step:64/2330 train_time:3704ms step_avg:57.87ms
step:65/2330 train_time:3759ms step_avg:57.84ms
step:66/2330 train_time:3818ms step_avg:57.85ms
step:67/2330 train_time:3874ms step_avg:57.82ms
step:68/2330 train_time:3933ms step_avg:57.84ms
step:69/2330 train_time:3989ms step_avg:57.81ms
step:70/2330 train_time:4048ms step_avg:57.83ms
step:71/2330 train_time:4104ms step_avg:57.81ms
step:72/2330 train_time:4163ms step_avg:57.83ms
step:73/2330 train_time:4220ms step_avg:57.81ms
step:74/2330 train_time:4279ms step_avg:57.82ms
step:75/2330 train_time:4334ms step_avg:57.79ms
step:76/2330 train_time:4396ms step_avg:57.84ms
step:77/2330 train_time:4452ms step_avg:57.82ms
step:78/2330 train_time:4512ms step_avg:57.84ms
step:79/2330 train_time:4568ms step_avg:57.83ms
step:80/2330 train_time:4628ms step_avg:57.85ms
step:81/2330 train_time:4684ms step_avg:57.82ms
step:82/2330 train_time:4742ms step_avg:57.83ms
step:83/2330 train_time:4798ms step_avg:57.81ms
step:84/2330 train_time:4857ms step_avg:57.82ms
step:85/2330 train_time:4913ms step_avg:57.80ms
step:86/2330 train_time:4973ms step_avg:57.83ms
step:87/2330 train_time:5031ms step_avg:57.83ms
step:88/2330 train_time:5090ms step_avg:57.84ms
step:89/2330 train_time:5146ms step_avg:57.82ms
step:90/2330 train_time:5205ms step_avg:57.84ms
step:91/2330 train_time:5261ms step_avg:57.82ms
step:92/2330 train_time:5320ms step_avg:57.83ms
step:93/2330 train_time:5376ms step_avg:57.81ms
step:94/2330 train_time:5437ms step_avg:57.84ms
step:95/2330 train_time:5493ms step_avg:57.82ms
step:96/2330 train_time:5552ms step_avg:57.84ms
step:97/2330 train_time:5609ms step_avg:57.83ms
step:98/2330 train_time:5669ms step_avg:57.84ms
step:99/2330 train_time:5724ms step_avg:57.82ms
step:100/2330 train_time:5783ms step_avg:57.83ms
step:101/2330 train_time:5839ms step_avg:57.81ms
step:102/2330 train_time:5897ms step_avg:57.81ms
step:103/2330 train_time:5953ms step_avg:57.80ms
step:104/2330 train_time:6012ms step_avg:57.81ms
step:105/2330 train_time:6069ms step_avg:57.80ms
step:106/2330 train_time:6128ms step_avg:57.81ms
step:107/2330 train_time:6185ms step_avg:57.80ms
step:108/2330 train_time:6244ms step_avg:57.81ms
step:109/2330 train_time:6300ms step_avg:57.80ms
step:110/2330 train_time:6358ms step_avg:57.80ms
step:111/2330 train_time:6414ms step_avg:57.79ms
step:112/2330 train_time:6474ms step_avg:57.80ms
step:113/2330 train_time:6530ms step_avg:57.79ms
step:114/2330 train_time:6590ms step_avg:57.81ms
step:115/2330 train_time:6647ms step_avg:57.80ms
step:116/2330 train_time:6705ms step_avg:57.80ms
step:117/2330 train_time:6761ms step_avg:57.79ms
step:118/2330 train_time:6820ms step_avg:57.79ms
step:119/2330 train_time:6875ms step_avg:57.77ms
step:120/2330 train_time:6935ms step_avg:57.79ms
step:121/2330 train_time:6991ms step_avg:57.77ms
step:122/2330 train_time:7050ms step_avg:57.79ms
step:123/2330 train_time:7107ms step_avg:57.78ms
step:124/2330 train_time:7166ms step_avg:57.79ms
step:125/2330 train_time:7222ms step_avg:57.78ms
step:126/2330 train_time:7281ms step_avg:57.78ms
step:127/2330 train_time:7337ms step_avg:57.77ms
step:128/2330 train_time:7397ms step_avg:57.79ms
step:129/2330 train_time:7453ms step_avg:57.77ms
step:130/2330 train_time:7512ms step_avg:57.78ms
step:131/2330 train_time:7569ms step_avg:57.78ms
step:132/2330 train_time:7628ms step_avg:57.79ms
step:133/2330 train_time:7684ms step_avg:57.77ms
step:134/2330 train_time:7743ms step_avg:57.78ms
step:135/2330 train_time:7799ms step_avg:57.77ms
step:136/2330 train_time:7857ms step_avg:57.77ms
step:137/2330 train_time:7913ms step_avg:57.76ms
step:138/2330 train_time:7973ms step_avg:57.78ms
step:139/2330 train_time:8029ms step_avg:57.76ms
step:140/2330 train_time:8088ms step_avg:57.77ms
step:141/2330 train_time:8145ms step_avg:57.76ms
step:142/2330 train_time:8203ms step_avg:57.77ms
step:143/2330 train_time:8259ms step_avg:57.76ms
step:144/2330 train_time:8319ms step_avg:57.77ms
step:145/2330 train_time:8375ms step_avg:57.76ms
step:146/2330 train_time:8435ms step_avg:57.77ms
step:147/2330 train_time:8491ms step_avg:57.76ms
step:148/2330 train_time:8550ms step_avg:57.77ms
step:149/2330 train_time:8606ms step_avg:57.76ms
step:150/2330 train_time:8665ms step_avg:57.77ms
step:151/2330 train_time:8721ms step_avg:57.75ms
step:152/2330 train_time:8780ms step_avg:57.77ms
step:153/2330 train_time:8836ms step_avg:57.75ms
step:154/2330 train_time:8896ms step_avg:57.77ms
step:155/2330 train_time:8951ms step_avg:57.75ms
step:156/2330 train_time:9012ms step_avg:57.77ms
step:157/2330 train_time:9068ms step_avg:57.76ms
step:158/2330 train_time:9128ms step_avg:57.77ms
step:159/2330 train_time:9184ms step_avg:57.76ms
step:160/2330 train_time:9243ms step_avg:57.77ms
step:161/2330 train_time:9299ms step_avg:57.76ms
step:162/2330 train_time:9358ms step_avg:57.77ms
step:163/2330 train_time:9414ms step_avg:57.75ms
step:164/2330 train_time:9474ms step_avg:57.77ms
step:165/2330 train_time:9531ms step_avg:57.76ms
step:166/2330 train_time:9590ms step_avg:57.77ms
step:167/2330 train_time:9647ms step_avg:57.77ms
step:168/2330 train_time:9706ms step_avg:57.77ms
step:169/2330 train_time:9762ms step_avg:57.76ms
step:170/2330 train_time:9821ms step_avg:57.77ms
step:171/2330 train_time:9877ms step_avg:57.76ms
step:172/2330 train_time:9936ms step_avg:57.77ms
step:173/2330 train_time:9992ms step_avg:57.76ms
step:174/2330 train_time:10051ms step_avg:57.76ms
step:175/2330 train_time:10107ms step_avg:57.75ms
step:176/2330 train_time:10167ms step_avg:57.76ms
step:177/2330 train_time:10223ms step_avg:57.76ms
step:178/2330 train_time:10281ms step_avg:57.76ms
step:179/2330 train_time:10337ms step_avg:57.75ms
step:180/2330 train_time:10396ms step_avg:57.76ms
step:181/2330 train_time:10452ms step_avg:57.75ms
step:182/2330 train_time:10513ms step_avg:57.76ms
step:183/2330 train_time:10569ms step_avg:57.76ms
step:184/2330 train_time:10628ms step_avg:57.76ms
step:185/2330 train_time:10684ms step_avg:57.75ms
step:186/2330 train_time:10743ms step_avg:57.76ms
step:187/2330 train_time:10799ms step_avg:57.75ms
step:188/2330 train_time:10857ms step_avg:57.75ms
step:189/2330 train_time:10913ms step_avg:57.74ms
step:190/2330 train_time:10973ms step_avg:57.75ms
step:191/2330 train_time:11029ms step_avg:57.74ms
step:192/2330 train_time:11089ms step_avg:57.75ms
step:193/2330 train_time:11145ms step_avg:57.75ms
step:194/2330 train_time:11204ms step_avg:57.75ms
step:195/2330 train_time:11260ms step_avg:57.74ms
step:196/2330 train_time:11320ms step_avg:57.75ms
step:197/2330 train_time:11375ms step_avg:57.74ms
step:198/2330 train_time:11435ms step_avg:57.75ms
step:199/2330 train_time:11491ms step_avg:57.74ms
step:200/2330 train_time:11551ms step_avg:57.76ms
step:201/2330 train_time:11607ms step_avg:57.75ms
step:202/2330 train_time:11667ms step_avg:57.76ms
step:203/2330 train_time:11723ms step_avg:57.75ms
step:204/2330 train_time:11781ms step_avg:57.75ms
step:205/2330 train_time:11838ms step_avg:57.75ms
step:206/2330 train_time:11896ms step_avg:57.75ms
step:207/2330 train_time:11952ms step_avg:57.74ms
step:208/2330 train_time:12012ms step_avg:57.75ms
step:209/2330 train_time:12068ms step_avg:57.74ms
step:210/2330 train_time:12127ms step_avg:57.75ms
step:211/2330 train_time:12184ms step_avg:57.75ms
step:212/2330 train_time:12244ms step_avg:57.75ms
step:213/2330 train_time:12300ms step_avg:57.75ms
step:214/2330 train_time:12358ms step_avg:57.75ms
step:215/2330 train_time:12414ms step_avg:57.74ms
step:216/2330 train_time:12473ms step_avg:57.75ms
step:217/2330 train_time:12530ms step_avg:57.74ms
step:218/2330 train_time:12589ms step_avg:57.75ms
step:219/2330 train_time:12646ms step_avg:57.74ms
step:220/2330 train_time:12705ms step_avg:57.75ms
step:221/2330 train_time:12761ms step_avg:57.74ms
step:222/2330 train_time:12820ms step_avg:57.75ms
step:223/2330 train_time:12875ms step_avg:57.74ms
step:224/2330 train_time:12935ms step_avg:57.74ms
step:225/2330 train_time:12991ms step_avg:57.74ms
step:226/2330 train_time:13050ms step_avg:57.75ms
step:227/2330 train_time:13107ms step_avg:57.74ms
step:228/2330 train_time:13166ms step_avg:57.75ms
step:229/2330 train_time:13223ms step_avg:57.74ms
step:230/2330 train_time:13282ms step_avg:57.75ms
step:231/2330 train_time:13338ms step_avg:57.74ms
step:232/2330 train_time:13397ms step_avg:57.74ms
step:233/2330 train_time:13453ms step_avg:57.74ms
step:234/2330 train_time:13512ms step_avg:57.74ms
step:235/2330 train_time:13569ms step_avg:57.74ms
step:236/2330 train_time:13628ms step_avg:57.75ms
step:237/2330 train_time:13684ms step_avg:57.74ms
step:238/2330 train_time:13743ms step_avg:57.74ms
step:239/2330 train_time:13799ms step_avg:57.74ms
step:240/2330 train_time:13857ms step_avg:57.74ms
step:241/2330 train_time:13913ms step_avg:57.73ms
step:242/2330 train_time:13974ms step_avg:57.74ms
step:243/2330 train_time:14029ms step_avg:57.73ms
step:244/2330 train_time:14089ms step_avg:57.74ms
step:245/2330 train_time:14145ms step_avg:57.73ms
step:246/2330 train_time:14204ms step_avg:57.74ms
step:247/2330 train_time:14260ms step_avg:57.73ms
step:248/2330 train_time:14319ms step_avg:57.74ms
step:249/2330 train_time:14375ms step_avg:57.73ms
step:250/2330 train_time:14435ms step_avg:57.74ms
step:250/2330 val_loss:4.9862 train_time:14514ms step_avg:58.06ms
step:251/2330 train_time:14533ms step_avg:57.90ms
step:252/2330 train_time:14552ms step_avg:57.75ms
step:253/2330 train_time:14607ms step_avg:57.73ms
step:254/2330 train_time:14674ms step_avg:57.77ms
step:255/2330 train_time:14729ms step_avg:57.76ms
step:256/2330 train_time:14794ms step_avg:57.79ms
step:257/2330 train_time:14849ms step_avg:57.78ms
step:258/2330 train_time:14908ms step_avg:57.78ms
step:259/2330 train_time:14964ms step_avg:57.78ms
step:260/2330 train_time:15023ms step_avg:57.78ms
step:261/2330 train_time:15079ms step_avg:57.77ms
step:262/2330 train_time:15138ms step_avg:57.78ms
step:263/2330 train_time:15193ms step_avg:57.77ms
step:264/2330 train_time:15251ms step_avg:57.77ms
step:265/2330 train_time:15307ms step_avg:57.76ms
step:266/2330 train_time:15365ms step_avg:57.76ms
step:267/2330 train_time:15422ms step_avg:57.76ms
step:268/2330 train_time:15481ms step_avg:57.77ms
step:269/2330 train_time:15539ms step_avg:57.77ms
step:270/2330 train_time:15599ms step_avg:57.77ms
step:271/2330 train_time:15656ms step_avg:57.77ms
step:272/2330 train_time:15715ms step_avg:57.78ms
step:273/2330 train_time:15771ms step_avg:57.77ms
step:274/2330 train_time:15832ms step_avg:57.78ms
step:275/2330 train_time:15888ms step_avg:57.77ms
step:276/2330 train_time:15947ms step_avg:57.78ms
step:277/2330 train_time:16002ms step_avg:57.77ms
step:278/2330 train_time:16062ms step_avg:57.78ms
step:279/2330 train_time:16118ms step_avg:57.77ms
step:280/2330 train_time:16177ms step_avg:57.78ms
step:281/2330 train_time:16233ms step_avg:57.77ms
step:282/2330 train_time:16291ms step_avg:57.77ms
step:283/2330 train_time:16347ms step_avg:57.76ms
step:284/2330 train_time:16406ms step_avg:57.77ms
step:285/2330 train_time:16462ms step_avg:57.76ms
step:286/2330 train_time:16521ms step_avg:57.77ms
step:287/2330 train_time:16578ms step_avg:57.76ms
step:288/2330 train_time:16638ms step_avg:57.77ms
step:289/2330 train_time:16695ms step_avg:57.77ms
step:290/2330 train_time:16754ms step_avg:57.77ms
step:291/2330 train_time:16810ms step_avg:57.77ms
step:292/2330 train_time:16870ms step_avg:57.77ms
step:293/2330 train_time:16925ms step_avg:57.77ms
step:294/2330 train_time:16986ms step_avg:57.77ms
step:295/2330 train_time:17042ms step_avg:57.77ms
step:296/2330 train_time:17101ms step_avg:57.77ms
step:297/2330 train_time:17157ms step_avg:57.77ms
step:298/2330 train_time:17215ms step_avg:57.77ms
step:299/2330 train_time:17271ms step_avg:57.76ms
step:300/2330 train_time:17330ms step_avg:57.77ms
step:301/2330 train_time:17386ms step_avg:57.76ms
step:302/2330 train_time:17445ms step_avg:57.76ms
step:303/2330 train_time:17501ms step_avg:57.76ms
step:304/2330 train_time:17560ms step_avg:57.76ms
step:305/2330 train_time:17617ms step_avg:57.76ms
step:306/2330 train_time:17677ms step_avg:57.77ms
step:307/2330 train_time:17734ms step_avg:57.76ms
step:308/2330 train_time:17793ms step_avg:57.77ms
step:309/2330 train_time:17849ms step_avg:57.76ms
step:310/2330 train_time:17909ms step_avg:57.77ms
step:311/2330 train_time:17965ms step_avg:57.76ms
step:312/2330 train_time:18025ms step_avg:57.77ms
step:313/2330 train_time:18081ms step_avg:57.77ms
step:314/2330 train_time:18140ms step_avg:57.77ms
step:315/2330 train_time:18196ms step_avg:57.77ms
step:316/2330 train_time:18255ms step_avg:57.77ms
step:317/2330 train_time:18311ms step_avg:57.76ms
step:318/2330 train_time:18370ms step_avg:57.77ms
step:319/2330 train_time:18426ms step_avg:57.76ms
step:320/2330 train_time:18485ms step_avg:57.76ms
step:321/2330 train_time:18541ms step_avg:57.76ms
step:322/2330 train_time:18600ms step_avg:57.76ms
step:323/2330 train_time:18657ms step_avg:57.76ms
step:324/2330 train_time:18717ms step_avg:57.77ms
step:325/2330 train_time:18774ms step_avg:57.77ms
step:326/2330 train_time:18833ms step_avg:57.77ms
step:327/2330 train_time:18889ms step_avg:57.76ms
step:328/2330 train_time:18949ms step_avg:57.77ms
step:329/2330 train_time:19005ms step_avg:57.77ms
step:330/2330 train_time:19064ms step_avg:57.77ms
step:331/2330 train_time:19119ms step_avg:57.76ms
step:332/2330 train_time:19179ms step_avg:57.77ms
step:333/2330 train_time:19236ms step_avg:57.76ms
step:334/2330 train_time:19295ms step_avg:57.77ms
step:335/2330 train_time:19351ms step_avg:57.77ms
step:336/2330 train_time:19410ms step_avg:57.77ms
step:337/2330 train_time:19466ms step_avg:57.76ms
step:338/2330 train_time:19525ms step_avg:57.76ms
step:339/2330 train_time:19581ms step_avg:57.76ms
step:340/2330 train_time:19640ms step_avg:57.77ms
step:341/2330 train_time:19697ms step_avg:57.76ms
step:342/2330 train_time:19757ms step_avg:57.77ms
step:343/2330 train_time:19813ms step_avg:57.76ms
step:344/2330 train_time:19872ms step_avg:57.77ms
step:345/2330 train_time:19928ms step_avg:57.76ms
step:346/2330 train_time:19987ms step_avg:57.77ms
step:347/2330 train_time:20043ms step_avg:57.76ms
step:348/2330 train_time:20103ms step_avg:57.77ms
step:349/2330 train_time:20159ms step_avg:57.76ms
step:350/2330 train_time:20218ms step_avg:57.77ms
step:351/2330 train_time:20273ms step_avg:57.76ms
step:352/2330 train_time:20333ms step_avg:57.76ms
step:353/2330 train_time:20389ms step_avg:57.76ms
step:354/2330 train_time:20448ms step_avg:57.76ms
step:355/2330 train_time:20503ms step_avg:57.76ms
step:356/2330 train_time:20564ms step_avg:57.76ms
step:357/2330 train_time:20620ms step_avg:57.76ms
step:358/2330 train_time:20679ms step_avg:57.76ms
step:359/2330 train_time:20736ms step_avg:57.76ms
step:360/2330 train_time:20795ms step_avg:57.76ms
step:361/2330 train_time:20852ms step_avg:57.76ms
step:362/2330 train_time:20910ms step_avg:57.76ms
step:363/2330 train_time:20967ms step_avg:57.76ms
step:364/2330 train_time:21026ms step_avg:57.76ms
step:365/2330 train_time:21082ms step_avg:57.76ms
step:366/2330 train_time:21142ms step_avg:57.76ms
step:367/2330 train_time:21199ms step_avg:57.76ms
step:368/2330 train_time:21258ms step_avg:57.77ms
step:369/2330 train_time:21314ms step_avg:57.76ms
step:370/2330 train_time:21372ms step_avg:57.76ms
step:371/2330 train_time:21428ms step_avg:57.76ms
step:372/2330 train_time:21488ms step_avg:57.76ms
step:373/2330 train_time:21544ms step_avg:57.76ms
step:374/2330 train_time:21604ms step_avg:57.76ms
step:375/2330 train_time:21660ms step_avg:57.76ms
step:376/2330 train_time:21720ms step_avg:57.77ms
step:377/2330 train_time:21777ms step_avg:57.76ms
step:378/2330 train_time:21837ms step_avg:57.77ms
step:379/2330 train_time:21894ms step_avg:57.77ms
step:380/2330 train_time:21952ms step_avg:57.77ms
step:381/2330 train_time:22008ms step_avg:57.76ms
step:382/2330 train_time:22067ms step_avg:57.77ms
step:383/2330 train_time:22123ms step_avg:57.76ms
step:384/2330 train_time:22184ms step_avg:57.77ms
step:385/2330 train_time:22240ms step_avg:57.77ms
step:386/2330 train_time:22299ms step_avg:57.77ms
step:387/2330 train_time:22356ms step_avg:57.77ms
step:388/2330 train_time:22415ms step_avg:57.77ms
step:389/2330 train_time:22471ms step_avg:57.77ms
step:390/2330 train_time:22530ms step_avg:57.77ms
step:391/2330 train_time:22586ms step_avg:57.77ms
step:392/2330 train_time:22646ms step_avg:57.77ms
step:393/2330 train_time:22702ms step_avg:57.77ms
step:394/2330 train_time:22763ms step_avg:57.77ms
step:395/2330 train_time:22819ms step_avg:57.77ms
step:396/2330 train_time:22880ms step_avg:57.78ms
step:397/2330 train_time:22936ms step_avg:57.77ms
step:398/2330 train_time:22995ms step_avg:57.78ms
step:399/2330 train_time:23051ms step_avg:57.77ms
step:400/2330 train_time:23109ms step_avg:57.77ms
step:401/2330 train_time:23165ms step_avg:57.77ms
step:402/2330 train_time:23225ms step_avg:57.77ms
step:403/2330 train_time:23282ms step_avg:57.77ms
step:404/2330 train_time:23341ms step_avg:57.77ms
step:405/2330 train_time:23398ms step_avg:57.77ms
step:406/2330 train_time:23456ms step_avg:57.77ms
step:407/2330 train_time:23512ms step_avg:57.77ms
step:408/2330 train_time:23571ms step_avg:57.77ms
step:409/2330 train_time:23627ms step_avg:57.77ms
step:410/2330 train_time:23687ms step_avg:57.77ms
step:411/2330 train_time:23743ms step_avg:57.77ms
step:412/2330 train_time:23802ms step_avg:57.77ms
step:413/2330 train_time:23859ms step_avg:57.77ms
step:414/2330 train_time:23918ms step_avg:57.77ms
step:415/2330 train_time:23975ms step_avg:57.77ms
step:416/2330 train_time:24033ms step_avg:57.77ms
step:417/2330 train_time:24089ms step_avg:57.77ms
step:418/2330 train_time:24148ms step_avg:57.77ms
step:419/2330 train_time:24204ms step_avg:57.77ms
step:420/2330 train_time:24264ms step_avg:57.77ms
step:421/2330 train_time:24321ms step_avg:57.77ms
step:422/2330 train_time:24380ms step_avg:57.77ms
step:423/2330 train_time:24437ms step_avg:57.77ms
step:424/2330 train_time:24496ms step_avg:57.77ms
step:425/2330 train_time:24552ms step_avg:57.77ms
step:426/2330 train_time:24611ms step_avg:57.77ms
step:427/2330 train_time:24667ms step_avg:57.77ms
step:428/2330 train_time:24726ms step_avg:57.77ms
step:429/2330 train_time:24782ms step_avg:57.77ms
step:430/2330 train_time:24842ms step_avg:57.77ms
step:431/2330 train_time:24898ms step_avg:57.77ms
step:432/2330 train_time:24957ms step_avg:57.77ms
step:433/2330 train_time:25013ms step_avg:57.77ms
step:434/2330 train_time:25072ms step_avg:57.77ms
step:435/2330 train_time:25128ms step_avg:57.77ms
step:436/2330 train_time:25187ms step_avg:57.77ms
step:437/2330 train_time:25242ms step_avg:57.76ms
step:438/2330 train_time:25303ms step_avg:57.77ms
step:439/2330 train_time:25359ms step_avg:57.77ms
step:440/2330 train_time:25420ms step_avg:57.77ms
step:441/2330 train_time:25476ms step_avg:57.77ms
step:442/2330 train_time:25536ms step_avg:57.77ms
step:443/2330 train_time:25592ms step_avg:57.77ms
step:444/2330 train_time:25651ms step_avg:57.77ms
step:445/2330 train_time:25707ms step_avg:57.77ms
step:446/2330 train_time:25766ms step_avg:57.77ms
step:447/2330 train_time:25822ms step_avg:57.77ms
step:448/2330 train_time:25882ms step_avg:57.77ms
step:449/2330 train_time:25939ms step_avg:57.77ms
step:450/2330 train_time:25997ms step_avg:57.77ms
step:451/2330 train_time:26053ms step_avg:57.77ms
step:452/2330 train_time:26112ms step_avg:57.77ms
step:453/2330 train_time:26168ms step_avg:57.77ms
step:454/2330 train_time:26227ms step_avg:57.77ms
step:455/2330 train_time:26283ms step_avg:57.76ms
step:456/2330 train_time:26344ms step_avg:57.77ms
step:457/2330 train_time:26400ms step_avg:57.77ms
step:458/2330 train_time:26459ms step_avg:57.77ms
step:459/2330 train_time:26516ms step_avg:57.77ms
step:460/2330 train_time:26575ms step_avg:57.77ms
step:461/2330 train_time:26631ms step_avg:57.77ms
step:462/2330 train_time:26690ms step_avg:57.77ms
step:463/2330 train_time:26747ms step_avg:57.77ms
step:464/2330 train_time:26806ms step_avg:57.77ms
step:465/2330 train_time:26862ms step_avg:57.77ms
step:466/2330 train_time:26921ms step_avg:57.77ms
step:467/2330 train_time:26978ms step_avg:57.77ms
step:468/2330 train_time:27037ms step_avg:57.77ms
step:469/2330 train_time:27093ms step_avg:57.77ms
step:470/2330 train_time:27152ms step_avg:57.77ms
step:471/2330 train_time:27209ms step_avg:57.77ms
step:472/2330 train_time:27267ms step_avg:57.77ms
step:473/2330 train_time:27323ms step_avg:57.76ms
step:474/2330 train_time:27384ms step_avg:57.77ms
step:475/2330 train_time:27441ms step_avg:57.77ms
step:476/2330 train_time:27500ms step_avg:57.77ms
step:477/2330 train_time:27555ms step_avg:57.77ms
step:478/2330 train_time:27615ms step_avg:57.77ms
step:479/2330 train_time:27671ms step_avg:57.77ms
step:480/2330 train_time:27730ms step_avg:57.77ms
step:481/2330 train_time:27786ms step_avg:57.77ms
step:482/2330 train_time:27845ms step_avg:57.77ms
step:483/2330 train_time:27902ms step_avg:57.77ms
step:484/2330 train_time:27961ms step_avg:57.77ms
step:485/2330 train_time:28018ms step_avg:57.77ms
step:486/2330 train_time:28077ms step_avg:57.77ms
step:487/2330 train_time:28133ms step_avg:57.77ms
step:488/2330 train_time:28192ms step_avg:57.77ms
step:489/2330 train_time:28248ms step_avg:57.77ms
step:490/2330 train_time:28307ms step_avg:57.77ms
step:491/2330 train_time:28363ms step_avg:57.77ms
step:492/2330 train_time:28422ms step_avg:57.77ms
step:493/2330 train_time:28480ms step_avg:57.77ms
step:494/2330 train_time:28540ms step_avg:57.77ms
step:495/2330 train_time:28596ms step_avg:57.77ms
step:496/2330 train_time:28655ms step_avg:57.77ms
step:497/2330 train_time:28711ms step_avg:57.77ms
step:498/2330 train_time:28769ms step_avg:57.77ms
step:499/2330 train_time:28826ms step_avg:57.77ms
step:500/2330 train_time:28886ms step_avg:57.77ms
step:500/2330 val_loss:4.4728 train_time:28965ms step_avg:57.93ms
step:501/2330 train_time:28983ms step_avg:57.85ms
step:502/2330 train_time:29003ms step_avg:57.78ms
step:503/2330 train_time:29060ms step_avg:57.77ms
step:504/2330 train_time:29122ms step_avg:57.78ms
step:505/2330 train_time:29178ms step_avg:57.78ms
step:506/2330 train_time:29242ms step_avg:57.79ms
step:507/2330 train_time:29297ms step_avg:57.78ms
step:508/2330 train_time:29356ms step_avg:57.79ms
step:509/2330 train_time:29412ms step_avg:57.78ms
step:510/2330 train_time:29472ms step_avg:57.79ms
step:511/2330 train_time:29528ms step_avg:57.79ms
step:512/2330 train_time:29586ms step_avg:57.79ms
step:513/2330 train_time:29642ms step_avg:57.78ms
step:514/2330 train_time:29700ms step_avg:57.78ms
step:515/2330 train_time:29755ms step_avg:57.78ms
step:516/2330 train_time:29814ms step_avg:57.78ms
step:517/2330 train_time:29870ms step_avg:57.77ms
step:518/2330 train_time:29929ms step_avg:57.78ms
step:519/2330 train_time:29987ms step_avg:57.78ms
step:520/2330 train_time:30046ms step_avg:57.78ms
step:521/2330 train_time:30103ms step_avg:57.78ms
step:522/2330 train_time:30163ms step_avg:57.78ms
step:523/2330 train_time:30220ms step_avg:57.78ms
step:524/2330 train_time:30279ms step_avg:57.78ms
step:525/2330 train_time:30336ms step_avg:57.78ms
step:526/2330 train_time:30395ms step_avg:57.79ms
step:527/2330 train_time:30450ms step_avg:57.78ms
step:528/2330 train_time:30510ms step_avg:57.78ms
step:529/2330 train_time:30566ms step_avg:57.78ms
step:530/2330 train_time:30624ms step_avg:57.78ms
step:531/2330 train_time:30680ms step_avg:57.78ms
step:532/2330 train_time:30738ms step_avg:57.78ms
step:533/2330 train_time:30794ms step_avg:57.78ms
step:534/2330 train_time:30853ms step_avg:57.78ms
step:535/2330 train_time:30909ms step_avg:57.77ms
step:536/2330 train_time:30968ms step_avg:57.78ms
step:537/2330 train_time:31025ms step_avg:57.77ms
step:538/2330 train_time:31085ms step_avg:57.78ms
step:539/2330 train_time:31142ms step_avg:57.78ms
step:540/2330 train_time:31201ms step_avg:57.78ms
step:541/2330 train_time:31258ms step_avg:57.78ms
step:542/2330 train_time:31318ms step_avg:57.78ms
step:543/2330 train_time:31374ms step_avg:57.78ms
step:544/2330 train_time:31434ms step_avg:57.78ms
step:545/2330 train_time:31490ms step_avg:57.78ms
step:546/2330 train_time:31549ms step_avg:57.78ms
step:547/2330 train_time:31604ms step_avg:57.78ms
step:548/2330 train_time:31663ms step_avg:57.78ms
step:549/2330 train_time:31719ms step_avg:57.78ms
step:550/2330 train_time:31777ms step_avg:57.78ms
step:551/2330 train_time:31833ms step_avg:57.77ms
step:552/2330 train_time:31892ms step_avg:57.78ms
step:553/2330 train_time:31949ms step_avg:57.77ms
step:554/2330 train_time:32009ms step_avg:57.78ms
step:555/2330 train_time:32065ms step_avg:57.77ms
step:556/2330 train_time:32125ms step_avg:57.78ms
step:557/2330 train_time:32181ms step_avg:57.78ms
step:558/2330 train_time:32240ms step_avg:57.78ms
step:559/2330 train_time:32296ms step_avg:57.77ms
step:560/2330 train_time:32356ms step_avg:57.78ms
step:561/2330 train_time:32412ms step_avg:57.77ms
step:562/2330 train_time:32473ms step_avg:57.78ms
step:563/2330 train_time:32529ms step_avg:57.78ms
step:564/2330 train_time:32589ms step_avg:57.78ms
step:565/2330 train_time:32644ms step_avg:57.78ms
step:566/2330 train_time:32703ms step_avg:57.78ms
step:567/2330 train_time:32758ms step_avg:57.77ms
step:568/2330 train_time:32817ms step_avg:57.78ms
step:569/2330 train_time:32873ms step_avg:57.77ms
step:570/2330 train_time:32933ms step_avg:57.78ms
step:571/2330 train_time:32989ms step_avg:57.77ms
step:572/2330 train_time:33049ms step_avg:57.78ms
step:573/2330 train_time:33106ms step_avg:57.78ms
step:574/2330 train_time:33165ms step_avg:57.78ms
step:575/2330 train_time:33222ms step_avg:57.78ms
step:576/2330 train_time:33281ms step_avg:57.78ms
step:577/2330 train_time:33338ms step_avg:57.78ms
step:578/2330 train_time:33397ms step_avg:57.78ms
step:579/2330 train_time:33452ms step_avg:57.78ms
step:580/2330 train_time:33512ms step_avg:57.78ms
step:581/2330 train_time:33568ms step_avg:57.78ms
step:582/2330 train_time:33628ms step_avg:57.78ms
step:583/2330 train_time:33684ms step_avg:57.78ms
step:584/2330 train_time:33743ms step_avg:57.78ms
step:585/2330 train_time:33799ms step_avg:57.78ms
step:586/2330 train_time:33858ms step_avg:57.78ms
step:587/2330 train_time:33914ms step_avg:57.77ms
step:588/2330 train_time:33974ms step_avg:57.78ms
step:589/2330 train_time:34030ms step_avg:57.78ms
step:590/2330 train_time:34090ms step_avg:57.78ms
step:591/2330 train_time:34147ms step_avg:57.78ms
step:592/2330 train_time:34206ms step_avg:57.78ms
step:593/2330 train_time:34263ms step_avg:57.78ms
step:594/2330 train_time:34322ms step_avg:57.78ms
step:595/2330 train_time:34379ms step_avg:57.78ms
step:596/2330 train_time:34438ms step_avg:57.78ms
step:597/2330 train_time:34495ms step_avg:57.78ms
step:598/2330 train_time:34554ms step_avg:57.78ms
step:599/2330 train_time:34610ms step_avg:57.78ms
step:600/2330 train_time:34670ms step_avg:57.78ms
step:601/2330 train_time:34726ms step_avg:57.78ms
step:602/2330 train_time:34785ms step_avg:57.78ms
step:603/2330 train_time:34841ms step_avg:57.78ms
step:604/2330 train_time:34900ms step_avg:57.78ms
step:605/2330 train_time:34955ms step_avg:57.78ms
step:606/2330 train_time:35015ms step_avg:57.78ms
step:607/2330 train_time:35071ms step_avg:57.78ms
step:608/2330 train_time:35130ms step_avg:57.78ms
step:609/2330 train_time:35187ms step_avg:57.78ms
step:610/2330 train_time:35246ms step_avg:57.78ms
step:611/2330 train_time:35302ms step_avg:57.78ms
step:612/2330 train_time:35361ms step_avg:57.78ms
step:613/2330 train_time:35417ms step_avg:57.78ms
step:614/2330 train_time:35477ms step_avg:57.78ms
step:615/2330 train_time:35533ms step_avg:57.78ms
step:616/2330 train_time:35593ms step_avg:57.78ms
step:617/2330 train_time:35649ms step_avg:57.78ms
step:618/2330 train_time:35708ms step_avg:57.78ms
step:619/2330 train_time:35765ms step_avg:57.78ms
step:620/2330 train_time:35823ms step_avg:57.78ms
step:621/2330 train_time:35879ms step_avg:57.78ms
step:622/2330 train_time:35938ms step_avg:57.78ms
step:623/2330 train_time:35994ms step_avg:57.78ms
step:624/2330 train_time:36055ms step_avg:57.78ms
step:625/2330 train_time:36111ms step_avg:57.78ms
step:626/2330 train_time:36171ms step_avg:57.78ms
step:627/2330 train_time:36228ms step_avg:57.78ms
step:628/2330 train_time:36287ms step_avg:57.78ms
step:629/2330 train_time:36344ms step_avg:57.78ms
step:630/2330 train_time:36402ms step_avg:57.78ms
step:631/2330 train_time:36458ms step_avg:57.78ms
step:632/2330 train_time:36517ms step_avg:57.78ms
step:633/2330 train_time:36573ms step_avg:57.78ms
step:634/2330 train_time:36632ms step_avg:57.78ms
step:635/2330 train_time:36689ms step_avg:57.78ms
step:636/2330 train_time:36748ms step_avg:57.78ms
step:637/2330 train_time:36804ms step_avg:57.78ms
step:638/2330 train_time:36863ms step_avg:57.78ms
step:639/2330 train_time:36919ms step_avg:57.78ms
step:640/2330 train_time:36978ms step_avg:57.78ms
step:641/2330 train_time:37035ms step_avg:57.78ms
step:642/2330 train_time:37094ms step_avg:57.78ms
step:643/2330 train_time:37150ms step_avg:57.78ms
step:644/2330 train_time:37211ms step_avg:57.78ms
step:645/2330 train_time:37266ms step_avg:57.78ms
step:646/2330 train_time:37326ms step_avg:57.78ms
step:647/2330 train_time:37382ms step_avg:57.78ms
step:648/2330 train_time:37442ms step_avg:57.78ms
step:649/2330 train_time:37498ms step_avg:57.78ms
step:650/2330 train_time:37557ms step_avg:57.78ms
step:651/2330 train_time:37613ms step_avg:57.78ms
step:652/2330 train_time:37673ms step_avg:57.78ms
step:653/2330 train_time:37729ms step_avg:57.78ms
step:654/2330 train_time:37788ms step_avg:57.78ms
step:655/2330 train_time:37844ms step_avg:57.78ms
step:656/2330 train_time:37903ms step_avg:57.78ms
step:657/2330 train_time:37959ms step_avg:57.78ms
step:658/2330 train_time:38018ms step_avg:57.78ms
step:659/2330 train_time:38075ms step_avg:57.78ms
step:660/2330 train_time:38135ms step_avg:57.78ms
step:661/2330 train_time:38191ms step_avg:57.78ms
step:662/2330 train_time:38251ms step_avg:57.78ms
step:663/2330 train_time:38306ms step_avg:57.78ms
step:664/2330 train_time:38367ms step_avg:57.78ms
step:665/2330 train_time:38422ms step_avg:57.78ms
step:666/2330 train_time:38482ms step_avg:57.78ms
step:667/2330 train_time:38538ms step_avg:57.78ms
step:668/2330 train_time:38597ms step_avg:57.78ms
step:669/2330 train_time:38653ms step_avg:57.78ms
step:670/2330 train_time:38713ms step_avg:57.78ms
step:671/2330 train_time:38770ms step_avg:57.78ms
step:672/2330 train_time:38829ms step_avg:57.78ms
step:673/2330 train_time:38885ms step_avg:57.78ms
step:674/2330 train_time:38944ms step_avg:57.78ms
step:675/2330 train_time:39001ms step_avg:57.78ms
step:676/2330 train_time:39060ms step_avg:57.78ms
step:677/2330 train_time:39116ms step_avg:57.78ms
step:678/2330 train_time:39176ms step_avg:57.78ms
step:679/2330 train_time:39232ms step_avg:57.78ms
step:680/2330 train_time:39292ms step_avg:57.78ms
step:681/2330 train_time:39348ms step_avg:57.78ms
step:682/2330 train_time:39408ms step_avg:57.78ms
step:683/2330 train_time:39464ms step_avg:57.78ms
step:684/2330 train_time:39523ms step_avg:57.78ms
step:685/2330 train_time:39580ms step_avg:57.78ms
step:686/2330 train_time:39639ms step_avg:57.78ms
step:687/2330 train_time:39695ms step_avg:57.78ms
step:688/2330 train_time:39753ms step_avg:57.78ms
step:689/2330 train_time:39810ms step_avg:57.78ms
step:690/2330 train_time:39869ms step_avg:57.78ms
step:691/2330 train_time:39926ms step_avg:57.78ms
step:692/2330 train_time:39984ms step_avg:57.78ms
step:693/2330 train_time:40041ms step_avg:57.78ms
step:694/2330 train_time:40099ms step_avg:57.78ms
step:695/2330 train_time:40155ms step_avg:57.78ms
step:696/2330 train_time:40216ms step_avg:57.78ms
step:697/2330 train_time:40271ms step_avg:57.78ms
step:698/2330 train_time:40332ms step_avg:57.78ms
step:699/2330 train_time:40389ms step_avg:57.78ms
step:700/2330 train_time:40448ms step_avg:57.78ms
step:701/2330 train_time:40504ms step_avg:57.78ms
step:702/2330 train_time:40563ms step_avg:57.78ms
step:703/2330 train_time:40619ms step_avg:57.78ms
step:704/2330 train_time:40678ms step_avg:57.78ms
step:705/2330 train_time:40733ms step_avg:57.78ms
step:706/2330 train_time:40793ms step_avg:57.78ms
step:707/2330 train_time:40849ms step_avg:57.78ms
step:708/2330 train_time:40909ms step_avg:57.78ms
step:709/2330 train_time:40965ms step_avg:57.78ms
step:710/2330 train_time:41024ms step_avg:57.78ms
step:711/2330 train_time:41080ms step_avg:57.78ms
step:712/2330 train_time:41139ms step_avg:57.78ms
step:713/2330 train_time:41195ms step_avg:57.78ms
step:714/2330 train_time:41255ms step_avg:57.78ms
step:715/2330 train_time:41311ms step_avg:57.78ms
step:716/2330 train_time:41371ms step_avg:57.78ms
step:717/2330 train_time:41427ms step_avg:57.78ms
step:718/2330 train_time:41487ms step_avg:57.78ms
step:719/2330 train_time:41542ms step_avg:57.78ms
step:720/2330 train_time:41601ms step_avg:57.78ms
step:721/2330 train_time:41657ms step_avg:57.78ms
step:722/2330 train_time:41715ms step_avg:57.78ms
step:723/2330 train_time:41771ms step_avg:57.77ms
step:724/2330 train_time:41831ms step_avg:57.78ms
step:725/2330 train_time:41888ms step_avg:57.78ms
step:726/2330 train_time:41947ms step_avg:57.78ms
step:727/2330 train_time:42004ms step_avg:57.78ms
step:728/2330 train_time:42062ms step_avg:57.78ms
step:729/2330 train_time:42119ms step_avg:57.78ms
step:730/2330 train_time:42177ms step_avg:57.78ms
step:731/2330 train_time:42234ms step_avg:57.78ms
step:732/2330 train_time:42293ms step_avg:57.78ms
step:733/2330 train_time:42350ms step_avg:57.78ms
step:734/2330 train_time:42409ms step_avg:57.78ms
step:735/2330 train_time:42465ms step_avg:57.78ms
step:736/2330 train_time:42524ms step_avg:57.78ms
step:737/2330 train_time:42580ms step_avg:57.77ms
step:738/2330 train_time:42639ms step_avg:57.78ms
step:739/2330 train_time:42694ms step_avg:57.77ms
step:740/2330 train_time:42754ms step_avg:57.78ms
step:741/2330 train_time:42810ms step_avg:57.77ms
step:742/2330 train_time:42870ms step_avg:57.78ms
step:743/2330 train_time:42926ms step_avg:57.77ms
step:744/2330 train_time:42985ms step_avg:57.78ms
step:745/2330 train_time:43041ms step_avg:57.77ms
step:746/2330 train_time:43099ms step_avg:57.77ms
step:747/2330 train_time:43155ms step_avg:57.77ms
step:748/2330 train_time:43215ms step_avg:57.77ms
step:749/2330 train_time:43271ms step_avg:57.77ms
step:750/2330 train_time:43331ms step_avg:57.77ms
step:750/2330 val_loss:4.2485 train_time:43411ms step_avg:57.88ms
step:751/2330 train_time:43430ms step_avg:57.83ms
step:752/2330 train_time:43449ms step_avg:57.78ms
step:753/2330 train_time:43504ms step_avg:57.77ms
step:754/2330 train_time:43569ms step_avg:57.78ms
step:755/2330 train_time:43627ms step_avg:57.78ms
step:756/2330 train_time:43687ms step_avg:57.79ms
step:757/2330 train_time:43742ms step_avg:57.78ms
step:758/2330 train_time:43801ms step_avg:57.79ms
step:759/2330 train_time:43857ms step_avg:57.78ms
step:760/2330 train_time:43916ms step_avg:57.78ms
step:761/2330 train_time:43971ms step_avg:57.78ms
step:762/2330 train_time:44030ms step_avg:57.78ms
step:763/2330 train_time:44086ms step_avg:57.78ms
step:764/2330 train_time:44144ms step_avg:57.78ms
step:765/2330 train_time:44201ms step_avg:57.78ms
step:766/2330 train_time:44258ms step_avg:57.78ms
step:767/2330 train_time:44315ms step_avg:57.78ms
step:768/2330 train_time:44375ms step_avg:57.78ms
step:769/2330 train_time:44434ms step_avg:57.78ms
step:770/2330 train_time:44495ms step_avg:57.79ms
step:771/2330 train_time:44552ms step_avg:57.78ms
step:772/2330 train_time:44614ms step_avg:57.79ms
step:773/2330 train_time:44672ms step_avg:57.79ms
step:774/2330 train_time:44733ms step_avg:57.79ms
step:775/2330 train_time:44790ms step_avg:57.79ms
step:776/2330 train_time:44849ms step_avg:57.80ms
step:777/2330 train_time:44905ms step_avg:57.79ms
step:778/2330 train_time:44964ms step_avg:57.79ms
step:779/2330 train_time:45020ms step_avg:57.79ms
step:780/2330 train_time:45080ms step_avg:57.79ms
step:781/2330 train_time:45136ms step_avg:57.79ms
step:782/2330 train_time:45195ms step_avg:57.79ms
step:783/2330 train_time:45251ms step_avg:57.79ms
step:784/2330 train_time:45311ms step_avg:57.79ms
step:785/2330 train_time:45368ms step_avg:57.79ms
step:786/2330 train_time:45429ms step_avg:57.80ms
step:787/2330 train_time:45487ms step_avg:57.80ms
step:788/2330 train_time:45547ms step_avg:57.80ms
step:789/2330 train_time:45604ms step_avg:57.80ms
step:790/2330 train_time:45665ms step_avg:57.80ms
step:791/2330 train_time:45722ms step_avg:57.80ms
step:792/2330 train_time:45782ms step_avg:57.81ms
step:793/2330 train_time:45838ms step_avg:57.80ms
step:794/2330 train_time:45900ms step_avg:57.81ms
step:795/2330 train_time:45956ms step_avg:57.81ms
step:796/2330 train_time:46016ms step_avg:57.81ms
step:797/2330 train_time:46072ms step_avg:57.81ms
step:798/2330 train_time:46132ms step_avg:57.81ms
step:799/2330 train_time:46188ms step_avg:57.81ms
step:800/2330 train_time:46247ms step_avg:57.81ms
step:801/2330 train_time:46304ms step_avg:57.81ms
step:802/2330 train_time:46363ms step_avg:57.81ms
step:803/2330 train_time:46420ms step_avg:57.81ms
step:804/2330 train_time:46481ms step_avg:57.81ms
step:805/2330 train_time:46538ms step_avg:57.81ms
step:806/2330 train_time:46599ms step_avg:57.81ms
step:807/2330 train_time:46656ms step_avg:57.81ms
step:808/2330 train_time:46717ms step_avg:57.82ms
step:809/2330 train_time:46774ms step_avg:57.82ms
step:810/2330 train_time:46834ms step_avg:57.82ms
step:811/2330 train_time:46891ms step_avg:57.82ms
step:812/2330 train_time:46951ms step_avg:57.82ms
step:813/2330 train_time:47008ms step_avg:57.82ms
step:814/2330 train_time:47067ms step_avg:57.82ms
step:815/2330 train_time:47124ms step_avg:57.82ms
step:816/2330 train_time:47182ms step_avg:57.82ms
step:817/2330 train_time:47239ms step_avg:57.82ms
step:818/2330 train_time:47299ms step_avg:57.82ms
step:819/2330 train_time:47355ms step_avg:57.82ms
step:820/2330 train_time:47416ms step_avg:57.82ms
step:821/2330 train_time:47474ms step_avg:57.82ms
step:822/2330 train_time:47533ms step_avg:57.83ms
step:823/2330 train_time:47591ms step_avg:57.83ms
step:824/2330 train_time:47652ms step_avg:57.83ms
step:825/2330 train_time:47709ms step_avg:57.83ms
step:826/2330 train_time:47768ms step_avg:57.83ms
step:827/2330 train_time:47825ms step_avg:57.83ms
step:828/2330 train_time:47885ms step_avg:57.83ms
step:829/2330 train_time:47942ms step_avg:57.83ms
step:830/2330 train_time:48001ms step_avg:57.83ms
step:831/2330 train_time:48058ms step_avg:57.83ms
step:832/2330 train_time:48117ms step_avg:57.83ms
step:833/2330 train_time:48174ms step_avg:57.83ms
step:834/2330 train_time:48234ms step_avg:57.83ms
step:835/2330 train_time:48290ms step_avg:57.83ms
step:836/2330 train_time:48350ms step_avg:57.84ms
step:837/2330 train_time:48407ms step_avg:57.83ms
step:838/2330 train_time:48467ms step_avg:57.84ms
step:839/2330 train_time:48524ms step_avg:57.84ms
step:840/2330 train_time:48583ms step_avg:57.84ms
step:841/2330 train_time:48640ms step_avg:57.84ms
step:842/2330 train_time:48701ms step_avg:57.84ms
step:843/2330 train_time:48758ms step_avg:57.84ms
step:844/2330 train_time:48818ms step_avg:57.84ms
step:845/2330 train_time:48875ms step_avg:57.84ms
step:846/2330 train_time:48935ms step_avg:57.84ms
step:847/2330 train_time:48992ms step_avg:57.84ms
step:848/2330 train_time:49052ms step_avg:57.84ms
step:849/2330 train_time:49109ms step_avg:57.84ms
step:850/2330 train_time:49169ms step_avg:57.85ms
step:851/2330 train_time:49226ms step_avg:57.84ms
step:852/2330 train_time:49285ms step_avg:57.85ms
step:853/2330 train_time:49341ms step_avg:57.84ms
step:854/2330 train_time:49401ms step_avg:57.85ms
step:855/2330 train_time:49458ms step_avg:57.85ms
step:856/2330 train_time:49519ms step_avg:57.85ms
step:857/2330 train_time:49575ms step_avg:57.85ms
step:858/2330 train_time:49636ms step_avg:57.85ms
step:859/2330 train_time:49694ms step_avg:57.85ms
step:860/2330 train_time:49754ms step_avg:57.85ms
step:861/2330 train_time:49810ms step_avg:57.85ms
step:862/2330 train_time:49871ms step_avg:57.85ms
step:863/2330 train_time:49928ms step_avg:57.85ms
step:864/2330 train_time:49987ms step_avg:57.86ms
step:865/2330 train_time:50043ms step_avg:57.85ms
step:866/2330 train_time:50104ms step_avg:57.86ms
step:867/2330 train_time:50160ms step_avg:57.85ms
step:868/2330 train_time:50221ms step_avg:57.86ms
step:869/2330 train_time:50277ms step_avg:57.86ms
step:870/2330 train_time:50337ms step_avg:57.86ms
step:871/2330 train_time:50394ms step_avg:57.86ms
step:872/2330 train_time:50455ms step_avg:57.86ms
step:873/2330 train_time:50511ms step_avg:57.86ms
step:874/2330 train_time:50572ms step_avg:57.86ms
step:875/2330 train_time:50629ms step_avg:57.86ms
step:876/2330 train_time:50689ms step_avg:57.86ms
step:877/2330 train_time:50746ms step_avg:57.86ms
step:878/2330 train_time:50805ms step_avg:57.86ms
step:879/2330 train_time:50861ms step_avg:57.86ms
step:880/2330 train_time:50922ms step_avg:57.87ms
step:881/2330 train_time:50979ms step_avg:57.86ms
step:882/2330 train_time:51038ms step_avg:57.87ms
step:883/2330 train_time:51095ms step_avg:57.86ms
step:884/2330 train_time:51155ms step_avg:57.87ms
step:885/2330 train_time:51211ms step_avg:57.87ms
step:886/2330 train_time:51272ms step_avg:57.87ms
step:887/2330 train_time:51328ms step_avg:57.87ms
step:888/2330 train_time:51388ms step_avg:57.87ms
step:889/2330 train_time:51445ms step_avg:57.87ms
step:890/2330 train_time:51504ms step_avg:57.87ms
step:891/2330 train_time:51560ms step_avg:57.87ms
step:892/2330 train_time:51621ms step_avg:57.87ms
step:893/2330 train_time:51678ms step_avg:57.87ms
step:894/2330 train_time:51739ms step_avg:57.87ms
step:895/2330 train_time:51795ms step_avg:57.87ms
step:896/2330 train_time:51856ms step_avg:57.88ms
step:897/2330 train_time:51913ms step_avg:57.87ms
step:898/2330 train_time:51974ms step_avg:57.88ms
step:899/2330 train_time:52031ms step_avg:57.88ms
step:900/2330 train_time:52090ms step_avg:57.88ms
step:901/2330 train_time:52147ms step_avg:57.88ms
step:902/2330 train_time:52206ms step_avg:57.88ms
step:903/2330 train_time:52263ms step_avg:57.88ms
step:904/2330 train_time:52323ms step_avg:57.88ms
step:905/2330 train_time:52379ms step_avg:57.88ms
step:906/2330 train_time:52439ms step_avg:57.88ms
step:907/2330 train_time:52496ms step_avg:57.88ms
step:908/2330 train_time:52556ms step_avg:57.88ms
step:909/2330 train_time:52612ms step_avg:57.88ms
step:910/2330 train_time:52672ms step_avg:57.88ms
step:911/2330 train_time:52729ms step_avg:57.88ms
step:912/2330 train_time:52789ms step_avg:57.88ms
step:913/2330 train_time:52846ms step_avg:57.88ms
step:914/2330 train_time:52906ms step_avg:57.88ms
step:915/2330 train_time:52963ms step_avg:57.88ms
step:916/2330 train_time:53022ms step_avg:57.88ms
step:917/2330 train_time:53078ms step_avg:57.88ms
step:918/2330 train_time:53139ms step_avg:57.89ms
step:919/2330 train_time:53195ms step_avg:57.88ms
step:920/2330 train_time:53256ms step_avg:57.89ms
step:921/2330 train_time:53312ms step_avg:57.89ms
step:922/2330 train_time:53373ms step_avg:57.89ms
step:923/2330 train_time:53430ms step_avg:57.89ms
step:924/2330 train_time:53489ms step_avg:57.89ms
step:925/2330 train_time:53546ms step_avg:57.89ms
step:926/2330 train_time:53605ms step_avg:57.89ms
step:927/2330 train_time:53662ms step_avg:57.89ms
step:928/2330 train_time:53722ms step_avg:57.89ms
step:929/2330 train_time:53779ms step_avg:57.89ms
step:930/2330 train_time:53839ms step_avg:57.89ms
step:931/2330 train_time:53895ms step_avg:57.89ms
step:932/2330 train_time:53956ms step_avg:57.89ms
step:933/2330 train_time:54013ms step_avg:57.89ms
step:934/2330 train_time:54072ms step_avg:57.89ms
step:935/2330 train_time:54130ms step_avg:57.89ms
step:936/2330 train_time:54189ms step_avg:57.89ms
step:937/2330 train_time:54245ms step_avg:57.89ms
step:938/2330 train_time:54304ms step_avg:57.89ms
step:939/2330 train_time:54361ms step_avg:57.89ms
step:940/2330 train_time:54422ms step_avg:57.90ms
step:941/2330 train_time:54478ms step_avg:57.89ms
step:942/2330 train_time:54538ms step_avg:57.90ms
step:943/2330 train_time:54594ms step_avg:57.89ms
step:944/2330 train_time:54656ms step_avg:57.90ms
step:945/2330 train_time:54713ms step_avg:57.90ms
step:946/2330 train_time:54773ms step_avg:57.90ms
step:947/2330 train_time:54831ms step_avg:57.90ms
step:948/2330 train_time:54890ms step_avg:57.90ms
step:949/2330 train_time:54948ms step_avg:57.90ms
step:950/2330 train_time:55007ms step_avg:57.90ms
step:951/2330 train_time:55064ms step_avg:57.90ms
step:952/2330 train_time:55124ms step_avg:57.90ms
step:953/2330 train_time:55181ms step_avg:57.90ms
step:954/2330 train_time:55240ms step_avg:57.90ms
step:955/2330 train_time:55296ms step_avg:57.90ms
step:956/2330 train_time:55357ms step_avg:57.91ms
step:957/2330 train_time:55414ms step_avg:57.90ms
step:958/2330 train_time:55475ms step_avg:57.91ms
step:959/2330 train_time:55532ms step_avg:57.91ms
step:960/2330 train_time:55592ms step_avg:57.91ms
step:961/2330 train_time:55650ms step_avg:57.91ms
step:962/2330 train_time:55709ms step_avg:57.91ms
step:963/2330 train_time:55766ms step_avg:57.91ms
step:964/2330 train_time:55825ms step_avg:57.91ms
step:965/2330 train_time:55882ms step_avg:57.91ms
step:966/2330 train_time:55943ms step_avg:57.91ms
step:967/2330 train_time:55999ms step_avg:57.91ms
step:968/2330 train_time:56060ms step_avg:57.91ms
step:969/2330 train_time:56117ms step_avg:57.91ms
step:970/2330 train_time:56177ms step_avg:57.91ms
step:971/2330 train_time:56234ms step_avg:57.91ms
step:972/2330 train_time:56293ms step_avg:57.91ms
step:973/2330 train_time:56350ms step_avg:57.91ms
step:974/2330 train_time:56410ms step_avg:57.92ms
step:975/2330 train_time:56466ms step_avg:57.91ms
step:976/2330 train_time:56527ms step_avg:57.92ms
step:977/2330 train_time:56583ms step_avg:57.92ms
step:978/2330 train_time:56643ms step_avg:57.92ms
step:979/2330 train_time:56700ms step_avg:57.92ms
step:980/2330 train_time:56760ms step_avg:57.92ms
step:981/2330 train_time:56816ms step_avg:57.92ms
step:982/2330 train_time:56877ms step_avg:57.92ms
step:983/2330 train_time:56934ms step_avg:57.92ms
step:984/2330 train_time:56994ms step_avg:57.92ms
step:985/2330 train_time:57052ms step_avg:57.92ms
step:986/2330 train_time:57111ms step_avg:57.92ms
step:987/2330 train_time:57168ms step_avg:57.92ms
step:988/2330 train_time:57227ms step_avg:57.92ms
step:989/2330 train_time:57283ms step_avg:57.92ms
step:990/2330 train_time:57343ms step_avg:57.92ms
step:991/2330 train_time:57400ms step_avg:57.92ms
step:992/2330 train_time:57460ms step_avg:57.92ms
step:993/2330 train_time:57516ms step_avg:57.92ms
step:994/2330 train_time:57577ms step_avg:57.92ms
step:995/2330 train_time:57634ms step_avg:57.92ms
step:996/2330 train_time:57694ms step_avg:57.93ms
step:997/2330 train_time:57751ms step_avg:57.92ms
step:998/2330 train_time:57812ms step_avg:57.93ms
step:999/2330 train_time:57868ms step_avg:57.93ms
step:1000/2330 train_time:57928ms step_avg:57.93ms
step:1000/2330 val_loss:4.1028 train_time:58009ms step_avg:58.01ms
step:1001/2330 train_time:58028ms step_avg:57.97ms
step:1002/2330 train_time:58047ms step_avg:57.93ms
step:1003/2330 train_time:58101ms step_avg:57.93ms
step:1004/2330 train_time:58173ms step_avg:57.94ms
step:1005/2330 train_time:58228ms step_avg:57.94ms
step:1006/2330 train_time:58294ms step_avg:57.95ms
step:1007/2330 train_time:58350ms step_avg:57.94ms
step:1008/2330 train_time:58410ms step_avg:57.95ms
step:1009/2330 train_time:58467ms step_avg:57.95ms
step:1010/2330 train_time:58526ms step_avg:57.95ms
step:1011/2330 train_time:58582ms step_avg:57.94ms
step:1012/2330 train_time:58641ms step_avg:57.95ms
step:1013/2330 train_time:58696ms step_avg:57.94ms
step:1014/2330 train_time:58756ms step_avg:57.94ms
step:1015/2330 train_time:58812ms step_avg:57.94ms
step:1016/2330 train_time:58872ms step_avg:57.94ms
step:1017/2330 train_time:58931ms step_avg:57.95ms
step:1018/2330 train_time:58994ms step_avg:57.95ms
step:1019/2330 train_time:59052ms step_avg:57.95ms
step:1020/2330 train_time:59113ms step_avg:57.95ms
step:1021/2330 train_time:59170ms step_avg:57.95ms
step:1022/2330 train_time:59231ms step_avg:57.96ms
step:1023/2330 train_time:59288ms step_avg:57.96ms
step:1024/2330 train_time:59349ms step_avg:57.96ms
step:1025/2330 train_time:59405ms step_avg:57.96ms
step:1026/2330 train_time:59465ms step_avg:57.96ms
step:1027/2330 train_time:59522ms step_avg:57.96ms
step:1028/2330 train_time:59581ms step_avg:57.96ms
step:1029/2330 train_time:59637ms step_avg:57.96ms
step:1030/2330 train_time:59696ms step_avg:57.96ms
step:1031/2330 train_time:59753ms step_avg:57.96ms
step:1032/2330 train_time:59812ms step_avg:57.96ms
step:1033/2330 train_time:59869ms step_avg:57.96ms
step:1034/2330 train_time:59930ms step_avg:57.96ms
step:1035/2330 train_time:59988ms step_avg:57.96ms
step:1036/2330 train_time:60050ms step_avg:57.96ms
step:1037/2330 train_time:60108ms step_avg:57.96ms
step:1038/2330 train_time:60168ms step_avg:57.97ms
step:1039/2330 train_time:60225ms step_avg:57.96ms
step:1040/2330 train_time:60285ms step_avg:57.97ms
step:1041/2330 train_time:60343ms step_avg:57.97ms
step:1042/2330 train_time:60403ms step_avg:57.97ms
step:1043/2330 train_time:60460ms step_avg:57.97ms
step:1044/2330 train_time:60519ms step_avg:57.97ms
step:1045/2330 train_time:60576ms step_avg:57.97ms
step:1046/2330 train_time:60635ms step_avg:57.97ms
step:1047/2330 train_time:60691ms step_avg:57.97ms
step:1048/2330 train_time:60752ms step_avg:57.97ms
step:1049/2330 train_time:60808ms step_avg:57.97ms
step:1050/2330 train_time:60868ms step_avg:57.97ms
step:1051/2330 train_time:60925ms step_avg:57.97ms
step:1052/2330 train_time:60985ms step_avg:57.97ms
step:1053/2330 train_time:61042ms step_avg:57.97ms
step:1054/2330 train_time:61103ms step_avg:57.97ms
step:1055/2330 train_time:61160ms step_avg:57.97ms
step:1056/2330 train_time:61220ms step_avg:57.97ms
step:1057/2330 train_time:61277ms step_avg:57.97ms
step:1058/2330 train_time:61338ms step_avg:57.98ms
step:1059/2330 train_time:61395ms step_avg:57.97ms
step:1060/2330 train_time:61455ms step_avg:57.98ms
step:1061/2330 train_time:61511ms step_avg:57.97ms
step:1062/2330 train_time:61571ms step_avg:57.98ms
step:1063/2330 train_time:61628ms step_avg:57.98ms
step:1064/2330 train_time:61687ms step_avg:57.98ms
step:1065/2330 train_time:61744ms step_avg:57.98ms
step:1066/2330 train_time:61803ms step_avg:57.98ms
step:1067/2330 train_time:61860ms step_avg:57.98ms
step:1068/2330 train_time:61920ms step_avg:57.98ms
step:1069/2330 train_time:61977ms step_avg:57.98ms
step:1070/2330 train_time:62037ms step_avg:57.98ms
step:1071/2330 train_time:62094ms step_avg:57.98ms
step:1072/2330 train_time:62155ms step_avg:57.98ms
step:1073/2330 train_time:62213ms step_avg:57.98ms
step:1074/2330 train_time:62274ms step_avg:57.98ms
step:1075/2330 train_time:62331ms step_avg:57.98ms
step:1076/2330 train_time:62391ms step_avg:57.98ms
step:1077/2330 train_time:62448ms step_avg:57.98ms
step:1078/2330 train_time:62508ms step_avg:57.98ms
step:1079/2330 train_time:62564ms step_avg:57.98ms
step:1080/2330 train_time:62624ms step_avg:57.99ms
step:1081/2330 train_time:62681ms step_avg:57.98ms
step:1082/2330 train_time:62740ms step_avg:57.99ms
step:1083/2330 train_time:62798ms step_avg:57.98ms
step:1084/2330 train_time:62857ms step_avg:57.99ms
step:1085/2330 train_time:62914ms step_avg:57.99ms
step:1086/2330 train_time:62975ms step_avg:57.99ms
step:1087/2330 train_time:63032ms step_avg:57.99ms
step:1088/2330 train_time:63093ms step_avg:57.99ms
step:1089/2330 train_time:63150ms step_avg:57.99ms
step:1090/2330 train_time:63211ms step_avg:57.99ms
step:1091/2330 train_time:63269ms step_avg:57.99ms
step:1092/2330 train_time:63329ms step_avg:57.99ms
step:1093/2330 train_time:63386ms step_avg:57.99ms
step:1094/2330 train_time:63446ms step_avg:57.99ms
step:1095/2330 train_time:63504ms step_avg:57.99ms
step:1096/2330 train_time:63563ms step_avg:58.00ms
step:1097/2330 train_time:63620ms step_avg:57.99ms
step:1098/2330 train_time:63679ms step_avg:58.00ms
step:1099/2330 train_time:63735ms step_avg:57.99ms
step:1100/2330 train_time:63796ms step_avg:58.00ms
step:1101/2330 train_time:63853ms step_avg:58.00ms
step:1102/2330 train_time:63913ms step_avg:58.00ms
step:1103/2330 train_time:63970ms step_avg:58.00ms
step:1104/2330 train_time:64031ms step_avg:58.00ms
step:1105/2330 train_time:64087ms step_avg:58.00ms
step:1106/2330 train_time:64148ms step_avg:58.00ms
step:1107/2330 train_time:64205ms step_avg:58.00ms
step:1108/2330 train_time:64265ms step_avg:58.00ms
step:1109/2330 train_time:64322ms step_avg:58.00ms
step:1110/2330 train_time:64382ms step_avg:58.00ms
step:1111/2330 train_time:64439ms step_avg:58.00ms
step:1112/2330 train_time:64499ms step_avg:58.00ms
step:1113/2330 train_time:64556ms step_avg:58.00ms
step:1114/2330 train_time:64616ms step_avg:58.00ms
step:1115/2330 train_time:64673ms step_avg:58.00ms
step:1116/2330 train_time:64733ms step_avg:58.00ms
step:1117/2330 train_time:64789ms step_avg:58.00ms
step:1118/2330 train_time:64850ms step_avg:58.01ms
step:1119/2330 train_time:64907ms step_avg:58.00ms
step:1120/2330 train_time:64967ms step_avg:58.01ms
step:1121/2330 train_time:65024ms step_avg:58.01ms
step:1122/2330 train_time:65084ms step_avg:58.01ms
step:1123/2330 train_time:65141ms step_avg:58.01ms
step:1124/2330 train_time:65201ms step_avg:58.01ms
step:1125/2330 train_time:65257ms step_avg:58.01ms
step:1126/2330 train_time:65319ms step_avg:58.01ms
step:1127/2330 train_time:65376ms step_avg:58.01ms
step:1128/2330 train_time:65436ms step_avg:58.01ms
step:1129/2330 train_time:65492ms step_avg:58.01ms
step:1130/2330 train_time:65553ms step_avg:58.01ms
step:1131/2330 train_time:65610ms step_avg:58.01ms
step:1132/2330 train_time:65670ms step_avg:58.01ms
step:1133/2330 train_time:65727ms step_avg:58.01ms
step:1134/2330 train_time:65787ms step_avg:58.01ms
step:1135/2330 train_time:65844ms step_avg:58.01ms
step:1136/2330 train_time:65904ms step_avg:58.01ms
step:1137/2330 train_time:65960ms step_avg:58.01ms
step:1138/2330 train_time:66020ms step_avg:58.01ms
step:1139/2330 train_time:66076ms step_avg:58.01ms
step:1140/2330 train_time:66136ms step_avg:58.01ms
step:1141/2330 train_time:66194ms step_avg:58.01ms
step:1142/2330 train_time:66254ms step_avg:58.02ms
step:1143/2330 train_time:66311ms step_avg:58.01ms
step:1144/2330 train_time:66372ms step_avg:58.02ms
step:1145/2330 train_time:66428ms step_avg:58.02ms
step:1146/2330 train_time:66488ms step_avg:58.02ms
step:1147/2330 train_time:66545ms step_avg:58.02ms
step:1148/2330 train_time:66605ms step_avg:58.02ms
step:1149/2330 train_time:66662ms step_avg:58.02ms
step:1150/2330 train_time:66722ms step_avg:58.02ms
step:1151/2330 train_time:66779ms step_avg:58.02ms
step:1152/2330 train_time:66839ms step_avg:58.02ms
step:1153/2330 train_time:66896ms step_avg:58.02ms
step:1154/2330 train_time:66956ms step_avg:58.02ms
step:1155/2330 train_time:67013ms step_avg:58.02ms
step:1156/2330 train_time:67073ms step_avg:58.02ms
step:1157/2330 train_time:67129ms step_avg:58.02ms
step:1158/2330 train_time:67191ms step_avg:58.02ms
step:1159/2330 train_time:67248ms step_avg:58.02ms
step:1160/2330 train_time:67309ms step_avg:58.02ms
step:1161/2330 train_time:67366ms step_avg:58.02ms
step:1162/2330 train_time:67426ms step_avg:58.03ms
step:1163/2330 train_time:67483ms step_avg:58.03ms
step:1164/2330 train_time:67543ms step_avg:58.03ms
step:1165/2330 train_time:67600ms step_avg:58.03ms
step:1166/2330 train_time:67660ms step_avg:58.03ms
step:1167/2330 train_time:67717ms step_avg:58.03ms
step:1168/2330 train_time:67776ms step_avg:58.03ms
step:1169/2330 train_time:67833ms step_avg:58.03ms
step:1170/2330 train_time:67894ms step_avg:58.03ms
step:1171/2330 train_time:67951ms step_avg:58.03ms
step:1172/2330 train_time:68012ms step_avg:58.03ms
step:1173/2330 train_time:68069ms step_avg:58.03ms
step:1174/2330 train_time:68129ms step_avg:58.03ms
step:1175/2330 train_time:68185ms step_avg:58.03ms
step:1176/2330 train_time:68246ms step_avg:58.03ms
step:1177/2330 train_time:68303ms step_avg:58.03ms
step:1178/2330 train_time:68363ms step_avg:58.03ms
step:1179/2330 train_time:68420ms step_avg:58.03ms
step:1180/2330 train_time:68479ms step_avg:58.03ms
step:1181/2330 train_time:68536ms step_avg:58.03ms
step:1182/2330 train_time:68598ms step_avg:58.04ms
step:1183/2330 train_time:68654ms step_avg:58.03ms
step:1184/2330 train_time:68714ms step_avg:58.04ms
step:1185/2330 train_time:68771ms step_avg:58.03ms
step:1186/2330 train_time:68832ms step_avg:58.04ms
step:1187/2330 train_time:68889ms step_avg:58.04ms
step:1188/2330 train_time:68949ms step_avg:58.04ms
step:1189/2330 train_time:69005ms step_avg:58.04ms
step:1190/2330 train_time:69065ms step_avg:58.04ms
step:1191/2330 train_time:69122ms step_avg:58.04ms
step:1192/2330 train_time:69182ms step_avg:58.04ms
step:1193/2330 train_time:69239ms step_avg:58.04ms
step:1194/2330 train_time:69298ms step_avg:58.04ms
step:1195/2330 train_time:69355ms step_avg:58.04ms
step:1196/2330 train_time:69415ms step_avg:58.04ms
step:1197/2330 train_time:69473ms step_avg:58.04ms
step:1198/2330 train_time:69533ms step_avg:58.04ms
step:1199/2330 train_time:69590ms step_avg:58.04ms
step:1200/2330 train_time:69652ms step_avg:58.04ms
step:1201/2330 train_time:69708ms step_avg:58.04ms
step:1202/2330 train_time:69769ms step_avg:58.04ms
step:1203/2330 train_time:69826ms step_avg:58.04ms
step:1204/2330 train_time:69886ms step_avg:58.05ms
step:1205/2330 train_time:69943ms step_avg:58.04ms
step:1206/2330 train_time:70003ms step_avg:58.05ms
step:1207/2330 train_time:70060ms step_avg:58.04ms
step:1208/2330 train_time:70120ms step_avg:58.05ms
step:1209/2330 train_time:70177ms step_avg:58.05ms
step:1210/2330 train_time:70237ms step_avg:58.05ms
step:1211/2330 train_time:70294ms step_avg:58.05ms
step:1212/2330 train_time:70354ms step_avg:58.05ms
step:1213/2330 train_time:70411ms step_avg:58.05ms
step:1214/2330 train_time:70472ms step_avg:58.05ms
step:1215/2330 train_time:70529ms step_avg:58.05ms
step:1216/2330 train_time:70590ms step_avg:58.05ms
step:1217/2330 train_time:70646ms step_avg:58.05ms
step:1218/2330 train_time:70707ms step_avg:58.05ms
step:1219/2330 train_time:70763ms step_avg:58.05ms
step:1220/2330 train_time:70823ms step_avg:58.05ms
step:1221/2330 train_time:70879ms step_avg:58.05ms
step:1222/2330 train_time:70939ms step_avg:58.05ms
step:1223/2330 train_time:70996ms step_avg:58.05ms
step:1224/2330 train_time:71057ms step_avg:58.05ms
step:1225/2330 train_time:71114ms step_avg:58.05ms
step:1226/2330 train_time:71174ms step_avg:58.05ms
step:1227/2330 train_time:71231ms step_avg:58.05ms
step:1228/2330 train_time:71292ms step_avg:58.06ms
step:1229/2330 train_time:71348ms step_avg:58.05ms
step:1230/2330 train_time:71410ms step_avg:58.06ms
step:1231/2330 train_time:71467ms step_avg:58.06ms
step:1232/2330 train_time:71527ms step_avg:58.06ms
step:1233/2330 train_time:71583ms step_avg:58.06ms
step:1234/2330 train_time:71643ms step_avg:58.06ms
step:1235/2330 train_time:71700ms step_avg:58.06ms
step:1236/2330 train_time:71759ms step_avg:58.06ms
step:1237/2330 train_time:71817ms step_avg:58.06ms
step:1238/2330 train_time:71877ms step_avg:58.06ms
step:1239/2330 train_time:71934ms step_avg:58.06ms
step:1240/2330 train_time:71995ms step_avg:58.06ms
step:1241/2330 train_time:72051ms step_avg:58.06ms
step:1242/2330 train_time:72112ms step_avg:58.06ms
step:1243/2330 train_time:72168ms step_avg:58.06ms
step:1244/2330 train_time:72229ms step_avg:58.06ms
step:1245/2330 train_time:72286ms step_avg:58.06ms
step:1246/2330 train_time:72347ms step_avg:58.06ms
step:1247/2330 train_time:72404ms step_avg:58.06ms
step:1248/2330 train_time:72464ms step_avg:58.06ms
step:1249/2330 train_time:72521ms step_avg:58.06ms
step:1250/2330 train_time:72580ms step_avg:58.06ms
step:1250/2330 val_loss:4.0194 train_time:72661ms step_avg:58.13ms
step:1251/2330 train_time:72680ms step_avg:58.10ms
step:1252/2330 train_time:72700ms step_avg:58.07ms
step:1253/2330 train_time:72760ms step_avg:58.07ms
step:1254/2330 train_time:72827ms step_avg:58.08ms
step:1255/2330 train_time:72885ms step_avg:58.08ms
step:1256/2330 train_time:72946ms step_avg:58.08ms
step:1257/2330 train_time:73002ms step_avg:58.08ms
step:1258/2330 train_time:73061ms step_avg:58.08ms
step:1259/2330 train_time:73118ms step_avg:58.08ms
step:1260/2330 train_time:73177ms step_avg:58.08ms
step:1261/2330 train_time:73234ms step_avg:58.08ms
step:1262/2330 train_time:73293ms step_avg:58.08ms
step:1263/2330 train_time:73349ms step_avg:58.08ms
step:1264/2330 train_time:73408ms step_avg:58.08ms
step:1265/2330 train_time:73464ms step_avg:58.07ms
step:1266/2330 train_time:73523ms step_avg:58.07ms
step:1267/2330 train_time:73579ms step_avg:58.07ms
step:1268/2330 train_time:73640ms step_avg:58.08ms
step:1269/2330 train_time:73699ms step_avg:58.08ms
step:1270/2330 train_time:73762ms step_avg:58.08ms
step:1271/2330 train_time:73819ms step_avg:58.08ms
step:1272/2330 train_time:73882ms step_avg:58.08ms
step:1273/2330 train_time:73939ms step_avg:58.08ms
step:1274/2330 train_time:74000ms step_avg:58.08ms
step:1275/2330 train_time:74056ms step_avg:58.08ms
step:1276/2330 train_time:74116ms step_avg:58.08ms
step:1277/2330 train_time:74173ms step_avg:58.08ms
step:1278/2330 train_time:74233ms step_avg:58.09ms
step:1279/2330 train_time:74289ms step_avg:58.08ms
step:1280/2330 train_time:74349ms step_avg:58.08ms
step:1281/2330 train_time:74405ms step_avg:58.08ms
step:1282/2330 train_time:74464ms step_avg:58.08ms
step:1283/2330 train_time:74522ms step_avg:58.08ms
step:1284/2330 train_time:74581ms step_avg:58.08ms
step:1285/2330 train_time:74637ms step_avg:58.08ms
step:1286/2330 train_time:74699ms step_avg:58.09ms
step:1287/2330 train_time:74756ms step_avg:58.09ms
step:1288/2330 train_time:74820ms step_avg:58.09ms
step:1289/2330 train_time:74878ms step_avg:58.09ms
step:1290/2330 train_time:74939ms step_avg:58.09ms
step:1291/2330 train_time:75477ms step_avg:58.46ms
step:1292/2330 train_time:75498ms step_avg:58.43ms
step:1293/2330 train_time:75538ms step_avg:58.42ms
step:1294/2330 train_time:75647ms step_avg:58.46ms
step:1295/2330 train_time:75703ms step_avg:58.46ms
step:1296/2330 train_time:75761ms step_avg:58.46ms
step:1297/2330 train_time:75817ms step_avg:58.46ms
step:1298/2330 train_time:75877ms step_avg:58.46ms
step:1299/2330 train_time:75933ms step_avg:58.45ms
step:1300/2330 train_time:75992ms step_avg:58.46ms
step:1301/2330 train_time:76048ms step_avg:58.45ms
step:1302/2330 train_time:76107ms step_avg:58.45ms
step:1303/2330 train_time:76163ms step_avg:58.45ms
step:1304/2330 train_time:76222ms step_avg:58.45ms
step:1305/2330 train_time:76278ms step_avg:58.45ms
step:1306/2330 train_time:76339ms step_avg:58.45ms
step:1307/2330 train_time:76395ms step_avg:58.45ms
step:1308/2330 train_time:76453ms step_avg:58.45ms
step:1309/2330 train_time:76514ms step_avg:58.45ms
step:1310/2330 train_time:76580ms step_avg:58.46ms
step:1311/2330 train_time:76639ms step_avg:58.46ms
step:1312/2330 train_time:76700ms step_avg:58.46ms
step:1313/2330 train_time:76756ms step_avg:58.46ms
step:1314/2330 train_time:76817ms step_avg:58.46ms
step:1315/2330 train_time:76873ms step_avg:58.46ms
step:1316/2330 train_time:76933ms step_avg:58.46ms
step:1317/2330 train_time:76989ms step_avg:58.46ms
step:1318/2330 train_time:77049ms step_avg:58.46ms
step:1319/2330 train_time:77105ms step_avg:58.46ms
step:1320/2330 train_time:77164ms step_avg:58.46ms
step:1321/2330 train_time:77220ms step_avg:58.46ms
step:1322/2330 train_time:77280ms step_avg:58.46ms
step:1323/2330 train_time:77336ms step_avg:58.46ms
step:1324/2330 train_time:77396ms step_avg:58.46ms
step:1325/2330 train_time:77454ms step_avg:58.46ms
step:1326/2330 train_time:77515ms step_avg:58.46ms
step:1327/2330 train_time:77573ms step_avg:58.46ms
step:1328/2330 train_time:77635ms step_avg:58.46ms
step:1329/2330 train_time:77693ms step_avg:58.46ms
step:1330/2330 train_time:77752ms step_avg:58.46ms
step:1331/2330 train_time:77810ms step_avg:58.46ms
step:1332/2330 train_time:77870ms step_avg:58.46ms
step:1333/2330 train_time:77926ms step_avg:58.46ms
step:1334/2330 train_time:77986ms step_avg:58.46ms
step:1335/2330 train_time:78043ms step_avg:58.46ms
step:1336/2330 train_time:78102ms step_avg:58.46ms
step:1337/2330 train_time:78157ms step_avg:58.46ms
step:1338/2330 train_time:78217ms step_avg:58.46ms
step:1339/2330 train_time:78273ms step_avg:58.46ms
step:1340/2330 train_time:78332ms step_avg:58.46ms
step:1341/2330 train_time:78390ms step_avg:58.46ms
step:1342/2330 train_time:78450ms step_avg:58.46ms
step:1343/2330 train_time:78507ms step_avg:58.46ms
step:1344/2330 train_time:78568ms step_avg:58.46ms
step:1345/2330 train_time:78626ms step_avg:58.46ms
step:1346/2330 train_time:78686ms step_avg:58.46ms
step:1347/2330 train_time:78743ms step_avg:58.46ms
step:1348/2330 train_time:78805ms step_avg:58.46ms
step:1349/2330 train_time:78860ms step_avg:58.46ms
step:1350/2330 train_time:78921ms step_avg:58.46ms
step:1351/2330 train_time:78978ms step_avg:58.46ms
step:1352/2330 train_time:79038ms step_avg:58.46ms
step:1353/2330 train_time:79095ms step_avg:58.46ms
step:1354/2330 train_time:79154ms step_avg:58.46ms
step:1355/2330 train_time:79211ms step_avg:58.46ms
step:1356/2330 train_time:79270ms step_avg:58.46ms
step:1357/2330 train_time:79327ms step_avg:58.46ms
step:1358/2330 train_time:79387ms step_avg:58.46ms
step:1359/2330 train_time:79444ms step_avg:58.46ms
step:1360/2330 train_time:79504ms step_avg:58.46ms
step:1361/2330 train_time:79560ms step_avg:58.46ms
step:1362/2330 train_time:79622ms step_avg:58.46ms
step:1363/2330 train_time:79679ms step_avg:58.46ms
step:1364/2330 train_time:79740ms step_avg:58.46ms
step:1365/2330 train_time:79796ms step_avg:58.46ms
step:1366/2330 train_time:79859ms step_avg:58.46ms
step:1367/2330 train_time:79915ms step_avg:58.46ms
step:1368/2330 train_time:79977ms step_avg:58.46ms
step:1369/2330 train_time:80033ms step_avg:58.46ms
step:1370/2330 train_time:80094ms step_avg:58.46ms
step:1371/2330 train_time:80150ms step_avg:58.46ms
step:1372/2330 train_time:80210ms step_avg:58.46ms
step:1373/2330 train_time:80266ms step_avg:58.46ms
step:1374/2330 train_time:80326ms step_avg:58.46ms
step:1375/2330 train_time:80382ms step_avg:58.46ms
step:1376/2330 train_time:80443ms step_avg:58.46ms
step:1377/2330 train_time:80500ms step_avg:58.46ms
step:1378/2330 train_time:80560ms step_avg:58.46ms
step:1379/2330 train_time:80617ms step_avg:58.46ms
step:1380/2330 train_time:80678ms step_avg:58.46ms
step:1381/2330 train_time:80735ms step_avg:58.46ms
step:1382/2330 train_time:80796ms step_avg:58.46ms
step:1383/2330 train_time:80853ms step_avg:58.46ms
step:1384/2330 train_time:80914ms step_avg:58.46ms
step:1385/2330 train_time:80971ms step_avg:58.46ms
step:1386/2330 train_time:81030ms step_avg:58.46ms
step:1387/2330 train_time:81087ms step_avg:58.46ms
step:1388/2330 train_time:81146ms step_avg:58.46ms
step:1389/2330 train_time:81202ms step_avg:58.46ms
step:1390/2330 train_time:81262ms step_avg:58.46ms
step:1391/2330 train_time:81319ms step_avg:58.46ms
step:1392/2330 train_time:81379ms step_avg:58.46ms
step:1393/2330 train_time:81437ms step_avg:58.46ms
step:1394/2330 train_time:81497ms step_avg:58.46ms
step:1395/2330 train_time:81553ms step_avg:58.46ms
step:1396/2330 train_time:81615ms step_avg:58.46ms
step:1397/2330 train_time:81671ms step_avg:58.46ms
step:1398/2330 train_time:81731ms step_avg:58.46ms
step:1399/2330 train_time:81788ms step_avg:58.46ms
step:1400/2330 train_time:81848ms step_avg:58.46ms
step:1401/2330 train_time:81905ms step_avg:58.46ms
step:1402/2330 train_time:81966ms step_avg:58.46ms
step:1403/2330 train_time:82023ms step_avg:58.46ms
step:1404/2330 train_time:82083ms step_avg:58.46ms
step:1405/2330 train_time:82140ms step_avg:58.46ms
step:1406/2330 train_time:82201ms step_avg:58.46ms
step:1407/2330 train_time:82257ms step_avg:58.46ms
step:1408/2330 train_time:82317ms step_avg:58.46ms
step:1409/2330 train_time:82374ms step_avg:58.46ms
step:1410/2330 train_time:82435ms step_avg:58.46ms
step:1411/2330 train_time:82493ms step_avg:58.46ms
step:1412/2330 train_time:82552ms step_avg:58.46ms
step:1413/2330 train_time:82609ms step_avg:58.46ms
step:1414/2330 train_time:82669ms step_avg:58.46ms
step:1415/2330 train_time:82726ms step_avg:58.46ms
step:1416/2330 train_time:82786ms step_avg:58.46ms
step:1417/2330 train_time:82842ms step_avg:58.46ms
step:1418/2330 train_time:82902ms step_avg:58.46ms
step:1419/2330 train_time:82959ms step_avg:58.46ms
step:1420/2330 train_time:83019ms step_avg:58.46ms
step:1421/2330 train_time:83076ms step_avg:58.46ms
step:1422/2330 train_time:83137ms step_avg:58.46ms
step:1423/2330 train_time:83194ms step_avg:58.46ms
step:1424/2330 train_time:83253ms step_avg:58.46ms
step:1425/2330 train_time:83310ms step_avg:58.46ms
step:1426/2330 train_time:83370ms step_avg:58.46ms
step:1427/2330 train_time:83427ms step_avg:58.46ms
step:1428/2330 train_time:83486ms step_avg:58.46ms
step:1429/2330 train_time:83543ms step_avg:58.46ms
step:1430/2330 train_time:83605ms step_avg:58.46ms
step:1431/2330 train_time:83661ms step_avg:58.46ms
step:1432/2330 train_time:83723ms step_avg:58.47ms
step:1433/2330 train_time:83780ms step_avg:58.46ms
step:1434/2330 train_time:83840ms step_avg:58.47ms
step:1435/2330 train_time:83897ms step_avg:58.46ms
step:1436/2330 train_time:83957ms step_avg:58.47ms
step:1437/2330 train_time:84014ms step_avg:58.47ms
step:1438/2330 train_time:84075ms step_avg:58.47ms
step:1439/2330 train_time:84132ms step_avg:58.47ms
step:1440/2330 train_time:84192ms step_avg:58.47ms
step:1441/2330 train_time:84250ms step_avg:58.47ms
step:1442/2330 train_time:84310ms step_avg:58.47ms
step:1443/2330 train_time:84367ms step_avg:58.47ms
step:1444/2330 train_time:84426ms step_avg:58.47ms
step:1445/2330 train_time:84484ms step_avg:58.47ms
step:1446/2330 train_time:84544ms step_avg:58.47ms
step:1447/2330 train_time:84601ms step_avg:58.47ms
step:1448/2330 train_time:84660ms step_avg:58.47ms
step:1449/2330 train_time:84717ms step_avg:58.47ms
step:1450/2330 train_time:84778ms step_avg:58.47ms
step:1451/2330 train_time:84835ms step_avg:58.47ms
step:1452/2330 train_time:84895ms step_avg:58.47ms
step:1453/2330 train_time:84952ms step_avg:58.47ms
step:1454/2330 train_time:85013ms step_avg:58.47ms
step:1455/2330 train_time:85070ms step_avg:58.47ms
step:1456/2330 train_time:85129ms step_avg:58.47ms
step:1457/2330 train_time:85185ms step_avg:58.47ms
step:1458/2330 train_time:85246ms step_avg:58.47ms
step:1459/2330 train_time:85303ms step_avg:58.47ms
step:1460/2330 train_time:85363ms step_avg:58.47ms
step:1461/2330 train_time:85419ms step_avg:58.47ms
step:1462/2330 train_time:85481ms step_avg:58.47ms
step:1463/2330 train_time:85538ms step_avg:58.47ms
step:1464/2330 train_time:85599ms step_avg:58.47ms
step:1465/2330 train_time:85656ms step_avg:58.47ms
step:1466/2330 train_time:85718ms step_avg:58.47ms
step:1467/2330 train_time:85775ms step_avg:58.47ms
step:1468/2330 train_time:85834ms step_avg:58.47ms
step:1469/2330 train_time:85892ms step_avg:58.47ms
step:1470/2330 train_time:85951ms step_avg:58.47ms
step:1471/2330 train_time:86009ms step_avg:58.47ms
step:1472/2330 train_time:86068ms step_avg:58.47ms
step:1473/2330 train_time:86125ms step_avg:58.47ms
step:1474/2330 train_time:86185ms step_avg:58.47ms
step:1475/2330 train_time:86242ms step_avg:58.47ms
step:1476/2330 train_time:86302ms step_avg:58.47ms
step:1477/2330 train_time:86358ms step_avg:58.47ms
step:1478/2330 train_time:86419ms step_avg:58.47ms
step:1479/2330 train_time:86476ms step_avg:58.47ms
step:1480/2330 train_time:86537ms step_avg:58.47ms
step:1481/2330 train_time:86593ms step_avg:58.47ms
step:1482/2330 train_time:86653ms step_avg:58.47ms
step:1483/2330 train_time:86710ms step_avg:58.47ms
step:1484/2330 train_time:86770ms step_avg:58.47ms
step:1485/2330 train_time:86826ms step_avg:58.47ms
step:1486/2330 train_time:86887ms step_avg:58.47ms
step:1487/2330 train_time:86944ms step_avg:58.47ms
step:1488/2330 train_time:87005ms step_avg:58.47ms
step:1489/2330 train_time:87061ms step_avg:58.47ms
step:1490/2330 train_time:87122ms step_avg:58.47ms
step:1491/2330 train_time:87179ms step_avg:58.47ms
step:1492/2330 train_time:87239ms step_avg:58.47ms
step:1493/2330 train_time:87296ms step_avg:58.47ms
step:1494/2330 train_time:87357ms step_avg:58.47ms
step:1495/2330 train_time:87414ms step_avg:58.47ms
step:1496/2330 train_time:87475ms step_avg:58.47ms
step:1497/2330 train_time:87531ms step_avg:58.47ms
step:1498/2330 train_time:87592ms step_avg:58.47ms
step:1499/2330 train_time:87650ms step_avg:58.47ms
step:1500/2330 train_time:87710ms step_avg:58.47ms
step:1500/2330 val_loss:3.9337 train_time:87789ms step_avg:58.53ms
step:1501/2330 train_time:87809ms step_avg:58.50ms
step:1502/2330 train_time:87829ms step_avg:58.47ms
step:1503/2330 train_time:87887ms step_avg:58.47ms
step:1504/2330 train_time:87949ms step_avg:58.48ms
step:1505/2330 train_time:88006ms step_avg:58.48ms
step:1506/2330 train_time:88067ms step_avg:58.48ms
step:1507/2330 train_time:88124ms step_avg:58.48ms
step:1508/2330 train_time:88183ms step_avg:58.48ms
step:1509/2330 train_time:88240ms step_avg:58.48ms
step:1510/2330 train_time:88299ms step_avg:58.48ms
step:1511/2330 train_time:88355ms step_avg:58.47ms
step:1512/2330 train_time:88415ms step_avg:58.48ms
step:1513/2330 train_time:88472ms step_avg:58.47ms
step:1514/2330 train_time:88531ms step_avg:58.47ms
step:1515/2330 train_time:88587ms step_avg:58.47ms
step:1516/2330 train_time:88647ms step_avg:58.47ms
step:1517/2330 train_time:88704ms step_avg:58.47ms
step:1518/2330 train_time:88764ms step_avg:58.47ms
step:1519/2330 train_time:88824ms step_avg:58.47ms
step:1520/2330 train_time:88885ms step_avg:58.48ms
step:1521/2330 train_time:88943ms step_avg:58.48ms
step:1522/2330 train_time:89003ms step_avg:58.48ms
step:1523/2330 train_time:89060ms step_avg:58.48ms
step:1524/2330 train_time:89119ms step_avg:58.48ms
step:1525/2330 train_time:89176ms step_avg:58.48ms
step:1526/2330 train_time:89235ms step_avg:58.48ms
step:1527/2330 train_time:89291ms step_avg:58.48ms
step:1528/2330 train_time:89352ms step_avg:58.48ms
step:1529/2330 train_time:89411ms step_avg:58.48ms
step:1530/2330 train_time:89469ms step_avg:58.48ms
step:1531/2330 train_time:89526ms step_avg:58.48ms
step:1532/2330 train_time:89586ms step_avg:58.48ms
step:1533/2330 train_time:89643ms step_avg:58.48ms
step:1534/2330 train_time:89704ms step_avg:58.48ms
step:1535/2330 train_time:89762ms step_avg:58.48ms
step:1536/2330 train_time:89822ms step_avg:58.48ms
step:1537/2330 train_time:89879ms step_avg:58.48ms
step:1538/2330 train_time:89940ms step_avg:58.48ms
step:1539/2330 train_time:89997ms step_avg:58.48ms
step:1540/2330 train_time:90058ms step_avg:58.48ms
step:1541/2330 train_time:90116ms step_avg:58.48ms
step:1542/2330 train_time:90176ms step_avg:58.48ms
step:1543/2330 train_time:90234ms step_avg:58.48ms
step:1544/2330 train_time:90294ms step_avg:58.48ms
step:1545/2330 train_time:90350ms step_avg:58.48ms
step:1546/2330 train_time:90411ms step_avg:58.48ms
step:1547/2330 train_time:90468ms step_avg:58.48ms
step:1548/2330 train_time:90529ms step_avg:58.48ms
step:1549/2330 train_time:90586ms step_avg:58.48ms
step:1550/2330 train_time:90647ms step_avg:58.48ms
step:1551/2330 train_time:90705ms step_avg:58.48ms
step:1552/2330 train_time:90765ms step_avg:58.48ms
step:1553/2330 train_time:90824ms step_avg:58.48ms
step:1554/2330 train_time:90884ms step_avg:58.48ms
step:1555/2330 train_time:90941ms step_avg:58.48ms
step:1556/2330 train_time:91002ms step_avg:58.48ms
step:1557/2330 train_time:91060ms step_avg:58.48ms
step:1558/2330 train_time:91121ms step_avg:58.49ms
step:1559/2330 train_time:91178ms step_avg:58.49ms
step:1560/2330 train_time:91238ms step_avg:58.49ms
step:1561/2330 train_time:91295ms step_avg:58.49ms
step:1562/2330 train_time:91357ms step_avg:58.49ms
step:1563/2330 train_time:91414ms step_avg:58.49ms
step:1564/2330 train_time:91474ms step_avg:58.49ms
step:1565/2330 train_time:91531ms step_avg:58.49ms
step:1566/2330 train_time:91591ms step_avg:58.49ms
step:1567/2330 train_time:91648ms step_avg:58.49ms
step:1568/2330 train_time:91710ms step_avg:58.49ms
step:1569/2330 train_time:91766ms step_avg:58.49ms
step:1570/2330 train_time:91829ms step_avg:58.49ms
step:1571/2330 train_time:91886ms step_avg:58.49ms
step:1572/2330 train_time:91948ms step_avg:58.49ms
step:1573/2330 train_time:92006ms step_avg:58.49ms
step:1574/2330 train_time:92067ms step_avg:58.49ms
step:1575/2330 train_time:92125ms step_avg:58.49ms
step:1576/2330 train_time:92186ms step_avg:58.49ms
step:1577/2330 train_time:92245ms step_avg:58.49ms
step:1578/2330 train_time:92305ms step_avg:58.50ms
step:1579/2330 train_time:92363ms step_avg:58.49ms
step:1580/2330 train_time:92424ms step_avg:58.50ms
step:1581/2330 train_time:92481ms step_avg:58.50ms
step:1582/2330 train_time:92541ms step_avg:58.50ms
step:1583/2330 train_time:92598ms step_avg:58.50ms
step:1584/2330 train_time:92659ms step_avg:58.50ms
step:1585/2330 train_time:92715ms step_avg:58.50ms
step:1586/2330 train_time:92776ms step_avg:58.50ms
step:1587/2330 train_time:92832ms step_avg:58.50ms
step:1588/2330 train_time:92894ms step_avg:58.50ms
step:1589/2330 train_time:92950ms step_avg:58.50ms
step:1590/2330 train_time:93014ms step_avg:58.50ms
step:1591/2330 train_time:93071ms step_avg:58.50ms
step:1592/2330 train_time:93133ms step_avg:58.50ms
step:1593/2330 train_time:93190ms step_avg:58.50ms
step:1594/2330 train_time:93253ms step_avg:58.50ms
step:1595/2330 train_time:93310ms step_avg:58.50ms
step:1596/2330 train_time:93371ms step_avg:58.50ms
step:1597/2330 train_time:93428ms step_avg:58.50ms
step:1598/2330 train_time:93489ms step_avg:58.50ms
step:1599/2330 train_time:93546ms step_avg:58.50ms
step:1600/2330 train_time:93606ms step_avg:58.50ms
step:1601/2330 train_time:93664ms step_avg:58.50ms
step:1602/2330 train_time:93725ms step_avg:58.50ms
step:1603/2330 train_time:93783ms step_avg:58.50ms
step:1604/2330 train_time:93843ms step_avg:58.51ms
step:1605/2330 train_time:93900ms step_avg:58.50ms
step:1606/2330 train_time:93960ms step_avg:58.51ms
step:1607/2330 train_time:94018ms step_avg:58.51ms
step:1608/2330 train_time:94079ms step_avg:58.51ms
step:1609/2330 train_time:94135ms step_avg:58.51ms
step:1610/2330 train_time:94196ms step_avg:58.51ms
step:1611/2330 train_time:94254ms step_avg:58.51ms
step:1612/2330 train_time:94314ms step_avg:58.51ms
step:1613/2330 train_time:94372ms step_avg:58.51ms
step:1614/2330 train_time:94433ms step_avg:58.51ms
step:1615/2330 train_time:94489ms step_avg:58.51ms
step:1616/2330 train_time:94551ms step_avg:58.51ms
step:1617/2330 train_time:94608ms step_avg:58.51ms
step:1618/2330 train_time:94670ms step_avg:58.51ms
step:1619/2330 train_time:94727ms step_avg:58.51ms
step:1620/2330 train_time:94789ms step_avg:58.51ms
step:1621/2330 train_time:94845ms step_avg:58.51ms
step:1622/2330 train_time:94907ms step_avg:58.51ms
step:1623/2330 train_time:94964ms step_avg:58.51ms
step:1624/2330 train_time:95026ms step_avg:58.51ms
step:1625/2330 train_time:95085ms step_avg:58.51ms
step:1626/2330 train_time:95145ms step_avg:58.51ms
step:1627/2330 train_time:95203ms step_avg:58.51ms
step:1628/2330 train_time:95264ms step_avg:58.52ms
step:1629/2330 train_time:95321ms step_avg:58.52ms
step:1630/2330 train_time:95382ms step_avg:58.52ms
step:1631/2330 train_time:95440ms step_avg:58.52ms
step:1632/2330 train_time:95499ms step_avg:58.52ms
step:1633/2330 train_time:95556ms step_avg:58.52ms
step:1634/2330 train_time:95617ms step_avg:58.52ms
step:1635/2330 train_time:95675ms step_avg:58.52ms
step:1636/2330 train_time:95735ms step_avg:58.52ms
step:1637/2330 train_time:95792ms step_avg:58.52ms
step:1638/2330 train_time:95853ms step_avg:58.52ms
step:1639/2330 train_time:95910ms step_avg:58.52ms
step:1640/2330 train_time:95972ms step_avg:58.52ms
step:1641/2330 train_time:96029ms step_avg:58.52ms
step:1642/2330 train_time:96091ms step_avg:58.52ms
step:1643/2330 train_time:96148ms step_avg:58.52ms
step:1644/2330 train_time:96210ms step_avg:58.52ms
step:1645/2330 train_time:96268ms step_avg:58.52ms
step:1646/2330 train_time:96329ms step_avg:58.52ms
step:1647/2330 train_time:96387ms step_avg:58.52ms
step:1648/2330 train_time:96448ms step_avg:58.52ms
step:1649/2330 train_time:96505ms step_avg:58.52ms
step:1650/2330 train_time:96566ms step_avg:58.52ms
step:1651/2330 train_time:96624ms step_avg:58.52ms
step:1652/2330 train_time:96684ms step_avg:58.53ms
step:1653/2330 train_time:96742ms step_avg:58.53ms
step:1654/2330 train_time:96803ms step_avg:58.53ms
step:1655/2330 train_time:96861ms step_avg:58.53ms
step:1656/2330 train_time:96920ms step_avg:58.53ms
step:1657/2330 train_time:96977ms step_avg:58.53ms
step:1658/2330 train_time:97038ms step_avg:58.53ms
step:1659/2330 train_time:97095ms step_avg:58.53ms
step:1660/2330 train_time:97155ms step_avg:58.53ms
step:1661/2330 train_time:97212ms step_avg:58.53ms
step:1662/2330 train_time:97274ms step_avg:58.53ms
step:1663/2330 train_time:97331ms step_avg:58.53ms
step:1664/2330 train_time:97392ms step_avg:58.53ms
step:1665/2330 train_time:97449ms step_avg:58.53ms
step:1666/2330 train_time:97510ms step_avg:58.53ms
step:1667/2330 train_time:97567ms step_avg:58.53ms
step:1668/2330 train_time:97628ms step_avg:58.53ms
step:1669/2330 train_time:97685ms step_avg:58.53ms
step:1670/2330 train_time:97746ms step_avg:58.53ms
step:1671/2330 train_time:97804ms step_avg:58.53ms
step:1672/2330 train_time:97865ms step_avg:58.53ms
step:1673/2330 train_time:97924ms step_avg:58.53ms
step:1674/2330 train_time:97984ms step_avg:58.53ms
step:1675/2330 train_time:98042ms step_avg:58.53ms
step:1676/2330 train_time:98103ms step_avg:58.53ms
step:1677/2330 train_time:98160ms step_avg:58.53ms
step:1678/2330 train_time:98220ms step_avg:58.53ms
step:1679/2330 train_time:98278ms step_avg:58.53ms
step:1680/2330 train_time:98338ms step_avg:58.53ms
step:1681/2330 train_time:98395ms step_avg:58.53ms
step:1682/2330 train_time:98456ms step_avg:58.54ms
step:1683/2330 train_time:98513ms step_avg:58.53ms
step:1684/2330 train_time:98576ms step_avg:58.54ms
step:1685/2330 train_time:98633ms step_avg:58.54ms
step:1686/2330 train_time:98694ms step_avg:58.54ms
step:1687/2330 train_time:98751ms step_avg:58.54ms
step:1688/2330 train_time:98813ms step_avg:58.54ms
step:1689/2330 train_time:98870ms step_avg:58.54ms
step:1690/2330 train_time:98931ms step_avg:58.54ms
step:1691/2330 train_time:98988ms step_avg:58.54ms
step:1692/2330 train_time:99050ms step_avg:58.54ms
step:1693/2330 train_time:99107ms step_avg:58.54ms
step:1694/2330 train_time:99168ms step_avg:58.54ms
step:1695/2330 train_time:99227ms step_avg:58.54ms
step:1696/2330 train_time:99287ms step_avg:58.54ms
step:1697/2330 train_time:99345ms step_avg:58.54ms
step:1698/2330 train_time:99406ms step_avg:58.54ms
step:1699/2330 train_time:99465ms step_avg:58.54ms
step:1700/2330 train_time:99525ms step_avg:58.54ms
step:1701/2330 train_time:99583ms step_avg:58.54ms
step:1702/2330 train_time:99643ms step_avg:58.54ms
step:1703/2330 train_time:99699ms step_avg:58.54ms
step:1704/2330 train_time:99760ms step_avg:58.54ms
step:1705/2330 train_time:99817ms step_avg:58.54ms
step:1706/2330 train_time:99877ms step_avg:58.54ms
step:1707/2330 train_time:99934ms step_avg:58.54ms
step:1708/2330 train_time:99995ms step_avg:58.55ms
step:1709/2330 train_time:100052ms step_avg:58.54ms
step:1710/2330 train_time:100114ms step_avg:58.55ms
step:1711/2330 train_time:100170ms step_avg:58.54ms
step:1712/2330 train_time:100232ms step_avg:58.55ms
step:1713/2330 train_time:100288ms step_avg:58.55ms
step:1714/2330 train_time:100351ms step_avg:58.55ms
step:1715/2330 train_time:100408ms step_avg:58.55ms
step:1716/2330 train_time:100471ms step_avg:58.55ms
step:1717/2330 train_time:100528ms step_avg:58.55ms
step:1718/2330 train_time:100590ms step_avg:58.55ms
step:1719/2330 train_time:100647ms step_avg:58.55ms
step:1720/2330 train_time:100709ms step_avg:58.55ms
step:1721/2330 train_time:100767ms step_avg:58.55ms
step:1722/2330 train_time:100827ms step_avg:58.55ms
step:1723/2330 train_time:100885ms step_avg:58.55ms
step:1724/2330 train_time:100945ms step_avg:58.55ms
step:1725/2330 train_time:101003ms step_avg:58.55ms
step:1726/2330 train_time:101064ms step_avg:58.55ms
step:1727/2330 train_time:101123ms step_avg:58.55ms
step:1728/2330 train_time:101183ms step_avg:58.55ms
step:1729/2330 train_time:101241ms step_avg:58.55ms
step:1730/2330 train_time:101301ms step_avg:58.56ms
step:1731/2330 train_time:101357ms step_avg:58.55ms
step:1732/2330 train_time:101420ms step_avg:58.56ms
step:1733/2330 train_time:101477ms step_avg:58.56ms
step:1734/2330 train_time:101538ms step_avg:58.56ms
step:1735/2330 train_time:101594ms step_avg:58.56ms
step:1736/2330 train_time:101657ms step_avg:58.56ms
step:1737/2330 train_time:101714ms step_avg:58.56ms
step:1738/2330 train_time:101777ms step_avg:58.56ms
step:1739/2330 train_time:101833ms step_avg:58.56ms
step:1740/2330 train_time:101895ms step_avg:58.56ms
step:1741/2330 train_time:101951ms step_avg:58.56ms
step:1742/2330 train_time:102013ms step_avg:58.56ms
step:1743/2330 train_time:102071ms step_avg:58.56ms
step:1744/2330 train_time:102132ms step_avg:58.56ms
step:1745/2330 train_time:102189ms step_avg:58.56ms
step:1746/2330 train_time:102251ms step_avg:58.56ms
step:1747/2330 train_time:102308ms step_avg:58.56ms
step:1748/2330 train_time:102371ms step_avg:58.56ms
step:1749/2330 train_time:102428ms step_avg:58.56ms
step:1750/2330 train_time:102489ms step_avg:58.57ms
step:1750/2330 val_loss:3.8468 train_time:102571ms step_avg:58.61ms
step:1751/2330 train_time:102590ms step_avg:58.59ms
step:1752/2330 train_time:102610ms step_avg:58.57ms
step:1753/2330 train_time:102668ms step_avg:58.57ms
step:1754/2330 train_time:102737ms step_avg:58.57ms
step:1755/2330 train_time:102796ms step_avg:58.57ms
step:1756/2330 train_time:102857ms step_avg:58.57ms
step:1757/2330 train_time:102915ms step_avg:58.57ms
step:1758/2330 train_time:102975ms step_avg:58.58ms
step:1759/2330 train_time:103032ms step_avg:58.57ms
step:1760/2330 train_time:103092ms step_avg:58.58ms
step:1761/2330 train_time:103149ms step_avg:58.57ms
step:1762/2330 train_time:103208ms step_avg:58.57ms
step:1763/2330 train_time:103264ms step_avg:58.57ms
step:1764/2330 train_time:103325ms step_avg:58.57ms
step:1765/2330 train_time:103381ms step_avg:58.57ms
step:1766/2330 train_time:103441ms step_avg:58.57ms
step:1767/2330 train_time:103498ms step_avg:58.57ms
step:1768/2330 train_time:103562ms step_avg:58.58ms
step:1769/2330 train_time:103622ms step_avg:58.58ms
step:1770/2330 train_time:103683ms step_avg:58.58ms
step:1771/2330 train_time:103741ms step_avg:58.58ms
step:1772/2330 train_time:103802ms step_avg:58.58ms
step:1773/2330 train_time:103860ms step_avg:58.58ms
step:1774/2330 train_time:103921ms step_avg:58.58ms
step:1775/2330 train_time:103979ms step_avg:58.58ms
step:1776/2330 train_time:104039ms step_avg:58.58ms
step:1777/2330 train_time:104097ms step_avg:58.58ms
step:1778/2330 train_time:104157ms step_avg:58.58ms
step:1779/2330 train_time:104214ms step_avg:58.58ms
step:1780/2330 train_time:104274ms step_avg:58.58ms
step:1781/2330 train_time:104331ms step_avg:58.58ms
step:1782/2330 train_time:104390ms step_avg:58.58ms
step:1783/2330 train_time:104446ms step_avg:58.58ms
step:1784/2330 train_time:104507ms step_avg:58.58ms
step:1785/2330 train_time:104563ms step_avg:58.58ms
step:1786/2330 train_time:104627ms step_avg:58.58ms
step:1787/2330 train_time:104684ms step_avg:58.58ms
step:1788/2330 train_time:104748ms step_avg:58.58ms
step:1789/2330 train_time:104806ms step_avg:58.58ms
step:1790/2330 train_time:104867ms step_avg:58.59ms
step:1791/2330 train_time:104924ms step_avg:58.58ms
step:1792/2330 train_time:104987ms step_avg:58.59ms
step:1793/2330 train_time:105046ms step_avg:58.59ms
step:1794/2330 train_time:105106ms step_avg:58.59ms
step:1795/2330 train_time:105162ms step_avg:58.59ms
step:1796/2330 train_time:105224ms step_avg:58.59ms
step:1797/2330 train_time:105281ms step_avg:58.59ms
step:1798/2330 train_time:105342ms step_avg:58.59ms
step:1799/2330 train_time:105399ms step_avg:58.59ms
step:1800/2330 train_time:105459ms step_avg:58.59ms
step:1801/2330 train_time:105517ms step_avg:58.59ms
step:1802/2330 train_time:105577ms step_avg:58.59ms
step:1803/2330 train_time:105637ms step_avg:58.59ms
step:1804/2330 train_time:105697ms step_avg:58.59ms
step:1805/2330 train_time:105756ms step_avg:58.59ms
step:1806/2330 train_time:105817ms step_avg:58.59ms
step:1807/2330 train_time:105874ms step_avg:58.59ms
step:1808/2330 train_time:105934ms step_avg:58.59ms
step:1809/2330 train_time:105992ms step_avg:58.59ms
step:1810/2330 train_time:106053ms step_avg:58.59ms
step:1811/2330 train_time:106110ms step_avg:58.59ms
step:1812/2330 train_time:106170ms step_avg:58.59ms
step:1813/2330 train_time:106227ms step_avg:58.59ms
step:1814/2330 train_time:106288ms step_avg:58.59ms
step:1815/2330 train_time:106345ms step_avg:58.59ms
step:1816/2330 train_time:106406ms step_avg:58.59ms
step:1817/2330 train_time:106463ms step_avg:58.59ms
step:1818/2330 train_time:106524ms step_avg:58.59ms
step:1819/2330 train_time:106581ms step_avg:58.59ms
step:1820/2330 train_time:106644ms step_avg:58.60ms
step:1821/2330 train_time:106702ms step_avg:58.60ms
step:1822/2330 train_time:106763ms step_avg:58.60ms
step:1823/2330 train_time:106820ms step_avg:58.60ms
step:1824/2330 train_time:106881ms step_avg:58.60ms
step:1825/2330 train_time:106939ms step_avg:58.60ms
step:1826/2330 train_time:107000ms step_avg:58.60ms
step:1827/2330 train_time:107058ms step_avg:58.60ms
step:1828/2330 train_time:107119ms step_avg:58.60ms
step:1829/2330 train_time:107177ms step_avg:58.60ms
step:1830/2330 train_time:107237ms step_avg:58.60ms
step:1831/2330 train_time:107295ms step_avg:58.60ms
step:1832/2330 train_time:107354ms step_avg:58.60ms
step:1833/2330 train_time:107411ms step_avg:58.60ms
step:1834/2330 train_time:107471ms step_avg:58.60ms
step:1835/2330 train_time:107528ms step_avg:58.60ms
step:1836/2330 train_time:107590ms step_avg:58.60ms
step:1837/2330 train_time:107647ms step_avg:58.60ms
step:1838/2330 train_time:107708ms step_avg:58.60ms
step:1839/2330 train_time:107765ms step_avg:58.60ms
step:1840/2330 train_time:107827ms step_avg:58.60ms
step:1841/2330 train_time:107885ms step_avg:58.60ms
step:1842/2330 train_time:107947ms step_avg:58.60ms
step:1843/2330 train_time:108004ms step_avg:58.60ms
step:1844/2330 train_time:108066ms step_avg:58.60ms
step:1845/2330 train_time:108123ms step_avg:58.60ms
step:1846/2330 train_time:108185ms step_avg:58.61ms
step:1847/2330 train_time:108243ms step_avg:58.60ms
step:1848/2330 train_time:108303ms step_avg:58.61ms
step:1849/2330 train_time:108360ms step_avg:58.60ms
step:1850/2330 train_time:108420ms step_avg:58.61ms
step:1851/2330 train_time:108479ms step_avg:58.61ms
step:1852/2330 train_time:108539ms step_avg:58.61ms
step:1853/2330 train_time:108598ms step_avg:58.61ms
step:1854/2330 train_time:108658ms step_avg:58.61ms
step:1855/2330 train_time:108716ms step_avg:58.61ms
step:1856/2330 train_time:108776ms step_avg:58.61ms
step:1857/2330 train_time:108834ms step_avg:58.61ms
step:1858/2330 train_time:108894ms step_avg:58.61ms
step:1859/2330 train_time:108952ms step_avg:58.61ms
step:1860/2330 train_time:109012ms step_avg:58.61ms
step:1861/2330 train_time:109069ms step_avg:58.61ms
step:1862/2330 train_time:109130ms step_avg:58.61ms
step:1863/2330 train_time:109187ms step_avg:58.61ms
step:1864/2330 train_time:109249ms step_avg:58.61ms
step:1865/2330 train_time:109306ms step_avg:58.61ms
step:1866/2330 train_time:109367ms step_avg:58.61ms
step:1867/2330 train_time:109424ms step_avg:58.61ms
step:1868/2330 train_time:109486ms step_avg:58.61ms
step:1869/2330 train_time:109543ms step_avg:58.61ms
step:1870/2330 train_time:109605ms step_avg:58.61ms
step:1871/2330 train_time:109662ms step_avg:58.61ms
step:1872/2330 train_time:109724ms step_avg:58.61ms
step:1873/2330 train_time:109781ms step_avg:58.61ms
step:1874/2330 train_time:109843ms step_avg:58.61ms
step:1875/2330 train_time:109901ms step_avg:58.61ms
step:1876/2330 train_time:109961ms step_avg:58.61ms
step:1877/2330 train_time:110019ms step_avg:58.61ms
step:1878/2330 train_time:110080ms step_avg:58.62ms
step:1879/2330 train_time:110138ms step_avg:58.62ms
step:1880/2330 train_time:110198ms step_avg:58.62ms
step:1881/2330 train_time:110256ms step_avg:58.62ms
step:1882/2330 train_time:110317ms step_avg:58.62ms
step:1883/2330 train_time:110374ms step_avg:58.62ms
step:1884/2330 train_time:110434ms step_avg:58.62ms
step:1885/2330 train_time:110491ms step_avg:58.62ms
step:1886/2330 train_time:110551ms step_avg:58.62ms
step:1887/2330 train_time:110608ms step_avg:58.62ms
step:1888/2330 train_time:110669ms step_avg:58.62ms
step:1889/2330 train_time:110727ms step_avg:58.62ms
step:1890/2330 train_time:110788ms step_avg:58.62ms
step:1891/2330 train_time:110845ms step_avg:58.62ms
step:1892/2330 train_time:110907ms step_avg:58.62ms
step:1893/2330 train_time:110964ms step_avg:58.62ms
step:1894/2330 train_time:111026ms step_avg:58.62ms
step:1895/2330 train_time:111083ms step_avg:58.62ms
step:1896/2330 train_time:111146ms step_avg:58.62ms
step:1897/2330 train_time:111203ms step_avg:58.62ms
step:1898/2330 train_time:111265ms step_avg:58.62ms
step:1899/2330 train_time:111322ms step_avg:58.62ms
step:1900/2330 train_time:111384ms step_avg:58.62ms
step:1901/2330 train_time:111443ms step_avg:58.62ms
step:1902/2330 train_time:111503ms step_avg:58.62ms
step:1903/2330 train_time:111560ms step_avg:58.62ms
step:1904/2330 train_time:111620ms step_avg:58.62ms
step:1905/2330 train_time:111678ms step_avg:58.62ms
step:1906/2330 train_time:111739ms step_avg:58.62ms
step:1907/2330 train_time:111797ms step_avg:58.62ms
step:1908/2330 train_time:111857ms step_avg:58.63ms
step:1909/2330 train_time:111915ms step_avg:58.62ms
step:1910/2330 train_time:111975ms step_avg:58.63ms
step:1911/2330 train_time:112033ms step_avg:58.63ms
step:1912/2330 train_time:112093ms step_avg:58.63ms
step:1913/2330 train_time:112150ms step_avg:58.62ms
step:1914/2330 train_time:112211ms step_avg:58.63ms
step:1915/2330 train_time:112268ms step_avg:58.63ms
step:1916/2330 train_time:112329ms step_avg:58.63ms
step:1917/2330 train_time:112387ms step_avg:58.63ms
step:1918/2330 train_time:112447ms step_avg:58.63ms
step:1919/2330 train_time:112504ms step_avg:58.63ms
step:1920/2330 train_time:112565ms step_avg:58.63ms
step:1921/2330 train_time:112623ms step_avg:58.63ms
step:1922/2330 train_time:112684ms step_avg:58.63ms
step:1923/2330 train_time:112741ms step_avg:58.63ms
step:1924/2330 train_time:112803ms step_avg:58.63ms
step:1925/2330 train_time:112860ms step_avg:58.63ms
step:1926/2330 train_time:112921ms step_avg:58.63ms
step:1927/2330 train_time:112979ms step_avg:58.63ms
step:1928/2330 train_time:113039ms step_avg:58.63ms
step:1929/2330 train_time:113098ms step_avg:58.63ms
step:1930/2330 train_time:113158ms step_avg:58.63ms
step:1931/2330 train_time:113216ms step_avg:58.63ms
step:1932/2330 train_time:113277ms step_avg:58.63ms
step:1933/2330 train_time:113336ms step_avg:58.63ms
step:1934/2330 train_time:113396ms step_avg:58.63ms
step:1935/2330 train_time:113454ms step_avg:58.63ms
step:1936/2330 train_time:113513ms step_avg:58.63ms
step:1937/2330 train_time:113570ms step_avg:58.63ms
step:1938/2330 train_time:113631ms step_avg:58.63ms
step:1939/2330 train_time:113688ms step_avg:58.63ms
step:1940/2330 train_time:113750ms step_avg:58.63ms
step:1941/2330 train_time:113807ms step_avg:58.63ms
step:1942/2330 train_time:113868ms step_avg:58.63ms
step:1943/2330 train_time:113924ms step_avg:58.63ms
step:1944/2330 train_time:113988ms step_avg:58.64ms
step:1945/2330 train_time:114044ms step_avg:58.63ms
step:1946/2330 train_time:114106ms step_avg:58.64ms
step:1947/2330 train_time:114163ms step_avg:58.64ms
step:1948/2330 train_time:114226ms step_avg:58.64ms
step:1949/2330 train_time:114283ms step_avg:58.64ms
step:1950/2330 train_time:114345ms step_avg:58.64ms
step:1951/2330 train_time:114402ms step_avg:58.64ms
step:1952/2330 train_time:114463ms step_avg:58.64ms
step:1953/2330 train_time:114521ms step_avg:58.64ms
step:1954/2330 train_time:114581ms step_avg:58.64ms
step:1955/2330 train_time:114640ms step_avg:58.64ms
step:1956/2330 train_time:114700ms step_avg:58.64ms
step:1957/2330 train_time:114758ms step_avg:58.64ms
step:1958/2330 train_time:114818ms step_avg:58.64ms
step:1959/2330 train_time:114875ms step_avg:58.64ms
step:1960/2330 train_time:114935ms step_avg:58.64ms
step:1961/2330 train_time:114993ms step_avg:58.64ms
step:1962/2330 train_time:115053ms step_avg:58.64ms
step:1963/2330 train_time:115109ms step_avg:58.64ms
step:1964/2330 train_time:115171ms step_avg:58.64ms
step:1965/2330 train_time:115228ms step_avg:58.64ms
step:1966/2330 train_time:115290ms step_avg:58.64ms
step:1967/2330 train_time:115347ms step_avg:58.64ms
step:1968/2330 train_time:115408ms step_avg:58.64ms
step:1969/2330 train_time:115465ms step_avg:58.64ms
step:1970/2330 train_time:115526ms step_avg:58.64ms
step:1971/2330 train_time:115584ms step_avg:58.64ms
step:1972/2330 train_time:115646ms step_avg:58.64ms
step:1973/2330 train_time:115703ms step_avg:58.64ms
step:1974/2330 train_time:115763ms step_avg:58.64ms
step:1975/2330 train_time:115821ms step_avg:58.64ms
step:1976/2330 train_time:115882ms step_avg:58.64ms
step:1977/2330 train_time:115940ms step_avg:58.64ms
step:1978/2330 train_time:116000ms step_avg:58.65ms
step:1979/2330 train_time:116058ms step_avg:58.64ms
step:1980/2330 train_time:116119ms step_avg:58.65ms
step:1981/2330 train_time:116177ms step_avg:58.65ms
step:1982/2330 train_time:116237ms step_avg:58.65ms
step:1983/2330 train_time:116295ms step_avg:58.65ms
step:1984/2330 train_time:116355ms step_avg:58.65ms
step:1985/2330 train_time:116413ms step_avg:58.65ms
step:1986/2330 train_time:116473ms step_avg:58.65ms
step:1987/2330 train_time:116530ms step_avg:58.65ms
step:1988/2330 train_time:116591ms step_avg:58.65ms
step:1989/2330 train_time:116647ms step_avg:58.65ms
step:1990/2330 train_time:116709ms step_avg:58.65ms
step:1991/2330 train_time:116766ms step_avg:58.65ms
step:1992/2330 train_time:116829ms step_avg:58.65ms
step:1993/2330 train_time:116886ms step_avg:58.65ms
step:1994/2330 train_time:116947ms step_avg:58.65ms
step:1995/2330 train_time:117004ms step_avg:58.65ms
step:1996/2330 train_time:117066ms step_avg:58.65ms
step:1997/2330 train_time:117123ms step_avg:58.65ms
step:1998/2330 train_time:117185ms step_avg:58.65ms
step:1999/2330 train_time:117242ms step_avg:58.65ms
step:2000/2330 train_time:117304ms step_avg:58.65ms
step:2000/2330 val_loss:3.7818 train_time:117386ms step_avg:58.69ms
step:2001/2330 train_time:117404ms step_avg:58.67ms
step:2002/2330 train_time:117424ms step_avg:58.65ms
step:2003/2330 train_time:117482ms step_avg:58.65ms
step:2004/2330 train_time:117552ms step_avg:58.66ms
step:2005/2330 train_time:117609ms step_avg:58.66ms
step:2006/2330 train_time:117672ms step_avg:58.66ms
step:2007/2330 train_time:117728ms step_avg:58.66ms
step:2008/2330 train_time:117788ms step_avg:58.66ms
step:2009/2330 train_time:117844ms step_avg:58.66ms
step:2010/2330 train_time:117906ms step_avg:58.66ms
step:2011/2330 train_time:117963ms step_avg:58.66ms
step:2012/2330 train_time:118024ms step_avg:58.66ms
step:2013/2330 train_time:118080ms step_avg:58.66ms
step:2014/2330 train_time:118140ms step_avg:58.66ms
step:2015/2330 train_time:118196ms step_avg:58.66ms
step:2016/2330 train_time:118257ms step_avg:58.66ms
step:2017/2330 train_time:118315ms step_avg:58.66ms
step:2018/2330 train_time:118375ms step_avg:58.66ms
step:2019/2330 train_time:118436ms step_avg:58.66ms
step:2020/2330 train_time:118498ms step_avg:58.66ms
step:2021/2330 train_time:118557ms step_avg:58.66ms
step:2022/2330 train_time:118619ms step_avg:58.66ms
step:2023/2330 train_time:118677ms step_avg:58.66ms
step:2024/2330 train_time:118738ms step_avg:58.67ms
step:2025/2330 train_time:118796ms step_avg:58.66ms
step:2026/2330 train_time:118857ms step_avg:58.67ms
step:2027/2330 train_time:118914ms step_avg:58.66ms
step:2028/2330 train_time:118973ms step_avg:58.67ms
step:2029/2330 train_time:119031ms step_avg:58.66ms
step:2030/2330 train_time:119090ms step_avg:58.67ms
step:2031/2330 train_time:119147ms step_avg:58.66ms
step:2032/2330 train_time:119207ms step_avg:58.66ms
step:2033/2330 train_time:119264ms step_avg:58.66ms
step:2034/2330 train_time:119326ms step_avg:58.67ms
step:2035/2330 train_time:119383ms step_avg:58.66ms
step:2036/2330 train_time:119446ms step_avg:58.67ms
step:2037/2330 train_time:119504ms step_avg:58.67ms
step:2038/2330 train_time:119566ms step_avg:58.67ms
step:2039/2330 train_time:119623ms step_avg:58.67ms
step:2040/2330 train_time:119685ms step_avg:58.67ms
step:2041/2330 train_time:119743ms step_avg:58.67ms
step:2042/2330 train_time:119806ms step_avg:58.67ms
step:2043/2330 train_time:119862ms step_avg:58.67ms
step:2044/2330 train_time:119923ms step_avg:58.67ms
step:2045/2330 train_time:119980ms step_avg:58.67ms
step:2046/2330 train_time:120041ms step_avg:58.67ms
step:2047/2330 train_time:120099ms step_avg:58.67ms
step:2048/2330 train_time:120159ms step_avg:58.67ms
step:2049/2330 train_time:120216ms step_avg:58.67ms
step:2050/2330 train_time:120276ms step_avg:58.67ms
step:2051/2330 train_time:120334ms step_avg:58.67ms
step:2052/2330 train_time:120394ms step_avg:58.67ms
step:2053/2330 train_time:120454ms step_avg:58.67ms
step:2054/2330 train_time:120514ms step_avg:58.67ms
step:2055/2330 train_time:120572ms step_avg:58.67ms
step:2056/2330 train_time:120633ms step_avg:58.67ms
step:2057/2330 train_time:120690ms step_avg:58.67ms
step:2058/2330 train_time:120751ms step_avg:58.67ms
step:2059/2330 train_time:120808ms step_avg:58.67ms
step:2060/2330 train_time:120868ms step_avg:58.67ms
step:2061/2330 train_time:120926ms step_avg:58.67ms
step:2062/2330 train_time:120986ms step_avg:58.67ms
step:2063/2330 train_time:121042ms step_avg:58.67ms
step:2064/2330 train_time:121104ms step_avg:58.67ms
step:2065/2330 train_time:121161ms step_avg:58.67ms
step:2066/2330 train_time:121222ms step_avg:58.67ms
step:2067/2330 train_time:121278ms step_avg:58.67ms
step:2068/2330 train_time:121342ms step_avg:58.68ms
step:2069/2330 train_time:121400ms step_avg:58.68ms
step:2070/2330 train_time:121464ms step_avg:58.68ms
step:2071/2330 train_time:121520ms step_avg:58.68ms
step:2072/2330 train_time:121584ms step_avg:58.68ms
step:2073/2330 train_time:121641ms step_avg:58.68ms
step:2074/2330 train_time:121704ms step_avg:58.68ms
step:2075/2330 train_time:121762ms step_avg:58.68ms
step:2076/2330 train_time:121822ms step_avg:58.68ms
step:2077/2330 train_time:121879ms step_avg:58.68ms
step:2078/2330 train_time:121940ms step_avg:58.68ms
step:2079/2330 train_time:121997ms step_avg:58.68ms
step:2080/2330 train_time:122057ms step_avg:58.68ms
step:2081/2330 train_time:122115ms step_avg:58.68ms
step:2082/2330 train_time:122175ms step_avg:58.68ms
step:2083/2330 train_time:122233ms step_avg:58.68ms
step:2084/2330 train_time:122294ms step_avg:58.68ms
step:2085/2330 train_time:122352ms step_avg:58.68ms
step:2086/2330 train_time:122413ms step_avg:58.68ms
step:2087/2330 train_time:122470ms step_avg:58.68ms
step:2088/2330 train_time:122530ms step_avg:58.68ms
step:2089/2330 train_time:122588ms step_avg:58.68ms
step:2090/2330 train_time:122648ms step_avg:58.68ms
step:2091/2330 train_time:122706ms step_avg:58.68ms
step:2092/2330 train_time:122766ms step_avg:58.68ms
step:2093/2330 train_time:122823ms step_avg:58.68ms
step:2094/2330 train_time:122885ms step_avg:58.68ms
step:2095/2330 train_time:122941ms step_avg:58.68ms
step:2096/2330 train_time:123004ms step_avg:58.69ms
step:2097/2330 train_time:123061ms step_avg:58.68ms
step:2098/2330 train_time:123123ms step_avg:58.69ms
step:2099/2330 train_time:123179ms step_avg:58.68ms
step:2100/2330 train_time:123241ms step_avg:58.69ms
step:2101/2330 train_time:123298ms step_avg:58.69ms
step:2102/2330 train_time:123360ms step_avg:58.69ms
step:2103/2330 train_time:123418ms step_avg:58.69ms
step:2104/2330 train_time:123479ms step_avg:58.69ms
step:2105/2330 train_time:123536ms step_avg:58.69ms
step:2106/2330 train_time:123597ms step_avg:58.69ms
step:2107/2330 train_time:123656ms step_avg:58.69ms
step:2108/2330 train_time:123717ms step_avg:58.69ms
step:2109/2330 train_time:123774ms step_avg:58.69ms
step:2110/2330 train_time:123835ms step_avg:58.69ms
step:2111/2330 train_time:123893ms step_avg:58.69ms
step:2112/2330 train_time:123953ms step_avg:58.69ms
step:2113/2330 train_time:124012ms step_avg:58.69ms
step:2114/2330 train_time:124073ms step_avg:58.69ms
step:2115/2330 train_time:124131ms step_avg:58.69ms
step:2116/2330 train_time:124191ms step_avg:58.69ms
step:2117/2330 train_time:124248ms step_avg:58.69ms
step:2118/2330 train_time:124309ms step_avg:58.69ms
step:2119/2330 train_time:124365ms step_avg:58.69ms
step:2120/2330 train_time:124427ms step_avg:58.69ms
step:2121/2330 train_time:124484ms step_avg:58.69ms
step:2122/2330 train_time:124547ms step_avg:58.69ms
step:2123/2330 train_time:124605ms step_avg:58.69ms
step:2124/2330 train_time:124667ms step_avg:58.69ms
step:2125/2330 train_time:124724ms step_avg:58.69ms
step:2126/2330 train_time:124784ms step_avg:58.69ms
step:2127/2330 train_time:124841ms step_avg:58.69ms
step:2128/2330 train_time:124903ms step_avg:58.70ms
step:2129/2330 train_time:124960ms step_avg:58.69ms
step:2130/2330 train_time:125022ms step_avg:58.70ms
step:2131/2330 train_time:125079ms step_avg:58.69ms
step:2132/2330 train_time:125140ms step_avg:58.70ms
step:2133/2330 train_time:125197ms step_avg:58.70ms
step:2134/2330 train_time:125258ms step_avg:58.70ms
step:2135/2330 train_time:125316ms step_avg:58.70ms
step:2136/2330 train_time:125376ms step_avg:58.70ms
step:2137/2330 train_time:125434ms step_avg:58.70ms
step:2138/2330 train_time:125494ms step_avg:58.70ms
step:2139/2330 train_time:125552ms step_avg:58.70ms
step:2140/2330 train_time:125612ms step_avg:58.70ms
step:2141/2330 train_time:125671ms step_avg:58.70ms
step:2142/2330 train_time:125730ms step_avg:58.70ms
step:2143/2330 train_time:125787ms step_avg:58.70ms
step:2144/2330 train_time:125848ms step_avg:58.70ms
step:2145/2330 train_time:125904ms step_avg:58.70ms
step:2146/2330 train_time:125966ms step_avg:58.70ms
step:2147/2330 train_time:126023ms step_avg:58.70ms
step:2148/2330 train_time:126083ms step_avg:58.70ms
step:2149/2330 train_time:126140ms step_avg:58.70ms
step:2150/2330 train_time:126202ms step_avg:58.70ms
step:2151/2330 train_time:126260ms step_avg:58.70ms
step:2152/2330 train_time:126320ms step_avg:58.70ms
step:2153/2330 train_time:126378ms step_avg:58.70ms
step:2154/2330 train_time:126439ms step_avg:58.70ms
step:2155/2330 train_time:126497ms step_avg:58.70ms
step:2156/2330 train_time:126558ms step_avg:58.70ms
step:2157/2330 train_time:126616ms step_avg:58.70ms
step:2158/2330 train_time:126677ms step_avg:58.70ms
step:2159/2330 train_time:126735ms step_avg:58.70ms
step:2160/2330 train_time:126796ms step_avg:58.70ms
step:2161/2330 train_time:126853ms step_avg:58.70ms
step:2162/2330 train_time:126913ms step_avg:58.70ms
step:2163/2330 train_time:126971ms step_avg:58.70ms
step:2164/2330 train_time:127032ms step_avg:58.70ms
step:2165/2330 train_time:127089ms step_avg:58.70ms
step:2166/2330 train_time:127151ms step_avg:58.70ms
step:2167/2330 train_time:127209ms step_avg:58.70ms
step:2168/2330 train_time:127269ms step_avg:58.70ms
step:2169/2330 train_time:127325ms step_avg:58.70ms
step:2170/2330 train_time:127387ms step_avg:58.70ms
step:2171/2330 train_time:127444ms step_avg:58.70ms
step:2172/2330 train_time:127506ms step_avg:58.70ms
step:2173/2330 train_time:127563ms step_avg:58.70ms
step:2174/2330 train_time:127626ms step_avg:58.71ms
step:2175/2330 train_time:127682ms step_avg:58.70ms
step:2176/2330 train_time:127744ms step_avg:58.71ms
step:2177/2330 train_time:127801ms step_avg:58.71ms
step:2178/2330 train_time:127863ms step_avg:58.71ms
step:2179/2330 train_time:127920ms step_avg:58.71ms
step:2180/2330 train_time:127982ms step_avg:58.71ms
step:2181/2330 train_time:128040ms step_avg:58.71ms
step:2182/2330 train_time:128101ms step_avg:58.71ms
step:2183/2330 train_time:128159ms step_avg:58.71ms
step:2184/2330 train_time:128219ms step_avg:58.71ms
step:2185/2330 train_time:128277ms step_avg:58.71ms
step:2186/2330 train_time:128337ms step_avg:58.71ms
step:2187/2330 train_time:128395ms step_avg:58.71ms
step:2188/2330 train_time:128456ms step_avg:58.71ms
step:2189/2330 train_time:128514ms step_avg:58.71ms
step:2190/2330 train_time:128574ms step_avg:58.71ms
step:2191/2330 train_time:128633ms step_avg:58.71ms
step:2192/2330 train_time:128693ms step_avg:58.71ms
step:2193/2330 train_time:128750ms step_avg:58.71ms
step:2194/2330 train_time:128810ms step_avg:58.71ms
step:2195/2330 train_time:128868ms step_avg:58.71ms
step:2196/2330 train_time:128928ms step_avg:58.71ms
step:2197/2330 train_time:128985ms step_avg:58.71ms
step:2198/2330 train_time:129047ms step_avg:58.71ms
step:2199/2330 train_time:129104ms step_avg:58.71ms
step:2200/2330 train_time:129167ms step_avg:58.71ms
step:2201/2330 train_time:129224ms step_avg:58.71ms
step:2202/2330 train_time:129286ms step_avg:58.71ms
step:2203/2330 train_time:129343ms step_avg:58.71ms
step:2204/2330 train_time:129405ms step_avg:58.71ms
step:2205/2330 train_time:129461ms step_avg:58.71ms
step:2206/2330 train_time:129524ms step_avg:58.71ms
step:2207/2330 train_time:129580ms step_avg:58.71ms
step:2208/2330 train_time:129643ms step_avg:58.72ms
step:2209/2330 train_time:129700ms step_avg:58.71ms
step:2210/2330 train_time:129763ms step_avg:58.72ms
step:2211/2330 train_time:129820ms step_avg:58.72ms
step:2212/2330 train_time:129881ms step_avg:58.72ms
step:2213/2330 train_time:129938ms step_avg:58.72ms
step:2214/2330 train_time:130000ms step_avg:58.72ms
step:2215/2330 train_time:130059ms step_avg:58.72ms
step:2216/2330 train_time:130119ms step_avg:58.72ms
step:2217/2330 train_time:130176ms step_avg:58.72ms
step:2218/2330 train_time:130238ms step_avg:58.72ms
step:2219/2330 train_time:130295ms step_avg:58.72ms
step:2220/2330 train_time:130355ms step_avg:58.72ms
step:2221/2330 train_time:130413ms step_avg:58.72ms
step:2222/2330 train_time:130473ms step_avg:58.72ms
step:2223/2330 train_time:130530ms step_avg:58.72ms
step:2224/2330 train_time:130590ms step_avg:58.72ms
step:2225/2330 train_time:130647ms step_avg:58.72ms
step:2226/2330 train_time:130708ms step_avg:58.72ms
step:2227/2330 train_time:130765ms step_avg:58.72ms
step:2228/2330 train_time:130826ms step_avg:58.72ms
step:2229/2330 train_time:130883ms step_avg:58.72ms
step:2230/2330 train_time:130945ms step_avg:58.72ms
step:2231/2330 train_time:131003ms step_avg:58.72ms
step:2232/2330 train_time:131063ms step_avg:58.72ms
step:2233/2330 train_time:131121ms step_avg:58.72ms
step:2234/2330 train_time:131182ms step_avg:58.72ms
step:2235/2330 train_time:131239ms step_avg:58.72ms
step:2236/2330 train_time:131302ms step_avg:58.72ms
step:2237/2330 train_time:131358ms step_avg:58.72ms
step:2238/2330 train_time:131421ms step_avg:58.72ms
step:2239/2330 train_time:131479ms step_avg:58.72ms
step:2240/2330 train_time:131540ms step_avg:58.72ms
step:2241/2330 train_time:131598ms step_avg:58.72ms
step:2242/2330 train_time:131659ms step_avg:58.72ms
step:2243/2330 train_time:131717ms step_avg:58.72ms
step:2244/2330 train_time:131777ms step_avg:58.72ms
step:2245/2330 train_time:131834ms step_avg:58.72ms
step:2246/2330 train_time:131896ms step_avg:58.72ms
step:2247/2330 train_time:131953ms step_avg:58.72ms
step:2248/2330 train_time:132014ms step_avg:58.72ms
step:2249/2330 train_time:132072ms step_avg:58.72ms
step:2250/2330 train_time:132132ms step_avg:58.73ms
step:2250/2330 val_loss:3.7316 train_time:132214ms step_avg:58.76ms
step:2251/2330 train_time:132234ms step_avg:58.74ms
step:2252/2330 train_time:132253ms step_avg:58.73ms
step:2253/2330 train_time:132311ms step_avg:58.73ms
step:2254/2330 train_time:132376ms step_avg:58.73ms
step:2255/2330 train_time:132434ms step_avg:58.73ms
step:2256/2330 train_time:132494ms step_avg:58.73ms
step:2257/2330 train_time:132552ms step_avg:58.73ms
step:2258/2330 train_time:132612ms step_avg:58.73ms
step:2259/2330 train_time:132670ms step_avg:58.73ms
step:2260/2330 train_time:132729ms step_avg:58.73ms
step:2261/2330 train_time:132787ms step_avg:58.73ms
step:2262/2330 train_time:132846ms step_avg:58.73ms
step:2263/2330 train_time:132903ms step_avg:58.73ms
step:2264/2330 train_time:132964ms step_avg:58.73ms
step:2265/2330 train_time:133020ms step_avg:58.73ms
step:2266/2330 train_time:133081ms step_avg:58.73ms
step:2267/2330 train_time:133139ms step_avg:58.73ms
step:2268/2330 train_time:133200ms step_avg:58.73ms
step:2269/2330 train_time:133259ms step_avg:58.73ms
step:2270/2330 train_time:133322ms step_avg:58.73ms
step:2271/2330 train_time:133380ms step_avg:58.73ms
step:2272/2330 train_time:133444ms step_avg:58.73ms
step:2273/2330 train_time:133500ms step_avg:58.73ms
step:2274/2330 train_time:133564ms step_avg:58.74ms
step:2275/2330 train_time:133621ms step_avg:58.73ms
step:2276/2330 train_time:133682ms step_avg:58.74ms
step:2277/2330 train_time:133738ms step_avg:58.73ms
step:2278/2330 train_time:133799ms step_avg:58.74ms
step:2279/2330 train_time:133856ms step_avg:58.73ms
step:2280/2330 train_time:133916ms step_avg:58.74ms
step:2281/2330 train_time:133973ms step_avg:58.73ms
step:2282/2330 train_time:134033ms step_avg:58.73ms
step:2283/2330 train_time:134090ms step_avg:58.73ms
step:2284/2330 train_time:134151ms step_avg:58.74ms
step:2285/2330 train_time:134209ms step_avg:58.73ms
step:2286/2330 train_time:134270ms step_avg:58.74ms
step:2287/2330 train_time:134327ms step_avg:58.74ms
step:2288/2330 train_time:134388ms step_avg:58.74ms
step:2289/2330 train_time:134446ms step_avg:58.74ms
step:2290/2330 train_time:134508ms step_avg:58.74ms
step:2291/2330 train_time:134564ms step_avg:58.74ms
step:2292/2330 train_time:134626ms step_avg:58.74ms
step:2293/2330 train_time:134682ms step_avg:58.74ms
step:2294/2330 train_time:134744ms step_avg:58.74ms
step:2295/2330 train_time:134801ms step_avg:58.74ms
step:2296/2330 train_time:134863ms step_avg:58.74ms
step:2297/2330 train_time:134919ms step_avg:58.74ms
step:2298/2330 train_time:134981ms step_avg:58.74ms
step:2299/2330 train_time:135038ms step_avg:58.74ms
step:2300/2330 train_time:135098ms step_avg:58.74ms
step:2301/2330 train_time:135155ms step_avg:58.74ms
step:2302/2330 train_time:135217ms step_avg:58.74ms
step:2303/2330 train_time:135276ms step_avg:58.74ms
step:2304/2330 train_time:135336ms step_avg:58.74ms
step:2305/2330 train_time:135395ms step_avg:58.74ms
step:2306/2330 train_time:135455ms step_avg:58.74ms
step:2307/2330 train_time:135514ms step_avg:58.74ms
step:2308/2330 train_time:135575ms step_avg:58.74ms
step:2309/2330 train_time:135633ms step_avg:58.74ms
step:2310/2330 train_time:135693ms step_avg:58.74ms
step:2311/2330 train_time:135750ms step_avg:58.74ms
step:2312/2330 train_time:135810ms step_avg:58.74ms
step:2313/2330 train_time:135866ms step_avg:58.74ms
step:2314/2330 train_time:135928ms step_avg:58.74ms
step:2315/2330 train_time:135984ms step_avg:58.74ms
step:2316/2330 train_time:136044ms step_avg:58.74ms
step:2317/2330 train_time:136101ms step_avg:58.74ms
step:2318/2330 train_time:136163ms step_avg:58.74ms
step:2319/2330 train_time:136220ms step_avg:58.74ms
step:2320/2330 train_time:136283ms step_avg:58.74ms
step:2321/2330 train_time:136340ms step_avg:58.74ms
step:2322/2330 train_time:136403ms step_avg:58.74ms
step:2323/2330 train_time:136460ms step_avg:58.74ms
step:2324/2330 train_time:136523ms step_avg:58.74ms
step:2325/2330 train_time:136580ms step_avg:58.74ms
step:2326/2330 train_time:136641ms step_avg:58.75ms
step:2327/2330 train_time:136698ms step_avg:58.74ms
step:2328/2330 train_time:136760ms step_avg:58.75ms
step:2329/2330 train_time:136817ms step_avg:58.75ms
step:2330/2330 train_time:136878ms step_avg:58.75ms
step:2330/2330 val_loss:3.7161 train_time:136959ms step_avg:58.78ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
