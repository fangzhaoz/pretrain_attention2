import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--beta1', type=str, required=True)
parser.add_argument('--beta2', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_beta_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=6e-4,  betas=(float(args2.beta1), float(args2.beta2)), eps=1e-10, weight_decay=0.0, fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt", exist_ok=True)
            torch.save(log, f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 04:05:45 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   36C    P0             117W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   35C    P0             117W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             118W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             115W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   34C    P0             119W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             114W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:85ms step_avg:85.27ms
step:2/2330 train_time:175ms step_avg:87.71ms
step:3/2330 train_time:194ms step_avg:64.55ms
step:4/2330 train_time:212ms step_avg:53.09ms
step:5/2330 train_time:266ms step_avg:53.25ms
step:6/2330 train_time:324ms step_avg:54.07ms
step:7/2330 train_time:379ms step_avg:54.16ms
step:8/2330 train_time:438ms step_avg:54.78ms
step:9/2330 train_time:494ms step_avg:54.84ms
step:10/2330 train_time:552ms step_avg:55.19ms
step:11/2330 train_time:607ms step_avg:55.20ms
step:12/2330 train_time:665ms step_avg:55.44ms
step:13/2330 train_time:721ms step_avg:55.43ms
step:14/2330 train_time:779ms step_avg:55.62ms
step:15/2330 train_time:834ms step_avg:55.61ms
step:16/2330 train_time:892ms step_avg:55.77ms
step:17/2330 train_time:948ms step_avg:55.75ms
step:18/2330 train_time:1007ms step_avg:55.96ms
step:19/2330 train_time:1065ms step_avg:56.05ms
step:20/2330 train_time:1126ms step_avg:56.29ms
step:21/2330 train_time:1183ms step_avg:56.36ms
step:22/2330 train_time:1245ms step_avg:56.59ms
step:23/2330 train_time:1301ms step_avg:56.56ms
step:24/2330 train_time:1361ms step_avg:56.71ms
step:25/2330 train_time:1417ms step_avg:56.68ms
step:26/2330 train_time:1476ms step_avg:56.79ms
step:27/2330 train_time:1532ms step_avg:56.74ms
step:28/2330 train_time:1591ms step_avg:56.83ms
step:29/2330 train_time:1646ms step_avg:56.77ms
step:30/2330 train_time:1705ms step_avg:56.83ms
step:31/2330 train_time:1760ms step_avg:56.77ms
step:32/2330 train_time:1819ms step_avg:56.84ms
step:33/2330 train_time:1874ms step_avg:56.80ms
step:34/2330 train_time:1933ms step_avg:56.86ms
step:35/2330 train_time:1989ms step_avg:56.82ms
step:36/2330 train_time:2048ms step_avg:56.90ms
step:37/2330 train_time:2106ms step_avg:56.91ms
step:38/2330 train_time:2165ms step_avg:56.99ms
step:39/2330 train_time:2222ms step_avg:56.97ms
step:40/2330 train_time:2282ms step_avg:57.05ms
step:41/2330 train_time:2339ms step_avg:57.05ms
step:42/2330 train_time:2397ms step_avg:57.08ms
step:43/2330 train_time:2452ms step_avg:57.03ms
step:44/2330 train_time:2513ms step_avg:57.10ms
step:45/2330 train_time:2568ms step_avg:57.07ms
step:46/2330 train_time:2627ms step_avg:57.11ms
step:47/2330 train_time:2682ms step_avg:57.07ms
step:48/2330 train_time:2741ms step_avg:57.10ms
step:49/2330 train_time:2796ms step_avg:57.06ms
step:50/2330 train_time:2855ms step_avg:57.09ms
step:51/2330 train_time:2910ms step_avg:57.06ms
step:52/2330 train_time:2969ms step_avg:57.10ms
step:53/2330 train_time:3026ms step_avg:57.09ms
step:54/2330 train_time:3085ms step_avg:57.13ms
step:55/2330 train_time:3141ms step_avg:57.11ms
step:56/2330 train_time:3201ms step_avg:57.16ms
step:57/2330 train_time:3258ms step_avg:57.15ms
step:58/2330 train_time:3317ms step_avg:57.20ms
step:59/2330 train_time:3373ms step_avg:57.17ms
step:60/2330 train_time:3434ms step_avg:57.23ms
step:61/2330 train_time:3490ms step_avg:57.22ms
step:62/2330 train_time:3548ms step_avg:57.23ms
step:63/2330 train_time:3604ms step_avg:57.20ms
step:64/2330 train_time:3663ms step_avg:57.23ms
step:65/2330 train_time:3719ms step_avg:57.21ms
step:66/2330 train_time:3778ms step_avg:57.24ms
step:67/2330 train_time:3833ms step_avg:57.21ms
step:68/2330 train_time:3892ms step_avg:57.23ms
step:69/2330 train_time:3947ms step_avg:57.21ms
step:70/2330 train_time:4007ms step_avg:57.24ms
step:71/2330 train_time:4063ms step_avg:57.22ms
step:72/2330 train_time:4121ms step_avg:57.24ms
step:73/2330 train_time:4178ms step_avg:57.23ms
step:74/2330 train_time:4238ms step_avg:57.27ms
step:75/2330 train_time:4294ms step_avg:57.25ms
step:76/2330 train_time:4354ms step_avg:57.29ms
step:77/2330 train_time:4410ms step_avg:57.27ms
step:78/2330 train_time:4469ms step_avg:57.30ms
step:79/2330 train_time:4526ms step_avg:57.29ms
step:80/2330 train_time:4585ms step_avg:57.31ms
step:81/2330 train_time:4641ms step_avg:57.30ms
step:82/2330 train_time:4700ms step_avg:57.32ms
step:83/2330 train_time:4756ms step_avg:57.30ms
step:84/2330 train_time:4814ms step_avg:57.32ms
step:85/2330 train_time:4870ms step_avg:57.29ms
step:86/2330 train_time:4929ms step_avg:57.31ms
step:87/2330 train_time:4986ms step_avg:57.31ms
step:88/2330 train_time:5045ms step_avg:57.33ms
step:89/2330 train_time:5101ms step_avg:57.32ms
step:90/2330 train_time:5160ms step_avg:57.34ms
step:91/2330 train_time:5217ms step_avg:57.33ms
step:92/2330 train_time:5276ms step_avg:57.35ms
step:93/2330 train_time:5332ms step_avg:57.34ms
step:94/2330 train_time:5392ms step_avg:57.36ms
step:95/2330 train_time:5448ms step_avg:57.35ms
step:96/2330 train_time:5507ms step_avg:57.37ms
step:97/2330 train_time:5563ms step_avg:57.35ms
step:98/2330 train_time:5622ms step_avg:57.37ms
step:99/2330 train_time:5678ms step_avg:57.35ms
step:100/2330 train_time:5738ms step_avg:57.38ms
step:101/2330 train_time:5793ms step_avg:57.35ms
step:102/2330 train_time:5853ms step_avg:57.38ms
step:103/2330 train_time:5909ms step_avg:57.37ms
step:104/2330 train_time:5969ms step_avg:57.39ms
step:105/2330 train_time:6025ms step_avg:57.38ms
step:106/2330 train_time:6083ms step_avg:57.39ms
step:107/2330 train_time:6140ms step_avg:57.38ms
step:108/2330 train_time:6199ms step_avg:57.40ms
step:109/2330 train_time:6255ms step_avg:57.39ms
step:110/2330 train_time:6317ms step_avg:57.42ms
step:111/2330 train_time:6372ms step_avg:57.41ms
step:112/2330 train_time:6433ms step_avg:57.44ms
step:113/2330 train_time:6490ms step_avg:57.43ms
step:114/2330 train_time:6549ms step_avg:57.44ms
step:115/2330 train_time:6606ms step_avg:57.44ms
step:116/2330 train_time:6665ms step_avg:57.45ms
step:117/2330 train_time:6721ms step_avg:57.44ms
step:118/2330 train_time:6779ms step_avg:57.45ms
step:119/2330 train_time:6835ms step_avg:57.44ms
step:120/2330 train_time:6894ms step_avg:57.45ms
step:121/2330 train_time:6951ms step_avg:57.44ms
step:122/2330 train_time:7009ms step_avg:57.45ms
step:123/2330 train_time:7066ms step_avg:57.44ms
step:124/2330 train_time:7124ms step_avg:57.45ms
step:125/2330 train_time:7180ms step_avg:57.44ms
step:126/2330 train_time:7240ms step_avg:57.46ms
step:127/2330 train_time:7295ms step_avg:57.44ms
step:128/2330 train_time:7355ms step_avg:57.46ms
step:129/2330 train_time:7411ms step_avg:57.45ms
step:130/2330 train_time:7470ms step_avg:57.46ms
step:131/2330 train_time:7527ms step_avg:57.46ms
step:132/2330 train_time:7586ms step_avg:57.47ms
step:133/2330 train_time:7643ms step_avg:57.46ms
step:134/2330 train_time:7701ms step_avg:57.47ms
step:135/2330 train_time:7757ms step_avg:57.46ms
step:136/2330 train_time:7816ms step_avg:57.47ms
step:137/2330 train_time:7871ms step_avg:57.46ms
step:138/2330 train_time:7932ms step_avg:57.47ms
step:139/2330 train_time:7988ms step_avg:57.47ms
step:140/2330 train_time:8046ms step_avg:57.47ms
step:141/2330 train_time:8102ms step_avg:57.46ms
step:142/2330 train_time:8161ms step_avg:57.47ms
step:143/2330 train_time:8217ms step_avg:57.46ms
step:144/2330 train_time:8277ms step_avg:57.48ms
step:145/2330 train_time:8333ms step_avg:57.47ms
step:146/2330 train_time:8392ms step_avg:57.48ms
step:147/2330 train_time:8448ms step_avg:57.47ms
step:148/2330 train_time:8508ms step_avg:57.48ms
step:149/2330 train_time:8564ms step_avg:57.47ms
step:150/2330 train_time:8622ms step_avg:57.48ms
step:151/2330 train_time:8679ms step_avg:57.47ms
step:152/2330 train_time:8738ms step_avg:57.49ms
step:153/2330 train_time:8794ms step_avg:57.47ms
step:154/2330 train_time:8853ms step_avg:57.48ms
step:155/2330 train_time:8909ms step_avg:57.48ms
step:156/2330 train_time:8967ms step_avg:57.48ms
step:157/2330 train_time:9023ms step_avg:57.47ms
step:158/2330 train_time:9081ms step_avg:57.48ms
step:159/2330 train_time:9137ms step_avg:57.47ms
step:160/2330 train_time:9196ms step_avg:57.48ms
step:161/2330 train_time:9252ms step_avg:57.46ms
step:162/2330 train_time:9311ms step_avg:57.47ms
step:163/2330 train_time:9367ms step_avg:57.47ms
step:164/2330 train_time:9427ms step_avg:57.48ms
step:165/2330 train_time:9483ms step_avg:57.47ms
step:166/2330 train_time:9542ms step_avg:57.48ms
step:167/2330 train_time:9597ms step_avg:57.47ms
step:168/2330 train_time:9658ms step_avg:57.49ms
step:169/2330 train_time:9714ms step_avg:57.48ms
step:170/2330 train_time:9773ms step_avg:57.49ms
step:171/2330 train_time:9830ms step_avg:57.48ms
step:172/2330 train_time:9888ms step_avg:57.49ms
step:173/2330 train_time:9945ms step_avg:57.48ms
step:174/2330 train_time:10003ms step_avg:57.49ms
step:175/2330 train_time:10059ms step_avg:57.48ms
step:176/2330 train_time:10117ms step_avg:57.48ms
step:177/2330 train_time:10173ms step_avg:57.47ms
step:178/2330 train_time:10233ms step_avg:57.49ms
step:179/2330 train_time:10289ms step_avg:57.48ms
step:180/2330 train_time:10348ms step_avg:57.49ms
step:181/2330 train_time:10403ms step_avg:57.48ms
step:182/2330 train_time:10462ms step_avg:57.48ms
step:183/2330 train_time:10518ms step_avg:57.47ms
step:184/2330 train_time:10578ms step_avg:57.49ms
step:185/2330 train_time:10634ms step_avg:57.48ms
step:186/2330 train_time:10693ms step_avg:57.49ms
step:187/2330 train_time:10748ms step_avg:57.48ms
step:188/2330 train_time:10808ms step_avg:57.49ms
step:189/2330 train_time:10865ms step_avg:57.49ms
step:190/2330 train_time:10923ms step_avg:57.49ms
step:191/2330 train_time:10979ms step_avg:57.48ms
step:192/2330 train_time:11038ms step_avg:57.49ms
step:193/2330 train_time:11094ms step_avg:57.48ms
step:194/2330 train_time:11153ms step_avg:57.49ms
step:195/2330 train_time:11209ms step_avg:57.48ms
step:196/2330 train_time:11267ms step_avg:57.48ms
step:197/2330 train_time:11322ms step_avg:57.47ms
step:198/2330 train_time:11381ms step_avg:57.48ms
step:199/2330 train_time:11437ms step_avg:57.47ms
step:200/2330 train_time:11496ms step_avg:57.48ms
step:201/2330 train_time:11552ms step_avg:57.47ms
step:202/2330 train_time:11612ms step_avg:57.49ms
step:203/2330 train_time:11669ms step_avg:57.48ms
step:204/2330 train_time:11727ms step_avg:57.49ms
step:205/2330 train_time:11783ms step_avg:57.48ms
step:206/2330 train_time:11842ms step_avg:57.49ms
step:207/2330 train_time:11898ms step_avg:57.48ms
step:208/2330 train_time:11957ms step_avg:57.49ms
step:209/2330 train_time:12013ms step_avg:57.48ms
step:210/2330 train_time:12073ms step_avg:57.49ms
step:211/2330 train_time:12129ms step_avg:57.48ms
step:212/2330 train_time:12187ms step_avg:57.49ms
step:213/2330 train_time:12243ms step_avg:57.48ms
step:214/2330 train_time:12302ms step_avg:57.49ms
step:215/2330 train_time:12358ms step_avg:57.48ms
step:216/2330 train_time:12417ms step_avg:57.49ms
step:217/2330 train_time:12473ms step_avg:57.48ms
step:218/2330 train_time:12533ms step_avg:57.49ms
step:219/2330 train_time:12590ms step_avg:57.49ms
step:220/2330 train_time:12649ms step_avg:57.49ms
step:221/2330 train_time:12704ms step_avg:57.49ms
step:222/2330 train_time:12764ms step_avg:57.50ms
step:223/2330 train_time:12820ms step_avg:57.49ms
step:224/2330 train_time:12878ms step_avg:57.49ms
step:225/2330 train_time:12934ms step_avg:57.49ms
step:226/2330 train_time:12994ms step_avg:57.49ms
step:227/2330 train_time:13050ms step_avg:57.49ms
step:228/2330 train_time:13108ms step_avg:57.49ms
step:229/2330 train_time:13164ms step_avg:57.49ms
step:230/2330 train_time:13223ms step_avg:57.49ms
step:231/2330 train_time:13279ms step_avg:57.48ms
step:232/2330 train_time:13338ms step_avg:57.49ms
step:233/2330 train_time:13393ms step_avg:57.48ms
step:234/2330 train_time:13453ms step_avg:57.49ms
step:235/2330 train_time:13510ms step_avg:57.49ms
step:236/2330 train_time:13569ms step_avg:57.50ms
step:237/2330 train_time:13625ms step_avg:57.49ms
step:238/2330 train_time:13684ms step_avg:57.50ms
step:239/2330 train_time:13741ms step_avg:57.49ms
step:240/2330 train_time:13799ms step_avg:57.50ms
step:241/2330 train_time:13855ms step_avg:57.49ms
step:242/2330 train_time:13915ms step_avg:57.50ms
step:243/2330 train_time:13971ms step_avg:57.49ms
step:244/2330 train_time:14029ms step_avg:57.50ms
step:245/2330 train_time:14085ms step_avg:57.49ms
step:246/2330 train_time:14144ms step_avg:57.50ms
step:247/2330 train_time:14200ms step_avg:57.49ms
step:248/2330 train_time:14258ms step_avg:57.49ms
step:249/2330 train_time:14314ms step_avg:57.49ms
step:250/2330 train_time:14375ms step_avg:57.50ms
step:250/2330 val_loss:4.9260 train_time:14454ms step_avg:57.81ms
step:251/2330 train_time:14472ms step_avg:57.66ms
step:252/2330 train_time:14493ms step_avg:57.51ms
step:253/2330 train_time:14550ms step_avg:57.51ms
step:254/2330 train_time:14612ms step_avg:57.53ms
step:255/2330 train_time:14668ms step_avg:57.52ms
step:256/2330 train_time:14730ms step_avg:57.54ms
step:257/2330 train_time:14786ms step_avg:57.53ms
step:258/2330 train_time:14846ms step_avg:57.54ms
step:259/2330 train_time:14902ms step_avg:57.54ms
step:260/2330 train_time:14960ms step_avg:57.54ms
step:261/2330 train_time:15016ms step_avg:57.53ms
step:262/2330 train_time:15074ms step_avg:57.54ms
step:263/2330 train_time:15129ms step_avg:57.53ms
step:264/2330 train_time:15188ms step_avg:57.53ms
step:265/2330 train_time:15243ms step_avg:57.52ms
step:266/2330 train_time:15302ms step_avg:57.53ms
step:267/2330 train_time:15358ms step_avg:57.52ms
step:268/2330 train_time:15417ms step_avg:57.53ms
step:269/2330 train_time:15474ms step_avg:57.52ms
step:270/2330 train_time:15533ms step_avg:57.53ms
step:271/2330 train_time:15591ms step_avg:57.53ms
step:272/2330 train_time:15651ms step_avg:57.54ms
step:273/2330 train_time:15707ms step_avg:57.54ms
step:274/2330 train_time:15768ms step_avg:57.55ms
step:275/2330 train_time:15824ms step_avg:57.54ms
step:276/2330 train_time:15884ms step_avg:57.55ms
step:277/2330 train_time:15940ms step_avg:57.54ms
step:278/2330 train_time:15998ms step_avg:57.55ms
step:279/2330 train_time:16054ms step_avg:57.54ms
step:280/2330 train_time:16113ms step_avg:57.54ms
step:281/2330 train_time:16168ms step_avg:57.54ms
step:282/2330 train_time:16227ms step_avg:57.54ms
step:283/2330 train_time:16282ms step_avg:57.53ms
step:284/2330 train_time:16341ms step_avg:57.54ms
step:285/2330 train_time:16398ms step_avg:57.54ms
step:286/2330 train_time:16457ms step_avg:57.54ms
step:287/2330 train_time:16513ms step_avg:57.54ms
step:288/2330 train_time:16573ms step_avg:57.54ms
step:289/2330 train_time:16629ms step_avg:57.54ms
step:290/2330 train_time:16690ms step_avg:57.55ms
step:291/2330 train_time:16745ms step_avg:57.54ms
step:292/2330 train_time:16807ms step_avg:57.56ms
step:293/2330 train_time:16863ms step_avg:57.55ms
step:294/2330 train_time:16922ms step_avg:57.56ms
step:295/2330 train_time:16978ms step_avg:57.55ms
step:296/2330 train_time:17037ms step_avg:57.56ms
step:297/2330 train_time:17093ms step_avg:57.55ms
step:298/2330 train_time:17151ms step_avg:57.55ms
step:299/2330 train_time:17207ms step_avg:57.55ms
step:300/2330 train_time:17266ms step_avg:57.55ms
step:301/2330 train_time:17321ms step_avg:57.54ms
step:302/2330 train_time:17380ms step_avg:57.55ms
step:303/2330 train_time:17435ms step_avg:57.54ms
step:304/2330 train_time:17494ms step_avg:57.55ms
step:305/2330 train_time:17551ms step_avg:57.54ms
step:306/2330 train_time:17610ms step_avg:57.55ms
step:307/2330 train_time:17666ms step_avg:57.54ms
step:308/2330 train_time:17727ms step_avg:57.56ms
step:309/2330 train_time:17783ms step_avg:57.55ms
step:310/2330 train_time:17842ms step_avg:57.56ms
step:311/2330 train_time:17899ms step_avg:57.55ms
step:312/2330 train_time:17958ms step_avg:57.56ms
step:313/2330 train_time:18013ms step_avg:57.55ms
step:314/2330 train_time:18072ms step_avg:57.56ms
step:315/2330 train_time:18128ms step_avg:57.55ms
step:316/2330 train_time:18189ms step_avg:57.56ms
step:317/2330 train_time:18245ms step_avg:57.55ms
step:318/2330 train_time:18303ms step_avg:57.56ms
step:319/2330 train_time:18359ms step_avg:57.55ms
step:320/2330 train_time:18418ms step_avg:57.56ms
step:321/2330 train_time:18475ms step_avg:57.55ms
step:322/2330 train_time:18533ms step_avg:57.56ms
step:323/2330 train_time:18590ms step_avg:57.55ms
step:324/2330 train_time:18649ms step_avg:57.56ms
step:325/2330 train_time:18706ms step_avg:57.56ms
step:326/2330 train_time:18765ms step_avg:57.56ms
step:327/2330 train_time:18822ms step_avg:57.56ms
step:328/2330 train_time:18881ms step_avg:57.56ms
step:329/2330 train_time:18937ms step_avg:57.56ms
step:330/2330 train_time:18996ms step_avg:57.56ms
step:331/2330 train_time:19053ms step_avg:57.56ms
step:332/2330 train_time:19112ms step_avg:57.57ms
step:333/2330 train_time:19168ms step_avg:57.56ms
step:334/2330 train_time:19227ms step_avg:57.57ms
step:335/2330 train_time:19283ms step_avg:57.56ms
step:336/2330 train_time:19341ms step_avg:57.56ms
step:337/2330 train_time:19397ms step_avg:57.56ms
step:338/2330 train_time:19456ms step_avg:57.56ms
step:339/2330 train_time:19513ms step_avg:57.56ms
step:340/2330 train_time:19571ms step_avg:57.56ms
step:341/2330 train_time:19627ms step_avg:57.56ms
step:342/2330 train_time:19687ms step_avg:57.57ms
step:343/2330 train_time:19744ms step_avg:57.56ms
step:344/2330 train_time:19802ms step_avg:57.57ms
step:345/2330 train_time:19859ms step_avg:57.56ms
step:346/2330 train_time:19918ms step_avg:57.57ms
step:347/2330 train_time:19974ms step_avg:57.56ms
step:348/2330 train_time:20033ms step_avg:57.57ms
step:349/2330 train_time:20089ms step_avg:57.56ms
step:350/2330 train_time:20148ms step_avg:57.57ms
step:351/2330 train_time:20204ms step_avg:57.56ms
step:352/2330 train_time:20263ms step_avg:57.57ms
step:353/2330 train_time:20319ms step_avg:57.56ms
step:354/2330 train_time:20379ms step_avg:57.57ms
step:355/2330 train_time:20435ms step_avg:57.56ms
step:356/2330 train_time:20493ms step_avg:57.57ms
step:357/2330 train_time:20549ms step_avg:57.56ms
step:358/2330 train_time:20609ms step_avg:57.57ms
step:359/2330 train_time:20665ms step_avg:57.56ms
step:360/2330 train_time:20724ms step_avg:57.57ms
step:361/2330 train_time:20781ms step_avg:57.57ms
step:362/2330 train_time:20840ms step_avg:57.57ms
step:363/2330 train_time:20897ms step_avg:57.57ms
step:364/2330 train_time:20956ms step_avg:57.57ms
step:365/2330 train_time:21012ms step_avg:57.57ms
step:366/2330 train_time:21071ms step_avg:57.57ms
step:367/2330 train_time:21127ms step_avg:57.57ms
step:368/2330 train_time:21187ms step_avg:57.57ms
step:369/2330 train_time:21243ms step_avg:57.57ms
step:370/2330 train_time:21302ms step_avg:57.57ms
step:371/2330 train_time:21358ms step_avg:57.57ms
step:372/2330 train_time:21417ms step_avg:57.57ms
step:373/2330 train_time:21473ms step_avg:57.57ms
step:374/2330 train_time:21532ms step_avg:57.57ms
step:375/2330 train_time:21588ms step_avg:57.57ms
step:376/2330 train_time:21648ms step_avg:57.57ms
step:377/2330 train_time:21705ms step_avg:57.57ms
step:378/2330 train_time:21764ms step_avg:57.58ms
step:379/2330 train_time:21820ms step_avg:57.57ms
step:380/2330 train_time:21880ms step_avg:57.58ms
step:381/2330 train_time:21937ms step_avg:57.58ms
step:382/2330 train_time:21995ms step_avg:57.58ms
step:383/2330 train_time:22051ms step_avg:57.58ms
step:384/2330 train_time:22111ms step_avg:57.58ms
step:385/2330 train_time:22167ms step_avg:57.58ms
step:386/2330 train_time:22227ms step_avg:57.58ms
step:387/2330 train_time:22283ms step_avg:57.58ms
step:388/2330 train_time:22343ms step_avg:57.58ms
step:389/2330 train_time:22400ms step_avg:57.58ms
step:390/2330 train_time:22459ms step_avg:57.59ms
step:391/2330 train_time:22515ms step_avg:57.58ms
step:392/2330 train_time:22574ms step_avg:57.59ms
step:393/2330 train_time:22629ms step_avg:57.58ms
step:394/2330 train_time:22690ms step_avg:57.59ms
step:395/2330 train_time:22745ms step_avg:57.58ms
step:396/2330 train_time:22805ms step_avg:57.59ms
step:397/2330 train_time:22862ms step_avg:57.59ms
step:398/2330 train_time:22921ms step_avg:57.59ms
step:399/2330 train_time:22977ms step_avg:57.59ms
step:400/2330 train_time:23035ms step_avg:57.59ms
step:401/2330 train_time:23091ms step_avg:57.58ms
step:402/2330 train_time:23151ms step_avg:57.59ms
step:403/2330 train_time:23207ms step_avg:57.59ms
step:404/2330 train_time:23269ms step_avg:57.60ms
step:405/2330 train_time:23324ms step_avg:57.59ms
step:406/2330 train_time:23384ms step_avg:57.60ms
step:407/2330 train_time:23440ms step_avg:57.59ms
step:408/2330 train_time:23499ms step_avg:57.60ms
step:409/2330 train_time:23555ms step_avg:57.59ms
step:410/2330 train_time:23615ms step_avg:57.60ms
step:411/2330 train_time:23671ms step_avg:57.59ms
step:412/2330 train_time:23730ms step_avg:57.60ms
step:413/2330 train_time:23786ms step_avg:57.59ms
step:414/2330 train_time:23845ms step_avg:57.60ms
step:415/2330 train_time:23902ms step_avg:57.59ms
step:416/2330 train_time:23961ms step_avg:57.60ms
step:417/2330 train_time:24018ms step_avg:57.60ms
step:418/2330 train_time:24077ms step_avg:57.60ms
step:419/2330 train_time:24133ms step_avg:57.60ms
step:420/2330 train_time:24192ms step_avg:57.60ms
step:421/2330 train_time:24248ms step_avg:57.60ms
step:422/2330 train_time:24308ms step_avg:57.60ms
step:423/2330 train_time:24365ms step_avg:57.60ms
step:424/2330 train_time:24424ms step_avg:57.60ms
step:425/2330 train_time:24480ms step_avg:57.60ms
step:426/2330 train_time:24539ms step_avg:57.60ms
step:427/2330 train_time:24595ms step_avg:57.60ms
step:428/2330 train_time:24654ms step_avg:57.60ms
step:429/2330 train_time:24710ms step_avg:57.60ms
step:430/2330 train_time:24770ms step_avg:57.61ms
step:431/2330 train_time:24826ms step_avg:57.60ms
step:432/2330 train_time:24886ms step_avg:57.61ms
step:433/2330 train_time:24943ms step_avg:57.60ms
step:434/2330 train_time:25001ms step_avg:57.61ms
step:435/2330 train_time:25057ms step_avg:57.60ms
step:436/2330 train_time:25116ms step_avg:57.61ms
step:437/2330 train_time:25173ms step_avg:57.60ms
step:438/2330 train_time:25232ms step_avg:57.61ms
step:439/2330 train_time:25288ms step_avg:57.60ms
step:440/2330 train_time:25348ms step_avg:57.61ms
step:441/2330 train_time:25403ms step_avg:57.60ms
step:442/2330 train_time:25463ms step_avg:57.61ms
step:443/2330 train_time:25520ms step_avg:57.61ms
step:444/2330 train_time:25579ms step_avg:57.61ms
step:445/2330 train_time:25635ms step_avg:57.61ms
step:446/2330 train_time:25694ms step_avg:57.61ms
step:447/2330 train_time:25750ms step_avg:57.61ms
step:448/2330 train_time:25809ms step_avg:57.61ms
step:449/2330 train_time:25865ms step_avg:57.61ms
step:450/2330 train_time:25925ms step_avg:57.61ms
step:451/2330 train_time:25981ms step_avg:57.61ms
step:452/2330 train_time:26040ms step_avg:57.61ms
step:453/2330 train_time:26096ms step_avg:57.61ms
step:454/2330 train_time:26155ms step_avg:57.61ms
step:455/2330 train_time:26211ms step_avg:57.61ms
step:456/2330 train_time:26271ms step_avg:57.61ms
step:457/2330 train_time:26327ms step_avg:57.61ms
step:458/2330 train_time:26386ms step_avg:57.61ms
step:459/2330 train_time:26443ms step_avg:57.61ms
step:460/2330 train_time:26501ms step_avg:57.61ms
step:461/2330 train_time:26558ms step_avg:57.61ms
step:462/2330 train_time:26617ms step_avg:57.61ms
step:463/2330 train_time:26673ms step_avg:57.61ms
step:464/2330 train_time:26732ms step_avg:57.61ms
step:465/2330 train_time:26787ms step_avg:57.61ms
step:466/2330 train_time:26847ms step_avg:57.61ms
step:467/2330 train_time:26904ms step_avg:57.61ms
step:468/2330 train_time:26962ms step_avg:57.61ms
step:469/2330 train_time:27019ms step_avg:57.61ms
step:470/2330 train_time:27078ms step_avg:57.61ms
step:471/2330 train_time:27134ms step_avg:57.61ms
step:472/2330 train_time:27193ms step_avg:57.61ms
step:473/2330 train_time:27249ms step_avg:57.61ms
step:474/2330 train_time:27309ms step_avg:57.61ms
step:475/2330 train_time:27365ms step_avg:57.61ms
step:476/2330 train_time:27424ms step_avg:57.61ms
step:477/2330 train_time:27480ms step_avg:57.61ms
step:478/2330 train_time:27540ms step_avg:57.61ms
step:479/2330 train_time:27596ms step_avg:57.61ms
step:480/2330 train_time:27655ms step_avg:57.61ms
step:481/2330 train_time:27711ms step_avg:57.61ms
step:482/2330 train_time:27771ms step_avg:57.62ms
step:483/2330 train_time:27826ms step_avg:57.61ms
step:484/2330 train_time:27887ms step_avg:57.62ms
step:485/2330 train_time:27943ms step_avg:57.61ms
step:486/2330 train_time:28002ms step_avg:57.62ms
step:487/2330 train_time:28059ms step_avg:57.62ms
step:488/2330 train_time:28118ms step_avg:57.62ms
step:489/2330 train_time:28175ms step_avg:57.62ms
step:490/2330 train_time:28234ms step_avg:57.62ms
step:491/2330 train_time:28290ms step_avg:57.62ms
step:492/2330 train_time:28350ms step_avg:57.62ms
step:493/2330 train_time:28406ms step_avg:57.62ms
step:494/2330 train_time:28466ms step_avg:57.62ms
step:495/2330 train_time:28522ms step_avg:57.62ms
step:496/2330 train_time:28581ms step_avg:57.62ms
step:497/2330 train_time:28637ms step_avg:57.62ms
step:498/2330 train_time:28695ms step_avg:57.62ms
step:499/2330 train_time:28752ms step_avg:57.62ms
step:500/2330 train_time:28811ms step_avg:57.62ms
step:500/2330 val_loss:4.4332 train_time:28890ms step_avg:57.78ms
step:501/2330 train_time:28909ms step_avg:57.70ms
step:502/2330 train_time:28929ms step_avg:57.63ms
step:503/2330 train_time:28985ms step_avg:57.62ms
step:504/2330 train_time:29051ms step_avg:57.64ms
step:505/2330 train_time:29109ms step_avg:57.64ms
step:506/2330 train_time:29168ms step_avg:57.64ms
step:507/2330 train_time:29224ms step_avg:57.64ms
step:508/2330 train_time:29283ms step_avg:57.64ms
step:509/2330 train_time:29339ms step_avg:57.64ms
step:510/2330 train_time:29397ms step_avg:57.64ms
step:511/2330 train_time:29453ms step_avg:57.64ms
step:512/2330 train_time:29512ms step_avg:57.64ms
step:513/2330 train_time:29567ms step_avg:57.64ms
step:514/2330 train_time:29626ms step_avg:57.64ms
step:515/2330 train_time:29681ms step_avg:57.63ms
step:516/2330 train_time:29739ms step_avg:57.63ms
step:517/2330 train_time:29795ms step_avg:57.63ms
step:518/2330 train_time:29854ms step_avg:57.63ms
step:519/2330 train_time:29910ms step_avg:57.63ms
step:520/2330 train_time:29971ms step_avg:57.64ms
step:521/2330 train_time:30030ms step_avg:57.64ms
step:522/2330 train_time:30090ms step_avg:57.64ms
step:523/2330 train_time:30147ms step_avg:57.64ms
step:524/2330 train_time:30207ms step_avg:57.65ms
step:525/2330 train_time:30264ms step_avg:57.64ms
step:526/2330 train_time:30323ms step_avg:57.65ms
step:527/2330 train_time:30379ms step_avg:57.65ms
step:528/2330 train_time:30438ms step_avg:57.65ms
step:529/2330 train_time:30493ms step_avg:57.64ms
step:530/2330 train_time:30553ms step_avg:57.65ms
step:531/2330 train_time:30608ms step_avg:57.64ms
step:532/2330 train_time:30666ms step_avg:57.64ms
step:533/2330 train_time:30722ms step_avg:57.64ms
step:534/2330 train_time:30781ms step_avg:57.64ms
step:535/2330 train_time:30836ms step_avg:57.64ms
step:536/2330 train_time:30896ms step_avg:57.64ms
step:537/2330 train_time:30953ms step_avg:57.64ms
step:538/2330 train_time:31014ms step_avg:57.65ms
step:539/2330 train_time:31070ms step_avg:57.64ms
step:540/2330 train_time:31132ms step_avg:57.65ms
step:541/2330 train_time:31189ms step_avg:57.65ms
step:542/2330 train_time:31249ms step_avg:57.65ms
step:543/2330 train_time:31306ms step_avg:57.65ms
step:544/2330 train_time:31365ms step_avg:57.66ms
step:545/2330 train_time:31421ms step_avg:57.65ms
step:546/2330 train_time:31480ms step_avg:57.66ms
step:547/2330 train_time:31536ms step_avg:57.65ms
step:548/2330 train_time:31594ms step_avg:57.65ms
step:549/2330 train_time:31650ms step_avg:57.65ms
step:550/2330 train_time:31710ms step_avg:57.65ms
step:551/2330 train_time:31765ms step_avg:57.65ms
step:552/2330 train_time:31824ms step_avg:57.65ms
step:553/2330 train_time:31881ms step_avg:57.65ms
step:554/2330 train_time:31940ms step_avg:57.65ms
step:555/2330 train_time:31996ms step_avg:57.65ms
step:556/2330 train_time:32055ms step_avg:57.65ms
step:557/2330 train_time:32111ms step_avg:57.65ms
step:558/2330 train_time:32173ms step_avg:57.66ms
step:559/2330 train_time:32229ms step_avg:57.66ms
step:560/2330 train_time:32290ms step_avg:57.66ms
step:561/2330 train_time:32346ms step_avg:57.66ms
step:562/2330 train_time:32405ms step_avg:57.66ms
step:563/2330 train_time:32462ms step_avg:57.66ms
step:564/2330 train_time:32520ms step_avg:57.66ms
step:565/2330 train_time:32576ms step_avg:57.66ms
step:566/2330 train_time:32635ms step_avg:57.66ms
step:567/2330 train_time:32691ms step_avg:57.66ms
step:568/2330 train_time:32751ms step_avg:57.66ms
step:569/2330 train_time:32807ms step_avg:57.66ms
step:570/2330 train_time:32865ms step_avg:57.66ms
step:571/2330 train_time:32922ms step_avg:57.66ms
step:572/2330 train_time:32981ms step_avg:57.66ms
step:573/2330 train_time:33038ms step_avg:57.66ms
step:574/2330 train_time:33097ms step_avg:57.66ms
step:575/2330 train_time:33153ms step_avg:57.66ms
step:576/2330 train_time:33214ms step_avg:57.66ms
step:577/2330 train_time:33269ms step_avg:57.66ms
step:578/2330 train_time:33331ms step_avg:57.67ms
step:579/2330 train_time:33387ms step_avg:57.66ms
step:580/2330 train_time:33446ms step_avg:57.67ms
step:581/2330 train_time:33502ms step_avg:57.66ms
step:582/2330 train_time:33561ms step_avg:57.66ms
step:583/2330 train_time:33617ms step_avg:57.66ms
step:584/2330 train_time:33676ms step_avg:57.66ms
step:585/2330 train_time:33732ms step_avg:57.66ms
step:586/2330 train_time:33792ms step_avg:57.67ms
step:587/2330 train_time:33848ms step_avg:57.66ms
step:588/2330 train_time:33907ms step_avg:57.66ms
step:589/2330 train_time:33963ms step_avg:57.66ms
step:590/2330 train_time:34022ms step_avg:57.66ms
step:591/2330 train_time:34078ms step_avg:57.66ms
step:592/2330 train_time:34138ms step_avg:57.67ms
step:593/2330 train_time:34194ms step_avg:57.66ms
step:594/2330 train_time:34255ms step_avg:57.67ms
step:595/2330 train_time:34310ms step_avg:57.66ms
step:596/2330 train_time:34370ms step_avg:57.67ms
step:597/2330 train_time:34426ms step_avg:57.67ms
step:598/2330 train_time:34485ms step_avg:57.67ms
step:599/2330 train_time:34541ms step_avg:57.66ms
step:600/2330 train_time:34600ms step_avg:57.67ms
step:601/2330 train_time:34655ms step_avg:57.66ms
step:602/2330 train_time:34715ms step_avg:57.67ms
step:603/2330 train_time:34771ms step_avg:57.66ms
step:604/2330 train_time:34831ms step_avg:57.67ms
step:605/2330 train_time:34887ms step_avg:57.66ms
step:606/2330 train_time:34945ms step_avg:57.67ms
step:607/2330 train_time:35002ms step_avg:57.66ms
step:608/2330 train_time:35061ms step_avg:57.67ms
step:609/2330 train_time:35117ms step_avg:57.66ms
step:610/2330 train_time:35176ms step_avg:57.67ms
step:611/2330 train_time:35233ms step_avg:57.66ms
step:612/2330 train_time:35292ms step_avg:57.67ms
step:613/2330 train_time:35349ms step_avg:57.67ms
step:614/2330 train_time:35409ms step_avg:57.67ms
step:615/2330 train_time:35466ms step_avg:57.67ms
step:616/2330 train_time:35524ms step_avg:57.67ms
step:617/2330 train_time:35581ms step_avg:57.67ms
step:618/2330 train_time:35640ms step_avg:57.67ms
step:619/2330 train_time:35696ms step_avg:57.67ms
step:620/2330 train_time:35754ms step_avg:57.67ms
step:621/2330 train_time:35810ms step_avg:57.67ms
step:622/2330 train_time:35870ms step_avg:57.67ms
step:623/2330 train_time:35927ms step_avg:57.67ms
step:624/2330 train_time:35986ms step_avg:57.67ms
step:625/2330 train_time:36042ms step_avg:57.67ms
step:626/2330 train_time:36101ms step_avg:57.67ms
step:627/2330 train_time:36158ms step_avg:57.67ms
step:628/2330 train_time:36216ms step_avg:57.67ms
step:629/2330 train_time:36273ms step_avg:57.67ms
step:630/2330 train_time:36332ms step_avg:57.67ms
step:631/2330 train_time:36388ms step_avg:57.67ms
step:632/2330 train_time:36447ms step_avg:57.67ms
step:633/2330 train_time:36504ms step_avg:57.67ms
step:634/2330 train_time:36564ms step_avg:57.67ms
step:635/2330 train_time:36621ms step_avg:57.67ms
step:636/2330 train_time:36679ms step_avg:57.67ms
step:637/2330 train_time:36735ms step_avg:57.67ms
step:638/2330 train_time:36794ms step_avg:57.67ms
step:639/2330 train_time:36850ms step_avg:57.67ms
step:640/2330 train_time:36910ms step_avg:57.67ms
step:641/2330 train_time:36966ms step_avg:57.67ms
step:642/2330 train_time:37025ms step_avg:57.67ms
step:643/2330 train_time:37081ms step_avg:57.67ms
step:644/2330 train_time:37140ms step_avg:57.67ms
step:645/2330 train_time:37197ms step_avg:57.67ms
step:646/2330 train_time:37257ms step_avg:57.67ms
step:647/2330 train_time:37313ms step_avg:57.67ms
step:648/2330 train_time:37373ms step_avg:57.67ms
step:649/2330 train_time:37430ms step_avg:57.67ms
step:650/2330 train_time:37489ms step_avg:57.68ms
step:651/2330 train_time:37547ms step_avg:57.68ms
step:652/2330 train_time:37605ms step_avg:57.68ms
step:653/2330 train_time:37662ms step_avg:57.68ms
step:654/2330 train_time:37721ms step_avg:57.68ms
step:655/2330 train_time:37777ms step_avg:57.67ms
step:656/2330 train_time:37837ms step_avg:57.68ms
step:657/2330 train_time:37893ms step_avg:57.68ms
step:658/2330 train_time:37953ms step_avg:57.68ms
step:659/2330 train_time:38008ms step_avg:57.68ms
step:660/2330 train_time:38067ms step_avg:57.68ms
step:661/2330 train_time:38124ms step_avg:57.68ms
step:662/2330 train_time:38183ms step_avg:57.68ms
step:663/2330 train_time:38239ms step_avg:57.68ms
step:664/2330 train_time:38298ms step_avg:57.68ms
step:665/2330 train_time:38354ms step_avg:57.68ms
step:666/2330 train_time:38415ms step_avg:57.68ms
step:667/2330 train_time:38471ms step_avg:57.68ms
step:668/2330 train_time:38532ms step_avg:57.68ms
step:669/2330 train_time:38588ms step_avg:57.68ms
step:670/2330 train_time:38648ms step_avg:57.68ms
step:671/2330 train_time:38705ms step_avg:57.68ms
step:672/2330 train_time:38764ms step_avg:57.68ms
step:673/2330 train_time:38821ms step_avg:57.68ms
step:674/2330 train_time:38879ms step_avg:57.68ms
step:675/2330 train_time:38935ms step_avg:57.68ms
step:676/2330 train_time:38995ms step_avg:57.68ms
step:677/2330 train_time:39050ms step_avg:57.68ms
step:678/2330 train_time:39112ms step_avg:57.69ms
step:679/2330 train_time:39167ms step_avg:57.68ms
step:680/2330 train_time:39226ms step_avg:57.69ms
step:681/2330 train_time:39283ms step_avg:57.68ms
step:682/2330 train_time:39342ms step_avg:57.69ms
step:683/2330 train_time:39399ms step_avg:57.68ms
step:684/2330 train_time:39459ms step_avg:57.69ms
step:685/2330 train_time:39515ms step_avg:57.69ms
step:686/2330 train_time:39574ms step_avg:57.69ms
step:687/2330 train_time:39630ms step_avg:57.69ms
step:688/2330 train_time:39690ms step_avg:57.69ms
step:689/2330 train_time:39746ms step_avg:57.69ms
step:690/2330 train_time:39805ms step_avg:57.69ms
step:691/2330 train_time:39862ms step_avg:57.69ms
step:692/2330 train_time:39921ms step_avg:57.69ms
step:693/2330 train_time:39977ms step_avg:57.69ms
step:694/2330 train_time:40036ms step_avg:57.69ms
step:695/2330 train_time:40091ms step_avg:57.69ms
step:696/2330 train_time:40153ms step_avg:57.69ms
step:697/2330 train_time:40209ms step_avg:57.69ms
step:698/2330 train_time:40267ms step_avg:57.69ms
step:699/2330 train_time:40324ms step_avg:57.69ms
step:700/2330 train_time:40383ms step_avg:57.69ms
step:701/2330 train_time:40439ms step_avg:57.69ms
step:702/2330 train_time:40498ms step_avg:57.69ms
step:703/2330 train_time:40555ms step_avg:57.69ms
step:704/2330 train_time:40615ms step_avg:57.69ms
step:705/2330 train_time:40671ms step_avg:57.69ms
step:706/2330 train_time:40732ms step_avg:57.69ms
step:707/2330 train_time:40787ms step_avg:57.69ms
step:708/2330 train_time:40847ms step_avg:57.69ms
step:709/2330 train_time:40903ms step_avg:57.69ms
step:710/2330 train_time:40962ms step_avg:57.69ms
step:711/2330 train_time:41018ms step_avg:57.69ms
step:712/2330 train_time:41077ms step_avg:57.69ms
step:713/2330 train_time:41133ms step_avg:57.69ms
step:714/2330 train_time:41194ms step_avg:57.69ms
step:715/2330 train_time:41250ms step_avg:57.69ms
step:716/2330 train_time:41309ms step_avg:57.69ms
step:717/2330 train_time:41366ms step_avg:57.69ms
step:718/2330 train_time:41425ms step_avg:57.70ms
step:719/2330 train_time:41482ms step_avg:57.69ms
step:720/2330 train_time:41541ms step_avg:57.70ms
step:721/2330 train_time:41597ms step_avg:57.69ms
step:722/2330 train_time:41656ms step_avg:57.70ms
step:723/2330 train_time:41712ms step_avg:57.69ms
step:724/2330 train_time:41773ms step_avg:57.70ms
step:725/2330 train_time:41829ms step_avg:57.70ms
step:726/2330 train_time:41888ms step_avg:57.70ms
step:727/2330 train_time:41944ms step_avg:57.70ms
step:728/2330 train_time:42004ms step_avg:57.70ms
step:729/2330 train_time:42060ms step_avg:57.70ms
step:730/2330 train_time:42119ms step_avg:57.70ms
step:731/2330 train_time:42174ms step_avg:57.69ms
step:732/2330 train_time:42234ms step_avg:57.70ms
step:733/2330 train_time:42290ms step_avg:57.69ms
step:734/2330 train_time:42351ms step_avg:57.70ms
step:735/2330 train_time:42407ms step_avg:57.70ms
step:736/2330 train_time:42466ms step_avg:57.70ms
step:737/2330 train_time:42522ms step_avg:57.70ms
step:738/2330 train_time:42581ms step_avg:57.70ms
step:739/2330 train_time:42637ms step_avg:57.70ms
step:740/2330 train_time:42696ms step_avg:57.70ms
step:741/2330 train_time:42753ms step_avg:57.70ms
step:742/2330 train_time:42813ms step_avg:57.70ms
step:743/2330 train_time:42869ms step_avg:57.70ms
step:744/2330 train_time:42930ms step_avg:57.70ms
step:745/2330 train_time:42987ms step_avg:57.70ms
step:746/2330 train_time:43046ms step_avg:57.70ms
step:747/2330 train_time:43102ms step_avg:57.70ms
step:748/2330 train_time:43161ms step_avg:57.70ms
step:749/2330 train_time:43217ms step_avg:57.70ms
step:750/2330 train_time:43276ms step_avg:57.70ms
step:750/2330 val_loss:4.2274 train_time:43357ms step_avg:57.81ms
step:751/2330 train_time:43375ms step_avg:57.76ms
step:752/2330 train_time:43394ms step_avg:57.71ms
step:753/2330 train_time:43450ms step_avg:57.70ms
step:754/2330 train_time:43514ms step_avg:57.71ms
step:755/2330 train_time:43570ms step_avg:57.71ms
step:756/2330 train_time:43632ms step_avg:57.71ms
step:757/2330 train_time:43688ms step_avg:57.71ms
step:758/2330 train_time:43746ms step_avg:57.71ms
step:759/2330 train_time:43803ms step_avg:57.71ms
step:760/2330 train_time:43861ms step_avg:57.71ms
step:761/2330 train_time:43916ms step_avg:57.71ms
step:762/2330 train_time:43975ms step_avg:57.71ms
step:763/2330 train_time:44030ms step_avg:57.71ms
step:764/2330 train_time:44090ms step_avg:57.71ms
step:765/2330 train_time:44147ms step_avg:57.71ms
step:766/2330 train_time:44205ms step_avg:57.71ms
step:767/2330 train_time:44262ms step_avg:57.71ms
step:768/2330 train_time:44322ms step_avg:57.71ms
step:769/2330 train_time:44380ms step_avg:57.71ms
step:770/2330 train_time:44442ms step_avg:57.72ms
step:771/2330 train_time:44499ms step_avg:57.72ms
step:772/2330 train_time:44562ms step_avg:57.72ms
step:773/2330 train_time:44618ms step_avg:57.72ms
step:774/2330 train_time:44680ms step_avg:57.73ms
step:775/2330 train_time:44737ms step_avg:57.72ms
step:776/2330 train_time:44797ms step_avg:57.73ms
step:777/2330 train_time:44853ms step_avg:57.73ms
step:778/2330 train_time:44914ms step_avg:57.73ms
step:779/2330 train_time:44971ms step_avg:57.73ms
step:780/2330 train_time:45031ms step_avg:57.73ms
step:781/2330 train_time:45087ms step_avg:57.73ms
step:782/2330 train_time:45147ms step_avg:57.73ms
step:783/2330 train_time:45204ms step_avg:57.73ms
step:784/2330 train_time:45263ms step_avg:57.73ms
step:785/2330 train_time:45321ms step_avg:57.73ms
step:786/2330 train_time:45381ms step_avg:57.74ms
step:787/2330 train_time:45438ms step_avg:57.74ms
step:788/2330 train_time:45499ms step_avg:57.74ms
step:789/2330 train_time:45557ms step_avg:57.74ms
step:790/2330 train_time:45618ms step_avg:57.74ms
step:791/2330 train_time:45675ms step_avg:57.74ms
step:792/2330 train_time:45736ms step_avg:57.75ms
step:793/2330 train_time:45793ms step_avg:57.75ms
step:794/2330 train_time:45854ms step_avg:57.75ms
step:795/2330 train_time:45910ms step_avg:57.75ms
step:796/2330 train_time:45970ms step_avg:57.75ms
step:797/2330 train_time:46027ms step_avg:57.75ms
step:798/2330 train_time:46086ms step_avg:57.75ms
step:799/2330 train_time:46143ms step_avg:57.75ms
step:800/2330 train_time:46203ms step_avg:57.75ms
step:801/2330 train_time:46260ms step_avg:57.75ms
step:802/2330 train_time:46320ms step_avg:57.76ms
step:803/2330 train_time:46377ms step_avg:57.75ms
step:804/2330 train_time:46437ms step_avg:57.76ms
step:805/2330 train_time:46496ms step_avg:57.76ms
step:806/2330 train_time:46556ms step_avg:57.76ms
step:807/2330 train_time:46613ms step_avg:57.76ms
step:808/2330 train_time:46675ms step_avg:57.77ms
step:809/2330 train_time:46732ms step_avg:57.77ms
step:810/2330 train_time:46793ms step_avg:57.77ms
step:811/2330 train_time:46850ms step_avg:57.77ms
step:812/2330 train_time:46910ms step_avg:57.77ms
step:813/2330 train_time:46966ms step_avg:57.77ms
step:814/2330 train_time:47026ms step_avg:57.77ms
step:815/2330 train_time:47083ms step_avg:57.77ms
step:816/2330 train_time:47142ms step_avg:57.77ms
step:817/2330 train_time:47199ms step_avg:57.77ms
step:818/2330 train_time:47260ms step_avg:57.77ms
step:819/2330 train_time:47316ms step_avg:57.77ms
step:820/2330 train_time:47376ms step_avg:57.78ms
step:821/2330 train_time:47434ms step_avg:57.78ms
step:822/2330 train_time:47495ms step_avg:57.78ms
step:823/2330 train_time:47553ms step_avg:57.78ms
step:824/2330 train_time:47613ms step_avg:57.78ms
step:825/2330 train_time:47670ms step_avg:57.78ms
step:826/2330 train_time:47730ms step_avg:57.78ms
step:827/2330 train_time:47788ms step_avg:57.78ms
step:828/2330 train_time:47848ms step_avg:57.79ms
step:829/2330 train_time:47904ms step_avg:57.79ms
step:830/2330 train_time:47964ms step_avg:57.79ms
step:831/2330 train_time:48021ms step_avg:57.79ms
step:832/2330 train_time:48080ms step_avg:57.79ms
step:833/2330 train_time:48137ms step_avg:57.79ms
step:834/2330 train_time:48197ms step_avg:57.79ms
step:835/2330 train_time:48253ms step_avg:57.79ms
step:836/2330 train_time:48313ms step_avg:57.79ms
step:837/2330 train_time:48369ms step_avg:57.79ms
step:838/2330 train_time:48430ms step_avg:57.79ms
step:839/2330 train_time:48489ms step_avg:57.79ms
step:840/2330 train_time:48549ms step_avg:57.80ms
step:841/2330 train_time:48606ms step_avg:57.80ms
step:842/2330 train_time:48666ms step_avg:57.80ms
step:843/2330 train_time:48723ms step_avg:57.80ms
step:844/2330 train_time:48783ms step_avg:57.80ms
step:845/2330 train_time:48840ms step_avg:57.80ms
step:846/2330 train_time:48900ms step_avg:57.80ms
step:847/2330 train_time:48957ms step_avg:57.80ms
step:848/2330 train_time:49018ms step_avg:57.80ms
step:849/2330 train_time:49075ms step_avg:57.80ms
step:850/2330 train_time:49135ms step_avg:57.81ms
step:851/2330 train_time:49191ms step_avg:57.80ms
step:852/2330 train_time:49252ms step_avg:57.81ms
step:853/2330 train_time:49309ms step_avg:57.81ms
step:854/2330 train_time:49369ms step_avg:57.81ms
step:855/2330 train_time:49427ms step_avg:57.81ms
step:856/2330 train_time:49486ms step_avg:57.81ms
step:857/2330 train_time:49543ms step_avg:57.81ms
step:858/2330 train_time:49603ms step_avg:57.81ms
step:859/2330 train_time:49660ms step_avg:57.81ms
step:860/2330 train_time:49721ms step_avg:57.81ms
step:861/2330 train_time:49778ms step_avg:57.81ms
step:862/2330 train_time:49839ms step_avg:57.82ms
step:863/2330 train_time:49896ms step_avg:57.82ms
step:864/2330 train_time:49957ms step_avg:57.82ms
step:865/2330 train_time:50013ms step_avg:57.82ms
step:866/2330 train_time:50074ms step_avg:57.82ms
step:867/2330 train_time:50131ms step_avg:57.82ms
step:868/2330 train_time:50191ms step_avg:57.82ms
step:869/2330 train_time:50248ms step_avg:57.82ms
step:870/2330 train_time:50307ms step_avg:57.82ms
step:871/2330 train_time:50364ms step_avg:57.82ms
step:872/2330 train_time:50423ms step_avg:57.83ms
step:873/2330 train_time:50481ms step_avg:57.82ms
step:874/2330 train_time:50541ms step_avg:57.83ms
step:875/2330 train_time:50599ms step_avg:57.83ms
step:876/2330 train_time:50658ms step_avg:57.83ms
step:877/2330 train_time:50715ms step_avg:57.83ms
step:878/2330 train_time:50776ms step_avg:57.83ms
step:879/2330 train_time:50833ms step_avg:57.83ms
step:880/2330 train_time:50894ms step_avg:57.83ms
step:881/2330 train_time:50951ms step_avg:57.83ms
step:882/2330 train_time:51010ms step_avg:57.83ms
step:883/2330 train_time:51068ms step_avg:57.83ms
step:884/2330 train_time:51127ms step_avg:57.84ms
step:885/2330 train_time:51183ms step_avg:57.83ms
step:886/2330 train_time:51244ms step_avg:57.84ms
step:887/2330 train_time:51301ms step_avg:57.84ms
step:888/2330 train_time:51361ms step_avg:57.84ms
step:889/2330 train_time:51417ms step_avg:57.84ms
step:890/2330 train_time:51478ms step_avg:57.84ms
step:891/2330 train_time:51535ms step_avg:57.84ms
step:892/2330 train_time:51596ms step_avg:57.84ms
step:893/2330 train_time:51653ms step_avg:57.84ms
step:894/2330 train_time:51713ms step_avg:57.84ms
step:895/2330 train_time:51771ms step_avg:57.84ms
step:896/2330 train_time:51831ms step_avg:57.85ms
step:897/2330 train_time:51889ms step_avg:57.85ms
step:898/2330 train_time:51949ms step_avg:57.85ms
step:899/2330 train_time:52006ms step_avg:57.85ms
step:900/2330 train_time:52066ms step_avg:57.85ms
step:901/2330 train_time:52123ms step_avg:57.85ms
step:902/2330 train_time:52182ms step_avg:57.85ms
step:903/2330 train_time:52240ms step_avg:57.85ms
step:904/2330 train_time:52299ms step_avg:57.85ms
step:905/2330 train_time:52357ms step_avg:57.85ms
step:906/2330 train_time:52416ms step_avg:57.85ms
step:907/2330 train_time:52474ms step_avg:57.85ms
step:908/2330 train_time:52534ms step_avg:57.86ms
step:909/2330 train_time:52592ms step_avg:57.86ms
step:910/2330 train_time:52651ms step_avg:57.86ms
step:911/2330 train_time:52708ms step_avg:57.86ms
step:912/2330 train_time:52768ms step_avg:57.86ms
step:913/2330 train_time:52825ms step_avg:57.86ms
step:914/2330 train_time:52885ms step_avg:57.86ms
step:915/2330 train_time:52942ms step_avg:57.86ms
step:916/2330 train_time:53003ms step_avg:57.86ms
step:917/2330 train_time:53060ms step_avg:57.86ms
step:918/2330 train_time:53120ms step_avg:57.87ms
step:919/2330 train_time:53177ms step_avg:57.86ms
step:920/2330 train_time:53238ms step_avg:57.87ms
step:921/2330 train_time:53295ms step_avg:57.87ms
step:922/2330 train_time:53355ms step_avg:57.87ms
step:923/2330 train_time:53411ms step_avg:57.87ms
step:924/2330 train_time:53472ms step_avg:57.87ms
step:925/2330 train_time:53530ms step_avg:57.87ms
step:926/2330 train_time:53589ms step_avg:57.87ms
step:927/2330 train_time:53647ms step_avg:57.87ms
step:928/2330 train_time:53707ms step_avg:57.87ms
step:929/2330 train_time:53764ms step_avg:57.87ms
step:930/2330 train_time:53823ms step_avg:57.87ms
step:931/2330 train_time:53881ms step_avg:57.87ms
step:932/2330 train_time:53941ms step_avg:57.88ms
step:933/2330 train_time:53998ms step_avg:57.88ms
step:934/2330 train_time:54057ms step_avg:57.88ms
step:935/2330 train_time:54114ms step_avg:57.88ms
step:936/2330 train_time:54176ms step_avg:57.88ms
step:937/2330 train_time:54233ms step_avg:57.88ms
step:938/2330 train_time:54294ms step_avg:57.88ms
step:939/2330 train_time:54350ms step_avg:57.88ms
step:940/2330 train_time:54409ms step_avg:57.88ms
step:941/2330 train_time:54467ms step_avg:57.88ms
step:942/2330 train_time:54527ms step_avg:57.88ms
step:943/2330 train_time:54584ms step_avg:57.88ms
step:944/2330 train_time:54643ms step_avg:57.88ms
step:945/2330 train_time:54700ms step_avg:57.88ms
step:946/2330 train_time:54760ms step_avg:57.89ms
step:947/2330 train_time:54817ms step_avg:57.88ms
step:948/2330 train_time:54878ms step_avg:57.89ms
step:949/2330 train_time:54936ms step_avg:57.89ms
step:950/2330 train_time:54997ms step_avg:57.89ms
step:951/2330 train_time:55053ms step_avg:57.89ms
step:952/2330 train_time:55113ms step_avg:57.89ms
step:953/2330 train_time:55170ms step_avg:57.89ms
step:954/2330 train_time:55232ms step_avg:57.89ms
step:955/2330 train_time:55289ms step_avg:57.89ms
step:956/2330 train_time:55349ms step_avg:57.90ms
step:957/2330 train_time:55405ms step_avg:57.89ms
step:958/2330 train_time:55465ms step_avg:57.90ms
step:959/2330 train_time:55522ms step_avg:57.90ms
step:960/2330 train_time:55583ms step_avg:57.90ms
step:961/2330 train_time:55639ms step_avg:57.90ms
step:962/2330 train_time:55699ms step_avg:57.90ms
step:963/2330 train_time:55756ms step_avg:57.90ms
step:964/2330 train_time:55816ms step_avg:57.90ms
step:965/2330 train_time:55874ms step_avg:57.90ms
step:966/2330 train_time:55935ms step_avg:57.90ms
step:967/2330 train_time:55991ms step_avg:57.90ms
step:968/2330 train_time:56052ms step_avg:57.91ms
step:969/2330 train_time:56108ms step_avg:57.90ms
step:970/2330 train_time:56169ms step_avg:57.91ms
step:971/2330 train_time:56226ms step_avg:57.91ms
step:972/2330 train_time:56286ms step_avg:57.91ms
step:973/2330 train_time:56343ms step_avg:57.91ms
step:974/2330 train_time:56403ms step_avg:57.91ms
step:975/2330 train_time:56459ms step_avg:57.91ms
step:976/2330 train_time:56520ms step_avg:57.91ms
step:977/2330 train_time:56577ms step_avg:57.91ms
step:978/2330 train_time:56637ms step_avg:57.91ms
step:979/2330 train_time:56693ms step_avg:57.91ms
step:980/2330 train_time:56754ms step_avg:57.91ms
step:981/2330 train_time:56811ms step_avg:57.91ms
step:982/2330 train_time:56872ms step_avg:57.91ms
step:983/2330 train_time:56930ms step_avg:57.91ms
step:984/2330 train_time:56990ms step_avg:57.92ms
step:985/2330 train_time:57047ms step_avg:57.92ms
step:986/2330 train_time:57106ms step_avg:57.92ms
step:987/2330 train_time:57163ms step_avg:57.92ms
step:988/2330 train_time:57224ms step_avg:57.92ms
step:989/2330 train_time:57280ms step_avg:57.92ms
step:990/2330 train_time:57342ms step_avg:57.92ms
step:991/2330 train_time:57399ms step_avg:57.92ms
step:992/2330 train_time:57459ms step_avg:57.92ms
step:993/2330 train_time:57515ms step_avg:57.92ms
step:994/2330 train_time:57576ms step_avg:57.92ms
step:995/2330 train_time:57633ms step_avg:57.92ms
step:996/2330 train_time:57694ms step_avg:57.93ms
step:997/2330 train_time:57751ms step_avg:57.92ms
step:998/2330 train_time:57811ms step_avg:57.93ms
step:999/2330 train_time:57868ms step_avg:57.93ms
step:1000/2330 train_time:57928ms step_avg:57.93ms
step:1000/2330 val_loss:4.0776 train_time:58009ms step_avg:58.01ms
step:1001/2330 train_time:58028ms step_avg:57.97ms
step:1002/2330 train_time:58048ms step_avg:57.93ms
step:1003/2330 train_time:58103ms step_avg:57.93ms
step:1004/2330 train_time:58167ms step_avg:57.94ms
step:1005/2330 train_time:58223ms step_avg:57.93ms
step:1006/2330 train_time:58287ms step_avg:57.94ms
step:1007/2330 train_time:58343ms step_avg:57.94ms
step:1008/2330 train_time:58403ms step_avg:57.94ms
step:1009/2330 train_time:58460ms step_avg:57.94ms
step:1010/2330 train_time:58520ms step_avg:57.94ms
step:1011/2330 train_time:58576ms step_avg:57.94ms
step:1012/2330 train_time:58636ms step_avg:57.94ms
step:1013/2330 train_time:58692ms step_avg:57.94ms
step:1014/2330 train_time:58751ms step_avg:57.94ms
step:1015/2330 train_time:58807ms step_avg:57.94ms
step:1016/2330 train_time:58866ms step_avg:57.94ms
step:1017/2330 train_time:58926ms step_avg:57.94ms
step:1018/2330 train_time:58988ms step_avg:57.94ms
step:1019/2330 train_time:59046ms step_avg:57.94ms
step:1020/2330 train_time:59106ms step_avg:57.95ms
step:1021/2330 train_time:59163ms step_avg:57.95ms
step:1022/2330 train_time:59224ms step_avg:57.95ms
step:1023/2330 train_time:59282ms step_avg:57.95ms
step:1024/2330 train_time:59342ms step_avg:57.95ms
step:1025/2330 train_time:59398ms step_avg:57.95ms
step:1026/2330 train_time:59460ms step_avg:57.95ms
step:1027/2330 train_time:59516ms step_avg:57.95ms
step:1028/2330 train_time:59577ms step_avg:57.95ms
step:1029/2330 train_time:59633ms step_avg:57.95ms
step:1030/2330 train_time:59693ms step_avg:57.95ms
step:1031/2330 train_time:59749ms step_avg:57.95ms
step:1032/2330 train_time:59809ms step_avg:57.95ms
step:1033/2330 train_time:59866ms step_avg:57.95ms
step:1034/2330 train_time:59926ms step_avg:57.96ms
step:1035/2330 train_time:59984ms step_avg:57.96ms
step:1036/2330 train_time:60044ms step_avg:57.96ms
step:1037/2330 train_time:60102ms step_avg:57.96ms
step:1038/2330 train_time:60162ms step_avg:57.96ms
step:1039/2330 train_time:60219ms step_avg:57.96ms
step:1040/2330 train_time:60280ms step_avg:57.96ms
step:1041/2330 train_time:60337ms step_avg:57.96ms
step:1042/2330 train_time:60398ms step_avg:57.96ms
step:1043/2330 train_time:60454ms step_avg:57.96ms
step:1044/2330 train_time:60515ms step_avg:57.96ms
step:1045/2330 train_time:60571ms step_avg:57.96ms
step:1046/2330 train_time:60631ms step_avg:57.96ms
step:1047/2330 train_time:60688ms step_avg:57.96ms
step:1048/2330 train_time:60748ms step_avg:57.97ms
step:1049/2330 train_time:60804ms step_avg:57.96ms
step:1050/2330 train_time:60864ms step_avg:57.97ms
step:1051/2330 train_time:60921ms step_avg:57.97ms
step:1052/2330 train_time:60981ms step_avg:57.97ms
step:1053/2330 train_time:61039ms step_avg:57.97ms
step:1054/2330 train_time:61099ms step_avg:57.97ms
step:1055/2330 train_time:61156ms step_avg:57.97ms
step:1056/2330 train_time:61217ms step_avg:57.97ms
step:1057/2330 train_time:61274ms step_avg:57.97ms
step:1058/2330 train_time:61336ms step_avg:57.97ms
step:1059/2330 train_time:61392ms step_avg:57.97ms
step:1060/2330 train_time:61453ms step_avg:57.97ms
step:1061/2330 train_time:61510ms step_avg:57.97ms
step:1062/2330 train_time:61571ms step_avg:57.98ms
step:1063/2330 train_time:61627ms step_avg:57.97ms
step:1064/2330 train_time:61687ms step_avg:57.98ms
step:1065/2330 train_time:61744ms step_avg:57.98ms
step:1066/2330 train_time:61804ms step_avg:57.98ms
step:1067/2330 train_time:61861ms step_avg:57.98ms
step:1068/2330 train_time:61920ms step_avg:57.98ms
step:1069/2330 train_time:61978ms step_avg:57.98ms
step:1070/2330 train_time:62038ms step_avg:57.98ms
step:1071/2330 train_time:62095ms step_avg:57.98ms
step:1072/2330 train_time:62155ms step_avg:57.98ms
step:1073/2330 train_time:62212ms step_avg:57.98ms
step:1074/2330 train_time:62273ms step_avg:57.98ms
step:1075/2330 train_time:62331ms step_avg:57.98ms
step:1076/2330 train_time:62391ms step_avg:57.98ms
step:1077/2330 train_time:62448ms step_avg:57.98ms
step:1078/2330 train_time:62507ms step_avg:57.98ms
step:1079/2330 train_time:62563ms step_avg:57.98ms
step:1080/2330 train_time:62623ms step_avg:57.98ms
step:1081/2330 train_time:62680ms step_avg:57.98ms
step:1082/2330 train_time:62740ms step_avg:57.98ms
step:1083/2330 train_time:62797ms step_avg:57.98ms
step:1084/2330 train_time:62857ms step_avg:57.99ms
step:1085/2330 train_time:62914ms step_avg:57.99ms
step:1086/2330 train_time:62974ms step_avg:57.99ms
step:1087/2330 train_time:63031ms step_avg:57.99ms
step:1088/2330 train_time:63092ms step_avg:57.99ms
step:1089/2330 train_time:63149ms step_avg:57.99ms
step:1090/2330 train_time:63208ms step_avg:57.99ms
step:1091/2330 train_time:63265ms step_avg:57.99ms
step:1092/2330 train_time:63326ms step_avg:57.99ms
step:1093/2330 train_time:63383ms step_avg:57.99ms
step:1094/2330 train_time:63443ms step_avg:57.99ms
step:1095/2330 train_time:63500ms step_avg:57.99ms
step:1096/2330 train_time:63560ms step_avg:57.99ms
step:1097/2330 train_time:63617ms step_avg:57.99ms
step:1098/2330 train_time:63678ms step_avg:57.99ms
step:1099/2330 train_time:63734ms step_avg:57.99ms
step:1100/2330 train_time:63795ms step_avg:58.00ms
step:1101/2330 train_time:63851ms step_avg:57.99ms
step:1102/2330 train_time:63912ms step_avg:58.00ms
step:1103/2330 train_time:63969ms step_avg:58.00ms
step:1104/2330 train_time:64029ms step_avg:58.00ms
step:1105/2330 train_time:64086ms step_avg:58.00ms
step:1106/2330 train_time:64146ms step_avg:58.00ms
step:1107/2330 train_time:64203ms step_avg:58.00ms
step:1108/2330 train_time:64263ms step_avg:58.00ms
step:1109/2330 train_time:64320ms step_avg:58.00ms
step:1110/2330 train_time:64381ms step_avg:58.00ms
step:1111/2330 train_time:64438ms step_avg:58.00ms
step:1112/2330 train_time:64498ms step_avg:58.00ms
step:1113/2330 train_time:64555ms step_avg:58.00ms
step:1114/2330 train_time:64615ms step_avg:58.00ms
step:1115/2330 train_time:64671ms step_avg:58.00ms
step:1116/2330 train_time:64731ms step_avg:58.00ms
step:1117/2330 train_time:64789ms step_avg:58.00ms
step:1118/2330 train_time:64848ms step_avg:58.00ms
step:1119/2330 train_time:64905ms step_avg:58.00ms
step:1120/2330 train_time:64965ms step_avg:58.00ms
step:1121/2330 train_time:65023ms step_avg:58.00ms
step:1122/2330 train_time:65083ms step_avg:58.01ms
step:1123/2330 train_time:65140ms step_avg:58.01ms
step:1124/2330 train_time:65201ms step_avg:58.01ms
step:1125/2330 train_time:65258ms step_avg:58.01ms
step:1126/2330 train_time:65318ms step_avg:58.01ms
step:1127/2330 train_time:65375ms step_avg:58.01ms
step:1128/2330 train_time:65436ms step_avg:58.01ms
step:1129/2330 train_time:65492ms step_avg:58.01ms
step:1130/2330 train_time:65553ms step_avg:58.01ms
step:1131/2330 train_time:65609ms step_avg:58.01ms
step:1132/2330 train_time:65670ms step_avg:58.01ms
step:1133/2330 train_time:65727ms step_avg:58.01ms
step:1134/2330 train_time:65787ms step_avg:58.01ms
step:1135/2330 train_time:65845ms step_avg:58.01ms
step:1136/2330 train_time:65905ms step_avg:58.01ms
step:1137/2330 train_time:65961ms step_avg:58.01ms
step:1138/2330 train_time:66021ms step_avg:58.01ms
step:1139/2330 train_time:66078ms step_avg:58.01ms
step:1140/2330 train_time:66138ms step_avg:58.02ms
step:1141/2330 train_time:66195ms step_avg:58.02ms
step:1142/2330 train_time:66256ms step_avg:58.02ms
step:1143/2330 train_time:66313ms step_avg:58.02ms
step:1144/2330 train_time:66374ms step_avg:58.02ms
step:1145/2330 train_time:66431ms step_avg:58.02ms
step:1146/2330 train_time:66491ms step_avg:58.02ms
step:1147/2330 train_time:66547ms step_avg:58.02ms
step:1148/2330 train_time:66608ms step_avg:58.02ms
step:1149/2330 train_time:66665ms step_avg:58.02ms
step:1150/2330 train_time:66725ms step_avg:58.02ms
step:1151/2330 train_time:66782ms step_avg:58.02ms
step:1152/2330 train_time:66841ms step_avg:58.02ms
step:1153/2330 train_time:66898ms step_avg:58.02ms
step:1154/2330 train_time:66958ms step_avg:58.02ms
step:1155/2330 train_time:67015ms step_avg:58.02ms
step:1156/2330 train_time:67075ms step_avg:58.02ms
step:1157/2330 train_time:67132ms step_avg:58.02ms
step:1158/2330 train_time:67194ms step_avg:58.03ms
step:1159/2330 train_time:67250ms step_avg:58.02ms
step:1160/2330 train_time:67310ms step_avg:58.03ms
step:1161/2330 train_time:67367ms step_avg:58.03ms
step:1162/2330 train_time:67429ms step_avg:58.03ms
step:1163/2330 train_time:67485ms step_avg:58.03ms
step:1164/2330 train_time:67545ms step_avg:58.03ms
step:1165/2330 train_time:67602ms step_avg:58.03ms
step:1166/2330 train_time:67662ms step_avg:58.03ms
step:1167/2330 train_time:67719ms step_avg:58.03ms
step:1168/2330 train_time:67780ms step_avg:58.03ms
step:1169/2330 train_time:67836ms step_avg:58.03ms
step:1170/2330 train_time:67897ms step_avg:58.03ms
step:1171/2330 train_time:67954ms step_avg:58.03ms
step:1172/2330 train_time:68014ms step_avg:58.03ms
step:1173/2330 train_time:68071ms step_avg:58.03ms
step:1174/2330 train_time:68132ms step_avg:58.03ms
step:1175/2330 train_time:68189ms step_avg:58.03ms
step:1176/2330 train_time:68249ms step_avg:58.03ms
step:1177/2330 train_time:68307ms step_avg:58.03ms
step:1178/2330 train_time:68367ms step_avg:58.04ms
step:1179/2330 train_time:68424ms step_avg:58.04ms
step:1180/2330 train_time:68483ms step_avg:58.04ms
step:1181/2330 train_time:68539ms step_avg:58.03ms
step:1182/2330 train_time:68600ms step_avg:58.04ms
step:1183/2330 train_time:68656ms step_avg:58.04ms
step:1184/2330 train_time:68718ms step_avg:58.04ms
step:1185/2330 train_time:68774ms step_avg:58.04ms
step:1186/2330 train_time:68835ms step_avg:58.04ms
step:1187/2330 train_time:68891ms step_avg:58.04ms
step:1188/2330 train_time:68952ms step_avg:58.04ms
step:1189/2330 train_time:69008ms step_avg:58.04ms
step:1190/2330 train_time:69068ms step_avg:58.04ms
step:1191/2330 train_time:69125ms step_avg:58.04ms
step:1192/2330 train_time:69185ms step_avg:58.04ms
step:1193/2330 train_time:69243ms step_avg:58.04ms
step:1194/2330 train_time:69302ms step_avg:58.04ms
step:1195/2330 train_time:69360ms step_avg:58.04ms
step:1196/2330 train_time:69420ms step_avg:58.04ms
step:1197/2330 train_time:69476ms step_avg:58.04ms
step:1198/2330 train_time:69538ms step_avg:58.04ms
step:1199/2330 train_time:69594ms step_avg:58.04ms
step:1200/2330 train_time:69655ms step_avg:58.05ms
step:1201/2330 train_time:69711ms step_avg:58.04ms
step:1202/2330 train_time:69773ms step_avg:58.05ms
step:1203/2330 train_time:69830ms step_avg:58.05ms
step:1204/2330 train_time:69889ms step_avg:58.05ms
step:1205/2330 train_time:69946ms step_avg:58.05ms
step:1206/2330 train_time:70006ms step_avg:58.05ms
step:1207/2330 train_time:70063ms step_avg:58.05ms
step:1208/2330 train_time:70122ms step_avg:58.05ms
step:1209/2330 train_time:70179ms step_avg:58.05ms
step:1210/2330 train_time:70240ms step_avg:58.05ms
step:1211/2330 train_time:70296ms step_avg:58.05ms
step:1212/2330 train_time:70356ms step_avg:58.05ms
step:1213/2330 train_time:70414ms step_avg:58.05ms
step:1214/2330 train_time:70474ms step_avg:58.05ms
step:1215/2330 train_time:70531ms step_avg:58.05ms
step:1216/2330 train_time:70592ms step_avg:58.05ms
step:1217/2330 train_time:70649ms step_avg:58.05ms
step:1218/2330 train_time:70710ms step_avg:58.05ms
step:1219/2330 train_time:70767ms step_avg:58.05ms
step:1220/2330 train_time:70827ms step_avg:58.05ms
step:1221/2330 train_time:70883ms step_avg:58.05ms
step:1222/2330 train_time:70943ms step_avg:58.05ms
step:1223/2330 train_time:71000ms step_avg:58.05ms
step:1224/2330 train_time:71060ms step_avg:58.06ms
step:1225/2330 train_time:71117ms step_avg:58.05ms
step:1226/2330 train_time:71177ms step_avg:58.06ms
step:1227/2330 train_time:71234ms step_avg:58.06ms
step:1228/2330 train_time:71295ms step_avg:58.06ms
step:1229/2330 train_time:71352ms step_avg:58.06ms
step:1230/2330 train_time:71413ms step_avg:58.06ms
step:1231/2330 train_time:71469ms step_avg:58.06ms
step:1232/2330 train_time:71530ms step_avg:58.06ms
step:1233/2330 train_time:71587ms step_avg:58.06ms
step:1234/2330 train_time:71647ms step_avg:58.06ms
step:1235/2330 train_time:71704ms step_avg:58.06ms
step:1236/2330 train_time:71764ms step_avg:58.06ms
step:1237/2330 train_time:71821ms step_avg:58.06ms
step:1238/2330 train_time:71881ms step_avg:58.06ms
step:1239/2330 train_time:71939ms step_avg:58.06ms
step:1240/2330 train_time:71998ms step_avg:58.06ms
step:1241/2330 train_time:72055ms step_avg:58.06ms
step:1242/2330 train_time:72115ms step_avg:58.06ms
step:1243/2330 train_time:72172ms step_avg:58.06ms
step:1244/2330 train_time:72233ms step_avg:58.07ms
step:1245/2330 train_time:72290ms step_avg:58.06ms
step:1246/2330 train_time:72350ms step_avg:58.07ms
step:1247/2330 train_time:72407ms step_avg:58.07ms
step:1248/2330 train_time:72467ms step_avg:58.07ms
step:1249/2330 train_time:72524ms step_avg:58.07ms
step:1250/2330 train_time:72583ms step_avg:58.07ms
step:1250/2330 val_loss:3.9955 train_time:72664ms step_avg:58.13ms
step:1251/2330 train_time:72683ms step_avg:58.10ms
step:1252/2330 train_time:72703ms step_avg:58.07ms
step:1253/2330 train_time:72760ms step_avg:58.07ms
step:1254/2330 train_time:72827ms step_avg:58.08ms
step:1255/2330 train_time:72884ms step_avg:58.07ms
step:1256/2330 train_time:72946ms step_avg:58.08ms
step:1257/2330 train_time:73004ms step_avg:58.08ms
step:1258/2330 train_time:73064ms step_avg:58.08ms
step:1259/2330 train_time:73120ms step_avg:58.08ms
step:1260/2330 train_time:73180ms step_avg:58.08ms
step:1261/2330 train_time:73236ms step_avg:58.08ms
step:1262/2330 train_time:73295ms step_avg:58.08ms
step:1263/2330 train_time:73352ms step_avg:58.08ms
step:1264/2330 train_time:73411ms step_avg:58.08ms
step:1265/2330 train_time:73468ms step_avg:58.08ms
step:1266/2330 train_time:73526ms step_avg:58.08ms
step:1267/2330 train_time:73583ms step_avg:58.08ms
step:1268/2330 train_time:73645ms step_avg:58.08ms
step:1269/2330 train_time:73703ms step_avg:58.08ms
step:1270/2330 train_time:73769ms step_avg:58.09ms
step:1271/2330 train_time:73826ms step_avg:58.08ms
step:1272/2330 train_time:73887ms step_avg:58.09ms
step:1273/2330 train_time:73944ms step_avg:58.09ms
step:1274/2330 train_time:74006ms step_avg:58.09ms
step:1275/2330 train_time:74062ms step_avg:58.09ms
step:1276/2330 train_time:74124ms step_avg:58.09ms
step:1277/2330 train_time:74180ms step_avg:58.09ms
step:1278/2330 train_time:74242ms step_avg:58.09ms
step:1279/2330 train_time:74298ms step_avg:58.09ms
step:1280/2330 train_time:74358ms step_avg:58.09ms
step:1281/2330 train_time:74415ms step_avg:58.09ms
step:1282/2330 train_time:74474ms step_avg:58.09ms
step:1283/2330 train_time:74531ms step_avg:58.09ms
step:1284/2330 train_time:74590ms step_avg:58.09ms
step:1285/2330 train_time:74647ms step_avg:58.09ms
step:1286/2330 train_time:74708ms step_avg:58.09ms
step:1287/2330 train_time:74766ms step_avg:58.09ms
step:1288/2330 train_time:74827ms step_avg:58.10ms
step:1289/2330 train_time:74884ms step_avg:58.09ms
step:1290/2330 train_time:74947ms step_avg:58.10ms
step:1291/2330 train_time:75005ms step_avg:58.10ms
step:1292/2330 train_time:75681ms step_avg:58.58ms
step:1293/2330 train_time:75737ms step_avg:58.57ms
step:1294/2330 train_time:75824ms step_avg:58.60ms
step:1295/2330 train_time:75880ms step_avg:58.59ms
step:1296/2330 train_time:75939ms step_avg:58.59ms
step:1297/2330 train_time:75995ms step_avg:58.59ms
step:1298/2330 train_time:76054ms step_avg:58.59ms
step:1299/2330 train_time:76110ms step_avg:58.59ms
step:1300/2330 train_time:76170ms step_avg:58.59ms
step:1301/2330 train_time:76226ms step_avg:58.59ms
step:1302/2330 train_time:76285ms step_avg:58.59ms
step:1303/2330 train_time:76342ms step_avg:58.59ms
step:1304/2330 train_time:76401ms step_avg:58.59ms
step:1305/2330 train_time:76458ms step_avg:58.59ms
step:1306/2330 train_time:76517ms step_avg:58.59ms
step:1307/2330 train_time:76576ms step_avg:58.59ms
step:1308/2330 train_time:76637ms step_avg:58.59ms
step:1309/2330 train_time:76695ms step_avg:58.59ms
step:1310/2330 train_time:76760ms step_avg:58.60ms
step:1311/2330 train_time:76819ms step_avg:58.60ms
step:1312/2330 train_time:76878ms step_avg:58.60ms
step:1313/2330 train_time:76935ms step_avg:58.59ms
step:1314/2330 train_time:76994ms step_avg:58.60ms
step:1315/2330 train_time:77050ms step_avg:58.59ms
step:1316/2330 train_time:77110ms step_avg:58.59ms
step:1317/2330 train_time:77167ms step_avg:58.59ms
step:1318/2330 train_time:77225ms step_avg:58.59ms
step:1319/2330 train_time:77281ms step_avg:58.59ms
step:1320/2330 train_time:77341ms step_avg:58.59ms
step:1321/2330 train_time:77398ms step_avg:58.59ms
step:1322/2330 train_time:77457ms step_avg:58.59ms
step:1323/2330 train_time:77515ms step_avg:58.59ms
step:1324/2330 train_time:77575ms step_avg:58.59ms
step:1325/2330 train_time:77633ms step_avg:58.59ms
step:1326/2330 train_time:77694ms step_avg:58.59ms
step:1327/2330 train_time:77753ms step_avg:58.59ms
step:1328/2330 train_time:77813ms step_avg:58.59ms
step:1329/2330 train_time:77872ms step_avg:58.59ms
step:1330/2330 train_time:77931ms step_avg:58.59ms
step:1331/2330 train_time:77987ms step_avg:58.59ms
step:1332/2330 train_time:78048ms step_avg:58.59ms
step:1333/2330 train_time:78105ms step_avg:58.59ms
step:1334/2330 train_time:78165ms step_avg:58.59ms
step:1335/2330 train_time:78221ms step_avg:58.59ms
step:1336/2330 train_time:78281ms step_avg:58.59ms
step:1337/2330 train_time:78337ms step_avg:58.59ms
step:1338/2330 train_time:78397ms step_avg:58.59ms
step:1339/2330 train_time:78453ms step_avg:58.59ms
step:1340/2330 train_time:78513ms step_avg:58.59ms
step:1341/2330 train_time:78570ms step_avg:58.59ms
step:1342/2330 train_time:78630ms step_avg:58.59ms
step:1343/2330 train_time:78688ms step_avg:58.59ms
step:1344/2330 train_time:78749ms step_avg:58.59ms
step:1345/2330 train_time:78806ms step_avg:58.59ms
step:1346/2330 train_time:78868ms step_avg:58.59ms
step:1347/2330 train_time:78925ms step_avg:58.59ms
step:1348/2330 train_time:78986ms step_avg:58.60ms
step:1349/2330 train_time:79043ms step_avg:58.59ms
step:1350/2330 train_time:79104ms step_avg:58.60ms
step:1351/2330 train_time:79161ms step_avg:58.59ms
step:1352/2330 train_time:79220ms step_avg:58.59ms
step:1353/2330 train_time:79276ms step_avg:58.59ms
step:1354/2330 train_time:79336ms step_avg:58.59ms
step:1355/2330 train_time:79393ms step_avg:58.59ms
step:1356/2330 train_time:79452ms step_avg:58.59ms
step:1357/2330 train_time:79509ms step_avg:58.59ms
step:1358/2330 train_time:79569ms step_avg:58.59ms
step:1359/2330 train_time:79626ms step_avg:58.59ms
step:1360/2330 train_time:79687ms step_avg:58.59ms
step:1361/2330 train_time:79745ms step_avg:58.59ms
step:1362/2330 train_time:79805ms step_avg:58.59ms
step:1363/2330 train_time:79862ms step_avg:58.59ms
step:1364/2330 train_time:79924ms step_avg:58.60ms
step:1365/2330 train_time:79981ms step_avg:58.59ms
step:1366/2330 train_time:80042ms step_avg:58.60ms
step:1367/2330 train_time:80099ms step_avg:58.59ms
step:1368/2330 train_time:80159ms step_avg:58.60ms
step:1369/2330 train_time:80216ms step_avg:58.59ms
step:1370/2330 train_time:80276ms step_avg:58.60ms
step:1371/2330 train_time:80332ms step_avg:58.59ms
step:1372/2330 train_time:80391ms step_avg:58.59ms
step:1373/2330 train_time:80449ms step_avg:58.59ms
step:1374/2330 train_time:80508ms step_avg:58.59ms
step:1375/2330 train_time:80565ms step_avg:58.59ms
step:1376/2330 train_time:80626ms step_avg:58.59ms
step:1377/2330 train_time:80682ms step_avg:58.59ms
step:1378/2330 train_time:80743ms step_avg:58.59ms
step:1379/2330 train_time:80800ms step_avg:58.59ms
step:1380/2330 train_time:80861ms step_avg:58.59ms
step:1381/2330 train_time:80919ms step_avg:58.59ms
step:1382/2330 train_time:80978ms step_avg:58.60ms
step:1383/2330 train_time:81036ms step_avg:58.59ms
step:1384/2330 train_time:81096ms step_avg:58.60ms
step:1385/2330 train_time:81153ms step_avg:58.59ms
step:1386/2330 train_time:81212ms step_avg:58.59ms
step:1387/2330 train_time:81269ms step_avg:58.59ms
step:1388/2330 train_time:81329ms step_avg:58.59ms
step:1389/2330 train_time:81386ms step_avg:58.59ms
step:1390/2330 train_time:81446ms step_avg:58.59ms
step:1391/2330 train_time:81502ms step_avg:58.59ms
step:1392/2330 train_time:81563ms step_avg:58.59ms
step:1393/2330 train_time:81621ms step_avg:58.59ms
step:1394/2330 train_time:81680ms step_avg:58.59ms
step:1395/2330 train_time:81738ms step_avg:58.59ms
step:1396/2330 train_time:81798ms step_avg:58.59ms
step:1397/2330 train_time:81855ms step_avg:58.59ms
step:1398/2330 train_time:81915ms step_avg:58.59ms
step:1399/2330 train_time:81972ms step_avg:58.59ms
step:1400/2330 train_time:82032ms step_avg:58.59ms
step:1401/2330 train_time:82088ms step_avg:58.59ms
step:1402/2330 train_time:82150ms step_avg:58.59ms
step:1403/2330 train_time:82206ms step_avg:58.59ms
step:1404/2330 train_time:82266ms step_avg:58.59ms
step:1405/2330 train_time:82323ms step_avg:58.59ms
step:1406/2330 train_time:82382ms step_avg:58.59ms
step:1407/2330 train_time:82439ms step_avg:58.59ms
step:1408/2330 train_time:82499ms step_avg:58.59ms
step:1409/2330 train_time:82557ms step_avg:58.59ms
step:1410/2330 train_time:82616ms step_avg:58.59ms
step:1411/2330 train_time:82674ms step_avg:58.59ms
step:1412/2330 train_time:82734ms step_avg:58.59ms
step:1413/2330 train_time:82791ms step_avg:58.59ms
step:1414/2330 train_time:82851ms step_avg:58.59ms
step:1415/2330 train_time:82908ms step_avg:58.59ms
step:1416/2330 train_time:82968ms step_avg:58.59ms
step:1417/2330 train_time:83025ms step_avg:58.59ms
step:1418/2330 train_time:83086ms step_avg:58.59ms
step:1419/2330 train_time:83142ms step_avg:58.59ms
step:1420/2330 train_time:83204ms step_avg:58.59ms
step:1421/2330 train_time:83261ms step_avg:58.59ms
step:1422/2330 train_time:83321ms step_avg:58.59ms
step:1423/2330 train_time:83378ms step_avg:58.59ms
step:1424/2330 train_time:83438ms step_avg:58.59ms
step:1425/2330 train_time:83494ms step_avg:58.59ms
step:1426/2330 train_time:83554ms step_avg:58.59ms
step:1427/2330 train_time:83611ms step_avg:58.59ms
step:1428/2330 train_time:83671ms step_avg:58.59ms
step:1429/2330 train_time:83728ms step_avg:58.59ms
step:1430/2330 train_time:83789ms step_avg:58.59ms
step:1431/2330 train_time:83846ms step_avg:58.59ms
step:1432/2330 train_time:83907ms step_avg:58.59ms
step:1433/2330 train_time:83964ms step_avg:58.59ms
step:1434/2330 train_time:84025ms step_avg:58.59ms
step:1435/2330 train_time:84081ms step_avg:58.59ms
step:1436/2330 train_time:84143ms step_avg:58.60ms
step:1437/2330 train_time:84200ms step_avg:58.59ms
step:1438/2330 train_time:84259ms step_avg:58.59ms
step:1439/2330 train_time:84316ms step_avg:58.59ms
step:1440/2330 train_time:84375ms step_avg:58.59ms
step:1441/2330 train_time:84433ms step_avg:58.59ms
step:1442/2330 train_time:84492ms step_avg:58.59ms
step:1443/2330 train_time:84548ms step_avg:58.59ms
step:1444/2330 train_time:84609ms step_avg:58.59ms
step:1445/2330 train_time:84665ms step_avg:58.59ms
step:1446/2330 train_time:84726ms step_avg:58.59ms
step:1447/2330 train_time:84782ms step_avg:58.59ms
step:1448/2330 train_time:84843ms step_avg:58.59ms
step:1449/2330 train_time:84901ms step_avg:58.59ms
step:1450/2330 train_time:84961ms step_avg:58.59ms
step:1451/2330 train_time:85018ms step_avg:58.59ms
step:1452/2330 train_time:85078ms step_avg:58.59ms
step:1453/2330 train_time:85136ms step_avg:58.59ms
step:1454/2330 train_time:85195ms step_avg:58.59ms
step:1455/2330 train_time:85252ms step_avg:58.59ms
step:1456/2330 train_time:85312ms step_avg:58.59ms
step:1457/2330 train_time:85370ms step_avg:58.59ms
step:1458/2330 train_time:85429ms step_avg:58.59ms
step:1459/2330 train_time:85486ms step_avg:58.59ms
step:1460/2330 train_time:85547ms step_avg:58.59ms
step:1461/2330 train_time:85604ms step_avg:58.59ms
step:1462/2330 train_time:85663ms step_avg:58.59ms
step:1463/2330 train_time:85720ms step_avg:58.59ms
step:1464/2330 train_time:85780ms step_avg:58.59ms
step:1465/2330 train_time:85838ms step_avg:58.59ms
step:1466/2330 train_time:85898ms step_avg:58.59ms
step:1467/2330 train_time:85955ms step_avg:58.59ms
step:1468/2330 train_time:86015ms step_avg:58.59ms
step:1469/2330 train_time:86072ms step_avg:58.59ms
step:1470/2330 train_time:86132ms step_avg:58.59ms
step:1471/2330 train_time:86189ms step_avg:58.59ms
step:1472/2330 train_time:86249ms step_avg:58.59ms
step:1473/2330 train_time:86307ms step_avg:58.59ms
step:1474/2330 train_time:86366ms step_avg:58.59ms
step:1475/2330 train_time:86423ms step_avg:58.59ms
step:1476/2330 train_time:86484ms step_avg:58.59ms
step:1477/2330 train_time:86540ms step_avg:58.59ms
step:1478/2330 train_time:86601ms step_avg:58.59ms
step:1479/2330 train_time:86659ms step_avg:58.59ms
step:1480/2330 train_time:86718ms step_avg:58.59ms
step:1481/2330 train_time:86775ms step_avg:58.59ms
step:1482/2330 train_time:86834ms step_avg:58.59ms
step:1483/2330 train_time:86891ms step_avg:58.59ms
step:1484/2330 train_time:86951ms step_avg:58.59ms
step:1485/2330 train_time:87007ms step_avg:58.59ms
step:1486/2330 train_time:87069ms step_avg:58.59ms
step:1487/2330 train_time:87125ms step_avg:58.59ms
step:1488/2330 train_time:87187ms step_avg:58.59ms
step:1489/2330 train_time:87244ms step_avg:58.59ms
step:1490/2330 train_time:87304ms step_avg:58.59ms
step:1491/2330 train_time:87361ms step_avg:58.59ms
step:1492/2330 train_time:87421ms step_avg:58.59ms
step:1493/2330 train_time:87478ms step_avg:58.59ms
step:1494/2330 train_time:87538ms step_avg:58.59ms
step:1495/2330 train_time:87595ms step_avg:58.59ms
step:1496/2330 train_time:87655ms step_avg:58.59ms
step:1497/2330 train_time:87711ms step_avg:58.59ms
step:1498/2330 train_time:87772ms step_avg:58.59ms
step:1499/2330 train_time:87829ms step_avg:58.59ms
step:1500/2330 train_time:87889ms step_avg:58.59ms
step:1500/2330 val_loss:3.9149 train_time:87970ms step_avg:58.65ms
step:1501/2330 train_time:87988ms step_avg:58.62ms
step:1502/2330 train_time:88009ms step_avg:58.59ms
step:1503/2330 train_time:88070ms step_avg:58.60ms
step:1504/2330 train_time:88134ms step_avg:58.60ms
step:1505/2330 train_time:88192ms step_avg:58.60ms
step:1506/2330 train_time:88251ms step_avg:58.60ms
step:1507/2330 train_time:88308ms step_avg:58.60ms
step:1508/2330 train_time:88369ms step_avg:58.60ms
step:1509/2330 train_time:88425ms step_avg:58.60ms
step:1510/2330 train_time:88485ms step_avg:58.60ms
step:1511/2330 train_time:88541ms step_avg:58.60ms
step:1512/2330 train_time:88600ms step_avg:58.60ms
step:1513/2330 train_time:88657ms step_avg:58.60ms
step:1514/2330 train_time:88715ms step_avg:58.60ms
step:1515/2330 train_time:88771ms step_avg:58.59ms
step:1516/2330 train_time:88832ms step_avg:58.60ms
step:1517/2330 train_time:88888ms step_avg:58.59ms
step:1518/2330 train_time:88949ms step_avg:58.60ms
step:1519/2330 train_time:89008ms step_avg:58.60ms
step:1520/2330 train_time:89070ms step_avg:58.60ms
step:1521/2330 train_time:89128ms step_avg:58.60ms
step:1522/2330 train_time:89189ms step_avg:58.60ms
step:1523/2330 train_time:89246ms step_avg:58.60ms
step:1524/2330 train_time:89306ms step_avg:58.60ms
step:1525/2330 train_time:89362ms step_avg:58.60ms
step:1526/2330 train_time:89422ms step_avg:58.60ms
step:1527/2330 train_time:89478ms step_avg:58.60ms
step:1528/2330 train_time:89538ms step_avg:58.60ms
step:1529/2330 train_time:89596ms step_avg:58.60ms
step:1530/2330 train_time:89654ms step_avg:58.60ms
step:1531/2330 train_time:89711ms step_avg:58.60ms
step:1532/2330 train_time:89770ms step_avg:58.60ms
step:1533/2330 train_time:89828ms step_avg:58.60ms
step:1534/2330 train_time:89888ms step_avg:58.60ms
step:1535/2330 train_time:89945ms step_avg:58.60ms
step:1536/2330 train_time:90005ms step_avg:58.60ms
step:1537/2330 train_time:90065ms step_avg:58.60ms
step:1538/2330 train_time:90126ms step_avg:58.60ms
step:1539/2330 train_time:90184ms step_avg:58.60ms
step:1540/2330 train_time:90245ms step_avg:58.60ms
step:1541/2330 train_time:90302ms step_avg:58.60ms
step:1542/2330 train_time:90363ms step_avg:58.60ms
step:1543/2330 train_time:90419ms step_avg:58.60ms
step:1544/2330 train_time:90480ms step_avg:58.60ms
step:1545/2330 train_time:90537ms step_avg:58.60ms
step:1546/2330 train_time:90598ms step_avg:58.60ms
step:1547/2330 train_time:90654ms step_avg:58.60ms
step:1548/2330 train_time:90714ms step_avg:58.60ms
step:1549/2330 train_time:90771ms step_avg:58.60ms
step:1550/2330 train_time:90833ms step_avg:58.60ms
step:1551/2330 train_time:90890ms step_avg:58.60ms
step:1552/2330 train_time:90950ms step_avg:58.60ms
step:1553/2330 train_time:91008ms step_avg:58.60ms
step:1554/2330 train_time:91068ms step_avg:58.60ms
step:1555/2330 train_time:91127ms step_avg:58.60ms
step:1556/2330 train_time:91188ms step_avg:58.60ms
step:1557/2330 train_time:91247ms step_avg:58.60ms
step:1558/2330 train_time:91307ms step_avg:58.61ms
step:1559/2330 train_time:91365ms step_avg:58.60ms
step:1560/2330 train_time:91426ms step_avg:58.61ms
step:1561/2330 train_time:91484ms step_avg:58.61ms
step:1562/2330 train_time:91545ms step_avg:58.61ms
step:1563/2330 train_time:91603ms step_avg:58.61ms
step:1564/2330 train_time:91662ms step_avg:58.61ms
step:1565/2330 train_time:91719ms step_avg:58.61ms
step:1566/2330 train_time:91779ms step_avg:58.61ms
step:1567/2330 train_time:91837ms step_avg:58.61ms
step:1568/2330 train_time:91897ms step_avg:58.61ms
step:1569/2330 train_time:91953ms step_avg:58.61ms
step:1570/2330 train_time:92016ms step_avg:58.61ms
step:1571/2330 train_time:92073ms step_avg:58.61ms
step:1572/2330 train_time:92137ms step_avg:58.61ms
step:1573/2330 train_time:92194ms step_avg:58.61ms
step:1574/2330 train_time:92255ms step_avg:58.61ms
step:1575/2330 train_time:92312ms step_avg:58.61ms
step:1576/2330 train_time:92374ms step_avg:58.61ms
step:1577/2330 train_time:92432ms step_avg:58.61ms
step:1578/2330 train_time:92493ms step_avg:58.61ms
step:1579/2330 train_time:92550ms step_avg:58.61ms
step:1580/2330 train_time:92610ms step_avg:58.61ms
step:1581/2330 train_time:92668ms step_avg:58.61ms
step:1582/2330 train_time:92727ms step_avg:58.61ms
step:1583/2330 train_time:92785ms step_avg:58.61ms
step:1584/2330 train_time:92846ms step_avg:58.62ms
step:1585/2330 train_time:92904ms step_avg:58.61ms
step:1586/2330 train_time:92964ms step_avg:58.62ms
step:1587/2330 train_time:93021ms step_avg:58.61ms
step:1588/2330 train_time:93082ms step_avg:58.62ms
step:1589/2330 train_time:93139ms step_avg:58.62ms
step:1590/2330 train_time:93201ms step_avg:58.62ms
step:1591/2330 train_time:93258ms step_avg:58.62ms
step:1592/2330 train_time:93319ms step_avg:58.62ms
step:1593/2330 train_time:93375ms step_avg:58.62ms
step:1594/2330 train_time:93438ms step_avg:58.62ms
step:1595/2330 train_time:93495ms step_avg:58.62ms
step:1596/2330 train_time:93556ms step_avg:58.62ms
step:1597/2330 train_time:93612ms step_avg:58.62ms
step:1598/2330 train_time:93674ms step_avg:58.62ms
step:1599/2330 train_time:93730ms step_avg:58.62ms
step:1600/2330 train_time:93792ms step_avg:58.62ms
step:1601/2330 train_time:93850ms step_avg:58.62ms
step:1602/2330 train_time:93910ms step_avg:58.62ms
step:1603/2330 train_time:93968ms step_avg:58.62ms
step:1604/2330 train_time:94029ms step_avg:58.62ms
step:1605/2330 train_time:94087ms step_avg:58.62ms
step:1606/2330 train_time:94148ms step_avg:58.62ms
step:1607/2330 train_time:94206ms step_avg:58.62ms
step:1608/2330 train_time:94267ms step_avg:58.62ms
step:1609/2330 train_time:94324ms step_avg:58.62ms
step:1610/2330 train_time:94385ms step_avg:58.62ms
step:1611/2330 train_time:94443ms step_avg:58.62ms
step:1612/2330 train_time:94503ms step_avg:58.62ms
step:1613/2330 train_time:94560ms step_avg:58.62ms
step:1614/2330 train_time:94621ms step_avg:58.63ms
step:1615/2330 train_time:94678ms step_avg:58.62ms
step:1616/2330 train_time:94740ms step_avg:58.63ms
step:1617/2330 train_time:94797ms step_avg:58.63ms
step:1618/2330 train_time:94859ms step_avg:58.63ms
step:1619/2330 train_time:94915ms step_avg:58.63ms
step:1620/2330 train_time:94977ms step_avg:58.63ms
step:1621/2330 train_time:95034ms step_avg:58.63ms
step:1622/2330 train_time:95096ms step_avg:58.63ms
step:1623/2330 train_time:95153ms step_avg:58.63ms
step:1624/2330 train_time:95215ms step_avg:58.63ms
step:1625/2330 train_time:95273ms step_avg:58.63ms
step:1626/2330 train_time:95334ms step_avg:58.63ms
step:1627/2330 train_time:95392ms step_avg:58.63ms
step:1628/2330 train_time:95452ms step_avg:58.63ms
step:1629/2330 train_time:95510ms step_avg:58.63ms
step:1630/2330 train_time:95569ms step_avg:58.63ms
step:1631/2330 train_time:95627ms step_avg:58.63ms
step:1632/2330 train_time:95688ms step_avg:58.63ms
step:1633/2330 train_time:95745ms step_avg:58.63ms
step:1634/2330 train_time:95805ms step_avg:58.63ms
step:1635/2330 train_time:95863ms step_avg:58.63ms
step:1636/2330 train_time:95924ms step_avg:58.63ms
step:1637/2330 train_time:95981ms step_avg:58.63ms
step:1638/2330 train_time:96041ms step_avg:58.63ms
step:1639/2330 train_time:96098ms step_avg:58.63ms
step:1640/2330 train_time:96160ms step_avg:58.63ms
step:1641/2330 train_time:96217ms step_avg:58.63ms
step:1642/2330 train_time:96278ms step_avg:58.63ms
step:1643/2330 train_time:96335ms step_avg:58.63ms
step:1644/2330 train_time:96398ms step_avg:58.64ms
step:1645/2330 train_time:96455ms step_avg:58.63ms
step:1646/2330 train_time:96516ms step_avg:58.64ms
step:1647/2330 train_time:96573ms step_avg:58.64ms
step:1648/2330 train_time:96636ms step_avg:58.64ms
step:1649/2330 train_time:96693ms step_avg:58.64ms
step:1650/2330 train_time:96754ms step_avg:58.64ms
step:1651/2330 train_time:96811ms step_avg:58.64ms
step:1652/2330 train_time:96873ms step_avg:58.64ms
step:1653/2330 train_time:96930ms step_avg:58.64ms
step:1654/2330 train_time:96991ms step_avg:58.64ms
step:1655/2330 train_time:97048ms step_avg:58.64ms
step:1656/2330 train_time:97109ms step_avg:58.64ms
step:1657/2330 train_time:97166ms step_avg:58.64ms
step:1658/2330 train_time:97227ms step_avg:58.64ms
step:1659/2330 train_time:97286ms step_avg:58.64ms
step:1660/2330 train_time:97346ms step_avg:58.64ms
step:1661/2330 train_time:97404ms step_avg:58.64ms
step:1662/2330 train_time:97463ms step_avg:58.64ms
step:1663/2330 train_time:97520ms step_avg:58.64ms
step:1664/2330 train_time:97582ms step_avg:58.64ms
step:1665/2330 train_time:97638ms step_avg:58.64ms
step:1666/2330 train_time:97700ms step_avg:58.64ms
step:1667/2330 train_time:97757ms step_avg:58.64ms
step:1668/2330 train_time:97818ms step_avg:58.64ms
step:1669/2330 train_time:97875ms step_avg:58.64ms
step:1670/2330 train_time:97937ms step_avg:58.64ms
step:1671/2330 train_time:97994ms step_avg:58.64ms
step:1672/2330 train_time:98056ms step_avg:58.65ms
step:1673/2330 train_time:98113ms step_avg:58.64ms
step:1674/2330 train_time:98175ms step_avg:58.65ms
step:1675/2330 train_time:98232ms step_avg:58.65ms
step:1676/2330 train_time:98295ms step_avg:58.65ms
step:1677/2330 train_time:98352ms step_avg:58.65ms
step:1678/2330 train_time:98414ms step_avg:58.65ms
step:1679/2330 train_time:98471ms step_avg:58.65ms
step:1680/2330 train_time:98531ms step_avg:58.65ms
step:1681/2330 train_time:98590ms step_avg:58.65ms
step:1682/2330 train_time:98649ms step_avg:58.65ms
step:1683/2330 train_time:98707ms step_avg:58.65ms
step:1684/2330 train_time:98768ms step_avg:58.65ms
step:1685/2330 train_time:98826ms step_avg:58.65ms
step:1686/2330 train_time:98886ms step_avg:58.65ms
step:1687/2330 train_time:98945ms step_avg:58.65ms
step:1688/2330 train_time:99005ms step_avg:58.65ms
step:1689/2330 train_time:99063ms step_avg:58.65ms
step:1690/2330 train_time:99124ms step_avg:58.65ms
step:1691/2330 train_time:99181ms step_avg:58.65ms
step:1692/2330 train_time:99242ms step_avg:58.65ms
step:1693/2330 train_time:99299ms step_avg:58.65ms
step:1694/2330 train_time:99361ms step_avg:58.65ms
step:1695/2330 train_time:99418ms step_avg:58.65ms
step:1696/2330 train_time:99481ms step_avg:58.66ms
step:1697/2330 train_time:99537ms step_avg:58.65ms
step:1698/2330 train_time:99599ms step_avg:58.66ms
step:1699/2330 train_time:99655ms step_avg:58.66ms
step:1700/2330 train_time:99718ms step_avg:58.66ms
step:1701/2330 train_time:99775ms step_avg:58.66ms
step:1702/2330 train_time:99836ms step_avg:58.66ms
step:1703/2330 train_time:99894ms step_avg:58.66ms
step:1704/2330 train_time:99955ms step_avg:58.66ms
step:1705/2330 train_time:100011ms step_avg:58.66ms
step:1706/2330 train_time:100073ms step_avg:58.66ms
step:1707/2330 train_time:100130ms step_avg:58.66ms
step:1708/2330 train_time:100191ms step_avg:58.66ms
step:1709/2330 train_time:100248ms step_avg:58.66ms
step:1710/2330 train_time:100309ms step_avg:58.66ms
step:1711/2330 train_time:100366ms step_avg:58.66ms
step:1712/2330 train_time:100426ms step_avg:58.66ms
step:1713/2330 train_time:100485ms step_avg:58.66ms
step:1714/2330 train_time:100545ms step_avg:58.66ms
step:1715/2330 train_time:100604ms step_avg:58.66ms
step:1716/2330 train_time:100664ms step_avg:58.66ms
step:1717/2330 train_time:100722ms step_avg:58.66ms
step:1718/2330 train_time:100782ms step_avg:58.66ms
step:1719/2330 train_time:100839ms step_avg:58.66ms
step:1720/2330 train_time:100900ms step_avg:58.66ms
step:1721/2330 train_time:100957ms step_avg:58.66ms
step:1722/2330 train_time:101018ms step_avg:58.66ms
step:1723/2330 train_time:101075ms step_avg:58.66ms
step:1724/2330 train_time:101137ms step_avg:58.66ms
step:1725/2330 train_time:101195ms step_avg:58.66ms
step:1726/2330 train_time:101256ms step_avg:58.67ms
step:1727/2330 train_time:101313ms step_avg:58.66ms
step:1728/2330 train_time:101375ms step_avg:58.67ms
step:1729/2330 train_time:101432ms step_avg:58.67ms
step:1730/2330 train_time:101494ms step_avg:58.67ms
step:1731/2330 train_time:101550ms step_avg:58.67ms
step:1732/2330 train_time:101612ms step_avg:58.67ms
step:1733/2330 train_time:101669ms step_avg:58.67ms
step:1734/2330 train_time:101729ms step_avg:58.67ms
step:1735/2330 train_time:101788ms step_avg:58.67ms
step:1736/2330 train_time:101848ms step_avg:58.67ms
step:1737/2330 train_time:101905ms step_avg:58.67ms
step:1738/2330 train_time:101966ms step_avg:58.67ms
step:1739/2330 train_time:102023ms step_avg:58.67ms
step:1740/2330 train_time:102084ms step_avg:58.67ms
step:1741/2330 train_time:102142ms step_avg:58.67ms
step:1742/2330 train_time:102202ms step_avg:58.67ms
step:1743/2330 train_time:102259ms step_avg:58.67ms
step:1744/2330 train_time:102320ms step_avg:58.67ms
step:1745/2330 train_time:102377ms step_avg:58.67ms
step:1746/2330 train_time:102439ms step_avg:58.67ms
step:1747/2330 train_time:102496ms step_avg:58.67ms
step:1748/2330 train_time:102557ms step_avg:58.67ms
step:1749/2330 train_time:102614ms step_avg:58.67ms
step:1750/2330 train_time:102675ms step_avg:58.67ms
step:1750/2330 val_loss:3.8274 train_time:102758ms step_avg:58.72ms
step:1751/2330 train_time:102776ms step_avg:58.70ms
step:1752/2330 train_time:102796ms step_avg:58.67ms
step:1753/2330 train_time:102853ms step_avg:58.67ms
step:1754/2330 train_time:102917ms step_avg:58.68ms
step:1755/2330 train_time:102973ms step_avg:58.67ms
step:1756/2330 train_time:103036ms step_avg:58.68ms
step:1757/2330 train_time:103092ms step_avg:58.68ms
step:1758/2330 train_time:103152ms step_avg:58.68ms
step:1759/2330 train_time:103209ms step_avg:58.67ms
step:1760/2330 train_time:103269ms step_avg:58.68ms
step:1761/2330 train_time:103326ms step_avg:58.67ms
step:1762/2330 train_time:103386ms step_avg:58.68ms
step:1763/2330 train_time:103442ms step_avg:58.67ms
step:1764/2330 train_time:103502ms step_avg:58.67ms
step:1765/2330 train_time:103558ms step_avg:58.67ms
step:1766/2330 train_time:103618ms step_avg:58.67ms
step:1767/2330 train_time:103680ms step_avg:58.68ms
step:1768/2330 train_time:103744ms step_avg:58.68ms
step:1769/2330 train_time:103803ms step_avg:58.68ms
step:1770/2330 train_time:103865ms step_avg:58.68ms
step:1771/2330 train_time:103922ms step_avg:58.68ms
step:1772/2330 train_time:103984ms step_avg:58.68ms
step:1773/2330 train_time:104041ms step_avg:58.68ms
step:1774/2330 train_time:104104ms step_avg:58.68ms
step:1775/2330 train_time:104161ms step_avg:58.68ms
step:1776/2330 train_time:104222ms step_avg:58.68ms
step:1777/2330 train_time:104279ms step_avg:58.68ms
step:1778/2330 train_time:104339ms step_avg:58.68ms
step:1779/2330 train_time:104396ms step_avg:58.68ms
step:1780/2330 train_time:104456ms step_avg:58.68ms
step:1781/2330 train_time:104513ms step_avg:58.68ms
step:1782/2330 train_time:104573ms step_avg:58.68ms
step:1783/2330 train_time:104633ms step_avg:58.68ms
step:1784/2330 train_time:104694ms step_avg:58.69ms
step:1785/2330 train_time:104754ms step_avg:58.69ms
step:1786/2330 train_time:104815ms step_avg:58.69ms
step:1787/2330 train_time:104873ms step_avg:58.69ms
step:1788/2330 train_time:104933ms step_avg:58.69ms
step:1789/2330 train_time:104992ms step_avg:58.69ms
step:1790/2330 train_time:105051ms step_avg:58.69ms
step:1791/2330 train_time:105109ms step_avg:58.69ms
step:1792/2330 train_time:105169ms step_avg:58.69ms
step:1793/2330 train_time:105226ms step_avg:58.69ms
step:1794/2330 train_time:105287ms step_avg:58.69ms
step:1795/2330 train_time:105343ms step_avg:58.69ms
step:1796/2330 train_time:105403ms step_avg:58.69ms
step:1797/2330 train_time:105460ms step_avg:58.69ms
step:1798/2330 train_time:105521ms step_avg:58.69ms
step:1799/2330 train_time:105578ms step_avg:58.69ms
step:1800/2330 train_time:105640ms step_avg:58.69ms
step:1801/2330 train_time:105699ms step_avg:58.69ms
step:1802/2330 train_time:105759ms step_avg:58.69ms
step:1803/2330 train_time:105817ms step_avg:58.69ms
step:1804/2330 train_time:105879ms step_avg:58.69ms
step:1805/2330 train_time:105937ms step_avg:58.69ms
step:1806/2330 train_time:105997ms step_avg:58.69ms
step:1807/2330 train_time:106055ms step_avg:58.69ms
step:1808/2330 train_time:106115ms step_avg:58.69ms
step:1809/2330 train_time:106173ms step_avg:58.69ms
step:1810/2330 train_time:106233ms step_avg:58.69ms
step:1811/2330 train_time:106290ms step_avg:58.69ms
step:1812/2330 train_time:106350ms step_avg:58.69ms
step:1813/2330 train_time:106407ms step_avg:58.69ms
step:1814/2330 train_time:106467ms step_avg:58.69ms
step:1815/2330 train_time:106524ms step_avg:58.69ms
step:1816/2330 train_time:106585ms step_avg:58.69ms
step:1817/2330 train_time:106643ms step_avg:58.69ms
step:1818/2330 train_time:106705ms step_avg:58.69ms
step:1819/2330 train_time:106762ms step_avg:58.69ms
step:1820/2330 train_time:106826ms step_avg:58.70ms
step:1821/2330 train_time:106883ms step_avg:58.69ms
step:1822/2330 train_time:106945ms step_avg:58.70ms
step:1823/2330 train_time:107002ms step_avg:58.70ms
step:1824/2330 train_time:107065ms step_avg:58.70ms
step:1825/2330 train_time:107122ms step_avg:58.70ms
step:1826/2330 train_time:107184ms step_avg:58.70ms
step:1827/2330 train_time:107240ms step_avg:58.70ms
step:1828/2330 train_time:107302ms step_avg:58.70ms
step:1829/2330 train_time:107359ms step_avg:58.70ms
step:1830/2330 train_time:107420ms step_avg:58.70ms
step:1831/2330 train_time:107477ms step_avg:58.70ms
step:1832/2330 train_time:107536ms step_avg:58.70ms
step:1833/2330 train_time:107594ms step_avg:58.70ms
step:1834/2330 train_time:107655ms step_avg:58.70ms
step:1835/2330 train_time:107712ms step_avg:58.70ms
step:1836/2330 train_time:107773ms step_avg:58.70ms
step:1837/2330 train_time:107832ms step_avg:58.70ms
step:1838/2330 train_time:107893ms step_avg:58.70ms
step:1839/2330 train_time:107952ms step_avg:58.70ms
step:1840/2330 train_time:108012ms step_avg:58.70ms
step:1841/2330 train_time:108071ms step_avg:58.70ms
step:1842/2330 train_time:108131ms step_avg:58.70ms
step:1843/2330 train_time:108189ms step_avg:58.70ms
step:1844/2330 train_time:108249ms step_avg:58.70ms
step:1845/2330 train_time:108305ms step_avg:58.70ms
step:1846/2330 train_time:108367ms step_avg:58.70ms
step:1847/2330 train_time:108424ms step_avg:58.70ms
step:1848/2330 train_time:108485ms step_avg:58.70ms
step:1849/2330 train_time:108542ms step_avg:58.70ms
step:1850/2330 train_time:108604ms step_avg:58.70ms
step:1851/2330 train_time:108661ms step_avg:58.70ms
step:1852/2330 train_time:108723ms step_avg:58.71ms
step:1853/2330 train_time:108780ms step_avg:58.70ms
step:1854/2330 train_time:108843ms step_avg:58.71ms
step:1855/2330 train_time:108900ms step_avg:58.71ms
step:1856/2330 train_time:108963ms step_avg:58.71ms
step:1857/2330 train_time:109020ms step_avg:58.71ms
step:1858/2330 train_time:109083ms step_avg:58.71ms
step:1859/2330 train_time:109140ms step_avg:58.71ms
step:1860/2330 train_time:109201ms step_avg:58.71ms
step:1861/2330 train_time:109259ms step_avg:58.71ms
step:1862/2330 train_time:109319ms step_avg:58.71ms
step:1863/2330 train_time:109377ms step_avg:58.71ms
step:1864/2330 train_time:109437ms step_avg:58.71ms
step:1865/2330 train_time:109494ms step_avg:58.71ms
step:1866/2330 train_time:109554ms step_avg:58.71ms
step:1867/2330 train_time:109611ms step_avg:58.71ms
step:1868/2330 train_time:109671ms step_avg:58.71ms
step:1869/2330 train_time:109728ms step_avg:58.71ms
step:1870/2330 train_time:109789ms step_avg:58.71ms
step:1871/2330 train_time:109847ms step_avg:58.71ms
step:1872/2330 train_time:109909ms step_avg:58.71ms
step:1873/2330 train_time:109965ms step_avg:58.71ms
step:1874/2330 train_time:110027ms step_avg:58.71ms
step:1875/2330 train_time:110085ms step_avg:58.71ms
step:1876/2330 train_time:110146ms step_avg:58.71ms
step:1877/2330 train_time:110203ms step_avg:58.71ms
step:1878/2330 train_time:110265ms step_avg:58.71ms
step:1879/2330 train_time:110322ms step_avg:58.71ms
step:1880/2330 train_time:110383ms step_avg:58.71ms
step:1881/2330 train_time:110440ms step_avg:58.71ms
step:1882/2330 train_time:110503ms step_avg:58.72ms
step:1883/2330 train_time:110559ms step_avg:58.71ms
step:1884/2330 train_time:110622ms step_avg:58.72ms
step:1885/2330 train_time:110680ms step_avg:58.72ms
step:1886/2330 train_time:110739ms step_avg:58.72ms
step:1887/2330 train_time:110797ms step_avg:58.72ms
step:1888/2330 train_time:110858ms step_avg:58.72ms
step:1889/2330 train_time:110916ms step_avg:58.72ms
step:1890/2330 train_time:110977ms step_avg:58.72ms
step:1891/2330 train_time:111035ms step_avg:58.72ms
step:1892/2330 train_time:111096ms step_avg:58.72ms
step:1893/2330 train_time:111155ms step_avg:58.72ms
step:1894/2330 train_time:111215ms step_avg:58.72ms
step:1895/2330 train_time:111274ms step_avg:58.72ms
step:1896/2330 train_time:111334ms step_avg:58.72ms
step:1897/2330 train_time:111392ms step_avg:58.72ms
step:1898/2330 train_time:111452ms step_avg:58.72ms
step:1899/2330 train_time:111510ms step_avg:58.72ms
step:1900/2330 train_time:111571ms step_avg:58.72ms
step:1901/2330 train_time:111628ms step_avg:58.72ms
step:1902/2330 train_time:111689ms step_avg:58.72ms
step:1903/2330 train_time:111745ms step_avg:58.72ms
step:1904/2330 train_time:111807ms step_avg:58.72ms
step:1905/2330 train_time:111865ms step_avg:58.72ms
step:1906/2330 train_time:111926ms step_avg:58.72ms
step:1907/2330 train_time:111983ms step_avg:58.72ms
step:1908/2330 train_time:112046ms step_avg:58.72ms
step:1909/2330 train_time:112102ms step_avg:58.72ms
step:1910/2330 train_time:112165ms step_avg:58.72ms
step:1911/2330 train_time:112222ms step_avg:58.72ms
step:1912/2330 train_time:112283ms step_avg:58.73ms
step:1913/2330 train_time:112340ms step_avg:58.72ms
step:1914/2330 train_time:112402ms step_avg:58.73ms
step:1915/2330 train_time:112458ms step_avg:58.72ms
step:1916/2330 train_time:112520ms step_avg:58.73ms
step:1917/2330 train_time:112578ms step_avg:58.73ms
step:1918/2330 train_time:112637ms step_avg:58.73ms
step:1919/2330 train_time:112695ms step_avg:58.73ms
step:1920/2330 train_time:112756ms step_avg:58.73ms
step:1921/2330 train_time:112814ms step_avg:58.73ms
step:1922/2330 train_time:112874ms step_avg:58.73ms
step:1923/2330 train_time:112933ms step_avg:58.73ms
step:1924/2330 train_time:112993ms step_avg:58.73ms
step:1925/2330 train_time:113051ms step_avg:58.73ms
step:1926/2330 train_time:113111ms step_avg:58.73ms
step:1927/2330 train_time:113168ms step_avg:58.73ms
step:1928/2330 train_time:113230ms step_avg:58.73ms
step:1929/2330 train_time:113288ms step_avg:58.73ms
step:1930/2330 train_time:113348ms step_avg:58.73ms
step:1931/2330 train_time:113405ms step_avg:58.73ms
step:1932/2330 train_time:113467ms step_avg:58.73ms
step:1933/2330 train_time:113524ms step_avg:58.73ms
step:1934/2330 train_time:113586ms step_avg:58.73ms
step:1935/2330 train_time:113643ms step_avg:58.73ms
step:1936/2330 train_time:113705ms step_avg:58.73ms
step:1937/2330 train_time:113762ms step_avg:58.73ms
step:1938/2330 train_time:113823ms step_avg:58.73ms
step:1939/2330 train_time:113879ms step_avg:58.73ms
step:1940/2330 train_time:113943ms step_avg:58.73ms
step:1941/2330 train_time:114000ms step_avg:58.73ms
step:1942/2330 train_time:114061ms step_avg:58.73ms
step:1943/2330 train_time:114119ms step_avg:58.73ms
step:1944/2330 train_time:114180ms step_avg:58.73ms
step:1945/2330 train_time:114237ms step_avg:58.73ms
step:1946/2330 train_time:114298ms step_avg:58.73ms
step:1947/2330 train_time:114357ms step_avg:58.73ms
step:1948/2330 train_time:114417ms step_avg:58.74ms
step:1949/2330 train_time:114475ms step_avg:58.74ms
step:1950/2330 train_time:114535ms step_avg:58.74ms
step:1951/2330 train_time:114593ms step_avg:58.74ms
step:1952/2330 train_time:114653ms step_avg:58.74ms
step:1953/2330 train_time:114711ms step_avg:58.74ms
step:1954/2330 train_time:114771ms step_avg:58.74ms
step:1955/2330 train_time:114828ms step_avg:58.74ms
step:1956/2330 train_time:114889ms step_avg:58.74ms
step:1957/2330 train_time:114947ms step_avg:58.74ms
step:1958/2330 train_time:115008ms step_avg:58.74ms
step:1959/2330 train_time:115065ms step_avg:58.74ms
step:1960/2330 train_time:115128ms step_avg:58.74ms
step:1961/2330 train_time:115185ms step_avg:58.74ms
step:1962/2330 train_time:115247ms step_avg:58.74ms
step:1963/2330 train_time:115303ms step_avg:58.74ms
step:1964/2330 train_time:115366ms step_avg:58.74ms
step:1965/2330 train_time:115423ms step_avg:58.74ms
step:1966/2330 train_time:115485ms step_avg:58.74ms
step:1967/2330 train_time:115542ms step_avg:58.74ms
step:1968/2330 train_time:115604ms step_avg:58.74ms
step:1969/2330 train_time:115661ms step_avg:58.74ms
step:1970/2330 train_time:115723ms step_avg:58.74ms
step:1971/2330 train_time:115781ms step_avg:58.74ms
step:1972/2330 train_time:115842ms step_avg:58.74ms
step:1973/2330 train_time:115899ms step_avg:58.74ms
step:1974/2330 train_time:115960ms step_avg:58.74ms
step:1975/2330 train_time:116018ms step_avg:58.74ms
step:1976/2330 train_time:116079ms step_avg:58.74ms
step:1977/2330 train_time:116138ms step_avg:58.74ms
step:1978/2330 train_time:116198ms step_avg:58.75ms
step:1979/2330 train_time:116256ms step_avg:58.74ms
step:1980/2330 train_time:116317ms step_avg:58.75ms
step:1981/2330 train_time:116374ms step_avg:58.74ms
step:1982/2330 train_time:116434ms step_avg:58.75ms
step:1983/2330 train_time:116492ms step_avg:58.75ms
step:1984/2330 train_time:116552ms step_avg:58.75ms
step:1985/2330 train_time:116609ms step_avg:58.75ms
step:1986/2330 train_time:116670ms step_avg:58.75ms
step:1987/2330 train_time:116728ms step_avg:58.75ms
step:1988/2330 train_time:116787ms step_avg:58.75ms
step:1989/2330 train_time:116845ms step_avg:58.75ms
step:1990/2330 train_time:116905ms step_avg:58.75ms
step:1991/2330 train_time:116962ms step_avg:58.75ms
step:1992/2330 train_time:117025ms step_avg:58.75ms
step:1993/2330 train_time:117083ms step_avg:58.75ms
step:1994/2330 train_time:117144ms step_avg:58.75ms
step:1995/2330 train_time:117201ms step_avg:58.75ms
step:1996/2330 train_time:117264ms step_avg:58.75ms
step:1997/2330 train_time:117320ms step_avg:58.75ms
step:1998/2330 train_time:117383ms step_avg:58.75ms
step:1999/2330 train_time:117439ms step_avg:58.75ms
step:2000/2330 train_time:117501ms step_avg:58.75ms
step:2000/2330 val_loss:3.7650 train_time:117582ms step_avg:58.79ms
step:2001/2330 train_time:117602ms step_avg:58.77ms
step:2002/2330 train_time:117623ms step_avg:58.75ms
step:2003/2330 train_time:117683ms step_avg:58.75ms
step:2004/2330 train_time:117748ms step_avg:58.76ms
step:2005/2330 train_time:117807ms step_avg:58.76ms
step:2006/2330 train_time:117866ms step_avg:58.76ms
step:2007/2330 train_time:117924ms step_avg:58.76ms
step:2008/2330 train_time:117984ms step_avg:58.76ms
step:2009/2330 train_time:118041ms step_avg:58.76ms
step:2010/2330 train_time:118101ms step_avg:58.76ms
step:2011/2330 train_time:118158ms step_avg:58.76ms
step:2012/2330 train_time:118218ms step_avg:58.76ms
step:2013/2330 train_time:118274ms step_avg:58.76ms
step:2014/2330 train_time:118335ms step_avg:58.76ms
step:2015/2330 train_time:118392ms step_avg:58.76ms
step:2016/2330 train_time:118451ms step_avg:58.76ms
step:2017/2330 train_time:118508ms step_avg:58.75ms
step:2018/2330 train_time:118570ms step_avg:58.76ms
step:2019/2330 train_time:118628ms step_avg:58.76ms
step:2020/2330 train_time:118692ms step_avg:58.76ms
step:2021/2330 train_time:118751ms step_avg:58.76ms
step:2022/2330 train_time:118812ms step_avg:58.76ms
step:2023/2330 train_time:118872ms step_avg:58.76ms
step:2024/2330 train_time:118932ms step_avg:58.76ms
step:2025/2330 train_time:118991ms step_avg:58.76ms
step:2026/2330 train_time:119051ms step_avg:58.76ms
step:2027/2330 train_time:119109ms step_avg:58.76ms
step:2028/2330 train_time:119168ms step_avg:58.76ms
step:2029/2330 train_time:119225ms step_avg:58.76ms
step:2030/2330 train_time:119284ms step_avg:58.76ms
step:2031/2330 train_time:119341ms step_avg:58.76ms
step:2032/2330 train_time:119402ms step_avg:58.76ms
step:2033/2330 train_time:119458ms step_avg:58.76ms
step:2034/2330 train_time:119521ms step_avg:58.76ms
step:2035/2330 train_time:119579ms step_avg:58.76ms
step:2036/2330 train_time:119641ms step_avg:58.76ms
step:2037/2330 train_time:119699ms step_avg:58.76ms
step:2038/2330 train_time:119762ms step_avg:58.76ms
step:2039/2330 train_time:119819ms step_avg:58.76ms
step:2040/2330 train_time:119883ms step_avg:58.77ms
step:2041/2330 train_time:119940ms step_avg:58.77ms
step:2042/2330 train_time:120002ms step_avg:58.77ms
step:2043/2330 train_time:120060ms step_avg:58.77ms
step:2044/2330 train_time:120121ms step_avg:58.77ms
step:2045/2330 train_time:120178ms step_avg:58.77ms
step:2046/2330 train_time:120240ms step_avg:58.77ms
step:2047/2330 train_time:120297ms step_avg:58.77ms
step:2048/2330 train_time:120357ms step_avg:58.77ms
step:2049/2330 train_time:120414ms step_avg:58.77ms
step:2050/2330 train_time:120475ms step_avg:58.77ms
step:2051/2330 train_time:120532ms step_avg:58.77ms
step:2052/2330 train_time:120594ms step_avg:58.77ms
step:2053/2330 train_time:120653ms step_avg:58.77ms
step:2054/2330 train_time:120713ms step_avg:58.77ms
step:2055/2330 train_time:120772ms step_avg:58.77ms
step:2056/2330 train_time:120832ms step_avg:58.77ms
step:2057/2330 train_time:120892ms step_avg:58.77ms
step:2058/2330 train_time:120952ms step_avg:58.77ms
step:2059/2330 train_time:121009ms step_avg:58.77ms
step:2060/2330 train_time:121069ms step_avg:58.77ms
step:2061/2330 train_time:121127ms step_avg:58.77ms
step:2062/2330 train_time:121187ms step_avg:58.77ms
step:2063/2330 train_time:121245ms step_avg:58.77ms
step:2064/2330 train_time:121305ms step_avg:58.77ms
step:2065/2330 train_time:121362ms step_avg:58.77ms
step:2066/2330 train_time:121422ms step_avg:58.77ms
step:2067/2330 train_time:121479ms step_avg:58.77ms
step:2068/2330 train_time:121542ms step_avg:58.77ms
step:2069/2330 train_time:121600ms step_avg:58.77ms
step:2070/2330 train_time:121661ms step_avg:58.77ms
step:2071/2330 train_time:121718ms step_avg:58.77ms
step:2072/2330 train_time:121783ms step_avg:58.78ms
step:2073/2330 train_time:121840ms step_avg:58.77ms
step:2074/2330 train_time:121901ms step_avg:58.78ms
step:2075/2330 train_time:121958ms step_avg:58.78ms
step:2076/2330 train_time:122020ms step_avg:58.78ms
step:2077/2330 train_time:122077ms step_avg:58.78ms
step:2078/2330 train_time:122139ms step_avg:58.78ms
step:2079/2330 train_time:122197ms step_avg:58.78ms
step:2080/2330 train_time:122256ms step_avg:58.78ms
step:2081/2330 train_time:122315ms step_avg:58.78ms
step:2082/2330 train_time:122375ms step_avg:58.78ms
step:2083/2330 train_time:122433ms step_avg:58.78ms
step:2084/2330 train_time:122493ms step_avg:58.78ms
step:2085/2330 train_time:122551ms step_avg:58.78ms
step:2086/2330 train_time:122611ms step_avg:58.78ms
step:2087/2330 train_time:122669ms step_avg:58.78ms
step:2088/2330 train_time:122729ms step_avg:58.78ms
step:2089/2330 train_time:122788ms step_avg:58.78ms
step:2090/2330 train_time:122848ms step_avg:58.78ms
step:2091/2330 train_time:122907ms step_avg:58.78ms
step:2092/2330 train_time:122967ms step_avg:58.78ms
step:2093/2330 train_time:123024ms step_avg:58.78ms
step:2094/2330 train_time:123086ms step_avg:58.78ms
step:2095/2330 train_time:123144ms step_avg:58.78ms
step:2096/2330 train_time:123204ms step_avg:58.78ms
step:2097/2330 train_time:123261ms step_avg:58.78ms
step:2098/2330 train_time:123322ms step_avg:58.78ms
step:2099/2330 train_time:123379ms step_avg:58.78ms
step:2100/2330 train_time:123441ms step_avg:58.78ms
step:2101/2330 train_time:123498ms step_avg:58.78ms
step:2102/2330 train_time:123560ms step_avg:58.78ms
step:2103/2330 train_time:123617ms step_avg:58.78ms
step:2104/2330 train_time:123679ms step_avg:58.78ms
step:2105/2330 train_time:123738ms step_avg:58.78ms
step:2106/2330 train_time:123798ms step_avg:58.78ms
step:2107/2330 train_time:123856ms step_avg:58.78ms
step:2108/2330 train_time:123917ms step_avg:58.78ms
step:2109/2330 train_time:123977ms step_avg:58.78ms
step:2110/2330 train_time:124037ms step_avg:58.79ms
step:2111/2330 train_time:124096ms step_avg:58.79ms
step:2112/2330 train_time:124156ms step_avg:58.79ms
step:2113/2330 train_time:124214ms step_avg:58.79ms
step:2114/2330 train_time:124274ms step_avg:58.79ms
step:2115/2330 train_time:124332ms step_avg:58.79ms
step:2116/2330 train_time:124393ms step_avg:58.79ms
step:2117/2330 train_time:124451ms step_avg:58.79ms
step:2118/2330 train_time:124511ms step_avg:58.79ms
step:2119/2330 train_time:124568ms step_avg:58.79ms
step:2120/2330 train_time:124628ms step_avg:58.79ms
step:2121/2330 train_time:124686ms step_avg:58.79ms
step:2122/2330 train_time:124747ms step_avg:58.79ms
step:2123/2330 train_time:124805ms step_avg:58.79ms
step:2124/2330 train_time:124867ms step_avg:58.79ms
step:2125/2330 train_time:124924ms step_avg:58.79ms
step:2126/2330 train_time:124987ms step_avg:58.79ms
step:2127/2330 train_time:125044ms step_avg:58.79ms
step:2128/2330 train_time:125107ms step_avg:58.79ms
step:2129/2330 train_time:125163ms step_avg:58.79ms
step:2130/2330 train_time:125224ms step_avg:58.79ms
step:2131/2330 train_time:125281ms step_avg:58.79ms
step:2132/2330 train_time:125343ms step_avg:58.79ms
step:2133/2330 train_time:125400ms step_avg:58.79ms
step:2134/2330 train_time:125462ms step_avg:58.79ms
step:2135/2330 train_time:125518ms step_avg:58.79ms
step:2136/2330 train_time:125579ms step_avg:58.79ms
step:2137/2330 train_time:125637ms step_avg:58.79ms
step:2138/2330 train_time:125698ms step_avg:58.79ms
step:2139/2330 train_time:125757ms step_avg:58.79ms
step:2140/2330 train_time:125818ms step_avg:58.79ms
step:2141/2330 train_time:125875ms step_avg:58.79ms
step:2142/2330 train_time:125936ms step_avg:58.79ms
step:2143/2330 train_time:125994ms step_avg:58.79ms
step:2144/2330 train_time:126054ms step_avg:58.79ms
step:2145/2330 train_time:126113ms step_avg:58.79ms
step:2146/2330 train_time:126173ms step_avg:58.79ms
step:2147/2330 train_time:126231ms step_avg:58.79ms
step:2148/2330 train_time:126292ms step_avg:58.79ms
step:2149/2330 train_time:126350ms step_avg:58.79ms
step:2150/2330 train_time:126409ms step_avg:58.80ms
step:2151/2330 train_time:126468ms step_avg:58.79ms
step:2152/2330 train_time:126527ms step_avg:58.80ms
step:2153/2330 train_time:126584ms step_avg:58.79ms
step:2154/2330 train_time:126646ms step_avg:58.80ms
step:2155/2330 train_time:126704ms step_avg:58.80ms
step:2156/2330 train_time:126765ms step_avg:58.80ms
step:2157/2330 train_time:126822ms step_avg:58.80ms
step:2158/2330 train_time:126885ms step_avg:58.80ms
step:2159/2330 train_time:126941ms step_avg:58.80ms
step:2160/2330 train_time:127005ms step_avg:58.80ms
step:2161/2330 train_time:127062ms step_avg:58.80ms
step:2162/2330 train_time:127123ms step_avg:58.80ms
step:2163/2330 train_time:127180ms step_avg:58.80ms
step:2164/2330 train_time:127241ms step_avg:58.80ms
step:2165/2330 train_time:127299ms step_avg:58.80ms
step:2166/2330 train_time:127360ms step_avg:58.80ms
step:2167/2330 train_time:127417ms step_avg:58.80ms
step:2168/2330 train_time:127477ms step_avg:58.80ms
step:2169/2330 train_time:127534ms step_avg:58.80ms
step:2170/2330 train_time:127596ms step_avg:58.80ms
step:2171/2330 train_time:127654ms step_avg:58.80ms
step:2172/2330 train_time:127714ms step_avg:58.80ms
step:2173/2330 train_time:127772ms step_avg:58.80ms
step:2174/2330 train_time:127832ms step_avg:58.80ms
step:2175/2330 train_time:127891ms step_avg:58.80ms
step:2176/2330 train_time:127951ms step_avg:58.80ms
step:2177/2330 train_time:128009ms step_avg:58.80ms
step:2178/2330 train_time:128069ms step_avg:58.80ms
step:2179/2330 train_time:128126ms step_avg:58.80ms
step:2180/2330 train_time:128188ms step_avg:58.80ms
step:2181/2330 train_time:128246ms step_avg:58.80ms
step:2182/2330 train_time:128307ms step_avg:58.80ms
step:2183/2330 train_time:128364ms step_avg:58.80ms
step:2184/2330 train_time:128427ms step_avg:58.80ms
step:2185/2330 train_time:128484ms step_avg:58.80ms
step:2186/2330 train_time:128545ms step_avg:58.80ms
step:2187/2330 train_time:128602ms step_avg:58.80ms
step:2188/2330 train_time:128665ms step_avg:58.80ms
step:2189/2330 train_time:128722ms step_avg:58.80ms
step:2190/2330 train_time:128783ms step_avg:58.81ms
step:2191/2330 train_time:128840ms step_avg:58.80ms
step:2192/2330 train_time:128903ms step_avg:58.81ms
step:2193/2330 train_time:128959ms step_avg:58.80ms
step:2194/2330 train_time:129022ms step_avg:58.81ms
step:2195/2330 train_time:129079ms step_avg:58.81ms
step:2196/2330 train_time:129141ms step_avg:58.81ms
step:2197/2330 train_time:129198ms step_avg:58.81ms
step:2198/2330 train_time:129260ms step_avg:58.81ms
step:2199/2330 train_time:129318ms step_avg:58.81ms
step:2200/2330 train_time:129379ms step_avg:58.81ms
step:2201/2330 train_time:129436ms step_avg:58.81ms
step:2202/2330 train_time:129497ms step_avg:58.81ms
step:2203/2330 train_time:129555ms step_avg:58.81ms
step:2204/2330 train_time:129615ms step_avg:58.81ms
step:2205/2330 train_time:129673ms step_avg:58.81ms
step:2206/2330 train_time:129733ms step_avg:58.81ms
step:2207/2330 train_time:129791ms step_avg:58.81ms
step:2208/2330 train_time:129851ms step_avg:58.81ms
step:2209/2330 train_time:129909ms step_avg:58.81ms
step:2210/2330 train_time:129969ms step_avg:58.81ms
step:2211/2330 train_time:130026ms step_avg:58.81ms
step:2212/2330 train_time:130087ms step_avg:58.81ms
step:2213/2330 train_time:130144ms step_avg:58.81ms
step:2214/2330 train_time:130205ms step_avg:58.81ms
step:2215/2330 train_time:130262ms step_avg:58.81ms
step:2216/2330 train_time:130323ms step_avg:58.81ms
step:2217/2330 train_time:130381ms step_avg:58.81ms
step:2218/2330 train_time:130443ms step_avg:58.81ms
step:2219/2330 train_time:130500ms step_avg:58.81ms
step:2220/2330 train_time:130562ms step_avg:58.81ms
step:2221/2330 train_time:130618ms step_avg:58.81ms
step:2222/2330 train_time:130681ms step_avg:58.81ms
step:2223/2330 train_time:130738ms step_avg:58.81ms
step:2224/2330 train_time:130801ms step_avg:58.81ms
step:2225/2330 train_time:130857ms step_avg:58.81ms
step:2226/2330 train_time:130919ms step_avg:58.81ms
step:2227/2330 train_time:130976ms step_avg:58.81ms
step:2228/2330 train_time:131037ms step_avg:58.81ms
step:2229/2330 train_time:131096ms step_avg:58.81ms
step:2230/2330 train_time:131156ms step_avg:58.81ms
step:2231/2330 train_time:131213ms step_avg:58.81ms
step:2232/2330 train_time:131274ms step_avg:58.81ms
step:2233/2330 train_time:131332ms step_avg:58.81ms
step:2234/2330 train_time:131392ms step_avg:58.81ms
step:2235/2330 train_time:131450ms step_avg:58.81ms
step:2236/2330 train_time:131511ms step_avg:58.82ms
step:2237/2330 train_time:131568ms step_avg:58.81ms
step:2238/2330 train_time:131628ms step_avg:58.82ms
step:2239/2330 train_time:131685ms step_avg:58.81ms
step:2240/2330 train_time:131746ms step_avg:58.82ms
step:2241/2330 train_time:131804ms step_avg:58.81ms
step:2242/2330 train_time:131865ms step_avg:58.82ms
step:2243/2330 train_time:131921ms step_avg:58.81ms
step:2244/2330 train_time:131982ms step_avg:58.82ms
step:2245/2330 train_time:132039ms step_avg:58.81ms
step:2246/2330 train_time:132102ms step_avg:58.82ms
step:2247/2330 train_time:132159ms step_avg:58.82ms
step:2248/2330 train_time:132222ms step_avg:58.82ms
step:2249/2330 train_time:132278ms step_avg:58.82ms
step:2250/2330 train_time:132342ms step_avg:58.82ms
step:2250/2330 val_loss:3.7163 train_time:132424ms step_avg:58.85ms
step:2251/2330 train_time:132443ms step_avg:58.84ms
step:2252/2330 train_time:132463ms step_avg:58.82ms
step:2253/2330 train_time:132521ms step_avg:58.82ms
step:2254/2330 train_time:132589ms step_avg:58.82ms
step:2255/2330 train_time:132647ms step_avg:58.82ms
step:2256/2330 train_time:132707ms step_avg:58.82ms
step:2257/2330 train_time:132764ms step_avg:58.82ms
step:2258/2330 train_time:132826ms step_avg:58.82ms
step:2259/2330 train_time:132883ms step_avg:58.82ms
step:2260/2330 train_time:132942ms step_avg:58.82ms
step:2261/2330 train_time:132999ms step_avg:58.82ms
step:2262/2330 train_time:133059ms step_avg:58.82ms
step:2263/2330 train_time:133116ms step_avg:58.82ms
step:2264/2330 train_time:133176ms step_avg:58.82ms
step:2265/2330 train_time:133235ms step_avg:58.82ms
step:2266/2330 train_time:133294ms step_avg:58.82ms
step:2267/2330 train_time:133352ms step_avg:58.82ms
step:2268/2330 train_time:133413ms step_avg:58.82ms
step:2269/2330 train_time:133472ms step_avg:58.82ms
step:2270/2330 train_time:133536ms step_avg:58.83ms
step:2271/2330 train_time:133595ms step_avg:58.83ms
step:2272/2330 train_time:133657ms step_avg:58.83ms
step:2273/2330 train_time:133715ms step_avg:58.83ms
step:2274/2330 train_time:133775ms step_avg:58.83ms
step:2275/2330 train_time:133832ms step_avg:58.83ms
step:2276/2330 train_time:133893ms step_avg:58.83ms
step:2277/2330 train_time:133950ms step_avg:58.83ms
step:2278/2330 train_time:134011ms step_avg:58.83ms
step:2279/2330 train_time:134068ms step_avg:58.83ms
step:2280/2330 train_time:134129ms step_avg:58.83ms
step:2281/2330 train_time:134186ms step_avg:58.83ms
step:2282/2330 train_time:134247ms step_avg:58.83ms
step:2283/2330 train_time:134303ms step_avg:58.83ms
step:2284/2330 train_time:134366ms step_avg:58.83ms
step:2285/2330 train_time:134424ms step_avg:58.83ms
step:2286/2330 train_time:134487ms step_avg:58.83ms
step:2287/2330 train_time:134544ms step_avg:58.83ms
step:2288/2330 train_time:134607ms step_avg:58.83ms
step:2289/2330 train_time:134664ms step_avg:58.83ms
step:2290/2330 train_time:134727ms step_avg:58.83ms
step:2291/2330 train_time:134784ms step_avg:58.83ms
step:2292/2330 train_time:134844ms step_avg:58.83ms
step:2293/2330 train_time:134902ms step_avg:58.83ms
step:2294/2330 train_time:134962ms step_avg:58.83ms
step:2295/2330 train_time:135021ms step_avg:58.83ms
step:2296/2330 train_time:135081ms step_avg:58.83ms
step:2297/2330 train_time:135139ms step_avg:58.83ms
step:2298/2330 train_time:135199ms step_avg:58.83ms
step:2299/2330 train_time:135257ms step_avg:58.83ms
step:2300/2330 train_time:135317ms step_avg:58.83ms
step:2301/2330 train_time:135377ms step_avg:58.83ms
step:2302/2330 train_time:135437ms step_avg:58.83ms
step:2303/2330 train_time:135496ms step_avg:58.83ms
step:2304/2330 train_time:135557ms step_avg:58.84ms
step:2305/2330 train_time:135614ms step_avg:58.83ms
step:2306/2330 train_time:135674ms step_avg:58.84ms
step:2307/2330 train_time:135732ms step_avg:58.84ms
step:2308/2330 train_time:135794ms step_avg:58.84ms
step:2309/2330 train_time:135851ms step_avg:58.84ms
step:2310/2330 train_time:135913ms step_avg:58.84ms
step:2311/2330 train_time:135969ms step_avg:58.84ms
step:2312/2330 train_time:136030ms step_avg:58.84ms
step:2313/2330 train_time:136087ms step_avg:58.84ms
step:2314/2330 train_time:136149ms step_avg:58.84ms
step:2315/2330 train_time:136206ms step_avg:58.84ms
step:2316/2330 train_time:136267ms step_avg:58.84ms
step:2317/2330 train_time:136325ms step_avg:58.84ms
step:2318/2330 train_time:136386ms step_avg:58.84ms
step:2319/2330 train_time:136443ms step_avg:58.84ms
step:2320/2330 train_time:136505ms step_avg:58.84ms
step:2321/2330 train_time:136562ms step_avg:58.84ms
step:2322/2330 train_time:136624ms step_avg:58.84ms
step:2323/2330 train_time:136682ms step_avg:58.84ms
step:2324/2330 train_time:136742ms step_avg:58.84ms
step:2325/2330 train_time:136801ms step_avg:58.84ms
step:2326/2330 train_time:136861ms step_avg:58.84ms
step:2327/2330 train_time:136919ms step_avg:58.84ms
step:2328/2330 train_time:136979ms step_avg:58.84ms
step:2329/2330 train_time:137036ms step_avg:58.84ms
step:2330/2330 train_time:137097ms step_avg:58.84ms
step:2330/2330 val_loss:3.7010 train_time:137178ms step_avg:58.87ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
