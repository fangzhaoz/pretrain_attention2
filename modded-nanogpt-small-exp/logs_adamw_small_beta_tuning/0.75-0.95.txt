import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--beta1', type=str, required=True)
parser.add_argument('--beta2', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_beta_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=6e-4,  betas=(float(args2.beta1), float(args2.beta2)), eps=1e-10, weight_decay=0.0, fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt", exist_ok=True)
            torch.save(log, f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 04:51:14 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   27C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   26C    P0             115W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   24C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   27C    P0             113W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   25C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   26C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:87ms step_avg:86.76ms
step:2/2330 train_time:181ms step_avg:90.56ms
step:3/2330 train_time:201ms step_avg:66.97ms
step:4/2330 train_time:221ms step_avg:55.32ms
step:5/2330 train_time:272ms step_avg:54.41ms
step:6/2330 train_time:330ms step_avg:55.03ms
step:7/2330 train_time:386ms step_avg:55.09ms
step:8/2330 train_time:444ms step_avg:55.47ms
step:9/2330 train_time:500ms step_avg:55.53ms
step:10/2330 train_time:558ms step_avg:55.78ms
step:11/2330 train_time:613ms step_avg:55.73ms
step:12/2330 train_time:671ms step_avg:55.91ms
step:13/2330 train_time:726ms step_avg:55.85ms
step:14/2330 train_time:784ms step_avg:56.01ms
step:15/2330 train_time:839ms step_avg:55.95ms
step:16/2330 train_time:897ms step_avg:56.08ms
step:17/2330 train_time:953ms step_avg:56.04ms
step:18/2330 train_time:1012ms step_avg:56.21ms
step:19/2330 train_time:1070ms step_avg:56.30ms
step:20/2330 train_time:1131ms step_avg:56.57ms
step:21/2330 train_time:1189ms step_avg:56.63ms
step:22/2330 train_time:1250ms step_avg:56.80ms
step:23/2330 train_time:1305ms step_avg:56.75ms
step:24/2330 train_time:1366ms step_avg:56.92ms
step:25/2330 train_time:1422ms step_avg:56.87ms
step:26/2330 train_time:1481ms step_avg:56.97ms
step:27/2330 train_time:1538ms step_avg:56.95ms
step:28/2330 train_time:1596ms step_avg:57.00ms
step:29/2330 train_time:1652ms step_avg:56.96ms
step:30/2330 train_time:1710ms step_avg:57.00ms
step:31/2330 train_time:1765ms step_avg:56.94ms
step:32/2330 train_time:1825ms step_avg:57.02ms
step:33/2330 train_time:1880ms step_avg:56.98ms
step:34/2330 train_time:1939ms step_avg:57.02ms
step:35/2330 train_time:1995ms step_avg:57.00ms
step:36/2330 train_time:2054ms step_avg:57.06ms
step:37/2330 train_time:2111ms step_avg:57.06ms
step:38/2330 train_time:2171ms step_avg:57.14ms
step:39/2330 train_time:2228ms step_avg:57.13ms
step:40/2330 train_time:2288ms step_avg:57.21ms
step:41/2330 train_time:2345ms step_avg:57.19ms
step:42/2330 train_time:2405ms step_avg:57.26ms
step:43/2330 train_time:2461ms step_avg:57.23ms
step:44/2330 train_time:2521ms step_avg:57.29ms
step:45/2330 train_time:2577ms step_avg:57.26ms
step:46/2330 train_time:2635ms step_avg:57.28ms
step:47/2330 train_time:2691ms step_avg:57.26ms
step:48/2330 train_time:2750ms step_avg:57.30ms
step:49/2330 train_time:2806ms step_avg:57.26ms
step:50/2330 train_time:2865ms step_avg:57.29ms
step:51/2330 train_time:2921ms step_avg:57.27ms
step:52/2330 train_time:2979ms step_avg:57.29ms
step:53/2330 train_time:3035ms step_avg:57.26ms
step:54/2330 train_time:3095ms step_avg:57.31ms
step:55/2330 train_time:3151ms step_avg:57.29ms
step:56/2330 train_time:3213ms step_avg:57.37ms
step:57/2330 train_time:3269ms step_avg:57.35ms
step:58/2330 train_time:3329ms step_avg:57.40ms
step:59/2330 train_time:3385ms step_avg:57.38ms
step:60/2330 train_time:3445ms step_avg:57.42ms
step:61/2330 train_time:3501ms step_avg:57.40ms
step:62/2330 train_time:3561ms step_avg:57.44ms
step:63/2330 train_time:3617ms step_avg:57.42ms
step:64/2330 train_time:3676ms step_avg:57.44ms
step:65/2330 train_time:3732ms step_avg:57.41ms
step:66/2330 train_time:3790ms step_avg:57.43ms
step:67/2330 train_time:3846ms step_avg:57.40ms
step:68/2330 train_time:3905ms step_avg:57.43ms
step:69/2330 train_time:3962ms step_avg:57.42ms
step:70/2330 train_time:4021ms step_avg:57.44ms
step:71/2330 train_time:4078ms step_avg:57.44ms
step:72/2330 train_time:4136ms step_avg:57.45ms
step:73/2330 train_time:4193ms step_avg:57.44ms
step:74/2330 train_time:4252ms step_avg:57.46ms
step:75/2330 train_time:4308ms step_avg:57.44ms
step:76/2330 train_time:4369ms step_avg:57.49ms
step:77/2330 train_time:4425ms step_avg:57.47ms
step:78/2330 train_time:4485ms step_avg:57.51ms
step:79/2330 train_time:4542ms step_avg:57.49ms
step:80/2330 train_time:4601ms step_avg:57.51ms
step:81/2330 train_time:4657ms step_avg:57.50ms
step:82/2330 train_time:4716ms step_avg:57.51ms
step:83/2330 train_time:4771ms step_avg:57.48ms
step:84/2330 train_time:4829ms step_avg:57.49ms
step:85/2330 train_time:4885ms step_avg:57.48ms
step:86/2330 train_time:4945ms step_avg:57.49ms
step:87/2330 train_time:5001ms step_avg:57.48ms
step:88/2330 train_time:5060ms step_avg:57.50ms
step:89/2330 train_time:5117ms step_avg:57.49ms
step:90/2330 train_time:5175ms step_avg:57.50ms
step:91/2330 train_time:5231ms step_avg:57.48ms
step:92/2330 train_time:5291ms step_avg:57.52ms
step:93/2330 train_time:5348ms step_avg:57.50ms
step:94/2330 train_time:5408ms step_avg:57.53ms
step:95/2330 train_time:5465ms step_avg:57.52ms
step:96/2330 train_time:5523ms step_avg:57.53ms
step:97/2330 train_time:5579ms step_avg:57.52ms
step:98/2330 train_time:5638ms step_avg:57.53ms
step:99/2330 train_time:5694ms step_avg:57.51ms
step:100/2330 train_time:5752ms step_avg:57.52ms
step:101/2330 train_time:5808ms step_avg:57.50ms
step:102/2330 train_time:5868ms step_avg:57.53ms
step:103/2330 train_time:5924ms step_avg:57.51ms
step:104/2330 train_time:5983ms step_avg:57.53ms
step:105/2330 train_time:6040ms step_avg:57.52ms
step:106/2330 train_time:6098ms step_avg:57.53ms
step:107/2330 train_time:6155ms step_avg:57.52ms
step:108/2330 train_time:6213ms step_avg:57.53ms
step:109/2330 train_time:6270ms step_avg:57.52ms
step:110/2330 train_time:6330ms step_avg:57.55ms
step:111/2330 train_time:6386ms step_avg:57.53ms
step:112/2330 train_time:6447ms step_avg:57.56ms
step:113/2330 train_time:6503ms step_avg:57.55ms
step:114/2330 train_time:6562ms step_avg:57.56ms
step:115/2330 train_time:6618ms step_avg:57.55ms
step:116/2330 train_time:6676ms step_avg:57.56ms
step:117/2330 train_time:6732ms step_avg:57.54ms
step:118/2330 train_time:6791ms step_avg:57.55ms
step:119/2330 train_time:6846ms step_avg:57.53ms
step:120/2330 train_time:6906ms step_avg:57.55ms
step:121/2330 train_time:6962ms step_avg:57.54ms
step:122/2330 train_time:7022ms step_avg:57.55ms
step:123/2330 train_time:7078ms step_avg:57.54ms
step:124/2330 train_time:7136ms step_avg:57.55ms
step:125/2330 train_time:7192ms step_avg:57.54ms
step:126/2330 train_time:7251ms step_avg:57.55ms
step:127/2330 train_time:7307ms step_avg:57.54ms
step:128/2330 train_time:7368ms step_avg:57.56ms
step:129/2330 train_time:7424ms step_avg:57.55ms
step:130/2330 train_time:7482ms step_avg:57.56ms
step:131/2330 train_time:7538ms step_avg:57.54ms
step:132/2330 train_time:7597ms step_avg:57.55ms
step:133/2330 train_time:7653ms step_avg:57.54ms
step:134/2330 train_time:7712ms step_avg:57.55ms
step:135/2330 train_time:7768ms step_avg:57.54ms
step:136/2330 train_time:7827ms step_avg:57.55ms
step:137/2330 train_time:7884ms step_avg:57.55ms
step:138/2330 train_time:7942ms step_avg:57.55ms
step:139/2330 train_time:7999ms step_avg:57.55ms
step:140/2330 train_time:8058ms step_avg:57.56ms
step:141/2330 train_time:8114ms step_avg:57.55ms
step:142/2330 train_time:8173ms step_avg:57.56ms
step:143/2330 train_time:8229ms step_avg:57.54ms
step:144/2330 train_time:8288ms step_avg:57.56ms
step:145/2330 train_time:8345ms step_avg:57.55ms
step:146/2330 train_time:8404ms step_avg:57.56ms
step:147/2330 train_time:8460ms step_avg:57.55ms
step:148/2330 train_time:8519ms step_avg:57.56ms
step:149/2330 train_time:8575ms step_avg:57.55ms
step:150/2330 train_time:8633ms step_avg:57.55ms
step:151/2330 train_time:8690ms step_avg:57.55ms
step:152/2330 train_time:8748ms step_avg:57.55ms
step:153/2330 train_time:8804ms step_avg:57.54ms
step:154/2330 train_time:8864ms step_avg:57.56ms
step:155/2330 train_time:8920ms step_avg:57.55ms
step:156/2330 train_time:8978ms step_avg:57.55ms
step:157/2330 train_time:9034ms step_avg:57.54ms
step:158/2330 train_time:9093ms step_avg:57.55ms
step:159/2330 train_time:9149ms step_avg:57.54ms
step:160/2330 train_time:9208ms step_avg:57.55ms
step:161/2330 train_time:9264ms step_avg:57.54ms
step:162/2330 train_time:9324ms step_avg:57.56ms
step:163/2330 train_time:9381ms step_avg:57.55ms
step:164/2330 train_time:9439ms step_avg:57.56ms
step:165/2330 train_time:9496ms step_avg:57.55ms
step:166/2330 train_time:9554ms step_avg:57.55ms
step:167/2330 train_time:9610ms step_avg:57.54ms
step:168/2330 train_time:9670ms step_avg:57.56ms
step:169/2330 train_time:9725ms step_avg:57.55ms
step:170/2330 train_time:9785ms step_avg:57.56ms
step:171/2330 train_time:9842ms step_avg:57.55ms
step:172/2330 train_time:9900ms step_avg:57.56ms
step:173/2330 train_time:9956ms step_avg:57.55ms
step:174/2330 train_time:10014ms step_avg:57.55ms
step:175/2330 train_time:10070ms step_avg:57.54ms
step:176/2330 train_time:10129ms step_avg:57.55ms
step:177/2330 train_time:10185ms step_avg:57.54ms
step:178/2330 train_time:10244ms step_avg:57.55ms
step:179/2330 train_time:10301ms step_avg:57.55ms
step:180/2330 train_time:10360ms step_avg:57.56ms
step:181/2330 train_time:10416ms step_avg:57.55ms
step:182/2330 train_time:10475ms step_avg:57.55ms
step:183/2330 train_time:10531ms step_avg:57.54ms
step:184/2330 train_time:10590ms step_avg:57.55ms
step:185/2330 train_time:10646ms step_avg:57.54ms
step:186/2330 train_time:10705ms step_avg:57.56ms
step:187/2330 train_time:10761ms step_avg:57.55ms
step:188/2330 train_time:10820ms step_avg:57.55ms
step:189/2330 train_time:10876ms step_avg:57.54ms
step:190/2330 train_time:10935ms step_avg:57.55ms
step:191/2330 train_time:10991ms step_avg:57.54ms
step:192/2330 train_time:11050ms step_avg:57.55ms
step:193/2330 train_time:11106ms step_avg:57.54ms
step:194/2330 train_time:11164ms step_avg:57.55ms
step:195/2330 train_time:11220ms step_avg:57.54ms
step:196/2330 train_time:11279ms step_avg:57.54ms
step:197/2330 train_time:11335ms step_avg:57.54ms
step:198/2330 train_time:11394ms step_avg:57.54ms
step:199/2330 train_time:11451ms step_avg:57.54ms
step:200/2330 train_time:11509ms step_avg:57.55ms
step:201/2330 train_time:11565ms step_avg:57.54ms
step:202/2330 train_time:11625ms step_avg:57.55ms
step:203/2330 train_time:11681ms step_avg:57.54ms
step:204/2330 train_time:11740ms step_avg:57.55ms
step:205/2330 train_time:11796ms step_avg:57.54ms
step:206/2330 train_time:11855ms step_avg:57.55ms
step:207/2330 train_time:11911ms step_avg:57.54ms
step:208/2330 train_time:11969ms step_avg:57.54ms
step:209/2330 train_time:12025ms step_avg:57.54ms
step:210/2330 train_time:12084ms step_avg:57.54ms
step:211/2330 train_time:12140ms step_avg:57.53ms
step:212/2330 train_time:12198ms step_avg:57.54ms
step:213/2330 train_time:12254ms step_avg:57.53ms
step:214/2330 train_time:12313ms step_avg:57.54ms
step:215/2330 train_time:12370ms step_avg:57.54ms
step:216/2330 train_time:12429ms step_avg:57.54ms
step:217/2330 train_time:12485ms step_avg:57.54ms
step:218/2330 train_time:12544ms step_avg:57.54ms
step:219/2330 train_time:12601ms step_avg:57.54ms
step:220/2330 train_time:12660ms step_avg:57.54ms
step:221/2330 train_time:12716ms step_avg:57.54ms
step:222/2330 train_time:12774ms step_avg:57.54ms
step:223/2330 train_time:12830ms step_avg:57.54ms
step:224/2330 train_time:12889ms step_avg:57.54ms
step:225/2330 train_time:12945ms step_avg:57.53ms
step:226/2330 train_time:13004ms step_avg:57.54ms
step:227/2330 train_time:13060ms step_avg:57.53ms
step:228/2330 train_time:13120ms step_avg:57.54ms
step:229/2330 train_time:13176ms step_avg:57.54ms
step:230/2330 train_time:13234ms step_avg:57.54ms
step:231/2330 train_time:13290ms step_avg:57.53ms
step:232/2330 train_time:13350ms step_avg:57.54ms
step:233/2330 train_time:13406ms step_avg:57.53ms
step:234/2330 train_time:13465ms step_avg:57.54ms
step:235/2330 train_time:13521ms step_avg:57.54ms
step:236/2330 train_time:13580ms step_avg:57.54ms
step:237/2330 train_time:13637ms step_avg:57.54ms
step:238/2330 train_time:13696ms step_avg:57.54ms
step:239/2330 train_time:13751ms step_avg:57.54ms
step:240/2330 train_time:13811ms step_avg:57.55ms
step:241/2330 train_time:13867ms step_avg:57.54ms
step:242/2330 train_time:13926ms step_avg:57.55ms
step:243/2330 train_time:13983ms step_avg:57.54ms
step:244/2330 train_time:14042ms step_avg:57.55ms
step:245/2330 train_time:14098ms step_avg:57.54ms
step:246/2330 train_time:14156ms step_avg:57.55ms
step:247/2330 train_time:14213ms step_avg:57.54ms
step:248/2330 train_time:14271ms step_avg:57.55ms
step:249/2330 train_time:14327ms step_avg:57.54ms
step:250/2330 train_time:14387ms step_avg:57.55ms
step:250/2330 val_loss:4.9340 train_time:14466ms step_avg:57.86ms
step:251/2330 train_time:14485ms step_avg:57.71ms
step:252/2330 train_time:14506ms step_avg:57.56ms
step:253/2330 train_time:14560ms step_avg:57.55ms
step:254/2330 train_time:14627ms step_avg:57.59ms
step:255/2330 train_time:14684ms step_avg:57.58ms
step:256/2330 train_time:14744ms step_avg:57.59ms
step:257/2330 train_time:14801ms step_avg:57.59ms
step:258/2330 train_time:14859ms step_avg:57.59ms
step:259/2330 train_time:14915ms step_avg:57.59ms
step:260/2330 train_time:14974ms step_avg:57.59ms
step:261/2330 train_time:15029ms step_avg:57.58ms
step:262/2330 train_time:15087ms step_avg:57.59ms
step:263/2330 train_time:15143ms step_avg:57.58ms
step:264/2330 train_time:15201ms step_avg:57.58ms
step:265/2330 train_time:15256ms step_avg:57.57ms
step:266/2330 train_time:15315ms step_avg:57.57ms
step:267/2330 train_time:15371ms step_avg:57.57ms
step:268/2330 train_time:15429ms step_avg:57.57ms
step:269/2330 train_time:15485ms step_avg:57.56ms
step:270/2330 train_time:15545ms step_avg:57.57ms
step:271/2330 train_time:15602ms step_avg:57.57ms
step:272/2330 train_time:15662ms step_avg:57.58ms
step:273/2330 train_time:15718ms step_avg:57.57ms
step:274/2330 train_time:15779ms step_avg:57.59ms
step:275/2330 train_time:15835ms step_avg:57.58ms
step:276/2330 train_time:15893ms step_avg:57.58ms
step:277/2330 train_time:15950ms step_avg:57.58ms
step:278/2330 train_time:16008ms step_avg:57.58ms
step:279/2330 train_time:16064ms step_avg:57.58ms
step:280/2330 train_time:16122ms step_avg:57.58ms
step:281/2330 train_time:16178ms step_avg:57.57ms
step:282/2330 train_time:16237ms step_avg:57.58ms
step:283/2330 train_time:16293ms step_avg:57.57ms
step:284/2330 train_time:16351ms step_avg:57.57ms
step:285/2330 train_time:16407ms step_avg:57.57ms
step:286/2330 train_time:16465ms step_avg:57.57ms
step:287/2330 train_time:16521ms step_avg:57.57ms
step:288/2330 train_time:16580ms step_avg:57.57ms
step:289/2330 train_time:16636ms step_avg:57.56ms
step:290/2330 train_time:16697ms step_avg:57.58ms
step:291/2330 train_time:16754ms step_avg:57.57ms
step:292/2330 train_time:16814ms step_avg:57.58ms
step:293/2330 train_time:16872ms step_avg:57.58ms
step:294/2330 train_time:16930ms step_avg:57.58ms
step:295/2330 train_time:16985ms step_avg:57.58ms
step:296/2330 train_time:17044ms step_avg:57.58ms
step:297/2330 train_time:17099ms step_avg:57.57ms
step:298/2330 train_time:17158ms step_avg:57.58ms
step:299/2330 train_time:17214ms step_avg:57.57ms
step:300/2330 train_time:17272ms step_avg:57.57ms
step:301/2330 train_time:17328ms step_avg:57.57ms
step:302/2330 train_time:17386ms step_avg:57.57ms
step:303/2330 train_time:17444ms step_avg:57.57ms
step:304/2330 train_time:17502ms step_avg:57.57ms
step:305/2330 train_time:17558ms step_avg:57.57ms
step:306/2330 train_time:17618ms step_avg:57.57ms
step:307/2330 train_time:17675ms step_avg:57.57ms
step:308/2330 train_time:17733ms step_avg:57.58ms
step:309/2330 train_time:17790ms step_avg:57.57ms
step:310/2330 train_time:17849ms step_avg:57.58ms
step:311/2330 train_time:17905ms step_avg:57.57ms
step:312/2330 train_time:17964ms step_avg:57.58ms
step:313/2330 train_time:18019ms step_avg:57.57ms
step:314/2330 train_time:18079ms step_avg:57.58ms
step:315/2330 train_time:18134ms step_avg:57.57ms
step:316/2330 train_time:18194ms step_avg:57.57ms
step:317/2330 train_time:18249ms step_avg:57.57ms
step:318/2330 train_time:18309ms step_avg:57.58ms
step:319/2330 train_time:18364ms step_avg:57.57ms
step:320/2330 train_time:18423ms step_avg:57.57ms
step:321/2330 train_time:18478ms step_avg:57.57ms
step:322/2330 train_time:18538ms step_avg:57.57ms
step:323/2330 train_time:18594ms step_avg:57.57ms
step:324/2330 train_time:18653ms step_avg:57.57ms
step:325/2330 train_time:18710ms step_avg:57.57ms
step:326/2330 train_time:18768ms step_avg:57.57ms
step:327/2330 train_time:18824ms step_avg:57.57ms
step:328/2330 train_time:18884ms step_avg:57.57ms
step:329/2330 train_time:18941ms step_avg:57.57ms
step:330/2330 train_time:18999ms step_avg:57.57ms
step:331/2330 train_time:19055ms step_avg:57.57ms
step:332/2330 train_time:19114ms step_avg:57.57ms
step:333/2330 train_time:19170ms step_avg:57.57ms
step:334/2330 train_time:19229ms step_avg:57.57ms
step:335/2330 train_time:19285ms step_avg:57.57ms
step:336/2330 train_time:19343ms step_avg:57.57ms
step:337/2330 train_time:19400ms step_avg:57.57ms
step:338/2330 train_time:19459ms step_avg:57.57ms
step:339/2330 train_time:19515ms step_avg:57.57ms
step:340/2330 train_time:19574ms step_avg:57.57ms
step:341/2330 train_time:19630ms step_avg:57.57ms
step:342/2330 train_time:19689ms step_avg:57.57ms
step:343/2330 train_time:19746ms step_avg:57.57ms
step:344/2330 train_time:19804ms step_avg:57.57ms
step:345/2330 train_time:19860ms step_avg:57.57ms
step:346/2330 train_time:19919ms step_avg:57.57ms
step:347/2330 train_time:19975ms step_avg:57.56ms
step:348/2330 train_time:20034ms step_avg:57.57ms
step:349/2330 train_time:20090ms step_avg:57.57ms
step:350/2330 train_time:20149ms step_avg:57.57ms
step:351/2330 train_time:20205ms step_avg:57.56ms
step:352/2330 train_time:20264ms step_avg:57.57ms
step:353/2330 train_time:20320ms step_avg:57.56ms
step:354/2330 train_time:20378ms step_avg:57.57ms
step:355/2330 train_time:20434ms step_avg:57.56ms
step:356/2330 train_time:20493ms step_avg:57.56ms
step:357/2330 train_time:20548ms step_avg:57.56ms
step:358/2330 train_time:20608ms step_avg:57.56ms
step:359/2330 train_time:20664ms step_avg:57.56ms
step:360/2330 train_time:20723ms step_avg:57.56ms
step:361/2330 train_time:20780ms step_avg:57.56ms
step:362/2330 train_time:20839ms step_avg:57.57ms
step:363/2330 train_time:20895ms step_avg:57.56ms
step:364/2330 train_time:20954ms step_avg:57.57ms
step:365/2330 train_time:21010ms step_avg:57.56ms
step:366/2330 train_time:21069ms step_avg:57.57ms
step:367/2330 train_time:21125ms step_avg:57.56ms
step:368/2330 train_time:21183ms step_avg:57.56ms
step:369/2330 train_time:21240ms step_avg:57.56ms
step:370/2330 train_time:21299ms step_avg:57.56ms
step:371/2330 train_time:21355ms step_avg:57.56ms
step:372/2330 train_time:21414ms step_avg:57.56ms
step:373/2330 train_time:21470ms step_avg:57.56ms
step:374/2330 train_time:21529ms step_avg:57.56ms
step:375/2330 train_time:21585ms step_avg:57.56ms
step:376/2330 train_time:21644ms step_avg:57.56ms
step:377/2330 train_time:21701ms step_avg:57.56ms
step:378/2330 train_time:21760ms step_avg:57.56ms
step:379/2330 train_time:21816ms step_avg:57.56ms
step:380/2330 train_time:21875ms step_avg:57.57ms
step:381/2330 train_time:21930ms step_avg:57.56ms
step:382/2330 train_time:21990ms step_avg:57.57ms
step:383/2330 train_time:22047ms step_avg:57.56ms
step:384/2330 train_time:22106ms step_avg:57.57ms
step:385/2330 train_time:22162ms step_avg:57.56ms
step:386/2330 train_time:22221ms step_avg:57.57ms
step:387/2330 train_time:22276ms step_avg:57.56ms
step:388/2330 train_time:22336ms step_avg:57.57ms
step:389/2330 train_time:22393ms step_avg:57.56ms
step:390/2330 train_time:22452ms step_avg:57.57ms
step:391/2330 train_time:22507ms step_avg:57.56ms
step:392/2330 train_time:22566ms step_avg:57.57ms
step:393/2330 train_time:22622ms step_avg:57.56ms
step:394/2330 train_time:22681ms step_avg:57.57ms
step:395/2330 train_time:22737ms step_avg:57.56ms
step:396/2330 train_time:22797ms step_avg:57.57ms
step:397/2330 train_time:22853ms step_avg:57.56ms
step:398/2330 train_time:22911ms step_avg:57.57ms
step:399/2330 train_time:22968ms step_avg:57.56ms
step:400/2330 train_time:23026ms step_avg:57.57ms
step:401/2330 train_time:23083ms step_avg:57.56ms
step:402/2330 train_time:23141ms step_avg:57.56ms
step:403/2330 train_time:23196ms step_avg:57.56ms
step:404/2330 train_time:23256ms step_avg:57.56ms
step:405/2330 train_time:23312ms step_avg:57.56ms
step:406/2330 train_time:23370ms step_avg:57.56ms
step:407/2330 train_time:23426ms step_avg:57.56ms
step:408/2330 train_time:23485ms step_avg:57.56ms
step:409/2330 train_time:23541ms step_avg:57.56ms
step:410/2330 train_time:23600ms step_avg:57.56ms
step:411/2330 train_time:23656ms step_avg:57.56ms
step:412/2330 train_time:23716ms step_avg:57.56ms
step:413/2330 train_time:23773ms step_avg:57.56ms
step:414/2330 train_time:23832ms step_avg:57.56ms
step:415/2330 train_time:23889ms step_avg:57.56ms
step:416/2330 train_time:23948ms step_avg:57.57ms
step:417/2330 train_time:24004ms step_avg:57.56ms
step:418/2330 train_time:24063ms step_avg:57.57ms
step:419/2330 train_time:24120ms step_avg:57.56ms
step:420/2330 train_time:24179ms step_avg:57.57ms
step:421/2330 train_time:24235ms step_avg:57.56ms
step:422/2330 train_time:24294ms step_avg:57.57ms
step:423/2330 train_time:24350ms step_avg:57.56ms
step:424/2330 train_time:24409ms step_avg:57.57ms
step:425/2330 train_time:24465ms step_avg:57.56ms
step:426/2330 train_time:24523ms step_avg:57.57ms
step:427/2330 train_time:24579ms step_avg:57.56ms
step:428/2330 train_time:24639ms step_avg:57.57ms
step:429/2330 train_time:24695ms step_avg:57.56ms
step:430/2330 train_time:24754ms step_avg:57.57ms
step:431/2330 train_time:24810ms step_avg:57.56ms
step:432/2330 train_time:24870ms step_avg:57.57ms
step:433/2330 train_time:24927ms step_avg:57.57ms
step:434/2330 train_time:24985ms step_avg:57.57ms
step:435/2330 train_time:25042ms step_avg:57.57ms
step:436/2330 train_time:25100ms step_avg:57.57ms
step:437/2330 train_time:25156ms step_avg:57.57ms
step:438/2330 train_time:25215ms step_avg:57.57ms
step:439/2330 train_time:25271ms step_avg:57.57ms
step:440/2330 train_time:25330ms step_avg:57.57ms
step:441/2330 train_time:25386ms step_avg:57.56ms
step:442/2330 train_time:25444ms step_avg:57.57ms
step:443/2330 train_time:25500ms step_avg:57.56ms
step:444/2330 train_time:25560ms step_avg:57.57ms
step:445/2330 train_time:25615ms step_avg:57.56ms
step:446/2330 train_time:25675ms step_avg:57.57ms
step:447/2330 train_time:25731ms step_avg:57.56ms
step:448/2330 train_time:25790ms step_avg:57.57ms
step:449/2330 train_time:25847ms step_avg:57.56ms
step:450/2330 train_time:25905ms step_avg:57.57ms
step:451/2330 train_time:25961ms step_avg:57.56ms
step:452/2330 train_time:26019ms step_avg:57.57ms
step:453/2330 train_time:26075ms step_avg:57.56ms
step:454/2330 train_time:26135ms step_avg:57.57ms
step:455/2330 train_time:26191ms step_avg:57.56ms
step:456/2330 train_time:26250ms step_avg:57.57ms
step:457/2330 train_time:26306ms step_avg:57.56ms
step:458/2330 train_time:26365ms step_avg:57.57ms
step:459/2330 train_time:26421ms step_avg:57.56ms
step:460/2330 train_time:26480ms step_avg:57.57ms
step:461/2330 train_time:26536ms step_avg:57.56ms
step:462/2330 train_time:26596ms step_avg:57.57ms
step:463/2330 train_time:26652ms step_avg:57.56ms
step:464/2330 train_time:26712ms step_avg:57.57ms
step:465/2330 train_time:26768ms step_avg:57.56ms
step:466/2330 train_time:26827ms step_avg:57.57ms
step:467/2330 train_time:26883ms step_avg:57.57ms
step:468/2330 train_time:26941ms step_avg:57.57ms
step:469/2330 train_time:26998ms step_avg:57.56ms
step:470/2330 train_time:27057ms step_avg:57.57ms
step:471/2330 train_time:27114ms step_avg:57.57ms
step:472/2330 train_time:27173ms step_avg:57.57ms
step:473/2330 train_time:27229ms step_avg:57.57ms
step:474/2330 train_time:27287ms step_avg:57.57ms
step:475/2330 train_time:27344ms step_avg:57.57ms
step:476/2330 train_time:27403ms step_avg:57.57ms
step:477/2330 train_time:27459ms step_avg:57.57ms
step:478/2330 train_time:27517ms step_avg:57.57ms
step:479/2330 train_time:27573ms step_avg:57.56ms
step:480/2330 train_time:27633ms step_avg:57.57ms
step:481/2330 train_time:27689ms step_avg:57.56ms
step:482/2330 train_time:27749ms step_avg:57.57ms
step:483/2330 train_time:27804ms step_avg:57.57ms
step:484/2330 train_time:27863ms step_avg:57.57ms
step:485/2330 train_time:27918ms step_avg:57.56ms
step:486/2330 train_time:27978ms step_avg:57.57ms
step:487/2330 train_time:28034ms step_avg:57.57ms
step:488/2330 train_time:28094ms step_avg:57.57ms
step:489/2330 train_time:28151ms step_avg:57.57ms
step:490/2330 train_time:28210ms step_avg:57.57ms
step:491/2330 train_time:28266ms step_avg:57.57ms
step:492/2330 train_time:28325ms step_avg:57.57ms
step:493/2330 train_time:28381ms step_avg:57.57ms
step:494/2330 train_time:28439ms step_avg:57.57ms
step:495/2330 train_time:28495ms step_avg:57.57ms
step:496/2330 train_time:28554ms step_avg:57.57ms
step:497/2330 train_time:28610ms step_avg:57.57ms
step:498/2330 train_time:28669ms step_avg:57.57ms
step:499/2330 train_time:28725ms step_avg:57.57ms
step:500/2330 train_time:28784ms step_avg:57.57ms
step:500/2330 val_loss:4.4320 train_time:28863ms step_avg:57.73ms
step:501/2330 train_time:28883ms step_avg:57.65ms
step:502/2330 train_time:28903ms step_avg:57.58ms
step:503/2330 train_time:28959ms step_avg:57.57ms
step:504/2330 train_time:29023ms step_avg:57.59ms
step:505/2330 train_time:29082ms step_avg:57.59ms
step:506/2330 train_time:29141ms step_avg:57.59ms
step:507/2330 train_time:29198ms step_avg:57.59ms
step:508/2330 train_time:29257ms step_avg:57.59ms
step:509/2330 train_time:29313ms step_avg:57.59ms
step:510/2330 train_time:29372ms step_avg:57.59ms
step:511/2330 train_time:29427ms step_avg:57.59ms
step:512/2330 train_time:29485ms step_avg:57.59ms
step:513/2330 train_time:29541ms step_avg:57.58ms
step:514/2330 train_time:29599ms step_avg:57.58ms
step:515/2330 train_time:29654ms step_avg:57.58ms
step:516/2330 train_time:29712ms step_avg:57.58ms
step:517/2330 train_time:29768ms step_avg:57.58ms
step:518/2330 train_time:29827ms step_avg:57.58ms
step:519/2330 train_time:29883ms step_avg:57.58ms
step:520/2330 train_time:29944ms step_avg:57.59ms
step:521/2330 train_time:30001ms step_avg:57.58ms
step:522/2330 train_time:30062ms step_avg:57.59ms
step:523/2330 train_time:30119ms step_avg:57.59ms
step:524/2330 train_time:30179ms step_avg:57.59ms
step:525/2330 train_time:30236ms step_avg:57.59ms
step:526/2330 train_time:30295ms step_avg:57.59ms
step:527/2330 train_time:30350ms step_avg:57.59ms
step:528/2330 train_time:30409ms step_avg:57.59ms
step:529/2330 train_time:30464ms step_avg:57.59ms
step:530/2330 train_time:30524ms step_avg:57.59ms
step:531/2330 train_time:30579ms step_avg:57.59ms
step:532/2330 train_time:30638ms step_avg:57.59ms
step:533/2330 train_time:30694ms step_avg:57.59ms
step:534/2330 train_time:30752ms step_avg:57.59ms
step:535/2330 train_time:30808ms step_avg:57.58ms
step:536/2330 train_time:30867ms step_avg:57.59ms
step:537/2330 train_time:30924ms step_avg:57.59ms
step:538/2330 train_time:30984ms step_avg:57.59ms
step:539/2330 train_time:31040ms step_avg:57.59ms
step:540/2330 train_time:31100ms step_avg:57.59ms
step:541/2330 train_time:31157ms step_avg:57.59ms
step:542/2330 train_time:31216ms step_avg:57.59ms
step:543/2330 train_time:31272ms step_avg:57.59ms
step:544/2330 train_time:31331ms step_avg:57.59ms
step:545/2330 train_time:31387ms step_avg:57.59ms
step:546/2330 train_time:31446ms step_avg:57.59ms
step:547/2330 train_time:31501ms step_avg:57.59ms
step:548/2330 train_time:31561ms step_avg:57.59ms
step:549/2330 train_time:31617ms step_avg:57.59ms
step:550/2330 train_time:31675ms step_avg:57.59ms
step:551/2330 train_time:31731ms step_avg:57.59ms
step:552/2330 train_time:31791ms step_avg:57.59ms
step:553/2330 train_time:31847ms step_avg:57.59ms
step:554/2330 train_time:31905ms step_avg:57.59ms
step:555/2330 train_time:31962ms step_avg:57.59ms
step:556/2330 train_time:32021ms step_avg:57.59ms
step:557/2330 train_time:32077ms step_avg:57.59ms
step:558/2330 train_time:32138ms step_avg:57.59ms
step:559/2330 train_time:32195ms step_avg:57.59ms
step:560/2330 train_time:32254ms step_avg:57.60ms
step:561/2330 train_time:32311ms step_avg:57.60ms
step:562/2330 train_time:32370ms step_avg:57.60ms
step:563/2330 train_time:32426ms step_avg:57.60ms
step:564/2330 train_time:32485ms step_avg:57.60ms
step:565/2330 train_time:32541ms step_avg:57.59ms
step:566/2330 train_time:32599ms step_avg:57.60ms
step:567/2330 train_time:32655ms step_avg:57.59ms
step:568/2330 train_time:32714ms step_avg:57.59ms
step:569/2330 train_time:32770ms step_avg:57.59ms
step:570/2330 train_time:32829ms step_avg:57.59ms
step:571/2330 train_time:32885ms step_avg:57.59ms
step:572/2330 train_time:32945ms step_avg:57.60ms
step:573/2330 train_time:33000ms step_avg:57.59ms
step:574/2330 train_time:33062ms step_avg:57.60ms
step:575/2330 train_time:33118ms step_avg:57.60ms
step:576/2330 train_time:33179ms step_avg:57.60ms
step:577/2330 train_time:33235ms step_avg:57.60ms
step:578/2330 train_time:33295ms step_avg:57.60ms
step:579/2330 train_time:33350ms step_avg:57.60ms
step:580/2330 train_time:33409ms step_avg:57.60ms
step:581/2330 train_time:33465ms step_avg:57.60ms
step:582/2330 train_time:33524ms step_avg:57.60ms
step:583/2330 train_time:33580ms step_avg:57.60ms
step:584/2330 train_time:33639ms step_avg:57.60ms
step:585/2330 train_time:33696ms step_avg:57.60ms
step:586/2330 train_time:33754ms step_avg:57.60ms
step:587/2330 train_time:33810ms step_avg:57.60ms
step:588/2330 train_time:33869ms step_avg:57.60ms
step:589/2330 train_time:33924ms step_avg:57.60ms
step:590/2330 train_time:33984ms step_avg:57.60ms
step:591/2330 train_time:34040ms step_avg:57.60ms
step:592/2330 train_time:34101ms step_avg:57.60ms
step:593/2330 train_time:34157ms step_avg:57.60ms
step:594/2330 train_time:34218ms step_avg:57.61ms
step:595/2330 train_time:34275ms step_avg:57.60ms
step:596/2330 train_time:34334ms step_avg:57.61ms
step:597/2330 train_time:34391ms step_avg:57.61ms
step:598/2330 train_time:34449ms step_avg:57.61ms
step:599/2330 train_time:34505ms step_avg:57.60ms
step:600/2330 train_time:34564ms step_avg:57.61ms
step:601/2330 train_time:34620ms step_avg:57.60ms
step:602/2330 train_time:34679ms step_avg:57.61ms
step:603/2330 train_time:34735ms step_avg:57.60ms
step:604/2330 train_time:34795ms step_avg:57.61ms
step:605/2330 train_time:34851ms step_avg:57.60ms
step:606/2330 train_time:34909ms step_avg:57.61ms
step:607/2330 train_time:34966ms step_avg:57.60ms
step:608/2330 train_time:35025ms step_avg:57.61ms
step:609/2330 train_time:35081ms step_avg:57.60ms
step:610/2330 train_time:35141ms step_avg:57.61ms
step:611/2330 train_time:35197ms step_avg:57.61ms
step:612/2330 train_time:35257ms step_avg:57.61ms
step:613/2330 train_time:35313ms step_avg:57.61ms
step:614/2330 train_time:35372ms step_avg:57.61ms
step:615/2330 train_time:35429ms step_avg:57.61ms
step:616/2330 train_time:35487ms step_avg:57.61ms
step:617/2330 train_time:35543ms step_avg:57.61ms
step:618/2330 train_time:35603ms step_avg:57.61ms
step:619/2330 train_time:35659ms step_avg:57.61ms
step:620/2330 train_time:35717ms step_avg:57.61ms
step:621/2330 train_time:35775ms step_avg:57.61ms
step:622/2330 train_time:35833ms step_avg:57.61ms
step:623/2330 train_time:35890ms step_avg:57.61ms
step:624/2330 train_time:35949ms step_avg:57.61ms
step:625/2330 train_time:36005ms step_avg:57.61ms
step:626/2330 train_time:36064ms step_avg:57.61ms
step:627/2330 train_time:36120ms step_avg:57.61ms
step:628/2330 train_time:36180ms step_avg:57.61ms
step:629/2330 train_time:36235ms step_avg:57.61ms
step:630/2330 train_time:36294ms step_avg:57.61ms
step:631/2330 train_time:36350ms step_avg:57.61ms
step:632/2330 train_time:36409ms step_avg:57.61ms
step:633/2330 train_time:36465ms step_avg:57.61ms
step:634/2330 train_time:36524ms step_avg:57.61ms
step:635/2330 train_time:36580ms step_avg:57.61ms
step:636/2330 train_time:36640ms step_avg:57.61ms
step:637/2330 train_time:36696ms step_avg:57.61ms
step:638/2330 train_time:36755ms step_avg:57.61ms
step:639/2330 train_time:36811ms step_avg:57.61ms
step:640/2330 train_time:36870ms step_avg:57.61ms
step:641/2330 train_time:36926ms step_avg:57.61ms
step:642/2330 train_time:36985ms step_avg:57.61ms
step:643/2330 train_time:37041ms step_avg:57.61ms
step:644/2330 train_time:37100ms step_avg:57.61ms
step:645/2330 train_time:37157ms step_avg:57.61ms
step:646/2330 train_time:37215ms step_avg:57.61ms
step:647/2330 train_time:37272ms step_avg:57.61ms
step:648/2330 train_time:37330ms step_avg:57.61ms
step:649/2330 train_time:37387ms step_avg:57.61ms
step:650/2330 train_time:37445ms step_avg:57.61ms
step:651/2330 train_time:37500ms step_avg:57.60ms
step:652/2330 train_time:37561ms step_avg:57.61ms
step:653/2330 train_time:37617ms step_avg:57.61ms
step:654/2330 train_time:37676ms step_avg:57.61ms
step:655/2330 train_time:37732ms step_avg:57.61ms
step:656/2330 train_time:37791ms step_avg:57.61ms
step:657/2330 train_time:37847ms step_avg:57.61ms
step:658/2330 train_time:37905ms step_avg:57.61ms
step:659/2330 train_time:37961ms step_avg:57.60ms
step:660/2330 train_time:38020ms step_avg:57.61ms
step:661/2330 train_time:38077ms step_avg:57.61ms
step:662/2330 train_time:38136ms step_avg:57.61ms
step:663/2330 train_time:38193ms step_avg:57.61ms
step:664/2330 train_time:38251ms step_avg:57.61ms
step:665/2330 train_time:38308ms step_avg:57.61ms
step:666/2330 train_time:38366ms step_avg:57.61ms
step:667/2330 train_time:38422ms step_avg:57.60ms
step:668/2330 train_time:38482ms step_avg:57.61ms
step:669/2330 train_time:38538ms step_avg:57.61ms
step:670/2330 train_time:38597ms step_avg:57.61ms
step:671/2330 train_time:38653ms step_avg:57.61ms
step:672/2330 train_time:38712ms step_avg:57.61ms
step:673/2330 train_time:38768ms step_avg:57.60ms
step:674/2330 train_time:38827ms step_avg:57.61ms
step:675/2330 train_time:38883ms step_avg:57.60ms
step:676/2330 train_time:38942ms step_avg:57.61ms
step:677/2330 train_time:38998ms step_avg:57.60ms
step:678/2330 train_time:39058ms step_avg:57.61ms
step:679/2330 train_time:39114ms step_avg:57.61ms
step:680/2330 train_time:39173ms step_avg:57.61ms
step:681/2330 train_time:39230ms step_avg:57.61ms
step:682/2330 train_time:39289ms step_avg:57.61ms
step:683/2330 train_time:39344ms step_avg:57.61ms
step:684/2330 train_time:39403ms step_avg:57.61ms
step:685/2330 train_time:39459ms step_avg:57.60ms
step:686/2330 train_time:39518ms step_avg:57.61ms
step:687/2330 train_time:39575ms step_avg:57.61ms
step:688/2330 train_time:39634ms step_avg:57.61ms
step:689/2330 train_time:39690ms step_avg:57.61ms
step:690/2330 train_time:39749ms step_avg:57.61ms
step:691/2330 train_time:39805ms step_avg:57.61ms
step:692/2330 train_time:39864ms step_avg:57.61ms
step:693/2330 train_time:39920ms step_avg:57.60ms
step:694/2330 train_time:39980ms step_avg:57.61ms
step:695/2330 train_time:40036ms step_avg:57.61ms
step:696/2330 train_time:40097ms step_avg:57.61ms
step:697/2330 train_time:40153ms step_avg:57.61ms
step:698/2330 train_time:40212ms step_avg:57.61ms
step:699/2330 train_time:40268ms step_avg:57.61ms
step:700/2330 train_time:40327ms step_avg:57.61ms
step:701/2330 train_time:40383ms step_avg:57.61ms
step:702/2330 train_time:40443ms step_avg:57.61ms
step:703/2330 train_time:40499ms step_avg:57.61ms
step:704/2330 train_time:40558ms step_avg:57.61ms
step:705/2330 train_time:40615ms step_avg:57.61ms
step:706/2330 train_time:40674ms step_avg:57.61ms
step:707/2330 train_time:40730ms step_avg:57.61ms
step:708/2330 train_time:40788ms step_avg:57.61ms
step:709/2330 train_time:40844ms step_avg:57.61ms
step:710/2330 train_time:40903ms step_avg:57.61ms
step:711/2330 train_time:40960ms step_avg:57.61ms
step:712/2330 train_time:41019ms step_avg:57.61ms
step:713/2330 train_time:41076ms step_avg:57.61ms
step:714/2330 train_time:41135ms step_avg:57.61ms
step:715/2330 train_time:41192ms step_avg:57.61ms
step:716/2330 train_time:41250ms step_avg:57.61ms
step:717/2330 train_time:41307ms step_avg:57.61ms
step:718/2330 train_time:41366ms step_avg:57.61ms
step:719/2330 train_time:41422ms step_avg:57.61ms
step:720/2330 train_time:41481ms step_avg:57.61ms
step:721/2330 train_time:41538ms step_avg:57.61ms
step:722/2330 train_time:41597ms step_avg:57.61ms
step:723/2330 train_time:41654ms step_avg:57.61ms
step:724/2330 train_time:41713ms step_avg:57.61ms
step:725/2330 train_time:41769ms step_avg:57.61ms
step:726/2330 train_time:41828ms step_avg:57.61ms
step:727/2330 train_time:41884ms step_avg:57.61ms
step:728/2330 train_time:41943ms step_avg:57.61ms
step:729/2330 train_time:41999ms step_avg:57.61ms
step:730/2330 train_time:42059ms step_avg:57.62ms
step:731/2330 train_time:42116ms step_avg:57.61ms
step:732/2330 train_time:42174ms step_avg:57.62ms
step:733/2330 train_time:42231ms step_avg:57.61ms
step:734/2330 train_time:42289ms step_avg:57.62ms
step:735/2330 train_time:42346ms step_avg:57.61ms
step:736/2330 train_time:42404ms step_avg:57.61ms
step:737/2330 train_time:42460ms step_avg:57.61ms
step:738/2330 train_time:42520ms step_avg:57.62ms
step:739/2330 train_time:42578ms step_avg:57.62ms
step:740/2330 train_time:42637ms step_avg:57.62ms
step:741/2330 train_time:42693ms step_avg:57.62ms
step:742/2330 train_time:42752ms step_avg:57.62ms
step:743/2330 train_time:42808ms step_avg:57.61ms
step:744/2330 train_time:42867ms step_avg:57.62ms
step:745/2330 train_time:42922ms step_avg:57.61ms
step:746/2330 train_time:42984ms step_avg:57.62ms
step:747/2330 train_time:43040ms step_avg:57.62ms
step:748/2330 train_time:43100ms step_avg:57.62ms
step:749/2330 train_time:43156ms step_avg:57.62ms
step:750/2330 train_time:43216ms step_avg:57.62ms
step:750/2330 val_loss:4.2323 train_time:43295ms step_avg:57.73ms
step:751/2330 train_time:43315ms step_avg:57.68ms
step:752/2330 train_time:43335ms step_avg:57.63ms
step:753/2330 train_time:43391ms step_avg:57.62ms
step:754/2330 train_time:43454ms step_avg:57.63ms
step:755/2330 train_time:43511ms step_avg:57.63ms
step:756/2330 train_time:43573ms step_avg:57.64ms
step:757/2330 train_time:43628ms step_avg:57.63ms
step:758/2330 train_time:43688ms step_avg:57.64ms
step:759/2330 train_time:43743ms step_avg:57.63ms
step:760/2330 train_time:43802ms step_avg:57.63ms
step:761/2330 train_time:43858ms step_avg:57.63ms
step:762/2330 train_time:43916ms step_avg:57.63ms
step:763/2330 train_time:43972ms step_avg:57.63ms
step:764/2330 train_time:44030ms step_avg:57.63ms
step:765/2330 train_time:44087ms step_avg:57.63ms
step:766/2330 train_time:44144ms step_avg:57.63ms
step:767/2330 train_time:44201ms step_avg:57.63ms
step:768/2330 train_time:44261ms step_avg:57.63ms
step:769/2330 train_time:44320ms step_avg:57.63ms
step:770/2330 train_time:44381ms step_avg:57.64ms
step:771/2330 train_time:44439ms step_avg:57.64ms
step:772/2330 train_time:44500ms step_avg:57.64ms
step:773/2330 train_time:44558ms step_avg:57.64ms
step:774/2330 train_time:44618ms step_avg:57.65ms
step:775/2330 train_time:44676ms step_avg:57.65ms
step:776/2330 train_time:44735ms step_avg:57.65ms
step:777/2330 train_time:44791ms step_avg:57.65ms
step:778/2330 train_time:44850ms step_avg:57.65ms
step:779/2330 train_time:44906ms step_avg:57.65ms
step:780/2330 train_time:44966ms step_avg:57.65ms
step:781/2330 train_time:45023ms step_avg:57.65ms
step:782/2330 train_time:45082ms step_avg:57.65ms
step:783/2330 train_time:45138ms step_avg:57.65ms
step:784/2330 train_time:45198ms step_avg:57.65ms
step:785/2330 train_time:45255ms step_avg:57.65ms
step:786/2330 train_time:45314ms step_avg:57.65ms
step:787/2330 train_time:45372ms step_avg:57.65ms
step:788/2330 train_time:45433ms step_avg:57.66ms
step:789/2330 train_time:45490ms step_avg:57.66ms
step:790/2330 train_time:45551ms step_avg:57.66ms
step:791/2330 train_time:45609ms step_avg:57.66ms
step:792/2330 train_time:45668ms step_avg:57.66ms
step:793/2330 train_time:45726ms step_avg:57.66ms
step:794/2330 train_time:45786ms step_avg:57.66ms
step:795/2330 train_time:45843ms step_avg:57.66ms
step:796/2330 train_time:45901ms step_avg:57.66ms
step:797/2330 train_time:45958ms step_avg:57.66ms
step:798/2330 train_time:46017ms step_avg:57.67ms
step:799/2330 train_time:46074ms step_avg:57.66ms
step:800/2330 train_time:46133ms step_avg:57.67ms
step:801/2330 train_time:46191ms step_avg:57.67ms
step:802/2330 train_time:46249ms step_avg:57.67ms
step:803/2330 train_time:46307ms step_avg:57.67ms
step:804/2330 train_time:46366ms step_avg:57.67ms
step:805/2330 train_time:46423ms step_avg:57.67ms
step:806/2330 train_time:46484ms step_avg:57.67ms
step:807/2330 train_time:46543ms step_avg:57.67ms
step:808/2330 train_time:46602ms step_avg:57.68ms
step:809/2330 train_time:46659ms step_avg:57.67ms
step:810/2330 train_time:46720ms step_avg:57.68ms
step:811/2330 train_time:46777ms step_avg:57.68ms
step:812/2330 train_time:46837ms step_avg:57.68ms
step:813/2330 train_time:46894ms step_avg:57.68ms
step:814/2330 train_time:46953ms step_avg:57.68ms
step:815/2330 train_time:47009ms step_avg:57.68ms
step:816/2330 train_time:47068ms step_avg:57.68ms
step:817/2330 train_time:47125ms step_avg:57.68ms
step:818/2330 train_time:47185ms step_avg:57.68ms
step:819/2330 train_time:47242ms step_avg:57.68ms
step:820/2330 train_time:47302ms step_avg:57.69ms
step:821/2330 train_time:47359ms step_avg:57.69ms
step:822/2330 train_time:47420ms step_avg:57.69ms
step:823/2330 train_time:47477ms step_avg:57.69ms
step:824/2330 train_time:47537ms step_avg:57.69ms
step:825/2330 train_time:47595ms step_avg:57.69ms
step:826/2330 train_time:47654ms step_avg:57.69ms
step:827/2330 train_time:47711ms step_avg:57.69ms
step:828/2330 train_time:47771ms step_avg:57.69ms
step:829/2330 train_time:47829ms step_avg:57.69ms
step:830/2330 train_time:47888ms step_avg:57.70ms
step:831/2330 train_time:47945ms step_avg:57.70ms
step:832/2330 train_time:48004ms step_avg:57.70ms
step:833/2330 train_time:48061ms step_avg:57.70ms
step:834/2330 train_time:48121ms step_avg:57.70ms
step:835/2330 train_time:48178ms step_avg:57.70ms
step:836/2330 train_time:48237ms step_avg:57.70ms
step:837/2330 train_time:48294ms step_avg:57.70ms
step:838/2330 train_time:48354ms step_avg:57.70ms
step:839/2330 train_time:48411ms step_avg:57.70ms
step:840/2330 train_time:48471ms step_avg:57.70ms
step:841/2330 train_time:48528ms step_avg:57.70ms
step:842/2330 train_time:48588ms step_avg:57.71ms
step:843/2330 train_time:48645ms step_avg:57.70ms
step:844/2330 train_time:48705ms step_avg:57.71ms
step:845/2330 train_time:48762ms step_avg:57.71ms
step:846/2330 train_time:48822ms step_avg:57.71ms
step:847/2330 train_time:48879ms step_avg:57.71ms
step:848/2330 train_time:48938ms step_avg:57.71ms
step:849/2330 train_time:48995ms step_avg:57.71ms
step:850/2330 train_time:49054ms step_avg:57.71ms
step:851/2330 train_time:49111ms step_avg:57.71ms
step:852/2330 train_time:49170ms step_avg:57.71ms
step:853/2330 train_time:49228ms step_avg:57.71ms
step:854/2330 train_time:49287ms step_avg:57.71ms
step:855/2330 train_time:49344ms step_avg:57.71ms
step:856/2330 train_time:49405ms step_avg:57.72ms
step:857/2330 train_time:49462ms step_avg:57.72ms
step:858/2330 train_time:49522ms step_avg:57.72ms
step:859/2330 train_time:49580ms step_avg:57.72ms
step:860/2330 train_time:49640ms step_avg:57.72ms
step:861/2330 train_time:49698ms step_avg:57.72ms
step:862/2330 train_time:49758ms step_avg:57.72ms
step:863/2330 train_time:49816ms step_avg:57.72ms
step:864/2330 train_time:49875ms step_avg:57.73ms
step:865/2330 train_time:49933ms step_avg:57.73ms
step:866/2330 train_time:49992ms step_avg:57.73ms
step:867/2330 train_time:50048ms step_avg:57.73ms
step:868/2330 train_time:50109ms step_avg:57.73ms
step:869/2330 train_time:50165ms step_avg:57.73ms
step:870/2330 train_time:50225ms step_avg:57.73ms
step:871/2330 train_time:50282ms step_avg:57.73ms
step:872/2330 train_time:50342ms step_avg:57.73ms
step:873/2330 train_time:50399ms step_avg:57.73ms
step:874/2330 train_time:50459ms step_avg:57.73ms
step:875/2330 train_time:50516ms step_avg:57.73ms
step:876/2330 train_time:50575ms step_avg:57.73ms
step:877/2330 train_time:50632ms step_avg:57.73ms
step:878/2330 train_time:50692ms step_avg:57.74ms
step:879/2330 train_time:50748ms step_avg:57.73ms
step:880/2330 train_time:50809ms step_avg:57.74ms
step:881/2330 train_time:50866ms step_avg:57.74ms
step:882/2330 train_time:50926ms step_avg:57.74ms
step:883/2330 train_time:50983ms step_avg:57.74ms
step:884/2330 train_time:51042ms step_avg:57.74ms
step:885/2330 train_time:51099ms step_avg:57.74ms
step:886/2330 train_time:51159ms step_avg:57.74ms
step:887/2330 train_time:51216ms step_avg:57.74ms
step:888/2330 train_time:51276ms step_avg:57.74ms
step:889/2330 train_time:51333ms step_avg:57.74ms
step:890/2330 train_time:51393ms step_avg:57.75ms
step:891/2330 train_time:51450ms step_avg:57.74ms
step:892/2330 train_time:51510ms step_avg:57.75ms
step:893/2330 train_time:51567ms step_avg:57.75ms
step:894/2330 train_time:51628ms step_avg:57.75ms
step:895/2330 train_time:51685ms step_avg:57.75ms
step:896/2330 train_time:51745ms step_avg:57.75ms
step:897/2330 train_time:51802ms step_avg:57.75ms
step:898/2330 train_time:51862ms step_avg:57.75ms
step:899/2330 train_time:51920ms step_avg:57.75ms
step:900/2330 train_time:51979ms step_avg:57.75ms
step:901/2330 train_time:52036ms step_avg:57.75ms
step:902/2330 train_time:52096ms step_avg:57.76ms
step:903/2330 train_time:52152ms step_avg:57.75ms
step:904/2330 train_time:52211ms step_avg:57.76ms
step:905/2330 train_time:52269ms step_avg:57.76ms
step:906/2330 train_time:52329ms step_avg:57.76ms
step:907/2330 train_time:52385ms step_avg:57.76ms
step:908/2330 train_time:52446ms step_avg:57.76ms
step:909/2330 train_time:52503ms step_avg:57.76ms
step:910/2330 train_time:52563ms step_avg:57.76ms
step:911/2330 train_time:52621ms step_avg:57.76ms
step:912/2330 train_time:52681ms step_avg:57.76ms
step:913/2330 train_time:52738ms step_avg:57.76ms
step:914/2330 train_time:52798ms step_avg:57.77ms
step:915/2330 train_time:52855ms step_avg:57.77ms
step:916/2330 train_time:52914ms step_avg:57.77ms
step:917/2330 train_time:52971ms step_avg:57.77ms
step:918/2330 train_time:53030ms step_avg:57.77ms
step:919/2330 train_time:53088ms step_avg:57.77ms
step:920/2330 train_time:53147ms step_avg:57.77ms
step:921/2330 train_time:53204ms step_avg:57.77ms
step:922/2330 train_time:53265ms step_avg:57.77ms
step:923/2330 train_time:53322ms step_avg:57.77ms
step:924/2330 train_time:53382ms step_avg:57.77ms
step:925/2330 train_time:53440ms step_avg:57.77ms
step:926/2330 train_time:53500ms step_avg:57.77ms
step:927/2330 train_time:53557ms step_avg:57.77ms
step:928/2330 train_time:53616ms step_avg:57.78ms
step:929/2330 train_time:53673ms step_avg:57.77ms
step:930/2330 train_time:53732ms step_avg:57.78ms
step:931/2330 train_time:53790ms step_avg:57.78ms
step:932/2330 train_time:53849ms step_avg:57.78ms
step:933/2330 train_time:53906ms step_avg:57.78ms
step:934/2330 train_time:53966ms step_avg:57.78ms
step:935/2330 train_time:54023ms step_avg:57.78ms
step:936/2330 train_time:54083ms step_avg:57.78ms
step:937/2330 train_time:54140ms step_avg:57.78ms
step:938/2330 train_time:54200ms step_avg:57.78ms
step:939/2330 train_time:54258ms step_avg:57.78ms
step:940/2330 train_time:54317ms step_avg:57.78ms
step:941/2330 train_time:54374ms step_avg:57.78ms
step:942/2330 train_time:54434ms step_avg:57.79ms
step:943/2330 train_time:54491ms step_avg:57.79ms
step:944/2330 train_time:54550ms step_avg:57.79ms
step:945/2330 train_time:54607ms step_avg:57.79ms
step:946/2330 train_time:54668ms step_avg:57.79ms
step:947/2330 train_time:54725ms step_avg:57.79ms
step:948/2330 train_time:54785ms step_avg:57.79ms
step:949/2330 train_time:54842ms step_avg:57.79ms
step:950/2330 train_time:54902ms step_avg:57.79ms
step:951/2330 train_time:54959ms step_avg:57.79ms
step:952/2330 train_time:55019ms step_avg:57.79ms
step:953/2330 train_time:55076ms step_avg:57.79ms
step:954/2330 train_time:55136ms step_avg:57.79ms
step:955/2330 train_time:55192ms step_avg:57.79ms
step:956/2330 train_time:55252ms step_avg:57.80ms
step:957/2330 train_time:55309ms step_avg:57.79ms
step:958/2330 train_time:55370ms step_avg:57.80ms
step:959/2330 train_time:55427ms step_avg:57.80ms
step:960/2330 train_time:55487ms step_avg:57.80ms
step:961/2330 train_time:55544ms step_avg:57.80ms
step:962/2330 train_time:55604ms step_avg:57.80ms
step:963/2330 train_time:55661ms step_avg:57.80ms
step:964/2330 train_time:55721ms step_avg:57.80ms
step:965/2330 train_time:55778ms step_avg:57.80ms
step:966/2330 train_time:55838ms step_avg:57.80ms
step:967/2330 train_time:55895ms step_avg:57.80ms
step:968/2330 train_time:55954ms step_avg:57.80ms
step:969/2330 train_time:56010ms step_avg:57.80ms
step:970/2330 train_time:56071ms step_avg:57.80ms
step:971/2330 train_time:56128ms step_avg:57.80ms
step:972/2330 train_time:56187ms step_avg:57.81ms
step:973/2330 train_time:56244ms step_avg:57.80ms
step:974/2330 train_time:56304ms step_avg:57.81ms
step:975/2330 train_time:56362ms step_avg:57.81ms
step:976/2330 train_time:56421ms step_avg:57.81ms
step:977/2330 train_time:56479ms step_avg:57.81ms
step:978/2330 train_time:56538ms step_avg:57.81ms
step:979/2330 train_time:56596ms step_avg:57.81ms
step:980/2330 train_time:56655ms step_avg:57.81ms
step:981/2330 train_time:56712ms step_avg:57.81ms
step:982/2330 train_time:56772ms step_avg:57.81ms
step:983/2330 train_time:56830ms step_avg:57.81ms
step:984/2330 train_time:56889ms step_avg:57.81ms
step:985/2330 train_time:56945ms step_avg:57.81ms
step:986/2330 train_time:57006ms step_avg:57.82ms
step:987/2330 train_time:57063ms step_avg:57.81ms
step:988/2330 train_time:57123ms step_avg:57.82ms
step:989/2330 train_time:57181ms step_avg:57.82ms
step:990/2330 train_time:57240ms step_avg:57.82ms
step:991/2330 train_time:57298ms step_avg:57.82ms
step:992/2330 train_time:57357ms step_avg:57.82ms
step:993/2330 train_time:57414ms step_avg:57.82ms
step:994/2330 train_time:57474ms step_avg:57.82ms
step:995/2330 train_time:57531ms step_avg:57.82ms
step:996/2330 train_time:57590ms step_avg:57.82ms
step:997/2330 train_time:57647ms step_avg:57.82ms
step:998/2330 train_time:57708ms step_avg:57.82ms
step:999/2330 train_time:57765ms step_avg:57.82ms
step:1000/2330 train_time:57825ms step_avg:57.83ms
step:1000/2330 val_loss:4.0787 train_time:57905ms step_avg:57.91ms
step:1001/2330 train_time:57926ms step_avg:57.87ms
step:1002/2330 train_time:57948ms step_avg:57.83ms
step:1003/2330 train_time:57999ms step_avg:57.83ms
step:1004/2330 train_time:58068ms step_avg:57.84ms
step:1005/2330 train_time:58125ms step_avg:57.84ms
step:1006/2330 train_time:58189ms step_avg:57.84ms
step:1007/2330 train_time:58245ms step_avg:57.84ms
step:1008/2330 train_time:58304ms step_avg:57.84ms
step:1009/2330 train_time:58360ms step_avg:57.84ms
step:1010/2330 train_time:58421ms step_avg:57.84ms
step:1011/2330 train_time:58477ms step_avg:57.84ms
step:1012/2330 train_time:58536ms step_avg:57.84ms
step:1013/2330 train_time:58593ms step_avg:57.84ms
step:1014/2330 train_time:58652ms step_avg:57.84ms
step:1015/2330 train_time:58707ms step_avg:57.84ms
step:1016/2330 train_time:58766ms step_avg:57.84ms
step:1017/2330 train_time:58825ms step_avg:57.84ms
step:1018/2330 train_time:58888ms step_avg:57.85ms
step:1019/2330 train_time:58945ms step_avg:57.85ms
step:1020/2330 train_time:59007ms step_avg:57.85ms
step:1021/2330 train_time:59064ms step_avg:57.85ms
step:1022/2330 train_time:59127ms step_avg:57.85ms
step:1023/2330 train_time:59183ms step_avg:57.85ms
step:1024/2330 train_time:59244ms step_avg:57.86ms
step:1025/2330 train_time:59300ms step_avg:57.85ms
step:1026/2330 train_time:59361ms step_avg:57.86ms
step:1027/2330 train_time:59417ms step_avg:57.85ms
step:1028/2330 train_time:59477ms step_avg:57.86ms
step:1029/2330 train_time:59532ms step_avg:57.85ms
step:1030/2330 train_time:59593ms step_avg:57.86ms
step:1031/2330 train_time:59649ms step_avg:57.86ms
step:1032/2330 train_time:59709ms step_avg:57.86ms
step:1033/2330 train_time:59767ms step_avg:57.86ms
step:1034/2330 train_time:59827ms step_avg:57.86ms
step:1035/2330 train_time:59883ms step_avg:57.86ms
step:1036/2330 train_time:59944ms step_avg:57.86ms
step:1037/2330 train_time:60002ms step_avg:57.86ms
step:1038/2330 train_time:60063ms step_avg:57.86ms
step:1039/2330 train_time:60120ms step_avg:57.86ms
step:1040/2330 train_time:60182ms step_avg:57.87ms
step:1041/2330 train_time:60238ms step_avg:57.87ms
step:1042/2330 train_time:60299ms step_avg:57.87ms
step:1043/2330 train_time:60355ms step_avg:57.87ms
step:1044/2330 train_time:60416ms step_avg:57.87ms
step:1045/2330 train_time:60472ms step_avg:57.87ms
step:1046/2330 train_time:60531ms step_avg:57.87ms
step:1047/2330 train_time:60588ms step_avg:57.87ms
step:1048/2330 train_time:60648ms step_avg:57.87ms
step:1049/2330 train_time:60705ms step_avg:57.87ms
step:1050/2330 train_time:60764ms step_avg:57.87ms
step:1051/2330 train_time:60822ms step_avg:57.87ms
step:1052/2330 train_time:60882ms step_avg:57.87ms
step:1053/2330 train_time:60940ms step_avg:57.87ms
step:1054/2330 train_time:61000ms step_avg:57.87ms
step:1055/2330 train_time:61058ms step_avg:57.87ms
step:1056/2330 train_time:61118ms step_avg:57.88ms
step:1057/2330 train_time:61176ms step_avg:57.88ms
step:1058/2330 train_time:61236ms step_avg:57.88ms
step:1059/2330 train_time:61292ms step_avg:57.88ms
step:1060/2330 train_time:61353ms step_avg:57.88ms
step:1061/2330 train_time:61409ms step_avg:57.88ms
step:1062/2330 train_time:61470ms step_avg:57.88ms
step:1063/2330 train_time:61526ms step_avg:57.88ms
step:1064/2330 train_time:61585ms step_avg:57.88ms
step:1065/2330 train_time:61641ms step_avg:57.88ms
step:1066/2330 train_time:61701ms step_avg:57.88ms
step:1067/2330 train_time:61758ms step_avg:57.88ms
step:1068/2330 train_time:61818ms step_avg:57.88ms
step:1069/2330 train_time:61875ms step_avg:57.88ms
step:1070/2330 train_time:61935ms step_avg:57.88ms
step:1071/2330 train_time:61993ms step_avg:57.88ms
step:1072/2330 train_time:62053ms step_avg:57.89ms
step:1073/2330 train_time:62110ms step_avg:57.88ms
step:1074/2330 train_time:62170ms step_avg:57.89ms
step:1075/2330 train_time:62228ms step_avg:57.89ms
step:1076/2330 train_time:62288ms step_avg:57.89ms
step:1077/2330 train_time:62344ms step_avg:57.89ms
step:1078/2330 train_time:62404ms step_avg:57.89ms
step:1079/2330 train_time:62461ms step_avg:57.89ms
step:1080/2330 train_time:62522ms step_avg:57.89ms
step:1081/2330 train_time:62578ms step_avg:57.89ms
step:1082/2330 train_time:62638ms step_avg:57.89ms
step:1083/2330 train_time:62695ms step_avg:57.89ms
step:1084/2330 train_time:62756ms step_avg:57.89ms
step:1085/2330 train_time:62813ms step_avg:57.89ms
step:1086/2330 train_time:62873ms step_avg:57.89ms
step:1087/2330 train_time:62931ms step_avg:57.89ms
step:1088/2330 train_time:62991ms step_avg:57.90ms
step:1089/2330 train_time:63048ms step_avg:57.90ms
step:1090/2330 train_time:63107ms step_avg:57.90ms
step:1091/2330 train_time:63164ms step_avg:57.90ms
step:1092/2330 train_time:63224ms step_avg:57.90ms
step:1093/2330 train_time:63282ms step_avg:57.90ms
step:1094/2330 train_time:63341ms step_avg:57.90ms
step:1095/2330 train_time:63398ms step_avg:57.90ms
step:1096/2330 train_time:63459ms step_avg:57.90ms
step:1097/2330 train_time:63516ms step_avg:57.90ms
step:1098/2330 train_time:63576ms step_avg:57.90ms
step:1099/2330 train_time:63632ms step_avg:57.90ms
step:1100/2330 train_time:63692ms step_avg:57.90ms
step:1101/2330 train_time:63749ms step_avg:57.90ms
step:1102/2330 train_time:63809ms step_avg:57.90ms
step:1103/2330 train_time:63866ms step_avg:57.90ms
step:1104/2330 train_time:63926ms step_avg:57.90ms
step:1105/2330 train_time:63983ms step_avg:57.90ms
step:1106/2330 train_time:64043ms step_avg:57.90ms
step:1107/2330 train_time:64100ms step_avg:57.90ms
step:1108/2330 train_time:64160ms step_avg:57.91ms
step:1109/2330 train_time:64218ms step_avg:57.91ms
step:1110/2330 train_time:64278ms step_avg:57.91ms
step:1111/2330 train_time:64335ms step_avg:57.91ms
step:1112/2330 train_time:64395ms step_avg:57.91ms
step:1113/2330 train_time:64452ms step_avg:57.91ms
step:1114/2330 train_time:64512ms step_avg:57.91ms
step:1115/2330 train_time:64569ms step_avg:57.91ms
step:1116/2330 train_time:64629ms step_avg:57.91ms
step:1117/2330 train_time:64685ms step_avg:57.91ms
step:1118/2330 train_time:64745ms step_avg:57.91ms
step:1119/2330 train_time:64802ms step_avg:57.91ms
step:1120/2330 train_time:64862ms step_avg:57.91ms
step:1121/2330 train_time:64918ms step_avg:57.91ms
step:1122/2330 train_time:64980ms step_avg:57.91ms
step:1123/2330 train_time:65036ms step_avg:57.91ms
step:1124/2330 train_time:65098ms step_avg:57.92ms
step:1125/2330 train_time:65155ms step_avg:57.92ms
step:1126/2330 train_time:65218ms step_avg:57.92ms
step:1127/2330 train_time:65275ms step_avg:57.92ms
step:1128/2330 train_time:65335ms step_avg:57.92ms
step:1129/2330 train_time:65392ms step_avg:57.92ms
step:1130/2330 train_time:65451ms step_avg:57.92ms
step:1131/2330 train_time:65508ms step_avg:57.92ms
step:1132/2330 train_time:65567ms step_avg:57.92ms
step:1133/2330 train_time:65625ms step_avg:57.92ms
step:1134/2330 train_time:65684ms step_avg:57.92ms
step:1135/2330 train_time:65741ms step_avg:57.92ms
step:1136/2330 train_time:65802ms step_avg:57.92ms
step:1137/2330 train_time:65859ms step_avg:57.92ms
step:1138/2330 train_time:65919ms step_avg:57.93ms
step:1139/2330 train_time:65976ms step_avg:57.92ms
step:1140/2330 train_time:66037ms step_avg:57.93ms
step:1141/2330 train_time:66094ms step_avg:57.93ms
step:1142/2330 train_time:66155ms step_avg:57.93ms
step:1143/2330 train_time:66212ms step_avg:57.93ms
step:1144/2330 train_time:66272ms step_avg:57.93ms
step:1145/2330 train_time:66330ms step_avg:57.93ms
step:1146/2330 train_time:66389ms step_avg:57.93ms
step:1147/2330 train_time:66446ms step_avg:57.93ms
step:1148/2330 train_time:66505ms step_avg:57.93ms
step:1149/2330 train_time:66563ms step_avg:57.93ms
step:1150/2330 train_time:66623ms step_avg:57.93ms
step:1151/2330 train_time:66680ms step_avg:57.93ms
step:1152/2330 train_time:66740ms step_avg:57.93ms
step:1153/2330 train_time:66797ms step_avg:57.93ms
step:1154/2330 train_time:66857ms step_avg:57.93ms
step:1155/2330 train_time:66914ms step_avg:57.93ms
step:1156/2330 train_time:66974ms step_avg:57.94ms
step:1157/2330 train_time:67031ms step_avg:57.94ms
step:1158/2330 train_time:67091ms step_avg:57.94ms
step:1159/2330 train_time:67147ms step_avg:57.94ms
step:1160/2330 train_time:67208ms step_avg:57.94ms
step:1161/2330 train_time:67264ms step_avg:57.94ms
step:1162/2330 train_time:67324ms step_avg:57.94ms
step:1163/2330 train_time:67381ms step_avg:57.94ms
step:1164/2330 train_time:67441ms step_avg:57.94ms
step:1165/2330 train_time:67498ms step_avg:57.94ms
step:1166/2330 train_time:67559ms step_avg:57.94ms
step:1167/2330 train_time:67616ms step_avg:57.94ms
step:1168/2330 train_time:67676ms step_avg:57.94ms
step:1169/2330 train_time:67733ms step_avg:57.94ms
step:1170/2330 train_time:67794ms step_avg:57.94ms
step:1171/2330 train_time:67851ms step_avg:57.94ms
step:1172/2330 train_time:67911ms step_avg:57.94ms
step:1173/2330 train_time:67968ms step_avg:57.94ms
step:1174/2330 train_time:68027ms step_avg:57.95ms
step:1175/2330 train_time:68085ms step_avg:57.94ms
step:1176/2330 train_time:68144ms step_avg:57.95ms
step:1177/2330 train_time:68202ms step_avg:57.95ms
step:1178/2330 train_time:68262ms step_avg:57.95ms
step:1179/2330 train_time:68319ms step_avg:57.95ms
step:1180/2330 train_time:68379ms step_avg:57.95ms
step:1181/2330 train_time:68436ms step_avg:57.95ms
step:1182/2330 train_time:68495ms step_avg:57.95ms
step:1183/2330 train_time:68552ms step_avg:57.95ms
step:1184/2330 train_time:68612ms step_avg:57.95ms
step:1185/2330 train_time:68670ms step_avg:57.95ms
step:1186/2330 train_time:68730ms step_avg:57.95ms
step:1187/2330 train_time:68787ms step_avg:57.95ms
step:1188/2330 train_time:68846ms step_avg:57.95ms
step:1189/2330 train_time:68903ms step_avg:57.95ms
step:1190/2330 train_time:68964ms step_avg:57.95ms
step:1191/2330 train_time:69021ms step_avg:57.95ms
step:1192/2330 train_time:69081ms step_avg:57.95ms
step:1193/2330 train_time:69138ms step_avg:57.95ms
step:1194/2330 train_time:69199ms step_avg:57.96ms
step:1195/2330 train_time:69255ms step_avg:57.95ms
step:1196/2330 train_time:69316ms step_avg:57.96ms
step:1197/2330 train_time:69372ms step_avg:57.96ms
step:1198/2330 train_time:69432ms step_avg:57.96ms
step:1199/2330 train_time:69489ms step_avg:57.96ms
step:1200/2330 train_time:69549ms step_avg:57.96ms
step:1201/2330 train_time:69606ms step_avg:57.96ms
step:1202/2330 train_time:69666ms step_avg:57.96ms
step:1203/2330 train_time:69724ms step_avg:57.96ms
step:1204/2330 train_time:69783ms step_avg:57.96ms
step:1205/2330 train_time:69839ms step_avg:57.96ms
step:1206/2330 train_time:69902ms step_avg:57.96ms
step:1207/2330 train_time:69959ms step_avg:57.96ms
step:1208/2330 train_time:70019ms step_avg:57.96ms
step:1209/2330 train_time:70075ms step_avg:57.96ms
step:1210/2330 train_time:70136ms step_avg:57.96ms
step:1211/2330 train_time:70192ms step_avg:57.96ms
step:1212/2330 train_time:70253ms step_avg:57.96ms
step:1213/2330 train_time:70309ms step_avg:57.96ms
step:1214/2330 train_time:70370ms step_avg:57.97ms
step:1215/2330 train_time:70427ms step_avg:57.96ms
step:1216/2330 train_time:70486ms step_avg:57.97ms
step:1217/2330 train_time:70542ms step_avg:57.96ms
step:1218/2330 train_time:70603ms step_avg:57.97ms
step:1219/2330 train_time:70660ms step_avg:57.97ms
step:1220/2330 train_time:70721ms step_avg:57.97ms
step:1221/2330 train_time:70778ms step_avg:57.97ms
step:1222/2330 train_time:70839ms step_avg:57.97ms
step:1223/2330 train_time:70896ms step_avg:57.97ms
step:1224/2330 train_time:70957ms step_avg:57.97ms
step:1225/2330 train_time:71014ms step_avg:57.97ms
step:1226/2330 train_time:71074ms step_avg:57.97ms
step:1227/2330 train_time:71130ms step_avg:57.97ms
step:1228/2330 train_time:71191ms step_avg:57.97ms
step:1229/2330 train_time:71247ms step_avg:57.97ms
step:1230/2330 train_time:71307ms step_avg:57.97ms
step:1231/2330 train_time:71364ms step_avg:57.97ms
step:1232/2330 train_time:71424ms step_avg:57.97ms
step:1233/2330 train_time:71481ms step_avg:57.97ms
step:1234/2330 train_time:71541ms step_avg:57.97ms
step:1235/2330 train_time:71598ms step_avg:57.97ms
step:1236/2330 train_time:71659ms step_avg:57.98ms
step:1237/2330 train_time:71716ms step_avg:57.98ms
step:1238/2330 train_time:71776ms step_avg:57.98ms
step:1239/2330 train_time:71833ms step_avg:57.98ms
step:1240/2330 train_time:71894ms step_avg:57.98ms
step:1241/2330 train_time:71951ms step_avg:57.98ms
step:1242/2330 train_time:72011ms step_avg:57.98ms
step:1243/2330 train_time:72068ms step_avg:57.98ms
step:1244/2330 train_time:72128ms step_avg:57.98ms
step:1245/2330 train_time:72184ms step_avg:57.98ms
step:1246/2330 train_time:72245ms step_avg:57.98ms
step:1247/2330 train_time:72302ms step_avg:57.98ms
step:1248/2330 train_time:72361ms step_avg:57.98ms
step:1249/2330 train_time:72419ms step_avg:57.98ms
step:1250/2330 train_time:72479ms step_avg:57.98ms
step:1250/2330 val_loss:3.9930 train_time:72559ms step_avg:58.05ms
step:1251/2330 train_time:72580ms step_avg:58.02ms
step:1252/2330 train_time:72600ms step_avg:57.99ms
step:1253/2330 train_time:72658ms step_avg:57.99ms
step:1254/2330 train_time:72722ms step_avg:57.99ms
step:1255/2330 train_time:72778ms step_avg:57.99ms
step:1256/2330 train_time:72840ms step_avg:57.99ms
step:1257/2330 train_time:72896ms step_avg:57.99ms
step:1258/2330 train_time:72956ms step_avg:57.99ms
step:1259/2330 train_time:73013ms step_avg:57.99ms
step:1260/2330 train_time:73073ms step_avg:57.99ms
step:1261/2330 train_time:73130ms step_avg:57.99ms
step:1262/2330 train_time:73189ms step_avg:57.99ms
step:1263/2330 train_time:73246ms step_avg:57.99ms
step:1264/2330 train_time:73305ms step_avg:57.99ms
step:1265/2330 train_time:73362ms step_avg:57.99ms
step:1266/2330 train_time:73421ms step_avg:57.99ms
step:1267/2330 train_time:73477ms step_avg:57.99ms
step:1268/2330 train_time:73539ms step_avg:58.00ms
step:1269/2330 train_time:73597ms step_avg:58.00ms
step:1270/2330 train_time:73660ms step_avg:58.00ms
step:1271/2330 train_time:73717ms step_avg:58.00ms
step:1272/2330 train_time:73778ms step_avg:58.00ms
step:1273/2330 train_time:73835ms step_avg:58.00ms
step:1274/2330 train_time:73896ms step_avg:58.00ms
step:1275/2330 train_time:73953ms step_avg:58.00ms
step:1276/2330 train_time:74013ms step_avg:58.00ms
step:1277/2330 train_time:74070ms step_avg:58.00ms
step:1278/2330 train_time:74130ms step_avg:58.00ms
step:1279/2330 train_time:74186ms step_avg:58.00ms
step:1280/2330 train_time:74246ms step_avg:58.00ms
step:1281/2330 train_time:74303ms step_avg:58.00ms
step:1282/2330 train_time:74362ms step_avg:58.00ms
step:1283/2330 train_time:74419ms step_avg:58.00ms
step:1284/2330 train_time:74478ms step_avg:58.01ms
step:1285/2330 train_time:74535ms step_avg:58.00ms
step:1286/2330 train_time:74598ms step_avg:58.01ms
step:1287/2330 train_time:74655ms step_avg:58.01ms
step:1288/2330 train_time:74716ms step_avg:58.01ms
step:1289/2330 train_time:74773ms step_avg:58.01ms
step:1290/2330 train_time:74835ms step_avg:58.01ms
step:1291/2330 train_time:74893ms step_avg:58.01ms
step:1292/2330 train_time:75366ms step_avg:58.33ms
step:1293/2330 train_time:75422ms step_avg:58.33ms
step:1294/2330 train_time:75480ms step_avg:58.33ms
step:1295/2330 train_time:75536ms step_avg:58.33ms
step:1296/2330 train_time:75595ms step_avg:58.33ms
step:1297/2330 train_time:75651ms step_avg:58.33ms
step:1298/2330 train_time:75711ms step_avg:58.33ms
step:1299/2330 train_time:75767ms step_avg:58.33ms
step:1300/2330 train_time:75826ms step_avg:58.33ms
step:1301/2330 train_time:75882ms step_avg:58.33ms
step:1302/2330 train_time:75941ms step_avg:58.33ms
step:1303/2330 train_time:75997ms step_avg:58.32ms
step:1304/2330 train_time:76056ms step_avg:58.32ms
step:1305/2330 train_time:76112ms step_avg:58.32ms
step:1306/2330 train_time:76172ms step_avg:58.32ms
step:1307/2330 train_time:76230ms step_avg:58.32ms
step:1308/2330 train_time:76297ms step_avg:58.33ms
step:1309/2330 train_time:76354ms step_avg:58.33ms
step:1310/2330 train_time:76417ms step_avg:58.33ms
step:1311/2330 train_time:76474ms step_avg:58.33ms
step:1312/2330 train_time:76535ms step_avg:58.33ms
step:1313/2330 train_time:76591ms step_avg:58.33ms
step:1314/2330 train_time:76651ms step_avg:58.33ms
step:1315/2330 train_time:76708ms step_avg:58.33ms
step:1316/2330 train_time:76767ms step_avg:58.33ms
step:1317/2330 train_time:76824ms step_avg:58.33ms
step:1318/2330 train_time:76883ms step_avg:58.33ms
step:1319/2330 train_time:76940ms step_avg:58.33ms
step:1320/2330 train_time:76998ms step_avg:58.33ms
step:1321/2330 train_time:77055ms step_avg:58.33ms
step:1322/2330 train_time:77114ms step_avg:58.33ms
step:1323/2330 train_time:77172ms step_avg:58.33ms
step:1324/2330 train_time:77234ms step_avg:58.33ms
step:1325/2330 train_time:77291ms step_avg:58.33ms
step:1326/2330 train_time:77354ms step_avg:58.34ms
step:1327/2330 train_time:77412ms step_avg:58.34ms
step:1328/2330 train_time:77472ms step_avg:58.34ms
step:1329/2330 train_time:77529ms step_avg:58.34ms
step:1330/2330 train_time:77589ms step_avg:58.34ms
step:1331/2330 train_time:77645ms step_avg:58.34ms
step:1332/2330 train_time:77705ms step_avg:58.34ms
step:1333/2330 train_time:77761ms step_avg:58.34ms
step:1334/2330 train_time:77820ms step_avg:58.34ms
step:1335/2330 train_time:77876ms step_avg:58.33ms
step:1336/2330 train_time:77937ms step_avg:58.34ms
step:1337/2330 train_time:77994ms step_avg:58.33ms
step:1338/2330 train_time:78053ms step_avg:58.34ms
step:1339/2330 train_time:78109ms step_avg:58.33ms
step:1340/2330 train_time:78170ms step_avg:58.34ms
step:1341/2330 train_time:78228ms step_avg:58.34ms
step:1342/2330 train_time:78288ms step_avg:58.34ms
step:1343/2330 train_time:78346ms step_avg:58.34ms
step:1344/2330 train_time:78407ms step_avg:58.34ms
step:1345/2330 train_time:78464ms step_avg:58.34ms
step:1346/2330 train_time:78523ms step_avg:58.34ms
step:1347/2330 train_time:78580ms step_avg:58.34ms
step:1348/2330 train_time:78641ms step_avg:58.34ms
step:1349/2330 train_time:78698ms step_avg:58.34ms
step:1350/2330 train_time:78758ms step_avg:58.34ms
step:1351/2330 train_time:78815ms step_avg:58.34ms
step:1352/2330 train_time:78874ms step_avg:58.34ms
step:1353/2330 train_time:78931ms step_avg:58.34ms
step:1354/2330 train_time:78990ms step_avg:58.34ms
step:1355/2330 train_time:79047ms step_avg:58.34ms
step:1356/2330 train_time:79107ms step_avg:58.34ms
step:1357/2330 train_time:79164ms step_avg:58.34ms
step:1358/2330 train_time:79224ms step_avg:58.34ms
step:1359/2330 train_time:79281ms step_avg:58.34ms
step:1360/2330 train_time:79341ms step_avg:58.34ms
step:1361/2330 train_time:79399ms step_avg:58.34ms
step:1362/2330 train_time:79460ms step_avg:58.34ms
step:1363/2330 train_time:79517ms step_avg:58.34ms
step:1364/2330 train_time:79578ms step_avg:58.34ms
step:1365/2330 train_time:79636ms step_avg:58.34ms
step:1366/2330 train_time:79696ms step_avg:58.34ms
step:1367/2330 train_time:79753ms step_avg:58.34ms
step:1368/2330 train_time:79813ms step_avg:58.34ms
step:1369/2330 train_time:79870ms step_avg:58.34ms
step:1370/2330 train_time:79929ms step_avg:58.34ms
step:1371/2330 train_time:79986ms step_avg:58.34ms
step:1372/2330 train_time:80045ms step_avg:58.34ms
step:1373/2330 train_time:80102ms step_avg:58.34ms
step:1374/2330 train_time:80163ms step_avg:58.34ms
step:1375/2330 train_time:80219ms step_avg:58.34ms
step:1376/2330 train_time:80280ms step_avg:58.34ms
step:1377/2330 train_time:80337ms step_avg:58.34ms
step:1378/2330 train_time:80398ms step_avg:58.34ms
step:1379/2330 train_time:80455ms step_avg:58.34ms
step:1380/2330 train_time:80515ms step_avg:58.34ms
step:1381/2330 train_time:80572ms step_avg:58.34ms
step:1382/2330 train_time:80633ms step_avg:58.35ms
step:1383/2330 train_time:80690ms step_avg:58.34ms
step:1384/2330 train_time:80750ms step_avg:58.35ms
step:1385/2330 train_time:80806ms step_avg:58.34ms
step:1386/2330 train_time:80866ms step_avg:58.34ms
step:1387/2330 train_time:80923ms step_avg:58.34ms
step:1388/2330 train_time:80982ms step_avg:58.34ms
step:1389/2330 train_time:81038ms step_avg:58.34ms
step:1390/2330 train_time:81099ms step_avg:58.34ms
step:1391/2330 train_time:81156ms step_avg:58.34ms
step:1392/2330 train_time:81216ms step_avg:58.34ms
step:1393/2330 train_time:81273ms step_avg:58.34ms
step:1394/2330 train_time:81333ms step_avg:58.35ms
step:1395/2330 train_time:81390ms step_avg:58.34ms
step:1396/2330 train_time:81451ms step_avg:58.35ms
step:1397/2330 train_time:81509ms step_avg:58.35ms
step:1398/2330 train_time:81569ms step_avg:58.35ms
step:1399/2330 train_time:81626ms step_avg:58.35ms
step:1400/2330 train_time:81686ms step_avg:58.35ms
step:1401/2330 train_time:81742ms step_avg:58.35ms
step:1402/2330 train_time:81802ms step_avg:58.35ms
step:1403/2330 train_time:81860ms step_avg:58.35ms
step:1404/2330 train_time:81919ms step_avg:58.35ms
step:1405/2330 train_time:81976ms step_avg:58.35ms
step:1406/2330 train_time:82037ms step_avg:58.35ms
step:1407/2330 train_time:82093ms step_avg:58.35ms
step:1408/2330 train_time:82154ms step_avg:58.35ms
step:1409/2330 train_time:82211ms step_avg:58.35ms
step:1410/2330 train_time:82271ms step_avg:58.35ms
step:1411/2330 train_time:82328ms step_avg:58.35ms
step:1412/2330 train_time:82389ms step_avg:58.35ms
step:1413/2330 train_time:82446ms step_avg:58.35ms
step:1414/2330 train_time:82505ms step_avg:58.35ms
step:1415/2330 train_time:82562ms step_avg:58.35ms
step:1416/2330 train_time:82623ms step_avg:58.35ms
step:1417/2330 train_time:82680ms step_avg:58.35ms
step:1418/2330 train_time:82740ms step_avg:58.35ms
step:1419/2330 train_time:82796ms step_avg:58.35ms
step:1420/2330 train_time:82857ms step_avg:58.35ms
step:1421/2330 train_time:82914ms step_avg:58.35ms
step:1422/2330 train_time:82975ms step_avg:58.35ms
step:1423/2330 train_time:83032ms step_avg:58.35ms
step:1424/2330 train_time:83092ms step_avg:58.35ms
step:1425/2330 train_time:83150ms step_avg:58.35ms
step:1426/2330 train_time:83210ms step_avg:58.35ms
step:1427/2330 train_time:83266ms step_avg:58.35ms
step:1428/2330 train_time:83326ms step_avg:58.35ms
step:1429/2330 train_time:83383ms step_avg:58.35ms
step:1430/2330 train_time:83443ms step_avg:58.35ms
step:1431/2330 train_time:83500ms step_avg:58.35ms
step:1432/2330 train_time:83559ms step_avg:58.35ms
step:1433/2330 train_time:83616ms step_avg:58.35ms
step:1434/2330 train_time:83677ms step_avg:58.35ms
step:1435/2330 train_time:83734ms step_avg:58.35ms
step:1436/2330 train_time:83796ms step_avg:58.35ms
step:1437/2330 train_time:83852ms step_avg:58.35ms
step:1438/2330 train_time:83913ms step_avg:58.35ms
step:1439/2330 train_time:83970ms step_avg:58.35ms
step:1440/2330 train_time:84030ms step_avg:58.35ms
step:1441/2330 train_time:84087ms step_avg:58.35ms
step:1442/2330 train_time:84147ms step_avg:58.35ms
step:1443/2330 train_time:84205ms step_avg:58.35ms
step:1444/2330 train_time:84264ms step_avg:58.35ms
step:1445/2330 train_time:84321ms step_avg:58.35ms
step:1446/2330 train_time:84381ms step_avg:58.35ms
step:1447/2330 train_time:84438ms step_avg:58.35ms
step:1448/2330 train_time:84498ms step_avg:58.35ms
step:1449/2330 train_time:84554ms step_avg:58.35ms
step:1450/2330 train_time:84615ms step_avg:58.36ms
step:1451/2330 train_time:84672ms step_avg:58.35ms
step:1452/2330 train_time:84732ms step_avg:58.36ms
step:1453/2330 train_time:84789ms step_avg:58.35ms
step:1454/2330 train_time:84850ms step_avg:58.36ms
step:1455/2330 train_time:84907ms step_avg:58.36ms
step:1456/2330 train_time:84967ms step_avg:58.36ms
step:1457/2330 train_time:85025ms step_avg:58.36ms
step:1458/2330 train_time:85084ms step_avg:58.36ms
step:1459/2330 train_time:85142ms step_avg:58.36ms
step:1460/2330 train_time:85201ms step_avg:58.36ms
step:1461/2330 train_time:85258ms step_avg:58.36ms
step:1462/2330 train_time:85319ms step_avg:58.36ms
step:1463/2330 train_time:85376ms step_avg:58.36ms
step:1464/2330 train_time:85436ms step_avg:58.36ms
step:1465/2330 train_time:85493ms step_avg:58.36ms
step:1466/2330 train_time:85554ms step_avg:58.36ms
step:1467/2330 train_time:85611ms step_avg:58.36ms
step:1468/2330 train_time:85670ms step_avg:58.36ms
step:1469/2330 train_time:85727ms step_avg:58.36ms
step:1470/2330 train_time:85787ms step_avg:58.36ms
step:1471/2330 train_time:85844ms step_avg:58.36ms
step:1472/2330 train_time:85903ms step_avg:58.36ms
step:1473/2330 train_time:85960ms step_avg:58.36ms
step:1474/2330 train_time:86021ms step_avg:58.36ms
step:1475/2330 train_time:86078ms step_avg:58.36ms
step:1476/2330 train_time:86139ms step_avg:58.36ms
step:1477/2330 train_time:86195ms step_avg:58.36ms
step:1478/2330 train_time:86256ms step_avg:58.36ms
step:1479/2330 train_time:86313ms step_avg:58.36ms
step:1480/2330 train_time:86373ms step_avg:58.36ms
step:1481/2330 train_time:86429ms step_avg:58.36ms
step:1482/2330 train_time:86491ms step_avg:58.36ms
step:1483/2330 train_time:86548ms step_avg:58.36ms
step:1484/2330 train_time:86607ms step_avg:58.36ms
step:1485/2330 train_time:86665ms step_avg:58.36ms
step:1486/2330 train_time:86724ms step_avg:58.36ms
step:1487/2330 train_time:86781ms step_avg:58.36ms
step:1488/2330 train_time:86841ms step_avg:58.36ms
step:1489/2330 train_time:86898ms step_avg:58.36ms
step:1490/2330 train_time:86959ms step_avg:58.36ms
step:1491/2330 train_time:87016ms step_avg:58.36ms
step:1492/2330 train_time:87075ms step_avg:58.36ms
step:1493/2330 train_time:87133ms step_avg:58.36ms
step:1494/2330 train_time:87193ms step_avg:58.36ms
step:1495/2330 train_time:87250ms step_avg:58.36ms
step:1496/2330 train_time:87311ms step_avg:58.36ms
step:1497/2330 train_time:87367ms step_avg:58.36ms
step:1498/2330 train_time:87427ms step_avg:58.36ms
step:1499/2330 train_time:87484ms step_avg:58.36ms
step:1500/2330 train_time:87544ms step_avg:58.36ms
step:1500/2330 val_loss:3.9095 train_time:87624ms step_avg:58.42ms
step:1501/2330 train_time:87644ms step_avg:58.39ms
step:1502/2330 train_time:87665ms step_avg:58.37ms
step:1503/2330 train_time:87723ms step_avg:58.37ms
step:1504/2330 train_time:87786ms step_avg:58.37ms
step:1505/2330 train_time:87844ms step_avg:58.37ms
step:1506/2330 train_time:87904ms step_avg:58.37ms
step:1507/2330 train_time:87961ms step_avg:58.37ms
step:1508/2330 train_time:88020ms step_avg:58.37ms
step:1509/2330 train_time:88077ms step_avg:58.37ms
step:1510/2330 train_time:88136ms step_avg:58.37ms
step:1511/2330 train_time:88192ms step_avg:58.37ms
step:1512/2330 train_time:88252ms step_avg:58.37ms
step:1513/2330 train_time:88308ms step_avg:58.37ms
step:1514/2330 train_time:88369ms step_avg:58.37ms
step:1515/2330 train_time:88425ms step_avg:58.37ms
step:1516/2330 train_time:88484ms step_avg:58.37ms
step:1517/2330 train_time:88542ms step_avg:58.37ms
step:1518/2330 train_time:88602ms step_avg:58.37ms
step:1519/2330 train_time:88659ms step_avg:58.37ms
step:1520/2330 train_time:88721ms step_avg:58.37ms
step:1521/2330 train_time:88779ms step_avg:58.37ms
step:1522/2330 train_time:88840ms step_avg:58.37ms
step:1523/2330 train_time:88896ms step_avg:58.37ms
step:1524/2330 train_time:88958ms step_avg:58.37ms
step:1525/2330 train_time:89015ms step_avg:58.37ms
step:1526/2330 train_time:89075ms step_avg:58.37ms
step:1527/2330 train_time:89131ms step_avg:58.37ms
step:1528/2330 train_time:89192ms step_avg:58.37ms
step:1529/2330 train_time:89250ms step_avg:58.37ms
step:1530/2330 train_time:89308ms step_avg:58.37ms
step:1531/2330 train_time:89365ms step_avg:58.37ms
step:1532/2330 train_time:89425ms step_avg:58.37ms
step:1533/2330 train_time:89482ms step_avg:58.37ms
step:1534/2330 train_time:89543ms step_avg:58.37ms
step:1535/2330 train_time:89602ms step_avg:58.37ms
step:1536/2330 train_time:89662ms step_avg:58.37ms
step:1537/2330 train_time:89720ms step_avg:58.37ms
step:1538/2330 train_time:89781ms step_avg:58.37ms
step:1539/2330 train_time:89839ms step_avg:58.37ms
step:1540/2330 train_time:89899ms step_avg:58.38ms
step:1541/2330 train_time:89957ms step_avg:58.38ms
step:1542/2330 train_time:90017ms step_avg:58.38ms
step:1543/2330 train_time:90075ms step_avg:58.38ms
step:1544/2330 train_time:90135ms step_avg:58.38ms
step:1545/2330 train_time:90191ms step_avg:58.38ms
step:1546/2330 train_time:90253ms step_avg:58.38ms
step:1547/2330 train_time:90309ms step_avg:58.38ms
step:1548/2330 train_time:90372ms step_avg:58.38ms
step:1549/2330 train_time:90429ms step_avg:58.38ms
step:1550/2330 train_time:90491ms step_avg:58.38ms
step:1551/2330 train_time:90548ms step_avg:58.38ms
step:1552/2330 train_time:90609ms step_avg:58.38ms
step:1553/2330 train_time:90666ms step_avg:58.38ms
step:1554/2330 train_time:90728ms step_avg:58.38ms
step:1555/2330 train_time:90787ms step_avg:58.38ms
step:1556/2330 train_time:90849ms step_avg:58.39ms
step:1557/2330 train_time:90908ms step_avg:58.39ms
step:1558/2330 train_time:90968ms step_avg:58.39ms
step:1559/2330 train_time:91027ms step_avg:58.39ms
step:1560/2330 train_time:91087ms step_avg:58.39ms
step:1561/2330 train_time:91145ms step_avg:58.39ms
step:1562/2330 train_time:91206ms step_avg:58.39ms
step:1563/2330 train_time:91264ms step_avg:58.39ms
step:1564/2330 train_time:91324ms step_avg:58.39ms
step:1565/2330 train_time:91381ms step_avg:58.39ms
step:1566/2330 train_time:91441ms step_avg:58.39ms
step:1567/2330 train_time:91499ms step_avg:58.39ms
step:1568/2330 train_time:91559ms step_avg:58.39ms
step:1569/2330 train_time:91615ms step_avg:58.39ms
step:1570/2330 train_time:91677ms step_avg:58.39ms
step:1571/2330 train_time:91734ms step_avg:58.39ms
step:1572/2330 train_time:91797ms step_avg:58.39ms
step:1573/2330 train_time:91853ms step_avg:58.39ms
step:1574/2330 train_time:91916ms step_avg:58.40ms
step:1575/2330 train_time:91973ms step_avg:58.40ms
step:1576/2330 train_time:92035ms step_avg:58.40ms
step:1577/2330 train_time:92092ms step_avg:58.40ms
step:1578/2330 train_time:92154ms step_avg:58.40ms
step:1579/2330 train_time:92211ms step_avg:58.40ms
step:1580/2330 train_time:92272ms step_avg:58.40ms
step:1581/2330 train_time:92329ms step_avg:58.40ms
step:1582/2330 train_time:92390ms step_avg:58.40ms
step:1583/2330 train_time:92447ms step_avg:58.40ms
step:1584/2330 train_time:92508ms step_avg:58.40ms
step:1585/2330 train_time:92566ms step_avg:58.40ms
step:1586/2330 train_time:92626ms step_avg:58.40ms
step:1587/2330 train_time:92683ms step_avg:58.40ms
step:1588/2330 train_time:92744ms step_avg:58.40ms
step:1589/2330 train_time:92802ms step_avg:58.40ms
step:1590/2330 train_time:92863ms step_avg:58.40ms
step:1591/2330 train_time:92921ms step_avg:58.40ms
step:1592/2330 train_time:92981ms step_avg:58.41ms
step:1593/2330 train_time:93039ms step_avg:58.40ms
step:1594/2330 train_time:93099ms step_avg:58.41ms
step:1595/2330 train_time:93156ms step_avg:58.40ms
step:1596/2330 train_time:93217ms step_avg:58.41ms
step:1597/2330 train_time:93274ms step_avg:58.41ms
step:1598/2330 train_time:93335ms step_avg:58.41ms
step:1599/2330 train_time:93391ms step_avg:58.41ms
step:1600/2330 train_time:93453ms step_avg:58.41ms
step:1601/2330 train_time:93510ms step_avg:58.41ms
step:1602/2330 train_time:93571ms step_avg:58.41ms
step:1603/2330 train_time:93628ms step_avg:58.41ms
step:1604/2330 train_time:93691ms step_avg:58.41ms
step:1605/2330 train_time:93749ms step_avg:58.41ms
step:1606/2330 train_time:93809ms step_avg:58.41ms
step:1607/2330 train_time:93867ms step_avg:58.41ms
step:1608/2330 train_time:93928ms step_avg:58.41ms
step:1609/2330 train_time:93986ms step_avg:58.41ms
step:1610/2330 train_time:94046ms step_avg:58.41ms
step:1611/2330 train_time:94105ms step_avg:58.41ms
step:1612/2330 train_time:94165ms step_avg:58.41ms
step:1613/2330 train_time:94223ms step_avg:58.41ms
step:1614/2330 train_time:94283ms step_avg:58.42ms
step:1615/2330 train_time:94340ms step_avg:58.41ms
step:1616/2330 train_time:94400ms step_avg:58.42ms
step:1617/2330 train_time:94456ms step_avg:58.41ms
step:1618/2330 train_time:94519ms step_avg:58.42ms
step:1619/2330 train_time:94576ms step_avg:58.42ms
step:1620/2330 train_time:94637ms step_avg:58.42ms
step:1621/2330 train_time:94694ms step_avg:58.42ms
step:1622/2330 train_time:94756ms step_avg:58.42ms
step:1623/2330 train_time:94812ms step_avg:58.42ms
step:1624/2330 train_time:94875ms step_avg:58.42ms
step:1625/2330 train_time:94932ms step_avg:58.42ms
step:1626/2330 train_time:94995ms step_avg:58.42ms
step:1627/2330 train_time:95052ms step_avg:58.42ms
step:1628/2330 train_time:95113ms step_avg:58.42ms
step:1629/2330 train_time:95170ms step_avg:58.42ms
step:1630/2330 train_time:95232ms step_avg:58.42ms
step:1631/2330 train_time:95290ms step_avg:58.42ms
step:1632/2330 train_time:95351ms step_avg:58.43ms
step:1633/2330 train_time:95409ms step_avg:58.43ms
step:1634/2330 train_time:95470ms step_avg:58.43ms
step:1635/2330 train_time:95527ms step_avg:58.43ms
step:1636/2330 train_time:95587ms step_avg:58.43ms
step:1637/2330 train_time:95646ms step_avg:58.43ms
step:1638/2330 train_time:95706ms step_avg:58.43ms
step:1639/2330 train_time:95764ms step_avg:58.43ms
step:1640/2330 train_time:95825ms step_avg:58.43ms
step:1641/2330 train_time:95883ms step_avg:58.43ms
step:1642/2330 train_time:95943ms step_avg:58.43ms
step:1643/2330 train_time:96001ms step_avg:58.43ms
step:1644/2330 train_time:96061ms step_avg:58.43ms
step:1645/2330 train_time:96118ms step_avg:58.43ms
step:1646/2330 train_time:96180ms step_avg:58.43ms
step:1647/2330 train_time:96237ms step_avg:58.43ms
step:1648/2330 train_time:96298ms step_avg:58.43ms
step:1649/2330 train_time:96354ms step_avg:58.43ms
step:1650/2330 train_time:96415ms step_avg:58.43ms
step:1651/2330 train_time:96472ms step_avg:58.43ms
step:1652/2330 train_time:96535ms step_avg:58.44ms
step:1653/2330 train_time:96592ms step_avg:58.43ms
step:1654/2330 train_time:96653ms step_avg:58.44ms
step:1655/2330 train_time:96710ms step_avg:58.44ms
step:1656/2330 train_time:96772ms step_avg:58.44ms
step:1657/2330 train_time:96829ms step_avg:58.44ms
step:1658/2330 train_time:96890ms step_avg:58.44ms
step:1659/2330 train_time:96947ms step_avg:58.44ms
step:1660/2330 train_time:97008ms step_avg:58.44ms
step:1661/2330 train_time:97066ms step_avg:58.44ms
step:1662/2330 train_time:97128ms step_avg:58.44ms
step:1663/2330 train_time:97185ms step_avg:58.44ms
step:1664/2330 train_time:97246ms step_avg:58.44ms
step:1665/2330 train_time:97305ms step_avg:58.44ms
step:1666/2330 train_time:97365ms step_avg:58.44ms
step:1667/2330 train_time:97423ms step_avg:58.44ms
step:1668/2330 train_time:97483ms step_avg:58.44ms
step:1669/2330 train_time:97541ms step_avg:58.44ms
step:1670/2330 train_time:97601ms step_avg:58.44ms
step:1671/2330 train_time:97658ms step_avg:58.44ms
step:1672/2330 train_time:97718ms step_avg:58.44ms
step:1673/2330 train_time:97776ms step_avg:58.44ms
step:1674/2330 train_time:97837ms step_avg:58.45ms
step:1675/2330 train_time:97894ms step_avg:58.44ms
step:1676/2330 train_time:97954ms step_avg:58.45ms
step:1677/2330 train_time:98011ms step_avg:58.44ms
step:1678/2330 train_time:98073ms step_avg:58.45ms
step:1679/2330 train_time:98130ms step_avg:58.45ms
step:1680/2330 train_time:98192ms step_avg:58.45ms
step:1681/2330 train_time:98249ms step_avg:58.45ms
step:1682/2330 train_time:98310ms step_avg:58.45ms
step:1683/2330 train_time:98368ms step_avg:58.45ms
step:1684/2330 train_time:98430ms step_avg:58.45ms
step:1685/2330 train_time:98488ms step_avg:58.45ms
step:1686/2330 train_time:98549ms step_avg:58.45ms
step:1687/2330 train_time:98607ms step_avg:58.45ms
step:1688/2330 train_time:98667ms step_avg:58.45ms
step:1689/2330 train_time:98725ms step_avg:58.45ms
step:1690/2330 train_time:98786ms step_avg:58.45ms
step:1691/2330 train_time:98844ms step_avg:58.45ms
step:1692/2330 train_time:98904ms step_avg:58.45ms
step:1693/2330 train_time:98963ms step_avg:58.45ms
step:1694/2330 train_time:99023ms step_avg:58.45ms
step:1695/2330 train_time:99080ms step_avg:58.45ms
step:1696/2330 train_time:99141ms step_avg:58.46ms
step:1697/2330 train_time:99198ms step_avg:58.46ms
step:1698/2330 train_time:99259ms step_avg:58.46ms
step:1699/2330 train_time:99316ms step_avg:58.46ms
step:1700/2330 train_time:99378ms step_avg:58.46ms
step:1701/2330 train_time:99434ms step_avg:58.46ms
step:1702/2330 train_time:99497ms step_avg:58.46ms
step:1703/2330 train_time:99554ms step_avg:58.46ms
step:1704/2330 train_time:99614ms step_avg:58.46ms
step:1705/2330 train_time:99671ms step_avg:58.46ms
step:1706/2330 train_time:99733ms step_avg:58.46ms
step:1707/2330 train_time:99790ms step_avg:58.46ms
step:1708/2330 train_time:99852ms step_avg:58.46ms
step:1709/2330 train_time:99909ms step_avg:58.46ms
step:1710/2330 train_time:99970ms step_avg:58.46ms
step:1711/2330 train_time:100028ms step_avg:58.46ms
step:1712/2330 train_time:100090ms step_avg:58.46ms
step:1713/2330 train_time:100148ms step_avg:58.46ms
step:1714/2330 train_time:100209ms step_avg:58.46ms
step:1715/2330 train_time:100266ms step_avg:58.46ms
step:1716/2330 train_time:100327ms step_avg:58.47ms
step:1717/2330 train_time:100386ms step_avg:58.47ms
step:1718/2330 train_time:100446ms step_avg:58.47ms
step:1719/2330 train_time:100505ms step_avg:58.47ms
step:1720/2330 train_time:100565ms step_avg:58.47ms
step:1721/2330 train_time:100623ms step_avg:58.47ms
step:1722/2330 train_time:100683ms step_avg:58.47ms
step:1723/2330 train_time:100740ms step_avg:58.47ms
step:1724/2330 train_time:100801ms step_avg:58.47ms
step:1725/2330 train_time:100857ms step_avg:58.47ms
step:1726/2330 train_time:100918ms step_avg:58.47ms
step:1727/2330 train_time:100974ms step_avg:58.47ms
step:1728/2330 train_time:101036ms step_avg:58.47ms
step:1729/2330 train_time:101093ms step_avg:58.47ms
step:1730/2330 train_time:101155ms step_avg:58.47ms
step:1731/2330 train_time:101212ms step_avg:58.47ms
step:1732/2330 train_time:101274ms step_avg:58.47ms
step:1733/2330 train_time:101331ms step_avg:58.47ms
step:1734/2330 train_time:101393ms step_avg:58.47ms
step:1735/2330 train_time:101451ms step_avg:58.47ms
step:1736/2330 train_time:101512ms step_avg:58.47ms
step:1737/2330 train_time:101570ms step_avg:58.47ms
step:1738/2330 train_time:101630ms step_avg:58.48ms
step:1739/2330 train_time:101687ms step_avg:58.47ms
step:1740/2330 train_time:101750ms step_avg:58.48ms
step:1741/2330 train_time:101807ms step_avg:58.48ms
step:1742/2330 train_time:101868ms step_avg:58.48ms
step:1743/2330 train_time:101925ms step_avg:58.48ms
step:1744/2330 train_time:101985ms step_avg:58.48ms
step:1745/2330 train_time:102043ms step_avg:58.48ms
step:1746/2330 train_time:102104ms step_avg:58.48ms
step:1747/2330 train_time:102162ms step_avg:58.48ms
step:1748/2330 train_time:102222ms step_avg:58.48ms
step:1749/2330 train_time:102280ms step_avg:58.48ms
step:1750/2330 train_time:102340ms step_avg:58.48ms
step:1750/2330 val_loss:3.8230 train_time:102423ms step_avg:58.53ms
step:1751/2330 train_time:102442ms step_avg:58.50ms
step:1752/2330 train_time:102461ms step_avg:58.48ms
step:1753/2330 train_time:102522ms step_avg:58.48ms
step:1754/2330 train_time:102590ms step_avg:58.49ms
step:1755/2330 train_time:102646ms step_avg:58.49ms
step:1756/2330 train_time:102711ms step_avg:58.49ms
step:1757/2330 train_time:102767ms step_avg:58.49ms
step:1758/2330 train_time:102828ms step_avg:58.49ms
step:1759/2330 train_time:102884ms step_avg:58.49ms
step:1760/2330 train_time:102944ms step_avg:58.49ms
step:1761/2330 train_time:103001ms step_avg:58.49ms
step:1762/2330 train_time:103061ms step_avg:58.49ms
step:1763/2330 train_time:103117ms step_avg:58.49ms
step:1764/2330 train_time:103178ms step_avg:58.49ms
step:1765/2330 train_time:103234ms step_avg:58.49ms
step:1766/2330 train_time:103294ms step_avg:58.49ms
step:1767/2330 train_time:103351ms step_avg:58.49ms
step:1768/2330 train_time:103413ms step_avg:58.49ms
step:1769/2330 train_time:103470ms step_avg:58.49ms
step:1770/2330 train_time:103534ms step_avg:58.49ms
step:1771/2330 train_time:103592ms step_avg:58.49ms
step:1772/2330 train_time:103654ms step_avg:58.50ms
step:1773/2330 train_time:103712ms step_avg:58.50ms
step:1774/2330 train_time:103773ms step_avg:58.50ms
step:1775/2330 train_time:103831ms step_avg:58.50ms
step:1776/2330 train_time:103891ms step_avg:58.50ms
step:1777/2330 train_time:103948ms step_avg:58.50ms
step:1778/2330 train_time:104007ms step_avg:58.50ms
step:1779/2330 train_time:104065ms step_avg:58.50ms
step:1780/2330 train_time:104124ms step_avg:58.50ms
step:1781/2330 train_time:104180ms step_avg:58.50ms
step:1782/2330 train_time:104240ms step_avg:58.50ms
step:1783/2330 train_time:104297ms step_avg:58.50ms
step:1784/2330 train_time:104358ms step_avg:58.50ms
step:1785/2330 train_time:104417ms step_avg:58.50ms
step:1786/2330 train_time:104479ms step_avg:58.50ms
step:1787/2330 train_time:104536ms step_avg:58.50ms
step:1788/2330 train_time:104598ms step_avg:58.50ms
step:1789/2330 train_time:104656ms step_avg:58.50ms
step:1790/2330 train_time:104719ms step_avg:58.50ms
step:1791/2330 train_time:104776ms step_avg:58.50ms
step:1792/2330 train_time:104836ms step_avg:58.50ms
step:1793/2330 train_time:104895ms step_avg:58.50ms
step:1794/2330 train_time:104955ms step_avg:58.50ms
step:1795/2330 train_time:105012ms step_avg:58.50ms
step:1796/2330 train_time:105072ms step_avg:58.50ms
step:1797/2330 train_time:105129ms step_avg:58.50ms
step:1798/2330 train_time:105188ms step_avg:58.50ms
step:1799/2330 train_time:105245ms step_avg:58.50ms
step:1800/2330 train_time:105306ms step_avg:58.50ms
step:1801/2330 train_time:105364ms step_avg:58.50ms
step:1802/2330 train_time:105425ms step_avg:58.50ms
step:1803/2330 train_time:105483ms step_avg:58.50ms
step:1804/2330 train_time:105544ms step_avg:58.51ms
step:1805/2330 train_time:105601ms step_avg:58.50ms
step:1806/2330 train_time:105664ms step_avg:58.51ms
step:1807/2330 train_time:105722ms step_avg:58.51ms
step:1808/2330 train_time:105785ms step_avg:58.51ms
step:1809/2330 train_time:105841ms step_avg:58.51ms
step:1810/2330 train_time:105903ms step_avg:58.51ms
step:1811/2330 train_time:105960ms step_avg:58.51ms
step:1812/2330 train_time:106022ms step_avg:58.51ms
step:1813/2330 train_time:106078ms step_avg:58.51ms
step:1814/2330 train_time:106140ms step_avg:58.51ms
step:1815/2330 train_time:106197ms step_avg:58.51ms
step:1816/2330 train_time:106258ms step_avg:58.51ms
step:1817/2330 train_time:106315ms step_avg:58.51ms
step:1818/2330 train_time:106375ms step_avg:58.51ms
step:1819/2330 train_time:106434ms step_avg:58.51ms
step:1820/2330 train_time:106494ms step_avg:58.51ms
step:1821/2330 train_time:106552ms step_avg:58.51ms
step:1822/2330 train_time:106612ms step_avg:58.51ms
step:1823/2330 train_time:106669ms step_avg:58.51ms
step:1824/2330 train_time:106731ms step_avg:58.51ms
step:1825/2330 train_time:106788ms step_avg:58.51ms
step:1826/2330 train_time:106848ms step_avg:58.51ms
step:1827/2330 train_time:106905ms step_avg:58.51ms
step:1828/2330 train_time:106966ms step_avg:58.52ms
step:1829/2330 train_time:107024ms step_avg:58.51ms
step:1830/2330 train_time:107085ms step_avg:58.52ms
step:1831/2330 train_time:107141ms step_avg:58.52ms
step:1832/2330 train_time:107203ms step_avg:58.52ms
step:1833/2330 train_time:107259ms step_avg:58.52ms
step:1834/2330 train_time:107321ms step_avg:58.52ms
step:1835/2330 train_time:107378ms step_avg:58.52ms
step:1836/2330 train_time:107440ms step_avg:58.52ms
step:1837/2330 train_time:107498ms step_avg:58.52ms
step:1838/2330 train_time:107559ms step_avg:58.52ms
step:1839/2330 train_time:107616ms step_avg:58.52ms
step:1840/2330 train_time:107678ms step_avg:58.52ms
step:1841/2330 train_time:107736ms step_avg:58.52ms
step:1842/2330 train_time:107797ms step_avg:58.52ms
step:1843/2330 train_time:107855ms step_avg:58.52ms
step:1844/2330 train_time:107916ms step_avg:58.52ms
step:1845/2330 train_time:107974ms step_avg:58.52ms
step:1846/2330 train_time:108034ms step_avg:58.52ms
step:1847/2330 train_time:108092ms step_avg:58.52ms
step:1848/2330 train_time:108152ms step_avg:58.52ms
step:1849/2330 train_time:108209ms step_avg:58.52ms
step:1850/2330 train_time:108270ms step_avg:58.52ms
step:1851/2330 train_time:108327ms step_avg:58.52ms
step:1852/2330 train_time:108388ms step_avg:58.53ms
step:1853/2330 train_time:108445ms step_avg:58.52ms
step:1854/2330 train_time:108506ms step_avg:58.53ms
step:1855/2330 train_time:108563ms step_avg:58.52ms
step:1856/2330 train_time:108626ms step_avg:58.53ms
step:1857/2330 train_time:108683ms step_avg:58.53ms
step:1858/2330 train_time:108745ms step_avg:58.53ms
step:1859/2330 train_time:108802ms step_avg:58.53ms
step:1860/2330 train_time:108865ms step_avg:58.53ms
step:1861/2330 train_time:108922ms step_avg:58.53ms
step:1862/2330 train_time:108984ms step_avg:58.53ms
step:1863/2330 train_time:109040ms step_avg:58.53ms
step:1864/2330 train_time:109102ms step_avg:58.53ms
step:1865/2330 train_time:109158ms step_avg:58.53ms
step:1866/2330 train_time:109220ms step_avg:58.53ms
step:1867/2330 train_time:109278ms step_avg:58.53ms
step:1868/2330 train_time:109339ms step_avg:58.53ms
step:1869/2330 train_time:109397ms step_avg:58.53ms
step:1870/2330 train_time:109457ms step_avg:58.53ms
step:1871/2330 train_time:109515ms step_avg:58.53ms
step:1872/2330 train_time:109575ms step_avg:58.53ms
step:1873/2330 train_time:109633ms step_avg:58.53ms
step:1874/2330 train_time:109694ms step_avg:58.53ms
step:1875/2330 train_time:109751ms step_avg:58.53ms
step:1876/2330 train_time:109811ms step_avg:58.53ms
step:1877/2330 train_time:109868ms step_avg:58.53ms
step:1878/2330 train_time:109928ms step_avg:58.53ms
step:1879/2330 train_time:109985ms step_avg:58.53ms
step:1880/2330 train_time:110045ms step_avg:58.53ms
step:1881/2330 train_time:110102ms step_avg:58.53ms
step:1882/2330 train_time:110165ms step_avg:58.54ms
step:1883/2330 train_time:110221ms step_avg:58.53ms
step:1884/2330 train_time:110283ms step_avg:58.54ms
step:1885/2330 train_time:110339ms step_avg:58.54ms
step:1886/2330 train_time:110401ms step_avg:58.54ms
step:1887/2330 train_time:110458ms step_avg:58.54ms
step:1888/2330 train_time:110520ms step_avg:58.54ms
step:1889/2330 train_time:110578ms step_avg:58.54ms
step:1890/2330 train_time:110639ms step_avg:58.54ms
step:1891/2330 train_time:110697ms step_avg:58.54ms
step:1892/2330 train_time:110758ms step_avg:58.54ms
step:1893/2330 train_time:110815ms step_avg:58.54ms
step:1894/2330 train_time:110876ms step_avg:58.54ms
step:1895/2330 train_time:110934ms step_avg:58.54ms
step:1896/2330 train_time:110994ms step_avg:58.54ms
step:1897/2330 train_time:111052ms step_avg:58.54ms
step:1898/2330 train_time:111112ms step_avg:58.54ms
step:1899/2330 train_time:111170ms step_avg:58.54ms
step:1900/2330 train_time:111230ms step_avg:58.54ms
step:1901/2330 train_time:111288ms step_avg:58.54ms
step:1902/2330 train_time:111348ms step_avg:58.54ms
step:1903/2330 train_time:111404ms step_avg:58.54ms
step:1904/2330 train_time:111467ms step_avg:58.54ms
step:1905/2330 train_time:111523ms step_avg:58.54ms
step:1906/2330 train_time:111585ms step_avg:58.54ms
step:1907/2330 train_time:111641ms step_avg:58.54ms
step:1908/2330 train_time:111704ms step_avg:58.55ms
step:1909/2330 train_time:111761ms step_avg:58.54ms
step:1910/2330 train_time:111824ms step_avg:58.55ms
step:1911/2330 train_time:111880ms step_avg:58.55ms
step:1912/2330 train_time:111941ms step_avg:58.55ms
step:1913/2330 train_time:111998ms step_avg:58.55ms
step:1914/2330 train_time:112060ms step_avg:58.55ms
step:1915/2330 train_time:112117ms step_avg:58.55ms
step:1916/2330 train_time:112179ms step_avg:58.55ms
step:1917/2330 train_time:112238ms step_avg:58.55ms
step:1918/2330 train_time:112299ms step_avg:58.55ms
step:1919/2330 train_time:112357ms step_avg:58.55ms
step:1920/2330 train_time:112417ms step_avg:58.55ms
step:1921/2330 train_time:112475ms step_avg:58.55ms
step:1922/2330 train_time:112535ms step_avg:58.55ms
step:1923/2330 train_time:112593ms step_avg:58.55ms
step:1924/2330 train_time:112653ms step_avg:58.55ms
step:1925/2330 train_time:112711ms step_avg:58.55ms
step:1926/2330 train_time:112771ms step_avg:58.55ms
step:1927/2330 train_time:112828ms step_avg:58.55ms
step:1928/2330 train_time:112888ms step_avg:58.55ms
step:1929/2330 train_time:112945ms step_avg:58.55ms
step:1930/2330 train_time:113006ms step_avg:58.55ms
step:1931/2330 train_time:113063ms step_avg:58.55ms
step:1932/2330 train_time:113124ms step_avg:58.55ms
step:1933/2330 train_time:113180ms step_avg:58.55ms
step:1934/2330 train_time:113242ms step_avg:58.55ms
step:1935/2330 train_time:113299ms step_avg:58.55ms
step:1936/2330 train_time:113362ms step_avg:58.55ms
step:1937/2330 train_time:113419ms step_avg:58.55ms
step:1938/2330 train_time:113481ms step_avg:58.56ms
step:1939/2330 train_time:113538ms step_avg:58.55ms
step:1940/2330 train_time:113599ms step_avg:58.56ms
step:1941/2330 train_time:113657ms step_avg:58.56ms
step:1942/2330 train_time:113719ms step_avg:58.56ms
step:1943/2330 train_time:113776ms step_avg:58.56ms
step:1944/2330 train_time:113838ms step_avg:58.56ms
step:1945/2330 train_time:113896ms step_avg:58.56ms
step:1946/2330 train_time:113956ms step_avg:58.56ms
step:1947/2330 train_time:114014ms step_avg:58.56ms
step:1948/2330 train_time:114074ms step_avg:58.56ms
step:1949/2330 train_time:114132ms step_avg:58.56ms
step:1950/2330 train_time:114192ms step_avg:58.56ms
step:1951/2330 train_time:114249ms step_avg:58.56ms
step:1952/2330 train_time:114310ms step_avg:58.56ms
step:1953/2330 train_time:114367ms step_avg:58.56ms
step:1954/2330 train_time:114427ms step_avg:58.56ms
step:1955/2330 train_time:114483ms step_avg:58.56ms
step:1956/2330 train_time:114546ms step_avg:58.56ms
step:1957/2330 train_time:114602ms step_avg:58.56ms
step:1958/2330 train_time:114665ms step_avg:58.56ms
step:1959/2330 train_time:114722ms step_avg:58.56ms
step:1960/2330 train_time:114784ms step_avg:58.56ms
step:1961/2330 train_time:114840ms step_avg:58.56ms
step:1962/2330 train_time:114902ms step_avg:58.56ms
step:1963/2330 train_time:114958ms step_avg:58.56ms
step:1964/2330 train_time:115021ms step_avg:58.56ms
step:1965/2330 train_time:115078ms step_avg:58.56ms
step:1966/2330 train_time:115140ms step_avg:58.57ms
step:1967/2330 train_time:115198ms step_avg:58.57ms
step:1968/2330 train_time:115259ms step_avg:58.57ms
step:1969/2330 train_time:115317ms step_avg:58.57ms
step:1970/2330 train_time:115377ms step_avg:58.57ms
step:1971/2330 train_time:115435ms step_avg:58.57ms
step:1972/2330 train_time:115495ms step_avg:58.57ms
step:1973/2330 train_time:115553ms step_avg:58.57ms
step:1974/2330 train_time:115613ms step_avg:58.57ms
step:1975/2330 train_time:115671ms step_avg:58.57ms
step:1976/2330 train_time:115732ms step_avg:58.57ms
step:1977/2330 train_time:115788ms step_avg:58.57ms
step:1978/2330 train_time:115848ms step_avg:58.57ms
step:1979/2330 train_time:115905ms step_avg:58.57ms
step:1980/2330 train_time:115967ms step_avg:58.57ms
step:1981/2330 train_time:116024ms step_avg:58.57ms
step:1982/2330 train_time:116086ms step_avg:58.57ms
step:1983/2330 train_time:116142ms step_avg:58.57ms
step:1984/2330 train_time:116204ms step_avg:58.57ms
step:1985/2330 train_time:116261ms step_avg:58.57ms
step:1986/2330 train_time:116323ms step_avg:58.57ms
step:1987/2330 train_time:116379ms step_avg:58.57ms
step:1988/2330 train_time:116441ms step_avg:58.57ms
step:1989/2330 train_time:116498ms step_avg:58.57ms
step:1990/2330 train_time:116559ms step_avg:58.57ms
step:1991/2330 train_time:116615ms step_avg:58.57ms
step:1992/2330 train_time:116678ms step_avg:58.57ms
step:1993/2330 train_time:116736ms step_avg:58.57ms
step:1994/2330 train_time:116796ms step_avg:58.57ms
step:1995/2330 train_time:116853ms step_avg:58.57ms
step:1996/2330 train_time:116914ms step_avg:58.57ms
step:1997/2330 train_time:116972ms step_avg:58.57ms
step:1998/2330 train_time:117033ms step_avg:58.57ms
step:1999/2330 train_time:117090ms step_avg:58.57ms
step:2000/2330 train_time:117151ms step_avg:58.58ms
step:2000/2330 val_loss:3.7610 train_time:117232ms step_avg:58.62ms
step:2001/2330 train_time:117250ms step_avg:58.60ms
step:2002/2330 train_time:117270ms step_avg:58.58ms
step:2003/2330 train_time:117328ms step_avg:58.58ms
step:2004/2330 train_time:117396ms step_avg:58.58ms
step:2005/2330 train_time:117454ms step_avg:58.58ms
step:2006/2330 train_time:117516ms step_avg:58.58ms
step:2007/2330 train_time:117573ms step_avg:58.58ms
step:2008/2330 train_time:117633ms step_avg:58.58ms
step:2009/2330 train_time:117690ms step_avg:58.58ms
step:2010/2330 train_time:117751ms step_avg:58.58ms
step:2011/2330 train_time:117807ms step_avg:58.58ms
step:2012/2330 train_time:117866ms step_avg:58.58ms
step:2013/2330 train_time:117923ms step_avg:58.58ms
step:2014/2330 train_time:117983ms step_avg:58.58ms
step:2015/2330 train_time:118040ms step_avg:58.58ms
step:2016/2330 train_time:118099ms step_avg:58.58ms
step:2017/2330 train_time:118157ms step_avg:58.58ms
step:2018/2330 train_time:118218ms step_avg:58.58ms
step:2019/2330 train_time:118277ms step_avg:58.58ms
step:2020/2330 train_time:118341ms step_avg:58.58ms
step:2021/2330 train_time:118398ms step_avg:58.58ms
step:2022/2330 train_time:118461ms step_avg:58.59ms
step:2023/2330 train_time:118518ms step_avg:58.59ms
step:2024/2330 train_time:118581ms step_avg:58.59ms
step:2025/2330 train_time:118638ms step_avg:58.59ms
step:2026/2330 train_time:118700ms step_avg:58.59ms
step:2027/2330 train_time:118756ms step_avg:58.59ms
step:2028/2330 train_time:118817ms step_avg:58.59ms
step:2029/2330 train_time:118873ms step_avg:58.59ms
step:2030/2330 train_time:118934ms step_avg:58.59ms
step:2031/2330 train_time:118992ms step_avg:58.59ms
step:2032/2330 train_time:119052ms step_avg:58.59ms
step:2033/2330 train_time:119110ms step_avg:58.59ms
step:2034/2330 train_time:119170ms step_avg:58.59ms
step:2035/2330 train_time:119228ms step_avg:58.59ms
step:2036/2330 train_time:119289ms step_avg:58.59ms
step:2037/2330 train_time:119347ms step_avg:58.59ms
step:2038/2330 train_time:119408ms step_avg:58.59ms
step:2039/2330 train_time:119466ms step_avg:58.59ms
step:2040/2330 train_time:119527ms step_avg:58.59ms
step:2041/2330 train_time:119585ms step_avg:58.59ms
step:2042/2330 train_time:119645ms step_avg:58.59ms
step:2043/2330 train_time:119702ms step_avg:58.59ms
step:2044/2330 train_time:119763ms step_avg:58.59ms
step:2045/2330 train_time:119820ms step_avg:58.59ms
step:2046/2330 train_time:119881ms step_avg:58.59ms
step:2047/2330 train_time:119938ms step_avg:58.59ms
step:2048/2330 train_time:119999ms step_avg:58.59ms
step:2049/2330 train_time:120055ms step_avg:58.59ms
step:2050/2330 train_time:120116ms step_avg:58.59ms
step:2051/2330 train_time:120174ms step_avg:58.59ms
step:2052/2330 train_time:120237ms step_avg:58.60ms
step:2053/2330 train_time:120295ms step_avg:58.59ms
step:2054/2330 train_time:120357ms step_avg:58.60ms
step:2055/2330 train_time:120415ms step_avg:58.60ms
step:2056/2330 train_time:120476ms step_avg:58.60ms
step:2057/2330 train_time:120536ms step_avg:58.60ms
step:2058/2330 train_time:120596ms step_avg:58.60ms
step:2059/2330 train_time:120655ms step_avg:58.60ms
step:2060/2330 train_time:120715ms step_avg:58.60ms
step:2061/2330 train_time:120772ms step_avg:58.60ms
step:2062/2330 train_time:120832ms step_avg:58.60ms
step:2063/2330 train_time:120889ms step_avg:58.60ms
step:2064/2330 train_time:120950ms step_avg:58.60ms
step:2065/2330 train_time:121007ms step_avg:58.60ms
step:2066/2330 train_time:121066ms step_avg:58.60ms
step:2067/2330 train_time:121123ms step_avg:58.60ms
step:2068/2330 train_time:121184ms step_avg:58.60ms
step:2069/2330 train_time:121241ms step_avg:58.60ms
step:2070/2330 train_time:121302ms step_avg:58.60ms
step:2071/2330 train_time:121359ms step_avg:58.60ms
step:2072/2330 train_time:121422ms step_avg:58.60ms
step:2073/2330 train_time:121480ms step_avg:58.60ms
step:2074/2330 train_time:121541ms step_avg:58.60ms
step:2075/2330 train_time:121598ms step_avg:58.60ms
step:2076/2330 train_time:121660ms step_avg:58.60ms
step:2077/2330 train_time:121717ms step_avg:58.60ms
step:2078/2330 train_time:121779ms step_avg:58.60ms
step:2079/2330 train_time:121836ms step_avg:58.60ms
step:2080/2330 train_time:121897ms step_avg:58.60ms
step:2081/2330 train_time:121954ms step_avg:58.60ms
step:2082/2330 train_time:122014ms step_avg:58.60ms
step:2083/2330 train_time:122072ms step_avg:58.60ms
step:2084/2330 train_time:122133ms step_avg:58.60ms
step:2085/2330 train_time:122190ms step_avg:58.60ms
step:2086/2330 train_time:122250ms step_avg:58.60ms
step:2087/2330 train_time:122308ms step_avg:58.60ms
step:2088/2330 train_time:122368ms step_avg:58.61ms
step:2089/2330 train_time:122426ms step_avg:58.61ms
step:2090/2330 train_time:122486ms step_avg:58.61ms
step:2091/2330 train_time:122544ms step_avg:58.61ms
step:2092/2330 train_time:122604ms step_avg:58.61ms
step:2093/2330 train_time:122661ms step_avg:58.61ms
step:2094/2330 train_time:122723ms step_avg:58.61ms
step:2095/2330 train_time:122780ms step_avg:58.61ms
step:2096/2330 train_time:122841ms step_avg:58.61ms
step:2097/2330 train_time:122898ms step_avg:58.61ms
step:2098/2330 train_time:122959ms step_avg:58.61ms
step:2099/2330 train_time:123015ms step_avg:58.61ms
step:2100/2330 train_time:123079ms step_avg:58.61ms
step:2101/2330 train_time:123135ms step_avg:58.61ms
step:2102/2330 train_time:123197ms step_avg:58.61ms
step:2103/2330 train_time:123254ms step_avg:58.61ms
step:2104/2330 train_time:123316ms step_avg:58.61ms
step:2105/2330 train_time:123374ms step_avg:58.61ms
step:2106/2330 train_time:123435ms step_avg:58.61ms
step:2107/2330 train_time:123493ms step_avg:58.61ms
step:2108/2330 train_time:123554ms step_avg:58.61ms
step:2109/2330 train_time:123612ms step_avg:58.61ms
step:2110/2330 train_time:123673ms step_avg:58.61ms
step:2111/2330 train_time:123730ms step_avg:58.61ms
step:2112/2330 train_time:123792ms step_avg:58.61ms
step:2113/2330 train_time:123850ms step_avg:58.61ms
step:2114/2330 train_time:123910ms step_avg:58.61ms
step:2115/2330 train_time:123967ms step_avg:58.61ms
step:2116/2330 train_time:124027ms step_avg:58.61ms
step:2117/2330 train_time:124084ms step_avg:58.61ms
step:2118/2330 train_time:124143ms step_avg:58.61ms
step:2119/2330 train_time:124200ms step_avg:58.61ms
step:2120/2330 train_time:124261ms step_avg:58.61ms
step:2121/2330 train_time:124317ms step_avg:58.61ms
step:2122/2330 train_time:124379ms step_avg:58.61ms
step:2123/2330 train_time:124436ms step_avg:58.61ms
step:2124/2330 train_time:124499ms step_avg:58.62ms
step:2125/2330 train_time:124556ms step_avg:58.61ms
step:2126/2330 train_time:124619ms step_avg:58.62ms
step:2127/2330 train_time:124676ms step_avg:58.62ms
step:2128/2330 train_time:124738ms step_avg:58.62ms
step:2129/2330 train_time:124795ms step_avg:58.62ms
step:2130/2330 train_time:124856ms step_avg:58.62ms
step:2131/2330 train_time:124913ms step_avg:58.62ms
step:2132/2330 train_time:124976ms step_avg:58.62ms
step:2133/2330 train_time:125033ms step_avg:58.62ms
step:2134/2330 train_time:125094ms step_avg:58.62ms
step:2135/2330 train_time:125151ms step_avg:58.62ms
step:2136/2330 train_time:125211ms step_avg:58.62ms
step:2137/2330 train_time:125269ms step_avg:58.62ms
step:2138/2330 train_time:125330ms step_avg:58.62ms
step:2139/2330 train_time:125388ms step_avg:58.62ms
step:2140/2330 train_time:125448ms step_avg:58.62ms
step:2141/2330 train_time:125506ms step_avg:58.62ms
step:2142/2330 train_time:125566ms step_avg:58.62ms
step:2143/2330 train_time:125623ms step_avg:58.62ms
step:2144/2330 train_time:125683ms step_avg:58.62ms
step:2145/2330 train_time:125740ms step_avg:58.62ms
step:2146/2330 train_time:125802ms step_avg:58.62ms
step:2147/2330 train_time:125858ms step_avg:58.62ms
step:2148/2330 train_time:125920ms step_avg:58.62ms
step:2149/2330 train_time:125977ms step_avg:58.62ms
step:2150/2330 train_time:126038ms step_avg:58.62ms
step:2151/2330 train_time:126095ms step_avg:58.62ms
step:2152/2330 train_time:126157ms step_avg:58.62ms
step:2153/2330 train_time:126215ms step_avg:58.62ms
step:2154/2330 train_time:126275ms step_avg:58.62ms
step:2155/2330 train_time:126333ms step_avg:58.62ms
step:2156/2330 train_time:126396ms step_avg:58.63ms
step:2157/2330 train_time:126454ms step_avg:58.62ms
step:2158/2330 train_time:126515ms step_avg:58.63ms
step:2159/2330 train_time:126572ms step_avg:58.63ms
step:2160/2330 train_time:126634ms step_avg:58.63ms
step:2161/2330 train_time:126691ms step_avg:58.63ms
step:2162/2330 train_time:126751ms step_avg:58.63ms
step:2163/2330 train_time:126808ms step_avg:58.63ms
step:2164/2330 train_time:126868ms step_avg:58.63ms
step:2165/2330 train_time:126926ms step_avg:58.63ms
step:2166/2330 train_time:126987ms step_avg:58.63ms
step:2167/2330 train_time:127044ms step_avg:58.63ms
step:2168/2330 train_time:127104ms step_avg:58.63ms
step:2169/2330 train_time:127160ms step_avg:58.63ms
step:2170/2330 train_time:127222ms step_avg:58.63ms
step:2171/2330 train_time:127279ms step_avg:58.63ms
step:2172/2330 train_time:127341ms step_avg:58.63ms
step:2173/2330 train_time:127398ms step_avg:58.63ms
step:2174/2330 train_time:127460ms step_avg:58.63ms
step:2175/2330 train_time:127517ms step_avg:58.63ms
step:2176/2330 train_time:127579ms step_avg:58.63ms
step:2177/2330 train_time:127636ms step_avg:58.63ms
step:2178/2330 train_time:127697ms step_avg:58.63ms
step:2179/2330 train_time:127755ms step_avg:58.63ms
step:2180/2330 train_time:127817ms step_avg:58.63ms
step:2181/2330 train_time:127874ms step_avg:58.63ms
step:2182/2330 train_time:127935ms step_avg:58.63ms
step:2183/2330 train_time:127993ms step_avg:58.63ms
step:2184/2330 train_time:128054ms step_avg:58.63ms
step:2185/2330 train_time:128111ms step_avg:58.63ms
step:2186/2330 train_time:128171ms step_avg:58.63ms
step:2187/2330 train_time:128230ms step_avg:58.63ms
step:2188/2330 train_time:128290ms step_avg:58.63ms
step:2189/2330 train_time:128348ms step_avg:58.63ms
step:2190/2330 train_time:128407ms step_avg:58.63ms
step:2191/2330 train_time:128464ms step_avg:58.63ms
step:2192/2330 train_time:128525ms step_avg:58.63ms
step:2193/2330 train_time:128582ms step_avg:58.63ms
step:2194/2330 train_time:128643ms step_avg:58.63ms
step:2195/2330 train_time:128700ms step_avg:58.63ms
step:2196/2330 train_time:128761ms step_avg:58.63ms
step:2197/2330 train_time:128817ms step_avg:58.63ms
step:2198/2330 train_time:128880ms step_avg:58.64ms
step:2199/2330 train_time:128937ms step_avg:58.63ms
step:2200/2330 train_time:129000ms step_avg:58.64ms
step:2201/2330 train_time:129056ms step_avg:58.64ms
step:2202/2330 train_time:129119ms step_avg:58.64ms
step:2203/2330 train_time:129176ms step_avg:58.64ms
step:2204/2330 train_time:129239ms step_avg:58.64ms
step:2205/2330 train_time:129296ms step_avg:58.64ms
step:2206/2330 train_time:129357ms step_avg:58.64ms
step:2207/2330 train_time:129415ms step_avg:58.64ms
step:2208/2330 train_time:129477ms step_avg:58.64ms
step:2209/2330 train_time:129535ms step_avg:58.64ms
step:2210/2330 train_time:129596ms step_avg:58.64ms
step:2211/2330 train_time:129655ms step_avg:58.64ms
step:2212/2330 train_time:129714ms step_avg:58.64ms
step:2213/2330 train_time:129771ms step_avg:58.64ms
step:2214/2330 train_time:129833ms step_avg:58.64ms
step:2215/2330 train_time:129890ms step_avg:58.64ms
step:2216/2330 train_time:129950ms step_avg:58.64ms
step:2217/2330 train_time:130007ms step_avg:58.64ms
step:2218/2330 train_time:130068ms step_avg:58.64ms
step:2219/2330 train_time:130125ms step_avg:58.64ms
step:2220/2330 train_time:130185ms step_avg:58.64ms
step:2221/2330 train_time:130242ms step_avg:58.64ms
step:2222/2330 train_time:130303ms step_avg:58.64ms
step:2223/2330 train_time:130359ms step_avg:58.64ms
step:2224/2330 train_time:130423ms step_avg:58.64ms
step:2225/2330 train_time:130480ms step_avg:58.64ms
step:2226/2330 train_time:130541ms step_avg:58.64ms
step:2227/2330 train_time:130598ms step_avg:58.64ms
step:2228/2330 train_time:130660ms step_avg:58.64ms
step:2229/2330 train_time:130717ms step_avg:58.64ms
step:2230/2330 train_time:130779ms step_avg:58.65ms
step:2231/2330 train_time:130836ms step_avg:58.64ms
step:2232/2330 train_time:130899ms step_avg:58.65ms
step:2233/2330 train_time:130955ms step_avg:58.65ms
step:2234/2330 train_time:131018ms step_avg:58.65ms
step:2235/2330 train_time:131075ms step_avg:58.65ms
step:2236/2330 train_time:131137ms step_avg:58.65ms
step:2237/2330 train_time:131195ms step_avg:58.65ms
step:2238/2330 train_time:131255ms step_avg:58.65ms
step:2239/2330 train_time:131313ms step_avg:58.65ms
step:2240/2330 train_time:131373ms step_avg:58.65ms
step:2241/2330 train_time:131432ms step_avg:58.65ms
step:2242/2330 train_time:131491ms step_avg:58.65ms
step:2243/2330 train_time:131549ms step_avg:58.65ms
step:2244/2330 train_time:131608ms step_avg:58.65ms
step:2245/2330 train_time:131666ms step_avg:58.65ms
step:2246/2330 train_time:131726ms step_avg:58.65ms
step:2247/2330 train_time:131783ms step_avg:58.65ms
step:2248/2330 train_time:131845ms step_avg:58.65ms
step:2249/2330 train_time:131901ms step_avg:58.65ms
step:2250/2330 train_time:131962ms step_avg:58.65ms
step:2250/2330 val_loss:3.7111 train_time:132044ms step_avg:58.69ms
step:2251/2330 train_time:132064ms step_avg:58.67ms
step:2252/2330 train_time:132085ms step_avg:58.65ms
step:2253/2330 train_time:132143ms step_avg:58.65ms
step:2254/2330 train_time:132210ms step_avg:58.66ms
step:2255/2330 train_time:132267ms step_avg:58.65ms
step:2256/2330 train_time:132329ms step_avg:58.66ms
step:2257/2330 train_time:132386ms step_avg:58.66ms
step:2258/2330 train_time:132446ms step_avg:58.66ms
step:2259/2330 train_time:132503ms step_avg:58.66ms
step:2260/2330 train_time:132562ms step_avg:58.66ms
step:2261/2330 train_time:132619ms step_avg:58.66ms
step:2262/2330 train_time:132678ms step_avg:58.66ms
step:2263/2330 train_time:132735ms step_avg:58.65ms
step:2264/2330 train_time:132795ms step_avg:58.66ms
step:2265/2330 train_time:132853ms step_avg:58.65ms
step:2266/2330 train_time:132912ms step_avg:58.65ms
step:2267/2330 train_time:132970ms step_avg:58.65ms
step:2268/2330 train_time:133031ms step_avg:58.66ms
step:2269/2330 train_time:133091ms step_avg:58.66ms
step:2270/2330 train_time:133152ms step_avg:58.66ms
step:2271/2330 train_time:133212ms step_avg:58.66ms
step:2272/2330 train_time:133274ms step_avg:58.66ms
step:2273/2330 train_time:133332ms step_avg:58.66ms
step:2274/2330 train_time:133392ms step_avg:58.66ms
step:2275/2330 train_time:133449ms step_avg:58.66ms
step:2276/2330 train_time:133508ms step_avg:58.66ms
step:2277/2330 train_time:133566ms step_avg:58.66ms
step:2278/2330 train_time:133625ms step_avg:58.66ms
step:2279/2330 train_time:133681ms step_avg:58.66ms
step:2280/2330 train_time:133742ms step_avg:58.66ms
step:2281/2330 train_time:133798ms step_avg:58.66ms
step:2282/2330 train_time:133859ms step_avg:58.66ms
step:2283/2330 train_time:133915ms step_avg:58.66ms
step:2284/2330 train_time:133977ms step_avg:58.66ms
step:2285/2330 train_time:134035ms step_avg:58.66ms
step:2286/2330 train_time:134098ms step_avg:58.66ms
step:2287/2330 train_time:134156ms step_avg:58.66ms
step:2288/2330 train_time:134219ms step_avg:58.66ms
step:2289/2330 train_time:134276ms step_avg:58.66ms
step:2290/2330 train_time:134339ms step_avg:58.66ms
step:2291/2330 train_time:134397ms step_avg:58.66ms
step:2292/2330 train_time:134458ms step_avg:58.66ms
step:2293/2330 train_time:134516ms step_avg:58.66ms
step:2294/2330 train_time:134576ms step_avg:58.66ms
step:2295/2330 train_time:134633ms step_avg:58.66ms
step:2296/2330 train_time:134693ms step_avg:58.66ms
step:2297/2330 train_time:134751ms step_avg:58.66ms
step:2298/2330 train_time:134811ms step_avg:58.66ms
step:2299/2330 train_time:134869ms step_avg:58.66ms
step:2300/2330 train_time:134929ms step_avg:58.66ms
step:2301/2330 train_time:134986ms step_avg:58.66ms
step:2302/2330 train_time:135047ms step_avg:58.67ms
step:2303/2330 train_time:135105ms step_avg:58.66ms
step:2304/2330 train_time:135166ms step_avg:58.67ms
step:2305/2330 train_time:135224ms step_avg:58.67ms
step:2306/2330 train_time:135285ms step_avg:58.67ms
step:2307/2330 train_time:135342ms step_avg:58.67ms
step:2308/2330 train_time:135404ms step_avg:58.67ms
step:2309/2330 train_time:135461ms step_avg:58.67ms
step:2310/2330 train_time:135522ms step_avg:58.67ms
step:2311/2330 train_time:135579ms step_avg:58.67ms
step:2312/2330 train_time:135640ms step_avg:58.67ms
step:2313/2330 train_time:135697ms step_avg:58.67ms
step:2314/2330 train_time:135758ms step_avg:58.67ms
step:2315/2330 train_time:135814ms step_avg:58.67ms
step:2316/2330 train_time:135875ms step_avg:58.67ms
step:2317/2330 train_time:135933ms step_avg:58.67ms
step:2318/2330 train_time:135994ms step_avg:58.67ms
step:2319/2330 train_time:136051ms step_avg:58.67ms
step:2320/2330 train_time:136113ms step_avg:58.67ms
step:2321/2330 train_time:136171ms step_avg:58.67ms
step:2322/2330 train_time:136232ms step_avg:58.67ms
step:2323/2330 train_time:136291ms step_avg:58.67ms
step:2324/2330 train_time:136351ms step_avg:58.67ms
step:2325/2330 train_time:136410ms step_avg:58.67ms
step:2326/2330 train_time:136470ms step_avg:58.67ms
step:2327/2330 train_time:136528ms step_avg:58.67ms
step:2328/2330 train_time:136588ms step_avg:58.67ms
step:2329/2330 train_time:136645ms step_avg:58.67ms
step:2330/2330 train_time:136704ms step_avg:58.67ms
step:2330/2330 val_loss:3.6961 train_time:136786ms step_avg:58.71ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
