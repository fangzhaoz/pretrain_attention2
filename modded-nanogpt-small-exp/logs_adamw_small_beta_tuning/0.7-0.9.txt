import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--beta1', type=str, required=True)
parser.add_argument('--beta2', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_beta_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=6e-4,  betas=(float(args2.beta1), float(args2.beta2)), eps=1e-10, weight_decay=0.0, fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt", exist_ok=True)
            torch.save(log, f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 05:21:14 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   27C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   26C    P0             115W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   24C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   27C    P0             113W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   25C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   26C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:84ms step_avg:84.32ms
step:2/2330 train_time:178ms step_avg:89.21ms
step:3/2330 train_time:198ms step_avg:65.85ms
step:4/2330 train_time:217ms step_avg:54.35ms
step:5/2330 train_time:269ms step_avg:53.90ms
step:6/2330 train_time:327ms step_avg:54.58ms
step:7/2330 train_time:383ms step_avg:54.64ms
step:8/2330 train_time:440ms step_avg:55.06ms
step:9/2330 train_time:496ms step_avg:55.12ms
step:10/2330 train_time:554ms step_avg:55.43ms
step:11/2330 train_time:610ms step_avg:55.43ms
step:12/2330 train_time:668ms step_avg:55.66ms
step:13/2330 train_time:724ms step_avg:55.67ms
step:14/2330 train_time:782ms step_avg:55.84ms
step:15/2330 train_time:837ms step_avg:55.79ms
step:16/2330 train_time:895ms step_avg:55.92ms
step:17/2330 train_time:950ms step_avg:55.91ms
step:18/2330 train_time:1010ms step_avg:56.13ms
step:19/2330 train_time:1069ms step_avg:56.28ms
step:20/2330 train_time:1130ms step_avg:56.49ms
step:21/2330 train_time:1187ms step_avg:56.54ms
step:22/2330 train_time:1248ms step_avg:56.72ms
step:23/2330 train_time:1304ms step_avg:56.70ms
step:24/2330 train_time:1365ms step_avg:56.86ms
step:25/2330 train_time:1420ms step_avg:56.80ms
step:26/2330 train_time:1481ms step_avg:56.95ms
step:27/2330 train_time:1536ms step_avg:56.90ms
step:28/2330 train_time:1594ms step_avg:56.94ms
step:29/2330 train_time:1650ms step_avg:56.89ms
step:30/2330 train_time:1708ms step_avg:56.94ms
step:31/2330 train_time:1763ms step_avg:56.89ms
step:32/2330 train_time:1822ms step_avg:56.94ms
step:33/2330 train_time:1878ms step_avg:56.90ms
step:34/2330 train_time:1936ms step_avg:56.93ms
step:35/2330 train_time:1992ms step_avg:56.90ms
step:36/2330 train_time:2052ms step_avg:56.99ms
step:37/2330 train_time:2109ms step_avg:57.00ms
step:38/2330 train_time:2170ms step_avg:57.09ms
step:39/2330 train_time:2227ms step_avg:57.09ms
step:40/2330 train_time:2286ms step_avg:57.16ms
step:41/2330 train_time:2343ms step_avg:57.15ms
step:42/2330 train_time:2403ms step_avg:57.21ms
step:43/2330 train_time:2459ms step_avg:57.19ms
step:44/2330 train_time:2518ms step_avg:57.22ms
step:45/2330 train_time:2574ms step_avg:57.19ms
step:46/2330 train_time:2632ms step_avg:57.21ms
step:47/2330 train_time:2688ms step_avg:57.19ms
step:48/2330 train_time:2746ms step_avg:57.21ms
step:49/2330 train_time:2802ms step_avg:57.18ms
step:50/2330 train_time:2860ms step_avg:57.20ms
step:51/2330 train_time:2916ms step_avg:57.17ms
step:52/2330 train_time:2974ms step_avg:57.20ms
step:53/2330 train_time:3031ms step_avg:57.18ms
step:54/2330 train_time:3091ms step_avg:57.25ms
step:55/2330 train_time:3148ms step_avg:57.24ms
step:56/2330 train_time:3207ms step_avg:57.28ms
step:57/2330 train_time:3264ms step_avg:57.27ms
step:58/2330 train_time:3325ms step_avg:57.32ms
step:59/2330 train_time:3381ms step_avg:57.31ms
step:60/2330 train_time:3441ms step_avg:57.34ms
step:61/2330 train_time:3497ms step_avg:57.33ms
step:62/2330 train_time:3555ms step_avg:57.35ms
step:63/2330 train_time:3611ms step_avg:57.32ms
step:64/2330 train_time:3671ms step_avg:57.35ms
step:65/2330 train_time:3727ms step_avg:57.33ms
step:66/2330 train_time:3785ms step_avg:57.35ms
step:67/2330 train_time:3840ms step_avg:57.32ms
step:68/2330 train_time:3900ms step_avg:57.36ms
step:69/2330 train_time:3956ms step_avg:57.34ms
step:70/2330 train_time:4015ms step_avg:57.36ms
step:71/2330 train_time:4072ms step_avg:57.35ms
step:72/2330 train_time:4131ms step_avg:57.37ms
step:73/2330 train_time:4188ms step_avg:57.37ms
step:74/2330 train_time:4248ms step_avg:57.41ms
step:75/2330 train_time:4306ms step_avg:57.41ms
step:76/2330 train_time:4365ms step_avg:57.43ms
step:77/2330 train_time:4421ms step_avg:57.42ms
step:78/2330 train_time:4480ms step_avg:57.44ms
step:79/2330 train_time:4536ms step_avg:57.41ms
step:80/2330 train_time:4594ms step_avg:57.43ms
step:81/2330 train_time:4650ms step_avg:57.40ms
step:82/2330 train_time:4709ms step_avg:57.43ms
step:83/2330 train_time:4764ms step_avg:57.40ms
step:84/2330 train_time:4824ms step_avg:57.43ms
step:85/2330 train_time:4880ms step_avg:57.41ms
step:86/2330 train_time:4938ms step_avg:57.42ms
step:87/2330 train_time:4994ms step_avg:57.41ms
step:88/2330 train_time:5054ms step_avg:57.43ms
step:89/2330 train_time:5109ms step_avg:57.41ms
step:90/2330 train_time:5169ms step_avg:57.43ms
step:91/2330 train_time:5225ms step_avg:57.42ms
step:92/2330 train_time:5284ms step_avg:57.44ms
step:93/2330 train_time:5341ms step_avg:57.43ms
step:94/2330 train_time:5400ms step_avg:57.45ms
step:95/2330 train_time:5457ms step_avg:57.44ms
step:96/2330 train_time:5515ms step_avg:57.45ms
step:97/2330 train_time:5571ms step_avg:57.43ms
step:98/2330 train_time:5629ms step_avg:57.44ms
step:99/2330 train_time:5685ms step_avg:57.43ms
step:100/2330 train_time:5744ms step_avg:57.44ms
step:101/2330 train_time:5801ms step_avg:57.43ms
step:102/2330 train_time:5859ms step_avg:57.44ms
step:103/2330 train_time:5915ms step_avg:57.42ms
step:104/2330 train_time:5973ms step_avg:57.43ms
step:105/2330 train_time:6029ms step_avg:57.42ms
step:106/2330 train_time:6088ms step_avg:57.44ms
step:107/2330 train_time:6145ms step_avg:57.43ms
step:108/2330 train_time:6204ms step_avg:57.44ms
step:109/2330 train_time:6261ms step_avg:57.44ms
step:110/2330 train_time:6320ms step_avg:57.45ms
step:111/2330 train_time:6376ms step_avg:57.44ms
step:112/2330 train_time:6435ms step_avg:57.46ms
step:113/2330 train_time:6491ms step_avg:57.44ms
step:114/2330 train_time:6551ms step_avg:57.47ms
step:115/2330 train_time:6607ms step_avg:57.46ms
step:116/2330 train_time:6666ms step_avg:57.47ms
step:117/2330 train_time:6722ms step_avg:57.45ms
step:118/2330 train_time:6781ms step_avg:57.47ms
step:119/2330 train_time:6838ms step_avg:57.46ms
step:120/2330 train_time:6896ms step_avg:57.46ms
step:121/2330 train_time:6952ms step_avg:57.45ms
step:122/2330 train_time:7011ms step_avg:57.47ms
step:123/2330 train_time:7067ms step_avg:57.45ms
step:124/2330 train_time:7126ms step_avg:57.47ms
step:125/2330 train_time:7182ms step_avg:57.46ms
step:126/2330 train_time:7242ms step_avg:57.48ms
step:127/2330 train_time:7298ms step_avg:57.47ms
step:128/2330 train_time:7357ms step_avg:57.47ms
step:129/2330 train_time:7413ms step_avg:57.46ms
step:130/2330 train_time:7472ms step_avg:57.48ms
step:131/2330 train_time:7528ms step_avg:57.46ms
step:132/2330 train_time:7586ms step_avg:57.47ms
step:133/2330 train_time:7642ms step_avg:57.46ms
step:134/2330 train_time:7702ms step_avg:57.48ms
step:135/2330 train_time:7758ms step_avg:57.47ms
step:136/2330 train_time:7817ms step_avg:57.48ms
step:137/2330 train_time:7873ms step_avg:57.47ms
step:138/2330 train_time:7932ms step_avg:57.48ms
step:139/2330 train_time:7988ms step_avg:57.47ms
step:140/2330 train_time:8046ms step_avg:57.47ms
step:141/2330 train_time:8102ms step_avg:57.46ms
step:142/2330 train_time:8161ms step_avg:57.48ms
step:143/2330 train_time:8217ms step_avg:57.46ms
step:144/2330 train_time:8276ms step_avg:57.47ms
step:145/2330 train_time:8332ms step_avg:57.46ms
step:146/2330 train_time:8391ms step_avg:57.47ms
step:147/2330 train_time:8447ms step_avg:57.47ms
step:148/2330 train_time:8506ms step_avg:57.47ms
step:149/2330 train_time:8562ms step_avg:57.46ms
step:150/2330 train_time:8621ms step_avg:57.47ms
step:151/2330 train_time:8677ms step_avg:57.46ms
step:152/2330 train_time:8735ms step_avg:57.47ms
step:153/2330 train_time:8791ms step_avg:57.46ms
step:154/2330 train_time:8849ms step_avg:57.46ms
step:155/2330 train_time:8905ms step_avg:57.45ms
step:156/2330 train_time:8964ms step_avg:57.46ms
step:157/2330 train_time:9019ms step_avg:57.45ms
step:158/2330 train_time:9078ms step_avg:57.46ms
step:159/2330 train_time:9135ms step_avg:57.45ms
step:160/2330 train_time:9193ms step_avg:57.46ms
step:161/2330 train_time:9249ms step_avg:57.45ms
step:162/2330 train_time:9309ms step_avg:57.46ms
step:163/2330 train_time:9364ms step_avg:57.45ms
step:164/2330 train_time:9423ms step_avg:57.46ms
step:165/2330 train_time:9479ms step_avg:57.45ms
step:166/2330 train_time:9538ms step_avg:57.46ms
step:167/2330 train_time:9594ms step_avg:57.45ms
step:168/2330 train_time:9654ms step_avg:57.46ms
step:169/2330 train_time:9710ms step_avg:57.46ms
step:170/2330 train_time:9768ms step_avg:57.46ms
step:171/2330 train_time:9824ms step_avg:57.45ms
step:172/2330 train_time:9883ms step_avg:57.46ms
step:173/2330 train_time:9939ms step_avg:57.45ms
step:174/2330 train_time:9998ms step_avg:57.46ms
step:175/2330 train_time:10054ms step_avg:57.45ms
step:176/2330 train_time:10113ms step_avg:57.46ms
step:177/2330 train_time:10169ms step_avg:57.45ms
step:178/2330 train_time:10227ms step_avg:57.46ms
step:179/2330 train_time:10284ms step_avg:57.45ms
step:180/2330 train_time:10343ms step_avg:57.46ms
step:181/2330 train_time:10399ms step_avg:57.45ms
step:182/2330 train_time:10457ms step_avg:57.46ms
step:183/2330 train_time:10513ms step_avg:57.45ms
step:184/2330 train_time:10572ms step_avg:57.46ms
step:185/2330 train_time:10629ms step_avg:57.46ms
step:186/2330 train_time:10688ms step_avg:57.46ms
step:187/2330 train_time:10744ms step_avg:57.46ms
step:188/2330 train_time:10804ms step_avg:57.47ms
step:189/2330 train_time:10859ms step_avg:57.46ms
step:190/2330 train_time:10918ms step_avg:57.46ms
step:191/2330 train_time:10973ms step_avg:57.45ms
step:192/2330 train_time:11033ms step_avg:57.46ms
step:193/2330 train_time:11089ms step_avg:57.45ms
step:194/2330 train_time:11148ms step_avg:57.46ms
step:195/2330 train_time:11204ms step_avg:57.46ms
step:196/2330 train_time:11263ms step_avg:57.47ms
step:197/2330 train_time:11319ms step_avg:57.46ms
step:198/2330 train_time:11378ms step_avg:57.47ms
step:199/2330 train_time:11434ms step_avg:57.46ms
step:200/2330 train_time:11493ms step_avg:57.47ms
step:201/2330 train_time:11550ms step_avg:57.46ms
step:202/2330 train_time:11608ms step_avg:57.47ms
step:203/2330 train_time:11665ms step_avg:57.46ms
step:204/2330 train_time:11724ms step_avg:57.47ms
step:205/2330 train_time:11780ms step_avg:57.46ms
step:206/2330 train_time:11838ms step_avg:57.47ms
step:207/2330 train_time:11894ms step_avg:57.46ms
step:208/2330 train_time:11952ms step_avg:57.46ms
step:209/2330 train_time:12008ms step_avg:57.46ms
step:210/2330 train_time:12066ms step_avg:57.46ms
step:211/2330 train_time:12122ms step_avg:57.45ms
step:212/2330 train_time:12182ms step_avg:57.46ms
step:213/2330 train_time:12238ms step_avg:57.45ms
step:214/2330 train_time:12297ms step_avg:57.46ms
step:215/2330 train_time:12352ms step_avg:57.45ms
step:216/2330 train_time:12411ms step_avg:57.46ms
step:217/2330 train_time:12467ms step_avg:57.45ms
step:218/2330 train_time:12526ms step_avg:57.46ms
step:219/2330 train_time:12582ms step_avg:57.45ms
step:220/2330 train_time:12640ms step_avg:57.46ms
step:221/2330 train_time:12696ms step_avg:57.45ms
step:222/2330 train_time:12756ms step_avg:57.46ms
step:223/2330 train_time:12812ms step_avg:57.45ms
step:224/2330 train_time:12871ms step_avg:57.46ms
step:225/2330 train_time:12927ms step_avg:57.45ms
step:226/2330 train_time:12985ms step_avg:57.46ms
step:227/2330 train_time:13041ms step_avg:57.45ms
step:228/2330 train_time:13101ms step_avg:57.46ms
step:229/2330 train_time:13157ms step_avg:57.45ms
step:230/2330 train_time:13215ms step_avg:57.46ms
step:231/2330 train_time:13271ms step_avg:57.45ms
step:232/2330 train_time:13330ms step_avg:57.46ms
step:233/2330 train_time:13386ms step_avg:57.45ms
step:234/2330 train_time:13445ms step_avg:57.46ms
step:235/2330 train_time:13501ms step_avg:57.45ms
step:236/2330 train_time:13559ms step_avg:57.45ms
step:237/2330 train_time:13615ms step_avg:57.45ms
step:238/2330 train_time:13674ms step_avg:57.45ms
step:239/2330 train_time:13730ms step_avg:57.45ms
step:240/2330 train_time:13790ms step_avg:57.46ms
step:241/2330 train_time:13846ms step_avg:57.45ms
step:242/2330 train_time:13905ms step_avg:57.46ms
step:243/2330 train_time:13961ms step_avg:57.45ms
step:244/2330 train_time:14019ms step_avg:57.46ms
step:245/2330 train_time:14076ms step_avg:57.45ms
step:246/2330 train_time:14134ms step_avg:57.46ms
step:247/2330 train_time:14191ms step_avg:57.45ms
step:248/2330 train_time:14249ms step_avg:57.46ms
step:249/2330 train_time:14306ms step_avg:57.45ms
step:250/2330 train_time:14364ms step_avg:57.46ms
step:250/2330 val_loss:4.9079 train_time:14443ms step_avg:57.77ms
step:251/2330 train_time:14463ms step_avg:57.62ms
step:252/2330 train_time:14484ms step_avg:57.47ms
step:253/2330 train_time:14536ms step_avg:57.46ms
step:254/2330 train_time:14600ms step_avg:57.48ms
step:255/2330 train_time:14655ms step_avg:57.47ms
step:256/2330 train_time:14719ms step_avg:57.50ms
step:257/2330 train_time:14774ms step_avg:57.49ms
step:258/2330 train_time:14834ms step_avg:57.50ms
step:259/2330 train_time:14889ms step_avg:57.49ms
step:260/2330 train_time:14948ms step_avg:57.49ms
step:261/2330 train_time:15004ms step_avg:57.49ms
step:262/2330 train_time:15062ms step_avg:57.49ms
step:263/2330 train_time:15117ms step_avg:57.48ms
step:264/2330 train_time:15176ms step_avg:57.49ms
step:265/2330 train_time:15232ms step_avg:57.48ms
step:266/2330 train_time:15290ms step_avg:57.48ms
step:267/2330 train_time:15347ms step_avg:57.48ms
step:268/2330 train_time:15407ms step_avg:57.49ms
step:269/2330 train_time:15464ms step_avg:57.49ms
step:270/2330 train_time:15523ms step_avg:57.49ms
step:271/2330 train_time:15580ms step_avg:57.49ms
step:272/2330 train_time:15640ms step_avg:57.50ms
step:273/2330 train_time:15696ms step_avg:57.50ms
step:274/2330 train_time:15756ms step_avg:57.50ms
step:275/2330 train_time:15812ms step_avg:57.50ms
step:276/2330 train_time:15871ms step_avg:57.50ms
step:277/2330 train_time:15927ms step_avg:57.50ms
step:278/2330 train_time:15985ms step_avg:57.50ms
step:279/2330 train_time:16041ms step_avg:57.49ms
step:280/2330 train_time:16099ms step_avg:57.50ms
step:281/2330 train_time:16155ms step_avg:57.49ms
step:282/2330 train_time:16213ms step_avg:57.49ms
step:283/2330 train_time:16269ms step_avg:57.49ms
step:284/2330 train_time:16327ms step_avg:57.49ms
step:285/2330 train_time:16384ms step_avg:57.49ms
step:286/2330 train_time:16442ms step_avg:57.49ms
step:287/2330 train_time:16499ms step_avg:57.49ms
step:288/2330 train_time:16559ms step_avg:57.50ms
step:289/2330 train_time:16615ms step_avg:57.49ms
step:290/2330 train_time:16676ms step_avg:57.50ms
step:291/2330 train_time:16732ms step_avg:57.50ms
step:292/2330 train_time:16792ms step_avg:57.51ms
step:293/2330 train_time:16848ms step_avg:57.50ms
step:294/2330 train_time:16907ms step_avg:57.51ms
step:295/2330 train_time:16963ms step_avg:57.50ms
step:296/2330 train_time:17021ms step_avg:57.50ms
step:297/2330 train_time:17077ms step_avg:57.50ms
step:298/2330 train_time:17135ms step_avg:57.50ms
step:299/2330 train_time:17192ms step_avg:57.50ms
step:300/2330 train_time:17250ms step_avg:57.50ms
step:301/2330 train_time:17307ms step_avg:57.50ms
step:302/2330 train_time:17366ms step_avg:57.50ms
step:303/2330 train_time:17422ms step_avg:57.50ms
step:304/2330 train_time:17480ms step_avg:57.50ms
step:305/2330 train_time:17536ms step_avg:57.50ms
step:306/2330 train_time:17596ms step_avg:57.50ms
step:307/2330 train_time:17652ms step_avg:57.50ms
step:308/2330 train_time:17713ms step_avg:57.51ms
step:309/2330 train_time:17770ms step_avg:57.51ms
step:310/2330 train_time:17829ms step_avg:57.51ms
step:311/2330 train_time:17885ms step_avg:57.51ms
step:312/2330 train_time:17944ms step_avg:57.51ms
step:313/2330 train_time:18000ms step_avg:57.51ms
step:314/2330 train_time:18058ms step_avg:57.51ms
step:315/2330 train_time:18114ms step_avg:57.50ms
step:316/2330 train_time:18172ms step_avg:57.51ms
step:317/2330 train_time:18228ms step_avg:57.50ms
step:318/2330 train_time:18287ms step_avg:57.51ms
step:319/2330 train_time:18343ms step_avg:57.50ms
step:320/2330 train_time:18402ms step_avg:57.51ms
step:321/2330 train_time:18459ms step_avg:57.50ms
step:322/2330 train_time:18517ms step_avg:57.51ms
step:323/2330 train_time:18573ms step_avg:57.50ms
step:324/2330 train_time:18633ms step_avg:57.51ms
step:325/2330 train_time:18689ms step_avg:57.51ms
step:326/2330 train_time:18749ms step_avg:57.51ms
step:327/2330 train_time:18805ms step_avg:57.51ms
step:328/2330 train_time:18864ms step_avg:57.51ms
step:329/2330 train_time:18921ms step_avg:57.51ms
step:330/2330 train_time:18979ms step_avg:57.51ms
step:331/2330 train_time:19034ms step_avg:57.50ms
step:332/2330 train_time:19094ms step_avg:57.51ms
step:333/2330 train_time:19150ms step_avg:57.51ms
step:334/2330 train_time:19209ms step_avg:57.51ms
step:335/2330 train_time:19265ms step_avg:57.51ms
step:336/2330 train_time:19323ms step_avg:57.51ms
step:337/2330 train_time:19380ms step_avg:57.51ms
step:338/2330 train_time:19438ms step_avg:57.51ms
step:339/2330 train_time:19493ms step_avg:57.50ms
step:340/2330 train_time:19553ms step_avg:57.51ms
step:341/2330 train_time:19610ms step_avg:57.51ms
step:342/2330 train_time:19669ms step_avg:57.51ms
step:343/2330 train_time:19726ms step_avg:57.51ms
step:344/2330 train_time:19785ms step_avg:57.51ms
step:345/2330 train_time:19841ms step_avg:57.51ms
step:346/2330 train_time:19900ms step_avg:57.51ms
step:347/2330 train_time:19955ms step_avg:57.51ms
step:348/2330 train_time:20015ms step_avg:57.51ms
step:349/2330 train_time:20070ms step_avg:57.51ms
step:350/2330 train_time:20130ms step_avg:57.52ms
step:351/2330 train_time:20186ms step_avg:57.51ms
step:352/2330 train_time:20246ms step_avg:57.52ms
step:353/2330 train_time:20303ms step_avg:57.51ms
step:354/2330 train_time:20361ms step_avg:57.52ms
step:355/2330 train_time:20417ms step_avg:57.51ms
step:356/2330 train_time:20475ms step_avg:57.51ms
step:357/2330 train_time:20531ms step_avg:57.51ms
step:358/2330 train_time:20590ms step_avg:57.51ms
step:359/2330 train_time:20646ms step_avg:57.51ms
step:360/2330 train_time:20706ms step_avg:57.52ms
step:361/2330 train_time:20762ms step_avg:57.51ms
step:362/2330 train_time:20821ms step_avg:57.52ms
step:363/2330 train_time:20877ms step_avg:57.51ms
step:364/2330 train_time:20936ms step_avg:57.52ms
step:365/2330 train_time:20992ms step_avg:57.51ms
step:366/2330 train_time:21051ms step_avg:57.52ms
step:367/2330 train_time:21108ms step_avg:57.51ms
step:368/2330 train_time:21168ms step_avg:57.52ms
step:369/2330 train_time:21223ms step_avg:57.52ms
step:370/2330 train_time:21283ms step_avg:57.52ms
step:371/2330 train_time:21338ms step_avg:57.52ms
step:372/2330 train_time:21397ms step_avg:57.52ms
step:373/2330 train_time:21453ms step_avg:57.52ms
step:374/2330 train_time:21512ms step_avg:57.52ms
step:375/2330 train_time:21568ms step_avg:57.51ms
step:376/2330 train_time:21628ms step_avg:57.52ms
step:377/2330 train_time:21684ms step_avg:57.52ms
step:378/2330 train_time:21744ms step_avg:57.52ms
step:379/2330 train_time:21800ms step_avg:57.52ms
step:380/2330 train_time:21858ms step_avg:57.52ms
step:381/2330 train_time:21914ms step_avg:57.52ms
step:382/2330 train_time:21973ms step_avg:57.52ms
step:383/2330 train_time:22030ms step_avg:57.52ms
step:384/2330 train_time:22089ms step_avg:57.52ms
step:385/2330 train_time:22145ms step_avg:57.52ms
step:386/2330 train_time:22204ms step_avg:57.52ms
step:387/2330 train_time:22260ms step_avg:57.52ms
step:388/2330 train_time:22319ms step_avg:57.52ms
step:389/2330 train_time:22374ms step_avg:57.52ms
step:390/2330 train_time:22434ms step_avg:57.52ms
step:391/2330 train_time:22491ms step_avg:57.52ms
step:392/2330 train_time:22550ms step_avg:57.52ms
step:393/2330 train_time:22606ms step_avg:57.52ms
step:394/2330 train_time:22665ms step_avg:57.53ms
step:395/2330 train_time:22721ms step_avg:57.52ms
step:396/2330 train_time:22780ms step_avg:57.53ms
step:397/2330 train_time:22836ms step_avg:57.52ms
step:398/2330 train_time:22895ms step_avg:57.52ms
step:399/2330 train_time:22951ms step_avg:57.52ms
step:400/2330 train_time:23009ms step_avg:57.52ms
step:401/2330 train_time:23066ms step_avg:57.52ms
step:402/2330 train_time:23125ms step_avg:57.52ms
step:403/2330 train_time:23181ms step_avg:57.52ms
step:404/2330 train_time:23240ms step_avg:57.52ms
step:405/2330 train_time:23296ms step_avg:57.52ms
step:406/2330 train_time:23355ms step_avg:57.53ms
step:407/2330 train_time:23411ms step_avg:57.52ms
step:408/2330 train_time:23470ms step_avg:57.53ms
step:409/2330 train_time:23526ms step_avg:57.52ms
step:410/2330 train_time:23586ms step_avg:57.53ms
step:411/2330 train_time:23642ms step_avg:57.52ms
step:412/2330 train_time:23701ms step_avg:57.53ms
step:413/2330 train_time:23757ms step_avg:57.52ms
step:414/2330 train_time:23816ms step_avg:57.53ms
step:415/2330 train_time:23872ms step_avg:57.52ms
step:416/2330 train_time:23931ms step_avg:57.53ms
step:417/2330 train_time:23987ms step_avg:57.52ms
step:418/2330 train_time:24047ms step_avg:57.53ms
step:419/2330 train_time:24103ms step_avg:57.52ms
step:420/2330 train_time:24161ms step_avg:57.53ms
step:421/2330 train_time:24218ms step_avg:57.52ms
step:422/2330 train_time:24276ms step_avg:57.53ms
step:423/2330 train_time:24333ms step_avg:57.52ms
step:424/2330 train_time:24392ms step_avg:57.53ms
step:425/2330 train_time:24448ms step_avg:57.53ms
step:426/2330 train_time:24507ms step_avg:57.53ms
step:427/2330 train_time:24564ms step_avg:57.53ms
step:428/2330 train_time:24623ms step_avg:57.53ms
step:429/2330 train_time:24679ms step_avg:57.53ms
step:430/2330 train_time:24738ms step_avg:57.53ms
step:431/2330 train_time:24794ms step_avg:57.53ms
step:432/2330 train_time:24853ms step_avg:57.53ms
step:433/2330 train_time:24910ms step_avg:57.53ms
step:434/2330 train_time:24969ms step_avg:57.53ms
step:435/2330 train_time:25025ms step_avg:57.53ms
step:436/2330 train_time:25084ms step_avg:57.53ms
step:437/2330 train_time:25141ms step_avg:57.53ms
step:438/2330 train_time:25200ms step_avg:57.53ms
step:439/2330 train_time:25256ms step_avg:57.53ms
step:440/2330 train_time:25315ms step_avg:57.53ms
step:441/2330 train_time:25372ms step_avg:57.53ms
step:442/2330 train_time:25431ms step_avg:57.54ms
step:443/2330 train_time:25487ms step_avg:57.53ms
step:444/2330 train_time:25546ms step_avg:57.54ms
step:445/2330 train_time:25602ms step_avg:57.53ms
step:446/2330 train_time:25661ms step_avg:57.54ms
step:447/2330 train_time:25718ms step_avg:57.53ms
step:448/2330 train_time:25776ms step_avg:57.54ms
step:449/2330 train_time:25832ms step_avg:57.53ms
step:450/2330 train_time:25891ms step_avg:57.54ms
step:451/2330 train_time:25948ms step_avg:57.53ms
step:452/2330 train_time:26006ms step_avg:57.54ms
step:453/2330 train_time:26063ms step_avg:57.53ms
step:454/2330 train_time:26121ms step_avg:57.54ms
step:455/2330 train_time:26177ms step_avg:57.53ms
step:456/2330 train_time:26236ms step_avg:57.54ms
step:457/2330 train_time:26292ms step_avg:57.53ms
step:458/2330 train_time:26352ms step_avg:57.54ms
step:459/2330 train_time:26409ms step_avg:57.54ms
step:460/2330 train_time:26469ms step_avg:57.54ms
step:461/2330 train_time:26525ms step_avg:57.54ms
step:462/2330 train_time:26584ms step_avg:57.54ms
step:463/2330 train_time:26640ms step_avg:57.54ms
step:464/2330 train_time:26698ms step_avg:57.54ms
step:465/2330 train_time:26755ms step_avg:57.54ms
step:466/2330 train_time:26813ms step_avg:57.54ms
step:467/2330 train_time:26869ms step_avg:57.54ms
step:468/2330 train_time:26929ms step_avg:57.54ms
step:469/2330 train_time:26985ms step_avg:57.54ms
step:470/2330 train_time:27044ms step_avg:57.54ms
step:471/2330 train_time:27101ms step_avg:57.54ms
step:472/2330 train_time:27159ms step_avg:57.54ms
step:473/2330 train_time:27215ms step_avg:57.54ms
step:474/2330 train_time:27274ms step_avg:57.54ms
step:475/2330 train_time:27331ms step_avg:57.54ms
step:476/2330 train_time:27390ms step_avg:57.54ms
step:477/2330 train_time:27447ms step_avg:57.54ms
step:478/2330 train_time:27506ms step_avg:57.54ms
step:479/2330 train_time:27562ms step_avg:57.54ms
step:480/2330 train_time:27621ms step_avg:57.54ms
step:481/2330 train_time:27677ms step_avg:57.54ms
step:482/2330 train_time:27737ms step_avg:57.55ms
step:483/2330 train_time:27793ms step_avg:57.54ms
step:484/2330 train_time:27852ms step_avg:57.55ms
step:485/2330 train_time:27909ms step_avg:57.54ms
step:486/2330 train_time:27968ms step_avg:57.55ms
step:487/2330 train_time:28025ms step_avg:57.55ms
step:488/2330 train_time:28084ms step_avg:57.55ms
step:489/2330 train_time:28140ms step_avg:57.55ms
step:490/2330 train_time:28199ms step_avg:57.55ms
step:491/2330 train_time:28255ms step_avg:57.55ms
step:492/2330 train_time:28314ms step_avg:57.55ms
step:493/2330 train_time:28371ms step_avg:57.55ms
step:494/2330 train_time:28430ms step_avg:57.55ms
step:495/2330 train_time:28487ms step_avg:57.55ms
step:496/2330 train_time:28546ms step_avg:57.55ms
step:497/2330 train_time:28602ms step_avg:57.55ms
step:498/2330 train_time:28661ms step_avg:57.55ms
step:499/2330 train_time:28716ms step_avg:57.55ms
step:500/2330 train_time:28776ms step_avg:57.55ms
step:500/2330 val_loss:4.4214 train_time:28855ms step_avg:57.71ms
step:501/2330 train_time:28874ms step_avg:57.63ms
step:502/2330 train_time:28895ms step_avg:57.56ms
step:503/2330 train_time:28948ms step_avg:57.55ms
step:504/2330 train_time:29013ms step_avg:57.57ms
step:505/2330 train_time:29071ms step_avg:57.57ms
step:506/2330 train_time:29131ms step_avg:57.57ms
step:507/2330 train_time:29187ms step_avg:57.57ms
step:508/2330 train_time:29246ms step_avg:57.57ms
step:509/2330 train_time:29301ms step_avg:57.57ms
step:510/2330 train_time:29360ms step_avg:57.57ms
step:511/2330 train_time:29416ms step_avg:57.56ms
step:512/2330 train_time:29475ms step_avg:57.57ms
step:513/2330 train_time:29530ms step_avg:57.56ms
step:514/2330 train_time:29588ms step_avg:57.57ms
step:515/2330 train_time:29644ms step_avg:57.56ms
step:516/2330 train_time:29702ms step_avg:57.56ms
step:517/2330 train_time:29758ms step_avg:57.56ms
step:518/2330 train_time:29817ms step_avg:57.56ms
step:519/2330 train_time:29873ms step_avg:57.56ms
step:520/2330 train_time:29934ms step_avg:57.57ms
step:521/2330 train_time:29992ms step_avg:57.57ms
step:522/2330 train_time:30052ms step_avg:57.57ms
step:523/2330 train_time:30110ms step_avg:57.57ms
step:524/2330 train_time:30169ms step_avg:57.57ms
step:525/2330 train_time:30225ms step_avg:57.57ms
step:526/2330 train_time:30283ms step_avg:57.57ms
step:527/2330 train_time:30338ms step_avg:57.57ms
step:528/2330 train_time:30399ms step_avg:57.57ms
step:529/2330 train_time:30456ms step_avg:57.57ms
step:530/2330 train_time:30514ms step_avg:57.57ms
step:531/2330 train_time:30570ms step_avg:57.57ms
step:532/2330 train_time:30628ms step_avg:57.57ms
step:533/2330 train_time:30684ms step_avg:57.57ms
step:534/2330 train_time:30743ms step_avg:57.57ms
step:535/2330 train_time:30799ms step_avg:57.57ms
step:536/2330 train_time:30858ms step_avg:57.57ms
step:537/2330 train_time:30914ms step_avg:57.57ms
step:538/2330 train_time:30975ms step_avg:57.57ms
step:539/2330 train_time:31032ms step_avg:57.57ms
step:540/2330 train_time:31091ms step_avg:57.58ms
step:541/2330 train_time:31147ms step_avg:57.57ms
step:542/2330 train_time:31206ms step_avg:57.58ms
step:543/2330 train_time:31262ms step_avg:57.57ms
step:544/2330 train_time:31321ms step_avg:57.58ms
step:545/2330 train_time:31377ms step_avg:57.57ms
step:546/2330 train_time:31437ms step_avg:57.58ms
step:547/2330 train_time:31494ms step_avg:57.58ms
step:548/2330 train_time:31552ms step_avg:57.58ms
step:549/2330 train_time:31608ms step_avg:57.57ms
step:550/2330 train_time:31666ms step_avg:57.58ms
step:551/2330 train_time:31722ms step_avg:57.57ms
step:552/2330 train_time:31781ms step_avg:57.57ms
step:553/2330 train_time:31837ms step_avg:57.57ms
step:554/2330 train_time:31897ms step_avg:57.58ms
step:555/2330 train_time:31953ms step_avg:57.57ms
step:556/2330 train_time:32013ms step_avg:57.58ms
step:557/2330 train_time:32070ms step_avg:57.58ms
step:558/2330 train_time:32129ms step_avg:57.58ms
step:559/2330 train_time:32185ms step_avg:57.58ms
step:560/2330 train_time:32244ms step_avg:57.58ms
step:561/2330 train_time:32300ms step_avg:57.58ms
step:562/2330 train_time:32359ms step_avg:57.58ms
step:563/2330 train_time:32415ms step_avg:57.58ms
step:564/2330 train_time:32475ms step_avg:57.58ms
step:565/2330 train_time:32531ms step_avg:57.58ms
step:566/2330 train_time:32589ms step_avg:57.58ms
step:567/2330 train_time:32646ms step_avg:57.58ms
step:568/2330 train_time:32704ms step_avg:57.58ms
step:569/2330 train_time:32760ms step_avg:57.58ms
step:570/2330 train_time:32819ms step_avg:57.58ms
step:571/2330 train_time:32875ms step_avg:57.57ms
step:572/2330 train_time:32935ms step_avg:57.58ms
step:573/2330 train_time:32992ms step_avg:57.58ms
step:574/2330 train_time:33051ms step_avg:57.58ms
step:575/2330 train_time:33108ms step_avg:57.58ms
step:576/2330 train_time:33167ms step_avg:57.58ms
step:577/2330 train_time:33223ms step_avg:57.58ms
step:578/2330 train_time:33283ms step_avg:57.58ms
step:579/2330 train_time:33339ms step_avg:57.58ms
step:580/2330 train_time:33397ms step_avg:57.58ms
step:581/2330 train_time:33454ms step_avg:57.58ms
step:582/2330 train_time:33512ms step_avg:57.58ms
step:583/2330 train_time:33568ms step_avg:57.58ms
step:584/2330 train_time:33627ms step_avg:57.58ms
step:585/2330 train_time:33684ms step_avg:57.58ms
step:586/2330 train_time:33742ms step_avg:57.58ms
step:587/2330 train_time:33799ms step_avg:57.58ms
step:588/2330 train_time:33859ms step_avg:57.58ms
step:589/2330 train_time:33915ms step_avg:57.58ms
step:590/2330 train_time:33974ms step_avg:57.58ms
step:591/2330 train_time:34031ms step_avg:57.58ms
step:592/2330 train_time:34089ms step_avg:57.58ms
step:593/2330 train_time:34145ms step_avg:57.58ms
step:594/2330 train_time:34205ms step_avg:57.58ms
step:595/2330 train_time:34261ms step_avg:57.58ms
step:596/2330 train_time:34320ms step_avg:57.58ms
step:597/2330 train_time:34377ms step_avg:57.58ms
step:598/2330 train_time:34436ms step_avg:57.59ms
step:599/2330 train_time:34492ms step_avg:57.58ms
step:600/2330 train_time:34551ms step_avg:57.59ms
step:601/2330 train_time:34607ms step_avg:57.58ms
step:602/2330 train_time:34666ms step_avg:57.58ms
step:603/2330 train_time:34721ms step_avg:57.58ms
step:604/2330 train_time:34781ms step_avg:57.59ms
step:605/2330 train_time:34837ms step_avg:57.58ms
step:606/2330 train_time:34897ms step_avg:57.59ms
step:607/2330 train_time:34954ms step_avg:57.58ms
step:608/2330 train_time:35014ms step_avg:57.59ms
step:609/2330 train_time:35071ms step_avg:57.59ms
step:610/2330 train_time:35130ms step_avg:57.59ms
step:611/2330 train_time:35187ms step_avg:57.59ms
step:612/2330 train_time:35245ms step_avg:57.59ms
step:613/2330 train_time:35301ms step_avg:57.59ms
step:614/2330 train_time:35360ms step_avg:57.59ms
step:615/2330 train_time:35417ms step_avg:57.59ms
step:616/2330 train_time:35476ms step_avg:57.59ms
step:617/2330 train_time:35532ms step_avg:57.59ms
step:618/2330 train_time:35591ms step_avg:57.59ms
step:619/2330 train_time:35647ms step_avg:57.59ms
step:620/2330 train_time:35706ms step_avg:57.59ms
step:621/2330 train_time:35762ms step_avg:57.59ms
step:622/2330 train_time:35822ms step_avg:57.59ms
step:623/2330 train_time:35878ms step_avg:57.59ms
step:624/2330 train_time:35938ms step_avg:57.59ms
step:625/2330 train_time:35994ms step_avg:57.59ms
step:626/2330 train_time:36054ms step_avg:57.59ms
step:627/2330 train_time:36111ms step_avg:57.59ms
step:628/2330 train_time:36170ms step_avg:57.60ms
step:629/2330 train_time:36226ms step_avg:57.59ms
step:630/2330 train_time:36285ms step_avg:57.59ms
step:631/2330 train_time:36340ms step_avg:57.59ms
step:632/2330 train_time:36399ms step_avg:57.59ms
step:633/2330 train_time:36456ms step_avg:57.59ms
step:634/2330 train_time:36516ms step_avg:57.60ms
step:635/2330 train_time:36572ms step_avg:57.59ms
step:636/2330 train_time:36630ms step_avg:57.60ms
step:637/2330 train_time:36686ms step_avg:57.59ms
step:638/2330 train_time:36745ms step_avg:57.59ms
step:639/2330 train_time:36801ms step_avg:57.59ms
step:640/2330 train_time:36861ms step_avg:57.59ms
step:641/2330 train_time:36917ms step_avg:57.59ms
step:642/2330 train_time:36977ms step_avg:57.60ms
step:643/2330 train_time:37034ms step_avg:57.60ms
step:644/2330 train_time:37093ms step_avg:57.60ms
step:645/2330 train_time:37150ms step_avg:57.60ms
step:646/2330 train_time:37209ms step_avg:57.60ms
step:647/2330 train_time:37265ms step_avg:57.60ms
step:648/2330 train_time:37323ms step_avg:57.60ms
step:649/2330 train_time:37379ms step_avg:57.60ms
step:650/2330 train_time:37439ms step_avg:57.60ms
step:651/2330 train_time:37495ms step_avg:57.60ms
step:652/2330 train_time:37554ms step_avg:57.60ms
step:653/2330 train_time:37611ms step_avg:57.60ms
step:654/2330 train_time:37670ms step_avg:57.60ms
step:655/2330 train_time:37726ms step_avg:57.60ms
step:656/2330 train_time:37785ms step_avg:57.60ms
step:657/2330 train_time:37841ms step_avg:57.60ms
step:658/2330 train_time:37900ms step_avg:57.60ms
step:659/2330 train_time:37956ms step_avg:57.60ms
step:660/2330 train_time:38016ms step_avg:57.60ms
step:661/2330 train_time:38073ms step_avg:57.60ms
step:662/2330 train_time:38132ms step_avg:57.60ms
step:663/2330 train_time:38189ms step_avg:57.60ms
step:664/2330 train_time:38248ms step_avg:57.60ms
step:665/2330 train_time:38304ms step_avg:57.60ms
step:666/2330 train_time:38363ms step_avg:57.60ms
step:667/2330 train_time:38419ms step_avg:57.60ms
step:668/2330 train_time:38478ms step_avg:57.60ms
step:669/2330 train_time:38534ms step_avg:57.60ms
step:670/2330 train_time:38594ms step_avg:57.60ms
step:671/2330 train_time:38650ms step_avg:57.60ms
step:672/2330 train_time:38710ms step_avg:57.60ms
step:673/2330 train_time:38766ms step_avg:57.60ms
step:674/2330 train_time:38824ms step_avg:57.60ms
step:675/2330 train_time:38880ms step_avg:57.60ms
step:676/2330 train_time:38940ms step_avg:57.60ms
step:677/2330 train_time:38996ms step_avg:57.60ms
step:678/2330 train_time:39055ms step_avg:57.60ms
step:679/2330 train_time:39112ms step_avg:57.60ms
step:680/2330 train_time:39171ms step_avg:57.61ms
step:681/2330 train_time:39227ms step_avg:57.60ms
step:682/2330 train_time:39287ms step_avg:57.60ms
step:683/2330 train_time:39343ms step_avg:57.60ms
step:684/2330 train_time:39402ms step_avg:57.61ms
step:685/2330 train_time:39458ms step_avg:57.60ms
step:686/2330 train_time:39517ms step_avg:57.61ms
step:687/2330 train_time:39574ms step_avg:57.60ms
step:688/2330 train_time:39633ms step_avg:57.61ms
step:689/2330 train_time:39689ms step_avg:57.60ms
step:690/2330 train_time:39748ms step_avg:57.61ms
step:691/2330 train_time:39804ms step_avg:57.60ms
step:692/2330 train_time:39863ms step_avg:57.60ms
step:693/2330 train_time:39919ms step_avg:57.60ms
step:694/2330 train_time:39979ms step_avg:57.61ms
step:695/2330 train_time:40036ms step_avg:57.61ms
step:696/2330 train_time:40095ms step_avg:57.61ms
step:697/2330 train_time:40151ms step_avg:57.61ms
step:698/2330 train_time:40210ms step_avg:57.61ms
step:699/2330 train_time:40266ms step_avg:57.61ms
step:700/2330 train_time:40325ms step_avg:57.61ms
step:701/2330 train_time:40381ms step_avg:57.60ms
step:702/2330 train_time:40440ms step_avg:57.61ms
step:703/2330 train_time:40496ms step_avg:57.60ms
step:704/2330 train_time:40556ms step_avg:57.61ms
step:705/2330 train_time:40612ms step_avg:57.61ms
step:706/2330 train_time:40672ms step_avg:57.61ms
step:707/2330 train_time:40728ms step_avg:57.61ms
step:708/2330 train_time:40787ms step_avg:57.61ms
step:709/2330 train_time:40843ms step_avg:57.61ms
step:710/2330 train_time:40902ms step_avg:57.61ms
step:711/2330 train_time:40959ms step_avg:57.61ms
step:712/2330 train_time:41018ms step_avg:57.61ms
step:713/2330 train_time:41075ms step_avg:57.61ms
step:714/2330 train_time:41134ms step_avg:57.61ms
step:715/2330 train_time:41191ms step_avg:57.61ms
step:716/2330 train_time:41249ms step_avg:57.61ms
step:717/2330 train_time:41306ms step_avg:57.61ms
step:718/2330 train_time:41364ms step_avg:57.61ms
step:719/2330 train_time:41420ms step_avg:57.61ms
step:720/2330 train_time:41479ms step_avg:57.61ms
step:721/2330 train_time:41536ms step_avg:57.61ms
step:722/2330 train_time:41595ms step_avg:57.61ms
step:723/2330 train_time:41651ms step_avg:57.61ms
step:724/2330 train_time:41710ms step_avg:57.61ms
step:725/2330 train_time:41766ms step_avg:57.61ms
step:726/2330 train_time:41825ms step_avg:57.61ms
step:727/2330 train_time:41881ms step_avg:57.61ms
step:728/2330 train_time:41940ms step_avg:57.61ms
step:729/2330 train_time:41997ms step_avg:57.61ms
step:730/2330 train_time:42058ms step_avg:57.61ms
step:731/2330 train_time:42115ms step_avg:57.61ms
step:732/2330 train_time:42174ms step_avg:57.61ms
step:733/2330 train_time:42231ms step_avg:57.61ms
step:734/2330 train_time:42289ms step_avg:57.61ms
step:735/2330 train_time:42346ms step_avg:57.61ms
step:736/2330 train_time:42404ms step_avg:57.61ms
step:737/2330 train_time:42460ms step_avg:57.61ms
step:738/2330 train_time:42520ms step_avg:57.61ms
step:739/2330 train_time:42576ms step_avg:57.61ms
step:740/2330 train_time:42635ms step_avg:57.62ms
step:741/2330 train_time:42692ms step_avg:57.61ms
step:742/2330 train_time:42751ms step_avg:57.62ms
step:743/2330 train_time:42807ms step_avg:57.61ms
step:744/2330 train_time:42865ms step_avg:57.61ms
step:745/2330 train_time:42921ms step_avg:57.61ms
step:746/2330 train_time:42982ms step_avg:57.62ms
step:747/2330 train_time:43039ms step_avg:57.62ms
step:748/2330 train_time:43098ms step_avg:57.62ms
step:749/2330 train_time:43155ms step_avg:57.62ms
step:750/2330 train_time:43213ms step_avg:57.62ms
step:750/2330 val_loss:4.2283 train_time:43292ms step_avg:57.72ms
step:751/2330 train_time:43311ms step_avg:57.67ms
step:752/2330 train_time:43331ms step_avg:57.62ms
step:753/2330 train_time:43387ms step_avg:57.62ms
step:754/2330 train_time:43450ms step_avg:57.63ms
step:755/2330 train_time:43507ms step_avg:57.63ms
step:756/2330 train_time:43571ms step_avg:57.63ms
step:757/2330 train_time:43627ms step_avg:57.63ms
step:758/2330 train_time:43687ms step_avg:57.64ms
step:759/2330 train_time:43743ms step_avg:57.63ms
step:760/2330 train_time:43802ms step_avg:57.63ms
step:761/2330 train_time:43857ms step_avg:57.63ms
step:762/2330 train_time:43915ms step_avg:57.63ms
step:763/2330 train_time:43971ms step_avg:57.63ms
step:764/2330 train_time:44029ms step_avg:57.63ms
step:765/2330 train_time:44086ms step_avg:57.63ms
step:766/2330 train_time:44143ms step_avg:57.63ms
step:767/2330 train_time:44200ms step_avg:57.63ms
step:768/2330 train_time:44259ms step_avg:57.63ms
step:769/2330 train_time:44317ms step_avg:57.63ms
step:770/2330 train_time:44379ms step_avg:57.64ms
step:771/2330 train_time:44438ms step_avg:57.64ms
step:772/2330 train_time:44500ms step_avg:57.64ms
step:773/2330 train_time:44558ms step_avg:57.64ms
step:774/2330 train_time:44619ms step_avg:57.65ms
step:775/2330 train_time:44677ms step_avg:57.65ms
step:776/2330 train_time:44737ms step_avg:57.65ms
step:777/2330 train_time:44795ms step_avg:57.65ms
step:778/2330 train_time:44854ms step_avg:57.65ms
step:779/2330 train_time:44911ms step_avg:57.65ms
step:780/2330 train_time:44970ms step_avg:57.65ms
step:781/2330 train_time:45027ms step_avg:57.65ms
step:782/2330 train_time:45086ms step_avg:57.65ms
step:783/2330 train_time:45143ms step_avg:57.65ms
step:784/2330 train_time:45202ms step_avg:57.66ms
step:785/2330 train_time:45258ms step_avg:57.65ms
step:786/2330 train_time:45319ms step_avg:57.66ms
step:787/2330 train_time:45376ms step_avg:57.66ms
step:788/2330 train_time:45437ms step_avg:57.66ms
step:789/2330 train_time:45496ms step_avg:57.66ms
step:790/2330 train_time:45557ms step_avg:57.67ms
step:791/2330 train_time:45615ms step_avg:57.67ms
step:792/2330 train_time:45676ms step_avg:57.67ms
step:793/2330 train_time:45734ms step_avg:57.67ms
step:794/2330 train_time:45793ms step_avg:57.67ms
step:795/2330 train_time:45850ms step_avg:57.67ms
step:796/2330 train_time:45910ms step_avg:57.68ms
step:797/2330 train_time:45967ms step_avg:57.68ms
step:798/2330 train_time:46026ms step_avg:57.68ms
step:799/2330 train_time:46082ms step_avg:57.68ms
step:800/2330 train_time:46142ms step_avg:57.68ms
step:801/2330 train_time:46199ms step_avg:57.68ms
step:802/2330 train_time:46259ms step_avg:57.68ms
step:803/2330 train_time:46316ms step_avg:57.68ms
step:804/2330 train_time:46376ms step_avg:57.68ms
step:805/2330 train_time:46435ms step_avg:57.68ms
step:806/2330 train_time:46495ms step_avg:57.69ms
step:807/2330 train_time:46553ms step_avg:57.69ms
step:808/2330 train_time:46614ms step_avg:57.69ms
step:809/2330 train_time:46671ms step_avg:57.69ms
step:810/2330 train_time:46732ms step_avg:57.69ms
step:811/2330 train_time:46790ms step_avg:57.69ms
step:812/2330 train_time:46850ms step_avg:57.70ms
step:813/2330 train_time:46907ms step_avg:57.70ms
step:814/2330 train_time:46966ms step_avg:57.70ms
step:815/2330 train_time:47024ms step_avg:57.70ms
step:816/2330 train_time:47082ms step_avg:57.70ms
step:817/2330 train_time:47139ms step_avg:57.70ms
step:818/2330 train_time:47198ms step_avg:57.70ms
step:819/2330 train_time:47254ms step_avg:57.70ms
step:820/2330 train_time:47315ms step_avg:57.70ms
step:821/2330 train_time:47372ms step_avg:57.70ms
step:822/2330 train_time:47433ms step_avg:57.70ms
step:823/2330 train_time:47490ms step_avg:57.70ms
step:824/2330 train_time:47550ms step_avg:57.71ms
step:825/2330 train_time:47608ms step_avg:57.71ms
step:826/2330 train_time:47667ms step_avg:57.71ms
step:827/2330 train_time:47724ms step_avg:57.71ms
step:828/2330 train_time:47784ms step_avg:57.71ms
step:829/2330 train_time:47842ms step_avg:57.71ms
step:830/2330 train_time:47902ms step_avg:57.71ms
step:831/2330 train_time:47958ms step_avg:57.71ms
step:832/2330 train_time:48018ms step_avg:57.71ms
step:833/2330 train_time:48076ms step_avg:57.71ms
step:834/2330 train_time:48135ms step_avg:57.72ms
step:835/2330 train_time:48193ms step_avg:57.72ms
step:836/2330 train_time:48252ms step_avg:57.72ms
step:837/2330 train_time:48309ms step_avg:57.72ms
step:838/2330 train_time:48369ms step_avg:57.72ms
step:839/2330 train_time:48426ms step_avg:57.72ms
step:840/2330 train_time:48487ms step_avg:57.72ms
step:841/2330 train_time:48545ms step_avg:57.72ms
step:842/2330 train_time:48604ms step_avg:57.72ms
step:843/2330 train_time:48661ms step_avg:57.72ms
step:844/2330 train_time:48723ms step_avg:57.73ms
step:845/2330 train_time:48779ms step_avg:57.73ms
step:846/2330 train_time:48841ms step_avg:57.73ms
step:847/2330 train_time:48898ms step_avg:57.73ms
step:848/2330 train_time:48958ms step_avg:57.73ms
step:849/2330 train_time:49016ms step_avg:57.73ms
step:850/2330 train_time:49075ms step_avg:57.74ms
step:851/2330 train_time:49132ms step_avg:57.73ms
step:852/2330 train_time:49192ms step_avg:57.74ms
step:853/2330 train_time:49249ms step_avg:57.74ms
step:854/2330 train_time:49309ms step_avg:57.74ms
step:855/2330 train_time:49366ms step_avg:57.74ms
step:856/2330 train_time:49426ms step_avg:57.74ms
step:857/2330 train_time:49483ms step_avg:57.74ms
step:858/2330 train_time:49543ms step_avg:57.74ms
step:859/2330 train_time:49600ms step_avg:57.74ms
step:860/2330 train_time:49660ms step_avg:57.74ms
step:861/2330 train_time:49717ms step_avg:57.74ms
step:862/2330 train_time:49778ms step_avg:57.75ms
step:863/2330 train_time:49835ms step_avg:57.75ms
step:864/2330 train_time:49896ms step_avg:57.75ms
step:865/2330 train_time:49952ms step_avg:57.75ms
step:866/2330 train_time:50012ms step_avg:57.75ms
step:867/2330 train_time:50068ms step_avg:57.75ms
step:868/2330 train_time:50128ms step_avg:57.75ms
step:869/2330 train_time:50185ms step_avg:57.75ms
step:870/2330 train_time:50245ms step_avg:57.75ms
step:871/2330 train_time:50302ms step_avg:57.75ms
step:872/2330 train_time:50361ms step_avg:57.75ms
step:873/2330 train_time:50419ms step_avg:57.75ms
step:874/2330 train_time:50479ms step_avg:57.76ms
step:875/2330 train_time:50537ms step_avg:57.76ms
step:876/2330 train_time:50597ms step_avg:57.76ms
step:877/2330 train_time:50653ms step_avg:57.76ms
step:878/2330 train_time:50714ms step_avg:57.76ms
step:879/2330 train_time:50771ms step_avg:57.76ms
step:880/2330 train_time:50831ms step_avg:57.76ms
step:881/2330 train_time:50889ms step_avg:57.76ms
step:882/2330 train_time:50948ms step_avg:57.76ms
step:883/2330 train_time:51005ms step_avg:57.76ms
step:884/2330 train_time:51065ms step_avg:57.77ms
step:885/2330 train_time:51122ms step_avg:57.76ms
step:886/2330 train_time:51182ms step_avg:57.77ms
step:887/2330 train_time:51239ms step_avg:57.77ms
step:888/2330 train_time:51300ms step_avg:57.77ms
step:889/2330 train_time:51356ms step_avg:57.77ms
step:890/2330 train_time:51416ms step_avg:57.77ms
step:891/2330 train_time:51473ms step_avg:57.77ms
step:892/2330 train_time:51534ms step_avg:57.77ms
step:893/2330 train_time:51592ms step_avg:57.77ms
step:894/2330 train_time:51651ms step_avg:57.78ms
step:895/2330 train_time:51708ms step_avg:57.77ms
step:896/2330 train_time:51768ms step_avg:57.78ms
step:897/2330 train_time:51825ms step_avg:57.78ms
step:898/2330 train_time:51885ms step_avg:57.78ms
step:899/2330 train_time:51942ms step_avg:57.78ms
step:900/2330 train_time:52002ms step_avg:57.78ms
step:901/2330 train_time:52058ms step_avg:57.78ms
step:902/2330 train_time:52119ms step_avg:57.78ms
step:903/2330 train_time:52176ms step_avg:57.78ms
step:904/2330 train_time:52237ms step_avg:57.78ms
step:905/2330 train_time:52294ms step_avg:57.78ms
step:906/2330 train_time:52353ms step_avg:57.79ms
step:907/2330 train_time:52411ms step_avg:57.78ms
step:908/2330 train_time:52470ms step_avg:57.79ms
step:909/2330 train_time:52527ms step_avg:57.79ms
step:910/2330 train_time:52586ms step_avg:57.79ms
step:911/2330 train_time:52643ms step_avg:57.79ms
step:912/2330 train_time:52704ms step_avg:57.79ms
step:913/2330 train_time:52760ms step_avg:57.79ms
step:914/2330 train_time:52822ms step_avg:57.79ms
step:915/2330 train_time:52878ms step_avg:57.79ms
step:916/2330 train_time:52939ms step_avg:57.79ms
step:917/2330 train_time:52995ms step_avg:57.79ms
step:918/2330 train_time:53056ms step_avg:57.79ms
step:919/2330 train_time:53113ms step_avg:57.79ms
step:920/2330 train_time:53172ms step_avg:57.80ms
step:921/2330 train_time:53229ms step_avg:57.80ms
step:922/2330 train_time:53289ms step_avg:57.80ms
step:923/2330 train_time:53346ms step_avg:57.80ms
step:924/2330 train_time:53406ms step_avg:57.80ms
step:925/2330 train_time:53462ms step_avg:57.80ms
step:926/2330 train_time:53522ms step_avg:57.80ms
step:927/2330 train_time:53579ms step_avg:57.80ms
step:928/2330 train_time:53640ms step_avg:57.80ms
step:929/2330 train_time:53697ms step_avg:57.80ms
step:930/2330 train_time:53757ms step_avg:57.80ms
step:931/2330 train_time:53815ms step_avg:57.80ms
step:932/2330 train_time:53874ms step_avg:57.80ms
step:933/2330 train_time:53931ms step_avg:57.80ms
step:934/2330 train_time:53992ms step_avg:57.81ms
step:935/2330 train_time:54049ms step_avg:57.81ms
step:936/2330 train_time:54108ms step_avg:57.81ms
step:937/2330 train_time:54165ms step_avg:57.81ms
step:938/2330 train_time:54224ms step_avg:57.81ms
step:939/2330 train_time:54282ms step_avg:57.81ms
step:940/2330 train_time:54342ms step_avg:57.81ms
step:941/2330 train_time:54398ms step_avg:57.81ms
step:942/2330 train_time:54458ms step_avg:57.81ms
step:943/2330 train_time:54514ms step_avg:57.81ms
step:944/2330 train_time:54575ms step_avg:57.81ms
step:945/2330 train_time:54633ms step_avg:57.81ms
step:946/2330 train_time:54693ms step_avg:57.82ms
step:947/2330 train_time:54751ms step_avg:57.82ms
step:948/2330 train_time:54811ms step_avg:57.82ms
step:949/2330 train_time:54868ms step_avg:57.82ms
step:950/2330 train_time:54927ms step_avg:57.82ms
step:951/2330 train_time:54984ms step_avg:57.82ms
step:952/2330 train_time:55044ms step_avg:57.82ms
step:953/2330 train_time:55102ms step_avg:57.82ms
step:954/2330 train_time:55161ms step_avg:57.82ms
step:955/2330 train_time:55218ms step_avg:57.82ms
step:956/2330 train_time:55280ms step_avg:57.82ms
step:957/2330 train_time:55336ms step_avg:57.82ms
step:958/2330 train_time:55397ms step_avg:57.83ms
step:959/2330 train_time:55454ms step_avg:57.83ms
step:960/2330 train_time:55515ms step_avg:57.83ms
step:961/2330 train_time:55572ms step_avg:57.83ms
step:962/2330 train_time:55632ms step_avg:57.83ms
step:963/2330 train_time:55689ms step_avg:57.83ms
step:964/2330 train_time:55750ms step_avg:57.83ms
step:965/2330 train_time:55808ms step_avg:57.83ms
step:966/2330 train_time:55867ms step_avg:57.83ms
step:967/2330 train_time:55924ms step_avg:57.83ms
step:968/2330 train_time:55984ms step_avg:57.83ms
step:969/2330 train_time:56041ms step_avg:57.83ms
step:970/2330 train_time:56101ms step_avg:57.84ms
step:971/2330 train_time:56157ms step_avg:57.83ms
step:972/2330 train_time:56218ms step_avg:57.84ms
step:973/2330 train_time:56275ms step_avg:57.84ms
step:974/2330 train_time:56335ms step_avg:57.84ms
step:975/2330 train_time:56392ms step_avg:57.84ms
step:976/2330 train_time:56452ms step_avg:57.84ms
step:977/2330 train_time:56509ms step_avg:57.84ms
step:978/2330 train_time:56568ms step_avg:57.84ms
step:979/2330 train_time:56624ms step_avg:57.84ms
step:980/2330 train_time:56685ms step_avg:57.84ms
step:981/2330 train_time:56741ms step_avg:57.84ms
step:982/2330 train_time:56802ms step_avg:57.84ms
step:983/2330 train_time:56858ms step_avg:57.84ms
step:984/2330 train_time:56919ms step_avg:57.84ms
step:985/2330 train_time:56977ms step_avg:57.84ms
step:986/2330 train_time:57037ms step_avg:57.85ms
step:987/2330 train_time:57095ms step_avg:57.85ms
step:988/2330 train_time:57154ms step_avg:57.85ms
step:989/2330 train_time:57212ms step_avg:57.85ms
step:990/2330 train_time:57271ms step_avg:57.85ms
step:991/2330 train_time:57328ms step_avg:57.85ms
step:992/2330 train_time:57388ms step_avg:57.85ms
step:993/2330 train_time:57445ms step_avg:57.85ms
step:994/2330 train_time:57504ms step_avg:57.85ms
step:995/2330 train_time:57562ms step_avg:57.85ms
step:996/2330 train_time:57622ms step_avg:57.85ms
step:997/2330 train_time:57679ms step_avg:57.85ms
step:998/2330 train_time:57739ms step_avg:57.85ms
step:999/2330 train_time:57797ms step_avg:57.85ms
step:1000/2330 train_time:57857ms step_avg:57.86ms
step:1000/2330 val_loss:4.0753 train_time:57937ms step_avg:57.94ms
step:1001/2330 train_time:57958ms step_avg:57.90ms
step:1002/2330 train_time:57980ms step_avg:57.86ms
step:1003/2330 train_time:58031ms step_avg:57.86ms
step:1004/2330 train_time:58100ms step_avg:57.87ms
step:1005/2330 train_time:58156ms step_avg:57.87ms
step:1006/2330 train_time:58222ms step_avg:57.87ms
step:1007/2330 train_time:58278ms step_avg:57.87ms
step:1008/2330 train_time:58338ms step_avg:57.87ms
step:1009/2330 train_time:58393ms step_avg:57.87ms
step:1010/2330 train_time:58453ms step_avg:57.87ms
step:1011/2330 train_time:58509ms step_avg:57.87ms
step:1012/2330 train_time:58568ms step_avg:57.87ms
step:1013/2330 train_time:58624ms step_avg:57.87ms
step:1014/2330 train_time:58683ms step_avg:57.87ms
step:1015/2330 train_time:58739ms step_avg:57.87ms
step:1016/2330 train_time:58798ms step_avg:57.87ms
step:1017/2330 train_time:58856ms step_avg:57.87ms
step:1018/2330 train_time:58920ms step_avg:57.88ms
step:1019/2330 train_time:58977ms step_avg:57.88ms
step:1020/2330 train_time:59041ms step_avg:57.88ms
step:1021/2330 train_time:59097ms step_avg:57.88ms
step:1022/2330 train_time:59160ms step_avg:57.89ms
step:1023/2330 train_time:59217ms step_avg:57.89ms
step:1024/2330 train_time:59277ms step_avg:57.89ms
step:1025/2330 train_time:59333ms step_avg:57.89ms
step:1026/2330 train_time:59393ms step_avg:57.89ms
step:1027/2330 train_time:59449ms step_avg:57.89ms
step:1028/2330 train_time:59509ms step_avg:57.89ms
step:1029/2330 train_time:59565ms step_avg:57.89ms
step:1030/2330 train_time:59624ms step_avg:57.89ms
step:1031/2330 train_time:59681ms step_avg:57.89ms
step:1032/2330 train_time:59740ms step_avg:57.89ms
step:1033/2330 train_time:59797ms step_avg:57.89ms
step:1034/2330 train_time:59857ms step_avg:57.89ms
step:1035/2330 train_time:59914ms step_avg:57.89ms
step:1036/2330 train_time:59976ms step_avg:57.89ms
step:1037/2330 train_time:60033ms step_avg:57.89ms
step:1038/2330 train_time:60095ms step_avg:57.89ms
step:1039/2330 train_time:60152ms step_avg:57.89ms
step:1040/2330 train_time:60213ms step_avg:57.90ms
step:1041/2330 train_time:60270ms step_avg:57.90ms
step:1042/2330 train_time:60330ms step_avg:57.90ms
step:1043/2330 train_time:60387ms step_avg:57.90ms
step:1044/2330 train_time:60446ms step_avg:57.90ms
step:1045/2330 train_time:60503ms step_avg:57.90ms
step:1046/2330 train_time:60562ms step_avg:57.90ms
step:1047/2330 train_time:60618ms step_avg:57.90ms
step:1048/2330 train_time:60677ms step_avg:57.90ms
step:1049/2330 train_time:60734ms step_avg:57.90ms
step:1050/2330 train_time:60794ms step_avg:57.90ms
step:1051/2330 train_time:60852ms step_avg:57.90ms
step:1052/2330 train_time:60913ms step_avg:57.90ms
step:1053/2330 train_time:60969ms step_avg:57.90ms
step:1054/2330 train_time:61031ms step_avg:57.90ms
step:1055/2330 train_time:61089ms step_avg:57.90ms
step:1056/2330 train_time:61150ms step_avg:57.91ms
step:1057/2330 train_time:61208ms step_avg:57.91ms
step:1058/2330 train_time:61268ms step_avg:57.91ms
step:1059/2330 train_time:61324ms step_avg:57.91ms
step:1060/2330 train_time:61384ms step_avg:57.91ms
step:1061/2330 train_time:61441ms step_avg:57.91ms
step:1062/2330 train_time:61501ms step_avg:57.91ms
step:1063/2330 train_time:61558ms step_avg:57.91ms
step:1064/2330 train_time:61617ms step_avg:57.91ms
step:1065/2330 train_time:61673ms step_avg:57.91ms
step:1066/2330 train_time:61734ms step_avg:57.91ms
step:1067/2330 train_time:61791ms step_avg:57.91ms
step:1068/2330 train_time:61851ms step_avg:57.91ms
step:1069/2330 train_time:61909ms step_avg:57.91ms
step:1070/2330 train_time:61969ms step_avg:57.91ms
step:1071/2330 train_time:62027ms step_avg:57.91ms
step:1072/2330 train_time:62087ms step_avg:57.92ms
step:1073/2330 train_time:62144ms step_avg:57.92ms
step:1074/2330 train_time:62204ms step_avg:57.92ms
step:1075/2330 train_time:62260ms step_avg:57.92ms
step:1076/2330 train_time:62321ms step_avg:57.92ms
step:1077/2330 train_time:62377ms step_avg:57.92ms
step:1078/2330 train_time:62437ms step_avg:57.92ms
step:1079/2330 train_time:62494ms step_avg:57.92ms
step:1080/2330 train_time:62554ms step_avg:57.92ms
step:1081/2330 train_time:62611ms step_avg:57.92ms
step:1082/2330 train_time:62670ms step_avg:57.92ms
step:1083/2330 train_time:62727ms step_avg:57.92ms
step:1084/2330 train_time:62787ms step_avg:57.92ms
step:1085/2330 train_time:62844ms step_avg:57.92ms
step:1086/2330 train_time:62904ms step_avg:57.92ms
step:1087/2330 train_time:62961ms step_avg:57.92ms
step:1088/2330 train_time:63021ms step_avg:57.92ms
step:1089/2330 train_time:63078ms step_avg:57.92ms
step:1090/2330 train_time:63138ms step_avg:57.92ms
step:1091/2330 train_time:63195ms step_avg:57.92ms
step:1092/2330 train_time:63255ms step_avg:57.93ms
step:1093/2330 train_time:63312ms step_avg:57.93ms
step:1094/2330 train_time:63373ms step_avg:57.93ms
step:1095/2330 train_time:63429ms step_avg:57.93ms
step:1096/2330 train_time:63491ms step_avg:57.93ms
step:1097/2330 train_time:63548ms step_avg:57.93ms
step:1098/2330 train_time:63608ms step_avg:57.93ms
step:1099/2330 train_time:63666ms step_avg:57.93ms
step:1100/2330 train_time:63726ms step_avg:57.93ms
step:1101/2330 train_time:63783ms step_avg:57.93ms
step:1102/2330 train_time:63842ms step_avg:57.93ms
step:1103/2330 train_time:63899ms step_avg:57.93ms
step:1104/2330 train_time:63959ms step_avg:57.93ms
step:1105/2330 train_time:64016ms step_avg:57.93ms
step:1106/2330 train_time:64076ms step_avg:57.93ms
step:1107/2330 train_time:64133ms step_avg:57.93ms
step:1108/2330 train_time:64194ms step_avg:57.94ms
step:1109/2330 train_time:64251ms step_avg:57.94ms
step:1110/2330 train_time:64312ms step_avg:57.94ms
step:1111/2330 train_time:64369ms step_avg:57.94ms
step:1112/2330 train_time:64428ms step_avg:57.94ms
step:1113/2330 train_time:64485ms step_avg:57.94ms
step:1114/2330 train_time:64544ms step_avg:57.94ms
step:1115/2330 train_time:64601ms step_avg:57.94ms
step:1116/2330 train_time:64662ms step_avg:57.94ms
step:1117/2330 train_time:64718ms step_avg:57.94ms
step:1118/2330 train_time:64777ms step_avg:57.94ms
step:1119/2330 train_time:64834ms step_avg:57.94ms
step:1120/2330 train_time:64895ms step_avg:57.94ms
step:1121/2330 train_time:64952ms step_avg:57.94ms
step:1122/2330 train_time:65012ms step_avg:57.94ms
step:1123/2330 train_time:65069ms step_avg:57.94ms
step:1124/2330 train_time:65130ms step_avg:57.94ms
step:1125/2330 train_time:65186ms step_avg:57.94ms
step:1126/2330 train_time:65247ms step_avg:57.95ms
step:1127/2330 train_time:65305ms step_avg:57.95ms
step:1128/2330 train_time:65365ms step_avg:57.95ms
step:1129/2330 train_time:65422ms step_avg:57.95ms
step:1130/2330 train_time:65481ms step_avg:57.95ms
step:1131/2330 train_time:65537ms step_avg:57.95ms
step:1132/2330 train_time:65598ms step_avg:57.95ms
step:1133/2330 train_time:65656ms step_avg:57.95ms
step:1134/2330 train_time:65715ms step_avg:57.95ms
step:1135/2330 train_time:65772ms step_avg:57.95ms
step:1136/2330 train_time:65833ms step_avg:57.95ms
step:1137/2330 train_time:65889ms step_avg:57.95ms
step:1138/2330 train_time:65949ms step_avg:57.95ms
step:1139/2330 train_time:66006ms step_avg:57.95ms
step:1140/2330 train_time:66067ms step_avg:57.95ms
step:1141/2330 train_time:66125ms step_avg:57.95ms
step:1142/2330 train_time:66184ms step_avg:57.95ms
step:1143/2330 train_time:66241ms step_avg:57.95ms
step:1144/2330 train_time:66301ms step_avg:57.96ms
step:1145/2330 train_time:66359ms step_avg:57.96ms
step:1146/2330 train_time:66841ms step_avg:58.33ms
step:1147/2330 train_time:66896ms step_avg:58.32ms
step:1148/2330 train_time:66956ms step_avg:58.32ms
step:1149/2330 train_time:67012ms step_avg:58.32ms
step:1150/2330 train_time:67071ms step_avg:58.32ms
step:1151/2330 train_time:67127ms step_avg:58.32ms
step:1152/2330 train_time:67186ms step_avg:58.32ms
step:1153/2330 train_time:67243ms step_avg:58.32ms
step:1154/2330 train_time:67301ms step_avg:58.32ms
step:1155/2330 train_time:67358ms step_avg:58.32ms
step:1156/2330 train_time:67416ms step_avg:58.32ms
step:1157/2330 train_time:67472ms step_avg:58.32ms
step:1158/2330 train_time:67532ms step_avg:58.32ms
step:1159/2330 train_time:67588ms step_avg:58.32ms
step:1160/2330 train_time:67647ms step_avg:58.32ms
step:1161/2330 train_time:67711ms step_avg:58.32ms
step:1162/2330 train_time:67775ms step_avg:58.33ms
step:1163/2330 train_time:67833ms step_avg:58.33ms
step:1164/2330 train_time:67894ms step_avg:58.33ms
step:1165/2330 train_time:67952ms step_avg:58.33ms
step:1166/2330 train_time:68012ms step_avg:58.33ms
step:1167/2330 train_time:68068ms step_avg:58.33ms
step:1168/2330 train_time:68129ms step_avg:58.33ms
step:1169/2330 train_time:68185ms step_avg:58.33ms
step:1170/2330 train_time:68245ms step_avg:58.33ms
step:1171/2330 train_time:68302ms step_avg:58.33ms
step:1172/2330 train_time:68360ms step_avg:58.33ms
step:1173/2330 train_time:68417ms step_avg:58.33ms
step:1174/2330 train_time:68476ms step_avg:58.33ms
step:1175/2330 train_time:68532ms step_avg:58.33ms
step:1176/2330 train_time:68591ms step_avg:58.33ms
step:1177/2330 train_time:68651ms step_avg:58.33ms
step:1178/2330 train_time:68713ms step_avg:58.33ms
step:1179/2330 train_time:68771ms step_avg:58.33ms
step:1180/2330 train_time:68831ms step_avg:58.33ms
step:1181/2330 train_time:68889ms step_avg:58.33ms
step:1182/2330 train_time:68950ms step_avg:58.33ms
step:1183/2330 train_time:69007ms step_avg:58.33ms
step:1184/2330 train_time:69067ms step_avg:58.33ms
step:1185/2330 train_time:69124ms step_avg:58.33ms
step:1186/2330 train_time:69183ms step_avg:58.33ms
step:1187/2330 train_time:69240ms step_avg:58.33ms
step:1188/2330 train_time:69300ms step_avg:58.33ms
step:1189/2330 train_time:69356ms step_avg:58.33ms
step:1190/2330 train_time:69415ms step_avg:58.33ms
step:1191/2330 train_time:69471ms step_avg:58.33ms
step:1192/2330 train_time:69531ms step_avg:58.33ms
step:1193/2330 train_time:69589ms step_avg:58.33ms
step:1194/2330 train_time:69649ms step_avg:58.33ms
step:1195/2330 train_time:69707ms step_avg:58.33ms
step:1196/2330 train_time:69767ms step_avg:58.33ms
step:1197/2330 train_time:69825ms step_avg:58.33ms
step:1198/2330 train_time:69885ms step_avg:58.34ms
step:1199/2330 train_time:69942ms step_avg:58.33ms
step:1200/2330 train_time:70002ms step_avg:58.33ms
step:1201/2330 train_time:70058ms step_avg:58.33ms
step:1202/2330 train_time:70119ms step_avg:58.34ms
step:1203/2330 train_time:70175ms step_avg:58.33ms
step:1204/2330 train_time:70235ms step_avg:58.33ms
step:1205/2330 train_time:70291ms step_avg:58.33ms
step:1206/2330 train_time:70352ms step_avg:58.33ms
step:1207/2330 train_time:70408ms step_avg:58.33ms
step:1208/2330 train_time:70468ms step_avg:58.33ms
step:1209/2330 train_time:70525ms step_avg:58.33ms
step:1210/2330 train_time:70585ms step_avg:58.33ms
step:1211/2330 train_time:70641ms step_avg:58.33ms
step:1212/2330 train_time:70703ms step_avg:58.34ms
step:1213/2330 train_time:70760ms step_avg:58.33ms
step:1214/2330 train_time:70821ms step_avg:58.34ms
step:1215/2330 train_time:70878ms step_avg:58.34ms
step:1216/2330 train_time:70938ms step_avg:58.34ms
step:1217/2330 train_time:70996ms step_avg:58.34ms
step:1218/2330 train_time:71055ms step_avg:58.34ms
step:1219/2330 train_time:71112ms step_avg:58.34ms
step:1220/2330 train_time:71173ms step_avg:58.34ms
step:1221/2330 train_time:71229ms step_avg:58.34ms
step:1222/2330 train_time:71290ms step_avg:58.34ms
step:1223/2330 train_time:71347ms step_avg:58.34ms
step:1224/2330 train_time:71407ms step_avg:58.34ms
step:1225/2330 train_time:71464ms step_avg:58.34ms
step:1226/2330 train_time:71523ms step_avg:58.34ms
step:1227/2330 train_time:71581ms step_avg:58.34ms
step:1228/2330 train_time:71640ms step_avg:58.34ms
step:1229/2330 train_time:71697ms step_avg:58.34ms
step:1230/2330 train_time:71757ms step_avg:58.34ms
step:1231/2330 train_time:71814ms step_avg:58.34ms
step:1232/2330 train_time:71875ms step_avg:58.34ms
step:1233/2330 train_time:71933ms step_avg:58.34ms
step:1234/2330 train_time:71993ms step_avg:58.34ms
step:1235/2330 train_time:72050ms step_avg:58.34ms
step:1236/2330 train_time:72110ms step_avg:58.34ms
step:1237/2330 train_time:72167ms step_avg:58.34ms
step:1238/2330 train_time:72226ms step_avg:58.34ms
step:1239/2330 train_time:72282ms step_avg:58.34ms
step:1240/2330 train_time:72342ms step_avg:58.34ms
step:1241/2330 train_time:72398ms step_avg:58.34ms
step:1242/2330 train_time:72458ms step_avg:58.34ms
step:1243/2330 train_time:72515ms step_avg:58.34ms
step:1244/2330 train_time:72575ms step_avg:58.34ms
step:1245/2330 train_time:72632ms step_avg:58.34ms
step:1246/2330 train_time:72693ms step_avg:58.34ms
step:1247/2330 train_time:72751ms step_avg:58.34ms
step:1248/2330 train_time:72811ms step_avg:58.34ms
step:1249/2330 train_time:72869ms step_avg:58.34ms
step:1250/2330 train_time:72929ms step_avg:58.34ms
step:1250/2330 val_loss:3.9924 train_time:73010ms step_avg:58.41ms
step:1251/2330 train_time:73030ms step_avg:58.38ms
step:1252/2330 train_time:73049ms step_avg:58.35ms
step:1253/2330 train_time:73109ms step_avg:58.35ms
step:1254/2330 train_time:73173ms step_avg:58.35ms
step:1255/2330 train_time:73231ms step_avg:58.35ms
step:1256/2330 train_time:73291ms step_avg:58.35ms
step:1257/2330 train_time:73347ms step_avg:58.35ms
step:1258/2330 train_time:73407ms step_avg:58.35ms
step:1259/2330 train_time:73463ms step_avg:58.35ms
step:1260/2330 train_time:73523ms step_avg:58.35ms
step:1261/2330 train_time:73580ms step_avg:58.35ms
step:1262/2330 train_time:73639ms step_avg:58.35ms
step:1263/2330 train_time:73695ms step_avg:58.35ms
step:1264/2330 train_time:73754ms step_avg:58.35ms
step:1265/2330 train_time:73810ms step_avg:58.35ms
step:1266/2330 train_time:73869ms step_avg:58.35ms
step:1267/2330 train_time:73925ms step_avg:58.35ms
step:1268/2330 train_time:73987ms step_avg:58.35ms
step:1269/2330 train_time:74045ms step_avg:58.35ms
step:1270/2330 train_time:74106ms step_avg:58.35ms
step:1271/2330 train_time:74164ms step_avg:58.35ms
step:1272/2330 train_time:74226ms step_avg:58.35ms
step:1273/2330 train_time:74283ms step_avg:58.35ms
step:1274/2330 train_time:74344ms step_avg:58.36ms
step:1275/2330 train_time:74401ms step_avg:58.35ms
step:1276/2330 train_time:74461ms step_avg:58.36ms
step:1277/2330 train_time:74518ms step_avg:58.35ms
step:1278/2330 train_time:74577ms step_avg:58.35ms
step:1279/2330 train_time:74634ms step_avg:58.35ms
step:1280/2330 train_time:74693ms step_avg:58.35ms
step:1281/2330 train_time:74750ms step_avg:58.35ms
step:1282/2330 train_time:74809ms step_avg:58.35ms
step:1283/2330 train_time:74866ms step_avg:58.35ms
step:1284/2330 train_time:74925ms step_avg:58.35ms
step:1285/2330 train_time:74982ms step_avg:58.35ms
step:1286/2330 train_time:75043ms step_avg:58.35ms
step:1287/2330 train_time:75102ms step_avg:58.35ms
step:1288/2330 train_time:75162ms step_avg:58.36ms
step:1289/2330 train_time:75219ms step_avg:58.35ms
step:1290/2330 train_time:75281ms step_avg:58.36ms
step:1291/2330 train_time:75339ms step_avg:58.36ms
step:1292/2330 train_time:75399ms step_avg:58.36ms
step:1293/2330 train_time:75457ms step_avg:58.36ms
step:1294/2330 train_time:75517ms step_avg:58.36ms
step:1295/2330 train_time:75573ms step_avg:58.36ms
step:1296/2330 train_time:75633ms step_avg:58.36ms
step:1297/2330 train_time:75689ms step_avg:58.36ms
step:1298/2330 train_time:75748ms step_avg:58.36ms
step:1299/2330 train_time:75804ms step_avg:58.36ms
step:1300/2330 train_time:75864ms step_avg:58.36ms
step:1301/2330 train_time:75921ms step_avg:58.36ms
step:1302/2330 train_time:75981ms step_avg:58.36ms
step:1303/2330 train_time:76039ms step_avg:58.36ms
step:1304/2330 train_time:76099ms step_avg:58.36ms
step:1305/2330 train_time:76157ms step_avg:58.36ms
step:1306/2330 train_time:76217ms step_avg:58.36ms
step:1307/2330 train_time:76276ms step_avg:58.36ms
step:1308/2330 train_time:76336ms step_avg:58.36ms
step:1309/2330 train_time:76393ms step_avg:58.36ms
step:1310/2330 train_time:76452ms step_avg:58.36ms
step:1311/2330 train_time:76509ms step_avg:58.36ms
step:1312/2330 train_time:76569ms step_avg:58.36ms
step:1313/2330 train_time:76625ms step_avg:58.36ms
step:1314/2330 train_time:76685ms step_avg:58.36ms
step:1315/2330 train_time:76741ms step_avg:58.36ms
step:1316/2330 train_time:76801ms step_avg:58.36ms
step:1317/2330 train_time:76857ms step_avg:58.36ms
step:1318/2330 train_time:76917ms step_avg:58.36ms
step:1319/2330 train_time:76974ms step_avg:58.36ms
step:1320/2330 train_time:77034ms step_avg:58.36ms
step:1321/2330 train_time:77091ms step_avg:58.36ms
step:1322/2330 train_time:77151ms step_avg:58.36ms
step:1323/2330 train_time:77208ms step_avg:58.36ms
step:1324/2330 train_time:77270ms step_avg:58.36ms
step:1325/2330 train_time:77327ms step_avg:58.36ms
step:1326/2330 train_time:77387ms step_avg:58.36ms
step:1327/2330 train_time:77444ms step_avg:58.36ms
step:1328/2330 train_time:77504ms step_avg:58.36ms
step:1329/2330 train_time:77561ms step_avg:58.36ms
step:1330/2330 train_time:77621ms step_avg:58.36ms
step:1331/2330 train_time:77678ms step_avg:58.36ms
step:1332/2330 train_time:77738ms step_avg:58.36ms
step:1333/2330 train_time:77795ms step_avg:58.36ms
step:1334/2330 train_time:77854ms step_avg:58.36ms
step:1335/2330 train_time:77911ms step_avg:58.36ms
step:1336/2330 train_time:77970ms step_avg:58.36ms
step:1337/2330 train_time:78027ms step_avg:58.36ms
step:1338/2330 train_time:78086ms step_avg:58.36ms
step:1339/2330 train_time:78143ms step_avg:58.36ms
step:1340/2330 train_time:78204ms step_avg:58.36ms
step:1341/2330 train_time:78261ms step_avg:58.36ms
step:1342/2330 train_time:78322ms step_avg:58.36ms
step:1343/2330 train_time:78380ms step_avg:58.36ms
step:1344/2330 train_time:78440ms step_avg:58.36ms
step:1345/2330 train_time:78496ms step_avg:58.36ms
step:1346/2330 train_time:78556ms step_avg:58.36ms
step:1347/2330 train_time:78613ms step_avg:58.36ms
step:1348/2330 train_time:78673ms step_avg:58.36ms
step:1349/2330 train_time:78729ms step_avg:58.36ms
step:1350/2330 train_time:78789ms step_avg:58.36ms
step:1351/2330 train_time:78845ms step_avg:58.36ms
step:1352/2330 train_time:78905ms step_avg:58.36ms
step:1353/2330 train_time:78962ms step_avg:58.36ms
step:1354/2330 train_time:79022ms step_avg:58.36ms
step:1355/2330 train_time:79080ms step_avg:58.36ms
step:1356/2330 train_time:79140ms step_avg:58.36ms
step:1357/2330 train_time:79198ms step_avg:58.36ms
step:1358/2330 train_time:79258ms step_avg:58.36ms
step:1359/2330 train_time:79316ms step_avg:58.36ms
step:1360/2330 train_time:79375ms step_avg:58.36ms
step:1361/2330 train_time:79432ms step_avg:58.36ms
step:1362/2330 train_time:79492ms step_avg:58.36ms
step:1363/2330 train_time:79549ms step_avg:58.36ms
step:1364/2330 train_time:79610ms step_avg:58.37ms
step:1365/2330 train_time:79667ms step_avg:58.36ms
step:1366/2330 train_time:79727ms step_avg:58.37ms
step:1367/2330 train_time:79783ms step_avg:58.36ms
step:1368/2330 train_time:79843ms step_avg:58.36ms
step:1369/2330 train_time:79900ms step_avg:58.36ms
step:1370/2330 train_time:79960ms step_avg:58.36ms
step:1371/2330 train_time:80017ms step_avg:58.36ms
step:1372/2330 train_time:80077ms step_avg:58.37ms
step:1373/2330 train_time:80135ms step_avg:58.36ms
step:1374/2330 train_time:80194ms step_avg:58.37ms
step:1375/2330 train_time:80251ms step_avg:58.36ms
step:1376/2330 train_time:80311ms step_avg:58.37ms
step:1377/2330 train_time:80368ms step_avg:58.36ms
step:1378/2330 train_time:80428ms step_avg:58.37ms
step:1379/2330 train_time:80485ms step_avg:58.36ms
step:1380/2330 train_time:80545ms step_avg:58.37ms
step:1381/2330 train_time:80602ms step_avg:58.37ms
step:1382/2330 train_time:80662ms step_avg:58.37ms
step:1383/2330 train_time:80718ms step_avg:58.36ms
step:1384/2330 train_time:80779ms step_avg:58.37ms
step:1385/2330 train_time:80836ms step_avg:58.37ms
step:1386/2330 train_time:80896ms step_avg:58.37ms
step:1387/2330 train_time:80953ms step_avg:58.37ms
step:1388/2330 train_time:81014ms step_avg:58.37ms
step:1389/2330 train_time:81071ms step_avg:58.37ms
step:1390/2330 train_time:81130ms step_avg:58.37ms
step:1391/2330 train_time:81187ms step_avg:58.37ms
step:1392/2330 train_time:81247ms step_avg:58.37ms
step:1393/2330 train_time:81305ms step_avg:58.37ms
step:1394/2330 train_time:81365ms step_avg:58.37ms
step:1395/2330 train_time:81422ms step_avg:58.37ms
step:1396/2330 train_time:81482ms step_avg:58.37ms
step:1397/2330 train_time:81539ms step_avg:58.37ms
step:1398/2330 train_time:81599ms step_avg:58.37ms
step:1399/2330 train_time:81656ms step_avg:58.37ms
step:1400/2330 train_time:81716ms step_avg:58.37ms
step:1401/2330 train_time:81773ms step_avg:58.37ms
step:1402/2330 train_time:81832ms step_avg:58.37ms
step:1403/2330 train_time:81889ms step_avg:58.37ms
step:1404/2330 train_time:81949ms step_avg:58.37ms
step:1405/2330 train_time:82006ms step_avg:58.37ms
step:1406/2330 train_time:82066ms step_avg:58.37ms
step:1407/2330 train_time:82122ms step_avg:58.37ms
step:1408/2330 train_time:82183ms step_avg:58.37ms
step:1409/2330 train_time:82240ms step_avg:58.37ms
step:1410/2330 train_time:82300ms step_avg:58.37ms
step:1411/2330 train_time:82358ms step_avg:58.37ms
step:1412/2330 train_time:82418ms step_avg:58.37ms
step:1413/2330 train_time:82474ms step_avg:58.37ms
step:1414/2330 train_time:82534ms step_avg:58.37ms
step:1415/2330 train_time:82591ms step_avg:58.37ms
step:1416/2330 train_time:82651ms step_avg:58.37ms
step:1417/2330 train_time:82708ms step_avg:58.37ms
step:1418/2330 train_time:82767ms step_avg:58.37ms
step:1419/2330 train_time:82824ms step_avg:58.37ms
step:1420/2330 train_time:82884ms step_avg:58.37ms
step:1421/2330 train_time:82941ms step_avg:58.37ms
step:1422/2330 train_time:83001ms step_avg:58.37ms
step:1423/2330 train_time:83058ms step_avg:58.37ms
step:1424/2330 train_time:83118ms step_avg:58.37ms
step:1425/2330 train_time:83175ms step_avg:58.37ms
step:1426/2330 train_time:83235ms step_avg:58.37ms
step:1427/2330 train_time:83292ms step_avg:58.37ms
step:1428/2330 train_time:83352ms step_avg:58.37ms
step:1429/2330 train_time:83409ms step_avg:58.37ms
step:1430/2330 train_time:83470ms step_avg:58.37ms
step:1431/2330 train_time:83526ms step_avg:58.37ms
step:1432/2330 train_time:83587ms step_avg:58.37ms
step:1433/2330 train_time:83643ms step_avg:58.37ms
step:1434/2330 train_time:83704ms step_avg:58.37ms
step:1435/2330 train_time:83762ms step_avg:58.37ms
step:1436/2330 train_time:83822ms step_avg:58.37ms
step:1437/2330 train_time:83880ms step_avg:58.37ms
step:1438/2330 train_time:83940ms step_avg:58.37ms
step:1439/2330 train_time:83997ms step_avg:58.37ms
step:1440/2330 train_time:84056ms step_avg:58.37ms
step:1441/2330 train_time:84113ms step_avg:58.37ms
step:1442/2330 train_time:84173ms step_avg:58.37ms
step:1443/2330 train_time:84230ms step_avg:58.37ms
step:1444/2330 train_time:84290ms step_avg:58.37ms
step:1445/2330 train_time:84347ms step_avg:58.37ms
step:1446/2330 train_time:84406ms step_avg:58.37ms
step:1447/2330 train_time:84464ms step_avg:58.37ms
step:1448/2330 train_time:84524ms step_avg:58.37ms
step:1449/2330 train_time:84581ms step_avg:58.37ms
step:1450/2330 train_time:84641ms step_avg:58.37ms
step:1451/2330 train_time:84697ms step_avg:58.37ms
step:1452/2330 train_time:84757ms step_avg:58.37ms
step:1453/2330 train_time:84814ms step_avg:58.37ms
step:1454/2330 train_time:84874ms step_avg:58.37ms
step:1455/2330 train_time:84930ms step_avg:58.37ms
step:1456/2330 train_time:84991ms step_avg:58.37ms
step:1457/2330 train_time:85048ms step_avg:58.37ms
step:1458/2330 train_time:85107ms step_avg:58.37ms
step:1459/2330 train_time:85164ms step_avg:58.37ms
step:1460/2330 train_time:85224ms step_avg:58.37ms
step:1461/2330 train_time:85281ms step_avg:58.37ms
step:1462/2330 train_time:85342ms step_avg:58.37ms
step:1463/2330 train_time:85399ms step_avg:58.37ms
step:1464/2330 train_time:85460ms step_avg:58.37ms
step:1465/2330 train_time:85517ms step_avg:58.37ms
step:1466/2330 train_time:85577ms step_avg:58.37ms
step:1467/2330 train_time:85634ms step_avg:58.37ms
step:1468/2330 train_time:85695ms step_avg:58.38ms
step:1469/2330 train_time:85752ms step_avg:58.37ms
step:1470/2330 train_time:85811ms step_avg:58.38ms
step:1471/2330 train_time:85868ms step_avg:58.37ms
step:1472/2330 train_time:85927ms step_avg:58.37ms
step:1473/2330 train_time:85984ms step_avg:58.37ms
step:1474/2330 train_time:86044ms step_avg:58.37ms
step:1475/2330 train_time:86101ms step_avg:58.37ms
step:1476/2330 train_time:86161ms step_avg:58.37ms
step:1477/2330 train_time:86218ms step_avg:58.37ms
step:1478/2330 train_time:86279ms step_avg:58.38ms
step:1479/2330 train_time:86336ms step_avg:58.37ms
step:1480/2330 train_time:86397ms step_avg:58.38ms
step:1481/2330 train_time:86454ms step_avg:58.38ms
step:1482/2330 train_time:86514ms step_avg:58.38ms
step:1483/2330 train_time:86570ms step_avg:58.38ms
step:1484/2330 train_time:86631ms step_avg:58.38ms
step:1485/2330 train_time:86688ms step_avg:58.38ms
step:1486/2330 train_time:86748ms step_avg:58.38ms
step:1487/2330 train_time:86805ms step_avg:58.38ms
step:1488/2330 train_time:86865ms step_avg:58.38ms
step:1489/2330 train_time:86922ms step_avg:58.38ms
step:1490/2330 train_time:86981ms step_avg:58.38ms
step:1491/2330 train_time:87039ms step_avg:58.38ms
step:1492/2330 train_time:87098ms step_avg:58.38ms
step:1493/2330 train_time:87155ms step_avg:58.38ms
step:1494/2330 train_time:87215ms step_avg:58.38ms
step:1495/2330 train_time:87272ms step_avg:58.38ms
step:1496/2330 train_time:87332ms step_avg:58.38ms
step:1497/2330 train_time:87389ms step_avg:58.38ms
step:1498/2330 train_time:87449ms step_avg:58.38ms
step:1499/2330 train_time:87506ms step_avg:58.38ms
step:1500/2330 train_time:87566ms step_avg:58.38ms
step:1500/2330 val_loss:3.9106 train_time:87646ms step_avg:58.43ms
step:1501/2330 train_time:87666ms step_avg:58.41ms
step:1502/2330 train_time:87686ms step_avg:58.38ms
step:1503/2330 train_time:87741ms step_avg:58.38ms
step:1504/2330 train_time:87806ms step_avg:58.38ms
step:1505/2330 train_time:87863ms step_avg:58.38ms
step:1506/2330 train_time:87924ms step_avg:58.38ms
step:1507/2330 train_time:87981ms step_avg:58.38ms
step:1508/2330 train_time:88041ms step_avg:58.38ms
step:1509/2330 train_time:88097ms step_avg:58.38ms
step:1510/2330 train_time:88156ms step_avg:58.38ms
step:1511/2330 train_time:88212ms step_avg:58.38ms
step:1512/2330 train_time:88272ms step_avg:58.38ms
step:1513/2330 train_time:88328ms step_avg:58.38ms
step:1514/2330 train_time:88387ms step_avg:58.38ms
step:1515/2330 train_time:88443ms step_avg:58.38ms
step:1516/2330 train_time:88503ms step_avg:58.38ms
step:1517/2330 train_time:88559ms step_avg:58.38ms
step:1518/2330 train_time:88620ms step_avg:58.38ms
step:1519/2330 train_time:88677ms step_avg:58.38ms
step:1520/2330 train_time:88740ms step_avg:58.38ms
step:1521/2330 train_time:88798ms step_avg:58.38ms
step:1522/2330 train_time:88861ms step_avg:58.38ms
step:1523/2330 train_time:88917ms step_avg:58.38ms
step:1524/2330 train_time:88978ms step_avg:58.38ms
step:1525/2330 train_time:89034ms step_avg:58.38ms
step:1526/2330 train_time:89094ms step_avg:58.38ms
step:1527/2330 train_time:89151ms step_avg:58.38ms
step:1528/2330 train_time:89211ms step_avg:58.38ms
step:1529/2330 train_time:89268ms step_avg:58.38ms
step:1530/2330 train_time:89327ms step_avg:58.38ms
step:1531/2330 train_time:89383ms step_avg:58.38ms
step:1532/2330 train_time:89443ms step_avg:58.38ms
step:1533/2330 train_time:89500ms step_avg:58.38ms
step:1534/2330 train_time:89559ms step_avg:58.38ms
step:1535/2330 train_time:89616ms step_avg:58.38ms
step:1536/2330 train_time:89678ms step_avg:58.38ms
step:1537/2330 train_time:89735ms step_avg:58.38ms
step:1538/2330 train_time:89798ms step_avg:58.39ms
step:1539/2330 train_time:89856ms step_avg:58.39ms
step:1540/2330 train_time:89918ms step_avg:58.39ms
step:1541/2330 train_time:89975ms step_avg:58.39ms
step:1542/2330 train_time:90035ms step_avg:58.39ms
step:1543/2330 train_time:90092ms step_avg:58.39ms
step:1544/2330 train_time:90152ms step_avg:58.39ms
step:1545/2330 train_time:90210ms step_avg:58.39ms
step:1546/2330 train_time:90271ms step_avg:58.39ms
step:1547/2330 train_time:90328ms step_avg:58.39ms
step:1548/2330 train_time:90388ms step_avg:58.39ms
step:1549/2330 train_time:90445ms step_avg:58.39ms
step:1550/2330 train_time:90505ms step_avg:58.39ms
step:1551/2330 train_time:90562ms step_avg:58.39ms
step:1552/2330 train_time:90624ms step_avg:58.39ms
step:1553/2330 train_time:90681ms step_avg:58.39ms
step:1554/2330 train_time:90743ms step_avg:58.39ms
step:1555/2330 train_time:90800ms step_avg:58.39ms
step:1556/2330 train_time:90863ms step_avg:58.40ms
step:1557/2330 train_time:90920ms step_avg:58.39ms
step:1558/2330 train_time:90981ms step_avg:58.40ms
step:1559/2330 train_time:91038ms step_avg:58.39ms
step:1560/2330 train_time:91099ms step_avg:58.40ms
step:1561/2330 train_time:91156ms step_avg:58.40ms
step:1562/2330 train_time:91218ms step_avg:58.40ms
step:1563/2330 train_time:91275ms step_avg:58.40ms
step:1564/2330 train_time:91335ms step_avg:58.40ms
step:1565/2330 train_time:91393ms step_avg:58.40ms
step:1566/2330 train_time:91453ms step_avg:58.40ms
step:1567/2330 train_time:91511ms step_avg:58.40ms
step:1568/2330 train_time:91572ms step_avg:58.40ms
step:1569/2330 train_time:91630ms step_avg:58.40ms
step:1570/2330 train_time:91690ms step_avg:58.40ms
step:1571/2330 train_time:91748ms step_avg:58.40ms
step:1572/2330 train_time:91810ms step_avg:58.40ms
step:1573/2330 train_time:91868ms step_avg:58.40ms
step:1574/2330 train_time:91931ms step_avg:58.41ms
step:1575/2330 train_time:91988ms step_avg:58.40ms
step:1576/2330 train_time:92049ms step_avg:58.41ms
step:1577/2330 train_time:92107ms step_avg:58.41ms
step:1578/2330 train_time:92168ms step_avg:58.41ms
step:1579/2330 train_time:92225ms step_avg:58.41ms
step:1580/2330 train_time:92285ms step_avg:58.41ms
step:1581/2330 train_time:92341ms step_avg:58.41ms
step:1582/2330 train_time:92402ms step_avg:58.41ms
step:1583/2330 train_time:92459ms step_avg:58.41ms
step:1584/2330 train_time:92521ms step_avg:58.41ms
step:1585/2330 train_time:92577ms step_avg:58.41ms
step:1586/2330 train_time:92637ms step_avg:58.41ms
step:1587/2330 train_time:92694ms step_avg:58.41ms
step:1588/2330 train_time:92757ms step_avg:58.41ms
step:1589/2330 train_time:92813ms step_avg:58.41ms
step:1590/2330 train_time:92876ms step_avg:58.41ms
step:1591/2330 train_time:92933ms step_avg:58.41ms
step:1592/2330 train_time:92995ms step_avg:58.41ms
step:1593/2330 train_time:93052ms step_avg:58.41ms
step:1594/2330 train_time:93114ms step_avg:58.42ms
step:1595/2330 train_time:93172ms step_avg:58.41ms
step:1596/2330 train_time:93232ms step_avg:58.42ms
step:1597/2330 train_time:93290ms step_avg:58.42ms
step:1598/2330 train_time:93350ms step_avg:58.42ms
step:1599/2330 train_time:93407ms step_avg:58.42ms
step:1600/2330 train_time:93468ms step_avg:58.42ms
step:1601/2330 train_time:93525ms step_avg:58.42ms
step:1602/2330 train_time:93586ms step_avg:58.42ms
step:1603/2330 train_time:93643ms step_avg:58.42ms
step:1604/2330 train_time:93705ms step_avg:58.42ms
step:1605/2330 train_time:93762ms step_avg:58.42ms
step:1606/2330 train_time:93823ms step_avg:58.42ms
step:1607/2330 train_time:93880ms step_avg:58.42ms
step:1608/2330 train_time:93942ms step_avg:58.42ms
step:1609/2330 train_time:93998ms step_avg:58.42ms
step:1610/2330 train_time:94060ms step_avg:58.42ms
step:1611/2330 train_time:94117ms step_avg:58.42ms
step:1612/2330 train_time:94178ms step_avg:58.42ms
step:1613/2330 train_time:94234ms step_avg:58.42ms
step:1614/2330 train_time:94296ms step_avg:58.42ms
step:1615/2330 train_time:94353ms step_avg:58.42ms
step:1616/2330 train_time:94414ms step_avg:58.42ms
step:1617/2330 train_time:94472ms step_avg:58.42ms
step:1618/2330 train_time:94533ms step_avg:58.43ms
step:1619/2330 train_time:94590ms step_avg:58.42ms
step:1620/2330 train_time:94652ms step_avg:58.43ms
step:1621/2330 train_time:94709ms step_avg:58.43ms
step:1622/2330 train_time:94772ms step_avg:58.43ms
step:1623/2330 train_time:94830ms step_avg:58.43ms
step:1624/2330 train_time:94891ms step_avg:58.43ms
step:1625/2330 train_time:94948ms step_avg:58.43ms
step:1626/2330 train_time:95009ms step_avg:58.43ms
step:1627/2330 train_time:95066ms step_avg:58.43ms
step:1628/2330 train_time:95129ms step_avg:58.43ms
step:1629/2330 train_time:95185ms step_avg:58.43ms
step:1630/2330 train_time:95246ms step_avg:58.43ms
step:1631/2330 train_time:95303ms step_avg:58.43ms
step:1632/2330 train_time:95364ms step_avg:58.43ms
step:1633/2330 train_time:95421ms step_avg:58.43ms
step:1634/2330 train_time:95481ms step_avg:58.43ms
step:1635/2330 train_time:95538ms step_avg:58.43ms
step:1636/2330 train_time:95599ms step_avg:58.43ms
step:1637/2330 train_time:95656ms step_avg:58.43ms
step:1638/2330 train_time:95718ms step_avg:58.44ms
step:1639/2330 train_time:95774ms step_avg:58.43ms
step:1640/2330 train_time:95836ms step_avg:58.44ms
step:1641/2330 train_time:95893ms step_avg:58.44ms
step:1642/2330 train_time:95955ms step_avg:58.44ms
step:1643/2330 train_time:96013ms step_avg:58.44ms
step:1644/2330 train_time:96074ms step_avg:58.44ms
step:1645/2330 train_time:96131ms step_avg:58.44ms
step:1646/2330 train_time:96193ms step_avg:58.44ms
step:1647/2330 train_time:96250ms step_avg:58.44ms
step:1648/2330 train_time:96312ms step_avg:58.44ms
step:1649/2330 train_time:96370ms step_avg:58.44ms
step:1650/2330 train_time:96430ms step_avg:58.44ms
step:1651/2330 train_time:96487ms step_avg:58.44ms
step:1652/2330 train_time:96548ms step_avg:58.44ms
step:1653/2330 train_time:96605ms step_avg:58.44ms
step:1654/2330 train_time:96667ms step_avg:58.44ms
step:1655/2330 train_time:96725ms step_avg:58.44ms
step:1656/2330 train_time:96785ms step_avg:58.45ms
step:1657/2330 train_time:96842ms step_avg:58.44ms
step:1658/2330 train_time:96904ms step_avg:58.45ms
step:1659/2330 train_time:96961ms step_avg:58.45ms
step:1660/2330 train_time:97022ms step_avg:58.45ms
step:1661/2330 train_time:97078ms step_avg:58.45ms
step:1662/2330 train_time:97139ms step_avg:58.45ms
step:1663/2330 train_time:97196ms step_avg:58.45ms
step:1664/2330 train_time:97258ms step_avg:58.45ms
step:1665/2330 train_time:97315ms step_avg:58.45ms
step:1666/2330 train_time:97376ms step_avg:58.45ms
step:1667/2330 train_time:97434ms step_avg:58.45ms
step:1668/2330 train_time:97494ms step_avg:58.45ms
step:1669/2330 train_time:97551ms step_avg:58.45ms
step:1670/2330 train_time:97613ms step_avg:58.45ms
step:1671/2330 train_time:97671ms step_avg:58.45ms
step:1672/2330 train_time:97733ms step_avg:58.45ms
step:1673/2330 train_time:97791ms step_avg:58.45ms
step:1674/2330 train_time:97851ms step_avg:58.45ms
step:1675/2330 train_time:97909ms step_avg:58.45ms
step:1676/2330 train_time:97970ms step_avg:58.45ms
step:1677/2330 train_time:98029ms step_avg:58.45ms
step:1678/2330 train_time:98089ms step_avg:58.46ms
step:1679/2330 train_time:98146ms step_avg:58.45ms
step:1680/2330 train_time:98207ms step_avg:58.46ms
step:1681/2330 train_time:98264ms step_avg:58.46ms
step:1682/2330 train_time:98326ms step_avg:58.46ms
step:1683/2330 train_time:98382ms step_avg:58.46ms
step:1684/2330 train_time:98444ms step_avg:58.46ms
step:1685/2330 train_time:98501ms step_avg:58.46ms
step:1686/2330 train_time:98562ms step_avg:58.46ms
step:1687/2330 train_time:98619ms step_avg:58.46ms
step:1688/2330 train_time:98680ms step_avg:58.46ms
step:1689/2330 train_time:98736ms step_avg:58.46ms
step:1690/2330 train_time:98798ms step_avg:58.46ms
step:1691/2330 train_time:98855ms step_avg:58.46ms
step:1692/2330 train_time:98917ms step_avg:58.46ms
step:1693/2330 train_time:98975ms step_avg:58.46ms
step:1694/2330 train_time:99035ms step_avg:58.46ms
step:1695/2330 train_time:99093ms step_avg:58.46ms
step:1696/2330 train_time:99153ms step_avg:58.46ms
step:1697/2330 train_time:99212ms step_avg:58.46ms
step:1698/2330 train_time:99272ms step_avg:58.46ms
step:1699/2330 train_time:99331ms step_avg:58.46ms
step:1700/2330 train_time:99391ms step_avg:58.47ms
step:1701/2330 train_time:99448ms step_avg:58.46ms
step:1702/2330 train_time:99510ms step_avg:58.47ms
step:1703/2330 train_time:99567ms step_avg:58.47ms
step:1704/2330 train_time:99629ms step_avg:58.47ms
step:1705/2330 train_time:99685ms step_avg:58.47ms
step:1706/2330 train_time:99747ms step_avg:58.47ms
step:1707/2330 train_time:99804ms step_avg:58.47ms
step:1708/2330 train_time:99865ms step_avg:58.47ms
step:1709/2330 train_time:99923ms step_avg:58.47ms
step:1710/2330 train_time:99983ms step_avg:58.47ms
step:1711/2330 train_time:100040ms step_avg:58.47ms
step:1712/2330 train_time:100100ms step_avg:58.47ms
step:1713/2330 train_time:100158ms step_avg:58.47ms
step:1714/2330 train_time:100218ms step_avg:58.47ms
step:1715/2330 train_time:100275ms step_avg:58.47ms
step:1716/2330 train_time:100338ms step_avg:58.47ms
step:1717/2330 train_time:100395ms step_avg:58.47ms
step:1718/2330 train_time:100456ms step_avg:58.47ms
step:1719/2330 train_time:100513ms step_avg:58.47ms
step:1720/2330 train_time:100575ms step_avg:58.47ms
step:1721/2330 train_time:100631ms step_avg:58.47ms
step:1722/2330 train_time:100693ms step_avg:58.47ms
step:1723/2330 train_time:100750ms step_avg:58.47ms
step:1724/2330 train_time:100811ms step_avg:58.48ms
step:1725/2330 train_time:100870ms step_avg:58.48ms
step:1726/2330 train_time:100930ms step_avg:58.48ms
step:1727/2330 train_time:100989ms step_avg:58.48ms
step:1728/2330 train_time:101049ms step_avg:58.48ms
step:1729/2330 train_time:101106ms step_avg:58.48ms
step:1730/2330 train_time:101168ms step_avg:58.48ms
step:1731/2330 train_time:101225ms step_avg:58.48ms
step:1732/2330 train_time:101286ms step_avg:58.48ms
step:1733/2330 train_time:101342ms step_avg:58.48ms
step:1734/2330 train_time:101404ms step_avg:58.48ms
step:1735/2330 train_time:101461ms step_avg:58.48ms
step:1736/2330 train_time:101522ms step_avg:58.48ms
step:1737/2330 train_time:101578ms step_avg:58.48ms
step:1738/2330 train_time:101639ms step_avg:58.48ms
step:1739/2330 train_time:101695ms step_avg:58.48ms
step:1740/2330 train_time:101758ms step_avg:58.48ms
step:1741/2330 train_time:101815ms step_avg:58.48ms
step:1742/2330 train_time:101876ms step_avg:58.48ms
step:1743/2330 train_time:101933ms step_avg:58.48ms
step:1744/2330 train_time:101994ms step_avg:58.48ms
step:1745/2330 train_time:102052ms step_avg:58.48ms
step:1746/2330 train_time:102113ms step_avg:58.48ms
step:1747/2330 train_time:102171ms step_avg:58.48ms
step:1748/2330 train_time:102232ms step_avg:58.49ms
step:1749/2330 train_time:102290ms step_avg:58.48ms
step:1750/2330 train_time:102351ms step_avg:58.49ms
step:1750/2330 val_loss:3.8251 train_time:102434ms step_avg:58.53ms
step:1751/2330 train_time:102453ms step_avg:58.51ms
step:1752/2330 train_time:102474ms step_avg:58.49ms
step:1753/2330 train_time:102532ms step_avg:58.49ms
step:1754/2330 train_time:102598ms step_avg:58.49ms
step:1755/2330 train_time:102655ms step_avg:58.49ms
step:1756/2330 train_time:102719ms step_avg:58.50ms
step:1757/2330 train_time:102775ms step_avg:58.49ms
step:1758/2330 train_time:102836ms step_avg:58.50ms
step:1759/2330 train_time:102893ms step_avg:58.50ms
step:1760/2330 train_time:102953ms step_avg:58.50ms
step:1761/2330 train_time:103010ms step_avg:58.49ms
step:1762/2330 train_time:103070ms step_avg:58.50ms
step:1763/2330 train_time:103126ms step_avg:58.49ms
step:1764/2330 train_time:103186ms step_avg:58.50ms
step:1765/2330 train_time:103242ms step_avg:58.49ms
step:1766/2330 train_time:103302ms step_avg:58.49ms
step:1767/2330 train_time:103361ms step_avg:58.50ms
step:1768/2330 train_time:103425ms step_avg:58.50ms
step:1769/2330 train_time:103483ms step_avg:58.50ms
step:1770/2330 train_time:103546ms step_avg:58.50ms
step:1771/2330 train_time:103603ms step_avg:58.50ms
step:1772/2330 train_time:103666ms step_avg:58.50ms
step:1773/2330 train_time:103723ms step_avg:58.50ms
step:1774/2330 train_time:103785ms step_avg:58.50ms
step:1775/2330 train_time:103841ms step_avg:58.50ms
step:1776/2330 train_time:103904ms step_avg:58.50ms
step:1777/2330 train_time:103961ms step_avg:58.50ms
step:1778/2330 train_time:104022ms step_avg:58.51ms
step:1779/2330 train_time:104080ms step_avg:58.50ms
step:1780/2330 train_time:104140ms step_avg:58.51ms
step:1781/2330 train_time:104197ms step_avg:58.50ms
step:1782/2330 train_time:104257ms step_avg:58.51ms
step:1783/2330 train_time:104315ms step_avg:58.51ms
step:1784/2330 train_time:104377ms step_avg:58.51ms
step:1785/2330 train_time:104436ms step_avg:58.51ms
step:1786/2330 train_time:104499ms step_avg:58.51ms
step:1787/2330 train_time:104557ms step_avg:58.51ms
step:1788/2330 train_time:104618ms step_avg:58.51ms
step:1789/2330 train_time:104676ms step_avg:58.51ms
step:1790/2330 train_time:104737ms step_avg:58.51ms
step:1791/2330 train_time:104795ms step_avg:58.51ms
step:1792/2330 train_time:104855ms step_avg:58.51ms
step:1793/2330 train_time:104912ms step_avg:58.51ms
step:1794/2330 train_time:104972ms step_avg:58.51ms
step:1795/2330 train_time:105029ms step_avg:58.51ms
step:1796/2330 train_time:105089ms step_avg:58.51ms
step:1797/2330 train_time:105145ms step_avg:58.51ms
step:1798/2330 train_time:105206ms step_avg:58.51ms
step:1799/2330 train_time:105263ms step_avg:58.51ms
step:1800/2330 train_time:105325ms step_avg:58.51ms
step:1801/2330 train_time:105383ms step_avg:58.51ms
step:1802/2330 train_time:105444ms step_avg:58.51ms
step:1803/2330 train_time:105502ms step_avg:58.51ms
step:1804/2330 train_time:105562ms step_avg:58.52ms
step:1805/2330 train_time:105620ms step_avg:58.52ms
step:1806/2330 train_time:105682ms step_avg:58.52ms
step:1807/2330 train_time:105739ms step_avg:58.52ms
step:1808/2330 train_time:105801ms step_avg:58.52ms
step:1809/2330 train_time:105858ms step_avg:58.52ms
step:1810/2330 train_time:105921ms step_avg:58.52ms
step:1811/2330 train_time:105979ms step_avg:58.52ms
step:1812/2330 train_time:106039ms step_avg:58.52ms
step:1813/2330 train_time:106096ms step_avg:58.52ms
step:1814/2330 train_time:106156ms step_avg:58.52ms
step:1815/2330 train_time:106213ms step_avg:58.52ms
step:1816/2330 train_time:106273ms step_avg:58.52ms
step:1817/2330 train_time:106331ms step_avg:58.52ms
step:1818/2330 train_time:106393ms step_avg:58.52ms
step:1819/2330 train_time:106451ms step_avg:58.52ms
step:1820/2330 train_time:106511ms step_avg:58.52ms
step:1821/2330 train_time:106569ms step_avg:58.52ms
step:1822/2330 train_time:106629ms step_avg:58.52ms
step:1823/2330 train_time:106687ms step_avg:58.52ms
step:1824/2330 train_time:106749ms step_avg:58.52ms
step:1825/2330 train_time:106805ms step_avg:58.52ms
step:1826/2330 train_time:106866ms step_avg:58.52ms
step:1827/2330 train_time:106923ms step_avg:58.52ms
step:1828/2330 train_time:106984ms step_avg:58.53ms
step:1829/2330 train_time:107040ms step_avg:58.52ms
step:1830/2330 train_time:107103ms step_avg:58.53ms
step:1831/2330 train_time:107159ms step_avg:58.53ms
step:1832/2330 train_time:107222ms step_avg:58.53ms
step:1833/2330 train_time:107279ms step_avg:58.53ms
step:1834/2330 train_time:107340ms step_avg:58.53ms
step:1835/2330 train_time:107397ms step_avg:58.53ms
step:1836/2330 train_time:107458ms step_avg:58.53ms
step:1837/2330 train_time:107517ms step_avg:58.53ms
step:1838/2330 train_time:107578ms step_avg:58.53ms
step:1839/2330 train_time:107637ms step_avg:58.53ms
step:1840/2330 train_time:107698ms step_avg:58.53ms
step:1841/2330 train_time:107755ms step_avg:58.53ms
step:1842/2330 train_time:107817ms step_avg:58.53ms
step:1843/2330 train_time:107876ms step_avg:58.53ms
step:1844/2330 train_time:107936ms step_avg:58.53ms
step:1845/2330 train_time:107993ms step_avg:58.53ms
step:1846/2330 train_time:108054ms step_avg:58.53ms
step:1847/2330 train_time:108110ms step_avg:58.53ms
step:1848/2330 train_time:108171ms step_avg:58.53ms
step:1849/2330 train_time:108228ms step_avg:58.53ms
step:1850/2330 train_time:108290ms step_avg:58.53ms
step:1851/2330 train_time:108346ms step_avg:58.53ms
step:1852/2330 train_time:108407ms step_avg:58.54ms
step:1853/2330 train_time:108464ms step_avg:58.53ms
step:1854/2330 train_time:108526ms step_avg:58.54ms
step:1855/2330 train_time:108583ms step_avg:58.54ms
step:1856/2330 train_time:108644ms step_avg:58.54ms
step:1857/2330 train_time:108702ms step_avg:58.54ms
step:1858/2330 train_time:108764ms step_avg:58.54ms
step:1859/2330 train_time:108822ms step_avg:58.54ms
step:1860/2330 train_time:108883ms step_avg:58.54ms
step:1861/2330 train_time:108940ms step_avg:58.54ms
step:1862/2330 train_time:109002ms step_avg:58.54ms
step:1863/2330 train_time:109059ms step_avg:58.54ms
step:1864/2330 train_time:109120ms step_avg:58.54ms
step:1865/2330 train_time:109178ms step_avg:58.54ms
step:1866/2330 train_time:109238ms step_avg:58.54ms
step:1867/2330 train_time:109296ms step_avg:58.54ms
step:1868/2330 train_time:109357ms step_avg:58.54ms
step:1869/2330 train_time:109415ms step_avg:58.54ms
step:1870/2330 train_time:109476ms step_avg:58.54ms
step:1871/2330 train_time:109535ms step_avg:58.54ms
step:1872/2330 train_time:109595ms step_avg:58.54ms
step:1873/2330 train_time:109653ms step_avg:58.54ms
step:1874/2330 train_time:109713ms step_avg:58.54ms
step:1875/2330 train_time:109771ms step_avg:58.54ms
step:1876/2330 train_time:109832ms step_avg:58.55ms
step:1877/2330 train_time:109889ms step_avg:58.55ms
step:1878/2330 train_time:109951ms step_avg:58.55ms
step:1879/2330 train_time:110008ms step_avg:58.55ms
step:1880/2330 train_time:110069ms step_avg:58.55ms
step:1881/2330 train_time:110126ms step_avg:58.55ms
step:1882/2330 train_time:110186ms step_avg:58.55ms
step:1883/2330 train_time:110243ms step_avg:58.55ms
step:1884/2330 train_time:110304ms step_avg:58.55ms
step:1885/2330 train_time:110361ms step_avg:58.55ms
step:1886/2330 train_time:110423ms step_avg:58.55ms
step:1887/2330 train_time:110480ms step_avg:58.55ms
step:1888/2330 train_time:110540ms step_avg:58.55ms
step:1889/2330 train_time:110598ms step_avg:58.55ms
step:1890/2330 train_time:110659ms step_avg:58.55ms
step:1891/2330 train_time:110718ms step_avg:58.55ms
step:1892/2330 train_time:110779ms step_avg:58.55ms
step:1893/2330 train_time:110837ms step_avg:58.55ms
step:1894/2330 train_time:110899ms step_avg:58.55ms
step:1895/2330 train_time:110956ms step_avg:58.55ms
step:1896/2330 train_time:111016ms step_avg:58.55ms
step:1897/2330 train_time:111075ms step_avg:58.55ms
step:1898/2330 train_time:111136ms step_avg:58.55ms
step:1899/2330 train_time:111194ms step_avg:58.55ms
step:1900/2330 train_time:111255ms step_avg:58.56ms
step:1901/2330 train_time:111312ms step_avg:58.55ms
step:1902/2330 train_time:111372ms step_avg:58.56ms
step:1903/2330 train_time:111429ms step_avg:58.55ms
step:1904/2330 train_time:111490ms step_avg:58.56ms
step:1905/2330 train_time:111547ms step_avg:58.55ms
step:1906/2330 train_time:111608ms step_avg:58.56ms
step:1907/2330 train_time:111665ms step_avg:58.56ms
step:1908/2330 train_time:111727ms step_avg:58.56ms
step:1909/2330 train_time:111783ms step_avg:58.56ms
step:1910/2330 train_time:111844ms step_avg:58.56ms
step:1911/2330 train_time:111902ms step_avg:58.56ms
step:1912/2330 train_time:111963ms step_avg:58.56ms
step:1913/2330 train_time:112021ms step_avg:58.56ms
step:1914/2330 train_time:112082ms step_avg:58.56ms
step:1915/2330 train_time:112139ms step_avg:58.56ms
step:1916/2330 train_time:112201ms step_avg:58.56ms
step:1917/2330 train_time:112259ms step_avg:58.56ms
step:1918/2330 train_time:112320ms step_avg:58.56ms
step:1919/2330 train_time:112377ms step_avg:58.56ms
step:1920/2330 train_time:112438ms step_avg:58.56ms
step:1921/2330 train_time:112497ms step_avg:58.56ms
step:1922/2330 train_time:112558ms step_avg:58.56ms
step:1923/2330 train_time:112615ms step_avg:58.56ms
step:1924/2330 train_time:112675ms step_avg:58.56ms
step:1925/2330 train_time:112733ms step_avg:58.56ms
step:1926/2330 train_time:112794ms step_avg:58.56ms
step:1927/2330 train_time:112852ms step_avg:58.56ms
step:1928/2330 train_time:112912ms step_avg:58.56ms
step:1929/2330 train_time:112969ms step_avg:58.56ms
step:1930/2330 train_time:113030ms step_avg:58.56ms
step:1931/2330 train_time:113087ms step_avg:58.56ms
step:1932/2330 train_time:113147ms step_avg:58.56ms
step:1933/2330 train_time:113204ms step_avg:58.56ms
step:1934/2330 train_time:113265ms step_avg:58.57ms
step:1935/2330 train_time:113323ms step_avg:58.56ms
step:1936/2330 train_time:113383ms step_avg:58.57ms
step:1937/2330 train_time:113440ms step_avg:58.56ms
step:1938/2330 train_time:113501ms step_avg:58.57ms
step:1939/2330 train_time:113559ms step_avg:58.57ms
step:1940/2330 train_time:113621ms step_avg:58.57ms
step:1941/2330 train_time:113679ms step_avg:58.57ms
step:1942/2330 train_time:113739ms step_avg:58.57ms
step:1943/2330 train_time:113797ms step_avg:58.57ms
step:1944/2330 train_time:113858ms step_avg:58.57ms
step:1945/2330 train_time:113916ms step_avg:58.57ms
step:1946/2330 train_time:113977ms step_avg:58.57ms
step:1947/2330 train_time:114035ms step_avg:58.57ms
step:1948/2330 train_time:114095ms step_avg:58.57ms
step:1949/2330 train_time:114153ms step_avg:58.57ms
step:1950/2330 train_time:114213ms step_avg:58.57ms
step:1951/2330 train_time:114270ms step_avg:58.57ms
step:1952/2330 train_time:114332ms step_avg:58.57ms
step:1953/2330 train_time:114389ms step_avg:58.57ms
step:1954/2330 train_time:114450ms step_avg:58.57ms
step:1955/2330 train_time:114507ms step_avg:58.57ms
step:1956/2330 train_time:114568ms step_avg:58.57ms
step:1957/2330 train_time:114625ms step_avg:58.57ms
step:1958/2330 train_time:114687ms step_avg:58.57ms
step:1959/2330 train_time:114743ms step_avg:58.57ms
step:1960/2330 train_time:114805ms step_avg:58.57ms
step:1961/2330 train_time:114861ms step_avg:58.57ms
step:1962/2330 train_time:114923ms step_avg:58.57ms
step:1963/2330 train_time:114981ms step_avg:58.57ms
step:1964/2330 train_time:115041ms step_avg:58.57ms
step:1965/2330 train_time:115098ms step_avg:58.57ms
step:1966/2330 train_time:115159ms step_avg:58.58ms
step:1967/2330 train_time:115217ms step_avg:58.58ms
step:1968/2330 train_time:115280ms step_avg:58.58ms
step:1969/2330 train_time:115337ms step_avg:58.58ms
step:1970/2330 train_time:115398ms step_avg:58.58ms
step:1971/2330 train_time:115456ms step_avg:58.58ms
step:1972/2330 train_time:115517ms step_avg:58.58ms
step:1973/2330 train_time:115575ms step_avg:58.58ms
step:1974/2330 train_time:115635ms step_avg:58.58ms
step:1975/2330 train_time:115693ms step_avg:58.58ms
step:1976/2330 train_time:115753ms step_avg:58.58ms
step:1977/2330 train_time:115811ms step_avg:58.58ms
step:1978/2330 train_time:115872ms step_avg:58.58ms
step:1979/2330 train_time:115930ms step_avg:58.58ms
step:1980/2330 train_time:115991ms step_avg:58.58ms
step:1981/2330 train_time:116048ms step_avg:58.58ms
step:1982/2330 train_time:116108ms step_avg:58.58ms
step:1983/2330 train_time:116165ms step_avg:58.58ms
step:1984/2330 train_time:116226ms step_avg:58.58ms
step:1985/2330 train_time:116283ms step_avg:58.58ms
step:1986/2330 train_time:116344ms step_avg:58.58ms
step:1987/2330 train_time:116401ms step_avg:58.58ms
step:1988/2330 train_time:116465ms step_avg:58.58ms
step:1989/2330 train_time:116521ms step_avg:58.58ms
step:1990/2330 train_time:116583ms step_avg:58.58ms
step:1991/2330 train_time:116641ms step_avg:58.58ms
step:1992/2330 train_time:116703ms step_avg:58.59ms
step:1993/2330 train_time:116761ms step_avg:58.59ms
step:1994/2330 train_time:116822ms step_avg:58.59ms
step:1995/2330 train_time:116881ms step_avg:58.59ms
step:1996/2330 train_time:116943ms step_avg:58.59ms
step:1997/2330 train_time:117001ms step_avg:58.59ms
step:1998/2330 train_time:117061ms step_avg:58.59ms
step:1999/2330 train_time:117119ms step_avg:58.59ms
step:2000/2330 train_time:117179ms step_avg:58.59ms
step:2000/2330 val_loss:3.7640 train_time:117260ms step_avg:58.63ms
step:2001/2330 train_time:117278ms step_avg:58.61ms
step:2002/2330 train_time:117299ms step_avg:58.59ms
step:2003/2330 train_time:117357ms step_avg:58.59ms
step:2004/2330 train_time:117422ms step_avg:58.59ms
step:2005/2330 train_time:117478ms step_avg:58.59ms
step:2006/2330 train_time:117540ms step_avg:58.59ms
step:2007/2330 train_time:117597ms step_avg:58.59ms
step:2008/2330 train_time:117657ms step_avg:58.59ms
step:2009/2330 train_time:117714ms step_avg:58.59ms
step:2010/2330 train_time:117775ms step_avg:58.59ms
step:2011/2330 train_time:117831ms step_avg:58.59ms
step:2012/2330 train_time:117891ms step_avg:58.59ms
step:2013/2330 train_time:117947ms step_avg:58.59ms
step:2014/2330 train_time:118009ms step_avg:58.59ms
step:2015/2330 train_time:118065ms step_avg:58.59ms
step:2016/2330 train_time:118126ms step_avg:58.59ms
step:2017/2330 train_time:118183ms step_avg:58.59ms
step:2018/2330 train_time:118245ms step_avg:58.60ms
step:2019/2330 train_time:118305ms step_avg:58.60ms
step:2020/2330 train_time:118366ms step_avg:58.60ms
step:2021/2330 train_time:118426ms step_avg:58.60ms
step:2022/2330 train_time:118488ms step_avg:58.60ms
step:2023/2330 train_time:118546ms step_avg:58.60ms
step:2024/2330 train_time:118606ms step_avg:58.60ms
step:2025/2330 train_time:118663ms step_avg:58.60ms
step:2026/2330 train_time:118723ms step_avg:58.60ms
step:2027/2330 train_time:118780ms step_avg:58.60ms
step:2028/2330 train_time:118840ms step_avg:58.60ms
step:2029/2330 train_time:118897ms step_avg:58.60ms
step:2030/2330 train_time:118957ms step_avg:58.60ms
step:2031/2330 train_time:119013ms step_avg:58.60ms
step:2032/2330 train_time:119074ms step_avg:58.60ms
step:2033/2330 train_time:119131ms step_avg:58.60ms
step:2034/2330 train_time:119192ms step_avg:58.60ms
step:2035/2330 train_time:119250ms step_avg:58.60ms
step:2036/2330 train_time:119313ms step_avg:58.60ms
step:2037/2330 train_time:119370ms step_avg:58.60ms
step:2038/2330 train_time:119433ms step_avg:58.60ms
step:2039/2330 train_time:119491ms step_avg:58.60ms
step:2040/2330 train_time:119553ms step_avg:58.60ms
step:2041/2330 train_time:119611ms step_avg:58.60ms
step:2042/2330 train_time:119671ms step_avg:58.60ms
step:2043/2330 train_time:119730ms step_avg:58.60ms
step:2044/2330 train_time:119789ms step_avg:58.61ms
step:2045/2330 train_time:119847ms step_avg:58.60ms
step:2046/2330 train_time:119908ms step_avg:58.61ms
step:2047/2330 train_time:119965ms step_avg:58.61ms
step:2048/2330 train_time:120024ms step_avg:58.61ms
step:2049/2330 train_time:120082ms step_avg:58.61ms
step:2050/2330 train_time:120142ms step_avg:58.61ms
step:2051/2330 train_time:120199ms step_avg:58.60ms
step:2052/2330 train_time:120260ms step_avg:58.61ms
step:2053/2330 train_time:120318ms step_avg:58.61ms
step:2054/2330 train_time:120379ms step_avg:58.61ms
step:2055/2330 train_time:120436ms step_avg:58.61ms
step:2056/2330 train_time:120498ms step_avg:58.61ms
step:2057/2330 train_time:120555ms step_avg:58.61ms
step:2058/2330 train_time:120616ms step_avg:58.61ms
step:2059/2330 train_time:120673ms step_avg:58.61ms
step:2060/2330 train_time:120735ms step_avg:58.61ms
step:2061/2330 train_time:120792ms step_avg:58.61ms
step:2062/2330 train_time:120853ms step_avg:58.61ms
step:2063/2330 train_time:120910ms step_avg:58.61ms
step:2064/2330 train_time:120970ms step_avg:58.61ms
step:2065/2330 train_time:121028ms step_avg:58.61ms
step:2066/2330 train_time:121088ms step_avg:58.61ms
step:2067/2330 train_time:121145ms step_avg:58.61ms
step:2068/2330 train_time:121206ms step_avg:58.61ms
step:2069/2330 train_time:121265ms step_avg:58.61ms
step:2070/2330 train_time:121325ms step_avg:58.61ms
step:2071/2330 train_time:121384ms step_avg:58.61ms
step:2072/2330 train_time:121444ms step_avg:58.61ms
step:2073/2330 train_time:121502ms step_avg:58.61ms
step:2074/2330 train_time:121564ms step_avg:58.61ms
step:2075/2330 train_time:121621ms step_avg:58.61ms
step:2076/2330 train_time:121683ms step_avg:58.61ms
step:2077/2330 train_time:121740ms step_avg:58.61ms
step:2078/2330 train_time:121801ms step_avg:58.61ms
step:2079/2330 train_time:121857ms step_avg:58.61ms
step:2080/2330 train_time:121919ms step_avg:58.61ms
step:2081/2330 train_time:121975ms step_avg:58.61ms
step:2082/2330 train_time:122037ms step_avg:58.62ms
step:2083/2330 train_time:122094ms step_avg:58.61ms
step:2084/2330 train_time:122154ms step_avg:58.62ms
step:2085/2330 train_time:122211ms step_avg:58.61ms
step:2086/2330 train_time:122273ms step_avg:58.62ms
step:2087/2330 train_time:122332ms step_avg:58.62ms
step:2088/2330 train_time:122394ms step_avg:58.62ms
step:2089/2330 train_time:122452ms step_avg:58.62ms
step:2090/2330 train_time:122513ms step_avg:58.62ms
step:2091/2330 train_time:122571ms step_avg:58.62ms
step:2092/2330 train_time:122632ms step_avg:58.62ms
step:2093/2330 train_time:122690ms step_avg:58.62ms
step:2094/2330 train_time:122751ms step_avg:58.62ms
step:2095/2330 train_time:122809ms step_avg:58.62ms
step:2096/2330 train_time:122869ms step_avg:58.62ms
step:2097/2330 train_time:122927ms step_avg:58.62ms
step:2098/2330 train_time:122987ms step_avg:58.62ms
step:2099/2330 train_time:123044ms step_avg:58.62ms
step:2100/2330 train_time:123105ms step_avg:58.62ms
step:2101/2330 train_time:123162ms step_avg:58.62ms
step:2102/2330 train_time:123223ms step_avg:58.62ms
step:2103/2330 train_time:123280ms step_avg:58.62ms
step:2104/2330 train_time:123342ms step_avg:58.62ms
step:2105/2330 train_time:123400ms step_avg:58.62ms
step:2106/2330 train_time:123459ms step_avg:58.62ms
step:2107/2330 train_time:123517ms step_avg:58.62ms
step:2108/2330 train_time:123578ms step_avg:58.62ms
step:2109/2330 train_time:123635ms step_avg:58.62ms
step:2110/2330 train_time:123697ms step_avg:58.62ms
step:2111/2330 train_time:123754ms step_avg:58.62ms
step:2112/2330 train_time:123815ms step_avg:58.62ms
step:2113/2330 train_time:123872ms step_avg:58.62ms
step:2114/2330 train_time:123933ms step_avg:58.62ms
step:2115/2330 train_time:123991ms step_avg:58.62ms
step:2116/2330 train_time:124050ms step_avg:58.62ms
step:2117/2330 train_time:124108ms step_avg:58.62ms
step:2118/2330 train_time:124168ms step_avg:58.63ms
step:2119/2330 train_time:124226ms step_avg:58.63ms
step:2120/2330 train_time:124288ms step_avg:58.63ms
step:2121/2330 train_time:124346ms step_avg:58.63ms
step:2122/2330 train_time:124407ms step_avg:58.63ms
step:2123/2330 train_time:124465ms step_avg:58.63ms
step:2124/2330 train_time:124526ms step_avg:58.63ms
step:2125/2330 train_time:124584ms step_avg:58.63ms
step:2126/2330 train_time:124644ms step_avg:58.63ms
step:2127/2330 train_time:124701ms step_avg:58.63ms
step:2128/2330 train_time:124763ms step_avg:58.63ms
step:2129/2330 train_time:124820ms step_avg:58.63ms
step:2130/2330 train_time:124882ms step_avg:58.63ms
step:2131/2330 train_time:124939ms step_avg:58.63ms
step:2132/2330 train_time:124999ms step_avg:58.63ms
step:2133/2330 train_time:125056ms step_avg:58.63ms
step:2134/2330 train_time:125117ms step_avg:58.63ms
step:2135/2330 train_time:125174ms step_avg:58.63ms
step:2136/2330 train_time:125236ms step_avg:58.63ms
step:2137/2330 train_time:125293ms step_avg:58.63ms
step:2138/2330 train_time:125354ms step_avg:58.63ms
step:2139/2330 train_time:125411ms step_avg:58.63ms
step:2140/2330 train_time:125472ms step_avg:58.63ms
step:2141/2330 train_time:125529ms step_avg:58.63ms
step:2142/2330 train_time:125591ms step_avg:58.63ms
step:2143/2330 train_time:125651ms step_avg:58.63ms
step:2144/2330 train_time:125711ms step_avg:58.63ms
step:2145/2330 train_time:125769ms step_avg:58.63ms
step:2146/2330 train_time:125829ms step_avg:58.63ms
step:2147/2330 train_time:125888ms step_avg:58.63ms
step:2148/2330 train_time:125948ms step_avg:58.64ms
step:2149/2330 train_time:126006ms step_avg:58.63ms
step:2150/2330 train_time:126066ms step_avg:58.64ms
step:2151/2330 train_time:126123ms step_avg:58.63ms
step:2152/2330 train_time:126184ms step_avg:58.64ms
step:2153/2330 train_time:126241ms step_avg:58.63ms
step:2154/2330 train_time:126302ms step_avg:58.64ms
step:2155/2330 train_time:126359ms step_avg:58.64ms
step:2156/2330 train_time:126420ms step_avg:58.64ms
step:2157/2330 train_time:126477ms step_avg:58.64ms
step:2158/2330 train_time:126538ms step_avg:58.64ms
step:2159/2330 train_time:126596ms step_avg:58.64ms
step:2160/2330 train_time:126657ms step_avg:58.64ms
step:2161/2330 train_time:126714ms step_avg:58.64ms
step:2162/2330 train_time:126775ms step_avg:58.64ms
step:2163/2330 train_time:126832ms step_avg:58.64ms
step:2164/2330 train_time:126893ms step_avg:58.64ms
step:2165/2330 train_time:126951ms step_avg:58.64ms
step:2166/2330 train_time:127011ms step_avg:58.64ms
step:2167/2330 train_time:127069ms step_avg:58.64ms
step:2168/2330 train_time:127129ms step_avg:58.64ms
step:2169/2330 train_time:127188ms step_avg:58.64ms
step:2170/2330 train_time:127249ms step_avg:58.64ms
step:2171/2330 train_time:127307ms step_avg:58.64ms
step:2172/2330 train_time:127367ms step_avg:58.64ms
step:2173/2330 train_time:127425ms step_avg:58.64ms
step:2174/2330 train_time:127485ms step_avg:58.64ms
step:2175/2330 train_time:127543ms step_avg:58.64ms
step:2176/2330 train_time:127604ms step_avg:58.64ms
step:2177/2330 train_time:127661ms step_avg:58.64ms
step:2178/2330 train_time:127722ms step_avg:58.64ms
step:2179/2330 train_time:127779ms step_avg:58.64ms
step:2180/2330 train_time:127840ms step_avg:58.64ms
step:2181/2330 train_time:127897ms step_avg:58.64ms
step:2182/2330 train_time:127958ms step_avg:58.64ms
step:2183/2330 train_time:128015ms step_avg:58.64ms
step:2184/2330 train_time:128076ms step_avg:58.64ms
step:2185/2330 train_time:128133ms step_avg:58.64ms
step:2186/2330 train_time:128194ms step_avg:58.64ms
step:2187/2330 train_time:128252ms step_avg:58.64ms
step:2188/2330 train_time:128313ms step_avg:58.64ms
step:2189/2330 train_time:128370ms step_avg:58.64ms
step:2190/2330 train_time:128433ms step_avg:58.65ms
step:2191/2330 train_time:128491ms step_avg:58.64ms
step:2192/2330 train_time:128551ms step_avg:58.65ms
step:2193/2330 train_time:128609ms step_avg:58.65ms
step:2194/2330 train_time:128671ms step_avg:58.65ms
step:2195/2330 train_time:128729ms step_avg:58.65ms
step:2196/2330 train_time:128789ms step_avg:58.65ms
step:2197/2330 train_time:128847ms step_avg:58.65ms
step:2198/2330 train_time:128907ms step_avg:58.65ms
step:2199/2330 train_time:128965ms step_avg:58.65ms
step:2200/2330 train_time:129026ms step_avg:58.65ms
step:2201/2330 train_time:129084ms step_avg:58.65ms
step:2202/2330 train_time:129145ms step_avg:58.65ms
step:2203/2330 train_time:129202ms step_avg:58.65ms
step:2204/2330 train_time:129263ms step_avg:58.65ms
step:2205/2330 train_time:129321ms step_avg:58.65ms
step:2206/2330 train_time:129380ms step_avg:58.65ms
step:2207/2330 train_time:129438ms step_avg:58.65ms
step:2208/2330 train_time:129498ms step_avg:58.65ms
step:2209/2330 train_time:129554ms step_avg:58.65ms
step:2210/2330 train_time:129616ms step_avg:58.65ms
step:2211/2330 train_time:129673ms step_avg:58.65ms
step:2212/2330 train_time:129735ms step_avg:58.65ms
step:2213/2330 train_time:129792ms step_avg:58.65ms
step:2214/2330 train_time:129853ms step_avg:58.65ms
step:2215/2330 train_time:129910ms step_avg:58.65ms
step:2216/2330 train_time:129971ms step_avg:58.65ms
step:2217/2330 train_time:130028ms step_avg:58.65ms
step:2218/2330 train_time:130089ms step_avg:58.65ms
step:2219/2330 train_time:130146ms step_avg:58.65ms
step:2220/2330 train_time:130207ms step_avg:58.65ms
step:2221/2330 train_time:130265ms step_avg:58.65ms
step:2222/2330 train_time:130324ms step_avg:58.65ms
step:2223/2330 train_time:130382ms step_avg:58.65ms
step:2224/2330 train_time:130443ms step_avg:58.65ms
step:2225/2330 train_time:130500ms step_avg:58.65ms
step:2226/2330 train_time:130561ms step_avg:58.65ms
step:2227/2330 train_time:130617ms step_avg:58.65ms
step:2228/2330 train_time:130678ms step_avg:58.65ms
step:2229/2330 train_time:130736ms step_avg:58.65ms
step:2230/2330 train_time:130797ms step_avg:58.65ms
step:2231/2330 train_time:130853ms step_avg:58.65ms
step:2232/2330 train_time:130915ms step_avg:58.65ms
step:2233/2330 train_time:130972ms step_avg:58.65ms
step:2234/2330 train_time:131034ms step_avg:58.65ms
step:2235/2330 train_time:131091ms step_avg:58.65ms
step:2236/2330 train_time:131152ms step_avg:58.65ms
step:2237/2330 train_time:131210ms step_avg:58.65ms
step:2238/2330 train_time:131271ms step_avg:58.66ms
step:2239/2330 train_time:131329ms step_avg:58.66ms
step:2240/2330 train_time:131390ms step_avg:58.66ms
step:2241/2330 train_time:131449ms step_avg:58.66ms
step:2242/2330 train_time:131509ms step_avg:58.66ms
step:2243/2330 train_time:131566ms step_avg:58.66ms
step:2244/2330 train_time:131627ms step_avg:58.66ms
step:2245/2330 train_time:131685ms step_avg:58.66ms
step:2246/2330 train_time:131746ms step_avg:58.66ms
step:2247/2330 train_time:131804ms step_avg:58.66ms
step:2248/2330 train_time:131865ms step_avg:58.66ms
step:2249/2330 train_time:131921ms step_avg:58.66ms
step:2250/2330 train_time:131983ms step_avg:58.66ms
step:2250/2330 val_loss:3.7151 train_time:132064ms step_avg:58.70ms
step:2251/2330 train_time:132083ms step_avg:58.68ms
step:2252/2330 train_time:132105ms step_avg:58.66ms
step:2253/2330 train_time:132163ms step_avg:58.66ms
step:2254/2330 train_time:132231ms step_avg:58.66ms
step:2255/2330 train_time:132288ms step_avg:58.66ms
step:2256/2330 train_time:132352ms step_avg:58.67ms
step:2257/2330 train_time:132408ms step_avg:58.67ms
step:2258/2330 train_time:132470ms step_avg:58.67ms
step:2259/2330 train_time:132527ms step_avg:58.67ms
step:2260/2330 train_time:132587ms step_avg:58.67ms
step:2261/2330 train_time:132645ms step_avg:58.67ms
step:2262/2330 train_time:132705ms step_avg:58.67ms
step:2263/2330 train_time:132762ms step_avg:58.67ms
step:2264/2330 train_time:132821ms step_avg:58.67ms
step:2265/2330 train_time:132878ms step_avg:58.67ms
step:2266/2330 train_time:132937ms step_avg:58.67ms
step:2267/2330 train_time:132995ms step_avg:58.67ms
step:2268/2330 train_time:133057ms step_avg:58.67ms
step:2269/2330 train_time:133116ms step_avg:58.67ms
step:2270/2330 train_time:133177ms step_avg:58.67ms
step:2271/2330 train_time:133235ms step_avg:58.67ms
step:2272/2330 train_time:133299ms step_avg:58.67ms
step:2273/2330 train_time:133356ms step_avg:58.67ms
step:2274/2330 train_time:133417ms step_avg:58.67ms
step:2275/2330 train_time:133473ms step_avg:58.67ms
step:2276/2330 train_time:133535ms step_avg:58.67ms
step:2277/2330 train_time:133592ms step_avg:58.67ms
step:2278/2330 train_time:133652ms step_avg:58.67ms
step:2279/2330 train_time:133709ms step_avg:58.67ms
step:2280/2330 train_time:133770ms step_avg:58.67ms
step:2281/2330 train_time:133827ms step_avg:58.67ms
step:2282/2330 train_time:133888ms step_avg:58.67ms
step:2283/2330 train_time:133946ms step_avg:58.67ms
step:2284/2330 train_time:134007ms step_avg:58.67ms
step:2285/2330 train_time:134067ms step_avg:58.67ms
step:2286/2330 train_time:134128ms step_avg:58.67ms
step:2287/2330 train_time:134187ms step_avg:58.67ms
step:2288/2330 train_time:134249ms step_avg:58.68ms
step:2289/2330 train_time:134307ms step_avg:58.68ms
step:2290/2330 train_time:134368ms step_avg:58.68ms
step:2291/2330 train_time:134425ms step_avg:58.68ms
step:2292/2330 train_time:134487ms step_avg:58.68ms
step:2293/2330 train_time:134544ms step_avg:58.68ms
step:2294/2330 train_time:134605ms step_avg:58.68ms
step:2295/2330 train_time:134662ms step_avg:58.68ms
step:2296/2330 train_time:134722ms step_avg:58.68ms
step:2297/2330 train_time:134778ms step_avg:58.68ms
step:2298/2330 train_time:134838ms step_avg:58.68ms
step:2299/2330 train_time:134895ms step_avg:58.68ms
step:2300/2330 train_time:134956ms step_avg:58.68ms
step:2301/2330 train_time:135014ms step_avg:58.68ms
step:2302/2330 train_time:135075ms step_avg:58.68ms
step:2303/2330 train_time:135134ms step_avg:58.68ms
step:2304/2330 train_time:135195ms step_avg:58.68ms
step:2305/2330 train_time:135254ms step_avg:58.68ms
step:2306/2330 train_time:135314ms step_avg:58.68ms
step:2307/2330 train_time:135372ms step_avg:58.68ms
step:2308/2330 train_time:135432ms step_avg:58.68ms
step:2309/2330 train_time:135490ms step_avg:58.68ms
step:2310/2330 train_time:135551ms step_avg:58.68ms
step:2311/2330 train_time:135609ms step_avg:58.68ms
step:2312/2330 train_time:135670ms step_avg:58.68ms
step:2313/2330 train_time:135727ms step_avg:58.68ms
step:2314/2330 train_time:135788ms step_avg:58.68ms
step:2315/2330 train_time:135845ms step_avg:58.68ms
step:2316/2330 train_time:135905ms step_avg:58.68ms
step:2317/2330 train_time:135962ms step_avg:58.68ms
step:2318/2330 train_time:136022ms step_avg:58.68ms
step:2319/2330 train_time:136079ms step_avg:58.68ms
step:2320/2330 train_time:136141ms step_avg:58.68ms
step:2321/2330 train_time:136199ms step_avg:58.68ms
step:2322/2330 train_time:136259ms step_avg:58.68ms
step:2323/2330 train_time:136317ms step_avg:58.68ms
step:2324/2330 train_time:136378ms step_avg:58.68ms
step:2325/2330 train_time:136436ms step_avg:58.68ms
step:2326/2330 train_time:136497ms step_avg:58.68ms
step:2327/2330 train_time:136554ms step_avg:58.68ms
step:2328/2330 train_time:136614ms step_avg:58.68ms
step:2329/2330 train_time:136672ms step_avg:58.68ms
step:2330/2330 train_time:136733ms step_avg:58.68ms
step:2330/2330 val_loss:3.6997 train_time:136814ms step_avg:58.72ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
