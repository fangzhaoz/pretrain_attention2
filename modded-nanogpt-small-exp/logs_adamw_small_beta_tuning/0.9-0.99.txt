import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--beta1', type=str, required=True)
parser.add_argument('--beta2', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_beta_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=6e-4,  betas=(float(args2.beta1), float(args2.beta2)), eps=1e-10, weight_decay=0.0, fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt", exist_ok=True)
            torch.save(log, f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 03:43:29 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   27C    P0             113W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   26C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   25C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   25C    P0             114W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   24C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   25C    P0             112W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   24C    P0             109W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   24C    P0             109W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:86ms step_avg:85.86ms
step:2/2330 train_time:189ms step_avg:94.58ms
step:3/2330 train_time:207ms step_avg:69.14ms
step:4/2330 train_time:227ms step_avg:56.65ms
step:5/2330 train_time:280ms step_avg:55.97ms
step:6/2330 train_time:337ms step_avg:56.22ms
step:7/2330 train_time:392ms step_avg:56.05ms
step:8/2330 train_time:451ms step_avg:56.41ms
step:9/2330 train_time:506ms step_avg:56.28ms
step:10/2330 train_time:565ms step_avg:56.48ms
step:11/2330 train_time:620ms step_avg:56.37ms
step:12/2330 train_time:678ms step_avg:56.53ms
step:13/2330 train_time:733ms step_avg:56.41ms
step:14/2330 train_time:793ms step_avg:56.61ms
step:15/2330 train_time:848ms step_avg:56.56ms
step:16/2330 train_time:906ms step_avg:56.65ms
step:17/2330 train_time:961ms step_avg:56.56ms
step:18/2330 train_time:1020ms step_avg:56.67ms
step:19/2330 train_time:1078ms step_avg:56.73ms
step:20/2330 train_time:1141ms step_avg:57.05ms
step:21/2330 train_time:1199ms step_avg:57.09ms
step:22/2330 train_time:1261ms step_avg:57.30ms
step:23/2330 train_time:1316ms step_avg:57.23ms
step:24/2330 train_time:1376ms step_avg:57.33ms
step:25/2330 train_time:1431ms step_avg:57.26ms
step:26/2330 train_time:1491ms step_avg:57.34ms
step:27/2330 train_time:1546ms step_avg:57.26ms
step:28/2330 train_time:1604ms step_avg:57.29ms
step:29/2330 train_time:1659ms step_avg:57.22ms
step:30/2330 train_time:1718ms step_avg:57.26ms
step:31/2330 train_time:1773ms step_avg:57.20ms
step:32/2330 train_time:1831ms step_avg:57.23ms
step:33/2330 train_time:1887ms step_avg:57.18ms
step:34/2330 train_time:1945ms step_avg:57.21ms
step:35/2330 train_time:2001ms step_avg:57.17ms
step:36/2330 train_time:2060ms step_avg:57.22ms
step:37/2330 train_time:2117ms step_avg:57.21ms
step:38/2330 train_time:2177ms step_avg:57.28ms
step:39/2330 train_time:2234ms step_avg:57.27ms
step:40/2330 train_time:2293ms step_avg:57.34ms
step:41/2330 train_time:2349ms step_avg:57.30ms
step:42/2330 train_time:2409ms step_avg:57.35ms
step:43/2330 train_time:2464ms step_avg:57.31ms
step:44/2330 train_time:2524ms step_avg:57.37ms
step:45/2330 train_time:2579ms step_avg:57.32ms
step:46/2330 train_time:2637ms step_avg:57.33ms
step:47/2330 train_time:2693ms step_avg:57.29ms
step:48/2330 train_time:2751ms step_avg:57.32ms
step:49/2330 train_time:2807ms step_avg:57.28ms
step:50/2330 train_time:2865ms step_avg:57.30ms
step:51/2330 train_time:2921ms step_avg:57.28ms
step:52/2330 train_time:2979ms step_avg:57.29ms
step:53/2330 train_time:3035ms step_avg:57.26ms
step:54/2330 train_time:3094ms step_avg:57.30ms
step:55/2330 train_time:3150ms step_avg:57.28ms
step:56/2330 train_time:3211ms step_avg:57.33ms
step:57/2330 train_time:3267ms step_avg:57.31ms
step:58/2330 train_time:3327ms step_avg:57.36ms
step:59/2330 train_time:3383ms step_avg:57.34ms
step:60/2330 train_time:3443ms step_avg:57.38ms
step:61/2330 train_time:3499ms step_avg:57.35ms
step:62/2330 train_time:3557ms step_avg:57.38ms
step:63/2330 train_time:3613ms step_avg:57.35ms
step:64/2330 train_time:3671ms step_avg:57.36ms
step:65/2330 train_time:3726ms step_avg:57.33ms
step:66/2330 train_time:3785ms step_avg:57.35ms
step:67/2330 train_time:3841ms step_avg:57.32ms
step:68/2330 train_time:3899ms step_avg:57.34ms
step:69/2330 train_time:3955ms step_avg:57.32ms
step:70/2330 train_time:4013ms step_avg:57.33ms
step:71/2330 train_time:4069ms step_avg:57.32ms
step:72/2330 train_time:4128ms step_avg:57.34ms
step:73/2330 train_time:4185ms step_avg:57.32ms
step:74/2330 train_time:4245ms step_avg:57.37ms
step:75/2330 train_time:4302ms step_avg:57.36ms
step:76/2330 train_time:4361ms step_avg:57.38ms
step:77/2330 train_time:4417ms step_avg:57.36ms
step:78/2330 train_time:4476ms step_avg:57.39ms
step:79/2330 train_time:4532ms step_avg:57.37ms
step:80/2330 train_time:4590ms step_avg:57.38ms
step:81/2330 train_time:4646ms step_avg:57.35ms
step:82/2330 train_time:4704ms step_avg:57.37ms
step:83/2330 train_time:4760ms step_avg:57.34ms
step:84/2330 train_time:4819ms step_avg:57.37ms
step:85/2330 train_time:4874ms step_avg:57.34ms
step:86/2330 train_time:4933ms step_avg:57.36ms
step:87/2330 train_time:4988ms step_avg:57.34ms
step:88/2330 train_time:5047ms step_avg:57.36ms
step:89/2330 train_time:5103ms step_avg:57.34ms
step:90/2330 train_time:5162ms step_avg:57.36ms
step:91/2330 train_time:5219ms step_avg:57.35ms
step:92/2330 train_time:5277ms step_avg:57.36ms
step:93/2330 train_time:5334ms step_avg:57.35ms
step:94/2330 train_time:5393ms step_avg:57.37ms
step:95/2330 train_time:5450ms step_avg:57.36ms
step:96/2330 train_time:5508ms step_avg:57.37ms
step:97/2330 train_time:5563ms step_avg:57.35ms
step:98/2330 train_time:5622ms step_avg:57.37ms
step:99/2330 train_time:5677ms step_avg:57.35ms
step:100/2330 train_time:5737ms step_avg:57.37ms
step:101/2330 train_time:5792ms step_avg:57.35ms
step:102/2330 train_time:5852ms step_avg:57.37ms
step:103/2330 train_time:5907ms step_avg:57.35ms
step:104/2330 train_time:5965ms step_avg:57.36ms
step:105/2330 train_time:6021ms step_avg:57.34ms
step:106/2330 train_time:6080ms step_avg:57.36ms
step:107/2330 train_time:6136ms step_avg:57.34ms
step:108/2330 train_time:6195ms step_avg:57.36ms
step:109/2330 train_time:6251ms step_avg:57.35ms
step:110/2330 train_time:6310ms step_avg:57.36ms
step:111/2330 train_time:6366ms step_avg:57.35ms
step:112/2330 train_time:6425ms step_avg:57.37ms
step:113/2330 train_time:6481ms step_avg:57.35ms
step:114/2330 train_time:6541ms step_avg:57.38ms
step:115/2330 train_time:6597ms step_avg:57.36ms
step:116/2330 train_time:6656ms step_avg:57.38ms
step:117/2330 train_time:6711ms step_avg:57.36ms
step:118/2330 train_time:6771ms step_avg:57.38ms
step:119/2330 train_time:6826ms step_avg:57.36ms
step:120/2330 train_time:6885ms step_avg:57.38ms
step:121/2330 train_time:6940ms step_avg:57.36ms
step:122/2330 train_time:7000ms step_avg:57.38ms
step:123/2330 train_time:7055ms step_avg:57.36ms
step:124/2330 train_time:7116ms step_avg:57.38ms
step:125/2330 train_time:7171ms step_avg:57.37ms
step:126/2330 train_time:7230ms step_avg:57.38ms
step:127/2330 train_time:7285ms step_avg:57.37ms
step:128/2330 train_time:7345ms step_avg:57.39ms
step:129/2330 train_time:7401ms step_avg:57.38ms
step:130/2330 train_time:7460ms step_avg:57.39ms
step:131/2330 train_time:7516ms step_avg:57.37ms
step:132/2330 train_time:7575ms step_avg:57.39ms
step:133/2330 train_time:7631ms step_avg:57.38ms
step:134/2330 train_time:7690ms step_avg:57.39ms
step:135/2330 train_time:7745ms step_avg:57.37ms
step:136/2330 train_time:7805ms step_avg:57.39ms
step:137/2330 train_time:7861ms step_avg:57.38ms
step:138/2330 train_time:7919ms step_avg:57.39ms
step:139/2330 train_time:7975ms step_avg:57.37ms
step:140/2330 train_time:8034ms step_avg:57.38ms
step:141/2330 train_time:8090ms step_avg:57.38ms
step:142/2330 train_time:8149ms step_avg:57.39ms
step:143/2330 train_time:8204ms step_avg:57.37ms
step:144/2330 train_time:8263ms step_avg:57.39ms
step:145/2330 train_time:8320ms step_avg:57.38ms
step:146/2330 train_time:8378ms step_avg:57.38ms
step:147/2330 train_time:8434ms step_avg:57.38ms
step:148/2330 train_time:8493ms step_avg:57.39ms
step:149/2330 train_time:8550ms step_avg:57.38ms
step:150/2330 train_time:8608ms step_avg:57.39ms
step:151/2330 train_time:8664ms step_avg:57.37ms
step:152/2330 train_time:8723ms step_avg:57.39ms
step:153/2330 train_time:8778ms step_avg:57.37ms
step:154/2330 train_time:8837ms step_avg:57.39ms
step:155/2330 train_time:8893ms step_avg:57.37ms
step:156/2330 train_time:8953ms step_avg:57.39ms
step:157/2330 train_time:9009ms step_avg:57.38ms
step:158/2330 train_time:9068ms step_avg:57.39ms
step:159/2330 train_time:9123ms step_avg:57.38ms
step:160/2330 train_time:9182ms step_avg:57.39ms
step:161/2330 train_time:9238ms step_avg:57.38ms
step:162/2330 train_time:9298ms step_avg:57.40ms
step:163/2330 train_time:9354ms step_avg:57.38ms
step:164/2330 train_time:9413ms step_avg:57.39ms
step:165/2330 train_time:9468ms step_avg:57.38ms
step:166/2330 train_time:9528ms step_avg:57.39ms
step:167/2330 train_time:9583ms step_avg:57.38ms
step:168/2330 train_time:9643ms step_avg:57.40ms
step:169/2330 train_time:9700ms step_avg:57.39ms
step:170/2330 train_time:9758ms step_avg:57.40ms
step:171/2330 train_time:9814ms step_avg:57.39ms
step:172/2330 train_time:9873ms step_avg:57.40ms
step:173/2330 train_time:9928ms step_avg:57.39ms
step:174/2330 train_time:9987ms step_avg:57.40ms
step:175/2330 train_time:10043ms step_avg:57.39ms
step:176/2330 train_time:10102ms step_avg:57.40ms
step:177/2330 train_time:10158ms step_avg:57.39ms
step:178/2330 train_time:10218ms step_avg:57.40ms
step:179/2330 train_time:10274ms step_avg:57.39ms
step:180/2330 train_time:10332ms step_avg:57.40ms
step:181/2330 train_time:10388ms step_avg:57.39ms
step:182/2330 train_time:10447ms step_avg:57.40ms
step:183/2330 train_time:10504ms step_avg:57.40ms
step:184/2330 train_time:10562ms step_avg:57.40ms
step:185/2330 train_time:10619ms step_avg:57.40ms
step:186/2330 train_time:10678ms step_avg:57.41ms
step:187/2330 train_time:10734ms step_avg:57.40ms
step:188/2330 train_time:10792ms step_avg:57.41ms
step:189/2330 train_time:10848ms step_avg:57.40ms
step:190/2330 train_time:10907ms step_avg:57.40ms
step:191/2330 train_time:10962ms step_avg:57.39ms
step:192/2330 train_time:11021ms step_avg:57.40ms
step:193/2330 train_time:11077ms step_avg:57.39ms
step:194/2330 train_time:11137ms step_avg:57.41ms
step:195/2330 train_time:11192ms step_avg:57.40ms
step:196/2330 train_time:11251ms step_avg:57.41ms
step:197/2330 train_time:11307ms step_avg:57.40ms
step:198/2330 train_time:11366ms step_avg:57.40ms
step:199/2330 train_time:11422ms step_avg:57.40ms
step:200/2330 train_time:11481ms step_avg:57.40ms
step:201/2330 train_time:11537ms step_avg:57.40ms
step:202/2330 train_time:11597ms step_avg:57.41ms
step:203/2330 train_time:11653ms step_avg:57.41ms
step:204/2330 train_time:11713ms step_avg:57.41ms
step:205/2330 train_time:11768ms step_avg:57.41ms
step:206/2330 train_time:11827ms step_avg:57.41ms
step:207/2330 train_time:11883ms step_avg:57.40ms
step:208/2330 train_time:11943ms step_avg:57.42ms
step:209/2330 train_time:11999ms step_avg:57.41ms
step:210/2330 train_time:12057ms step_avg:57.41ms
step:211/2330 train_time:12113ms step_avg:57.41ms
step:212/2330 train_time:12172ms step_avg:57.42ms
step:213/2330 train_time:12228ms step_avg:57.41ms
step:214/2330 train_time:12289ms step_avg:57.42ms
step:215/2330 train_time:12344ms step_avg:57.42ms
step:216/2330 train_time:12404ms step_avg:57.42ms
step:217/2330 train_time:12460ms step_avg:57.42ms
step:218/2330 train_time:12519ms step_avg:57.43ms
step:219/2330 train_time:12575ms step_avg:57.42ms
step:220/2330 train_time:12634ms step_avg:57.43ms
step:221/2330 train_time:12689ms step_avg:57.42ms
step:222/2330 train_time:12748ms step_avg:57.43ms
step:223/2330 train_time:12804ms step_avg:57.42ms
step:224/2330 train_time:12863ms step_avg:57.42ms
step:225/2330 train_time:12918ms step_avg:57.42ms
step:226/2330 train_time:12977ms step_avg:57.42ms
step:227/2330 train_time:13033ms step_avg:57.41ms
step:228/2330 train_time:13092ms step_avg:57.42ms
step:229/2330 train_time:13147ms step_avg:57.41ms
step:230/2330 train_time:13207ms step_avg:57.42ms
step:231/2330 train_time:13263ms step_avg:57.41ms
step:232/2330 train_time:13322ms step_avg:57.42ms
step:233/2330 train_time:13377ms step_avg:57.41ms
step:234/2330 train_time:13436ms step_avg:57.42ms
step:235/2330 train_time:13493ms step_avg:57.42ms
step:236/2330 train_time:13551ms step_avg:57.42ms
step:237/2330 train_time:13607ms step_avg:57.41ms
step:238/2330 train_time:13665ms step_avg:57.42ms
step:239/2330 train_time:13721ms step_avg:57.41ms
step:240/2330 train_time:13780ms step_avg:57.42ms
step:241/2330 train_time:13836ms step_avg:57.41ms
step:242/2330 train_time:13895ms step_avg:57.42ms
step:243/2330 train_time:13951ms step_avg:57.41ms
step:244/2330 train_time:14010ms step_avg:57.42ms
step:245/2330 train_time:14065ms step_avg:57.41ms
step:246/2330 train_time:14125ms step_avg:57.42ms
step:247/2330 train_time:14182ms step_avg:57.42ms
step:248/2330 train_time:14241ms step_avg:57.42ms
step:249/2330 train_time:14297ms step_avg:57.42ms
step:250/2330 train_time:14356ms step_avg:57.43ms
step:250/2330 val_loss:4.9595 train_time:14436ms step_avg:57.74ms
step:251/2330 train_time:14454ms step_avg:57.59ms
step:252/2330 train_time:14473ms step_avg:57.43ms
step:253/2330 train_time:14529ms step_avg:57.43ms
step:254/2330 train_time:14590ms step_avg:57.44ms
step:255/2330 train_time:14644ms step_avg:57.43ms
step:256/2330 train_time:14713ms step_avg:57.47ms
step:257/2330 train_time:14769ms step_avg:57.47ms
step:258/2330 train_time:14830ms step_avg:57.48ms
step:259/2330 train_time:14885ms step_avg:57.47ms
step:260/2330 train_time:14946ms step_avg:57.48ms
step:261/2330 train_time:15001ms step_avg:57.47ms
step:262/2330 train_time:15060ms step_avg:57.48ms
step:263/2330 train_time:15115ms step_avg:57.47ms
step:264/2330 train_time:15174ms step_avg:57.48ms
step:265/2330 train_time:15229ms step_avg:57.47ms
step:266/2330 train_time:15287ms step_avg:57.47ms
step:267/2330 train_time:15342ms step_avg:57.46ms
step:268/2330 train_time:15401ms step_avg:57.47ms
step:269/2330 train_time:15457ms step_avg:57.46ms
step:270/2330 train_time:15517ms step_avg:57.47ms
step:271/2330 train_time:15573ms step_avg:57.47ms
step:272/2330 train_time:15635ms step_avg:57.48ms
step:273/2330 train_time:15692ms step_avg:57.48ms
step:274/2330 train_time:15752ms step_avg:57.49ms
step:275/2330 train_time:15808ms step_avg:57.48ms
step:276/2330 train_time:15868ms step_avg:57.49ms
step:277/2330 train_time:15924ms step_avg:57.49ms
step:278/2330 train_time:15983ms step_avg:57.49ms
step:279/2330 train_time:16038ms step_avg:57.48ms
step:280/2330 train_time:16098ms step_avg:57.49ms
step:281/2330 train_time:16153ms step_avg:57.49ms
step:282/2330 train_time:16212ms step_avg:57.49ms
step:283/2330 train_time:16267ms step_avg:57.48ms
step:284/2330 train_time:16326ms step_avg:57.49ms
step:285/2330 train_time:16382ms step_avg:57.48ms
step:286/2330 train_time:16440ms step_avg:57.48ms
step:287/2330 train_time:16496ms step_avg:57.48ms
step:288/2330 train_time:16555ms step_avg:57.48ms
step:289/2330 train_time:16612ms step_avg:57.48ms
step:290/2330 train_time:16671ms step_avg:57.49ms
step:291/2330 train_time:16727ms step_avg:57.48ms
step:292/2330 train_time:16787ms step_avg:57.49ms
step:293/2330 train_time:16843ms step_avg:57.48ms
step:294/2330 train_time:16902ms step_avg:57.49ms
step:295/2330 train_time:16958ms step_avg:57.48ms
step:296/2330 train_time:17018ms step_avg:57.49ms
step:297/2330 train_time:17073ms step_avg:57.49ms
step:298/2330 train_time:17132ms step_avg:57.49ms
step:299/2330 train_time:17188ms step_avg:57.48ms
step:300/2330 train_time:17246ms step_avg:57.49ms
step:301/2330 train_time:17302ms step_avg:57.48ms
step:302/2330 train_time:17361ms step_avg:57.49ms
step:303/2330 train_time:17416ms step_avg:57.48ms
step:304/2330 train_time:17475ms step_avg:57.48ms
step:305/2330 train_time:17531ms step_avg:57.48ms
step:306/2330 train_time:17591ms step_avg:57.49ms
step:307/2330 train_time:17646ms step_avg:57.48ms
step:308/2330 train_time:17707ms step_avg:57.49ms
step:309/2330 train_time:17762ms step_avg:57.48ms
step:310/2330 train_time:17822ms step_avg:57.49ms
step:311/2330 train_time:17878ms step_avg:57.48ms
step:312/2330 train_time:17937ms step_avg:57.49ms
step:313/2330 train_time:17994ms step_avg:57.49ms
step:314/2330 train_time:18053ms step_avg:57.49ms
step:315/2330 train_time:18109ms step_avg:57.49ms
step:316/2330 train_time:18168ms step_avg:57.49ms
step:317/2330 train_time:18224ms step_avg:57.49ms
step:318/2330 train_time:18284ms step_avg:57.50ms
step:319/2330 train_time:18338ms step_avg:57.49ms
step:320/2330 train_time:18398ms step_avg:57.49ms
step:321/2330 train_time:18454ms step_avg:57.49ms
step:322/2330 train_time:18513ms step_avg:57.49ms
step:323/2330 train_time:18569ms step_avg:57.49ms
step:324/2330 train_time:18628ms step_avg:57.49ms
step:325/2330 train_time:18683ms step_avg:57.49ms
step:326/2330 train_time:18744ms step_avg:57.50ms
step:327/2330 train_time:18799ms step_avg:57.49ms
step:328/2330 train_time:18859ms step_avg:57.50ms
step:329/2330 train_time:18915ms step_avg:57.49ms
step:330/2330 train_time:18975ms step_avg:57.50ms
step:331/2330 train_time:19031ms step_avg:57.50ms
step:332/2330 train_time:19090ms step_avg:57.50ms
step:333/2330 train_time:19146ms step_avg:57.50ms
step:334/2330 train_time:19204ms step_avg:57.50ms
step:335/2330 train_time:19260ms step_avg:57.49ms
step:336/2330 train_time:19319ms step_avg:57.50ms
step:337/2330 train_time:19374ms step_avg:57.49ms
step:338/2330 train_time:19434ms step_avg:57.50ms
step:339/2330 train_time:19490ms step_avg:57.49ms
step:340/2330 train_time:19549ms step_avg:57.50ms
step:341/2330 train_time:19606ms step_avg:57.50ms
step:342/2330 train_time:19665ms step_avg:57.50ms
step:343/2330 train_time:19721ms step_avg:57.50ms
step:344/2330 train_time:19780ms step_avg:57.50ms
step:345/2330 train_time:19836ms step_avg:57.49ms
step:346/2330 train_time:19895ms step_avg:57.50ms
step:347/2330 train_time:19951ms step_avg:57.50ms
step:348/2330 train_time:20010ms step_avg:57.50ms
step:349/2330 train_time:20066ms step_avg:57.50ms
step:350/2330 train_time:20124ms step_avg:57.50ms
step:351/2330 train_time:20180ms step_avg:57.49ms
step:352/2330 train_time:20239ms step_avg:57.50ms
step:353/2330 train_time:20295ms step_avg:57.49ms
step:354/2330 train_time:20353ms step_avg:57.50ms
step:355/2330 train_time:20409ms step_avg:57.49ms
step:356/2330 train_time:20467ms step_avg:57.49ms
step:357/2330 train_time:20524ms step_avg:57.49ms
step:358/2330 train_time:20583ms step_avg:57.49ms
step:359/2330 train_time:20639ms step_avg:57.49ms
step:360/2330 train_time:20699ms step_avg:57.50ms
step:361/2330 train_time:20755ms step_avg:57.49ms
step:362/2330 train_time:20814ms step_avg:57.50ms
step:363/2330 train_time:20870ms step_avg:57.49ms
step:364/2330 train_time:20929ms step_avg:57.50ms
step:365/2330 train_time:20985ms step_avg:57.49ms
step:366/2330 train_time:21044ms step_avg:57.50ms
step:367/2330 train_time:21100ms step_avg:57.49ms
step:368/2330 train_time:21159ms step_avg:57.50ms
step:369/2330 train_time:21215ms step_avg:57.49ms
step:370/2330 train_time:21274ms step_avg:57.50ms
step:371/2330 train_time:21330ms step_avg:57.49ms
step:372/2330 train_time:21389ms step_avg:57.50ms
step:373/2330 train_time:21444ms step_avg:57.49ms
step:374/2330 train_time:21504ms step_avg:57.50ms
step:375/2330 train_time:21559ms step_avg:57.49ms
step:376/2330 train_time:21619ms step_avg:57.50ms
step:377/2330 train_time:21675ms step_avg:57.49ms
step:378/2330 train_time:21734ms step_avg:57.50ms
step:379/2330 train_time:21790ms step_avg:57.49ms
step:380/2330 train_time:21848ms step_avg:57.50ms
step:381/2330 train_time:21904ms step_avg:57.49ms
step:382/2330 train_time:21964ms step_avg:57.50ms
step:383/2330 train_time:22019ms step_avg:57.49ms
step:384/2330 train_time:22079ms step_avg:57.50ms
step:385/2330 train_time:22135ms step_avg:57.49ms
step:386/2330 train_time:22194ms step_avg:57.50ms
step:387/2330 train_time:22250ms step_avg:57.49ms
step:388/2330 train_time:22310ms step_avg:57.50ms
step:389/2330 train_time:22365ms step_avg:57.49ms
step:390/2330 train_time:22424ms step_avg:57.50ms
step:391/2330 train_time:22480ms step_avg:57.49ms
step:392/2330 train_time:22539ms step_avg:57.50ms
step:393/2330 train_time:22596ms step_avg:57.50ms
step:394/2330 train_time:22654ms step_avg:57.50ms
step:395/2330 train_time:22711ms step_avg:57.50ms
step:396/2330 train_time:22769ms step_avg:57.50ms
step:397/2330 train_time:22825ms step_avg:57.49ms
step:398/2330 train_time:22885ms step_avg:57.50ms
step:399/2330 train_time:22941ms step_avg:57.50ms
step:400/2330 train_time:23000ms step_avg:57.50ms
step:401/2330 train_time:23055ms step_avg:57.49ms
step:402/2330 train_time:23115ms step_avg:57.50ms
step:403/2330 train_time:23171ms step_avg:57.50ms
step:404/2330 train_time:23231ms step_avg:57.50ms
step:405/2330 train_time:23286ms step_avg:57.50ms
step:406/2330 train_time:23345ms step_avg:57.50ms
step:407/2330 train_time:23401ms step_avg:57.50ms
step:408/2330 train_time:23460ms step_avg:57.50ms
step:409/2330 train_time:23516ms step_avg:57.50ms
step:410/2330 train_time:23575ms step_avg:57.50ms
step:411/2330 train_time:23630ms step_avg:57.49ms
step:412/2330 train_time:23691ms step_avg:57.50ms
step:413/2330 train_time:23747ms step_avg:57.50ms
step:414/2330 train_time:23805ms step_avg:57.50ms
step:415/2330 train_time:23861ms step_avg:57.50ms
step:416/2330 train_time:23921ms step_avg:57.50ms
step:417/2330 train_time:23977ms step_avg:57.50ms
step:418/2330 train_time:24036ms step_avg:57.50ms
step:419/2330 train_time:24092ms step_avg:57.50ms
step:420/2330 train_time:24151ms step_avg:57.50ms
step:421/2330 train_time:24207ms step_avg:57.50ms
step:422/2330 train_time:24266ms step_avg:57.50ms
step:423/2330 train_time:24321ms step_avg:57.50ms
step:424/2330 train_time:24380ms step_avg:57.50ms
step:425/2330 train_time:24436ms step_avg:57.50ms
step:426/2330 train_time:24495ms step_avg:57.50ms
step:427/2330 train_time:24552ms step_avg:57.50ms
step:428/2330 train_time:24611ms step_avg:57.50ms
step:429/2330 train_time:24668ms step_avg:57.50ms
step:430/2330 train_time:24727ms step_avg:57.50ms
step:431/2330 train_time:24782ms step_avg:57.50ms
step:432/2330 train_time:24841ms step_avg:57.50ms
step:433/2330 train_time:24896ms step_avg:57.50ms
step:434/2330 train_time:24957ms step_avg:57.50ms
step:435/2330 train_time:25014ms step_avg:57.50ms
step:436/2330 train_time:25072ms step_avg:57.50ms
step:437/2330 train_time:25128ms step_avg:57.50ms
step:438/2330 train_time:25187ms step_avg:57.50ms
step:439/2330 train_time:25242ms step_avg:57.50ms
step:440/2330 train_time:25302ms step_avg:57.50ms
step:441/2330 train_time:25357ms step_avg:57.50ms
step:442/2330 train_time:25416ms step_avg:57.50ms
step:443/2330 train_time:25472ms step_avg:57.50ms
step:444/2330 train_time:25531ms step_avg:57.50ms
step:445/2330 train_time:25587ms step_avg:57.50ms
step:446/2330 train_time:25647ms step_avg:57.50ms
step:447/2330 train_time:25703ms step_avg:57.50ms
step:448/2330 train_time:25762ms step_avg:57.50ms
step:449/2330 train_time:25817ms step_avg:57.50ms
step:450/2330 train_time:25877ms step_avg:57.50ms
step:451/2330 train_time:25932ms step_avg:57.50ms
step:452/2330 train_time:25992ms step_avg:57.50ms
step:453/2330 train_time:26048ms step_avg:57.50ms
step:454/2330 train_time:26108ms step_avg:57.51ms
step:455/2330 train_time:26164ms step_avg:57.50ms
step:456/2330 train_time:26223ms step_avg:57.51ms
step:457/2330 train_time:26280ms step_avg:57.50ms
step:458/2330 train_time:26338ms step_avg:57.51ms
step:459/2330 train_time:26393ms step_avg:57.50ms
step:460/2330 train_time:26453ms step_avg:57.51ms
step:461/2330 train_time:26509ms step_avg:57.50ms
step:462/2330 train_time:26568ms step_avg:57.51ms
step:463/2330 train_time:26624ms step_avg:57.50ms
step:464/2330 train_time:26683ms step_avg:57.51ms
step:465/2330 train_time:26738ms step_avg:57.50ms
step:466/2330 train_time:26798ms step_avg:57.51ms
step:467/2330 train_time:26854ms step_avg:57.50ms
step:468/2330 train_time:26913ms step_avg:57.51ms
step:469/2330 train_time:26969ms step_avg:57.50ms
step:470/2330 train_time:27028ms step_avg:57.51ms
step:471/2330 train_time:27084ms step_avg:57.50ms
step:472/2330 train_time:27144ms step_avg:57.51ms
step:473/2330 train_time:27199ms step_avg:57.50ms
step:474/2330 train_time:27258ms step_avg:57.51ms
step:475/2330 train_time:27314ms step_avg:57.50ms
step:476/2330 train_time:27373ms step_avg:57.51ms
step:477/2330 train_time:27430ms step_avg:57.50ms
step:478/2330 train_time:27489ms step_avg:57.51ms
step:479/2330 train_time:27545ms step_avg:57.50ms
step:480/2330 train_time:27604ms step_avg:57.51ms
step:481/2330 train_time:27660ms step_avg:57.51ms
step:482/2330 train_time:27720ms step_avg:57.51ms
step:483/2330 train_time:27775ms step_avg:57.51ms
step:484/2330 train_time:27833ms step_avg:57.51ms
step:485/2330 train_time:27890ms step_avg:57.50ms
step:486/2330 train_time:27948ms step_avg:57.51ms
step:487/2330 train_time:28004ms step_avg:57.50ms
step:488/2330 train_time:28063ms step_avg:57.51ms
step:489/2330 train_time:28119ms step_avg:57.50ms
step:490/2330 train_time:28179ms step_avg:57.51ms
step:491/2330 train_time:28234ms step_avg:57.50ms
step:492/2330 train_time:28294ms step_avg:57.51ms
step:493/2330 train_time:28349ms step_avg:57.50ms
step:494/2330 train_time:28409ms step_avg:57.51ms
step:495/2330 train_time:28465ms step_avg:57.51ms
step:496/2330 train_time:28524ms step_avg:57.51ms
step:497/2330 train_time:28580ms step_avg:57.50ms
step:498/2330 train_time:28639ms step_avg:57.51ms
step:499/2330 train_time:28695ms step_avg:57.50ms
step:500/2330 train_time:28754ms step_avg:57.51ms
step:500/2330 val_loss:4.4513 train_time:28834ms step_avg:57.67ms
step:501/2330 train_time:28852ms step_avg:57.59ms
step:502/2330 train_time:28871ms step_avg:57.51ms
step:503/2330 train_time:28927ms step_avg:57.51ms
step:504/2330 train_time:28994ms step_avg:57.53ms
step:505/2330 train_time:29049ms step_avg:57.52ms
step:506/2330 train_time:29112ms step_avg:57.53ms
step:507/2330 train_time:29168ms step_avg:57.53ms
step:508/2330 train_time:29228ms step_avg:57.54ms
step:509/2330 train_time:29283ms step_avg:57.53ms
step:510/2330 train_time:29342ms step_avg:57.53ms
step:511/2330 train_time:29398ms step_avg:57.53ms
step:512/2330 train_time:29456ms step_avg:57.53ms
step:513/2330 train_time:29512ms step_avg:57.53ms
step:514/2330 train_time:29570ms step_avg:57.53ms
step:515/2330 train_time:29625ms step_avg:57.52ms
step:516/2330 train_time:29685ms step_avg:57.53ms
step:517/2330 train_time:29740ms step_avg:57.52ms
step:518/2330 train_time:29800ms step_avg:57.53ms
step:519/2330 train_time:29857ms step_avg:57.53ms
step:520/2330 train_time:29916ms step_avg:57.53ms
step:521/2330 train_time:29975ms step_avg:57.53ms
step:522/2330 train_time:30034ms step_avg:57.54ms
step:523/2330 train_time:30091ms step_avg:57.54ms
step:524/2330 train_time:30150ms step_avg:57.54ms
step:525/2330 train_time:30206ms step_avg:57.54ms
step:526/2330 train_time:30266ms step_avg:57.54ms
step:527/2330 train_time:30321ms step_avg:57.53ms
step:528/2330 train_time:30380ms step_avg:57.54ms
step:529/2330 train_time:30436ms step_avg:57.53ms
step:530/2330 train_time:30495ms step_avg:57.54ms
step:531/2330 train_time:30550ms step_avg:57.53ms
step:532/2330 train_time:30609ms step_avg:57.54ms
step:533/2330 train_time:30664ms step_avg:57.53ms
step:534/2330 train_time:30723ms step_avg:57.53ms
step:535/2330 train_time:30778ms step_avg:57.53ms
step:536/2330 train_time:30838ms step_avg:57.53ms
step:537/2330 train_time:30895ms step_avg:57.53ms
step:538/2330 train_time:30955ms step_avg:57.54ms
step:539/2330 train_time:31011ms step_avg:57.54ms
step:540/2330 train_time:31073ms step_avg:57.54ms
step:541/2330 train_time:31129ms step_avg:57.54ms
step:542/2330 train_time:31189ms step_avg:57.54ms
step:543/2330 train_time:31245ms step_avg:57.54ms
step:544/2330 train_time:31305ms step_avg:57.55ms
step:545/2330 train_time:31361ms step_avg:57.54ms
step:546/2330 train_time:31421ms step_avg:57.55ms
step:547/2330 train_time:31476ms step_avg:57.54ms
step:548/2330 train_time:31534ms step_avg:57.54ms
step:549/2330 train_time:31590ms step_avg:57.54ms
step:550/2330 train_time:31648ms step_avg:57.54ms
step:551/2330 train_time:31704ms step_avg:57.54ms
step:552/2330 train_time:31764ms step_avg:57.54ms
step:553/2330 train_time:31820ms step_avg:57.54ms
step:554/2330 train_time:31879ms step_avg:57.54ms
step:555/2330 train_time:31936ms step_avg:57.54ms
step:556/2330 train_time:31995ms step_avg:57.55ms
step:557/2330 train_time:32051ms step_avg:57.54ms
step:558/2330 train_time:32111ms step_avg:57.55ms
step:559/2330 train_time:32167ms step_avg:57.54ms
step:560/2330 train_time:32227ms step_avg:57.55ms
step:561/2330 train_time:32282ms step_avg:57.54ms
step:562/2330 train_time:32341ms step_avg:57.55ms
step:563/2330 train_time:32397ms step_avg:57.54ms
step:564/2330 train_time:32456ms step_avg:57.55ms
step:565/2330 train_time:32512ms step_avg:57.54ms
step:566/2330 train_time:32570ms step_avg:57.54ms
step:567/2330 train_time:32626ms step_avg:57.54ms
step:568/2330 train_time:32685ms step_avg:57.54ms
step:569/2330 train_time:32740ms step_avg:57.54ms
step:570/2330 train_time:32800ms step_avg:57.54ms
step:571/2330 train_time:32856ms step_avg:57.54ms
step:572/2330 train_time:32915ms step_avg:57.54ms
step:573/2330 train_time:32971ms step_avg:57.54ms
step:574/2330 train_time:33031ms step_avg:57.54ms
step:575/2330 train_time:33087ms step_avg:57.54ms
step:576/2330 train_time:33147ms step_avg:57.55ms
step:577/2330 train_time:33203ms step_avg:57.54ms
step:578/2330 train_time:33263ms step_avg:57.55ms
step:579/2330 train_time:33318ms step_avg:57.54ms
step:580/2330 train_time:33378ms step_avg:57.55ms
step:581/2330 train_time:33434ms step_avg:57.55ms
step:582/2330 train_time:33492ms step_avg:57.55ms
step:583/2330 train_time:33548ms step_avg:57.54ms
step:584/2330 train_time:33607ms step_avg:57.55ms
step:585/2330 train_time:33662ms step_avg:57.54ms
step:586/2330 train_time:33723ms step_avg:57.55ms
step:587/2330 train_time:33778ms step_avg:57.54ms
step:588/2330 train_time:33838ms step_avg:57.55ms
step:589/2330 train_time:33893ms step_avg:57.54ms
step:590/2330 train_time:33953ms step_avg:57.55ms
step:591/2330 train_time:34009ms step_avg:57.54ms
step:592/2330 train_time:34068ms step_avg:57.55ms
step:593/2330 train_time:34125ms step_avg:57.55ms
step:594/2330 train_time:34184ms step_avg:57.55ms
step:595/2330 train_time:34239ms step_avg:57.54ms
step:596/2330 train_time:34299ms step_avg:57.55ms
step:597/2330 train_time:34356ms step_avg:57.55ms
step:598/2330 train_time:34415ms step_avg:57.55ms
step:599/2330 train_time:34471ms step_avg:57.55ms
step:600/2330 train_time:34530ms step_avg:57.55ms
step:601/2330 train_time:34585ms step_avg:57.55ms
step:602/2330 train_time:34645ms step_avg:57.55ms
step:603/2330 train_time:34701ms step_avg:57.55ms
step:604/2330 train_time:34761ms step_avg:57.55ms
step:605/2330 train_time:34816ms step_avg:57.55ms
step:606/2330 train_time:34876ms step_avg:57.55ms
step:607/2330 train_time:34932ms step_avg:57.55ms
step:608/2330 train_time:34991ms step_avg:57.55ms
step:609/2330 train_time:35046ms step_avg:57.55ms
step:610/2330 train_time:35106ms step_avg:57.55ms
step:611/2330 train_time:35161ms step_avg:57.55ms
step:612/2330 train_time:35222ms step_avg:57.55ms
step:613/2330 train_time:35277ms step_avg:57.55ms
step:614/2330 train_time:35337ms step_avg:57.55ms
step:615/2330 train_time:35393ms step_avg:57.55ms
step:616/2330 train_time:35452ms step_avg:57.55ms
step:617/2330 train_time:35508ms step_avg:57.55ms
step:618/2330 train_time:35567ms step_avg:57.55ms
step:619/2330 train_time:35623ms step_avg:57.55ms
step:620/2330 train_time:35682ms step_avg:57.55ms
step:621/2330 train_time:35738ms step_avg:57.55ms
step:622/2330 train_time:35797ms step_avg:57.55ms
step:623/2330 train_time:35853ms step_avg:57.55ms
step:624/2330 train_time:35912ms step_avg:57.55ms
step:625/2330 train_time:35968ms step_avg:57.55ms
step:626/2330 train_time:36028ms step_avg:57.55ms
step:627/2330 train_time:36085ms step_avg:57.55ms
step:628/2330 train_time:36144ms step_avg:57.55ms
step:629/2330 train_time:36200ms step_avg:57.55ms
step:630/2330 train_time:36259ms step_avg:57.55ms
step:631/2330 train_time:36315ms step_avg:57.55ms
step:632/2330 train_time:36375ms step_avg:57.56ms
step:633/2330 train_time:36431ms step_avg:57.55ms
step:634/2330 train_time:36490ms step_avg:57.56ms
step:635/2330 train_time:36546ms step_avg:57.55ms
step:636/2330 train_time:36606ms step_avg:57.56ms
step:637/2330 train_time:36661ms step_avg:57.55ms
step:638/2330 train_time:36722ms step_avg:57.56ms
step:639/2330 train_time:36778ms step_avg:57.56ms
step:640/2330 train_time:36837ms step_avg:57.56ms
step:641/2330 train_time:36893ms step_avg:57.56ms
step:642/2330 train_time:36953ms step_avg:57.56ms
step:643/2330 train_time:37008ms step_avg:57.56ms
step:644/2330 train_time:37068ms step_avg:57.56ms
step:645/2330 train_time:37124ms step_avg:57.56ms
step:646/2330 train_time:37184ms step_avg:57.56ms
step:647/2330 train_time:37240ms step_avg:57.56ms
step:648/2330 train_time:37300ms step_avg:57.56ms
step:649/2330 train_time:37356ms step_avg:57.56ms
step:650/2330 train_time:37416ms step_avg:57.56ms
step:651/2330 train_time:37472ms step_avg:57.56ms
step:652/2330 train_time:37531ms step_avg:57.56ms
step:653/2330 train_time:37587ms step_avg:57.56ms
step:654/2330 train_time:37647ms step_avg:57.56ms
step:655/2330 train_time:37703ms step_avg:57.56ms
step:656/2330 train_time:37763ms step_avg:57.57ms
step:657/2330 train_time:37818ms step_avg:57.56ms
step:658/2330 train_time:37878ms step_avg:57.56ms
step:659/2330 train_time:37935ms step_avg:57.56ms
step:660/2330 train_time:37994ms step_avg:57.57ms
step:661/2330 train_time:38050ms step_avg:57.56ms
step:662/2330 train_time:38109ms step_avg:57.57ms
step:663/2330 train_time:38165ms step_avg:57.56ms
step:664/2330 train_time:38225ms step_avg:57.57ms
step:665/2330 train_time:38280ms step_avg:57.56ms
step:666/2330 train_time:38341ms step_avg:57.57ms
step:667/2330 train_time:38397ms step_avg:57.57ms
step:668/2330 train_time:38456ms step_avg:57.57ms
step:669/2330 train_time:38512ms step_avg:57.57ms
step:670/2330 train_time:38572ms step_avg:57.57ms
step:671/2330 train_time:38627ms step_avg:57.57ms
step:672/2330 train_time:38687ms step_avg:57.57ms
step:673/2330 train_time:38742ms step_avg:57.57ms
step:674/2330 train_time:38803ms step_avg:57.57ms
step:675/2330 train_time:38858ms step_avg:57.57ms
step:676/2330 train_time:38918ms step_avg:57.57ms
step:677/2330 train_time:38975ms step_avg:57.57ms
step:678/2330 train_time:39034ms step_avg:57.57ms
step:679/2330 train_time:39090ms step_avg:57.57ms
step:680/2330 train_time:39148ms step_avg:57.57ms
step:681/2330 train_time:39204ms step_avg:57.57ms
step:682/2330 train_time:39264ms step_avg:57.57ms
step:683/2330 train_time:39319ms step_avg:57.57ms
step:684/2330 train_time:39378ms step_avg:57.57ms
step:685/2330 train_time:39435ms step_avg:57.57ms
step:686/2330 train_time:39494ms step_avg:57.57ms
step:687/2330 train_time:39550ms step_avg:57.57ms
step:688/2330 train_time:39609ms step_avg:57.57ms
step:689/2330 train_time:39665ms step_avg:57.57ms
step:690/2330 train_time:39725ms step_avg:57.57ms
step:691/2330 train_time:39780ms step_avg:57.57ms
step:692/2330 train_time:39841ms step_avg:57.57ms
step:693/2330 train_time:39896ms step_avg:57.57ms
step:694/2330 train_time:39957ms step_avg:57.57ms
step:695/2330 train_time:40013ms step_avg:57.57ms
step:696/2330 train_time:40072ms step_avg:57.57ms
step:697/2330 train_time:40128ms step_avg:57.57ms
step:698/2330 train_time:40187ms step_avg:57.57ms
step:699/2330 train_time:40243ms step_avg:57.57ms
step:700/2330 train_time:40303ms step_avg:57.58ms
step:701/2330 train_time:40359ms step_avg:57.57ms
step:702/2330 train_time:40419ms step_avg:57.58ms
step:703/2330 train_time:40475ms step_avg:57.57ms
step:704/2330 train_time:40533ms step_avg:57.58ms
step:705/2330 train_time:40589ms step_avg:57.57ms
step:706/2330 train_time:40648ms step_avg:57.58ms
step:707/2330 train_time:40704ms step_avg:57.57ms
step:708/2330 train_time:40764ms step_avg:57.58ms
step:709/2330 train_time:40819ms step_avg:57.57ms
step:710/2330 train_time:40879ms step_avg:57.58ms
step:711/2330 train_time:40935ms step_avg:57.57ms
step:712/2330 train_time:40994ms step_avg:57.58ms
step:713/2330 train_time:41050ms step_avg:57.57ms
step:714/2330 train_time:41108ms step_avg:57.57ms
step:715/2330 train_time:41165ms step_avg:57.57ms
step:716/2330 train_time:41225ms step_avg:57.58ms
step:717/2330 train_time:41280ms step_avg:57.57ms
step:718/2330 train_time:41340ms step_avg:57.58ms
step:719/2330 train_time:41396ms step_avg:57.57ms
step:720/2330 train_time:41455ms step_avg:57.58ms
step:721/2330 train_time:41511ms step_avg:57.57ms
step:722/2330 train_time:41570ms step_avg:57.58ms
step:723/2330 train_time:41626ms step_avg:57.57ms
step:724/2330 train_time:41686ms step_avg:57.58ms
step:725/2330 train_time:41742ms step_avg:57.58ms
step:726/2330 train_time:41802ms step_avg:57.58ms
step:727/2330 train_time:41858ms step_avg:57.58ms
step:728/2330 train_time:41917ms step_avg:57.58ms
step:729/2330 train_time:41973ms step_avg:57.58ms
step:730/2330 train_time:42033ms step_avg:57.58ms
step:731/2330 train_time:42089ms step_avg:57.58ms
step:732/2330 train_time:42149ms step_avg:57.58ms
step:733/2330 train_time:42204ms step_avg:57.58ms
step:734/2330 train_time:42265ms step_avg:57.58ms
step:735/2330 train_time:42321ms step_avg:57.58ms
step:736/2330 train_time:42380ms step_avg:57.58ms
step:737/2330 train_time:42437ms step_avg:57.58ms
step:738/2330 train_time:42496ms step_avg:57.58ms
step:739/2330 train_time:42552ms step_avg:57.58ms
step:740/2330 train_time:42611ms step_avg:57.58ms
step:741/2330 train_time:42667ms step_avg:57.58ms
step:742/2330 train_time:42727ms step_avg:57.58ms
step:743/2330 train_time:42782ms step_avg:57.58ms
step:744/2330 train_time:42843ms step_avg:57.58ms
step:745/2330 train_time:42898ms step_avg:57.58ms
step:746/2330 train_time:42958ms step_avg:57.58ms
step:747/2330 train_time:43014ms step_avg:57.58ms
step:748/2330 train_time:43073ms step_avg:57.58ms
step:749/2330 train_time:43129ms step_avg:57.58ms
step:750/2330 train_time:43189ms step_avg:57.58ms
step:750/2330 val_loss:4.2389 train_time:43269ms step_avg:57.69ms
step:751/2330 train_time:43287ms step_avg:57.64ms
step:752/2330 train_time:43309ms step_avg:57.59ms
step:753/2330 train_time:43365ms step_avg:57.59ms
step:754/2330 train_time:43426ms step_avg:57.59ms
step:755/2330 train_time:43484ms step_avg:57.59ms
step:756/2330 train_time:43544ms step_avg:57.60ms
step:757/2330 train_time:43599ms step_avg:57.60ms
step:758/2330 train_time:43660ms step_avg:57.60ms
step:759/2330 train_time:43715ms step_avg:57.60ms
step:760/2330 train_time:43774ms step_avg:57.60ms
step:761/2330 train_time:43829ms step_avg:57.59ms
step:762/2330 train_time:43887ms step_avg:57.59ms
step:763/2330 train_time:43943ms step_avg:57.59ms
step:764/2330 train_time:44001ms step_avg:57.59ms
step:765/2330 train_time:44057ms step_avg:57.59ms
step:766/2330 train_time:44115ms step_avg:57.59ms
step:767/2330 train_time:44171ms step_avg:57.59ms
step:768/2330 train_time:44232ms step_avg:57.59ms
step:769/2330 train_time:44289ms step_avg:57.59ms
step:770/2330 train_time:44351ms step_avg:57.60ms
step:771/2330 train_time:44408ms step_avg:57.60ms
step:772/2330 train_time:44469ms step_avg:57.60ms
step:773/2330 train_time:44526ms step_avg:57.60ms
step:774/2330 train_time:44587ms step_avg:57.61ms
step:775/2330 train_time:44644ms step_avg:57.61ms
step:776/2330 train_time:44704ms step_avg:57.61ms
step:777/2330 train_time:44761ms step_avg:57.61ms
step:778/2330 train_time:44820ms step_avg:57.61ms
step:779/2330 train_time:44877ms step_avg:57.61ms
step:780/2330 train_time:44935ms step_avg:57.61ms
step:781/2330 train_time:44992ms step_avg:57.61ms
step:782/2330 train_time:45051ms step_avg:57.61ms
step:783/2330 train_time:45108ms step_avg:57.61ms
step:784/2330 train_time:45167ms step_avg:57.61ms
step:785/2330 train_time:45224ms step_avg:57.61ms
step:786/2330 train_time:45285ms step_avg:57.61ms
step:787/2330 train_time:45342ms step_avg:57.61ms
step:788/2330 train_time:45403ms step_avg:57.62ms
step:789/2330 train_time:45460ms step_avg:57.62ms
step:790/2330 train_time:45521ms step_avg:57.62ms
step:791/2330 train_time:45578ms step_avg:57.62ms
step:792/2330 train_time:45638ms step_avg:57.62ms
step:793/2330 train_time:45695ms step_avg:57.62ms
step:794/2330 train_time:45754ms step_avg:57.62ms
step:795/2330 train_time:45810ms step_avg:57.62ms
step:796/2330 train_time:45870ms step_avg:57.63ms
step:797/2330 train_time:45927ms step_avg:57.62ms
step:798/2330 train_time:45985ms step_avg:57.63ms
step:799/2330 train_time:46042ms step_avg:57.62ms
step:800/2330 train_time:46102ms step_avg:57.63ms
step:801/2330 train_time:46159ms step_avg:57.63ms
step:802/2330 train_time:46218ms step_avg:57.63ms
step:803/2330 train_time:46275ms step_avg:57.63ms
step:804/2330 train_time:46335ms step_avg:57.63ms
step:805/2330 train_time:46392ms step_avg:57.63ms
step:806/2330 train_time:46453ms step_avg:57.63ms
step:807/2330 train_time:46509ms step_avg:57.63ms
step:808/2330 train_time:46571ms step_avg:57.64ms
step:809/2330 train_time:46627ms step_avg:57.63ms
step:810/2330 train_time:46688ms step_avg:57.64ms
step:811/2330 train_time:46745ms step_avg:57.64ms
step:812/2330 train_time:46805ms step_avg:57.64ms
step:813/2330 train_time:46862ms step_avg:57.64ms
step:814/2330 train_time:46922ms step_avg:57.64ms
step:815/2330 train_time:46979ms step_avg:57.64ms
step:816/2330 train_time:47038ms step_avg:57.64ms
step:817/2330 train_time:47094ms step_avg:57.64ms
step:818/2330 train_time:47154ms step_avg:57.65ms
step:819/2330 train_time:47211ms step_avg:57.64ms
step:820/2330 train_time:47271ms step_avg:57.65ms
step:821/2330 train_time:47328ms step_avg:57.65ms
step:822/2330 train_time:47388ms step_avg:57.65ms
step:823/2330 train_time:47445ms step_avg:57.65ms
step:824/2330 train_time:47505ms step_avg:57.65ms
step:825/2330 train_time:47562ms step_avg:57.65ms
step:826/2330 train_time:47622ms step_avg:57.65ms
step:827/2330 train_time:47679ms step_avg:57.65ms
step:828/2330 train_time:47740ms step_avg:57.66ms
step:829/2330 train_time:47796ms step_avg:57.66ms
step:830/2330 train_time:47857ms step_avg:57.66ms
step:831/2330 train_time:47913ms step_avg:57.66ms
step:832/2330 train_time:47973ms step_avg:57.66ms
step:833/2330 train_time:48029ms step_avg:57.66ms
step:834/2330 train_time:48089ms step_avg:57.66ms
step:835/2330 train_time:48145ms step_avg:57.66ms
step:836/2330 train_time:48206ms step_avg:57.66ms
step:837/2330 train_time:48262ms step_avg:57.66ms
step:838/2330 train_time:48322ms step_avg:57.66ms
step:839/2330 train_time:48378ms step_avg:57.66ms
step:840/2330 train_time:48439ms step_avg:57.67ms
step:841/2330 train_time:48496ms step_avg:57.66ms
step:842/2330 train_time:48556ms step_avg:57.67ms
step:843/2330 train_time:48612ms step_avg:57.67ms
step:844/2330 train_time:48675ms step_avg:57.67ms
step:845/2330 train_time:48731ms step_avg:57.67ms
step:846/2330 train_time:48791ms step_avg:57.67ms
step:847/2330 train_time:48847ms step_avg:57.67ms
step:848/2330 train_time:48908ms step_avg:57.67ms
step:849/2330 train_time:48965ms step_avg:57.67ms
step:850/2330 train_time:49024ms step_avg:57.67ms
step:851/2330 train_time:49080ms step_avg:57.67ms
step:852/2330 train_time:49140ms step_avg:57.68ms
step:853/2330 train_time:49196ms step_avg:57.67ms
step:854/2330 train_time:49257ms step_avg:57.68ms
step:855/2330 train_time:49314ms step_avg:57.68ms
step:856/2330 train_time:49374ms step_avg:57.68ms
step:857/2330 train_time:49430ms step_avg:57.68ms
step:858/2330 train_time:49491ms step_avg:57.68ms
step:859/2330 train_time:49548ms step_avg:57.68ms
step:860/2330 train_time:49609ms step_avg:57.68ms
step:861/2330 train_time:49666ms step_avg:57.68ms
step:862/2330 train_time:49726ms step_avg:57.69ms
step:863/2330 train_time:49782ms step_avg:57.69ms
step:864/2330 train_time:49842ms step_avg:57.69ms
step:865/2330 train_time:49899ms step_avg:57.69ms
step:866/2330 train_time:49959ms step_avg:57.69ms
step:867/2330 train_time:50015ms step_avg:57.69ms
step:868/2330 train_time:50076ms step_avg:57.69ms
step:869/2330 train_time:50132ms step_avg:57.69ms
step:870/2330 train_time:50193ms step_avg:57.69ms
step:871/2330 train_time:50250ms step_avg:57.69ms
step:872/2330 train_time:50310ms step_avg:57.70ms
step:873/2330 train_time:50367ms step_avg:57.69ms
step:874/2330 train_time:50427ms step_avg:57.70ms
step:875/2330 train_time:50484ms step_avg:57.70ms
step:876/2330 train_time:50544ms step_avg:57.70ms
step:877/2330 train_time:50601ms step_avg:57.70ms
step:878/2330 train_time:50662ms step_avg:57.70ms
step:879/2330 train_time:50718ms step_avg:57.70ms
step:880/2330 train_time:50779ms step_avg:57.70ms
step:881/2330 train_time:50835ms step_avg:57.70ms
step:882/2330 train_time:50896ms step_avg:57.70ms
step:883/2330 train_time:50952ms step_avg:57.70ms
step:884/2330 train_time:51012ms step_avg:57.71ms
step:885/2330 train_time:51069ms step_avg:57.70ms
step:886/2330 train_time:51129ms step_avg:57.71ms
step:887/2330 train_time:51185ms step_avg:57.71ms
step:888/2330 train_time:51245ms step_avg:57.71ms
step:889/2330 train_time:51303ms step_avg:57.71ms
step:890/2330 train_time:51362ms step_avg:57.71ms
step:891/2330 train_time:51419ms step_avg:57.71ms
step:892/2330 train_time:51479ms step_avg:57.71ms
step:893/2330 train_time:51535ms step_avg:57.71ms
step:894/2330 train_time:51596ms step_avg:57.71ms
step:895/2330 train_time:51653ms step_avg:57.71ms
step:896/2330 train_time:51714ms step_avg:57.72ms
step:897/2330 train_time:51770ms step_avg:57.71ms
step:898/2330 train_time:51830ms step_avg:57.72ms
step:899/2330 train_time:51886ms step_avg:57.72ms
step:900/2330 train_time:51946ms step_avg:57.72ms
step:901/2330 train_time:52003ms step_avg:57.72ms
step:902/2330 train_time:52063ms step_avg:57.72ms
step:903/2330 train_time:52120ms step_avg:57.72ms
step:904/2330 train_time:52180ms step_avg:57.72ms
step:905/2330 train_time:52237ms step_avg:57.72ms
step:906/2330 train_time:52297ms step_avg:57.72ms
step:907/2330 train_time:52353ms step_avg:57.72ms
step:908/2330 train_time:52413ms step_avg:57.72ms
step:909/2330 train_time:52470ms step_avg:57.72ms
step:910/2330 train_time:52530ms step_avg:57.72ms
step:911/2330 train_time:52586ms step_avg:57.72ms
step:912/2330 train_time:52647ms step_avg:57.73ms
step:913/2330 train_time:52704ms step_avg:57.73ms
step:914/2330 train_time:52764ms step_avg:57.73ms
step:915/2330 train_time:52821ms step_avg:57.73ms
step:916/2330 train_time:52881ms step_avg:57.73ms
step:917/2330 train_time:52937ms step_avg:57.73ms
step:918/2330 train_time:52996ms step_avg:57.73ms
step:919/2330 train_time:53053ms step_avg:57.73ms
step:920/2330 train_time:53114ms step_avg:57.73ms
step:921/2330 train_time:53170ms step_avg:57.73ms
step:922/2330 train_time:53230ms step_avg:57.73ms
step:923/2330 train_time:53287ms step_avg:57.73ms
step:924/2330 train_time:53347ms step_avg:57.74ms
step:925/2330 train_time:53404ms step_avg:57.73ms
step:926/2330 train_time:53464ms step_avg:57.74ms
step:927/2330 train_time:53521ms step_avg:57.74ms
step:928/2330 train_time:53581ms step_avg:57.74ms
step:929/2330 train_time:53637ms step_avg:57.74ms
step:930/2330 train_time:53698ms step_avg:57.74ms
step:931/2330 train_time:53755ms step_avg:57.74ms
step:932/2330 train_time:53815ms step_avg:57.74ms
step:933/2330 train_time:53872ms step_avg:57.74ms
step:934/2330 train_time:53931ms step_avg:57.74ms
step:935/2330 train_time:53988ms step_avg:57.74ms
step:936/2330 train_time:54048ms step_avg:57.74ms
step:937/2330 train_time:54105ms step_avg:57.74ms
step:938/2330 train_time:54164ms step_avg:57.74ms
step:939/2330 train_time:54221ms step_avg:57.74ms
step:940/2330 train_time:54281ms step_avg:57.75ms
step:941/2330 train_time:54337ms step_avg:57.74ms
step:942/2330 train_time:54397ms step_avg:57.75ms
step:943/2330 train_time:54454ms step_avg:57.75ms
step:944/2330 train_time:54514ms step_avg:57.75ms
step:945/2330 train_time:54571ms step_avg:57.75ms
step:946/2330 train_time:54631ms step_avg:57.75ms
step:947/2330 train_time:54688ms step_avg:57.75ms
step:948/2330 train_time:54748ms step_avg:57.75ms
step:949/2330 train_time:54805ms step_avg:57.75ms
step:950/2330 train_time:54865ms step_avg:57.75ms
step:951/2330 train_time:54921ms step_avg:57.75ms
step:952/2330 train_time:54981ms step_avg:57.75ms
step:953/2330 train_time:55038ms step_avg:57.75ms
step:954/2330 train_time:55099ms step_avg:57.76ms
step:955/2330 train_time:55155ms step_avg:57.75ms
step:956/2330 train_time:55215ms step_avg:57.76ms
step:957/2330 train_time:55271ms step_avg:57.75ms
step:958/2330 train_time:55332ms step_avg:57.76ms
step:959/2330 train_time:55388ms step_avg:57.76ms
step:960/2330 train_time:55449ms step_avg:57.76ms
step:961/2330 train_time:55506ms step_avg:57.76ms
step:962/2330 train_time:55565ms step_avg:57.76ms
step:963/2330 train_time:55622ms step_avg:57.76ms
step:964/2330 train_time:55682ms step_avg:57.76ms
step:965/2330 train_time:55739ms step_avg:57.76ms
step:966/2330 train_time:55799ms step_avg:57.76ms
step:967/2330 train_time:55855ms step_avg:57.76ms
step:968/2330 train_time:55916ms step_avg:57.76ms
step:969/2330 train_time:55972ms step_avg:57.76ms
step:970/2330 train_time:56033ms step_avg:57.77ms
step:971/2330 train_time:56090ms step_avg:57.76ms
step:972/2330 train_time:56151ms step_avg:57.77ms
step:973/2330 train_time:56207ms step_avg:57.77ms
step:974/2330 train_time:56267ms step_avg:57.77ms
step:975/2330 train_time:56324ms step_avg:57.77ms
step:976/2330 train_time:56385ms step_avg:57.77ms
step:977/2330 train_time:56441ms step_avg:57.77ms
step:978/2330 train_time:56502ms step_avg:57.77ms
step:979/2330 train_time:56559ms step_avg:57.77ms
step:980/2330 train_time:56619ms step_avg:57.77ms
step:981/2330 train_time:56675ms step_avg:57.77ms
step:982/2330 train_time:56735ms step_avg:57.78ms
step:983/2330 train_time:56793ms step_avg:57.77ms
step:984/2330 train_time:56853ms step_avg:57.78ms
step:985/2330 train_time:56910ms step_avg:57.78ms
step:986/2330 train_time:56969ms step_avg:57.78ms
step:987/2330 train_time:57026ms step_avg:57.78ms
step:988/2330 train_time:57086ms step_avg:57.78ms
step:989/2330 train_time:57142ms step_avg:57.78ms
step:990/2330 train_time:57202ms step_avg:57.78ms
step:991/2330 train_time:57258ms step_avg:57.78ms
step:992/2330 train_time:57318ms step_avg:57.78ms
step:993/2330 train_time:57374ms step_avg:57.78ms
step:994/2330 train_time:57434ms step_avg:57.78ms
step:995/2330 train_time:57490ms step_avg:57.78ms
step:996/2330 train_time:57551ms step_avg:57.78ms
step:997/2330 train_time:57607ms step_avg:57.78ms
step:998/2330 train_time:57668ms step_avg:57.78ms
step:999/2330 train_time:57725ms step_avg:57.78ms
step:1000/2330 train_time:57784ms step_avg:57.78ms
step:1000/2330 val_loss:4.0922 train_time:57865ms step_avg:57.86ms
step:1001/2330 train_time:57886ms step_avg:57.83ms
step:1002/2330 train_time:57907ms step_avg:57.79ms
step:1003/2330 train_time:57965ms step_avg:57.79ms
step:1004/2330 train_time:58027ms step_avg:57.80ms
step:1005/2330 train_time:58084ms step_avg:57.80ms
step:1006/2330 train_time:58144ms step_avg:57.80ms
step:1007/2330 train_time:58200ms step_avg:57.80ms
step:1008/2330 train_time:58259ms step_avg:57.80ms
step:1009/2330 train_time:58315ms step_avg:57.80ms
step:1010/2330 train_time:58374ms step_avg:57.80ms
step:1011/2330 train_time:58431ms step_avg:57.79ms
step:1012/2330 train_time:58489ms step_avg:57.80ms
step:1013/2330 train_time:58545ms step_avg:57.79ms
step:1014/2330 train_time:58604ms step_avg:57.80ms
step:1015/2330 train_time:58660ms step_avg:57.79ms
step:1016/2330 train_time:58719ms step_avg:57.79ms
step:1017/2330 train_time:58775ms step_avg:57.79ms
step:1018/2330 train_time:58839ms step_avg:57.80ms
step:1019/2330 train_time:58897ms step_avg:57.80ms
step:1020/2330 train_time:58959ms step_avg:57.80ms
step:1021/2330 train_time:59016ms step_avg:57.80ms
step:1022/2330 train_time:59077ms step_avg:57.80ms
step:1023/2330 train_time:59135ms step_avg:57.81ms
step:1024/2330 train_time:59194ms step_avg:57.81ms
step:1025/2330 train_time:59251ms step_avg:57.81ms
step:1026/2330 train_time:59311ms step_avg:57.81ms
step:1027/2330 train_time:59367ms step_avg:57.81ms
step:1028/2330 train_time:59426ms step_avg:57.81ms
step:1029/2330 train_time:59482ms step_avg:57.81ms
step:1030/2330 train_time:59541ms step_avg:57.81ms
step:1031/2330 train_time:59597ms step_avg:57.81ms
step:1032/2330 train_time:59656ms step_avg:57.81ms
step:1033/2330 train_time:59713ms step_avg:57.81ms
step:1034/2330 train_time:59774ms step_avg:57.81ms
step:1035/2330 train_time:59830ms step_avg:57.81ms
step:1036/2330 train_time:59893ms step_avg:57.81ms
step:1037/2330 train_time:59950ms step_avg:57.81ms
step:1038/2330 train_time:60012ms step_avg:57.82ms
step:1039/2330 train_time:60069ms step_avg:57.81ms
step:1040/2330 train_time:60131ms step_avg:57.82ms
step:1041/2330 train_time:60188ms step_avg:57.82ms
step:1042/2330 train_time:60248ms step_avg:57.82ms
step:1043/2330 train_time:60304ms step_avg:57.82ms
step:1044/2330 train_time:60364ms step_avg:57.82ms
step:1045/2330 train_time:60420ms step_avg:57.82ms
step:1046/2330 train_time:60481ms step_avg:57.82ms
step:1047/2330 train_time:60537ms step_avg:57.82ms
step:1048/2330 train_time:60597ms step_avg:57.82ms
step:1049/2330 train_time:60653ms step_avg:57.82ms
step:1050/2330 train_time:60712ms step_avg:57.82ms
step:1051/2330 train_time:60769ms step_avg:57.82ms
step:1052/2330 train_time:60829ms step_avg:57.82ms
step:1053/2330 train_time:60885ms step_avg:57.82ms
step:1054/2330 train_time:60948ms step_avg:57.83ms
step:1055/2330 train_time:61004ms step_avg:57.82ms
step:1056/2330 train_time:61067ms step_avg:57.83ms
step:1057/2330 train_time:61123ms step_avg:57.83ms
step:1058/2330 train_time:61185ms step_avg:57.83ms
step:1059/2330 train_time:61241ms step_avg:57.83ms
step:1060/2330 train_time:61302ms step_avg:57.83ms
step:1061/2330 train_time:61358ms step_avg:57.83ms
step:1062/2330 train_time:61418ms step_avg:57.83ms
step:1063/2330 train_time:61475ms step_avg:57.83ms
step:1064/2330 train_time:61534ms step_avg:57.83ms
step:1065/2330 train_time:61590ms step_avg:57.83ms
step:1066/2330 train_time:61650ms step_avg:57.83ms
step:1067/2330 train_time:61707ms step_avg:57.83ms
step:1068/2330 train_time:61767ms step_avg:57.83ms
step:1069/2330 train_time:61823ms step_avg:57.83ms
step:1070/2330 train_time:61885ms step_avg:57.84ms
step:1071/2330 train_time:61942ms step_avg:57.84ms
step:1072/2330 train_time:62003ms step_avg:57.84ms
step:1073/2330 train_time:62059ms step_avg:57.84ms
step:1074/2330 train_time:62120ms step_avg:57.84ms
step:1075/2330 train_time:62177ms step_avg:57.84ms
step:1076/2330 train_time:62237ms step_avg:57.84ms
step:1077/2330 train_time:62293ms step_avg:57.84ms
step:1078/2330 train_time:62353ms step_avg:57.84ms
step:1079/2330 train_time:62410ms step_avg:57.84ms
step:1080/2330 train_time:62469ms step_avg:57.84ms
step:1081/2330 train_time:62525ms step_avg:57.84ms
step:1082/2330 train_time:62585ms step_avg:57.84ms
step:1083/2330 train_time:62642ms step_avg:57.84ms
step:1084/2330 train_time:62702ms step_avg:57.84ms
step:1085/2330 train_time:62759ms step_avg:57.84ms
step:1086/2330 train_time:62819ms step_avg:57.84ms
step:1087/2330 train_time:62876ms step_avg:57.84ms
step:1088/2330 train_time:62936ms step_avg:57.85ms
step:1089/2330 train_time:62993ms step_avg:57.84ms
step:1090/2330 train_time:63054ms step_avg:57.85ms
step:1091/2330 train_time:63111ms step_avg:57.85ms
step:1092/2330 train_time:63172ms step_avg:57.85ms
step:1093/2330 train_time:63228ms step_avg:57.85ms
step:1094/2330 train_time:63289ms step_avg:57.85ms
step:1095/2330 train_time:63345ms step_avg:57.85ms
step:1096/2330 train_time:63407ms step_avg:57.85ms
step:1097/2330 train_time:63464ms step_avg:57.85ms
step:1098/2330 train_time:63523ms step_avg:57.85ms
step:1099/2330 train_time:63580ms step_avg:57.85ms
step:1100/2330 train_time:63640ms step_avg:57.85ms
step:1101/2330 train_time:63696ms step_avg:57.85ms
step:1102/2330 train_time:63756ms step_avg:57.85ms
step:1103/2330 train_time:63812ms step_avg:57.85ms
step:1104/2330 train_time:63872ms step_avg:57.86ms
step:1105/2330 train_time:63929ms step_avg:57.85ms
step:1106/2330 train_time:63988ms step_avg:57.86ms
step:1107/2330 train_time:64045ms step_avg:57.85ms
step:1108/2330 train_time:64107ms step_avg:57.86ms
step:1109/2330 train_time:64163ms step_avg:57.86ms
step:1110/2330 train_time:64225ms step_avg:57.86ms
step:1111/2330 train_time:64282ms step_avg:57.86ms
step:1112/2330 train_time:64342ms step_avg:57.86ms
step:1113/2330 train_time:64398ms step_avg:57.86ms
step:1114/2330 train_time:64458ms step_avg:57.86ms
step:1115/2330 train_time:64515ms step_avg:57.86ms
step:1116/2330 train_time:64575ms step_avg:57.86ms
step:1117/2330 train_time:64631ms step_avg:57.86ms
step:1118/2330 train_time:64691ms step_avg:57.86ms
step:1119/2330 train_time:64747ms step_avg:57.86ms
step:1120/2330 train_time:64808ms step_avg:57.86ms
step:1121/2330 train_time:64865ms step_avg:57.86ms
step:1122/2330 train_time:64925ms step_avg:57.87ms
step:1123/2330 train_time:64982ms step_avg:57.86ms
step:1124/2330 train_time:65042ms step_avg:57.87ms
step:1125/2330 train_time:65099ms step_avg:57.87ms
step:1126/2330 train_time:65158ms step_avg:57.87ms
step:1127/2330 train_time:65216ms step_avg:57.87ms
step:1128/2330 train_time:65276ms step_avg:57.87ms
step:1129/2330 train_time:65334ms step_avg:57.87ms
step:1130/2330 train_time:65393ms step_avg:57.87ms
step:1131/2330 train_time:65449ms step_avg:57.87ms
step:1132/2330 train_time:65510ms step_avg:57.87ms
step:1133/2330 train_time:65567ms step_avg:57.87ms
step:1134/2330 train_time:65628ms step_avg:57.87ms
step:1135/2330 train_time:65684ms step_avg:57.87ms
step:1136/2330 train_time:65744ms step_avg:57.87ms
step:1137/2330 train_time:65801ms step_avg:57.87ms
step:1138/2330 train_time:65860ms step_avg:57.87ms
step:1139/2330 train_time:65917ms step_avg:57.87ms
step:1140/2330 train_time:65977ms step_avg:57.87ms
step:1141/2330 train_time:66034ms step_avg:57.87ms
step:1142/2330 train_time:66094ms step_avg:57.88ms
step:1143/2330 train_time:66150ms step_avg:57.87ms
step:1144/2330 train_time:66211ms step_avg:57.88ms
step:1145/2330 train_time:66267ms step_avg:57.88ms
step:1146/2330 train_time:66327ms step_avg:57.88ms
step:1147/2330 train_time:66383ms step_avg:57.88ms
step:1148/2330 train_time:66445ms step_avg:57.88ms
step:1149/2330 train_time:66501ms step_avg:57.88ms
step:1150/2330 train_time:66562ms step_avg:57.88ms
step:1151/2330 train_time:66619ms step_avg:57.88ms
step:1152/2330 train_time:66678ms step_avg:57.88ms
step:1153/2330 train_time:66736ms step_avg:57.88ms
step:1154/2330 train_time:66795ms step_avg:57.88ms
step:1155/2330 train_time:66852ms step_avg:57.88ms
step:1156/2330 train_time:66912ms step_avg:57.88ms
step:1157/2330 train_time:66969ms step_avg:57.88ms
step:1158/2330 train_time:67029ms step_avg:57.88ms
step:1159/2330 train_time:67085ms step_avg:57.88ms
step:1160/2330 train_time:67146ms step_avg:57.88ms
step:1161/2330 train_time:67203ms step_avg:57.88ms
step:1162/2330 train_time:67264ms step_avg:57.89ms
step:1163/2330 train_time:67320ms step_avg:57.88ms
step:1164/2330 train_time:67380ms step_avg:57.89ms
step:1165/2330 train_time:67437ms step_avg:57.89ms
step:1166/2330 train_time:67496ms step_avg:57.89ms
step:1167/2330 train_time:67554ms step_avg:57.89ms
step:1168/2330 train_time:67613ms step_avg:57.89ms
step:1169/2330 train_time:67670ms step_avg:57.89ms
step:1170/2330 train_time:67730ms step_avg:57.89ms
step:1171/2330 train_time:67786ms step_avg:57.89ms
step:1172/2330 train_time:67847ms step_avg:57.89ms
step:1173/2330 train_time:67904ms step_avg:57.89ms
step:1174/2330 train_time:67964ms step_avg:57.89ms
step:1175/2330 train_time:68021ms step_avg:57.89ms
step:1176/2330 train_time:68080ms step_avg:57.89ms
step:1177/2330 train_time:68137ms step_avg:57.89ms
step:1178/2330 train_time:68198ms step_avg:57.89ms
step:1179/2330 train_time:68254ms step_avg:57.89ms
step:1180/2330 train_time:68315ms step_avg:57.89ms
step:1181/2330 train_time:68371ms step_avg:57.89ms
step:1182/2330 train_time:68431ms step_avg:57.89ms
step:1183/2330 train_time:68487ms step_avg:57.89ms
step:1184/2330 train_time:68548ms step_avg:57.90ms
step:1185/2330 train_time:68605ms step_avg:57.89ms
step:1186/2330 train_time:68665ms step_avg:57.90ms
step:1187/2330 train_time:68721ms step_avg:57.89ms
step:1188/2330 train_time:68782ms step_avg:57.90ms
step:1189/2330 train_time:68838ms step_avg:57.90ms
step:1190/2330 train_time:68898ms step_avg:57.90ms
step:1191/2330 train_time:68955ms step_avg:57.90ms
step:1192/2330 train_time:69016ms step_avg:57.90ms
step:1193/2330 train_time:69072ms step_avg:57.90ms
step:1194/2330 train_time:69132ms step_avg:57.90ms
step:1195/2330 train_time:69188ms step_avg:57.90ms
step:1196/2330 train_time:69249ms step_avg:57.90ms
step:1197/2330 train_time:69305ms step_avg:57.90ms
step:1198/2330 train_time:69366ms step_avg:57.90ms
step:1199/2330 train_time:69422ms step_avg:57.90ms
step:1200/2330 train_time:69483ms step_avg:57.90ms
step:1201/2330 train_time:69540ms step_avg:57.90ms
step:1202/2330 train_time:69600ms step_avg:57.90ms
step:1203/2330 train_time:69658ms step_avg:57.90ms
step:1204/2330 train_time:69717ms step_avg:57.90ms
step:1205/2330 train_time:69774ms step_avg:57.90ms
step:1206/2330 train_time:69833ms step_avg:57.90ms
step:1207/2330 train_time:69890ms step_avg:57.90ms
step:1208/2330 train_time:69951ms step_avg:57.91ms
step:1209/2330 train_time:70007ms step_avg:57.91ms
step:1210/2330 train_time:70067ms step_avg:57.91ms
step:1211/2330 train_time:70125ms step_avg:57.91ms
step:1212/2330 train_time:70185ms step_avg:57.91ms
step:1213/2330 train_time:70241ms step_avg:57.91ms
step:1214/2330 train_time:70303ms step_avg:57.91ms
step:1215/2330 train_time:70360ms step_avg:57.91ms
step:1216/2330 train_time:70419ms step_avg:57.91ms
step:1217/2330 train_time:70476ms step_avg:57.91ms
step:1218/2330 train_time:70536ms step_avg:57.91ms
step:1219/2330 train_time:70593ms step_avg:57.91ms
step:1220/2330 train_time:70652ms step_avg:57.91ms
step:1221/2330 train_time:70709ms step_avg:57.91ms
step:1222/2330 train_time:70769ms step_avg:57.91ms
step:1223/2330 train_time:70825ms step_avg:57.91ms
step:1224/2330 train_time:70886ms step_avg:57.91ms
step:1225/2330 train_time:70942ms step_avg:57.91ms
step:1226/2330 train_time:71003ms step_avg:57.91ms
step:1227/2330 train_time:71059ms step_avg:57.91ms
step:1228/2330 train_time:71120ms step_avg:57.92ms
step:1229/2330 train_time:71178ms step_avg:57.91ms
step:1230/2330 train_time:71237ms step_avg:57.92ms
step:1231/2330 train_time:71294ms step_avg:57.92ms
step:1232/2330 train_time:71354ms step_avg:57.92ms
step:1233/2330 train_time:71410ms step_avg:57.92ms
step:1234/2330 train_time:71470ms step_avg:57.92ms
step:1235/2330 train_time:71527ms step_avg:57.92ms
step:1236/2330 train_time:71587ms step_avg:57.92ms
step:1237/2330 train_time:71643ms step_avg:57.92ms
step:1238/2330 train_time:71705ms step_avg:57.92ms
step:1239/2330 train_time:71762ms step_avg:57.92ms
step:1240/2330 train_time:71823ms step_avg:57.92ms
step:1241/2330 train_time:71879ms step_avg:57.92ms
step:1242/2330 train_time:71939ms step_avg:57.92ms
step:1243/2330 train_time:71996ms step_avg:57.92ms
step:1244/2330 train_time:72055ms step_avg:57.92ms
step:1245/2330 train_time:72112ms step_avg:57.92ms
step:1246/2330 train_time:72173ms step_avg:57.92ms
step:1247/2330 train_time:72229ms step_avg:57.92ms
step:1248/2330 train_time:72289ms step_avg:57.92ms
step:1249/2330 train_time:72346ms step_avg:57.92ms
step:1250/2330 train_time:72407ms step_avg:57.93ms
step:1250/2330 val_loss:4.0091 train_time:72488ms step_avg:57.99ms
step:1251/2330 train_time:72508ms step_avg:57.96ms
step:1252/2330 train_time:72527ms step_avg:57.93ms
step:1253/2330 train_time:72589ms step_avg:57.93ms
step:1254/2330 train_time:72653ms step_avg:57.94ms
step:1255/2330 train_time:72709ms step_avg:57.94ms
step:1256/2330 train_time:72768ms step_avg:57.94ms
step:1257/2330 train_time:72825ms step_avg:57.94ms
step:1258/2330 train_time:72884ms step_avg:57.94ms
step:1259/2330 train_time:72941ms step_avg:57.94ms
step:1260/2330 train_time:73001ms step_avg:57.94ms
step:1261/2330 train_time:73057ms step_avg:57.94ms
step:1262/2330 train_time:73116ms step_avg:57.94ms
step:1263/2330 train_time:73172ms step_avg:57.94ms
step:1264/2330 train_time:73231ms step_avg:57.94ms
step:1265/2330 train_time:73287ms step_avg:57.93ms
step:1266/2330 train_time:73347ms step_avg:57.94ms
step:1267/2330 train_time:73403ms step_avg:57.93ms
step:1268/2330 train_time:73463ms step_avg:57.94ms
step:1269/2330 train_time:73522ms step_avg:57.94ms
step:1270/2330 train_time:73585ms step_avg:57.94ms
step:1271/2330 train_time:73642ms step_avg:57.94ms
step:1272/2330 train_time:73703ms step_avg:57.94ms
step:1273/2330 train_time:73760ms step_avg:57.94ms
step:1274/2330 train_time:73820ms step_avg:57.94ms
step:1275/2330 train_time:73877ms step_avg:57.94ms
step:1276/2330 train_time:73937ms step_avg:57.94ms
step:1277/2330 train_time:73993ms step_avg:57.94ms
step:1278/2330 train_time:74053ms step_avg:57.94ms
step:1279/2330 train_time:74109ms step_avg:57.94ms
step:1280/2330 train_time:74168ms step_avg:57.94ms
step:1281/2330 train_time:74224ms step_avg:57.94ms
step:1282/2330 train_time:74284ms step_avg:57.94ms
step:1283/2330 train_time:74341ms step_avg:57.94ms
step:1284/2330 train_time:74400ms step_avg:57.94ms
step:1285/2330 train_time:74456ms step_avg:57.94ms
step:1286/2330 train_time:74518ms step_avg:57.95ms
step:1287/2330 train_time:74576ms step_avg:57.95ms
step:1288/2330 train_time:74637ms step_avg:57.95ms
step:1289/2330 train_time:74694ms step_avg:57.95ms
step:1290/2330 train_time:74755ms step_avg:57.95ms
step:1291/2330 train_time:74811ms step_avg:57.95ms
step:1292/2330 train_time:74874ms step_avg:57.95ms
step:1293/2330 train_time:74930ms step_avg:57.95ms
step:1294/2330 train_time:74992ms step_avg:57.95ms
step:1295/2330 train_time:75048ms step_avg:57.95ms
step:1296/2330 train_time:75108ms step_avg:57.95ms
step:1297/2330 train_time:75164ms step_avg:57.95ms
step:1298/2330 train_time:75224ms step_avg:57.95ms
step:1299/2330 train_time:75280ms step_avg:57.95ms
step:1300/2330 train_time:75340ms step_avg:57.95ms
step:1301/2330 train_time:75396ms step_avg:57.95ms
step:1302/2330 train_time:75457ms step_avg:57.95ms
step:1303/2330 train_time:75513ms step_avg:57.95ms
step:1304/2330 train_time:75575ms step_avg:57.96ms
step:1305/2330 train_time:75632ms step_avg:57.96ms
step:1306/2330 train_time:75693ms step_avg:57.96ms
step:1307/2330 train_time:75749ms step_avg:57.96ms
step:1308/2330 train_time:75810ms step_avg:57.96ms
step:1309/2330 train_time:75866ms step_avg:57.96ms
step:1310/2330 train_time:75927ms step_avg:57.96ms
step:1311/2330 train_time:75984ms step_avg:57.96ms
step:1312/2330 train_time:76044ms step_avg:57.96ms
step:1313/2330 train_time:76101ms step_avg:57.96ms
step:1314/2330 train_time:76160ms step_avg:57.96ms
step:1315/2330 train_time:76217ms step_avg:57.96ms
step:1316/2330 train_time:76276ms step_avg:57.96ms
step:1317/2330 train_time:76332ms step_avg:57.96ms
step:1318/2330 train_time:76392ms step_avg:57.96ms
step:1319/2330 train_time:76449ms step_avg:57.96ms
step:1320/2330 train_time:76509ms step_avg:57.96ms
step:1321/2330 train_time:76565ms step_avg:57.96ms
step:1322/2330 train_time:76627ms step_avg:57.96ms
step:1323/2330 train_time:76684ms step_avg:57.96ms
step:1324/2330 train_time:76745ms step_avg:57.96ms
step:1325/2330 train_time:76802ms step_avg:57.96ms
step:1326/2330 train_time:76862ms step_avg:57.97ms
step:1327/2330 train_time:76919ms step_avg:57.96ms
step:1328/2330 train_time:76979ms step_avg:57.97ms
step:1329/2330 train_time:77036ms step_avg:57.97ms
step:1330/2330 train_time:77096ms step_avg:57.97ms
step:1331/2330 train_time:77153ms step_avg:57.97ms
step:1332/2330 train_time:77213ms step_avg:57.97ms
step:1333/2330 train_time:77269ms step_avg:57.97ms
step:1334/2330 train_time:77329ms step_avg:57.97ms
step:1335/2330 train_time:77385ms step_avg:57.97ms
step:1336/2330 train_time:77446ms step_avg:57.97ms
step:1337/2330 train_time:77503ms step_avg:57.97ms
step:1338/2330 train_time:77563ms step_avg:57.97ms
step:1339/2330 train_time:77619ms step_avg:57.97ms
step:1340/2330 train_time:77680ms step_avg:57.97ms
step:1341/2330 train_time:77737ms step_avg:57.97ms
step:1342/2330 train_time:77797ms step_avg:57.97ms
step:1343/2330 train_time:77853ms step_avg:57.97ms
step:1344/2330 train_time:77915ms step_avg:57.97ms
step:1345/2330 train_time:77971ms step_avg:57.97ms
step:1346/2330 train_time:78032ms step_avg:57.97ms
step:1347/2330 train_time:78089ms step_avg:57.97ms
step:1348/2330 train_time:78149ms step_avg:57.97ms
step:1349/2330 train_time:78205ms step_avg:57.97ms
step:1350/2330 train_time:78265ms step_avg:57.97ms
step:1351/2330 train_time:78321ms step_avg:57.97ms
step:1352/2330 train_time:78381ms step_avg:57.97ms
step:1353/2330 train_time:78438ms step_avg:57.97ms
step:1354/2330 train_time:78498ms step_avg:57.97ms
step:1355/2330 train_time:78554ms step_avg:57.97ms
step:1356/2330 train_time:78615ms step_avg:57.98ms
step:1357/2330 train_time:78672ms step_avg:57.97ms
step:1358/2330 train_time:78732ms step_avg:57.98ms
step:1359/2330 train_time:78789ms step_avg:57.98ms
step:1360/2330 train_time:78849ms step_avg:57.98ms
step:1361/2330 train_time:78905ms step_avg:57.98ms
step:1362/2330 train_time:78966ms step_avg:57.98ms
step:1363/2330 train_time:79023ms step_avg:57.98ms
step:1364/2330 train_time:79083ms step_avg:57.98ms
step:1365/2330 train_time:79139ms step_avg:57.98ms
step:1366/2330 train_time:79199ms step_avg:57.98ms
step:1367/2330 train_time:79256ms step_avg:57.98ms
step:1368/2330 train_time:79316ms step_avg:57.98ms
step:1369/2330 train_time:79372ms step_avg:57.98ms
step:1370/2330 train_time:79433ms step_avg:57.98ms
step:1371/2330 train_time:79490ms step_avg:57.98ms
step:1372/2330 train_time:79551ms step_avg:57.98ms
step:1373/2330 train_time:79607ms step_avg:57.98ms
step:1374/2330 train_time:79668ms step_avg:57.98ms
step:1375/2330 train_time:79724ms step_avg:57.98ms
step:1376/2330 train_time:79784ms step_avg:57.98ms
step:1377/2330 train_time:79841ms step_avg:57.98ms
step:1378/2330 train_time:79901ms step_avg:57.98ms
step:1379/2330 train_time:79957ms step_avg:57.98ms
step:1380/2330 train_time:80017ms step_avg:57.98ms
step:1381/2330 train_time:80074ms step_avg:57.98ms
step:1382/2330 train_time:80134ms step_avg:57.98ms
step:1383/2330 train_time:80191ms step_avg:57.98ms
step:1384/2330 train_time:80251ms step_avg:57.98ms
step:1385/2330 train_time:80308ms step_avg:57.98ms
step:1386/2330 train_time:80367ms step_avg:57.99ms
step:1387/2330 train_time:80424ms step_avg:57.98ms
step:1388/2330 train_time:80484ms step_avg:57.99ms
step:1389/2330 train_time:80541ms step_avg:57.98ms
step:1390/2330 train_time:80601ms step_avg:57.99ms
step:1391/2330 train_time:80658ms step_avg:57.99ms
step:1392/2330 train_time:80718ms step_avg:57.99ms
step:1393/2330 train_time:80774ms step_avg:57.99ms
step:1394/2330 train_time:80835ms step_avg:57.99ms
step:1395/2330 train_time:80891ms step_avg:57.99ms
step:1396/2330 train_time:80952ms step_avg:57.99ms
step:1397/2330 train_time:81008ms step_avg:57.99ms
step:1398/2330 train_time:81068ms step_avg:57.99ms
step:1399/2330 train_time:81125ms step_avg:57.99ms
step:1400/2330 train_time:81185ms step_avg:57.99ms
step:1401/2330 train_time:81242ms step_avg:57.99ms
step:1402/2330 train_time:81302ms step_avg:57.99ms
step:1403/2330 train_time:81358ms step_avg:57.99ms
step:1404/2330 train_time:81418ms step_avg:57.99ms
step:1405/2330 train_time:81474ms step_avg:57.99ms
step:1406/2330 train_time:81536ms step_avg:57.99ms
step:1407/2330 train_time:81592ms step_avg:57.99ms
step:1408/2330 train_time:81652ms step_avg:57.99ms
step:1409/2330 train_time:81708ms step_avg:57.99ms
step:1410/2330 train_time:81770ms step_avg:57.99ms
step:1411/2330 train_time:81826ms step_avg:57.99ms
step:1412/2330 train_time:81887ms step_avg:57.99ms
step:1413/2330 train_time:81943ms step_avg:57.99ms
step:1414/2330 train_time:82003ms step_avg:57.99ms
step:1415/2330 train_time:82060ms step_avg:57.99ms
step:1416/2330 train_time:82121ms step_avg:58.00ms
step:1417/2330 train_time:82178ms step_avg:57.99ms
step:1418/2330 train_time:82238ms step_avg:58.00ms
step:1419/2330 train_time:82294ms step_avg:57.99ms
step:1420/2330 train_time:82354ms step_avg:58.00ms
step:1421/2330 train_time:82410ms step_avg:57.99ms
step:1422/2330 train_time:82472ms step_avg:58.00ms
step:1423/2330 train_time:82528ms step_avg:58.00ms
step:1424/2330 train_time:82589ms step_avg:58.00ms
step:1425/2330 train_time:82646ms step_avg:58.00ms
step:1426/2330 train_time:82705ms step_avg:58.00ms
step:1427/2330 train_time:82762ms step_avg:58.00ms
step:1428/2330 train_time:82821ms step_avg:58.00ms
step:1429/2330 train_time:82878ms step_avg:58.00ms
step:1430/2330 train_time:82938ms step_avg:58.00ms
step:1431/2330 train_time:82995ms step_avg:58.00ms
step:1432/2330 train_time:83056ms step_avg:58.00ms
step:1433/2330 train_time:83112ms step_avg:58.00ms
step:1434/2330 train_time:83174ms step_avg:58.00ms
step:1435/2330 train_time:83230ms step_avg:58.00ms
step:1436/2330 train_time:83290ms step_avg:58.00ms
step:1437/2330 train_time:83347ms step_avg:58.00ms
step:1438/2330 train_time:83407ms step_avg:58.00ms
step:1439/2330 train_time:83464ms step_avg:58.00ms
step:1440/2330 train_time:83523ms step_avg:58.00ms
step:1441/2330 train_time:83580ms step_avg:58.00ms
step:1442/2330 train_time:83641ms step_avg:58.00ms
step:1443/2330 train_time:83698ms step_avg:58.00ms
step:1444/2330 train_time:83757ms step_avg:58.00ms
step:1445/2330 train_time:83813ms step_avg:58.00ms
step:1446/2330 train_time:83875ms step_avg:58.00ms
step:1447/2330 train_time:83931ms step_avg:58.00ms
step:1448/2330 train_time:83992ms step_avg:58.01ms
step:1449/2330 train_time:84048ms step_avg:58.00ms
step:1450/2330 train_time:84109ms step_avg:58.01ms
step:1451/2330 train_time:84165ms step_avg:58.00ms
step:1452/2330 train_time:84226ms step_avg:58.01ms
step:1453/2330 train_time:84282ms step_avg:58.01ms
step:1454/2330 train_time:84343ms step_avg:58.01ms
step:1455/2330 train_time:84400ms step_avg:58.01ms
step:1456/2330 train_time:84460ms step_avg:58.01ms
step:1457/2330 train_time:84517ms step_avg:58.01ms
step:1458/2330 train_time:84578ms step_avg:58.01ms
step:1459/2330 train_time:84635ms step_avg:58.01ms
step:1460/2330 train_time:84694ms step_avg:58.01ms
step:1461/2330 train_time:84750ms step_avg:58.01ms
step:1462/2330 train_time:84811ms step_avg:58.01ms
step:1463/2330 train_time:84867ms step_avg:58.01ms
step:1464/2330 train_time:84928ms step_avg:58.01ms
step:1465/2330 train_time:84985ms step_avg:58.01ms
step:1466/2330 train_time:85045ms step_avg:58.01ms
step:1467/2330 train_time:85102ms step_avg:58.01ms
step:1468/2330 train_time:85162ms step_avg:58.01ms
step:1469/2330 train_time:85218ms step_avg:58.01ms
step:1470/2330 train_time:85278ms step_avg:58.01ms
step:1471/2330 train_time:85335ms step_avg:58.01ms
step:1472/2330 train_time:85395ms step_avg:58.01ms
step:1473/2330 train_time:85451ms step_avg:58.01ms
step:1474/2330 train_time:85512ms step_avg:58.01ms
step:1475/2330 train_time:85568ms step_avg:58.01ms
step:1476/2330 train_time:85630ms step_avg:58.01ms
step:1477/2330 train_time:85686ms step_avg:58.01ms
step:1478/2330 train_time:85746ms step_avg:58.02ms
step:1479/2330 train_time:85803ms step_avg:58.01ms
step:1480/2330 train_time:85863ms step_avg:58.02ms
step:1481/2330 train_time:85919ms step_avg:58.01ms
step:1482/2330 train_time:85980ms step_avg:58.02ms
step:1483/2330 train_time:86036ms step_avg:58.01ms
step:1484/2330 train_time:86097ms step_avg:58.02ms
step:1485/2330 train_time:86153ms step_avg:58.02ms
step:1486/2330 train_time:86214ms step_avg:58.02ms
step:1487/2330 train_time:86270ms step_avg:58.02ms
step:1488/2330 train_time:86331ms step_avg:58.02ms
step:1489/2330 train_time:86387ms step_avg:58.02ms
step:1490/2330 train_time:86448ms step_avg:58.02ms
step:1491/2330 train_time:86504ms step_avg:58.02ms
step:1492/2330 train_time:86565ms step_avg:58.02ms
step:1493/2330 train_time:86621ms step_avg:58.02ms
step:1494/2330 train_time:86681ms step_avg:58.02ms
step:1495/2330 train_time:86737ms step_avg:58.02ms
step:1496/2330 train_time:86797ms step_avg:58.02ms
step:1497/2330 train_time:86854ms step_avg:58.02ms
step:1498/2330 train_time:86915ms step_avg:58.02ms
step:1499/2330 train_time:86971ms step_avg:58.02ms
step:1500/2330 train_time:87032ms step_avg:58.02ms
step:1500/2330 val_loss:3.9262 train_time:87113ms step_avg:58.08ms
step:1501/2330 train_time:87133ms step_avg:58.05ms
step:1502/2330 train_time:87153ms step_avg:58.02ms
step:1503/2330 train_time:87210ms step_avg:58.02ms
step:1504/2330 train_time:87275ms step_avg:58.03ms
step:1505/2330 train_time:87332ms step_avg:58.03ms
step:1506/2330 train_time:87392ms step_avg:58.03ms
step:1507/2330 train_time:87449ms step_avg:58.03ms
step:1508/2330 train_time:87508ms step_avg:58.03ms
step:1509/2330 train_time:87564ms step_avg:58.03ms
step:1510/2330 train_time:87624ms step_avg:58.03ms
step:1511/2330 train_time:87680ms step_avg:58.03ms
step:1512/2330 train_time:87740ms step_avg:58.03ms
step:1513/2330 train_time:87795ms step_avg:58.03ms
step:1514/2330 train_time:87856ms step_avg:58.03ms
step:1515/2330 train_time:87912ms step_avg:58.03ms
step:1516/2330 train_time:87973ms step_avg:58.03ms
step:1517/2330 train_time:88030ms step_avg:58.03ms
step:1518/2330 train_time:88090ms step_avg:58.03ms
step:1519/2330 train_time:88147ms step_avg:58.03ms
step:1520/2330 train_time:88209ms step_avg:58.03ms
step:1521/2330 train_time:88268ms step_avg:58.03ms
step:1522/2330 train_time:88328ms step_avg:58.03ms
step:1523/2330 train_time:88385ms step_avg:58.03ms
step:1524/2330 train_time:88446ms step_avg:58.04ms
step:1525/2330 train_time:88502ms step_avg:58.03ms
step:1526/2330 train_time:88563ms step_avg:58.04ms
step:1527/2330 train_time:88619ms step_avg:58.03ms
step:1528/2330 train_time:88680ms step_avg:58.04ms
step:1529/2330 train_time:88738ms step_avg:58.04ms
step:1530/2330 train_time:88796ms step_avg:58.04ms
step:1531/2330 train_time:88852ms step_avg:58.04ms
step:1532/2330 train_time:88915ms step_avg:58.04ms
step:1533/2330 train_time:88971ms step_avg:58.04ms
step:1534/2330 train_time:89032ms step_avg:58.04ms
step:1535/2330 train_time:89089ms step_avg:58.04ms
step:1536/2330 train_time:89151ms step_avg:58.04ms
step:1537/2330 train_time:89208ms step_avg:58.04ms
step:1538/2330 train_time:89270ms step_avg:58.04ms
step:1539/2330 train_time:89328ms step_avg:58.04ms
step:1540/2330 train_time:89388ms step_avg:58.04ms
step:1541/2330 train_time:89446ms step_avg:58.04ms
step:1542/2330 train_time:89506ms step_avg:58.05ms
step:1543/2330 train_time:89563ms step_avg:58.04ms
step:1544/2330 train_time:89623ms step_avg:58.05ms
step:1545/2330 train_time:89681ms step_avg:58.05ms
step:1546/2330 train_time:89742ms step_avg:58.05ms
step:1547/2330 train_time:89798ms step_avg:58.05ms
step:1548/2330 train_time:89859ms step_avg:58.05ms
step:1549/2330 train_time:89916ms step_avg:58.05ms
step:1550/2330 train_time:89978ms step_avg:58.05ms
step:1551/2330 train_time:90035ms step_avg:58.05ms
step:1552/2330 train_time:90097ms step_avg:58.05ms
step:1553/2330 train_time:90153ms step_avg:58.05ms
step:1554/2330 train_time:90217ms step_avg:58.05ms
step:1555/2330 train_time:90273ms step_avg:58.05ms
step:1556/2330 train_time:90336ms step_avg:58.06ms
step:1557/2330 train_time:90392ms step_avg:58.06ms
step:1558/2330 train_time:90455ms step_avg:58.06ms
step:1559/2330 train_time:90512ms step_avg:58.06ms
step:1560/2330 train_time:90573ms step_avg:58.06ms
step:1561/2330 train_time:90631ms step_avg:58.06ms
step:1562/2330 train_time:90692ms step_avg:58.06ms
step:1563/2330 train_time:90750ms step_avg:58.06ms
step:1564/2330 train_time:90810ms step_avg:58.06ms
step:1565/2330 train_time:90868ms step_avg:58.06ms
step:1566/2330 train_time:90928ms step_avg:58.06ms
step:1567/2330 train_time:90986ms step_avg:58.06ms
step:1568/2330 train_time:91047ms step_avg:58.07ms
step:1569/2330 train_time:91104ms step_avg:58.06ms
step:1570/2330 train_time:91165ms step_avg:58.07ms
step:1571/2330 train_time:91223ms step_avg:58.07ms
step:1572/2330 train_time:91284ms step_avg:58.07ms
step:1573/2330 train_time:91341ms step_avg:58.07ms
step:1574/2330 train_time:91402ms step_avg:58.07ms
step:1575/2330 train_time:91460ms step_avg:58.07ms
step:1576/2330 train_time:91521ms step_avg:58.07ms
step:1577/2330 train_time:91578ms step_avg:58.07ms
step:1578/2330 train_time:91641ms step_avg:58.07ms
step:1579/2330 train_time:91697ms step_avg:58.07ms
step:1580/2330 train_time:91759ms step_avg:58.08ms
step:1581/2330 train_time:91815ms step_avg:58.07ms
step:1582/2330 train_time:91877ms step_avg:58.08ms
step:1583/2330 train_time:91933ms step_avg:58.08ms
step:1584/2330 train_time:91996ms step_avg:58.08ms
step:1585/2330 train_time:92052ms step_avg:58.08ms
step:1586/2330 train_time:92113ms step_avg:58.08ms
step:1587/2330 train_time:92170ms step_avg:58.08ms
step:1588/2330 train_time:92232ms step_avg:58.08ms
step:1589/2330 train_time:92289ms step_avg:58.08ms
step:1590/2330 train_time:92350ms step_avg:58.08ms
step:1591/2330 train_time:92407ms step_avg:58.08ms
step:1592/2330 train_time:92468ms step_avg:58.08ms
step:1593/2330 train_time:92526ms step_avg:58.08ms
step:1594/2330 train_time:92587ms step_avg:58.08ms
step:1595/2330 train_time:92645ms step_avg:58.08ms
step:1596/2330 train_time:92706ms step_avg:58.09ms
step:1597/2330 train_time:92764ms step_avg:58.09ms
step:1598/2330 train_time:92824ms step_avg:58.09ms
step:1599/2330 train_time:92881ms step_avg:58.09ms
step:1600/2330 train_time:92942ms step_avg:58.09ms
step:1601/2330 train_time:92999ms step_avg:58.09ms
step:1602/2330 train_time:93061ms step_avg:58.09ms
step:1603/2330 train_time:93118ms step_avg:58.09ms
step:1604/2330 train_time:93179ms step_avg:58.09ms
step:1605/2330 train_time:93236ms step_avg:58.09ms
step:1606/2330 train_time:93298ms step_avg:58.09ms
step:1607/2330 train_time:93355ms step_avg:58.09ms
step:1608/2330 train_time:93418ms step_avg:58.10ms
step:1609/2330 train_time:93474ms step_avg:58.09ms
step:1610/2330 train_time:93538ms step_avg:58.10ms
step:1611/2330 train_time:93594ms step_avg:58.10ms
step:1612/2330 train_time:93656ms step_avg:58.10ms
step:1613/2330 train_time:93713ms step_avg:58.10ms
step:1614/2330 train_time:93773ms step_avg:58.10ms
step:1615/2330 train_time:93831ms step_avg:58.10ms
step:1616/2330 train_time:93891ms step_avg:58.10ms
step:1617/2330 train_time:93949ms step_avg:58.10ms
step:1618/2330 train_time:94009ms step_avg:58.10ms
step:1619/2330 train_time:94067ms step_avg:58.10ms
step:1620/2330 train_time:94127ms step_avg:58.10ms
step:1621/2330 train_time:94185ms step_avg:58.10ms
step:1622/2330 train_time:94245ms step_avg:58.10ms
step:1623/2330 train_time:94303ms step_avg:58.10ms
step:1624/2330 train_time:94363ms step_avg:58.11ms
step:1625/2330 train_time:94420ms step_avg:58.10ms
step:1626/2330 train_time:94483ms step_avg:58.11ms
step:1627/2330 train_time:94539ms step_avg:58.11ms
step:1628/2330 train_time:94601ms step_avg:58.11ms
step:1629/2330 train_time:94658ms step_avg:58.11ms
step:1630/2330 train_time:94721ms step_avg:58.11ms
step:1631/2330 train_time:94777ms step_avg:58.11ms
step:1632/2330 train_time:94840ms step_avg:58.11ms
step:1633/2330 train_time:94896ms step_avg:58.11ms
step:1634/2330 train_time:94958ms step_avg:58.11ms
step:1635/2330 train_time:95014ms step_avg:58.11ms
step:1636/2330 train_time:95076ms step_avg:58.12ms
step:1637/2330 train_time:95133ms step_avg:58.11ms
step:1638/2330 train_time:95194ms step_avg:58.12ms
step:1639/2330 train_time:95251ms step_avg:58.12ms
step:1640/2330 train_time:95312ms step_avg:58.12ms
step:1641/2330 train_time:95370ms step_avg:58.12ms
step:1642/2330 train_time:95430ms step_avg:58.12ms
step:1643/2330 train_time:95488ms step_avg:58.12ms
step:1644/2330 train_time:95549ms step_avg:58.12ms
step:1645/2330 train_time:95606ms step_avg:58.12ms
step:1646/2330 train_time:95667ms step_avg:58.12ms
step:1647/2330 train_time:95724ms step_avg:58.12ms
step:1648/2330 train_time:95786ms step_avg:58.12ms
step:1649/2330 train_time:95843ms step_avg:58.12ms
step:1650/2330 train_time:95904ms step_avg:58.12ms
step:1651/2330 train_time:95961ms step_avg:58.12ms
step:1652/2330 train_time:96022ms step_avg:58.12ms
step:1653/2330 train_time:96078ms step_avg:58.12ms
step:1654/2330 train_time:96141ms step_avg:58.13ms
step:1655/2330 train_time:96197ms step_avg:58.12ms
step:1656/2330 train_time:96259ms step_avg:58.13ms
step:1657/2330 train_time:96315ms step_avg:58.13ms
step:1658/2330 train_time:96378ms step_avg:58.13ms
step:1659/2330 train_time:96434ms step_avg:58.13ms
step:1660/2330 train_time:96496ms step_avg:58.13ms
step:1661/2330 train_time:96552ms step_avg:58.13ms
step:1662/2330 train_time:96615ms step_avg:58.13ms
step:1663/2330 train_time:96671ms step_avg:58.13ms
step:1664/2330 train_time:96733ms step_avg:58.13ms
step:1665/2330 train_time:96790ms step_avg:58.13ms
step:1666/2330 train_time:96851ms step_avg:58.13ms
step:1667/2330 train_time:96909ms step_avg:58.13ms
step:1668/2330 train_time:96969ms step_avg:58.13ms
step:1669/2330 train_time:97027ms step_avg:58.13ms
step:1670/2330 train_time:97087ms step_avg:58.14ms
step:1671/2330 train_time:97145ms step_avg:58.14ms
step:1672/2330 train_time:97206ms step_avg:58.14ms
step:1673/2330 train_time:97264ms step_avg:58.14ms
step:1674/2330 train_time:97323ms step_avg:58.14ms
step:1675/2330 train_time:97380ms step_avg:58.14ms
step:1676/2330 train_time:97442ms step_avg:58.14ms
step:1677/2330 train_time:97498ms step_avg:58.14ms
step:1678/2330 train_time:97562ms step_avg:58.14ms
step:1679/2330 train_time:97618ms step_avg:58.14ms
step:1680/2330 train_time:97681ms step_avg:58.14ms
step:1681/2330 train_time:97737ms step_avg:58.14ms
step:1682/2330 train_time:97800ms step_avg:58.14ms
step:1683/2330 train_time:97856ms step_avg:58.14ms
step:1684/2330 train_time:97919ms step_avg:58.15ms
step:1685/2330 train_time:97976ms step_avg:58.15ms
step:1686/2330 train_time:98038ms step_avg:58.15ms
step:1687/2330 train_time:98094ms step_avg:58.15ms
step:1688/2330 train_time:98156ms step_avg:58.15ms
step:1689/2330 train_time:98213ms step_avg:58.15ms
step:1690/2330 train_time:98274ms step_avg:58.15ms
step:1691/2330 train_time:98331ms step_avg:58.15ms
step:1692/2330 train_time:98391ms step_avg:58.15ms
step:1693/2330 train_time:98448ms step_avg:58.15ms
step:1694/2330 train_time:98509ms step_avg:58.15ms
step:1695/2330 train_time:98567ms step_avg:58.15ms
step:1696/2330 train_time:98628ms step_avg:58.15ms
step:1697/2330 train_time:98686ms step_avg:58.15ms
step:1698/2330 train_time:98746ms step_avg:58.15ms
step:1699/2330 train_time:98804ms step_avg:58.15ms
step:1700/2330 train_time:98865ms step_avg:58.16ms
step:1701/2330 train_time:98922ms step_avg:58.16ms
step:1702/2330 train_time:98984ms step_avg:58.16ms
step:1703/2330 train_time:99041ms step_avg:58.16ms
step:1704/2330 train_time:99102ms step_avg:58.16ms
step:1705/2330 train_time:99158ms step_avg:58.16ms
step:1706/2330 train_time:99220ms step_avg:58.16ms
step:1707/2330 train_time:99277ms step_avg:58.16ms
step:1708/2330 train_time:99340ms step_avg:58.16ms
step:1709/2330 train_time:99396ms step_avg:58.16ms
step:1710/2330 train_time:99459ms step_avg:58.16ms
step:1711/2330 train_time:99515ms step_avg:58.16ms
step:1712/2330 train_time:99578ms step_avg:58.16ms
step:1713/2330 train_time:99634ms step_avg:58.16ms
step:1714/2330 train_time:99697ms step_avg:58.17ms
step:1715/2330 train_time:99753ms step_avg:58.17ms
step:1716/2330 train_time:99817ms step_avg:58.17ms
step:1717/2330 train_time:99874ms step_avg:58.17ms
step:1718/2330 train_time:99936ms step_avg:58.17ms
step:1719/2330 train_time:99993ms step_avg:58.17ms
step:1720/2330 train_time:100054ms step_avg:58.17ms
step:1721/2330 train_time:100111ms step_avg:58.17ms
step:1722/2330 train_time:100173ms step_avg:58.17ms
step:1723/2330 train_time:100230ms step_avg:58.17ms
step:1724/2330 train_time:100290ms step_avg:58.17ms
step:1725/2330 train_time:100348ms step_avg:58.17ms
step:1726/2330 train_time:100408ms step_avg:58.17ms
step:1727/2330 train_time:100466ms step_avg:58.17ms
step:1728/2330 train_time:100526ms step_avg:58.17ms
step:1729/2330 train_time:100584ms step_avg:58.17ms
step:1730/2330 train_time:100644ms step_avg:58.18ms
step:1731/2330 train_time:100702ms step_avg:58.18ms
step:1732/2330 train_time:100763ms step_avg:58.18ms
step:1733/2330 train_time:100820ms step_avg:58.18ms
step:1734/2330 train_time:100882ms step_avg:58.18ms
step:1735/2330 train_time:100939ms step_avg:58.18ms
step:1736/2330 train_time:101002ms step_avg:58.18ms
step:1737/2330 train_time:101058ms step_avg:58.18ms
step:1738/2330 train_time:101119ms step_avg:58.18ms
step:1739/2330 train_time:101176ms step_avg:58.18ms
step:1740/2330 train_time:101238ms step_avg:58.18ms
step:1741/2330 train_time:101295ms step_avg:58.18ms
step:1742/2330 train_time:101356ms step_avg:58.18ms
step:1743/2330 train_time:101413ms step_avg:58.18ms
step:1744/2330 train_time:101476ms step_avg:58.19ms
step:1745/2330 train_time:101532ms step_avg:58.18ms
step:1746/2330 train_time:101593ms step_avg:58.19ms
step:1747/2330 train_time:101651ms step_avg:58.19ms
step:1748/2330 train_time:101711ms step_avg:58.19ms
step:1749/2330 train_time:101770ms step_avg:58.19ms
step:1750/2330 train_time:101830ms step_avg:58.19ms
step:1750/2330 val_loss:3.8420 train_time:101912ms step_avg:58.24ms
step:1751/2330 train_time:101932ms step_avg:58.21ms
step:1752/2330 train_time:101952ms step_avg:58.19ms
step:1753/2330 train_time:102008ms step_avg:58.19ms
step:1754/2330 train_time:102078ms step_avg:58.20ms
step:1755/2330 train_time:102135ms step_avg:58.20ms
step:1756/2330 train_time:102196ms step_avg:58.20ms
step:1757/2330 train_time:102253ms step_avg:58.20ms
step:1758/2330 train_time:102312ms step_avg:58.20ms
step:1759/2330 train_time:102368ms step_avg:58.20ms
step:1760/2330 train_time:102428ms step_avg:58.20ms
step:1761/2330 train_time:102485ms step_avg:58.20ms
step:1762/2330 train_time:102544ms step_avg:58.20ms
step:1763/2330 train_time:102600ms step_avg:58.20ms
step:1764/2330 train_time:102660ms step_avg:58.20ms
step:1765/2330 train_time:102716ms step_avg:58.20ms
step:1766/2330 train_time:102775ms step_avg:58.20ms
step:1767/2330 train_time:102832ms step_avg:58.20ms
step:1768/2330 train_time:102898ms step_avg:58.20ms
step:1769/2330 train_time:102955ms step_avg:58.20ms
step:1770/2330 train_time:103020ms step_avg:58.20ms
step:1771/2330 train_time:103077ms step_avg:58.20ms
step:1772/2330 train_time:103139ms step_avg:58.20ms
step:1773/2330 train_time:103196ms step_avg:58.20ms
step:1774/2330 train_time:103258ms step_avg:58.21ms
step:1775/2330 train_time:103314ms step_avg:58.20ms
step:1776/2330 train_time:103375ms step_avg:58.21ms
step:1777/2330 train_time:103431ms step_avg:58.21ms
step:1778/2330 train_time:103493ms step_avg:58.21ms
step:1779/2330 train_time:103549ms step_avg:58.21ms
step:1780/2330 train_time:103609ms step_avg:58.21ms
step:1781/2330 train_time:103665ms step_avg:58.21ms
step:1782/2330 train_time:103725ms step_avg:58.21ms
step:1783/2330 train_time:103782ms step_avg:58.21ms
step:1784/2330 train_time:103843ms step_avg:58.21ms
step:1785/2330 train_time:103900ms step_avg:58.21ms
step:1786/2330 train_time:103963ms step_avg:58.21ms
step:1787/2330 train_time:104021ms step_avg:58.21ms
step:1788/2330 train_time:104082ms step_avg:58.21ms
step:1789/2330 train_time:104140ms step_avg:58.21ms
step:1790/2330 train_time:104200ms step_avg:58.21ms
step:1791/2330 train_time:104257ms step_avg:58.21ms
step:1792/2330 train_time:104318ms step_avg:58.21ms
step:1793/2330 train_time:104375ms step_avg:58.21ms
step:1794/2330 train_time:104436ms step_avg:58.21ms
step:1795/2330 train_time:104492ms step_avg:58.21ms
step:1796/2330 train_time:104554ms step_avg:58.21ms
step:1797/2330 train_time:104610ms step_avg:58.21ms
step:1798/2330 train_time:104671ms step_avg:58.22ms
step:1799/2330 train_time:104728ms step_avg:58.21ms
step:1800/2330 train_time:104788ms step_avg:58.22ms
step:1801/2330 train_time:104845ms step_avg:58.22ms
step:1802/2330 train_time:104905ms step_avg:58.22ms
step:1803/2330 train_time:104964ms step_avg:58.22ms
step:1804/2330 train_time:105024ms step_avg:58.22ms
step:1805/2330 train_time:105081ms step_avg:58.22ms
step:1806/2330 train_time:105143ms step_avg:58.22ms
step:1807/2330 train_time:105200ms step_avg:58.22ms
step:1808/2330 train_time:105262ms step_avg:58.22ms
step:1809/2330 train_time:105319ms step_avg:58.22ms
step:1810/2330 train_time:105380ms step_avg:58.22ms
step:1811/2330 train_time:105437ms step_avg:58.22ms
step:1812/2330 train_time:105497ms step_avg:58.22ms
step:1813/2330 train_time:105554ms step_avg:58.22ms
step:1814/2330 train_time:105615ms step_avg:58.22ms
step:1815/2330 train_time:105672ms step_avg:58.22ms
step:1816/2330 train_time:105734ms step_avg:58.22ms
step:1817/2330 train_time:105790ms step_avg:58.22ms
step:1818/2330 train_time:105852ms step_avg:58.22ms
step:1819/2330 train_time:105908ms step_avg:58.22ms
step:1820/2330 train_time:105970ms step_avg:58.23ms
step:1821/2330 train_time:106027ms step_avg:58.22ms
step:1822/2330 train_time:106088ms step_avg:58.23ms
step:1823/2330 train_time:106146ms step_avg:58.23ms
step:1824/2330 train_time:106206ms step_avg:58.23ms
step:1825/2330 train_time:106264ms step_avg:58.23ms
step:1826/2330 train_time:106325ms step_avg:58.23ms
step:1827/2330 train_time:106383ms step_avg:58.23ms
step:1828/2330 train_time:106443ms step_avg:58.23ms
step:1829/2330 train_time:106501ms step_avg:58.23ms
step:1830/2330 train_time:106562ms step_avg:58.23ms
step:1831/2330 train_time:106619ms step_avg:58.23ms
step:1832/2330 train_time:106679ms step_avg:58.23ms
step:1833/2330 train_time:106735ms step_avg:58.23ms
step:1834/2330 train_time:106797ms step_avg:58.23ms
step:1835/2330 train_time:106853ms step_avg:58.23ms
step:1836/2330 train_time:106915ms step_avg:58.23ms
step:1837/2330 train_time:106971ms step_avg:58.23ms
step:1838/2330 train_time:107034ms step_avg:58.23ms
step:1839/2330 train_time:107090ms step_avg:58.23ms
step:1840/2330 train_time:107155ms step_avg:58.24ms
step:1841/2330 train_time:107211ms step_avg:58.24ms
step:1842/2330 train_time:107273ms step_avg:58.24ms
step:1843/2330 train_time:107330ms step_avg:58.24ms
step:1844/2330 train_time:107390ms step_avg:58.24ms
step:1845/2330 train_time:107447ms step_avg:58.24ms
step:1846/2330 train_time:107508ms step_avg:58.24ms
step:1847/2330 train_time:107566ms step_avg:58.24ms
step:1848/2330 train_time:107626ms step_avg:58.24ms
step:1849/2330 train_time:107684ms step_avg:58.24ms
step:1850/2330 train_time:107745ms step_avg:58.24ms
step:1851/2330 train_time:107802ms step_avg:58.24ms
step:1852/2330 train_time:107862ms step_avg:58.24ms
step:1853/2330 train_time:107919ms step_avg:58.24ms
step:1854/2330 train_time:107980ms step_avg:58.24ms
step:1855/2330 train_time:108037ms step_avg:58.24ms
step:1856/2330 train_time:108098ms step_avg:58.24ms
step:1857/2330 train_time:108156ms step_avg:58.24ms
step:1858/2330 train_time:108217ms step_avg:58.24ms
step:1859/2330 train_time:108274ms step_avg:58.24ms
step:1860/2330 train_time:108335ms step_avg:58.24ms
step:1861/2330 train_time:108391ms step_avg:58.24ms
step:1862/2330 train_time:108453ms step_avg:58.25ms
step:1863/2330 train_time:108510ms step_avg:58.24ms
step:1864/2330 train_time:108571ms step_avg:58.25ms
step:1865/2330 train_time:108628ms step_avg:58.25ms
step:1866/2330 train_time:108689ms step_avg:58.25ms
step:1867/2330 train_time:108746ms step_avg:58.25ms
step:1868/2330 train_time:108806ms step_avg:58.25ms
step:1869/2330 train_time:108864ms step_avg:58.25ms
step:1870/2330 train_time:108924ms step_avg:58.25ms
step:1871/2330 train_time:108981ms step_avg:58.25ms
step:1872/2330 train_time:109042ms step_avg:58.25ms
step:1873/2330 train_time:109099ms step_avg:58.25ms
step:1874/2330 train_time:109160ms step_avg:58.25ms
step:1875/2330 train_time:109217ms step_avg:58.25ms
step:1876/2330 train_time:109278ms step_avg:58.25ms
step:1877/2330 train_time:109335ms step_avg:58.25ms
step:1878/2330 train_time:109396ms step_avg:58.25ms
step:1879/2330 train_time:109453ms step_avg:58.25ms
step:1880/2330 train_time:109515ms step_avg:58.25ms
step:1881/2330 train_time:109572ms step_avg:58.25ms
step:1882/2330 train_time:109634ms step_avg:58.25ms
step:1883/2330 train_time:109690ms step_avg:58.25ms
step:1884/2330 train_time:109753ms step_avg:58.26ms
step:1885/2330 train_time:109809ms step_avg:58.25ms
step:1886/2330 train_time:109871ms step_avg:58.26ms
step:1887/2330 train_time:109928ms step_avg:58.26ms
step:1888/2330 train_time:109987ms step_avg:58.26ms
step:1889/2330 train_time:110044ms step_avg:58.26ms
step:1890/2330 train_time:110106ms step_avg:58.26ms
step:1891/2330 train_time:110164ms step_avg:58.26ms
step:1892/2330 train_time:110225ms step_avg:58.26ms
step:1893/2330 train_time:110283ms step_avg:58.26ms
step:1894/2330 train_time:110343ms step_avg:58.26ms
step:1895/2330 train_time:110401ms step_avg:58.26ms
step:1896/2330 train_time:110461ms step_avg:58.26ms
step:1897/2330 train_time:110518ms step_avg:58.26ms
step:1898/2330 train_time:110580ms step_avg:58.26ms
step:1899/2330 train_time:110636ms step_avg:58.26ms
step:1900/2330 train_time:110698ms step_avg:58.26ms
step:1901/2330 train_time:110755ms step_avg:58.26ms
step:1902/2330 train_time:110817ms step_avg:58.26ms
step:1903/2330 train_time:110872ms step_avg:58.26ms
step:1904/2330 train_time:110935ms step_avg:58.26ms
step:1905/2330 train_time:110991ms step_avg:58.26ms
step:1906/2330 train_time:111053ms step_avg:58.27ms
step:1907/2330 train_time:111110ms step_avg:58.26ms
step:1908/2330 train_time:111171ms step_avg:58.27ms
step:1909/2330 train_time:111228ms step_avg:58.27ms
step:1910/2330 train_time:111289ms step_avg:58.27ms
step:1911/2330 train_time:111346ms step_avg:58.27ms
step:1912/2330 train_time:111407ms step_avg:58.27ms
step:1913/2330 train_time:111465ms step_avg:58.27ms
step:1914/2330 train_time:111526ms step_avg:58.27ms
step:1915/2330 train_time:111584ms step_avg:58.27ms
step:1916/2330 train_time:111645ms step_avg:58.27ms
step:1917/2330 train_time:111702ms step_avg:58.27ms
step:1918/2330 train_time:111763ms step_avg:58.27ms
step:1919/2330 train_time:111821ms step_avg:58.27ms
step:1920/2330 train_time:111881ms step_avg:58.27ms
step:1921/2330 train_time:111938ms step_avg:58.27ms
step:1922/2330 train_time:112000ms step_avg:58.27ms
step:1923/2330 train_time:112056ms step_avg:58.27ms
step:1924/2330 train_time:112117ms step_avg:58.27ms
step:1925/2330 train_time:112173ms step_avg:58.27ms
step:1926/2330 train_time:112236ms step_avg:58.27ms
step:1927/2330 train_time:112292ms step_avg:58.27ms
step:1928/2330 train_time:112355ms step_avg:58.28ms
step:1929/2330 train_time:112412ms step_avg:58.27ms
step:1930/2330 train_time:112474ms step_avg:58.28ms
step:1931/2330 train_time:112531ms step_avg:58.28ms
step:1932/2330 train_time:112593ms step_avg:58.28ms
step:1933/2330 train_time:112649ms step_avg:58.28ms
step:1934/2330 train_time:112711ms step_avg:58.28ms
step:1935/2330 train_time:112767ms step_avg:58.28ms
step:1936/2330 train_time:112829ms step_avg:58.28ms
step:1937/2330 train_time:112886ms step_avg:58.28ms
step:1938/2330 train_time:112947ms step_avg:58.28ms
step:1939/2330 train_time:113005ms step_avg:58.28ms
step:1940/2330 train_time:113066ms step_avg:58.28ms
step:1941/2330 train_time:113124ms step_avg:58.28ms
step:1942/2330 train_time:113184ms step_avg:58.28ms
step:1943/2330 train_time:113242ms step_avg:58.28ms
step:1944/2330 train_time:113302ms step_avg:58.28ms
step:1945/2330 train_time:113358ms step_avg:58.28ms
step:1946/2330 train_time:113420ms step_avg:58.28ms
step:1947/2330 train_time:113477ms step_avg:58.28ms
step:1948/2330 train_time:113538ms step_avg:58.28ms
step:1949/2330 train_time:113595ms step_avg:58.28ms
step:1950/2330 train_time:113657ms step_avg:58.29ms
step:1951/2330 train_time:113712ms step_avg:58.28ms
step:1952/2330 train_time:113776ms step_avg:58.29ms
step:1953/2330 train_time:113832ms step_avg:58.29ms
step:1954/2330 train_time:113893ms step_avg:58.29ms
step:1955/2330 train_time:113949ms step_avg:58.29ms
step:1956/2330 train_time:114012ms step_avg:58.29ms
step:1957/2330 train_time:114069ms step_avg:58.29ms
step:1958/2330 train_time:114130ms step_avg:58.29ms
step:1959/2330 train_time:114187ms step_avg:58.29ms
step:1960/2330 train_time:114247ms step_avg:58.29ms
step:1961/2330 train_time:114304ms step_avg:58.29ms
step:1962/2330 train_time:114365ms step_avg:58.29ms
step:1963/2330 train_time:114423ms step_avg:58.29ms
step:1964/2330 train_time:114484ms step_avg:58.29ms
step:1965/2330 train_time:114542ms step_avg:58.29ms
step:1966/2330 train_time:114603ms step_avg:58.29ms
step:1967/2330 train_time:114661ms step_avg:58.29ms
step:1968/2330 train_time:114721ms step_avg:58.29ms
step:1969/2330 train_time:114779ms step_avg:58.29ms
step:1970/2330 train_time:114840ms step_avg:58.29ms
step:1971/2330 train_time:114897ms step_avg:58.29ms
step:1972/2330 train_time:114958ms step_avg:58.30ms
step:1973/2330 train_time:115015ms step_avg:58.29ms
step:1974/2330 train_time:115077ms step_avg:58.30ms
step:1975/2330 train_time:115133ms step_avg:58.30ms
step:1976/2330 train_time:115196ms step_avg:58.30ms
step:1977/2330 train_time:115253ms step_avg:58.30ms
step:1978/2330 train_time:115314ms step_avg:58.30ms
step:1979/2330 train_time:115371ms step_avg:58.30ms
step:1980/2330 train_time:115433ms step_avg:58.30ms
step:1981/2330 train_time:115489ms step_avg:58.30ms
step:1982/2330 train_time:115550ms step_avg:58.30ms
step:1983/2330 train_time:115607ms step_avg:58.30ms
step:1984/2330 train_time:115667ms step_avg:58.30ms
step:1985/2330 train_time:115724ms step_avg:58.30ms
step:1986/2330 train_time:115785ms step_avg:58.30ms
step:1987/2330 train_time:115842ms step_avg:58.30ms
step:1988/2330 train_time:115902ms step_avg:58.30ms
step:1989/2330 train_time:115960ms step_avg:58.30ms
step:1990/2330 train_time:116020ms step_avg:58.30ms
step:1991/2330 train_time:116078ms step_avg:58.30ms
step:1992/2330 train_time:116139ms step_avg:58.30ms
step:1993/2330 train_time:116195ms step_avg:58.30ms
step:1994/2330 train_time:116257ms step_avg:58.30ms
step:1995/2330 train_time:116314ms step_avg:58.30ms
step:1996/2330 train_time:116376ms step_avg:58.30ms
step:1997/2330 train_time:116432ms step_avg:58.30ms
step:1998/2330 train_time:116496ms step_avg:58.31ms
step:1999/2330 train_time:116552ms step_avg:58.31ms
step:2000/2330 train_time:116614ms step_avg:58.31ms
step:2000/2330 val_loss:3.7787 train_time:116696ms step_avg:58.35ms
step:2001/2330 train_time:116716ms step_avg:58.33ms
step:2002/2330 train_time:116737ms step_avg:58.31ms
step:2003/2330 train_time:116795ms step_avg:58.31ms
step:2004/2330 train_time:116858ms step_avg:58.31ms
step:2005/2330 train_time:116915ms step_avg:58.31ms
step:2006/2330 train_time:116979ms step_avg:58.31ms
step:2007/2330 train_time:117035ms step_avg:58.31ms
step:2008/2330 train_time:117096ms step_avg:58.31ms
step:2009/2330 train_time:117152ms step_avg:58.31ms
step:2010/2330 train_time:117213ms step_avg:58.31ms
step:2011/2330 train_time:117269ms step_avg:58.31ms
step:2012/2330 train_time:117329ms step_avg:58.31ms
step:2013/2330 train_time:117385ms step_avg:58.31ms
step:2014/2330 train_time:117445ms step_avg:58.31ms
step:2015/2330 train_time:117501ms step_avg:58.31ms
step:2016/2330 train_time:117561ms step_avg:58.31ms
step:2017/2330 train_time:117618ms step_avg:58.31ms
step:2018/2330 train_time:117680ms step_avg:58.32ms
step:2019/2330 train_time:117738ms step_avg:58.32ms
step:2020/2330 train_time:117800ms step_avg:58.32ms
step:2021/2330 train_time:117857ms step_avg:58.32ms
step:2022/2330 train_time:117921ms step_avg:58.32ms
step:2023/2330 train_time:117978ms step_avg:58.32ms
step:2024/2330 train_time:118041ms step_avg:58.32ms
step:2025/2330 train_time:118096ms step_avg:58.32ms
step:2026/2330 train_time:118158ms step_avg:58.32ms
step:2027/2330 train_time:118214ms step_avg:58.32ms
step:2028/2330 train_time:118276ms step_avg:58.32ms
step:2029/2330 train_time:118332ms step_avg:58.32ms
step:2030/2330 train_time:118392ms step_avg:58.32ms
step:2031/2330 train_time:118449ms step_avg:58.32ms
step:2032/2330 train_time:118508ms step_avg:58.32ms
step:2033/2330 train_time:118565ms step_avg:58.32ms
step:2034/2330 train_time:118625ms step_avg:58.32ms
step:2035/2330 train_time:118683ms step_avg:58.32ms
step:2036/2330 train_time:118746ms step_avg:58.32ms
step:2037/2330 train_time:118804ms step_avg:58.32ms
step:2038/2330 train_time:118866ms step_avg:58.32ms
step:2039/2330 train_time:118925ms step_avg:58.32ms
step:2040/2330 train_time:118986ms step_avg:58.33ms
step:2041/2330 train_time:119043ms step_avg:58.33ms
step:2042/2330 train_time:119104ms step_avg:58.33ms
step:2043/2330 train_time:119162ms step_avg:58.33ms
step:2044/2330 train_time:119222ms step_avg:58.33ms
step:2045/2330 train_time:119278ms step_avg:58.33ms
step:2046/2330 train_time:119340ms step_avg:58.33ms
step:2047/2330 train_time:119396ms step_avg:58.33ms
step:2048/2330 train_time:119457ms step_avg:58.33ms
step:2049/2330 train_time:119513ms step_avg:58.33ms
step:2050/2330 train_time:119574ms step_avg:58.33ms
step:2051/2330 train_time:119631ms step_avg:58.33ms
step:2052/2330 train_time:119692ms step_avg:58.33ms
step:2053/2330 train_time:119749ms step_avg:58.33ms
step:2054/2330 train_time:119811ms step_avg:58.33ms
step:2055/2330 train_time:119869ms step_avg:58.33ms
step:2056/2330 train_time:119930ms step_avg:58.33ms
step:2057/2330 train_time:119989ms step_avg:58.33ms
step:2058/2330 train_time:120049ms step_avg:58.33ms
step:2059/2330 train_time:120107ms step_avg:58.33ms
step:2060/2330 train_time:120167ms step_avg:58.33ms
step:2061/2330 train_time:120224ms step_avg:58.33ms
step:2062/2330 train_time:120284ms step_avg:58.33ms
step:2063/2330 train_time:120342ms step_avg:58.33ms
step:2064/2330 train_time:120403ms step_avg:58.33ms
step:2065/2330 train_time:120459ms step_avg:58.33ms
step:2066/2330 train_time:120520ms step_avg:58.33ms
step:2067/2330 train_time:120576ms step_avg:58.33ms
step:2068/2330 train_time:120638ms step_avg:58.34ms
step:2069/2330 train_time:120696ms step_avg:58.34ms
step:2070/2330 train_time:120757ms step_avg:58.34ms
step:2071/2330 train_time:120813ms step_avg:58.34ms
step:2072/2330 train_time:120875ms step_avg:58.34ms
step:2073/2330 train_time:120932ms step_avg:58.34ms
step:2074/2330 train_time:120994ms step_avg:58.34ms
step:2075/2330 train_time:121052ms step_avg:58.34ms
step:2076/2330 train_time:121112ms step_avg:58.34ms
step:2077/2330 train_time:121169ms step_avg:58.34ms
step:2078/2330 train_time:121230ms step_avg:58.34ms
step:2079/2330 train_time:121288ms step_avg:58.34ms
step:2080/2330 train_time:121348ms step_avg:58.34ms
step:2081/2330 train_time:121406ms step_avg:58.34ms
step:2082/2330 train_time:121466ms step_avg:58.34ms
step:2083/2330 train_time:121523ms step_avg:58.34ms
step:2084/2330 train_time:121584ms step_avg:58.34ms
step:2085/2330 train_time:121640ms step_avg:58.34ms
step:2086/2330 train_time:121702ms step_avg:58.34ms
step:2087/2330 train_time:121759ms step_avg:58.34ms
step:2088/2330 train_time:121821ms step_avg:58.34ms
step:2089/2330 train_time:121878ms step_avg:58.34ms
step:2090/2330 train_time:121941ms step_avg:58.35ms
step:2091/2330 train_time:121997ms step_avg:58.34ms
step:2092/2330 train_time:122059ms step_avg:58.35ms
step:2093/2330 train_time:122115ms step_avg:58.34ms
step:2094/2330 train_time:122179ms step_avg:58.35ms
step:2095/2330 train_time:122236ms step_avg:58.35ms
step:2096/2330 train_time:122297ms step_avg:58.35ms
step:2097/2330 train_time:122353ms step_avg:58.35ms
step:2098/2330 train_time:122414ms step_avg:58.35ms
step:2099/2330 train_time:122471ms step_avg:58.35ms
step:2100/2330 train_time:122532ms step_avg:58.35ms
step:2101/2330 train_time:122589ms step_avg:58.35ms
step:2102/2330 train_time:122650ms step_avg:58.35ms
step:2103/2330 train_time:122708ms step_avg:58.35ms
step:2104/2330 train_time:122769ms step_avg:58.35ms
step:2105/2330 train_time:122827ms step_avg:58.35ms
step:2106/2330 train_time:122887ms step_avg:58.35ms
step:2107/2330 train_time:122946ms step_avg:58.35ms
step:2108/2330 train_time:123006ms step_avg:58.35ms
step:2109/2330 train_time:123064ms step_avg:58.35ms
step:2110/2330 train_time:123125ms step_avg:58.35ms
step:2111/2330 train_time:123181ms step_avg:58.35ms
step:2112/2330 train_time:123243ms step_avg:58.35ms
step:2113/2330 train_time:123299ms step_avg:58.35ms
step:2114/2330 train_time:123362ms step_avg:58.35ms
step:2115/2330 train_time:123418ms step_avg:58.35ms
step:2116/2330 train_time:123479ms step_avg:58.36ms
step:2117/2330 train_time:123536ms step_avg:58.35ms
step:2118/2330 train_time:123597ms step_avg:58.36ms
step:2119/2330 train_time:123654ms step_avg:58.35ms
step:2120/2330 train_time:123714ms step_avg:58.36ms
step:2121/2330 train_time:123771ms step_avg:58.36ms
step:2122/2330 train_time:123832ms step_avg:58.36ms
step:2123/2330 train_time:123890ms step_avg:58.36ms
step:2124/2330 train_time:123951ms step_avg:58.36ms
step:2125/2330 train_time:124009ms step_avg:58.36ms
step:2126/2330 train_time:124069ms step_avg:58.36ms
step:2127/2330 train_time:124126ms step_avg:58.36ms
step:2128/2330 train_time:124186ms step_avg:58.36ms
step:2129/2330 train_time:124243ms step_avg:58.36ms
step:2130/2330 train_time:124305ms step_avg:58.36ms
step:2131/2330 train_time:124362ms step_avg:58.36ms
step:2132/2330 train_time:124423ms step_avg:58.36ms
step:2133/2330 train_time:124480ms step_avg:58.36ms
step:2134/2330 train_time:124542ms step_avg:58.36ms
step:2135/2330 train_time:124598ms step_avg:58.36ms
step:2136/2330 train_time:124659ms step_avg:58.36ms
step:2137/2330 train_time:124715ms step_avg:58.36ms
step:2138/2330 train_time:124777ms step_avg:58.36ms
step:2139/2330 train_time:124833ms step_avg:58.36ms
step:2140/2330 train_time:124896ms step_avg:58.36ms
step:2141/2330 train_time:124952ms step_avg:58.36ms
step:2142/2330 train_time:125013ms step_avg:58.36ms
step:2143/2330 train_time:125070ms step_avg:58.36ms
step:2144/2330 train_time:125130ms step_avg:58.36ms
step:2145/2330 train_time:125188ms step_avg:58.36ms
step:2146/2330 train_time:125249ms step_avg:58.36ms
step:2147/2330 train_time:125306ms step_avg:58.36ms
step:2148/2330 train_time:125366ms step_avg:58.36ms
step:2149/2330 train_time:125424ms step_avg:58.36ms
step:2150/2330 train_time:125485ms step_avg:58.36ms
step:2151/2330 train_time:125543ms step_avg:58.37ms
step:2152/2330 train_time:125603ms step_avg:58.37ms
step:2153/2330 train_time:125661ms step_avg:58.37ms
step:2154/2330 train_time:125721ms step_avg:58.37ms
step:2155/2330 train_time:125777ms step_avg:58.37ms
step:2156/2330 train_time:125840ms step_avg:58.37ms
step:2157/2330 train_time:125896ms step_avg:58.37ms
step:2158/2330 train_time:125960ms step_avg:58.37ms
step:2159/2330 train_time:126016ms step_avg:58.37ms
step:2160/2330 train_time:126080ms step_avg:58.37ms
step:2161/2330 train_time:126137ms step_avg:58.37ms
step:2162/2330 train_time:126198ms step_avg:58.37ms
step:2163/2330 train_time:126255ms step_avg:58.37ms
step:2164/2330 train_time:126316ms step_avg:58.37ms
step:2165/2330 train_time:126372ms step_avg:58.37ms
step:2166/2330 train_time:126434ms step_avg:58.37ms
step:2167/2330 train_time:126491ms step_avg:58.37ms
step:2168/2330 train_time:126551ms step_avg:58.37ms
step:2169/2330 train_time:126609ms step_avg:58.37ms
step:2170/2330 train_time:126669ms step_avg:58.37ms
step:2171/2330 train_time:126727ms step_avg:58.37ms
step:2172/2330 train_time:126787ms step_avg:58.37ms
step:2173/2330 train_time:126846ms step_avg:58.37ms
step:2174/2330 train_time:126907ms step_avg:58.37ms
step:2175/2330 train_time:126965ms step_avg:58.37ms
step:2176/2330 train_time:127026ms step_avg:58.38ms
step:2177/2330 train_time:127082ms step_avg:58.37ms
step:2178/2330 train_time:127144ms step_avg:58.38ms
step:2179/2330 train_time:127200ms step_avg:58.38ms
step:2180/2330 train_time:127262ms step_avg:58.38ms
step:2181/2330 train_time:127318ms step_avg:58.38ms
step:2182/2330 train_time:127380ms step_avg:58.38ms
step:2183/2330 train_time:127437ms step_avg:58.38ms
step:2184/2330 train_time:127499ms step_avg:58.38ms
step:2185/2330 train_time:127555ms step_avg:58.38ms
step:2186/2330 train_time:127617ms step_avg:58.38ms
step:2187/2330 train_time:127674ms step_avg:58.38ms
step:2188/2330 train_time:127736ms step_avg:58.38ms
step:2189/2330 train_time:127793ms step_avg:58.38ms
step:2190/2330 train_time:127853ms step_avg:58.38ms
step:2191/2330 train_time:127910ms step_avg:58.38ms
step:2192/2330 train_time:127971ms step_avg:58.38ms
step:2193/2330 train_time:128028ms step_avg:58.38ms
step:2194/2330 train_time:128090ms step_avg:58.38ms
step:2195/2330 train_time:128148ms step_avg:58.38ms
step:2196/2330 train_time:128208ms step_avg:58.38ms
step:2197/2330 train_time:128266ms step_avg:58.38ms
step:2198/2330 train_time:128326ms step_avg:58.38ms
step:2199/2330 train_time:128385ms step_avg:58.38ms
step:2200/2330 train_time:128445ms step_avg:58.38ms
step:2201/2330 train_time:128503ms step_avg:58.38ms
step:2202/2330 train_time:128563ms step_avg:58.38ms
step:2203/2330 train_time:128619ms step_avg:58.38ms
step:2204/2330 train_time:128681ms step_avg:58.39ms
step:2205/2330 train_time:128738ms step_avg:58.38ms
step:2206/2330 train_time:128799ms step_avg:58.39ms
step:2207/2330 train_time:128856ms step_avg:58.39ms
step:2208/2330 train_time:128918ms step_avg:58.39ms
step:2209/2330 train_time:128974ms step_avg:58.39ms
step:2210/2330 train_time:129038ms step_avg:58.39ms
step:2211/2330 train_time:129094ms step_avg:58.39ms
step:2212/2330 train_time:129156ms step_avg:58.39ms
step:2213/2330 train_time:129212ms step_avg:58.39ms
step:2214/2330 train_time:129273ms step_avg:58.39ms
step:2215/2330 train_time:129330ms step_avg:58.39ms
step:2216/2330 train_time:129391ms step_avg:58.39ms
step:2217/2330 train_time:129449ms step_avg:58.39ms
step:2218/2330 train_time:129510ms step_avg:58.39ms
step:2219/2330 train_time:129567ms step_avg:58.39ms
step:2220/2330 train_time:129628ms step_avg:58.39ms
step:2221/2330 train_time:129686ms step_avg:58.39ms
step:2222/2330 train_time:129747ms step_avg:58.39ms
step:2223/2330 train_time:129805ms step_avg:58.39ms
step:2224/2330 train_time:129865ms step_avg:58.39ms
step:2225/2330 train_time:129922ms step_avg:58.39ms
step:2226/2330 train_time:129983ms step_avg:58.39ms
step:2227/2330 train_time:130039ms step_avg:58.39ms
step:2228/2330 train_time:130102ms step_avg:58.39ms
step:2229/2330 train_time:130159ms step_avg:58.39ms
step:2230/2330 train_time:130220ms step_avg:58.39ms
step:2231/2330 train_time:130276ms step_avg:58.39ms
step:2232/2330 train_time:130338ms step_avg:58.40ms
step:2233/2330 train_time:130394ms step_avg:58.39ms
step:2234/2330 train_time:130457ms step_avg:58.40ms
step:2235/2330 train_time:130512ms step_avg:58.39ms
step:2236/2330 train_time:130575ms step_avg:58.40ms
step:2237/2330 train_time:130631ms step_avg:58.40ms
step:2238/2330 train_time:130693ms step_avg:58.40ms
step:2239/2330 train_time:130751ms step_avg:58.40ms
step:2240/2330 train_time:130811ms step_avg:58.40ms
step:2241/2330 train_time:130868ms step_avg:58.40ms
step:2242/2330 train_time:130929ms step_avg:58.40ms
step:2243/2330 train_time:130987ms step_avg:58.40ms
step:2244/2330 train_time:131047ms step_avg:58.40ms
step:2245/2330 train_time:131105ms step_avg:58.40ms
step:2246/2330 train_time:131166ms step_avg:58.40ms
step:2247/2330 train_time:131223ms step_avg:58.40ms
step:2248/2330 train_time:131285ms step_avg:58.40ms
step:2249/2330 train_time:131342ms step_avg:58.40ms
step:2250/2330 train_time:131404ms step_avg:58.40ms
step:2250/2330 val_loss:3.7302 train_time:131487ms step_avg:58.44ms
step:2251/2330 train_time:131506ms step_avg:58.42ms
step:2252/2330 train_time:131527ms step_avg:58.40ms
step:2253/2330 train_time:131586ms step_avg:58.40ms
step:2254/2330 train_time:131652ms step_avg:58.41ms
step:2255/2330 train_time:131709ms step_avg:58.41ms
step:2256/2330 train_time:131771ms step_avg:58.41ms
step:2257/2330 train_time:131828ms step_avg:58.41ms
step:2258/2330 train_time:131889ms step_avg:58.41ms
step:2259/2330 train_time:131945ms step_avg:58.41ms
step:2260/2330 train_time:132005ms step_avg:58.41ms
step:2261/2330 train_time:132062ms step_avg:58.41ms
step:2262/2330 train_time:132121ms step_avg:58.41ms
step:2263/2330 train_time:132178ms step_avg:58.41ms
step:2264/2330 train_time:132238ms step_avg:58.41ms
step:2265/2330 train_time:132294ms step_avg:58.41ms
step:2266/2330 train_time:132354ms step_avg:58.41ms
step:2267/2330 train_time:132412ms step_avg:58.41ms
step:2268/2330 train_time:132473ms step_avg:58.41ms
step:2269/2330 train_time:132531ms step_avg:58.41ms
step:2270/2330 train_time:132596ms step_avg:58.41ms
step:2271/2330 train_time:132653ms step_avg:58.41ms
step:2272/2330 train_time:132717ms step_avg:58.41ms
step:2273/2330 train_time:132773ms step_avg:58.41ms
step:2274/2330 train_time:132837ms step_avg:58.42ms
step:2275/2330 train_time:132894ms step_avg:58.41ms
step:2276/2330 train_time:132955ms step_avg:58.42ms
step:2277/2330 train_time:133012ms step_avg:58.42ms
step:2278/2330 train_time:133073ms step_avg:58.42ms
step:2279/2330 train_time:133129ms step_avg:58.42ms
step:2280/2330 train_time:133190ms step_avg:58.42ms
step:2281/2330 train_time:133246ms step_avg:58.42ms
step:2282/2330 train_time:133306ms step_avg:58.42ms
step:2283/2330 train_time:133362ms step_avg:58.42ms
step:2284/2330 train_time:133422ms step_avg:58.42ms
step:2285/2330 train_time:133480ms step_avg:58.42ms
step:2286/2330 train_time:133540ms step_avg:58.42ms
step:2287/2330 train_time:133599ms step_avg:58.42ms
step:2288/2330 train_time:133661ms step_avg:58.42ms
step:2289/2330 train_time:133720ms step_avg:58.42ms
step:2290/2330 train_time:133781ms step_avg:58.42ms
step:2291/2330 train_time:133839ms step_avg:58.42ms
step:2292/2330 train_time:133900ms step_avg:58.42ms
step:2293/2330 train_time:133957ms step_avg:58.42ms
step:2294/2330 train_time:134019ms step_avg:58.42ms
step:2295/2330 train_time:134076ms step_avg:58.42ms
step:2296/2330 train_time:134136ms step_avg:58.42ms
step:2297/2330 train_time:134193ms step_avg:58.42ms
step:2298/2330 train_time:134254ms step_avg:58.42ms
step:2299/2330 train_time:134310ms step_avg:58.42ms
step:2300/2330 train_time:134371ms step_avg:58.42ms
step:2301/2330 train_time:134427ms step_avg:58.42ms
step:2302/2330 train_time:134489ms step_avg:58.42ms
step:2303/2330 train_time:134546ms step_avg:58.42ms
step:2304/2330 train_time:134607ms step_avg:58.42ms
step:2305/2330 train_time:134664ms step_avg:58.42ms
step:2306/2330 train_time:134725ms step_avg:58.42ms
step:2307/2330 train_time:134783ms step_avg:58.42ms
step:2308/2330 train_time:134844ms step_avg:58.42ms
step:2309/2330 train_time:134901ms step_avg:58.42ms
step:2310/2330 train_time:134962ms step_avg:58.43ms
step:2311/2330 train_time:135021ms step_avg:58.43ms
step:2312/2330 train_time:135081ms step_avg:58.43ms
step:2313/2330 train_time:135138ms step_avg:58.43ms
step:2314/2330 train_time:135198ms step_avg:58.43ms
step:2315/2330 train_time:135254ms step_avg:58.43ms
step:2316/2330 train_time:135316ms step_avg:58.43ms
step:2317/2330 train_time:135373ms step_avg:58.43ms
step:2318/2330 train_time:135434ms step_avg:58.43ms
step:2319/2330 train_time:135490ms step_avg:58.43ms
step:2320/2330 train_time:135554ms step_avg:58.43ms
step:2321/2330 train_time:135611ms step_avg:58.43ms
step:2322/2330 train_time:135673ms step_avg:58.43ms
step:2323/2330 train_time:135729ms step_avg:58.43ms
step:2324/2330 train_time:135792ms step_avg:58.43ms
step:2325/2330 train_time:135849ms step_avg:58.43ms
step:2326/2330 train_time:135911ms step_avg:58.43ms
step:2327/2330 train_time:135967ms step_avg:58.43ms
step:2328/2330 train_time:136029ms step_avg:58.43ms
step:2329/2330 train_time:136086ms step_avg:58.43ms
step:2330/2330 train_time:136146ms step_avg:58.43ms
step:2330/2330 val_loss:3.7149 train_time:136227ms step_avg:58.47ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
