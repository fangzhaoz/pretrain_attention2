import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--beta1', type=str, required=True)
parser.add_argument('--beta2', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_beta_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=6e-4,  betas=(float(args2.beta1), float(args2.beta2)), eps=1e-10, weight_decay=0.0, fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt", exist_ok=True)
            torch.save(log, f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 04:02:14 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   36C    P0             118W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   35C    P0             117W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   30C    P0             118W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             115W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   34C    P0             119W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   33C    P0             114W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:89ms step_avg:88.59ms
step:2/2330 train_time:181ms step_avg:90.60ms
step:3/2330 train_time:199ms step_avg:66.28ms
step:4/2330 train_time:218ms step_avg:54.57ms
step:5/2330 train_time:272ms step_avg:54.47ms
step:6/2330 train_time:330ms step_avg:55.01ms
step:7/2330 train_time:385ms step_avg:54.97ms
step:8/2330 train_time:444ms step_avg:55.54ms
step:9/2330 train_time:500ms step_avg:55.54ms
step:10/2330 train_time:558ms step_avg:55.84ms
step:11/2330 train_time:614ms step_avg:55.84ms
step:12/2330 train_time:673ms step_avg:56.05ms
step:13/2330 train_time:728ms step_avg:56.04ms
step:14/2330 train_time:787ms step_avg:56.19ms
step:15/2330 train_time:842ms step_avg:56.12ms
step:16/2330 train_time:900ms step_avg:56.26ms
step:17/2330 train_time:956ms step_avg:56.22ms
step:18/2330 train_time:1016ms step_avg:56.45ms
step:19/2330 train_time:1076ms step_avg:56.61ms
step:20/2330 train_time:1138ms step_avg:56.89ms
step:21/2330 train_time:1196ms step_avg:56.94ms
step:22/2330 train_time:1256ms step_avg:57.08ms
step:23/2330 train_time:1312ms step_avg:57.05ms
step:24/2330 train_time:1371ms step_avg:57.14ms
step:25/2330 train_time:1427ms step_avg:57.07ms
step:26/2330 train_time:1485ms step_avg:57.12ms
step:27/2330 train_time:1540ms step_avg:57.04ms
step:28/2330 train_time:1600ms step_avg:57.13ms
step:29/2330 train_time:1655ms step_avg:57.08ms
step:30/2330 train_time:1714ms step_avg:57.13ms
step:31/2330 train_time:1770ms step_avg:57.08ms
step:32/2330 train_time:1827ms step_avg:57.10ms
step:33/2330 train_time:1883ms step_avg:57.06ms
step:34/2330 train_time:1942ms step_avg:57.12ms
step:35/2330 train_time:1998ms step_avg:57.10ms
step:36/2330 train_time:2058ms step_avg:57.18ms
step:37/2330 train_time:2116ms step_avg:57.18ms
step:38/2330 train_time:2175ms step_avg:57.25ms
step:39/2330 train_time:2233ms step_avg:57.25ms
step:40/2330 train_time:2292ms step_avg:57.30ms
step:41/2330 train_time:2349ms step_avg:57.29ms
step:42/2330 train_time:2407ms step_avg:57.31ms
step:43/2330 train_time:2463ms step_avg:57.27ms
step:44/2330 train_time:2522ms step_avg:57.31ms
step:45/2330 train_time:2577ms step_avg:57.26ms
step:46/2330 train_time:2637ms step_avg:57.32ms
step:47/2330 train_time:2692ms step_avg:57.28ms
step:48/2330 train_time:2751ms step_avg:57.31ms
step:49/2330 train_time:2807ms step_avg:57.28ms
step:50/2330 train_time:2865ms step_avg:57.30ms
step:51/2330 train_time:2921ms step_avg:57.27ms
step:52/2330 train_time:2980ms step_avg:57.31ms
step:53/2330 train_time:3037ms step_avg:57.30ms
step:54/2330 train_time:3096ms step_avg:57.34ms
step:55/2330 train_time:3153ms step_avg:57.33ms
step:56/2330 train_time:3213ms step_avg:57.37ms
step:57/2330 train_time:3269ms step_avg:57.35ms
step:58/2330 train_time:3328ms step_avg:57.38ms
step:59/2330 train_time:3385ms step_avg:57.37ms
step:60/2330 train_time:3444ms step_avg:57.39ms
step:61/2330 train_time:3500ms step_avg:57.37ms
step:62/2330 train_time:3558ms step_avg:57.39ms
step:63/2330 train_time:3614ms step_avg:57.37ms
step:64/2330 train_time:3672ms step_avg:57.38ms
step:65/2330 train_time:3728ms step_avg:57.36ms
step:66/2330 train_time:3786ms step_avg:57.37ms
step:67/2330 train_time:3842ms step_avg:57.34ms
step:68/2330 train_time:3902ms step_avg:57.38ms
step:69/2330 train_time:3957ms step_avg:57.35ms
step:70/2330 train_time:4017ms step_avg:57.38ms
step:71/2330 train_time:4073ms step_avg:57.36ms
step:72/2330 train_time:4132ms step_avg:57.40ms
step:73/2330 train_time:4189ms step_avg:57.38ms
step:74/2330 train_time:4249ms step_avg:57.42ms
step:75/2330 train_time:4305ms step_avg:57.40ms
step:76/2330 train_time:4365ms step_avg:57.43ms
step:77/2330 train_time:4421ms step_avg:57.41ms
step:78/2330 train_time:4480ms step_avg:57.43ms
step:79/2330 train_time:4536ms step_avg:57.42ms
step:80/2330 train_time:4595ms step_avg:57.44ms
step:81/2330 train_time:4650ms step_avg:57.41ms
step:82/2330 train_time:4709ms step_avg:57.43ms
step:83/2330 train_time:4765ms step_avg:57.41ms
step:84/2330 train_time:4823ms step_avg:57.42ms
step:85/2330 train_time:4879ms step_avg:57.40ms
step:86/2330 train_time:4938ms step_avg:57.42ms
step:87/2330 train_time:4994ms step_avg:57.41ms
step:88/2330 train_time:5054ms step_avg:57.43ms
step:89/2330 train_time:5110ms step_avg:57.41ms
step:90/2330 train_time:5169ms step_avg:57.43ms
step:91/2330 train_time:5225ms step_avg:57.41ms
step:92/2330 train_time:5284ms step_avg:57.43ms
step:93/2330 train_time:5340ms step_avg:57.42ms
step:94/2330 train_time:5400ms step_avg:57.45ms
step:95/2330 train_time:5456ms step_avg:57.43ms
step:96/2330 train_time:5515ms step_avg:57.45ms
step:97/2330 train_time:5571ms step_avg:57.44ms
step:98/2330 train_time:5630ms step_avg:57.45ms
step:99/2330 train_time:5686ms step_avg:57.44ms
step:100/2330 train_time:5745ms step_avg:57.45ms
step:101/2330 train_time:5801ms step_avg:57.43ms
step:102/2330 train_time:5860ms step_avg:57.45ms
step:103/2330 train_time:5916ms step_avg:57.44ms
step:104/2330 train_time:5975ms step_avg:57.45ms
step:105/2330 train_time:6031ms step_avg:57.44ms
step:106/2330 train_time:6090ms step_avg:57.45ms
step:107/2330 train_time:6147ms step_avg:57.45ms
step:108/2330 train_time:6206ms step_avg:57.46ms
step:109/2330 train_time:6262ms step_avg:57.45ms
step:110/2330 train_time:6322ms step_avg:57.47ms
step:111/2330 train_time:6378ms step_avg:57.46ms
step:112/2330 train_time:6438ms step_avg:57.48ms
step:113/2330 train_time:6494ms step_avg:57.47ms
step:114/2330 train_time:6552ms step_avg:57.48ms
step:115/2330 train_time:6608ms step_avg:57.46ms
step:116/2330 train_time:6667ms step_avg:57.47ms
step:117/2330 train_time:6723ms step_avg:57.46ms
step:118/2330 train_time:6783ms step_avg:57.48ms
step:119/2330 train_time:6839ms step_avg:57.47ms
step:120/2330 train_time:6897ms step_avg:57.48ms
step:121/2330 train_time:6953ms step_avg:57.46ms
step:122/2330 train_time:7012ms step_avg:57.47ms
step:123/2330 train_time:7068ms step_avg:57.47ms
step:124/2330 train_time:7127ms step_avg:57.48ms
step:125/2330 train_time:7183ms step_avg:57.47ms
step:126/2330 train_time:7243ms step_avg:57.48ms
step:127/2330 train_time:7299ms step_avg:57.48ms
step:128/2330 train_time:7359ms step_avg:57.49ms
step:129/2330 train_time:7415ms step_avg:57.48ms
step:130/2330 train_time:7474ms step_avg:57.49ms
step:131/2330 train_time:7530ms step_avg:57.48ms
step:132/2330 train_time:7588ms step_avg:57.49ms
step:133/2330 train_time:7644ms step_avg:57.47ms
step:134/2330 train_time:7702ms step_avg:57.48ms
step:135/2330 train_time:7758ms step_avg:57.47ms
step:136/2330 train_time:7817ms step_avg:57.48ms
step:137/2330 train_time:7873ms step_avg:57.47ms
step:138/2330 train_time:7932ms step_avg:57.48ms
step:139/2330 train_time:7988ms step_avg:57.47ms
step:140/2330 train_time:8047ms step_avg:57.48ms
step:141/2330 train_time:8103ms step_avg:57.47ms
step:142/2330 train_time:8163ms step_avg:57.48ms
step:143/2330 train_time:8218ms step_avg:57.47ms
step:144/2330 train_time:8278ms step_avg:57.49ms
step:145/2330 train_time:8334ms step_avg:57.47ms
step:146/2330 train_time:8393ms step_avg:57.49ms
step:147/2330 train_time:8449ms step_avg:57.48ms
step:148/2330 train_time:8508ms step_avg:57.49ms
step:149/2330 train_time:8565ms step_avg:57.48ms
step:150/2330 train_time:8624ms step_avg:57.49ms
step:151/2330 train_time:8679ms step_avg:57.48ms
step:152/2330 train_time:8739ms step_avg:57.49ms
step:153/2330 train_time:8795ms step_avg:57.48ms
step:154/2330 train_time:8854ms step_avg:57.49ms
step:155/2330 train_time:8910ms step_avg:57.48ms
step:156/2330 train_time:8968ms step_avg:57.49ms
step:157/2330 train_time:9024ms step_avg:57.48ms
step:158/2330 train_time:9085ms step_avg:57.50ms
step:159/2330 train_time:9140ms step_avg:57.49ms
step:160/2330 train_time:9200ms step_avg:57.50ms
step:161/2330 train_time:9256ms step_avg:57.49ms
step:162/2330 train_time:9316ms step_avg:57.50ms
step:163/2330 train_time:9372ms step_avg:57.50ms
step:164/2330 train_time:9431ms step_avg:57.50ms
step:165/2330 train_time:9487ms step_avg:57.50ms
step:166/2330 train_time:9546ms step_avg:57.50ms
step:167/2330 train_time:9602ms step_avg:57.50ms
step:168/2330 train_time:9661ms step_avg:57.51ms
step:169/2330 train_time:9717ms step_avg:57.50ms
step:170/2330 train_time:9777ms step_avg:57.51ms
step:171/2330 train_time:9833ms step_avg:57.50ms
step:172/2330 train_time:9891ms step_avg:57.51ms
step:173/2330 train_time:9948ms step_avg:57.50ms
step:174/2330 train_time:10006ms step_avg:57.51ms
step:175/2330 train_time:10062ms step_avg:57.50ms
step:176/2330 train_time:10122ms step_avg:57.51ms
step:177/2330 train_time:10178ms step_avg:57.50ms
step:178/2330 train_time:10237ms step_avg:57.51ms
step:179/2330 train_time:10293ms step_avg:57.50ms
step:180/2330 train_time:10352ms step_avg:57.51ms
step:181/2330 train_time:10408ms step_avg:57.51ms
step:182/2330 train_time:10467ms step_avg:57.51ms
step:183/2330 train_time:10523ms step_avg:57.50ms
step:184/2330 train_time:10582ms step_avg:57.51ms
step:185/2330 train_time:10637ms step_avg:57.50ms
step:186/2330 train_time:10697ms step_avg:57.51ms
step:187/2330 train_time:10755ms step_avg:57.51ms
step:188/2330 train_time:10814ms step_avg:57.52ms
step:189/2330 train_time:10870ms step_avg:57.51ms
step:190/2330 train_time:10928ms step_avg:57.52ms
step:191/2330 train_time:10984ms step_avg:57.51ms
step:192/2330 train_time:11044ms step_avg:57.52ms
step:193/2330 train_time:11100ms step_avg:57.51ms
step:194/2330 train_time:11160ms step_avg:57.52ms
step:195/2330 train_time:11215ms step_avg:57.51ms
step:196/2330 train_time:11275ms step_avg:57.53ms
step:197/2330 train_time:11332ms step_avg:57.52ms
step:198/2330 train_time:11391ms step_avg:57.53ms
step:199/2330 train_time:11447ms step_avg:57.52ms
step:200/2330 train_time:11505ms step_avg:57.53ms
step:201/2330 train_time:11562ms step_avg:57.52ms
step:202/2330 train_time:11621ms step_avg:57.53ms
step:203/2330 train_time:11676ms step_avg:57.52ms
step:204/2330 train_time:11736ms step_avg:57.53ms
step:205/2330 train_time:11792ms step_avg:57.52ms
step:206/2330 train_time:11851ms step_avg:57.53ms
step:207/2330 train_time:11907ms step_avg:57.52ms
step:208/2330 train_time:11966ms step_avg:57.53ms
step:209/2330 train_time:12022ms step_avg:57.52ms
step:210/2330 train_time:12082ms step_avg:57.53ms
step:211/2330 train_time:12137ms step_avg:57.52ms
step:212/2330 train_time:12198ms step_avg:57.54ms
step:213/2330 train_time:12254ms step_avg:57.53ms
step:214/2330 train_time:12313ms step_avg:57.54ms
step:215/2330 train_time:12369ms step_avg:57.53ms
step:216/2330 train_time:12427ms step_avg:57.53ms
step:217/2330 train_time:12483ms step_avg:57.53ms
step:218/2330 train_time:12543ms step_avg:57.54ms
step:219/2330 train_time:12599ms step_avg:57.53ms
step:220/2330 train_time:12658ms step_avg:57.54ms
step:221/2330 train_time:12714ms step_avg:57.53ms
step:222/2330 train_time:12773ms step_avg:57.53ms
step:223/2330 train_time:12829ms step_avg:57.53ms
step:224/2330 train_time:12887ms step_avg:57.53ms
step:225/2330 train_time:12945ms step_avg:57.53ms
step:226/2330 train_time:13003ms step_avg:57.54ms
step:227/2330 train_time:13059ms step_avg:57.53ms
step:228/2330 train_time:13118ms step_avg:57.54ms
step:229/2330 train_time:13174ms step_avg:57.53ms
step:230/2330 train_time:13234ms step_avg:57.54ms
step:231/2330 train_time:13289ms step_avg:57.53ms
step:232/2330 train_time:13349ms step_avg:57.54ms
step:233/2330 train_time:13404ms step_avg:57.53ms
step:234/2330 train_time:13464ms step_avg:57.54ms
step:235/2330 train_time:13520ms step_avg:57.53ms
step:236/2330 train_time:13579ms step_avg:57.54ms
step:237/2330 train_time:13635ms step_avg:57.53ms
step:238/2330 train_time:13694ms step_avg:57.54ms
step:239/2330 train_time:13750ms step_avg:57.53ms
step:240/2330 train_time:13809ms step_avg:57.54ms
step:241/2330 train_time:13866ms step_avg:57.54ms
step:242/2330 train_time:13925ms step_avg:57.54ms
step:243/2330 train_time:13981ms step_avg:57.53ms
step:244/2330 train_time:14041ms step_avg:57.54ms
step:245/2330 train_time:14096ms step_avg:57.54ms
step:246/2330 train_time:14156ms step_avg:57.54ms
step:247/2330 train_time:14212ms step_avg:57.54ms
step:248/2330 train_time:14271ms step_avg:57.54ms
step:249/2330 train_time:14327ms step_avg:57.54ms
step:250/2330 train_time:14385ms step_avg:57.54ms
step:250/2330 val_loss:4.9631 train_time:14465ms step_avg:57.86ms
step:251/2330 train_time:14483ms step_avg:57.70ms
step:252/2330 train_time:14503ms step_avg:57.55ms
step:253/2330 train_time:14558ms step_avg:57.54ms
step:254/2330 train_time:14624ms step_avg:57.58ms
step:255/2330 train_time:14680ms step_avg:57.57ms
step:256/2330 train_time:14744ms step_avg:57.59ms
step:257/2330 train_time:14799ms step_avg:57.58ms
step:258/2330 train_time:14859ms step_avg:57.59ms
step:259/2330 train_time:14914ms step_avg:57.58ms
step:260/2330 train_time:14974ms step_avg:57.59ms
step:261/2330 train_time:15029ms step_avg:57.58ms
step:262/2330 train_time:15088ms step_avg:57.59ms
step:263/2330 train_time:15143ms step_avg:57.58ms
step:264/2330 train_time:15202ms step_avg:57.58ms
step:265/2330 train_time:15257ms step_avg:57.57ms
step:266/2330 train_time:15316ms step_avg:57.58ms
step:267/2330 train_time:15372ms step_avg:57.57ms
step:268/2330 train_time:15432ms step_avg:57.58ms
step:269/2330 train_time:15489ms step_avg:57.58ms
step:270/2330 train_time:15549ms step_avg:57.59ms
step:271/2330 train_time:15606ms step_avg:57.59ms
step:272/2330 train_time:15666ms step_avg:57.60ms
step:273/2330 train_time:15723ms step_avg:57.59ms
step:274/2330 train_time:15783ms step_avg:57.60ms
step:275/2330 train_time:15839ms step_avg:57.60ms
step:276/2330 train_time:15898ms step_avg:57.60ms
step:277/2330 train_time:15953ms step_avg:57.59ms
step:278/2330 train_time:16013ms step_avg:57.60ms
step:279/2330 train_time:16068ms step_avg:57.59ms
step:280/2330 train_time:16127ms step_avg:57.60ms
step:281/2330 train_time:16183ms step_avg:57.59ms
step:282/2330 train_time:16241ms step_avg:57.59ms
step:283/2330 train_time:16297ms step_avg:57.59ms
step:284/2330 train_time:16356ms step_avg:57.59ms
step:285/2330 train_time:16413ms step_avg:57.59ms
step:286/2330 train_time:16472ms step_avg:57.60ms
step:287/2330 train_time:16528ms step_avg:57.59ms
step:288/2330 train_time:16589ms step_avg:57.60ms
step:289/2330 train_time:16646ms step_avg:57.60ms
step:290/2330 train_time:16705ms step_avg:57.60ms
step:291/2330 train_time:16762ms step_avg:57.60ms
step:292/2330 train_time:16820ms step_avg:57.60ms
step:293/2330 train_time:16876ms step_avg:57.60ms
step:294/2330 train_time:16936ms step_avg:57.61ms
step:295/2330 train_time:16992ms step_avg:57.60ms
step:296/2330 train_time:17052ms step_avg:57.61ms
step:297/2330 train_time:17108ms step_avg:57.60ms
step:298/2330 train_time:17167ms step_avg:57.61ms
step:299/2330 train_time:17224ms step_avg:57.60ms
step:300/2330 train_time:17283ms step_avg:57.61ms
step:301/2330 train_time:17338ms step_avg:57.60ms
step:302/2330 train_time:17397ms step_avg:57.61ms
step:303/2330 train_time:17453ms step_avg:57.60ms
step:304/2330 train_time:17515ms step_avg:57.61ms
step:305/2330 train_time:17571ms step_avg:57.61ms
step:306/2330 train_time:17632ms step_avg:57.62ms
step:307/2330 train_time:17688ms step_avg:57.62ms
step:308/2330 train_time:17748ms step_avg:57.62ms
step:309/2330 train_time:17804ms step_avg:57.62ms
step:310/2330 train_time:17864ms step_avg:57.62ms
step:311/2330 train_time:17921ms step_avg:57.62ms
step:312/2330 train_time:17979ms step_avg:57.63ms
step:313/2330 train_time:18035ms step_avg:57.62ms
step:314/2330 train_time:18095ms step_avg:57.63ms
step:315/2330 train_time:18150ms step_avg:57.62ms
step:316/2330 train_time:18211ms step_avg:57.63ms
step:317/2330 train_time:18267ms step_avg:57.62ms
step:318/2330 train_time:18326ms step_avg:57.63ms
step:319/2330 train_time:18382ms step_avg:57.62ms
step:320/2330 train_time:18441ms step_avg:57.63ms
step:321/2330 train_time:18497ms step_avg:57.62ms
step:322/2330 train_time:18557ms step_avg:57.63ms
step:323/2330 train_time:18612ms step_avg:57.62ms
step:324/2330 train_time:18673ms step_avg:57.63ms
step:325/2330 train_time:18729ms step_avg:57.63ms
step:326/2330 train_time:18790ms step_avg:57.64ms
step:327/2330 train_time:18847ms step_avg:57.64ms
step:328/2330 train_time:18906ms step_avg:57.64ms
step:329/2330 train_time:18961ms step_avg:57.63ms
step:330/2330 train_time:19020ms step_avg:57.64ms
step:331/2330 train_time:19076ms step_avg:57.63ms
step:332/2330 train_time:19136ms step_avg:57.64ms
step:333/2330 train_time:19191ms step_avg:57.63ms
step:334/2330 train_time:19251ms step_avg:57.64ms
step:335/2330 train_time:19306ms step_avg:57.63ms
step:336/2330 train_time:19365ms step_avg:57.63ms
step:337/2330 train_time:19421ms step_avg:57.63ms
step:338/2330 train_time:19481ms step_avg:57.64ms
step:339/2330 train_time:19537ms step_avg:57.63ms
step:340/2330 train_time:19597ms step_avg:57.64ms
step:341/2330 train_time:19652ms step_avg:57.63ms
step:342/2330 train_time:19713ms step_avg:57.64ms
step:343/2330 train_time:19769ms step_avg:57.64ms
step:344/2330 train_time:19830ms step_avg:57.64ms
step:345/2330 train_time:19886ms step_avg:57.64ms
step:346/2330 train_time:19944ms step_avg:57.64ms
step:347/2330 train_time:20000ms step_avg:57.64ms
step:348/2330 train_time:20059ms step_avg:57.64ms
step:349/2330 train_time:20115ms step_avg:57.64ms
step:350/2330 train_time:20174ms step_avg:57.64ms
step:351/2330 train_time:20230ms step_avg:57.63ms
step:352/2330 train_time:20291ms step_avg:57.64ms
step:353/2330 train_time:20346ms step_avg:57.64ms
step:354/2330 train_time:20405ms step_avg:57.64ms
step:355/2330 train_time:20462ms step_avg:57.64ms
step:356/2330 train_time:20520ms step_avg:57.64ms
step:357/2330 train_time:20577ms step_avg:57.64ms
step:358/2330 train_time:20637ms step_avg:57.64ms
step:359/2330 train_time:20692ms step_avg:57.64ms
step:360/2330 train_time:20753ms step_avg:57.65ms
step:361/2330 train_time:20809ms step_avg:57.64ms
step:362/2330 train_time:20869ms step_avg:57.65ms
step:363/2330 train_time:20925ms step_avg:57.64ms
step:364/2330 train_time:20984ms step_avg:57.65ms
step:365/2330 train_time:21040ms step_avg:57.64ms
step:366/2330 train_time:21099ms step_avg:57.65ms
step:367/2330 train_time:21154ms step_avg:57.64ms
step:368/2330 train_time:21214ms step_avg:57.65ms
step:369/2330 train_time:21270ms step_avg:57.64ms
step:370/2330 train_time:21330ms step_avg:57.65ms
step:371/2330 train_time:21386ms step_avg:57.64ms
step:372/2330 train_time:21444ms step_avg:57.65ms
step:373/2330 train_time:21500ms step_avg:57.64ms
step:374/2330 train_time:21559ms step_avg:57.64ms
step:375/2330 train_time:21615ms step_avg:57.64ms
step:376/2330 train_time:21675ms step_avg:57.65ms
step:377/2330 train_time:21731ms step_avg:57.64ms
step:378/2330 train_time:21792ms step_avg:57.65ms
step:379/2330 train_time:21848ms step_avg:57.65ms
step:380/2330 train_time:21906ms step_avg:57.65ms
step:381/2330 train_time:21963ms step_avg:57.64ms
step:382/2330 train_time:22021ms step_avg:57.65ms
step:383/2330 train_time:22077ms step_avg:57.64ms
step:384/2330 train_time:22137ms step_avg:57.65ms
step:385/2330 train_time:22193ms step_avg:57.64ms
step:386/2330 train_time:22252ms step_avg:57.65ms
step:387/2330 train_time:22308ms step_avg:57.64ms
step:388/2330 train_time:22368ms step_avg:57.65ms
step:389/2330 train_time:22424ms step_avg:57.64ms
step:390/2330 train_time:22483ms step_avg:57.65ms
step:391/2330 train_time:22539ms step_avg:57.65ms
step:392/2330 train_time:22598ms step_avg:57.65ms
step:393/2330 train_time:22654ms step_avg:57.64ms
step:394/2330 train_time:22714ms step_avg:57.65ms
step:395/2330 train_time:22770ms step_avg:57.64ms
step:396/2330 train_time:22831ms step_avg:57.65ms
step:397/2330 train_time:22886ms step_avg:57.65ms
step:398/2330 train_time:22945ms step_avg:57.65ms
step:399/2330 train_time:23002ms step_avg:57.65ms
step:400/2330 train_time:23061ms step_avg:57.65ms
step:401/2330 train_time:23117ms step_avg:57.65ms
step:402/2330 train_time:23176ms step_avg:57.65ms
step:403/2330 train_time:23233ms step_avg:57.65ms
step:404/2330 train_time:23292ms step_avg:57.65ms
step:405/2330 train_time:23348ms step_avg:57.65ms
step:406/2330 train_time:23407ms step_avg:57.65ms
step:407/2330 train_time:23464ms step_avg:57.65ms
step:408/2330 train_time:23523ms step_avg:57.65ms
step:409/2330 train_time:23579ms step_avg:57.65ms
step:410/2330 train_time:23639ms step_avg:57.65ms
step:411/2330 train_time:23694ms step_avg:57.65ms
step:412/2330 train_time:23754ms step_avg:57.66ms
step:413/2330 train_time:23810ms step_avg:57.65ms
step:414/2330 train_time:23870ms step_avg:57.66ms
step:415/2330 train_time:23927ms step_avg:57.66ms
step:416/2330 train_time:23986ms step_avg:57.66ms
step:417/2330 train_time:24042ms step_avg:57.66ms
step:418/2330 train_time:24101ms step_avg:57.66ms
step:419/2330 train_time:24157ms step_avg:57.65ms
step:420/2330 train_time:24217ms step_avg:57.66ms
step:421/2330 train_time:24272ms step_avg:57.65ms
step:422/2330 train_time:24332ms step_avg:57.66ms
step:423/2330 train_time:24388ms step_avg:57.66ms
step:424/2330 train_time:24447ms step_avg:57.66ms
step:425/2330 train_time:24503ms step_avg:57.65ms
step:426/2330 train_time:24562ms step_avg:57.66ms
step:427/2330 train_time:24618ms step_avg:57.65ms
step:428/2330 train_time:24678ms step_avg:57.66ms
step:429/2330 train_time:24734ms step_avg:57.65ms
step:430/2330 train_time:24794ms step_avg:57.66ms
step:431/2330 train_time:24850ms step_avg:57.66ms
step:432/2330 train_time:24910ms step_avg:57.66ms
step:433/2330 train_time:24966ms step_avg:57.66ms
step:434/2330 train_time:25025ms step_avg:57.66ms
step:435/2330 train_time:25081ms step_avg:57.66ms
step:436/2330 train_time:25140ms step_avg:57.66ms
step:437/2330 train_time:25196ms step_avg:57.66ms
step:438/2330 train_time:25256ms step_avg:57.66ms
step:439/2330 train_time:25311ms step_avg:57.66ms
step:440/2330 train_time:25371ms step_avg:57.66ms
step:441/2330 train_time:25427ms step_avg:57.66ms
step:442/2330 train_time:25487ms step_avg:57.66ms
step:443/2330 train_time:25543ms step_avg:57.66ms
step:444/2330 train_time:25602ms step_avg:57.66ms
step:445/2330 train_time:25658ms step_avg:57.66ms
step:446/2330 train_time:25717ms step_avg:57.66ms
step:447/2330 train_time:25773ms step_avg:57.66ms
step:448/2330 train_time:25833ms step_avg:57.66ms
step:449/2330 train_time:25889ms step_avg:57.66ms
step:450/2330 train_time:25949ms step_avg:57.66ms
step:451/2330 train_time:26005ms step_avg:57.66ms
step:452/2330 train_time:26064ms step_avg:57.66ms
step:453/2330 train_time:26121ms step_avg:57.66ms
step:454/2330 train_time:26179ms step_avg:57.66ms
step:455/2330 train_time:26235ms step_avg:57.66ms
step:456/2330 train_time:26295ms step_avg:57.66ms
step:457/2330 train_time:26350ms step_avg:57.66ms
step:458/2330 train_time:26411ms step_avg:57.67ms
step:459/2330 train_time:26466ms step_avg:57.66ms
step:460/2330 train_time:26525ms step_avg:57.66ms
step:461/2330 train_time:26582ms step_avg:57.66ms
step:462/2330 train_time:26641ms step_avg:57.66ms
step:463/2330 train_time:26698ms step_avg:57.66ms
step:464/2330 train_time:26756ms step_avg:57.66ms
step:465/2330 train_time:26812ms step_avg:57.66ms
step:466/2330 train_time:26873ms step_avg:57.67ms
step:467/2330 train_time:26928ms step_avg:57.66ms
step:468/2330 train_time:26989ms step_avg:57.67ms
step:469/2330 train_time:27045ms step_avg:57.66ms
step:470/2330 train_time:27104ms step_avg:57.67ms
step:471/2330 train_time:27160ms step_avg:57.67ms
step:472/2330 train_time:27219ms step_avg:57.67ms
step:473/2330 train_time:27274ms step_avg:57.66ms
step:474/2330 train_time:27334ms step_avg:57.67ms
step:475/2330 train_time:27390ms step_avg:57.66ms
step:476/2330 train_time:27450ms step_avg:57.67ms
step:477/2330 train_time:27506ms step_avg:57.66ms
step:478/2330 train_time:27565ms step_avg:57.67ms
step:479/2330 train_time:27621ms step_avg:57.66ms
step:480/2330 train_time:27680ms step_avg:57.67ms
step:481/2330 train_time:27736ms step_avg:57.66ms
step:482/2330 train_time:27795ms step_avg:57.67ms
step:483/2330 train_time:27851ms step_avg:57.66ms
step:484/2330 train_time:27912ms step_avg:57.67ms
step:485/2330 train_time:27967ms step_avg:57.66ms
step:486/2330 train_time:28027ms step_avg:57.67ms
step:487/2330 train_time:28084ms step_avg:57.67ms
step:488/2330 train_time:28143ms step_avg:57.67ms
step:489/2330 train_time:28200ms step_avg:57.67ms
step:490/2330 train_time:28258ms step_avg:57.67ms
step:491/2330 train_time:28314ms step_avg:57.67ms
step:492/2330 train_time:28374ms step_avg:57.67ms
step:493/2330 train_time:28430ms step_avg:57.67ms
step:494/2330 train_time:28490ms step_avg:57.67ms
step:495/2330 train_time:28546ms step_avg:57.67ms
step:496/2330 train_time:28605ms step_avg:57.67ms
step:497/2330 train_time:28661ms step_avg:57.67ms
step:498/2330 train_time:28720ms step_avg:57.67ms
step:499/2330 train_time:28776ms step_avg:57.67ms
step:500/2330 train_time:28835ms step_avg:57.67ms
step:500/2330 val_loss:4.4471 train_time:28916ms step_avg:57.83ms
step:501/2330 train_time:28934ms step_avg:57.75ms
step:502/2330 train_time:28954ms step_avg:57.68ms
step:503/2330 train_time:29012ms step_avg:57.68ms
step:504/2330 train_time:29074ms step_avg:57.69ms
step:505/2330 train_time:29131ms step_avg:57.69ms
step:506/2330 train_time:29192ms step_avg:57.69ms
step:507/2330 train_time:29248ms step_avg:57.69ms
step:508/2330 train_time:29307ms step_avg:57.69ms
step:509/2330 train_time:29363ms step_avg:57.69ms
step:510/2330 train_time:29422ms step_avg:57.69ms
step:511/2330 train_time:29477ms step_avg:57.69ms
step:512/2330 train_time:29537ms step_avg:57.69ms
step:513/2330 train_time:29592ms step_avg:57.68ms
step:514/2330 train_time:29651ms step_avg:57.69ms
step:515/2330 train_time:29706ms step_avg:57.68ms
step:516/2330 train_time:29766ms step_avg:57.69ms
step:517/2330 train_time:29822ms step_avg:57.68ms
step:518/2330 train_time:29881ms step_avg:57.69ms
step:519/2330 train_time:29938ms step_avg:57.68ms
step:520/2330 train_time:29997ms step_avg:57.69ms
step:521/2330 train_time:30055ms step_avg:57.69ms
step:522/2330 train_time:30115ms step_avg:57.69ms
step:523/2330 train_time:30172ms step_avg:57.69ms
step:524/2330 train_time:30231ms step_avg:57.69ms
step:525/2330 train_time:30288ms step_avg:57.69ms
step:526/2330 train_time:30347ms step_avg:57.69ms
step:527/2330 train_time:30402ms step_avg:57.69ms
step:528/2330 train_time:30462ms step_avg:57.69ms
step:529/2330 train_time:30517ms step_avg:57.69ms
step:530/2330 train_time:30577ms step_avg:57.69ms
step:531/2330 train_time:30633ms step_avg:57.69ms
step:532/2330 train_time:30691ms step_avg:57.69ms
step:533/2330 train_time:30747ms step_avg:57.69ms
step:534/2330 train_time:30805ms step_avg:57.69ms
step:535/2330 train_time:30861ms step_avg:57.68ms
step:536/2330 train_time:30921ms step_avg:57.69ms
step:537/2330 train_time:30978ms step_avg:57.69ms
step:538/2330 train_time:31039ms step_avg:57.69ms
step:539/2330 train_time:31095ms step_avg:57.69ms
step:540/2330 train_time:31155ms step_avg:57.69ms
step:541/2330 train_time:31211ms step_avg:57.69ms
step:542/2330 train_time:31271ms step_avg:57.70ms
step:543/2330 train_time:31328ms step_avg:57.69ms
step:544/2330 train_time:31387ms step_avg:57.70ms
step:545/2330 train_time:31443ms step_avg:57.69ms
step:546/2330 train_time:31502ms step_avg:57.70ms
step:547/2330 train_time:31557ms step_avg:57.69ms
step:548/2330 train_time:31617ms step_avg:57.70ms
step:549/2330 train_time:31673ms step_avg:57.69ms
step:550/2330 train_time:31731ms step_avg:57.69ms
step:551/2330 train_time:31787ms step_avg:57.69ms
step:552/2330 train_time:31846ms step_avg:57.69ms
step:553/2330 train_time:31902ms step_avg:57.69ms
step:554/2330 train_time:31963ms step_avg:57.69ms
step:555/2330 train_time:32018ms step_avg:57.69ms
step:556/2330 train_time:32079ms step_avg:57.70ms
step:557/2330 train_time:32135ms step_avg:57.69ms
step:558/2330 train_time:32195ms step_avg:57.70ms
step:559/2330 train_time:32251ms step_avg:57.69ms
step:560/2330 train_time:32310ms step_avg:57.70ms
step:561/2330 train_time:32367ms step_avg:57.70ms
step:562/2330 train_time:32426ms step_avg:57.70ms
step:563/2330 train_time:32482ms step_avg:57.69ms
step:564/2330 train_time:32542ms step_avg:57.70ms
step:565/2330 train_time:32598ms step_avg:57.69ms
step:566/2330 train_time:32658ms step_avg:57.70ms
step:567/2330 train_time:32714ms step_avg:57.70ms
step:568/2330 train_time:32773ms step_avg:57.70ms
step:569/2330 train_time:32829ms step_avg:57.70ms
step:570/2330 train_time:32888ms step_avg:57.70ms
step:571/2330 train_time:32944ms step_avg:57.70ms
step:572/2330 train_time:33003ms step_avg:57.70ms
step:573/2330 train_time:33059ms step_avg:57.69ms
step:574/2330 train_time:33120ms step_avg:57.70ms
step:575/2330 train_time:33177ms step_avg:57.70ms
step:576/2330 train_time:33236ms step_avg:57.70ms
step:577/2330 train_time:33292ms step_avg:57.70ms
step:578/2330 train_time:33351ms step_avg:57.70ms
step:579/2330 train_time:33408ms step_avg:57.70ms
step:580/2330 train_time:33467ms step_avg:57.70ms
step:581/2330 train_time:33522ms step_avg:57.70ms
step:582/2330 train_time:33583ms step_avg:57.70ms
step:583/2330 train_time:33638ms step_avg:57.70ms
step:584/2330 train_time:33698ms step_avg:57.70ms
step:585/2330 train_time:33754ms step_avg:57.70ms
step:586/2330 train_time:33813ms step_avg:57.70ms
step:587/2330 train_time:33869ms step_avg:57.70ms
step:588/2330 train_time:33928ms step_avg:57.70ms
step:589/2330 train_time:33984ms step_avg:57.70ms
step:590/2330 train_time:34043ms step_avg:57.70ms
step:591/2330 train_time:34099ms step_avg:57.70ms
step:592/2330 train_time:34160ms step_avg:57.70ms
step:593/2330 train_time:34215ms step_avg:57.70ms
step:594/2330 train_time:34276ms step_avg:57.70ms
step:595/2330 train_time:34332ms step_avg:57.70ms
step:596/2330 train_time:34391ms step_avg:57.70ms
step:597/2330 train_time:34448ms step_avg:57.70ms
step:598/2330 train_time:34507ms step_avg:57.70ms
step:599/2330 train_time:34563ms step_avg:57.70ms
step:600/2330 train_time:34624ms step_avg:57.71ms
step:601/2330 train_time:34679ms step_avg:57.70ms
step:602/2330 train_time:34740ms step_avg:57.71ms
step:603/2330 train_time:34795ms step_avg:57.70ms
step:604/2330 train_time:34855ms step_avg:57.71ms
step:605/2330 train_time:34910ms step_avg:57.70ms
step:606/2330 train_time:34970ms step_avg:57.71ms
step:607/2330 train_time:35026ms step_avg:57.70ms
step:608/2330 train_time:35085ms step_avg:57.71ms
step:609/2330 train_time:35141ms step_avg:57.70ms
step:610/2330 train_time:35201ms step_avg:57.71ms
step:611/2330 train_time:35257ms step_avg:57.70ms
step:612/2330 train_time:35317ms step_avg:57.71ms
step:613/2330 train_time:35374ms step_avg:57.71ms
step:614/2330 train_time:35433ms step_avg:57.71ms
step:615/2330 train_time:35489ms step_avg:57.71ms
step:616/2330 train_time:35548ms step_avg:57.71ms
step:617/2330 train_time:35604ms step_avg:57.70ms
step:618/2330 train_time:35663ms step_avg:57.71ms
step:619/2330 train_time:35719ms step_avg:57.70ms
step:620/2330 train_time:35780ms step_avg:57.71ms
step:621/2330 train_time:35836ms step_avg:57.71ms
step:622/2330 train_time:35894ms step_avg:57.71ms
step:623/2330 train_time:35951ms step_avg:57.71ms
step:624/2330 train_time:36010ms step_avg:57.71ms
step:625/2330 train_time:36065ms step_avg:57.70ms
step:626/2330 train_time:36125ms step_avg:57.71ms
step:627/2330 train_time:36181ms step_avg:57.70ms
step:628/2330 train_time:36242ms step_avg:57.71ms
step:629/2330 train_time:36297ms step_avg:57.71ms
step:630/2330 train_time:36358ms step_avg:57.71ms
step:631/2330 train_time:36414ms step_avg:57.71ms
step:632/2330 train_time:36473ms step_avg:57.71ms
step:633/2330 train_time:36530ms step_avg:57.71ms
step:634/2330 train_time:36590ms step_avg:57.71ms
step:635/2330 train_time:36646ms step_avg:57.71ms
step:636/2330 train_time:36704ms step_avg:57.71ms
step:637/2330 train_time:36760ms step_avg:57.71ms
step:638/2330 train_time:36821ms step_avg:57.71ms
step:639/2330 train_time:36877ms step_avg:57.71ms
step:640/2330 train_time:36936ms step_avg:57.71ms
step:641/2330 train_time:36992ms step_avg:57.71ms
step:642/2330 train_time:37051ms step_avg:57.71ms
step:643/2330 train_time:37108ms step_avg:57.71ms
step:644/2330 train_time:37167ms step_avg:57.71ms
step:645/2330 train_time:37223ms step_avg:57.71ms
step:646/2330 train_time:37283ms step_avg:57.71ms
step:647/2330 train_time:37339ms step_avg:57.71ms
step:648/2330 train_time:37400ms step_avg:57.72ms
step:649/2330 train_time:37456ms step_avg:57.71ms
step:650/2330 train_time:37515ms step_avg:57.72ms
step:651/2330 train_time:37571ms step_avg:57.71ms
step:652/2330 train_time:37630ms step_avg:57.71ms
step:653/2330 train_time:37686ms step_avg:57.71ms
step:654/2330 train_time:37746ms step_avg:57.72ms
step:655/2330 train_time:37802ms step_avg:57.71ms
step:656/2330 train_time:37862ms step_avg:57.72ms
step:657/2330 train_time:37917ms step_avg:57.71ms
step:658/2330 train_time:37978ms step_avg:57.72ms
step:659/2330 train_time:38034ms step_avg:57.72ms
step:660/2330 train_time:38093ms step_avg:57.72ms
step:661/2330 train_time:38149ms step_avg:57.71ms
step:662/2330 train_time:38208ms step_avg:57.72ms
step:663/2330 train_time:38264ms step_avg:57.71ms
step:664/2330 train_time:38324ms step_avg:57.72ms
step:665/2330 train_time:38380ms step_avg:57.71ms
step:666/2330 train_time:38441ms step_avg:57.72ms
step:667/2330 train_time:38496ms step_avg:57.72ms
step:668/2330 train_time:38556ms step_avg:57.72ms
step:669/2330 train_time:38611ms step_avg:57.72ms
step:670/2330 train_time:38671ms step_avg:57.72ms
step:671/2330 train_time:38727ms step_avg:57.72ms
step:672/2330 train_time:38787ms step_avg:57.72ms
step:673/2330 train_time:38842ms step_avg:57.72ms
step:674/2330 train_time:38902ms step_avg:57.72ms
step:675/2330 train_time:38958ms step_avg:57.72ms
step:676/2330 train_time:39017ms step_avg:57.72ms
step:677/2330 train_time:39075ms step_avg:57.72ms
step:678/2330 train_time:39134ms step_avg:57.72ms
step:679/2330 train_time:39190ms step_avg:57.72ms
step:680/2330 train_time:39249ms step_avg:57.72ms
step:681/2330 train_time:39305ms step_avg:57.72ms
step:682/2330 train_time:39364ms step_avg:57.72ms
step:683/2330 train_time:39420ms step_avg:57.72ms
step:684/2330 train_time:39482ms step_avg:57.72ms
step:685/2330 train_time:39537ms step_avg:57.72ms
step:686/2330 train_time:39596ms step_avg:57.72ms
step:687/2330 train_time:39653ms step_avg:57.72ms
step:688/2330 train_time:39712ms step_avg:57.72ms
step:689/2330 train_time:39769ms step_avg:57.72ms
step:690/2330 train_time:39827ms step_avg:57.72ms
step:691/2330 train_time:39884ms step_avg:57.72ms
step:692/2330 train_time:39944ms step_avg:57.72ms
step:693/2330 train_time:40000ms step_avg:57.72ms
step:694/2330 train_time:40059ms step_avg:57.72ms
step:695/2330 train_time:40115ms step_avg:57.72ms
step:696/2330 train_time:40175ms step_avg:57.72ms
step:697/2330 train_time:40232ms step_avg:57.72ms
step:698/2330 train_time:40291ms step_avg:57.72ms
step:699/2330 train_time:40346ms step_avg:57.72ms
step:700/2330 train_time:40406ms step_avg:57.72ms
step:701/2330 train_time:40462ms step_avg:57.72ms
step:702/2330 train_time:40522ms step_avg:57.72ms
step:703/2330 train_time:40578ms step_avg:57.72ms
step:704/2330 train_time:40638ms step_avg:57.72ms
step:705/2330 train_time:40694ms step_avg:57.72ms
step:706/2330 train_time:40753ms step_avg:57.72ms
step:707/2330 train_time:40809ms step_avg:57.72ms
step:708/2330 train_time:40868ms step_avg:57.72ms
step:709/2330 train_time:40925ms step_avg:57.72ms
step:710/2330 train_time:40984ms step_avg:57.72ms
step:711/2330 train_time:41039ms step_avg:57.72ms
step:712/2330 train_time:41099ms step_avg:57.72ms
step:713/2330 train_time:41155ms step_avg:57.72ms
step:714/2330 train_time:41216ms step_avg:57.73ms
step:715/2330 train_time:41272ms step_avg:57.72ms
step:716/2330 train_time:41331ms step_avg:57.72ms
step:717/2330 train_time:41387ms step_avg:57.72ms
step:718/2330 train_time:41446ms step_avg:57.72ms
step:719/2330 train_time:41502ms step_avg:57.72ms
step:720/2330 train_time:41562ms step_avg:57.73ms
step:721/2330 train_time:41618ms step_avg:57.72ms
step:722/2330 train_time:41678ms step_avg:57.73ms
step:723/2330 train_time:41734ms step_avg:57.72ms
step:724/2330 train_time:41793ms step_avg:57.73ms
step:725/2330 train_time:41849ms step_avg:57.72ms
step:726/2330 train_time:41909ms step_avg:57.73ms
step:727/2330 train_time:41965ms step_avg:57.72ms
step:728/2330 train_time:42024ms step_avg:57.73ms
step:729/2330 train_time:42080ms step_avg:57.72ms
step:730/2330 train_time:42141ms step_avg:57.73ms
step:731/2330 train_time:42196ms step_avg:57.72ms
step:732/2330 train_time:42256ms step_avg:57.73ms
step:733/2330 train_time:42312ms step_avg:57.72ms
step:734/2330 train_time:42371ms step_avg:57.73ms
step:735/2330 train_time:42428ms step_avg:57.73ms
step:736/2330 train_time:42487ms step_avg:57.73ms
step:737/2330 train_time:42543ms step_avg:57.72ms
step:738/2330 train_time:42603ms step_avg:57.73ms
step:739/2330 train_time:42659ms step_avg:57.72ms
step:740/2330 train_time:42719ms step_avg:57.73ms
step:741/2330 train_time:42776ms step_avg:57.73ms
step:742/2330 train_time:42835ms step_avg:57.73ms
step:743/2330 train_time:42892ms step_avg:57.73ms
step:744/2330 train_time:42950ms step_avg:57.73ms
step:745/2330 train_time:43006ms step_avg:57.73ms
step:746/2330 train_time:43066ms step_avg:57.73ms
step:747/2330 train_time:43122ms step_avg:57.73ms
step:748/2330 train_time:43182ms step_avg:57.73ms
step:749/2330 train_time:43238ms step_avg:57.73ms
step:750/2330 train_time:43298ms step_avg:57.73ms
step:750/2330 val_loss:4.2426 train_time:43376ms step_avg:57.84ms
step:751/2330 train_time:43394ms step_avg:57.78ms
step:752/2330 train_time:43416ms step_avg:57.73ms
step:753/2330 train_time:43472ms step_avg:57.73ms
step:754/2330 train_time:43535ms step_avg:57.74ms
step:755/2330 train_time:43592ms step_avg:57.74ms
step:756/2330 train_time:43651ms step_avg:57.74ms
step:757/2330 train_time:43707ms step_avg:57.74ms
step:758/2330 train_time:43766ms step_avg:57.74ms
step:759/2330 train_time:43822ms step_avg:57.74ms
step:760/2330 train_time:43882ms step_avg:57.74ms
step:761/2330 train_time:43937ms step_avg:57.74ms
step:762/2330 train_time:43996ms step_avg:57.74ms
step:763/2330 train_time:44052ms step_avg:57.73ms
step:764/2330 train_time:44110ms step_avg:57.74ms
step:765/2330 train_time:44167ms step_avg:57.73ms
step:766/2330 train_time:44225ms step_avg:57.73ms
step:767/2330 train_time:44281ms step_avg:57.73ms
step:768/2330 train_time:44342ms step_avg:57.74ms
step:769/2330 train_time:44399ms step_avg:57.74ms
step:770/2330 train_time:44464ms step_avg:57.75ms
step:771/2330 train_time:44521ms step_avg:57.74ms
step:772/2330 train_time:44583ms step_avg:57.75ms
step:773/2330 train_time:44640ms step_avg:57.75ms
step:774/2330 train_time:44702ms step_avg:57.75ms
step:775/2330 train_time:44758ms step_avg:57.75ms
step:776/2330 train_time:44818ms step_avg:57.75ms
step:777/2330 train_time:44875ms step_avg:57.75ms
step:778/2330 train_time:44935ms step_avg:57.76ms
step:779/2330 train_time:44992ms step_avg:57.76ms
step:780/2330 train_time:45051ms step_avg:57.76ms
step:781/2330 train_time:45108ms step_avg:57.76ms
step:782/2330 train_time:45167ms step_avg:57.76ms
step:783/2330 train_time:45224ms step_avg:57.76ms
step:784/2330 train_time:45283ms step_avg:57.76ms
step:785/2330 train_time:45340ms step_avg:57.76ms
step:786/2330 train_time:45402ms step_avg:57.76ms
step:787/2330 train_time:45459ms step_avg:57.76ms
step:788/2330 train_time:45519ms step_avg:57.77ms
step:789/2330 train_time:45577ms step_avg:57.77ms
step:790/2330 train_time:45636ms step_avg:57.77ms
step:791/2330 train_time:45693ms step_avg:57.77ms
step:792/2330 train_time:45753ms step_avg:57.77ms
step:793/2330 train_time:45811ms step_avg:57.77ms
step:794/2330 train_time:45870ms step_avg:57.77ms
step:795/2330 train_time:45927ms step_avg:57.77ms
step:796/2330 train_time:45987ms step_avg:57.77ms
step:797/2330 train_time:46044ms step_avg:57.77ms
step:798/2330 train_time:46104ms step_avg:57.77ms
step:799/2330 train_time:46161ms step_avg:57.77ms
step:800/2330 train_time:46220ms step_avg:57.78ms
step:801/2330 train_time:46277ms step_avg:57.77ms
step:802/2330 train_time:46337ms step_avg:57.78ms
step:803/2330 train_time:46394ms step_avg:57.78ms
step:804/2330 train_time:46454ms step_avg:57.78ms
step:805/2330 train_time:46512ms step_avg:57.78ms
step:806/2330 train_time:46571ms step_avg:57.78ms
step:807/2330 train_time:46629ms step_avg:57.78ms
step:808/2330 train_time:46690ms step_avg:57.78ms
step:809/2330 train_time:46746ms step_avg:57.78ms
step:810/2330 train_time:46807ms step_avg:57.79ms
step:811/2330 train_time:46864ms step_avg:57.79ms
step:812/2330 train_time:46924ms step_avg:57.79ms
step:813/2330 train_time:46981ms step_avg:57.79ms
step:814/2330 train_time:47040ms step_avg:57.79ms
step:815/2330 train_time:47097ms step_avg:57.79ms
step:816/2330 train_time:47157ms step_avg:57.79ms
step:817/2330 train_time:47214ms step_avg:57.79ms
step:818/2330 train_time:47274ms step_avg:57.79ms
step:819/2330 train_time:47331ms step_avg:57.79ms
step:820/2330 train_time:47391ms step_avg:57.79ms
step:821/2330 train_time:47447ms step_avg:57.79ms
step:822/2330 train_time:47507ms step_avg:57.79ms
step:823/2330 train_time:47564ms step_avg:57.79ms
step:824/2330 train_time:47625ms step_avg:57.80ms
step:825/2330 train_time:47682ms step_avg:57.80ms
step:826/2330 train_time:47743ms step_avg:57.80ms
step:827/2330 train_time:47800ms step_avg:57.80ms
step:828/2330 train_time:47860ms step_avg:57.80ms
step:829/2330 train_time:47917ms step_avg:57.80ms
step:830/2330 train_time:47977ms step_avg:57.80ms
step:831/2330 train_time:48034ms step_avg:57.80ms
step:832/2330 train_time:48094ms step_avg:57.80ms
step:833/2330 train_time:48150ms step_avg:57.80ms
step:834/2330 train_time:48209ms step_avg:57.80ms
step:835/2330 train_time:48266ms step_avg:57.80ms
step:836/2330 train_time:48326ms step_avg:57.81ms
step:837/2330 train_time:48383ms step_avg:57.81ms
step:838/2330 train_time:48443ms step_avg:57.81ms
step:839/2330 train_time:48500ms step_avg:57.81ms
step:840/2330 train_time:48560ms step_avg:57.81ms
step:841/2330 train_time:48618ms step_avg:57.81ms
step:842/2330 train_time:48678ms step_avg:57.81ms
step:843/2330 train_time:48734ms step_avg:57.81ms
step:844/2330 train_time:48795ms step_avg:57.81ms
step:845/2330 train_time:48852ms step_avg:57.81ms
step:846/2330 train_time:48911ms step_avg:57.81ms
step:847/2330 train_time:48968ms step_avg:57.81ms
step:848/2330 train_time:49028ms step_avg:57.82ms
step:849/2330 train_time:49085ms step_avg:57.82ms
step:850/2330 train_time:49145ms step_avg:57.82ms
step:851/2330 train_time:49202ms step_avg:57.82ms
step:852/2330 train_time:49262ms step_avg:57.82ms
step:853/2330 train_time:49318ms step_avg:57.82ms
step:854/2330 train_time:49378ms step_avg:57.82ms
step:855/2330 train_time:49435ms step_avg:57.82ms
step:856/2330 train_time:49495ms step_avg:57.82ms
step:857/2330 train_time:49552ms step_avg:57.82ms
step:858/2330 train_time:49612ms step_avg:57.82ms
step:859/2330 train_time:49668ms step_avg:57.82ms
step:860/2330 train_time:49728ms step_avg:57.82ms
step:861/2330 train_time:49786ms step_avg:57.82ms
step:862/2330 train_time:49847ms step_avg:57.83ms
step:863/2330 train_time:49904ms step_avg:57.83ms
step:864/2330 train_time:49965ms step_avg:57.83ms
step:865/2330 train_time:50021ms step_avg:57.83ms
step:866/2330 train_time:50081ms step_avg:57.83ms
step:867/2330 train_time:50138ms step_avg:57.83ms
step:868/2330 train_time:50198ms step_avg:57.83ms
step:869/2330 train_time:50255ms step_avg:57.83ms
step:870/2330 train_time:50315ms step_avg:57.83ms
step:871/2330 train_time:50372ms step_avg:57.83ms
step:872/2330 train_time:50433ms step_avg:57.84ms
step:873/2330 train_time:50490ms step_avg:57.84ms
step:874/2330 train_time:50549ms step_avg:57.84ms
step:875/2330 train_time:50607ms step_avg:57.84ms
step:876/2330 train_time:50667ms step_avg:57.84ms
step:877/2330 train_time:50724ms step_avg:57.84ms
step:878/2330 train_time:50785ms step_avg:57.84ms
step:879/2330 train_time:50841ms step_avg:57.84ms
step:880/2330 train_time:50903ms step_avg:57.84ms
step:881/2330 train_time:50959ms step_avg:57.84ms
step:882/2330 train_time:51020ms step_avg:57.85ms
step:883/2330 train_time:51077ms step_avg:57.84ms
step:884/2330 train_time:51136ms step_avg:57.85ms
step:885/2330 train_time:51193ms step_avg:57.85ms
step:886/2330 train_time:51253ms step_avg:57.85ms
step:887/2330 train_time:51310ms step_avg:57.85ms
step:888/2330 train_time:51369ms step_avg:57.85ms
step:889/2330 train_time:51426ms step_avg:57.85ms
step:890/2330 train_time:51487ms step_avg:57.85ms
step:891/2330 train_time:51544ms step_avg:57.85ms
step:892/2330 train_time:51604ms step_avg:57.85ms
step:893/2330 train_time:51661ms step_avg:57.85ms
step:894/2330 train_time:51722ms step_avg:57.85ms
step:895/2330 train_time:51779ms step_avg:57.85ms
step:896/2330 train_time:51839ms step_avg:57.86ms
step:897/2330 train_time:51896ms step_avg:57.86ms
step:898/2330 train_time:51956ms step_avg:57.86ms
step:899/2330 train_time:52012ms step_avg:57.86ms
step:900/2330 train_time:52072ms step_avg:57.86ms
step:901/2330 train_time:52129ms step_avg:57.86ms
step:902/2330 train_time:52188ms step_avg:57.86ms
step:903/2330 train_time:52244ms step_avg:57.86ms
step:904/2330 train_time:52305ms step_avg:57.86ms
step:905/2330 train_time:52362ms step_avg:57.86ms
step:906/2330 train_time:52422ms step_avg:57.86ms
step:907/2330 train_time:52479ms step_avg:57.86ms
step:908/2330 train_time:52539ms step_avg:57.86ms
step:909/2330 train_time:52596ms step_avg:57.86ms
step:910/2330 train_time:52656ms step_avg:57.86ms
step:911/2330 train_time:52714ms step_avg:57.86ms
step:912/2330 train_time:52773ms step_avg:57.87ms
step:913/2330 train_time:52830ms step_avg:57.86ms
step:914/2330 train_time:52890ms step_avg:57.87ms
step:915/2330 train_time:52947ms step_avg:57.87ms
step:916/2330 train_time:53007ms step_avg:57.87ms
step:917/2330 train_time:53064ms step_avg:57.87ms
step:918/2330 train_time:53124ms step_avg:57.87ms
step:919/2330 train_time:53181ms step_avg:57.87ms
step:920/2330 train_time:53241ms step_avg:57.87ms
step:921/2330 train_time:53298ms step_avg:57.87ms
step:922/2330 train_time:53358ms step_avg:57.87ms
step:923/2330 train_time:53415ms step_avg:57.87ms
step:924/2330 train_time:53475ms step_avg:57.87ms
step:925/2330 train_time:53533ms step_avg:57.87ms
step:926/2330 train_time:53592ms step_avg:57.87ms
step:927/2330 train_time:53649ms step_avg:57.87ms
step:928/2330 train_time:53708ms step_avg:57.88ms
step:929/2330 train_time:53765ms step_avg:57.87ms
step:930/2330 train_time:53826ms step_avg:57.88ms
step:931/2330 train_time:53883ms step_avg:57.88ms
step:932/2330 train_time:53943ms step_avg:57.88ms
step:933/2330 train_time:54000ms step_avg:57.88ms
step:934/2330 train_time:54060ms step_avg:57.88ms
step:935/2330 train_time:54117ms step_avg:57.88ms
step:936/2330 train_time:54177ms step_avg:57.88ms
step:937/2330 train_time:54234ms step_avg:57.88ms
step:938/2330 train_time:54294ms step_avg:57.88ms
step:939/2330 train_time:54350ms step_avg:57.88ms
step:940/2330 train_time:54410ms step_avg:57.88ms
step:941/2330 train_time:54467ms step_avg:57.88ms
step:942/2330 train_time:54527ms step_avg:57.88ms
step:943/2330 train_time:54584ms step_avg:57.88ms
step:944/2330 train_time:54644ms step_avg:57.89ms
step:945/2330 train_time:54701ms step_avg:57.88ms
step:946/2330 train_time:54762ms step_avg:57.89ms
step:947/2330 train_time:54820ms step_avg:57.89ms
step:948/2330 train_time:54879ms step_avg:57.89ms
step:949/2330 train_time:54937ms step_avg:57.89ms
step:950/2330 train_time:54996ms step_avg:57.89ms
step:951/2330 train_time:55053ms step_avg:57.89ms
step:952/2330 train_time:55113ms step_avg:57.89ms
step:953/2330 train_time:55170ms step_avg:57.89ms
step:954/2330 train_time:55229ms step_avg:57.89ms
step:955/2330 train_time:55286ms step_avg:57.89ms
step:956/2330 train_time:55347ms step_avg:57.89ms
step:957/2330 train_time:55403ms step_avg:57.89ms
step:958/2330 train_time:55464ms step_avg:57.90ms
step:959/2330 train_time:55521ms step_avg:57.90ms
step:960/2330 train_time:55582ms step_avg:57.90ms
step:961/2330 train_time:55638ms step_avg:57.90ms
step:962/2330 train_time:55699ms step_avg:57.90ms
step:963/2330 train_time:55756ms step_avg:57.90ms
step:964/2330 train_time:55815ms step_avg:57.90ms
step:965/2330 train_time:55873ms step_avg:57.90ms
step:966/2330 train_time:55933ms step_avg:57.90ms
step:967/2330 train_time:55990ms step_avg:57.90ms
step:968/2330 train_time:56049ms step_avg:57.90ms
step:969/2330 train_time:56107ms step_avg:57.90ms
step:970/2330 train_time:56167ms step_avg:57.90ms
step:971/2330 train_time:56224ms step_avg:57.90ms
step:972/2330 train_time:56284ms step_avg:57.91ms
step:973/2330 train_time:56340ms step_avg:57.90ms
step:974/2330 train_time:56401ms step_avg:57.91ms
step:975/2330 train_time:56458ms step_avg:57.91ms
step:976/2330 train_time:56518ms step_avg:57.91ms
step:977/2330 train_time:56576ms step_avg:57.91ms
step:978/2330 train_time:56636ms step_avg:57.91ms
step:979/2330 train_time:56693ms step_avg:57.91ms
step:980/2330 train_time:56752ms step_avg:57.91ms
step:981/2330 train_time:56809ms step_avg:57.91ms
step:982/2330 train_time:56869ms step_avg:57.91ms
step:983/2330 train_time:56925ms step_avg:57.91ms
step:984/2330 train_time:56986ms step_avg:57.91ms
step:985/2330 train_time:57043ms step_avg:57.91ms
step:986/2330 train_time:57103ms step_avg:57.91ms
step:987/2330 train_time:57160ms step_avg:57.91ms
step:988/2330 train_time:57221ms step_avg:57.92ms
step:989/2330 train_time:57278ms step_avg:57.91ms
step:990/2330 train_time:57337ms step_avg:57.92ms
step:991/2330 train_time:57394ms step_avg:57.92ms
step:992/2330 train_time:57454ms step_avg:57.92ms
step:993/2330 train_time:57510ms step_avg:57.92ms
step:994/2330 train_time:57570ms step_avg:57.92ms
step:995/2330 train_time:57627ms step_avg:57.92ms
step:996/2330 train_time:57687ms step_avg:57.92ms
step:997/2330 train_time:57744ms step_avg:57.92ms
step:998/2330 train_time:57805ms step_avg:57.92ms
step:999/2330 train_time:57862ms step_avg:57.92ms
step:1000/2330 train_time:57922ms step_avg:57.92ms
step:1000/2330 val_loss:4.0948 train_time:58002ms step_avg:58.00ms
step:1001/2330 train_time:58021ms step_avg:57.96ms
step:1002/2330 train_time:58040ms step_avg:57.92ms
step:1003/2330 train_time:58094ms step_avg:57.92ms
step:1004/2330 train_time:58159ms step_avg:57.93ms
step:1005/2330 train_time:58215ms step_avg:57.93ms
step:1006/2330 train_time:58279ms step_avg:57.93ms
step:1007/2330 train_time:58335ms step_avg:57.93ms
step:1008/2330 train_time:58394ms step_avg:57.93ms
step:1009/2330 train_time:58451ms step_avg:57.93ms
step:1010/2330 train_time:58510ms step_avg:57.93ms
step:1011/2330 train_time:58566ms step_avg:57.93ms
step:1012/2330 train_time:58624ms step_avg:57.93ms
step:1013/2330 train_time:58681ms step_avg:57.93ms
step:1014/2330 train_time:58740ms step_avg:57.93ms
step:1015/2330 train_time:58795ms step_avg:57.93ms
step:1016/2330 train_time:58856ms step_avg:57.93ms
step:1017/2330 train_time:58914ms step_avg:57.93ms
step:1018/2330 train_time:58980ms step_avg:57.94ms
step:1019/2330 train_time:59037ms step_avg:57.94ms
step:1020/2330 train_time:59100ms step_avg:57.94ms
step:1021/2330 train_time:59157ms step_avg:57.94ms
step:1022/2330 train_time:59218ms step_avg:57.94ms
step:1023/2330 train_time:59274ms step_avg:57.94ms
step:1024/2330 train_time:59335ms step_avg:57.94ms
step:1025/2330 train_time:59392ms step_avg:57.94ms
step:1026/2330 train_time:59451ms step_avg:57.94ms
step:1027/2330 train_time:59508ms step_avg:57.94ms
step:1028/2330 train_time:59567ms step_avg:57.94ms
step:1029/2330 train_time:59624ms step_avg:57.94ms
step:1030/2330 train_time:59683ms step_avg:57.94ms
step:1031/2330 train_time:59739ms step_avg:57.94ms
step:1032/2330 train_time:59799ms step_avg:57.94ms
step:1033/2330 train_time:59856ms step_avg:57.94ms
step:1034/2330 train_time:59917ms step_avg:57.95ms
step:1035/2330 train_time:59975ms step_avg:57.95ms
step:1036/2330 train_time:60036ms step_avg:57.95ms
step:1037/2330 train_time:60094ms step_avg:57.95ms
step:1038/2330 train_time:60156ms step_avg:57.95ms
step:1039/2330 train_time:60212ms step_avg:57.95ms
step:1040/2330 train_time:60273ms step_avg:57.95ms
step:1041/2330 train_time:60329ms step_avg:57.95ms
step:1042/2330 train_time:60389ms step_avg:57.96ms
step:1043/2330 train_time:60446ms step_avg:57.95ms
step:1044/2330 train_time:60505ms step_avg:57.96ms
step:1045/2330 train_time:60562ms step_avg:57.95ms
step:1046/2330 train_time:60621ms step_avg:57.95ms
step:1047/2330 train_time:60677ms step_avg:57.95ms
step:1048/2330 train_time:60737ms step_avg:57.96ms
step:1049/2330 train_time:60794ms step_avg:57.95ms
step:1050/2330 train_time:60854ms step_avg:57.96ms
step:1051/2330 train_time:60911ms step_avg:57.96ms
step:1052/2330 train_time:60971ms step_avg:57.96ms
step:1053/2330 train_time:61028ms step_avg:57.96ms
step:1054/2330 train_time:61088ms step_avg:57.96ms
step:1055/2330 train_time:61145ms step_avg:57.96ms
step:1056/2330 train_time:61205ms step_avg:57.96ms
step:1057/2330 train_time:61262ms step_avg:57.96ms
step:1058/2330 train_time:61323ms step_avg:57.96ms
step:1059/2330 train_time:61379ms step_avg:57.96ms
step:1060/2330 train_time:61440ms step_avg:57.96ms
step:1061/2330 train_time:61496ms step_avg:57.96ms
step:1062/2330 train_time:61556ms step_avg:57.96ms
step:1063/2330 train_time:61613ms step_avg:57.96ms
step:1064/2330 train_time:61672ms step_avg:57.96ms
step:1065/2330 train_time:61729ms step_avg:57.96ms
step:1066/2330 train_time:61788ms step_avg:57.96ms
step:1067/2330 train_time:61845ms step_avg:57.96ms
step:1068/2330 train_time:61905ms step_avg:57.96ms
step:1069/2330 train_time:61963ms step_avg:57.96ms
step:1070/2330 train_time:62023ms step_avg:57.97ms
step:1071/2330 train_time:62080ms step_avg:57.96ms
step:1072/2330 train_time:62141ms step_avg:57.97ms
step:1073/2330 train_time:62198ms step_avg:57.97ms
step:1074/2330 train_time:62259ms step_avg:57.97ms
step:1075/2330 train_time:62315ms step_avg:57.97ms
step:1076/2330 train_time:62376ms step_avg:57.97ms
step:1077/2330 train_time:62432ms step_avg:57.97ms
step:1078/2330 train_time:62492ms step_avg:57.97ms
step:1079/2330 train_time:62549ms step_avg:57.97ms
step:1080/2330 train_time:62608ms step_avg:57.97ms
step:1081/2330 train_time:62664ms step_avg:57.97ms
step:1082/2330 train_time:62724ms step_avg:57.97ms
step:1083/2330 train_time:62781ms step_avg:57.97ms
step:1084/2330 train_time:62841ms step_avg:57.97ms
step:1085/2330 train_time:62899ms step_avg:57.97ms
step:1086/2330 train_time:62958ms step_avg:57.97ms
step:1087/2330 train_time:63015ms step_avg:57.97ms
step:1088/2330 train_time:63076ms step_avg:57.97ms
step:1089/2330 train_time:63134ms step_avg:57.97ms
step:1090/2330 train_time:63194ms step_avg:57.98ms
step:1091/2330 train_time:63251ms step_avg:57.98ms
step:1092/2330 train_time:63311ms step_avg:57.98ms
step:1093/2330 train_time:63368ms step_avg:57.98ms
step:1094/2330 train_time:63428ms step_avg:57.98ms
step:1095/2330 train_time:63484ms step_avg:57.98ms
step:1096/2330 train_time:63545ms step_avg:57.98ms
step:1097/2330 train_time:63602ms step_avg:57.98ms
step:1098/2330 train_time:63662ms step_avg:57.98ms
step:1099/2330 train_time:63718ms step_avg:57.98ms
step:1100/2330 train_time:63778ms step_avg:57.98ms
step:1101/2330 train_time:63835ms step_avg:57.98ms
step:1102/2330 train_time:63894ms step_avg:57.98ms
step:1103/2330 train_time:63951ms step_avg:57.98ms
step:1104/2330 train_time:64012ms step_avg:57.98ms
step:1105/2330 train_time:64070ms step_avg:57.98ms
step:1106/2330 train_time:64130ms step_avg:57.98ms
step:1107/2330 train_time:64188ms step_avg:57.98ms
step:1108/2330 train_time:64247ms step_avg:57.98ms
step:1109/2330 train_time:64305ms step_avg:57.98ms
step:1110/2330 train_time:64365ms step_avg:57.99ms
step:1111/2330 train_time:64421ms step_avg:57.98ms
step:1112/2330 train_time:64481ms step_avg:57.99ms
step:1113/2330 train_time:64538ms step_avg:57.99ms
step:1114/2330 train_time:64598ms step_avg:57.99ms
step:1115/2330 train_time:64654ms step_avg:57.99ms
step:1116/2330 train_time:64715ms step_avg:57.99ms
step:1117/2330 train_time:64771ms step_avg:57.99ms
step:1118/2330 train_time:64831ms step_avg:57.99ms
step:1119/2330 train_time:64888ms step_avg:57.99ms
step:1120/2330 train_time:64947ms step_avg:57.99ms
step:1121/2330 train_time:65005ms step_avg:57.99ms
step:1122/2330 train_time:65066ms step_avg:57.99ms
step:1123/2330 train_time:65122ms step_avg:57.99ms
step:1124/2330 train_time:65183ms step_avg:57.99ms
step:1125/2330 train_time:65240ms step_avg:57.99ms
step:1126/2330 train_time:65301ms step_avg:57.99ms
step:1127/2330 train_time:65358ms step_avg:57.99ms
step:1128/2330 train_time:65417ms step_avg:57.99ms
step:1129/2330 train_time:65474ms step_avg:57.99ms
step:1130/2330 train_time:65534ms step_avg:57.99ms
step:1131/2330 train_time:65591ms step_avg:57.99ms
step:1132/2330 train_time:65651ms step_avg:58.00ms
step:1133/2330 train_time:65708ms step_avg:57.99ms
step:1134/2330 train_time:65767ms step_avg:58.00ms
step:1135/2330 train_time:65824ms step_avg:57.99ms
step:1136/2330 train_time:65885ms step_avg:58.00ms
step:1137/2330 train_time:65942ms step_avg:58.00ms
step:1138/2330 train_time:66002ms step_avg:58.00ms
step:1139/2330 train_time:66058ms step_avg:58.00ms
step:1140/2330 train_time:66120ms step_avg:58.00ms
step:1141/2330 train_time:66176ms step_avg:58.00ms
step:1142/2330 train_time:66238ms step_avg:58.00ms
step:1143/2330 train_time:66296ms step_avg:58.00ms
step:1144/2330 train_time:66356ms step_avg:58.00ms
step:1145/2330 train_time:66413ms step_avg:58.00ms
step:1146/2330 train_time:66472ms step_avg:58.00ms
step:1147/2330 train_time:66529ms step_avg:58.00ms
step:1148/2330 train_time:66589ms step_avg:58.00ms
step:1149/2330 train_time:66646ms step_avg:58.00ms
step:1150/2330 train_time:66706ms step_avg:58.00ms
step:1151/2330 train_time:66762ms step_avg:58.00ms
step:1152/2330 train_time:66822ms step_avg:58.01ms
step:1153/2330 train_time:66879ms step_avg:58.00ms
step:1154/2330 train_time:66939ms step_avg:58.01ms
step:1155/2330 train_time:66996ms step_avg:58.00ms
step:1156/2330 train_time:67055ms step_avg:58.01ms
step:1157/2330 train_time:67112ms step_avg:58.01ms
step:1158/2330 train_time:67173ms step_avg:58.01ms
step:1159/2330 train_time:67230ms step_avg:58.01ms
step:1160/2330 train_time:67290ms step_avg:58.01ms
step:1161/2330 train_time:67347ms step_avg:58.01ms
step:1162/2330 train_time:67407ms step_avg:58.01ms
step:1163/2330 train_time:67464ms step_avg:58.01ms
step:1164/2330 train_time:67523ms step_avg:58.01ms
step:1165/2330 train_time:67580ms step_avg:58.01ms
step:1166/2330 train_time:67640ms step_avg:58.01ms
step:1167/2330 train_time:67696ms step_avg:58.01ms
step:1168/2330 train_time:67758ms step_avg:58.01ms
step:1169/2330 train_time:67814ms step_avg:58.01ms
step:1170/2330 train_time:67875ms step_avg:58.01ms
step:1171/2330 train_time:67931ms step_avg:58.01ms
step:1172/2330 train_time:67991ms step_avg:58.01ms
step:1173/2330 train_time:68049ms step_avg:58.01ms
step:1174/2330 train_time:68109ms step_avg:58.01ms
step:1175/2330 train_time:68166ms step_avg:58.01ms
step:1176/2330 train_time:68225ms step_avg:58.01ms
step:1177/2330 train_time:68282ms step_avg:58.01ms
step:1178/2330 train_time:68343ms step_avg:58.02ms
step:1179/2330 train_time:68399ms step_avg:58.01ms
step:1180/2330 train_time:68461ms step_avg:58.02ms
step:1181/2330 train_time:68517ms step_avg:58.02ms
step:1182/2330 train_time:68577ms step_avg:58.02ms
step:1183/2330 train_time:68634ms step_avg:58.02ms
step:1184/2330 train_time:68694ms step_avg:58.02ms
step:1185/2330 train_time:68750ms step_avg:58.02ms
step:1186/2330 train_time:68811ms step_avg:58.02ms
step:1187/2330 train_time:68868ms step_avg:58.02ms
step:1188/2330 train_time:68928ms step_avg:58.02ms
step:1189/2330 train_time:68985ms step_avg:58.02ms
step:1190/2330 train_time:69044ms step_avg:58.02ms
step:1191/2330 train_time:69100ms step_avg:58.02ms
step:1192/2330 train_time:69161ms step_avg:58.02ms
step:1193/2330 train_time:69218ms step_avg:58.02ms
step:1194/2330 train_time:69279ms step_avg:58.02ms
step:1195/2330 train_time:69335ms step_avg:58.02ms
step:1196/2330 train_time:69396ms step_avg:58.02ms
step:1197/2330 train_time:69452ms step_avg:58.02ms
step:1198/2330 train_time:69513ms step_avg:58.02ms
step:1199/2330 train_time:69570ms step_avg:58.02ms
step:1200/2330 train_time:69630ms step_avg:58.02ms
step:1201/2330 train_time:69687ms step_avg:58.02ms
step:1202/2330 train_time:69746ms step_avg:58.03ms
step:1203/2330 train_time:69803ms step_avg:58.02ms
step:1204/2330 train_time:69863ms step_avg:58.03ms
step:1205/2330 train_time:69919ms step_avg:58.02ms
step:1206/2330 train_time:69981ms step_avg:58.03ms
step:1207/2330 train_time:70037ms step_avg:58.03ms
step:1208/2330 train_time:70098ms step_avg:58.03ms
step:1209/2330 train_time:70154ms step_avg:58.03ms
step:1210/2330 train_time:70216ms step_avg:58.03ms
step:1211/2330 train_time:70272ms step_avg:58.03ms
step:1212/2330 train_time:70332ms step_avg:58.03ms
step:1213/2330 train_time:70390ms step_avg:58.03ms
step:1214/2330 train_time:70449ms step_avg:58.03ms
step:1215/2330 train_time:70505ms step_avg:58.03ms
step:1216/2330 train_time:70565ms step_avg:58.03ms
step:1217/2330 train_time:70621ms step_avg:58.03ms
step:1218/2330 train_time:70682ms step_avg:58.03ms
step:1219/2330 train_time:70738ms step_avg:58.03ms
step:1220/2330 train_time:70799ms step_avg:58.03ms
step:1221/2330 train_time:70856ms step_avg:58.03ms
step:1222/2330 train_time:70917ms step_avg:58.03ms
step:1223/2330 train_time:70974ms step_avg:58.03ms
step:1224/2330 train_time:71035ms step_avg:58.03ms
step:1225/2330 train_time:71092ms step_avg:58.03ms
step:1226/2330 train_time:71151ms step_avg:58.04ms
step:1227/2330 train_time:71209ms step_avg:58.03ms
step:1228/2330 train_time:71269ms step_avg:58.04ms
step:1229/2330 train_time:71325ms step_avg:58.04ms
step:1230/2330 train_time:71385ms step_avg:58.04ms
step:1231/2330 train_time:71441ms step_avg:58.03ms
step:1232/2330 train_time:71502ms step_avg:58.04ms
step:1233/2330 train_time:71559ms step_avg:58.04ms
step:1234/2330 train_time:71619ms step_avg:58.04ms
step:1235/2330 train_time:71676ms step_avg:58.04ms
step:1236/2330 train_time:71736ms step_avg:58.04ms
step:1237/2330 train_time:71793ms step_avg:58.04ms
step:1238/2330 train_time:71854ms step_avg:58.04ms
step:1239/2330 train_time:71911ms step_avg:58.04ms
step:1240/2330 train_time:71972ms step_avg:58.04ms
step:1241/2330 train_time:72029ms step_avg:58.04ms
step:1242/2330 train_time:72088ms step_avg:58.04ms
step:1243/2330 train_time:72145ms step_avg:58.04ms
step:1244/2330 train_time:72205ms step_avg:58.04ms
step:1245/2330 train_time:72261ms step_avg:58.04ms
step:1246/2330 train_time:72322ms step_avg:58.04ms
step:1247/2330 train_time:72379ms step_avg:58.04ms
step:1248/2330 train_time:72439ms step_avg:58.04ms
step:1249/2330 train_time:72496ms step_avg:58.04ms
step:1250/2330 train_time:72557ms step_avg:58.05ms
step:1250/2330 val_loss:4.0106 train_time:72637ms step_avg:58.11ms
step:1251/2330 train_time:72655ms step_avg:58.08ms
step:1252/2330 train_time:72675ms step_avg:58.05ms
step:1253/2330 train_time:72738ms step_avg:58.05ms
step:1254/2330 train_time:72801ms step_avg:58.05ms
step:1255/2330 train_time:72858ms step_avg:58.05ms
step:1256/2330 train_time:72918ms step_avg:58.06ms
step:1257/2330 train_time:72975ms step_avg:58.05ms
step:1258/2330 train_time:73034ms step_avg:58.06ms
step:1259/2330 train_time:73090ms step_avg:58.05ms
step:1260/2330 train_time:73149ms step_avg:58.05ms
step:1261/2330 train_time:73205ms step_avg:58.05ms
step:1262/2330 train_time:73265ms step_avg:58.05ms
step:1263/2330 train_time:73321ms step_avg:58.05ms
step:1264/2330 train_time:73381ms step_avg:58.05ms
step:1265/2330 train_time:73437ms step_avg:58.05ms
step:1266/2330 train_time:73496ms step_avg:58.05ms
step:1267/2330 train_time:73552ms step_avg:58.05ms
step:1268/2330 train_time:73612ms step_avg:58.05ms
step:1269/2330 train_time:73677ms step_avg:58.06ms
step:1270/2330 train_time:73733ms step_avg:58.06ms
step:1271/2330 train_time:73791ms step_avg:58.06ms
step:1272/2330 train_time:73852ms step_avg:58.06ms
step:1273/2330 train_time:73909ms step_avg:58.06ms
step:1274/2330 train_time:73971ms step_avg:58.06ms
step:1275/2330 train_time:74026ms step_avg:58.06ms
step:1276/2330 train_time:74088ms step_avg:58.06ms
step:1277/2330 train_time:74143ms step_avg:58.06ms
step:1278/2330 train_time:74205ms step_avg:58.06ms
step:1279/2330 train_time:74260ms step_avg:58.06ms
step:1280/2330 train_time:74321ms step_avg:58.06ms
step:1281/2330 train_time:74377ms step_avg:58.06ms
step:1282/2330 train_time:74436ms step_avg:58.06ms
step:1283/2330 train_time:74492ms step_avg:58.06ms
step:1284/2330 train_time:74552ms step_avg:58.06ms
step:1285/2330 train_time:74608ms step_avg:58.06ms
step:1286/2330 train_time:74670ms step_avg:58.06ms
step:1287/2330 train_time:74727ms step_avg:58.06ms
step:1288/2330 train_time:74789ms step_avg:58.07ms
step:1289/2330 train_time:74845ms step_avg:58.06ms
step:1290/2330 train_time:74908ms step_avg:58.07ms
step:1291/2330 train_time:74964ms step_avg:58.07ms
step:1292/2330 train_time:75367ms step_avg:58.33ms
step:1293/2330 train_time:75423ms step_avg:58.33ms
step:1294/2330 train_time:75482ms step_avg:58.33ms
step:1295/2330 train_time:75538ms step_avg:58.33ms
step:1296/2330 train_time:75597ms step_avg:58.33ms
step:1297/2330 train_time:75653ms step_avg:58.33ms
step:1298/2330 train_time:75712ms step_avg:58.33ms
step:1299/2330 train_time:75767ms step_avg:58.33ms
step:1300/2330 train_time:75827ms step_avg:58.33ms
step:1301/2330 train_time:75883ms step_avg:58.33ms
step:1302/2330 train_time:75943ms step_avg:58.33ms
step:1303/2330 train_time:75999ms step_avg:58.33ms
step:1304/2330 train_time:76058ms step_avg:58.33ms
step:1305/2330 train_time:76114ms step_avg:58.33ms
step:1306/2330 train_time:76173ms step_avg:58.33ms
step:1307/2330 train_time:76234ms step_avg:58.33ms
step:1308/2330 train_time:76298ms step_avg:58.33ms
step:1309/2330 train_time:76357ms step_avg:58.33ms
step:1310/2330 train_time:76417ms step_avg:58.33ms
step:1311/2330 train_time:76474ms step_avg:58.33ms
step:1312/2330 train_time:76534ms step_avg:58.33ms
step:1313/2330 train_time:76590ms step_avg:58.33ms
step:1314/2330 train_time:76650ms step_avg:58.33ms
step:1315/2330 train_time:76706ms step_avg:58.33ms
step:1316/2330 train_time:76766ms step_avg:58.33ms
step:1317/2330 train_time:76823ms step_avg:58.33ms
step:1318/2330 train_time:76882ms step_avg:58.33ms
step:1319/2330 train_time:76938ms step_avg:58.33ms
step:1320/2330 train_time:76997ms step_avg:58.33ms
step:1321/2330 train_time:77054ms step_avg:58.33ms
step:1322/2330 train_time:77113ms step_avg:58.33ms
step:1323/2330 train_time:77171ms step_avg:58.33ms
step:1324/2330 train_time:77232ms step_avg:58.33ms
step:1325/2330 train_time:77291ms step_avg:58.33ms
step:1326/2330 train_time:77351ms step_avg:58.33ms
step:1327/2330 train_time:77408ms step_avg:58.33ms
step:1328/2330 train_time:77470ms step_avg:58.34ms
step:1329/2330 train_time:77527ms step_avg:58.33ms
step:1330/2330 train_time:77587ms step_avg:58.34ms
step:1331/2330 train_time:77644ms step_avg:58.33ms
step:1332/2330 train_time:77704ms step_avg:58.34ms
step:1333/2330 train_time:77760ms step_avg:58.33ms
step:1334/2330 train_time:77820ms step_avg:58.34ms
step:1335/2330 train_time:77876ms step_avg:58.33ms
step:1336/2330 train_time:77935ms step_avg:58.33ms
step:1337/2330 train_time:77992ms step_avg:58.33ms
step:1338/2330 train_time:78050ms step_avg:58.33ms
step:1339/2330 train_time:78107ms step_avg:58.33ms
step:1340/2330 train_time:78167ms step_avg:58.33ms
step:1341/2330 train_time:78224ms step_avg:58.33ms
step:1342/2330 train_time:78285ms step_avg:58.33ms
step:1343/2330 train_time:78342ms step_avg:58.33ms
step:1344/2330 train_time:78405ms step_avg:58.34ms
step:1345/2330 train_time:78461ms step_avg:58.34ms
step:1346/2330 train_time:78522ms step_avg:58.34ms
step:1347/2330 train_time:78579ms step_avg:58.34ms
step:1348/2330 train_time:78638ms step_avg:58.34ms
step:1349/2330 train_time:78695ms step_avg:58.34ms
step:1350/2330 train_time:78755ms step_avg:58.34ms
step:1351/2330 train_time:78812ms step_avg:58.34ms
step:1352/2330 train_time:78871ms step_avg:58.34ms
step:1353/2330 train_time:78927ms step_avg:58.33ms
step:1354/2330 train_time:78987ms step_avg:58.34ms
step:1355/2330 train_time:79043ms step_avg:58.33ms
step:1356/2330 train_time:79103ms step_avg:58.34ms
step:1357/2330 train_time:79161ms step_avg:58.34ms
step:1358/2330 train_time:79221ms step_avg:58.34ms
step:1359/2330 train_time:79277ms step_avg:58.33ms
step:1360/2330 train_time:79338ms step_avg:58.34ms
step:1361/2330 train_time:79396ms step_avg:58.34ms
step:1362/2330 train_time:79456ms step_avg:58.34ms
step:1363/2330 train_time:79515ms step_avg:58.34ms
step:1364/2330 train_time:79574ms step_avg:58.34ms
step:1365/2330 train_time:79632ms step_avg:58.34ms
step:1366/2330 train_time:79691ms step_avg:58.34ms
step:1367/2330 train_time:79748ms step_avg:58.34ms
step:1368/2330 train_time:79808ms step_avg:58.34ms
step:1369/2330 train_time:79864ms step_avg:58.34ms
step:1370/2330 train_time:79924ms step_avg:58.34ms
step:1371/2330 train_time:79981ms step_avg:58.34ms
step:1372/2330 train_time:80041ms step_avg:58.34ms
step:1373/2330 train_time:80098ms step_avg:58.34ms
step:1374/2330 train_time:80158ms step_avg:58.34ms
step:1375/2330 train_time:80216ms step_avg:58.34ms
step:1376/2330 train_time:80276ms step_avg:58.34ms
step:1377/2330 train_time:80335ms step_avg:58.34ms
step:1378/2330 train_time:80394ms step_avg:58.34ms
step:1379/2330 train_time:80452ms step_avg:58.34ms
step:1380/2330 train_time:80512ms step_avg:58.34ms
step:1381/2330 train_time:80570ms step_avg:58.34ms
step:1382/2330 train_time:80630ms step_avg:58.34ms
step:1383/2330 train_time:80687ms step_avg:58.34ms
step:1384/2330 train_time:80746ms step_avg:58.34ms
step:1385/2330 train_time:80803ms step_avg:58.34ms
step:1386/2330 train_time:80862ms step_avg:58.34ms
step:1387/2330 train_time:80919ms step_avg:58.34ms
step:1388/2330 train_time:80978ms step_avg:58.34ms
step:1389/2330 train_time:81035ms step_avg:58.34ms
step:1390/2330 train_time:81095ms step_avg:58.34ms
step:1391/2330 train_time:81152ms step_avg:58.34ms
step:1392/2330 train_time:81212ms step_avg:58.34ms
step:1393/2330 train_time:81269ms step_avg:58.34ms
step:1394/2330 train_time:81329ms step_avg:58.34ms
step:1395/2330 train_time:81385ms step_avg:58.34ms
step:1396/2330 train_time:81446ms step_avg:58.34ms
step:1397/2330 train_time:81502ms step_avg:58.34ms
step:1398/2330 train_time:81564ms step_avg:58.34ms
step:1399/2330 train_time:81621ms step_avg:58.34ms
step:1400/2330 train_time:81680ms step_avg:58.34ms
step:1401/2330 train_time:81738ms step_avg:58.34ms
step:1402/2330 train_time:81797ms step_avg:58.34ms
step:1403/2330 train_time:81854ms step_avg:58.34ms
step:1404/2330 train_time:81914ms step_avg:58.34ms
step:1405/2330 train_time:81971ms step_avg:58.34ms
step:1406/2330 train_time:82030ms step_avg:58.34ms
step:1407/2330 train_time:82087ms step_avg:58.34ms
step:1408/2330 train_time:82147ms step_avg:58.34ms
step:1409/2330 train_time:82204ms step_avg:58.34ms
step:1410/2330 train_time:82265ms step_avg:58.34ms
step:1411/2330 train_time:82322ms step_avg:58.34ms
step:1412/2330 train_time:82383ms step_avg:58.34ms
step:1413/2330 train_time:82439ms step_avg:58.34ms
step:1414/2330 train_time:82499ms step_avg:58.34ms
step:1415/2330 train_time:82557ms step_avg:58.34ms
step:1416/2330 train_time:82617ms step_avg:58.35ms
step:1417/2330 train_time:82674ms step_avg:58.34ms
step:1418/2330 train_time:82734ms step_avg:58.35ms
step:1419/2330 train_time:82792ms step_avg:58.35ms
step:1420/2330 train_time:82851ms step_avg:58.35ms
step:1421/2330 train_time:82908ms step_avg:58.34ms
step:1422/2330 train_time:82968ms step_avg:58.35ms
step:1423/2330 train_time:83025ms step_avg:58.35ms
step:1424/2330 train_time:83085ms step_avg:58.35ms
step:1425/2330 train_time:83141ms step_avg:58.34ms
step:1426/2330 train_time:83202ms step_avg:58.35ms
step:1427/2330 train_time:83258ms step_avg:58.34ms
step:1428/2330 train_time:83319ms step_avg:58.35ms
step:1429/2330 train_time:83377ms step_avg:58.35ms
step:1430/2330 train_time:83436ms step_avg:58.35ms
step:1431/2330 train_time:83494ms step_avg:58.35ms
step:1432/2330 train_time:83554ms step_avg:58.35ms
step:1433/2330 train_time:83611ms step_avg:58.35ms
step:1434/2330 train_time:83671ms step_avg:58.35ms
step:1435/2330 train_time:83728ms step_avg:58.35ms
step:1436/2330 train_time:83789ms step_avg:58.35ms
step:1437/2330 train_time:83845ms step_avg:58.35ms
step:1438/2330 train_time:83905ms step_avg:58.35ms
step:1439/2330 train_time:83962ms step_avg:58.35ms
step:1440/2330 train_time:84022ms step_avg:58.35ms
step:1441/2330 train_time:84079ms step_avg:58.35ms
step:1442/2330 train_time:84139ms step_avg:58.35ms
step:1443/2330 train_time:84196ms step_avg:58.35ms
step:1444/2330 train_time:84255ms step_avg:58.35ms
step:1445/2330 train_time:84312ms step_avg:58.35ms
step:1446/2330 train_time:84371ms step_avg:58.35ms
step:1447/2330 train_time:84427ms step_avg:58.35ms
step:1448/2330 train_time:84488ms step_avg:58.35ms
step:1449/2330 train_time:84544ms step_avg:58.35ms
step:1450/2330 train_time:84605ms step_avg:58.35ms
step:1451/2330 train_time:84661ms step_avg:58.35ms
step:1452/2330 train_time:84722ms step_avg:58.35ms
step:1453/2330 train_time:84779ms step_avg:58.35ms
step:1454/2330 train_time:84839ms step_avg:58.35ms
step:1455/2330 train_time:84897ms step_avg:58.35ms
step:1456/2330 train_time:84957ms step_avg:58.35ms
step:1457/2330 train_time:85015ms step_avg:58.35ms
step:1458/2330 train_time:85075ms step_avg:58.35ms
step:1459/2330 train_time:85132ms step_avg:58.35ms
step:1460/2330 train_time:85192ms step_avg:58.35ms
step:1461/2330 train_time:85249ms step_avg:58.35ms
step:1462/2330 train_time:85308ms step_avg:58.35ms
step:1463/2330 train_time:85366ms step_avg:58.35ms
step:1464/2330 train_time:85426ms step_avg:58.35ms
step:1465/2330 train_time:85483ms step_avg:58.35ms
step:1466/2330 train_time:85543ms step_avg:58.35ms
step:1467/2330 train_time:85600ms step_avg:58.35ms
step:1468/2330 train_time:85660ms step_avg:58.35ms
step:1469/2330 train_time:85717ms step_avg:58.35ms
step:1470/2330 train_time:85777ms step_avg:58.35ms
step:1471/2330 train_time:85834ms step_avg:58.35ms
step:1472/2330 train_time:85894ms step_avg:58.35ms
step:1473/2330 train_time:85951ms step_avg:58.35ms
step:1474/2330 train_time:86010ms step_avg:58.35ms
step:1475/2330 train_time:86067ms step_avg:58.35ms
step:1476/2330 train_time:86127ms step_avg:58.35ms
step:1477/2330 train_time:86184ms step_avg:58.35ms
step:1478/2330 train_time:86245ms step_avg:58.35ms
step:1479/2330 train_time:86301ms step_avg:58.35ms
step:1480/2330 train_time:86362ms step_avg:58.35ms
step:1481/2330 train_time:86419ms step_avg:58.35ms
step:1482/2330 train_time:86479ms step_avg:58.35ms
step:1483/2330 train_time:86535ms step_avg:58.35ms
step:1484/2330 train_time:86595ms step_avg:58.35ms
step:1485/2330 train_time:86652ms step_avg:58.35ms
step:1486/2330 train_time:86712ms step_avg:58.35ms
step:1487/2330 train_time:86768ms step_avg:58.35ms
step:1488/2330 train_time:86829ms step_avg:58.35ms
step:1489/2330 train_time:86886ms step_avg:58.35ms
step:1490/2330 train_time:86946ms step_avg:58.35ms
step:1491/2330 train_time:87003ms step_avg:58.35ms
step:1492/2330 train_time:87064ms step_avg:58.35ms
step:1493/2330 train_time:87122ms step_avg:58.35ms
step:1494/2330 train_time:87181ms step_avg:58.35ms
step:1495/2330 train_time:87238ms step_avg:58.35ms
step:1496/2330 train_time:87298ms step_avg:58.35ms
step:1497/2330 train_time:87355ms step_avg:58.35ms
step:1498/2330 train_time:87415ms step_avg:58.35ms
step:1499/2330 train_time:87473ms step_avg:58.35ms
step:1500/2330 train_time:87532ms step_avg:58.35ms
step:1500/2330 val_loss:3.9253 train_time:87612ms step_avg:58.41ms
step:1501/2330 train_time:87634ms step_avg:58.38ms
step:1502/2330 train_time:87654ms step_avg:58.36ms
step:1503/2330 train_time:87709ms step_avg:58.36ms
step:1504/2330 train_time:87777ms step_avg:58.36ms
step:1505/2330 train_time:87835ms step_avg:58.36ms
step:1506/2330 train_time:87896ms step_avg:58.36ms
step:1507/2330 train_time:87953ms step_avg:58.36ms
step:1508/2330 train_time:88013ms step_avg:58.36ms
step:1509/2330 train_time:88069ms step_avg:58.36ms
step:1510/2330 train_time:88130ms step_avg:58.36ms
step:1511/2330 train_time:88186ms step_avg:58.36ms
step:1512/2330 train_time:88246ms step_avg:58.36ms
step:1513/2330 train_time:88302ms step_avg:58.36ms
step:1514/2330 train_time:88362ms step_avg:58.36ms
step:1515/2330 train_time:88417ms step_avg:58.36ms
step:1516/2330 train_time:88477ms step_avg:58.36ms
step:1517/2330 train_time:88534ms step_avg:58.36ms
step:1518/2330 train_time:88596ms step_avg:58.36ms
step:1519/2330 train_time:88653ms step_avg:58.36ms
step:1520/2330 train_time:88716ms step_avg:58.37ms
step:1521/2330 train_time:88773ms step_avg:58.37ms
step:1522/2330 train_time:88836ms step_avg:58.37ms
step:1523/2330 train_time:88893ms step_avg:58.37ms
step:1524/2330 train_time:88954ms step_avg:58.37ms
step:1525/2330 train_time:89010ms step_avg:58.37ms
step:1526/2330 train_time:89071ms step_avg:58.37ms
step:1527/2330 train_time:89127ms step_avg:58.37ms
step:1528/2330 train_time:89187ms step_avg:58.37ms
step:1529/2330 train_time:89245ms step_avg:58.37ms
step:1530/2330 train_time:89303ms step_avg:58.37ms
step:1531/2330 train_time:89359ms step_avg:58.37ms
step:1532/2330 train_time:89419ms step_avg:58.37ms
step:1533/2330 train_time:89476ms step_avg:58.37ms
step:1534/2330 train_time:89536ms step_avg:58.37ms
step:1535/2330 train_time:89593ms step_avg:58.37ms
step:1536/2330 train_time:89654ms step_avg:58.37ms
step:1537/2330 train_time:89712ms step_avg:58.37ms
step:1538/2330 train_time:89772ms step_avg:58.37ms
step:1539/2330 train_time:89830ms step_avg:58.37ms
step:1540/2330 train_time:89892ms step_avg:58.37ms
step:1541/2330 train_time:89950ms step_avg:58.37ms
step:1542/2330 train_time:90011ms step_avg:58.37ms
step:1543/2330 train_time:90067ms step_avg:58.37ms
step:1544/2330 train_time:90129ms step_avg:58.37ms
step:1545/2330 train_time:90186ms step_avg:58.37ms
step:1546/2330 train_time:90247ms step_avg:58.37ms
step:1547/2330 train_time:90304ms step_avg:58.37ms
step:1548/2330 train_time:90363ms step_avg:58.37ms
step:1549/2330 train_time:90420ms step_avg:58.37ms
step:1550/2330 train_time:90480ms step_avg:58.37ms
step:1551/2330 train_time:90538ms step_avg:58.37ms
step:1552/2330 train_time:90598ms step_avg:58.37ms
step:1553/2330 train_time:90656ms step_avg:58.37ms
step:1554/2330 train_time:90716ms step_avg:58.38ms
step:1555/2330 train_time:90773ms step_avg:58.38ms
step:1556/2330 train_time:90835ms step_avg:58.38ms
step:1557/2330 train_time:90893ms step_avg:58.38ms
step:1558/2330 train_time:90954ms step_avg:58.38ms
step:1559/2330 train_time:91011ms step_avg:58.38ms
step:1560/2330 train_time:91073ms step_avg:58.38ms
step:1561/2330 train_time:91130ms step_avg:58.38ms
step:1562/2330 train_time:91193ms step_avg:58.38ms
step:1563/2330 train_time:91249ms step_avg:58.38ms
step:1564/2330 train_time:91310ms step_avg:58.38ms
step:1565/2330 train_time:91367ms step_avg:58.38ms
step:1566/2330 train_time:91428ms step_avg:58.38ms
step:1567/2330 train_time:91485ms step_avg:58.38ms
step:1568/2330 train_time:91545ms step_avg:58.38ms
step:1569/2330 train_time:91603ms step_avg:58.38ms
step:1570/2330 train_time:91663ms step_avg:58.38ms
step:1571/2330 train_time:91721ms step_avg:58.38ms
step:1572/2330 train_time:91781ms step_avg:58.39ms
step:1573/2330 train_time:91839ms step_avg:58.38ms
step:1574/2330 train_time:91899ms step_avg:58.39ms
step:1575/2330 train_time:91958ms step_avg:58.39ms
step:1576/2330 train_time:92018ms step_avg:58.39ms
step:1577/2330 train_time:92075ms step_avg:58.39ms
step:1578/2330 train_time:92138ms step_avg:58.39ms
step:1579/2330 train_time:92195ms step_avg:58.39ms
step:1580/2330 train_time:92256ms step_avg:58.39ms
step:1581/2330 train_time:92312ms step_avg:58.39ms
step:1582/2330 train_time:92374ms step_avg:58.39ms
step:1583/2330 train_time:92430ms step_avg:58.39ms
step:1584/2330 train_time:92493ms step_avg:58.39ms
step:1585/2330 train_time:92550ms step_avg:58.39ms
step:1586/2330 train_time:92610ms step_avg:58.39ms
step:1587/2330 train_time:92666ms step_avg:58.39ms
step:1588/2330 train_time:92729ms step_avg:58.39ms
step:1589/2330 train_time:92786ms step_avg:58.39ms
step:1590/2330 train_time:92846ms step_avg:58.39ms
step:1591/2330 train_time:92905ms step_avg:58.39ms
step:1592/2330 train_time:92965ms step_avg:58.40ms
step:1593/2330 train_time:93025ms step_avg:58.40ms
step:1594/2330 train_time:93085ms step_avg:58.40ms
step:1595/2330 train_time:93144ms step_avg:58.40ms
step:1596/2330 train_time:93204ms step_avg:58.40ms
step:1597/2330 train_time:93261ms step_avg:58.40ms
step:1598/2330 train_time:93320ms step_avg:58.40ms
step:1599/2330 train_time:93377ms step_avg:58.40ms
step:1600/2330 train_time:93439ms step_avg:58.40ms
step:1601/2330 train_time:93495ms step_avg:58.40ms
step:1602/2330 train_time:93556ms step_avg:58.40ms
step:1603/2330 train_time:93612ms step_avg:58.40ms
step:1604/2330 train_time:93674ms step_avg:58.40ms
step:1605/2330 train_time:93730ms step_avg:58.40ms
step:1606/2330 train_time:93792ms step_avg:58.40ms
step:1607/2330 train_time:93849ms step_avg:58.40ms
step:1608/2330 train_time:93910ms step_avg:58.40ms
step:1609/2330 train_time:93968ms step_avg:58.40ms
step:1610/2330 train_time:94030ms step_avg:58.40ms
step:1611/2330 train_time:94087ms step_avg:58.40ms
step:1612/2330 train_time:94148ms step_avg:58.40ms
step:1613/2330 train_time:94206ms step_avg:58.40ms
step:1614/2330 train_time:94266ms step_avg:58.41ms
step:1615/2330 train_time:94324ms step_avg:58.41ms
step:1616/2330 train_time:94384ms step_avg:58.41ms
step:1617/2330 train_time:94442ms step_avg:58.41ms
step:1618/2330 train_time:94502ms step_avg:58.41ms
step:1619/2330 train_time:94558ms step_avg:58.41ms
step:1620/2330 train_time:94619ms step_avg:58.41ms
step:1621/2330 train_time:94676ms step_avg:58.41ms
step:1622/2330 train_time:94737ms step_avg:58.41ms
step:1623/2330 train_time:94794ms step_avg:58.41ms
step:1624/2330 train_time:94855ms step_avg:58.41ms
step:1625/2330 train_time:94912ms step_avg:58.41ms
step:1626/2330 train_time:94974ms step_avg:58.41ms
step:1627/2330 train_time:95030ms step_avg:58.41ms
step:1628/2330 train_time:95093ms step_avg:58.41ms
step:1629/2330 train_time:95150ms step_avg:58.41ms
step:1630/2330 train_time:95211ms step_avg:58.41ms
step:1631/2330 train_time:95268ms step_avg:58.41ms
step:1632/2330 train_time:95329ms step_avg:58.41ms
step:1633/2330 train_time:95387ms step_avg:58.41ms
step:1634/2330 train_time:95448ms step_avg:58.41ms
step:1635/2330 train_time:95506ms step_avg:58.41ms
step:1636/2330 train_time:95566ms step_avg:58.41ms
step:1637/2330 train_time:95624ms step_avg:58.41ms
step:1638/2330 train_time:95684ms step_avg:58.42ms
step:1639/2330 train_time:95741ms step_avg:58.41ms
step:1640/2330 train_time:95801ms step_avg:58.42ms
step:1641/2330 train_time:95858ms step_avg:58.41ms
step:1642/2330 train_time:95919ms step_avg:58.42ms
step:1643/2330 train_time:95976ms step_avg:58.41ms
step:1644/2330 train_time:96037ms step_avg:58.42ms
step:1645/2330 train_time:96093ms step_avg:58.42ms
step:1646/2330 train_time:96156ms step_avg:58.42ms
step:1647/2330 train_time:96212ms step_avg:58.42ms
step:1648/2330 train_time:96276ms step_avg:58.42ms
step:1649/2330 train_time:96332ms step_avg:58.42ms
step:1650/2330 train_time:96394ms step_avg:58.42ms
step:1651/2330 train_time:96450ms step_avg:58.42ms
step:1652/2330 train_time:96513ms step_avg:58.42ms
step:1653/2330 train_time:96570ms step_avg:58.42ms
step:1654/2330 train_time:96631ms step_avg:58.42ms
step:1655/2330 train_time:96688ms step_avg:58.42ms
step:1656/2330 train_time:96748ms step_avg:58.42ms
step:1657/2330 train_time:96807ms step_avg:58.42ms
step:1658/2330 train_time:96867ms step_avg:58.42ms
step:1659/2330 train_time:96925ms step_avg:58.42ms
step:1660/2330 train_time:96984ms step_avg:58.42ms
step:1661/2330 train_time:97043ms step_avg:58.42ms
step:1662/2330 train_time:97103ms step_avg:58.43ms
step:1663/2330 train_time:97161ms step_avg:58.42ms
step:1664/2330 train_time:97221ms step_avg:58.43ms
step:1665/2330 train_time:97277ms step_avg:58.42ms
step:1666/2330 train_time:97340ms step_avg:58.43ms
step:1667/2330 train_time:97397ms step_avg:58.43ms
step:1668/2330 train_time:97457ms step_avg:58.43ms
step:1669/2330 train_time:97514ms step_avg:58.43ms
step:1670/2330 train_time:97576ms step_avg:58.43ms
step:1671/2330 train_time:97633ms step_avg:58.43ms
step:1672/2330 train_time:97695ms step_avg:58.43ms
step:1673/2330 train_time:97752ms step_avg:58.43ms
step:1674/2330 train_time:97813ms step_avg:58.43ms
step:1675/2330 train_time:97869ms step_avg:58.43ms
step:1676/2330 train_time:97931ms step_avg:58.43ms
step:1677/2330 train_time:97988ms step_avg:58.43ms
step:1678/2330 train_time:98050ms step_avg:58.43ms
step:1679/2330 train_time:98107ms step_avg:58.43ms
step:1680/2330 train_time:98168ms step_avg:58.43ms
step:1681/2330 train_time:98227ms step_avg:58.43ms
step:1682/2330 train_time:98287ms step_avg:58.43ms
step:1683/2330 train_time:98345ms step_avg:58.43ms
step:1684/2330 train_time:98406ms step_avg:58.44ms
step:1685/2330 train_time:98464ms step_avg:58.44ms
step:1686/2330 train_time:98524ms step_avg:58.44ms
step:1687/2330 train_time:98581ms step_avg:58.44ms
step:1688/2330 train_time:98641ms step_avg:58.44ms
step:1689/2330 train_time:98699ms step_avg:58.44ms
step:1690/2330 train_time:98759ms step_avg:58.44ms
step:1691/2330 train_time:98816ms step_avg:58.44ms
step:1692/2330 train_time:98877ms step_avg:58.44ms
step:1693/2330 train_time:98934ms step_avg:58.44ms
step:1694/2330 train_time:98996ms step_avg:58.44ms
step:1695/2330 train_time:99052ms step_avg:58.44ms
step:1696/2330 train_time:99114ms step_avg:58.44ms
step:1697/2330 train_time:99170ms step_avg:58.44ms
step:1698/2330 train_time:99233ms step_avg:58.44ms
step:1699/2330 train_time:99290ms step_avg:58.44ms
step:1700/2330 train_time:99352ms step_avg:58.44ms
step:1701/2330 train_time:99408ms step_avg:58.44ms
step:1702/2330 train_time:99471ms step_avg:58.44ms
step:1703/2330 train_time:99527ms step_avg:58.44ms
step:1704/2330 train_time:99588ms step_avg:58.44ms
step:1705/2330 train_time:99646ms step_avg:58.44ms
step:1706/2330 train_time:99706ms step_avg:58.44ms
step:1707/2330 train_time:99763ms step_avg:58.44ms
step:1708/2330 train_time:99823ms step_avg:58.44ms
step:1709/2330 train_time:99882ms step_avg:58.44ms
step:1710/2330 train_time:99941ms step_avg:58.45ms
step:1711/2330 train_time:100000ms step_avg:58.45ms
step:1712/2330 train_time:100060ms step_avg:58.45ms
step:1713/2330 train_time:100117ms step_avg:58.45ms
step:1714/2330 train_time:100177ms step_avg:58.45ms
step:1715/2330 train_time:100234ms step_avg:58.45ms
step:1716/2330 train_time:100297ms step_avg:58.45ms
step:1717/2330 train_time:100354ms step_avg:58.45ms
step:1718/2330 train_time:100416ms step_avg:58.45ms
step:1719/2330 train_time:100472ms step_avg:58.45ms
step:1720/2330 train_time:100535ms step_avg:58.45ms
step:1721/2330 train_time:100591ms step_avg:58.45ms
step:1722/2330 train_time:100653ms step_avg:58.45ms
step:1723/2330 train_time:100709ms step_avg:58.45ms
step:1724/2330 train_time:100772ms step_avg:58.45ms
step:1725/2330 train_time:100828ms step_avg:58.45ms
step:1726/2330 train_time:100889ms step_avg:58.45ms
step:1727/2330 train_time:100947ms step_avg:58.45ms
step:1728/2330 train_time:101008ms step_avg:58.45ms
step:1729/2330 train_time:101066ms step_avg:58.45ms
step:1730/2330 train_time:101126ms step_avg:58.45ms
step:1731/2330 train_time:101183ms step_avg:58.45ms
step:1732/2330 train_time:101244ms step_avg:58.45ms
step:1733/2330 train_time:101301ms step_avg:58.45ms
step:1734/2330 train_time:101361ms step_avg:58.46ms
step:1735/2330 train_time:101419ms step_avg:58.45ms
step:1736/2330 train_time:101479ms step_avg:58.46ms
step:1737/2330 train_time:101537ms step_avg:58.46ms
step:1738/2330 train_time:101597ms step_avg:58.46ms
step:1739/2330 train_time:101653ms step_avg:58.45ms
step:1740/2330 train_time:101715ms step_avg:58.46ms
step:1741/2330 train_time:101772ms step_avg:58.46ms
step:1742/2330 train_time:101834ms step_avg:58.46ms
step:1743/2330 train_time:101890ms step_avg:58.46ms
step:1744/2330 train_time:101953ms step_avg:58.46ms
step:1745/2330 train_time:102010ms step_avg:58.46ms
step:1746/2330 train_time:102073ms step_avg:58.46ms
step:1747/2330 train_time:102130ms step_avg:58.46ms
step:1748/2330 train_time:102191ms step_avg:58.46ms
step:1749/2330 train_time:102249ms step_avg:58.46ms
step:1750/2330 train_time:102308ms step_avg:58.46ms
step:1750/2330 val_loss:3.8408 train_time:102389ms step_avg:58.51ms
step:1751/2330 train_time:102409ms step_avg:58.49ms
step:1752/2330 train_time:102430ms step_avg:58.46ms
step:1753/2330 train_time:102492ms step_avg:58.47ms
step:1754/2330 train_time:102557ms step_avg:58.47ms
step:1755/2330 train_time:102617ms step_avg:58.47ms
step:1756/2330 train_time:102677ms step_avg:58.47ms
step:1757/2330 train_time:102735ms step_avg:58.47ms
step:1758/2330 train_time:102794ms step_avg:58.47ms
step:1759/2330 train_time:102851ms step_avg:58.47ms
step:1760/2330 train_time:102910ms step_avg:58.47ms
step:1761/2330 train_time:102967ms step_avg:58.47ms
step:1762/2330 train_time:103026ms step_avg:58.47ms
step:1763/2330 train_time:103083ms step_avg:58.47ms
step:1764/2330 train_time:103143ms step_avg:58.47ms
step:1765/2330 train_time:103199ms step_avg:58.47ms
step:1766/2330 train_time:103259ms step_avg:58.47ms
step:1767/2330 train_time:103315ms step_avg:58.47ms
step:1768/2330 train_time:103378ms step_avg:58.47ms
step:1769/2330 train_time:103437ms step_avg:58.47ms
step:1770/2330 train_time:103499ms step_avg:58.47ms
step:1771/2330 train_time:103558ms step_avg:58.47ms
step:1772/2330 train_time:103618ms step_avg:58.48ms
step:1773/2330 train_time:103677ms step_avg:58.48ms
step:1774/2330 train_time:103737ms step_avg:58.48ms
step:1775/2330 train_time:103795ms step_avg:58.48ms
step:1776/2330 train_time:103854ms step_avg:58.48ms
step:1777/2330 train_time:103911ms step_avg:58.48ms
step:1778/2330 train_time:103970ms step_avg:58.48ms
step:1779/2330 train_time:104028ms step_avg:58.48ms
step:1780/2330 train_time:104088ms step_avg:58.48ms
step:1781/2330 train_time:104144ms step_avg:58.48ms
step:1782/2330 train_time:104204ms step_avg:58.48ms
step:1783/2330 train_time:104260ms step_avg:58.47ms
step:1784/2330 train_time:104323ms step_avg:58.48ms
step:1785/2330 train_time:104381ms step_avg:58.48ms
step:1786/2330 train_time:104444ms step_avg:58.48ms
step:1787/2330 train_time:104501ms step_avg:58.48ms
step:1788/2330 train_time:104566ms step_avg:58.48ms
step:1789/2330 train_time:104624ms step_avg:58.48ms
step:1790/2330 train_time:104686ms step_avg:58.48ms
step:1791/2330 train_time:104743ms step_avg:58.48ms
step:1792/2330 train_time:104804ms step_avg:58.48ms
step:1793/2330 train_time:104861ms step_avg:58.48ms
step:1794/2330 train_time:104922ms step_avg:58.48ms
step:1795/2330 train_time:104978ms step_avg:58.48ms
step:1796/2330 train_time:105038ms step_avg:58.48ms
step:1797/2330 train_time:105094ms step_avg:58.48ms
step:1798/2330 train_time:105154ms step_avg:58.48ms
step:1799/2330 train_time:105212ms step_avg:58.48ms
step:1800/2330 train_time:105271ms step_avg:58.48ms
step:1801/2330 train_time:105330ms step_avg:58.48ms
step:1802/2330 train_time:105390ms step_avg:58.49ms
step:1803/2330 train_time:105448ms step_avg:58.48ms
step:1804/2330 train_time:105510ms step_avg:58.49ms
step:1805/2330 train_time:105567ms step_avg:58.49ms
step:1806/2330 train_time:105630ms step_avg:58.49ms
step:1807/2330 train_time:105688ms step_avg:58.49ms
step:1808/2330 train_time:105748ms step_avg:58.49ms
step:1809/2330 train_time:105805ms step_avg:58.49ms
step:1810/2330 train_time:105866ms step_avg:58.49ms
step:1811/2330 train_time:105923ms step_avg:58.49ms
step:1812/2330 train_time:105984ms step_avg:58.49ms
step:1813/2330 train_time:106040ms step_avg:58.49ms
step:1814/2330 train_time:106101ms step_avg:58.49ms
step:1815/2330 train_time:106158ms step_avg:58.49ms
step:1816/2330 train_time:106218ms step_avg:58.49ms
step:1817/2330 train_time:106275ms step_avg:58.49ms
step:1818/2330 train_time:106335ms step_avg:58.49ms
step:1819/2330 train_time:106393ms step_avg:58.49ms
step:1820/2330 train_time:106453ms step_avg:58.49ms
step:1821/2330 train_time:106512ms step_avg:58.49ms
step:1822/2330 train_time:106572ms step_avg:58.49ms
step:1823/2330 train_time:106631ms step_avg:58.49ms
step:1824/2330 train_time:106691ms step_avg:58.49ms
step:1825/2330 train_time:106749ms step_avg:58.49ms
step:1826/2330 train_time:106808ms step_avg:58.49ms
step:1827/2330 train_time:106865ms step_avg:58.49ms
step:1828/2330 train_time:106927ms step_avg:58.49ms
step:1829/2330 train_time:106984ms step_avg:58.49ms
step:1830/2330 train_time:107045ms step_avg:58.49ms
step:1831/2330 train_time:107101ms step_avg:58.49ms
step:1832/2330 train_time:107163ms step_avg:58.49ms
step:1833/2330 train_time:107219ms step_avg:58.49ms
step:1834/2330 train_time:107281ms step_avg:58.50ms
step:1835/2330 train_time:107338ms step_avg:58.49ms
step:1836/2330 train_time:107399ms step_avg:58.50ms
step:1837/2330 train_time:107458ms step_avg:58.50ms
step:1838/2330 train_time:107518ms step_avg:58.50ms
step:1839/2330 train_time:107576ms step_avg:58.50ms
step:1840/2330 train_time:107636ms step_avg:58.50ms
step:1841/2330 train_time:107695ms step_avg:58.50ms
step:1842/2330 train_time:107756ms step_avg:58.50ms
step:1843/2330 train_time:107814ms step_avg:58.50ms
step:1844/2330 train_time:107875ms step_avg:58.50ms
step:1845/2330 train_time:107933ms step_avg:58.50ms
step:1846/2330 train_time:107994ms step_avg:58.50ms
step:1847/2330 train_time:108050ms step_avg:58.50ms
step:1848/2330 train_time:108110ms step_avg:58.50ms
step:1849/2330 train_time:108167ms step_avg:58.50ms
step:1850/2330 train_time:108229ms step_avg:58.50ms
step:1851/2330 train_time:108285ms step_avg:58.50ms
step:1852/2330 train_time:108347ms step_avg:58.50ms
step:1853/2330 train_time:108403ms step_avg:58.50ms
step:1854/2330 train_time:108466ms step_avg:58.50ms
step:1855/2330 train_time:108522ms step_avg:58.50ms
step:1856/2330 train_time:108585ms step_avg:58.50ms
step:1857/2330 train_time:108643ms step_avg:58.50ms
step:1858/2330 train_time:108705ms step_avg:58.51ms
step:1859/2330 train_time:108761ms step_avg:58.51ms
step:1860/2330 train_time:108823ms step_avg:58.51ms
step:1861/2330 train_time:108880ms step_avg:58.51ms
step:1862/2330 train_time:108941ms step_avg:58.51ms
step:1863/2330 train_time:108998ms step_avg:58.51ms
step:1864/2330 train_time:109059ms step_avg:58.51ms
step:1865/2330 train_time:109117ms step_avg:58.51ms
step:1866/2330 train_time:109177ms step_avg:58.51ms
step:1867/2330 train_time:109235ms step_avg:58.51ms
step:1868/2330 train_time:109295ms step_avg:58.51ms
step:1869/2330 train_time:109353ms step_avg:58.51ms
step:1870/2330 train_time:109413ms step_avg:58.51ms
step:1871/2330 train_time:109470ms step_avg:58.51ms
step:1872/2330 train_time:109530ms step_avg:58.51ms
step:1873/2330 train_time:109588ms step_avg:58.51ms
step:1874/2330 train_time:109649ms step_avg:58.51ms
step:1875/2330 train_time:109705ms step_avg:58.51ms
step:1876/2330 train_time:109767ms step_avg:58.51ms
step:1877/2330 train_time:109824ms step_avg:58.51ms
step:1878/2330 train_time:109886ms step_avg:58.51ms
step:1879/2330 train_time:109942ms step_avg:58.51ms
step:1880/2330 train_time:110005ms step_avg:58.51ms
step:1881/2330 train_time:110061ms step_avg:58.51ms
step:1882/2330 train_time:110123ms step_avg:58.51ms
step:1883/2330 train_time:110180ms step_avg:58.51ms
step:1884/2330 train_time:110242ms step_avg:58.51ms
step:1885/2330 train_time:110299ms step_avg:58.51ms
step:1886/2330 train_time:110359ms step_avg:58.52ms
step:1887/2330 train_time:110417ms step_avg:58.51ms
step:1888/2330 train_time:110477ms step_avg:58.52ms
step:1889/2330 train_time:110536ms step_avg:58.52ms
step:1890/2330 train_time:110596ms step_avg:58.52ms
step:1891/2330 train_time:110655ms step_avg:58.52ms
step:1892/2330 train_time:110715ms step_avg:58.52ms
step:1893/2330 train_time:110774ms step_avg:58.52ms
step:1894/2330 train_time:110833ms step_avg:58.52ms
step:1895/2330 train_time:110891ms step_avg:58.52ms
step:1896/2330 train_time:110951ms step_avg:58.52ms
step:1897/2330 train_time:111008ms step_avg:58.52ms
step:1898/2330 train_time:111068ms step_avg:58.52ms
step:1899/2330 train_time:111126ms step_avg:58.52ms
step:1900/2330 train_time:111187ms step_avg:58.52ms
step:1901/2330 train_time:111243ms step_avg:58.52ms
step:1902/2330 train_time:111306ms step_avg:58.52ms
step:1903/2330 train_time:111362ms step_avg:58.52ms
step:1904/2330 train_time:111425ms step_avg:58.52ms
step:1905/2330 train_time:111481ms step_avg:58.52ms
step:1906/2330 train_time:111544ms step_avg:58.52ms
step:1907/2330 train_time:111600ms step_avg:58.52ms
step:1908/2330 train_time:111663ms step_avg:58.52ms
step:1909/2330 train_time:111720ms step_avg:58.52ms
step:1910/2330 train_time:111782ms step_avg:58.52ms
step:1911/2330 train_time:111839ms step_avg:58.52ms
step:1912/2330 train_time:111899ms step_avg:58.52ms
step:1913/2330 train_time:111957ms step_avg:58.52ms
step:1914/2330 train_time:112017ms step_avg:58.52ms
step:1915/2330 train_time:112074ms step_avg:58.52ms
step:1916/2330 train_time:112134ms step_avg:58.53ms
step:1917/2330 train_time:112191ms step_avg:58.52ms
step:1918/2330 train_time:112251ms step_avg:58.53ms
step:1919/2330 train_time:112309ms step_avg:58.52ms
step:1920/2330 train_time:112369ms step_avg:58.53ms
step:1921/2330 train_time:112426ms step_avg:58.52ms
step:1922/2330 train_time:112487ms step_avg:58.53ms
step:1923/2330 train_time:112544ms step_avg:58.53ms
step:1924/2330 train_time:112605ms step_avg:58.53ms
step:1925/2330 train_time:112662ms step_avg:58.53ms
step:1926/2330 train_time:112725ms step_avg:58.53ms
step:1927/2330 train_time:112782ms step_avg:58.53ms
step:1928/2330 train_time:112844ms step_avg:58.53ms
step:1929/2330 train_time:112901ms step_avg:58.53ms
step:1930/2330 train_time:112963ms step_avg:58.53ms
step:1931/2330 train_time:113020ms step_avg:58.53ms
step:1932/2330 train_time:113081ms step_avg:58.53ms
step:1933/2330 train_time:113139ms step_avg:58.53ms
step:1934/2330 train_time:113200ms step_avg:58.53ms
step:1935/2330 train_time:113257ms step_avg:58.53ms
step:1936/2330 train_time:113317ms step_avg:58.53ms
step:1937/2330 train_time:113374ms step_avg:58.53ms
step:1938/2330 train_time:113434ms step_avg:58.53ms
step:1939/2330 train_time:113492ms step_avg:58.53ms
step:1940/2330 train_time:113552ms step_avg:58.53ms
step:1941/2330 train_time:113610ms step_avg:58.53ms
step:1942/2330 train_time:113670ms step_avg:58.53ms
step:1943/2330 train_time:113728ms step_avg:58.53ms
step:1944/2330 train_time:113788ms step_avg:58.53ms
step:1945/2330 train_time:113845ms step_avg:58.53ms
step:1946/2330 train_time:113906ms step_avg:58.53ms
step:1947/2330 train_time:113963ms step_avg:58.53ms
step:1948/2330 train_time:114025ms step_avg:58.53ms
step:1949/2330 train_time:114082ms step_avg:58.53ms
step:1950/2330 train_time:114144ms step_avg:58.54ms
step:1951/2330 train_time:114200ms step_avg:58.53ms
step:1952/2330 train_time:114262ms step_avg:58.54ms
step:1953/2330 train_time:114319ms step_avg:58.54ms
step:1954/2330 train_time:114380ms step_avg:58.54ms
step:1955/2330 train_time:114438ms step_avg:58.54ms
step:1956/2330 train_time:114498ms step_avg:58.54ms
step:1957/2330 train_time:114556ms step_avg:58.54ms
step:1958/2330 train_time:114616ms step_avg:58.54ms
step:1959/2330 train_time:114674ms step_avg:58.54ms
step:1960/2330 train_time:114734ms step_avg:58.54ms
step:1961/2330 train_time:114791ms step_avg:58.54ms
step:1962/2330 train_time:114851ms step_avg:58.54ms
step:1963/2330 train_time:114909ms step_avg:58.54ms
step:1964/2330 train_time:114969ms step_avg:58.54ms
step:1965/2330 train_time:115027ms step_avg:58.54ms
step:1966/2330 train_time:115088ms step_avg:58.54ms
step:1967/2330 train_time:115144ms step_avg:58.54ms
step:1968/2330 train_time:115206ms step_avg:58.54ms
step:1969/2330 train_time:115263ms step_avg:58.54ms
step:1970/2330 train_time:115325ms step_avg:58.54ms
step:1971/2330 train_time:115382ms step_avg:58.54ms
step:1972/2330 train_time:115445ms step_avg:58.54ms
step:1973/2330 train_time:115500ms step_avg:58.54ms
step:1974/2330 train_time:115564ms step_avg:58.54ms
step:1975/2330 train_time:115620ms step_avg:58.54ms
step:1976/2330 train_time:115682ms step_avg:58.54ms
step:1977/2330 train_time:115740ms step_avg:58.54ms
step:1978/2330 train_time:115800ms step_avg:58.54ms
step:1979/2330 train_time:115857ms step_avg:58.54ms
step:1980/2330 train_time:115918ms step_avg:58.54ms
step:1981/2330 train_time:115976ms step_avg:58.54ms
step:1982/2330 train_time:116037ms step_avg:58.55ms
step:1983/2330 train_time:116095ms step_avg:58.55ms
step:1984/2330 train_time:116155ms step_avg:58.55ms
step:1985/2330 train_time:116213ms step_avg:58.55ms
step:1986/2330 train_time:116273ms step_avg:58.55ms
step:1987/2330 train_time:116330ms step_avg:58.55ms
step:1988/2330 train_time:116390ms step_avg:58.55ms
step:1989/2330 train_time:116447ms step_avg:58.55ms
step:1990/2330 train_time:116508ms step_avg:58.55ms
step:1991/2330 train_time:116565ms step_avg:58.55ms
step:1992/2330 train_time:116627ms step_avg:58.55ms
step:1993/2330 train_time:116684ms step_avg:58.55ms
step:1994/2330 train_time:116746ms step_avg:58.55ms
step:1995/2330 train_time:116802ms step_avg:58.55ms
step:1996/2330 train_time:116865ms step_avg:58.55ms
step:1997/2330 train_time:116921ms step_avg:58.55ms
step:1998/2330 train_time:116983ms step_avg:58.55ms
step:1999/2330 train_time:117040ms step_avg:58.55ms
step:2000/2330 train_time:117102ms step_avg:58.55ms
step:2000/2330 val_loss:3.7785 train_time:117184ms step_avg:58.59ms
step:2001/2330 train_time:117203ms step_avg:58.57ms
step:2002/2330 train_time:117225ms step_avg:58.55ms
step:2003/2330 train_time:117284ms step_avg:58.55ms
step:2004/2330 train_time:117348ms step_avg:58.56ms
step:2005/2330 train_time:117407ms step_avg:58.56ms
step:2006/2330 train_time:117468ms step_avg:58.56ms
step:2007/2330 train_time:117524ms step_avg:58.56ms
step:2008/2330 train_time:117585ms step_avg:58.56ms
step:2009/2330 train_time:117641ms step_avg:58.56ms
step:2010/2330 train_time:117702ms step_avg:58.56ms
step:2011/2330 train_time:117759ms step_avg:58.56ms
step:2012/2330 train_time:117818ms step_avg:58.56ms
step:2013/2330 train_time:117875ms step_avg:58.56ms
step:2014/2330 train_time:117934ms step_avg:58.56ms
step:2015/2330 train_time:117990ms step_avg:58.56ms
step:2016/2330 train_time:118051ms step_avg:58.56ms
step:2017/2330 train_time:118108ms step_avg:58.56ms
step:2018/2330 train_time:118169ms step_avg:58.56ms
step:2019/2330 train_time:118228ms step_avg:58.56ms
step:2020/2330 train_time:118292ms step_avg:58.56ms
step:2021/2330 train_time:118350ms step_avg:58.56ms
step:2022/2330 train_time:118413ms step_avg:58.56ms
step:2023/2330 train_time:118469ms step_avg:58.56ms
step:2024/2330 train_time:118532ms step_avg:58.56ms
step:2025/2330 train_time:118588ms step_avg:58.56ms
step:2026/2330 train_time:118651ms step_avg:58.56ms
step:2027/2330 train_time:118708ms step_avg:58.56ms
step:2028/2330 train_time:118768ms step_avg:58.56ms
step:2029/2330 train_time:118824ms step_avg:58.56ms
step:2030/2330 train_time:118885ms step_avg:58.56ms
step:2031/2330 train_time:118941ms step_avg:58.56ms
step:2032/2330 train_time:119001ms step_avg:58.56ms
step:2033/2330 train_time:119058ms step_avg:58.56ms
step:2034/2330 train_time:119117ms step_avg:58.56ms
step:2035/2330 train_time:119177ms step_avg:58.56ms
step:2036/2330 train_time:119238ms step_avg:58.56ms
step:2037/2330 train_time:119296ms step_avg:58.56ms
step:2038/2330 train_time:119358ms step_avg:58.57ms
step:2039/2330 train_time:119417ms step_avg:58.57ms
step:2040/2330 train_time:119478ms step_avg:58.57ms
step:2041/2330 train_time:119536ms step_avg:58.57ms
step:2042/2330 train_time:119596ms step_avg:58.57ms
step:2043/2330 train_time:119654ms step_avg:58.57ms
step:2044/2330 train_time:119714ms step_avg:58.57ms
step:2045/2330 train_time:119771ms step_avg:58.57ms
step:2046/2330 train_time:119832ms step_avg:58.57ms
step:2047/2330 train_time:119889ms step_avg:58.57ms
step:2048/2330 train_time:119949ms step_avg:58.57ms
step:2049/2330 train_time:120006ms step_avg:58.57ms
step:2050/2330 train_time:120067ms step_avg:58.57ms
step:2051/2330 train_time:120123ms step_avg:58.57ms
step:2052/2330 train_time:120186ms step_avg:58.57ms
step:2053/2330 train_time:120243ms step_avg:58.57ms
step:2054/2330 train_time:120305ms step_avg:58.57ms
step:2055/2330 train_time:120363ms step_avg:58.57ms
step:2056/2330 train_time:120423ms step_avg:58.57ms
step:2057/2330 train_time:120482ms step_avg:58.57ms
step:2058/2330 train_time:120542ms step_avg:58.57ms
step:2059/2330 train_time:120601ms step_avg:58.57ms
step:2060/2330 train_time:120661ms step_avg:58.57ms
step:2061/2330 train_time:120719ms step_avg:58.57ms
step:2062/2330 train_time:120779ms step_avg:58.57ms
step:2063/2330 train_time:120838ms step_avg:58.57ms
step:2064/2330 train_time:120899ms step_avg:58.58ms
step:2065/2330 train_time:120957ms step_avg:58.57ms
step:2066/2330 train_time:121017ms step_avg:58.58ms
step:2067/2330 train_time:121074ms step_avg:58.57ms
step:2068/2330 train_time:121134ms step_avg:58.58ms
step:2069/2330 train_time:121191ms step_avg:58.57ms
step:2070/2330 train_time:121253ms step_avg:58.58ms
step:2071/2330 train_time:121309ms step_avg:58.58ms
step:2072/2330 train_time:121372ms step_avg:58.58ms
step:2073/2330 train_time:121430ms step_avg:58.58ms
step:2074/2330 train_time:121491ms step_avg:58.58ms
step:2075/2330 train_time:121548ms step_avg:58.58ms
step:2076/2330 train_time:121610ms step_avg:58.58ms
step:2077/2330 train_time:121667ms step_avg:58.58ms
step:2078/2330 train_time:121728ms step_avg:58.58ms
step:2079/2330 train_time:121784ms step_avg:58.58ms
step:2080/2330 train_time:121847ms step_avg:58.58ms
step:2081/2330 train_time:121903ms step_avg:58.58ms
step:2082/2330 train_time:121965ms step_avg:58.58ms
step:2083/2330 train_time:122022ms step_avg:58.58ms
step:2084/2330 train_time:122082ms step_avg:58.58ms
step:2085/2330 train_time:122139ms step_avg:58.58ms
step:2086/2330 train_time:122199ms step_avg:58.58ms
step:2087/2330 train_time:122257ms step_avg:58.58ms
step:2088/2330 train_time:122317ms step_avg:58.58ms
step:2089/2330 train_time:122375ms step_avg:58.58ms
step:2090/2330 train_time:122435ms step_avg:58.58ms
step:2091/2330 train_time:122492ms step_avg:58.58ms
step:2092/2330 train_time:122553ms step_avg:58.58ms
step:2093/2330 train_time:122612ms step_avg:58.58ms
step:2094/2330 train_time:122672ms step_avg:58.58ms
step:2095/2330 train_time:122729ms step_avg:58.58ms
step:2096/2330 train_time:122790ms step_avg:58.58ms
step:2097/2330 train_time:122846ms step_avg:58.58ms
step:2098/2330 train_time:122909ms step_avg:58.58ms
step:2099/2330 train_time:122965ms step_avg:58.58ms
step:2100/2330 train_time:123027ms step_avg:58.58ms
step:2101/2330 train_time:123084ms step_avg:58.58ms
step:2102/2330 train_time:123145ms step_avg:58.58ms
step:2103/2330 train_time:123202ms step_avg:58.58ms
step:2104/2330 train_time:123263ms step_avg:58.59ms
step:2105/2330 train_time:123322ms step_avg:58.59ms
step:2106/2330 train_time:123382ms step_avg:58.59ms
step:2107/2330 train_time:123439ms step_avg:58.59ms
step:2108/2330 train_time:123499ms step_avg:58.59ms
step:2109/2330 train_time:123558ms step_avg:58.59ms
step:2110/2330 train_time:123619ms step_avg:58.59ms
step:2111/2330 train_time:123677ms step_avg:58.59ms
step:2112/2330 train_time:123738ms step_avg:58.59ms
step:2113/2330 train_time:123797ms step_avg:58.59ms
step:2114/2330 train_time:123857ms step_avg:58.59ms
step:2115/2330 train_time:123915ms step_avg:58.59ms
step:2116/2330 train_time:123974ms step_avg:58.59ms
step:2117/2330 train_time:124032ms step_avg:58.59ms
step:2118/2330 train_time:124093ms step_avg:58.59ms
step:2119/2330 train_time:124149ms step_avg:58.59ms
step:2120/2330 train_time:124211ms step_avg:58.59ms
step:2121/2330 train_time:124268ms step_avg:58.59ms
step:2122/2330 train_time:124329ms step_avg:58.59ms
step:2123/2330 train_time:124387ms step_avg:58.59ms
step:2124/2330 train_time:124448ms step_avg:58.59ms
step:2125/2330 train_time:124505ms step_avg:58.59ms
step:2126/2330 train_time:124567ms step_avg:58.59ms
step:2127/2330 train_time:124624ms step_avg:58.59ms
step:2128/2330 train_time:124685ms step_avg:58.59ms
step:2129/2330 train_time:124743ms step_avg:58.59ms
step:2130/2330 train_time:124804ms step_avg:58.59ms
step:2131/2330 train_time:124862ms step_avg:58.59ms
step:2132/2330 train_time:124922ms step_avg:58.59ms
step:2133/2330 train_time:124981ms step_avg:58.59ms
step:2134/2330 train_time:125041ms step_avg:58.59ms
step:2135/2330 train_time:125099ms step_avg:58.59ms
step:2136/2330 train_time:125159ms step_avg:58.59ms
step:2137/2330 train_time:125216ms step_avg:58.59ms
step:2138/2330 train_time:125276ms step_avg:58.59ms
step:2139/2330 train_time:125334ms step_avg:58.59ms
step:2140/2330 train_time:125394ms step_avg:58.60ms
step:2141/2330 train_time:125451ms step_avg:58.59ms
step:2142/2330 train_time:125512ms step_avg:58.60ms
step:2143/2330 train_time:125570ms step_avg:58.60ms
step:2144/2330 train_time:125630ms step_avg:58.60ms
step:2145/2330 train_time:125688ms step_avg:58.60ms
step:2146/2330 train_time:125749ms step_avg:58.60ms
step:2147/2330 train_time:125806ms step_avg:58.60ms
step:2148/2330 train_time:125867ms step_avg:58.60ms
step:2149/2330 train_time:125924ms step_avg:58.60ms
step:2150/2330 train_time:125986ms step_avg:58.60ms
step:2151/2330 train_time:126043ms step_avg:58.60ms
step:2152/2330 train_time:126104ms step_avg:58.60ms
step:2153/2330 train_time:126162ms step_avg:58.60ms
step:2154/2330 train_time:126222ms step_avg:58.60ms
step:2155/2330 train_time:126279ms step_avg:58.60ms
step:2156/2330 train_time:126339ms step_avg:58.60ms
step:2157/2330 train_time:126397ms step_avg:58.60ms
step:2158/2330 train_time:126457ms step_avg:58.60ms
step:2159/2330 train_time:126516ms step_avg:58.60ms
step:2160/2330 train_time:126576ms step_avg:58.60ms
step:2161/2330 train_time:126633ms step_avg:58.60ms
step:2162/2330 train_time:126694ms step_avg:58.60ms
step:2163/2330 train_time:126752ms step_avg:58.60ms
step:2164/2330 train_time:126813ms step_avg:58.60ms
step:2165/2330 train_time:126870ms step_avg:58.60ms
step:2166/2330 train_time:126931ms step_avg:58.60ms
step:2167/2330 train_time:126988ms step_avg:58.60ms
step:2168/2330 train_time:127051ms step_avg:58.60ms
step:2169/2330 train_time:127108ms step_avg:58.60ms
step:2170/2330 train_time:127170ms step_avg:58.60ms
step:2171/2330 train_time:127227ms step_avg:58.60ms
step:2172/2330 train_time:127288ms step_avg:58.60ms
step:2173/2330 train_time:127345ms step_avg:58.60ms
step:2174/2330 train_time:127407ms step_avg:58.60ms
step:2175/2330 train_time:127464ms step_avg:58.60ms
step:2176/2330 train_time:127524ms step_avg:58.60ms
step:2177/2330 train_time:127583ms step_avg:58.60ms
step:2178/2330 train_time:127643ms step_avg:58.61ms
step:2179/2330 train_time:127702ms step_avg:58.61ms
step:2180/2330 train_time:127762ms step_avg:58.61ms
step:2181/2330 train_time:127820ms step_avg:58.61ms
step:2182/2330 train_time:127880ms step_avg:58.61ms
step:2183/2330 train_time:127939ms step_avg:58.61ms
step:2184/2330 train_time:128000ms step_avg:58.61ms
step:2185/2330 train_time:128058ms step_avg:58.61ms
step:2186/2330 train_time:128119ms step_avg:58.61ms
step:2187/2330 train_time:128177ms step_avg:58.61ms
step:2188/2330 train_time:128237ms step_avg:58.61ms
step:2189/2330 train_time:128294ms step_avg:58.61ms
step:2190/2330 train_time:128355ms step_avg:58.61ms
step:2191/2330 train_time:128412ms step_avg:58.61ms
step:2192/2330 train_time:128473ms step_avg:58.61ms
step:2193/2330 train_time:128529ms step_avg:58.61ms
step:2194/2330 train_time:128591ms step_avg:58.61ms
step:2195/2330 train_time:128648ms step_avg:58.61ms
step:2196/2330 train_time:128709ms step_avg:58.61ms
step:2197/2330 train_time:128766ms step_avg:58.61ms
step:2198/2330 train_time:128827ms step_avg:58.61ms
step:2199/2330 train_time:128884ms step_avg:58.61ms
step:2200/2330 train_time:128946ms step_avg:58.61ms
step:2201/2330 train_time:129004ms step_avg:58.61ms
step:2202/2330 train_time:129065ms step_avg:58.61ms
step:2203/2330 train_time:129123ms step_avg:58.61ms
step:2204/2330 train_time:129183ms step_avg:58.61ms
step:2205/2330 train_time:129241ms step_avg:58.61ms
step:2206/2330 train_time:129301ms step_avg:58.61ms
step:2207/2330 train_time:129359ms step_avg:58.61ms
step:2208/2330 train_time:129420ms step_avg:58.61ms
step:2209/2330 train_time:129478ms step_avg:58.61ms
step:2210/2330 train_time:129538ms step_avg:58.61ms
step:2211/2330 train_time:129596ms step_avg:58.61ms
step:2212/2330 train_time:129655ms step_avg:58.61ms
step:2213/2330 train_time:129713ms step_avg:58.61ms
step:2214/2330 train_time:129774ms step_avg:58.62ms
step:2215/2330 train_time:129831ms step_avg:58.61ms
step:2216/2330 train_time:129892ms step_avg:58.62ms
step:2217/2330 train_time:129950ms step_avg:58.62ms
step:2218/2330 train_time:130011ms step_avg:58.62ms
step:2219/2330 train_time:130067ms step_avg:58.61ms
step:2220/2330 train_time:130129ms step_avg:58.62ms
step:2221/2330 train_time:130185ms step_avg:58.62ms
step:2222/2330 train_time:130248ms step_avg:58.62ms
step:2223/2330 train_time:130304ms step_avg:58.62ms
step:2224/2330 train_time:130366ms step_avg:58.62ms
step:2225/2330 train_time:130424ms step_avg:58.62ms
step:2226/2330 train_time:130484ms step_avg:58.62ms
step:2227/2330 train_time:130542ms step_avg:58.62ms
step:2228/2330 train_time:130603ms step_avg:58.62ms
step:2229/2330 train_time:130661ms step_avg:58.62ms
step:2230/2330 train_time:130720ms step_avg:58.62ms
step:2231/2330 train_time:130778ms step_avg:58.62ms
step:2232/2330 train_time:130839ms step_avg:58.62ms
step:2233/2330 train_time:130897ms step_avg:58.62ms
step:2234/2330 train_time:130957ms step_avg:58.62ms
step:2235/2330 train_time:131015ms step_avg:58.62ms
step:2236/2330 train_time:131075ms step_avg:58.62ms
step:2237/2330 train_time:131132ms step_avg:58.62ms
step:2238/2330 train_time:131193ms step_avg:58.62ms
step:2239/2330 train_time:131250ms step_avg:58.62ms
step:2240/2330 train_time:131312ms step_avg:58.62ms
step:2241/2330 train_time:131369ms step_avg:58.62ms
step:2242/2330 train_time:131430ms step_avg:58.62ms
step:2243/2330 train_time:131487ms step_avg:58.62ms
step:2244/2330 train_time:131549ms step_avg:58.62ms
step:2245/2330 train_time:131606ms step_avg:58.62ms
step:2246/2330 train_time:131667ms step_avg:58.62ms
step:2247/2330 train_time:131724ms step_avg:58.62ms
step:2248/2330 train_time:131785ms step_avg:58.62ms
step:2249/2330 train_time:131842ms step_avg:58.62ms
step:2250/2330 train_time:131903ms step_avg:58.62ms
step:2250/2330 val_loss:3.7291 train_time:131984ms step_avg:58.66ms
step:2251/2330 train_time:132005ms step_avg:58.64ms
step:2252/2330 train_time:132026ms step_avg:58.63ms
step:2253/2330 train_time:132085ms step_avg:58.63ms
step:2254/2330 train_time:132148ms step_avg:58.63ms
step:2255/2330 train_time:132206ms step_avg:58.63ms
step:2256/2330 train_time:132268ms step_avg:58.63ms
step:2257/2330 train_time:132326ms step_avg:58.63ms
step:2258/2330 train_time:132385ms step_avg:58.63ms
step:2259/2330 train_time:132443ms step_avg:58.63ms
step:2260/2330 train_time:132503ms step_avg:58.63ms
step:2261/2330 train_time:132559ms step_avg:58.63ms
step:2262/2330 train_time:132619ms step_avg:58.63ms
step:2263/2330 train_time:132675ms step_avg:58.63ms
step:2264/2330 train_time:132737ms step_avg:58.63ms
step:2265/2330 train_time:132793ms step_avg:58.63ms
step:2266/2330 train_time:132854ms step_avg:58.63ms
step:2267/2330 train_time:132911ms step_avg:58.63ms
step:2268/2330 train_time:132973ms step_avg:58.63ms
step:2269/2330 train_time:133033ms step_avg:58.63ms
step:2270/2330 train_time:133094ms step_avg:58.63ms
step:2271/2330 train_time:133152ms step_avg:58.63ms
step:2272/2330 train_time:133214ms step_avg:58.63ms
step:2273/2330 train_time:133272ms step_avg:58.63ms
step:2274/2330 train_time:133334ms step_avg:58.63ms
step:2275/2330 train_time:133391ms step_avg:58.63ms
step:2276/2330 train_time:133451ms step_avg:58.63ms
step:2277/2330 train_time:133508ms step_avg:58.63ms
step:2278/2330 train_time:133568ms step_avg:58.63ms
step:2279/2330 train_time:133626ms step_avg:58.63ms
step:2280/2330 train_time:133686ms step_avg:58.63ms
step:2281/2330 train_time:133743ms step_avg:58.63ms
step:2282/2330 train_time:133802ms step_avg:58.63ms
step:2283/2330 train_time:133859ms step_avg:58.63ms
step:2284/2330 train_time:133920ms step_avg:58.63ms
step:2285/2330 train_time:133978ms step_avg:58.63ms
step:2286/2330 train_time:134039ms step_avg:58.63ms
step:2287/2330 train_time:134097ms step_avg:58.63ms
step:2288/2330 train_time:134159ms step_avg:58.64ms
step:2289/2330 train_time:134215ms step_avg:58.63ms
step:2290/2330 train_time:134279ms step_avg:58.64ms
step:2291/2330 train_time:134336ms step_avg:58.64ms
step:2292/2330 train_time:134398ms step_avg:58.64ms
step:2293/2330 train_time:134455ms step_avg:58.64ms
step:2294/2330 train_time:134516ms step_avg:58.64ms
step:2295/2330 train_time:134573ms step_avg:58.64ms
step:2296/2330 train_time:134634ms step_avg:58.64ms
step:2297/2330 train_time:134691ms step_avg:58.64ms
step:2298/2330 train_time:134751ms step_avg:58.64ms
step:2299/2330 train_time:134809ms step_avg:58.64ms
step:2300/2330 train_time:134869ms step_avg:58.64ms
step:2301/2330 train_time:134928ms step_avg:58.64ms
step:2302/2330 train_time:134989ms step_avg:58.64ms
step:2303/2330 train_time:135048ms step_avg:58.64ms
step:2304/2330 train_time:135108ms step_avg:58.64ms
step:2305/2330 train_time:135167ms step_avg:58.64ms
step:2306/2330 train_time:135227ms step_avg:58.64ms
step:2307/2330 train_time:135286ms step_avg:58.64ms
step:2308/2330 train_time:135346ms step_avg:58.64ms
step:2309/2330 train_time:135404ms step_avg:58.64ms
step:2310/2330 train_time:135464ms step_avg:58.64ms
step:2311/2330 train_time:135521ms step_avg:58.64ms
step:2312/2330 train_time:135582ms step_avg:58.64ms
step:2313/2330 train_time:135638ms step_avg:58.64ms
step:2314/2330 train_time:135699ms step_avg:58.64ms
step:2315/2330 train_time:135756ms step_avg:58.64ms
step:2316/2330 train_time:135817ms step_avg:58.64ms
step:2317/2330 train_time:135874ms step_avg:58.64ms
step:2318/2330 train_time:135935ms step_avg:58.64ms
step:2319/2330 train_time:135993ms step_avg:58.64ms
step:2320/2330 train_time:136054ms step_avg:58.64ms
step:2321/2330 train_time:136111ms step_avg:58.64ms
step:2322/2330 train_time:136172ms step_avg:58.64ms
step:2323/2330 train_time:136231ms step_avg:58.64ms
step:2324/2330 train_time:136291ms step_avg:58.65ms
step:2325/2330 train_time:136350ms step_avg:58.65ms
step:2326/2330 train_time:136410ms step_avg:58.65ms
step:2327/2330 train_time:136467ms step_avg:58.65ms
step:2328/2330 train_time:136528ms step_avg:58.65ms
step:2329/2330 train_time:136586ms step_avg:58.65ms
step:2330/2330 train_time:136646ms step_avg:58.65ms
step:2330/2330 val_loss:3.7141 train_time:136727ms step_avg:58.68ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
