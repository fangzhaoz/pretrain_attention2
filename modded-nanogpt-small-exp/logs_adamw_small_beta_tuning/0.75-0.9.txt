import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--beta1', type=str, required=True)
parser.add_argument('--beta2', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_beta_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=6e-4,  betas=(float(args2.beta1), float(args2.beta2)), eps=1e-10, weight_decay=0.0, fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt", exist_ok=True)
            torch.save(log, f"logs_adamw_small_beta_tuning/{args2.beta1}-{args2.beta2}.txt/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 04:56:13 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   27C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   26C    P0             115W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   24C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   27C    P0             113W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   25C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   26C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:110ms step_avg:109.74ms
step:2/2330 train_time:202ms step_avg:100.98ms
step:3/2330 train_time:220ms step_avg:73.19ms
step:4/2330 train_time:239ms step_avg:59.70ms
step:5/2330 train_time:292ms step_avg:58.49ms
step:6/2330 train_time:351ms step_avg:58.46ms
step:7/2330 train_time:406ms step_avg:57.99ms
step:8/2330 train_time:464ms step_avg:58.06ms
step:9/2330 train_time:520ms step_avg:57.78ms
step:10/2330 train_time:578ms step_avg:57.79ms
step:11/2330 train_time:633ms step_avg:57.54ms
step:12/2330 train_time:691ms step_avg:57.59ms
step:13/2330 train_time:746ms step_avg:57.42ms
step:14/2330 train_time:805ms step_avg:57.47ms
step:15/2330 train_time:860ms step_avg:57.32ms
step:16/2330 train_time:919ms step_avg:57.43ms
step:17/2330 train_time:974ms step_avg:57.30ms
step:18/2330 train_time:1033ms step_avg:57.40ms
step:19/2330 train_time:1090ms step_avg:57.35ms
step:20/2330 train_time:1152ms step_avg:57.58ms
step:21/2330 train_time:1208ms step_avg:57.51ms
step:22/2330 train_time:1272ms step_avg:57.80ms
step:23/2330 train_time:1327ms step_avg:57.69ms
step:24/2330 train_time:1388ms step_avg:57.84ms
step:25/2330 train_time:1443ms step_avg:57.74ms
step:26/2330 train_time:1503ms step_avg:57.80ms
step:27/2330 train_time:1558ms step_avg:57.72ms
step:28/2330 train_time:1617ms step_avg:57.76ms
step:29/2330 train_time:1673ms step_avg:57.68ms
step:30/2330 train_time:1731ms step_avg:57.69ms
step:31/2330 train_time:1786ms step_avg:57.62ms
step:32/2330 train_time:1845ms step_avg:57.65ms
step:33/2330 train_time:1900ms step_avg:57.59ms
step:34/2330 train_time:1959ms step_avg:57.62ms
step:35/2330 train_time:2016ms step_avg:57.59ms
step:36/2330 train_time:2075ms step_avg:57.63ms
step:37/2330 train_time:2132ms step_avg:57.63ms
step:38/2330 train_time:2191ms step_avg:57.67ms
step:39/2330 train_time:2249ms step_avg:57.66ms
step:40/2330 train_time:2308ms step_avg:57.70ms
step:41/2330 train_time:2364ms step_avg:57.66ms
step:42/2330 train_time:2425ms step_avg:57.73ms
step:43/2330 train_time:2480ms step_avg:57.68ms
step:44/2330 train_time:2540ms step_avg:57.72ms
step:45/2330 train_time:2595ms step_avg:57.67ms
step:46/2330 train_time:2654ms step_avg:57.69ms
step:47/2330 train_time:2709ms step_avg:57.64ms
step:48/2330 train_time:2768ms step_avg:57.66ms
step:49/2330 train_time:2823ms step_avg:57.61ms
step:50/2330 train_time:2883ms step_avg:57.66ms
step:51/2330 train_time:2938ms step_avg:57.61ms
step:52/2330 train_time:2999ms step_avg:57.66ms
step:53/2330 train_time:3055ms step_avg:57.64ms
step:54/2330 train_time:3115ms step_avg:57.69ms
step:55/2330 train_time:3173ms step_avg:57.69ms
step:56/2330 train_time:3232ms step_avg:57.71ms
step:57/2330 train_time:3288ms step_avg:57.68ms
step:58/2330 train_time:3347ms step_avg:57.71ms
step:59/2330 train_time:3403ms step_avg:57.67ms
step:60/2330 train_time:3464ms step_avg:57.73ms
step:61/2330 train_time:3519ms step_avg:57.70ms
step:62/2330 train_time:3578ms step_avg:57.72ms
step:63/2330 train_time:3634ms step_avg:57.69ms
step:64/2330 train_time:3693ms step_avg:57.70ms
step:65/2330 train_time:3748ms step_avg:57.67ms
step:66/2330 train_time:3807ms step_avg:57.69ms
step:67/2330 train_time:3862ms step_avg:57.65ms
step:68/2330 train_time:3923ms step_avg:57.69ms
step:69/2330 train_time:3978ms step_avg:57.66ms
step:70/2330 train_time:4039ms step_avg:57.70ms
step:71/2330 train_time:4095ms step_avg:57.68ms
step:72/2330 train_time:4154ms step_avg:57.69ms
step:73/2330 train_time:4211ms step_avg:57.69ms
step:74/2330 train_time:4270ms step_avg:57.71ms
step:75/2330 train_time:4327ms step_avg:57.69ms
step:76/2330 train_time:4386ms step_avg:57.71ms
step:77/2330 train_time:4441ms step_avg:57.68ms
step:78/2330 train_time:4501ms step_avg:57.71ms
step:79/2330 train_time:4557ms step_avg:57.69ms
step:80/2330 train_time:4617ms step_avg:57.71ms
step:81/2330 train_time:4673ms step_avg:57.69ms
step:82/2330 train_time:4731ms step_avg:57.70ms
step:83/2330 train_time:4787ms step_avg:57.67ms
step:84/2330 train_time:4846ms step_avg:57.69ms
step:85/2330 train_time:4901ms step_avg:57.66ms
step:86/2330 train_time:4961ms step_avg:57.69ms
step:87/2330 train_time:5017ms step_avg:57.67ms
step:88/2330 train_time:5078ms step_avg:57.70ms
step:89/2330 train_time:5133ms step_avg:57.68ms
step:90/2330 train_time:5193ms step_avg:57.70ms
step:91/2330 train_time:5249ms step_avg:57.69ms
step:92/2330 train_time:5308ms step_avg:57.70ms
step:93/2330 train_time:5364ms step_avg:57.68ms
step:94/2330 train_time:5424ms step_avg:57.70ms
step:95/2330 train_time:5480ms step_avg:57.68ms
step:96/2330 train_time:5539ms step_avg:57.70ms
step:97/2330 train_time:5595ms step_avg:57.69ms
step:98/2330 train_time:5655ms step_avg:57.70ms
step:99/2330 train_time:5710ms step_avg:57.68ms
step:100/2330 train_time:5769ms step_avg:57.69ms
step:101/2330 train_time:5825ms step_avg:57.67ms
step:102/2330 train_time:5884ms step_avg:57.69ms
step:103/2330 train_time:5940ms step_avg:57.67ms
step:104/2330 train_time:6000ms step_avg:57.70ms
step:105/2330 train_time:6056ms step_avg:57.68ms
step:106/2330 train_time:6115ms step_avg:57.69ms
step:107/2330 train_time:6172ms step_avg:57.68ms
step:108/2330 train_time:6231ms step_avg:57.69ms
step:109/2330 train_time:6287ms step_avg:57.67ms
step:110/2330 train_time:6345ms step_avg:57.69ms
step:111/2330 train_time:6401ms step_avg:57.67ms
step:112/2330 train_time:6461ms step_avg:57.69ms
step:113/2330 train_time:6517ms step_avg:57.67ms
step:114/2330 train_time:6576ms step_avg:57.68ms
step:115/2330 train_time:6632ms step_avg:57.67ms
step:116/2330 train_time:6690ms step_avg:57.67ms
step:117/2330 train_time:6745ms step_avg:57.65ms
step:118/2330 train_time:6804ms step_avg:57.66ms
step:119/2330 train_time:6861ms step_avg:57.65ms
step:120/2330 train_time:6919ms step_avg:57.66ms
step:121/2330 train_time:6975ms step_avg:57.65ms
step:122/2330 train_time:7034ms step_avg:57.66ms
step:123/2330 train_time:7090ms step_avg:57.64ms
step:124/2330 train_time:7149ms step_avg:57.65ms
step:125/2330 train_time:7205ms step_avg:57.64ms
step:126/2330 train_time:7266ms step_avg:57.66ms
step:127/2330 train_time:7321ms step_avg:57.65ms
step:128/2330 train_time:7381ms step_avg:57.67ms
step:129/2330 train_time:7438ms step_avg:57.66ms
step:130/2330 train_time:7496ms step_avg:57.66ms
step:131/2330 train_time:7553ms step_avg:57.65ms
step:132/2330 train_time:7611ms step_avg:57.66ms
step:133/2330 train_time:7668ms step_avg:57.65ms
step:134/2330 train_time:7725ms step_avg:57.65ms
step:135/2330 train_time:7781ms step_avg:57.64ms
step:136/2330 train_time:7842ms step_avg:57.66ms
step:137/2330 train_time:7898ms step_avg:57.65ms
step:138/2330 train_time:7957ms step_avg:57.66ms
step:139/2330 train_time:8012ms step_avg:57.64ms
step:140/2330 train_time:8071ms step_avg:57.65ms
step:141/2330 train_time:8127ms step_avg:57.64ms
step:142/2330 train_time:8186ms step_avg:57.65ms
step:143/2330 train_time:8242ms step_avg:57.63ms
step:144/2330 train_time:8301ms step_avg:57.65ms
step:145/2330 train_time:8357ms step_avg:57.63ms
step:146/2330 train_time:8416ms step_avg:57.64ms
step:147/2330 train_time:8472ms step_avg:57.63ms
step:148/2330 train_time:8531ms step_avg:57.64ms
step:149/2330 train_time:8587ms step_avg:57.63ms
step:150/2330 train_time:8646ms step_avg:57.64ms
step:151/2330 train_time:8701ms step_avg:57.62ms
step:152/2330 train_time:8761ms step_avg:57.64ms
step:153/2330 train_time:8816ms step_avg:57.62ms
step:154/2330 train_time:8876ms step_avg:57.63ms
step:155/2330 train_time:8931ms step_avg:57.62ms
step:156/2330 train_time:8991ms step_avg:57.63ms
step:157/2330 train_time:9047ms step_avg:57.62ms
step:158/2330 train_time:9106ms step_avg:57.63ms
step:159/2330 train_time:9161ms step_avg:57.62ms
step:160/2330 train_time:9221ms step_avg:57.63ms
step:161/2330 train_time:9277ms step_avg:57.62ms
step:162/2330 train_time:9336ms step_avg:57.63ms
step:163/2330 train_time:9392ms step_avg:57.62ms
step:164/2330 train_time:9451ms step_avg:57.63ms
step:165/2330 train_time:9507ms step_avg:57.62ms
step:166/2330 train_time:9566ms step_avg:57.63ms
step:167/2330 train_time:9622ms step_avg:57.61ms
step:168/2330 train_time:9681ms step_avg:57.62ms
step:169/2330 train_time:9737ms step_avg:57.61ms
step:170/2330 train_time:9795ms step_avg:57.62ms
step:171/2330 train_time:9851ms step_avg:57.61ms
step:172/2330 train_time:9910ms step_avg:57.61ms
step:173/2330 train_time:9966ms step_avg:57.60ms
step:174/2330 train_time:10025ms step_avg:57.62ms
step:175/2330 train_time:10081ms step_avg:57.61ms
step:176/2330 train_time:10140ms step_avg:57.61ms
step:177/2330 train_time:10196ms step_avg:57.60ms
step:178/2330 train_time:10255ms step_avg:57.61ms
step:179/2330 train_time:10311ms step_avg:57.60ms
step:180/2330 train_time:10370ms step_avg:57.61ms
step:181/2330 train_time:10426ms step_avg:57.60ms
step:182/2330 train_time:10485ms step_avg:57.61ms
step:183/2330 train_time:10541ms step_avg:57.60ms
step:184/2330 train_time:10600ms step_avg:57.61ms
step:185/2330 train_time:10656ms step_avg:57.60ms
step:186/2330 train_time:10715ms step_avg:57.61ms
step:187/2330 train_time:10771ms step_avg:57.60ms
step:188/2330 train_time:10830ms step_avg:57.61ms
step:189/2330 train_time:10886ms step_avg:57.60ms
step:190/2330 train_time:10945ms step_avg:57.61ms
step:191/2330 train_time:11000ms step_avg:57.59ms
step:192/2330 train_time:11060ms step_avg:57.61ms
step:193/2330 train_time:11117ms step_avg:57.60ms
step:194/2330 train_time:11176ms step_avg:57.61ms
step:195/2330 train_time:11231ms step_avg:57.60ms
step:196/2330 train_time:11289ms step_avg:57.60ms
step:197/2330 train_time:11346ms step_avg:57.59ms
step:198/2330 train_time:11404ms step_avg:57.60ms
step:199/2330 train_time:11461ms step_avg:57.59ms
step:200/2330 train_time:11520ms step_avg:57.60ms
step:201/2330 train_time:11575ms step_avg:57.59ms
step:202/2330 train_time:11634ms step_avg:57.59ms
step:203/2330 train_time:11690ms step_avg:57.58ms
step:204/2330 train_time:11749ms step_avg:57.59ms
step:205/2330 train_time:11804ms step_avg:57.58ms
step:206/2330 train_time:11865ms step_avg:57.60ms
step:207/2330 train_time:11921ms step_avg:57.59ms
step:208/2330 train_time:11980ms step_avg:57.60ms
step:209/2330 train_time:12036ms step_avg:57.59ms
step:210/2330 train_time:12095ms step_avg:57.60ms
step:211/2330 train_time:12151ms step_avg:57.59ms
step:212/2330 train_time:12210ms step_avg:57.59ms
step:213/2330 train_time:12266ms step_avg:57.59ms
step:214/2330 train_time:12326ms step_avg:57.60ms
step:215/2330 train_time:12382ms step_avg:57.59ms
step:216/2330 train_time:12440ms step_avg:57.59ms
step:217/2330 train_time:12496ms step_avg:57.58ms
step:218/2330 train_time:12555ms step_avg:57.59ms
step:219/2330 train_time:12611ms step_avg:57.58ms
step:220/2330 train_time:12669ms step_avg:57.59ms
step:221/2330 train_time:12725ms step_avg:57.58ms
step:222/2330 train_time:12784ms step_avg:57.59ms
step:223/2330 train_time:12840ms step_avg:57.58ms
step:224/2330 train_time:12899ms step_avg:57.59ms
step:225/2330 train_time:12955ms step_avg:57.58ms
step:226/2330 train_time:13014ms step_avg:57.59ms
step:227/2330 train_time:13071ms step_avg:57.58ms
step:228/2330 train_time:13129ms step_avg:57.58ms
step:229/2330 train_time:13185ms step_avg:57.58ms
step:230/2330 train_time:13245ms step_avg:57.59ms
step:231/2330 train_time:13301ms step_avg:57.58ms
step:232/2330 train_time:13360ms step_avg:57.59ms
step:233/2330 train_time:13417ms step_avg:57.58ms
step:234/2330 train_time:13476ms step_avg:57.59ms
step:235/2330 train_time:13532ms step_avg:57.58ms
step:236/2330 train_time:13590ms step_avg:57.58ms
step:237/2330 train_time:13646ms step_avg:57.58ms
step:238/2330 train_time:13705ms step_avg:57.58ms
step:239/2330 train_time:13761ms step_avg:57.58ms
step:240/2330 train_time:13820ms step_avg:57.58ms
step:241/2330 train_time:13876ms step_avg:57.58ms
step:242/2330 train_time:13934ms step_avg:57.58ms
step:243/2330 train_time:13990ms step_avg:57.57ms
step:244/2330 train_time:14049ms step_avg:57.58ms
step:245/2330 train_time:14105ms step_avg:57.57ms
step:246/2330 train_time:14165ms step_avg:57.58ms
step:247/2330 train_time:14221ms step_avg:57.57ms
step:248/2330 train_time:14281ms step_avg:57.59ms
step:249/2330 train_time:14337ms step_avg:57.58ms
step:250/2330 train_time:14397ms step_avg:57.59ms
step:250/2330 val_loss:4.9513 train_time:14475ms step_avg:57.90ms
step:251/2330 train_time:14492ms step_avg:57.74ms
step:252/2330 train_time:14513ms step_avg:57.59ms
step:253/2330 train_time:14572ms step_avg:57.60ms
step:254/2330 train_time:14634ms step_avg:57.61ms
step:255/2330 train_time:14690ms step_avg:57.61ms
step:256/2330 train_time:14752ms step_avg:57.63ms
step:257/2330 train_time:14808ms step_avg:57.62ms
step:258/2330 train_time:14868ms step_avg:57.63ms
step:259/2330 train_time:14924ms step_avg:57.62ms
step:260/2330 train_time:14983ms step_avg:57.63ms
step:261/2330 train_time:15038ms step_avg:57.62ms
step:262/2330 train_time:15096ms step_avg:57.62ms
step:263/2330 train_time:15151ms step_avg:57.61ms
step:264/2330 train_time:15210ms step_avg:57.61ms
step:265/2330 train_time:15265ms step_avg:57.60ms
step:266/2330 train_time:15323ms step_avg:57.61ms
step:267/2330 train_time:15378ms step_avg:57.60ms
step:268/2330 train_time:15437ms step_avg:57.60ms
step:269/2330 train_time:15493ms step_avg:57.60ms
step:270/2330 train_time:15555ms step_avg:57.61ms
step:271/2330 train_time:15612ms step_avg:57.61ms
step:272/2330 train_time:15671ms step_avg:57.62ms
step:273/2330 train_time:15729ms step_avg:57.61ms
step:274/2330 train_time:15788ms step_avg:57.62ms
step:275/2330 train_time:15843ms step_avg:57.61ms
step:276/2330 train_time:15903ms step_avg:57.62ms
step:277/2330 train_time:15960ms step_avg:57.62ms
step:278/2330 train_time:16018ms step_avg:57.62ms
step:279/2330 train_time:16074ms step_avg:57.61ms
step:280/2330 train_time:16132ms step_avg:57.61ms
step:281/2330 train_time:16188ms step_avg:57.61ms
step:282/2330 train_time:16246ms step_avg:57.61ms
step:283/2330 train_time:16302ms step_avg:57.60ms
step:284/2330 train_time:16360ms step_avg:57.60ms
step:285/2330 train_time:16416ms step_avg:57.60ms
step:286/2330 train_time:16474ms step_avg:57.60ms
step:287/2330 train_time:16531ms step_avg:57.60ms
step:288/2330 train_time:16590ms step_avg:57.61ms
step:289/2330 train_time:16647ms step_avg:57.60ms
step:290/2330 train_time:16707ms step_avg:57.61ms
step:291/2330 train_time:16763ms step_avg:57.60ms
step:292/2330 train_time:16822ms step_avg:57.61ms
step:293/2330 train_time:16878ms step_avg:57.60ms
step:294/2330 train_time:16937ms step_avg:57.61ms
step:295/2330 train_time:16993ms step_avg:57.60ms
step:296/2330 train_time:17051ms step_avg:57.61ms
step:297/2330 train_time:17107ms step_avg:57.60ms
step:298/2330 train_time:17166ms step_avg:57.60ms
step:299/2330 train_time:17221ms step_avg:57.60ms
step:300/2330 train_time:17280ms step_avg:57.60ms
step:301/2330 train_time:17336ms step_avg:57.60ms
step:302/2330 train_time:17395ms step_avg:57.60ms
step:303/2330 train_time:17451ms step_avg:57.60ms
step:304/2330 train_time:17510ms step_avg:57.60ms
step:305/2330 train_time:17566ms step_avg:57.59ms
step:306/2330 train_time:17625ms step_avg:57.60ms
step:307/2330 train_time:17682ms step_avg:57.59ms
step:308/2330 train_time:17741ms step_avg:57.60ms
step:309/2330 train_time:17798ms step_avg:57.60ms
step:310/2330 train_time:17857ms step_avg:57.60ms
step:311/2330 train_time:17913ms step_avg:57.60ms
step:312/2330 train_time:17971ms step_avg:57.60ms
step:313/2330 train_time:18028ms step_avg:57.60ms
step:314/2330 train_time:18086ms step_avg:57.60ms
step:315/2330 train_time:18142ms step_avg:57.59ms
step:316/2330 train_time:18201ms step_avg:57.60ms
step:317/2330 train_time:18257ms step_avg:57.59ms
step:318/2330 train_time:18315ms step_avg:57.60ms
step:319/2330 train_time:18371ms step_avg:57.59ms
step:320/2330 train_time:18429ms step_avg:57.59ms
step:321/2330 train_time:18485ms step_avg:57.59ms
step:322/2330 train_time:18544ms step_avg:57.59ms
step:323/2330 train_time:18600ms step_avg:57.58ms
step:324/2330 train_time:18659ms step_avg:57.59ms
step:325/2330 train_time:18716ms step_avg:57.59ms
step:326/2330 train_time:18775ms step_avg:57.59ms
step:327/2330 train_time:18831ms step_avg:57.59ms
step:328/2330 train_time:18890ms step_avg:57.59ms
step:329/2330 train_time:18946ms step_avg:57.59ms
step:330/2330 train_time:19006ms step_avg:57.59ms
step:331/2330 train_time:19062ms step_avg:57.59ms
step:332/2330 train_time:19121ms step_avg:57.59ms
step:333/2330 train_time:19177ms step_avg:57.59ms
step:334/2330 train_time:19235ms step_avg:57.59ms
step:335/2330 train_time:19291ms step_avg:57.59ms
step:336/2330 train_time:19349ms step_avg:57.59ms
step:337/2330 train_time:19405ms step_avg:57.58ms
step:338/2330 train_time:19464ms step_avg:57.59ms
step:339/2330 train_time:19521ms step_avg:57.58ms
step:340/2330 train_time:19580ms step_avg:57.59ms
step:341/2330 train_time:19636ms step_avg:57.58ms
step:342/2330 train_time:19695ms step_avg:57.59ms
step:343/2330 train_time:19751ms step_avg:57.58ms
step:344/2330 train_time:19810ms step_avg:57.59ms
step:345/2330 train_time:19866ms step_avg:57.58ms
step:346/2330 train_time:19926ms step_avg:57.59ms
step:347/2330 train_time:19982ms step_avg:57.58ms
step:348/2330 train_time:20041ms step_avg:57.59ms
step:349/2330 train_time:20097ms step_avg:57.58ms
step:350/2330 train_time:20156ms step_avg:57.59ms
step:351/2330 train_time:20212ms step_avg:57.58ms
step:352/2330 train_time:20271ms step_avg:57.59ms
step:353/2330 train_time:20326ms step_avg:57.58ms
step:354/2330 train_time:20385ms step_avg:57.58ms
step:355/2330 train_time:20441ms step_avg:57.58ms
step:356/2330 train_time:20501ms step_avg:57.59ms
step:357/2330 train_time:20556ms step_avg:57.58ms
step:358/2330 train_time:20617ms step_avg:57.59ms
step:359/2330 train_time:20673ms step_avg:57.58ms
step:360/2330 train_time:20732ms step_avg:57.59ms
step:361/2330 train_time:20788ms step_avg:57.58ms
step:362/2330 train_time:20847ms step_avg:57.59ms
step:363/2330 train_time:20903ms step_avg:57.58ms
step:364/2330 train_time:20963ms step_avg:57.59ms
step:365/2330 train_time:21020ms step_avg:57.59ms
step:366/2330 train_time:21079ms step_avg:57.59ms
step:367/2330 train_time:21135ms step_avg:57.59ms
step:368/2330 train_time:21193ms step_avg:57.59ms
step:369/2330 train_time:21250ms step_avg:57.59ms
step:370/2330 train_time:21308ms step_avg:57.59ms
step:371/2330 train_time:21363ms step_avg:57.58ms
step:372/2330 train_time:21424ms step_avg:57.59ms
step:373/2330 train_time:21480ms step_avg:57.59ms
step:374/2330 train_time:21540ms step_avg:57.59ms
step:375/2330 train_time:21596ms step_avg:57.59ms
step:376/2330 train_time:21654ms step_avg:57.59ms
step:377/2330 train_time:21710ms step_avg:57.59ms
step:378/2330 train_time:21769ms step_avg:57.59ms
step:379/2330 train_time:21825ms step_avg:57.58ms
step:380/2330 train_time:21885ms step_avg:57.59ms
step:381/2330 train_time:21942ms step_avg:57.59ms
step:382/2330 train_time:22001ms step_avg:57.59ms
step:383/2330 train_time:22058ms step_avg:57.59ms
step:384/2330 train_time:22117ms step_avg:57.60ms
step:385/2330 train_time:22173ms step_avg:57.59ms
step:386/2330 train_time:22232ms step_avg:57.59ms
step:387/2330 train_time:22288ms step_avg:57.59ms
step:388/2330 train_time:22347ms step_avg:57.59ms
step:389/2330 train_time:22403ms step_avg:57.59ms
step:390/2330 train_time:22462ms step_avg:57.59ms
step:391/2330 train_time:22518ms step_avg:57.59ms
step:392/2330 train_time:22577ms step_avg:57.59ms
step:393/2330 train_time:22633ms step_avg:57.59ms
step:394/2330 train_time:22691ms step_avg:57.59ms
step:395/2330 train_time:22747ms step_avg:57.59ms
step:396/2330 train_time:22806ms step_avg:57.59ms
step:397/2330 train_time:22862ms step_avg:57.59ms
step:398/2330 train_time:22921ms step_avg:57.59ms
step:399/2330 train_time:22978ms step_avg:57.59ms
step:400/2330 train_time:23037ms step_avg:57.59ms
step:401/2330 train_time:23092ms step_avg:57.59ms
step:402/2330 train_time:23151ms step_avg:57.59ms
step:403/2330 train_time:23206ms step_avg:57.58ms
step:404/2330 train_time:23266ms step_avg:57.59ms
step:405/2330 train_time:23322ms step_avg:57.58ms
step:406/2330 train_time:23381ms step_avg:57.59ms
step:407/2330 train_time:23437ms step_avg:57.58ms
step:408/2330 train_time:23496ms step_avg:57.59ms
step:409/2330 train_time:23552ms step_avg:57.58ms
step:410/2330 train_time:23611ms step_avg:57.59ms
step:411/2330 train_time:23668ms step_avg:57.59ms
step:412/2330 train_time:23726ms step_avg:57.59ms
step:413/2330 train_time:23782ms step_avg:57.58ms
step:414/2330 train_time:23841ms step_avg:57.59ms
step:415/2330 train_time:23897ms step_avg:57.58ms
step:416/2330 train_time:23955ms step_avg:57.59ms
step:417/2330 train_time:24012ms step_avg:57.58ms
step:418/2330 train_time:24070ms step_avg:57.58ms
step:419/2330 train_time:24126ms step_avg:57.58ms
step:420/2330 train_time:24186ms step_avg:57.59ms
step:421/2330 train_time:24242ms step_avg:57.58ms
step:422/2330 train_time:24302ms step_avg:57.59ms
step:423/2330 train_time:24359ms step_avg:57.59ms
step:424/2330 train_time:24418ms step_avg:57.59ms
step:425/2330 train_time:24474ms step_avg:57.59ms
step:426/2330 train_time:24533ms step_avg:57.59ms
step:427/2330 train_time:24589ms step_avg:57.59ms
step:428/2330 train_time:24648ms step_avg:57.59ms
step:429/2330 train_time:24704ms step_avg:57.58ms
step:430/2330 train_time:24763ms step_avg:57.59ms
step:431/2330 train_time:24820ms step_avg:57.59ms
step:432/2330 train_time:24878ms step_avg:57.59ms
step:433/2330 train_time:24934ms step_avg:57.58ms
step:434/2330 train_time:24993ms step_avg:57.59ms
step:435/2330 train_time:25049ms step_avg:57.58ms
step:436/2330 train_time:25109ms step_avg:57.59ms
step:437/2330 train_time:25164ms step_avg:57.58ms
step:438/2330 train_time:25224ms step_avg:57.59ms
step:439/2330 train_time:25279ms step_avg:57.58ms
step:440/2330 train_time:25340ms step_avg:57.59ms
step:441/2330 train_time:25396ms step_avg:57.59ms
step:442/2330 train_time:25455ms step_avg:57.59ms
step:443/2330 train_time:25511ms step_avg:57.59ms
step:444/2330 train_time:25569ms step_avg:57.59ms
step:445/2330 train_time:25624ms step_avg:57.58ms
step:446/2330 train_time:25684ms step_avg:57.59ms
step:447/2330 train_time:25741ms step_avg:57.59ms
step:448/2330 train_time:25799ms step_avg:57.59ms
step:449/2330 train_time:25856ms step_avg:57.58ms
step:450/2330 train_time:25915ms step_avg:57.59ms
step:451/2330 train_time:25970ms step_avg:57.58ms
step:452/2330 train_time:26029ms step_avg:57.59ms
step:453/2330 train_time:26084ms step_avg:57.58ms
step:454/2330 train_time:26145ms step_avg:57.59ms
step:455/2330 train_time:26201ms step_avg:57.59ms
step:456/2330 train_time:26260ms step_avg:57.59ms
step:457/2330 train_time:26316ms step_avg:57.58ms
step:458/2330 train_time:26375ms step_avg:57.59ms
step:459/2330 train_time:26431ms step_avg:57.58ms
step:460/2330 train_time:26489ms step_avg:57.59ms
step:461/2330 train_time:26545ms step_avg:57.58ms
step:462/2330 train_time:26605ms step_avg:57.59ms
step:463/2330 train_time:26661ms step_avg:57.58ms
step:464/2330 train_time:26720ms step_avg:57.59ms
step:465/2330 train_time:26776ms step_avg:57.58ms
step:466/2330 train_time:26835ms step_avg:57.59ms
step:467/2330 train_time:26892ms step_avg:57.58ms
step:468/2330 train_time:26951ms step_avg:57.59ms
step:469/2330 train_time:27007ms step_avg:57.58ms
step:470/2330 train_time:27066ms step_avg:57.59ms
step:471/2330 train_time:27122ms step_avg:57.58ms
step:472/2330 train_time:27181ms step_avg:57.59ms
step:473/2330 train_time:27237ms step_avg:57.58ms
step:474/2330 train_time:27296ms step_avg:57.59ms
step:475/2330 train_time:27352ms step_avg:57.58ms
step:476/2330 train_time:27411ms step_avg:57.59ms
step:477/2330 train_time:27467ms step_avg:57.58ms
step:478/2330 train_time:27526ms step_avg:57.58ms
step:479/2330 train_time:27581ms step_avg:57.58ms
step:480/2330 train_time:27641ms step_avg:57.59ms
step:481/2330 train_time:27697ms step_avg:57.58ms
step:482/2330 train_time:27756ms step_avg:57.59ms
step:483/2330 train_time:27812ms step_avg:57.58ms
step:484/2330 train_time:27872ms step_avg:57.59ms
step:485/2330 train_time:27927ms step_avg:57.58ms
step:486/2330 train_time:27987ms step_avg:57.59ms
step:487/2330 train_time:28043ms step_avg:57.58ms
step:488/2330 train_time:28102ms step_avg:57.59ms
step:489/2330 train_time:28158ms step_avg:57.58ms
step:490/2330 train_time:28217ms step_avg:57.59ms
step:491/2330 train_time:28273ms step_avg:57.58ms
step:492/2330 train_time:28331ms step_avg:57.58ms
step:493/2330 train_time:28388ms step_avg:57.58ms
step:494/2330 train_time:28446ms step_avg:57.58ms
step:495/2330 train_time:28502ms step_avg:57.58ms
step:496/2330 train_time:28562ms step_avg:57.58ms
step:497/2330 train_time:28618ms step_avg:57.58ms
step:498/2330 train_time:28677ms step_avg:57.58ms
step:499/2330 train_time:28733ms step_avg:57.58ms
step:500/2330 train_time:28792ms step_avg:57.58ms
step:500/2330 val_loss:4.4210 train_time:28872ms step_avg:57.74ms
step:501/2330 train_time:28890ms step_avg:57.66ms
step:502/2330 train_time:28910ms step_avg:57.59ms
step:503/2330 train_time:28968ms step_avg:57.59ms
step:504/2330 train_time:29030ms step_avg:57.60ms
step:505/2330 train_time:29087ms step_avg:57.60ms
step:506/2330 train_time:29148ms step_avg:57.60ms
step:507/2330 train_time:29203ms step_avg:57.60ms
step:508/2330 train_time:29262ms step_avg:57.60ms
step:509/2330 train_time:29318ms step_avg:57.60ms
step:510/2330 train_time:29377ms step_avg:57.60ms
step:511/2330 train_time:29432ms step_avg:57.60ms
step:512/2330 train_time:29491ms step_avg:57.60ms
step:513/2330 train_time:29547ms step_avg:57.60ms
step:514/2330 train_time:29605ms step_avg:57.60ms
step:515/2330 train_time:29660ms step_avg:57.59ms
step:516/2330 train_time:29718ms step_avg:57.59ms
step:517/2330 train_time:29775ms step_avg:57.59ms
step:518/2330 train_time:29833ms step_avg:57.59ms
step:519/2330 train_time:29891ms step_avg:57.59ms
step:520/2330 train_time:29951ms step_avg:57.60ms
step:521/2330 train_time:30008ms step_avg:57.60ms
step:522/2330 train_time:30068ms step_avg:57.60ms
step:523/2330 train_time:30124ms step_avg:57.60ms
step:524/2330 train_time:30183ms step_avg:57.60ms
step:525/2330 train_time:30239ms step_avg:57.60ms
step:526/2330 train_time:30298ms step_avg:57.60ms
step:527/2330 train_time:30354ms step_avg:57.60ms
step:528/2330 train_time:30413ms step_avg:57.60ms
step:529/2330 train_time:30469ms step_avg:57.60ms
step:530/2330 train_time:30528ms step_avg:57.60ms
step:531/2330 train_time:30583ms step_avg:57.60ms
step:532/2330 train_time:30642ms step_avg:57.60ms
step:533/2330 train_time:30698ms step_avg:57.59ms
step:534/2330 train_time:30757ms step_avg:57.60ms
step:535/2330 train_time:30812ms step_avg:57.59ms
step:536/2330 train_time:30872ms step_avg:57.60ms
step:537/2330 train_time:30929ms step_avg:57.60ms
step:538/2330 train_time:30989ms step_avg:57.60ms
step:539/2330 train_time:31046ms step_avg:57.60ms
step:540/2330 train_time:31105ms step_avg:57.60ms
step:541/2330 train_time:31162ms step_avg:57.60ms
step:542/2330 train_time:31220ms step_avg:57.60ms
step:543/2330 train_time:31276ms step_avg:57.60ms
step:544/2330 train_time:31335ms step_avg:57.60ms
step:545/2330 train_time:31391ms step_avg:57.60ms
step:546/2330 train_time:31452ms step_avg:57.60ms
step:547/2330 train_time:31508ms step_avg:57.60ms
step:548/2330 train_time:31566ms step_avg:57.60ms
step:549/2330 train_time:31621ms step_avg:57.60ms
step:550/2330 train_time:31680ms step_avg:57.60ms
step:551/2330 train_time:31736ms step_avg:57.60ms
step:552/2330 train_time:31795ms step_avg:57.60ms
step:553/2330 train_time:31851ms step_avg:57.60ms
step:554/2330 train_time:31910ms step_avg:57.60ms
step:555/2330 train_time:31967ms step_avg:57.60ms
step:556/2330 train_time:32026ms step_avg:57.60ms
step:557/2330 train_time:32083ms step_avg:57.60ms
step:558/2330 train_time:32142ms step_avg:57.60ms
step:559/2330 train_time:32199ms step_avg:57.60ms
step:560/2330 train_time:32257ms step_avg:57.60ms
step:561/2330 train_time:32313ms step_avg:57.60ms
step:562/2330 train_time:32373ms step_avg:57.60ms
step:563/2330 train_time:32428ms step_avg:57.60ms
step:564/2330 train_time:32487ms step_avg:57.60ms
step:565/2330 train_time:32543ms step_avg:57.60ms
step:566/2330 train_time:32601ms step_avg:57.60ms
step:567/2330 train_time:32657ms step_avg:57.60ms
step:568/2330 train_time:32716ms step_avg:57.60ms
step:569/2330 train_time:32772ms step_avg:57.60ms
step:570/2330 train_time:32831ms step_avg:57.60ms
step:571/2330 train_time:32887ms step_avg:57.60ms
step:572/2330 train_time:32947ms step_avg:57.60ms
step:573/2330 train_time:33004ms step_avg:57.60ms
step:574/2330 train_time:33062ms step_avg:57.60ms
step:575/2330 train_time:33119ms step_avg:57.60ms
step:576/2330 train_time:33178ms step_avg:57.60ms
step:577/2330 train_time:33234ms step_avg:57.60ms
step:578/2330 train_time:33294ms step_avg:57.60ms
step:579/2330 train_time:33350ms step_avg:57.60ms
step:580/2330 train_time:33409ms step_avg:57.60ms
step:581/2330 train_time:33465ms step_avg:57.60ms
step:582/2330 train_time:33523ms step_avg:57.60ms
step:583/2330 train_time:33579ms step_avg:57.60ms
step:584/2330 train_time:33638ms step_avg:57.60ms
step:585/2330 train_time:33694ms step_avg:57.60ms
step:586/2330 train_time:33753ms step_avg:57.60ms
step:587/2330 train_time:33809ms step_avg:57.60ms
step:588/2330 train_time:33869ms step_avg:57.60ms
step:589/2330 train_time:33925ms step_avg:57.60ms
step:590/2330 train_time:33984ms step_avg:57.60ms
step:591/2330 train_time:34040ms step_avg:57.60ms
step:592/2330 train_time:34099ms step_avg:57.60ms
step:593/2330 train_time:34155ms step_avg:57.60ms
step:594/2330 train_time:34215ms step_avg:57.60ms
step:595/2330 train_time:34272ms step_avg:57.60ms
step:596/2330 train_time:34331ms step_avg:57.60ms
step:597/2330 train_time:34387ms step_avg:57.60ms
step:598/2330 train_time:34446ms step_avg:57.60ms
step:599/2330 train_time:34502ms step_avg:57.60ms
step:600/2330 train_time:34561ms step_avg:57.60ms
step:601/2330 train_time:34617ms step_avg:57.60ms
step:602/2330 train_time:34676ms step_avg:57.60ms
step:603/2330 train_time:34732ms step_avg:57.60ms
step:604/2330 train_time:34791ms step_avg:57.60ms
step:605/2330 train_time:34847ms step_avg:57.60ms
step:606/2330 train_time:34905ms step_avg:57.60ms
step:607/2330 train_time:34962ms step_avg:57.60ms
step:608/2330 train_time:35021ms step_avg:57.60ms
step:609/2330 train_time:35077ms step_avg:57.60ms
step:610/2330 train_time:35136ms step_avg:57.60ms
step:611/2330 train_time:35192ms step_avg:57.60ms
step:612/2330 train_time:35253ms step_avg:57.60ms
step:613/2330 train_time:35309ms step_avg:57.60ms
step:614/2330 train_time:35369ms step_avg:57.60ms
step:615/2330 train_time:35425ms step_avg:57.60ms
step:616/2330 train_time:35484ms step_avg:57.60ms
step:617/2330 train_time:35541ms step_avg:57.60ms
step:618/2330 train_time:35599ms step_avg:57.60ms
step:619/2330 train_time:35655ms step_avg:57.60ms
step:620/2330 train_time:35714ms step_avg:57.60ms
step:621/2330 train_time:35770ms step_avg:57.60ms
step:622/2330 train_time:35829ms step_avg:57.60ms
step:623/2330 train_time:35886ms step_avg:57.60ms
step:624/2330 train_time:35945ms step_avg:57.60ms
step:625/2330 train_time:36000ms step_avg:57.60ms
step:626/2330 train_time:36059ms step_avg:57.60ms
step:627/2330 train_time:36115ms step_avg:57.60ms
step:628/2330 train_time:36175ms step_avg:57.60ms
step:629/2330 train_time:36231ms step_avg:57.60ms
step:630/2330 train_time:36291ms step_avg:57.60ms
step:631/2330 train_time:36347ms step_avg:57.60ms
step:632/2330 train_time:36405ms step_avg:57.60ms
step:633/2330 train_time:36462ms step_avg:57.60ms
step:634/2330 train_time:36521ms step_avg:57.60ms
step:635/2330 train_time:36577ms step_avg:57.60ms
step:636/2330 train_time:36636ms step_avg:57.60ms
step:637/2330 train_time:36692ms step_avg:57.60ms
step:638/2330 train_time:36752ms step_avg:57.60ms
step:639/2330 train_time:36808ms step_avg:57.60ms
step:640/2330 train_time:36867ms step_avg:57.60ms
step:641/2330 train_time:36923ms step_avg:57.60ms
step:642/2330 train_time:36982ms step_avg:57.60ms
step:643/2330 train_time:37038ms step_avg:57.60ms
step:644/2330 train_time:37097ms step_avg:57.60ms
step:645/2330 train_time:37153ms step_avg:57.60ms
step:646/2330 train_time:37213ms step_avg:57.61ms
step:647/2330 train_time:37269ms step_avg:57.60ms
step:648/2330 train_time:37329ms step_avg:57.61ms
step:649/2330 train_time:37386ms step_avg:57.61ms
step:650/2330 train_time:37444ms step_avg:57.61ms
step:651/2330 train_time:37501ms step_avg:57.60ms
step:652/2330 train_time:37559ms step_avg:57.61ms
step:653/2330 train_time:37614ms step_avg:57.60ms
step:654/2330 train_time:37675ms step_avg:57.61ms
step:655/2330 train_time:37731ms step_avg:57.60ms
step:656/2330 train_time:37791ms step_avg:57.61ms
step:657/2330 train_time:37847ms step_avg:57.61ms
step:658/2330 train_time:37906ms step_avg:57.61ms
step:659/2330 train_time:37963ms step_avg:57.61ms
step:660/2330 train_time:38021ms step_avg:57.61ms
step:661/2330 train_time:38078ms step_avg:57.61ms
step:662/2330 train_time:38136ms step_avg:57.61ms
step:663/2330 train_time:38192ms step_avg:57.60ms
step:664/2330 train_time:38252ms step_avg:57.61ms
step:665/2330 train_time:38309ms step_avg:57.61ms
step:666/2330 train_time:38369ms step_avg:57.61ms
step:667/2330 train_time:38425ms step_avg:57.61ms
step:668/2330 train_time:38484ms step_avg:57.61ms
step:669/2330 train_time:38540ms step_avg:57.61ms
step:670/2330 train_time:38599ms step_avg:57.61ms
step:671/2330 train_time:38655ms step_avg:57.61ms
step:672/2330 train_time:38714ms step_avg:57.61ms
step:673/2330 train_time:38771ms step_avg:57.61ms
step:674/2330 train_time:38830ms step_avg:57.61ms
step:675/2330 train_time:38887ms step_avg:57.61ms
step:676/2330 train_time:38946ms step_avg:57.61ms
step:677/2330 train_time:39002ms step_avg:57.61ms
step:678/2330 train_time:39061ms step_avg:57.61ms
step:679/2330 train_time:39116ms step_avg:57.61ms
step:680/2330 train_time:39176ms step_avg:57.61ms
step:681/2330 train_time:39231ms step_avg:57.61ms
step:682/2330 train_time:39291ms step_avg:57.61ms
step:683/2330 train_time:39348ms step_avg:57.61ms
step:684/2330 train_time:39407ms step_avg:57.61ms
step:685/2330 train_time:39463ms step_avg:57.61ms
step:686/2330 train_time:39522ms step_avg:57.61ms
step:687/2330 train_time:39578ms step_avg:57.61ms
step:688/2330 train_time:39637ms step_avg:57.61ms
step:689/2330 train_time:39692ms step_avg:57.61ms
step:690/2330 train_time:39752ms step_avg:57.61ms
step:691/2330 train_time:39808ms step_avg:57.61ms
step:692/2330 train_time:39867ms step_avg:57.61ms
step:693/2330 train_time:39923ms step_avg:57.61ms
step:694/2330 train_time:39982ms step_avg:57.61ms
step:695/2330 train_time:40038ms step_avg:57.61ms
step:696/2330 train_time:40097ms step_avg:57.61ms
step:697/2330 train_time:40152ms step_avg:57.61ms
step:698/2330 train_time:40212ms step_avg:57.61ms
step:699/2330 train_time:40269ms step_avg:57.61ms
step:700/2330 train_time:40329ms step_avg:57.61ms
step:701/2330 train_time:40385ms step_avg:57.61ms
step:702/2330 train_time:40444ms step_avg:57.61ms
step:703/2330 train_time:40500ms step_avg:57.61ms
step:704/2330 train_time:40558ms step_avg:57.61ms
step:705/2330 train_time:40614ms step_avg:57.61ms
step:706/2330 train_time:40674ms step_avg:57.61ms
step:707/2330 train_time:40731ms step_avg:57.61ms
step:708/2330 train_time:40791ms step_avg:57.61ms
step:709/2330 train_time:40846ms step_avg:57.61ms
step:710/2330 train_time:40905ms step_avg:57.61ms
step:711/2330 train_time:40961ms step_avg:57.61ms
step:712/2330 train_time:41020ms step_avg:57.61ms
step:713/2330 train_time:41076ms step_avg:57.61ms
step:714/2330 train_time:41135ms step_avg:57.61ms
step:715/2330 train_time:41191ms step_avg:57.61ms
step:716/2330 train_time:41250ms step_avg:57.61ms
step:717/2330 train_time:41306ms step_avg:57.61ms
step:718/2330 train_time:41366ms step_avg:57.61ms
step:719/2330 train_time:41422ms step_avg:57.61ms
step:720/2330 train_time:41481ms step_avg:57.61ms
step:721/2330 train_time:41537ms step_avg:57.61ms
step:722/2330 train_time:41596ms step_avg:57.61ms
step:723/2330 train_time:41652ms step_avg:57.61ms
step:724/2330 train_time:41711ms step_avg:57.61ms
step:725/2330 train_time:41767ms step_avg:57.61ms
step:726/2330 train_time:41827ms step_avg:57.61ms
step:727/2330 train_time:41883ms step_avg:57.61ms
step:728/2330 train_time:41942ms step_avg:57.61ms
step:729/2330 train_time:41998ms step_avg:57.61ms
step:730/2330 train_time:42057ms step_avg:57.61ms
step:731/2330 train_time:42112ms step_avg:57.61ms
step:732/2330 train_time:42172ms step_avg:57.61ms
step:733/2330 train_time:42228ms step_avg:57.61ms
step:734/2330 train_time:42288ms step_avg:57.61ms
step:735/2330 train_time:42344ms step_avg:57.61ms
step:736/2330 train_time:42403ms step_avg:57.61ms
step:737/2330 train_time:42458ms step_avg:57.61ms
step:738/2330 train_time:42518ms step_avg:57.61ms
step:739/2330 train_time:42574ms step_avg:57.61ms
step:740/2330 train_time:42634ms step_avg:57.61ms
step:741/2330 train_time:42690ms step_avg:57.61ms
step:742/2330 train_time:42750ms step_avg:57.61ms
step:743/2330 train_time:42806ms step_avg:57.61ms
step:744/2330 train_time:42864ms step_avg:57.61ms
step:745/2330 train_time:42920ms step_avg:57.61ms
step:746/2330 train_time:42979ms step_avg:57.61ms
step:747/2330 train_time:43035ms step_avg:57.61ms
step:748/2330 train_time:43095ms step_avg:57.61ms
step:749/2330 train_time:43151ms step_avg:57.61ms
step:750/2330 train_time:43210ms step_avg:57.61ms
step:750/2330 val_loss:4.2201 train_time:43290ms step_avg:57.72ms
step:751/2330 train_time:43307ms step_avg:57.67ms
step:752/2330 train_time:43328ms step_avg:57.62ms
step:753/2330 train_time:43386ms step_avg:57.62ms
step:754/2330 train_time:43447ms step_avg:57.62ms
step:755/2330 train_time:43504ms step_avg:57.62ms
step:756/2330 train_time:43564ms step_avg:57.62ms
step:757/2330 train_time:43619ms step_avg:57.62ms
step:758/2330 train_time:43678ms step_avg:57.62ms
step:759/2330 train_time:43734ms step_avg:57.62ms
step:760/2330 train_time:43794ms step_avg:57.62ms
step:761/2330 train_time:43850ms step_avg:57.62ms
step:762/2330 train_time:43908ms step_avg:57.62ms
step:763/2330 train_time:43963ms step_avg:57.62ms
step:764/2330 train_time:44022ms step_avg:57.62ms
step:765/2330 train_time:44079ms step_avg:57.62ms
step:766/2330 train_time:44137ms step_avg:57.62ms
step:767/2330 train_time:44193ms step_avg:57.62ms
step:768/2330 train_time:44254ms step_avg:57.62ms
step:769/2330 train_time:44313ms step_avg:57.62ms
step:770/2330 train_time:44374ms step_avg:57.63ms
step:771/2330 train_time:44433ms step_avg:57.63ms
step:772/2330 train_time:44495ms step_avg:57.64ms
step:773/2330 train_time:44552ms step_avg:57.64ms
step:774/2330 train_time:44613ms step_avg:57.64ms
step:775/2330 train_time:44670ms step_avg:57.64ms
step:776/2330 train_time:44729ms step_avg:57.64ms
step:777/2330 train_time:44786ms step_avg:57.64ms
step:778/2330 train_time:44845ms step_avg:57.64ms
step:779/2330 train_time:44902ms step_avg:57.64ms
step:780/2330 train_time:44960ms step_avg:57.64ms
step:781/2330 train_time:45017ms step_avg:57.64ms
step:782/2330 train_time:45077ms step_avg:57.64ms
step:783/2330 train_time:45133ms step_avg:57.64ms
step:784/2330 train_time:45193ms step_avg:57.64ms
step:785/2330 train_time:45250ms step_avg:57.64ms
step:786/2330 train_time:45311ms step_avg:57.65ms
step:787/2330 train_time:45369ms step_avg:57.65ms
step:788/2330 train_time:45429ms step_avg:57.65ms
step:789/2330 train_time:45488ms step_avg:57.65ms
step:790/2330 train_time:45547ms step_avg:57.66ms
step:791/2330 train_time:45604ms step_avg:57.65ms
step:792/2330 train_time:45663ms step_avg:57.66ms
step:793/2330 train_time:45720ms step_avg:57.65ms
step:794/2330 train_time:45781ms step_avg:57.66ms
step:795/2330 train_time:45838ms step_avg:57.66ms
step:796/2330 train_time:45897ms step_avg:57.66ms
step:797/2330 train_time:45953ms step_avg:57.66ms
step:798/2330 train_time:46013ms step_avg:57.66ms
step:799/2330 train_time:46070ms step_avg:57.66ms
step:800/2330 train_time:46129ms step_avg:57.66ms
step:801/2330 train_time:46186ms step_avg:57.66ms
step:802/2330 train_time:46246ms step_avg:57.66ms
step:803/2330 train_time:46303ms step_avg:57.66ms
step:804/2330 train_time:46363ms step_avg:57.67ms
step:805/2330 train_time:46421ms step_avg:57.67ms
step:806/2330 train_time:46481ms step_avg:57.67ms
step:807/2330 train_time:46538ms step_avg:57.67ms
step:808/2330 train_time:46599ms step_avg:57.67ms
step:809/2330 train_time:46655ms step_avg:57.67ms
step:810/2330 train_time:46716ms step_avg:57.67ms
step:811/2330 train_time:46773ms step_avg:57.67ms
step:812/2330 train_time:46834ms step_avg:57.68ms
step:813/2330 train_time:46891ms step_avg:57.68ms
step:814/2330 train_time:46951ms step_avg:57.68ms
step:815/2330 train_time:47008ms step_avg:57.68ms
step:816/2330 train_time:47068ms step_avg:57.68ms
step:817/2330 train_time:47124ms step_avg:57.68ms
step:818/2330 train_time:47183ms step_avg:57.68ms
step:819/2330 train_time:47240ms step_avg:57.68ms
step:820/2330 train_time:47300ms step_avg:57.68ms
step:821/2330 train_time:47357ms step_avg:57.68ms
step:822/2330 train_time:47417ms step_avg:57.69ms
step:823/2330 train_time:47475ms step_avg:57.69ms
step:824/2330 train_time:47536ms step_avg:57.69ms
step:825/2330 train_time:47593ms step_avg:57.69ms
step:826/2330 train_time:47653ms step_avg:57.69ms
step:827/2330 train_time:47710ms step_avg:57.69ms
step:828/2330 train_time:47770ms step_avg:57.69ms
step:829/2330 train_time:47828ms step_avg:57.69ms
step:830/2330 train_time:47887ms step_avg:57.70ms
step:831/2330 train_time:47944ms step_avg:57.69ms
step:832/2330 train_time:48003ms step_avg:57.70ms
step:833/2330 train_time:48060ms step_avg:57.69ms
step:834/2330 train_time:48120ms step_avg:57.70ms
step:835/2330 train_time:48176ms step_avg:57.70ms
step:836/2330 train_time:48236ms step_avg:57.70ms
step:837/2330 train_time:48293ms step_avg:57.70ms
step:838/2330 train_time:48354ms step_avg:57.70ms
step:839/2330 train_time:48411ms step_avg:57.70ms
step:840/2330 train_time:48471ms step_avg:57.70ms
step:841/2330 train_time:48529ms step_avg:57.70ms
step:842/2330 train_time:48589ms step_avg:57.71ms
step:843/2330 train_time:48647ms step_avg:57.71ms
step:844/2330 train_time:48707ms step_avg:57.71ms
step:845/2330 train_time:48764ms step_avg:57.71ms
step:846/2330 train_time:48823ms step_avg:57.71ms
step:847/2330 train_time:48880ms step_avg:57.71ms
step:848/2330 train_time:48940ms step_avg:57.71ms
step:849/2330 train_time:48997ms step_avg:57.71ms
step:850/2330 train_time:49056ms step_avg:57.71ms
step:851/2330 train_time:49113ms step_avg:57.71ms
step:852/2330 train_time:49173ms step_avg:57.71ms
step:853/2330 train_time:49229ms step_avg:57.71ms
step:854/2330 train_time:49290ms step_avg:57.72ms
step:855/2330 train_time:49347ms step_avg:57.72ms
step:856/2330 train_time:49407ms step_avg:57.72ms
step:857/2330 train_time:49464ms step_avg:57.72ms
step:858/2330 train_time:49523ms step_avg:57.72ms
step:859/2330 train_time:49581ms step_avg:57.72ms
step:860/2330 train_time:49641ms step_avg:57.72ms
step:861/2330 train_time:49697ms step_avg:57.72ms
step:862/2330 train_time:49758ms step_avg:57.72ms
step:863/2330 train_time:49814ms step_avg:57.72ms
step:864/2330 train_time:49876ms step_avg:57.73ms
step:865/2330 train_time:49934ms step_avg:57.73ms
step:866/2330 train_time:49993ms step_avg:57.73ms
step:867/2330 train_time:50050ms step_avg:57.73ms
step:868/2330 train_time:50109ms step_avg:57.73ms
step:869/2330 train_time:50166ms step_avg:57.73ms
step:870/2330 train_time:50226ms step_avg:57.73ms
step:871/2330 train_time:50283ms step_avg:57.73ms
step:872/2330 train_time:50343ms step_avg:57.73ms
step:873/2330 train_time:50400ms step_avg:57.73ms
step:874/2330 train_time:50459ms step_avg:57.73ms
step:875/2330 train_time:50516ms step_avg:57.73ms
step:876/2330 train_time:50578ms step_avg:57.74ms
step:877/2330 train_time:50635ms step_avg:57.74ms
step:878/2330 train_time:50696ms step_avg:57.74ms
step:879/2330 train_time:50752ms step_avg:57.74ms
step:880/2330 train_time:50813ms step_avg:57.74ms
step:881/2330 train_time:50871ms step_avg:57.74ms
step:882/2330 train_time:50930ms step_avg:57.74ms
step:883/2330 train_time:50988ms step_avg:57.74ms
step:884/2330 train_time:51047ms step_avg:57.75ms
step:885/2330 train_time:51104ms step_avg:57.75ms
step:886/2330 train_time:51164ms step_avg:57.75ms
step:887/2330 train_time:51220ms step_avg:57.75ms
step:888/2330 train_time:51280ms step_avg:57.75ms
step:889/2330 train_time:51338ms step_avg:57.75ms
step:890/2330 train_time:51397ms step_avg:57.75ms
step:891/2330 train_time:51454ms step_avg:57.75ms
step:892/2330 train_time:51515ms step_avg:57.75ms
step:893/2330 train_time:51572ms step_avg:57.75ms
step:894/2330 train_time:51633ms step_avg:57.75ms
step:895/2330 train_time:51690ms step_avg:57.75ms
step:896/2330 train_time:51750ms step_avg:57.76ms
step:897/2330 train_time:51807ms step_avg:57.76ms
step:898/2330 train_time:51867ms step_avg:57.76ms
step:899/2330 train_time:51924ms step_avg:57.76ms
step:900/2330 train_time:51984ms step_avg:57.76ms
step:901/2330 train_time:52040ms step_avg:57.76ms
step:902/2330 train_time:52101ms step_avg:57.76ms
step:903/2330 train_time:52158ms step_avg:57.76ms
step:904/2330 train_time:52218ms step_avg:57.76ms
step:905/2330 train_time:52275ms step_avg:57.76ms
step:906/2330 train_time:52335ms step_avg:57.77ms
step:907/2330 train_time:52392ms step_avg:57.76ms
step:908/2330 train_time:52452ms step_avg:57.77ms
step:909/2330 train_time:52509ms step_avg:57.77ms
step:910/2330 train_time:52569ms step_avg:57.77ms
step:911/2330 train_time:52626ms step_avg:57.77ms
step:912/2330 train_time:52686ms step_avg:57.77ms
step:913/2330 train_time:52742ms step_avg:57.77ms
step:914/2330 train_time:52803ms step_avg:57.77ms
step:915/2330 train_time:52859ms step_avg:57.77ms
step:916/2330 train_time:52920ms step_avg:57.77ms
step:917/2330 train_time:52976ms step_avg:57.77ms
step:918/2330 train_time:53037ms step_avg:57.77ms
step:919/2330 train_time:53094ms step_avg:57.77ms
step:920/2330 train_time:53155ms step_avg:57.78ms
step:921/2330 train_time:53211ms step_avg:57.78ms
step:922/2330 train_time:53271ms step_avg:57.78ms
step:923/2330 train_time:53329ms step_avg:57.78ms
step:924/2330 train_time:53389ms step_avg:57.78ms
step:925/2330 train_time:53446ms step_avg:57.78ms
step:926/2330 train_time:53506ms step_avg:57.78ms
step:927/2330 train_time:53562ms step_avg:57.78ms
step:928/2330 train_time:53622ms step_avg:57.78ms
step:929/2330 train_time:53679ms step_avg:57.78ms
step:930/2330 train_time:53739ms step_avg:57.78ms
step:931/2330 train_time:53795ms step_avg:57.78ms
step:932/2330 train_time:53856ms step_avg:57.79ms
step:933/2330 train_time:53913ms step_avg:57.78ms
step:934/2330 train_time:53974ms step_avg:57.79ms
step:935/2330 train_time:54031ms step_avg:57.79ms
step:936/2330 train_time:54091ms step_avg:57.79ms
step:937/2330 train_time:54148ms step_avg:57.79ms
step:938/2330 train_time:54208ms step_avg:57.79ms
step:939/2330 train_time:54265ms step_avg:57.79ms
step:940/2330 train_time:54323ms step_avg:57.79ms
step:941/2330 train_time:54381ms step_avg:57.79ms
step:942/2330 train_time:54440ms step_avg:57.79ms
step:943/2330 train_time:54497ms step_avg:57.79ms
step:944/2330 train_time:54558ms step_avg:57.79ms
step:945/2330 train_time:54615ms step_avg:57.79ms
step:946/2330 train_time:54675ms step_avg:57.80ms
step:947/2330 train_time:54733ms step_avg:57.80ms
step:948/2330 train_time:54793ms step_avg:57.80ms
step:949/2330 train_time:54850ms step_avg:57.80ms
step:950/2330 train_time:54910ms step_avg:57.80ms
step:951/2330 train_time:54967ms step_avg:57.80ms
step:952/2330 train_time:55026ms step_avg:57.80ms
step:953/2330 train_time:55083ms step_avg:57.80ms
step:954/2330 train_time:55143ms step_avg:57.80ms
step:955/2330 train_time:55200ms step_avg:57.80ms
step:956/2330 train_time:55260ms step_avg:57.80ms
step:957/2330 train_time:55316ms step_avg:57.80ms
step:958/2330 train_time:55378ms step_avg:57.81ms
step:959/2330 train_time:55434ms step_avg:57.80ms
step:960/2330 train_time:55495ms step_avg:57.81ms
step:961/2330 train_time:55552ms step_avg:57.81ms
step:962/2330 train_time:55614ms step_avg:57.81ms
step:963/2330 train_time:55671ms step_avg:57.81ms
step:964/2330 train_time:55731ms step_avg:57.81ms
step:965/2330 train_time:55789ms step_avg:57.81ms
step:966/2330 train_time:55848ms step_avg:57.81ms
step:967/2330 train_time:55905ms step_avg:57.81ms
step:968/2330 train_time:55965ms step_avg:57.81ms
step:969/2330 train_time:56021ms step_avg:57.81ms
step:970/2330 train_time:56081ms step_avg:57.82ms
step:971/2330 train_time:56138ms step_avg:57.81ms
step:972/2330 train_time:56198ms step_avg:57.82ms
step:973/2330 train_time:56254ms step_avg:57.82ms
step:974/2330 train_time:56315ms step_avg:57.82ms
step:975/2330 train_time:56372ms step_avg:57.82ms
step:976/2330 train_time:56432ms step_avg:57.82ms
step:977/2330 train_time:56489ms step_avg:57.82ms
step:978/2330 train_time:56549ms step_avg:57.82ms
step:979/2330 train_time:56605ms step_avg:57.82ms
step:980/2330 train_time:56665ms step_avg:57.82ms
step:981/2330 train_time:56722ms step_avg:57.82ms
step:982/2330 train_time:56782ms step_avg:57.82ms
step:983/2330 train_time:56839ms step_avg:57.82ms
step:984/2330 train_time:56900ms step_avg:57.83ms
step:985/2330 train_time:56956ms step_avg:57.82ms
step:986/2330 train_time:57018ms step_avg:57.83ms
step:987/2330 train_time:57075ms step_avg:57.83ms
step:988/2330 train_time:57135ms step_avg:57.83ms
step:989/2330 train_time:57192ms step_avg:57.83ms
step:990/2330 train_time:57252ms step_avg:57.83ms
step:991/2330 train_time:57309ms step_avg:57.83ms
step:992/2330 train_time:57369ms step_avg:57.83ms
step:993/2330 train_time:57426ms step_avg:57.83ms
step:994/2330 train_time:57485ms step_avg:57.83ms
step:995/2330 train_time:57543ms step_avg:57.83ms
step:996/2330 train_time:57601ms step_avg:57.83ms
step:997/2330 train_time:57658ms step_avg:57.83ms
step:998/2330 train_time:57719ms step_avg:57.83ms
step:999/2330 train_time:57776ms step_avg:57.83ms
step:1000/2330 train_time:57836ms step_avg:57.84ms
step:1000/2330 val_loss:4.0790 train_time:57917ms step_avg:57.92ms
step:1001/2330 train_time:57935ms step_avg:57.88ms
step:1002/2330 train_time:57955ms step_avg:57.84ms
step:1003/2330 train_time:58009ms step_avg:57.84ms
step:1004/2330 train_time:58078ms step_avg:57.85ms
step:1005/2330 train_time:58134ms step_avg:57.84ms
step:1006/2330 train_time:58195ms step_avg:57.85ms
step:1007/2330 train_time:58252ms step_avg:57.85ms
step:1008/2330 train_time:58311ms step_avg:57.85ms
step:1009/2330 train_time:58367ms step_avg:57.85ms
step:1010/2330 train_time:58426ms step_avg:57.85ms
step:1011/2330 train_time:58482ms step_avg:57.85ms
step:1012/2330 train_time:58541ms step_avg:57.85ms
step:1013/2330 train_time:58597ms step_avg:57.85ms
step:1014/2330 train_time:58656ms step_avg:57.85ms
step:1015/2330 train_time:58712ms step_avg:57.84ms
step:1016/2330 train_time:58771ms step_avg:57.85ms
step:1017/2330 train_time:58832ms step_avg:57.85ms
step:1018/2330 train_time:58894ms step_avg:57.85ms
step:1019/2330 train_time:58953ms step_avg:57.85ms
step:1020/2330 train_time:59013ms step_avg:57.86ms
step:1021/2330 train_time:59070ms step_avg:57.86ms
step:1022/2330 train_time:59132ms step_avg:57.86ms
step:1023/2330 train_time:59188ms step_avg:57.86ms
step:1024/2330 train_time:59249ms step_avg:57.86ms
step:1025/2330 train_time:59305ms step_avg:57.86ms
step:1026/2330 train_time:59365ms step_avg:57.86ms
step:1027/2330 train_time:59421ms step_avg:57.86ms
step:1028/2330 train_time:59480ms step_avg:57.86ms
step:1029/2330 train_time:59536ms step_avg:57.86ms
step:1030/2330 train_time:59596ms step_avg:57.86ms
step:1031/2330 train_time:59652ms step_avg:57.86ms
step:1032/2330 train_time:59712ms step_avg:57.86ms
step:1033/2330 train_time:59769ms step_avg:57.86ms
step:1034/2330 train_time:59829ms step_avg:57.86ms
step:1035/2330 train_time:59888ms step_avg:57.86ms
step:1036/2330 train_time:59949ms step_avg:57.87ms
step:1037/2330 train_time:60006ms step_avg:57.86ms
step:1038/2330 train_time:60068ms step_avg:57.87ms
step:1039/2330 train_time:60126ms step_avg:57.87ms
step:1040/2330 train_time:60186ms step_avg:57.87ms
step:1041/2330 train_time:60243ms step_avg:57.87ms
step:1042/2330 train_time:60303ms step_avg:57.87ms
step:1043/2330 train_time:60359ms step_avg:57.87ms
step:1044/2330 train_time:60419ms step_avg:57.87ms
step:1045/2330 train_time:60475ms step_avg:57.87ms
step:1046/2330 train_time:60535ms step_avg:57.87ms
step:1047/2330 train_time:60591ms step_avg:57.87ms
step:1048/2330 train_time:60650ms step_avg:57.87ms
step:1049/2330 train_time:60707ms step_avg:57.87ms
step:1050/2330 train_time:60767ms step_avg:57.87ms
step:1051/2330 train_time:60825ms step_avg:57.87ms
step:1052/2330 train_time:60885ms step_avg:57.88ms
step:1053/2330 train_time:60942ms step_avg:57.87ms
step:1054/2330 train_time:61004ms step_avg:57.88ms
step:1055/2330 train_time:61060ms step_avg:57.88ms
step:1056/2330 train_time:61123ms step_avg:57.88ms
step:1057/2330 train_time:61180ms step_avg:57.88ms
step:1058/2330 train_time:61240ms step_avg:57.88ms
step:1059/2330 train_time:61297ms step_avg:57.88ms
step:1060/2330 train_time:61356ms step_avg:57.88ms
step:1061/2330 train_time:61413ms step_avg:57.88ms
step:1062/2330 train_time:61472ms step_avg:57.88ms
step:1063/2330 train_time:61529ms step_avg:57.88ms
step:1064/2330 train_time:61588ms step_avg:57.88ms
step:1065/2330 train_time:61645ms step_avg:57.88ms
step:1066/2330 train_time:61704ms step_avg:57.88ms
step:1067/2330 train_time:61761ms step_avg:57.88ms
step:1068/2330 train_time:61822ms step_avg:57.89ms
step:1069/2330 train_time:61880ms step_avg:57.89ms
step:1070/2330 train_time:61940ms step_avg:57.89ms
step:1071/2330 train_time:61998ms step_avg:57.89ms
step:1072/2330 train_time:62058ms step_avg:57.89ms
step:1073/2330 train_time:62115ms step_avg:57.89ms
step:1074/2330 train_time:62175ms step_avg:57.89ms
step:1075/2330 train_time:62233ms step_avg:57.89ms
step:1076/2330 train_time:62292ms step_avg:57.89ms
step:1077/2330 train_time:62349ms step_avg:57.89ms
step:1078/2330 train_time:62408ms step_avg:57.89ms
step:1079/2330 train_time:62465ms step_avg:57.89ms
step:1080/2330 train_time:62524ms step_avg:57.89ms
step:1081/2330 train_time:62581ms step_avg:57.89ms
step:1082/2330 train_time:62643ms step_avg:57.90ms
step:1083/2330 train_time:62700ms step_avg:57.89ms
step:1084/2330 train_time:62759ms step_avg:57.90ms
step:1085/2330 train_time:62816ms step_avg:57.90ms
step:1086/2330 train_time:62876ms step_avg:57.90ms
step:1087/2330 train_time:62934ms step_avg:57.90ms
step:1088/2330 train_time:62994ms step_avg:57.90ms
step:1089/2330 train_time:63051ms step_avg:57.90ms
step:1090/2330 train_time:63110ms step_avg:57.90ms
step:1091/2330 train_time:63167ms step_avg:57.90ms
step:1092/2330 train_time:63228ms step_avg:57.90ms
step:1093/2330 train_time:63284ms step_avg:57.90ms
step:1094/2330 train_time:63345ms step_avg:57.90ms
step:1095/2330 train_time:63401ms step_avg:57.90ms
step:1096/2330 train_time:63461ms step_avg:57.90ms
step:1097/2330 train_time:63518ms step_avg:57.90ms
step:1098/2330 train_time:63578ms step_avg:57.90ms
step:1099/2330 train_time:63635ms step_avg:57.90ms
step:1100/2330 train_time:63695ms step_avg:57.90ms
step:1101/2330 train_time:63752ms step_avg:57.90ms
step:1102/2330 train_time:63811ms step_avg:57.91ms
step:1103/2330 train_time:63869ms step_avg:57.90ms
step:1104/2330 train_time:63928ms step_avg:57.91ms
step:1105/2330 train_time:63985ms step_avg:57.91ms
step:1106/2330 train_time:64047ms step_avg:57.91ms
step:1107/2330 train_time:64104ms step_avg:57.91ms
step:1108/2330 train_time:64164ms step_avg:57.91ms
step:1109/2330 train_time:64221ms step_avg:57.91ms
step:1110/2330 train_time:64283ms step_avg:57.91ms
step:1111/2330 train_time:64340ms step_avg:57.91ms
step:1112/2330 train_time:64399ms step_avg:57.91ms
step:1113/2330 train_time:64456ms step_avg:57.91ms
step:1114/2330 train_time:64515ms step_avg:57.91ms
step:1115/2330 train_time:64572ms step_avg:57.91ms
step:1116/2330 train_time:64632ms step_avg:57.91ms
step:1117/2330 train_time:64688ms step_avg:57.91ms
step:1118/2330 train_time:64747ms step_avg:57.91ms
step:1119/2330 train_time:64804ms step_avg:57.91ms
step:1120/2330 train_time:64865ms step_avg:57.91ms
step:1121/2330 train_time:64921ms step_avg:57.91ms
step:1122/2330 train_time:64983ms step_avg:57.92ms
step:1123/2330 train_time:65040ms step_avg:57.92ms
step:1124/2330 train_time:65102ms step_avg:57.92ms
step:1125/2330 train_time:65158ms step_avg:57.92ms
step:1126/2330 train_time:65219ms step_avg:57.92ms
step:1127/2330 train_time:65276ms step_avg:57.92ms
step:1128/2330 train_time:65335ms step_avg:57.92ms
step:1129/2330 train_time:65392ms step_avg:57.92ms
step:1130/2330 train_time:65452ms step_avg:57.92ms
step:1131/2330 train_time:65508ms step_avg:57.92ms
step:1132/2330 train_time:65569ms step_avg:57.92ms
step:1133/2330 train_time:65626ms step_avg:57.92ms
step:1134/2330 train_time:65685ms step_avg:57.92ms
step:1135/2330 train_time:65742ms step_avg:57.92ms
step:1136/2330 train_time:65803ms step_avg:57.93ms
step:1137/2330 train_time:65860ms step_avg:57.92ms
step:1138/2330 train_time:65920ms step_avg:57.93ms
step:1139/2330 train_time:65977ms step_avg:57.93ms
step:1140/2330 train_time:66038ms step_avg:57.93ms
step:1141/2330 train_time:66095ms step_avg:57.93ms
step:1142/2330 train_time:66156ms step_avg:57.93ms
step:1143/2330 train_time:66213ms step_avg:57.93ms
step:1144/2330 train_time:66272ms step_avg:57.93ms
step:1145/2330 train_time:66329ms step_avg:57.93ms
step:1146/2330 train_time:66774ms step_avg:58.27ms
step:1147/2330 train_time:66829ms step_avg:58.26ms
step:1148/2330 train_time:66888ms step_avg:58.27ms
step:1149/2330 train_time:66945ms step_avg:58.26ms
step:1150/2330 train_time:67004ms step_avg:58.26ms
step:1151/2330 train_time:67059ms step_avg:58.26ms
step:1152/2330 train_time:67119ms step_avg:58.26ms
step:1153/2330 train_time:67175ms step_avg:58.26ms
step:1154/2330 train_time:67234ms step_avg:58.26ms
step:1155/2330 train_time:67290ms step_avg:58.26ms
step:1156/2330 train_time:67349ms step_avg:58.26ms
step:1157/2330 train_time:67405ms step_avg:58.26ms
step:1158/2330 train_time:67465ms step_avg:58.26ms
step:1159/2330 train_time:67521ms step_avg:58.26ms
step:1160/2330 train_time:67580ms step_avg:58.26ms
step:1161/2330 train_time:67642ms step_avg:58.26ms
step:1162/2330 train_time:67706ms step_avg:58.27ms
step:1163/2330 train_time:67764ms step_avg:58.27ms
step:1164/2330 train_time:67827ms step_avg:58.27ms
step:1165/2330 train_time:67884ms step_avg:58.27ms
step:1166/2330 train_time:67944ms step_avg:58.27ms
step:1167/2330 train_time:68000ms step_avg:58.27ms
step:1168/2330 train_time:68061ms step_avg:58.27ms
step:1169/2330 train_time:68117ms step_avg:58.27ms
step:1170/2330 train_time:68177ms step_avg:58.27ms
step:1171/2330 train_time:68233ms step_avg:58.27ms
step:1172/2330 train_time:68292ms step_avg:58.27ms
step:1173/2330 train_time:68349ms step_avg:58.27ms
step:1174/2330 train_time:68408ms step_avg:58.27ms
step:1175/2330 train_time:68464ms step_avg:58.27ms
step:1176/2330 train_time:68524ms step_avg:58.27ms
step:1177/2330 train_time:68582ms step_avg:58.27ms
step:1178/2330 train_time:68644ms step_avg:58.27ms
step:1179/2330 train_time:68702ms step_avg:58.27ms
step:1180/2330 train_time:68764ms step_avg:58.27ms
step:1181/2330 train_time:68822ms step_avg:58.27ms
step:1182/2330 train_time:68885ms step_avg:58.28ms
step:1183/2330 train_time:68941ms step_avg:58.28ms
step:1184/2330 train_time:69001ms step_avg:58.28ms
step:1185/2330 train_time:69058ms step_avg:58.28ms
step:1186/2330 train_time:69118ms step_avg:58.28ms
step:1187/2330 train_time:69174ms step_avg:58.28ms
step:1188/2330 train_time:69234ms step_avg:58.28ms
step:1189/2330 train_time:69290ms step_avg:58.28ms
step:1190/2330 train_time:69349ms step_avg:58.28ms
step:1191/2330 train_time:69405ms step_avg:58.27ms
step:1192/2330 train_time:69465ms step_avg:58.28ms
step:1193/2330 train_time:69521ms step_avg:58.27ms
step:1194/2330 train_time:69582ms step_avg:58.28ms
step:1195/2330 train_time:69640ms step_avg:58.28ms
step:1196/2330 train_time:69700ms step_avg:58.28ms
step:1197/2330 train_time:69759ms step_avg:58.28ms
step:1198/2330 train_time:69820ms step_avg:58.28ms
step:1199/2330 train_time:69877ms step_avg:58.28ms
step:1200/2330 train_time:69937ms step_avg:58.28ms
step:1201/2330 train_time:69995ms step_avg:58.28ms
step:1202/2330 train_time:70055ms step_avg:58.28ms
step:1203/2330 train_time:70111ms step_avg:58.28ms
step:1204/2330 train_time:70170ms step_avg:58.28ms
step:1205/2330 train_time:70227ms step_avg:58.28ms
step:1206/2330 train_time:70287ms step_avg:58.28ms
step:1207/2330 train_time:70343ms step_avg:58.28ms
step:1208/2330 train_time:70402ms step_avg:58.28ms
step:1209/2330 train_time:70459ms step_avg:58.28ms
step:1210/2330 train_time:70519ms step_avg:58.28ms
step:1211/2330 train_time:70576ms step_avg:58.28ms
step:1212/2330 train_time:70636ms step_avg:58.28ms
step:1213/2330 train_time:70694ms step_avg:58.28ms
step:1214/2330 train_time:70753ms step_avg:58.28ms
step:1215/2330 train_time:70811ms step_avg:58.28ms
step:1216/2330 train_time:70870ms step_avg:58.28ms
step:1217/2330 train_time:70927ms step_avg:58.28ms
step:1218/2330 train_time:70988ms step_avg:58.28ms
step:1219/2330 train_time:71044ms step_avg:58.28ms
step:1220/2330 train_time:71105ms step_avg:58.28ms
step:1221/2330 train_time:71161ms step_avg:58.28ms
step:1222/2330 train_time:71222ms step_avg:58.28ms
step:1223/2330 train_time:71279ms step_avg:58.28ms
step:1224/2330 train_time:71339ms step_avg:58.28ms
step:1225/2330 train_time:71396ms step_avg:58.28ms
step:1226/2330 train_time:71455ms step_avg:58.28ms
step:1227/2330 train_time:71511ms step_avg:58.28ms
step:1228/2330 train_time:71571ms step_avg:58.28ms
step:1229/2330 train_time:71628ms step_avg:58.28ms
step:1230/2330 train_time:71688ms step_avg:58.28ms
step:1231/2330 train_time:71745ms step_avg:58.28ms
step:1232/2330 train_time:71806ms step_avg:58.28ms
step:1233/2330 train_time:71863ms step_avg:58.28ms
step:1234/2330 train_time:71925ms step_avg:58.29ms
step:1235/2330 train_time:71982ms step_avg:58.29ms
step:1236/2330 train_time:72043ms step_avg:58.29ms
step:1237/2330 train_time:72100ms step_avg:58.29ms
step:1238/2330 train_time:72161ms step_avg:58.29ms
step:1239/2330 train_time:72218ms step_avg:58.29ms
step:1240/2330 train_time:72278ms step_avg:58.29ms
step:1241/2330 train_time:72335ms step_avg:58.29ms
step:1242/2330 train_time:72394ms step_avg:58.29ms
step:1243/2330 train_time:72451ms step_avg:58.29ms
step:1244/2330 train_time:72510ms step_avg:58.29ms
step:1245/2330 train_time:72566ms step_avg:58.29ms
step:1246/2330 train_time:72627ms step_avg:58.29ms
step:1247/2330 train_time:72685ms step_avg:58.29ms
step:1248/2330 train_time:72745ms step_avg:58.29ms
step:1249/2330 train_time:72802ms step_avg:58.29ms
step:1250/2330 train_time:72863ms step_avg:58.29ms
step:1250/2330 val_loss:3.9936 train_time:72943ms step_avg:58.35ms
step:1251/2330 train_time:72961ms step_avg:58.32ms
step:1252/2330 train_time:72983ms step_avg:58.29ms
step:1253/2330 train_time:73044ms step_avg:58.30ms
step:1254/2330 train_time:73108ms step_avg:58.30ms
step:1255/2330 train_time:73165ms step_avg:58.30ms
step:1256/2330 train_time:73227ms step_avg:58.30ms
step:1257/2330 train_time:73283ms step_avg:58.30ms
step:1258/2330 train_time:73344ms step_avg:58.30ms
step:1259/2330 train_time:73401ms step_avg:58.30ms
step:1260/2330 train_time:73460ms step_avg:58.30ms
step:1261/2330 train_time:73517ms step_avg:58.30ms
step:1262/2330 train_time:73575ms step_avg:58.30ms
step:1263/2330 train_time:73631ms step_avg:58.30ms
step:1264/2330 train_time:73690ms step_avg:58.30ms
step:1265/2330 train_time:73746ms step_avg:58.30ms
step:1266/2330 train_time:73806ms step_avg:58.30ms
step:1267/2330 train_time:73863ms step_avg:58.30ms
step:1268/2330 train_time:73923ms step_avg:58.30ms
step:1269/2330 train_time:73983ms step_avg:58.30ms
step:1270/2330 train_time:74044ms step_avg:58.30ms
step:1271/2330 train_time:74103ms step_avg:58.30ms
step:1272/2330 train_time:74164ms step_avg:58.30ms
step:1273/2330 train_time:74221ms step_avg:58.30ms
step:1274/2330 train_time:74282ms step_avg:58.31ms
step:1275/2330 train_time:74338ms step_avg:58.30ms
step:1276/2330 train_time:74398ms step_avg:58.31ms
step:1277/2330 train_time:74454ms step_avg:58.30ms
step:1278/2330 train_time:74514ms step_avg:58.30ms
step:1279/2330 train_time:74570ms step_avg:58.30ms
step:1280/2330 train_time:74629ms step_avg:58.30ms
step:1281/2330 train_time:74685ms step_avg:58.30ms
step:1282/2330 train_time:74745ms step_avg:58.30ms
step:1283/2330 train_time:74802ms step_avg:58.30ms
step:1284/2330 train_time:74861ms step_avg:58.30ms
step:1285/2330 train_time:74919ms step_avg:58.30ms
step:1286/2330 train_time:74980ms step_avg:58.30ms
step:1287/2330 train_time:75038ms step_avg:58.30ms
step:1288/2330 train_time:75098ms step_avg:58.31ms
step:1289/2330 train_time:75155ms step_avg:58.31ms
step:1290/2330 train_time:75215ms step_avg:58.31ms
step:1291/2330 train_time:75272ms step_avg:58.31ms
step:1292/2330 train_time:75332ms step_avg:58.31ms
step:1293/2330 train_time:75388ms step_avg:58.30ms
step:1294/2330 train_time:75449ms step_avg:58.31ms
step:1295/2330 train_time:75505ms step_avg:58.31ms
step:1296/2330 train_time:75564ms step_avg:58.31ms
step:1297/2330 train_time:75620ms step_avg:58.30ms
step:1298/2330 train_time:75682ms step_avg:58.31ms
step:1299/2330 train_time:75738ms step_avg:58.30ms
step:1300/2330 train_time:75797ms step_avg:58.31ms
step:1301/2330 train_time:75854ms step_avg:58.30ms
step:1302/2330 train_time:75913ms step_avg:58.31ms
step:1303/2330 train_time:75970ms step_avg:58.30ms
step:1304/2330 train_time:76031ms step_avg:58.31ms
step:1305/2330 train_time:76089ms step_avg:58.31ms
step:1306/2330 train_time:76149ms step_avg:58.31ms
step:1307/2330 train_time:76208ms step_avg:58.31ms
step:1308/2330 train_time:76267ms step_avg:58.31ms
step:1309/2330 train_time:76323ms step_avg:58.31ms
step:1310/2330 train_time:76385ms step_avg:58.31ms
step:1311/2330 train_time:76442ms step_avg:58.31ms
step:1312/2330 train_time:76501ms step_avg:58.31ms
step:1313/2330 train_time:76557ms step_avg:58.31ms
step:1314/2330 train_time:76617ms step_avg:58.31ms
step:1315/2330 train_time:76674ms step_avg:58.31ms
step:1316/2330 train_time:76733ms step_avg:58.31ms
step:1317/2330 train_time:76789ms step_avg:58.31ms
step:1318/2330 train_time:76849ms step_avg:58.31ms
step:1319/2330 train_time:76906ms step_avg:58.31ms
step:1320/2330 train_time:76966ms step_avg:58.31ms
step:1321/2330 train_time:77023ms step_avg:58.31ms
step:1322/2330 train_time:77084ms step_avg:58.31ms
step:1323/2330 train_time:77141ms step_avg:58.31ms
step:1324/2330 train_time:77202ms step_avg:58.31ms
step:1325/2330 train_time:77260ms step_avg:58.31ms
step:1326/2330 train_time:77320ms step_avg:58.31ms
step:1327/2330 train_time:77377ms step_avg:58.31ms
step:1328/2330 train_time:77436ms step_avg:58.31ms
step:1329/2330 train_time:77492ms step_avg:58.31ms
step:1330/2330 train_time:77552ms step_avg:58.31ms
step:1331/2330 train_time:77608ms step_avg:58.31ms
step:1332/2330 train_time:77669ms step_avg:58.31ms
step:1333/2330 train_time:77725ms step_avg:58.31ms
step:1334/2330 train_time:77786ms step_avg:58.31ms
step:1335/2330 train_time:77842ms step_avg:58.31ms
step:1336/2330 train_time:77902ms step_avg:58.31ms
step:1337/2330 train_time:77960ms step_avg:58.31ms
step:1338/2330 train_time:78020ms step_avg:58.31ms
step:1339/2330 train_time:78077ms step_avg:58.31ms
step:1340/2330 train_time:78136ms step_avg:58.31ms
step:1341/2330 train_time:78194ms step_avg:58.31ms
step:1342/2330 train_time:78253ms step_avg:58.31ms
step:1343/2330 train_time:78310ms step_avg:58.31ms
step:1344/2330 train_time:78371ms step_avg:58.31ms
step:1345/2330 train_time:78427ms step_avg:58.31ms
step:1346/2330 train_time:78488ms step_avg:58.31ms
step:1347/2330 train_time:78544ms step_avg:58.31ms
step:1348/2330 train_time:78605ms step_avg:58.31ms
step:1349/2330 train_time:78662ms step_avg:58.31ms
step:1350/2330 train_time:78722ms step_avg:58.31ms
step:1351/2330 train_time:78778ms step_avg:58.31ms
step:1352/2330 train_time:78838ms step_avg:58.31ms
step:1353/2330 train_time:78895ms step_avg:58.31ms
step:1354/2330 train_time:78954ms step_avg:58.31ms
step:1355/2330 train_time:79011ms step_avg:58.31ms
step:1356/2330 train_time:79072ms step_avg:58.31ms
step:1357/2330 train_time:79128ms step_avg:58.31ms
step:1358/2330 train_time:79189ms step_avg:58.31ms
step:1359/2330 train_time:79246ms step_avg:58.31ms
step:1360/2330 train_time:79307ms step_avg:58.31ms
step:1361/2330 train_time:79365ms step_avg:58.31ms
step:1362/2330 train_time:79425ms step_avg:58.31ms
step:1363/2330 train_time:79482ms step_avg:58.31ms
step:1364/2330 train_time:79542ms step_avg:58.31ms
step:1365/2330 train_time:79599ms step_avg:58.31ms
step:1366/2330 train_time:79659ms step_avg:58.32ms
step:1367/2330 train_time:79716ms step_avg:58.31ms
step:1368/2330 train_time:79775ms step_avg:58.31ms
step:1369/2330 train_time:79831ms step_avg:58.31ms
step:1370/2330 train_time:79891ms step_avg:58.31ms
step:1371/2330 train_time:79948ms step_avg:58.31ms
step:1372/2330 train_time:80009ms step_avg:58.32ms
step:1373/2330 train_time:80065ms step_avg:58.31ms
step:1374/2330 train_time:80126ms step_avg:58.32ms
step:1375/2330 train_time:80183ms step_avg:58.31ms
step:1376/2330 train_time:80243ms step_avg:58.32ms
step:1377/2330 train_time:80300ms step_avg:58.32ms
step:1378/2330 train_time:80360ms step_avg:58.32ms
step:1379/2330 train_time:80417ms step_avg:58.32ms
step:1380/2330 train_time:80477ms step_avg:58.32ms
step:1381/2330 train_time:80534ms step_avg:58.32ms
step:1382/2330 train_time:80593ms step_avg:58.32ms
step:1383/2330 train_time:80651ms step_avg:58.32ms
step:1384/2330 train_time:80710ms step_avg:58.32ms
step:1385/2330 train_time:80767ms step_avg:58.32ms
step:1386/2330 train_time:80827ms step_avg:58.32ms
step:1387/2330 train_time:80884ms step_avg:58.32ms
step:1388/2330 train_time:80944ms step_avg:58.32ms
step:1389/2330 train_time:81002ms step_avg:58.32ms
step:1390/2330 train_time:81061ms step_avg:58.32ms
step:1391/2330 train_time:81120ms step_avg:58.32ms
step:1392/2330 train_time:81179ms step_avg:58.32ms
step:1393/2330 train_time:81235ms step_avg:58.32ms
step:1394/2330 train_time:81295ms step_avg:58.32ms
step:1395/2330 train_time:81351ms step_avg:58.32ms
step:1396/2330 train_time:81412ms step_avg:58.32ms
step:1397/2330 train_time:81469ms step_avg:58.32ms
step:1398/2330 train_time:81529ms step_avg:58.32ms
step:1399/2330 train_time:81585ms step_avg:58.32ms
step:1400/2330 train_time:81646ms step_avg:58.32ms
step:1401/2330 train_time:81703ms step_avg:58.32ms
step:1402/2330 train_time:81763ms step_avg:58.32ms
step:1403/2330 train_time:81820ms step_avg:58.32ms
step:1404/2330 train_time:81880ms step_avg:58.32ms
step:1405/2330 train_time:81937ms step_avg:58.32ms
step:1406/2330 train_time:81997ms step_avg:58.32ms
step:1407/2330 train_time:82054ms step_avg:58.32ms
step:1408/2330 train_time:82113ms step_avg:58.32ms
step:1409/2330 train_time:82169ms step_avg:58.32ms
step:1410/2330 train_time:82230ms step_avg:58.32ms
step:1411/2330 train_time:82287ms step_avg:58.32ms
step:1412/2330 train_time:82349ms step_avg:58.32ms
step:1413/2330 train_time:82405ms step_avg:58.32ms
step:1414/2330 train_time:82466ms step_avg:58.32ms
step:1415/2330 train_time:82523ms step_avg:58.32ms
step:1416/2330 train_time:82583ms step_avg:58.32ms
step:1417/2330 train_time:82640ms step_avg:58.32ms
step:1418/2330 train_time:82700ms step_avg:58.32ms
step:1419/2330 train_time:82758ms step_avg:58.32ms
step:1420/2330 train_time:82817ms step_avg:58.32ms
step:1421/2330 train_time:82874ms step_avg:58.32ms
step:1422/2330 train_time:82933ms step_avg:58.32ms
step:1423/2330 train_time:82990ms step_avg:58.32ms
step:1424/2330 train_time:83049ms step_avg:58.32ms
step:1425/2330 train_time:83106ms step_avg:58.32ms
step:1426/2330 train_time:83166ms step_avg:58.32ms
step:1427/2330 train_time:83224ms step_avg:58.32ms
step:1428/2330 train_time:83284ms step_avg:58.32ms
step:1429/2330 train_time:83340ms step_avg:58.32ms
step:1430/2330 train_time:83401ms step_avg:58.32ms
step:1431/2330 train_time:83458ms step_avg:58.32ms
step:1432/2330 train_time:83518ms step_avg:58.32ms
step:1433/2330 train_time:83576ms step_avg:58.32ms
step:1434/2330 train_time:83634ms step_avg:58.32ms
step:1435/2330 train_time:83691ms step_avg:58.32ms
step:1436/2330 train_time:83751ms step_avg:58.32ms
step:1437/2330 train_time:83808ms step_avg:58.32ms
step:1438/2330 train_time:83868ms step_avg:58.32ms
step:1439/2330 train_time:83924ms step_avg:58.32ms
step:1440/2330 train_time:83985ms step_avg:58.32ms
step:1441/2330 train_time:84042ms step_avg:58.32ms
step:1442/2330 train_time:84101ms step_avg:58.32ms
step:1443/2330 train_time:84159ms step_avg:58.32ms
step:1444/2330 train_time:84219ms step_avg:58.32ms
step:1445/2330 train_time:84276ms step_avg:58.32ms
step:1446/2330 train_time:84335ms step_avg:58.32ms
step:1447/2330 train_time:84392ms step_avg:58.32ms
step:1448/2330 train_time:84451ms step_avg:58.32ms
step:1449/2330 train_time:84508ms step_avg:58.32ms
step:1450/2330 train_time:84569ms step_avg:58.32ms
step:1451/2330 train_time:84626ms step_avg:58.32ms
step:1452/2330 train_time:84687ms step_avg:58.32ms
step:1453/2330 train_time:84744ms step_avg:58.32ms
step:1454/2330 train_time:84804ms step_avg:58.32ms
step:1455/2330 train_time:84861ms step_avg:58.32ms
step:1456/2330 train_time:84921ms step_avg:58.33ms
step:1457/2330 train_time:84978ms step_avg:58.32ms
step:1458/2330 train_time:85038ms step_avg:58.33ms
step:1459/2330 train_time:85096ms step_avg:58.32ms
step:1460/2330 train_time:85155ms step_avg:58.33ms
step:1461/2330 train_time:85212ms step_avg:58.32ms
step:1462/2330 train_time:85272ms step_avg:58.33ms
step:1463/2330 train_time:85329ms step_avg:58.32ms
step:1464/2330 train_time:85388ms step_avg:58.33ms
step:1465/2330 train_time:85445ms step_avg:58.32ms
step:1466/2330 train_time:85506ms step_avg:58.33ms
step:1467/2330 train_time:85563ms step_avg:58.33ms
step:1468/2330 train_time:85623ms step_avg:58.33ms
step:1469/2330 train_time:85680ms step_avg:58.33ms
step:1470/2330 train_time:85741ms step_avg:58.33ms
step:1471/2330 train_time:85797ms step_avg:58.33ms
step:1472/2330 train_time:85857ms step_avg:58.33ms
step:1473/2330 train_time:85914ms step_avg:58.33ms
step:1474/2330 train_time:85974ms step_avg:58.33ms
step:1475/2330 train_time:86030ms step_avg:58.33ms
step:1476/2330 train_time:86091ms step_avg:58.33ms
step:1477/2330 train_time:86147ms step_avg:58.33ms
step:1478/2330 train_time:86208ms step_avg:58.33ms
step:1479/2330 train_time:86265ms step_avg:58.33ms
step:1480/2330 train_time:86325ms step_avg:58.33ms
step:1481/2330 train_time:86382ms step_avg:58.33ms
step:1482/2330 train_time:86442ms step_avg:58.33ms
step:1483/2330 train_time:86500ms step_avg:58.33ms
step:1484/2330 train_time:86560ms step_avg:58.33ms
step:1485/2330 train_time:86617ms step_avg:58.33ms
step:1486/2330 train_time:86676ms step_avg:58.33ms
step:1487/2330 train_time:86733ms step_avg:58.33ms
step:1488/2330 train_time:86792ms step_avg:58.33ms
step:1489/2330 train_time:86849ms step_avg:58.33ms
step:1490/2330 train_time:86909ms step_avg:58.33ms
step:1491/2330 train_time:86966ms step_avg:58.33ms
step:1492/2330 train_time:87027ms step_avg:58.33ms
step:1493/2330 train_time:87083ms step_avg:58.33ms
step:1494/2330 train_time:87144ms step_avg:58.33ms
step:1495/2330 train_time:87202ms step_avg:58.33ms
step:1496/2330 train_time:87261ms step_avg:58.33ms
step:1497/2330 train_time:87319ms step_avg:58.33ms
step:1498/2330 train_time:87379ms step_avg:58.33ms
step:1499/2330 train_time:87436ms step_avg:58.33ms
step:1500/2330 train_time:87495ms step_avg:58.33ms
step:1500/2330 val_loss:3.9097 train_time:87575ms step_avg:58.38ms
step:1501/2330 train_time:87593ms step_avg:58.36ms
step:1502/2330 train_time:87613ms step_avg:58.33ms
step:1503/2330 train_time:87670ms step_avg:58.33ms
step:1504/2330 train_time:87739ms step_avg:58.34ms
step:1505/2330 train_time:87797ms step_avg:58.34ms
step:1506/2330 train_time:87857ms step_avg:58.34ms
step:1507/2330 train_time:87914ms step_avg:58.34ms
step:1508/2330 train_time:87974ms step_avg:58.34ms
step:1509/2330 train_time:88030ms step_avg:58.34ms
step:1510/2330 train_time:88089ms step_avg:58.34ms
step:1511/2330 train_time:88145ms step_avg:58.34ms
step:1512/2330 train_time:88204ms step_avg:58.34ms
step:1513/2330 train_time:88260ms step_avg:58.33ms
step:1514/2330 train_time:88319ms step_avg:58.33ms
step:1515/2330 train_time:88375ms step_avg:58.33ms
step:1516/2330 train_time:88436ms step_avg:58.33ms
step:1517/2330 train_time:88493ms step_avg:58.33ms
step:1518/2330 train_time:88553ms step_avg:58.34ms
step:1519/2330 train_time:88612ms step_avg:58.34ms
step:1520/2330 train_time:88673ms step_avg:58.34ms
step:1521/2330 train_time:88732ms step_avg:58.34ms
step:1522/2330 train_time:88793ms step_avg:58.34ms
step:1523/2330 train_time:88850ms step_avg:58.34ms
step:1524/2330 train_time:88910ms step_avg:58.34ms
step:1525/2330 train_time:88967ms step_avg:58.34ms
step:1526/2330 train_time:89026ms step_avg:58.34ms
step:1527/2330 train_time:89083ms step_avg:58.34ms
step:1528/2330 train_time:89142ms step_avg:58.34ms
step:1529/2330 train_time:89199ms step_avg:58.34ms
step:1530/2330 train_time:89258ms step_avg:58.34ms
step:1531/2330 train_time:89314ms step_avg:58.34ms
step:1532/2330 train_time:89374ms step_avg:58.34ms
step:1533/2330 train_time:89431ms step_avg:58.34ms
step:1534/2330 train_time:89491ms step_avg:58.34ms
step:1535/2330 train_time:89548ms step_avg:58.34ms
step:1536/2330 train_time:89610ms step_avg:58.34ms
step:1537/2330 train_time:89668ms step_avg:58.34ms
step:1538/2330 train_time:89730ms step_avg:58.34ms
step:1539/2330 train_time:89787ms step_avg:58.34ms
step:1540/2330 train_time:89848ms step_avg:58.34ms
step:1541/2330 train_time:89905ms step_avg:58.34ms
step:1542/2330 train_time:89966ms step_avg:58.34ms
step:1543/2330 train_time:90023ms step_avg:58.34ms
step:1544/2330 train_time:90083ms step_avg:58.34ms
step:1545/2330 train_time:90140ms step_avg:58.34ms
step:1546/2330 train_time:90200ms step_avg:58.34ms
step:1547/2330 train_time:90256ms step_avg:58.34ms
step:1548/2330 train_time:90318ms step_avg:58.34ms
step:1549/2330 train_time:90374ms step_avg:58.34ms
step:1550/2330 train_time:90436ms step_avg:58.35ms
step:1551/2330 train_time:90493ms step_avg:58.34ms
step:1552/2330 train_time:90554ms step_avg:58.35ms
step:1553/2330 train_time:90612ms step_avg:58.35ms
step:1554/2330 train_time:90673ms step_avg:58.35ms
step:1555/2330 train_time:90731ms step_avg:58.35ms
step:1556/2330 train_time:90793ms step_avg:58.35ms
step:1557/2330 train_time:90851ms step_avg:58.35ms
step:1558/2330 train_time:90912ms step_avg:58.35ms
step:1559/2330 train_time:90969ms step_avg:58.35ms
step:1560/2330 train_time:91030ms step_avg:58.35ms
step:1561/2330 train_time:91087ms step_avg:58.35ms
step:1562/2330 train_time:91147ms step_avg:58.35ms
step:1563/2330 train_time:91204ms step_avg:58.35ms
step:1564/2330 train_time:91263ms step_avg:58.35ms
step:1565/2330 train_time:91320ms step_avg:58.35ms
step:1566/2330 train_time:91381ms step_avg:58.35ms
step:1567/2330 train_time:91437ms step_avg:58.35ms
step:1568/2330 train_time:91499ms step_avg:58.35ms
step:1569/2330 train_time:91556ms step_avg:58.35ms
step:1570/2330 train_time:91619ms step_avg:58.36ms
step:1571/2330 train_time:91676ms step_avg:58.36ms
step:1572/2330 train_time:91739ms step_avg:58.36ms
step:1573/2330 train_time:91796ms step_avg:58.36ms
step:1574/2330 train_time:91858ms step_avg:58.36ms
step:1575/2330 train_time:91915ms step_avg:58.36ms
step:1576/2330 train_time:91977ms step_avg:58.36ms
step:1577/2330 train_time:92034ms step_avg:58.36ms
step:1578/2330 train_time:92097ms step_avg:58.36ms
step:1579/2330 train_time:92155ms step_avg:58.36ms
step:1580/2330 train_time:92215ms step_avg:58.36ms
step:1581/2330 train_time:92273ms step_avg:58.36ms
step:1582/2330 train_time:92333ms step_avg:58.36ms
step:1583/2330 train_time:92391ms step_avg:58.36ms
step:1584/2330 train_time:92451ms step_avg:58.37ms
step:1585/2330 train_time:92507ms step_avg:58.36ms
step:1586/2330 train_time:92567ms step_avg:58.36ms
step:1587/2330 train_time:92624ms step_avg:58.36ms
step:1588/2330 train_time:92686ms step_avg:58.37ms
step:1589/2330 train_time:92742ms step_avg:58.37ms
step:1590/2330 train_time:92805ms step_avg:58.37ms
step:1591/2330 train_time:92861ms step_avg:58.37ms
step:1592/2330 train_time:92924ms step_avg:58.37ms
step:1593/2330 train_time:92980ms step_avg:58.37ms
step:1594/2330 train_time:93043ms step_avg:58.37ms
step:1595/2330 train_time:93100ms step_avg:58.37ms
step:1596/2330 train_time:93161ms step_avg:58.37ms
step:1597/2330 train_time:93217ms step_avg:58.37ms
step:1598/2330 train_time:93279ms step_avg:58.37ms
step:1599/2330 train_time:93335ms step_avg:58.37ms
step:1600/2330 train_time:93397ms step_avg:58.37ms
step:1601/2330 train_time:93454ms step_avg:58.37ms
step:1602/2330 train_time:93514ms step_avg:58.37ms
step:1603/2330 train_time:93571ms step_avg:58.37ms
step:1604/2330 train_time:93632ms step_avg:58.37ms
step:1605/2330 train_time:93691ms step_avg:58.37ms
step:1606/2330 train_time:93752ms step_avg:58.38ms
step:1607/2330 train_time:93810ms step_avg:58.38ms
step:1608/2330 train_time:93871ms step_avg:58.38ms
step:1609/2330 train_time:93929ms step_avg:58.38ms
step:1610/2330 train_time:93990ms step_avg:58.38ms
step:1611/2330 train_time:94047ms step_avg:58.38ms
step:1612/2330 train_time:94106ms step_avg:58.38ms
step:1613/2330 train_time:94163ms step_avg:58.38ms
step:1614/2330 train_time:94225ms step_avg:58.38ms
step:1615/2330 train_time:94282ms step_avg:58.38ms
step:1616/2330 train_time:94343ms step_avg:58.38ms
step:1617/2330 train_time:94399ms step_avg:58.38ms
step:1618/2330 train_time:94461ms step_avg:58.38ms
step:1619/2330 train_time:94517ms step_avg:58.38ms
step:1620/2330 train_time:94579ms step_avg:58.38ms
step:1621/2330 train_time:94635ms step_avg:58.38ms
step:1622/2330 train_time:94698ms step_avg:58.38ms
step:1623/2330 train_time:94756ms step_avg:58.38ms
step:1624/2330 train_time:94817ms step_avg:58.38ms
step:1625/2330 train_time:94874ms step_avg:58.38ms
step:1626/2330 train_time:94935ms step_avg:58.39ms
step:1627/2330 train_time:94993ms step_avg:58.39ms
step:1628/2330 train_time:95054ms step_avg:58.39ms
step:1629/2330 train_time:95112ms step_avg:58.39ms
step:1630/2330 train_time:95172ms step_avg:58.39ms
step:1631/2330 train_time:95229ms step_avg:58.39ms
step:1632/2330 train_time:95289ms step_avg:58.39ms
step:1633/2330 train_time:95346ms step_avg:58.39ms
step:1634/2330 train_time:95406ms step_avg:58.39ms
step:1635/2330 train_time:95463ms step_avg:58.39ms
step:1636/2330 train_time:95523ms step_avg:58.39ms
step:1637/2330 train_time:95580ms step_avg:58.39ms
step:1638/2330 train_time:95641ms step_avg:58.39ms
step:1639/2330 train_time:95697ms step_avg:58.39ms
step:1640/2330 train_time:95761ms step_avg:58.39ms
step:1641/2330 train_time:95818ms step_avg:58.39ms
step:1642/2330 train_time:95880ms step_avg:58.39ms
step:1643/2330 train_time:95936ms step_avg:58.39ms
step:1644/2330 train_time:95998ms step_avg:58.39ms
step:1645/2330 train_time:96056ms step_avg:58.39ms
step:1646/2330 train_time:96118ms step_avg:58.40ms
step:1647/2330 train_time:96176ms step_avg:58.39ms
step:1648/2330 train_time:96237ms step_avg:58.40ms
step:1649/2330 train_time:96295ms step_avg:58.40ms
step:1650/2330 train_time:96355ms step_avg:58.40ms
step:1651/2330 train_time:96412ms step_avg:58.40ms
step:1652/2330 train_time:96474ms step_avg:58.40ms
step:1653/2330 train_time:96531ms step_avg:58.40ms
step:1654/2330 train_time:96592ms step_avg:58.40ms
step:1655/2330 train_time:96650ms step_avg:58.40ms
step:1656/2330 train_time:96710ms step_avg:58.40ms
step:1657/2330 train_time:96767ms step_avg:58.40ms
step:1658/2330 train_time:96827ms step_avg:58.40ms
step:1659/2330 train_time:96884ms step_avg:58.40ms
step:1660/2330 train_time:96945ms step_avg:58.40ms
step:1661/2330 train_time:97001ms step_avg:58.40ms
step:1662/2330 train_time:97063ms step_avg:58.40ms
step:1663/2330 train_time:97120ms step_avg:58.40ms
step:1664/2330 train_time:97181ms step_avg:58.40ms
step:1665/2330 train_time:97238ms step_avg:58.40ms
step:1666/2330 train_time:97300ms step_avg:58.40ms
step:1667/2330 train_time:97357ms step_avg:58.40ms
step:1668/2330 train_time:97417ms step_avg:58.40ms
step:1669/2330 train_time:97474ms step_avg:58.40ms
step:1670/2330 train_time:97535ms step_avg:58.40ms
step:1671/2330 train_time:97593ms step_avg:58.40ms
step:1672/2330 train_time:97655ms step_avg:58.41ms
step:1673/2330 train_time:97713ms step_avg:58.41ms
step:1674/2330 train_time:97774ms step_avg:58.41ms
step:1675/2330 train_time:97832ms step_avg:58.41ms
step:1676/2330 train_time:97893ms step_avg:58.41ms
step:1677/2330 train_time:97950ms step_avg:58.41ms
step:1678/2330 train_time:98011ms step_avg:58.41ms
step:1679/2330 train_time:98068ms step_avg:58.41ms
step:1680/2330 train_time:98128ms step_avg:58.41ms
step:1681/2330 train_time:98185ms step_avg:58.41ms
step:1682/2330 train_time:98246ms step_avg:58.41ms
step:1683/2330 train_time:98303ms step_avg:58.41ms
step:1684/2330 train_time:98364ms step_avg:58.41ms
step:1685/2330 train_time:98420ms step_avg:58.41ms
step:1686/2330 train_time:98482ms step_avg:58.41ms
step:1687/2330 train_time:98539ms step_avg:58.41ms
step:1688/2330 train_time:98601ms step_avg:58.41ms
step:1689/2330 train_time:98658ms step_avg:58.41ms
step:1690/2330 train_time:98720ms step_avg:58.41ms
step:1691/2330 train_time:98777ms step_avg:58.41ms
step:1692/2330 train_time:98839ms step_avg:58.42ms
step:1693/2330 train_time:98896ms step_avg:58.41ms
step:1694/2330 train_time:98958ms step_avg:58.42ms
step:1695/2330 train_time:99016ms step_avg:58.42ms
step:1696/2330 train_time:99077ms step_avg:58.42ms
step:1697/2330 train_time:99135ms step_avg:58.42ms
step:1698/2330 train_time:99195ms step_avg:58.42ms
step:1699/2330 train_time:99253ms step_avg:58.42ms
step:1700/2330 train_time:99313ms step_avg:58.42ms
step:1701/2330 train_time:99370ms step_avg:58.42ms
step:1702/2330 train_time:99430ms step_avg:58.42ms
step:1703/2330 train_time:99488ms step_avg:58.42ms
step:1704/2330 train_time:99548ms step_avg:58.42ms
step:1705/2330 train_time:99605ms step_avg:58.42ms
step:1706/2330 train_time:99665ms step_avg:58.42ms
step:1707/2330 train_time:99722ms step_avg:58.42ms
step:1708/2330 train_time:99784ms step_avg:58.42ms
step:1709/2330 train_time:99841ms step_avg:58.42ms
step:1710/2330 train_time:99902ms step_avg:58.42ms
step:1711/2330 train_time:99958ms step_avg:58.42ms
step:1712/2330 train_time:100020ms step_avg:58.42ms
step:1713/2330 train_time:100077ms step_avg:58.42ms
step:1714/2330 train_time:100139ms step_avg:58.42ms
step:1715/2330 train_time:100196ms step_avg:58.42ms
step:1716/2330 train_time:100258ms step_avg:58.43ms
step:1717/2330 train_time:100316ms step_avg:58.43ms
step:1718/2330 train_time:100376ms step_avg:58.43ms
step:1719/2330 train_time:100434ms step_avg:58.43ms
step:1720/2330 train_time:100494ms step_avg:58.43ms
step:1721/2330 train_time:100551ms step_avg:58.43ms
step:1722/2330 train_time:100612ms step_avg:58.43ms
step:1723/2330 train_time:100670ms step_avg:58.43ms
step:1724/2330 train_time:100730ms step_avg:58.43ms
step:1725/2330 train_time:100788ms step_avg:58.43ms
step:1726/2330 train_time:100848ms step_avg:58.43ms
step:1727/2330 train_time:100906ms step_avg:58.43ms
step:1728/2330 train_time:100965ms step_avg:58.43ms
step:1729/2330 train_time:101022ms step_avg:58.43ms
step:1730/2330 train_time:101083ms step_avg:58.43ms
step:1731/2330 train_time:101140ms step_avg:58.43ms
step:1732/2330 train_time:101202ms step_avg:58.43ms
step:1733/2330 train_time:101257ms step_avg:58.43ms
step:1734/2330 train_time:101321ms step_avg:58.43ms
step:1735/2330 train_time:101378ms step_avg:58.43ms
step:1736/2330 train_time:101439ms step_avg:58.43ms
step:1737/2330 train_time:101496ms step_avg:58.43ms
step:1738/2330 train_time:101558ms step_avg:58.43ms
step:1739/2330 train_time:101615ms step_avg:58.43ms
step:1740/2330 train_time:101677ms step_avg:58.44ms
step:1741/2330 train_time:101735ms step_avg:58.43ms
step:1742/2330 train_time:101795ms step_avg:58.44ms
step:1743/2330 train_time:101853ms step_avg:58.44ms
step:1744/2330 train_time:101914ms step_avg:58.44ms
step:1745/2330 train_time:101972ms step_avg:58.44ms
step:1746/2330 train_time:102032ms step_avg:58.44ms
step:1747/2330 train_time:102090ms step_avg:58.44ms
step:1748/2330 train_time:102151ms step_avg:58.44ms
step:1749/2330 train_time:102209ms step_avg:58.44ms
step:1750/2330 train_time:102269ms step_avg:58.44ms
step:1750/2330 val_loss:3.8216 train_time:102350ms step_avg:58.49ms
step:1751/2330 train_time:102369ms step_avg:58.46ms
step:1752/2330 train_time:102389ms step_avg:58.44ms
step:1753/2330 train_time:102448ms step_avg:58.44ms
step:1754/2330 train_time:102513ms step_avg:58.45ms
step:1755/2330 train_time:102570ms step_avg:58.44ms
step:1756/2330 train_time:102630ms step_avg:58.45ms
step:1757/2330 train_time:102687ms step_avg:58.44ms
step:1758/2330 train_time:102747ms step_avg:58.45ms
step:1759/2330 train_time:102803ms step_avg:58.44ms
step:1760/2330 train_time:102865ms step_avg:58.45ms
step:1761/2330 train_time:102921ms step_avg:58.44ms
step:1762/2330 train_time:102982ms step_avg:58.45ms
step:1763/2330 train_time:103038ms step_avg:58.44ms
step:1764/2330 train_time:103098ms step_avg:58.45ms
step:1765/2330 train_time:103154ms step_avg:58.44ms
step:1766/2330 train_time:103214ms step_avg:58.44ms
step:1767/2330 train_time:103273ms step_avg:58.45ms
step:1768/2330 train_time:103334ms step_avg:58.45ms
step:1769/2330 train_time:103395ms step_avg:58.45ms
step:1770/2330 train_time:103456ms step_avg:58.45ms
step:1771/2330 train_time:103514ms step_avg:58.45ms
step:1772/2330 train_time:103574ms step_avg:58.45ms
step:1773/2330 train_time:103633ms step_avg:58.45ms
step:1774/2330 train_time:103692ms step_avg:58.45ms
step:1775/2330 train_time:103750ms step_avg:58.45ms
step:1776/2330 train_time:103809ms step_avg:58.45ms
step:1777/2330 train_time:103866ms step_avg:58.45ms
step:1778/2330 train_time:103926ms step_avg:58.45ms
step:1779/2330 train_time:103983ms step_avg:58.45ms
step:1780/2330 train_time:104043ms step_avg:58.45ms
step:1781/2330 train_time:104099ms step_avg:58.45ms
step:1782/2330 train_time:104160ms step_avg:58.45ms
step:1783/2330 train_time:104217ms step_avg:58.45ms
step:1784/2330 train_time:104278ms step_avg:58.45ms
step:1785/2330 train_time:104336ms step_avg:58.45ms
step:1786/2330 train_time:104399ms step_avg:58.45ms
step:1787/2330 train_time:104458ms step_avg:58.45ms
step:1788/2330 train_time:104521ms step_avg:58.46ms
step:1789/2330 train_time:104578ms step_avg:58.46ms
step:1790/2330 train_time:104640ms step_avg:58.46ms
step:1791/2330 train_time:104698ms step_avg:58.46ms
step:1792/2330 train_time:104758ms step_avg:58.46ms
step:1793/2330 train_time:104817ms step_avg:58.46ms
step:1794/2330 train_time:104876ms step_avg:58.46ms
step:1795/2330 train_time:104933ms step_avg:58.46ms
step:1796/2330 train_time:104992ms step_avg:58.46ms
step:1797/2330 train_time:105048ms step_avg:58.46ms
step:1798/2330 train_time:105108ms step_avg:58.46ms
step:1799/2330 train_time:105165ms step_avg:58.46ms
step:1800/2330 train_time:105225ms step_avg:58.46ms
step:1801/2330 train_time:105282ms step_avg:58.46ms
step:1802/2330 train_time:105345ms step_avg:58.46ms
step:1803/2330 train_time:105403ms step_avg:58.46ms
step:1804/2330 train_time:105465ms step_avg:58.46ms
step:1805/2330 train_time:105522ms step_avg:58.46ms
step:1806/2330 train_time:105584ms step_avg:58.46ms
step:1807/2330 train_time:105641ms step_avg:58.46ms
step:1808/2330 train_time:105704ms step_avg:58.46ms
step:1809/2330 train_time:105761ms step_avg:58.46ms
step:1810/2330 train_time:105822ms step_avg:58.47ms
step:1811/2330 train_time:105880ms step_avg:58.46ms
step:1812/2330 train_time:105940ms step_avg:58.47ms
step:1813/2330 train_time:105997ms step_avg:58.46ms
step:1814/2330 train_time:106057ms step_avg:58.47ms
step:1815/2330 train_time:106115ms step_avg:58.47ms
step:1816/2330 train_time:106175ms step_avg:58.47ms
step:1817/2330 train_time:106233ms step_avg:58.47ms
step:1818/2330 train_time:106293ms step_avg:58.47ms
step:1819/2330 train_time:106351ms step_avg:58.47ms
step:1820/2330 train_time:106411ms step_avg:58.47ms
step:1821/2330 train_time:106469ms step_avg:58.47ms
step:1822/2330 train_time:106530ms step_avg:58.47ms
step:1823/2330 train_time:106587ms step_avg:58.47ms
step:1824/2330 train_time:106648ms step_avg:58.47ms
step:1825/2330 train_time:106705ms step_avg:58.47ms
step:1826/2330 train_time:106767ms step_avg:58.47ms
step:1827/2330 train_time:106823ms step_avg:58.47ms
step:1828/2330 train_time:106885ms step_avg:58.47ms
step:1829/2330 train_time:106941ms step_avg:58.47ms
step:1830/2330 train_time:107003ms step_avg:58.47ms
step:1831/2330 train_time:107060ms step_avg:58.47ms
step:1832/2330 train_time:107122ms step_avg:58.47ms
step:1833/2330 train_time:107179ms step_avg:58.47ms
step:1834/2330 train_time:107240ms step_avg:58.47ms
step:1835/2330 train_time:107298ms step_avg:58.47ms
step:1836/2330 train_time:107358ms step_avg:58.47ms
step:1837/2330 train_time:107418ms step_avg:58.47ms
step:1838/2330 train_time:107479ms step_avg:58.48ms
step:1839/2330 train_time:107537ms step_avg:58.48ms
step:1840/2330 train_time:107598ms step_avg:58.48ms
step:1841/2330 train_time:107656ms step_avg:58.48ms
step:1842/2330 train_time:107717ms step_avg:58.48ms
step:1843/2330 train_time:107776ms step_avg:58.48ms
step:1844/2330 train_time:107836ms step_avg:58.48ms
step:1845/2330 train_time:107893ms step_avg:58.48ms
step:1846/2330 train_time:107952ms step_avg:58.48ms
step:1847/2330 train_time:108009ms step_avg:58.48ms
step:1848/2330 train_time:108070ms step_avg:58.48ms
step:1849/2330 train_time:108126ms step_avg:58.48ms
step:1850/2330 train_time:108187ms step_avg:58.48ms
step:1851/2330 train_time:108243ms step_avg:58.48ms
step:1852/2330 train_time:108305ms step_avg:58.48ms
step:1853/2330 train_time:108361ms step_avg:58.48ms
step:1854/2330 train_time:108424ms step_avg:58.48ms
step:1855/2330 train_time:108481ms step_avg:58.48ms
step:1856/2330 train_time:108543ms step_avg:58.48ms
step:1857/2330 train_time:108601ms step_avg:58.48ms
step:1858/2330 train_time:108663ms step_avg:58.48ms
step:1859/2330 train_time:108720ms step_avg:58.48ms
step:1860/2330 train_time:108782ms step_avg:58.49ms
step:1861/2330 train_time:108840ms step_avg:58.48ms
step:1862/2330 train_time:108901ms step_avg:58.49ms
step:1863/2330 train_time:108958ms step_avg:58.49ms
step:1864/2330 train_time:109018ms step_avg:58.49ms
step:1865/2330 train_time:109075ms step_avg:58.49ms
step:1866/2330 train_time:109136ms step_avg:58.49ms
step:1867/2330 train_time:109194ms step_avg:58.49ms
step:1868/2330 train_time:109254ms step_avg:58.49ms
step:1869/2330 train_time:109312ms step_avg:58.49ms
step:1870/2330 train_time:109372ms step_avg:58.49ms
step:1871/2330 train_time:109429ms step_avg:58.49ms
step:1872/2330 train_time:109488ms step_avg:58.49ms
step:1873/2330 train_time:109545ms step_avg:58.49ms
step:1874/2330 train_time:109607ms step_avg:58.49ms
step:1875/2330 train_time:109664ms step_avg:58.49ms
step:1876/2330 train_time:109725ms step_avg:58.49ms
step:1877/2330 train_time:109782ms step_avg:58.49ms
step:1878/2330 train_time:109844ms step_avg:58.49ms
step:1879/2330 train_time:109901ms step_avg:58.49ms
step:1880/2330 train_time:109962ms step_avg:58.49ms
step:1881/2330 train_time:110018ms step_avg:58.49ms
step:1882/2330 train_time:110080ms step_avg:58.49ms
step:1883/2330 train_time:110137ms step_avg:58.49ms
step:1884/2330 train_time:110197ms step_avg:58.49ms
step:1885/2330 train_time:110255ms step_avg:58.49ms
step:1886/2330 train_time:110316ms step_avg:58.49ms
step:1887/2330 train_time:110374ms step_avg:58.49ms
step:1888/2330 train_time:110434ms step_avg:58.49ms
step:1889/2330 train_time:110491ms step_avg:58.49ms
step:1890/2330 train_time:110552ms step_avg:58.49ms
step:1891/2330 train_time:110610ms step_avg:58.49ms
step:1892/2330 train_time:110671ms step_avg:58.49ms
step:1893/2330 train_time:110728ms step_avg:58.49ms
step:1894/2330 train_time:110788ms step_avg:58.49ms
step:1895/2330 train_time:110845ms step_avg:58.49ms
step:1896/2330 train_time:110906ms step_avg:58.49ms
step:1897/2330 train_time:110963ms step_avg:58.49ms
step:1898/2330 train_time:111025ms step_avg:58.50ms
step:1899/2330 train_time:111081ms step_avg:58.49ms
step:1900/2330 train_time:111143ms step_avg:58.50ms
step:1901/2330 train_time:111201ms step_avg:58.50ms
step:1902/2330 train_time:111262ms step_avg:58.50ms
step:1903/2330 train_time:111319ms step_avg:58.50ms
step:1904/2330 train_time:111381ms step_avg:58.50ms
step:1905/2330 train_time:111438ms step_avg:58.50ms
step:1906/2330 train_time:111500ms step_avg:58.50ms
step:1907/2330 train_time:111558ms step_avg:58.50ms
step:1908/2330 train_time:111620ms step_avg:58.50ms
step:1909/2330 train_time:111678ms step_avg:58.50ms
step:1910/2330 train_time:111739ms step_avg:58.50ms
step:1911/2330 train_time:111796ms step_avg:58.50ms
step:1912/2330 train_time:111856ms step_avg:58.50ms
step:1913/2330 train_time:111914ms step_avg:58.50ms
step:1914/2330 train_time:111974ms step_avg:58.50ms
step:1915/2330 train_time:112032ms step_avg:58.50ms
step:1916/2330 train_time:112092ms step_avg:58.50ms
step:1917/2330 train_time:112148ms step_avg:58.50ms
step:1918/2330 train_time:112210ms step_avg:58.50ms
step:1919/2330 train_time:112267ms step_avg:58.50ms
step:1920/2330 train_time:112328ms step_avg:58.50ms
step:1921/2330 train_time:112385ms step_avg:58.50ms
step:1922/2330 train_time:112446ms step_avg:58.50ms
step:1923/2330 train_time:112503ms step_avg:58.50ms
step:1924/2330 train_time:112566ms step_avg:58.51ms
step:1925/2330 train_time:112623ms step_avg:58.51ms
step:1926/2330 train_time:112684ms step_avg:58.51ms
step:1927/2330 train_time:112741ms step_avg:58.51ms
step:1928/2330 train_time:112802ms step_avg:58.51ms
step:1929/2330 train_time:112860ms step_avg:58.51ms
step:1930/2330 train_time:112921ms step_avg:58.51ms
step:1931/2330 train_time:112979ms step_avg:58.51ms
step:1932/2330 train_time:113040ms step_avg:58.51ms
step:1933/2330 train_time:113098ms step_avg:58.51ms
step:1934/2330 train_time:113158ms step_avg:58.51ms
step:1935/2330 train_time:113217ms step_avg:58.51ms
step:1936/2330 train_time:113277ms step_avg:58.51ms
step:1937/2330 train_time:113334ms step_avg:58.51ms
step:1938/2330 train_time:113393ms step_avg:58.51ms
step:1939/2330 train_time:113451ms step_avg:58.51ms
step:1940/2330 train_time:113511ms step_avg:58.51ms
step:1941/2330 train_time:113570ms step_avg:58.51ms
step:1942/2330 train_time:113629ms step_avg:58.51ms
step:1943/2330 train_time:113686ms step_avg:58.51ms
step:1944/2330 train_time:113748ms step_avg:58.51ms
step:1945/2330 train_time:113804ms step_avg:58.51ms
step:1946/2330 train_time:113866ms step_avg:58.51ms
step:1947/2330 train_time:113923ms step_avg:58.51ms
step:1948/2330 train_time:113985ms step_avg:58.51ms
step:1949/2330 train_time:114042ms step_avg:58.51ms
step:1950/2330 train_time:114103ms step_avg:58.51ms
step:1951/2330 train_time:114160ms step_avg:58.51ms
step:1952/2330 train_time:114222ms step_avg:58.52ms
step:1953/2330 train_time:114279ms step_avg:58.51ms
step:1954/2330 train_time:114341ms step_avg:58.52ms
step:1955/2330 train_time:114398ms step_avg:58.52ms
step:1956/2330 train_time:114460ms step_avg:58.52ms
step:1957/2330 train_time:114518ms step_avg:58.52ms
step:1958/2330 train_time:114578ms step_avg:58.52ms
step:1959/2330 train_time:114636ms step_avg:58.52ms
step:1960/2330 train_time:114696ms step_avg:58.52ms
step:1961/2330 train_time:114753ms step_avg:58.52ms
step:1962/2330 train_time:114814ms step_avg:58.52ms
step:1963/2330 train_time:114871ms step_avg:58.52ms
step:1964/2330 train_time:114931ms step_avg:58.52ms
step:1965/2330 train_time:114988ms step_avg:58.52ms
step:1966/2330 train_time:115049ms step_avg:58.52ms
step:1967/2330 train_time:115106ms step_avg:58.52ms
step:1968/2330 train_time:115168ms step_avg:58.52ms
step:1969/2330 train_time:115224ms step_avg:58.52ms
step:1970/2330 train_time:115286ms step_avg:58.52ms
step:1971/2330 train_time:115342ms step_avg:58.52ms
step:1972/2330 train_time:115403ms step_avg:58.52ms
step:1973/2330 train_time:115460ms step_avg:58.52ms
step:1974/2330 train_time:115523ms step_avg:58.52ms
step:1975/2330 train_time:115580ms step_avg:58.52ms
step:1976/2330 train_time:115643ms step_avg:58.52ms
step:1977/2330 train_time:115699ms step_avg:58.52ms
step:1978/2330 train_time:115761ms step_avg:58.52ms
step:1979/2330 train_time:115818ms step_avg:58.52ms
step:1980/2330 train_time:115879ms step_avg:58.52ms
step:1981/2330 train_time:115938ms step_avg:58.52ms
step:1982/2330 train_time:115998ms step_avg:58.53ms
step:1983/2330 train_time:116056ms step_avg:58.53ms
step:1984/2330 train_time:116116ms step_avg:58.53ms
step:1985/2330 train_time:116174ms step_avg:58.53ms
step:1986/2330 train_time:116235ms step_avg:58.53ms
step:1987/2330 train_time:116292ms step_avg:58.53ms
step:1988/2330 train_time:116351ms step_avg:58.53ms
step:1989/2330 train_time:116409ms step_avg:58.53ms
step:1990/2330 train_time:116470ms step_avg:58.53ms
step:1991/2330 train_time:116527ms step_avg:58.53ms
step:1992/2330 train_time:116587ms step_avg:58.53ms
step:1993/2330 train_time:116644ms step_avg:58.53ms
step:1994/2330 train_time:116705ms step_avg:58.53ms
step:1995/2330 train_time:116761ms step_avg:58.53ms
step:1996/2330 train_time:116824ms step_avg:58.53ms
step:1997/2330 train_time:116880ms step_avg:58.53ms
step:1998/2330 train_time:116942ms step_avg:58.53ms
step:1999/2330 train_time:116999ms step_avg:58.53ms
step:2000/2330 train_time:117062ms step_avg:58.53ms
step:2000/2330 val_loss:3.7592 train_time:117144ms step_avg:58.57ms
step:2001/2330 train_time:117163ms step_avg:58.55ms
step:2002/2330 train_time:117183ms step_avg:58.53ms
step:2003/2330 train_time:117243ms step_avg:58.53ms
step:2004/2330 train_time:117307ms step_avg:58.54ms
step:2005/2330 train_time:117365ms step_avg:58.54ms
step:2006/2330 train_time:117426ms step_avg:58.54ms
step:2007/2330 train_time:117483ms step_avg:58.54ms
step:2008/2330 train_time:117544ms step_avg:58.54ms
step:2009/2330 train_time:117600ms step_avg:58.54ms
step:2010/2330 train_time:117660ms step_avg:58.54ms
step:2011/2330 train_time:117717ms step_avg:58.54ms
step:2012/2330 train_time:117777ms step_avg:58.54ms
step:2013/2330 train_time:117833ms step_avg:58.54ms
step:2014/2330 train_time:117892ms step_avg:58.54ms
step:2015/2330 train_time:117948ms step_avg:58.54ms
step:2016/2330 train_time:118009ms step_avg:58.54ms
step:2017/2330 train_time:118066ms step_avg:58.54ms
step:2018/2330 train_time:118128ms step_avg:58.54ms
step:2019/2330 train_time:118186ms step_avg:58.54ms
step:2020/2330 train_time:118249ms step_avg:58.54ms
step:2021/2330 train_time:118306ms step_avg:58.54ms
step:2022/2330 train_time:118371ms step_avg:58.54ms
step:2023/2330 train_time:118428ms step_avg:58.54ms
step:2024/2330 train_time:118489ms step_avg:58.54ms
step:2025/2330 train_time:118546ms step_avg:58.54ms
step:2026/2330 train_time:118607ms step_avg:58.54ms
step:2027/2330 train_time:118664ms step_avg:58.54ms
step:2028/2330 train_time:118725ms step_avg:58.54ms
step:2029/2330 train_time:118782ms step_avg:58.54ms
step:2030/2330 train_time:118842ms step_avg:58.54ms
step:2031/2330 train_time:118900ms step_avg:58.54ms
step:2032/2330 train_time:118960ms step_avg:58.54ms
step:2033/2330 train_time:119017ms step_avg:58.54ms
step:2034/2330 train_time:119077ms step_avg:58.54ms
step:2035/2330 train_time:119135ms step_avg:58.54ms
step:2036/2330 train_time:119197ms step_avg:58.54ms
step:2037/2330 train_time:119256ms step_avg:58.55ms
step:2038/2330 train_time:119317ms step_avg:58.55ms
step:2039/2330 train_time:119375ms step_avg:58.55ms
step:2040/2330 train_time:119436ms step_avg:58.55ms
step:2041/2330 train_time:119495ms step_avg:58.55ms
step:2042/2330 train_time:119554ms step_avg:58.55ms
step:2043/2330 train_time:119611ms step_avg:58.55ms
step:2044/2330 train_time:119671ms step_avg:58.55ms
step:2045/2330 train_time:119728ms step_avg:58.55ms
step:2046/2330 train_time:119788ms step_avg:58.55ms
step:2047/2330 train_time:119845ms step_avg:58.55ms
step:2048/2330 train_time:119906ms step_avg:58.55ms
step:2049/2330 train_time:119962ms step_avg:58.55ms
step:2050/2330 train_time:120024ms step_avg:58.55ms
step:2051/2330 train_time:120081ms step_avg:58.55ms
step:2052/2330 train_time:120143ms step_avg:58.55ms
step:2053/2330 train_time:120201ms step_avg:58.55ms
step:2054/2330 train_time:120262ms step_avg:58.55ms
step:2055/2330 train_time:120320ms step_avg:58.55ms
step:2056/2330 train_time:120383ms step_avg:58.55ms
step:2057/2330 train_time:120441ms step_avg:58.55ms
step:2058/2330 train_time:120502ms step_avg:58.55ms
step:2059/2330 train_time:120558ms step_avg:58.55ms
step:2060/2330 train_time:120620ms step_avg:58.55ms
step:2061/2330 train_time:120678ms step_avg:58.55ms
step:2062/2330 train_time:120739ms step_avg:58.55ms
step:2063/2330 train_time:120796ms step_avg:58.55ms
step:2064/2330 train_time:120856ms step_avg:58.55ms
step:2065/2330 train_time:120913ms step_avg:58.55ms
step:2066/2330 train_time:120973ms step_avg:58.55ms
step:2067/2330 train_time:121030ms step_avg:58.55ms
step:2068/2330 train_time:121090ms step_avg:58.55ms
step:2069/2330 train_time:121147ms step_avg:58.55ms
step:2070/2330 train_time:121208ms step_avg:58.55ms
step:2071/2330 train_time:121265ms step_avg:58.55ms
step:2072/2330 train_time:121327ms step_avg:58.56ms
step:2073/2330 train_time:121384ms step_avg:58.55ms
step:2074/2330 train_time:121447ms step_avg:58.56ms
step:2075/2330 train_time:121504ms step_avg:58.56ms
step:2076/2330 train_time:121566ms step_avg:58.56ms
step:2077/2330 train_time:121623ms step_avg:58.56ms
step:2078/2330 train_time:121684ms step_avg:58.56ms
step:2079/2330 train_time:121741ms step_avg:58.56ms
step:2080/2330 train_time:121802ms step_avg:58.56ms
step:2081/2330 train_time:121860ms step_avg:58.56ms
step:2082/2330 train_time:121920ms step_avg:58.56ms
step:2083/2330 train_time:121978ms step_avg:58.56ms
step:2084/2330 train_time:122038ms step_avg:58.56ms
step:2085/2330 train_time:122095ms step_avg:58.56ms
step:2086/2330 train_time:122155ms step_avg:58.56ms
step:2087/2330 train_time:122214ms step_avg:58.56ms
step:2088/2330 train_time:122273ms step_avg:58.56ms
step:2089/2330 train_time:122331ms step_avg:58.56ms
step:2090/2330 train_time:122390ms step_avg:58.56ms
step:2091/2330 train_time:122447ms step_avg:58.56ms
step:2092/2330 train_time:122508ms step_avg:58.56ms
step:2093/2330 train_time:122565ms step_avg:58.56ms
step:2094/2330 train_time:122627ms step_avg:58.56ms
step:2095/2330 train_time:122684ms step_avg:58.56ms
step:2096/2330 train_time:122745ms step_avg:58.56ms
step:2097/2330 train_time:122802ms step_avg:58.56ms
step:2098/2330 train_time:122863ms step_avg:58.56ms
step:2099/2330 train_time:122920ms step_avg:58.56ms
step:2100/2330 train_time:122982ms step_avg:58.56ms
step:2101/2330 train_time:123040ms step_avg:58.56ms
step:2102/2330 train_time:123101ms step_avg:58.56ms
step:2103/2330 train_time:123158ms step_avg:58.56ms
step:2104/2330 train_time:123219ms step_avg:58.56ms
step:2105/2330 train_time:123277ms step_avg:58.56ms
step:2106/2330 train_time:123337ms step_avg:58.56ms
step:2107/2330 train_time:123396ms step_avg:58.56ms
step:2108/2330 train_time:123456ms step_avg:58.57ms
step:2109/2330 train_time:123513ms step_avg:58.56ms
step:2110/2330 train_time:123574ms step_avg:58.57ms
step:2111/2330 train_time:123631ms step_avg:58.57ms
step:2112/2330 train_time:123692ms step_avg:58.57ms
step:2113/2330 train_time:123749ms step_avg:58.57ms
step:2114/2330 train_time:123809ms step_avg:58.57ms
step:2115/2330 train_time:123866ms step_avg:58.57ms
step:2116/2330 train_time:123927ms step_avg:58.57ms
step:2117/2330 train_time:123984ms step_avg:58.57ms
step:2118/2330 train_time:124046ms step_avg:58.57ms
step:2119/2330 train_time:124103ms step_avg:58.57ms
step:2120/2330 train_time:124164ms step_avg:58.57ms
step:2121/2330 train_time:124221ms step_avg:58.57ms
step:2122/2330 train_time:124283ms step_avg:58.57ms
step:2123/2330 train_time:124341ms step_avg:58.57ms
step:2124/2330 train_time:124401ms step_avg:58.57ms
step:2125/2330 train_time:124458ms step_avg:58.57ms
step:2126/2330 train_time:124519ms step_avg:58.57ms
step:2127/2330 train_time:124578ms step_avg:58.57ms
step:2128/2330 train_time:124638ms step_avg:58.57ms
step:2129/2330 train_time:124696ms step_avg:58.57ms
step:2130/2330 train_time:124756ms step_avg:58.57ms
step:2131/2330 train_time:124812ms step_avg:58.57ms
step:2132/2330 train_time:124872ms step_avg:58.57ms
step:2133/2330 train_time:124930ms step_avg:58.57ms
step:2134/2330 train_time:124990ms step_avg:58.57ms
step:2135/2330 train_time:125047ms step_avg:58.57ms
step:2136/2330 train_time:125109ms step_avg:58.57ms
step:2137/2330 train_time:125166ms step_avg:58.57ms
step:2138/2330 train_time:125227ms step_avg:58.57ms
step:2139/2330 train_time:125284ms step_avg:58.57ms
step:2140/2330 train_time:125347ms step_avg:58.57ms
step:2141/2330 train_time:125403ms step_avg:58.57ms
step:2142/2330 train_time:125466ms step_avg:58.57ms
step:2143/2330 train_time:125523ms step_avg:58.57ms
step:2144/2330 train_time:125585ms step_avg:58.57ms
step:2145/2330 train_time:125642ms step_avg:58.57ms
step:2146/2330 train_time:125704ms step_avg:58.58ms
step:2147/2330 train_time:125762ms step_avg:58.58ms
step:2148/2330 train_time:125823ms step_avg:58.58ms
step:2149/2330 train_time:125880ms step_avg:58.58ms
step:2150/2330 train_time:125941ms step_avg:58.58ms
step:2151/2330 train_time:125999ms step_avg:58.58ms
step:2152/2330 train_time:126060ms step_avg:58.58ms
step:2153/2330 train_time:126118ms step_avg:58.58ms
step:2154/2330 train_time:126178ms step_avg:58.58ms
step:2155/2330 train_time:126236ms step_avg:58.58ms
step:2156/2330 train_time:126297ms step_avg:58.58ms
step:2157/2330 train_time:126355ms step_avg:58.58ms
step:2158/2330 train_time:126415ms step_avg:58.58ms
step:2159/2330 train_time:126472ms step_avg:58.58ms
step:2160/2330 train_time:126533ms step_avg:58.58ms
step:2161/2330 train_time:126590ms step_avg:58.58ms
step:2162/2330 train_time:126651ms step_avg:58.58ms
step:2163/2330 train_time:126708ms step_avg:58.58ms
step:2164/2330 train_time:126769ms step_avg:58.58ms
step:2165/2330 train_time:126826ms step_avg:58.58ms
step:2166/2330 train_time:126888ms step_avg:58.58ms
step:2167/2330 train_time:126945ms step_avg:58.58ms
step:2168/2330 train_time:127006ms step_avg:58.58ms
step:2169/2330 train_time:127062ms step_avg:58.58ms
step:2170/2330 train_time:127125ms step_avg:58.58ms
step:2171/2330 train_time:127182ms step_avg:58.58ms
step:2172/2330 train_time:127243ms step_avg:58.58ms
step:2173/2330 train_time:127300ms step_avg:58.58ms
step:2174/2330 train_time:127363ms step_avg:58.58ms
step:2175/2330 train_time:127421ms step_avg:58.58ms
step:2176/2330 train_time:127482ms step_avg:58.59ms
step:2177/2330 train_time:127540ms step_avg:58.59ms
step:2178/2330 train_time:127601ms step_avg:58.59ms
step:2179/2330 train_time:127659ms step_avg:58.59ms
step:2180/2330 train_time:127721ms step_avg:58.59ms
step:2181/2330 train_time:127778ms step_avg:58.59ms
step:2182/2330 train_time:127839ms step_avg:58.59ms
step:2183/2330 train_time:127896ms step_avg:58.59ms
step:2184/2330 train_time:127956ms step_avg:58.59ms
step:2185/2330 train_time:128014ms step_avg:58.59ms
step:2186/2330 train_time:128074ms step_avg:58.59ms
step:2187/2330 train_time:128131ms step_avg:58.59ms
step:2188/2330 train_time:128191ms step_avg:58.59ms
step:2189/2330 train_time:128248ms step_avg:58.59ms
step:2190/2330 train_time:128309ms step_avg:58.59ms
step:2191/2330 train_time:128366ms step_avg:58.59ms
step:2192/2330 train_time:128428ms step_avg:58.59ms
step:2193/2330 train_time:128484ms step_avg:58.59ms
step:2194/2330 train_time:128546ms step_avg:58.59ms
step:2195/2330 train_time:128603ms step_avg:58.59ms
step:2196/2330 train_time:128665ms step_avg:58.59ms
step:2197/2330 train_time:128722ms step_avg:58.59ms
step:2198/2330 train_time:128784ms step_avg:58.59ms
step:2199/2330 train_time:128842ms step_avg:58.59ms
step:2200/2330 train_time:128902ms step_avg:58.59ms
step:2201/2330 train_time:128960ms step_avg:58.59ms
step:2202/2330 train_time:129021ms step_avg:58.59ms
step:2203/2330 train_time:129079ms step_avg:58.59ms
step:2204/2330 train_time:129140ms step_avg:58.59ms
step:2205/2330 train_time:129198ms step_avg:58.59ms
step:2206/2330 train_time:129258ms step_avg:58.59ms
step:2207/2330 train_time:129315ms step_avg:58.59ms
step:2208/2330 train_time:129376ms step_avg:58.59ms
step:2209/2330 train_time:129433ms step_avg:58.59ms
step:2210/2330 train_time:129493ms step_avg:58.59ms
step:2211/2330 train_time:129550ms step_avg:58.59ms
step:2212/2330 train_time:129611ms step_avg:58.59ms
step:2213/2330 train_time:129668ms step_avg:58.59ms
step:2214/2330 train_time:129729ms step_avg:58.59ms
step:2215/2330 train_time:129786ms step_avg:58.59ms
step:2216/2330 train_time:129847ms step_avg:58.60ms
step:2217/2330 train_time:129904ms step_avg:58.59ms
step:2218/2330 train_time:129966ms step_avg:58.60ms
step:2219/2330 train_time:130021ms step_avg:58.59ms
step:2220/2330 train_time:130083ms step_avg:58.60ms
step:2221/2330 train_time:130140ms step_avg:58.60ms
step:2222/2330 train_time:130202ms step_avg:58.60ms
step:2223/2330 train_time:130258ms step_avg:58.60ms
step:2224/2330 train_time:130320ms step_avg:58.60ms
step:2225/2330 train_time:130378ms step_avg:58.60ms
step:2226/2330 train_time:130438ms step_avg:58.60ms
step:2227/2330 train_time:130496ms step_avg:58.60ms
step:2228/2330 train_time:130556ms step_avg:58.60ms
step:2229/2330 train_time:130614ms step_avg:58.60ms
step:2230/2330 train_time:130673ms step_avg:58.60ms
step:2231/2330 train_time:130731ms step_avg:58.60ms
step:2232/2330 train_time:130792ms step_avg:58.60ms
step:2233/2330 train_time:130849ms step_avg:58.60ms
step:2234/2330 train_time:130909ms step_avg:58.60ms
step:2235/2330 train_time:130967ms step_avg:58.60ms
step:2236/2330 train_time:131028ms step_avg:58.60ms
step:2237/2330 train_time:131085ms step_avg:58.60ms
step:2238/2330 train_time:131147ms step_avg:58.60ms
step:2239/2330 train_time:131203ms step_avg:58.60ms
step:2240/2330 train_time:131264ms step_avg:58.60ms
step:2241/2330 train_time:131321ms step_avg:58.60ms
step:2242/2330 train_time:131383ms step_avg:58.60ms
step:2243/2330 train_time:131440ms step_avg:58.60ms
step:2244/2330 train_time:131502ms step_avg:58.60ms
step:2245/2330 train_time:131559ms step_avg:58.60ms
step:2246/2330 train_time:131622ms step_avg:58.60ms
step:2247/2330 train_time:131679ms step_avg:58.60ms
step:2248/2330 train_time:131740ms step_avg:58.60ms
step:2249/2330 train_time:131798ms step_avg:58.60ms
step:2250/2330 train_time:131858ms step_avg:58.60ms
step:2250/2330 val_loss:3.7099 train_time:131940ms step_avg:58.64ms
step:2251/2330 train_time:131960ms step_avg:58.62ms
step:2252/2330 train_time:131980ms step_avg:58.61ms
step:2253/2330 train_time:132042ms step_avg:58.61ms
step:2254/2330 train_time:132107ms step_avg:58.61ms
step:2255/2330 train_time:132166ms step_avg:58.61ms
step:2256/2330 train_time:132226ms step_avg:58.61ms
step:2257/2330 train_time:132283ms step_avg:58.61ms
step:2258/2330 train_time:132343ms step_avg:58.61ms
step:2259/2330 train_time:132401ms step_avg:58.61ms
step:2260/2330 train_time:132460ms step_avg:58.61ms
step:2261/2330 train_time:132517ms step_avg:58.61ms
step:2262/2330 train_time:132576ms step_avg:58.61ms
step:2263/2330 train_time:132632ms step_avg:58.61ms
step:2264/2330 train_time:132692ms step_avg:58.61ms
step:2265/2330 train_time:132748ms step_avg:58.61ms
step:2266/2330 train_time:132808ms step_avg:58.61ms
step:2267/2330 train_time:132865ms step_avg:58.61ms
step:2268/2330 train_time:132927ms step_avg:58.61ms
step:2269/2330 train_time:132986ms step_avg:58.61ms
step:2270/2330 train_time:133049ms step_avg:58.61ms
step:2271/2330 train_time:133108ms step_avg:58.61ms
step:2272/2330 train_time:133170ms step_avg:58.61ms
step:2273/2330 train_time:133227ms step_avg:58.61ms
step:2274/2330 train_time:133289ms step_avg:58.61ms
step:2275/2330 train_time:133346ms step_avg:58.61ms
step:2276/2330 train_time:133406ms step_avg:58.61ms
step:2277/2330 train_time:133463ms step_avg:58.61ms
step:2278/2330 train_time:133524ms step_avg:58.61ms
step:2279/2330 train_time:133581ms step_avg:58.61ms
step:2280/2330 train_time:133641ms step_avg:58.61ms
step:2281/2330 train_time:133697ms step_avg:58.61ms
step:2282/2330 train_time:133757ms step_avg:58.61ms
step:2283/2330 train_time:133813ms step_avg:58.61ms
step:2284/2330 train_time:133874ms step_avg:58.61ms
step:2285/2330 train_time:133931ms step_avg:58.61ms
step:2286/2330 train_time:133993ms step_avg:58.61ms
step:2287/2330 train_time:134051ms step_avg:58.61ms
step:2288/2330 train_time:134112ms step_avg:58.62ms
step:2289/2330 train_time:134170ms step_avg:58.62ms
step:2290/2330 train_time:134231ms step_avg:58.62ms
step:2291/2330 train_time:134288ms step_avg:58.62ms
step:2292/2330 train_time:134350ms step_avg:58.62ms
step:2293/2330 train_time:134406ms step_avg:58.62ms
step:2294/2330 train_time:134468ms step_avg:58.62ms
step:2295/2330 train_time:134525ms step_avg:58.62ms
step:2296/2330 train_time:134587ms step_avg:58.62ms
step:2297/2330 train_time:134644ms step_avg:58.62ms
step:2298/2330 train_time:134704ms step_avg:58.62ms
step:2299/2330 train_time:134761ms step_avg:58.62ms
step:2300/2330 train_time:134821ms step_avg:58.62ms
step:2301/2330 train_time:134879ms step_avg:58.62ms
step:2302/2330 train_time:134939ms step_avg:58.62ms
step:2303/2330 train_time:134998ms step_avg:58.62ms
step:2304/2330 train_time:135058ms step_avg:58.62ms
step:2305/2330 train_time:135117ms step_avg:58.62ms
step:2306/2330 train_time:135177ms step_avg:58.62ms
step:2307/2330 train_time:135235ms step_avg:58.62ms
step:2308/2330 train_time:135296ms step_avg:58.62ms
step:2309/2330 train_time:135352ms step_avg:58.62ms
step:2310/2330 train_time:135414ms step_avg:58.62ms
step:2311/2330 train_time:135471ms step_avg:58.62ms
step:2312/2330 train_time:135532ms step_avg:58.62ms
step:2313/2330 train_time:135589ms step_avg:58.62ms
step:2314/2330 train_time:135650ms step_avg:58.62ms
step:2315/2330 train_time:135706ms step_avg:58.62ms
step:2316/2330 train_time:135768ms step_avg:58.62ms
step:2317/2330 train_time:135824ms step_avg:58.62ms
step:2318/2330 train_time:135886ms step_avg:58.62ms
step:2319/2330 train_time:135943ms step_avg:58.62ms
step:2320/2330 train_time:136006ms step_avg:58.62ms
step:2321/2330 train_time:136065ms step_avg:58.62ms
step:2322/2330 train_time:136126ms step_avg:58.62ms
step:2323/2330 train_time:136184ms step_avg:58.62ms
step:2324/2330 train_time:136245ms step_avg:58.63ms
step:2325/2330 train_time:136302ms step_avg:58.62ms
step:2326/2330 train_time:136363ms step_avg:58.63ms
step:2327/2330 train_time:136421ms step_avg:58.63ms
step:2328/2330 train_time:136481ms step_avg:58.63ms
step:2329/2330 train_time:136539ms step_avg:58.63ms
step:2330/2330 train_time:136599ms step_avg:58.63ms
step:2330/2330 val_loss:3.6943 train_time:136680ms step_avg:58.66ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
