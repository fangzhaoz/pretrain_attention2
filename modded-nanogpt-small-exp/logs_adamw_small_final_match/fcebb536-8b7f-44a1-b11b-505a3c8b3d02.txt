import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 8000  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 3000  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_final_match", exist_ok=True)
    logfile = f"logs_adamw_small_final_match/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=6e-4,  betas=(0.8, 0.8), eps=1e-10, weight_decay=9e-1, fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations 
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_final_match/{run_id}.txt", exist_ok=True)
            torch.save(log, f"logs_adamw_small_final_match/{run_id}.txt/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Nov 12 00:50:20 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   34C    P0             117W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   33C    P0             117W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   29C    P0             118W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   33C    P0             117W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   31C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/8000 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/8000 train_time:98ms step_avg:98.28ms
step:2/8000 train_time:190ms step_avg:95.15ms
step:3/8000 train_time:208ms step_avg:69.33ms
step:4/8000 train_time:227ms step_avg:56.80ms
step:5/8000 train_time:281ms step_avg:56.23ms
step:6/8000 train_time:340ms step_avg:56.75ms
step:7/8000 train_time:395ms step_avg:56.48ms
step:8/8000 train_time:455ms step_avg:56.86ms
step:9/8000 train_time:510ms step_avg:56.63ms
step:10/8000 train_time:569ms step_avg:56.89ms
step:11/8000 train_time:624ms step_avg:56.76ms
step:12/8000 train_time:683ms step_avg:56.91ms
step:13/8000 train_time:738ms step_avg:56.76ms
step:14/8000 train_time:796ms step_avg:56.86ms
step:15/8000 train_time:852ms step_avg:56.77ms
step:16/8000 train_time:910ms step_avg:56.86ms
step:17/8000 train_time:965ms step_avg:56.78ms
step:18/8000 train_time:1024ms step_avg:56.91ms
step:19/8000 train_time:1082ms step_avg:56.93ms
step:20/8000 train_time:1143ms step_avg:57.14ms
step:21/8000 train_time:1200ms step_avg:57.16ms
step:22/8000 train_time:1261ms step_avg:57.30ms
step:23/8000 train_time:1317ms step_avg:57.24ms
step:24/8000 train_time:1376ms step_avg:57.33ms
step:25/8000 train_time:1433ms step_avg:57.31ms
step:26/8000 train_time:1491ms step_avg:57.35ms
step:27/8000 train_time:1547ms step_avg:57.30ms
step:28/8000 train_time:1606ms step_avg:57.36ms
step:29/8000 train_time:1661ms step_avg:57.29ms
step:30/8000 train_time:1720ms step_avg:57.35ms
step:31/8000 train_time:1776ms step_avg:57.30ms
step:32/8000 train_time:1835ms step_avg:57.33ms
step:33/8000 train_time:1890ms step_avg:57.27ms
step:34/8000 train_time:1948ms step_avg:57.31ms
step:35/8000 train_time:2004ms step_avg:57.26ms
step:36/8000 train_time:2065ms step_avg:57.35ms
step:37/8000 train_time:2121ms step_avg:57.32ms
step:38/8000 train_time:2182ms step_avg:57.43ms
step:39/8000 train_time:2239ms step_avg:57.40ms
step:40/8000 train_time:2298ms step_avg:57.45ms
step:41/8000 train_time:2354ms step_avg:57.42ms
step:42/8000 train_time:2413ms step_avg:57.45ms
step:43/8000 train_time:2470ms step_avg:57.43ms
step:44/8000 train_time:2528ms step_avg:57.46ms
step:45/8000 train_time:2584ms step_avg:57.42ms
step:46/8000 train_time:2643ms step_avg:57.46ms
step:47/8000 train_time:2699ms step_avg:57.42ms
step:48/8000 train_time:2757ms step_avg:57.43ms
step:49/8000 train_time:2812ms step_avg:57.38ms
step:50/8000 train_time:2871ms step_avg:57.42ms
step:51/8000 train_time:2926ms step_avg:57.38ms
step:52/8000 train_time:2986ms step_avg:57.41ms
step:53/8000 train_time:3041ms step_avg:57.39ms
step:54/8000 train_time:3101ms step_avg:57.43ms
step:55/8000 train_time:3157ms step_avg:57.40ms
step:56/8000 train_time:3218ms step_avg:57.46ms
step:57/8000 train_time:3274ms step_avg:57.44ms
step:58/8000 train_time:3333ms step_avg:57.46ms
step:59/8000 train_time:3389ms step_avg:57.44ms
step:60/8000 train_time:3448ms step_avg:57.47ms
step:61/8000 train_time:3503ms step_avg:57.43ms
step:62/8000 train_time:3564ms step_avg:57.48ms
step:63/8000 train_time:3619ms step_avg:57.45ms
step:64/8000 train_time:3678ms step_avg:57.46ms
step:65/8000 train_time:3733ms step_avg:57.43ms
step:66/8000 train_time:3791ms step_avg:57.45ms
step:67/8000 train_time:3847ms step_avg:57.42ms
step:68/8000 train_time:3906ms step_avg:57.43ms
step:69/8000 train_time:3961ms step_avg:57.41ms
step:70/8000 train_time:4020ms step_avg:57.43ms
step:71/8000 train_time:4076ms step_avg:57.41ms
step:72/8000 train_time:4136ms step_avg:57.45ms
step:73/8000 train_time:4193ms step_avg:57.43ms
step:74/8000 train_time:4252ms step_avg:57.46ms
step:75/8000 train_time:4309ms step_avg:57.46ms
step:76/8000 train_time:4368ms step_avg:57.47ms
step:77/8000 train_time:4424ms step_avg:57.46ms
step:78/8000 train_time:4484ms step_avg:57.49ms
step:79/8000 train_time:4540ms step_avg:57.46ms
step:80/8000 train_time:4599ms step_avg:57.48ms
step:81/8000 train_time:4654ms step_avg:57.46ms
step:82/8000 train_time:4713ms step_avg:57.47ms
step:83/8000 train_time:4769ms step_avg:57.46ms
step:84/8000 train_time:4827ms step_avg:57.47ms
step:85/8000 train_time:4883ms step_avg:57.44ms
step:86/8000 train_time:4942ms step_avg:57.46ms
step:87/8000 train_time:4997ms step_avg:57.44ms
step:88/8000 train_time:5056ms step_avg:57.46ms
step:89/8000 train_time:5112ms step_avg:57.44ms
step:90/8000 train_time:5172ms step_avg:57.46ms
step:91/8000 train_time:5229ms step_avg:57.46ms
step:92/8000 train_time:5288ms step_avg:57.48ms
step:93/8000 train_time:5343ms step_avg:57.46ms
step:94/8000 train_time:5403ms step_avg:57.48ms
step:95/8000 train_time:5460ms step_avg:57.47ms
step:96/8000 train_time:5519ms step_avg:57.48ms
step:97/8000 train_time:5574ms step_avg:57.47ms
step:98/8000 train_time:5634ms step_avg:57.49ms
step:99/8000 train_time:5690ms step_avg:57.47ms
step:100/8000 train_time:5748ms step_avg:57.48ms
step:101/8000 train_time:5804ms step_avg:57.46ms
step:102/8000 train_time:5863ms step_avg:57.48ms
step:103/8000 train_time:5919ms step_avg:57.46ms
step:104/8000 train_time:5979ms step_avg:57.49ms
step:105/8000 train_time:6034ms step_avg:57.47ms
step:106/8000 train_time:6093ms step_avg:57.48ms
step:107/8000 train_time:6149ms step_avg:57.47ms
step:108/8000 train_time:6209ms step_avg:57.49ms
step:109/8000 train_time:6265ms step_avg:57.48ms
step:110/8000 train_time:6326ms step_avg:57.50ms
step:111/8000 train_time:6381ms step_avg:57.49ms
step:112/8000 train_time:6441ms step_avg:57.51ms
step:113/8000 train_time:6497ms step_avg:57.49ms
step:114/8000 train_time:6556ms step_avg:57.51ms
step:115/8000 train_time:6612ms step_avg:57.49ms
step:116/8000 train_time:6671ms step_avg:57.51ms
step:117/8000 train_time:6727ms step_avg:57.49ms
step:118/8000 train_time:6786ms step_avg:57.51ms
step:119/8000 train_time:6842ms step_avg:57.49ms
step:120/8000 train_time:6900ms step_avg:57.50ms
step:121/8000 train_time:6956ms step_avg:57.48ms
step:122/8000 train_time:7015ms step_avg:57.50ms
step:123/8000 train_time:7071ms step_avg:57.48ms
step:124/8000 train_time:7129ms step_avg:57.49ms
step:125/8000 train_time:7185ms step_avg:57.48ms
step:126/8000 train_time:7244ms step_avg:57.49ms
step:127/8000 train_time:7300ms step_avg:57.48ms
step:128/8000 train_time:7359ms step_avg:57.49ms
step:129/8000 train_time:7415ms step_avg:57.48ms
step:130/8000 train_time:7475ms step_avg:57.50ms
step:131/8000 train_time:7530ms step_avg:57.48ms
step:132/8000 train_time:7589ms step_avg:57.50ms
step:133/8000 train_time:7645ms step_avg:57.48ms
step:134/8000 train_time:7705ms step_avg:57.50ms
step:135/8000 train_time:7761ms step_avg:57.49ms
step:136/8000 train_time:7820ms step_avg:57.50ms
step:137/8000 train_time:7875ms step_avg:57.48ms
step:138/8000 train_time:7935ms step_avg:57.50ms
step:139/8000 train_time:7990ms step_avg:57.48ms
step:140/8000 train_time:8049ms step_avg:57.50ms
step:141/8000 train_time:8105ms step_avg:57.48ms
step:142/8000 train_time:8165ms step_avg:57.50ms
step:143/8000 train_time:8221ms step_avg:57.49ms
step:144/8000 train_time:8280ms step_avg:57.50ms
step:145/8000 train_time:8336ms step_avg:57.49ms
step:146/8000 train_time:8395ms step_avg:57.50ms
step:147/8000 train_time:8451ms step_avg:57.49ms
step:148/8000 train_time:8510ms step_avg:57.50ms
step:149/8000 train_time:8566ms step_avg:57.49ms
step:150/8000 train_time:8626ms step_avg:57.50ms
step:151/8000 train_time:8681ms step_avg:57.49ms
step:152/8000 train_time:8741ms step_avg:57.51ms
step:153/8000 train_time:8797ms step_avg:57.49ms
step:154/8000 train_time:8855ms step_avg:57.50ms
step:155/8000 train_time:8911ms step_avg:57.49ms
step:156/8000 train_time:8969ms step_avg:57.50ms
step:157/8000 train_time:9025ms step_avg:57.48ms
step:158/8000 train_time:9084ms step_avg:57.50ms
step:159/8000 train_time:9140ms step_avg:57.48ms
step:160/8000 train_time:9199ms step_avg:57.50ms
step:161/8000 train_time:9255ms step_avg:57.49ms
step:162/8000 train_time:9315ms step_avg:57.50ms
step:163/8000 train_time:9371ms step_avg:57.49ms
step:164/8000 train_time:9430ms step_avg:57.50ms
step:165/8000 train_time:9486ms step_avg:57.49ms
step:166/8000 train_time:9545ms step_avg:57.50ms
step:167/8000 train_time:9601ms step_avg:57.49ms
step:168/8000 train_time:9660ms step_avg:57.50ms
step:169/8000 train_time:9716ms step_avg:57.49ms
step:170/8000 train_time:9775ms step_avg:57.50ms
step:171/8000 train_time:9831ms step_avg:57.49ms
step:172/8000 train_time:9890ms step_avg:57.50ms
step:173/8000 train_time:9946ms step_avg:57.49ms
step:174/8000 train_time:10004ms step_avg:57.50ms
step:175/8000 train_time:10060ms step_avg:57.49ms
step:176/8000 train_time:10119ms step_avg:57.49ms
step:177/8000 train_time:10174ms step_avg:57.48ms
step:178/8000 train_time:10233ms step_avg:57.49ms
step:179/8000 train_time:10290ms step_avg:57.49ms
step:180/8000 train_time:10349ms step_avg:57.49ms
step:181/8000 train_time:10405ms step_avg:57.48ms
step:182/8000 train_time:10465ms step_avg:57.50ms
step:183/8000 train_time:10521ms step_avg:57.49ms
step:184/8000 train_time:10579ms step_avg:57.50ms
step:185/8000 train_time:10635ms step_avg:57.49ms
step:186/8000 train_time:10694ms step_avg:57.50ms
step:187/8000 train_time:10751ms step_avg:57.49ms
step:188/8000 train_time:10810ms step_avg:57.50ms
step:189/8000 train_time:10865ms step_avg:57.49ms
step:190/8000 train_time:10924ms step_avg:57.50ms
step:191/8000 train_time:10980ms step_avg:57.49ms
step:192/8000 train_time:11039ms step_avg:57.49ms
step:193/8000 train_time:11094ms step_avg:57.48ms
step:194/8000 train_time:11154ms step_avg:57.49ms
step:195/8000 train_time:11210ms step_avg:57.49ms
step:196/8000 train_time:11269ms step_avg:57.49ms
step:197/8000 train_time:11325ms step_avg:57.49ms
step:198/8000 train_time:11384ms step_avg:57.50ms
step:199/8000 train_time:11440ms step_avg:57.49ms
step:200/8000 train_time:11499ms step_avg:57.49ms
step:201/8000 train_time:11554ms step_avg:57.48ms
step:202/8000 train_time:11614ms step_avg:57.50ms
step:203/8000 train_time:11671ms step_avg:57.49ms
step:204/8000 train_time:11729ms step_avg:57.50ms
step:205/8000 train_time:11785ms step_avg:57.49ms
step:206/8000 train_time:11845ms step_avg:57.50ms
step:207/8000 train_time:11901ms step_avg:57.49ms
step:208/8000 train_time:11959ms step_avg:57.50ms
step:209/8000 train_time:12015ms step_avg:57.49ms
step:210/8000 train_time:12074ms step_avg:57.50ms
step:211/8000 train_time:12130ms step_avg:57.49ms
step:212/8000 train_time:12189ms step_avg:57.50ms
step:213/8000 train_time:12245ms step_avg:57.49ms
step:214/8000 train_time:12304ms step_avg:57.50ms
step:215/8000 train_time:12360ms step_avg:57.49ms
step:216/8000 train_time:12419ms step_avg:57.50ms
step:217/8000 train_time:12475ms step_avg:57.49ms
step:218/8000 train_time:12534ms step_avg:57.50ms
step:219/8000 train_time:12590ms step_avg:57.49ms
step:220/8000 train_time:12649ms step_avg:57.50ms
step:221/8000 train_time:12706ms step_avg:57.49ms
step:222/8000 train_time:12765ms step_avg:57.50ms
step:223/8000 train_time:12822ms step_avg:57.50ms
step:224/8000 train_time:12881ms step_avg:57.50ms
step:225/8000 train_time:12937ms step_avg:57.50ms
step:226/8000 train_time:12996ms step_avg:57.50ms
step:227/8000 train_time:13051ms step_avg:57.50ms
step:228/8000 train_time:13111ms step_avg:57.50ms
step:229/8000 train_time:13166ms step_avg:57.49ms
step:230/8000 train_time:13226ms step_avg:57.50ms
step:231/8000 train_time:13281ms step_avg:57.50ms
step:232/8000 train_time:13340ms step_avg:57.50ms
step:233/8000 train_time:13395ms step_avg:57.49ms
step:234/8000 train_time:13455ms step_avg:57.50ms
step:235/8000 train_time:13511ms step_avg:57.49ms
step:236/8000 train_time:13570ms step_avg:57.50ms
step:237/8000 train_time:13626ms step_avg:57.49ms
step:238/8000 train_time:13686ms step_avg:57.50ms
step:239/8000 train_time:13742ms step_avg:57.50ms
step:240/8000 train_time:13800ms step_avg:57.50ms
step:241/8000 train_time:13856ms step_avg:57.49ms
step:242/8000 train_time:13915ms step_avg:57.50ms
step:243/8000 train_time:13971ms step_avg:57.49ms
step:244/8000 train_time:14030ms step_avg:57.50ms
step:245/8000 train_time:14086ms step_avg:57.49ms
step:246/8000 train_time:14145ms step_avg:57.50ms
step:247/8000 train_time:14200ms step_avg:57.49ms
step:248/8000 train_time:14260ms step_avg:57.50ms
step:249/8000 train_time:14315ms step_avg:57.49ms
step:250/8000 train_time:14376ms step_avg:57.50ms
step:250/8000 val_loss:4.8801 train_time:14454ms step_avg:57.81ms
step:251/8000 train_time:14471ms step_avg:57.65ms
step:252/8000 train_time:14491ms step_avg:57.50ms
step:253/8000 train_time:14548ms step_avg:57.50ms
step:254/8000 train_time:14610ms step_avg:57.52ms
step:255/8000 train_time:14666ms step_avg:57.51ms
step:256/8000 train_time:14728ms step_avg:57.53ms
step:257/8000 train_time:14784ms step_avg:57.52ms
step:258/8000 train_time:14842ms step_avg:57.53ms
step:259/8000 train_time:14898ms step_avg:57.52ms
step:260/8000 train_time:14958ms step_avg:57.53ms
step:261/8000 train_time:15013ms step_avg:57.52ms
step:262/8000 train_time:15072ms step_avg:57.53ms
step:263/8000 train_time:15127ms step_avg:57.52ms
step:264/8000 train_time:15186ms step_avg:57.52ms
step:265/8000 train_time:15241ms step_avg:57.51ms
step:266/8000 train_time:15299ms step_avg:57.52ms
step:267/8000 train_time:15355ms step_avg:57.51ms
step:268/8000 train_time:15416ms step_avg:57.52ms
step:269/8000 train_time:15472ms step_avg:57.52ms
step:270/8000 train_time:15533ms step_avg:57.53ms
step:271/8000 train_time:15590ms step_avg:57.53ms
step:272/8000 train_time:15652ms step_avg:57.55ms
step:273/8000 train_time:15708ms step_avg:57.54ms
step:274/8000 train_time:15768ms step_avg:57.55ms
step:275/8000 train_time:15825ms step_avg:57.54ms
step:276/8000 train_time:15883ms step_avg:57.55ms
step:277/8000 train_time:15939ms step_avg:57.54ms
step:278/8000 train_time:15998ms step_avg:57.55ms
step:279/8000 train_time:16053ms step_avg:57.54ms
step:280/8000 train_time:16113ms step_avg:57.55ms
step:281/8000 train_time:16168ms step_avg:57.54ms
step:282/8000 train_time:16227ms step_avg:57.54ms
step:283/8000 train_time:16283ms step_avg:57.54ms
step:284/8000 train_time:16342ms step_avg:57.54ms
step:285/8000 train_time:16398ms step_avg:57.54ms
step:286/8000 train_time:16458ms step_avg:57.54ms
step:287/8000 train_time:16514ms step_avg:57.54ms
step:288/8000 train_time:16574ms step_avg:57.55ms
step:289/8000 train_time:16631ms step_avg:57.55ms
step:290/8000 train_time:16691ms step_avg:57.55ms
step:291/8000 train_time:16747ms step_avg:57.55ms
step:292/8000 train_time:16807ms step_avg:57.56ms
step:293/8000 train_time:16862ms step_avg:57.55ms
step:294/8000 train_time:16922ms step_avg:57.56ms
step:295/8000 train_time:16977ms step_avg:57.55ms
step:296/8000 train_time:17038ms step_avg:57.56ms
step:297/8000 train_time:17093ms step_avg:57.55ms
step:298/8000 train_time:17152ms step_avg:57.56ms
step:299/8000 train_time:17207ms step_avg:57.55ms
step:300/8000 train_time:17266ms step_avg:57.55ms
step:301/8000 train_time:17323ms step_avg:57.55ms
step:302/8000 train_time:17381ms step_avg:57.55ms
step:303/8000 train_time:17438ms step_avg:57.55ms
step:304/8000 train_time:17497ms step_avg:57.56ms
step:305/8000 train_time:17553ms step_avg:57.55ms
step:306/8000 train_time:17613ms step_avg:57.56ms
step:307/8000 train_time:17669ms step_avg:57.56ms
step:308/8000 train_time:17730ms step_avg:57.56ms
step:309/8000 train_time:17786ms step_avg:57.56ms
step:310/8000 train_time:17847ms step_avg:57.57ms
step:311/8000 train_time:17903ms step_avg:57.57ms
step:312/8000 train_time:17961ms step_avg:57.57ms
step:313/8000 train_time:18017ms step_avg:57.56ms
step:314/8000 train_time:18077ms step_avg:57.57ms
step:315/8000 train_time:18132ms step_avg:57.56ms
step:316/8000 train_time:18191ms step_avg:57.57ms
step:317/8000 train_time:18247ms step_avg:57.56ms
step:318/8000 train_time:18306ms step_avg:57.57ms
step:319/8000 train_time:18362ms step_avg:57.56ms
step:320/8000 train_time:18421ms step_avg:57.57ms
step:321/8000 train_time:18476ms step_avg:57.56ms
step:322/8000 train_time:18536ms step_avg:57.57ms
step:323/8000 train_time:18592ms step_avg:57.56ms
step:324/8000 train_time:18652ms step_avg:57.57ms
step:325/8000 train_time:18707ms step_avg:57.56ms
step:326/8000 train_time:18768ms step_avg:57.57ms
step:327/8000 train_time:18823ms step_avg:57.56ms
step:328/8000 train_time:18883ms step_avg:57.57ms
step:329/8000 train_time:18939ms step_avg:57.57ms
step:330/8000 train_time:18998ms step_avg:57.57ms
step:331/8000 train_time:19054ms step_avg:57.57ms
step:332/8000 train_time:19115ms step_avg:57.57ms
step:333/8000 train_time:19170ms step_avg:57.57ms
step:334/8000 train_time:19229ms step_avg:57.57ms
step:335/8000 train_time:19285ms step_avg:57.57ms
step:336/8000 train_time:19344ms step_avg:57.57ms
step:337/8000 train_time:19401ms step_avg:57.57ms
step:338/8000 train_time:19460ms step_avg:57.57ms
step:339/8000 train_time:19516ms step_avg:57.57ms
step:340/8000 train_time:19575ms step_avg:57.57ms
step:341/8000 train_time:19632ms step_avg:57.57ms
step:342/8000 train_time:19691ms step_avg:57.58ms
step:343/8000 train_time:19747ms step_avg:57.57ms
step:344/8000 train_time:19807ms step_avg:57.58ms
step:345/8000 train_time:19863ms step_avg:57.57ms
step:346/8000 train_time:19922ms step_avg:57.58ms
step:347/8000 train_time:19978ms step_avg:57.57ms
step:348/8000 train_time:20038ms step_avg:57.58ms
step:349/8000 train_time:20095ms step_avg:57.58ms
step:350/8000 train_time:20154ms step_avg:57.58ms
step:351/8000 train_time:20210ms step_avg:57.58ms
step:352/8000 train_time:20269ms step_avg:57.58ms
step:353/8000 train_time:20325ms step_avg:57.58ms
step:354/8000 train_time:20384ms step_avg:57.58ms
step:355/8000 train_time:20440ms step_avg:57.58ms
step:356/8000 train_time:20499ms step_avg:57.58ms
step:357/8000 train_time:20555ms step_avg:57.58ms
step:358/8000 train_time:20614ms step_avg:57.58ms
step:359/8000 train_time:20670ms step_avg:57.58ms
step:360/8000 train_time:20730ms step_avg:57.58ms
step:361/8000 train_time:20786ms step_avg:57.58ms
step:362/8000 train_time:20845ms step_avg:57.58ms
step:363/8000 train_time:20902ms step_avg:57.58ms
step:364/8000 train_time:20960ms step_avg:57.58ms
step:365/8000 train_time:21016ms step_avg:57.58ms
step:366/8000 train_time:21076ms step_avg:57.58ms
step:367/8000 train_time:21132ms step_avg:57.58ms
step:368/8000 train_time:21191ms step_avg:57.58ms
step:369/8000 train_time:21247ms step_avg:57.58ms
step:370/8000 train_time:21306ms step_avg:57.58ms
step:371/8000 train_time:21362ms step_avg:57.58ms
step:372/8000 train_time:21421ms step_avg:57.58ms
step:373/8000 train_time:21477ms step_avg:57.58ms
step:374/8000 train_time:21537ms step_avg:57.59ms
step:375/8000 train_time:21594ms step_avg:57.58ms
step:376/8000 train_time:21653ms step_avg:57.59ms
step:377/8000 train_time:21709ms step_avg:57.58ms
step:378/8000 train_time:21769ms step_avg:57.59ms
step:379/8000 train_time:21824ms step_avg:57.58ms
step:380/8000 train_time:21884ms step_avg:57.59ms
step:381/8000 train_time:21940ms step_avg:57.59ms
step:382/8000 train_time:21999ms step_avg:57.59ms
step:383/8000 train_time:22056ms step_avg:57.59ms
step:384/8000 train_time:22116ms step_avg:57.59ms
step:385/8000 train_time:22172ms step_avg:57.59ms
step:386/8000 train_time:22231ms step_avg:57.59ms
step:387/8000 train_time:22287ms step_avg:57.59ms
step:388/8000 train_time:22346ms step_avg:57.59ms
step:389/8000 train_time:22402ms step_avg:57.59ms
step:390/8000 train_time:22461ms step_avg:57.59ms
step:391/8000 train_time:22517ms step_avg:57.59ms
step:392/8000 train_time:22577ms step_avg:57.60ms
step:393/8000 train_time:22633ms step_avg:57.59ms
step:394/8000 train_time:22693ms step_avg:57.60ms
step:395/8000 train_time:22748ms step_avg:57.59ms
step:396/8000 train_time:22807ms step_avg:57.59ms
step:397/8000 train_time:22863ms step_avg:57.59ms
step:398/8000 train_time:22922ms step_avg:57.59ms
step:399/8000 train_time:22980ms step_avg:57.59ms
step:400/8000 train_time:23038ms step_avg:57.60ms
step:401/8000 train_time:23094ms step_avg:57.59ms
step:402/8000 train_time:23154ms step_avg:57.60ms
step:403/8000 train_time:23210ms step_avg:57.59ms
step:404/8000 train_time:23269ms step_avg:57.60ms
step:405/8000 train_time:23325ms step_avg:57.59ms
step:406/8000 train_time:23384ms step_avg:57.60ms
step:407/8000 train_time:23441ms step_avg:57.59ms
step:408/8000 train_time:23500ms step_avg:57.60ms
step:409/8000 train_time:23557ms step_avg:57.60ms
step:410/8000 train_time:23615ms step_avg:57.60ms
step:411/8000 train_time:23672ms step_avg:57.60ms
step:412/8000 train_time:23730ms step_avg:57.60ms
step:413/8000 train_time:23786ms step_avg:57.59ms
step:414/8000 train_time:23846ms step_avg:57.60ms
step:415/8000 train_time:23902ms step_avg:57.60ms
step:416/8000 train_time:23961ms step_avg:57.60ms
step:417/8000 train_time:24017ms step_avg:57.59ms
step:418/8000 train_time:24077ms step_avg:57.60ms
step:419/8000 train_time:24133ms step_avg:57.60ms
step:420/8000 train_time:24193ms step_avg:57.60ms
step:421/8000 train_time:24248ms step_avg:57.60ms
step:422/8000 train_time:24308ms step_avg:57.60ms
step:423/8000 train_time:24363ms step_avg:57.60ms
step:424/8000 train_time:24423ms step_avg:57.60ms
step:425/8000 train_time:24480ms step_avg:57.60ms
step:426/8000 train_time:24539ms step_avg:57.60ms
step:427/8000 train_time:24596ms step_avg:57.60ms
step:428/8000 train_time:24655ms step_avg:57.60ms
step:429/8000 train_time:24711ms step_avg:57.60ms
step:430/8000 train_time:24770ms step_avg:57.60ms
step:431/8000 train_time:24826ms step_avg:57.60ms
step:432/8000 train_time:24885ms step_avg:57.60ms
step:433/8000 train_time:24941ms step_avg:57.60ms
step:434/8000 train_time:25001ms step_avg:57.61ms
step:435/8000 train_time:25057ms step_avg:57.60ms
step:436/8000 train_time:25117ms step_avg:57.61ms
step:437/8000 train_time:25172ms step_avg:57.60ms
step:438/8000 train_time:25232ms step_avg:57.61ms
step:439/8000 train_time:25287ms step_avg:57.60ms
step:440/8000 train_time:25348ms step_avg:57.61ms
step:441/8000 train_time:25403ms step_avg:57.60ms
step:442/8000 train_time:25462ms step_avg:57.61ms
step:443/8000 train_time:25519ms step_avg:57.61ms
step:444/8000 train_time:25578ms step_avg:57.61ms
step:445/8000 train_time:25634ms step_avg:57.60ms
step:446/8000 train_time:25694ms step_avg:57.61ms
step:447/8000 train_time:25750ms step_avg:57.61ms
step:448/8000 train_time:25810ms step_avg:57.61ms
step:449/8000 train_time:25865ms step_avg:57.61ms
step:450/8000 train_time:25926ms step_avg:57.61ms
step:451/8000 train_time:25982ms step_avg:57.61ms
step:452/8000 train_time:26042ms step_avg:57.61ms
step:453/8000 train_time:26099ms step_avg:57.61ms
step:454/8000 train_time:26159ms step_avg:57.62ms
step:455/8000 train_time:26215ms step_avg:57.61ms
step:456/8000 train_time:26275ms step_avg:57.62ms
step:457/8000 train_time:26331ms step_avg:57.62ms
step:458/8000 train_time:26389ms step_avg:57.62ms
step:459/8000 train_time:26445ms step_avg:57.61ms
step:460/8000 train_time:26505ms step_avg:57.62ms
step:461/8000 train_time:26561ms step_avg:57.62ms
step:462/8000 train_time:26620ms step_avg:57.62ms
step:463/8000 train_time:26677ms step_avg:57.62ms
step:464/8000 train_time:26736ms step_avg:57.62ms
step:465/8000 train_time:26793ms step_avg:57.62ms
step:466/8000 train_time:26851ms step_avg:57.62ms
step:467/8000 train_time:26907ms step_avg:57.62ms
step:468/8000 train_time:26968ms step_avg:57.62ms
step:469/8000 train_time:27023ms step_avg:57.62ms
step:470/8000 train_time:27083ms step_avg:57.62ms
step:471/8000 train_time:27140ms step_avg:57.62ms
step:472/8000 train_time:27199ms step_avg:57.63ms
step:473/8000 train_time:27256ms step_avg:57.62ms
step:474/8000 train_time:27314ms step_avg:57.63ms
step:475/8000 train_time:27370ms step_avg:57.62ms
step:476/8000 train_time:27429ms step_avg:57.62ms
step:477/8000 train_time:27485ms step_avg:57.62ms
step:478/8000 train_time:27544ms step_avg:57.62ms
step:479/8000 train_time:27600ms step_avg:57.62ms
step:480/8000 train_time:27660ms step_avg:57.62ms
step:481/8000 train_time:27716ms step_avg:57.62ms
step:482/8000 train_time:27775ms step_avg:57.63ms
step:483/8000 train_time:27831ms step_avg:57.62ms
step:484/8000 train_time:27891ms step_avg:57.63ms
step:485/8000 train_time:27946ms step_avg:57.62ms
step:486/8000 train_time:28006ms step_avg:57.63ms
step:487/8000 train_time:28062ms step_avg:57.62ms
step:488/8000 train_time:28121ms step_avg:57.63ms
step:489/8000 train_time:28177ms step_avg:57.62ms
step:490/8000 train_time:28237ms step_avg:57.63ms
step:491/8000 train_time:28293ms step_avg:57.62ms
step:492/8000 train_time:28353ms step_avg:57.63ms
step:493/8000 train_time:28408ms step_avg:57.62ms
step:494/8000 train_time:28469ms step_avg:57.63ms
step:495/8000 train_time:28524ms step_avg:57.62ms
step:496/8000 train_time:28583ms step_avg:57.63ms
step:497/8000 train_time:28640ms step_avg:57.63ms
step:498/8000 train_time:28699ms step_avg:57.63ms
step:499/8000 train_time:28755ms step_avg:57.63ms
step:500/8000 train_time:28815ms step_avg:57.63ms
step:500/8000 val_loss:4.3965 train_time:28895ms step_avg:57.79ms
step:501/8000 train_time:28913ms step_avg:57.71ms
step:502/8000 train_time:28933ms step_avg:57.64ms
step:503/8000 train_time:28990ms step_avg:57.64ms
step:504/8000 train_time:29056ms step_avg:57.65ms
step:505/8000 train_time:29113ms step_avg:57.65ms
step:506/8000 train_time:29172ms step_avg:57.65ms
step:507/8000 train_time:29228ms step_avg:57.65ms
step:508/8000 train_time:29287ms step_avg:57.65ms
step:509/8000 train_time:29343ms step_avg:57.65ms
step:510/8000 train_time:29402ms step_avg:57.65ms
step:511/8000 train_time:29457ms step_avg:57.65ms
step:512/8000 train_time:29515ms step_avg:57.65ms
step:513/8000 train_time:29571ms step_avg:57.64ms
step:514/8000 train_time:29630ms step_avg:57.65ms
step:515/8000 train_time:29685ms step_avg:57.64ms
step:516/8000 train_time:29744ms step_avg:57.64ms
step:517/8000 train_time:29799ms step_avg:57.64ms
step:518/8000 train_time:29858ms step_avg:57.64ms
step:519/8000 train_time:29914ms step_avg:57.64ms
step:520/8000 train_time:29974ms step_avg:57.64ms
step:521/8000 train_time:30031ms step_avg:57.64ms
step:522/8000 train_time:30092ms step_avg:57.65ms
step:523/8000 train_time:30148ms step_avg:57.65ms
step:524/8000 train_time:30208ms step_avg:57.65ms
step:525/8000 train_time:30265ms step_avg:57.65ms
step:526/8000 train_time:30324ms step_avg:57.65ms
step:527/8000 train_time:30380ms step_avg:57.65ms
step:528/8000 train_time:30439ms step_avg:57.65ms
step:529/8000 train_time:30494ms step_avg:57.65ms
step:530/8000 train_time:30553ms step_avg:57.65ms
step:531/8000 train_time:30608ms step_avg:57.64ms
step:532/8000 train_time:30667ms step_avg:57.64ms
step:533/8000 train_time:30723ms step_avg:57.64ms
step:534/8000 train_time:30781ms step_avg:57.64ms
step:535/8000 train_time:30837ms step_avg:57.64ms
step:536/8000 train_time:30896ms step_avg:57.64ms
step:537/8000 train_time:30952ms step_avg:57.64ms
step:538/8000 train_time:31012ms step_avg:57.64ms
step:539/8000 train_time:31069ms step_avg:57.64ms
step:540/8000 train_time:31129ms step_avg:57.65ms
step:541/8000 train_time:31185ms step_avg:57.64ms
step:542/8000 train_time:31245ms step_avg:57.65ms
step:543/8000 train_time:31302ms step_avg:57.65ms
step:544/8000 train_time:31361ms step_avg:57.65ms
step:545/8000 train_time:31418ms step_avg:57.65ms
step:546/8000 train_time:31477ms step_avg:57.65ms
step:547/8000 train_time:31532ms step_avg:57.65ms
step:548/8000 train_time:31591ms step_avg:57.65ms
step:549/8000 train_time:31646ms step_avg:57.64ms
step:550/8000 train_time:31706ms step_avg:57.65ms
step:551/8000 train_time:31762ms step_avg:57.64ms
step:552/8000 train_time:31821ms step_avg:57.65ms
step:553/8000 train_time:31877ms step_avg:57.64ms
step:554/8000 train_time:31937ms step_avg:57.65ms
step:555/8000 train_time:31993ms step_avg:57.65ms
step:556/8000 train_time:32053ms step_avg:57.65ms
step:557/8000 train_time:32109ms step_avg:57.65ms
step:558/8000 train_time:32169ms step_avg:57.65ms
step:559/8000 train_time:32226ms step_avg:57.65ms
step:560/8000 train_time:32285ms step_avg:57.65ms
step:561/8000 train_time:32342ms step_avg:57.65ms
step:562/8000 train_time:32401ms step_avg:57.65ms
step:563/8000 train_time:32457ms step_avg:57.65ms
step:564/8000 train_time:32516ms step_avg:57.65ms
step:565/8000 train_time:32572ms step_avg:57.65ms
step:566/8000 train_time:32632ms step_avg:57.65ms
step:567/8000 train_time:32687ms step_avg:57.65ms
step:568/8000 train_time:32746ms step_avg:57.65ms
step:569/8000 train_time:32802ms step_avg:57.65ms
step:570/8000 train_time:32862ms step_avg:57.65ms
step:571/8000 train_time:32918ms step_avg:57.65ms
step:572/8000 train_time:32977ms step_avg:57.65ms
step:573/8000 train_time:33033ms step_avg:57.65ms
step:574/8000 train_time:33093ms step_avg:57.65ms
step:575/8000 train_time:33148ms step_avg:57.65ms
step:576/8000 train_time:33208ms step_avg:57.65ms
step:577/8000 train_time:33264ms step_avg:57.65ms
step:578/8000 train_time:33323ms step_avg:57.65ms
step:579/8000 train_time:33380ms step_avg:57.65ms
step:580/8000 train_time:33439ms step_avg:57.65ms
step:581/8000 train_time:33496ms step_avg:57.65ms
step:582/8000 train_time:33554ms step_avg:57.65ms
step:583/8000 train_time:33610ms step_avg:57.65ms
step:584/8000 train_time:33669ms step_avg:57.65ms
step:585/8000 train_time:33725ms step_avg:57.65ms
step:586/8000 train_time:33784ms step_avg:57.65ms
step:587/8000 train_time:33839ms step_avg:57.65ms
step:588/8000 train_time:33900ms step_avg:57.65ms
step:589/8000 train_time:33955ms step_avg:57.65ms
step:590/8000 train_time:34015ms step_avg:57.65ms
step:591/8000 train_time:34071ms step_avg:57.65ms
step:592/8000 train_time:34130ms step_avg:57.65ms
step:593/8000 train_time:34186ms step_avg:57.65ms
step:594/8000 train_time:34246ms step_avg:57.65ms
step:595/8000 train_time:34303ms step_avg:57.65ms
step:596/8000 train_time:34362ms step_avg:57.65ms
step:597/8000 train_time:34419ms step_avg:57.65ms
step:598/8000 train_time:34477ms step_avg:57.65ms
step:599/8000 train_time:34533ms step_avg:57.65ms
step:600/8000 train_time:34593ms step_avg:57.66ms
step:601/8000 train_time:34649ms step_avg:57.65ms
step:602/8000 train_time:34707ms step_avg:57.65ms
step:603/8000 train_time:34763ms step_avg:57.65ms
step:604/8000 train_time:34822ms step_avg:57.65ms
step:605/8000 train_time:34879ms step_avg:57.65ms
step:606/8000 train_time:34938ms step_avg:57.65ms
step:607/8000 train_time:34994ms step_avg:57.65ms
step:608/8000 train_time:35054ms step_avg:57.65ms
step:609/8000 train_time:35109ms step_avg:57.65ms
step:610/8000 train_time:35169ms step_avg:57.65ms
step:611/8000 train_time:35225ms step_avg:57.65ms
step:612/8000 train_time:35284ms step_avg:57.65ms
step:613/8000 train_time:35341ms step_avg:57.65ms
step:614/8000 train_time:35400ms step_avg:57.65ms
step:615/8000 train_time:35456ms step_avg:57.65ms
step:616/8000 train_time:35515ms step_avg:57.66ms
step:617/8000 train_time:35571ms step_avg:57.65ms
step:618/8000 train_time:35631ms step_avg:57.65ms
step:619/8000 train_time:35686ms step_avg:57.65ms
step:620/8000 train_time:35745ms step_avg:57.65ms
step:621/8000 train_time:35801ms step_avg:57.65ms
step:622/8000 train_time:35861ms step_avg:57.65ms
step:623/8000 train_time:35917ms step_avg:57.65ms
step:624/8000 train_time:35978ms step_avg:57.66ms
step:625/8000 train_time:36033ms step_avg:57.65ms
step:626/8000 train_time:36093ms step_avg:57.66ms
step:627/8000 train_time:36148ms step_avg:57.65ms
step:628/8000 train_time:36208ms step_avg:57.66ms
step:629/8000 train_time:36263ms step_avg:57.65ms
step:630/8000 train_time:36323ms step_avg:57.66ms
step:631/8000 train_time:36380ms step_avg:57.65ms
step:632/8000 train_time:36439ms step_avg:57.66ms
step:633/8000 train_time:36495ms step_avg:57.65ms
step:634/8000 train_time:36555ms step_avg:57.66ms
step:635/8000 train_time:36611ms step_avg:57.65ms
step:636/8000 train_time:36671ms step_avg:57.66ms
step:637/8000 train_time:36727ms step_avg:57.66ms
step:638/8000 train_time:36786ms step_avg:57.66ms
step:639/8000 train_time:36843ms step_avg:57.66ms
step:640/8000 train_time:36902ms step_avg:57.66ms
step:641/8000 train_time:36959ms step_avg:57.66ms
step:642/8000 train_time:37017ms step_avg:57.66ms
step:643/8000 train_time:37074ms step_avg:57.66ms
step:644/8000 train_time:37132ms step_avg:57.66ms
step:645/8000 train_time:37188ms step_avg:57.66ms
step:646/8000 train_time:37247ms step_avg:57.66ms
step:647/8000 train_time:37303ms step_avg:57.66ms
step:648/8000 train_time:37363ms step_avg:57.66ms
step:649/8000 train_time:37419ms step_avg:57.66ms
step:650/8000 train_time:37480ms step_avg:57.66ms
step:651/8000 train_time:37536ms step_avg:57.66ms
step:652/8000 train_time:37596ms step_avg:57.66ms
step:653/8000 train_time:37651ms step_avg:57.66ms
step:654/8000 train_time:37710ms step_avg:57.66ms
step:655/8000 train_time:37766ms step_avg:57.66ms
step:656/8000 train_time:37825ms step_avg:57.66ms
step:657/8000 train_time:37882ms step_avg:57.66ms
step:658/8000 train_time:37942ms step_avg:57.66ms
step:659/8000 train_time:37998ms step_avg:57.66ms
step:660/8000 train_time:38058ms step_avg:57.66ms
step:661/8000 train_time:38113ms step_avg:57.66ms
step:662/8000 train_time:38172ms step_avg:57.66ms
step:663/8000 train_time:38228ms step_avg:57.66ms
step:664/8000 train_time:38287ms step_avg:57.66ms
step:665/8000 train_time:38343ms step_avg:57.66ms
step:666/8000 train_time:38403ms step_avg:57.66ms
step:667/8000 train_time:38458ms step_avg:57.66ms
step:668/8000 train_time:38519ms step_avg:57.66ms
step:669/8000 train_time:38575ms step_avg:57.66ms
step:670/8000 train_time:38635ms step_avg:57.66ms
step:671/8000 train_time:38691ms step_avg:57.66ms
step:672/8000 train_time:38750ms step_avg:57.66ms
step:673/8000 train_time:38806ms step_avg:57.66ms
step:674/8000 train_time:38866ms step_avg:57.66ms
step:675/8000 train_time:38922ms step_avg:57.66ms
step:676/8000 train_time:38981ms step_avg:57.66ms
step:677/8000 train_time:39037ms step_avg:57.66ms
step:678/8000 train_time:39096ms step_avg:57.66ms
step:679/8000 train_time:39152ms step_avg:57.66ms
step:680/8000 train_time:39211ms step_avg:57.66ms
step:681/8000 train_time:39267ms step_avg:57.66ms
step:682/8000 train_time:39325ms step_avg:57.66ms
step:683/8000 train_time:39382ms step_avg:57.66ms
step:684/8000 train_time:39441ms step_avg:57.66ms
step:685/8000 train_time:39497ms step_avg:57.66ms
step:686/8000 train_time:39556ms step_avg:57.66ms
step:687/8000 train_time:39612ms step_avg:57.66ms
step:688/8000 train_time:39672ms step_avg:57.66ms
step:689/8000 train_time:39727ms step_avg:57.66ms
step:690/8000 train_time:39787ms step_avg:57.66ms
step:691/8000 train_time:39843ms step_avg:57.66ms
step:692/8000 train_time:39902ms step_avg:57.66ms
step:693/8000 train_time:39958ms step_avg:57.66ms
step:694/8000 train_time:40018ms step_avg:57.66ms
step:695/8000 train_time:40075ms step_avg:57.66ms
step:696/8000 train_time:40134ms step_avg:57.66ms
step:697/8000 train_time:40190ms step_avg:57.66ms
step:698/8000 train_time:40248ms step_avg:57.66ms
step:699/8000 train_time:40304ms step_avg:57.66ms
step:700/8000 train_time:40363ms step_avg:57.66ms
step:701/8000 train_time:40419ms step_avg:57.66ms
step:702/8000 train_time:40479ms step_avg:57.66ms
step:703/8000 train_time:40535ms step_avg:57.66ms
step:704/8000 train_time:40594ms step_avg:57.66ms
step:705/8000 train_time:40651ms step_avg:57.66ms
step:706/8000 train_time:40710ms step_avg:57.66ms
step:707/8000 train_time:40766ms step_avg:57.66ms
step:708/8000 train_time:40825ms step_avg:57.66ms
step:709/8000 train_time:40881ms step_avg:57.66ms
step:710/8000 train_time:40941ms step_avg:57.66ms
step:711/8000 train_time:40997ms step_avg:57.66ms
step:712/8000 train_time:41056ms step_avg:57.66ms
step:713/8000 train_time:41112ms step_avg:57.66ms
step:714/8000 train_time:41172ms step_avg:57.66ms
step:715/8000 train_time:41227ms step_avg:57.66ms
step:716/8000 train_time:41287ms step_avg:57.66ms
step:717/8000 train_time:41343ms step_avg:57.66ms
step:718/8000 train_time:41402ms step_avg:57.66ms
step:719/8000 train_time:41458ms step_avg:57.66ms
step:720/8000 train_time:41517ms step_avg:57.66ms
step:721/8000 train_time:41572ms step_avg:57.66ms
step:722/8000 train_time:41633ms step_avg:57.66ms
step:723/8000 train_time:41688ms step_avg:57.66ms
step:724/8000 train_time:41748ms step_avg:57.66ms
step:725/8000 train_time:41804ms step_avg:57.66ms
step:726/8000 train_time:41863ms step_avg:57.66ms
step:727/8000 train_time:41919ms step_avg:57.66ms
step:728/8000 train_time:41979ms step_avg:57.66ms
step:729/8000 train_time:42036ms step_avg:57.66ms
step:730/8000 train_time:42095ms step_avg:57.66ms
step:731/8000 train_time:42151ms step_avg:57.66ms
step:732/8000 train_time:42210ms step_avg:57.66ms
step:733/8000 train_time:42267ms step_avg:57.66ms
step:734/8000 train_time:42326ms step_avg:57.66ms
step:735/8000 train_time:42382ms step_avg:57.66ms
step:736/8000 train_time:42441ms step_avg:57.66ms
step:737/8000 train_time:42498ms step_avg:57.66ms
step:738/8000 train_time:42557ms step_avg:57.67ms
step:739/8000 train_time:42613ms step_avg:57.66ms
step:740/8000 train_time:42672ms step_avg:57.66ms
step:741/8000 train_time:42728ms step_avg:57.66ms
step:742/8000 train_time:42787ms step_avg:57.66ms
step:743/8000 train_time:42842ms step_avg:57.66ms
step:744/8000 train_time:42902ms step_avg:57.66ms
step:745/8000 train_time:42958ms step_avg:57.66ms
step:746/8000 train_time:43018ms step_avg:57.66ms
step:747/8000 train_time:43073ms step_avg:57.66ms
step:748/8000 train_time:43133ms step_avg:57.66ms
step:749/8000 train_time:43189ms step_avg:57.66ms
step:750/8000 train_time:43248ms step_avg:57.66ms
step:750/8000 val_loss:4.2172 train_time:43327ms step_avg:57.77ms
step:751/8000 train_time:43345ms step_avg:57.72ms
step:752/8000 train_time:43365ms step_avg:57.67ms
step:753/8000 train_time:43424ms step_avg:57.67ms
step:754/8000 train_time:43486ms step_avg:57.67ms
step:755/8000 train_time:43543ms step_avg:57.67ms
step:756/8000 train_time:43601ms step_avg:57.67ms
step:757/8000 train_time:43657ms step_avg:57.67ms
step:758/8000 train_time:43717ms step_avg:57.67ms
step:759/8000 train_time:43772ms step_avg:57.67ms
step:760/8000 train_time:43831ms step_avg:57.67ms
step:761/8000 train_time:43887ms step_avg:57.67ms
step:762/8000 train_time:43945ms step_avg:57.67ms
step:763/8000 train_time:44001ms step_avg:57.67ms
step:764/8000 train_time:44060ms step_avg:57.67ms
step:765/8000 train_time:44115ms step_avg:57.67ms
step:766/8000 train_time:44174ms step_avg:57.67ms
step:767/8000 train_time:44229ms step_avg:57.67ms
step:768/8000 train_time:44289ms step_avg:57.67ms
step:769/8000 train_time:44346ms step_avg:57.67ms
step:770/8000 train_time:44407ms step_avg:57.67ms
step:771/8000 train_time:44464ms step_avg:57.67ms
step:772/8000 train_time:44524ms step_avg:57.67ms
step:773/8000 train_time:44581ms step_avg:57.67ms
step:774/8000 train_time:44641ms step_avg:57.68ms
step:775/8000 train_time:44698ms step_avg:57.67ms
step:776/8000 train_time:44757ms step_avg:57.68ms
step:777/8000 train_time:44813ms step_avg:57.67ms
step:778/8000 train_time:44873ms step_avg:57.68ms
step:779/8000 train_time:44928ms step_avg:57.67ms
step:780/8000 train_time:44987ms step_avg:57.68ms
step:781/8000 train_time:45042ms step_avg:57.67ms
step:782/8000 train_time:45101ms step_avg:57.67ms
step:783/8000 train_time:45157ms step_avg:57.67ms
step:784/8000 train_time:45215ms step_avg:57.67ms
step:785/8000 train_time:45270ms step_avg:57.67ms
step:786/8000 train_time:45331ms step_avg:57.67ms
step:787/8000 train_time:45387ms step_avg:57.67ms
step:788/8000 train_time:45447ms step_avg:57.67ms
step:789/8000 train_time:45503ms step_avg:57.67ms
step:790/8000 train_time:45563ms step_avg:57.67ms
step:791/8000 train_time:45619ms step_avg:57.67ms
step:792/8000 train_time:45680ms step_avg:57.68ms
step:793/8000 train_time:45736ms step_avg:57.67ms
step:794/8000 train_time:45796ms step_avg:57.68ms
step:795/8000 train_time:45852ms step_avg:57.68ms
step:796/8000 train_time:45912ms step_avg:57.68ms
step:797/8000 train_time:45967ms step_avg:57.67ms
step:798/8000 train_time:46026ms step_avg:57.68ms
step:799/8000 train_time:46082ms step_avg:57.67ms
step:800/8000 train_time:46140ms step_avg:57.68ms
step:801/8000 train_time:46196ms step_avg:57.67ms
step:802/8000 train_time:46256ms step_avg:57.68ms
step:803/8000 train_time:46312ms step_avg:57.67ms
step:804/8000 train_time:46372ms step_avg:57.68ms
step:805/8000 train_time:46428ms step_avg:57.67ms
step:806/8000 train_time:46488ms step_avg:57.68ms
step:807/8000 train_time:46544ms step_avg:57.68ms
step:808/8000 train_time:46603ms step_avg:57.68ms
step:809/8000 train_time:46659ms step_avg:57.68ms
step:810/8000 train_time:46719ms step_avg:57.68ms
step:811/8000 train_time:46775ms step_avg:57.68ms
step:812/8000 train_time:46836ms step_avg:57.68ms
step:813/8000 train_time:46892ms step_avg:57.68ms
step:814/8000 train_time:46952ms step_avg:57.68ms
step:815/8000 train_time:47007ms step_avg:57.68ms
step:816/8000 train_time:47066ms step_avg:57.68ms
step:817/8000 train_time:47122ms step_avg:57.68ms
step:818/8000 train_time:47181ms step_avg:57.68ms
step:819/8000 train_time:47237ms step_avg:57.68ms
step:820/8000 train_time:47297ms step_avg:57.68ms
step:821/8000 train_time:47352ms step_avg:57.68ms
step:822/8000 train_time:47412ms step_avg:57.68ms
step:823/8000 train_time:47468ms step_avg:57.68ms
step:824/8000 train_time:47528ms step_avg:57.68ms
step:825/8000 train_time:47584ms step_avg:57.68ms
step:826/8000 train_time:47643ms step_avg:57.68ms
step:827/8000 train_time:47699ms step_avg:57.68ms
step:828/8000 train_time:47758ms step_avg:57.68ms
step:829/8000 train_time:47814ms step_avg:57.68ms
step:830/8000 train_time:47874ms step_avg:57.68ms
step:831/8000 train_time:47930ms step_avg:57.68ms
step:832/8000 train_time:47989ms step_avg:57.68ms
step:833/8000 train_time:48044ms step_avg:57.68ms
step:834/8000 train_time:48104ms step_avg:57.68ms
step:835/8000 train_time:48161ms step_avg:57.68ms
step:836/8000 train_time:48220ms step_avg:57.68ms
step:837/8000 train_time:48277ms step_avg:57.68ms
step:838/8000 train_time:48336ms step_avg:57.68ms
step:839/8000 train_time:48392ms step_avg:57.68ms
step:840/8000 train_time:48452ms step_avg:57.68ms
step:841/8000 train_time:48507ms step_avg:57.68ms
step:842/8000 train_time:48566ms step_avg:57.68ms
step:843/8000 train_time:48622ms step_avg:57.68ms
step:844/8000 train_time:48682ms step_avg:57.68ms
step:845/8000 train_time:48738ms step_avg:57.68ms
step:846/8000 train_time:48797ms step_avg:57.68ms
step:847/8000 train_time:48854ms step_avg:57.68ms
step:848/8000 train_time:48913ms step_avg:57.68ms
step:849/8000 train_time:48969ms step_avg:57.68ms
step:850/8000 train_time:49028ms step_avg:57.68ms
step:851/8000 train_time:49084ms step_avg:57.68ms
step:852/8000 train_time:49143ms step_avg:57.68ms
step:853/8000 train_time:49200ms step_avg:57.68ms
step:854/8000 train_time:49259ms step_avg:57.68ms
step:855/8000 train_time:49315ms step_avg:57.68ms
step:856/8000 train_time:49375ms step_avg:57.68ms
step:857/8000 train_time:49431ms step_avg:57.68ms
step:858/8000 train_time:49491ms step_avg:57.68ms
step:859/8000 train_time:49546ms step_avg:57.68ms
step:860/8000 train_time:49606ms step_avg:57.68ms
step:861/8000 train_time:49662ms step_avg:57.68ms
step:862/8000 train_time:49722ms step_avg:57.68ms
step:863/8000 train_time:49779ms step_avg:57.68ms
step:864/8000 train_time:49838ms step_avg:57.68ms
step:865/8000 train_time:49894ms step_avg:57.68ms
step:866/8000 train_time:49954ms step_avg:57.68ms
step:867/8000 train_time:50009ms step_avg:57.68ms
step:868/8000 train_time:50069ms step_avg:57.68ms
step:869/8000 train_time:50124ms step_avg:57.68ms
step:870/8000 train_time:50184ms step_avg:57.68ms
step:871/8000 train_time:50240ms step_avg:57.68ms
step:872/8000 train_time:50300ms step_avg:57.68ms
step:873/8000 train_time:50356ms step_avg:57.68ms
step:874/8000 train_time:50415ms step_avg:57.68ms
step:875/8000 train_time:50471ms step_avg:57.68ms
step:876/8000 train_time:50530ms step_avg:57.68ms
step:877/8000 train_time:50585ms step_avg:57.68ms
step:878/8000 train_time:50645ms step_avg:57.68ms
step:879/8000 train_time:50702ms step_avg:57.68ms
step:880/8000 train_time:50761ms step_avg:57.68ms
step:881/8000 train_time:50818ms step_avg:57.68ms
step:882/8000 train_time:50877ms step_avg:57.68ms
step:883/8000 train_time:50933ms step_avg:57.68ms
step:884/8000 train_time:50992ms step_avg:57.68ms
step:885/8000 train_time:51048ms step_avg:57.68ms
step:886/8000 train_time:51108ms step_avg:57.68ms
step:887/8000 train_time:51164ms step_avg:57.68ms
step:888/8000 train_time:51223ms step_avg:57.68ms
step:889/8000 train_time:51279ms step_avg:57.68ms
step:890/8000 train_time:51339ms step_avg:57.68ms
step:891/8000 train_time:51395ms step_avg:57.68ms
step:892/8000 train_time:51455ms step_avg:57.68ms
step:893/8000 train_time:51511ms step_avg:57.68ms
step:894/8000 train_time:51570ms step_avg:57.68ms
step:895/8000 train_time:51625ms step_avg:57.68ms
step:896/8000 train_time:51686ms step_avg:57.69ms
step:897/8000 train_time:51742ms step_avg:57.68ms
step:898/8000 train_time:51801ms step_avg:57.69ms
step:899/8000 train_time:51858ms step_avg:57.68ms
step:900/8000 train_time:51917ms step_avg:57.69ms
step:901/8000 train_time:51973ms step_avg:57.68ms
step:902/8000 train_time:52033ms step_avg:57.69ms
step:903/8000 train_time:52089ms step_avg:57.68ms
step:904/8000 train_time:52149ms step_avg:57.69ms
step:905/8000 train_time:52204ms step_avg:57.68ms
step:906/8000 train_time:52264ms step_avg:57.69ms
step:907/8000 train_time:52322ms step_avg:57.69ms
step:908/8000 train_time:52382ms step_avg:57.69ms
step:909/8000 train_time:52438ms step_avg:57.69ms
step:910/8000 train_time:52497ms step_avg:57.69ms
step:911/8000 train_time:52553ms step_avg:57.69ms
step:912/8000 train_time:52613ms step_avg:57.69ms
step:913/8000 train_time:52669ms step_avg:57.69ms
step:914/8000 train_time:52728ms step_avg:57.69ms
step:915/8000 train_time:52784ms step_avg:57.69ms
step:916/8000 train_time:52844ms step_avg:57.69ms
step:917/8000 train_time:52899ms step_avg:57.69ms
step:918/8000 train_time:52960ms step_avg:57.69ms
step:919/8000 train_time:53016ms step_avg:57.69ms
step:920/8000 train_time:53077ms step_avg:57.69ms
step:921/8000 train_time:53133ms step_avg:57.69ms
step:922/8000 train_time:53192ms step_avg:57.69ms
step:923/8000 train_time:53248ms step_avg:57.69ms
step:924/8000 train_time:53309ms step_avg:57.69ms
step:925/8000 train_time:53365ms step_avg:57.69ms
step:926/8000 train_time:53424ms step_avg:57.69ms
step:927/8000 train_time:53480ms step_avg:57.69ms
step:928/8000 train_time:53540ms step_avg:57.69ms
step:929/8000 train_time:53596ms step_avg:57.69ms
step:930/8000 train_time:53656ms step_avg:57.69ms
step:931/8000 train_time:53712ms step_avg:57.69ms
step:932/8000 train_time:53771ms step_avg:57.69ms
step:933/8000 train_time:53827ms step_avg:57.69ms
step:934/8000 train_time:53887ms step_avg:57.69ms
step:935/8000 train_time:53942ms step_avg:57.69ms
step:936/8000 train_time:54002ms step_avg:57.69ms
step:937/8000 train_time:54058ms step_avg:57.69ms
step:938/8000 train_time:54118ms step_avg:57.69ms
step:939/8000 train_time:54173ms step_avg:57.69ms
step:940/8000 train_time:54235ms step_avg:57.70ms
step:941/8000 train_time:54291ms step_avg:57.69ms
step:942/8000 train_time:54350ms step_avg:57.70ms
step:943/8000 train_time:54405ms step_avg:57.69ms
step:944/8000 train_time:54465ms step_avg:57.70ms
step:945/8000 train_time:54521ms step_avg:57.69ms
step:946/8000 train_time:54581ms step_avg:57.70ms
step:947/8000 train_time:54638ms step_avg:57.70ms
step:948/8000 train_time:54697ms step_avg:57.70ms
step:949/8000 train_time:54752ms step_avg:57.69ms
step:950/8000 train_time:54813ms step_avg:57.70ms
step:951/8000 train_time:54868ms step_avg:57.70ms
step:952/8000 train_time:54929ms step_avg:57.70ms
step:953/8000 train_time:54985ms step_avg:57.70ms
step:954/8000 train_time:55044ms step_avg:57.70ms
step:955/8000 train_time:55100ms step_avg:57.70ms
step:956/8000 train_time:55159ms step_avg:57.70ms
step:957/8000 train_time:55215ms step_avg:57.70ms
step:958/8000 train_time:55275ms step_avg:57.70ms
step:959/8000 train_time:55332ms step_avg:57.70ms
step:960/8000 train_time:55390ms step_avg:57.70ms
step:961/8000 train_time:55445ms step_avg:57.70ms
step:962/8000 train_time:55505ms step_avg:57.70ms
step:963/8000 train_time:55561ms step_avg:57.70ms
step:964/8000 train_time:55620ms step_avg:57.70ms
step:965/8000 train_time:55677ms step_avg:57.70ms
step:966/8000 train_time:55737ms step_avg:57.70ms
step:967/8000 train_time:55793ms step_avg:57.70ms
step:968/8000 train_time:55852ms step_avg:57.70ms
step:969/8000 train_time:55908ms step_avg:57.70ms
step:970/8000 train_time:55967ms step_avg:57.70ms
step:971/8000 train_time:56023ms step_avg:57.70ms
step:972/8000 train_time:56082ms step_avg:57.70ms
step:973/8000 train_time:56139ms step_avg:57.70ms
step:974/8000 train_time:56198ms step_avg:57.70ms
step:975/8000 train_time:56254ms step_avg:57.70ms
step:976/8000 train_time:56314ms step_avg:57.70ms
step:977/8000 train_time:56369ms step_avg:57.70ms
step:978/8000 train_time:56429ms step_avg:57.70ms
step:979/8000 train_time:56485ms step_avg:57.70ms
step:980/8000 train_time:56544ms step_avg:57.70ms
step:981/8000 train_time:56600ms step_avg:57.70ms
step:982/8000 train_time:56659ms step_avg:57.70ms
step:983/8000 train_time:56716ms step_avg:57.70ms
step:984/8000 train_time:56775ms step_avg:57.70ms
step:985/8000 train_time:56831ms step_avg:57.70ms
step:986/8000 train_time:56891ms step_avg:57.70ms
step:987/8000 train_time:56946ms step_avg:57.70ms
step:988/8000 train_time:57007ms step_avg:57.70ms
step:989/8000 train_time:57062ms step_avg:57.70ms
step:990/8000 train_time:57122ms step_avg:57.70ms
step:991/8000 train_time:57178ms step_avg:57.70ms
step:992/8000 train_time:57238ms step_avg:57.70ms
step:993/8000 train_time:57294ms step_avg:57.70ms
step:994/8000 train_time:57353ms step_avg:57.70ms
step:995/8000 train_time:57408ms step_avg:57.70ms
step:996/8000 train_time:57468ms step_avg:57.70ms
step:997/8000 train_time:57524ms step_avg:57.70ms
step:998/8000 train_time:57583ms step_avg:57.70ms
step:999/8000 train_time:57640ms step_avg:57.70ms
step:1000/8000 train_time:57699ms step_avg:57.70ms
step:1000/8000 val_loss:4.1165 train_time:57779ms step_avg:57.78ms
step:1001/8000 train_time:57797ms step_avg:57.74ms
step:1002/8000 train_time:57817ms step_avg:57.70ms
step:1003/8000 train_time:57873ms step_avg:57.70ms
step:1004/8000 train_time:57937ms step_avg:57.71ms
step:1005/8000 train_time:57993ms step_avg:57.70ms
step:1006/8000 train_time:58055ms step_avg:57.71ms
step:1007/8000 train_time:58112ms step_avg:57.71ms
step:1008/8000 train_time:58170ms step_avg:57.71ms
step:1009/8000 train_time:58226ms step_avg:57.71ms
step:1010/8000 train_time:58284ms step_avg:57.71ms
step:1011/8000 train_time:58340ms step_avg:57.71ms
step:1012/8000 train_time:58398ms step_avg:57.71ms
step:1013/8000 train_time:58454ms step_avg:57.70ms
step:1014/8000 train_time:58513ms step_avg:57.71ms
step:1015/8000 train_time:58569ms step_avg:57.70ms
step:1016/8000 train_time:58627ms step_avg:57.70ms
step:1017/8000 train_time:58683ms step_avg:57.70ms
step:1018/8000 train_time:58744ms step_avg:57.71ms
step:1019/8000 train_time:58801ms step_avg:57.70ms
step:1020/8000 train_time:58864ms step_avg:57.71ms
step:1021/8000 train_time:58920ms step_avg:57.71ms
step:1022/8000 train_time:58982ms step_avg:57.71ms
step:1023/8000 train_time:59038ms step_avg:57.71ms
step:1024/8000 train_time:59098ms step_avg:57.71ms
step:1025/8000 train_time:59154ms step_avg:57.71ms
step:1026/8000 train_time:59213ms step_avg:57.71ms
step:1027/8000 train_time:59269ms step_avg:57.71ms
step:1028/8000 train_time:59328ms step_avg:57.71ms
step:1029/8000 train_time:59384ms step_avg:57.71ms
step:1030/8000 train_time:59443ms step_avg:57.71ms
step:1031/8000 train_time:59498ms step_avg:57.71ms
step:1032/8000 train_time:59558ms step_avg:57.71ms
step:1033/8000 train_time:59613ms step_avg:57.71ms
step:1034/8000 train_time:59673ms step_avg:57.71ms
step:1035/8000 train_time:59729ms step_avg:57.71ms
step:1036/8000 train_time:59789ms step_avg:57.71ms
step:1037/8000 train_time:59846ms step_avg:57.71ms
step:1038/8000 train_time:59907ms step_avg:57.71ms
step:1039/8000 train_time:59965ms step_avg:57.71ms
step:1040/8000 train_time:60025ms step_avg:57.72ms
step:1041/8000 train_time:60080ms step_avg:57.71ms
step:1042/8000 train_time:60141ms step_avg:57.72ms
step:1043/8000 train_time:60197ms step_avg:57.72ms
step:1044/8000 train_time:60257ms step_avg:57.72ms
step:1045/8000 train_time:60313ms step_avg:57.72ms
step:1046/8000 train_time:60372ms step_avg:57.72ms
step:1047/8000 train_time:60429ms step_avg:57.72ms
step:1048/8000 train_time:60488ms step_avg:57.72ms
step:1049/8000 train_time:60543ms step_avg:57.72ms
step:1050/8000 train_time:60603ms step_avg:57.72ms
step:1051/8000 train_time:60659ms step_avg:57.72ms
step:1052/8000 train_time:60719ms step_avg:57.72ms
step:1053/8000 train_time:60774ms step_avg:57.72ms
step:1054/8000 train_time:60836ms step_avg:57.72ms
step:1055/8000 train_time:60894ms step_avg:57.72ms
step:1056/8000 train_time:60953ms step_avg:57.72ms
step:1057/8000 train_time:61009ms step_avg:57.72ms
step:1058/8000 train_time:61069ms step_avg:57.72ms
step:1059/8000 train_time:61125ms step_avg:57.72ms
step:1060/8000 train_time:61185ms step_avg:57.72ms
step:1061/8000 train_time:61241ms step_avg:57.72ms
step:1062/8000 train_time:61301ms step_avg:57.72ms
step:1063/8000 train_time:61357ms step_avg:57.72ms
step:1064/8000 train_time:61416ms step_avg:57.72ms
step:1065/8000 train_time:61472ms step_avg:57.72ms
step:1066/8000 train_time:61531ms step_avg:57.72ms
step:1067/8000 train_time:61587ms step_avg:57.72ms
step:1068/8000 train_time:61646ms step_avg:57.72ms
step:1069/8000 train_time:61702ms step_avg:57.72ms
step:1070/8000 train_time:61761ms step_avg:57.72ms
step:1071/8000 train_time:61817ms step_avg:57.72ms
step:1072/8000 train_time:61879ms step_avg:57.72ms
step:1073/8000 train_time:61935ms step_avg:57.72ms
step:1074/8000 train_time:61995ms step_avg:57.72ms
step:1075/8000 train_time:62053ms step_avg:57.72ms
step:1076/8000 train_time:62112ms step_avg:57.73ms
step:1077/8000 train_time:62169ms step_avg:57.72ms
step:1078/8000 train_time:62228ms step_avg:57.73ms
step:1079/8000 train_time:62284ms step_avg:57.72ms
step:1080/8000 train_time:62344ms step_avg:57.73ms
step:1081/8000 train_time:62400ms step_avg:57.72ms
step:1082/8000 train_time:62460ms step_avg:57.73ms
step:1083/8000 train_time:62516ms step_avg:57.72ms
step:1084/8000 train_time:62576ms step_avg:57.73ms
step:1085/8000 train_time:62632ms step_avg:57.73ms
step:1086/8000 train_time:62690ms step_avg:57.73ms
step:1087/8000 train_time:62746ms step_avg:57.72ms
step:1088/8000 train_time:62806ms step_avg:57.73ms
step:1089/8000 train_time:62861ms step_avg:57.72ms
step:1090/8000 train_time:62922ms step_avg:57.73ms
step:1091/8000 train_time:62977ms step_avg:57.72ms
step:1092/8000 train_time:63037ms step_avg:57.73ms
step:1093/8000 train_time:63093ms step_avg:57.72ms
step:1094/8000 train_time:63155ms step_avg:57.73ms
step:1095/8000 train_time:63211ms step_avg:57.73ms
step:1096/8000 train_time:63270ms step_avg:57.73ms
step:1097/8000 train_time:63327ms step_avg:57.73ms
step:1098/8000 train_time:63386ms step_avg:57.73ms
step:1099/8000 train_time:63442ms step_avg:57.73ms
step:1100/8000 train_time:63501ms step_avg:57.73ms
step:1101/8000 train_time:63557ms step_avg:57.73ms
step:1102/8000 train_time:63618ms step_avg:57.73ms
step:1103/8000 train_time:63673ms step_avg:57.73ms
step:1104/8000 train_time:63733ms step_avg:57.73ms
step:1105/8000 train_time:63789ms step_avg:57.73ms
step:1106/8000 train_time:63848ms step_avg:57.73ms
step:1107/8000 train_time:63904ms step_avg:57.73ms
step:1108/8000 train_time:63965ms step_avg:57.73ms
step:1109/8000 train_time:64020ms step_avg:57.73ms
step:1110/8000 train_time:64080ms step_avg:57.73ms
step:1111/8000 train_time:64136ms step_avg:57.73ms
step:1112/8000 train_time:64198ms step_avg:57.73ms
step:1113/8000 train_time:64253ms step_avg:57.73ms
step:1114/8000 train_time:64313ms step_avg:57.73ms
step:1115/8000 train_time:64370ms step_avg:57.73ms
step:1116/8000 train_time:64429ms step_avg:57.73ms
step:1117/8000 train_time:64486ms step_avg:57.73ms
step:1118/8000 train_time:64545ms step_avg:57.73ms
step:1119/8000 train_time:64601ms step_avg:57.73ms
step:1120/8000 train_time:64661ms step_avg:57.73ms
step:1121/8000 train_time:64717ms step_avg:57.73ms
step:1122/8000 train_time:64777ms step_avg:57.73ms
step:1123/8000 train_time:64833ms step_avg:57.73ms
step:1124/8000 train_time:64892ms step_avg:57.73ms
step:1125/8000 train_time:64948ms step_avg:57.73ms
step:1126/8000 train_time:65008ms step_avg:57.73ms
step:1127/8000 train_time:65064ms step_avg:57.73ms
step:1128/8000 train_time:65125ms step_avg:57.73ms
step:1129/8000 train_time:65181ms step_avg:57.73ms
step:1130/8000 train_time:65240ms step_avg:57.73ms
step:1131/8000 train_time:65296ms step_avg:57.73ms
step:1132/8000 train_time:65357ms step_avg:57.74ms
step:1133/8000 train_time:65413ms step_avg:57.73ms
step:1134/8000 train_time:65474ms step_avg:57.74ms
step:1135/8000 train_time:65530ms step_avg:57.74ms
step:1136/8000 train_time:65589ms step_avg:57.74ms
step:1137/8000 train_time:65646ms step_avg:57.74ms
step:1138/8000 train_time:65705ms step_avg:57.74ms
step:1139/8000 train_time:65761ms step_avg:57.74ms
step:1140/8000 train_time:65821ms step_avg:57.74ms
step:1141/8000 train_time:65877ms step_avg:57.74ms
step:1142/8000 train_time:65938ms step_avg:57.74ms
step:1143/8000 train_time:65993ms step_avg:57.74ms
step:1144/8000 train_time:66053ms step_avg:57.74ms
step:1145/8000 train_time:66110ms step_avg:57.74ms
step:1146/8000 train_time:66169ms step_avg:57.74ms
step:1147/8000 train_time:66225ms step_avg:57.74ms
step:1148/8000 train_time:66285ms step_avg:57.74ms
step:1149/8000 train_time:66341ms step_avg:57.74ms
step:1150/8000 train_time:66402ms step_avg:57.74ms
step:1151/8000 train_time:66457ms step_avg:57.74ms
step:1152/8000 train_time:66519ms step_avg:57.74ms
step:1153/8000 train_time:66575ms step_avg:57.74ms
step:1154/8000 train_time:66635ms step_avg:57.74ms
step:1155/8000 train_time:66691ms step_avg:57.74ms
step:1156/8000 train_time:66750ms step_avg:57.74ms
step:1157/8000 train_time:66807ms step_avg:57.74ms
step:1158/8000 train_time:66867ms step_avg:57.74ms
step:1159/8000 train_time:66923ms step_avg:57.74ms
step:1160/8000 train_time:66982ms step_avg:57.74ms
step:1161/8000 train_time:67038ms step_avg:57.74ms
step:1162/8000 train_time:67098ms step_avg:57.74ms
step:1163/8000 train_time:67153ms step_avg:57.74ms
step:1164/8000 train_time:67213ms step_avg:57.74ms
step:1165/8000 train_time:67270ms step_avg:57.74ms
step:1166/8000 train_time:67329ms step_avg:57.74ms
step:1167/8000 train_time:67385ms step_avg:57.74ms
step:1168/8000 train_time:67445ms step_avg:57.74ms
step:1169/8000 train_time:67501ms step_avg:57.74ms
step:1170/8000 train_time:67561ms step_avg:57.74ms
step:1171/8000 train_time:67617ms step_avg:57.74ms
step:1172/8000 train_time:67676ms step_avg:57.74ms
step:1173/8000 train_time:67732ms step_avg:57.74ms
step:1174/8000 train_time:67792ms step_avg:57.74ms
step:1175/8000 train_time:67848ms step_avg:57.74ms
step:1176/8000 train_time:67908ms step_avg:57.74ms
step:1177/8000 train_time:67964ms step_avg:57.74ms
step:1178/8000 train_time:68022ms step_avg:57.74ms
step:1179/8000 train_time:68079ms step_avg:57.74ms
step:1180/8000 train_time:68137ms step_avg:57.74ms
step:1181/8000 train_time:68193ms step_avg:57.74ms
step:1182/8000 train_time:68253ms step_avg:57.74ms
step:1183/8000 train_time:68310ms step_avg:57.74ms
step:1184/8000 train_time:68369ms step_avg:57.74ms
step:1185/8000 train_time:68425ms step_avg:57.74ms
step:1186/8000 train_time:68484ms step_avg:57.74ms
step:1187/8000 train_time:68540ms step_avg:57.74ms
step:1188/8000 train_time:68601ms step_avg:57.74ms
step:1189/8000 train_time:68657ms step_avg:57.74ms
step:1190/8000 train_time:68716ms step_avg:57.74ms
step:1191/8000 train_time:68772ms step_avg:57.74ms
step:1192/8000 train_time:68831ms step_avg:57.74ms
step:1193/8000 train_time:68888ms step_avg:57.74ms
step:1194/8000 train_time:68947ms step_avg:57.74ms
step:1195/8000 train_time:69004ms step_avg:57.74ms
step:1196/8000 train_time:69063ms step_avg:57.75ms
step:1197/8000 train_time:69119ms step_avg:57.74ms
step:1198/8000 train_time:69179ms step_avg:57.75ms
step:1199/8000 train_time:69235ms step_avg:57.74ms
step:1200/8000 train_time:69294ms step_avg:57.74ms
step:1201/8000 train_time:69350ms step_avg:57.74ms
step:1202/8000 train_time:69409ms step_avg:57.74ms
step:1203/8000 train_time:69466ms step_avg:57.74ms
step:1204/8000 train_time:69525ms step_avg:57.74ms
step:1205/8000 train_time:69581ms step_avg:57.74ms
step:1206/8000 train_time:69641ms step_avg:57.75ms
step:1207/8000 train_time:69697ms step_avg:57.74ms
step:1208/8000 train_time:69756ms step_avg:57.75ms
step:1209/8000 train_time:69812ms step_avg:57.74ms
step:1210/8000 train_time:69871ms step_avg:57.74ms
step:1211/8000 train_time:69928ms step_avg:57.74ms
step:1212/8000 train_time:69987ms step_avg:57.75ms
step:1213/8000 train_time:70043ms step_avg:57.74ms
step:1214/8000 train_time:70102ms step_avg:57.74ms
step:1215/8000 train_time:70158ms step_avg:57.74ms
step:1216/8000 train_time:70219ms step_avg:57.75ms
step:1217/8000 train_time:70275ms step_avg:57.74ms
step:1218/8000 train_time:70335ms step_avg:57.75ms
step:1219/8000 train_time:70391ms step_avg:57.75ms
step:1220/8000 train_time:70451ms step_avg:57.75ms
step:1221/8000 train_time:70508ms step_avg:57.75ms
step:1222/8000 train_time:70567ms step_avg:57.75ms
step:1223/8000 train_time:70624ms step_avg:57.75ms
step:1224/8000 train_time:70682ms step_avg:57.75ms
step:1225/8000 train_time:70738ms step_avg:57.75ms
step:1226/8000 train_time:70797ms step_avg:57.75ms
step:1227/8000 train_time:70853ms step_avg:57.74ms
step:1228/8000 train_time:70913ms step_avg:57.75ms
step:1229/8000 train_time:70970ms step_avg:57.75ms
step:1230/8000 train_time:71029ms step_avg:57.75ms
step:1231/8000 train_time:71086ms step_avg:57.75ms
step:1232/8000 train_time:71145ms step_avg:57.75ms
step:1233/8000 train_time:71202ms step_avg:57.75ms
step:1234/8000 train_time:71261ms step_avg:57.75ms
step:1235/8000 train_time:71317ms step_avg:57.75ms
step:1236/8000 train_time:71377ms step_avg:57.75ms
step:1237/8000 train_time:71433ms step_avg:57.75ms
step:1238/8000 train_time:71493ms step_avg:57.75ms
step:1239/8000 train_time:71550ms step_avg:57.75ms
step:1240/8000 train_time:71610ms step_avg:57.75ms
step:1241/8000 train_time:71666ms step_avg:57.75ms
step:1242/8000 train_time:71726ms step_avg:57.75ms
step:1243/8000 train_time:71782ms step_avg:57.75ms
step:1244/8000 train_time:71841ms step_avg:57.75ms
step:1245/8000 train_time:71897ms step_avg:57.75ms
step:1246/8000 train_time:71958ms step_avg:57.75ms
step:1247/8000 train_time:72014ms step_avg:57.75ms
step:1248/8000 train_time:72072ms step_avg:57.75ms
step:1249/8000 train_time:72129ms step_avg:57.75ms
step:1250/8000 train_time:72189ms step_avg:57.75ms
step:1250/8000 val_loss:4.0403 train_time:72269ms step_avg:57.81ms
step:1251/8000 train_time:72287ms step_avg:57.78ms
step:1252/8000 train_time:72309ms step_avg:57.75ms
step:1253/8000 train_time:72365ms step_avg:57.75ms
step:1254/8000 train_time:72429ms step_avg:57.76ms
step:1255/8000 train_time:72485ms step_avg:57.76ms
step:1256/8000 train_time:72546ms step_avg:57.76ms
step:1257/8000 train_time:72602ms step_avg:57.76ms
step:1258/8000 train_time:72662ms step_avg:57.76ms
step:1259/8000 train_time:72717ms step_avg:57.76ms
step:1260/8000 train_time:72777ms step_avg:57.76ms
step:1261/8000 train_time:72832ms step_avg:57.76ms
step:1262/8000 train_time:72890ms step_avg:57.76ms
step:1263/8000 train_time:72946ms step_avg:57.76ms
step:1264/8000 train_time:73005ms step_avg:57.76ms
step:1265/8000 train_time:73060ms step_avg:57.76ms
step:1266/8000 train_time:73118ms step_avg:57.76ms
step:1267/8000 train_time:73174ms step_avg:57.75ms
step:1268/8000 train_time:73233ms step_avg:57.76ms
step:1269/8000 train_time:73291ms step_avg:57.76ms
step:1270/8000 train_time:73351ms step_avg:57.76ms
step:1271/8000 train_time:73409ms step_avg:57.76ms
step:1272/8000 train_time:73470ms step_avg:57.76ms
step:1273/8000 train_time:73526ms step_avg:57.76ms
step:1274/8000 train_time:73586ms step_avg:57.76ms
step:1275/8000 train_time:73642ms step_avg:57.76ms
step:1276/8000 train_time:73702ms step_avg:57.76ms
step:1277/8000 train_time:73757ms step_avg:57.76ms
step:1278/8000 train_time:73816ms step_avg:57.76ms
step:1279/8000 train_time:73872ms step_avg:57.76ms
step:1280/8000 train_time:73931ms step_avg:57.76ms
step:1281/8000 train_time:73987ms step_avg:57.76ms
step:1282/8000 train_time:74046ms step_avg:57.76ms
step:1283/8000 train_time:74102ms step_avg:57.76ms
step:1284/8000 train_time:74161ms step_avg:57.76ms
step:1285/8000 train_time:74217ms step_avg:57.76ms
step:1286/8000 train_time:74276ms step_avg:57.76ms
step:1287/8000 train_time:74333ms step_avg:57.76ms
step:1288/8000 train_time:74394ms step_avg:57.76ms
step:1289/8000 train_time:74452ms step_avg:57.76ms
step:1290/8000 train_time:74512ms step_avg:57.76ms
step:1291/8000 train_time:74569ms step_avg:57.76ms
step:1292/8000 train_time:74628ms step_avg:57.76ms
step:1293/8000 train_time:74684ms step_avg:57.76ms
step:1294/8000 train_time:74743ms step_avg:57.76ms
step:1295/8000 train_time:74799ms step_avg:57.76ms
step:1296/8000 train_time:74859ms step_avg:57.76ms
step:1297/8000 train_time:74915ms step_avg:57.76ms
step:1298/8000 train_time:74974ms step_avg:57.76ms
step:1299/8000 train_time:75029ms step_avg:57.76ms
step:1300/8000 train_time:75088ms step_avg:57.76ms
step:1301/8000 train_time:75144ms step_avg:57.76ms
step:1302/8000 train_time:75203ms step_avg:57.76ms
step:1303/8000 train_time:75259ms step_avg:57.76ms
step:1304/8000 train_time:75320ms step_avg:57.76ms
step:1305/8000 train_time:75377ms step_avg:57.76ms
step:1306/8000 train_time:75436ms step_avg:57.76ms
step:1307/8000 train_time:75494ms step_avg:57.76ms
step:1308/8000 train_time:75553ms step_avg:57.76ms
step:1309/8000 train_time:75611ms step_avg:57.76ms
step:1310/8000 train_time:75670ms step_avg:57.76ms
step:1311/8000 train_time:75726ms step_avg:57.76ms
step:1312/8000 train_time:75785ms step_avg:57.76ms
step:1313/8000 train_time:75841ms step_avg:57.76ms
step:1314/8000 train_time:75901ms step_avg:57.76ms
step:1315/8000 train_time:75957ms step_avg:57.76ms
step:1316/8000 train_time:76016ms step_avg:57.76ms
step:1317/8000 train_time:76072ms step_avg:57.76ms
step:1318/8000 train_time:76131ms step_avg:57.76ms
step:1319/8000 train_time:76187ms step_avg:57.76ms
step:1320/8000 train_time:76247ms step_avg:57.76ms
step:1321/8000 train_time:76303ms step_avg:57.76ms
step:1322/8000 train_time:76363ms step_avg:57.76ms
step:1323/8000 train_time:76418ms step_avg:57.76ms
step:1324/8000 train_time:76479ms step_avg:57.76ms
step:1325/8000 train_time:76536ms step_avg:57.76ms
step:1326/8000 train_time:76595ms step_avg:57.76ms
step:1327/8000 train_time:76653ms step_avg:57.76ms
step:1328/8000 train_time:76713ms step_avg:57.77ms
step:1329/8000 train_time:76769ms step_avg:57.76ms
step:1330/8000 train_time:76828ms step_avg:57.77ms
step:1331/8000 train_time:76884ms step_avg:57.76ms
step:1332/8000 train_time:76944ms step_avg:57.77ms
step:1333/8000 train_time:77000ms step_avg:57.76ms
step:1334/8000 train_time:77060ms step_avg:57.77ms
step:1335/8000 train_time:77116ms step_avg:57.76ms
step:1336/8000 train_time:77175ms step_avg:57.77ms
step:1337/8000 train_time:77231ms step_avg:57.76ms
step:1338/8000 train_time:77291ms step_avg:57.77ms
step:1339/8000 train_time:77347ms step_avg:57.76ms
step:1340/8000 train_time:77407ms step_avg:57.77ms
step:1341/8000 train_time:77463ms step_avg:57.76ms
step:1342/8000 train_time:77522ms step_avg:57.77ms
step:1343/8000 train_time:77579ms step_avg:57.77ms
step:1344/8000 train_time:77638ms step_avg:57.77ms
step:1345/8000 train_time:77695ms step_avg:57.77ms
step:1346/8000 train_time:77754ms step_avg:57.77ms
step:1347/8000 train_time:77809ms step_avg:57.76ms
step:1348/8000 train_time:77869ms step_avg:57.77ms
step:1349/8000 train_time:77924ms step_avg:57.76ms
step:1350/8000 train_time:77985ms step_avg:57.77ms
step:1351/8000 train_time:78041ms step_avg:57.77ms
step:1352/8000 train_time:78101ms step_avg:57.77ms
step:1353/8000 train_time:78156ms step_avg:57.77ms
step:1354/8000 train_time:78216ms step_avg:57.77ms
step:1355/8000 train_time:78272ms step_avg:57.77ms
step:1356/8000 train_time:78332ms step_avg:57.77ms
step:1357/8000 train_time:78389ms step_avg:57.77ms
step:1358/8000 train_time:78448ms step_avg:57.77ms
step:1359/8000 train_time:78504ms step_avg:57.77ms
step:1360/8000 train_time:78564ms step_avg:57.77ms
step:1361/8000 train_time:78620ms step_avg:57.77ms
step:1362/8000 train_time:78682ms step_avg:57.77ms
step:1363/8000 train_time:78738ms step_avg:57.77ms
step:1364/8000 train_time:78798ms step_avg:57.77ms
step:1365/8000 train_time:78856ms step_avg:57.77ms
step:1366/8000 train_time:78915ms step_avg:57.77ms
step:1367/8000 train_time:78971ms step_avg:57.77ms
step:1368/8000 train_time:79030ms step_avg:57.77ms
step:1369/8000 train_time:79086ms step_avg:57.77ms
step:1370/8000 train_time:79145ms step_avg:57.77ms
step:1371/8000 train_time:79202ms step_avg:57.77ms
step:1372/8000 train_time:79262ms step_avg:57.77ms
step:1373/8000 train_time:79317ms step_avg:57.77ms
step:1374/8000 train_time:79378ms step_avg:57.77ms
step:1375/8000 train_time:79435ms step_avg:57.77ms
step:1376/8000 train_time:79494ms step_avg:57.77ms
step:1377/8000 train_time:79550ms step_avg:57.77ms
step:1378/8000 train_time:79611ms step_avg:57.77ms
step:1379/8000 train_time:79668ms step_avg:57.77ms
step:1380/8000 train_time:79727ms step_avg:57.77ms
step:1381/8000 train_time:79784ms step_avg:57.77ms
step:1382/8000 train_time:79844ms step_avg:57.77ms
step:1383/8000 train_time:79900ms step_avg:57.77ms
step:1384/8000 train_time:79960ms step_avg:57.77ms
step:1385/8000 train_time:80016ms step_avg:57.77ms
step:1386/8000 train_time:80075ms step_avg:57.77ms
step:1387/8000 train_time:80132ms step_avg:57.77ms
step:1388/8000 train_time:80191ms step_avg:57.77ms
step:1389/8000 train_time:80246ms step_avg:57.77ms
step:1390/8000 train_time:80307ms step_avg:57.77ms
step:1391/8000 train_time:80363ms step_avg:57.77ms
step:1392/8000 train_time:80423ms step_avg:57.77ms
step:1393/8000 train_time:80479ms step_avg:57.77ms
step:1394/8000 train_time:80539ms step_avg:57.78ms
step:1395/8000 train_time:80596ms step_avg:57.77ms
step:1396/8000 train_time:80655ms step_avg:57.78ms
step:1397/8000 train_time:80712ms step_avg:57.78ms
step:1398/8000 train_time:80771ms step_avg:57.78ms
step:1399/8000 train_time:80828ms step_avg:57.78ms
step:1400/8000 train_time:80887ms step_avg:57.78ms
step:1401/8000 train_time:80943ms step_avg:57.78ms
step:1402/8000 train_time:81002ms step_avg:57.78ms
step:1403/8000 train_time:81058ms step_avg:57.77ms
step:1404/8000 train_time:81118ms step_avg:57.78ms
step:1405/8000 train_time:81174ms step_avg:57.78ms
step:1406/8000 train_time:81234ms step_avg:57.78ms
step:1407/8000 train_time:81291ms step_avg:57.78ms
step:1408/8000 train_time:81350ms step_avg:57.78ms
step:1409/8000 train_time:81406ms step_avg:57.78ms
step:1410/8000 train_time:81466ms step_avg:57.78ms
step:1411/8000 train_time:81522ms step_avg:57.78ms
step:1412/8000 train_time:81582ms step_avg:57.78ms
step:1413/8000 train_time:81637ms step_avg:57.78ms
step:1414/8000 train_time:81698ms step_avg:57.78ms
step:1415/8000 train_time:81756ms step_avg:57.78ms
step:1416/8000 train_time:81815ms step_avg:57.78ms
step:1417/8000 train_time:81872ms step_avg:57.78ms
step:1418/8000 train_time:81931ms step_avg:57.78ms
step:1419/8000 train_time:81988ms step_avg:57.78ms
step:1420/8000 train_time:82046ms step_avg:57.78ms
step:1421/8000 train_time:82102ms step_avg:57.78ms
step:1422/8000 train_time:82162ms step_avg:57.78ms
step:1423/8000 train_time:82217ms step_avg:57.78ms
step:1424/8000 train_time:82277ms step_avg:57.78ms
step:1425/8000 train_time:82334ms step_avg:57.78ms
step:1426/8000 train_time:82393ms step_avg:57.78ms
step:1427/8000 train_time:82450ms step_avg:57.78ms
step:1428/8000 train_time:82509ms step_avg:57.78ms
step:1429/8000 train_time:82565ms step_avg:57.78ms
step:1430/8000 train_time:82625ms step_avg:57.78ms
step:1431/8000 train_time:82681ms step_avg:57.78ms
step:1432/8000 train_time:82741ms step_avg:57.78ms
step:1433/8000 train_time:82797ms step_avg:57.78ms
step:1434/8000 train_time:82856ms step_avg:57.78ms
step:1435/8000 train_time:82913ms step_avg:57.78ms
step:1436/8000 train_time:82972ms step_avg:57.78ms
step:1437/8000 train_time:83028ms step_avg:57.78ms
step:1438/8000 train_time:83088ms step_avg:57.78ms
step:1439/8000 train_time:83145ms step_avg:57.78ms
step:1440/8000 train_time:83203ms step_avg:57.78ms
step:1441/8000 train_time:83259ms step_avg:57.78ms
step:1442/8000 train_time:83320ms step_avg:57.78ms
step:1443/8000 train_time:83377ms step_avg:57.78ms
step:1444/8000 train_time:83436ms step_avg:57.78ms
step:1445/8000 train_time:83492ms step_avg:57.78ms
step:1446/8000 train_time:83551ms step_avg:57.78ms
step:1447/8000 train_time:83607ms step_avg:57.78ms
step:1448/8000 train_time:83667ms step_avg:57.78ms
step:1449/8000 train_time:83723ms step_avg:57.78ms
step:1450/8000 train_time:83784ms step_avg:57.78ms
step:1451/8000 train_time:83839ms step_avg:57.78ms
step:1452/8000 train_time:83901ms step_avg:57.78ms
step:1453/8000 train_time:83958ms step_avg:57.78ms
step:1454/8000 train_time:84017ms step_avg:57.78ms
step:1455/8000 train_time:84073ms step_avg:57.78ms
step:1456/8000 train_time:84132ms step_avg:57.78ms
step:1457/8000 train_time:84188ms step_avg:57.78ms
step:1458/8000 train_time:84247ms step_avg:57.78ms
step:1459/8000 train_time:84304ms step_avg:57.78ms
step:1460/8000 train_time:84363ms step_avg:57.78ms
step:1461/8000 train_time:84419ms step_avg:57.78ms
step:1462/8000 train_time:84480ms step_avg:57.78ms
step:1463/8000 train_time:84536ms step_avg:57.78ms
step:1464/8000 train_time:84595ms step_avg:57.78ms
step:1465/8000 train_time:84652ms step_avg:57.78ms
step:1466/8000 train_time:84712ms step_avg:57.78ms
step:1467/8000 train_time:84768ms step_avg:57.78ms
step:1468/8000 train_time:84827ms step_avg:57.78ms
step:1469/8000 train_time:84883ms step_avg:57.78ms
step:1470/8000 train_time:84942ms step_avg:57.78ms
step:1471/8000 train_time:84998ms step_avg:57.78ms
step:1472/8000 train_time:85057ms step_avg:57.78ms
step:1473/8000 train_time:85113ms step_avg:57.78ms
step:1474/8000 train_time:85174ms step_avg:57.78ms
step:1475/8000 train_time:85230ms step_avg:57.78ms
step:1476/8000 train_time:85290ms step_avg:57.78ms
step:1477/8000 train_time:85345ms step_avg:57.78ms
step:1478/8000 train_time:85405ms step_avg:57.78ms
step:1479/8000 train_time:85461ms step_avg:57.78ms
step:1480/8000 train_time:85520ms step_avg:57.78ms
step:1481/8000 train_time:85576ms step_avg:57.78ms
step:1482/8000 train_time:85636ms step_avg:57.78ms
step:1483/8000 train_time:85692ms step_avg:57.78ms
step:1484/8000 train_time:85752ms step_avg:57.78ms
step:1485/8000 train_time:85809ms step_avg:57.78ms
step:1486/8000 train_time:85869ms step_avg:57.79ms
step:1487/8000 train_time:85925ms step_avg:57.78ms
step:1488/8000 train_time:85985ms step_avg:57.79ms
step:1489/8000 train_time:86040ms step_avg:57.78ms
step:1490/8000 train_time:86101ms step_avg:57.79ms
step:1491/8000 train_time:86157ms step_avg:57.78ms
step:1492/8000 train_time:86216ms step_avg:57.79ms
step:1493/8000 train_time:86274ms step_avg:57.79ms
step:1494/8000 train_time:86334ms step_avg:57.79ms
step:1495/8000 train_time:86391ms step_avg:57.79ms
step:1496/8000 train_time:86450ms step_avg:57.79ms
step:1497/8000 train_time:86506ms step_avg:57.79ms
step:1498/8000 train_time:86565ms step_avg:57.79ms
step:1499/8000 train_time:86621ms step_avg:57.79ms
step:1500/8000 train_time:86682ms step_avg:57.79ms
step:1500/8000 val_loss:3.9912 train_time:86761ms step_avg:57.84ms
step:1501/8000 train_time:86779ms step_avg:57.81ms
step:1502/8000 train_time:86799ms step_avg:57.79ms
step:1503/8000 train_time:86854ms step_avg:57.79ms
step:1504/8000 train_time:86923ms step_avg:57.79ms
step:1505/8000 train_time:86981ms step_avg:57.79ms
step:1506/8000 train_time:87040ms step_avg:57.80ms
step:1507/8000 train_time:87096ms step_avg:57.79ms
step:1508/8000 train_time:87155ms step_avg:57.79ms
step:1509/8000 train_time:87211ms step_avg:57.79ms
step:1510/8000 train_time:87270ms step_avg:57.79ms
step:1511/8000 train_time:87326ms step_avg:57.79ms
step:1512/8000 train_time:87384ms step_avg:57.79ms
step:1513/8000 train_time:87441ms step_avg:57.79ms
step:1514/8000 train_time:87498ms step_avg:57.79ms
step:1515/8000 train_time:87553ms step_avg:57.79ms
step:1516/8000 train_time:87612ms step_avg:57.79ms
step:1517/8000 train_time:87668ms step_avg:57.79ms
step:1518/8000 train_time:87727ms step_avg:57.79ms
step:1519/8000 train_time:87783ms step_avg:57.79ms
step:1520/8000 train_time:87846ms step_avg:57.79ms
step:1521/8000 train_time:87903ms step_avg:57.79ms
step:1522/8000 train_time:87963ms step_avg:57.79ms
step:1523/8000 train_time:88020ms step_avg:57.79ms
step:1524/8000 train_time:88080ms step_avg:57.80ms
step:1525/8000 train_time:88136ms step_avg:57.79ms
step:1526/8000 train_time:88194ms step_avg:57.79ms
step:1527/8000 train_time:88250ms step_avg:57.79ms
step:1528/8000 train_time:88310ms step_avg:57.79ms
step:1529/8000 train_time:88366ms step_avg:57.79ms
step:1530/8000 train_time:88424ms step_avg:57.79ms
step:1531/8000 train_time:88480ms step_avg:57.79ms
step:1532/8000 train_time:88539ms step_avg:57.79ms
step:1533/8000 train_time:88594ms step_avg:57.79ms
step:1534/8000 train_time:88653ms step_avg:57.79ms
step:1535/8000 train_time:88709ms step_avg:57.79ms
step:1536/8000 train_time:88769ms step_avg:57.79ms
step:1537/8000 train_time:88826ms step_avg:57.79ms
step:1538/8000 train_time:88886ms step_avg:57.79ms
step:1539/8000 train_time:88944ms step_avg:57.79ms
step:1540/8000 train_time:89003ms step_avg:57.79ms
step:1541/8000 train_time:89059ms step_avg:57.79ms
step:1542/8000 train_time:89119ms step_avg:57.79ms
step:1543/8000 train_time:89175ms step_avg:57.79ms
step:1544/8000 train_time:89236ms step_avg:57.80ms
step:1545/8000 train_time:89291ms step_avg:57.79ms
step:1546/8000 train_time:89350ms step_avg:57.79ms
step:1547/8000 train_time:89405ms step_avg:57.79ms
step:1548/8000 train_time:89464ms step_avg:57.79ms
step:1549/8000 train_time:89520ms step_avg:57.79ms
step:1550/8000 train_time:89579ms step_avg:57.79ms
step:1551/8000 train_time:89635ms step_avg:57.79ms
step:1552/8000 train_time:89695ms step_avg:57.79ms
step:1553/8000 train_time:89751ms step_avg:57.79ms
step:1554/8000 train_time:89810ms step_avg:57.79ms
step:1555/8000 train_time:89867ms step_avg:57.79ms
step:1556/8000 train_time:89927ms step_avg:57.79ms
step:1557/8000 train_time:89984ms step_avg:57.79ms
step:1558/8000 train_time:90044ms step_avg:57.79ms
step:1559/8000 train_time:90100ms step_avg:57.79ms
step:1560/8000 train_time:90160ms step_avg:57.80ms
step:1561/8000 train_time:90216ms step_avg:57.79ms
step:1562/8000 train_time:90276ms step_avg:57.80ms
step:1563/8000 train_time:90332ms step_avg:57.79ms
step:1564/8000 train_time:90391ms step_avg:57.80ms
step:1565/8000 train_time:90448ms step_avg:57.79ms
step:1566/8000 train_time:90507ms step_avg:57.79ms
step:1567/8000 train_time:90563ms step_avg:57.79ms
step:1568/8000 train_time:90622ms step_avg:57.79ms
step:1569/8000 train_time:90678ms step_avg:57.79ms
step:1570/8000 train_time:90738ms step_avg:57.79ms
step:1571/8000 train_time:90794ms step_avg:57.79ms
step:1572/8000 train_time:90854ms step_avg:57.80ms
step:1573/8000 train_time:90911ms step_avg:57.79ms
step:1574/8000 train_time:90970ms step_avg:57.80ms
step:1575/8000 train_time:91027ms step_avg:57.79ms
step:1576/8000 train_time:91086ms step_avg:57.80ms
step:1577/8000 train_time:91143ms step_avg:57.80ms
step:1578/8000 train_time:91202ms step_avg:57.80ms
step:1579/8000 train_time:91258ms step_avg:57.79ms
step:1580/8000 train_time:91318ms step_avg:57.80ms
step:1581/8000 train_time:91373ms step_avg:57.79ms
step:1582/8000 train_time:91432ms step_avg:57.80ms
step:1583/8000 train_time:91489ms step_avg:57.79ms
step:1584/8000 train_time:91548ms step_avg:57.80ms
step:1585/8000 train_time:91604ms step_avg:57.79ms
step:1586/8000 train_time:91663ms step_avg:57.79ms
step:1587/8000 train_time:91719ms step_avg:57.79ms
step:1588/8000 train_time:91778ms step_avg:57.79ms
step:1589/8000 train_time:91834ms step_avg:57.79ms
step:1590/8000 train_time:91893ms step_avg:57.79ms
step:1591/8000 train_time:91950ms step_avg:57.79ms
step:1592/8000 train_time:92010ms step_avg:57.80ms
step:1593/8000 train_time:92067ms step_avg:57.79ms
step:1594/8000 train_time:92126ms step_avg:57.80ms
step:1595/8000 train_time:92182ms step_avg:57.79ms
step:1596/8000 train_time:92242ms step_avg:57.80ms
step:1597/8000 train_time:92298ms step_avg:57.79ms
step:1598/8000 train_time:92357ms step_avg:57.80ms
step:1599/8000 train_time:92413ms step_avg:57.79ms
step:1600/8000 train_time:92472ms step_avg:57.79ms
step:1601/8000 train_time:92528ms step_avg:57.79ms
step:1602/8000 train_time:92587ms step_avg:57.79ms
step:1603/8000 train_time:92643ms step_avg:57.79ms
step:1604/8000 train_time:92702ms step_avg:57.79ms
step:1605/8000 train_time:92758ms step_avg:57.79ms
step:1606/8000 train_time:92817ms step_avg:57.79ms
step:1607/8000 train_time:92873ms step_avg:57.79ms
step:1608/8000 train_time:92933ms step_avg:57.79ms
step:1609/8000 train_time:92990ms step_avg:57.79ms
step:1610/8000 train_time:93049ms step_avg:57.79ms
step:1611/8000 train_time:93105ms step_avg:57.79ms
step:1612/8000 train_time:93164ms step_avg:57.79ms
step:1613/8000 train_time:93221ms step_avg:57.79ms
step:1614/8000 train_time:93280ms step_avg:57.79ms
step:1615/8000 train_time:93336ms step_avg:57.79ms
step:1616/8000 train_time:93395ms step_avg:57.79ms
step:1617/8000 train_time:93451ms step_avg:57.79ms
step:1618/8000 train_time:93511ms step_avg:57.79ms
step:1619/8000 train_time:93567ms step_avg:57.79ms
step:1620/8000 train_time:93626ms step_avg:57.79ms
step:1621/8000 train_time:93682ms step_avg:57.79ms
step:1622/8000 train_time:93742ms step_avg:57.79ms
step:1623/8000 train_time:93798ms step_avg:57.79ms
step:1624/8000 train_time:93857ms step_avg:57.79ms
step:1625/8000 train_time:93913ms step_avg:57.79ms
step:1626/8000 train_time:93973ms step_avg:57.79ms
step:1627/8000 train_time:94029ms step_avg:57.79ms
step:1628/8000 train_time:94089ms step_avg:57.79ms
step:1629/8000 train_time:94145ms step_avg:57.79ms
step:1630/8000 train_time:94205ms step_avg:57.79ms
step:1631/8000 train_time:94261ms step_avg:57.79ms
step:1632/8000 train_time:94321ms step_avg:57.79ms
step:1633/8000 train_time:94377ms step_avg:57.79ms
step:1634/8000 train_time:94437ms step_avg:57.79ms
step:1635/8000 train_time:94493ms step_avg:57.79ms
step:1636/8000 train_time:94552ms step_avg:57.79ms
step:1637/8000 train_time:94608ms step_avg:57.79ms
step:1638/8000 train_time:94668ms step_avg:57.79ms
step:1639/8000 train_time:94724ms step_avg:57.79ms
step:1640/8000 train_time:94784ms step_avg:57.80ms
step:1641/8000 train_time:94841ms step_avg:57.79ms
step:1642/8000 train_time:94899ms step_avg:57.80ms
step:1643/8000 train_time:94955ms step_avg:57.79ms
step:1644/8000 train_time:95016ms step_avg:57.80ms
step:1645/8000 train_time:95072ms step_avg:57.79ms
step:1646/8000 train_time:95131ms step_avg:57.80ms
step:1647/8000 train_time:95188ms step_avg:57.79ms
step:1648/8000 train_time:95248ms step_avg:57.80ms
step:1649/8000 train_time:95304ms step_avg:57.80ms
step:1650/8000 train_time:95363ms step_avg:57.80ms
step:1651/8000 train_time:95420ms step_avg:57.80ms
step:1652/8000 train_time:95479ms step_avg:57.80ms
step:1653/8000 train_time:95534ms step_avg:57.79ms
step:1654/8000 train_time:95595ms step_avg:57.80ms
step:1655/8000 train_time:95651ms step_avg:57.80ms
step:1656/8000 train_time:95710ms step_avg:57.80ms
step:1657/8000 train_time:95766ms step_avg:57.80ms
step:1658/8000 train_time:95826ms step_avg:57.80ms
step:1659/8000 train_time:95882ms step_avg:57.79ms
step:1660/8000 train_time:95942ms step_avg:57.80ms
step:1661/8000 train_time:95998ms step_avg:57.80ms
step:1662/8000 train_time:96058ms step_avg:57.80ms
step:1663/8000 train_time:96113ms step_avg:57.80ms
step:1664/8000 train_time:96174ms step_avg:57.80ms
step:1665/8000 train_time:96230ms step_avg:57.80ms
step:1666/8000 train_time:96289ms step_avg:57.80ms
step:1667/8000 train_time:96345ms step_avg:57.80ms
step:1668/8000 train_time:96405ms step_avg:57.80ms
step:1669/8000 train_time:96461ms step_avg:57.80ms
step:1670/8000 train_time:96520ms step_avg:57.80ms
step:1671/8000 train_time:96576ms step_avg:57.80ms
step:1672/8000 train_time:96636ms step_avg:57.80ms
step:1673/8000 train_time:96692ms step_avg:57.80ms
step:1674/8000 train_time:96751ms step_avg:57.80ms
step:1675/8000 train_time:96807ms step_avg:57.80ms
step:1676/8000 train_time:96867ms step_avg:57.80ms
step:1677/8000 train_time:96923ms step_avg:57.80ms
step:1678/8000 train_time:96982ms step_avg:57.80ms
step:1679/8000 train_time:97038ms step_avg:57.80ms
step:1680/8000 train_time:97098ms step_avg:57.80ms
step:1681/8000 train_time:97154ms step_avg:57.80ms
step:1682/8000 train_time:97214ms step_avg:57.80ms
step:1683/8000 train_time:97271ms step_avg:57.80ms
step:1684/8000 train_time:97330ms step_avg:57.80ms
step:1685/8000 train_time:97386ms step_avg:57.80ms
step:1686/8000 train_time:97446ms step_avg:57.80ms
step:1687/8000 train_time:97503ms step_avg:57.80ms
step:1688/8000 train_time:97562ms step_avg:57.80ms
step:1689/8000 train_time:97618ms step_avg:57.80ms
step:1690/8000 train_time:97677ms step_avg:57.80ms
step:1691/8000 train_time:97734ms step_avg:57.80ms
step:1692/8000 train_time:97793ms step_avg:57.80ms
step:1693/8000 train_time:97850ms step_avg:57.80ms
step:1694/8000 train_time:97909ms step_avg:57.80ms
step:1695/8000 train_time:97965ms step_avg:57.80ms
step:1696/8000 train_time:98025ms step_avg:57.80ms
step:1697/8000 train_time:98081ms step_avg:57.80ms
step:1698/8000 train_time:98140ms step_avg:57.80ms
step:1699/8000 train_time:98197ms step_avg:57.80ms
step:1700/8000 train_time:98255ms step_avg:57.80ms
step:1701/8000 train_time:98311ms step_avg:57.80ms
step:1702/8000 train_time:98370ms step_avg:57.80ms
step:1703/8000 train_time:98427ms step_avg:57.80ms
step:1704/8000 train_time:98486ms step_avg:57.80ms
step:1705/8000 train_time:98542ms step_avg:57.80ms
step:1706/8000 train_time:98602ms step_avg:57.80ms
step:1707/8000 train_time:98658ms step_avg:57.80ms
step:1708/8000 train_time:98718ms step_avg:57.80ms
step:1709/8000 train_time:98774ms step_avg:57.80ms
step:1710/8000 train_time:98833ms step_avg:57.80ms
step:1711/8000 train_time:98889ms step_avg:57.80ms
step:1712/8000 train_time:98949ms step_avg:57.80ms
step:1713/8000 train_time:99005ms step_avg:57.80ms
step:1714/8000 train_time:99064ms step_avg:57.80ms
step:1715/8000 train_time:99121ms step_avg:57.80ms
step:1716/8000 train_time:99179ms step_avg:57.80ms
step:1717/8000 train_time:99236ms step_avg:57.80ms
step:1718/8000 train_time:99296ms step_avg:57.80ms
step:1719/8000 train_time:99353ms step_avg:57.80ms
step:1720/8000 train_time:99411ms step_avg:57.80ms
step:1721/8000 train_time:99468ms step_avg:57.80ms
step:1722/8000 train_time:99527ms step_avg:57.80ms
step:1723/8000 train_time:99583ms step_avg:57.80ms
step:1724/8000 train_time:99643ms step_avg:57.80ms
step:1725/8000 train_time:99699ms step_avg:57.80ms
step:1726/8000 train_time:99758ms step_avg:57.80ms
