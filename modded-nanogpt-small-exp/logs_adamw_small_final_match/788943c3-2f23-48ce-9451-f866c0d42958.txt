import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 8000  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 3000  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_final_match", exist_ok=True)
    logfile = f"logs_adamw_small_final_match/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=6e-4,  betas=(0.8, 0.8), eps=1e-10, weight_decay=9e-1, fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_scheduled_iterations + args.num_extension_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_final_match/{run_id}.txt", exist_ok=True)
            torch.save(log, f"logs_adamw_small_final_match/{run_id}.txt/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 22:27:12 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   27C    P0             113W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   27C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   25C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   25C    P0             115W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   24C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   26C    P0             113W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   25C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   25C    P0             109W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/11000 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/11000 train_time:93ms step_avg:92.91ms
step:2/11000 train_time:202ms step_avg:100.89ms
step:3/11000 train_time:222ms step_avg:73.96ms
step:4/11000 train_time:243ms step_avg:60.75ms
step:5/11000 train_time:293ms step_avg:58.50ms
step:6/11000 train_time:351ms step_avg:58.48ms
step:7/11000 train_time:406ms step_avg:57.97ms
step:8/11000 train_time:464ms step_avg:58.02ms
step:9/11000 train_time:520ms step_avg:57.75ms
step:10/11000 train_time:578ms step_avg:57.80ms
step:11/11000 train_time:634ms step_avg:57.60ms
step:12/11000 train_time:692ms step_avg:57.65ms
step:13/11000 train_time:748ms step_avg:57.51ms
step:14/11000 train_time:805ms step_avg:57.53ms
step:15/11000 train_time:861ms step_avg:57.43ms
step:16/11000 train_time:919ms step_avg:57.45ms
step:17/11000 train_time:975ms step_avg:57.35ms
step:18/11000 train_time:1034ms step_avg:57.44ms
step:19/11000 train_time:1092ms step_avg:57.47ms
step:20/11000 train_time:1156ms step_avg:57.80ms
step:21/11000 train_time:1215ms step_avg:57.86ms
step:22/11000 train_time:1275ms step_avg:57.94ms
step:23/11000 train_time:1331ms step_avg:57.89ms
step:24/11000 train_time:1390ms step_avg:57.91ms
step:25/11000 train_time:1445ms step_avg:57.81ms
step:26/11000 train_time:1506ms step_avg:57.90ms
step:27/11000 train_time:1561ms step_avg:57.83ms
step:28/11000 train_time:1619ms step_avg:57.82ms
step:29/11000 train_time:1675ms step_avg:57.75ms
step:30/11000 train_time:1732ms step_avg:57.75ms
step:31/11000 train_time:1788ms step_avg:57.68ms
step:32/11000 train_time:1846ms step_avg:57.69ms
step:33/11000 train_time:1902ms step_avg:57.64ms
step:34/11000 train_time:1960ms step_avg:57.64ms
step:35/11000 train_time:2015ms step_avg:57.58ms
step:36/11000 train_time:2075ms step_avg:57.64ms
step:37/11000 train_time:2133ms step_avg:57.64ms
step:38/11000 train_time:2192ms step_avg:57.69ms
step:39/11000 train_time:2249ms step_avg:57.68ms
step:40/11000 train_time:2309ms step_avg:57.72ms
step:41/11000 train_time:2365ms step_avg:57.69ms
step:42/11000 train_time:2424ms step_avg:57.72ms
step:43/11000 train_time:2480ms step_avg:57.67ms
step:44/11000 train_time:2538ms step_avg:57.69ms
step:45/11000 train_time:2594ms step_avg:57.65ms
step:46/11000 train_time:2653ms step_avg:57.68ms
step:47/11000 train_time:2709ms step_avg:57.64ms
step:48/11000 train_time:2768ms step_avg:57.67ms
step:49/11000 train_time:2823ms step_avg:57.62ms
step:50/11000 train_time:2883ms step_avg:57.66ms
step:51/11000 train_time:2938ms step_avg:57.61ms
step:52/11000 train_time:2997ms step_avg:57.64ms
step:53/11000 train_time:3054ms step_avg:57.63ms
step:54/11000 train_time:3113ms step_avg:57.65ms
step:55/11000 train_time:3170ms step_avg:57.64ms
step:56/11000 train_time:3229ms step_avg:57.66ms
step:57/11000 train_time:3286ms step_avg:57.64ms
step:58/11000 train_time:3345ms step_avg:57.67ms
step:59/11000 train_time:3401ms step_avg:57.64ms
step:60/11000 train_time:3460ms step_avg:57.67ms
step:61/11000 train_time:3516ms step_avg:57.64ms
step:62/11000 train_time:3575ms step_avg:57.66ms
step:63/11000 train_time:3631ms step_avg:57.64ms
step:64/11000 train_time:3689ms step_avg:57.65ms
step:65/11000 train_time:3746ms step_avg:57.63ms
step:66/11000 train_time:3804ms step_avg:57.63ms
step:67/11000 train_time:3859ms step_avg:57.60ms
step:68/11000 train_time:3918ms step_avg:57.61ms
step:69/11000 train_time:3974ms step_avg:57.59ms
step:70/11000 train_time:4032ms step_avg:57.60ms
step:71/11000 train_time:4088ms step_avg:57.58ms
step:72/11000 train_time:4147ms step_avg:57.60ms
step:73/11000 train_time:4204ms step_avg:57.59ms
step:74/11000 train_time:4263ms step_avg:57.61ms
step:75/11000 train_time:4320ms step_avg:57.60ms
step:76/11000 train_time:4379ms step_avg:57.61ms
step:77/11000 train_time:4436ms step_avg:57.61ms
step:78/11000 train_time:4494ms step_avg:57.61ms
step:79/11000 train_time:4550ms step_avg:57.60ms
step:80/11000 train_time:4609ms step_avg:57.61ms
step:81/11000 train_time:4665ms step_avg:57.59ms
step:82/11000 train_time:4724ms step_avg:57.61ms
step:83/11000 train_time:4780ms step_avg:57.58ms
step:84/11000 train_time:4839ms step_avg:57.60ms
step:85/11000 train_time:4894ms step_avg:57.58ms
step:86/11000 train_time:4953ms step_avg:57.59ms
step:87/11000 train_time:5009ms step_avg:57.57ms
step:88/11000 train_time:5067ms step_avg:57.58ms
step:89/11000 train_time:5123ms step_avg:57.56ms
step:90/11000 train_time:5182ms step_avg:57.57ms
step:91/11000 train_time:5238ms step_avg:57.56ms
step:92/11000 train_time:5297ms step_avg:57.58ms
step:93/11000 train_time:5354ms step_avg:57.57ms
step:94/11000 train_time:5413ms step_avg:57.58ms
step:95/11000 train_time:5470ms step_avg:57.58ms
step:96/11000 train_time:5528ms step_avg:57.59ms
step:97/11000 train_time:5584ms step_avg:57.57ms
step:98/11000 train_time:5643ms step_avg:57.58ms
step:99/11000 train_time:5699ms step_avg:57.56ms
step:100/11000 train_time:5758ms step_avg:57.58ms
step:101/11000 train_time:5814ms step_avg:57.56ms
step:102/11000 train_time:5873ms step_avg:57.57ms
step:103/11000 train_time:5929ms step_avg:57.56ms
step:104/11000 train_time:5988ms step_avg:57.58ms
step:105/11000 train_time:6044ms step_avg:57.56ms
step:106/11000 train_time:6103ms step_avg:57.57ms
step:107/11000 train_time:6159ms step_avg:57.56ms
step:108/11000 train_time:6217ms step_avg:57.57ms
step:109/11000 train_time:6274ms step_avg:57.56ms
step:110/11000 train_time:6333ms step_avg:57.57ms
step:111/11000 train_time:6389ms step_avg:57.56ms
step:112/11000 train_time:6449ms step_avg:57.58ms
step:113/11000 train_time:6505ms step_avg:57.57ms
step:114/11000 train_time:6564ms step_avg:57.58ms
step:115/11000 train_time:6620ms step_avg:57.57ms
step:116/11000 train_time:6679ms step_avg:57.58ms
step:117/11000 train_time:6735ms step_avg:57.56ms
step:118/11000 train_time:6793ms step_avg:57.57ms
step:119/11000 train_time:6850ms step_avg:57.56ms
step:120/11000 train_time:6908ms step_avg:57.57ms
step:121/11000 train_time:6964ms step_avg:57.56ms
step:122/11000 train_time:7023ms step_avg:57.56ms
step:123/11000 train_time:7079ms step_avg:57.55ms
step:124/11000 train_time:7137ms step_avg:57.56ms
step:125/11000 train_time:7194ms step_avg:57.55ms
step:126/11000 train_time:7252ms step_avg:57.56ms
step:127/11000 train_time:7309ms step_avg:57.55ms
step:128/11000 train_time:7367ms step_avg:57.56ms
step:129/11000 train_time:7424ms step_avg:57.55ms
step:130/11000 train_time:7482ms step_avg:57.55ms
step:131/11000 train_time:7538ms step_avg:57.54ms
step:132/11000 train_time:7597ms step_avg:57.55ms
step:133/11000 train_time:7654ms step_avg:57.55ms
step:134/11000 train_time:7713ms step_avg:57.56ms
step:135/11000 train_time:7768ms step_avg:57.54ms
step:136/11000 train_time:7827ms step_avg:57.55ms
step:137/11000 train_time:7883ms step_avg:57.54ms
step:138/11000 train_time:7942ms step_avg:57.55ms
step:139/11000 train_time:7998ms step_avg:57.54ms
step:140/11000 train_time:8057ms step_avg:57.55ms
step:141/11000 train_time:8113ms step_avg:57.54ms
step:142/11000 train_time:8171ms step_avg:57.54ms
step:143/11000 train_time:8227ms step_avg:57.53ms
step:144/11000 train_time:8286ms step_avg:57.54ms
step:145/11000 train_time:8342ms step_avg:57.53ms
step:146/11000 train_time:8401ms step_avg:57.54ms
step:147/11000 train_time:8457ms step_avg:57.53ms
step:148/11000 train_time:8516ms step_avg:57.54ms
step:149/11000 train_time:8571ms step_avg:57.53ms
step:150/11000 train_time:8630ms step_avg:57.54ms
step:151/11000 train_time:8686ms step_avg:57.53ms
step:152/11000 train_time:8745ms step_avg:57.53ms
step:153/11000 train_time:8801ms step_avg:57.52ms
step:154/11000 train_time:8860ms step_avg:57.53ms
step:155/11000 train_time:8916ms step_avg:57.52ms
step:156/11000 train_time:8974ms step_avg:57.53ms
step:157/11000 train_time:9031ms step_avg:57.52ms
step:158/11000 train_time:9090ms step_avg:57.53ms
step:159/11000 train_time:9146ms step_avg:57.52ms
step:160/11000 train_time:9204ms step_avg:57.52ms
step:161/11000 train_time:9261ms step_avg:57.52ms
step:162/11000 train_time:9320ms step_avg:57.53ms
step:163/11000 train_time:9376ms step_avg:57.52ms
step:164/11000 train_time:9435ms step_avg:57.53ms
step:165/11000 train_time:9491ms step_avg:57.52ms
step:166/11000 train_time:9549ms step_avg:57.53ms
step:167/11000 train_time:9605ms step_avg:57.52ms
step:168/11000 train_time:9664ms step_avg:57.52ms
step:169/11000 train_time:9720ms step_avg:57.52ms
step:170/11000 train_time:9779ms step_avg:57.52ms
step:171/11000 train_time:9835ms step_avg:57.52ms
step:172/11000 train_time:9894ms step_avg:57.52ms
step:173/11000 train_time:9951ms step_avg:57.52ms
step:174/11000 train_time:10009ms step_avg:57.52ms
step:175/11000 train_time:10066ms step_avg:57.52ms
step:176/11000 train_time:10124ms step_avg:57.52ms
step:177/11000 train_time:10180ms step_avg:57.51ms
step:178/11000 train_time:10239ms step_avg:57.52ms
step:179/11000 train_time:10294ms step_avg:57.51ms
step:180/11000 train_time:10353ms step_avg:57.52ms
step:181/11000 train_time:10409ms step_avg:57.51ms
step:182/11000 train_time:10468ms step_avg:57.52ms
step:183/11000 train_time:10525ms step_avg:57.51ms
step:184/11000 train_time:10583ms step_avg:57.52ms
step:185/11000 train_time:10639ms step_avg:57.51ms
step:186/11000 train_time:10698ms step_avg:57.52ms
step:187/11000 train_time:10755ms step_avg:57.51ms
step:188/11000 train_time:10814ms step_avg:57.52ms
step:189/11000 train_time:10870ms step_avg:57.51ms
step:190/11000 train_time:10928ms step_avg:57.52ms
step:191/11000 train_time:10984ms step_avg:57.51ms
step:192/11000 train_time:11042ms step_avg:57.51ms
step:193/11000 train_time:11099ms step_avg:57.51ms
step:194/11000 train_time:11157ms step_avg:57.51ms
step:195/11000 train_time:11214ms step_avg:57.51ms
step:196/11000 train_time:11273ms step_avg:57.51ms
step:197/11000 train_time:11329ms step_avg:57.51ms
step:198/11000 train_time:11387ms step_avg:57.51ms
step:199/11000 train_time:11443ms step_avg:57.50ms
step:200/11000 train_time:11502ms step_avg:57.51ms
step:201/11000 train_time:11558ms step_avg:57.50ms
step:202/11000 train_time:11617ms step_avg:57.51ms
step:203/11000 train_time:11674ms step_avg:57.51ms
step:204/11000 train_time:11733ms step_avg:57.51ms
step:205/11000 train_time:11789ms step_avg:57.51ms
step:206/11000 train_time:11847ms step_avg:57.51ms
step:207/11000 train_time:11903ms step_avg:57.50ms
step:208/11000 train_time:11962ms step_avg:57.51ms
step:209/11000 train_time:12018ms step_avg:57.50ms
step:210/11000 train_time:12077ms step_avg:57.51ms
step:211/11000 train_time:12134ms step_avg:57.50ms
step:212/11000 train_time:12192ms step_avg:57.51ms
step:213/11000 train_time:12248ms step_avg:57.50ms
step:214/11000 train_time:12307ms step_avg:57.51ms
step:215/11000 train_time:12363ms step_avg:57.50ms
step:216/11000 train_time:12422ms step_avg:57.51ms
step:217/11000 train_time:12478ms step_avg:57.50ms
step:218/11000 train_time:12537ms step_avg:57.51ms
step:219/11000 train_time:12593ms step_avg:57.50ms
step:220/11000 train_time:12652ms step_avg:57.51ms
step:221/11000 train_time:12708ms step_avg:57.50ms
step:222/11000 train_time:12767ms step_avg:57.51ms
step:223/11000 train_time:12823ms step_avg:57.50ms
step:224/11000 train_time:12882ms step_avg:57.51ms
step:225/11000 train_time:12938ms step_avg:57.50ms
step:226/11000 train_time:12997ms step_avg:57.51ms
step:227/11000 train_time:13053ms step_avg:57.50ms
step:228/11000 train_time:13112ms step_avg:57.51ms
step:229/11000 train_time:13169ms step_avg:57.50ms
step:230/11000 train_time:13227ms step_avg:57.51ms
step:231/11000 train_time:13284ms step_avg:57.51ms
step:232/11000 train_time:13342ms step_avg:57.51ms
step:233/11000 train_time:13398ms step_avg:57.50ms
step:234/11000 train_time:13458ms step_avg:57.51ms
step:235/11000 train_time:13514ms step_avg:57.51ms
step:236/11000 train_time:13572ms step_avg:57.51ms
step:237/11000 train_time:13628ms step_avg:57.50ms
step:238/11000 train_time:13687ms step_avg:57.51ms
step:239/11000 train_time:13743ms step_avg:57.50ms
step:240/11000 train_time:13802ms step_avg:57.51ms
step:241/11000 train_time:13858ms step_avg:57.50ms
step:242/11000 train_time:13917ms step_avg:57.51ms
step:243/11000 train_time:13973ms step_avg:57.50ms
step:244/11000 train_time:14031ms step_avg:57.51ms
step:245/11000 train_time:14087ms step_avg:57.50ms
step:246/11000 train_time:14146ms step_avg:57.50ms
step:247/11000 train_time:14202ms step_avg:57.50ms
step:248/11000 train_time:14261ms step_avg:57.50ms
step:249/11000 train_time:14316ms step_avg:57.50ms
step:250/11000 train_time:14376ms step_avg:57.50ms
step:250/11000 val_loss:4.8959 train_time:14456ms step_avg:57.82ms
step:251/11000 train_time:14475ms step_avg:57.67ms
step:252/11000 train_time:14494ms step_avg:57.52ms
step:253/11000 train_time:14549ms step_avg:57.51ms
step:254/11000 train_time:14616ms step_avg:57.54ms
step:255/11000 train_time:14671ms step_avg:57.53ms
step:256/11000 train_time:14736ms step_avg:57.56ms
step:257/11000 train_time:14791ms step_avg:57.55ms
step:258/11000 train_time:14851ms step_avg:57.56ms
step:259/11000 train_time:14906ms step_avg:57.55ms
step:260/11000 train_time:14965ms step_avg:57.56ms
step:261/11000 train_time:15021ms step_avg:57.55ms
step:262/11000 train_time:15079ms step_avg:57.55ms
step:263/11000 train_time:15134ms step_avg:57.55ms
step:264/11000 train_time:15192ms step_avg:57.55ms
step:265/11000 train_time:15248ms step_avg:57.54ms
step:266/11000 train_time:15306ms step_avg:57.54ms
step:267/11000 train_time:15363ms step_avg:57.54ms
step:268/11000 train_time:15423ms step_avg:57.55ms
step:269/11000 train_time:15481ms step_avg:57.55ms
step:270/11000 train_time:15541ms step_avg:57.56ms
step:271/11000 train_time:15598ms step_avg:57.56ms
step:272/11000 train_time:15657ms step_avg:57.56ms
step:273/11000 train_time:15714ms step_avg:57.56ms
step:274/11000 train_time:15772ms step_avg:57.56ms
step:275/11000 train_time:15828ms step_avg:57.56ms
step:276/11000 train_time:15888ms step_avg:57.56ms
step:277/11000 train_time:15944ms step_avg:57.56ms
step:278/11000 train_time:16003ms step_avg:57.56ms
step:279/11000 train_time:16059ms step_avg:57.56ms
step:280/11000 train_time:16117ms step_avg:57.56ms
step:281/11000 train_time:16173ms step_avg:57.55ms
step:282/11000 train_time:16231ms step_avg:57.56ms
step:283/11000 train_time:16286ms step_avg:57.55ms
step:284/11000 train_time:16345ms step_avg:57.55ms
step:285/11000 train_time:16402ms step_avg:57.55ms
step:286/11000 train_time:16461ms step_avg:57.55ms
step:287/11000 train_time:16517ms step_avg:57.55ms
step:288/11000 train_time:16576ms step_avg:57.56ms
step:289/11000 train_time:16633ms step_avg:57.55ms
step:290/11000 train_time:16692ms step_avg:57.56ms
step:291/11000 train_time:16748ms step_avg:57.55ms
step:292/11000 train_time:16807ms step_avg:57.56ms
step:293/11000 train_time:16863ms step_avg:57.55ms
step:294/11000 train_time:16923ms step_avg:57.56ms
step:295/11000 train_time:16978ms step_avg:57.55ms
step:296/11000 train_time:17038ms step_avg:57.56ms
step:297/11000 train_time:17094ms step_avg:57.56ms
step:298/11000 train_time:17152ms step_avg:57.56ms
step:299/11000 train_time:17208ms step_avg:57.55ms
step:300/11000 train_time:17266ms step_avg:57.55ms
step:301/11000 train_time:17323ms step_avg:57.55ms
step:302/11000 train_time:17381ms step_avg:57.55ms
step:303/11000 train_time:17437ms step_avg:57.55ms
step:304/11000 train_time:17496ms step_avg:57.55ms
step:305/11000 train_time:17552ms step_avg:57.55ms
step:306/11000 train_time:17611ms step_avg:57.55ms
step:307/11000 train_time:17668ms step_avg:57.55ms
step:308/11000 train_time:17728ms step_avg:57.56ms
step:309/11000 train_time:17784ms step_avg:57.55ms
step:310/11000 train_time:17843ms step_avg:57.56ms
step:311/11000 train_time:17899ms step_avg:57.55ms
step:312/11000 train_time:17958ms step_avg:57.56ms
step:313/11000 train_time:18014ms step_avg:57.55ms
step:314/11000 train_time:18073ms step_avg:57.56ms
step:315/11000 train_time:18128ms step_avg:57.55ms
step:316/11000 train_time:18187ms step_avg:57.55ms
step:317/11000 train_time:18243ms step_avg:57.55ms
step:318/11000 train_time:18301ms step_avg:57.55ms
step:319/11000 train_time:18357ms step_avg:57.55ms
step:320/11000 train_time:18417ms step_avg:57.55ms
step:321/11000 train_time:18473ms step_avg:57.55ms
step:322/11000 train_time:18532ms step_avg:57.55ms
step:323/11000 train_time:18589ms step_avg:57.55ms
step:324/11000 train_time:18648ms step_avg:57.55ms
step:325/11000 train_time:18704ms step_avg:57.55ms
step:326/11000 train_time:18764ms step_avg:57.56ms
step:327/11000 train_time:18820ms step_avg:57.55ms
step:328/11000 train_time:18879ms step_avg:57.56ms
step:329/11000 train_time:18936ms step_avg:57.56ms
step:330/11000 train_time:18994ms step_avg:57.56ms
step:331/11000 train_time:19050ms step_avg:57.55ms
step:332/11000 train_time:19109ms step_avg:57.56ms
step:333/11000 train_time:19165ms step_avg:57.55ms
step:334/11000 train_time:19224ms step_avg:57.56ms
step:335/11000 train_time:19280ms step_avg:57.55ms
step:336/11000 train_time:19340ms step_avg:57.56ms
step:337/11000 train_time:19396ms step_avg:57.56ms
step:338/11000 train_time:19454ms step_avg:57.56ms
step:339/11000 train_time:19510ms step_avg:57.55ms
step:340/11000 train_time:19569ms step_avg:57.56ms
step:341/11000 train_time:19625ms step_avg:57.55ms
step:342/11000 train_time:19684ms step_avg:57.56ms
step:343/11000 train_time:19741ms step_avg:57.55ms
step:344/11000 train_time:19800ms step_avg:57.56ms
step:345/11000 train_time:19856ms step_avg:57.55ms
step:346/11000 train_time:19916ms step_avg:57.56ms
step:347/11000 train_time:19972ms step_avg:57.55ms
step:348/11000 train_time:20030ms step_avg:57.56ms
step:349/11000 train_time:20086ms step_avg:57.55ms
step:350/11000 train_time:20145ms step_avg:57.56ms
step:351/11000 train_time:20202ms step_avg:57.55ms
step:352/11000 train_time:20260ms step_avg:57.56ms
step:353/11000 train_time:20317ms step_avg:57.55ms
step:354/11000 train_time:20376ms step_avg:57.56ms
step:355/11000 train_time:20432ms step_avg:57.55ms
step:356/11000 train_time:20490ms step_avg:57.56ms
step:357/11000 train_time:20546ms step_avg:57.55ms
step:358/11000 train_time:20605ms step_avg:57.56ms
step:359/11000 train_time:20661ms step_avg:57.55ms
step:360/11000 train_time:20721ms step_avg:57.56ms
step:361/11000 train_time:20778ms step_avg:57.56ms
step:362/11000 train_time:20836ms step_avg:57.56ms
step:363/11000 train_time:20893ms step_avg:57.56ms
step:364/11000 train_time:20952ms step_avg:57.56ms
step:365/11000 train_time:21008ms step_avg:57.56ms
step:366/11000 train_time:21066ms step_avg:57.56ms
step:367/11000 train_time:21122ms step_avg:57.55ms
step:368/11000 train_time:21182ms step_avg:57.56ms
step:369/11000 train_time:21238ms step_avg:57.55ms
step:370/11000 train_time:21297ms step_avg:57.56ms
step:371/11000 train_time:21353ms step_avg:57.56ms
step:372/11000 train_time:21412ms step_avg:57.56ms
step:373/11000 train_time:21468ms step_avg:57.55ms
step:374/11000 train_time:21526ms step_avg:57.56ms
step:375/11000 train_time:21582ms step_avg:57.55ms
step:376/11000 train_time:21642ms step_avg:57.56ms
step:377/11000 train_time:21698ms step_avg:57.55ms
step:378/11000 train_time:21756ms step_avg:57.56ms
step:379/11000 train_time:21812ms step_avg:57.55ms
step:380/11000 train_time:21871ms step_avg:57.56ms
step:381/11000 train_time:21928ms step_avg:57.55ms
step:382/11000 train_time:21987ms step_avg:57.56ms
step:383/11000 train_time:22043ms step_avg:57.55ms
step:384/11000 train_time:22101ms step_avg:57.56ms
step:385/11000 train_time:22158ms step_avg:57.55ms
step:386/11000 train_time:22217ms step_avg:57.56ms
step:387/11000 train_time:22273ms step_avg:57.55ms
step:388/11000 train_time:22332ms step_avg:57.56ms
step:389/11000 train_time:22387ms step_avg:57.55ms
step:390/11000 train_time:22447ms step_avg:57.56ms
step:391/11000 train_time:22503ms step_avg:57.55ms
step:392/11000 train_time:22562ms step_avg:57.56ms
step:393/11000 train_time:22619ms step_avg:57.56ms
step:394/11000 train_time:22678ms step_avg:57.56ms
step:395/11000 train_time:22735ms step_avg:57.56ms
step:396/11000 train_time:22793ms step_avg:57.56ms
step:397/11000 train_time:22849ms step_avg:57.55ms
step:398/11000 train_time:22908ms step_avg:57.56ms
step:399/11000 train_time:22964ms step_avg:57.55ms
step:400/11000 train_time:23024ms step_avg:57.56ms
step:401/11000 train_time:23080ms step_avg:57.56ms
step:402/11000 train_time:23139ms step_avg:57.56ms
step:403/11000 train_time:23196ms step_avg:57.56ms
step:404/11000 train_time:23254ms step_avg:57.56ms
step:405/11000 train_time:23311ms step_avg:57.56ms
step:406/11000 train_time:23369ms step_avg:57.56ms
step:407/11000 train_time:23425ms step_avg:57.55ms
step:408/11000 train_time:23484ms step_avg:57.56ms
step:409/11000 train_time:23540ms step_avg:57.56ms
step:410/11000 train_time:23600ms step_avg:57.56ms
step:411/11000 train_time:23655ms step_avg:57.55ms
step:412/11000 train_time:23714ms step_avg:57.56ms
step:413/11000 train_time:23769ms step_avg:57.55ms
step:414/11000 train_time:23830ms step_avg:57.56ms
step:415/11000 train_time:23886ms step_avg:57.56ms
step:416/11000 train_time:23944ms step_avg:57.56ms
step:417/11000 train_time:24000ms step_avg:57.55ms
step:418/11000 train_time:24060ms step_avg:57.56ms
step:419/11000 train_time:24116ms step_avg:57.56ms
step:420/11000 train_time:24175ms step_avg:57.56ms
step:421/11000 train_time:24231ms step_avg:57.56ms
step:422/11000 train_time:24290ms step_avg:57.56ms
step:423/11000 train_time:24346ms step_avg:57.56ms
step:424/11000 train_time:24405ms step_avg:57.56ms
step:425/11000 train_time:24461ms step_avg:57.56ms
step:426/11000 train_time:24521ms step_avg:57.56ms
step:427/11000 train_time:24577ms step_avg:57.56ms
step:428/11000 train_time:24637ms step_avg:57.56ms
step:429/11000 train_time:24693ms step_avg:57.56ms
step:430/11000 train_time:24751ms step_avg:57.56ms
step:431/11000 train_time:24808ms step_avg:57.56ms
step:432/11000 train_time:24867ms step_avg:57.56ms
step:433/11000 train_time:24923ms step_avg:57.56ms
step:434/11000 train_time:24982ms step_avg:57.56ms
step:435/11000 train_time:25039ms step_avg:57.56ms
step:436/11000 train_time:25098ms step_avg:57.56ms
step:437/11000 train_time:25154ms step_avg:57.56ms
step:438/11000 train_time:25213ms step_avg:57.56ms
step:439/11000 train_time:25269ms step_avg:57.56ms
step:440/11000 train_time:25328ms step_avg:57.56ms
step:441/11000 train_time:25385ms step_avg:57.56ms
step:442/11000 train_time:25443ms step_avg:57.56ms
step:443/11000 train_time:25500ms step_avg:57.56ms
step:444/11000 train_time:25559ms step_avg:57.56ms
step:445/11000 train_time:25615ms step_avg:57.56ms
step:446/11000 train_time:25674ms step_avg:57.56ms
step:447/11000 train_time:25729ms step_avg:57.56ms
step:448/11000 train_time:25789ms step_avg:57.56ms
step:449/11000 train_time:25846ms step_avg:57.56ms
step:450/11000 train_time:25905ms step_avg:57.57ms
step:451/11000 train_time:25961ms step_avg:57.56ms
step:452/11000 train_time:26020ms step_avg:57.57ms
step:453/11000 train_time:26076ms step_avg:57.56ms
step:454/11000 train_time:26136ms step_avg:57.57ms
step:455/11000 train_time:26191ms step_avg:57.56ms
step:456/11000 train_time:26250ms step_avg:57.57ms
step:457/11000 train_time:26306ms step_avg:57.56ms
step:458/11000 train_time:26365ms step_avg:57.57ms
step:459/11000 train_time:26421ms step_avg:57.56ms
step:460/11000 train_time:26481ms step_avg:57.57ms
step:461/11000 train_time:26537ms step_avg:57.56ms
step:462/11000 train_time:26596ms step_avg:57.57ms
step:463/11000 train_time:26652ms step_avg:57.56ms
step:464/11000 train_time:26711ms step_avg:57.57ms
step:465/11000 train_time:26766ms step_avg:57.56ms
step:466/11000 train_time:26825ms step_avg:57.57ms
step:467/11000 train_time:26881ms step_avg:57.56ms
step:468/11000 train_time:26940ms step_avg:57.56ms
step:469/11000 train_time:26997ms step_avg:57.56ms
step:470/11000 train_time:27057ms step_avg:57.57ms
step:471/11000 train_time:27112ms step_avg:57.56ms
step:472/11000 train_time:27170ms step_avg:57.56ms
step:473/11000 train_time:27226ms step_avg:57.56ms
step:474/11000 train_time:27285ms step_avg:57.56ms
step:475/11000 train_time:27342ms step_avg:57.56ms
step:476/11000 train_time:27401ms step_avg:57.57ms
step:477/11000 train_time:27457ms step_avg:57.56ms
step:478/11000 train_time:27516ms step_avg:57.56ms
step:479/11000 train_time:27572ms step_avg:57.56ms
step:480/11000 train_time:27632ms step_avg:57.57ms
step:481/11000 train_time:27688ms step_avg:57.56ms
step:482/11000 train_time:27747ms step_avg:57.57ms
step:483/11000 train_time:27803ms step_avg:57.56ms
step:484/11000 train_time:27861ms step_avg:57.57ms
step:485/11000 train_time:27918ms step_avg:57.56ms
step:486/11000 train_time:27978ms step_avg:57.57ms
step:487/11000 train_time:28034ms step_avg:57.56ms
step:488/11000 train_time:28092ms step_avg:57.57ms
step:489/11000 train_time:28148ms step_avg:57.56ms
step:490/11000 train_time:28207ms step_avg:57.56ms
step:491/11000 train_time:28262ms step_avg:57.56ms
step:492/11000 train_time:28323ms step_avg:57.57ms
step:493/11000 train_time:28379ms step_avg:57.56ms
step:494/11000 train_time:28438ms step_avg:57.57ms
step:495/11000 train_time:28494ms step_avg:57.56ms
step:496/11000 train_time:28553ms step_avg:57.57ms
step:497/11000 train_time:28609ms step_avg:57.56ms
step:498/11000 train_time:28667ms step_avg:57.56ms
step:499/11000 train_time:28723ms step_avg:57.56ms
step:500/11000 train_time:28782ms step_avg:57.56ms
step:500/11000 val_loss:4.3994 train_time:28862ms step_avg:57.72ms
step:501/11000 train_time:28881ms step_avg:57.65ms
step:502/11000 train_time:28902ms step_avg:57.57ms
step:503/11000 train_time:28958ms step_avg:57.57ms
step:504/11000 train_time:29020ms step_avg:57.58ms
step:505/11000 train_time:29078ms step_avg:57.58ms
step:506/11000 train_time:29138ms step_avg:57.58ms
step:507/11000 train_time:29193ms step_avg:57.58ms
step:508/11000 train_time:29253ms step_avg:57.58ms
step:509/11000 train_time:29309ms step_avg:57.58ms
step:510/11000 train_time:29368ms step_avg:57.58ms
step:511/11000 train_time:29424ms step_avg:57.58ms
step:512/11000 train_time:29482ms step_avg:57.58ms
step:513/11000 train_time:29538ms step_avg:57.58ms
step:514/11000 train_time:29596ms step_avg:57.58ms
step:515/11000 train_time:29651ms step_avg:57.57ms
step:516/11000 train_time:29709ms step_avg:57.58ms
step:517/11000 train_time:29765ms step_avg:57.57ms
step:518/11000 train_time:29824ms step_avg:57.57ms
step:519/11000 train_time:29881ms step_avg:57.57ms
step:520/11000 train_time:29940ms step_avg:57.58ms
step:521/11000 train_time:29996ms step_avg:57.57ms
step:522/11000 train_time:30057ms step_avg:57.58ms
step:523/11000 train_time:30113ms step_avg:57.58ms
step:524/11000 train_time:30173ms step_avg:57.58ms
step:525/11000 train_time:30230ms step_avg:57.58ms
step:526/11000 train_time:30288ms step_avg:57.58ms
step:527/11000 train_time:30345ms step_avg:57.58ms
step:528/11000 train_time:30403ms step_avg:57.58ms
step:529/11000 train_time:30459ms step_avg:57.58ms
step:530/11000 train_time:30517ms step_avg:57.58ms
step:531/11000 train_time:30573ms step_avg:57.58ms
step:532/11000 train_time:30631ms step_avg:57.58ms
step:533/11000 train_time:30688ms step_avg:57.58ms
step:534/11000 train_time:30747ms step_avg:57.58ms
step:535/11000 train_time:30803ms step_avg:57.58ms
step:536/11000 train_time:30861ms step_avg:57.58ms
step:537/11000 train_time:30918ms step_avg:57.57ms
step:538/11000 train_time:30977ms step_avg:57.58ms
step:539/11000 train_time:31034ms step_avg:57.58ms
step:540/11000 train_time:31093ms step_avg:57.58ms
step:541/11000 train_time:31149ms step_avg:57.58ms
step:542/11000 train_time:31209ms step_avg:57.58ms
step:543/11000 train_time:31265ms step_avg:57.58ms
step:544/11000 train_time:31324ms step_avg:57.58ms
step:545/11000 train_time:31380ms step_avg:57.58ms
step:546/11000 train_time:31439ms step_avg:57.58ms
step:547/11000 train_time:31495ms step_avg:57.58ms
step:548/11000 train_time:31554ms step_avg:57.58ms
step:549/11000 train_time:31610ms step_avg:57.58ms
step:550/11000 train_time:31669ms step_avg:57.58ms
step:551/11000 train_time:31726ms step_avg:57.58ms
step:552/11000 train_time:31785ms step_avg:57.58ms
step:553/11000 train_time:31841ms step_avg:57.58ms
step:554/11000 train_time:31901ms step_avg:57.58ms
step:555/11000 train_time:31958ms step_avg:57.58ms
step:556/11000 train_time:32017ms step_avg:57.58ms
step:557/11000 train_time:32073ms step_avg:57.58ms
step:558/11000 train_time:32132ms step_avg:57.58ms
step:559/11000 train_time:32189ms step_avg:57.58ms
step:560/11000 train_time:32247ms step_avg:57.58ms
step:561/11000 train_time:32303ms step_avg:57.58ms
step:562/11000 train_time:32363ms step_avg:57.59ms
step:563/11000 train_time:32420ms step_avg:57.58ms
step:564/11000 train_time:32479ms step_avg:57.59ms
step:565/11000 train_time:32534ms step_avg:57.58ms
step:566/11000 train_time:32593ms step_avg:57.58ms
step:567/11000 train_time:32649ms step_avg:57.58ms
step:568/11000 train_time:32708ms step_avg:57.58ms
step:569/11000 train_time:32765ms step_avg:57.58ms
step:570/11000 train_time:32823ms step_avg:57.58ms
step:571/11000 train_time:32880ms step_avg:57.58ms
step:572/11000 train_time:32939ms step_avg:57.59ms
step:573/11000 train_time:32995ms step_avg:57.58ms
step:574/11000 train_time:33054ms step_avg:57.59ms
step:575/11000 train_time:33110ms step_avg:57.58ms
step:576/11000 train_time:33170ms step_avg:57.59ms
step:577/11000 train_time:33227ms step_avg:57.59ms
step:578/11000 train_time:33286ms step_avg:57.59ms
step:579/11000 train_time:33343ms step_avg:57.59ms
step:580/11000 train_time:33402ms step_avg:57.59ms
step:581/11000 train_time:33458ms step_avg:57.59ms
step:582/11000 train_time:33516ms step_avg:57.59ms
step:583/11000 train_time:33572ms step_avg:57.58ms
step:584/11000 train_time:33631ms step_avg:57.59ms
step:585/11000 train_time:33687ms step_avg:57.58ms
step:586/11000 train_time:33747ms step_avg:57.59ms
step:587/11000 train_time:33803ms step_avg:57.59ms
step:588/11000 train_time:33862ms step_avg:57.59ms
step:589/11000 train_time:33918ms step_avg:57.59ms
step:590/11000 train_time:33977ms step_avg:57.59ms
step:591/11000 train_time:34033ms step_avg:57.59ms
step:592/11000 train_time:34094ms step_avg:57.59ms
step:593/11000 train_time:34150ms step_avg:57.59ms
step:594/11000 train_time:34209ms step_avg:57.59ms
step:595/11000 train_time:34265ms step_avg:57.59ms
step:596/11000 train_time:34324ms step_avg:57.59ms
step:597/11000 train_time:34381ms step_avg:57.59ms
step:598/11000 train_time:34440ms step_avg:57.59ms
step:599/11000 train_time:34496ms step_avg:57.59ms
step:600/11000 train_time:34555ms step_avg:57.59ms
step:601/11000 train_time:34611ms step_avg:57.59ms
step:602/11000 train_time:34670ms step_avg:57.59ms
step:603/11000 train_time:34727ms step_avg:57.59ms
step:604/11000 train_time:34785ms step_avg:57.59ms
step:605/11000 train_time:34842ms step_avg:57.59ms
step:606/11000 train_time:34901ms step_avg:57.59ms
step:607/11000 train_time:34957ms step_avg:57.59ms
step:608/11000 train_time:35016ms step_avg:57.59ms
step:609/11000 train_time:35072ms step_avg:57.59ms
step:610/11000 train_time:35131ms step_avg:57.59ms
step:611/11000 train_time:35188ms step_avg:57.59ms
step:612/11000 train_time:35248ms step_avg:57.60ms
step:613/11000 train_time:35304ms step_avg:57.59ms
step:614/11000 train_time:35364ms step_avg:57.60ms
step:615/11000 train_time:35420ms step_avg:57.59ms
step:616/11000 train_time:35480ms step_avg:57.60ms
step:617/11000 train_time:35536ms step_avg:57.59ms
step:618/11000 train_time:35594ms step_avg:57.60ms
step:619/11000 train_time:35650ms step_avg:57.59ms
step:620/11000 train_time:35710ms step_avg:57.60ms
step:621/11000 train_time:35767ms step_avg:57.60ms
step:622/11000 train_time:35826ms step_avg:57.60ms
step:623/11000 train_time:35883ms step_avg:57.60ms
step:624/11000 train_time:35942ms step_avg:57.60ms
step:625/11000 train_time:35997ms step_avg:57.60ms
step:626/11000 train_time:36056ms step_avg:57.60ms
step:627/11000 train_time:36112ms step_avg:57.59ms
step:628/11000 train_time:36171ms step_avg:57.60ms
step:629/11000 train_time:36228ms step_avg:57.60ms
step:630/11000 train_time:36289ms step_avg:57.60ms
step:631/11000 train_time:36345ms step_avg:57.60ms
step:632/11000 train_time:36404ms step_avg:57.60ms
step:633/11000 train_time:36461ms step_avg:57.60ms
step:634/11000 train_time:36520ms step_avg:57.60ms
step:635/11000 train_time:36575ms step_avg:57.60ms
step:636/11000 train_time:36634ms step_avg:57.60ms
step:637/11000 train_time:36690ms step_avg:57.60ms
step:638/11000 train_time:36749ms step_avg:57.60ms
step:639/11000 train_time:36806ms step_avg:57.60ms
step:640/11000 train_time:36864ms step_avg:57.60ms
step:641/11000 train_time:36920ms step_avg:57.60ms
step:642/11000 train_time:36979ms step_avg:57.60ms
step:643/11000 train_time:37036ms step_avg:57.60ms
step:644/11000 train_time:37094ms step_avg:57.60ms
step:645/11000 train_time:37150ms step_avg:57.60ms
step:646/11000 train_time:37210ms step_avg:57.60ms
step:647/11000 train_time:37267ms step_avg:57.60ms
step:648/11000 train_time:37326ms step_avg:57.60ms
step:649/11000 train_time:37383ms step_avg:57.60ms
step:650/11000 train_time:37441ms step_avg:57.60ms
step:651/11000 train_time:37497ms step_avg:57.60ms
step:652/11000 train_time:37556ms step_avg:57.60ms
step:653/11000 train_time:37612ms step_avg:57.60ms
step:654/11000 train_time:37671ms step_avg:57.60ms
step:655/11000 train_time:37727ms step_avg:57.60ms
step:656/11000 train_time:37786ms step_avg:57.60ms
step:657/11000 train_time:37842ms step_avg:57.60ms
step:658/11000 train_time:37901ms step_avg:57.60ms
step:659/11000 train_time:37957ms step_avg:57.60ms
step:660/11000 train_time:38015ms step_avg:57.60ms
step:661/11000 train_time:38071ms step_avg:57.60ms
step:662/11000 train_time:38130ms step_avg:57.60ms
step:663/11000 train_time:38187ms step_avg:57.60ms
step:664/11000 train_time:38246ms step_avg:57.60ms
step:665/11000 train_time:38302ms step_avg:57.60ms
step:666/11000 train_time:38361ms step_avg:57.60ms
step:667/11000 train_time:38417ms step_avg:57.60ms
step:668/11000 train_time:38476ms step_avg:57.60ms
step:669/11000 train_time:38532ms step_avg:57.60ms
step:670/11000 train_time:38591ms step_avg:57.60ms
step:671/11000 train_time:38647ms step_avg:57.60ms
step:672/11000 train_time:38706ms step_avg:57.60ms
step:673/11000 train_time:38763ms step_avg:57.60ms
step:674/11000 train_time:38822ms step_avg:57.60ms
step:675/11000 train_time:38878ms step_avg:57.60ms
step:676/11000 train_time:38936ms step_avg:57.60ms
step:677/11000 train_time:38992ms step_avg:57.60ms
step:678/11000 train_time:39051ms step_avg:57.60ms
step:679/11000 train_time:39108ms step_avg:57.60ms
step:680/11000 train_time:39166ms step_avg:57.60ms
step:681/11000 train_time:39223ms step_avg:57.60ms
step:682/11000 train_time:39282ms step_avg:57.60ms
step:683/11000 train_time:39338ms step_avg:57.60ms
step:684/11000 train_time:39396ms step_avg:57.60ms
step:685/11000 train_time:39452ms step_avg:57.59ms
step:686/11000 train_time:39511ms step_avg:57.60ms
step:687/11000 train_time:39568ms step_avg:57.59ms
step:688/11000 train_time:39626ms step_avg:57.60ms
step:689/11000 train_time:39683ms step_avg:57.59ms
step:690/11000 train_time:39742ms step_avg:57.60ms
step:691/11000 train_time:39798ms step_avg:57.59ms
step:692/11000 train_time:39857ms step_avg:57.60ms
step:693/11000 train_time:39913ms step_avg:57.59ms
step:694/11000 train_time:39972ms step_avg:57.60ms
step:695/11000 train_time:40028ms step_avg:57.59ms
step:696/11000 train_time:40087ms step_avg:57.60ms
step:697/11000 train_time:40143ms step_avg:57.59ms
step:698/11000 train_time:40203ms step_avg:57.60ms
step:699/11000 train_time:40260ms step_avg:57.60ms
step:700/11000 train_time:40319ms step_avg:57.60ms
step:701/11000 train_time:40375ms step_avg:57.60ms
step:702/11000 train_time:40434ms step_avg:57.60ms
step:703/11000 train_time:40490ms step_avg:57.60ms
step:704/11000 train_time:40549ms step_avg:57.60ms
step:705/11000 train_time:40605ms step_avg:57.60ms
step:706/11000 train_time:40665ms step_avg:57.60ms
step:707/11000 train_time:40722ms step_avg:57.60ms
step:708/11000 train_time:40781ms step_avg:57.60ms
step:709/11000 train_time:40837ms step_avg:57.60ms
step:710/11000 train_time:40895ms step_avg:57.60ms
step:711/11000 train_time:40951ms step_avg:57.60ms
step:712/11000 train_time:41010ms step_avg:57.60ms
step:713/11000 train_time:41066ms step_avg:57.60ms
step:714/11000 train_time:41126ms step_avg:57.60ms
step:715/11000 train_time:41183ms step_avg:57.60ms
step:716/11000 train_time:41241ms step_avg:57.60ms
step:717/11000 train_time:41297ms step_avg:57.60ms
step:718/11000 train_time:41356ms step_avg:57.60ms
step:719/11000 train_time:41412ms step_avg:57.60ms
step:720/11000 train_time:41470ms step_avg:57.60ms
step:721/11000 train_time:41527ms step_avg:57.60ms
step:722/11000 train_time:41586ms step_avg:57.60ms
step:723/11000 train_time:41643ms step_avg:57.60ms
step:724/11000 train_time:41701ms step_avg:57.60ms
step:725/11000 train_time:41759ms step_avg:57.60ms
step:726/11000 train_time:41817ms step_avg:57.60ms
step:727/11000 train_time:41872ms step_avg:57.60ms
step:728/11000 train_time:41932ms step_avg:57.60ms
step:729/11000 train_time:41988ms step_avg:57.60ms
step:730/11000 train_time:42048ms step_avg:57.60ms
step:731/11000 train_time:42105ms step_avg:57.60ms
step:732/11000 train_time:42163ms step_avg:57.60ms
step:733/11000 train_time:42219ms step_avg:57.60ms
step:734/11000 train_time:42278ms step_avg:57.60ms
step:735/11000 train_time:42334ms step_avg:57.60ms
step:736/11000 train_time:42393ms step_avg:57.60ms
step:737/11000 train_time:42449ms step_avg:57.60ms
step:738/11000 train_time:42509ms step_avg:57.60ms
step:739/11000 train_time:42565ms step_avg:57.60ms
step:740/11000 train_time:42624ms step_avg:57.60ms
step:741/11000 train_time:42680ms step_avg:57.60ms
step:742/11000 train_time:42740ms step_avg:57.60ms
step:743/11000 train_time:42795ms step_avg:57.60ms
step:744/11000 train_time:42854ms step_avg:57.60ms
step:745/11000 train_time:42910ms step_avg:57.60ms
step:746/11000 train_time:42970ms step_avg:57.60ms
step:747/11000 train_time:43026ms step_avg:57.60ms
step:748/11000 train_time:43087ms step_avg:57.60ms
step:749/11000 train_time:43143ms step_avg:57.60ms
step:750/11000 train_time:43203ms step_avg:57.60ms
step:750/11000 val_loss:4.2153 train_time:43281ms step_avg:57.71ms
step:751/11000 train_time:43301ms step_avg:57.66ms
step:752/11000 train_time:43321ms step_avg:57.61ms
step:753/11000 train_time:43376ms step_avg:57.60ms
step:754/11000 train_time:43438ms step_avg:57.61ms
step:755/11000 train_time:43495ms step_avg:57.61ms
step:756/11000 train_time:43559ms step_avg:57.62ms
step:757/11000 train_time:43615ms step_avg:57.62ms
step:758/11000 train_time:43673ms step_avg:57.62ms
step:759/11000 train_time:43729ms step_avg:57.61ms
step:760/11000 train_time:43787ms step_avg:57.61ms
step:761/11000 train_time:43843ms step_avg:57.61ms
step:762/11000 train_time:43901ms step_avg:57.61ms
step:763/11000 train_time:43956ms step_avg:57.61ms
step:764/11000 train_time:44015ms step_avg:57.61ms
step:765/11000 train_time:44071ms step_avg:57.61ms
step:766/11000 train_time:44129ms step_avg:57.61ms
step:767/11000 train_time:44185ms step_avg:57.61ms
step:768/11000 train_time:44244ms step_avg:57.61ms
step:769/11000 train_time:44301ms step_avg:57.61ms
step:770/11000 train_time:44362ms step_avg:57.61ms
step:771/11000 train_time:44418ms step_avg:57.61ms
step:772/11000 train_time:44478ms step_avg:57.61ms
step:773/11000 train_time:44535ms step_avg:57.61ms
step:774/11000 train_time:44595ms step_avg:57.62ms
step:775/11000 train_time:44651ms step_avg:57.61ms
step:776/11000 train_time:44710ms step_avg:57.62ms
step:777/11000 train_time:44766ms step_avg:57.61ms
step:778/11000 train_time:44825ms step_avg:57.62ms
step:779/11000 train_time:44881ms step_avg:57.61ms
step:780/11000 train_time:44940ms step_avg:57.62ms
step:781/11000 train_time:44996ms step_avg:57.61ms
step:782/11000 train_time:45054ms step_avg:57.61ms
step:783/11000 train_time:45110ms step_avg:57.61ms
step:784/11000 train_time:45168ms step_avg:57.61ms
step:785/11000 train_time:45225ms step_avg:57.61ms
step:786/11000 train_time:45285ms step_avg:57.61ms
step:787/11000 train_time:45341ms step_avg:57.61ms
step:788/11000 train_time:45402ms step_avg:57.62ms
step:789/11000 train_time:45458ms step_avg:57.61ms
step:790/11000 train_time:45518ms step_avg:57.62ms
step:791/11000 train_time:45574ms step_avg:57.62ms
step:792/11000 train_time:45634ms step_avg:57.62ms
step:793/11000 train_time:45690ms step_avg:57.62ms
step:794/11000 train_time:45750ms step_avg:57.62ms
step:795/11000 train_time:45807ms step_avg:57.62ms
step:796/11000 train_time:45866ms step_avg:57.62ms
step:797/11000 train_time:45922ms step_avg:57.62ms
step:798/11000 train_time:45980ms step_avg:57.62ms
step:799/11000 train_time:46036ms step_avg:57.62ms
step:800/11000 train_time:46094ms step_avg:57.62ms
step:801/11000 train_time:46150ms step_avg:57.62ms
step:802/11000 train_time:46209ms step_avg:57.62ms
step:803/11000 train_time:46266ms step_avg:57.62ms
step:804/11000 train_time:46326ms step_avg:57.62ms
step:805/11000 train_time:46383ms step_avg:57.62ms
step:806/11000 train_time:46442ms step_avg:57.62ms
step:807/11000 train_time:46498ms step_avg:57.62ms
step:808/11000 train_time:46558ms step_avg:57.62ms
step:809/11000 train_time:46614ms step_avg:57.62ms
step:810/11000 train_time:46673ms step_avg:57.62ms
step:811/11000 train_time:46729ms step_avg:57.62ms
step:812/11000 train_time:46789ms step_avg:57.62ms
step:813/11000 train_time:46845ms step_avg:57.62ms
step:814/11000 train_time:46905ms step_avg:57.62ms
step:815/11000 train_time:46961ms step_avg:57.62ms
step:816/11000 train_time:47020ms step_avg:57.62ms
step:817/11000 train_time:47075ms step_avg:57.62ms
step:818/11000 train_time:47134ms step_avg:57.62ms
step:819/11000 train_time:47190ms step_avg:57.62ms
step:820/11000 train_time:47249ms step_avg:57.62ms
step:821/11000 train_time:47305ms step_avg:57.62ms
step:822/11000 train_time:47366ms step_avg:57.62ms
step:823/11000 train_time:47423ms step_avg:57.62ms
step:824/11000 train_time:47482ms step_avg:57.62ms
step:825/11000 train_time:47538ms step_avg:57.62ms
step:826/11000 train_time:47597ms step_avg:57.62ms
step:827/11000 train_time:47653ms step_avg:57.62ms
step:828/11000 train_time:47713ms step_avg:57.62ms
step:829/11000 train_time:47770ms step_avg:57.62ms
step:830/11000 train_time:47829ms step_avg:57.62ms
step:831/11000 train_time:47885ms step_avg:57.62ms
step:832/11000 train_time:47944ms step_avg:57.62ms
step:833/11000 train_time:48000ms step_avg:57.62ms
step:834/11000 train_time:48059ms step_avg:57.62ms
step:835/11000 train_time:48115ms step_avg:57.62ms
step:836/11000 train_time:48173ms step_avg:57.62ms
step:837/11000 train_time:48229ms step_avg:57.62ms
step:838/11000 train_time:48289ms step_avg:57.62ms
step:839/11000 train_time:48345ms step_avg:57.62ms
step:840/11000 train_time:48405ms step_avg:57.63ms
step:841/11000 train_time:48462ms step_avg:57.62ms
step:842/11000 train_time:48523ms step_avg:57.63ms
step:843/11000 train_time:48579ms step_avg:57.63ms
step:844/11000 train_time:48637ms step_avg:57.63ms
step:845/11000 train_time:48693ms step_avg:57.63ms
step:846/11000 train_time:48753ms step_avg:57.63ms
step:847/11000 train_time:48809ms step_avg:57.63ms
step:848/11000 train_time:48869ms step_avg:57.63ms
step:849/11000 train_time:48926ms step_avg:57.63ms
step:850/11000 train_time:48986ms step_avg:57.63ms
step:851/11000 train_time:49041ms step_avg:57.63ms
step:852/11000 train_time:49101ms step_avg:57.63ms
step:853/11000 train_time:49157ms step_avg:57.63ms
step:854/11000 train_time:49216ms step_avg:57.63ms
step:855/11000 train_time:49272ms step_avg:57.63ms
step:856/11000 train_time:49330ms step_avg:57.63ms
step:857/11000 train_time:49388ms step_avg:57.63ms
step:858/11000 train_time:49447ms step_avg:57.63ms
step:859/11000 train_time:49504ms step_avg:57.63ms
step:860/11000 train_time:49564ms step_avg:57.63ms
step:861/11000 train_time:49621ms step_avg:57.63ms
step:862/11000 train_time:49679ms step_avg:57.63ms
step:863/11000 train_time:49734ms step_avg:57.63ms
step:864/11000 train_time:49794ms step_avg:57.63ms
step:865/11000 train_time:49850ms step_avg:57.63ms
step:866/11000 train_time:49910ms step_avg:57.63ms
step:867/11000 train_time:49967ms step_avg:57.63ms
step:868/11000 train_time:50027ms step_avg:57.63ms
step:869/11000 train_time:50083ms step_avg:57.63ms
step:870/11000 train_time:50142ms step_avg:57.63ms
step:871/11000 train_time:50197ms step_avg:57.63ms
step:872/11000 train_time:50256ms step_avg:57.63ms
step:873/11000 train_time:50313ms step_avg:57.63ms
step:874/11000 train_time:50372ms step_avg:57.63ms
step:875/11000 train_time:50429ms step_avg:57.63ms
step:876/11000 train_time:50488ms step_avg:57.63ms
step:877/11000 train_time:50544ms step_avg:57.63ms
step:878/11000 train_time:50604ms step_avg:57.64ms
step:879/11000 train_time:50660ms step_avg:57.63ms
step:880/11000 train_time:50720ms step_avg:57.64ms
step:881/11000 train_time:50775ms step_avg:57.63ms
step:882/11000 train_time:50834ms step_avg:57.64ms
step:883/11000 train_time:50890ms step_avg:57.63ms
step:884/11000 train_time:50949ms step_avg:57.64ms
step:885/11000 train_time:51005ms step_avg:57.63ms
step:886/11000 train_time:51066ms step_avg:57.64ms
step:887/11000 train_time:51122ms step_avg:57.63ms
step:888/11000 train_time:51181ms step_avg:57.64ms
step:889/11000 train_time:51237ms step_avg:57.63ms
step:890/11000 train_time:51297ms step_avg:57.64ms
step:891/11000 train_time:51352ms step_avg:57.63ms
step:892/11000 train_time:51412ms step_avg:57.64ms
step:893/11000 train_time:51470ms step_avg:57.64ms
step:894/11000 train_time:51529ms step_avg:57.64ms
step:895/11000 train_time:51586ms step_avg:57.64ms
step:896/11000 train_time:51645ms step_avg:57.64ms
step:897/11000 train_time:51700ms step_avg:57.64ms
step:898/11000 train_time:51761ms step_avg:57.64ms
step:899/11000 train_time:51816ms step_avg:57.64ms
step:900/11000 train_time:51875ms step_avg:57.64ms
step:901/11000 train_time:51931ms step_avg:57.64ms
step:902/11000 train_time:51990ms step_avg:57.64ms
step:903/11000 train_time:52047ms step_avg:57.64ms
step:904/11000 train_time:52106ms step_avg:57.64ms
step:905/11000 train_time:52162ms step_avg:57.64ms
step:906/11000 train_time:52222ms step_avg:57.64ms
step:907/11000 train_time:52278ms step_avg:57.64ms
step:908/11000 train_time:52336ms step_avg:57.64ms
step:909/11000 train_time:52392ms step_avg:57.64ms
step:910/11000 train_time:52452ms step_avg:57.64ms
step:911/11000 train_time:52508ms step_avg:57.64ms
step:912/11000 train_time:52567ms step_avg:57.64ms
step:913/11000 train_time:52624ms step_avg:57.64ms
step:914/11000 train_time:52683ms step_avg:57.64ms
step:915/11000 train_time:52738ms step_avg:57.64ms
step:916/11000 train_time:52798ms step_avg:57.64ms
step:917/11000 train_time:52854ms step_avg:57.64ms
step:918/11000 train_time:52913ms step_avg:57.64ms
step:919/11000 train_time:52969ms step_avg:57.64ms
step:920/11000 train_time:53028ms step_avg:57.64ms
step:921/11000 train_time:53084ms step_avg:57.64ms
step:922/11000 train_time:53144ms step_avg:57.64ms
step:923/11000 train_time:53200ms step_avg:57.64ms
step:924/11000 train_time:53259ms step_avg:57.64ms
step:925/11000 train_time:53315ms step_avg:57.64ms
step:926/11000 train_time:53373ms step_avg:57.64ms
step:927/11000 train_time:53430ms step_avg:57.64ms
step:928/11000 train_time:53489ms step_avg:57.64ms
step:929/11000 train_time:53545ms step_avg:57.64ms
step:930/11000 train_time:53605ms step_avg:57.64ms
step:931/11000 train_time:53661ms step_avg:57.64ms
step:932/11000 train_time:53721ms step_avg:57.64ms
step:933/11000 train_time:53776ms step_avg:57.64ms
step:934/11000 train_time:53835ms step_avg:57.64ms
step:935/11000 train_time:53891ms step_avg:57.64ms
step:936/11000 train_time:53950ms step_avg:57.64ms
step:937/11000 train_time:54007ms step_avg:57.64ms
step:938/11000 train_time:54066ms step_avg:57.64ms
step:939/11000 train_time:54122ms step_avg:57.64ms
step:940/11000 train_time:54182ms step_avg:57.64ms
step:941/11000 train_time:54238ms step_avg:57.64ms
step:942/11000 train_time:54296ms step_avg:57.64ms
step:943/11000 train_time:54352ms step_avg:57.64ms
step:944/11000 train_time:54411ms step_avg:57.64ms
step:945/11000 train_time:54468ms step_avg:57.64ms
step:946/11000 train_time:54527ms step_avg:57.64ms
step:947/11000 train_time:54583ms step_avg:57.64ms
step:948/11000 train_time:54642ms step_avg:57.64ms
step:949/11000 train_time:54699ms step_avg:57.64ms
step:950/11000 train_time:54758ms step_avg:57.64ms
step:951/11000 train_time:54814ms step_avg:57.64ms
step:952/11000 train_time:54873ms step_avg:57.64ms
step:953/11000 train_time:54929ms step_avg:57.64ms
step:954/11000 train_time:54988ms step_avg:57.64ms
step:955/11000 train_time:55043ms step_avg:57.64ms
step:956/11000 train_time:55105ms step_avg:57.64ms
step:957/11000 train_time:55160ms step_avg:57.64ms
step:958/11000 train_time:55220ms step_avg:57.64ms
step:959/11000 train_time:55276ms step_avg:57.64ms
step:960/11000 train_time:55335ms step_avg:57.64ms
step:961/11000 train_time:55391ms step_avg:57.64ms
step:962/11000 train_time:55449ms step_avg:57.64ms
step:963/11000 train_time:55506ms step_avg:57.64ms
step:964/11000 train_time:55565ms step_avg:57.64ms
step:965/11000 train_time:55622ms step_avg:57.64ms
step:966/11000 train_time:55681ms step_avg:57.64ms
step:967/11000 train_time:55737ms step_avg:57.64ms
step:968/11000 train_time:55796ms step_avg:57.64ms
step:969/11000 train_time:55852ms step_avg:57.64ms
step:970/11000 train_time:55912ms step_avg:57.64ms
step:971/11000 train_time:55967ms step_avg:57.64ms
step:972/11000 train_time:56027ms step_avg:57.64ms
step:973/11000 train_time:56083ms step_avg:57.64ms
step:974/11000 train_time:56143ms step_avg:57.64ms
step:975/11000 train_time:56199ms step_avg:57.64ms
step:976/11000 train_time:56258ms step_avg:57.64ms
step:977/11000 train_time:56314ms step_avg:57.64ms
step:978/11000 train_time:56373ms step_avg:57.64ms
step:979/11000 train_time:56429ms step_avg:57.64ms
step:980/11000 train_time:56489ms step_avg:57.64ms
step:981/11000 train_time:56545ms step_avg:57.64ms
step:982/11000 train_time:56605ms step_avg:57.64ms
step:983/11000 train_time:56661ms step_avg:57.64ms
step:984/11000 train_time:56721ms step_avg:57.64ms
step:985/11000 train_time:56776ms step_avg:57.64ms
step:986/11000 train_time:56835ms step_avg:57.64ms
step:987/11000 train_time:56891ms step_avg:57.64ms
step:988/11000 train_time:56951ms step_avg:57.64ms
step:989/11000 train_time:57008ms step_avg:57.64ms
step:990/11000 train_time:57068ms step_avg:57.64ms
step:991/11000 train_time:57124ms step_avg:57.64ms
step:992/11000 train_time:57183ms step_avg:57.64ms
step:993/11000 train_time:57239ms step_avg:57.64ms
step:994/11000 train_time:57298ms step_avg:57.64ms
step:995/11000 train_time:57353ms step_avg:57.64ms
step:996/11000 train_time:57413ms step_avg:57.64ms
step:997/11000 train_time:57468ms step_avg:57.64ms
step:998/11000 train_time:57528ms step_avg:57.64ms
step:999/11000 train_time:57585ms step_avg:57.64ms
step:1000/11000 train_time:57644ms step_avg:57.64ms
step:1000/11000 val_loss:4.1106 train_time:57724ms step_avg:57.72ms
step:1001/11000 train_time:57744ms step_avg:57.69ms
step:1002/11000 train_time:57764ms step_avg:57.65ms
step:1003/11000 train_time:57818ms step_avg:57.65ms
step:1004/11000 train_time:57881ms step_avg:57.65ms
step:1005/11000 train_time:57938ms step_avg:57.65ms
step:1006/11000 train_time:58000ms step_avg:57.65ms
step:1007/11000 train_time:58056ms step_avg:57.65ms
step:1008/11000 train_time:58117ms step_avg:57.66ms
step:1009/11000 train_time:58172ms step_avg:57.65ms
step:1010/11000 train_time:58231ms step_avg:57.65ms
step:1011/11000 train_time:58287ms step_avg:57.65ms
step:1012/11000 train_time:58346ms step_avg:57.65ms
step:1013/11000 train_time:58401ms step_avg:57.65ms
step:1014/11000 train_time:58460ms step_avg:57.65ms
step:1015/11000 train_time:58516ms step_avg:57.65ms
step:1016/11000 train_time:58574ms step_avg:57.65ms
step:1017/11000 train_time:58630ms step_avg:57.65ms
step:1018/11000 train_time:58689ms step_avg:57.65ms
step:1019/11000 train_time:58746ms step_avg:57.65ms
step:1020/11000 train_time:58807ms step_avg:57.65ms
step:1021/11000 train_time:58864ms step_avg:57.65ms
step:1022/11000 train_time:58926ms step_avg:57.66ms
step:1023/11000 train_time:58983ms step_avg:57.66ms
step:1024/11000 train_time:59043ms step_avg:57.66ms
step:1025/11000 train_time:59100ms step_avg:57.66ms
step:1026/11000 train_time:59160ms step_avg:57.66ms
step:1027/11000 train_time:59216ms step_avg:57.66ms
step:1028/11000 train_time:59274ms step_avg:57.66ms
step:1029/11000 train_time:59330ms step_avg:57.66ms
step:1030/11000 train_time:59388ms step_avg:57.66ms
step:1031/11000 train_time:59444ms step_avg:57.66ms
step:1032/11000 train_time:59503ms step_avg:57.66ms
step:1033/11000 train_time:59559ms step_avg:57.66ms
step:1034/11000 train_time:59618ms step_avg:57.66ms
step:1035/11000 train_time:59674ms step_avg:57.66ms
step:1036/11000 train_time:59734ms step_avg:57.66ms
step:1037/11000 train_time:59790ms step_avg:57.66ms
step:1038/11000 train_time:59852ms step_avg:57.66ms
step:1039/11000 train_time:59908ms step_avg:57.66ms
step:1040/11000 train_time:59968ms step_avg:57.66ms
step:1041/11000 train_time:60026ms step_avg:57.66ms
step:1042/11000 train_time:60085ms step_avg:57.66ms
step:1043/11000 train_time:60142ms step_avg:57.66ms
step:1044/11000 train_time:60202ms step_avg:57.66ms
step:1045/11000 train_time:60258ms step_avg:57.66ms
step:1046/11000 train_time:60316ms step_avg:57.66ms
step:1047/11000 train_time:60372ms step_avg:57.66ms
step:1048/11000 train_time:60430ms step_avg:57.66ms
step:1049/11000 train_time:60486ms step_avg:57.66ms
step:1050/11000 train_time:60545ms step_avg:57.66ms
step:1051/11000 train_time:60601ms step_avg:57.66ms
step:1052/11000 train_time:60661ms step_avg:57.66ms
step:1053/11000 train_time:60716ms step_avg:57.66ms
step:1054/11000 train_time:60776ms step_avg:57.66ms
step:1055/11000 train_time:60832ms step_avg:57.66ms
step:1056/11000 train_time:60892ms step_avg:57.66ms
step:1057/11000 train_time:60948ms step_avg:57.66ms
step:1058/11000 train_time:61009ms step_avg:57.66ms
step:1059/11000 train_time:61065ms step_avg:57.66ms
step:1060/11000 train_time:61125ms step_avg:57.67ms
step:1061/11000 train_time:61182ms step_avg:57.66ms
step:1062/11000 train_time:61241ms step_avg:57.67ms
step:1063/11000 train_time:61298ms step_avg:57.66ms
step:1064/11000 train_time:61357ms step_avg:57.67ms
step:1065/11000 train_time:61413ms step_avg:57.66ms
step:1066/11000 train_time:61471ms step_avg:57.67ms
step:1067/11000 train_time:61528ms step_avg:57.66ms
step:1068/11000 train_time:61587ms step_avg:57.67ms
step:1069/11000 train_time:61644ms step_avg:57.66ms
step:1070/11000 train_time:61703ms step_avg:57.67ms
step:1071/11000 train_time:61759ms step_avg:57.66ms
step:1072/11000 train_time:61818ms step_avg:57.67ms
step:1073/11000 train_time:61874ms step_avg:57.66ms
step:1074/11000 train_time:61934ms step_avg:57.67ms
step:1075/11000 train_time:61989ms step_avg:57.66ms
step:1076/11000 train_time:62049ms step_avg:57.67ms
step:1077/11000 train_time:62105ms step_avg:57.67ms
step:1078/11000 train_time:62166ms step_avg:57.67ms
step:1079/11000 train_time:62223ms step_avg:57.67ms
step:1080/11000 train_time:62283ms step_avg:57.67ms
step:1081/11000 train_time:62339ms step_avg:57.67ms
step:1082/11000 train_time:62400ms step_avg:57.67ms
step:1083/11000 train_time:62455ms step_avg:57.67ms
step:1084/11000 train_time:62513ms step_avg:57.67ms
step:1085/11000 train_time:62569ms step_avg:57.67ms
step:1086/11000 train_time:62628ms step_avg:57.67ms
step:1087/11000 train_time:62685ms step_avg:57.67ms
step:1088/11000 train_time:62744ms step_avg:57.67ms
step:1089/11000 train_time:62801ms step_avg:57.67ms
step:1090/11000 train_time:62860ms step_avg:57.67ms
step:1091/11000 train_time:62917ms step_avg:57.67ms
step:1092/11000 train_time:62976ms step_avg:57.67ms
step:1093/11000 train_time:63032ms step_avg:57.67ms
step:1094/11000 train_time:63092ms step_avg:57.67ms
step:1095/11000 train_time:63147ms step_avg:57.67ms
step:1096/11000 train_time:63207ms step_avg:57.67ms
step:1097/11000 train_time:63263ms step_avg:57.67ms
step:1098/11000 train_time:63323ms step_avg:57.67ms
step:1099/11000 train_time:63380ms step_avg:57.67ms
step:1100/11000 train_time:63439ms step_avg:57.67ms
step:1101/11000 train_time:63494ms step_avg:57.67ms
step:1102/11000 train_time:63554ms step_avg:57.67ms
step:1103/11000 train_time:63610ms step_avg:57.67ms
step:1104/11000 train_time:63669ms step_avg:57.67ms
step:1105/11000 train_time:63726ms step_avg:57.67ms
step:1106/11000 train_time:63785ms step_avg:57.67ms
step:1107/11000 train_time:63841ms step_avg:57.67ms
step:1108/11000 train_time:63901ms step_avg:57.67ms
step:1109/11000 train_time:63957ms step_avg:57.67ms
step:1110/11000 train_time:64016ms step_avg:57.67ms
step:1111/11000 train_time:64073ms step_avg:57.67ms
step:1112/11000 train_time:64132ms step_avg:57.67ms
step:1113/11000 train_time:64188ms step_avg:57.67ms
step:1114/11000 train_time:64248ms step_avg:57.67ms
step:1115/11000 train_time:64304ms step_avg:57.67ms
step:1116/11000 train_time:64364ms step_avg:57.67ms
step:1117/11000 train_time:64420ms step_avg:57.67ms
step:1118/11000 train_time:64480ms step_avg:57.67ms
step:1119/11000 train_time:64536ms step_avg:57.67ms
step:1120/11000 train_time:64595ms step_avg:57.67ms
step:1121/11000 train_time:64651ms step_avg:57.67ms
step:1122/11000 train_time:64710ms step_avg:57.67ms
step:1123/11000 train_time:64766ms step_avg:57.67ms
step:1124/11000 train_time:64826ms step_avg:57.67ms
step:1125/11000 train_time:64883ms step_avg:57.67ms
step:1126/11000 train_time:64942ms step_avg:57.68ms
step:1127/11000 train_time:64998ms step_avg:57.67ms
step:1128/11000 train_time:65058ms step_avg:57.68ms
step:1129/11000 train_time:65114ms step_avg:57.67ms
step:1130/11000 train_time:65174ms step_avg:57.68ms
step:1131/11000 train_time:65229ms step_avg:57.67ms
step:1132/11000 train_time:65288ms step_avg:57.68ms
step:1133/11000 train_time:65346ms step_avg:57.67ms
step:1134/11000 train_time:65405ms step_avg:57.68ms
step:1135/11000 train_time:65461ms step_avg:57.67ms
step:1136/11000 train_time:65520ms step_avg:57.68ms
step:1137/11000 train_time:65576ms step_avg:57.67ms
step:1138/11000 train_time:65635ms step_avg:57.68ms
step:1139/11000 train_time:65691ms step_avg:57.67ms
step:1140/11000 train_time:65751ms step_avg:57.68ms
step:1141/11000 train_time:65807ms step_avg:57.67ms
step:1142/11000 train_time:65867ms step_avg:57.68ms
step:1143/11000 train_time:65923ms step_avg:57.68ms
step:1144/11000 train_time:65983ms step_avg:57.68ms
step:1145/11000 train_time:66040ms step_avg:57.68ms
step:1146/11000 train_time:66100ms step_avg:57.68ms
step:1147/11000 train_time:66156ms step_avg:57.68ms
step:1148/11000 train_time:66216ms step_avg:57.68ms
step:1149/11000 train_time:66272ms step_avg:57.68ms
step:1150/11000 train_time:66330ms step_avg:57.68ms
step:1151/11000 train_time:66386ms step_avg:57.68ms
step:1152/11000 train_time:66447ms step_avg:57.68ms
step:1153/11000 train_time:66503ms step_avg:57.68ms
step:1154/11000 train_time:66563ms step_avg:57.68ms
step:1155/11000 train_time:66619ms step_avg:57.68ms
step:1156/11000 train_time:66679ms step_avg:57.68ms
step:1157/11000 train_time:66734ms step_avg:57.68ms
step:1158/11000 train_time:66793ms step_avg:57.68ms
step:1159/11000 train_time:66848ms step_avg:57.68ms
step:1160/11000 train_time:66909ms step_avg:57.68ms
step:1161/11000 train_time:66965ms step_avg:57.68ms
step:1162/11000 train_time:67025ms step_avg:57.68ms
step:1163/11000 train_time:67081ms step_avg:57.68ms
step:1164/11000 train_time:67141ms step_avg:57.68ms
step:1165/11000 train_time:67197ms step_avg:57.68ms
step:1166/11000 train_time:67256ms step_avg:57.68ms
step:1167/11000 train_time:67312ms step_avg:57.68ms
step:1168/11000 train_time:67371ms step_avg:57.68ms
step:1169/11000 train_time:67427ms step_avg:57.68ms
step:1170/11000 train_time:67487ms step_avg:57.68ms
step:1171/11000 train_time:67543ms step_avg:57.68ms
step:1172/11000 train_time:67603ms step_avg:57.68ms
step:1173/11000 train_time:67660ms step_avg:57.68ms
step:1174/11000 train_time:67719ms step_avg:57.68ms
step:1175/11000 train_time:67775ms step_avg:57.68ms
step:1176/11000 train_time:67834ms step_avg:57.68ms
step:1177/11000 train_time:67890ms step_avg:57.68ms
step:1178/11000 train_time:67950ms step_avg:57.68ms
step:1179/11000 train_time:68007ms step_avg:57.68ms
step:1180/11000 train_time:68066ms step_avg:57.68ms
step:1181/11000 train_time:68122ms step_avg:57.68ms
step:1182/11000 train_time:68182ms step_avg:57.68ms
step:1183/11000 train_time:68238ms step_avg:57.68ms
step:1184/11000 train_time:68298ms step_avg:57.68ms
step:1185/11000 train_time:68354ms step_avg:57.68ms
step:1186/11000 train_time:68413ms step_avg:57.68ms
step:1187/11000 train_time:68468ms step_avg:57.68ms
step:1188/11000 train_time:68529ms step_avg:57.68ms
step:1189/11000 train_time:68585ms step_avg:57.68ms
step:1190/11000 train_time:68645ms step_avg:57.68ms
step:1191/11000 train_time:68702ms step_avg:57.68ms
step:1192/11000 train_time:68761ms step_avg:57.69ms
step:1193/11000 train_time:68817ms step_avg:57.68ms
step:1194/11000 train_time:68876ms step_avg:57.68ms
step:1195/11000 train_time:68933ms step_avg:57.68ms
step:1196/11000 train_time:68992ms step_avg:57.69ms
step:1197/11000 train_time:69048ms step_avg:57.68ms
step:1198/11000 train_time:69107ms step_avg:57.69ms
step:1199/11000 train_time:69164ms step_avg:57.68ms
step:1200/11000 train_time:69224ms step_avg:57.69ms
step:1201/11000 train_time:69280ms step_avg:57.69ms
step:1202/11000 train_time:69340ms step_avg:57.69ms
step:1203/11000 train_time:69396ms step_avg:57.69ms
step:1204/11000 train_time:69455ms step_avg:57.69ms
step:1205/11000 train_time:69510ms step_avg:57.68ms
step:1206/11000 train_time:69569ms step_avg:57.69ms
step:1207/11000 train_time:69626ms step_avg:57.69ms
step:1208/11000 train_time:69685ms step_avg:57.69ms
step:1209/11000 train_time:69741ms step_avg:57.69ms
step:1210/11000 train_time:69801ms step_avg:57.69ms
step:1211/11000 train_time:69858ms step_avg:57.69ms
step:1212/11000 train_time:69917ms step_avg:57.69ms
step:1213/11000 train_time:69973ms step_avg:57.69ms
step:1214/11000 train_time:70032ms step_avg:57.69ms
step:1215/11000 train_time:70088ms step_avg:57.69ms
step:1216/11000 train_time:70148ms step_avg:57.69ms
step:1217/11000 train_time:70204ms step_avg:57.69ms
step:1218/11000 train_time:70264ms step_avg:57.69ms
step:1219/11000 train_time:70320ms step_avg:57.69ms
step:1220/11000 train_time:70381ms step_avg:57.69ms
step:1221/11000 train_time:70437ms step_avg:57.69ms
step:1222/11000 train_time:70496ms step_avg:57.69ms
step:1223/11000 train_time:70552ms step_avg:57.69ms
step:1224/11000 train_time:70611ms step_avg:57.69ms
step:1225/11000 train_time:70667ms step_avg:57.69ms
step:1226/11000 train_time:70727ms step_avg:57.69ms
step:1227/11000 train_time:70783ms step_avg:57.69ms
step:1228/11000 train_time:70843ms step_avg:57.69ms
step:1229/11000 train_time:70900ms step_avg:57.69ms
step:1230/11000 train_time:70959ms step_avg:57.69ms
step:1231/11000 train_time:71015ms step_avg:57.69ms
step:1232/11000 train_time:71074ms step_avg:57.69ms
step:1233/11000 train_time:71130ms step_avg:57.69ms
step:1234/11000 train_time:71189ms step_avg:57.69ms
step:1235/11000 train_time:71245ms step_avg:57.69ms
step:1236/11000 train_time:71304ms step_avg:57.69ms
step:1237/11000 train_time:71361ms step_avg:57.69ms
step:1238/11000 train_time:71421ms step_avg:57.69ms
step:1239/11000 train_time:71477ms step_avg:57.69ms
step:1240/11000 train_time:71537ms step_avg:57.69ms
step:1241/11000 train_time:71592ms step_avg:57.69ms
step:1242/11000 train_time:71652ms step_avg:57.69ms
step:1243/11000 train_time:71708ms step_avg:57.69ms
step:1244/11000 train_time:71768ms step_avg:57.69ms
step:1245/11000 train_time:71825ms step_avg:57.69ms
step:1246/11000 train_time:71884ms step_avg:57.69ms
step:1247/11000 train_time:71941ms step_avg:57.69ms
step:1248/11000 train_time:72000ms step_avg:57.69ms
step:1249/11000 train_time:72057ms step_avg:57.69ms
step:1250/11000 train_time:72116ms step_avg:57.69ms
step:1250/11000 val_loss:4.0406 train_time:72194ms step_avg:57.76ms
step:1251/11000 train_time:72214ms step_avg:57.73ms
step:1252/11000 train_time:72234ms step_avg:57.69ms
step:1253/11000 train_time:72293ms step_avg:57.70ms
step:1254/11000 train_time:72354ms step_avg:57.70ms
step:1255/11000 train_time:72410ms step_avg:57.70ms
step:1256/11000 train_time:72474ms step_avg:57.70ms
step:1257/11000 train_time:72529ms step_avg:57.70ms
step:1258/11000 train_time:72589ms step_avg:57.70ms
step:1259/11000 train_time:72645ms step_avg:57.70ms
step:1260/11000 train_time:72704ms step_avg:57.70ms
step:1261/11000 train_time:72759ms step_avg:57.70ms
step:1262/11000 train_time:72818ms step_avg:57.70ms
step:1263/11000 train_time:72873ms step_avg:57.70ms
step:1264/11000 train_time:72931ms step_avg:57.70ms
step:1265/11000 train_time:72987ms step_avg:57.70ms
step:1266/11000 train_time:73046ms step_avg:57.70ms
step:1267/11000 train_time:73101ms step_avg:57.70ms
step:1268/11000 train_time:73161ms step_avg:57.70ms
step:1269/11000 train_time:73219ms step_avg:57.70ms
step:1270/11000 train_time:73279ms step_avg:57.70ms
step:1271/11000 train_time:73336ms step_avg:57.70ms
step:1272/11000 train_time:73397ms step_avg:57.70ms
step:1273/11000 train_time:73454ms step_avg:57.70ms
step:1274/11000 train_time:73513ms step_avg:57.70ms
step:1275/11000 train_time:73569ms step_avg:57.70ms
step:1276/11000 train_time:73628ms step_avg:57.70ms
step:1277/11000 train_time:73684ms step_avg:57.70ms
step:1278/11000 train_time:73744ms step_avg:57.70ms
step:1279/11000 train_time:73799ms step_avg:57.70ms
step:1280/11000 train_time:73858ms step_avg:57.70ms
step:1281/11000 train_time:73913ms step_avg:57.70ms
step:1282/11000 train_time:73971ms step_avg:57.70ms
step:1283/11000 train_time:74027ms step_avg:57.70ms
step:1284/11000 train_time:74086ms step_avg:57.70ms
step:1285/11000 train_time:74144ms step_avg:57.70ms
