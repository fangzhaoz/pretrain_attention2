import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 8000  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 3000  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_final_match", exist_ok=True)
    logfile = f"logs_adamw_small_final_match/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=6e-4,  betas=(0.8, 0.8), eps=1e-10, weight_decay=9e-1, fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations 
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_final_match/{run_id}.txt", exist_ok=True)
            torch.save(log, f"logs_adamw_small_final_match/{run_id}.txt/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Wed Nov 12 00:47:55 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   37C    P0             118W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   36C    P0             119W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   31C    P0             118W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   29C    P0             115W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   35C    P0             120W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   30C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   34C    P0             115W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/8000 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/8000 train_time:81ms step_avg:80.64ms
step:2/8000 train_time:177ms step_avg:88.63ms
step:3/8000 train_time:196ms step_avg:65.17ms
step:4/8000 train_time:215ms step_avg:53.75ms
step:5/8000 train_time:268ms step_avg:53.64ms
step:6/8000 train_time:326ms step_avg:54.37ms
step:7/8000 train_time:382ms step_avg:54.54ms
step:8/8000 train_time:440ms step_avg:55.01ms
step:9/8000 train_time:495ms step_avg:55.04ms
step:10/8000 train_time:554ms step_avg:55.39ms
step:11/8000 train_time:609ms step_avg:55.39ms
step:12/8000 train_time:667ms step_avg:55.61ms
step:13/8000 train_time:723ms step_avg:55.60ms
step:14/8000 train_time:782ms step_avg:55.82ms
step:15/8000 train_time:838ms step_avg:55.84ms
step:16/8000 train_time:896ms step_avg:56.00ms
step:17/8000 train_time:952ms step_avg:55.99ms
step:18/8000 train_time:1010ms step_avg:56.12ms
step:19/8000 train_time:1068ms step_avg:56.22ms
step:20/8000 train_time:1130ms step_avg:56.50ms
step:21/8000 train_time:1188ms step_avg:56.55ms
step:22/8000 train_time:1247ms step_avg:56.69ms
step:23/8000 train_time:1304ms step_avg:56.67ms
step:24/8000 train_time:1362ms step_avg:56.76ms
step:25/8000 train_time:1418ms step_avg:56.73ms
step:26/8000 train_time:1478ms step_avg:56.84ms
step:27/8000 train_time:1533ms step_avg:56.77ms
step:28/8000 train_time:1592ms step_avg:56.86ms
step:29/8000 train_time:1648ms step_avg:56.81ms
step:30/8000 train_time:1706ms step_avg:56.86ms
step:31/8000 train_time:1761ms step_avg:56.81ms
step:32/8000 train_time:1820ms step_avg:56.86ms
step:33/8000 train_time:1875ms step_avg:56.80ms
step:34/8000 train_time:1934ms step_avg:56.88ms
step:35/8000 train_time:1990ms step_avg:56.85ms
step:36/8000 train_time:2049ms step_avg:56.92ms
step:37/8000 train_time:2106ms step_avg:56.92ms
step:38/8000 train_time:2166ms step_avg:57.01ms
step:39/8000 train_time:2223ms step_avg:56.99ms
step:40/8000 train_time:2283ms step_avg:57.08ms
step:41/8000 train_time:2340ms step_avg:57.07ms
step:42/8000 train_time:2399ms step_avg:57.11ms
step:43/8000 train_time:2454ms step_avg:57.08ms
step:44/8000 train_time:2514ms step_avg:57.13ms
step:45/8000 train_time:2569ms step_avg:57.09ms
step:46/8000 train_time:2628ms step_avg:57.13ms
step:47/8000 train_time:2684ms step_avg:57.10ms
step:48/8000 train_time:2742ms step_avg:57.12ms
step:49/8000 train_time:2797ms step_avg:57.09ms
step:50/8000 train_time:2856ms step_avg:57.12ms
step:51/8000 train_time:2911ms step_avg:57.09ms
step:52/8000 train_time:2971ms step_avg:57.13ms
step:53/8000 train_time:3027ms step_avg:57.11ms
step:54/8000 train_time:3087ms step_avg:57.17ms
step:55/8000 train_time:3144ms step_avg:57.16ms
step:56/8000 train_time:3205ms step_avg:57.22ms
step:57/8000 train_time:3261ms step_avg:57.20ms
step:58/8000 train_time:3321ms step_avg:57.25ms
step:59/8000 train_time:3376ms step_avg:57.23ms
step:60/8000 train_time:3436ms step_avg:57.27ms
step:61/8000 train_time:3492ms step_avg:57.24ms
step:62/8000 train_time:3552ms step_avg:57.30ms
step:63/8000 train_time:3608ms step_avg:57.27ms
step:64/8000 train_time:3667ms step_avg:57.29ms
step:65/8000 train_time:3722ms step_avg:57.26ms
step:66/8000 train_time:3781ms step_avg:57.28ms
step:67/8000 train_time:3837ms step_avg:57.26ms
step:68/8000 train_time:3896ms step_avg:57.29ms
step:69/8000 train_time:3951ms step_avg:57.26ms
step:70/8000 train_time:4010ms step_avg:57.29ms
step:71/8000 train_time:4066ms step_avg:57.27ms
step:72/8000 train_time:4126ms step_avg:57.31ms
step:73/8000 train_time:4183ms step_avg:57.31ms
step:74/8000 train_time:4242ms step_avg:57.33ms
step:75/8000 train_time:4299ms step_avg:57.32ms
step:76/8000 train_time:4358ms step_avg:57.34ms
step:77/8000 train_time:4415ms step_avg:57.33ms
step:78/8000 train_time:4474ms step_avg:57.36ms
step:79/8000 train_time:4530ms step_avg:57.34ms
step:80/8000 train_time:4588ms step_avg:57.35ms
step:81/8000 train_time:4644ms step_avg:57.33ms
step:82/8000 train_time:4703ms step_avg:57.35ms
step:83/8000 train_time:4759ms step_avg:57.34ms
step:84/8000 train_time:4818ms step_avg:57.36ms
step:85/8000 train_time:4874ms step_avg:57.34ms
step:86/8000 train_time:4933ms step_avg:57.36ms
step:87/8000 train_time:4989ms step_avg:57.34ms
step:88/8000 train_time:5048ms step_avg:57.37ms
step:89/8000 train_time:5104ms step_avg:57.35ms
step:90/8000 train_time:5163ms step_avg:57.37ms
step:91/8000 train_time:5220ms step_avg:57.36ms
step:92/8000 train_time:5279ms step_avg:57.38ms
step:93/8000 train_time:5335ms step_avg:57.37ms
step:94/8000 train_time:5395ms step_avg:57.39ms
step:95/8000 train_time:5450ms step_avg:57.37ms
step:96/8000 train_time:5509ms step_avg:57.39ms
step:97/8000 train_time:5565ms step_avg:57.37ms
step:98/8000 train_time:5623ms step_avg:57.38ms
step:99/8000 train_time:5679ms step_avg:57.37ms
step:100/8000 train_time:5738ms step_avg:57.38ms
step:101/8000 train_time:5793ms step_avg:57.36ms
step:102/8000 train_time:5854ms step_avg:57.39ms
step:103/8000 train_time:5909ms step_avg:57.37ms
step:104/8000 train_time:5968ms step_avg:57.39ms
step:105/8000 train_time:6024ms step_avg:57.37ms
step:106/8000 train_time:6083ms step_avg:57.39ms
step:107/8000 train_time:6139ms step_avg:57.38ms
step:108/8000 train_time:6199ms step_avg:57.40ms
step:109/8000 train_time:6255ms step_avg:57.39ms
step:110/8000 train_time:6314ms step_avg:57.40ms
step:111/8000 train_time:6371ms step_avg:57.40ms
step:112/8000 train_time:6429ms step_avg:57.40ms
step:113/8000 train_time:6486ms step_avg:57.40ms
step:114/8000 train_time:6545ms step_avg:57.41ms
step:115/8000 train_time:6601ms step_avg:57.40ms
step:116/8000 train_time:6660ms step_avg:57.41ms
step:117/8000 train_time:6716ms step_avg:57.40ms
step:118/8000 train_time:6775ms step_avg:57.41ms
step:119/8000 train_time:6831ms step_avg:57.40ms
step:120/8000 train_time:6889ms step_avg:57.41ms
step:121/8000 train_time:6944ms step_avg:57.39ms
step:122/8000 train_time:7004ms step_avg:57.41ms
step:123/8000 train_time:7060ms step_avg:57.40ms
step:124/8000 train_time:7119ms step_avg:57.41ms
step:125/8000 train_time:7175ms step_avg:57.40ms
step:126/8000 train_time:7235ms step_avg:57.42ms
step:127/8000 train_time:7292ms step_avg:57.41ms
step:128/8000 train_time:7350ms step_avg:57.42ms
step:129/8000 train_time:7406ms step_avg:57.41ms
step:130/8000 train_time:7466ms step_avg:57.43ms
step:131/8000 train_time:7522ms step_avg:57.42ms
step:132/8000 train_time:7581ms step_avg:57.43ms
step:133/8000 train_time:7637ms step_avg:57.42ms
step:134/8000 train_time:7697ms step_avg:57.44ms
step:135/8000 train_time:7753ms step_avg:57.43ms
step:136/8000 train_time:7811ms step_avg:57.44ms
step:137/8000 train_time:7867ms step_avg:57.43ms
step:138/8000 train_time:7926ms step_avg:57.44ms
step:139/8000 train_time:7983ms step_avg:57.43ms
step:140/8000 train_time:8041ms step_avg:57.44ms
step:141/8000 train_time:8096ms step_avg:57.42ms
step:142/8000 train_time:8155ms step_avg:57.43ms
step:143/8000 train_time:8211ms step_avg:57.42ms
step:144/8000 train_time:8270ms step_avg:57.43ms
step:145/8000 train_time:8326ms step_avg:57.42ms
step:146/8000 train_time:8386ms step_avg:57.44ms
step:147/8000 train_time:8443ms step_avg:57.43ms
step:148/8000 train_time:8501ms step_avg:57.44ms
step:149/8000 train_time:8558ms step_avg:57.43ms
step:150/8000 train_time:8616ms step_avg:57.44ms
step:151/8000 train_time:8672ms step_avg:57.43ms
step:152/8000 train_time:8731ms step_avg:57.44ms
step:153/8000 train_time:8787ms step_avg:57.43ms
step:154/8000 train_time:8846ms step_avg:57.44ms
step:155/8000 train_time:8901ms step_avg:57.43ms
step:156/8000 train_time:8960ms step_avg:57.44ms
step:157/8000 train_time:9016ms step_avg:57.43ms
step:158/8000 train_time:9077ms step_avg:57.45ms
step:159/8000 train_time:9132ms step_avg:57.43ms
step:160/8000 train_time:9192ms step_avg:57.45ms
step:161/8000 train_time:9248ms step_avg:57.44ms
step:162/8000 train_time:9307ms step_avg:57.45ms
step:163/8000 train_time:9363ms step_avg:57.44ms
step:164/8000 train_time:9422ms step_avg:57.45ms
step:165/8000 train_time:9478ms step_avg:57.44ms
step:166/8000 train_time:9537ms step_avg:57.45ms
step:167/8000 train_time:9593ms step_avg:57.44ms
step:168/8000 train_time:9652ms step_avg:57.45ms
step:169/8000 train_time:9708ms step_avg:57.44ms
step:170/8000 train_time:9767ms step_avg:57.45ms
step:171/8000 train_time:9823ms step_avg:57.45ms
step:172/8000 train_time:9882ms step_avg:57.45ms
step:173/8000 train_time:9938ms step_avg:57.44ms
step:174/8000 train_time:9997ms step_avg:57.45ms
step:175/8000 train_time:10052ms step_avg:57.44ms
step:176/8000 train_time:10111ms step_avg:57.45ms
step:177/8000 train_time:10167ms step_avg:57.44ms
step:178/8000 train_time:10226ms step_avg:57.45ms
step:179/8000 train_time:10281ms step_avg:57.44ms
step:180/8000 train_time:10341ms step_avg:57.45ms
step:181/8000 train_time:10397ms step_avg:57.44ms
step:182/8000 train_time:10456ms step_avg:57.45ms
step:183/8000 train_time:10512ms step_avg:57.44ms
step:184/8000 train_time:10571ms step_avg:57.45ms
step:185/8000 train_time:10628ms step_avg:57.45ms
step:186/8000 train_time:10687ms step_avg:57.46ms
step:187/8000 train_time:10743ms step_avg:57.45ms
step:188/8000 train_time:10802ms step_avg:57.46ms
step:189/8000 train_time:10859ms step_avg:57.45ms
step:190/8000 train_time:10917ms step_avg:57.46ms
step:191/8000 train_time:10973ms step_avg:57.45ms
step:192/8000 train_time:11032ms step_avg:57.46ms
step:193/8000 train_time:11088ms step_avg:57.45ms
step:194/8000 train_time:11147ms step_avg:57.46ms
step:195/8000 train_time:11203ms step_avg:57.45ms
step:196/8000 train_time:11262ms step_avg:57.46ms
step:197/8000 train_time:11318ms step_avg:57.45ms
step:198/8000 train_time:11377ms step_avg:57.46ms
step:199/8000 train_time:11434ms step_avg:57.46ms
step:200/8000 train_time:11493ms step_avg:57.47ms
step:201/8000 train_time:11549ms step_avg:57.46ms
step:202/8000 train_time:11608ms step_avg:57.47ms
step:203/8000 train_time:11664ms step_avg:57.46ms
step:204/8000 train_time:11723ms step_avg:57.47ms
step:205/8000 train_time:11780ms step_avg:57.46ms
step:206/8000 train_time:11839ms step_avg:57.47ms
step:207/8000 train_time:11894ms step_avg:57.46ms
step:208/8000 train_time:11953ms step_avg:57.47ms
step:209/8000 train_time:12009ms step_avg:57.46ms
step:210/8000 train_time:12068ms step_avg:57.47ms
step:211/8000 train_time:12124ms step_avg:57.46ms
step:212/8000 train_time:12184ms step_avg:57.47ms
step:213/8000 train_time:12240ms step_avg:57.46ms
step:214/8000 train_time:12299ms step_avg:57.47ms
step:215/8000 train_time:12354ms step_avg:57.46ms
step:216/8000 train_time:12414ms step_avg:57.47ms
step:217/8000 train_time:12470ms step_avg:57.47ms
step:218/8000 train_time:12529ms step_avg:57.47ms
step:219/8000 train_time:12585ms step_avg:57.46ms
step:220/8000 train_time:12644ms step_avg:57.47ms
step:221/8000 train_time:12699ms step_avg:57.46ms
step:222/8000 train_time:12759ms step_avg:57.47ms
step:223/8000 train_time:12814ms step_avg:57.46ms
step:224/8000 train_time:12874ms step_avg:57.47ms
step:225/8000 train_time:12929ms step_avg:57.46ms
step:226/8000 train_time:12989ms step_avg:57.47ms
step:227/8000 train_time:13044ms step_avg:57.46ms
step:228/8000 train_time:13103ms step_avg:57.47ms
step:229/8000 train_time:13159ms step_avg:57.46ms
step:230/8000 train_time:13219ms step_avg:57.47ms
step:231/8000 train_time:13274ms step_avg:57.46ms
step:232/8000 train_time:13336ms step_avg:57.48ms
step:233/8000 train_time:13392ms step_avg:57.48ms
step:234/8000 train_time:13451ms step_avg:57.48ms
step:235/8000 train_time:13507ms step_avg:57.47ms
step:236/8000 train_time:13567ms step_avg:57.49ms
step:237/8000 train_time:13622ms step_avg:57.48ms
step:238/8000 train_time:13682ms step_avg:57.49ms
step:239/8000 train_time:13738ms step_avg:57.48ms
step:240/8000 train_time:13798ms step_avg:57.49ms
step:241/8000 train_time:13853ms step_avg:57.48ms
step:242/8000 train_time:13913ms step_avg:57.49ms
step:243/8000 train_time:13968ms step_avg:57.48ms
step:244/8000 train_time:14028ms step_avg:57.49ms
step:245/8000 train_time:14084ms step_avg:57.48ms
step:246/8000 train_time:14143ms step_avg:57.49ms
step:247/8000 train_time:14199ms step_avg:57.49ms
step:248/8000 train_time:14259ms step_avg:57.49ms
step:249/8000 train_time:14315ms step_avg:57.49ms
step:250/8000 train_time:14374ms step_avg:57.49ms
step:250/8000 val_loss:4.8997 train_time:14453ms step_avg:57.81ms
step:251/8000 train_time:14471ms step_avg:57.65ms
step:252/8000 train_time:14490ms step_avg:57.50ms
step:253/8000 train_time:14545ms step_avg:57.49ms
step:254/8000 train_time:14610ms step_avg:57.52ms
step:255/8000 train_time:14665ms step_avg:57.51ms
step:256/8000 train_time:14730ms step_avg:57.54ms
step:257/8000 train_time:14786ms step_avg:57.53ms
step:258/8000 train_time:14845ms step_avg:57.54ms
step:259/8000 train_time:14901ms step_avg:57.53ms
step:260/8000 train_time:14960ms step_avg:57.54ms
step:261/8000 train_time:15016ms step_avg:57.53ms
step:262/8000 train_time:15075ms step_avg:57.54ms
step:263/8000 train_time:15130ms step_avg:57.53ms
step:264/8000 train_time:15189ms step_avg:57.53ms
step:265/8000 train_time:15244ms step_avg:57.53ms
step:266/8000 train_time:15302ms step_avg:57.53ms
step:267/8000 train_time:15361ms step_avg:57.53ms
step:268/8000 train_time:15421ms step_avg:57.54ms
step:269/8000 train_time:15478ms step_avg:57.54ms
step:270/8000 train_time:15537ms step_avg:57.54ms
step:271/8000 train_time:15594ms step_avg:57.54ms
step:272/8000 train_time:15654ms step_avg:57.55ms
step:273/8000 train_time:15710ms step_avg:57.54ms
step:274/8000 train_time:15769ms step_avg:57.55ms
step:275/8000 train_time:15825ms step_avg:57.54ms
step:276/8000 train_time:15884ms step_avg:57.55ms
step:277/8000 train_time:15940ms step_avg:57.54ms
step:278/8000 train_time:16000ms step_avg:57.55ms
step:279/8000 train_time:16055ms step_avg:57.54ms
step:280/8000 train_time:16114ms step_avg:57.55ms
step:281/8000 train_time:16169ms step_avg:57.54ms
step:282/8000 train_time:16228ms step_avg:57.55ms
step:283/8000 train_time:16284ms step_avg:57.54ms
step:284/8000 train_time:16344ms step_avg:57.55ms
step:285/8000 train_time:16402ms step_avg:57.55ms
step:286/8000 train_time:16462ms step_avg:57.56ms
step:287/8000 train_time:16518ms step_avg:57.56ms
step:288/8000 train_time:16577ms step_avg:57.56ms
step:289/8000 train_time:16633ms step_avg:57.55ms
step:290/8000 train_time:16693ms step_avg:57.56ms
step:291/8000 train_time:16749ms step_avg:57.56ms
step:292/8000 train_time:16809ms step_avg:57.57ms
step:293/8000 train_time:16865ms step_avg:57.56ms
step:294/8000 train_time:16924ms step_avg:57.56ms
step:295/8000 train_time:16980ms step_avg:57.56ms
step:296/8000 train_time:17040ms step_avg:57.57ms
step:297/8000 train_time:17096ms step_avg:57.56ms
step:298/8000 train_time:17155ms step_avg:57.57ms
step:299/8000 train_time:17211ms step_avg:57.56ms
step:300/8000 train_time:17269ms step_avg:57.56ms
step:301/8000 train_time:17325ms step_avg:57.56ms
step:302/8000 train_time:17384ms step_avg:57.56ms
step:303/8000 train_time:17440ms step_avg:57.56ms
step:304/8000 train_time:17500ms step_avg:57.56ms
step:305/8000 train_time:17556ms step_avg:57.56ms
step:306/8000 train_time:17615ms step_avg:57.57ms
step:307/8000 train_time:17672ms step_avg:57.56ms
step:308/8000 train_time:17731ms step_avg:57.57ms
step:309/8000 train_time:17786ms step_avg:57.56ms
step:310/8000 train_time:17846ms step_avg:57.57ms
step:311/8000 train_time:17902ms step_avg:57.56ms
step:312/8000 train_time:17962ms step_avg:57.57ms
step:313/8000 train_time:18018ms step_avg:57.56ms
step:314/8000 train_time:18077ms step_avg:57.57ms
step:315/8000 train_time:18133ms step_avg:57.57ms
step:316/8000 train_time:18192ms step_avg:57.57ms
step:317/8000 train_time:18248ms step_avg:57.56ms
step:318/8000 train_time:18307ms step_avg:57.57ms
step:319/8000 train_time:18363ms step_avg:57.56ms
step:320/8000 train_time:18422ms step_avg:57.57ms
step:321/8000 train_time:18478ms step_avg:57.56ms
step:322/8000 train_time:18538ms step_avg:57.57ms
step:323/8000 train_time:18594ms step_avg:57.57ms
step:324/8000 train_time:18654ms step_avg:57.57ms
step:325/8000 train_time:18710ms step_avg:57.57ms
step:326/8000 train_time:18770ms step_avg:57.58ms
step:327/8000 train_time:18826ms step_avg:57.57ms
step:328/8000 train_time:18885ms step_avg:57.58ms
step:329/8000 train_time:18943ms step_avg:57.58ms
step:330/8000 train_time:19002ms step_avg:57.58ms
step:331/8000 train_time:19059ms step_avg:57.58ms
step:332/8000 train_time:19117ms step_avg:57.58ms
step:333/8000 train_time:19173ms step_avg:57.58ms
step:334/8000 train_time:19232ms step_avg:57.58ms
step:335/8000 train_time:19288ms step_avg:57.57ms
step:336/8000 train_time:19347ms step_avg:57.58ms
step:337/8000 train_time:19404ms step_avg:57.58ms
step:338/8000 train_time:19463ms step_avg:57.58ms
step:339/8000 train_time:19520ms step_avg:57.58ms
step:340/8000 train_time:19579ms step_avg:57.59ms
step:341/8000 train_time:19635ms step_avg:57.58ms
step:342/8000 train_time:19695ms step_avg:57.59ms
step:343/8000 train_time:19751ms step_avg:57.58ms
step:344/8000 train_time:19810ms step_avg:57.59ms
step:345/8000 train_time:19866ms step_avg:57.58ms
step:346/8000 train_time:19926ms step_avg:57.59ms
step:347/8000 train_time:19983ms step_avg:57.59ms
step:348/8000 train_time:20042ms step_avg:57.59ms
step:349/8000 train_time:20099ms step_avg:57.59ms
step:350/8000 train_time:20159ms step_avg:57.60ms
step:351/8000 train_time:20214ms step_avg:57.59ms
step:352/8000 train_time:20273ms step_avg:57.59ms
step:353/8000 train_time:20328ms step_avg:57.59ms
step:354/8000 train_time:20389ms step_avg:57.60ms
step:355/8000 train_time:20445ms step_avg:57.59ms
step:356/8000 train_time:20504ms step_avg:57.59ms
step:357/8000 train_time:20560ms step_avg:57.59ms
step:358/8000 train_time:20619ms step_avg:57.60ms
step:359/8000 train_time:20675ms step_avg:57.59ms
step:360/8000 train_time:20735ms step_avg:57.60ms
step:361/8000 train_time:20792ms step_avg:57.59ms
step:362/8000 train_time:20850ms step_avg:57.60ms
step:363/8000 train_time:20906ms step_avg:57.59ms
step:364/8000 train_time:20965ms step_avg:57.60ms
step:365/8000 train_time:21021ms step_avg:57.59ms
step:366/8000 train_time:21081ms step_avg:57.60ms
step:367/8000 train_time:21137ms step_avg:57.60ms
step:368/8000 train_time:21198ms step_avg:57.60ms
step:369/8000 train_time:21253ms step_avg:57.60ms
step:370/8000 train_time:21313ms step_avg:57.60ms
step:371/8000 train_time:21368ms step_avg:57.60ms
step:372/8000 train_time:21429ms step_avg:57.60ms
step:373/8000 train_time:21485ms step_avg:57.60ms
step:374/8000 train_time:21544ms step_avg:57.60ms
step:375/8000 train_time:21600ms step_avg:57.60ms
step:376/8000 train_time:21659ms step_avg:57.60ms
step:377/8000 train_time:21715ms step_avg:57.60ms
step:378/8000 train_time:21775ms step_avg:57.61ms
step:379/8000 train_time:21830ms step_avg:57.60ms
step:380/8000 train_time:21890ms step_avg:57.60ms
step:381/8000 train_time:21945ms step_avg:57.60ms
step:382/8000 train_time:22004ms step_avg:57.60ms
step:383/8000 train_time:22060ms step_avg:57.60ms
step:384/8000 train_time:22120ms step_avg:57.60ms
step:385/8000 train_time:22176ms step_avg:57.60ms
step:386/8000 train_time:22235ms step_avg:57.60ms
step:387/8000 train_time:22291ms step_avg:57.60ms
step:388/8000 train_time:22351ms step_avg:57.60ms
step:389/8000 train_time:22407ms step_avg:57.60ms
step:390/8000 train_time:22465ms step_avg:57.60ms
step:391/8000 train_time:22521ms step_avg:57.60ms
step:392/8000 train_time:22581ms step_avg:57.60ms
step:393/8000 train_time:22637ms step_avg:57.60ms
step:394/8000 train_time:22696ms step_avg:57.60ms
step:395/8000 train_time:22752ms step_avg:57.60ms
step:396/8000 train_time:22812ms step_avg:57.61ms
step:397/8000 train_time:22868ms step_avg:57.60ms
step:398/8000 train_time:22929ms step_avg:57.61ms
step:399/8000 train_time:22985ms step_avg:57.61ms
step:400/8000 train_time:23044ms step_avg:57.61ms
step:401/8000 train_time:23100ms step_avg:57.61ms
step:402/8000 train_time:23160ms step_avg:57.61ms
step:403/8000 train_time:23216ms step_avg:57.61ms
step:404/8000 train_time:23274ms step_avg:57.61ms
step:405/8000 train_time:23330ms step_avg:57.60ms
step:406/8000 train_time:23390ms step_avg:57.61ms
step:407/8000 train_time:23446ms step_avg:57.61ms
step:408/8000 train_time:23506ms step_avg:57.61ms
step:409/8000 train_time:23562ms step_avg:57.61ms
step:410/8000 train_time:23621ms step_avg:57.61ms
step:411/8000 train_time:23677ms step_avg:57.61ms
step:412/8000 train_time:23737ms step_avg:57.61ms
step:413/8000 train_time:23793ms step_avg:57.61ms
step:414/8000 train_time:23852ms step_avg:57.61ms
step:415/8000 train_time:23908ms step_avg:57.61ms
step:416/8000 train_time:23968ms step_avg:57.62ms
step:417/8000 train_time:24025ms step_avg:57.61ms
step:418/8000 train_time:24084ms step_avg:57.62ms
step:419/8000 train_time:24140ms step_avg:57.61ms
step:420/8000 train_time:24199ms step_avg:57.62ms
step:421/8000 train_time:24255ms step_avg:57.61ms
step:422/8000 train_time:24314ms step_avg:57.62ms
step:423/8000 train_time:24370ms step_avg:57.61ms
step:424/8000 train_time:24429ms step_avg:57.62ms
step:425/8000 train_time:24485ms step_avg:57.61ms
step:426/8000 train_time:24544ms step_avg:57.61ms
step:427/8000 train_time:24601ms step_avg:57.61ms
step:428/8000 train_time:24661ms step_avg:57.62ms
step:429/8000 train_time:24717ms step_avg:57.62ms
step:430/8000 train_time:24776ms step_avg:57.62ms
step:431/8000 train_time:24832ms step_avg:57.62ms
step:432/8000 train_time:24891ms step_avg:57.62ms
step:433/8000 train_time:24947ms step_avg:57.61ms
step:434/8000 train_time:25006ms step_avg:57.62ms
step:435/8000 train_time:25063ms step_avg:57.62ms
step:436/8000 train_time:25122ms step_avg:57.62ms
step:437/8000 train_time:25179ms step_avg:57.62ms
step:438/8000 train_time:25238ms step_avg:57.62ms
step:439/8000 train_time:25294ms step_avg:57.62ms
step:440/8000 train_time:25353ms step_avg:57.62ms
step:441/8000 train_time:25409ms step_avg:57.62ms
step:442/8000 train_time:25468ms step_avg:57.62ms
step:443/8000 train_time:25524ms step_avg:57.62ms
step:444/8000 train_time:25583ms step_avg:57.62ms
step:445/8000 train_time:25639ms step_avg:57.62ms
step:446/8000 train_time:25699ms step_avg:57.62ms
step:447/8000 train_time:25756ms step_avg:57.62ms
step:448/8000 train_time:25815ms step_avg:57.62ms
step:449/8000 train_time:25870ms step_avg:57.62ms
step:450/8000 train_time:25930ms step_avg:57.62ms
step:451/8000 train_time:25986ms step_avg:57.62ms
step:452/8000 train_time:26045ms step_avg:57.62ms
step:453/8000 train_time:26102ms step_avg:57.62ms
step:454/8000 train_time:26162ms step_avg:57.62ms
step:455/8000 train_time:26218ms step_avg:57.62ms
step:456/8000 train_time:26277ms step_avg:57.62ms
step:457/8000 train_time:26333ms step_avg:57.62ms
step:458/8000 train_time:26392ms step_avg:57.63ms
step:459/8000 train_time:26449ms step_avg:57.62ms
step:460/8000 train_time:26508ms step_avg:57.63ms
step:461/8000 train_time:26564ms step_avg:57.62ms
step:462/8000 train_time:26623ms step_avg:57.63ms
step:463/8000 train_time:26680ms step_avg:57.62ms
step:464/8000 train_time:26740ms step_avg:57.63ms
step:465/8000 train_time:26796ms step_avg:57.63ms
step:466/8000 train_time:26856ms step_avg:57.63ms
step:467/8000 train_time:26911ms step_avg:57.63ms
step:468/8000 train_time:26971ms step_avg:57.63ms
step:469/8000 train_time:27026ms step_avg:57.62ms
step:470/8000 train_time:27086ms step_avg:57.63ms
step:471/8000 train_time:27142ms step_avg:57.63ms
step:472/8000 train_time:27202ms step_avg:57.63ms
step:473/8000 train_time:27258ms step_avg:57.63ms
step:474/8000 train_time:27318ms step_avg:57.63ms
step:475/8000 train_time:27373ms step_avg:57.63ms
step:476/8000 train_time:27434ms step_avg:57.63ms
step:477/8000 train_time:27490ms step_avg:57.63ms
step:478/8000 train_time:27549ms step_avg:57.63ms
step:479/8000 train_time:27605ms step_avg:57.63ms
step:480/8000 train_time:27664ms step_avg:57.63ms
step:481/8000 train_time:27720ms step_avg:57.63ms
step:482/8000 train_time:27780ms step_avg:57.63ms
step:483/8000 train_time:27836ms step_avg:57.63ms
step:484/8000 train_time:27897ms step_avg:57.64ms
step:485/8000 train_time:27952ms step_avg:57.63ms
step:486/8000 train_time:28012ms step_avg:57.64ms
step:487/8000 train_time:28067ms step_avg:57.63ms
step:488/8000 train_time:28127ms step_avg:57.64ms
step:489/8000 train_time:28184ms step_avg:57.64ms
step:490/8000 train_time:28243ms step_avg:57.64ms
step:491/8000 train_time:28298ms step_avg:57.63ms
step:492/8000 train_time:28358ms step_avg:57.64ms
step:493/8000 train_time:28414ms step_avg:57.63ms
step:494/8000 train_time:28473ms step_avg:57.64ms
step:495/8000 train_time:28529ms step_avg:57.63ms
step:496/8000 train_time:28589ms step_avg:57.64ms
step:497/8000 train_time:28645ms step_avg:57.64ms
step:498/8000 train_time:28704ms step_avg:57.64ms
step:499/8000 train_time:28761ms step_avg:57.64ms
step:500/8000 train_time:28820ms step_avg:57.64ms
step:500/8000 val_loss:4.4073 train_time:28900ms step_avg:57.80ms
step:501/8000 train_time:28917ms step_avg:57.72ms
step:502/8000 train_time:28937ms step_avg:57.64ms
step:503/8000 train_time:28992ms step_avg:57.64ms
step:504/8000 train_time:29057ms step_avg:57.65ms
step:505/8000 train_time:29113ms step_avg:57.65ms
step:506/8000 train_time:29180ms step_avg:57.67ms
step:507/8000 train_time:29236ms step_avg:57.66ms
step:508/8000 train_time:29296ms step_avg:57.67ms
step:509/8000 train_time:29352ms step_avg:57.67ms
step:510/8000 train_time:29411ms step_avg:57.67ms
step:511/8000 train_time:29467ms step_avg:57.67ms
step:512/8000 train_time:29525ms step_avg:57.67ms
step:513/8000 train_time:29581ms step_avg:57.66ms
step:514/8000 train_time:29640ms step_avg:57.67ms
step:515/8000 train_time:29696ms step_avg:57.66ms
step:516/8000 train_time:29754ms step_avg:57.66ms
step:517/8000 train_time:29810ms step_avg:57.66ms
step:518/8000 train_time:29869ms step_avg:57.66ms
step:519/8000 train_time:29926ms step_avg:57.66ms
step:520/8000 train_time:29986ms step_avg:57.67ms
step:521/8000 train_time:30042ms step_avg:57.66ms
step:522/8000 train_time:30105ms step_avg:57.67ms
step:523/8000 train_time:30161ms step_avg:57.67ms
step:524/8000 train_time:30223ms step_avg:57.68ms
step:525/8000 train_time:30279ms step_avg:57.67ms
step:526/8000 train_time:30338ms step_avg:57.68ms
step:527/8000 train_time:30394ms step_avg:57.67ms
step:528/8000 train_time:30453ms step_avg:57.68ms
step:529/8000 train_time:30508ms step_avg:57.67ms
step:530/8000 train_time:30567ms step_avg:57.67ms
step:531/8000 train_time:30623ms step_avg:57.67ms
step:532/8000 train_time:30682ms step_avg:57.67ms
step:533/8000 train_time:30738ms step_avg:57.67ms
step:534/8000 train_time:30797ms step_avg:57.67ms
step:535/8000 train_time:30853ms step_avg:57.67ms
step:536/8000 train_time:30913ms step_avg:57.67ms
step:537/8000 train_time:30970ms step_avg:57.67ms
step:538/8000 train_time:31029ms step_avg:57.68ms
step:539/8000 train_time:31085ms step_avg:57.67ms
step:540/8000 train_time:31147ms step_avg:57.68ms
step:541/8000 train_time:31202ms step_avg:57.68ms
step:542/8000 train_time:31264ms step_avg:57.68ms
step:543/8000 train_time:31320ms step_avg:57.68ms
step:544/8000 train_time:31379ms step_avg:57.68ms
step:545/8000 train_time:31435ms step_avg:57.68ms
step:546/8000 train_time:31494ms step_avg:57.68ms
step:547/8000 train_time:31550ms step_avg:57.68ms
step:548/8000 train_time:31609ms step_avg:57.68ms
step:549/8000 train_time:31665ms step_avg:57.68ms
step:550/8000 train_time:31723ms step_avg:57.68ms
step:551/8000 train_time:31779ms step_avg:57.68ms
step:552/8000 train_time:31839ms step_avg:57.68ms
step:553/8000 train_time:31896ms step_avg:57.68ms
step:554/8000 train_time:31955ms step_avg:57.68ms
step:555/8000 train_time:32011ms step_avg:57.68ms
step:556/8000 train_time:32071ms step_avg:57.68ms
step:557/8000 train_time:32128ms step_avg:57.68ms
step:558/8000 train_time:32188ms step_avg:57.68ms
step:559/8000 train_time:32244ms step_avg:57.68ms
step:560/8000 train_time:32304ms step_avg:57.69ms
step:561/8000 train_time:32360ms step_avg:57.68ms
step:562/8000 train_time:32420ms step_avg:57.69ms
step:563/8000 train_time:32476ms step_avg:57.68ms
step:564/8000 train_time:32535ms step_avg:57.69ms
step:565/8000 train_time:32591ms step_avg:57.68ms
step:566/8000 train_time:32650ms step_avg:57.69ms
step:567/8000 train_time:32705ms step_avg:57.68ms
step:568/8000 train_time:32766ms step_avg:57.69ms
step:569/8000 train_time:32821ms step_avg:57.68ms
step:570/8000 train_time:32881ms step_avg:57.69ms
step:571/8000 train_time:32937ms step_avg:57.68ms
step:572/8000 train_time:32997ms step_avg:57.69ms
step:573/8000 train_time:33054ms step_avg:57.69ms
step:574/8000 train_time:33113ms step_avg:57.69ms
step:575/8000 train_time:33169ms step_avg:57.69ms
step:576/8000 train_time:33229ms step_avg:57.69ms
step:577/8000 train_time:33285ms step_avg:57.69ms
step:578/8000 train_time:33346ms step_avg:57.69ms
step:579/8000 train_time:33402ms step_avg:57.69ms
step:580/8000 train_time:33463ms step_avg:57.69ms
step:581/8000 train_time:33519ms step_avg:57.69ms
step:582/8000 train_time:33577ms step_avg:57.69ms
step:583/8000 train_time:33633ms step_avg:57.69ms
step:584/8000 train_time:33692ms step_avg:57.69ms
step:585/8000 train_time:33748ms step_avg:57.69ms
step:586/8000 train_time:33807ms step_avg:57.69ms
step:587/8000 train_time:33863ms step_avg:57.69ms
step:588/8000 train_time:33923ms step_avg:57.69ms
step:589/8000 train_time:33979ms step_avg:57.69ms
step:590/8000 train_time:34039ms step_avg:57.69ms
step:591/8000 train_time:34096ms step_avg:57.69ms
step:592/8000 train_time:34157ms step_avg:57.70ms
step:593/8000 train_time:34213ms step_avg:57.69ms
step:594/8000 train_time:34273ms step_avg:57.70ms
step:595/8000 train_time:34328ms step_avg:57.69ms
step:596/8000 train_time:34388ms step_avg:57.70ms
step:597/8000 train_time:34444ms step_avg:57.70ms
step:598/8000 train_time:34505ms step_avg:57.70ms
step:599/8000 train_time:34561ms step_avg:57.70ms
step:600/8000 train_time:34620ms step_avg:57.70ms
step:601/8000 train_time:34676ms step_avg:57.70ms
step:602/8000 train_time:34735ms step_avg:57.70ms
step:603/8000 train_time:34792ms step_avg:57.70ms
step:604/8000 train_time:34850ms step_avg:57.70ms
step:605/8000 train_time:34906ms step_avg:57.70ms
step:606/8000 train_time:34965ms step_avg:57.70ms
step:607/8000 train_time:35021ms step_avg:57.70ms
step:608/8000 train_time:35081ms step_avg:57.70ms
step:609/8000 train_time:35137ms step_avg:57.70ms
step:610/8000 train_time:35197ms step_avg:57.70ms
step:611/8000 train_time:35254ms step_avg:57.70ms
step:612/8000 train_time:35313ms step_avg:57.70ms
step:613/8000 train_time:35369ms step_avg:57.70ms
step:614/8000 train_time:35429ms step_avg:57.70ms
step:615/8000 train_time:35486ms step_avg:57.70ms
step:616/8000 train_time:35545ms step_avg:57.70ms
step:617/8000 train_time:35600ms step_avg:57.70ms
step:618/8000 train_time:35659ms step_avg:57.70ms
step:619/8000 train_time:35715ms step_avg:57.70ms
step:620/8000 train_time:35774ms step_avg:57.70ms
step:621/8000 train_time:35830ms step_avg:57.70ms
step:622/8000 train_time:35889ms step_avg:57.70ms
step:623/8000 train_time:35945ms step_avg:57.70ms
step:624/8000 train_time:36005ms step_avg:57.70ms
step:625/8000 train_time:36062ms step_avg:57.70ms
step:626/8000 train_time:36121ms step_avg:57.70ms
step:627/8000 train_time:36178ms step_avg:57.70ms
step:628/8000 train_time:36237ms step_avg:57.70ms
step:629/8000 train_time:36294ms step_avg:57.70ms
step:630/8000 train_time:36353ms step_avg:57.70ms
step:631/8000 train_time:36409ms step_avg:57.70ms
step:632/8000 train_time:36469ms step_avg:57.70ms
step:633/8000 train_time:36525ms step_avg:57.70ms
step:634/8000 train_time:36585ms step_avg:57.71ms
step:635/8000 train_time:36640ms step_avg:57.70ms
step:636/8000 train_time:36700ms step_avg:57.70ms
step:637/8000 train_time:36756ms step_avg:57.70ms
step:638/8000 train_time:36815ms step_avg:57.70ms
step:639/8000 train_time:36871ms step_avg:57.70ms
step:640/8000 train_time:36930ms step_avg:57.70ms
step:641/8000 train_time:36986ms step_avg:57.70ms
step:642/8000 train_time:37046ms step_avg:57.70ms
step:643/8000 train_time:37102ms step_avg:57.70ms
step:644/8000 train_time:37162ms step_avg:57.70ms
step:645/8000 train_time:37218ms step_avg:57.70ms
step:646/8000 train_time:37278ms step_avg:57.71ms
step:647/8000 train_time:37335ms step_avg:57.71ms
step:648/8000 train_time:37395ms step_avg:57.71ms
step:649/8000 train_time:37451ms step_avg:57.71ms
step:650/8000 train_time:37511ms step_avg:57.71ms
step:651/8000 train_time:37566ms step_avg:57.71ms
step:652/8000 train_time:37625ms step_avg:57.71ms
step:653/8000 train_time:37681ms step_avg:57.70ms
step:654/8000 train_time:37742ms step_avg:57.71ms
step:655/8000 train_time:37798ms step_avg:57.71ms
step:656/8000 train_time:37857ms step_avg:57.71ms
step:657/8000 train_time:37913ms step_avg:57.71ms
step:658/8000 train_time:37973ms step_avg:57.71ms
step:659/8000 train_time:38028ms step_avg:57.71ms
step:660/8000 train_time:38089ms step_avg:57.71ms
step:661/8000 train_time:38145ms step_avg:57.71ms
step:662/8000 train_time:38205ms step_avg:57.71ms
step:663/8000 train_time:38260ms step_avg:57.71ms
step:664/8000 train_time:38320ms step_avg:57.71ms
step:665/8000 train_time:38377ms step_avg:57.71ms
step:666/8000 train_time:38436ms step_avg:57.71ms
step:667/8000 train_time:38492ms step_avg:57.71ms
step:668/8000 train_time:38551ms step_avg:57.71ms
step:669/8000 train_time:38607ms step_avg:57.71ms
step:670/8000 train_time:38667ms step_avg:57.71ms
step:671/8000 train_time:38723ms step_avg:57.71ms
step:672/8000 train_time:38783ms step_avg:57.71ms
step:673/8000 train_time:38839ms step_avg:57.71ms
step:674/8000 train_time:38897ms step_avg:57.71ms
step:675/8000 train_time:38953ms step_avg:57.71ms
step:676/8000 train_time:39013ms step_avg:57.71ms
step:677/8000 train_time:39069ms step_avg:57.71ms
step:678/8000 train_time:39129ms step_avg:57.71ms
step:679/8000 train_time:39185ms step_avg:57.71ms
step:680/8000 train_time:39245ms step_avg:57.71ms
step:681/8000 train_time:39300ms step_avg:57.71ms
step:682/8000 train_time:39361ms step_avg:57.71ms
step:683/8000 train_time:39417ms step_avg:57.71ms
step:684/8000 train_time:39476ms step_avg:57.71ms
step:685/8000 train_time:39533ms step_avg:57.71ms
step:686/8000 train_time:39591ms step_avg:57.71ms
step:687/8000 train_time:39647ms step_avg:57.71ms
step:688/8000 train_time:39707ms step_avg:57.71ms
step:689/8000 train_time:39763ms step_avg:57.71ms
step:690/8000 train_time:39823ms step_avg:57.71ms
step:691/8000 train_time:39879ms step_avg:57.71ms
step:692/8000 train_time:39937ms step_avg:57.71ms
step:693/8000 train_time:39993ms step_avg:57.71ms
step:694/8000 train_time:40052ms step_avg:57.71ms
step:695/8000 train_time:40108ms step_avg:57.71ms
step:696/8000 train_time:40167ms step_avg:57.71ms
step:697/8000 train_time:40223ms step_avg:57.71ms
step:698/8000 train_time:40283ms step_avg:57.71ms
step:699/8000 train_time:40339ms step_avg:57.71ms
step:700/8000 train_time:40398ms step_avg:57.71ms
step:701/8000 train_time:40455ms step_avg:57.71ms
step:702/8000 train_time:40515ms step_avg:57.71ms
step:703/8000 train_time:40571ms step_avg:57.71ms
step:704/8000 train_time:40630ms step_avg:57.71ms
step:705/8000 train_time:40687ms step_avg:57.71ms
step:706/8000 train_time:40746ms step_avg:57.71ms
step:707/8000 train_time:40802ms step_avg:57.71ms
step:708/8000 train_time:40862ms step_avg:57.72ms
step:709/8000 train_time:40919ms step_avg:57.71ms
step:710/8000 train_time:40978ms step_avg:57.72ms
step:711/8000 train_time:41034ms step_avg:57.71ms
step:712/8000 train_time:41094ms step_avg:57.72ms
step:713/8000 train_time:41150ms step_avg:57.71ms
step:714/8000 train_time:41209ms step_avg:57.72ms
step:715/8000 train_time:41265ms step_avg:57.71ms
step:716/8000 train_time:41325ms step_avg:57.72ms
step:717/8000 train_time:41381ms step_avg:57.71ms
step:718/8000 train_time:41440ms step_avg:57.72ms
step:719/8000 train_time:41496ms step_avg:57.71ms
step:720/8000 train_time:41556ms step_avg:57.72ms
step:721/8000 train_time:41612ms step_avg:57.71ms
step:722/8000 train_time:41672ms step_avg:57.72ms
step:723/8000 train_time:41728ms step_avg:57.72ms
step:724/8000 train_time:41788ms step_avg:57.72ms
step:725/8000 train_time:41843ms step_avg:57.72ms
step:726/8000 train_time:41904ms step_avg:57.72ms
step:727/8000 train_time:41960ms step_avg:57.72ms
step:728/8000 train_time:42019ms step_avg:57.72ms
step:729/8000 train_time:42075ms step_avg:57.72ms
step:730/8000 train_time:42135ms step_avg:57.72ms
step:731/8000 train_time:42192ms step_avg:57.72ms
step:732/8000 train_time:42251ms step_avg:57.72ms
step:733/8000 train_time:42307ms step_avg:57.72ms
step:734/8000 train_time:42366ms step_avg:57.72ms
step:735/8000 train_time:42421ms step_avg:57.72ms
step:736/8000 train_time:42482ms step_avg:57.72ms
step:737/8000 train_time:42538ms step_avg:57.72ms
step:738/8000 train_time:42597ms step_avg:57.72ms
step:739/8000 train_time:42654ms step_avg:57.72ms
step:740/8000 train_time:42713ms step_avg:57.72ms
step:741/8000 train_time:42770ms step_avg:57.72ms
step:742/8000 train_time:42829ms step_avg:57.72ms
step:743/8000 train_time:42885ms step_avg:57.72ms
step:744/8000 train_time:42944ms step_avg:57.72ms
step:745/8000 train_time:43000ms step_avg:57.72ms
step:746/8000 train_time:43059ms step_avg:57.72ms
step:747/8000 train_time:43115ms step_avg:57.72ms
step:748/8000 train_time:43175ms step_avg:57.72ms
step:749/8000 train_time:43231ms step_avg:57.72ms
step:750/8000 train_time:43290ms step_avg:57.72ms
step:750/8000 val_loss:4.2218 train_time:43370ms step_avg:57.83ms
step:751/8000 train_time:43388ms step_avg:57.77ms
step:752/8000 train_time:43408ms step_avg:57.72ms
step:753/8000 train_time:43466ms step_avg:57.72ms
step:754/8000 train_time:43532ms step_avg:57.73ms
step:755/8000 train_time:43588ms step_avg:57.73ms
step:756/8000 train_time:43647ms step_avg:57.73ms
step:757/8000 train_time:43703ms step_avg:57.73ms
step:758/8000 train_time:43762ms step_avg:57.73ms
step:759/8000 train_time:43818ms step_avg:57.73ms
step:760/8000 train_time:43879ms step_avg:57.74ms
step:761/8000 train_time:43934ms step_avg:57.73ms
step:762/8000 train_time:43993ms step_avg:57.73ms
step:763/8000 train_time:44048ms step_avg:57.73ms
step:764/8000 train_time:44107ms step_avg:57.73ms
step:765/8000 train_time:44162ms step_avg:57.73ms
step:766/8000 train_time:44220ms step_avg:57.73ms
step:767/8000 train_time:44276ms step_avg:57.73ms
step:768/8000 train_time:44335ms step_avg:57.73ms
step:769/8000 train_time:44392ms step_avg:57.73ms
step:770/8000 train_time:44453ms step_avg:57.73ms
step:771/8000 train_time:44510ms step_avg:57.73ms
step:772/8000 train_time:44570ms step_avg:57.73ms
step:773/8000 train_time:44625ms step_avg:57.73ms
step:774/8000 train_time:44687ms step_avg:57.74ms
step:775/8000 train_time:44743ms step_avg:57.73ms
step:776/8000 train_time:44803ms step_avg:57.74ms
step:777/8000 train_time:44858ms step_avg:57.73ms
step:778/8000 train_time:44918ms step_avg:57.73ms
step:779/8000 train_time:44973ms step_avg:57.73ms
step:780/8000 train_time:45032ms step_avg:57.73ms
step:781/8000 train_time:45087ms step_avg:57.73ms
step:782/8000 train_time:45145ms step_avg:57.73ms
step:783/8000 train_time:45201ms step_avg:57.73ms
step:784/8000 train_time:45259ms step_avg:57.73ms
step:785/8000 train_time:45315ms step_avg:57.73ms
step:786/8000 train_time:45375ms step_avg:57.73ms
step:787/8000 train_time:45432ms step_avg:57.73ms
step:788/8000 train_time:45492ms step_avg:57.73ms
step:789/8000 train_time:45548ms step_avg:57.73ms
step:790/8000 train_time:45610ms step_avg:57.73ms
step:791/8000 train_time:45666ms step_avg:57.73ms
step:792/8000 train_time:45728ms step_avg:57.74ms
step:793/8000 train_time:45783ms step_avg:57.73ms
step:794/8000 train_time:45843ms step_avg:57.74ms
step:795/8000 train_time:45899ms step_avg:57.73ms
step:796/8000 train_time:45957ms step_avg:57.74ms
step:797/8000 train_time:46013ms step_avg:57.73ms
step:798/8000 train_time:46072ms step_avg:57.73ms
step:799/8000 train_time:46127ms step_avg:57.73ms
step:800/8000 train_time:46187ms step_avg:57.73ms
step:801/8000 train_time:46243ms step_avg:57.73ms
step:802/8000 train_time:46302ms step_avg:57.73ms
step:803/8000 train_time:46359ms step_avg:57.73ms
step:804/8000 train_time:46418ms step_avg:57.73ms
step:805/8000 train_time:46475ms step_avg:57.73ms
step:806/8000 train_time:46535ms step_avg:57.74ms
step:807/8000 train_time:46591ms step_avg:57.73ms
step:808/8000 train_time:46651ms step_avg:57.74ms
step:809/8000 train_time:46706ms step_avg:57.73ms
step:810/8000 train_time:46768ms step_avg:57.74ms
step:811/8000 train_time:46824ms step_avg:57.74ms
step:812/8000 train_time:46885ms step_avg:57.74ms
step:813/8000 train_time:46941ms step_avg:57.74ms
step:814/8000 train_time:47000ms step_avg:57.74ms
step:815/8000 train_time:47056ms step_avg:57.74ms
step:816/8000 train_time:47115ms step_avg:57.74ms
step:817/8000 train_time:47170ms step_avg:57.74ms
step:818/8000 train_time:47230ms step_avg:57.74ms
step:819/8000 train_time:47285ms step_avg:57.74ms
step:820/8000 train_time:47346ms step_avg:57.74ms
step:821/8000 train_time:47402ms step_avg:57.74ms
step:822/8000 train_time:47461ms step_avg:57.74ms
step:823/8000 train_time:47517ms step_avg:57.74ms
step:824/8000 train_time:47577ms step_avg:57.74ms
step:825/8000 train_time:47633ms step_avg:57.74ms
step:826/8000 train_time:47693ms step_avg:57.74ms
step:827/8000 train_time:47749ms step_avg:57.74ms
step:828/8000 train_time:47811ms step_avg:57.74ms
step:829/8000 train_time:47866ms step_avg:57.74ms
step:830/8000 train_time:47927ms step_avg:57.74ms
step:831/8000 train_time:47982ms step_avg:57.74ms
step:832/8000 train_time:48042ms step_avg:57.74ms
step:833/8000 train_time:48097ms step_avg:57.74ms
step:834/8000 train_time:48157ms step_avg:57.74ms
step:835/8000 train_time:48212ms step_avg:57.74ms
step:836/8000 train_time:48272ms step_avg:57.74ms
step:837/8000 train_time:48328ms step_avg:57.74ms
step:838/8000 train_time:48389ms step_avg:57.74ms
step:839/8000 train_time:48444ms step_avg:57.74ms
step:840/8000 train_time:48504ms step_avg:57.74ms
step:841/8000 train_time:48560ms step_avg:57.74ms
step:842/8000 train_time:48620ms step_avg:57.74ms
step:843/8000 train_time:48676ms step_avg:57.74ms
step:844/8000 train_time:48738ms step_avg:57.75ms
step:845/8000 train_time:48793ms step_avg:57.74ms
step:846/8000 train_time:48853ms step_avg:57.75ms
step:847/8000 train_time:48909ms step_avg:57.74ms
step:848/8000 train_time:48970ms step_avg:57.75ms
step:849/8000 train_time:49026ms step_avg:57.75ms
step:850/8000 train_time:49085ms step_avg:57.75ms
step:851/8000 train_time:49140ms step_avg:57.74ms
step:852/8000 train_time:49199ms step_avg:57.75ms
step:853/8000 train_time:49255ms step_avg:57.74ms
step:854/8000 train_time:49315ms step_avg:57.75ms
step:855/8000 train_time:49371ms step_avg:57.74ms
step:856/8000 train_time:49430ms step_avg:57.75ms
step:857/8000 train_time:49486ms step_avg:57.74ms
step:858/8000 train_time:49546ms step_avg:57.75ms
step:859/8000 train_time:49602ms step_avg:57.74ms
step:860/8000 train_time:49661ms step_avg:57.75ms
step:861/8000 train_time:49718ms step_avg:57.74ms
step:862/8000 train_time:49778ms step_avg:57.75ms
step:863/8000 train_time:49834ms step_avg:57.75ms
step:864/8000 train_time:49894ms step_avg:57.75ms
step:865/8000 train_time:49950ms step_avg:57.75ms
step:866/8000 train_time:50010ms step_avg:57.75ms
step:867/8000 train_time:50065ms step_avg:57.75ms
step:868/8000 train_time:50125ms step_avg:57.75ms
step:869/8000 train_time:50180ms step_avg:57.75ms
step:870/8000 train_time:50240ms step_avg:57.75ms
step:871/8000 train_time:50296ms step_avg:57.75ms
step:872/8000 train_time:50357ms step_avg:57.75ms
step:873/8000 train_time:50413ms step_avg:57.75ms
step:874/8000 train_time:50473ms step_avg:57.75ms
step:875/8000 train_time:50529ms step_avg:57.75ms
step:876/8000 train_time:50588ms step_avg:57.75ms
step:877/8000 train_time:50644ms step_avg:57.75ms
step:878/8000 train_time:50705ms step_avg:57.75ms
step:879/8000 train_time:50761ms step_avg:57.75ms
step:880/8000 train_time:50821ms step_avg:57.75ms
step:881/8000 train_time:50878ms step_avg:57.75ms
step:882/8000 train_time:50938ms step_avg:57.75ms
step:883/8000 train_time:50994ms step_avg:57.75ms
step:884/8000 train_time:51053ms step_avg:57.75ms
step:885/8000 train_time:51109ms step_avg:57.75ms
step:886/8000 train_time:51168ms step_avg:57.75ms
step:887/8000 train_time:51224ms step_avg:57.75ms
step:888/8000 train_time:51284ms step_avg:57.75ms
step:889/8000 train_time:51340ms step_avg:57.75ms
step:890/8000 train_time:51399ms step_avg:57.75ms
step:891/8000 train_time:51454ms step_avg:57.75ms
step:892/8000 train_time:51514ms step_avg:57.75ms
step:893/8000 train_time:51570ms step_avg:57.75ms
step:894/8000 train_time:51630ms step_avg:57.75ms
step:895/8000 train_time:51686ms step_avg:57.75ms
step:896/8000 train_time:51746ms step_avg:57.75ms
step:897/8000 train_time:51803ms step_avg:57.75ms
step:898/8000 train_time:51861ms step_avg:57.75ms
step:899/8000 train_time:51917ms step_avg:57.75ms
step:900/8000 train_time:51977ms step_avg:57.75ms
step:901/8000 train_time:52034ms step_avg:57.75ms
step:902/8000 train_time:52093ms step_avg:57.75ms
step:903/8000 train_time:52148ms step_avg:57.75ms
step:904/8000 train_time:52208ms step_avg:57.75ms
step:905/8000 train_time:52263ms step_avg:57.75ms
step:906/8000 train_time:52323ms step_avg:57.75ms
step:907/8000 train_time:52379ms step_avg:57.75ms
step:908/8000 train_time:52439ms step_avg:57.75ms
step:909/8000 train_time:52495ms step_avg:57.75ms
step:910/8000 train_time:52554ms step_avg:57.75ms
step:911/8000 train_time:52610ms step_avg:57.75ms
step:912/8000 train_time:52671ms step_avg:57.75ms
step:913/8000 train_time:52727ms step_avg:57.75ms
step:914/8000 train_time:52788ms step_avg:57.75ms
step:915/8000 train_time:52843ms step_avg:57.75ms
step:916/8000 train_time:52904ms step_avg:57.76ms
step:917/8000 train_time:52960ms step_avg:57.75ms
step:918/8000 train_time:53019ms step_avg:57.75ms
step:919/8000 train_time:53075ms step_avg:57.75ms
step:920/8000 train_time:53134ms step_avg:57.75ms
step:921/8000 train_time:53190ms step_avg:57.75ms
step:922/8000 train_time:53249ms step_avg:57.75ms
step:923/8000 train_time:53305ms step_avg:57.75ms
step:924/8000 train_time:53365ms step_avg:57.75ms
step:925/8000 train_time:53422ms step_avg:57.75ms
step:926/8000 train_time:53481ms step_avg:57.75ms
step:927/8000 train_time:53537ms step_avg:57.75ms
step:928/8000 train_time:53597ms step_avg:57.76ms
step:929/8000 train_time:53652ms step_avg:57.75ms
step:930/8000 train_time:53712ms step_avg:57.76ms
step:931/8000 train_time:53768ms step_avg:57.75ms
step:932/8000 train_time:53829ms step_avg:57.76ms
step:933/8000 train_time:53884ms step_avg:57.75ms
step:934/8000 train_time:53946ms step_avg:57.76ms
step:935/8000 train_time:54001ms step_avg:57.76ms
step:936/8000 train_time:54061ms step_avg:57.76ms
step:937/8000 train_time:54118ms step_avg:57.76ms
step:938/8000 train_time:54177ms step_avg:57.76ms
step:939/8000 train_time:54233ms step_avg:57.76ms
step:940/8000 train_time:54293ms step_avg:57.76ms
step:941/8000 train_time:54348ms step_avg:57.76ms
step:942/8000 train_time:54408ms step_avg:57.76ms
step:943/8000 train_time:54463ms step_avg:57.75ms
step:944/8000 train_time:54523ms step_avg:57.76ms
step:945/8000 train_time:54579ms step_avg:57.76ms
step:946/8000 train_time:54639ms step_avg:57.76ms
step:947/8000 train_time:54695ms step_avg:57.76ms
step:948/8000 train_time:54755ms step_avg:57.76ms
step:949/8000 train_time:54811ms step_avg:57.76ms
step:950/8000 train_time:54870ms step_avg:57.76ms
step:951/8000 train_time:54926ms step_avg:57.76ms
step:952/8000 train_time:54986ms step_avg:57.76ms
step:953/8000 train_time:55042ms step_avg:57.76ms
step:954/8000 train_time:55102ms step_avg:57.76ms
step:955/8000 train_time:55158ms step_avg:57.76ms
step:956/8000 train_time:55218ms step_avg:57.76ms
step:957/8000 train_time:55273ms step_avg:57.76ms
step:958/8000 train_time:55334ms step_avg:57.76ms
step:959/8000 train_time:55389ms step_avg:57.76ms
step:960/8000 train_time:55448ms step_avg:57.76ms
step:961/8000 train_time:55504ms step_avg:57.76ms
step:962/8000 train_time:55564ms step_avg:57.76ms
step:963/8000 train_time:55621ms step_avg:57.76ms
step:964/8000 train_time:55680ms step_avg:57.76ms
step:965/8000 train_time:55737ms step_avg:57.76ms
step:966/8000 train_time:55796ms step_avg:57.76ms
step:967/8000 train_time:55851ms step_avg:57.76ms
step:968/8000 train_time:55911ms step_avg:57.76ms
step:969/8000 train_time:55967ms step_avg:57.76ms
step:970/8000 train_time:56028ms step_avg:57.76ms
step:971/8000 train_time:56083ms step_avg:57.76ms
step:972/8000 train_time:56144ms step_avg:57.76ms
step:973/8000 train_time:56200ms step_avg:57.76ms
step:974/8000 train_time:56259ms step_avg:57.76ms
step:975/8000 train_time:56315ms step_avg:57.76ms
step:976/8000 train_time:56375ms step_avg:57.76ms
step:977/8000 train_time:56431ms step_avg:57.76ms
step:978/8000 train_time:56490ms step_avg:57.76ms
step:979/8000 train_time:56545ms step_avg:57.76ms
step:980/8000 train_time:56607ms step_avg:57.76ms
step:981/8000 train_time:56663ms step_avg:57.76ms
step:982/8000 train_time:56723ms step_avg:57.76ms
step:983/8000 train_time:56779ms step_avg:57.76ms
step:984/8000 train_time:56839ms step_avg:57.76ms
step:985/8000 train_time:56895ms step_avg:57.76ms
step:986/8000 train_time:56955ms step_avg:57.76ms
step:987/8000 train_time:57011ms step_avg:57.76ms
step:988/8000 train_time:57072ms step_avg:57.77ms
step:989/8000 train_time:57127ms step_avg:57.76ms
step:990/8000 train_time:57188ms step_avg:57.77ms
step:991/8000 train_time:57244ms step_avg:57.76ms
step:992/8000 train_time:57304ms step_avg:57.77ms
step:993/8000 train_time:57359ms step_avg:57.76ms
step:994/8000 train_time:57419ms step_avg:57.77ms
step:995/8000 train_time:57475ms step_avg:57.76ms
step:996/8000 train_time:57534ms step_avg:57.77ms
step:997/8000 train_time:57590ms step_avg:57.76ms
step:998/8000 train_time:57649ms step_avg:57.76ms
step:999/8000 train_time:57705ms step_avg:57.76ms
step:1000/8000 train_time:57765ms step_avg:57.77ms
step:1000/8000 val_loss:4.1198 train_time:57845ms step_avg:57.85ms
step:1001/8000 train_time:57863ms step_avg:57.80ms
step:1002/8000 train_time:57884ms step_avg:57.77ms
step:1003/8000 train_time:57940ms step_avg:57.77ms
step:1004/8000 train_time:58006ms step_avg:57.77ms
step:1005/8000 train_time:58063ms step_avg:57.77ms
step:1006/8000 train_time:58126ms step_avg:57.78ms
step:1007/8000 train_time:58182ms step_avg:57.78ms
step:1008/8000 train_time:58241ms step_avg:57.78ms
step:1009/8000 train_time:58297ms step_avg:57.78ms
step:1010/8000 train_time:58357ms step_avg:57.78ms
step:1011/8000 train_time:58412ms step_avg:57.78ms
step:1012/8000 train_time:58471ms step_avg:57.78ms
step:1013/8000 train_time:58526ms step_avg:57.78ms
step:1014/8000 train_time:58585ms step_avg:57.78ms
step:1015/8000 train_time:58641ms step_avg:57.77ms
step:1016/8000 train_time:58700ms step_avg:57.78ms
step:1017/8000 train_time:58756ms step_avg:57.77ms
step:1018/8000 train_time:58815ms step_avg:57.77ms
step:1019/8000 train_time:58872ms step_avg:57.77ms
step:1020/8000 train_time:58933ms step_avg:57.78ms
step:1021/8000 train_time:58989ms step_avg:57.78ms
step:1022/8000 train_time:59051ms step_avg:57.78ms
step:1023/8000 train_time:59106ms step_avg:57.78ms
step:1024/8000 train_time:59169ms step_avg:57.78ms
step:1025/8000 train_time:59224ms step_avg:57.78ms
step:1026/8000 train_time:59284ms step_avg:57.78ms
step:1027/8000 train_time:59340ms step_avg:57.78ms
step:1028/8000 train_time:59399ms step_avg:57.78ms
step:1029/8000 train_time:59455ms step_avg:57.78ms
step:1030/8000 train_time:59514ms step_avg:57.78ms
step:1031/8000 train_time:59570ms step_avg:57.78ms
step:1032/8000 train_time:59629ms step_avg:57.78ms
step:1033/8000 train_time:59684ms step_avg:57.78ms
step:1034/8000 train_time:59744ms step_avg:57.78ms
step:1035/8000 train_time:59801ms step_avg:57.78ms
step:1036/8000 train_time:59861ms step_avg:57.78ms
step:1037/8000 train_time:59918ms step_avg:57.78ms
step:1038/8000 train_time:59978ms step_avg:57.78ms
step:1039/8000 train_time:60035ms step_avg:57.78ms
step:1040/8000 train_time:60095ms step_avg:57.78ms
step:1041/8000 train_time:60150ms step_avg:57.78ms
step:1042/8000 train_time:60211ms step_avg:57.78ms
step:1043/8000 train_time:60267ms step_avg:57.78ms
step:1044/8000 train_time:60327ms step_avg:57.78ms
step:1045/8000 train_time:60383ms step_avg:57.78ms
step:1046/8000 train_time:60442ms step_avg:57.78ms
step:1047/8000 train_time:60498ms step_avg:57.78ms
step:1048/8000 train_time:60557ms step_avg:57.78ms
step:1049/8000 train_time:60613ms step_avg:57.78ms
step:1050/8000 train_time:60672ms step_avg:57.78ms
step:1051/8000 train_time:60729ms step_avg:57.78ms
step:1052/8000 train_time:60787ms step_avg:57.78ms
step:1053/8000 train_time:60843ms step_avg:57.78ms
step:1054/8000 train_time:60904ms step_avg:57.78ms
step:1055/8000 train_time:60960ms step_avg:57.78ms
step:1056/8000 train_time:61021ms step_avg:57.78ms
step:1057/8000 train_time:61078ms step_avg:57.78ms
step:1058/8000 train_time:61138ms step_avg:57.79ms
step:1059/8000 train_time:61194ms step_avg:57.78ms
step:1060/8000 train_time:61254ms step_avg:57.79ms
step:1061/8000 train_time:61309ms step_avg:57.78ms
step:1062/8000 train_time:61370ms step_avg:57.79ms
step:1063/8000 train_time:61425ms step_avg:57.78ms
step:1064/8000 train_time:61484ms step_avg:57.79ms
step:1065/8000 train_time:61540ms step_avg:57.78ms
step:1066/8000 train_time:61599ms step_avg:57.79ms
step:1067/8000 train_time:61655ms step_avg:57.78ms
step:1068/8000 train_time:61714ms step_avg:57.78ms
step:1069/8000 train_time:61770ms step_avg:57.78ms
step:1070/8000 train_time:61829ms step_avg:57.78ms
step:1071/8000 train_time:61885ms step_avg:57.78ms
step:1072/8000 train_time:61945ms step_avg:57.78ms
step:1073/8000 train_time:62001ms step_avg:57.78ms
step:1074/8000 train_time:62061ms step_avg:57.79ms
step:1075/8000 train_time:62119ms step_avg:57.78ms
step:1076/8000 train_time:62178ms step_avg:57.79ms
step:1077/8000 train_time:62234ms step_avg:57.78ms
step:1078/8000 train_time:62295ms step_avg:57.79ms
step:1079/8000 train_time:62350ms step_avg:57.79ms
step:1080/8000 train_time:62410ms step_avg:57.79ms
step:1081/8000 train_time:62465ms step_avg:57.78ms
step:1082/8000 train_time:62526ms step_avg:57.79ms
step:1083/8000 train_time:62581ms step_avg:57.79ms
step:1084/8000 train_time:62641ms step_avg:57.79ms
step:1085/8000 train_time:62697ms step_avg:57.78ms
step:1086/8000 train_time:62756ms step_avg:57.79ms
step:1087/8000 train_time:62812ms step_avg:57.78ms
step:1088/8000 train_time:62872ms step_avg:57.79ms
step:1089/8000 train_time:62928ms step_avg:57.78ms
step:1090/8000 train_time:62989ms step_avg:57.79ms
step:1091/8000 train_time:63044ms step_avg:57.79ms
step:1092/8000 train_time:63105ms step_avg:57.79ms
step:1093/8000 train_time:63162ms step_avg:57.79ms
step:1094/8000 train_time:63221ms step_avg:57.79ms
step:1095/8000 train_time:63278ms step_avg:57.79ms
step:1096/8000 train_time:63337ms step_avg:57.79ms
step:1097/8000 train_time:63394ms step_avg:57.79ms
step:1098/8000 train_time:63453ms step_avg:57.79ms
step:1099/8000 train_time:63509ms step_avg:57.79ms
step:1100/8000 train_time:63568ms step_avg:57.79ms
step:1101/8000 train_time:63624ms step_avg:57.79ms
step:1102/8000 train_time:63683ms step_avg:57.79ms
step:1103/8000 train_time:63740ms step_avg:57.79ms
step:1104/8000 train_time:63799ms step_avg:57.79ms
step:1105/8000 train_time:63856ms step_avg:57.79ms
step:1106/8000 train_time:63916ms step_avg:57.79ms
step:1107/8000 train_time:63972ms step_avg:57.79ms
step:1108/8000 train_time:64031ms step_avg:57.79ms
step:1109/8000 train_time:64087ms step_avg:57.79ms
step:1110/8000 train_time:64148ms step_avg:57.79ms
step:1111/8000 train_time:64204ms step_avg:57.79ms
step:1112/8000 train_time:64265ms step_avg:57.79ms
step:1113/8000 train_time:64321ms step_avg:57.79ms
step:1114/8000 train_time:64380ms step_avg:57.79ms
step:1115/8000 train_time:64437ms step_avg:57.79ms
step:1116/8000 train_time:64496ms step_avg:57.79ms
step:1117/8000 train_time:64552ms step_avg:57.79ms
step:1118/8000 train_time:64611ms step_avg:57.79ms
step:1119/8000 train_time:64667ms step_avg:57.79ms
step:1120/8000 train_time:64727ms step_avg:57.79ms
step:1121/8000 train_time:64783ms step_avg:57.79ms
step:1122/8000 train_time:64842ms step_avg:57.79ms
step:1123/8000 train_time:64899ms step_avg:57.79ms
step:1124/8000 train_time:64959ms step_avg:57.79ms
step:1125/8000 train_time:65015ms step_avg:57.79ms
step:1126/8000 train_time:65075ms step_avg:57.79ms
step:1127/8000 train_time:65131ms step_avg:57.79ms
step:1128/8000 train_time:65191ms step_avg:57.79ms
step:1129/8000 train_time:65246ms step_avg:57.79ms
step:1130/8000 train_time:65307ms step_avg:57.79ms
step:1131/8000 train_time:65363ms step_avg:57.79ms
step:1132/8000 train_time:65423ms step_avg:57.79ms
step:1133/8000 train_time:65480ms step_avg:57.79ms
step:1134/8000 train_time:65539ms step_avg:57.79ms
step:1135/8000 train_time:65596ms step_avg:57.79ms
step:1136/8000 train_time:65654ms step_avg:57.79ms
step:1137/8000 train_time:65711ms step_avg:57.79ms
step:1138/8000 train_time:65770ms step_avg:57.79ms
step:1139/8000 train_time:65825ms step_avg:57.79ms
step:1140/8000 train_time:65886ms step_avg:57.79ms
step:1141/8000 train_time:65942ms step_avg:57.79ms
step:1142/8000 train_time:66002ms step_avg:57.79ms
step:1143/8000 train_time:66058ms step_avg:57.79ms
step:1144/8000 train_time:66118ms step_avg:57.80ms
step:1145/8000 train_time:66174ms step_avg:57.79ms
step:1146/8000 train_time:66234ms step_avg:57.80ms
step:1147/8000 train_time:66289ms step_avg:57.79ms
step:1148/8000 train_time:66350ms step_avg:57.80ms
step:1149/8000 train_time:66405ms step_avg:57.79ms
step:1150/8000 train_time:66466ms step_avg:57.80ms
step:1151/8000 train_time:66523ms step_avg:57.80ms
step:1152/8000 train_time:66582ms step_avg:57.80ms
step:1153/8000 train_time:66638ms step_avg:57.80ms
step:1154/8000 train_time:66697ms step_avg:57.80ms
step:1155/8000 train_time:66753ms step_avg:57.79ms
step:1156/8000 train_time:66813ms step_avg:57.80ms
step:1157/8000 train_time:66869ms step_avg:57.79ms
step:1158/8000 train_time:66929ms step_avg:57.80ms
step:1159/8000 train_time:66985ms step_avg:57.80ms
step:1160/8000 train_time:67044ms step_avg:57.80ms
step:1161/8000 train_time:67100ms step_avg:57.80ms
step:1162/8000 train_time:67160ms step_avg:57.80ms
step:1163/8000 train_time:67217ms step_avg:57.80ms
step:1164/8000 train_time:67276ms step_avg:57.80ms
step:1165/8000 train_time:67333ms step_avg:57.80ms
step:1166/8000 train_time:67392ms step_avg:57.80ms
step:1167/8000 train_time:67448ms step_avg:57.80ms
step:1168/8000 train_time:67508ms step_avg:57.80ms
step:1169/8000 train_time:67564ms step_avg:57.80ms
step:1170/8000 train_time:67623ms step_avg:57.80ms
step:1171/8000 train_time:67680ms step_avg:57.80ms
step:1172/8000 train_time:67739ms step_avg:57.80ms
step:1173/8000 train_time:67795ms step_avg:57.80ms
step:1174/8000 train_time:67855ms step_avg:57.80ms
step:1175/8000 train_time:67911ms step_avg:57.80ms
step:1176/8000 train_time:67970ms step_avg:57.80ms
step:1177/8000 train_time:68026ms step_avg:57.80ms
step:1178/8000 train_time:68087ms step_avg:57.80ms
step:1179/8000 train_time:68143ms step_avg:57.80ms
step:1180/8000 train_time:68202ms step_avg:57.80ms
step:1181/8000 train_time:68259ms step_avg:57.80ms
step:1182/8000 train_time:68319ms step_avg:57.80ms
step:1183/8000 train_time:68375ms step_avg:57.80ms
step:1184/8000 train_time:68435ms step_avg:57.80ms
step:1185/8000 train_time:68492ms step_avg:57.80ms
step:1186/8000 train_time:68551ms step_avg:57.80ms
step:1187/8000 train_time:68607ms step_avg:57.80ms
step:1188/8000 train_time:68667ms step_avg:57.80ms
step:1189/8000 train_time:68723ms step_avg:57.80ms
step:1190/8000 train_time:68782ms step_avg:57.80ms
step:1191/8000 train_time:68838ms step_avg:57.80ms
step:1192/8000 train_time:68898ms step_avg:57.80ms
step:1193/8000 train_time:68955ms step_avg:57.80ms
step:1194/8000 train_time:69014ms step_avg:57.80ms
step:1195/8000 train_time:69070ms step_avg:57.80ms
step:1196/8000 train_time:69130ms step_avg:57.80ms
step:1197/8000 train_time:69185ms step_avg:57.80ms
step:1198/8000 train_time:69247ms step_avg:57.80ms
step:1199/8000 train_time:69303ms step_avg:57.80ms
step:1200/8000 train_time:69363ms step_avg:57.80ms
step:1201/8000 train_time:69420ms step_avg:57.80ms
step:1202/8000 train_time:69479ms step_avg:57.80ms
step:1203/8000 train_time:69536ms step_avg:57.80ms
step:1204/8000 train_time:69595ms step_avg:57.80ms
step:1205/8000 train_time:69651ms step_avg:57.80ms
step:1206/8000 train_time:69710ms step_avg:57.80ms
step:1207/8000 train_time:69766ms step_avg:57.80ms
step:1208/8000 train_time:69826ms step_avg:57.80ms
step:1209/8000 train_time:69882ms step_avg:57.80ms
step:1210/8000 train_time:69942ms step_avg:57.80ms
step:1211/8000 train_time:69998ms step_avg:57.80ms
step:1212/8000 train_time:70058ms step_avg:57.80ms
step:1213/8000 train_time:70114ms step_avg:57.80ms
step:1214/8000 train_time:70173ms step_avg:57.80ms
step:1215/8000 train_time:70229ms step_avg:57.80ms
step:1216/8000 train_time:70288ms step_avg:57.80ms
step:1217/8000 train_time:70344ms step_avg:57.80ms
step:1218/8000 train_time:70405ms step_avg:57.80ms
step:1219/8000 train_time:70462ms step_avg:57.80ms
step:1220/8000 train_time:70521ms step_avg:57.80ms
step:1221/8000 train_time:70577ms step_avg:57.80ms
step:1222/8000 train_time:70636ms step_avg:57.80ms
step:1223/8000 train_time:70693ms step_avg:57.80ms
step:1224/8000 train_time:70753ms step_avg:57.80ms
step:1225/8000 train_time:70808ms step_avg:57.80ms
step:1226/8000 train_time:70868ms step_avg:57.80ms
step:1227/8000 train_time:70924ms step_avg:57.80ms
step:1228/8000 train_time:70984ms step_avg:57.80ms
step:1229/8000 train_time:71040ms step_avg:57.80ms
step:1230/8000 train_time:71100ms step_avg:57.81ms
step:1231/8000 train_time:71157ms step_avg:57.80ms
step:1232/8000 train_time:71216ms step_avg:57.81ms
step:1233/8000 train_time:71272ms step_avg:57.80ms
step:1234/8000 train_time:71332ms step_avg:57.81ms
step:1235/8000 train_time:71388ms step_avg:57.80ms
step:1236/8000 train_time:71448ms step_avg:57.81ms
step:1237/8000 train_time:71504ms step_avg:57.80ms
step:1238/8000 train_time:71564ms step_avg:57.81ms
step:1239/8000 train_time:71620ms step_avg:57.80ms
step:1240/8000 train_time:71679ms step_avg:57.81ms
step:1241/8000 train_time:71735ms step_avg:57.80ms
step:1242/8000 train_time:71795ms step_avg:57.81ms
step:1243/8000 train_time:71851ms step_avg:57.80ms
step:1244/8000 train_time:71910ms step_avg:57.81ms
step:1245/8000 train_time:71966ms step_avg:57.80ms
step:1246/8000 train_time:72025ms step_avg:57.80ms
