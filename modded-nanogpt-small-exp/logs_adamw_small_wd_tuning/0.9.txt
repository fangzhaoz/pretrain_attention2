import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--wd', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_wd_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_wd_tuning/{args2.wd}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=6e-4,  betas=(0.8, 0.8), eps=1e-10, weight_decay=float(args2.wd), fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_wd_tuning/{args2.wd}.txt", exist_ok=True)
            torch.save(log, f"logs_adamw_small_wd_tuning/{args2.wd}.txt/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 08:38:14 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   29C    P0             114W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   26C    P0             116W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   25C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   26C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   27C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:101ms step_avg:101.19ms
step:2/2330 train_time:193ms step_avg:96.28ms
step:3/2330 train_time:211ms step_avg:70.38ms
step:4/2330 train_time:230ms step_avg:57.44ms
step:5/2330 train_time:284ms step_avg:56.74ms
step:6/2330 train_time:342ms step_avg:56.94ms
step:7/2330 train_time:397ms step_avg:56.65ms
step:8/2330 train_time:456ms step_avg:56.97ms
step:9/2330 train_time:511ms step_avg:56.75ms
step:10/2330 train_time:570ms step_avg:56.97ms
step:11/2330 train_time:625ms step_avg:56.85ms
step:12/2330 train_time:683ms step_avg:56.95ms
step:13/2330 train_time:739ms step_avg:56.84ms
step:14/2330 train_time:797ms step_avg:56.96ms
step:15/2330 train_time:853ms step_avg:56.87ms
step:16/2330 train_time:911ms step_avg:56.93ms
step:17/2330 train_time:966ms step_avg:56.85ms
step:18/2330 train_time:1027ms step_avg:57.04ms
step:19/2330 train_time:1086ms step_avg:57.16ms
step:20/2330 train_time:1148ms step_avg:57.39ms
step:21/2330 train_time:1206ms step_avg:57.42ms
step:22/2330 train_time:1265ms step_avg:57.51ms
step:23/2330 train_time:1321ms step_avg:57.45ms
step:24/2330 train_time:1380ms step_avg:57.52ms
step:25/2330 train_time:1436ms step_avg:57.43ms
step:26/2330 train_time:1495ms step_avg:57.52ms
step:27/2330 train_time:1551ms step_avg:57.44ms
step:28/2330 train_time:1610ms step_avg:57.49ms
step:29/2330 train_time:1665ms step_avg:57.42ms
step:30/2330 train_time:1724ms step_avg:57.45ms
step:31/2330 train_time:1778ms step_avg:57.37ms
step:32/2330 train_time:1838ms step_avg:57.42ms
step:33/2330 train_time:1893ms step_avg:57.36ms
step:34/2330 train_time:1951ms step_avg:57.39ms
step:35/2330 train_time:2007ms step_avg:57.35ms
step:36/2330 train_time:2067ms step_avg:57.41ms
step:37/2330 train_time:2124ms step_avg:57.40ms
step:38/2330 train_time:2184ms step_avg:57.47ms
step:39/2330 train_time:2241ms step_avg:57.47ms
step:40/2330 train_time:2300ms step_avg:57.51ms
step:41/2330 train_time:2357ms step_avg:57.48ms
step:42/2330 train_time:2416ms step_avg:57.51ms
step:43/2330 train_time:2471ms step_avg:57.47ms
step:44/2330 train_time:2531ms step_avg:57.51ms
step:45/2330 train_time:2586ms step_avg:57.46ms
step:46/2330 train_time:2646ms step_avg:57.51ms
step:47/2330 train_time:2702ms step_avg:57.49ms
step:48/2330 train_time:2760ms step_avg:57.51ms
step:49/2330 train_time:2816ms step_avg:57.46ms
step:50/2330 train_time:2874ms step_avg:57.49ms
step:51/2330 train_time:2930ms step_avg:57.45ms
step:52/2330 train_time:2989ms step_avg:57.47ms
step:53/2330 train_time:3046ms step_avg:57.48ms
step:54/2330 train_time:3106ms step_avg:57.52ms
step:55/2330 train_time:3162ms step_avg:57.49ms
step:56/2330 train_time:3222ms step_avg:57.54ms
step:57/2330 train_time:3278ms step_avg:57.52ms
step:58/2330 train_time:3339ms step_avg:57.57ms
step:59/2330 train_time:3394ms step_avg:57.53ms
step:60/2330 train_time:3454ms step_avg:57.57ms
step:61/2330 train_time:3510ms step_avg:57.54ms
step:62/2330 train_time:3570ms step_avg:57.57ms
step:63/2330 train_time:3625ms step_avg:57.54ms
step:64/2330 train_time:3684ms step_avg:57.56ms
step:65/2330 train_time:3739ms step_avg:57.52ms
step:66/2330 train_time:3798ms step_avg:57.54ms
step:67/2330 train_time:3853ms step_avg:57.51ms
step:68/2330 train_time:3912ms step_avg:57.53ms
step:69/2330 train_time:3967ms step_avg:57.49ms
step:70/2330 train_time:4028ms step_avg:57.54ms
step:71/2330 train_time:4084ms step_avg:57.52ms
step:72/2330 train_time:4143ms step_avg:57.54ms
step:73/2330 train_time:4199ms step_avg:57.53ms
step:74/2330 train_time:4259ms step_avg:57.55ms
step:75/2330 train_time:4315ms step_avg:57.54ms
step:76/2330 train_time:4375ms step_avg:57.56ms
step:77/2330 train_time:4430ms step_avg:57.54ms
step:78/2330 train_time:4490ms step_avg:57.56ms
step:79/2330 train_time:4546ms step_avg:57.55ms
step:80/2330 train_time:4605ms step_avg:57.56ms
step:81/2330 train_time:4660ms step_avg:57.54ms
step:82/2330 train_time:4719ms step_avg:57.55ms
step:83/2330 train_time:4775ms step_avg:57.53ms
step:84/2330 train_time:4833ms step_avg:57.54ms
step:85/2330 train_time:4889ms step_avg:57.52ms
step:86/2330 train_time:4948ms step_avg:57.54ms
step:87/2330 train_time:5004ms step_avg:57.52ms
step:88/2330 train_time:5063ms step_avg:57.53ms
step:89/2330 train_time:5119ms step_avg:57.51ms
step:90/2330 train_time:5178ms step_avg:57.53ms
step:91/2330 train_time:5234ms step_avg:57.52ms
step:92/2330 train_time:5293ms step_avg:57.53ms
step:93/2330 train_time:5349ms step_avg:57.52ms
step:94/2330 train_time:5409ms step_avg:57.54ms
step:95/2330 train_time:5465ms step_avg:57.53ms
step:96/2330 train_time:5524ms step_avg:57.54ms
step:97/2330 train_time:5580ms step_avg:57.52ms
step:98/2330 train_time:5638ms step_avg:57.53ms
step:99/2330 train_time:5694ms step_avg:57.51ms
step:100/2330 train_time:5753ms step_avg:57.53ms
step:101/2330 train_time:5808ms step_avg:57.51ms
step:102/2330 train_time:5869ms step_avg:57.54ms
step:103/2330 train_time:5925ms step_avg:57.53ms
step:104/2330 train_time:5984ms step_avg:57.53ms
step:105/2330 train_time:6040ms step_avg:57.52ms
step:106/2330 train_time:6098ms step_avg:57.53ms
step:107/2330 train_time:6155ms step_avg:57.52ms
step:108/2330 train_time:6213ms step_avg:57.53ms
step:109/2330 train_time:6269ms step_avg:57.51ms
step:110/2330 train_time:6329ms step_avg:57.54ms
step:111/2330 train_time:6385ms step_avg:57.53ms
step:112/2330 train_time:6444ms step_avg:57.53ms
step:113/2330 train_time:6500ms step_avg:57.52ms
step:114/2330 train_time:6559ms step_avg:57.53ms
step:115/2330 train_time:6614ms step_avg:57.52ms
step:116/2330 train_time:6674ms step_avg:57.53ms
step:117/2330 train_time:6729ms step_avg:57.51ms
step:118/2330 train_time:6789ms step_avg:57.53ms
step:119/2330 train_time:6845ms step_avg:57.52ms
step:120/2330 train_time:6903ms step_avg:57.53ms
step:121/2330 train_time:6959ms step_avg:57.51ms
step:122/2330 train_time:7019ms step_avg:57.53ms
step:123/2330 train_time:7074ms step_avg:57.52ms
step:124/2330 train_time:7134ms step_avg:57.53ms
step:125/2330 train_time:7190ms step_avg:57.52ms
step:126/2330 train_time:7250ms step_avg:57.54ms
step:127/2330 train_time:7306ms step_avg:57.53ms
step:128/2330 train_time:7366ms step_avg:57.55ms
step:129/2330 train_time:7422ms step_avg:57.53ms
step:130/2330 train_time:7481ms step_avg:57.54ms
step:131/2330 train_time:7537ms step_avg:57.54ms
step:132/2330 train_time:7596ms step_avg:57.55ms
step:133/2330 train_time:7652ms step_avg:57.53ms
step:134/2330 train_time:7711ms step_avg:57.54ms
step:135/2330 train_time:7766ms step_avg:57.53ms
step:136/2330 train_time:7827ms step_avg:57.55ms
step:137/2330 train_time:7883ms step_avg:57.54ms
step:138/2330 train_time:7941ms step_avg:57.54ms
step:139/2330 train_time:7997ms step_avg:57.53ms
step:140/2330 train_time:8057ms step_avg:57.55ms
step:141/2330 train_time:8113ms step_avg:57.54ms
step:142/2330 train_time:8172ms step_avg:57.55ms
step:143/2330 train_time:8227ms step_avg:57.53ms
step:144/2330 train_time:8287ms step_avg:57.55ms
step:145/2330 train_time:8342ms step_avg:57.53ms
step:146/2330 train_time:8402ms step_avg:57.55ms
step:147/2330 train_time:8458ms step_avg:57.53ms
step:148/2330 train_time:8516ms step_avg:57.54ms
step:149/2330 train_time:8572ms step_avg:57.53ms
step:150/2330 train_time:8632ms step_avg:57.54ms
step:151/2330 train_time:8688ms step_avg:57.53ms
step:152/2330 train_time:8747ms step_avg:57.54ms
step:153/2330 train_time:8803ms step_avg:57.53ms
step:154/2330 train_time:8861ms step_avg:57.54ms
step:155/2330 train_time:8917ms step_avg:57.53ms
step:156/2330 train_time:8976ms step_avg:57.54ms
step:157/2330 train_time:9032ms step_avg:57.53ms
step:158/2330 train_time:9091ms step_avg:57.54ms
step:159/2330 train_time:9147ms step_avg:57.53ms
step:160/2330 train_time:9206ms step_avg:57.54ms
step:161/2330 train_time:9262ms step_avg:57.53ms
step:162/2330 train_time:9321ms step_avg:57.54ms
step:163/2330 train_time:9377ms step_avg:57.53ms
step:164/2330 train_time:9436ms step_avg:57.54ms
step:165/2330 train_time:9492ms step_avg:57.53ms
step:166/2330 train_time:9552ms step_avg:57.54ms
step:167/2330 train_time:9608ms step_avg:57.53ms
step:168/2330 train_time:9666ms step_avg:57.54ms
step:169/2330 train_time:9723ms step_avg:57.53ms
step:170/2330 train_time:9781ms step_avg:57.54ms
step:171/2330 train_time:9837ms step_avg:57.53ms
step:172/2330 train_time:9896ms step_avg:57.54ms
step:173/2330 train_time:9952ms step_avg:57.53ms
step:174/2330 train_time:10011ms step_avg:57.54ms
step:175/2330 train_time:10067ms step_avg:57.53ms
step:176/2330 train_time:10126ms step_avg:57.54ms
step:177/2330 train_time:10182ms step_avg:57.52ms
step:178/2330 train_time:10241ms step_avg:57.53ms
step:179/2330 train_time:10297ms step_avg:57.53ms
step:180/2330 train_time:10357ms step_avg:57.54ms
step:181/2330 train_time:10413ms step_avg:57.53ms
step:182/2330 train_time:10472ms step_avg:57.54ms
step:183/2330 train_time:10528ms step_avg:57.53ms
step:184/2330 train_time:10587ms step_avg:57.54ms
step:185/2330 train_time:10644ms step_avg:57.53ms
step:186/2330 train_time:10702ms step_avg:57.54ms
step:187/2330 train_time:10759ms step_avg:57.53ms
step:188/2330 train_time:10817ms step_avg:57.54ms
step:189/2330 train_time:10873ms step_avg:57.53ms
step:190/2330 train_time:10931ms step_avg:57.53ms
step:191/2330 train_time:10987ms step_avg:57.52ms
step:192/2330 train_time:11046ms step_avg:57.53ms
step:193/2330 train_time:11103ms step_avg:57.53ms
step:194/2330 train_time:11161ms step_avg:57.53ms
step:195/2330 train_time:11218ms step_avg:57.53ms
step:196/2330 train_time:11277ms step_avg:57.54ms
step:197/2330 train_time:11333ms step_avg:57.53ms
step:198/2330 train_time:11392ms step_avg:57.54ms
step:199/2330 train_time:11448ms step_avg:57.53ms
step:200/2330 train_time:11507ms step_avg:57.54ms
step:201/2330 train_time:11563ms step_avg:57.53ms
step:202/2330 train_time:11622ms step_avg:57.53ms
step:203/2330 train_time:11678ms step_avg:57.52ms
step:204/2330 train_time:11737ms step_avg:57.53ms
step:205/2330 train_time:11793ms step_avg:57.52ms
step:206/2330 train_time:11853ms step_avg:57.54ms
step:207/2330 train_time:11909ms step_avg:57.53ms
step:208/2330 train_time:11969ms step_avg:57.54ms
step:209/2330 train_time:12025ms step_avg:57.54ms
step:210/2330 train_time:12084ms step_avg:57.54ms
step:211/2330 train_time:12141ms step_avg:57.54ms
step:212/2330 train_time:12199ms step_avg:57.54ms
step:213/2330 train_time:12256ms step_avg:57.54ms
step:214/2330 train_time:12314ms step_avg:57.54ms
step:215/2330 train_time:12370ms step_avg:57.53ms
step:216/2330 train_time:12429ms step_avg:57.54ms
step:217/2330 train_time:12485ms step_avg:57.53ms
step:218/2330 train_time:12543ms step_avg:57.54ms
step:219/2330 train_time:12599ms step_avg:57.53ms
step:220/2330 train_time:12658ms step_avg:57.54ms
step:221/2330 train_time:12715ms step_avg:57.53ms
step:222/2330 train_time:12773ms step_avg:57.54ms
step:223/2330 train_time:12829ms step_avg:57.53ms
step:224/2330 train_time:12888ms step_avg:57.54ms
step:225/2330 train_time:12944ms step_avg:57.53ms
step:226/2330 train_time:13004ms step_avg:57.54ms
step:227/2330 train_time:13060ms step_avg:57.53ms
step:228/2330 train_time:13119ms step_avg:57.54ms
step:229/2330 train_time:13175ms step_avg:57.53ms
step:230/2330 train_time:13234ms step_avg:57.54ms
step:231/2330 train_time:13290ms step_avg:57.53ms
step:232/2330 train_time:13349ms step_avg:57.54ms
step:233/2330 train_time:13405ms step_avg:57.53ms
step:234/2330 train_time:13463ms step_avg:57.54ms
step:235/2330 train_time:13519ms step_avg:57.53ms
step:236/2330 train_time:13579ms step_avg:57.54ms
step:237/2330 train_time:13635ms step_avg:57.53ms
step:238/2330 train_time:13694ms step_avg:57.54ms
step:239/2330 train_time:13750ms step_avg:57.53ms
step:240/2330 train_time:13809ms step_avg:57.54ms
step:241/2330 train_time:13864ms step_avg:57.53ms
step:242/2330 train_time:13923ms step_avg:57.53ms
step:243/2330 train_time:13978ms step_avg:57.52ms
step:244/2330 train_time:14038ms step_avg:57.53ms
step:245/2330 train_time:14093ms step_avg:57.52ms
step:246/2330 train_time:14153ms step_avg:57.53ms
step:247/2330 train_time:14209ms step_avg:57.53ms
step:248/2330 train_time:14269ms step_avg:57.54ms
step:249/2330 train_time:14325ms step_avg:57.53ms
step:250/2330 train_time:14383ms step_avg:57.53ms
step:250/2330 val_loss:4.9020 train_time:14462ms step_avg:57.85ms
step:251/2330 train_time:14481ms step_avg:57.69ms
step:252/2330 train_time:14500ms step_avg:57.54ms
step:253/2330 train_time:14555ms step_avg:57.53ms
step:254/2330 train_time:14619ms step_avg:57.55ms
step:255/2330 train_time:14675ms step_avg:57.55ms
step:256/2330 train_time:14737ms step_avg:57.57ms
step:257/2330 train_time:14792ms step_avg:57.56ms
step:258/2330 train_time:14853ms step_avg:57.57ms
step:259/2330 train_time:14908ms step_avg:57.56ms
step:260/2330 train_time:14967ms step_avg:57.57ms
step:261/2330 train_time:15023ms step_avg:57.56ms
step:262/2330 train_time:15081ms step_avg:57.56ms
step:263/2330 train_time:15136ms step_avg:57.55ms
step:264/2330 train_time:15196ms step_avg:57.56ms
step:265/2330 train_time:15251ms step_avg:57.55ms
step:266/2330 train_time:15309ms step_avg:57.55ms
step:267/2330 train_time:15365ms step_avg:57.55ms
step:268/2330 train_time:15424ms step_avg:57.55ms
step:269/2330 train_time:15481ms step_avg:57.55ms
step:270/2330 train_time:15541ms step_avg:57.56ms
step:271/2330 train_time:15598ms step_avg:57.56ms
step:272/2330 train_time:15657ms step_avg:57.56ms
step:273/2330 train_time:15714ms step_avg:57.56ms
step:274/2330 train_time:15773ms step_avg:57.57ms
step:275/2330 train_time:15828ms step_avg:57.56ms
step:276/2330 train_time:15889ms step_avg:57.57ms
step:277/2330 train_time:15944ms step_avg:57.56ms
step:278/2330 train_time:16003ms step_avg:57.57ms
step:279/2330 train_time:16059ms step_avg:57.56ms
step:280/2330 train_time:16118ms step_avg:57.56ms
step:281/2330 train_time:16174ms step_avg:57.56ms
step:282/2330 train_time:16232ms step_avg:57.56ms
step:283/2330 train_time:16287ms step_avg:57.55ms
step:284/2330 train_time:16346ms step_avg:57.56ms
step:285/2330 train_time:16403ms step_avg:57.55ms
step:286/2330 train_time:16461ms step_avg:57.56ms
step:287/2330 train_time:16518ms step_avg:57.55ms
step:288/2330 train_time:16577ms step_avg:57.56ms
step:289/2330 train_time:16633ms step_avg:57.55ms
step:290/2330 train_time:16693ms step_avg:57.56ms
step:291/2330 train_time:16750ms step_avg:57.56ms
step:292/2330 train_time:16810ms step_avg:57.57ms
step:293/2330 train_time:16866ms step_avg:57.56ms
step:294/2330 train_time:16924ms step_avg:57.57ms
step:295/2330 train_time:16980ms step_avg:57.56ms
step:296/2330 train_time:17040ms step_avg:57.57ms
step:297/2330 train_time:17096ms step_avg:57.56ms
step:298/2330 train_time:17154ms step_avg:57.56ms
step:299/2330 train_time:17209ms step_avg:57.56ms
step:300/2330 train_time:17269ms step_avg:57.56ms
step:301/2330 train_time:17324ms step_avg:57.56ms
step:302/2330 train_time:17383ms step_avg:57.56ms
step:303/2330 train_time:17439ms step_avg:57.55ms
step:304/2330 train_time:17498ms step_avg:57.56ms
step:305/2330 train_time:17554ms step_avg:57.55ms
step:306/2330 train_time:17613ms step_avg:57.56ms
step:307/2330 train_time:17669ms step_avg:57.55ms
step:308/2330 train_time:17729ms step_avg:57.56ms
step:309/2330 train_time:17785ms step_avg:57.56ms
step:310/2330 train_time:17845ms step_avg:57.56ms
step:311/2330 train_time:17901ms step_avg:57.56ms
step:312/2330 train_time:17960ms step_avg:57.56ms
step:313/2330 train_time:18015ms step_avg:57.56ms
step:314/2330 train_time:18075ms step_avg:57.56ms
step:315/2330 train_time:18131ms step_avg:57.56ms
step:316/2330 train_time:18190ms step_avg:57.56ms
step:317/2330 train_time:18246ms step_avg:57.56ms
step:318/2330 train_time:18305ms step_avg:57.56ms
step:319/2330 train_time:18361ms step_avg:57.56ms
step:320/2330 train_time:18420ms step_avg:57.56ms
step:321/2330 train_time:18476ms step_avg:57.56ms
step:322/2330 train_time:18535ms step_avg:57.56ms
step:323/2330 train_time:18591ms step_avg:57.56ms
step:324/2330 train_time:18650ms step_avg:57.56ms
step:325/2330 train_time:18707ms step_avg:57.56ms
step:326/2330 train_time:18766ms step_avg:57.56ms
step:327/2330 train_time:18822ms step_avg:57.56ms
step:328/2330 train_time:18881ms step_avg:57.56ms
step:329/2330 train_time:18938ms step_avg:57.56ms
step:330/2330 train_time:18997ms step_avg:57.57ms
step:331/2330 train_time:19052ms step_avg:57.56ms
step:332/2330 train_time:19112ms step_avg:57.57ms
step:333/2330 train_time:19168ms step_avg:57.56ms
step:334/2330 train_time:19227ms step_avg:57.57ms
step:335/2330 train_time:19283ms step_avg:57.56ms
step:336/2330 train_time:19342ms step_avg:57.57ms
step:337/2330 train_time:19398ms step_avg:57.56ms
step:338/2330 train_time:19457ms step_avg:57.57ms
step:339/2330 train_time:19513ms step_avg:57.56ms
step:340/2330 train_time:19572ms step_avg:57.56ms
step:341/2330 train_time:19627ms step_avg:57.56ms
step:342/2330 train_time:19688ms step_avg:57.57ms
step:343/2330 train_time:19745ms step_avg:57.57ms
step:344/2330 train_time:19804ms step_avg:57.57ms
step:345/2330 train_time:19860ms step_avg:57.56ms
step:346/2330 train_time:19919ms step_avg:57.57ms
step:347/2330 train_time:19975ms step_avg:57.56ms
step:348/2330 train_time:20035ms step_avg:57.57ms
step:349/2330 train_time:20090ms step_avg:57.56ms
step:350/2330 train_time:20150ms step_avg:57.57ms
step:351/2330 train_time:20206ms step_avg:57.57ms
step:352/2330 train_time:20266ms step_avg:57.57ms
step:353/2330 train_time:20321ms step_avg:57.57ms
step:354/2330 train_time:20380ms step_avg:57.57ms
step:355/2330 train_time:20436ms step_avg:57.57ms
step:356/2330 train_time:20496ms step_avg:57.57ms
step:357/2330 train_time:20552ms step_avg:57.57ms
step:358/2330 train_time:20611ms step_avg:57.57ms
step:359/2330 train_time:20667ms step_avg:57.57ms
step:360/2330 train_time:20727ms step_avg:57.58ms
step:361/2330 train_time:20783ms step_avg:57.57ms
step:362/2330 train_time:20842ms step_avg:57.58ms
step:363/2330 train_time:20898ms step_avg:57.57ms
step:364/2330 train_time:20957ms step_avg:57.57ms
step:365/2330 train_time:21013ms step_avg:57.57ms
step:366/2330 train_time:21072ms step_avg:57.57ms
step:367/2330 train_time:21128ms step_avg:57.57ms
step:368/2330 train_time:21188ms step_avg:57.58ms
step:369/2330 train_time:21243ms step_avg:57.57ms
step:370/2330 train_time:21302ms step_avg:57.57ms
step:371/2330 train_time:21358ms step_avg:57.57ms
step:372/2330 train_time:21418ms step_avg:57.57ms
step:373/2330 train_time:21474ms step_avg:57.57ms
step:374/2330 train_time:21533ms step_avg:57.57ms
step:375/2330 train_time:21588ms step_avg:57.57ms
step:376/2330 train_time:21648ms step_avg:57.57ms
step:377/2330 train_time:21704ms step_avg:57.57ms
step:378/2330 train_time:21763ms step_avg:57.57ms
step:379/2330 train_time:21819ms step_avg:57.57ms
step:380/2330 train_time:21878ms step_avg:57.57ms
step:381/2330 train_time:21933ms step_avg:57.57ms
step:382/2330 train_time:21993ms step_avg:57.57ms
step:383/2330 train_time:22049ms step_avg:57.57ms
step:384/2330 train_time:22109ms step_avg:57.57ms
step:385/2330 train_time:22165ms step_avg:57.57ms
step:386/2330 train_time:22223ms step_avg:57.57ms
step:387/2330 train_time:22279ms step_avg:57.57ms
step:388/2330 train_time:22338ms step_avg:57.57ms
step:389/2330 train_time:22395ms step_avg:57.57ms
step:390/2330 train_time:22453ms step_avg:57.57ms
step:391/2330 train_time:22509ms step_avg:57.57ms
step:392/2330 train_time:22569ms step_avg:57.57ms
step:393/2330 train_time:22624ms step_avg:57.57ms
step:394/2330 train_time:22684ms step_avg:57.57ms
step:395/2330 train_time:22740ms step_avg:57.57ms
step:396/2330 train_time:22800ms step_avg:57.57ms
step:397/2330 train_time:22855ms step_avg:57.57ms
step:398/2330 train_time:22915ms step_avg:57.57ms
step:399/2330 train_time:22970ms step_avg:57.57ms
step:400/2330 train_time:23030ms step_avg:57.58ms
step:401/2330 train_time:23087ms step_avg:57.57ms
step:402/2330 train_time:23146ms step_avg:57.58ms
step:403/2330 train_time:23203ms step_avg:57.58ms
step:404/2330 train_time:23262ms step_avg:57.58ms
step:405/2330 train_time:23318ms step_avg:57.58ms
step:406/2330 train_time:23377ms step_avg:57.58ms
step:407/2330 train_time:23433ms step_avg:57.58ms
step:408/2330 train_time:23493ms step_avg:57.58ms
step:409/2330 train_time:23548ms step_avg:57.58ms
step:410/2330 train_time:23609ms step_avg:57.58ms
step:411/2330 train_time:23665ms step_avg:57.58ms
step:412/2330 train_time:23724ms step_avg:57.58ms
step:413/2330 train_time:23780ms step_avg:57.58ms
step:414/2330 train_time:23839ms step_avg:57.58ms
step:415/2330 train_time:23895ms step_avg:57.58ms
step:416/2330 train_time:23955ms step_avg:57.58ms
step:417/2330 train_time:24010ms step_avg:57.58ms
step:418/2330 train_time:24070ms step_avg:57.58ms
step:419/2330 train_time:24126ms step_avg:57.58ms
step:420/2330 train_time:24186ms step_avg:57.59ms
step:421/2330 train_time:24242ms step_avg:57.58ms
step:422/2330 train_time:24301ms step_avg:57.58ms
step:423/2330 train_time:24357ms step_avg:57.58ms
step:424/2330 train_time:24416ms step_avg:57.59ms
step:425/2330 train_time:24472ms step_avg:57.58ms
step:426/2330 train_time:24532ms step_avg:57.59ms
step:427/2330 train_time:24587ms step_avg:57.58ms
step:428/2330 train_time:24647ms step_avg:57.59ms
step:429/2330 train_time:24703ms step_avg:57.58ms
step:430/2330 train_time:24762ms step_avg:57.59ms
step:431/2330 train_time:24819ms step_avg:57.58ms
step:432/2330 train_time:24877ms step_avg:57.59ms
step:433/2330 train_time:24933ms step_avg:57.58ms
step:434/2330 train_time:24992ms step_avg:57.59ms
step:435/2330 train_time:25048ms step_avg:57.58ms
step:436/2330 train_time:25108ms step_avg:57.59ms
step:437/2330 train_time:25165ms step_avg:57.59ms
step:438/2330 train_time:25224ms step_avg:57.59ms
step:439/2330 train_time:25281ms step_avg:57.59ms
step:440/2330 train_time:25340ms step_avg:57.59ms
step:441/2330 train_time:25396ms step_avg:57.59ms
step:442/2330 train_time:25455ms step_avg:57.59ms
step:443/2330 train_time:25511ms step_avg:57.59ms
step:444/2330 train_time:25571ms step_avg:57.59ms
step:445/2330 train_time:25626ms step_avg:57.59ms
step:446/2330 train_time:25686ms step_avg:57.59ms
step:447/2330 train_time:25742ms step_avg:57.59ms
step:448/2330 train_time:25801ms step_avg:57.59ms
step:449/2330 train_time:25857ms step_avg:57.59ms
step:450/2330 train_time:25917ms step_avg:57.59ms
step:451/2330 train_time:25973ms step_avg:57.59ms
step:452/2330 train_time:26032ms step_avg:57.59ms
step:453/2330 train_time:26088ms step_avg:57.59ms
step:454/2330 train_time:26147ms step_avg:57.59ms
step:455/2330 train_time:26204ms step_avg:57.59ms
step:456/2330 train_time:26263ms step_avg:57.59ms
step:457/2330 train_time:26319ms step_avg:57.59ms
step:458/2330 train_time:26377ms step_avg:57.59ms
step:459/2330 train_time:26433ms step_avg:57.59ms
step:460/2330 train_time:26492ms step_avg:57.59ms
step:461/2330 train_time:26548ms step_avg:57.59ms
step:462/2330 train_time:26608ms step_avg:57.59ms
step:463/2330 train_time:26664ms step_avg:57.59ms
step:464/2330 train_time:26724ms step_avg:57.59ms
step:465/2330 train_time:26779ms step_avg:57.59ms
step:466/2330 train_time:26839ms step_avg:57.59ms
step:467/2330 train_time:26895ms step_avg:57.59ms
step:468/2330 train_time:26954ms step_avg:57.59ms
step:469/2330 train_time:27010ms step_avg:57.59ms
step:470/2330 train_time:27070ms step_avg:57.60ms
step:471/2330 train_time:27126ms step_avg:57.59ms
step:472/2330 train_time:27185ms step_avg:57.60ms
step:473/2330 train_time:27241ms step_avg:57.59ms
step:474/2330 train_time:27300ms step_avg:57.59ms
step:475/2330 train_time:27356ms step_avg:57.59ms
step:476/2330 train_time:27416ms step_avg:57.60ms
step:477/2330 train_time:27472ms step_avg:57.59ms
step:478/2330 train_time:27531ms step_avg:57.60ms
step:479/2330 train_time:27587ms step_avg:57.59ms
step:480/2330 train_time:27647ms step_avg:57.60ms
step:481/2330 train_time:27703ms step_avg:57.59ms
step:482/2330 train_time:27762ms step_avg:57.60ms
step:483/2330 train_time:27818ms step_avg:57.59ms
step:484/2330 train_time:27877ms step_avg:57.60ms
step:485/2330 train_time:27933ms step_avg:57.59ms
step:486/2330 train_time:27992ms step_avg:57.60ms
step:487/2330 train_time:28048ms step_avg:57.59ms
step:488/2330 train_time:28108ms step_avg:57.60ms
step:489/2330 train_time:28164ms step_avg:57.59ms
step:490/2330 train_time:28223ms step_avg:57.60ms
step:491/2330 train_time:28279ms step_avg:57.60ms
step:492/2330 train_time:28338ms step_avg:57.60ms
step:493/2330 train_time:28394ms step_avg:57.59ms
step:494/2330 train_time:28453ms step_avg:57.60ms
step:495/2330 train_time:28509ms step_avg:57.59ms
step:496/2330 train_time:28568ms step_avg:57.60ms
step:497/2330 train_time:28625ms step_avg:57.59ms
step:498/2330 train_time:28684ms step_avg:57.60ms
step:499/2330 train_time:28740ms step_avg:57.60ms
step:500/2330 train_time:28799ms step_avg:57.60ms
step:500/2330 val_loss:4.4170 train_time:28879ms step_avg:57.76ms
step:501/2330 train_time:28897ms step_avg:57.68ms
step:502/2330 train_time:28918ms step_avg:57.61ms
step:503/2330 train_time:28975ms step_avg:57.60ms
step:504/2330 train_time:29038ms step_avg:57.61ms
step:505/2330 train_time:29095ms step_avg:57.61ms
step:506/2330 train_time:29157ms step_avg:57.62ms
step:507/2330 train_time:29212ms step_avg:57.62ms
step:508/2330 train_time:29271ms step_avg:57.62ms
step:509/2330 train_time:29327ms step_avg:57.62ms
step:510/2330 train_time:29386ms step_avg:57.62ms
step:511/2330 train_time:29442ms step_avg:57.62ms
step:512/2330 train_time:29500ms step_avg:57.62ms
step:513/2330 train_time:29555ms step_avg:57.61ms
step:514/2330 train_time:29614ms step_avg:57.62ms
step:515/2330 train_time:29670ms step_avg:57.61ms
step:516/2330 train_time:29728ms step_avg:57.61ms
step:517/2330 train_time:29784ms step_avg:57.61ms
step:518/2330 train_time:29844ms step_avg:57.61ms
step:519/2330 train_time:29900ms step_avg:57.61ms
step:520/2330 train_time:29961ms step_avg:57.62ms
step:521/2330 train_time:30017ms step_avg:57.61ms
step:522/2330 train_time:30078ms step_avg:57.62ms
step:523/2330 train_time:30133ms step_avg:57.62ms
step:524/2330 train_time:30196ms step_avg:57.63ms
step:525/2330 train_time:30252ms step_avg:57.62ms
step:526/2330 train_time:30312ms step_avg:57.63ms
step:527/2330 train_time:30368ms step_avg:57.63ms
step:528/2330 train_time:30427ms step_avg:57.63ms
step:529/2330 train_time:30482ms step_avg:57.62ms
step:530/2330 train_time:30541ms step_avg:57.62ms
step:531/2330 train_time:30597ms step_avg:57.62ms
step:532/2330 train_time:30656ms step_avg:57.62ms
step:533/2330 train_time:30711ms step_avg:57.62ms
step:534/2330 train_time:30771ms step_avg:57.62ms
step:535/2330 train_time:30827ms step_avg:57.62ms
step:536/2330 train_time:30886ms step_avg:57.62ms
step:537/2330 train_time:30943ms step_avg:57.62ms
step:538/2330 train_time:31003ms step_avg:57.63ms
step:539/2330 train_time:31060ms step_avg:57.63ms
step:540/2330 train_time:31121ms step_avg:57.63ms
step:541/2330 train_time:31177ms step_avg:57.63ms
step:542/2330 train_time:31239ms step_avg:57.64ms
step:543/2330 train_time:31294ms step_avg:57.63ms
step:544/2330 train_time:31355ms step_avg:57.64ms
step:545/2330 train_time:31410ms step_avg:57.63ms
step:546/2330 train_time:31470ms step_avg:57.64ms
step:547/2330 train_time:31526ms step_avg:57.63ms
step:548/2330 train_time:31584ms step_avg:57.64ms
step:549/2330 train_time:31639ms step_avg:57.63ms
step:550/2330 train_time:31698ms step_avg:57.63ms
step:551/2330 train_time:31754ms step_avg:57.63ms
step:552/2330 train_time:31814ms step_avg:57.63ms
step:553/2330 train_time:31870ms step_avg:57.63ms
step:554/2330 train_time:31930ms step_avg:57.64ms
step:555/2330 train_time:31986ms step_avg:57.63ms
step:556/2330 train_time:32047ms step_avg:57.64ms
step:557/2330 train_time:32103ms step_avg:57.64ms
step:558/2330 train_time:32162ms step_avg:57.64ms
step:559/2330 train_time:32218ms step_avg:57.63ms
step:560/2330 train_time:32280ms step_avg:57.64ms
step:561/2330 train_time:32335ms step_avg:57.64ms
step:562/2330 train_time:32396ms step_avg:57.64ms
step:563/2330 train_time:32452ms step_avg:57.64ms
step:564/2330 train_time:32512ms step_avg:57.65ms
step:565/2330 train_time:32567ms step_avg:57.64ms
step:566/2330 train_time:32626ms step_avg:57.64ms
step:567/2330 train_time:32681ms step_avg:57.64ms
step:568/2330 train_time:32740ms step_avg:57.64ms
step:569/2330 train_time:32795ms step_avg:57.64ms
step:570/2330 train_time:32856ms step_avg:57.64ms
step:571/2330 train_time:32911ms step_avg:57.64ms
step:572/2330 train_time:32971ms step_avg:57.64ms
step:573/2330 train_time:33028ms step_avg:57.64ms
step:574/2330 train_time:33087ms step_avg:57.64ms
step:575/2330 train_time:33144ms step_avg:57.64ms
step:576/2330 train_time:33204ms step_avg:57.65ms
step:577/2330 train_time:33260ms step_avg:57.64ms
step:578/2330 train_time:33321ms step_avg:57.65ms
step:579/2330 train_time:33376ms step_avg:57.64ms
step:580/2330 train_time:33437ms step_avg:57.65ms
step:581/2330 train_time:33492ms step_avg:57.65ms
step:582/2330 train_time:33553ms step_avg:57.65ms
step:583/2330 train_time:33608ms step_avg:57.65ms
step:584/2330 train_time:33668ms step_avg:57.65ms
step:585/2330 train_time:33723ms step_avg:57.65ms
step:586/2330 train_time:33782ms step_avg:57.65ms
step:587/2330 train_time:33838ms step_avg:57.65ms
step:588/2330 train_time:33898ms step_avg:57.65ms
step:589/2330 train_time:33953ms step_avg:57.65ms
step:590/2330 train_time:34013ms step_avg:57.65ms
step:591/2330 train_time:34069ms step_avg:57.65ms
step:592/2330 train_time:34129ms step_avg:57.65ms
step:593/2330 train_time:34186ms step_avg:57.65ms
step:594/2330 train_time:34246ms step_avg:57.65ms
step:595/2330 train_time:34302ms step_avg:57.65ms
step:596/2330 train_time:34363ms step_avg:57.66ms
step:597/2330 train_time:34419ms step_avg:57.65ms
step:598/2330 train_time:34479ms step_avg:57.66ms
step:599/2330 train_time:34534ms step_avg:57.65ms
step:600/2330 train_time:34596ms step_avg:57.66ms
step:601/2330 train_time:34651ms step_avg:57.66ms
step:602/2330 train_time:34710ms step_avg:57.66ms
step:603/2330 train_time:34766ms step_avg:57.66ms
step:604/2330 train_time:34826ms step_avg:57.66ms
step:605/2330 train_time:34882ms step_avg:57.66ms
step:606/2330 train_time:34941ms step_avg:57.66ms
step:607/2330 train_time:34996ms step_avg:57.65ms
step:608/2330 train_time:35057ms step_avg:57.66ms
step:609/2330 train_time:35113ms step_avg:57.66ms
step:610/2330 train_time:35173ms step_avg:57.66ms
step:611/2330 train_time:35230ms step_avg:57.66ms
step:612/2330 train_time:35289ms step_avg:57.66ms
step:613/2330 train_time:35347ms step_avg:57.66ms
step:614/2330 train_time:35406ms step_avg:57.66ms
step:615/2330 train_time:35462ms step_avg:57.66ms
step:616/2330 train_time:35521ms step_avg:57.66ms
step:617/2330 train_time:35577ms step_avg:57.66ms
step:618/2330 train_time:35636ms step_avg:57.66ms
step:619/2330 train_time:35691ms step_avg:57.66ms
step:620/2330 train_time:35751ms step_avg:57.66ms
step:621/2330 train_time:35807ms step_avg:57.66ms
step:622/2330 train_time:35866ms step_avg:57.66ms
step:623/2330 train_time:35922ms step_avg:57.66ms
step:624/2330 train_time:35982ms step_avg:57.66ms
step:625/2330 train_time:36038ms step_avg:57.66ms
step:626/2330 train_time:36098ms step_avg:57.66ms
step:627/2330 train_time:36153ms step_avg:57.66ms
step:628/2330 train_time:36215ms step_avg:57.67ms
step:629/2330 train_time:36271ms step_avg:57.66ms
step:630/2330 train_time:36331ms step_avg:57.67ms
step:631/2330 train_time:36388ms step_avg:57.67ms
step:632/2330 train_time:36447ms step_avg:57.67ms
step:633/2330 train_time:36504ms step_avg:57.67ms
step:634/2330 train_time:36563ms step_avg:57.67ms
step:635/2330 train_time:36618ms step_avg:57.67ms
step:636/2330 train_time:36678ms step_avg:57.67ms
step:637/2330 train_time:36733ms step_avg:57.67ms
step:638/2330 train_time:36795ms step_avg:57.67ms
step:639/2330 train_time:36851ms step_avg:57.67ms
step:640/2330 train_time:36911ms step_avg:57.67ms
step:641/2330 train_time:36967ms step_avg:57.67ms
step:642/2330 train_time:37026ms step_avg:57.67ms
step:643/2330 train_time:37082ms step_avg:57.67ms
step:644/2330 train_time:37141ms step_avg:57.67ms
step:645/2330 train_time:37197ms step_avg:57.67ms
step:646/2330 train_time:37257ms step_avg:57.67ms
step:647/2330 train_time:37313ms step_avg:57.67ms
step:648/2330 train_time:37374ms step_avg:57.68ms
step:649/2330 train_time:37430ms step_avg:57.67ms
step:650/2330 train_time:37490ms step_avg:57.68ms
step:651/2330 train_time:37547ms step_avg:57.68ms
step:652/2330 train_time:37606ms step_avg:57.68ms
step:653/2330 train_time:37663ms step_avg:57.68ms
step:654/2330 train_time:37721ms step_avg:57.68ms
step:655/2330 train_time:37777ms step_avg:57.67ms
step:656/2330 train_time:37836ms step_avg:57.68ms
step:657/2330 train_time:37892ms step_avg:57.67ms
step:658/2330 train_time:37953ms step_avg:57.68ms
step:659/2330 train_time:38009ms step_avg:57.68ms
step:660/2330 train_time:38067ms step_avg:57.68ms
step:661/2330 train_time:38123ms step_avg:57.67ms
step:662/2330 train_time:38184ms step_avg:57.68ms
step:663/2330 train_time:38240ms step_avg:57.68ms
step:664/2330 train_time:38299ms step_avg:57.68ms
step:665/2330 train_time:38355ms step_avg:57.68ms
step:666/2330 train_time:38415ms step_avg:57.68ms
step:667/2330 train_time:38471ms step_avg:57.68ms
step:668/2330 train_time:38531ms step_avg:57.68ms
step:669/2330 train_time:38588ms step_avg:57.68ms
step:670/2330 train_time:38647ms step_avg:57.68ms
step:671/2330 train_time:38703ms step_avg:57.68ms
step:672/2330 train_time:38762ms step_avg:57.68ms
step:673/2330 train_time:38818ms step_avg:57.68ms
step:674/2330 train_time:38878ms step_avg:57.68ms
step:675/2330 train_time:38933ms step_avg:57.68ms
step:676/2330 train_time:38994ms step_avg:57.68ms
step:677/2330 train_time:39050ms step_avg:57.68ms
step:678/2330 train_time:39109ms step_avg:57.68ms
step:679/2330 train_time:39165ms step_avg:57.68ms
step:680/2330 train_time:39224ms step_avg:57.68ms
step:681/2330 train_time:39280ms step_avg:57.68ms
step:682/2330 train_time:39340ms step_avg:57.68ms
step:683/2330 train_time:39395ms step_avg:57.68ms
step:684/2330 train_time:39456ms step_avg:57.68ms
step:685/2330 train_time:39512ms step_avg:57.68ms
step:686/2330 train_time:39571ms step_avg:57.68ms
step:687/2330 train_time:39628ms step_avg:57.68ms
step:688/2330 train_time:39687ms step_avg:57.68ms
step:689/2330 train_time:39743ms step_avg:57.68ms
step:690/2330 train_time:39802ms step_avg:57.68ms
step:691/2330 train_time:39858ms step_avg:57.68ms
step:692/2330 train_time:39918ms step_avg:57.69ms
step:693/2330 train_time:39974ms step_avg:57.68ms
step:694/2330 train_time:40035ms step_avg:57.69ms
step:695/2330 train_time:40092ms step_avg:57.69ms
step:696/2330 train_time:40151ms step_avg:57.69ms
step:697/2330 train_time:40208ms step_avg:57.69ms
step:698/2330 train_time:40267ms step_avg:57.69ms
step:699/2330 train_time:40324ms step_avg:57.69ms
step:700/2330 train_time:40384ms step_avg:57.69ms
step:701/2330 train_time:40440ms step_avg:57.69ms
step:702/2330 train_time:40499ms step_avg:57.69ms
step:703/2330 train_time:40555ms step_avg:57.69ms
step:704/2330 train_time:40615ms step_avg:57.69ms
step:705/2330 train_time:40670ms step_avg:57.69ms
step:706/2330 train_time:40730ms step_avg:57.69ms
step:707/2330 train_time:40787ms step_avg:57.69ms
step:708/2330 train_time:40846ms step_avg:57.69ms
step:709/2330 train_time:40902ms step_avg:57.69ms
step:710/2330 train_time:40962ms step_avg:57.69ms
step:711/2330 train_time:41018ms step_avg:57.69ms
step:712/2330 train_time:41078ms step_avg:57.69ms
step:713/2330 train_time:41134ms step_avg:57.69ms
step:714/2330 train_time:41194ms step_avg:57.70ms
step:715/2330 train_time:41251ms step_avg:57.69ms
step:716/2330 train_time:41310ms step_avg:57.70ms
step:717/2330 train_time:41366ms step_avg:57.69ms
step:718/2330 train_time:41426ms step_avg:57.70ms
step:719/2330 train_time:41481ms step_avg:57.69ms
step:720/2330 train_time:41542ms step_avg:57.70ms
step:721/2330 train_time:41598ms step_avg:57.69ms
step:722/2330 train_time:41657ms step_avg:57.70ms
step:723/2330 train_time:41713ms step_avg:57.69ms
step:724/2330 train_time:41773ms step_avg:57.70ms
step:725/2330 train_time:41830ms step_avg:57.70ms
step:726/2330 train_time:41888ms step_avg:57.70ms
step:727/2330 train_time:41945ms step_avg:57.70ms
step:728/2330 train_time:42004ms step_avg:57.70ms
step:729/2330 train_time:42060ms step_avg:57.70ms
step:730/2330 train_time:42119ms step_avg:57.70ms
step:731/2330 train_time:42175ms step_avg:57.69ms
step:732/2330 train_time:42235ms step_avg:57.70ms
step:733/2330 train_time:42291ms step_avg:57.70ms
step:734/2330 train_time:42351ms step_avg:57.70ms
step:735/2330 train_time:42407ms step_avg:57.70ms
step:736/2330 train_time:42467ms step_avg:57.70ms
step:737/2330 train_time:42523ms step_avg:57.70ms
step:738/2330 train_time:42581ms step_avg:57.70ms
step:739/2330 train_time:42637ms step_avg:57.70ms
step:740/2330 train_time:42697ms step_avg:57.70ms
step:741/2330 train_time:42753ms step_avg:57.70ms
step:742/2330 train_time:42812ms step_avg:57.70ms
step:743/2330 train_time:42868ms step_avg:57.70ms
step:744/2330 train_time:42928ms step_avg:57.70ms
step:745/2330 train_time:42985ms step_avg:57.70ms
step:746/2330 train_time:43046ms step_avg:57.70ms
step:747/2330 train_time:43101ms step_avg:57.70ms
step:748/2330 train_time:43162ms step_avg:57.70ms
step:749/2330 train_time:43218ms step_avg:57.70ms
step:750/2330 train_time:43277ms step_avg:57.70ms
step:750/2330 val_loss:4.2348 train_time:43358ms step_avg:57.81ms
step:751/2330 train_time:43376ms step_avg:57.76ms
step:752/2330 train_time:43396ms step_avg:57.71ms
step:753/2330 train_time:43452ms step_avg:57.71ms
step:754/2330 train_time:43514ms step_avg:57.71ms
step:755/2330 train_time:43569ms step_avg:57.71ms
step:756/2330 train_time:43632ms step_avg:57.71ms
step:757/2330 train_time:43688ms step_avg:57.71ms
step:758/2330 train_time:43747ms step_avg:57.71ms
step:759/2330 train_time:43803ms step_avg:57.71ms
step:760/2330 train_time:43862ms step_avg:57.71ms
step:761/2330 train_time:43917ms step_avg:57.71ms
step:762/2330 train_time:43975ms step_avg:57.71ms
step:763/2330 train_time:44031ms step_avg:57.71ms
step:764/2330 train_time:44089ms step_avg:57.71ms
step:765/2330 train_time:44147ms step_avg:57.71ms
step:766/2330 train_time:44205ms step_avg:57.71ms
step:767/2330 train_time:44260ms step_avg:57.71ms
step:768/2330 train_time:44323ms step_avg:57.71ms
step:769/2330 train_time:44380ms step_avg:57.71ms
step:770/2330 train_time:44443ms step_avg:57.72ms
step:771/2330 train_time:44500ms step_avg:57.72ms
step:772/2330 train_time:44563ms step_avg:57.72ms
step:773/2330 train_time:44619ms step_avg:57.72ms
step:774/2330 train_time:44681ms step_avg:57.73ms
step:775/2330 train_time:44737ms step_avg:57.73ms
step:776/2330 train_time:44798ms step_avg:57.73ms
step:777/2330 train_time:44854ms step_avg:57.73ms
step:778/2330 train_time:44915ms step_avg:57.73ms
step:779/2330 train_time:44971ms step_avg:57.73ms
step:780/2330 train_time:45030ms step_avg:57.73ms
step:781/2330 train_time:45087ms step_avg:57.73ms
step:782/2330 train_time:45146ms step_avg:57.73ms
step:783/2330 train_time:45204ms step_avg:57.73ms
step:784/2330 train_time:45263ms step_avg:57.73ms
step:785/2330 train_time:45320ms step_avg:57.73ms
step:786/2330 train_time:45382ms step_avg:57.74ms
step:787/2330 train_time:45439ms step_avg:57.74ms
step:788/2330 train_time:45499ms step_avg:57.74ms
step:789/2330 train_time:45556ms step_avg:57.74ms
step:790/2330 train_time:45618ms step_avg:57.74ms
step:791/2330 train_time:45674ms step_avg:57.74ms
step:792/2330 train_time:45735ms step_avg:57.75ms
step:793/2330 train_time:45791ms step_avg:57.74ms
step:794/2330 train_time:45851ms step_avg:57.75ms
step:795/2330 train_time:45908ms step_avg:57.75ms
step:796/2330 train_time:45968ms step_avg:57.75ms
step:797/2330 train_time:46024ms step_avg:57.75ms
step:798/2330 train_time:46084ms step_avg:57.75ms
step:799/2330 train_time:46140ms step_avg:57.75ms
step:800/2330 train_time:46201ms step_avg:57.75ms
step:801/2330 train_time:46258ms step_avg:57.75ms
step:802/2330 train_time:46317ms step_avg:57.75ms
step:803/2330 train_time:46374ms step_avg:57.75ms
step:804/2330 train_time:46435ms step_avg:57.76ms
step:805/2330 train_time:46492ms step_avg:57.75ms
step:806/2330 train_time:46552ms step_avg:57.76ms
step:807/2330 train_time:46609ms step_avg:57.76ms
step:808/2330 train_time:46670ms step_avg:57.76ms
step:809/2330 train_time:46726ms step_avg:57.76ms
step:810/2330 train_time:46787ms step_avg:57.76ms
step:811/2330 train_time:46843ms step_avg:57.76ms
step:812/2330 train_time:46904ms step_avg:57.76ms
step:813/2330 train_time:46960ms step_avg:57.76ms
step:814/2330 train_time:47020ms step_avg:57.76ms
step:815/2330 train_time:47076ms step_avg:57.76ms
step:816/2330 train_time:47137ms step_avg:57.77ms
step:817/2330 train_time:47194ms step_avg:57.76ms
step:818/2330 train_time:47255ms step_avg:57.77ms
step:819/2330 train_time:47311ms step_avg:57.77ms
step:820/2330 train_time:47372ms step_avg:57.77ms
step:821/2330 train_time:47429ms step_avg:57.77ms
step:822/2330 train_time:47489ms step_avg:57.77ms
step:823/2330 train_time:47547ms step_avg:57.77ms
step:824/2330 train_time:47606ms step_avg:57.77ms
step:825/2330 train_time:47663ms step_avg:57.77ms
step:826/2330 train_time:47723ms step_avg:57.78ms
step:827/2330 train_time:47780ms step_avg:57.78ms
step:828/2330 train_time:47840ms step_avg:57.78ms
step:829/2330 train_time:47897ms step_avg:57.78ms
step:830/2330 train_time:47958ms step_avg:57.78ms
step:831/2330 train_time:48014ms step_avg:57.78ms
step:832/2330 train_time:48074ms step_avg:57.78ms
step:833/2330 train_time:48130ms step_avg:57.78ms
step:834/2330 train_time:48191ms step_avg:57.78ms
step:835/2330 train_time:48247ms step_avg:57.78ms
step:836/2330 train_time:48307ms step_avg:57.78ms
step:837/2330 train_time:48364ms step_avg:57.78ms
step:838/2330 train_time:48425ms step_avg:57.79ms
step:839/2330 train_time:48482ms step_avg:57.79ms
step:840/2330 train_time:48543ms step_avg:57.79ms
step:841/2330 train_time:48599ms step_avg:57.79ms
step:842/2330 train_time:48661ms step_avg:57.79ms
step:843/2330 train_time:48717ms step_avg:57.79ms
step:844/2330 train_time:48779ms step_avg:57.79ms
step:845/2330 train_time:48835ms step_avg:57.79ms
step:846/2330 train_time:48896ms step_avg:57.80ms
step:847/2330 train_time:48952ms step_avg:57.79ms
step:848/2330 train_time:49013ms step_avg:57.80ms
step:849/2330 train_time:49069ms step_avg:57.80ms
step:850/2330 train_time:49129ms step_avg:57.80ms
step:851/2330 train_time:49186ms step_avg:57.80ms
step:852/2330 train_time:49246ms step_avg:57.80ms
step:853/2330 train_time:49303ms step_avg:57.80ms
step:854/2330 train_time:49362ms step_avg:57.80ms
step:855/2330 train_time:49418ms step_avg:57.80ms
step:856/2330 train_time:49479ms step_avg:57.80ms
step:857/2330 train_time:49536ms step_avg:57.80ms
step:858/2330 train_time:49596ms step_avg:57.80ms
step:859/2330 train_time:49653ms step_avg:57.80ms
step:860/2330 train_time:49713ms step_avg:57.81ms
step:861/2330 train_time:49770ms step_avg:57.80ms
step:862/2330 train_time:49830ms step_avg:57.81ms
step:863/2330 train_time:49887ms step_avg:57.81ms
step:864/2330 train_time:49947ms step_avg:57.81ms
step:865/2330 train_time:50005ms step_avg:57.81ms
step:866/2330 train_time:50064ms step_avg:57.81ms
step:867/2330 train_time:50121ms step_avg:57.81ms
step:868/2330 train_time:50181ms step_avg:57.81ms
step:869/2330 train_time:50237ms step_avg:57.81ms
step:870/2330 train_time:50298ms step_avg:57.81ms
step:871/2330 train_time:50354ms step_avg:57.81ms
step:872/2330 train_time:50415ms step_avg:57.82ms
step:873/2330 train_time:50472ms step_avg:57.81ms
step:874/2330 train_time:50532ms step_avg:57.82ms
step:875/2330 train_time:50589ms step_avg:57.82ms
step:876/2330 train_time:50649ms step_avg:57.82ms
step:877/2330 train_time:50705ms step_avg:57.82ms
step:878/2330 train_time:50765ms step_avg:57.82ms
step:879/2330 train_time:50821ms step_avg:57.82ms
step:880/2330 train_time:50882ms step_avg:57.82ms
step:881/2330 train_time:50938ms step_avg:57.82ms
step:882/2330 train_time:51000ms step_avg:57.82ms
step:883/2330 train_time:51056ms step_avg:57.82ms
step:884/2330 train_time:51116ms step_avg:57.82ms
step:885/2330 train_time:51172ms step_avg:57.82ms
step:886/2330 train_time:51233ms step_avg:57.82ms
step:887/2330 train_time:51290ms step_avg:57.82ms
step:888/2330 train_time:51350ms step_avg:57.83ms
step:889/2330 train_time:51406ms step_avg:57.83ms
step:890/2330 train_time:51466ms step_avg:57.83ms
step:891/2330 train_time:51523ms step_avg:57.83ms
step:892/2330 train_time:51583ms step_avg:57.83ms
step:893/2330 train_time:51640ms step_avg:57.83ms
step:894/2330 train_time:51699ms step_avg:57.83ms
step:895/2330 train_time:51757ms step_avg:57.83ms
step:896/2330 train_time:51817ms step_avg:57.83ms
step:897/2330 train_time:51874ms step_avg:57.83ms
step:898/2330 train_time:51934ms step_avg:57.83ms
step:899/2330 train_time:51991ms step_avg:57.83ms
step:900/2330 train_time:52051ms step_avg:57.83ms
step:901/2330 train_time:52109ms step_avg:57.83ms
step:902/2330 train_time:52168ms step_avg:57.84ms
step:903/2330 train_time:52226ms step_avg:57.84ms
step:904/2330 train_time:52285ms step_avg:57.84ms
step:905/2330 train_time:52342ms step_avg:57.84ms
step:906/2330 train_time:52401ms step_avg:57.84ms
step:907/2330 train_time:52457ms step_avg:57.84ms
step:908/2330 train_time:52517ms step_avg:57.84ms
step:909/2330 train_time:52574ms step_avg:57.84ms
step:910/2330 train_time:52635ms step_avg:57.84ms
step:911/2330 train_time:52691ms step_avg:57.84ms
step:912/2330 train_time:52752ms step_avg:57.84ms
step:913/2330 train_time:52809ms step_avg:57.84ms
step:914/2330 train_time:52869ms step_avg:57.84ms
step:915/2330 train_time:52926ms step_avg:57.84ms
step:916/2330 train_time:52986ms step_avg:57.85ms
step:917/2330 train_time:53043ms step_avg:57.84ms
step:918/2330 train_time:53103ms step_avg:57.85ms
step:919/2330 train_time:53159ms step_avg:57.84ms
step:920/2330 train_time:53220ms step_avg:57.85ms
step:921/2330 train_time:53277ms step_avg:57.85ms
step:922/2330 train_time:53338ms step_avg:57.85ms
step:923/2330 train_time:53395ms step_avg:57.85ms
step:924/2330 train_time:53455ms step_avg:57.85ms
step:925/2330 train_time:53511ms step_avg:57.85ms
step:926/2330 train_time:53572ms step_avg:57.85ms
step:927/2330 train_time:53628ms step_avg:57.85ms
step:928/2330 train_time:53689ms step_avg:57.85ms
step:929/2330 train_time:53746ms step_avg:57.85ms
step:930/2330 train_time:53806ms step_avg:57.86ms
step:931/2330 train_time:53863ms step_avg:57.86ms
step:932/2330 train_time:53923ms step_avg:57.86ms
step:933/2330 train_time:53980ms step_avg:57.86ms
step:934/2330 train_time:54040ms step_avg:57.86ms
step:935/2330 train_time:54097ms step_avg:57.86ms
step:936/2330 train_time:54157ms step_avg:57.86ms
step:937/2330 train_time:54213ms step_avg:57.86ms
step:938/2330 train_time:54275ms step_avg:57.86ms
step:939/2330 train_time:54331ms step_avg:57.86ms
step:940/2330 train_time:54391ms step_avg:57.86ms
step:941/2330 train_time:54447ms step_avg:57.86ms
step:942/2330 train_time:54508ms step_avg:57.86ms
step:943/2330 train_time:54565ms step_avg:57.86ms
step:944/2330 train_time:54625ms step_avg:57.87ms
step:945/2330 train_time:54682ms step_avg:57.86ms
step:946/2330 train_time:54742ms step_avg:57.87ms
step:947/2330 train_time:54799ms step_avg:57.87ms
step:948/2330 train_time:54859ms step_avg:57.87ms
step:949/2330 train_time:54916ms step_avg:57.87ms
step:950/2330 train_time:54976ms step_avg:57.87ms
step:951/2330 train_time:55033ms step_avg:57.87ms
step:952/2330 train_time:55093ms step_avg:57.87ms
step:953/2330 train_time:55150ms step_avg:57.87ms
step:954/2330 train_time:55210ms step_avg:57.87ms
step:955/2330 train_time:55267ms step_avg:57.87ms
step:956/2330 train_time:55327ms step_avg:57.87ms
step:957/2330 train_time:55384ms step_avg:57.87ms
step:958/2330 train_time:55444ms step_avg:57.87ms
step:959/2330 train_time:55501ms step_avg:57.87ms
step:960/2330 train_time:55561ms step_avg:57.88ms
step:961/2330 train_time:55617ms step_avg:57.87ms
step:962/2330 train_time:55678ms step_avg:57.88ms
step:963/2330 train_time:55734ms step_avg:57.88ms
step:964/2330 train_time:55795ms step_avg:57.88ms
step:965/2330 train_time:55852ms step_avg:57.88ms
step:966/2330 train_time:55912ms step_avg:57.88ms
step:967/2330 train_time:55969ms step_avg:57.88ms
step:968/2330 train_time:56028ms step_avg:57.88ms
step:969/2330 train_time:56085ms step_avg:57.88ms
step:970/2330 train_time:56147ms step_avg:57.88ms
step:971/2330 train_time:56204ms step_avg:57.88ms
step:972/2330 train_time:56263ms step_avg:57.88ms
step:973/2330 train_time:56320ms step_avg:57.88ms
step:974/2330 train_time:56380ms step_avg:57.89ms
step:975/2330 train_time:56436ms step_avg:57.88ms
step:976/2330 train_time:56498ms step_avg:57.89ms
step:977/2330 train_time:56554ms step_avg:57.89ms
step:978/2330 train_time:56615ms step_avg:57.89ms
step:979/2330 train_time:56671ms step_avg:57.89ms
step:980/2330 train_time:56732ms step_avg:57.89ms
step:981/2330 train_time:56789ms step_avg:57.89ms
step:982/2330 train_time:56849ms step_avg:57.89ms
step:983/2330 train_time:56906ms step_avg:57.89ms
step:984/2330 train_time:56966ms step_avg:57.89ms
step:985/2330 train_time:57023ms step_avg:57.89ms
step:986/2330 train_time:57083ms step_avg:57.89ms
step:987/2330 train_time:57139ms step_avg:57.89ms
step:988/2330 train_time:57200ms step_avg:57.90ms
step:989/2330 train_time:57257ms step_avg:57.89ms
step:990/2330 train_time:57317ms step_avg:57.90ms
step:991/2330 train_time:57374ms step_avg:57.90ms
step:992/2330 train_time:57434ms step_avg:57.90ms
step:993/2330 train_time:57492ms step_avg:57.90ms
step:994/2330 train_time:57552ms step_avg:57.90ms
step:995/2330 train_time:57610ms step_avg:57.90ms
step:996/2330 train_time:57669ms step_avg:57.90ms
step:997/2330 train_time:57726ms step_avg:57.90ms
step:998/2330 train_time:57785ms step_avg:57.90ms
step:999/2330 train_time:57842ms step_avg:57.90ms
step:1000/2330 train_time:57903ms step_avg:57.90ms
step:1000/2330 val_loss:4.0823 train_time:57984ms step_avg:57.98ms
step:1001/2330 train_time:58003ms step_avg:57.94ms
step:1002/2330 train_time:58023ms step_avg:57.91ms
step:1003/2330 train_time:58083ms step_avg:57.91ms
step:1004/2330 train_time:58146ms step_avg:57.91ms
step:1005/2330 train_time:58204ms step_avg:57.91ms
step:1006/2330 train_time:58266ms step_avg:57.92ms
step:1007/2330 train_time:58323ms step_avg:57.92ms
step:1008/2330 train_time:58383ms step_avg:57.92ms
step:1009/2330 train_time:58439ms step_avg:57.92ms
step:1010/2330 train_time:58499ms step_avg:57.92ms
step:1011/2330 train_time:58555ms step_avg:57.92ms
step:1012/2330 train_time:58614ms step_avg:57.92ms
step:1013/2330 train_time:58670ms step_avg:57.92ms
step:1014/2330 train_time:58729ms step_avg:57.92ms
step:1015/2330 train_time:58785ms step_avg:57.92ms
step:1016/2330 train_time:58844ms step_avg:57.92ms
step:1017/2330 train_time:58901ms step_avg:57.92ms
step:1018/2330 train_time:58967ms step_avg:57.92ms
step:1019/2330 train_time:59025ms step_avg:57.92ms
step:1020/2330 train_time:59089ms step_avg:57.93ms
step:1021/2330 train_time:59146ms step_avg:57.93ms
step:1022/2330 train_time:59207ms step_avg:57.93ms
step:1023/2330 train_time:59265ms step_avg:57.93ms
step:1024/2330 train_time:59324ms step_avg:57.93ms
step:1025/2330 train_time:59381ms step_avg:57.93ms
step:1026/2330 train_time:59442ms step_avg:57.94ms
step:1027/2330 train_time:59498ms step_avg:57.93ms
step:1028/2330 train_time:59558ms step_avg:57.94ms
step:1029/2330 train_time:59614ms step_avg:57.93ms
step:1030/2330 train_time:59675ms step_avg:57.94ms
step:1031/2330 train_time:59731ms step_avg:57.94ms
step:1032/2330 train_time:59791ms step_avg:57.94ms
step:1033/2330 train_time:59847ms step_avg:57.94ms
step:1034/2330 train_time:59909ms step_avg:57.94ms
step:1035/2330 train_time:59966ms step_avg:57.94ms
step:1036/2330 train_time:60029ms step_avg:57.94ms
step:1037/2330 train_time:60086ms step_avg:57.94ms
step:1038/2330 train_time:60147ms step_avg:57.95ms
step:1039/2330 train_time:60206ms step_avg:57.95ms
step:1040/2330 train_time:60266ms step_avg:57.95ms
step:1041/2330 train_time:60325ms step_avg:57.95ms
step:1042/2330 train_time:60386ms step_avg:57.95ms
step:1043/2330 train_time:60443ms step_avg:57.95ms
step:1044/2330 train_time:60503ms step_avg:57.95ms
step:1045/2330 train_time:60560ms step_avg:57.95ms
step:1046/2330 train_time:60620ms step_avg:57.95ms
step:1047/2330 train_time:60676ms step_avg:57.95ms
step:1048/2330 train_time:60737ms step_avg:57.96ms
step:1049/2330 train_time:60793ms step_avg:57.95ms
step:1050/2330 train_time:60855ms step_avg:57.96ms
step:1051/2330 train_time:60911ms step_avg:57.96ms
step:1052/2330 train_time:60974ms step_avg:57.96ms
step:1053/2330 train_time:61031ms step_avg:57.96ms
step:1054/2330 train_time:61094ms step_avg:57.96ms
step:1055/2330 train_time:61151ms step_avg:57.96ms
step:1056/2330 train_time:61211ms step_avg:57.96ms
step:1057/2330 train_time:61268ms step_avg:57.96ms
step:1058/2330 train_time:61328ms step_avg:57.97ms
step:1059/2330 train_time:61385ms step_avg:57.96ms
step:1060/2330 train_time:61446ms step_avg:57.97ms
step:1061/2330 train_time:61503ms step_avg:57.97ms
step:1062/2330 train_time:61563ms step_avg:57.97ms
step:1063/2330 train_time:61620ms step_avg:57.97ms
step:1064/2330 train_time:61680ms step_avg:57.97ms
step:1065/2330 train_time:61736ms step_avg:57.97ms
step:1066/2330 train_time:61797ms step_avg:57.97ms
step:1067/2330 train_time:61853ms step_avg:57.97ms
step:1068/2330 train_time:61914ms step_avg:57.97ms
step:1069/2330 train_time:61970ms step_avg:57.97ms
step:1070/2330 train_time:62033ms step_avg:57.98ms
step:1071/2330 train_time:62089ms step_avg:57.97ms
step:1072/2330 train_time:62151ms step_avg:57.98ms
step:1073/2330 train_time:62208ms step_avg:57.98ms
step:1074/2330 train_time:62269ms step_avg:57.98ms
step:1075/2330 train_time:62326ms step_avg:57.98ms
step:1076/2330 train_time:62386ms step_avg:57.98ms
step:1077/2330 train_time:62443ms step_avg:57.98ms
step:1078/2330 train_time:62503ms step_avg:57.98ms
step:1079/2330 train_time:62561ms step_avg:57.98ms
step:1080/2330 train_time:62620ms step_avg:57.98ms
step:1081/2330 train_time:62677ms step_avg:57.98ms
step:1082/2330 train_time:62736ms step_avg:57.98ms
step:1083/2330 train_time:62793ms step_avg:57.98ms
step:1084/2330 train_time:62854ms step_avg:57.98ms
step:1085/2330 train_time:62911ms step_avg:57.98ms
step:1086/2330 train_time:62972ms step_avg:57.99ms
step:1087/2330 train_time:63029ms step_avg:57.98ms
step:1088/2330 train_time:63090ms step_avg:57.99ms
step:1089/2330 train_time:63146ms step_avg:57.99ms
step:1090/2330 train_time:63207ms step_avg:57.99ms
step:1091/2330 train_time:63263ms step_avg:57.99ms
step:1092/2330 train_time:63324ms step_avg:57.99ms
step:1093/2330 train_time:63380ms step_avg:57.99ms
step:1094/2330 train_time:63441ms step_avg:57.99ms
step:1095/2330 train_time:63497ms step_avg:57.99ms
step:1096/2330 train_time:63557ms step_avg:57.99ms
step:1097/2330 train_time:63614ms step_avg:57.99ms
step:1098/2330 train_time:63674ms step_avg:57.99ms
step:1099/2330 train_time:63731ms step_avg:57.99ms
step:1100/2330 train_time:63791ms step_avg:57.99ms
step:1101/2330 train_time:63848ms step_avg:57.99ms
step:1102/2330 train_time:63909ms step_avg:57.99ms
step:1103/2330 train_time:63966ms step_avg:57.99ms
step:1104/2330 train_time:64025ms step_avg:57.99ms
step:1105/2330 train_time:64082ms step_avg:57.99ms
step:1106/2330 train_time:64143ms step_avg:58.00ms
step:1107/2330 train_time:64200ms step_avg:57.99ms
step:1108/2330 train_time:64261ms step_avg:58.00ms
step:1109/2330 train_time:64318ms step_avg:58.00ms
step:1110/2330 train_time:64378ms step_avg:58.00ms
step:1111/2330 train_time:64434ms step_avg:58.00ms
step:1112/2330 train_time:64495ms step_avg:58.00ms
step:1113/2330 train_time:64552ms step_avg:58.00ms
step:1114/2330 train_time:64613ms step_avg:58.00ms
step:1115/2330 train_time:64670ms step_avg:58.00ms
step:1116/2330 train_time:64729ms step_avg:58.00ms
step:1117/2330 train_time:64786ms step_avg:58.00ms
step:1118/2330 train_time:64847ms step_avg:58.00ms
step:1119/2330 train_time:64903ms step_avg:58.00ms
step:1120/2330 train_time:64964ms step_avg:58.00ms
step:1121/2330 train_time:65021ms step_avg:58.00ms
step:1122/2330 train_time:65081ms step_avg:58.00ms
step:1123/2330 train_time:65138ms step_avg:58.00ms
step:1124/2330 train_time:65198ms step_avg:58.01ms
step:1125/2330 train_time:65255ms step_avg:58.00ms
step:1126/2330 train_time:65315ms step_avg:58.01ms
step:1127/2330 train_time:65371ms step_avg:58.00ms
step:1128/2330 train_time:65434ms step_avg:58.01ms
step:1129/2330 train_time:65490ms step_avg:58.01ms
step:1130/2330 train_time:65551ms step_avg:58.01ms
step:1131/2330 train_time:65607ms step_avg:58.01ms
step:1132/2330 train_time:65667ms step_avg:58.01ms
step:1133/2330 train_time:65724ms step_avg:58.01ms
step:1134/2330 train_time:65784ms step_avg:58.01ms
step:1135/2330 train_time:65841ms step_avg:58.01ms
step:1136/2330 train_time:65901ms step_avg:58.01ms
step:1137/2330 train_time:65958ms step_avg:58.01ms
step:1138/2330 train_time:66018ms step_avg:58.01ms
step:1139/2330 train_time:66075ms step_avg:58.01ms
step:1140/2330 train_time:66136ms step_avg:58.01ms
step:1141/2330 train_time:66193ms step_avg:58.01ms
step:1142/2330 train_time:66254ms step_avg:58.02ms
step:1143/2330 train_time:66311ms step_avg:58.01ms
step:1144/2330 train_time:66371ms step_avg:58.02ms
step:1145/2330 train_time:66428ms step_avg:58.02ms
step:1146/2330 train_time:66488ms step_avg:58.02ms
step:1147/2330 train_time:66544ms step_avg:58.02ms
step:1148/2330 train_time:66604ms step_avg:58.02ms
step:1149/2330 train_time:66662ms step_avg:58.02ms
step:1150/2330 train_time:66722ms step_avg:58.02ms
step:1151/2330 train_time:66779ms step_avg:58.02ms
step:1152/2330 train_time:66840ms step_avg:58.02ms
step:1153/2330 train_time:66897ms step_avg:58.02ms
step:1154/2330 train_time:66957ms step_avg:58.02ms
step:1155/2330 train_time:67013ms step_avg:58.02ms
step:1156/2330 train_time:67074ms step_avg:58.02ms
step:1157/2330 train_time:67131ms step_avg:58.02ms
step:1158/2330 train_time:67191ms step_avg:58.02ms
step:1159/2330 train_time:67248ms step_avg:58.02ms
step:1160/2330 train_time:67308ms step_avg:58.02ms
step:1161/2330 train_time:67365ms step_avg:58.02ms
step:1162/2330 train_time:67425ms step_avg:58.03ms
step:1163/2330 train_time:67482ms step_avg:58.02ms
step:1164/2330 train_time:67543ms step_avg:58.03ms
step:1165/2330 train_time:67599ms step_avg:58.02ms
step:1166/2330 train_time:67659ms step_avg:58.03ms
step:1167/2330 train_time:67716ms step_avg:58.03ms
step:1168/2330 train_time:67777ms step_avg:58.03ms
step:1169/2330 train_time:67833ms step_avg:58.03ms
step:1170/2330 train_time:67895ms step_avg:58.03ms
step:1171/2330 train_time:67952ms step_avg:58.03ms
step:1172/2330 train_time:68013ms step_avg:58.03ms
step:1173/2330 train_time:68070ms step_avg:58.03ms
step:1174/2330 train_time:68130ms step_avg:58.03ms
step:1175/2330 train_time:68186ms step_avg:58.03ms
step:1176/2330 train_time:68246ms step_avg:58.03ms
step:1177/2330 train_time:68303ms step_avg:58.03ms
step:1178/2330 train_time:68363ms step_avg:58.03ms
step:1179/2330 train_time:68420ms step_avg:58.03ms
step:1180/2330 train_time:68481ms step_avg:58.03ms
step:1181/2330 train_time:68538ms step_avg:58.03ms
step:1182/2330 train_time:68598ms step_avg:58.04ms
step:1183/2330 train_time:68654ms step_avg:58.03ms
step:1184/2330 train_time:68717ms step_avg:58.04ms
step:1185/2330 train_time:68772ms step_avg:58.04ms
step:1186/2330 train_time:68834ms step_avg:58.04ms
step:1187/2330 train_time:68890ms step_avg:58.04ms
step:1188/2330 train_time:68951ms step_avg:58.04ms
step:1189/2330 train_time:69008ms step_avg:58.04ms
step:1190/2330 train_time:69068ms step_avg:58.04ms
step:1191/2330 train_time:69125ms step_avg:58.04ms
step:1192/2330 train_time:69184ms step_avg:58.04ms
step:1193/2330 train_time:69241ms step_avg:58.04ms
step:1194/2330 train_time:69302ms step_avg:58.04ms
step:1195/2330 train_time:69359ms step_avg:58.04ms
step:1196/2330 train_time:69420ms step_avg:58.04ms
step:1197/2330 train_time:69476ms step_avg:58.04ms
step:1198/2330 train_time:69536ms step_avg:58.04ms
step:1199/2330 train_time:69593ms step_avg:58.04ms
step:1200/2330 train_time:69653ms step_avg:58.04ms
step:1201/2330 train_time:69710ms step_avg:58.04ms
step:1202/2330 train_time:69770ms step_avg:58.05ms
step:1203/2330 train_time:69827ms step_avg:58.04ms
step:1204/2330 train_time:69888ms step_avg:58.05ms
step:1205/2330 train_time:69945ms step_avg:58.05ms
step:1206/2330 train_time:70004ms step_avg:58.05ms
step:1207/2330 train_time:70061ms step_avg:58.05ms
step:1208/2330 train_time:70122ms step_avg:58.05ms
step:1209/2330 train_time:70178ms step_avg:58.05ms
step:1210/2330 train_time:70240ms step_avg:58.05ms
step:1211/2330 train_time:70297ms step_avg:58.05ms
step:1212/2330 train_time:70358ms step_avg:58.05ms
step:1213/2330 train_time:70414ms step_avg:58.05ms
step:1214/2330 train_time:70475ms step_avg:58.05ms
step:1215/2330 train_time:70532ms step_avg:58.05ms
step:1216/2330 train_time:70592ms step_avg:58.05ms
step:1217/2330 train_time:70649ms step_avg:58.05ms
step:1218/2330 train_time:70708ms step_avg:58.05ms
step:1219/2330 train_time:70765ms step_avg:58.05ms
step:1220/2330 train_time:70826ms step_avg:58.05ms
step:1221/2330 train_time:70883ms step_avg:58.05ms
step:1222/2330 train_time:70943ms step_avg:58.05ms
step:1223/2330 train_time:71000ms step_avg:58.05ms
step:1224/2330 train_time:71061ms step_avg:58.06ms
step:1225/2330 train_time:71118ms step_avg:58.06ms
step:1226/2330 train_time:71178ms step_avg:58.06ms
step:1227/2330 train_time:71235ms step_avg:58.06ms
step:1228/2330 train_time:71296ms step_avg:58.06ms
step:1229/2330 train_time:71353ms step_avg:58.06ms
step:1230/2330 train_time:71413ms step_avg:58.06ms
step:1231/2330 train_time:71470ms step_avg:58.06ms
step:1232/2330 train_time:71530ms step_avg:58.06ms
step:1233/2330 train_time:71586ms step_avg:58.06ms
step:1234/2330 train_time:71647ms step_avg:58.06ms
step:1235/2330 train_time:71704ms step_avg:58.06ms
step:1236/2330 train_time:71764ms step_avg:58.06ms
step:1237/2330 train_time:71820ms step_avg:58.06ms
step:1238/2330 train_time:71881ms step_avg:58.06ms
step:1239/2330 train_time:71937ms step_avg:58.06ms
step:1240/2330 train_time:71999ms step_avg:58.06ms
step:1241/2330 train_time:72055ms step_avg:58.06ms
step:1242/2330 train_time:72116ms step_avg:58.06ms
step:1243/2330 train_time:72172ms step_avg:58.06ms
step:1244/2330 train_time:72233ms step_avg:58.07ms
step:1245/2330 train_time:72290ms step_avg:58.06ms
step:1246/2330 train_time:72350ms step_avg:58.07ms
step:1247/2330 train_time:72407ms step_avg:58.07ms
step:1248/2330 train_time:72467ms step_avg:58.07ms
step:1249/2330 train_time:72524ms step_avg:58.07ms
step:1250/2330 train_time:72584ms step_avg:58.07ms
step:1250/2330 val_loss:4.0159 train_time:72664ms step_avg:58.13ms
step:1251/2330 train_time:72683ms step_avg:58.10ms
step:1252/2330 train_time:72703ms step_avg:58.07ms
step:1253/2330 train_time:72762ms step_avg:58.07ms
step:1254/2330 train_time:72827ms step_avg:58.08ms
step:1255/2330 train_time:72883ms step_avg:58.07ms
step:1256/2330 train_time:72945ms step_avg:58.08ms
step:1257/2330 train_time:73002ms step_avg:58.08ms
step:1258/2330 train_time:73062ms step_avg:58.08ms
step:1259/2330 train_time:73118ms step_avg:58.08ms
step:1260/2330 train_time:73177ms step_avg:58.08ms
step:1261/2330 train_time:73234ms step_avg:58.08ms
step:1262/2330 train_time:73293ms step_avg:58.08ms
step:1263/2330 train_time:73350ms step_avg:58.08ms
step:1264/2330 train_time:73409ms step_avg:58.08ms
step:1265/2330 train_time:73465ms step_avg:58.08ms
step:1266/2330 train_time:73525ms step_avg:58.08ms
step:1267/2330 train_time:73582ms step_avg:58.08ms
step:1268/2330 train_time:73642ms step_avg:58.08ms
step:1269/2330 train_time:73703ms step_avg:58.08ms
step:1270/2330 train_time:73764ms step_avg:58.08ms
step:1271/2330 train_time:73822ms step_avg:58.08ms
step:1272/2330 train_time:73883ms step_avg:58.08ms
step:1273/2330 train_time:73941ms step_avg:58.08ms
step:1274/2330 train_time:74001ms step_avg:58.09ms
step:1275/2330 train_time:74057ms step_avg:58.08ms
step:1276/2330 train_time:74117ms step_avg:58.09ms
step:1277/2330 train_time:74173ms step_avg:58.08ms
step:1278/2330 train_time:74233ms step_avg:58.09ms
step:1279/2330 train_time:74290ms step_avg:58.08ms
step:1280/2330 train_time:74350ms step_avg:58.09ms
step:1281/2330 train_time:74406ms step_avg:58.08ms
step:1282/2330 train_time:74466ms step_avg:58.09ms
step:1283/2330 train_time:74522ms step_avg:58.08ms
step:1284/2330 train_time:74583ms step_avg:58.09ms
step:1285/2330 train_time:74641ms step_avg:58.09ms
step:1286/2330 train_time:74701ms step_avg:58.09ms
step:1287/2330 train_time:74758ms step_avg:58.09ms
step:1288/2330 train_time:74820ms step_avg:58.09ms
step:1289/2330 train_time:74878ms step_avg:58.09ms
step:1290/2330 train_time:74938ms step_avg:58.09ms
step:1291/2330 train_time:74994ms step_avg:58.09ms
step:1292/2330 train_time:75054ms step_avg:58.09ms
step:1293/2330 train_time:75111ms step_avg:58.09ms
step:1294/2330 train_time:75172ms step_avg:58.09ms
step:1295/2330 train_time:75228ms step_avg:58.09ms
step:1296/2330 train_time:75288ms step_avg:58.09ms
step:1297/2330 train_time:75344ms step_avg:58.09ms
step:1298/2330 train_time:75404ms step_avg:58.09ms
step:1299/2330 train_time:75460ms step_avg:58.09ms
step:1300/2330 train_time:75520ms step_avg:58.09ms
step:1301/2330 train_time:75577ms step_avg:58.09ms
step:1302/2330 train_time:75638ms step_avg:58.09ms
step:1303/2330 train_time:75697ms step_avg:58.09ms
step:1304/2330 train_time:75757ms step_avg:58.10ms
step:1305/2330 train_time:75814ms step_avg:58.10ms
step:1306/2330 train_time:75875ms step_avg:58.10ms
step:1307/2330 train_time:75933ms step_avg:58.10ms
step:1308/2330 train_time:75994ms step_avg:58.10ms
step:1309/2330 train_time:76050ms step_avg:58.10ms
step:1310/2330 train_time:76110ms step_avg:58.10ms
step:1311/2330 train_time:76167ms step_avg:58.10ms
step:1312/2330 train_time:76227ms step_avg:58.10ms
step:1313/2330 train_time:76284ms step_avg:58.10ms
step:1314/2330 train_time:76344ms step_avg:58.10ms
step:1315/2330 train_time:76400ms step_avg:58.10ms
step:1316/2330 train_time:76460ms step_avg:58.10ms
step:1317/2330 train_time:76517ms step_avg:58.10ms
step:1318/2330 train_time:76577ms step_avg:58.10ms
step:1319/2330 train_time:76634ms step_avg:58.10ms
step:1320/2330 train_time:76694ms step_avg:58.10ms
step:1321/2330 train_time:76751ms step_avg:58.10ms
step:1322/2330 train_time:76812ms step_avg:58.10ms
step:1323/2330 train_time:76870ms step_avg:58.10ms
step:1324/2330 train_time:76930ms step_avg:58.10ms
step:1325/2330 train_time:76987ms step_avg:58.10ms
step:1326/2330 train_time:77048ms step_avg:58.11ms
step:1327/2330 train_time:77104ms step_avg:58.10ms
step:1328/2330 train_time:77165ms step_avg:58.11ms
step:1329/2330 train_time:77223ms step_avg:58.11ms
step:1330/2330 train_time:77283ms step_avg:58.11ms
step:1331/2330 train_time:77340ms step_avg:58.11ms
step:1332/2330 train_time:77399ms step_avg:58.11ms
step:1333/2330 train_time:77456ms step_avg:58.11ms
step:1334/2330 train_time:77515ms step_avg:58.11ms
step:1335/2330 train_time:77571ms step_avg:58.11ms
step:1336/2330 train_time:77632ms step_avg:58.11ms
step:1337/2330 train_time:77690ms step_avg:58.11ms
step:1338/2330 train_time:77750ms step_avg:58.11ms
step:1339/2330 train_time:77807ms step_avg:58.11ms
step:1340/2330 train_time:77868ms step_avg:58.11ms
step:1341/2330 train_time:77925ms step_avg:58.11ms
step:1342/2330 train_time:77987ms step_avg:58.11ms
step:1343/2330 train_time:78043ms step_avg:58.11ms
step:1344/2330 train_time:78104ms step_avg:58.11ms
step:1345/2330 train_time:78160ms step_avg:58.11ms
step:1346/2330 train_time:78222ms step_avg:58.11ms
step:1347/2330 train_time:78279ms step_avg:58.11ms
step:1348/2330 train_time:78338ms step_avg:58.11ms
step:1349/2330 train_time:78396ms step_avg:58.11ms
step:1350/2330 train_time:78456ms step_avg:58.12ms
step:1351/2330 train_time:78513ms step_avg:58.11ms
step:1352/2330 train_time:78572ms step_avg:58.12ms
step:1353/2330 train_time:78628ms step_avg:58.11ms
step:1354/2330 train_time:78689ms step_avg:58.12ms
step:1355/2330 train_time:78746ms step_avg:58.12ms
step:1356/2330 train_time:78807ms step_avg:58.12ms
step:1357/2330 train_time:78864ms step_avg:58.12ms
step:1358/2330 train_time:78924ms step_avg:58.12ms
step:1359/2330 train_time:78981ms step_avg:58.12ms
step:1360/2330 train_time:79041ms step_avg:58.12ms
step:1361/2330 train_time:79099ms step_avg:58.12ms
step:1362/2330 train_time:79159ms step_avg:58.12ms
step:1363/2330 train_time:79215ms step_avg:58.12ms
step:1364/2330 train_time:79276ms step_avg:58.12ms
step:1365/2330 train_time:79333ms step_avg:58.12ms
step:1366/2330 train_time:79394ms step_avg:58.12ms
step:1367/2330 train_time:79451ms step_avg:58.12ms
step:1368/2330 train_time:79510ms step_avg:58.12ms
step:1369/2330 train_time:79568ms step_avg:58.12ms
step:1370/2330 train_time:79627ms step_avg:58.12ms
step:1371/2330 train_time:79685ms step_avg:58.12ms
step:1372/2330 train_time:79745ms step_avg:58.12ms
step:1373/2330 train_time:79802ms step_avg:58.12ms
step:1374/2330 train_time:79863ms step_avg:58.12ms
step:1375/2330 train_time:79919ms step_avg:58.12ms
step:1376/2330 train_time:79980ms step_avg:58.12ms
step:1377/2330 train_time:80037ms step_avg:58.12ms
step:1378/2330 train_time:80097ms step_avg:58.13ms
step:1379/2330 train_time:80154ms step_avg:58.12ms
step:1380/2330 train_time:80214ms step_avg:58.13ms
step:1381/2330 train_time:80271ms step_avg:58.13ms
step:1382/2330 train_time:80331ms step_avg:58.13ms
step:1383/2330 train_time:80387ms step_avg:58.13ms
step:1384/2330 train_time:80448ms step_avg:58.13ms
step:1385/2330 train_time:80505ms step_avg:58.13ms
step:1386/2330 train_time:80566ms step_avg:58.13ms
step:1387/2330 train_time:80623ms step_avg:58.13ms
step:1388/2330 train_time:80683ms step_avg:58.13ms
step:1389/2330 train_time:80740ms step_avg:58.13ms
step:1390/2330 train_time:80800ms step_avg:58.13ms
step:1391/2330 train_time:80857ms step_avg:58.13ms
step:1392/2330 train_time:80918ms step_avg:58.13ms
step:1393/2330 train_time:80974ms step_avg:58.13ms
step:1394/2330 train_time:81035ms step_avg:58.13ms
step:1395/2330 train_time:81092ms step_avg:58.13ms
step:1396/2330 train_time:81152ms step_avg:58.13ms
step:1397/2330 train_time:81208ms step_avg:58.13ms
step:1398/2330 train_time:81269ms step_avg:58.13ms
step:1399/2330 train_time:81327ms step_avg:58.13ms
step:1400/2330 train_time:81387ms step_avg:58.13ms
step:1401/2330 train_time:81443ms step_avg:58.13ms
step:1402/2330 train_time:81504ms step_avg:58.13ms
step:1403/2330 train_time:81561ms step_avg:58.13ms
step:1404/2330 train_time:81621ms step_avg:58.13ms
step:1405/2330 train_time:81678ms step_avg:58.13ms
step:1406/2330 train_time:81738ms step_avg:58.14ms
step:1407/2330 train_time:81796ms step_avg:58.13ms
step:1408/2330 train_time:81855ms step_avg:58.14ms
step:1409/2330 train_time:81912ms step_avg:58.13ms
step:1410/2330 train_time:81972ms step_avg:58.14ms
step:1411/2330 train_time:82028ms step_avg:58.14ms
step:1412/2330 train_time:82090ms step_avg:58.14ms
step:1413/2330 train_time:82147ms step_avg:58.14ms
step:1414/2330 train_time:82208ms step_avg:58.14ms
step:1415/2330 train_time:82264ms step_avg:58.14ms
step:1416/2330 train_time:82325ms step_avg:58.14ms
step:1417/2330 train_time:82382ms step_avg:58.14ms
step:1418/2330 train_time:82442ms step_avg:58.14ms
step:1419/2330 train_time:82499ms step_avg:58.14ms
step:1420/2330 train_time:82559ms step_avg:58.14ms
step:1421/2330 train_time:82616ms step_avg:58.14ms
step:1422/2330 train_time:82676ms step_avg:58.14ms
step:1423/2330 train_time:82732ms step_avg:58.14ms
step:1424/2330 train_time:82793ms step_avg:58.14ms
step:1425/2330 train_time:82849ms step_avg:58.14ms
step:1426/2330 train_time:82910ms step_avg:58.14ms
step:1427/2330 train_time:82967ms step_avg:58.14ms
step:1428/2330 train_time:83028ms step_avg:58.14ms
step:1429/2330 train_time:83085ms step_avg:58.14ms
step:1430/2330 train_time:83146ms step_avg:58.14ms
step:1431/2330 train_time:83202ms step_avg:58.14ms
step:1432/2330 train_time:83264ms step_avg:58.14ms
step:1433/2330 train_time:83320ms step_avg:58.14ms
step:1434/2330 train_time:83381ms step_avg:58.15ms
step:1435/2330 train_time:83438ms step_avg:58.14ms
step:1436/2330 train_time:83498ms step_avg:58.15ms
step:1437/2330 train_time:83556ms step_avg:58.15ms
step:1438/2330 train_time:83616ms step_avg:58.15ms
step:1439/2330 train_time:83673ms step_avg:58.15ms
step:1440/2330 train_time:83732ms step_avg:58.15ms
step:1441/2330 train_time:83789ms step_avg:58.15ms
step:1442/2330 train_time:83850ms step_avg:58.15ms
step:1443/2330 train_time:83906ms step_avg:58.15ms
step:1444/2330 train_time:83968ms step_avg:58.15ms
step:1445/2330 train_time:84024ms step_avg:58.15ms
step:1446/2330 train_time:84085ms step_avg:58.15ms
step:1447/2330 train_time:84142ms step_avg:58.15ms
step:1448/2330 train_time:84203ms step_avg:58.15ms
step:1449/2330 train_time:84260ms step_avg:58.15ms
step:1450/2330 train_time:84320ms step_avg:58.15ms
step:1451/2330 train_time:84377ms step_avg:58.15ms
step:1452/2330 train_time:84438ms step_avg:58.15ms
step:1453/2330 train_time:84494ms step_avg:58.15ms
step:1454/2330 train_time:84556ms step_avg:58.15ms
step:1455/2330 train_time:84613ms step_avg:58.15ms
step:1456/2330 train_time:84673ms step_avg:58.15ms
step:1457/2330 train_time:84730ms step_avg:58.15ms
step:1458/2330 train_time:84790ms step_avg:58.16ms
step:1459/2330 train_time:84847ms step_avg:58.15ms
step:1460/2330 train_time:84908ms step_avg:58.16ms
step:1461/2330 train_time:84965ms step_avg:58.16ms
step:1462/2330 train_time:85025ms step_avg:58.16ms
step:1463/2330 train_time:85082ms step_avg:58.16ms
step:1464/2330 train_time:85143ms step_avg:58.16ms
step:1465/2330 train_time:85200ms step_avg:58.16ms
step:1466/2330 train_time:85260ms step_avg:58.16ms
step:1467/2330 train_time:85316ms step_avg:58.16ms
step:1468/2330 train_time:85377ms step_avg:58.16ms
step:1469/2330 train_time:85433ms step_avg:58.16ms
step:1470/2330 train_time:85494ms step_avg:58.16ms
step:1471/2330 train_time:85551ms step_avg:58.16ms
step:1472/2330 train_time:85611ms step_avg:58.16ms
step:1473/2330 train_time:85668ms step_avg:58.16ms
step:1474/2330 train_time:85728ms step_avg:58.16ms
step:1475/2330 train_time:85785ms step_avg:58.16ms
step:1476/2330 train_time:85847ms step_avg:58.16ms
step:1477/2330 train_time:85903ms step_avg:58.16ms
step:1478/2330 train_time:85964ms step_avg:58.16ms
step:1479/2330 train_time:86021ms step_avg:58.16ms
step:1480/2330 train_time:86080ms step_avg:58.16ms
step:1481/2330 train_time:86137ms step_avg:58.16ms
step:1482/2330 train_time:86197ms step_avg:58.16ms
step:1483/2330 train_time:86254ms step_avg:58.16ms
step:1484/2330 train_time:86315ms step_avg:58.16ms
step:1485/2330 train_time:86372ms step_avg:58.16ms
step:1486/2330 train_time:86431ms step_avg:58.16ms
step:1487/2330 train_time:86488ms step_avg:58.16ms
step:1488/2330 train_time:86549ms step_avg:58.16ms
step:1489/2330 train_time:86605ms step_avg:58.16ms
step:1490/2330 train_time:86666ms step_avg:58.17ms
step:1491/2330 train_time:86723ms step_avg:58.16ms
step:1492/2330 train_time:86784ms step_avg:58.17ms
step:1493/2330 train_time:86841ms step_avg:58.17ms
step:1494/2330 train_time:86900ms step_avg:58.17ms
step:1495/2330 train_time:86957ms step_avg:58.17ms
step:1496/2330 train_time:87018ms step_avg:58.17ms
step:1497/2330 train_time:87076ms step_avg:58.17ms
step:1498/2330 train_time:87135ms step_avg:58.17ms
step:1499/2330 train_time:87192ms step_avg:58.17ms
step:1500/2330 train_time:87253ms step_avg:58.17ms
step:1500/2330 val_loss:3.9241 train_time:87334ms step_avg:58.22ms
step:1501/2330 train_time:87355ms step_avg:58.20ms
step:1502/2330 train_time:87375ms step_avg:58.17ms
step:1503/2330 train_time:87431ms step_avg:58.17ms
step:1504/2330 train_time:87496ms step_avg:58.18ms
step:1505/2330 train_time:87552ms step_avg:58.17ms
step:1506/2330 train_time:87615ms step_avg:58.18ms
step:1507/2330 train_time:87672ms step_avg:58.18ms
step:1508/2330 train_time:87731ms step_avg:58.18ms
step:1509/2330 train_time:87789ms step_avg:58.18ms
step:1510/2330 train_time:87848ms step_avg:58.18ms
step:1511/2330 train_time:87904ms step_avg:58.18ms
step:1512/2330 train_time:87964ms step_avg:58.18ms
step:1513/2330 train_time:88020ms step_avg:58.18ms
step:1514/2330 train_time:88079ms step_avg:58.18ms
step:1515/2330 train_time:88135ms step_avg:58.17ms
step:1516/2330 train_time:88196ms step_avg:58.18ms
step:1517/2330 train_time:88252ms step_avg:58.18ms
step:1518/2330 train_time:88313ms step_avg:58.18ms
step:1519/2330 train_time:88371ms step_avg:58.18ms
step:1520/2330 train_time:88434ms step_avg:58.18ms
step:1521/2330 train_time:88492ms step_avg:58.18ms
step:1522/2330 train_time:88555ms step_avg:58.18ms
step:1523/2330 train_time:88612ms step_avg:58.18ms
step:1524/2330 train_time:88672ms step_avg:58.18ms
step:1525/2330 train_time:88730ms step_avg:58.18ms
step:1526/2330 train_time:88789ms step_avg:58.18ms
step:1527/2330 train_time:88846ms step_avg:58.18ms
step:1528/2330 train_time:88906ms step_avg:58.18ms
step:1529/2330 train_time:88964ms step_avg:58.18ms
step:1530/2330 train_time:89023ms step_avg:58.19ms
step:1531/2330 train_time:89080ms step_avg:58.18ms
step:1532/2330 train_time:89140ms step_avg:58.19ms
step:1533/2330 train_time:89197ms step_avg:58.18ms
step:1534/2330 train_time:89258ms step_avg:58.19ms
step:1535/2330 train_time:89315ms step_avg:58.19ms
step:1536/2330 train_time:89379ms step_avg:58.19ms
step:1537/2330 train_time:89436ms step_avg:58.19ms
step:1538/2330 train_time:89500ms step_avg:58.19ms
step:1539/2330 train_time:89558ms step_avg:58.19ms
step:1540/2330 train_time:89620ms step_avg:58.20ms
step:1541/2330 train_time:89677ms step_avg:58.19ms
step:1542/2330 train_time:89739ms step_avg:58.20ms
step:1543/2330 train_time:89795ms step_avg:58.20ms
step:1544/2330 train_time:89858ms step_avg:58.20ms
step:1545/2330 train_time:89915ms step_avg:58.20ms
step:1546/2330 train_time:89975ms step_avg:58.20ms
step:1547/2330 train_time:90033ms step_avg:58.20ms
step:1548/2330 train_time:90093ms step_avg:58.20ms
step:1549/2330 train_time:90150ms step_avg:58.20ms
step:1550/2330 train_time:90211ms step_avg:58.20ms
step:1551/2330 train_time:90270ms step_avg:58.20ms
step:1552/2330 train_time:90330ms step_avg:58.20ms
step:1553/2330 train_time:90389ms step_avg:58.20ms
step:1554/2330 train_time:90450ms step_avg:58.20ms
step:1555/2330 train_time:90509ms step_avg:58.21ms
step:1556/2330 train_time:90570ms step_avg:58.21ms
step:1557/2330 train_time:90628ms step_avg:58.21ms
step:1558/2330 train_time:90688ms step_avg:58.21ms
step:1559/2330 train_time:90746ms step_avg:58.21ms
step:1560/2330 train_time:90807ms step_avg:58.21ms
step:1561/2330 train_time:90864ms step_avg:58.21ms
step:1562/2330 train_time:90924ms step_avg:58.21ms
step:1563/2330 train_time:90982ms step_avg:58.21ms
step:1564/2330 train_time:91041ms step_avg:58.21ms
step:1565/2330 train_time:91097ms step_avg:58.21ms
step:1566/2330 train_time:91159ms step_avg:58.21ms
step:1567/2330 train_time:91216ms step_avg:58.21ms
step:1568/2330 train_time:91278ms step_avg:58.21ms
step:1569/2330 train_time:91335ms step_avg:58.21ms
step:1570/2330 train_time:91398ms step_avg:58.22ms
step:1571/2330 train_time:91456ms step_avg:58.21ms
step:1572/2330 train_time:91518ms step_avg:58.22ms
step:1573/2330 train_time:91575ms step_avg:58.22ms
step:1574/2330 train_time:91638ms step_avg:58.22ms
step:1575/2330 train_time:91696ms step_avg:58.22ms
step:1576/2330 train_time:91756ms step_avg:58.22ms
step:1577/2330 train_time:91814ms step_avg:58.22ms
step:1578/2330 train_time:91875ms step_avg:58.22ms
step:1579/2330 train_time:91933ms step_avg:58.22ms
step:1580/2330 train_time:91993ms step_avg:58.22ms
step:1581/2330 train_time:92051ms step_avg:58.22ms
step:1582/2330 train_time:92111ms step_avg:58.22ms
step:1583/2330 train_time:92169ms step_avg:58.22ms
step:1584/2330 train_time:92230ms step_avg:58.23ms
step:1585/2330 train_time:92287ms step_avg:58.23ms
step:1586/2330 train_time:92347ms step_avg:58.23ms
step:1587/2330 train_time:92404ms step_avg:58.23ms
step:1588/2330 train_time:92466ms step_avg:58.23ms
step:1589/2330 train_time:92523ms step_avg:58.23ms
step:1590/2330 train_time:92586ms step_avg:58.23ms
step:1591/2330 train_time:92643ms step_avg:58.23ms
step:1592/2330 train_time:92704ms step_avg:58.23ms
step:1593/2330 train_time:92761ms step_avg:58.23ms
step:1594/2330 train_time:92823ms step_avg:58.23ms
step:1595/2330 train_time:92880ms step_avg:58.23ms
step:1596/2330 train_time:92942ms step_avg:58.23ms
step:1597/2330 train_time:92998ms step_avg:58.23ms
step:1598/2330 train_time:93060ms step_avg:58.24ms
step:1599/2330 train_time:93116ms step_avg:58.23ms
step:1600/2330 train_time:93179ms step_avg:58.24ms
step:1601/2330 train_time:93236ms step_avg:58.24ms
step:1602/2330 train_time:93297ms step_avg:58.24ms
step:1603/2330 train_time:93355ms step_avg:58.24ms
step:1604/2330 train_time:93416ms step_avg:58.24ms
step:1605/2330 train_time:93474ms step_avg:58.24ms
step:1606/2330 train_time:93534ms step_avg:58.24ms
step:1607/2330 train_time:93592ms step_avg:58.24ms
step:1608/2330 train_time:93652ms step_avg:58.24ms
step:1609/2330 train_time:93710ms step_avg:58.24ms
step:1610/2330 train_time:93771ms step_avg:58.24ms
step:1611/2330 train_time:93829ms step_avg:58.24ms
step:1612/2330 train_time:93890ms step_avg:58.24ms
step:1613/2330 train_time:93948ms step_avg:58.24ms
step:1614/2330 train_time:94010ms step_avg:58.25ms
step:1615/2330 train_time:94067ms step_avg:58.25ms
step:1616/2330 train_time:94129ms step_avg:58.25ms
step:1617/2330 train_time:94186ms step_avg:58.25ms
step:1618/2330 train_time:94246ms step_avg:58.25ms
step:1619/2330 train_time:94304ms step_avg:58.25ms
step:1620/2330 train_time:94364ms step_avg:58.25ms
step:1621/2330 train_time:94422ms step_avg:58.25ms
step:1622/2330 train_time:94484ms step_avg:58.25ms
step:1623/2330 train_time:94540ms step_avg:58.25ms
step:1624/2330 train_time:94602ms step_avg:58.25ms
step:1625/2330 train_time:94660ms step_avg:58.25ms
step:1626/2330 train_time:94722ms step_avg:58.25ms
step:1627/2330 train_time:94778ms step_avg:58.25ms
step:1628/2330 train_time:94840ms step_avg:58.26ms
step:1629/2330 train_time:94897ms step_avg:58.25ms
step:1630/2330 train_time:94959ms step_avg:58.26ms
step:1631/2330 train_time:95016ms step_avg:58.26ms
step:1632/2330 train_time:95078ms step_avg:58.26ms
step:1633/2330 train_time:95135ms step_avg:58.26ms
step:1634/2330 train_time:95197ms step_avg:58.26ms
step:1635/2330 train_time:95254ms step_avg:58.26ms
step:1636/2330 train_time:95315ms step_avg:58.26ms
step:1637/2330 train_time:95373ms step_avg:58.26ms
step:1638/2330 train_time:95433ms step_avg:58.26ms
step:1639/2330 train_time:95491ms step_avg:58.26ms
step:1640/2330 train_time:95551ms step_avg:58.26ms
step:1641/2330 train_time:95610ms step_avg:58.26ms
step:1642/2330 train_time:95670ms step_avg:58.26ms
step:1643/2330 train_time:95729ms step_avg:58.26ms
step:1644/2330 train_time:95789ms step_avg:58.27ms
step:1645/2330 train_time:95845ms step_avg:58.26ms
step:1646/2330 train_time:95909ms step_avg:58.27ms
step:1647/2330 train_time:95965ms step_avg:58.27ms
step:1648/2330 train_time:96028ms step_avg:58.27ms
step:1649/2330 train_time:96085ms step_avg:58.27ms
step:1650/2330 train_time:96147ms step_avg:58.27ms
step:1651/2330 train_time:96203ms step_avg:58.27ms
step:1652/2330 train_time:96265ms step_avg:58.27ms
step:1653/2330 train_time:96322ms step_avg:58.27ms
step:1654/2330 train_time:96384ms step_avg:58.27ms
step:1655/2330 train_time:96441ms step_avg:58.27ms
step:1656/2330 train_time:96503ms step_avg:58.27ms
step:1657/2330 train_time:96559ms step_avg:58.27ms
step:1658/2330 train_time:96622ms step_avg:58.28ms
step:1659/2330 train_time:96679ms step_avg:58.28ms
step:1660/2330 train_time:96740ms step_avg:58.28ms
step:1661/2330 train_time:96797ms step_avg:58.28ms
step:1662/2330 train_time:96859ms step_avg:58.28ms
step:1663/2330 train_time:96915ms step_avg:58.28ms
step:1664/2330 train_time:96978ms step_avg:58.28ms
step:1665/2330 train_time:97034ms step_avg:58.28ms
step:1666/2330 train_time:97096ms step_avg:58.28ms
step:1667/2330 train_time:97153ms step_avg:58.28ms
step:1668/2330 train_time:97214ms step_avg:58.28ms
step:1669/2330 train_time:97272ms step_avg:58.28ms
step:1670/2330 train_time:97333ms step_avg:58.28ms
step:1671/2330 train_time:97392ms step_avg:58.28ms
step:1672/2330 train_time:97452ms step_avg:58.28ms
step:1673/2330 train_time:97511ms step_avg:58.29ms
step:1674/2330 train_time:97571ms step_avg:58.29ms
step:1675/2330 train_time:97629ms step_avg:58.29ms
step:1676/2330 train_time:97690ms step_avg:58.29ms
step:1677/2330 train_time:97746ms step_avg:58.29ms
step:1678/2330 train_time:97809ms step_avg:58.29ms
step:1679/2330 train_time:97866ms step_avg:58.29ms
step:1680/2330 train_time:97928ms step_avg:58.29ms
step:1681/2330 train_time:97985ms step_avg:58.29ms
step:1682/2330 train_time:98046ms step_avg:58.29ms
step:1683/2330 train_time:98103ms step_avg:58.29ms
step:1684/2330 train_time:98165ms step_avg:58.29ms
step:1685/2330 train_time:98222ms step_avg:58.29ms
step:1686/2330 train_time:98286ms step_avg:58.30ms
step:1687/2330 train_time:98342ms step_avg:58.29ms
step:1688/2330 train_time:98405ms step_avg:58.30ms
step:1689/2330 train_time:98462ms step_avg:58.30ms
step:1690/2330 train_time:98524ms step_avg:58.30ms
step:1691/2330 train_time:98581ms step_avg:58.30ms
step:1692/2330 train_time:98643ms step_avg:58.30ms
step:1693/2330 train_time:98700ms step_avg:58.30ms
step:1694/2330 train_time:98762ms step_avg:58.30ms
step:1695/2330 train_time:98819ms step_avg:58.30ms
step:1696/2330 train_time:98881ms step_avg:58.30ms
step:1697/2330 train_time:98937ms step_avg:58.30ms
step:1698/2330 train_time:98999ms step_avg:58.30ms
step:1699/2330 train_time:99056ms step_avg:58.30ms
step:1700/2330 train_time:99117ms step_avg:58.30ms
step:1701/2330 train_time:99174ms step_avg:58.30ms
step:1702/2330 train_time:99235ms step_avg:58.30ms
step:1703/2330 train_time:99293ms step_avg:58.30ms
step:1704/2330 train_time:99353ms step_avg:58.31ms
step:1705/2330 train_time:99411ms step_avg:58.31ms
step:1706/2330 train_time:99472ms step_avg:58.31ms
step:1707/2330 train_time:99529ms step_avg:58.31ms
step:1708/2330 train_time:99591ms step_avg:58.31ms
step:1709/2330 train_time:99648ms step_avg:58.31ms
step:1710/2330 train_time:99709ms step_avg:58.31ms
step:1711/2330 train_time:99767ms step_avg:58.31ms
step:1712/2330 train_time:99829ms step_avg:58.31ms
step:1713/2330 train_time:99885ms step_avg:58.31ms
step:1714/2330 train_time:99947ms step_avg:58.31ms
step:1715/2330 train_time:100004ms step_avg:58.31ms
step:1716/2330 train_time:100066ms step_avg:58.31ms
step:1717/2330 train_time:100122ms step_avg:58.31ms
step:1718/2330 train_time:100185ms step_avg:58.31ms
step:1719/2330 train_time:100241ms step_avg:58.31ms
step:1720/2330 train_time:100304ms step_avg:58.32ms
step:1721/2330 train_time:100360ms step_avg:58.31ms
step:1722/2330 train_time:100423ms step_avg:58.32ms
step:1723/2330 train_time:100479ms step_avg:58.32ms
step:1724/2330 train_time:100542ms step_avg:58.32ms
step:1725/2330 train_time:100599ms step_avg:58.32ms
step:1726/2330 train_time:100661ms step_avg:58.32ms
step:1727/2330 train_time:100718ms step_avg:58.32ms
step:1728/2330 train_time:100779ms step_avg:58.32ms
step:1729/2330 train_time:100835ms step_avg:58.32ms
step:1730/2330 train_time:100898ms step_avg:58.32ms
step:1731/2330 train_time:100955ms step_avg:58.32ms
step:1732/2330 train_time:101016ms step_avg:58.32ms
step:1733/2330 train_time:101073ms step_avg:58.32ms
step:1734/2330 train_time:101134ms step_avg:58.32ms
step:1735/2330 train_time:101192ms step_avg:58.32ms
step:1736/2330 train_time:101252ms step_avg:58.32ms
step:1737/2330 train_time:101309ms step_avg:58.32ms
step:1738/2330 train_time:101369ms step_avg:58.33ms
step:1739/2330 train_time:101427ms step_avg:58.32ms
step:1740/2330 train_time:101488ms step_avg:58.33ms
step:1741/2330 train_time:101546ms step_avg:58.33ms
step:1742/2330 train_time:101607ms step_avg:58.33ms
step:1743/2330 train_time:101665ms step_avg:58.33ms
step:1744/2330 train_time:101726ms step_avg:58.33ms
step:1745/2330 train_time:101783ms step_avg:58.33ms
step:1746/2330 train_time:101845ms step_avg:58.33ms
step:1747/2330 train_time:101902ms step_avg:58.33ms
step:1748/2330 train_time:101963ms step_avg:58.33ms
step:1749/2330 train_time:102020ms step_avg:58.33ms
step:1750/2330 train_time:102081ms step_avg:58.33ms
step:1750/2330 val_loss:3.8345 train_time:102164ms step_avg:58.38ms
step:1751/2330 train_time:102184ms step_avg:58.36ms
step:1752/2330 train_time:102204ms step_avg:58.34ms
step:1753/2330 train_time:102257ms step_avg:58.33ms
step:1754/2330 train_time:102326ms step_avg:58.34ms
step:1755/2330 train_time:102382ms step_avg:58.34ms
step:1756/2330 train_time:102445ms step_avg:58.34ms
step:1757/2330 train_time:102501ms step_avg:58.34ms
step:1758/2330 train_time:102561ms step_avg:58.34ms
step:1759/2330 train_time:102618ms step_avg:58.34ms
step:1760/2330 train_time:102678ms step_avg:58.34ms
step:1761/2330 train_time:102735ms step_avg:58.34ms
step:1762/2330 train_time:102794ms step_avg:58.34ms
step:1763/2330 train_time:102851ms step_avg:58.34ms
step:1764/2330 train_time:102911ms step_avg:58.34ms
step:1765/2330 train_time:102968ms step_avg:58.34ms
step:1766/2330 train_time:103027ms step_avg:58.34ms
step:1767/2330 train_time:103087ms step_avg:58.34ms
step:1768/2330 train_time:103150ms step_avg:58.34ms
step:1769/2330 train_time:103208ms step_avg:58.34ms
step:1770/2330 train_time:103271ms step_avg:58.35ms
step:1771/2330 train_time:103330ms step_avg:58.35ms
step:1772/2330 train_time:103390ms step_avg:58.35ms
step:1773/2330 train_time:103446ms step_avg:58.35ms
step:1774/2330 train_time:103508ms step_avg:58.35ms
step:1775/2330 train_time:103565ms step_avg:58.35ms
step:1776/2330 train_time:103626ms step_avg:58.35ms
step:1777/2330 train_time:103682ms step_avg:58.35ms
step:1778/2330 train_time:103743ms step_avg:58.35ms
step:1779/2330 train_time:103800ms step_avg:58.35ms
step:1780/2330 train_time:103860ms step_avg:58.35ms
step:1781/2330 train_time:103917ms step_avg:58.35ms
step:1782/2330 train_time:103977ms step_avg:58.35ms
step:1783/2330 train_time:104034ms step_avg:58.35ms
step:1784/2330 train_time:104096ms step_avg:58.35ms
step:1785/2330 train_time:104154ms step_avg:58.35ms
step:1786/2330 train_time:104216ms step_avg:58.35ms
step:1787/2330 train_time:104275ms step_avg:58.35ms
step:1788/2330 train_time:104336ms step_avg:58.35ms
step:1789/2330 train_time:104393ms step_avg:58.35ms
step:1790/2330 train_time:104454ms step_avg:58.35ms
step:1791/2330 train_time:104513ms step_avg:58.35ms
step:1792/2330 train_time:104573ms step_avg:58.36ms
step:1793/2330 train_time:104632ms step_avg:58.36ms
step:1794/2330 train_time:104692ms step_avg:58.36ms
step:1795/2330 train_time:104750ms step_avg:58.36ms
step:1796/2330 train_time:104810ms step_avg:58.36ms
step:1797/2330 train_time:104867ms step_avg:58.36ms
step:1798/2330 train_time:104927ms step_avg:58.36ms
step:1799/2330 train_time:104984ms step_avg:58.36ms
step:1800/2330 train_time:105045ms step_avg:58.36ms
step:1801/2330 train_time:105102ms step_avg:58.36ms
step:1802/2330 train_time:105164ms step_avg:58.36ms
step:1803/2330 train_time:105222ms step_avg:58.36ms
step:1804/2330 train_time:105284ms step_avg:58.36ms
step:1805/2330 train_time:105340ms step_avg:58.36ms
step:1806/2330 train_time:105403ms step_avg:58.36ms
step:1807/2330 train_time:105460ms step_avg:58.36ms
step:1808/2330 train_time:105522ms step_avg:58.36ms
step:1809/2330 train_time:105578ms step_avg:58.36ms
step:1810/2330 train_time:105641ms step_avg:58.37ms
step:1811/2330 train_time:105698ms step_avg:58.36ms
step:1812/2330 train_time:105759ms step_avg:58.37ms
step:1813/2330 train_time:105817ms step_avg:58.37ms
step:1814/2330 train_time:105877ms step_avg:58.37ms
step:1815/2330 train_time:105935ms step_avg:58.37ms
step:1816/2330 train_time:105995ms step_avg:58.37ms
step:1817/2330 train_time:106053ms step_avg:58.37ms
step:1818/2330 train_time:106114ms step_avg:58.37ms
step:1819/2330 train_time:106173ms step_avg:58.37ms
step:1820/2330 train_time:106233ms step_avg:58.37ms
step:1821/2330 train_time:106291ms step_avg:58.37ms
step:1822/2330 train_time:106352ms step_avg:58.37ms
step:1823/2330 train_time:106410ms step_avg:58.37ms
step:1824/2330 train_time:106469ms step_avg:58.37ms
step:1825/2330 train_time:106526ms step_avg:58.37ms
step:1826/2330 train_time:106587ms step_avg:58.37ms
step:1827/2330 train_time:106644ms step_avg:58.37ms
step:1828/2330 train_time:106706ms step_avg:58.37ms
step:1829/2330 train_time:106763ms step_avg:58.37ms
step:1830/2330 train_time:106823ms step_avg:58.37ms
step:1831/2330 train_time:106880ms step_avg:58.37ms
step:1832/2330 train_time:106943ms step_avg:58.37ms
step:1833/2330 train_time:107000ms step_avg:58.37ms
step:1834/2330 train_time:107060ms step_avg:58.38ms
step:1835/2330 train_time:107118ms step_avg:58.37ms
step:1836/2330 train_time:107179ms step_avg:58.38ms
step:1837/2330 train_time:107237ms step_avg:58.38ms
step:1838/2330 train_time:107298ms step_avg:58.38ms
step:1839/2330 train_time:107356ms step_avg:58.38ms
step:1840/2330 train_time:107416ms step_avg:58.38ms
step:1841/2330 train_time:107475ms step_avg:58.38ms
step:1842/2330 train_time:107535ms step_avg:58.38ms
step:1843/2330 train_time:107594ms step_avg:58.38ms
step:1844/2330 train_time:107654ms step_avg:58.38ms
step:1845/2330 train_time:107713ms step_avg:58.38ms
step:1846/2330 train_time:107773ms step_avg:58.38ms
step:1847/2330 train_time:107832ms step_avg:58.38ms
step:1848/2330 train_time:107891ms step_avg:58.38ms
step:1849/2330 train_time:107948ms step_avg:58.38ms
step:1850/2330 train_time:108009ms step_avg:58.38ms
step:1851/2330 train_time:108067ms step_avg:58.38ms
step:1852/2330 train_time:108127ms step_avg:58.38ms
step:1853/2330 train_time:108184ms step_avg:58.38ms
step:1854/2330 train_time:108246ms step_avg:58.38ms
step:1855/2330 train_time:108302ms step_avg:58.38ms
step:1856/2330 train_time:108365ms step_avg:58.39ms
step:1857/2330 train_time:108422ms step_avg:58.39ms
step:1858/2330 train_time:108483ms step_avg:58.39ms
step:1859/2330 train_time:108540ms step_avg:58.39ms
step:1860/2330 train_time:108603ms step_avg:58.39ms
step:1861/2330 train_time:108660ms step_avg:58.39ms
step:1862/2330 train_time:108721ms step_avg:58.39ms
step:1863/2330 train_time:108779ms step_avg:58.39ms
step:1864/2330 train_time:108840ms step_avg:58.39ms
step:1865/2330 train_time:108897ms step_avg:58.39ms
step:1866/2330 train_time:108957ms step_avg:58.39ms
step:1867/2330 train_time:109016ms step_avg:58.39ms
step:1868/2330 train_time:109076ms step_avg:58.39ms
step:1869/2330 train_time:109133ms step_avg:58.39ms
step:1870/2330 train_time:109194ms step_avg:58.39ms
step:1871/2330 train_time:109251ms step_avg:58.39ms
step:1872/2330 train_time:109312ms step_avg:58.39ms
step:1873/2330 train_time:109370ms step_avg:58.39ms
step:1874/2330 train_time:109430ms step_avg:58.39ms
step:1875/2330 train_time:109487ms step_avg:58.39ms
step:1876/2330 train_time:109549ms step_avg:58.40ms
step:1877/2330 train_time:109607ms step_avg:58.39ms
step:1878/2330 train_time:109668ms step_avg:58.40ms
step:1879/2330 train_time:109725ms step_avg:58.40ms
step:1880/2330 train_time:109786ms step_avg:58.40ms
step:1881/2330 train_time:109843ms step_avg:58.40ms
step:1882/2330 train_time:109904ms step_avg:58.40ms
step:1883/2330 train_time:109961ms step_avg:58.40ms
step:1884/2330 train_time:110021ms step_avg:58.40ms
step:1885/2330 train_time:110078ms step_avg:58.40ms
step:1886/2330 train_time:110141ms step_avg:58.40ms
step:1887/2330 train_time:110198ms step_avg:58.40ms
step:1888/2330 train_time:110260ms step_avg:58.40ms
step:1889/2330 train_time:110317ms step_avg:58.40ms
step:1890/2330 train_time:110379ms step_avg:58.40ms
step:1891/2330 train_time:110436ms step_avg:58.40ms
step:1892/2330 train_time:110498ms step_avg:58.40ms
step:1893/2330 train_time:110556ms step_avg:58.40ms
step:1894/2330 train_time:110616ms step_avg:58.40ms
step:1895/2330 train_time:110674ms step_avg:58.40ms
step:1896/2330 train_time:110734ms step_avg:58.40ms
step:1897/2330 train_time:110792ms step_avg:58.40ms
step:1898/2330 train_time:110852ms step_avg:58.40ms
step:1899/2330 train_time:110910ms step_avg:58.40ms
step:1900/2330 train_time:110971ms step_avg:58.41ms
step:1901/2330 train_time:111030ms step_avg:58.41ms
step:1902/2330 train_time:111089ms step_avg:58.41ms
step:1903/2330 train_time:111147ms step_avg:58.41ms
step:1904/2330 train_time:111208ms step_avg:58.41ms
step:1905/2330 train_time:111266ms step_avg:58.41ms
step:1906/2330 train_time:111327ms step_avg:58.41ms
step:1907/2330 train_time:111384ms step_avg:58.41ms
step:1908/2330 train_time:111446ms step_avg:58.41ms
step:1909/2330 train_time:111502ms step_avg:58.41ms
step:1910/2330 train_time:111564ms step_avg:58.41ms
step:1911/2330 train_time:111621ms step_avg:58.41ms
step:1912/2330 train_time:111682ms step_avg:58.41ms
step:1913/2330 train_time:111738ms step_avg:58.41ms
step:1914/2330 train_time:111801ms step_avg:58.41ms
step:1915/2330 train_time:111858ms step_avg:58.41ms
step:1916/2330 train_time:111920ms step_avg:58.41ms
step:1917/2330 train_time:111978ms step_avg:58.41ms
step:1918/2330 train_time:112038ms step_avg:58.41ms
step:1919/2330 train_time:112096ms step_avg:58.41ms
step:1920/2330 train_time:112156ms step_avg:58.41ms
step:1921/2330 train_time:112214ms step_avg:58.41ms
step:1922/2330 train_time:112274ms step_avg:58.42ms
step:1923/2330 train_time:112332ms step_avg:58.42ms
step:1924/2330 train_time:112393ms step_avg:58.42ms
step:1925/2330 train_time:112451ms step_avg:58.42ms
step:1926/2330 train_time:112512ms step_avg:58.42ms
step:1927/2330 train_time:112570ms step_avg:58.42ms
step:1928/2330 train_time:112630ms step_avg:58.42ms
step:1929/2330 train_time:112688ms step_avg:58.42ms
step:1930/2330 train_time:112748ms step_avg:58.42ms
step:1931/2330 train_time:112806ms step_avg:58.42ms
step:1932/2330 train_time:112867ms step_avg:58.42ms
step:1933/2330 train_time:112924ms step_avg:58.42ms
step:1934/2330 train_time:112984ms step_avg:58.42ms
step:1935/2330 train_time:113041ms step_avg:58.42ms
step:1936/2330 train_time:113104ms step_avg:58.42ms
step:1937/2330 train_time:113160ms step_avg:58.42ms
step:1938/2330 train_time:113222ms step_avg:58.42ms
step:1939/2330 train_time:113279ms step_avg:58.42ms
step:1940/2330 train_time:113342ms step_avg:58.42ms
step:1941/2330 train_time:113399ms step_avg:58.42ms
step:1942/2330 train_time:113461ms step_avg:58.42ms
step:1943/2330 train_time:113518ms step_avg:58.42ms
step:1944/2330 train_time:113580ms step_avg:58.43ms
step:1945/2330 train_time:113637ms step_avg:58.42ms
step:1946/2330 train_time:113699ms step_avg:58.43ms
step:1947/2330 train_time:113756ms step_avg:58.43ms
step:1948/2330 train_time:113817ms step_avg:58.43ms
step:1949/2330 train_time:113875ms step_avg:58.43ms
step:1950/2330 train_time:113935ms step_avg:58.43ms
step:1951/2330 train_time:113993ms step_avg:58.43ms
step:1952/2330 train_time:114053ms step_avg:58.43ms
step:1953/2330 train_time:114111ms step_avg:58.43ms
step:1954/2330 train_time:114172ms step_avg:58.43ms
step:1955/2330 train_time:114230ms step_avg:58.43ms
step:1956/2330 train_time:114290ms step_avg:58.43ms
step:1957/2330 train_time:114347ms step_avg:58.43ms
step:1958/2330 train_time:114409ms step_avg:58.43ms
step:1959/2330 train_time:114466ms step_avg:58.43ms
step:1960/2330 train_time:114527ms step_avg:58.43ms
step:1961/2330 train_time:114584ms step_avg:58.43ms
step:1962/2330 train_time:114645ms step_avg:58.43ms
step:1963/2330 train_time:114701ms step_avg:58.43ms
step:1964/2330 train_time:114765ms step_avg:58.43ms
step:1965/2330 train_time:114821ms step_avg:58.43ms
step:1966/2330 train_time:114883ms step_avg:58.43ms
step:1967/2330 train_time:114940ms step_avg:58.43ms
step:1968/2330 train_time:115002ms step_avg:58.44ms
step:1969/2330 train_time:115058ms step_avg:58.43ms
step:1970/2330 train_time:115121ms step_avg:58.44ms
step:1971/2330 train_time:115179ms step_avg:58.44ms
step:1972/2330 train_time:115239ms step_avg:58.44ms
step:1973/2330 train_time:115296ms step_avg:58.44ms
step:1974/2330 train_time:115357ms step_avg:58.44ms
step:1975/2330 train_time:115416ms step_avg:58.44ms
step:1976/2330 train_time:115477ms step_avg:58.44ms
step:1977/2330 train_time:115534ms step_avg:58.44ms
step:1978/2330 train_time:115594ms step_avg:58.44ms
step:1979/2330 train_time:115652ms step_avg:58.44ms
step:1980/2330 train_time:115712ms step_avg:58.44ms
step:1981/2330 train_time:115771ms step_avg:58.44ms
step:1982/2330 train_time:115832ms step_avg:58.44ms
step:1983/2330 train_time:115890ms step_avg:58.44ms
step:1984/2330 train_time:115950ms step_avg:58.44ms
step:1985/2330 train_time:116007ms step_avg:58.44ms
step:1986/2330 train_time:116069ms step_avg:58.44ms
step:1987/2330 train_time:116126ms step_avg:58.44ms
step:1988/2330 train_time:116187ms step_avg:58.44ms
step:1989/2330 train_time:116244ms step_avg:58.44ms
step:1990/2330 train_time:116305ms step_avg:58.44ms
step:1991/2330 train_time:116362ms step_avg:58.44ms
step:1992/2330 train_time:116423ms step_avg:58.45ms
step:1993/2330 train_time:116481ms step_avg:58.45ms
step:1994/2330 train_time:116543ms step_avg:58.45ms
step:1995/2330 train_time:116600ms step_avg:58.45ms
step:1996/2330 train_time:116662ms step_avg:58.45ms
step:1997/2330 train_time:116719ms step_avg:58.45ms
step:1998/2330 train_time:116782ms step_avg:58.45ms
step:1999/2330 train_time:116838ms step_avg:58.45ms
step:2000/2330 train_time:116901ms step_avg:58.45ms
step:2000/2330 val_loss:3.7635 train_time:116982ms step_avg:58.49ms
step:2001/2330 train_time:117002ms step_avg:58.47ms
step:2002/2330 train_time:117022ms step_avg:58.45ms
step:2003/2330 train_time:117082ms step_avg:58.45ms
step:2004/2330 train_time:117147ms step_avg:58.46ms
step:2005/2330 train_time:117205ms step_avg:58.46ms
step:2006/2330 train_time:117265ms step_avg:58.46ms
step:2007/2330 train_time:117323ms step_avg:58.46ms
step:2008/2330 train_time:117383ms step_avg:58.46ms
step:2009/2330 train_time:117440ms step_avg:58.46ms
step:2010/2330 train_time:117501ms step_avg:58.46ms
step:2011/2330 train_time:117557ms step_avg:58.46ms
step:2012/2330 train_time:117617ms step_avg:58.46ms
step:2013/2330 train_time:117674ms step_avg:58.46ms
step:2014/2330 train_time:117734ms step_avg:58.46ms
step:2015/2330 train_time:117790ms step_avg:58.46ms
step:2016/2330 train_time:117851ms step_avg:58.46ms
step:2017/2330 train_time:117907ms step_avg:58.46ms
step:2018/2330 train_time:117969ms step_avg:58.46ms
step:2019/2330 train_time:118027ms step_avg:58.46ms
step:2020/2330 train_time:118093ms step_avg:58.46ms
step:2021/2330 train_time:118151ms step_avg:58.46ms
step:2022/2330 train_time:118215ms step_avg:58.46ms
step:2023/2330 train_time:118272ms step_avg:58.46ms
step:2024/2330 train_time:118334ms step_avg:58.47ms
step:2025/2330 train_time:118391ms step_avg:58.46ms
step:2026/2330 train_time:118452ms step_avg:58.47ms
step:2027/2330 train_time:118509ms step_avg:58.47ms
step:2028/2330 train_time:118568ms step_avg:58.47ms
step:2029/2330 train_time:118625ms step_avg:58.46ms
step:2030/2330 train_time:118685ms step_avg:58.47ms
step:2031/2330 train_time:118742ms step_avg:58.46ms
step:2032/2330 train_time:118802ms step_avg:58.47ms
step:2033/2330 train_time:118860ms step_avg:58.47ms
step:2034/2330 train_time:118919ms step_avg:58.47ms
step:2035/2330 train_time:118978ms step_avg:58.47ms
step:2036/2330 train_time:119038ms step_avg:58.47ms
step:2037/2330 train_time:119098ms step_avg:58.47ms
step:2038/2330 train_time:119159ms step_avg:58.47ms
step:2039/2330 train_time:119217ms step_avg:58.47ms
step:2040/2330 train_time:119278ms step_avg:58.47ms
step:2041/2330 train_time:119336ms step_avg:58.47ms
step:2042/2330 train_time:119396ms step_avg:58.47ms
step:2043/2330 train_time:119454ms step_avg:58.47ms
step:2044/2330 train_time:119513ms step_avg:58.47ms
step:2045/2330 train_time:119571ms step_avg:58.47ms
step:2046/2330 train_time:119631ms step_avg:58.47ms
step:2047/2330 train_time:119688ms step_avg:58.47ms
step:2048/2330 train_time:119749ms step_avg:58.47ms
step:2049/2330 train_time:119806ms step_avg:58.47ms
step:2050/2330 train_time:119866ms step_avg:58.47ms
step:2051/2330 train_time:119923ms step_avg:58.47ms
step:2052/2330 train_time:119985ms step_avg:58.47ms
step:2053/2330 train_time:120043ms step_avg:58.47ms
step:2054/2330 train_time:120106ms step_avg:58.47ms
step:2055/2330 train_time:120163ms step_avg:58.47ms
step:2056/2330 train_time:120227ms step_avg:58.48ms
step:2057/2330 train_time:120285ms step_avg:58.48ms
step:2058/2330 train_time:120346ms step_avg:58.48ms
step:2059/2330 train_time:120403ms step_avg:58.48ms
step:2060/2330 train_time:120465ms step_avg:58.48ms
step:2061/2330 train_time:120522ms step_avg:58.48ms
step:2062/2330 train_time:120582ms step_avg:58.48ms
step:2063/2330 train_time:120640ms step_avg:58.48ms
step:2064/2330 train_time:120700ms step_avg:58.48ms
step:2065/2330 train_time:120757ms step_avg:58.48ms
step:2066/2330 train_time:120817ms step_avg:58.48ms
step:2067/2330 train_time:120875ms step_avg:58.48ms
step:2068/2330 train_time:120935ms step_avg:58.48ms
step:2069/2330 train_time:120994ms step_avg:58.48ms
step:2070/2330 train_time:121055ms step_avg:58.48ms
step:2071/2330 train_time:121112ms step_avg:58.48ms
step:2072/2330 train_time:121173ms step_avg:58.48ms
step:2073/2330 train_time:121231ms step_avg:58.48ms
step:2074/2330 train_time:121293ms step_avg:58.48ms
step:2075/2330 train_time:121350ms step_avg:58.48ms
step:2076/2330 train_time:121411ms step_avg:58.48ms
step:2077/2330 train_time:121468ms step_avg:58.48ms
step:2078/2330 train_time:121528ms step_avg:58.48ms
step:2079/2330 train_time:121585ms step_avg:58.48ms
step:2080/2330 train_time:121646ms step_avg:58.48ms
step:2081/2330 train_time:121702ms step_avg:58.48ms
step:2082/2330 train_time:121765ms step_avg:58.48ms
step:2083/2330 train_time:121822ms step_avg:58.48ms
step:2084/2330 train_time:121883ms step_avg:58.49ms
step:2085/2330 train_time:121940ms step_avg:58.48ms
step:2086/2330 train_time:122001ms step_avg:58.49ms
step:2087/2330 train_time:122060ms step_avg:58.49ms
step:2088/2330 train_time:122120ms step_avg:58.49ms
step:2089/2330 train_time:122179ms step_avg:58.49ms
step:2090/2330 train_time:122239ms step_avg:58.49ms
step:2091/2330 train_time:122297ms step_avg:58.49ms
step:2092/2330 train_time:122358ms step_avg:58.49ms
step:2093/2330 train_time:122416ms step_avg:58.49ms
step:2094/2330 train_time:122476ms step_avg:58.49ms
step:2095/2330 train_time:122535ms step_avg:58.49ms
step:2096/2330 train_time:122595ms step_avg:58.49ms
step:2097/2330 train_time:122652ms step_avg:58.49ms
step:2098/2330 train_time:122712ms step_avg:58.49ms
step:2099/2330 train_time:122769ms step_avg:58.49ms
step:2100/2330 train_time:122830ms step_avg:58.49ms
step:2101/2330 train_time:122888ms step_avg:58.49ms
step:2102/2330 train_time:122949ms step_avg:58.49ms
step:2103/2330 train_time:123006ms step_avg:58.49ms
step:2104/2330 train_time:123067ms step_avg:58.49ms
step:2105/2330 train_time:123124ms step_avg:58.49ms
step:2106/2330 train_time:123185ms step_avg:58.49ms
step:2107/2330 train_time:123242ms step_avg:58.49ms
step:2108/2330 train_time:123305ms step_avg:58.49ms
step:2109/2330 train_time:123363ms step_avg:58.49ms
step:2110/2330 train_time:123424ms step_avg:58.49ms
step:2111/2330 train_time:123482ms step_avg:58.49ms
step:2112/2330 train_time:123542ms step_avg:58.50ms
step:2113/2330 train_time:123600ms step_avg:58.49ms
step:2114/2330 train_time:123660ms step_avg:58.50ms
step:2115/2330 train_time:123718ms step_avg:58.50ms
step:2116/2330 train_time:123778ms step_avg:58.50ms
step:2117/2330 train_time:123836ms step_avg:58.50ms
step:2118/2330 train_time:123895ms step_avg:58.50ms
step:2119/2330 train_time:123953ms step_avg:58.50ms
step:2120/2330 train_time:124014ms step_avg:58.50ms
step:2121/2330 train_time:124071ms step_avg:58.50ms
step:2122/2330 train_time:124131ms step_avg:58.50ms
step:2123/2330 train_time:124189ms step_avg:58.50ms
step:2124/2330 train_time:124251ms step_avg:58.50ms
step:2125/2330 train_time:124309ms step_avg:58.50ms
step:2126/2330 train_time:124370ms step_avg:58.50ms
step:2127/2330 train_time:124426ms step_avg:58.50ms
step:2128/2330 train_time:124488ms step_avg:58.50ms
step:2129/2330 train_time:124545ms step_avg:58.50ms
step:2130/2330 train_time:124606ms step_avg:58.50ms
step:2131/2330 train_time:124663ms step_avg:58.50ms
step:2132/2330 train_time:124726ms step_avg:58.50ms
step:2133/2330 train_time:124783ms step_avg:58.50ms
step:2134/2330 train_time:124845ms step_avg:58.50ms
step:2135/2330 train_time:124902ms step_avg:58.50ms
step:2136/2330 train_time:124963ms step_avg:58.50ms
step:2137/2330 train_time:125020ms step_avg:58.50ms
step:2138/2330 train_time:125082ms step_avg:58.50ms
step:2139/2330 train_time:125140ms step_avg:58.50ms
step:2140/2330 train_time:125200ms step_avg:58.50ms
step:2141/2330 train_time:125258ms step_avg:58.50ms
step:2142/2330 train_time:125319ms step_avg:58.51ms
step:2143/2330 train_time:125377ms step_avg:58.51ms
step:2144/2330 train_time:125437ms step_avg:58.51ms
step:2145/2330 train_time:125496ms step_avg:58.51ms
step:2146/2330 train_time:125556ms step_avg:58.51ms
step:2147/2330 train_time:125615ms step_avg:58.51ms
step:2148/2330 train_time:125675ms step_avg:58.51ms
step:2149/2330 train_time:125734ms step_avg:58.51ms
step:2150/2330 train_time:125793ms step_avg:58.51ms
step:2151/2330 train_time:125851ms step_avg:58.51ms
step:2152/2330 train_time:125911ms step_avg:58.51ms
step:2153/2330 train_time:125968ms step_avg:58.51ms
step:2154/2330 train_time:126028ms step_avg:58.51ms
step:2155/2330 train_time:126086ms step_avg:58.51ms
step:2156/2330 train_time:126146ms step_avg:58.51ms
step:2157/2330 train_time:126204ms step_avg:58.51ms
step:2158/2330 train_time:126265ms step_avg:58.51ms
step:2159/2330 train_time:126322ms step_avg:58.51ms
step:2160/2330 train_time:126384ms step_avg:58.51ms
step:2161/2330 train_time:126441ms step_avg:58.51ms
step:2162/2330 train_time:126502ms step_avg:58.51ms
step:2163/2330 train_time:126560ms step_avg:58.51ms
step:2164/2330 train_time:126621ms step_avg:58.51ms
step:2165/2330 train_time:126678ms step_avg:58.51ms
step:2166/2330 train_time:126739ms step_avg:58.51ms
step:2167/2330 train_time:126797ms step_avg:58.51ms
step:2168/2330 train_time:126857ms step_avg:58.51ms
step:2169/2330 train_time:126915ms step_avg:58.51ms
step:2170/2330 train_time:126976ms step_avg:58.51ms
step:2171/2330 train_time:127034ms step_avg:58.51ms
step:2172/2330 train_time:127094ms step_avg:58.51ms
step:2173/2330 train_time:127153ms step_avg:58.52ms
step:2174/2330 train_time:127213ms step_avg:58.52ms
step:2175/2330 train_time:127271ms step_avg:58.52ms
step:2176/2330 train_time:127332ms step_avg:58.52ms
step:2177/2330 train_time:127389ms step_avg:58.52ms
step:2178/2330 train_time:127450ms step_avg:58.52ms
step:2179/2330 train_time:127507ms step_avg:58.52ms
step:2180/2330 train_time:127569ms step_avg:58.52ms
step:2181/2330 train_time:127625ms step_avg:58.52ms
step:2182/2330 train_time:127687ms step_avg:58.52ms
step:2183/2330 train_time:127744ms step_avg:58.52ms
step:2184/2330 train_time:127805ms step_avg:58.52ms
step:2185/2330 train_time:127861ms step_avg:58.52ms
step:2186/2330 train_time:127923ms step_avg:58.52ms
step:2187/2330 train_time:127980ms step_avg:58.52ms
step:2188/2330 train_time:128041ms step_avg:58.52ms
step:2189/2330 train_time:128099ms step_avg:58.52ms
step:2190/2330 train_time:128159ms step_avg:58.52ms
step:2191/2330 train_time:128217ms step_avg:58.52ms
step:2192/2330 train_time:128278ms step_avg:58.52ms
step:2193/2330 train_time:128336ms step_avg:58.52ms
step:2194/2330 train_time:128397ms step_avg:58.52ms
step:2195/2330 train_time:128456ms step_avg:58.52ms
step:2196/2330 train_time:128516ms step_avg:58.52ms
step:2197/2330 train_time:128574ms step_avg:58.52ms
step:2198/2330 train_time:128635ms step_avg:58.52ms
step:2199/2330 train_time:128693ms step_avg:58.52ms
step:2200/2330 train_time:128753ms step_avg:58.52ms
step:2201/2330 train_time:128810ms step_avg:58.52ms
step:2202/2330 train_time:128872ms step_avg:58.52ms
step:2203/2330 train_time:128928ms step_avg:58.52ms
step:2204/2330 train_time:128990ms step_avg:58.53ms
step:2205/2330 train_time:129046ms step_avg:58.52ms
step:2206/2330 train_time:129107ms step_avg:58.53ms
step:2207/2330 train_time:129163ms step_avg:58.52ms
step:2208/2330 train_time:129225ms step_avg:58.53ms
step:2209/2330 train_time:129282ms step_avg:58.53ms
step:2210/2330 train_time:129346ms step_avg:58.53ms
step:2211/2330 train_time:129402ms step_avg:58.53ms
step:2212/2330 train_time:129465ms step_avg:58.53ms
step:2213/2330 train_time:129522ms step_avg:58.53ms
step:2214/2330 train_time:129584ms step_avg:58.53ms
step:2215/2330 train_time:129641ms step_avg:58.53ms
step:2216/2330 train_time:129702ms step_avg:58.53ms
step:2217/2330 train_time:129766ms step_avg:58.53ms
step:2218/2330 train_time:129821ms step_avg:58.53ms
step:2219/2330 train_time:129878ms step_avg:58.53ms
step:2220/2330 train_time:129938ms step_avg:58.53ms
step:2221/2330 train_time:129996ms step_avg:58.53ms
step:2222/2330 train_time:130057ms step_avg:58.53ms
step:2223/2330 train_time:130115ms step_avg:58.53ms
step:2224/2330 train_time:130175ms step_avg:58.53ms
step:2225/2330 train_time:130233ms step_avg:58.53ms
step:2226/2330 train_time:130295ms step_avg:58.53ms
step:2227/2330 train_time:130352ms step_avg:58.53ms
step:2228/2330 train_time:130414ms step_avg:58.53ms
step:2229/2330 train_time:130471ms step_avg:58.53ms
step:2230/2330 train_time:130531ms step_avg:58.53ms
step:2231/2330 train_time:130588ms step_avg:58.53ms
step:2232/2330 train_time:130651ms step_avg:58.54ms
step:2233/2330 train_time:130708ms step_avg:58.53ms
step:2234/2330 train_time:130769ms step_avg:58.54ms
step:2235/2330 train_time:130825ms step_avg:58.53ms
step:2236/2330 train_time:130887ms step_avg:58.54ms
step:2237/2330 train_time:130944ms step_avg:58.54ms
step:2238/2330 train_time:131005ms step_avg:58.54ms
step:2239/2330 train_time:131062ms step_avg:58.54ms
step:2240/2330 train_time:131124ms step_avg:58.54ms
step:2241/2330 train_time:131181ms step_avg:58.54ms
step:2242/2330 train_time:131243ms step_avg:58.54ms
step:2243/2330 train_time:131300ms step_avg:58.54ms
step:2244/2330 train_time:131362ms step_avg:58.54ms
step:2245/2330 train_time:131419ms step_avg:58.54ms
step:2246/2330 train_time:131480ms step_avg:58.54ms
step:2247/2330 train_time:131538ms step_avg:58.54ms
step:2248/2330 train_time:131599ms step_avg:58.54ms
step:2249/2330 train_time:131657ms step_avg:58.54ms
step:2250/2330 train_time:131717ms step_avg:58.54ms
step:2250/2330 val_loss:3.7048 train_time:131798ms step_avg:58.58ms
step:2251/2330 train_time:131818ms step_avg:58.56ms
step:2252/2330 train_time:131838ms step_avg:58.54ms
step:2253/2330 train_time:131895ms step_avg:58.54ms
step:2254/2330 train_time:131961ms step_avg:58.55ms
step:2255/2330 train_time:132019ms step_avg:58.55ms
step:2256/2330 train_time:132082ms step_avg:58.55ms
step:2257/2330 train_time:132139ms step_avg:58.55ms
step:2258/2330 train_time:132199ms step_avg:58.55ms
step:2259/2330 train_time:132257ms step_avg:58.55ms
step:2260/2330 train_time:132317ms step_avg:58.55ms
step:2261/2330 train_time:132374ms step_avg:58.55ms
step:2262/2330 train_time:132433ms step_avg:58.55ms
step:2263/2330 train_time:132490ms step_avg:58.55ms
step:2264/2330 train_time:132551ms step_avg:58.55ms
step:2265/2330 train_time:132608ms step_avg:58.55ms
step:2266/2330 train_time:132668ms step_avg:58.55ms
step:2267/2330 train_time:132726ms step_avg:58.55ms
step:2268/2330 train_time:132787ms step_avg:58.55ms
step:2269/2330 train_time:132845ms step_avg:58.55ms
step:2270/2330 train_time:132908ms step_avg:58.55ms
step:2271/2330 train_time:132966ms step_avg:58.55ms
step:2272/2330 train_time:133030ms step_avg:58.55ms
step:2273/2330 train_time:133087ms step_avg:58.55ms
step:2274/2330 train_time:133148ms step_avg:58.55ms
step:2275/2330 train_time:133206ms step_avg:58.55ms
step:2276/2330 train_time:133266ms step_avg:58.55ms
step:2277/2330 train_time:133323ms step_avg:58.55ms
step:2278/2330 train_time:133383ms step_avg:58.55ms
step:2279/2330 train_time:133440ms step_avg:58.55ms
step:2280/2330 train_time:133500ms step_avg:58.55ms
step:2281/2330 train_time:133556ms step_avg:58.55ms
step:2282/2330 train_time:133617ms step_avg:58.55ms
step:2283/2330 train_time:133674ms step_avg:58.55ms
step:2284/2330 train_time:133734ms step_avg:58.55ms
step:2285/2330 train_time:133792ms step_avg:58.55ms
step:2286/2330 train_time:133853ms step_avg:58.55ms
step:2287/2330 train_time:133911ms step_avg:58.55ms
step:2288/2330 train_time:133976ms step_avg:58.56ms
step:2289/2330 train_time:134033ms step_avg:58.56ms
step:2290/2330 train_time:134096ms step_avg:58.56ms
step:2291/2330 train_time:134153ms step_avg:58.56ms
step:2292/2330 train_time:134214ms step_avg:58.56ms
step:2293/2330 train_time:134271ms step_avg:58.56ms
step:2294/2330 train_time:134332ms step_avg:58.56ms
step:2295/2330 train_time:134389ms step_avg:58.56ms
step:2296/2330 train_time:134450ms step_avg:58.56ms
step:2297/2330 train_time:134506ms step_avg:58.56ms
step:2298/2330 train_time:134567ms step_avg:58.56ms
step:2299/2330 train_time:134624ms step_avg:58.56ms
step:2300/2330 train_time:134684ms step_avg:58.56ms
step:2301/2330 train_time:134742ms step_avg:58.56ms
step:2302/2330 train_time:134803ms step_avg:58.56ms
step:2303/2330 train_time:134862ms step_avg:58.56ms
step:2304/2330 train_time:134923ms step_avg:58.56ms
step:2305/2330 train_time:134981ms step_avg:58.56ms
step:2306/2330 train_time:135042ms step_avg:58.56ms
step:2307/2330 train_time:135100ms step_avg:58.56ms
step:2308/2330 train_time:135160ms step_avg:58.56ms
step:2309/2330 train_time:135219ms step_avg:58.56ms
step:2310/2330 train_time:135279ms step_avg:58.56ms
step:2311/2330 train_time:135336ms step_avg:58.56ms
step:2312/2330 train_time:135397ms step_avg:58.56ms
step:2313/2330 train_time:135453ms step_avg:58.56ms
step:2314/2330 train_time:135514ms step_avg:58.56ms
step:2315/2330 train_time:135571ms step_avg:58.56ms
step:2316/2330 train_time:135632ms step_avg:58.56ms
step:2317/2330 train_time:135689ms step_avg:58.56ms
step:2318/2330 train_time:135749ms step_avg:58.56ms
step:2319/2330 train_time:135807ms step_avg:58.56ms
step:2320/2330 train_time:135870ms step_avg:58.56ms
step:2321/2330 train_time:135927ms step_avg:58.56ms
step:2322/2330 train_time:135988ms step_avg:58.57ms
step:2323/2330 train_time:136046ms step_avg:58.56ms
step:2324/2330 train_time:136108ms step_avg:58.57ms
step:2325/2330 train_time:136166ms step_avg:58.57ms
step:2326/2330 train_time:136227ms step_avg:58.57ms
step:2327/2330 train_time:136284ms step_avg:58.57ms
step:2328/2330 train_time:136346ms step_avg:58.57ms
step:2329/2330 train_time:136403ms step_avg:58.57ms
step:2330/2330 train_time:136463ms step_avg:58.57ms
step:2330/2330 val_loss:3.6856 train_time:136544ms step_avg:58.60ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
