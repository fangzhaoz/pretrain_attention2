import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--wd', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_wd_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_wd_tuning/{args2.wd}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=6e-4,  betas=(0.8, 0.8), eps=1e-10, weight_decay=float(args2.wd), fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_wd_tuning/{args2.wd}.txt", exist_ok=True)
            torch.save(log, f"logs_adamw_small_wd_tuning/{args2.wd}.txt/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 06:35:55 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   29C    P0             114W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   26C    P0             116W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   25C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   28C    P0             115W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   27C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2330 train_time:88ms step_avg:87.57ms
step:2/2330 train_time:178ms step_avg:88.80ms
step:3/2330 train_time:197ms step_avg:65.53ms
step:4/2330 train_time:215ms step_avg:53.76ms
step:5/2330 train_time:269ms step_avg:53.74ms
step:6/2330 train_time:326ms step_avg:54.41ms
step:7/2330 train_time:381ms step_avg:54.42ms
step:8/2330 train_time:441ms step_avg:55.08ms
step:9/2330 train_time:496ms step_avg:55.06ms
step:10/2330 train_time:554ms step_avg:55.43ms
step:11/2330 train_time:610ms step_avg:55.45ms
step:12/2330 train_time:668ms step_avg:55.67ms
step:13/2330 train_time:723ms step_avg:55.64ms
step:14/2330 train_time:782ms step_avg:55.85ms
step:15/2330 train_time:837ms step_avg:55.83ms
step:16/2330 train_time:896ms step_avg:55.99ms
step:17/2330 train_time:952ms step_avg:55.98ms
step:18/2330 train_time:1012ms step_avg:56.24ms
step:19/2330 train_time:1072ms step_avg:56.41ms
step:20/2330 train_time:1133ms step_avg:56.64ms
step:21/2330 train_time:1190ms step_avg:56.68ms
step:22/2330 train_time:1249ms step_avg:56.79ms
step:23/2330 train_time:1305ms step_avg:56.75ms
step:24/2330 train_time:1365ms step_avg:56.85ms
step:25/2330 train_time:1420ms step_avg:56.80ms
step:26/2330 train_time:1479ms step_avg:56.89ms
step:27/2330 train_time:1535ms step_avg:56.84ms
step:28/2330 train_time:1593ms step_avg:56.89ms
step:29/2330 train_time:1648ms step_avg:56.83ms
step:30/2330 train_time:1707ms step_avg:56.90ms
step:31/2330 train_time:1762ms step_avg:56.85ms
step:32/2330 train_time:1820ms step_avg:56.89ms
step:33/2330 train_time:1876ms step_avg:56.84ms
step:34/2330 train_time:1934ms step_avg:56.89ms
step:35/2330 train_time:1990ms step_avg:56.87ms
step:36/2330 train_time:2050ms step_avg:56.95ms
step:37/2330 train_time:2107ms step_avg:56.95ms
step:38/2330 train_time:2166ms step_avg:57.01ms
step:39/2330 train_time:2223ms step_avg:57.01ms
step:40/2330 train_time:2284ms step_avg:57.09ms
step:41/2330 train_time:2340ms step_avg:57.07ms
step:42/2330 train_time:2398ms step_avg:57.10ms
step:43/2330 train_time:2454ms step_avg:57.06ms
step:44/2330 train_time:2513ms step_avg:57.10ms
step:45/2330 train_time:2569ms step_avg:57.09ms
step:46/2330 train_time:2628ms step_avg:57.13ms
step:47/2330 train_time:2684ms step_avg:57.10ms
step:48/2330 train_time:2742ms step_avg:57.12ms
step:49/2330 train_time:2797ms step_avg:57.08ms
step:50/2330 train_time:2856ms step_avg:57.12ms
step:51/2330 train_time:2912ms step_avg:57.10ms
step:52/2330 train_time:2970ms step_avg:57.12ms
step:53/2330 train_time:3026ms step_avg:57.10ms
step:54/2330 train_time:3086ms step_avg:57.14ms
step:55/2330 train_time:3142ms step_avg:57.12ms
step:56/2330 train_time:3202ms step_avg:57.17ms
step:57/2330 train_time:3258ms step_avg:57.16ms
step:58/2330 train_time:3319ms step_avg:57.22ms
step:59/2330 train_time:3374ms step_avg:57.19ms
step:60/2330 train_time:3433ms step_avg:57.22ms
step:61/2330 train_time:3489ms step_avg:57.20ms
step:62/2330 train_time:3548ms step_avg:57.23ms
step:63/2330 train_time:3604ms step_avg:57.20ms
step:64/2330 train_time:3662ms step_avg:57.23ms
step:65/2330 train_time:3718ms step_avg:57.20ms
step:66/2330 train_time:3777ms step_avg:57.23ms
step:67/2330 train_time:3832ms step_avg:57.20ms
step:68/2330 train_time:3891ms step_avg:57.21ms
step:69/2330 train_time:3946ms step_avg:57.19ms
step:70/2330 train_time:4006ms step_avg:57.23ms
step:71/2330 train_time:4063ms step_avg:57.22ms
step:72/2330 train_time:4121ms step_avg:57.24ms
step:73/2330 train_time:4178ms step_avg:57.23ms
step:74/2330 train_time:4237ms step_avg:57.25ms
step:75/2330 train_time:4293ms step_avg:57.24ms
step:76/2330 train_time:4352ms step_avg:57.27ms
step:77/2330 train_time:4408ms step_avg:57.24ms
step:78/2330 train_time:4467ms step_avg:57.27ms
step:79/2330 train_time:4522ms step_avg:57.25ms
step:80/2330 train_time:4582ms step_avg:57.28ms
step:81/2330 train_time:4638ms step_avg:57.26ms
step:82/2330 train_time:4697ms step_avg:57.28ms
step:83/2330 train_time:4752ms step_avg:57.26ms
step:84/2330 train_time:4811ms step_avg:57.27ms
step:85/2330 train_time:4866ms step_avg:57.25ms
step:86/2330 train_time:4925ms step_avg:57.27ms
step:87/2330 train_time:4981ms step_avg:57.26ms
step:88/2330 train_time:5040ms step_avg:57.27ms
step:89/2330 train_time:5095ms step_avg:57.25ms
step:90/2330 train_time:5155ms step_avg:57.28ms
step:91/2330 train_time:5211ms step_avg:57.27ms
step:92/2330 train_time:5270ms step_avg:57.28ms
step:93/2330 train_time:5327ms step_avg:57.28ms
step:94/2330 train_time:5386ms step_avg:57.30ms
step:95/2330 train_time:5442ms step_avg:57.28ms
step:96/2330 train_time:5500ms step_avg:57.29ms
step:97/2330 train_time:5556ms step_avg:57.28ms
step:98/2330 train_time:5615ms step_avg:57.30ms
step:99/2330 train_time:5671ms step_avg:57.29ms
step:100/2330 train_time:5730ms step_avg:57.30ms
step:101/2330 train_time:5786ms step_avg:57.29ms
step:102/2330 train_time:5844ms step_avg:57.30ms
step:103/2330 train_time:5900ms step_avg:57.28ms
step:104/2330 train_time:5958ms step_avg:57.29ms
step:105/2330 train_time:6013ms step_avg:57.27ms
step:106/2330 train_time:6072ms step_avg:57.29ms
step:107/2330 train_time:6129ms step_avg:57.28ms
step:108/2330 train_time:6188ms step_avg:57.29ms
step:109/2330 train_time:6243ms step_avg:57.28ms
step:110/2330 train_time:6303ms step_avg:57.30ms
step:111/2330 train_time:6360ms step_avg:57.29ms
step:112/2330 train_time:6420ms step_avg:57.32ms
step:113/2330 train_time:6476ms step_avg:57.31ms
step:114/2330 train_time:6534ms step_avg:57.32ms
step:115/2330 train_time:6590ms step_avg:57.30ms
step:116/2330 train_time:6650ms step_avg:57.32ms
step:117/2330 train_time:6705ms step_avg:57.30ms
step:118/2330 train_time:6764ms step_avg:57.33ms
step:119/2330 train_time:6820ms step_avg:57.31ms
step:120/2330 train_time:6879ms step_avg:57.32ms
step:121/2330 train_time:6934ms step_avg:57.31ms
step:122/2330 train_time:6993ms step_avg:57.32ms
step:123/2330 train_time:7050ms step_avg:57.32ms
step:124/2330 train_time:7109ms step_avg:57.33ms
step:125/2330 train_time:7165ms step_avg:57.32ms
step:126/2330 train_time:7224ms step_avg:57.33ms
step:127/2330 train_time:7279ms step_avg:57.32ms
step:128/2330 train_time:7339ms step_avg:57.34ms
step:129/2330 train_time:7395ms step_avg:57.32ms
step:130/2330 train_time:7455ms step_avg:57.35ms
step:131/2330 train_time:7511ms step_avg:57.33ms
step:132/2330 train_time:7569ms step_avg:57.34ms
step:133/2330 train_time:7625ms step_avg:57.33ms
step:134/2330 train_time:7684ms step_avg:57.34ms
step:135/2330 train_time:7740ms step_avg:57.33ms
step:136/2330 train_time:7798ms step_avg:57.34ms
step:137/2330 train_time:7854ms step_avg:57.33ms
step:138/2330 train_time:7913ms step_avg:57.34ms
step:139/2330 train_time:7969ms step_avg:57.33ms
step:140/2330 train_time:8028ms step_avg:57.34ms
step:141/2330 train_time:8084ms step_avg:57.33ms
step:142/2330 train_time:8142ms step_avg:57.34ms
step:143/2330 train_time:8198ms step_avg:57.33ms
step:144/2330 train_time:8257ms step_avg:57.34ms
step:145/2330 train_time:8313ms step_avg:57.33ms
step:146/2330 train_time:8372ms step_avg:57.34ms
step:147/2330 train_time:8428ms step_avg:57.33ms
step:148/2330 train_time:8487ms step_avg:57.34ms
step:149/2330 train_time:8543ms step_avg:57.33ms
step:150/2330 train_time:8602ms step_avg:57.35ms
step:151/2330 train_time:8658ms step_avg:57.34ms
step:152/2330 train_time:8716ms step_avg:57.34ms
step:153/2330 train_time:8772ms step_avg:57.33ms
step:154/2330 train_time:8831ms step_avg:57.34ms
step:155/2330 train_time:8887ms step_avg:57.33ms
step:156/2330 train_time:8945ms step_avg:57.34ms
step:157/2330 train_time:9001ms step_avg:57.33ms
step:158/2330 train_time:9061ms step_avg:57.35ms
step:159/2330 train_time:9117ms step_avg:57.34ms
step:160/2330 train_time:9176ms step_avg:57.35ms
step:161/2330 train_time:9232ms step_avg:57.34ms
step:162/2330 train_time:9291ms step_avg:57.35ms
step:163/2330 train_time:9346ms step_avg:57.34ms
step:164/2330 train_time:9406ms step_avg:57.35ms
step:165/2330 train_time:9463ms step_avg:57.35ms
step:166/2330 train_time:9521ms step_avg:57.36ms
step:167/2330 train_time:9578ms step_avg:57.35ms
step:168/2330 train_time:9636ms step_avg:57.36ms
step:169/2330 train_time:9693ms step_avg:57.35ms
step:170/2330 train_time:9751ms step_avg:57.36ms
step:171/2330 train_time:9807ms step_avg:57.35ms
step:172/2330 train_time:9866ms step_avg:57.36ms
step:173/2330 train_time:9922ms step_avg:57.35ms
step:174/2330 train_time:9981ms step_avg:57.36ms
step:175/2330 train_time:10036ms step_avg:57.35ms
step:176/2330 train_time:10096ms step_avg:57.36ms
step:177/2330 train_time:10152ms step_avg:57.36ms
step:178/2330 train_time:10211ms step_avg:57.37ms
step:179/2330 train_time:10268ms step_avg:57.36ms
step:180/2330 train_time:10326ms step_avg:57.37ms
step:181/2330 train_time:10382ms step_avg:57.36ms
step:182/2330 train_time:10441ms step_avg:57.37ms
step:183/2330 train_time:10496ms step_avg:57.35ms
step:184/2330 train_time:10556ms step_avg:57.37ms
step:185/2330 train_time:10612ms step_avg:57.36ms
step:186/2330 train_time:10671ms step_avg:57.37ms
step:187/2330 train_time:10727ms step_avg:57.36ms
step:188/2330 train_time:10786ms step_avg:57.37ms
step:189/2330 train_time:10841ms step_avg:57.36ms
step:190/2330 train_time:10900ms step_avg:57.37ms
step:191/2330 train_time:10956ms step_avg:57.36ms
step:192/2330 train_time:11015ms step_avg:57.37ms
step:193/2330 train_time:11071ms step_avg:57.36ms
step:194/2330 train_time:11130ms step_avg:57.37ms
step:195/2330 train_time:11185ms step_avg:57.36ms
step:196/2330 train_time:11245ms step_avg:57.37ms
step:197/2330 train_time:11301ms step_avg:57.36ms
step:198/2330 train_time:11360ms step_avg:57.37ms
step:199/2330 train_time:11415ms step_avg:57.36ms
step:200/2330 train_time:11475ms step_avg:57.37ms
step:201/2330 train_time:11531ms step_avg:57.37ms
step:202/2330 train_time:11590ms step_avg:57.38ms
step:203/2330 train_time:11646ms step_avg:57.37ms
step:204/2330 train_time:11705ms step_avg:57.38ms
step:205/2330 train_time:11760ms step_avg:57.37ms
step:206/2330 train_time:11819ms step_avg:57.37ms
step:207/2330 train_time:11874ms step_avg:57.36ms
step:208/2330 train_time:11934ms step_avg:57.38ms
step:209/2330 train_time:11990ms step_avg:57.37ms
step:210/2330 train_time:12049ms step_avg:57.38ms
step:211/2330 train_time:12105ms step_avg:57.37ms
step:212/2330 train_time:12164ms step_avg:57.38ms
step:213/2330 train_time:12219ms step_avg:57.37ms
step:214/2330 train_time:12279ms step_avg:57.38ms
step:215/2330 train_time:12334ms step_avg:57.37ms
step:216/2330 train_time:12394ms step_avg:57.38ms
step:217/2330 train_time:12450ms step_avg:57.37ms
step:218/2330 train_time:12509ms step_avg:57.38ms
step:219/2330 train_time:12565ms step_avg:57.37ms
step:220/2330 train_time:12623ms step_avg:57.38ms
step:221/2330 train_time:12679ms step_avg:57.37ms
step:222/2330 train_time:12738ms step_avg:57.38ms
step:223/2330 train_time:12794ms step_avg:57.37ms
step:224/2330 train_time:12852ms step_avg:57.38ms
step:225/2330 train_time:12908ms step_avg:57.37ms
step:226/2330 train_time:12968ms step_avg:57.38ms
step:227/2330 train_time:13023ms step_avg:57.37ms
step:228/2330 train_time:13083ms step_avg:57.38ms
step:229/2330 train_time:13139ms step_avg:57.37ms
step:230/2330 train_time:13197ms step_avg:57.38ms
step:231/2330 train_time:13253ms step_avg:57.37ms
step:232/2330 train_time:13313ms step_avg:57.38ms
step:233/2330 train_time:13369ms step_avg:57.38ms
step:234/2330 train_time:13428ms step_avg:57.39ms
step:235/2330 train_time:13485ms step_avg:57.38ms
step:236/2330 train_time:13543ms step_avg:57.39ms
step:237/2330 train_time:13599ms step_avg:57.38ms
step:238/2330 train_time:13658ms step_avg:57.38ms
step:239/2330 train_time:13714ms step_avg:57.38ms
step:240/2330 train_time:13772ms step_avg:57.38ms
step:241/2330 train_time:13829ms step_avg:57.38ms
step:242/2330 train_time:13888ms step_avg:57.39ms
step:243/2330 train_time:13944ms step_avg:57.38ms
step:244/2330 train_time:14003ms step_avg:57.39ms
step:245/2330 train_time:14058ms step_avg:57.38ms
step:246/2330 train_time:14117ms step_avg:57.39ms
step:247/2330 train_time:14173ms step_avg:57.38ms
step:248/2330 train_time:14233ms step_avg:57.39ms
step:249/2330 train_time:14289ms step_avg:57.38ms
step:250/2330 train_time:14347ms step_avg:57.39ms
step:250/2330 val_loss:4.8830 train_time:14427ms step_avg:57.71ms
step:251/2330 train_time:14445ms step_avg:57.55ms
step:252/2330 train_time:14465ms step_avg:57.40ms
step:253/2330 train_time:14520ms step_avg:57.39ms
step:254/2330 train_time:14586ms step_avg:57.43ms
step:255/2330 train_time:14642ms step_avg:57.42ms
step:256/2330 train_time:14705ms step_avg:57.44ms
step:257/2330 train_time:14760ms step_avg:57.43ms
step:258/2330 train_time:14821ms step_avg:57.45ms
step:259/2330 train_time:14877ms step_avg:57.44ms
step:260/2330 train_time:14936ms step_avg:57.45ms
step:261/2330 train_time:14991ms step_avg:57.44ms
step:262/2330 train_time:15049ms step_avg:57.44ms
step:263/2330 train_time:15105ms step_avg:57.43ms
step:264/2330 train_time:15163ms step_avg:57.43ms
step:265/2330 train_time:15218ms step_avg:57.43ms
step:266/2330 train_time:15276ms step_avg:57.43ms
step:267/2330 train_time:15332ms step_avg:57.42ms
step:268/2330 train_time:15391ms step_avg:57.43ms
step:269/2330 train_time:15449ms step_avg:57.43ms
step:270/2330 train_time:15508ms step_avg:57.44ms
step:271/2330 train_time:15564ms step_avg:57.43ms
step:272/2330 train_time:15625ms step_avg:57.44ms
step:273/2330 train_time:15681ms step_avg:57.44ms
step:274/2330 train_time:15742ms step_avg:57.45ms
step:275/2330 train_time:15797ms step_avg:57.44ms
step:276/2330 train_time:15857ms step_avg:57.45ms
step:277/2330 train_time:15913ms step_avg:57.45ms
step:278/2330 train_time:15972ms step_avg:57.45ms
step:279/2330 train_time:16028ms step_avg:57.45ms
step:280/2330 train_time:16087ms step_avg:57.45ms
step:281/2330 train_time:16143ms step_avg:57.45ms
step:282/2330 train_time:16201ms step_avg:57.45ms
step:283/2330 train_time:16257ms step_avg:57.45ms
step:284/2330 train_time:16316ms step_avg:57.45ms
step:285/2330 train_time:16372ms step_avg:57.45ms
step:286/2330 train_time:16431ms step_avg:57.45ms
step:287/2330 train_time:16487ms step_avg:57.45ms
step:288/2330 train_time:16547ms step_avg:57.45ms
step:289/2330 train_time:16604ms step_avg:57.45ms
step:290/2330 train_time:16663ms step_avg:57.46ms
step:291/2330 train_time:16719ms step_avg:57.45ms
step:292/2330 train_time:16778ms step_avg:57.46ms
step:293/2330 train_time:16834ms step_avg:57.46ms
step:294/2330 train_time:16893ms step_avg:57.46ms
step:295/2330 train_time:16949ms step_avg:57.46ms
step:296/2330 train_time:17008ms step_avg:57.46ms
step:297/2330 train_time:17064ms step_avg:57.45ms
step:298/2330 train_time:17123ms step_avg:57.46ms
step:299/2330 train_time:17179ms step_avg:57.45ms
step:300/2330 train_time:17237ms step_avg:57.46ms
step:301/2330 train_time:17293ms step_avg:57.45ms
step:302/2330 train_time:17351ms step_avg:57.45ms
step:303/2330 train_time:17408ms step_avg:57.45ms
step:304/2330 train_time:17466ms step_avg:57.45ms
step:305/2330 train_time:17522ms step_avg:57.45ms
step:306/2330 train_time:17581ms step_avg:57.46ms
step:307/2330 train_time:17637ms step_avg:57.45ms
step:308/2330 train_time:17697ms step_avg:57.46ms
step:309/2330 train_time:17753ms step_avg:57.45ms
step:310/2330 train_time:17812ms step_avg:57.46ms
step:311/2330 train_time:17868ms step_avg:57.45ms
step:312/2330 train_time:17927ms step_avg:57.46ms
step:313/2330 train_time:17983ms step_avg:57.45ms
step:314/2330 train_time:18042ms step_avg:57.46ms
step:315/2330 train_time:18097ms step_avg:57.45ms
step:316/2330 train_time:18156ms step_avg:57.45ms
step:317/2330 train_time:18211ms step_avg:57.45ms
step:318/2330 train_time:18271ms step_avg:57.45ms
step:319/2330 train_time:18326ms step_avg:57.45ms
step:320/2330 train_time:18385ms step_avg:57.45ms
step:321/2330 train_time:18440ms step_avg:57.45ms
step:322/2330 train_time:18499ms step_avg:57.45ms
step:323/2330 train_time:18556ms step_avg:57.45ms
step:324/2330 train_time:18616ms step_avg:57.46ms
step:325/2330 train_time:18672ms step_avg:57.45ms
step:326/2330 train_time:18732ms step_avg:57.46ms
step:327/2330 train_time:18787ms step_avg:57.45ms
step:328/2330 train_time:18847ms step_avg:57.46ms
step:329/2330 train_time:18904ms step_avg:57.46ms
step:330/2330 train_time:18962ms step_avg:57.46ms
step:331/2330 train_time:19018ms step_avg:57.46ms
step:332/2330 train_time:19077ms step_avg:57.46ms
step:333/2330 train_time:19132ms step_avg:57.45ms
step:334/2330 train_time:19191ms step_avg:57.46ms
step:335/2330 train_time:19247ms step_avg:57.46ms
step:336/2330 train_time:19306ms step_avg:57.46ms
step:337/2330 train_time:19361ms step_avg:57.45ms
step:338/2330 train_time:19420ms step_avg:57.46ms
step:339/2330 train_time:19476ms step_avg:57.45ms
step:340/2330 train_time:19536ms step_avg:57.46ms
step:341/2330 train_time:19592ms step_avg:57.46ms
step:342/2330 train_time:19651ms step_avg:57.46ms
step:343/2330 train_time:19707ms step_avg:57.46ms
step:344/2330 train_time:19766ms step_avg:57.46ms
step:345/2330 train_time:19822ms step_avg:57.46ms
step:346/2330 train_time:19882ms step_avg:57.46ms
step:347/2330 train_time:19938ms step_avg:57.46ms
step:348/2330 train_time:19997ms step_avg:57.46ms
step:349/2330 train_time:20053ms step_avg:57.46ms
step:350/2330 train_time:20112ms step_avg:57.46ms
step:351/2330 train_time:20167ms step_avg:57.46ms
step:352/2330 train_time:20226ms step_avg:57.46ms
step:353/2330 train_time:20282ms step_avg:57.46ms
step:354/2330 train_time:20341ms step_avg:57.46ms
step:355/2330 train_time:20397ms step_avg:57.46ms
step:356/2330 train_time:20456ms step_avg:57.46ms
step:357/2330 train_time:20512ms step_avg:57.46ms
step:358/2330 train_time:20571ms step_avg:57.46ms
step:359/2330 train_time:20627ms step_avg:57.46ms
step:360/2330 train_time:20687ms step_avg:57.46ms
step:361/2330 train_time:20742ms step_avg:57.46ms
step:362/2330 train_time:20802ms step_avg:57.47ms
step:363/2330 train_time:20858ms step_avg:57.46ms
step:364/2330 train_time:20919ms step_avg:57.47ms
step:365/2330 train_time:20974ms step_avg:57.46ms
step:366/2330 train_time:21035ms step_avg:57.47ms
step:367/2330 train_time:21091ms step_avg:57.47ms
step:368/2330 train_time:21149ms step_avg:57.47ms
step:369/2330 train_time:21205ms step_avg:57.47ms
step:370/2330 train_time:21264ms step_avg:57.47ms
step:371/2330 train_time:21319ms step_avg:57.46ms
step:372/2330 train_time:21379ms step_avg:57.47ms
step:373/2330 train_time:21435ms step_avg:57.47ms
step:374/2330 train_time:21494ms step_avg:57.47ms
step:375/2330 train_time:21550ms step_avg:57.47ms
step:376/2330 train_time:21610ms step_avg:57.47ms
step:377/2330 train_time:21665ms step_avg:57.47ms
step:378/2330 train_time:21725ms step_avg:57.47ms
step:379/2330 train_time:21780ms step_avg:57.47ms
step:380/2330 train_time:21840ms step_avg:57.47ms
step:381/2330 train_time:21895ms step_avg:57.47ms
step:382/2330 train_time:21955ms step_avg:57.47ms
step:383/2330 train_time:22011ms step_avg:57.47ms
step:384/2330 train_time:22070ms step_avg:57.47ms
step:385/2330 train_time:22126ms step_avg:57.47ms
step:386/2330 train_time:22184ms step_avg:57.47ms
step:387/2330 train_time:22240ms step_avg:57.47ms
step:388/2330 train_time:22299ms step_avg:57.47ms
step:389/2330 train_time:22355ms step_avg:57.47ms
step:390/2330 train_time:22413ms step_avg:57.47ms
step:391/2330 train_time:22469ms step_avg:57.46ms
step:392/2330 train_time:22528ms step_avg:57.47ms
step:393/2330 train_time:22584ms step_avg:57.47ms
step:394/2330 train_time:22643ms step_avg:57.47ms
step:395/2330 train_time:22699ms step_avg:57.47ms
step:396/2330 train_time:22759ms step_avg:57.47ms
step:397/2330 train_time:22815ms step_avg:57.47ms
step:398/2330 train_time:22874ms step_avg:57.47ms
step:399/2330 train_time:22930ms step_avg:57.47ms
step:400/2330 train_time:22989ms step_avg:57.47ms
step:401/2330 train_time:23045ms step_avg:57.47ms
step:402/2330 train_time:23105ms step_avg:57.47ms
step:403/2330 train_time:23160ms step_avg:57.47ms
step:404/2330 train_time:23220ms step_avg:57.48ms
step:405/2330 train_time:23276ms step_avg:57.47ms
step:406/2330 train_time:23335ms step_avg:57.47ms
step:407/2330 train_time:23391ms step_avg:57.47ms
step:408/2330 train_time:23450ms step_avg:57.48ms
step:409/2330 train_time:23506ms step_avg:57.47ms
step:410/2330 train_time:23564ms step_avg:57.47ms
step:411/2330 train_time:23620ms step_avg:57.47ms
step:412/2330 train_time:23679ms step_avg:57.47ms
step:413/2330 train_time:23734ms step_avg:57.47ms
step:414/2330 train_time:23795ms step_avg:57.47ms
step:415/2330 train_time:23851ms step_avg:57.47ms
step:416/2330 train_time:23910ms step_avg:57.48ms
step:417/2330 train_time:23966ms step_avg:57.47ms
step:418/2330 train_time:24025ms step_avg:57.48ms
step:419/2330 train_time:24080ms step_avg:57.47ms
step:420/2330 train_time:24140ms step_avg:57.48ms
step:421/2330 train_time:24196ms step_avg:57.47ms
step:422/2330 train_time:24255ms step_avg:57.48ms
step:423/2330 train_time:24311ms step_avg:57.47ms
step:424/2330 train_time:24370ms step_avg:57.48ms
step:425/2330 train_time:24426ms step_avg:57.47ms
step:426/2330 train_time:24485ms step_avg:57.48ms
step:427/2330 train_time:24541ms step_avg:57.47ms
step:428/2330 train_time:24600ms step_avg:57.48ms
step:429/2330 train_time:24656ms step_avg:57.47ms
step:430/2330 train_time:24715ms step_avg:57.48ms
step:431/2330 train_time:24771ms step_avg:57.47ms
step:432/2330 train_time:24831ms step_avg:57.48ms
step:433/2330 train_time:24886ms step_avg:57.47ms
step:434/2330 train_time:24946ms step_avg:57.48ms
step:435/2330 train_time:25002ms step_avg:57.48ms
step:436/2330 train_time:25062ms step_avg:57.48ms
step:437/2330 train_time:25118ms step_avg:57.48ms
step:438/2330 train_time:25177ms step_avg:57.48ms
step:439/2330 train_time:25233ms step_avg:57.48ms
step:440/2330 train_time:25292ms step_avg:57.48ms
step:441/2330 train_time:25348ms step_avg:57.48ms
step:442/2330 train_time:25407ms step_avg:57.48ms
step:443/2330 train_time:25462ms step_avg:57.48ms
step:444/2330 train_time:25522ms step_avg:57.48ms
step:445/2330 train_time:25577ms step_avg:57.48ms
step:446/2330 train_time:25636ms step_avg:57.48ms
step:447/2330 train_time:25693ms step_avg:57.48ms
step:448/2330 train_time:25753ms step_avg:57.48ms
step:449/2330 train_time:25808ms step_avg:57.48ms
step:450/2330 train_time:25868ms step_avg:57.48ms
step:451/2330 train_time:25924ms step_avg:57.48ms
step:452/2330 train_time:25983ms step_avg:57.48ms
step:453/2330 train_time:26039ms step_avg:57.48ms
step:454/2330 train_time:26098ms step_avg:57.48ms
step:455/2330 train_time:26154ms step_avg:57.48ms
step:456/2330 train_time:26213ms step_avg:57.49ms
step:457/2330 train_time:26270ms step_avg:57.48ms
step:458/2330 train_time:26328ms step_avg:57.49ms
step:459/2330 train_time:26384ms step_avg:57.48ms
step:460/2330 train_time:26443ms step_avg:57.49ms
step:461/2330 train_time:26499ms step_avg:57.48ms
step:462/2330 train_time:26558ms step_avg:57.48ms
step:463/2330 train_time:26615ms step_avg:57.48ms
step:464/2330 train_time:26674ms step_avg:57.49ms
step:465/2330 train_time:26730ms step_avg:57.48ms
step:466/2330 train_time:26789ms step_avg:57.49ms
step:467/2330 train_time:26845ms step_avg:57.48ms
step:468/2330 train_time:26904ms step_avg:57.49ms
step:469/2330 train_time:26959ms step_avg:57.48ms
step:470/2330 train_time:27019ms step_avg:57.49ms
step:471/2330 train_time:27076ms step_avg:57.49ms
step:472/2330 train_time:27134ms step_avg:57.49ms
step:473/2330 train_time:27190ms step_avg:57.48ms
step:474/2330 train_time:27250ms step_avg:57.49ms
step:475/2330 train_time:27306ms step_avg:57.49ms
step:476/2330 train_time:27364ms step_avg:57.49ms
step:477/2330 train_time:27420ms step_avg:57.48ms
step:478/2330 train_time:27480ms step_avg:57.49ms
step:479/2330 train_time:27536ms step_avg:57.49ms
step:480/2330 train_time:27595ms step_avg:57.49ms
step:481/2330 train_time:27651ms step_avg:57.49ms
step:482/2330 train_time:27710ms step_avg:57.49ms
step:483/2330 train_time:27766ms step_avg:57.49ms
step:484/2330 train_time:27826ms step_avg:57.49ms
step:485/2330 train_time:27882ms step_avg:57.49ms
step:486/2330 train_time:27941ms step_avg:57.49ms
step:487/2330 train_time:27997ms step_avg:57.49ms
step:488/2330 train_time:28055ms step_avg:57.49ms
step:489/2330 train_time:28112ms step_avg:57.49ms
step:490/2330 train_time:28171ms step_avg:57.49ms
step:491/2330 train_time:28228ms step_avg:57.49ms
step:492/2330 train_time:28287ms step_avg:57.49ms
step:493/2330 train_time:28343ms step_avg:57.49ms
step:494/2330 train_time:28402ms step_avg:57.49ms
step:495/2330 train_time:28458ms step_avg:57.49ms
step:496/2330 train_time:28517ms step_avg:57.49ms
step:497/2330 train_time:28573ms step_avg:57.49ms
step:498/2330 train_time:28632ms step_avg:57.49ms
step:499/2330 train_time:28688ms step_avg:57.49ms
step:500/2330 train_time:28747ms step_avg:57.49ms
step:500/2330 val_loss:4.4204 train_time:28827ms step_avg:57.65ms
step:501/2330 train_time:28845ms step_avg:57.58ms
step:502/2330 train_time:28865ms step_avg:57.50ms
step:503/2330 train_time:28921ms step_avg:57.50ms
step:504/2330 train_time:28987ms step_avg:57.51ms
step:505/2330 train_time:29043ms step_avg:57.51ms
step:506/2330 train_time:29103ms step_avg:57.52ms
step:507/2330 train_time:29158ms step_avg:57.51ms
step:508/2330 train_time:29219ms step_avg:57.52ms
step:509/2330 train_time:29275ms step_avg:57.51ms
step:510/2330 train_time:29333ms step_avg:57.52ms
step:511/2330 train_time:29388ms step_avg:57.51ms
step:512/2330 train_time:29447ms step_avg:57.51ms
step:513/2330 train_time:29502ms step_avg:57.51ms
step:514/2330 train_time:29560ms step_avg:57.51ms
step:515/2330 train_time:29616ms step_avg:57.51ms
step:516/2330 train_time:29674ms step_avg:57.51ms
step:517/2330 train_time:29730ms step_avg:57.51ms
step:518/2330 train_time:29790ms step_avg:57.51ms
step:519/2330 train_time:29847ms step_avg:57.51ms
step:520/2330 train_time:29906ms step_avg:57.51ms
step:521/2330 train_time:29963ms step_avg:57.51ms
step:522/2330 train_time:30024ms step_avg:57.52ms
step:523/2330 train_time:30080ms step_avg:57.51ms
step:524/2330 train_time:30140ms step_avg:57.52ms
step:525/2330 train_time:30197ms step_avg:57.52ms
step:526/2330 train_time:30256ms step_avg:57.52ms
step:527/2330 train_time:30311ms step_avg:57.52ms
step:528/2330 train_time:30370ms step_avg:57.52ms
step:529/2330 train_time:30425ms step_avg:57.51ms
step:530/2330 train_time:30485ms step_avg:57.52ms
step:531/2330 train_time:30540ms step_avg:57.51ms
step:532/2330 train_time:30599ms step_avg:57.52ms
step:533/2330 train_time:30655ms step_avg:57.51ms
step:534/2330 train_time:30713ms step_avg:57.52ms
step:535/2330 train_time:30770ms step_avg:57.51ms
step:536/2330 train_time:30830ms step_avg:57.52ms
step:537/2330 train_time:30886ms step_avg:57.52ms
step:538/2330 train_time:30946ms step_avg:57.52ms
step:539/2330 train_time:31002ms step_avg:57.52ms
step:540/2330 train_time:31064ms step_avg:57.53ms
step:541/2330 train_time:31120ms step_avg:57.52ms
step:542/2330 train_time:31181ms step_avg:57.53ms
step:543/2330 train_time:31237ms step_avg:57.53ms
step:544/2330 train_time:31296ms step_avg:57.53ms
step:545/2330 train_time:31353ms step_avg:57.53ms
step:546/2330 train_time:31412ms step_avg:57.53ms
step:547/2330 train_time:31468ms step_avg:57.53ms
step:548/2330 train_time:31526ms step_avg:57.53ms
step:549/2330 train_time:31581ms step_avg:57.53ms
step:550/2330 train_time:31641ms step_avg:57.53ms
step:551/2330 train_time:31696ms step_avg:57.52ms
step:552/2330 train_time:31755ms step_avg:57.53ms
step:553/2330 train_time:31812ms step_avg:57.53ms
step:554/2330 train_time:31871ms step_avg:57.53ms
step:555/2330 train_time:31928ms step_avg:57.53ms
step:556/2330 train_time:31987ms step_avg:57.53ms
step:557/2330 train_time:32043ms step_avg:57.53ms
step:558/2330 train_time:32103ms step_avg:57.53ms
step:559/2330 train_time:32159ms step_avg:57.53ms
step:560/2330 train_time:32219ms step_avg:57.53ms
step:561/2330 train_time:32274ms step_avg:57.53ms
step:562/2330 train_time:32334ms step_avg:57.53ms
step:563/2330 train_time:32390ms step_avg:57.53ms
step:564/2330 train_time:32450ms step_avg:57.53ms
step:565/2330 train_time:32506ms step_avg:57.53ms
step:566/2330 train_time:32565ms step_avg:57.54ms
step:567/2330 train_time:32621ms step_avg:57.53ms
step:568/2330 train_time:32679ms step_avg:57.53ms
step:569/2330 train_time:32735ms step_avg:57.53ms
step:570/2330 train_time:32794ms step_avg:57.53ms
step:571/2330 train_time:32851ms step_avg:57.53ms
step:572/2330 train_time:32910ms step_avg:57.54ms
step:573/2330 train_time:32967ms step_avg:57.53ms
step:574/2330 train_time:33027ms step_avg:57.54ms
step:575/2330 train_time:33083ms step_avg:57.54ms
step:576/2330 train_time:33142ms step_avg:57.54ms
step:577/2330 train_time:33198ms step_avg:57.54ms
step:578/2330 train_time:33258ms step_avg:57.54ms
step:579/2330 train_time:33314ms step_avg:57.54ms
step:580/2330 train_time:33373ms step_avg:57.54ms
step:581/2330 train_time:33429ms step_avg:57.54ms
step:582/2330 train_time:33488ms step_avg:57.54ms
step:583/2330 train_time:33544ms step_avg:57.54ms
step:584/2330 train_time:33603ms step_avg:57.54ms
step:585/2330 train_time:33659ms step_avg:57.54ms
step:586/2330 train_time:33718ms step_avg:57.54ms
step:587/2330 train_time:33775ms step_avg:57.54ms
step:588/2330 train_time:33834ms step_avg:57.54ms
step:589/2330 train_time:33890ms step_avg:57.54ms
step:590/2330 train_time:33950ms step_avg:57.54ms
step:591/2330 train_time:34007ms step_avg:57.54ms
step:592/2330 train_time:34067ms step_avg:57.54ms
step:593/2330 train_time:34122ms step_avg:57.54ms
step:594/2330 train_time:34183ms step_avg:57.55ms
step:595/2330 train_time:34239ms step_avg:57.54ms
step:596/2330 train_time:34299ms step_avg:57.55ms
step:597/2330 train_time:34355ms step_avg:57.55ms
step:598/2330 train_time:34415ms step_avg:57.55ms
step:599/2330 train_time:34471ms step_avg:57.55ms
step:600/2330 train_time:34530ms step_avg:57.55ms
step:601/2330 train_time:34586ms step_avg:57.55ms
step:602/2330 train_time:34644ms step_avg:57.55ms
step:603/2330 train_time:34701ms step_avg:57.55ms
step:604/2330 train_time:34759ms step_avg:57.55ms
step:605/2330 train_time:34815ms step_avg:57.54ms
step:606/2330 train_time:34875ms step_avg:57.55ms
step:607/2330 train_time:34932ms step_avg:57.55ms
step:608/2330 train_time:34992ms step_avg:57.55ms
step:609/2330 train_time:35048ms step_avg:57.55ms
step:610/2330 train_time:35108ms step_avg:57.55ms
step:611/2330 train_time:35164ms step_avg:57.55ms
step:612/2330 train_time:35224ms step_avg:57.56ms
step:613/2330 train_time:35279ms step_avg:57.55ms
step:614/2330 train_time:35340ms step_avg:57.56ms
step:615/2330 train_time:35395ms step_avg:57.55ms
step:616/2330 train_time:35454ms step_avg:57.56ms
step:617/2330 train_time:35511ms step_avg:57.55ms
step:618/2330 train_time:35570ms step_avg:57.56ms
step:619/2330 train_time:35626ms step_avg:57.55ms
step:620/2330 train_time:35685ms step_avg:57.56ms
step:621/2330 train_time:35741ms step_avg:57.55ms
step:622/2330 train_time:35801ms step_avg:57.56ms
step:623/2330 train_time:35856ms step_avg:57.55ms
step:624/2330 train_time:35917ms step_avg:57.56ms
step:625/2330 train_time:35973ms step_avg:57.56ms
step:626/2330 train_time:36033ms step_avg:57.56ms
step:627/2330 train_time:36090ms step_avg:57.56ms
step:628/2330 train_time:36150ms step_avg:57.56ms
step:629/2330 train_time:36206ms step_avg:57.56ms
step:630/2330 train_time:36264ms step_avg:57.56ms
step:631/2330 train_time:36320ms step_avg:57.56ms
step:632/2330 train_time:36380ms step_avg:57.56ms
step:633/2330 train_time:36436ms step_avg:57.56ms
step:634/2330 train_time:36496ms step_avg:57.56ms
step:635/2330 train_time:36552ms step_avg:57.56ms
step:636/2330 train_time:36612ms step_avg:57.57ms
step:637/2330 train_time:36668ms step_avg:57.56ms
step:638/2330 train_time:36727ms step_avg:57.57ms
step:639/2330 train_time:36783ms step_avg:57.56ms
step:640/2330 train_time:36842ms step_avg:57.57ms
step:641/2330 train_time:36898ms step_avg:57.56ms
step:642/2330 train_time:36957ms step_avg:57.57ms
step:643/2330 train_time:37013ms step_avg:57.56ms
step:644/2330 train_time:37074ms step_avg:57.57ms
step:645/2330 train_time:37130ms step_avg:57.57ms
step:646/2330 train_time:37190ms step_avg:57.57ms
step:647/2330 train_time:37246ms step_avg:57.57ms
step:648/2330 train_time:37305ms step_avg:57.57ms
step:649/2330 train_time:37361ms step_avg:57.57ms
step:650/2330 train_time:37420ms step_avg:57.57ms
step:651/2330 train_time:37476ms step_avg:57.57ms
step:652/2330 train_time:37535ms step_avg:57.57ms
step:653/2330 train_time:37592ms step_avg:57.57ms
step:654/2330 train_time:37651ms step_avg:57.57ms
step:655/2330 train_time:37707ms step_avg:57.57ms
step:656/2330 train_time:37766ms step_avg:57.57ms
step:657/2330 train_time:37822ms step_avg:57.57ms
step:658/2330 train_time:37881ms step_avg:57.57ms
step:659/2330 train_time:37936ms step_avg:57.57ms
step:660/2330 train_time:37996ms step_avg:57.57ms
step:661/2330 train_time:38052ms step_avg:57.57ms
step:662/2330 train_time:38112ms step_avg:57.57ms
step:663/2330 train_time:38168ms step_avg:57.57ms
step:664/2330 train_time:38228ms step_avg:57.57ms
step:665/2330 train_time:38284ms step_avg:57.57ms
step:666/2330 train_time:38343ms step_avg:57.57ms
step:667/2330 train_time:38398ms step_avg:57.57ms
step:668/2330 train_time:38458ms step_avg:57.57ms
step:669/2330 train_time:38514ms step_avg:57.57ms
step:670/2330 train_time:38574ms step_avg:57.57ms
step:671/2330 train_time:38630ms step_avg:57.57ms
step:672/2330 train_time:38691ms step_avg:57.58ms
step:673/2330 train_time:38747ms step_avg:57.57ms
step:674/2330 train_time:38806ms step_avg:57.58ms
step:675/2330 train_time:38862ms step_avg:57.57ms
step:676/2330 train_time:38921ms step_avg:57.58ms
step:677/2330 train_time:38977ms step_avg:57.57ms
step:678/2330 train_time:39036ms step_avg:57.58ms
step:679/2330 train_time:39092ms step_avg:57.57ms
step:680/2330 train_time:39152ms step_avg:57.58ms
step:681/2330 train_time:39208ms step_avg:57.57ms
step:682/2330 train_time:39267ms step_avg:57.58ms
step:683/2330 train_time:39322ms step_avg:57.57ms
step:684/2330 train_time:39383ms step_avg:57.58ms
step:685/2330 train_time:39439ms step_avg:57.58ms
step:686/2330 train_time:39498ms step_avg:57.58ms
step:687/2330 train_time:39554ms step_avg:57.58ms
step:688/2330 train_time:39615ms step_avg:57.58ms
step:689/2330 train_time:39671ms step_avg:57.58ms
step:690/2330 train_time:39730ms step_avg:57.58ms
step:691/2330 train_time:39786ms step_avg:57.58ms
step:692/2330 train_time:39845ms step_avg:57.58ms
step:693/2330 train_time:39902ms step_avg:57.58ms
step:694/2330 train_time:39960ms step_avg:57.58ms
step:695/2330 train_time:40016ms step_avg:57.58ms
step:696/2330 train_time:40076ms step_avg:57.58ms
step:697/2330 train_time:40133ms step_avg:57.58ms
step:698/2330 train_time:40192ms step_avg:57.58ms
step:699/2330 train_time:40249ms step_avg:57.58ms
step:700/2330 train_time:40308ms step_avg:57.58ms
step:701/2330 train_time:40364ms step_avg:57.58ms
step:702/2330 train_time:40423ms step_avg:57.58ms
step:703/2330 train_time:40479ms step_avg:57.58ms
step:704/2330 train_time:40538ms step_avg:57.58ms
step:705/2330 train_time:40594ms step_avg:57.58ms
step:706/2330 train_time:40655ms step_avg:57.58ms
step:707/2330 train_time:40710ms step_avg:57.58ms
step:708/2330 train_time:40770ms step_avg:57.58ms
step:709/2330 train_time:40826ms step_avg:57.58ms
step:710/2330 train_time:40886ms step_avg:57.59ms
step:711/2330 train_time:40942ms step_avg:57.58ms
step:712/2330 train_time:41000ms step_avg:57.58ms
step:713/2330 train_time:41057ms step_avg:57.58ms
step:714/2330 train_time:41116ms step_avg:57.59ms
step:715/2330 train_time:41173ms step_avg:57.58ms
step:716/2330 train_time:41231ms step_avg:57.59ms
step:717/2330 train_time:41287ms step_avg:57.58ms
step:718/2330 train_time:41347ms step_avg:57.59ms
step:719/2330 train_time:41403ms step_avg:57.58ms
step:720/2330 train_time:41463ms step_avg:57.59ms
step:721/2330 train_time:41519ms step_avg:57.59ms
step:722/2330 train_time:41578ms step_avg:57.59ms
step:723/2330 train_time:41634ms step_avg:57.59ms
step:724/2330 train_time:41694ms step_avg:57.59ms
step:725/2330 train_time:41751ms step_avg:57.59ms
step:726/2330 train_time:41810ms step_avg:57.59ms
step:727/2330 train_time:41866ms step_avg:57.59ms
step:728/2330 train_time:41925ms step_avg:57.59ms
step:729/2330 train_time:41982ms step_avg:57.59ms
step:730/2330 train_time:42041ms step_avg:57.59ms
step:731/2330 train_time:42097ms step_avg:57.59ms
step:732/2330 train_time:42156ms step_avg:57.59ms
step:733/2330 train_time:42211ms step_avg:57.59ms
step:734/2330 train_time:42271ms step_avg:57.59ms
step:735/2330 train_time:42327ms step_avg:57.59ms
step:736/2330 train_time:42387ms step_avg:57.59ms
step:737/2330 train_time:42443ms step_avg:57.59ms
step:738/2330 train_time:42501ms step_avg:57.59ms
step:739/2330 train_time:42557ms step_avg:57.59ms
step:740/2330 train_time:42617ms step_avg:57.59ms
step:741/2330 train_time:42674ms step_avg:57.59ms
step:742/2330 train_time:42733ms step_avg:57.59ms
step:743/2330 train_time:42789ms step_avg:57.59ms
step:744/2330 train_time:42847ms step_avg:57.59ms
step:745/2330 train_time:42904ms step_avg:57.59ms
step:746/2330 train_time:42963ms step_avg:57.59ms
step:747/2330 train_time:43019ms step_avg:57.59ms
step:748/2330 train_time:43078ms step_avg:57.59ms
step:749/2330 train_time:43134ms step_avg:57.59ms
step:750/2330 train_time:43194ms step_avg:57.59ms
step:750/2330 val_loss:4.2125 train_time:43274ms step_avg:57.70ms
step:751/2330 train_time:43293ms step_avg:57.65ms
step:752/2330 train_time:43312ms step_avg:57.60ms
step:753/2330 train_time:43368ms step_avg:57.59ms
step:754/2330 train_time:43433ms step_avg:57.60ms
step:755/2330 train_time:43489ms step_avg:57.60ms
step:756/2330 train_time:43550ms step_avg:57.61ms
step:757/2330 train_time:43607ms step_avg:57.60ms
step:758/2330 train_time:43665ms step_avg:57.61ms
step:759/2330 train_time:43721ms step_avg:57.60ms
step:760/2330 train_time:43781ms step_avg:57.61ms
step:761/2330 train_time:43836ms step_avg:57.60ms
step:762/2330 train_time:43895ms step_avg:57.60ms
step:763/2330 train_time:43950ms step_avg:57.60ms
step:764/2330 train_time:44009ms step_avg:57.60ms
step:765/2330 train_time:44066ms step_avg:57.60ms
step:766/2330 train_time:44124ms step_avg:57.60ms
step:767/2330 train_time:44182ms step_avg:57.60ms
step:768/2330 train_time:44242ms step_avg:57.61ms
step:769/2330 train_time:44301ms step_avg:57.61ms
step:770/2330 train_time:44362ms step_avg:57.61ms
step:771/2330 train_time:44420ms step_avg:57.61ms
step:772/2330 train_time:44481ms step_avg:57.62ms
step:773/2330 train_time:44539ms step_avg:57.62ms
step:774/2330 train_time:44600ms step_avg:57.62ms
step:775/2330 train_time:44657ms step_avg:57.62ms
step:776/2330 train_time:44717ms step_avg:57.62ms
step:777/2330 train_time:44773ms step_avg:57.62ms
step:778/2330 train_time:44833ms step_avg:57.63ms
step:779/2330 train_time:44889ms step_avg:57.62ms
step:780/2330 train_time:44948ms step_avg:57.63ms
step:781/2330 train_time:45005ms step_avg:57.62ms
step:782/2330 train_time:45064ms step_avg:57.63ms
step:783/2330 train_time:45121ms step_avg:57.63ms
step:784/2330 train_time:45180ms step_avg:57.63ms
step:785/2330 train_time:45237ms step_avg:57.63ms
step:786/2330 train_time:45297ms step_avg:57.63ms
step:787/2330 train_time:45355ms step_avg:57.63ms
step:788/2330 train_time:45416ms step_avg:57.63ms
step:789/2330 train_time:45472ms step_avg:57.63ms
step:790/2330 train_time:45534ms step_avg:57.64ms
step:791/2330 train_time:45591ms step_avg:57.64ms
step:792/2330 train_time:45651ms step_avg:57.64ms
step:793/2330 train_time:45708ms step_avg:57.64ms
step:794/2330 train_time:45768ms step_avg:57.64ms
step:795/2330 train_time:45824ms step_avg:57.64ms
step:796/2330 train_time:45884ms step_avg:57.64ms
step:797/2330 train_time:45941ms step_avg:57.64ms
step:798/2330 train_time:46001ms step_avg:57.65ms
step:799/2330 train_time:46057ms step_avg:57.64ms
step:800/2330 train_time:46117ms step_avg:57.65ms
step:801/2330 train_time:46173ms step_avg:57.64ms
step:802/2330 train_time:46234ms step_avg:57.65ms
step:803/2330 train_time:46291ms step_avg:57.65ms
step:804/2330 train_time:46352ms step_avg:57.65ms
step:805/2330 train_time:46410ms step_avg:57.65ms
step:806/2330 train_time:46470ms step_avg:57.66ms
step:807/2330 train_time:46527ms step_avg:57.65ms
step:808/2330 train_time:46588ms step_avg:57.66ms
step:809/2330 train_time:46645ms step_avg:57.66ms
step:810/2330 train_time:46705ms step_avg:57.66ms
step:811/2330 train_time:46763ms step_avg:57.66ms
step:812/2330 train_time:46822ms step_avg:57.66ms
step:813/2330 train_time:46878ms step_avg:57.66ms
step:814/2330 train_time:46938ms step_avg:57.66ms
step:815/2330 train_time:46995ms step_avg:57.66ms
step:816/2330 train_time:47054ms step_avg:57.66ms
step:817/2330 train_time:47110ms step_avg:57.66ms
step:818/2330 train_time:47171ms step_avg:57.67ms
step:819/2330 train_time:47228ms step_avg:57.66ms
step:820/2330 train_time:47289ms step_avg:57.67ms
step:821/2330 train_time:47346ms step_avg:57.67ms
step:822/2330 train_time:47406ms step_avg:57.67ms
step:823/2330 train_time:47464ms step_avg:57.67ms
step:824/2330 train_time:47525ms step_avg:57.68ms
step:825/2330 train_time:47582ms step_avg:57.68ms
step:826/2330 train_time:47642ms step_avg:57.68ms
step:827/2330 train_time:47700ms step_avg:57.68ms
step:828/2330 train_time:47759ms step_avg:57.68ms
step:829/2330 train_time:47816ms step_avg:57.68ms
step:830/2330 train_time:47875ms step_avg:57.68ms
step:831/2330 train_time:47931ms step_avg:57.68ms
step:832/2330 train_time:47991ms step_avg:57.68ms
step:833/2330 train_time:48048ms step_avg:57.68ms
step:834/2330 train_time:48109ms step_avg:57.68ms
step:835/2330 train_time:48166ms step_avg:57.68ms
step:836/2330 train_time:48225ms step_avg:57.69ms
step:837/2330 train_time:48282ms step_avg:57.68ms
step:838/2330 train_time:48343ms step_avg:57.69ms
step:839/2330 train_time:48400ms step_avg:57.69ms
step:840/2330 train_time:48460ms step_avg:57.69ms
step:841/2330 train_time:48518ms step_avg:57.69ms
step:842/2330 train_time:48577ms step_avg:57.69ms
step:843/2330 train_time:48634ms step_avg:57.69ms
step:844/2330 train_time:48695ms step_avg:57.70ms
step:845/2330 train_time:48752ms step_avg:57.69ms
step:846/2330 train_time:48812ms step_avg:57.70ms
step:847/2330 train_time:48869ms step_avg:57.70ms
step:848/2330 train_time:48929ms step_avg:57.70ms
step:849/2330 train_time:48986ms step_avg:57.70ms
step:850/2330 train_time:49046ms step_avg:57.70ms
step:851/2330 train_time:49102ms step_avg:57.70ms
step:852/2330 train_time:49163ms step_avg:57.70ms
step:853/2330 train_time:49220ms step_avg:57.70ms
step:854/2330 train_time:49280ms step_avg:57.70ms
step:855/2330 train_time:49336ms step_avg:57.70ms
step:856/2330 train_time:49396ms step_avg:57.71ms
step:857/2330 train_time:49452ms step_avg:57.70ms
step:858/2330 train_time:49513ms step_avg:57.71ms
step:859/2330 train_time:49570ms step_avg:57.71ms
step:860/2330 train_time:49631ms step_avg:57.71ms
step:861/2330 train_time:49687ms step_avg:57.71ms
step:862/2330 train_time:49748ms step_avg:57.71ms
step:863/2330 train_time:49806ms step_avg:57.71ms
step:864/2330 train_time:49866ms step_avg:57.71ms
step:865/2330 train_time:49923ms step_avg:57.71ms
step:866/2330 train_time:49983ms step_avg:57.72ms
step:867/2330 train_time:50040ms step_avg:57.72ms
step:868/2330 train_time:50099ms step_avg:57.72ms
step:869/2330 train_time:50156ms step_avg:57.72ms
step:870/2330 train_time:50216ms step_avg:57.72ms
step:871/2330 train_time:50273ms step_avg:57.72ms
step:872/2330 train_time:50332ms step_avg:57.72ms
step:873/2330 train_time:50389ms step_avg:57.72ms
step:874/2330 train_time:50449ms step_avg:57.72ms
step:875/2330 train_time:50506ms step_avg:57.72ms
step:876/2330 train_time:50566ms step_avg:57.72ms
step:877/2330 train_time:50623ms step_avg:57.72ms
step:878/2330 train_time:50684ms step_avg:57.73ms
step:879/2330 train_time:50741ms step_avg:57.73ms
step:880/2330 train_time:50801ms step_avg:57.73ms
step:881/2330 train_time:50857ms step_avg:57.73ms
step:882/2330 train_time:50918ms step_avg:57.73ms
step:883/2330 train_time:50974ms step_avg:57.73ms
step:884/2330 train_time:51034ms step_avg:57.73ms
step:885/2330 train_time:51091ms step_avg:57.73ms
step:886/2330 train_time:51151ms step_avg:57.73ms
step:887/2330 train_time:51208ms step_avg:57.73ms
step:888/2330 train_time:51268ms step_avg:57.73ms
step:889/2330 train_time:51325ms step_avg:57.73ms
step:890/2330 train_time:51385ms step_avg:57.74ms
step:891/2330 train_time:51441ms step_avg:57.73ms
step:892/2330 train_time:51501ms step_avg:57.74ms
step:893/2330 train_time:51558ms step_avg:57.74ms
step:894/2330 train_time:51619ms step_avg:57.74ms
step:895/2330 train_time:51675ms step_avg:57.74ms
step:896/2330 train_time:51736ms step_avg:57.74ms
step:897/2330 train_time:51792ms step_avg:57.74ms
step:898/2330 train_time:51852ms step_avg:57.74ms
step:899/2330 train_time:51909ms step_avg:57.74ms
step:900/2330 train_time:51970ms step_avg:57.74ms
step:901/2330 train_time:52026ms step_avg:57.74ms
step:902/2330 train_time:52086ms step_avg:57.75ms
step:903/2330 train_time:52144ms step_avg:57.74ms
step:904/2330 train_time:52203ms step_avg:57.75ms
step:905/2330 train_time:52261ms step_avg:57.75ms
step:906/2330 train_time:52321ms step_avg:57.75ms
step:907/2330 train_time:52379ms step_avg:57.75ms
step:908/2330 train_time:52438ms step_avg:57.75ms
step:909/2330 train_time:52495ms step_avg:57.75ms
step:910/2330 train_time:52554ms step_avg:57.75ms
step:911/2330 train_time:52611ms step_avg:57.75ms
step:912/2330 train_time:52671ms step_avg:57.75ms
step:913/2330 train_time:52728ms step_avg:57.75ms
step:914/2330 train_time:52788ms step_avg:57.76ms
step:915/2330 train_time:52845ms step_avg:57.75ms
step:916/2330 train_time:52905ms step_avg:57.76ms
step:917/2330 train_time:52962ms step_avg:57.76ms
step:918/2330 train_time:53022ms step_avg:57.76ms
step:919/2330 train_time:53079ms step_avg:57.76ms
step:920/2330 train_time:53139ms step_avg:57.76ms
step:921/2330 train_time:53196ms step_avg:57.76ms
step:922/2330 train_time:53256ms step_avg:57.76ms
step:923/2330 train_time:53314ms step_avg:57.76ms
step:924/2330 train_time:53373ms step_avg:57.76ms
step:925/2330 train_time:53430ms step_avg:57.76ms
step:926/2330 train_time:53490ms step_avg:57.77ms
step:927/2330 train_time:53547ms step_avg:57.76ms
step:928/2330 train_time:53608ms step_avg:57.77ms
step:929/2330 train_time:53664ms step_avg:57.77ms
step:930/2330 train_time:53726ms step_avg:57.77ms
step:931/2330 train_time:53782ms step_avg:57.77ms
step:932/2330 train_time:53843ms step_avg:57.77ms
step:933/2330 train_time:53899ms step_avg:57.77ms
step:934/2330 train_time:53959ms step_avg:57.77ms
step:935/2330 train_time:54016ms step_avg:57.77ms
step:936/2330 train_time:54076ms step_avg:57.77ms
step:937/2330 train_time:54133ms step_avg:57.77ms
step:938/2330 train_time:54193ms step_avg:57.78ms
step:939/2330 train_time:54250ms step_avg:57.77ms
step:940/2330 train_time:54311ms step_avg:57.78ms
step:941/2330 train_time:54367ms step_avg:57.78ms
step:942/2330 train_time:54428ms step_avg:57.78ms
step:943/2330 train_time:54485ms step_avg:57.78ms
step:944/2330 train_time:54545ms step_avg:57.78ms
step:945/2330 train_time:54603ms step_avg:57.78ms
step:946/2330 train_time:54663ms step_avg:57.78ms
step:947/2330 train_time:54720ms step_avg:57.78ms
step:948/2330 train_time:54780ms step_avg:57.78ms
step:949/2330 train_time:54836ms step_avg:57.78ms
step:950/2330 train_time:54896ms step_avg:57.79ms
step:951/2330 train_time:54952ms step_avg:57.78ms
step:952/2330 train_time:55013ms step_avg:57.79ms
step:953/2330 train_time:55070ms step_avg:57.79ms
step:954/2330 train_time:55131ms step_avg:57.79ms
step:955/2330 train_time:55187ms step_avg:57.79ms
step:956/2330 train_time:55248ms step_avg:57.79ms
step:957/2330 train_time:55305ms step_avg:57.79ms
step:958/2330 train_time:55365ms step_avg:57.79ms
step:959/2330 train_time:55422ms step_avg:57.79ms
step:960/2330 train_time:55482ms step_avg:57.79ms
step:961/2330 train_time:55540ms step_avg:57.79ms
step:962/2330 train_time:55600ms step_avg:57.80ms
step:963/2330 train_time:55658ms step_avg:57.80ms
step:964/2330 train_time:55717ms step_avg:57.80ms
step:965/2330 train_time:55774ms step_avg:57.80ms
step:966/2330 train_time:55834ms step_avg:57.80ms
step:967/2330 train_time:55890ms step_avg:57.80ms
step:968/2330 train_time:55951ms step_avg:57.80ms
step:969/2330 train_time:56008ms step_avg:57.80ms
step:970/2330 train_time:56068ms step_avg:57.80ms
step:971/2330 train_time:56124ms step_avg:57.80ms
step:972/2330 train_time:56185ms step_avg:57.80ms
step:973/2330 train_time:56241ms step_avg:57.80ms
step:974/2330 train_time:56303ms step_avg:57.81ms
step:975/2330 train_time:56359ms step_avg:57.80ms
step:976/2330 train_time:56420ms step_avg:57.81ms
step:977/2330 train_time:56476ms step_avg:57.81ms
step:978/2330 train_time:56536ms step_avg:57.81ms
step:979/2330 train_time:56592ms step_avg:57.81ms
step:980/2330 train_time:56653ms step_avg:57.81ms
step:981/2330 train_time:56710ms step_avg:57.81ms
step:982/2330 train_time:56770ms step_avg:57.81ms
step:983/2330 train_time:56827ms step_avg:57.81ms
step:984/2330 train_time:56887ms step_avg:57.81ms
step:985/2330 train_time:56945ms step_avg:57.81ms
step:986/2330 train_time:57005ms step_avg:57.81ms
step:987/2330 train_time:57063ms step_avg:57.81ms
step:988/2330 train_time:57122ms step_avg:57.82ms
step:989/2330 train_time:57179ms step_avg:57.81ms
step:990/2330 train_time:57240ms step_avg:57.82ms
step:991/2330 train_time:57297ms step_avg:57.82ms
step:992/2330 train_time:57357ms step_avg:57.82ms
step:993/2330 train_time:57413ms step_avg:57.82ms
step:994/2330 train_time:57474ms step_avg:57.82ms
step:995/2330 train_time:57530ms step_avg:57.82ms
step:996/2330 train_time:57591ms step_avg:57.82ms
step:997/2330 train_time:57647ms step_avg:57.82ms
step:998/2330 train_time:57708ms step_avg:57.82ms
step:999/2330 train_time:57765ms step_avg:57.82ms
step:1000/2330 train_time:57824ms step_avg:57.82ms
step:1000/2330 val_loss:4.0722 train_time:57906ms step_avg:57.91ms
step:1001/2330 train_time:57926ms step_avg:57.87ms
step:1002/2330 train_time:57946ms step_avg:57.83ms
step:1003/2330 train_time:58000ms step_avg:57.83ms
step:1004/2330 train_time:58071ms step_avg:57.84ms
step:1005/2330 train_time:58126ms step_avg:57.84ms
step:1006/2330 train_time:58188ms step_avg:57.84ms
step:1007/2330 train_time:58243ms step_avg:57.84ms
step:1008/2330 train_time:58303ms step_avg:57.84ms
step:1009/2330 train_time:58359ms step_avg:57.84ms
step:1010/2330 train_time:58418ms step_avg:57.84ms
step:1011/2330 train_time:58475ms step_avg:57.84ms
step:1012/2330 train_time:58534ms step_avg:57.84ms
step:1013/2330 train_time:58590ms step_avg:57.84ms
step:1014/2330 train_time:58649ms step_avg:57.84ms
step:1015/2330 train_time:58705ms step_avg:57.84ms
step:1016/2330 train_time:58764ms step_avg:57.84ms
step:1017/2330 train_time:58822ms step_avg:57.84ms
step:1018/2330 train_time:58885ms step_avg:57.84ms
step:1019/2330 train_time:58942ms step_avg:57.84ms
step:1020/2330 train_time:59006ms step_avg:57.85ms
step:1021/2330 train_time:59063ms step_avg:57.85ms
step:1022/2330 train_time:59123ms step_avg:57.85ms
step:1023/2330 train_time:59180ms step_avg:57.85ms
step:1024/2330 train_time:59241ms step_avg:57.85ms
step:1025/2330 train_time:59297ms step_avg:57.85ms
step:1026/2330 train_time:59357ms step_avg:57.85ms
step:1027/2330 train_time:59413ms step_avg:57.85ms
step:1028/2330 train_time:59473ms step_avg:57.85ms
step:1029/2330 train_time:59530ms step_avg:57.85ms
step:1030/2330 train_time:59589ms step_avg:57.85ms
step:1031/2330 train_time:59645ms step_avg:57.85ms
step:1032/2330 train_time:59704ms step_avg:57.85ms
step:1033/2330 train_time:59760ms step_avg:57.85ms
step:1034/2330 train_time:59822ms step_avg:57.85ms
step:1035/2330 train_time:59881ms step_avg:57.86ms
step:1036/2330 train_time:59941ms step_avg:57.86ms
step:1037/2330 train_time:59998ms step_avg:57.86ms
step:1038/2330 train_time:60060ms step_avg:57.86ms
step:1039/2330 train_time:60117ms step_avg:57.86ms
step:1040/2330 train_time:60177ms step_avg:57.86ms
step:1041/2330 train_time:60233ms step_avg:57.86ms
step:1042/2330 train_time:60294ms step_avg:57.86ms
step:1043/2330 train_time:60351ms step_avg:57.86ms
step:1044/2330 train_time:60410ms step_avg:57.86ms
step:1045/2330 train_time:60466ms step_avg:57.86ms
step:1046/2330 train_time:60526ms step_avg:57.86ms
step:1047/2330 train_time:60582ms step_avg:57.86ms
step:1048/2330 train_time:60643ms step_avg:57.86ms
step:1049/2330 train_time:60698ms step_avg:57.86ms
step:1050/2330 train_time:60759ms step_avg:57.87ms
step:1051/2330 train_time:60816ms step_avg:57.87ms
step:1052/2330 train_time:60877ms step_avg:57.87ms
step:1053/2330 train_time:60935ms step_avg:57.87ms
step:1054/2330 train_time:60996ms step_avg:57.87ms
step:1055/2330 train_time:61054ms step_avg:57.87ms
step:1056/2330 train_time:61114ms step_avg:57.87ms
step:1057/2330 train_time:61171ms step_avg:57.87ms
step:1058/2330 train_time:61232ms step_avg:57.87ms
step:1059/2330 train_time:61288ms step_avg:57.87ms
step:1060/2330 train_time:61349ms step_avg:57.88ms
step:1061/2330 train_time:61404ms step_avg:57.87ms
step:1062/2330 train_time:61465ms step_avg:57.88ms
step:1063/2330 train_time:61521ms step_avg:57.88ms
step:1064/2330 train_time:61581ms step_avg:57.88ms
step:1065/2330 train_time:61637ms step_avg:57.87ms
step:1066/2330 train_time:61698ms step_avg:57.88ms
step:1067/2330 train_time:61755ms step_avg:57.88ms
step:1068/2330 train_time:61815ms step_avg:57.88ms
step:1069/2330 train_time:61872ms step_avg:57.88ms
step:1070/2330 train_time:61932ms step_avg:57.88ms
step:1071/2330 train_time:61990ms step_avg:57.88ms
step:1072/2330 train_time:62049ms step_avg:57.88ms
step:1073/2330 train_time:62105ms step_avg:57.88ms
step:1074/2330 train_time:62166ms step_avg:57.88ms
step:1075/2330 train_time:62223ms step_avg:57.88ms
step:1076/2330 train_time:62283ms step_avg:57.88ms
step:1077/2330 train_time:62339ms step_avg:57.88ms
step:1078/2330 train_time:62399ms step_avg:57.88ms
step:1079/2330 train_time:62455ms step_avg:57.88ms
step:1080/2330 train_time:62516ms step_avg:57.88ms
step:1081/2330 train_time:62572ms step_avg:57.88ms
step:1082/2330 train_time:62633ms step_avg:57.89ms
step:1083/2330 train_time:62690ms step_avg:57.89ms
step:1084/2330 train_time:62749ms step_avg:57.89ms
step:1085/2330 train_time:62805ms step_avg:57.89ms
step:1086/2330 train_time:62865ms step_avg:57.89ms
step:1087/2330 train_time:62922ms step_avg:57.89ms
step:1088/2330 train_time:62983ms step_avg:57.89ms
step:1089/2330 train_time:63040ms step_avg:57.89ms
step:1090/2330 train_time:63101ms step_avg:57.89ms
step:1091/2330 train_time:63158ms step_avg:57.89ms
step:1092/2330 train_time:63219ms step_avg:57.89ms
step:1093/2330 train_time:63275ms step_avg:57.89ms
step:1094/2330 train_time:63335ms step_avg:57.89ms
step:1095/2330 train_time:63392ms step_avg:57.89ms
step:1096/2330 train_time:63452ms step_avg:57.89ms
step:1097/2330 train_time:63508ms step_avg:57.89ms
step:1098/2330 train_time:63568ms step_avg:57.89ms
step:1099/2330 train_time:63624ms step_avg:57.89ms
step:1100/2330 train_time:63684ms step_avg:57.89ms
step:1101/2330 train_time:63740ms step_avg:57.89ms
step:1102/2330 train_time:63802ms step_avg:57.90ms
step:1103/2330 train_time:63859ms step_avg:57.90ms
step:1104/2330 train_time:63919ms step_avg:57.90ms
step:1105/2330 train_time:63975ms step_avg:57.90ms
step:1106/2330 train_time:64036ms step_avg:57.90ms
step:1107/2330 train_time:64094ms step_avg:57.90ms
step:1108/2330 train_time:64153ms step_avg:57.90ms
step:1109/2330 train_time:64211ms step_avg:57.90ms
step:1110/2330 train_time:64271ms step_avg:57.90ms
step:1111/2330 train_time:64328ms step_avg:57.90ms
step:1112/2330 train_time:64387ms step_avg:57.90ms
step:1113/2330 train_time:64443ms step_avg:57.90ms
step:1114/2330 train_time:64503ms step_avg:57.90ms
step:1115/2330 train_time:64559ms step_avg:57.90ms
step:1116/2330 train_time:64621ms step_avg:57.90ms
step:1117/2330 train_time:64678ms step_avg:57.90ms
step:1118/2330 train_time:64738ms step_avg:57.90ms
step:1119/2330 train_time:64795ms step_avg:57.90ms
step:1120/2330 train_time:64855ms step_avg:57.91ms
step:1121/2330 train_time:64912ms step_avg:57.91ms
step:1122/2330 train_time:64972ms step_avg:57.91ms
step:1123/2330 train_time:65029ms step_avg:57.91ms
step:1124/2330 train_time:65089ms step_avg:57.91ms
step:1125/2330 train_time:65146ms step_avg:57.91ms
step:1126/2330 train_time:65206ms step_avg:57.91ms
step:1127/2330 train_time:65262ms step_avg:57.91ms
step:1128/2330 train_time:65323ms step_avg:57.91ms
step:1129/2330 train_time:65380ms step_avg:57.91ms
step:1130/2330 train_time:65441ms step_avg:57.91ms
step:1131/2330 train_time:65497ms step_avg:57.91ms
step:1132/2330 train_time:65557ms step_avg:57.91ms
step:1133/2330 train_time:65614ms step_avg:57.91ms
step:1134/2330 train_time:65674ms step_avg:57.91ms
step:1135/2330 train_time:65731ms step_avg:57.91ms
step:1136/2330 train_time:65791ms step_avg:57.91ms
step:1137/2330 train_time:65848ms step_avg:57.91ms
step:1138/2330 train_time:65908ms step_avg:57.92ms
step:1139/2330 train_time:65965ms step_avg:57.91ms
step:1140/2330 train_time:66026ms step_avg:57.92ms
step:1141/2330 train_time:66083ms step_avg:57.92ms
step:1142/2330 train_time:66143ms step_avg:57.92ms
step:1143/2330 train_time:66199ms step_avg:57.92ms
step:1144/2330 train_time:66260ms step_avg:57.92ms
step:1145/2330 train_time:66317ms step_avg:57.92ms
step:1146/2330 train_time:66377ms step_avg:57.92ms
step:1147/2330 train_time:66434ms step_avg:57.92ms
step:1148/2330 train_time:66493ms step_avg:57.92ms
step:1149/2330 train_time:66550ms step_avg:57.92ms
step:1150/2330 train_time:66611ms step_avg:57.92ms
step:1151/2330 train_time:66667ms step_avg:57.92ms
step:1152/2330 train_time:66728ms step_avg:57.92ms
step:1153/2330 train_time:66785ms step_avg:57.92ms
step:1154/2330 train_time:66844ms step_avg:57.92ms
step:1155/2330 train_time:66901ms step_avg:57.92ms
step:1156/2330 train_time:66962ms step_avg:57.93ms
step:1157/2330 train_time:67018ms step_avg:57.92ms
step:1158/2330 train_time:67079ms step_avg:57.93ms
step:1159/2330 train_time:67136ms step_avg:57.93ms
step:1160/2330 train_time:67196ms step_avg:57.93ms
step:1161/2330 train_time:67252ms step_avg:57.93ms
step:1162/2330 train_time:67313ms step_avg:57.93ms
step:1163/2330 train_time:67370ms step_avg:57.93ms
step:1164/2330 train_time:67430ms step_avg:57.93ms
step:1165/2330 train_time:67487ms step_avg:57.93ms
step:1166/2330 train_time:67546ms step_avg:57.93ms
step:1167/2330 train_time:67602ms step_avg:57.93ms
step:1168/2330 train_time:67663ms step_avg:57.93ms
step:1169/2330 train_time:67720ms step_avg:57.93ms
step:1170/2330 train_time:67781ms step_avg:57.93ms
step:1171/2330 train_time:67837ms step_avg:57.93ms
step:1172/2330 train_time:67898ms step_avg:57.93ms
step:1173/2330 train_time:67955ms step_avg:57.93ms
step:1174/2330 train_time:68016ms step_avg:57.93ms
step:1175/2330 train_time:68073ms step_avg:57.93ms
step:1176/2330 train_time:68132ms step_avg:57.94ms
step:1177/2330 train_time:68190ms step_avg:57.94ms
step:1178/2330 train_time:68249ms step_avg:57.94ms
step:1179/2330 train_time:68305ms step_avg:57.94ms
step:1180/2330 train_time:68366ms step_avg:57.94ms
step:1181/2330 train_time:68422ms step_avg:57.94ms
step:1182/2330 train_time:68482ms step_avg:57.94ms
step:1183/2330 train_time:68539ms step_avg:57.94ms
step:1184/2330 train_time:68599ms step_avg:57.94ms
step:1185/2330 train_time:68656ms step_avg:57.94ms
step:1186/2330 train_time:68716ms step_avg:57.94ms
step:1187/2330 train_time:68773ms step_avg:57.94ms
step:1188/2330 train_time:68832ms step_avg:57.94ms
step:1189/2330 train_time:68889ms step_avg:57.94ms
step:1190/2330 train_time:68949ms step_avg:57.94ms
step:1191/2330 train_time:69005ms step_avg:57.94ms
step:1192/2330 train_time:69066ms step_avg:57.94ms
step:1193/2330 train_time:69123ms step_avg:57.94ms
step:1194/2330 train_time:69184ms step_avg:57.94ms
step:1195/2330 train_time:69240ms step_avg:57.94ms
step:1196/2330 train_time:69301ms step_avg:57.94ms
step:1197/2330 train_time:69357ms step_avg:57.94ms
step:1198/2330 train_time:69418ms step_avg:57.94ms
step:1199/2330 train_time:69474ms step_avg:57.94ms
step:1200/2330 train_time:69534ms step_avg:57.95ms
step:1201/2330 train_time:69590ms step_avg:57.94ms
step:1202/2330 train_time:69651ms step_avg:57.95ms
step:1203/2330 train_time:69708ms step_avg:57.94ms
step:1204/2330 train_time:69767ms step_avg:57.95ms
step:1205/2330 train_time:69824ms step_avg:57.94ms
step:1206/2330 train_time:69884ms step_avg:57.95ms
step:1207/2330 train_time:69940ms step_avg:57.95ms
step:1208/2330 train_time:70002ms step_avg:57.95ms
step:1209/2330 train_time:70058ms step_avg:57.95ms
step:1210/2330 train_time:70119ms step_avg:57.95ms
step:1211/2330 train_time:70175ms step_avg:57.95ms
step:1212/2330 train_time:70236ms step_avg:57.95ms
step:1213/2330 train_time:70293ms step_avg:57.95ms
step:1214/2330 train_time:70354ms step_avg:57.95ms
step:1215/2330 train_time:70410ms step_avg:57.95ms
step:1216/2330 train_time:70470ms step_avg:57.95ms
step:1217/2330 train_time:70526ms step_avg:57.95ms
step:1218/2330 train_time:70588ms step_avg:57.95ms
step:1219/2330 train_time:70644ms step_avg:57.95ms
step:1220/2330 train_time:70705ms step_avg:57.95ms
step:1221/2330 train_time:70761ms step_avg:57.95ms
step:1222/2330 train_time:70822ms step_avg:57.96ms
step:1223/2330 train_time:70879ms step_avg:57.95ms
step:1224/2330 train_time:70939ms step_avg:57.96ms
step:1225/2330 train_time:70996ms step_avg:57.96ms
step:1226/2330 train_time:71056ms step_avg:57.96ms
step:1227/2330 train_time:71112ms step_avg:57.96ms
step:1228/2330 train_time:71172ms step_avg:57.96ms
step:1229/2330 train_time:71229ms step_avg:57.96ms
step:1230/2330 train_time:71289ms step_avg:57.96ms
step:1231/2330 train_time:71345ms step_avg:57.96ms
step:1232/2330 train_time:71405ms step_avg:57.96ms
step:1233/2330 train_time:71462ms step_avg:57.96ms
step:1234/2330 train_time:71523ms step_avg:57.96ms
step:1235/2330 train_time:71580ms step_avg:57.96ms
step:1236/2330 train_time:71640ms step_avg:57.96ms
step:1237/2330 train_time:71696ms step_avg:57.96ms
step:1238/2330 train_time:71756ms step_avg:57.96ms
step:1239/2330 train_time:71813ms step_avg:57.96ms
step:1240/2330 train_time:71873ms step_avg:57.96ms
step:1241/2330 train_time:71930ms step_avg:57.96ms
step:1242/2330 train_time:71990ms step_avg:57.96ms
step:1243/2330 train_time:72046ms step_avg:57.96ms
step:1244/2330 train_time:72106ms step_avg:57.96ms
step:1245/2330 train_time:72162ms step_avg:57.96ms
step:1246/2330 train_time:72222ms step_avg:57.96ms
step:1247/2330 train_time:72279ms step_avg:57.96ms
step:1248/2330 train_time:72340ms step_avg:57.96ms
step:1249/2330 train_time:72396ms step_avg:57.96ms
step:1250/2330 train_time:72458ms step_avg:57.97ms
step:1250/2330 val_loss:3.9890 train_time:72538ms step_avg:58.03ms
step:1251/2330 train_time:72558ms step_avg:58.00ms
step:1252/2330 train_time:72579ms step_avg:57.97ms
step:1253/2330 train_time:72635ms step_avg:57.97ms
step:1254/2330 train_time:72702ms step_avg:57.98ms
step:1255/2330 train_time:72760ms step_avg:57.98ms
step:1256/2330 train_time:72823ms step_avg:57.98ms
step:1257/2330 train_time:72880ms step_avg:57.98ms
step:1258/2330 train_time:72941ms step_avg:57.98ms
step:1259/2330 train_time:72997ms step_avg:57.98ms
step:1260/2330 train_time:73057ms step_avg:57.98ms
step:1261/2330 train_time:73113ms step_avg:57.98ms
step:1262/2330 train_time:73172ms step_avg:57.98ms
step:1263/2330 train_time:73228ms step_avg:57.98ms
step:1264/2330 train_time:73289ms step_avg:57.98ms
step:1265/2330 train_time:73345ms step_avg:57.98ms
step:1266/2330 train_time:73404ms step_avg:57.98ms
step:1267/2330 train_time:73460ms step_avg:57.98ms
step:1268/2330 train_time:73520ms step_avg:57.98ms
step:1269/2330 train_time:73578ms step_avg:57.98ms
step:1270/2330 train_time:73640ms step_avg:57.98ms
step:1271/2330 train_time:73697ms step_avg:57.98ms
step:1272/2330 train_time:73760ms step_avg:57.99ms
step:1273/2330 train_time:73818ms step_avg:57.99ms
step:1274/2330 train_time:73879ms step_avg:57.99ms
step:1275/2330 train_time:73935ms step_avg:57.99ms
step:1276/2330 train_time:73996ms step_avg:57.99ms
step:1277/2330 train_time:74052ms step_avg:57.99ms
step:1278/2330 train_time:74113ms step_avg:57.99ms
step:1279/2330 train_time:74169ms step_avg:57.99ms
step:1280/2330 train_time:74229ms step_avg:57.99ms
step:1281/2330 train_time:74285ms step_avg:57.99ms
step:1282/2330 train_time:74345ms step_avg:57.99ms
step:1283/2330 train_time:74401ms step_avg:57.99ms
step:1284/2330 train_time:74461ms step_avg:57.99ms
step:1285/2330 train_time:74517ms step_avg:57.99ms
step:1286/2330 train_time:74579ms step_avg:57.99ms
step:1287/2330 train_time:74636ms step_avg:57.99ms
step:1288/2330 train_time:74697ms step_avg:57.99ms
step:1289/2330 train_time:74754ms step_avg:57.99ms
step:1290/2330 train_time:74816ms step_avg:58.00ms
step:1291/2330 train_time:74873ms step_avg:58.00ms
step:1292/2330 train_time:74934ms step_avg:58.00ms
step:1293/2330 train_time:74990ms step_avg:58.00ms
step:1294/2330 train_time:75052ms step_avg:58.00ms
step:1295/2330 train_time:75108ms step_avg:58.00ms
step:1296/2330 train_time:75168ms step_avg:58.00ms
step:1297/2330 train_time:75224ms step_avg:58.00ms
step:1298/2330 train_time:75284ms step_avg:58.00ms
step:1299/2330 train_time:75340ms step_avg:58.00ms
step:1300/2330 train_time:75400ms step_avg:58.00ms
step:1301/2330 train_time:75457ms step_avg:58.00ms
step:1302/2330 train_time:75517ms step_avg:58.00ms
step:1303/2330 train_time:75574ms step_avg:58.00ms
step:1304/2330 train_time:75634ms step_avg:58.00ms
step:1305/2330 train_time:75691ms step_avg:58.00ms
step:1306/2330 train_time:75753ms step_avg:58.00ms
step:1307/2330 train_time:75810ms step_avg:58.00ms
step:1308/2330 train_time:75870ms step_avg:58.00ms
step:1309/2330 train_time:75927ms step_avg:58.00ms
step:1310/2330 train_time:75987ms step_avg:58.01ms
step:1311/2330 train_time:76043ms step_avg:58.00ms
step:1312/2330 train_time:76105ms step_avg:58.01ms
step:1313/2330 train_time:76162ms step_avg:58.01ms
step:1314/2330 train_time:76221ms step_avg:58.01ms
step:1315/2330 train_time:76277ms step_avg:58.01ms
step:1316/2330 train_time:76337ms step_avg:58.01ms
step:1317/2330 train_time:76393ms step_avg:58.01ms
step:1318/2330 train_time:76453ms step_avg:58.01ms
step:1319/2330 train_time:76509ms step_avg:58.01ms
step:1320/2330 train_time:76570ms step_avg:58.01ms
step:1321/2330 train_time:76627ms step_avg:58.01ms
step:1322/2330 train_time:76688ms step_avg:58.01ms
step:1323/2330 train_time:76745ms step_avg:58.01ms
step:1324/2330 train_time:76806ms step_avg:58.01ms
step:1325/2330 train_time:76864ms step_avg:58.01ms
step:1326/2330 train_time:76924ms step_avg:58.01ms
step:1327/2330 train_time:76981ms step_avg:58.01ms
step:1328/2330 train_time:77042ms step_avg:58.01ms
step:1329/2330 train_time:77099ms step_avg:58.01ms
step:1330/2330 train_time:77158ms step_avg:58.01ms
step:1331/2330 train_time:77215ms step_avg:58.01ms
step:1332/2330 train_time:77275ms step_avg:58.01ms
step:1333/2330 train_time:77331ms step_avg:58.01ms
step:1334/2330 train_time:77391ms step_avg:58.01ms
step:1335/2330 train_time:77447ms step_avg:58.01ms
step:1336/2330 train_time:77508ms step_avg:58.02ms
step:1337/2330 train_time:77565ms step_avg:58.01ms
step:1338/2330 train_time:77625ms step_avg:58.02ms
step:1339/2330 train_time:77682ms step_avg:58.02ms
step:1340/2330 train_time:77743ms step_avg:58.02ms
step:1341/2330 train_time:77800ms step_avg:58.02ms
step:1342/2330 train_time:77860ms step_avg:58.02ms
step:1343/2330 train_time:77917ms step_avg:58.02ms
step:1344/2330 train_time:77978ms step_avg:58.02ms
step:1345/2330 train_time:78034ms step_avg:58.02ms
step:1346/2330 train_time:78095ms step_avg:58.02ms
step:1347/2330 train_time:78152ms step_avg:58.02ms
step:1348/2330 train_time:78212ms step_avg:58.02ms
step:1349/2330 train_time:78268ms step_avg:58.02ms
step:1350/2330 train_time:78329ms step_avg:58.02ms
step:1351/2330 train_time:78385ms step_avg:58.02ms
step:1352/2330 train_time:78446ms step_avg:58.02ms
step:1353/2330 train_time:78502ms step_avg:58.02ms
step:1354/2330 train_time:78563ms step_avg:58.02ms
step:1355/2330 train_time:78619ms step_avg:58.02ms
step:1356/2330 train_time:78680ms step_avg:58.02ms
step:1357/2330 train_time:78736ms step_avg:58.02ms
step:1358/2330 train_time:78796ms step_avg:58.02ms
step:1359/2330 train_time:78853ms step_avg:58.02ms
step:1360/2330 train_time:78914ms step_avg:58.02ms
step:1361/2330 train_time:78970ms step_avg:58.02ms
step:1362/2330 train_time:79032ms step_avg:58.03ms
step:1363/2330 train_time:79088ms step_avg:58.03ms
step:1364/2330 train_time:79149ms step_avg:58.03ms
step:1365/2330 train_time:79206ms step_avg:58.03ms
step:1366/2330 train_time:79266ms step_avg:58.03ms
step:1367/2330 train_time:79323ms step_avg:58.03ms
step:1368/2330 train_time:79382ms step_avg:58.03ms
step:1369/2330 train_time:79439ms step_avg:58.03ms
step:1370/2330 train_time:79499ms step_avg:58.03ms
step:1371/2330 train_time:79556ms step_avg:58.03ms
step:1372/2330 train_time:79616ms step_avg:58.03ms
step:1373/2330 train_time:79673ms step_avg:58.03ms
step:1374/2330 train_time:79732ms step_avg:58.03ms
step:1375/2330 train_time:79789ms step_avg:58.03ms
step:1376/2330 train_time:79850ms step_avg:58.03ms
step:1377/2330 train_time:79906ms step_avg:58.03ms
step:1378/2330 train_time:79968ms step_avg:58.03ms
step:1379/2330 train_time:80025ms step_avg:58.03ms
step:1380/2330 train_time:80085ms step_avg:58.03ms
step:1381/2330 train_time:80142ms step_avg:58.03ms
step:1382/2330 train_time:80203ms step_avg:58.03ms
step:1383/2330 train_time:80259ms step_avg:58.03ms
step:1384/2330 train_time:80320ms step_avg:58.03ms
step:1385/2330 train_time:80376ms step_avg:58.03ms
step:1386/2330 train_time:80438ms step_avg:58.04ms
step:1387/2330 train_time:80494ms step_avg:58.03ms
step:1388/2330 train_time:80555ms step_avg:58.04ms
step:1389/2330 train_time:80613ms step_avg:58.04ms
step:1390/2330 train_time:80671ms step_avg:58.04ms
step:1391/2330 train_time:80727ms step_avg:58.04ms
step:1392/2330 train_time:80789ms step_avg:58.04ms
step:1393/2330 train_time:80845ms step_avg:58.04ms
step:1394/2330 train_time:80906ms step_avg:58.04ms
step:1395/2330 train_time:80963ms step_avg:58.04ms
step:1396/2330 train_time:81024ms step_avg:58.04ms
step:1397/2330 train_time:81080ms step_avg:58.04ms
step:1398/2330 train_time:81141ms step_avg:58.04ms
step:1399/2330 train_time:81198ms step_avg:58.04ms
step:1400/2330 train_time:81259ms step_avg:58.04ms
step:1401/2330 train_time:81316ms step_avg:58.04ms
step:1402/2330 train_time:81376ms step_avg:58.04ms
step:1403/2330 train_time:81433ms step_avg:58.04ms
step:1404/2330 train_time:81493ms step_avg:58.04ms
step:1405/2330 train_time:81550ms step_avg:58.04ms
step:1406/2330 train_time:81611ms step_avg:58.04ms
step:1407/2330 train_time:81667ms step_avg:58.04ms
step:1408/2330 train_time:81728ms step_avg:58.05ms
step:1409/2330 train_time:81784ms step_avg:58.04ms
step:1410/2330 train_time:81845ms step_avg:58.05ms
step:1411/2330 train_time:81902ms step_avg:58.05ms
step:1412/2330 train_time:81962ms step_avg:58.05ms
step:1413/2330 train_time:82019ms step_avg:58.05ms
step:1414/2330 train_time:82079ms step_avg:58.05ms
step:1415/2330 train_time:82136ms step_avg:58.05ms
step:1416/2330 train_time:82196ms step_avg:58.05ms
step:1417/2330 train_time:82253ms step_avg:58.05ms
step:1418/2330 train_time:82314ms step_avg:58.05ms
step:1419/2330 train_time:82370ms step_avg:58.05ms
step:1420/2330 train_time:82430ms step_avg:58.05ms
step:1421/2330 train_time:82486ms step_avg:58.05ms
step:1422/2330 train_time:82548ms step_avg:58.05ms
step:1423/2330 train_time:82605ms step_avg:58.05ms
step:1424/2330 train_time:82665ms step_avg:58.05ms
step:1425/2330 train_time:82722ms step_avg:58.05ms
step:1426/2330 train_time:82782ms step_avg:58.05ms
step:1427/2330 train_time:82839ms step_avg:58.05ms
step:1428/2330 train_time:82899ms step_avg:58.05ms
step:1429/2330 train_time:82956ms step_avg:58.05ms
step:1430/2330 train_time:83017ms step_avg:58.05ms
step:1431/2330 train_time:83073ms step_avg:58.05ms
step:1432/2330 train_time:83135ms step_avg:58.05ms
step:1433/2330 train_time:83191ms step_avg:58.05ms
step:1434/2330 train_time:83252ms step_avg:58.06ms
step:1435/2330 train_time:83309ms step_avg:58.05ms
step:1436/2330 train_time:83370ms step_avg:58.06ms
step:1437/2330 train_time:83426ms step_avg:58.06ms
step:1438/2330 train_time:83487ms step_avg:58.06ms
step:1439/2330 train_time:83543ms step_avg:58.06ms
step:1440/2330 train_time:83605ms step_avg:58.06ms
step:1441/2330 train_time:83662ms step_avg:58.06ms
step:1442/2330 train_time:83722ms step_avg:58.06ms
step:1443/2330 train_time:83778ms step_avg:58.06ms
step:1444/2330 train_time:83838ms step_avg:58.06ms
step:1445/2330 train_time:83895ms step_avg:58.06ms
step:1446/2330 train_time:83955ms step_avg:58.06ms
step:1447/2330 train_time:84011ms step_avg:58.06ms
step:1448/2330 train_time:84071ms step_avg:58.06ms
step:1449/2330 train_time:84128ms step_avg:58.06ms
step:1450/2330 train_time:84189ms step_avg:58.06ms
step:1451/2330 train_time:84247ms step_avg:58.06ms
step:1452/2330 train_time:84306ms step_avg:58.06ms
step:1453/2330 train_time:84362ms step_avg:58.06ms
step:1454/2330 train_time:84423ms step_avg:58.06ms
step:1455/2330 train_time:84479ms step_avg:58.06ms
step:1456/2330 train_time:84540ms step_avg:58.06ms
step:1457/2330 train_time:84597ms step_avg:58.06ms
step:1458/2330 train_time:84658ms step_avg:58.06ms
step:1459/2330 train_time:84714ms step_avg:58.06ms
step:1460/2330 train_time:84775ms step_avg:58.07ms
step:1461/2330 train_time:84831ms step_avg:58.06ms
step:1462/2330 train_time:84893ms step_avg:58.07ms
step:1463/2330 train_time:84949ms step_avg:58.07ms
step:1464/2330 train_time:85010ms step_avg:58.07ms
step:1465/2330 train_time:85067ms step_avg:58.07ms
step:1466/2330 train_time:85128ms step_avg:58.07ms
step:1467/2330 train_time:85185ms step_avg:58.07ms
step:1468/2330 train_time:85245ms step_avg:58.07ms
step:1469/2330 train_time:85302ms step_avg:58.07ms
step:1470/2330 train_time:85362ms step_avg:58.07ms
step:1471/2330 train_time:85419ms step_avg:58.07ms
step:1472/2330 train_time:85479ms step_avg:58.07ms
step:1473/2330 train_time:85535ms step_avg:58.07ms
step:1474/2330 train_time:85597ms step_avg:58.07ms
step:1475/2330 train_time:85653ms step_avg:58.07ms
step:1476/2330 train_time:85713ms step_avg:58.07ms
step:1477/2330 train_time:85770ms step_avg:58.07ms
step:1478/2330 train_time:85830ms step_avg:58.07ms
step:1479/2330 train_time:85887ms step_avg:58.07ms
step:1480/2330 train_time:85947ms step_avg:58.07ms
step:1481/2330 train_time:86004ms step_avg:58.07ms
step:1482/2330 train_time:86064ms step_avg:58.07ms
step:1483/2330 train_time:86121ms step_avg:58.07ms
step:1484/2330 train_time:86181ms step_avg:58.07ms
step:1485/2330 train_time:86237ms step_avg:58.07ms
step:1486/2330 train_time:86298ms step_avg:58.07ms
step:1487/2330 train_time:86354ms step_avg:58.07ms
step:1488/2330 train_time:86416ms step_avg:58.08ms
step:1489/2330 train_time:86472ms step_avg:58.07ms
step:1490/2330 train_time:86532ms step_avg:58.07ms
step:1491/2330 train_time:86588ms step_avg:58.07ms
step:1492/2330 train_time:86649ms step_avg:58.08ms
step:1493/2330 train_time:86705ms step_avg:58.07ms
step:1494/2330 train_time:86766ms step_avg:58.08ms
step:1495/2330 train_time:86823ms step_avg:58.08ms
step:1496/2330 train_time:86884ms step_avg:58.08ms
step:1497/2330 train_time:86941ms step_avg:58.08ms
step:1498/2330 train_time:87001ms step_avg:58.08ms
step:1499/2330 train_time:87058ms step_avg:58.08ms
step:1500/2330 train_time:87119ms step_avg:58.08ms
step:1500/2330 val_loss:3.9049 train_time:87199ms step_avg:58.13ms
step:1501/2330 train_time:87219ms step_avg:58.11ms
step:1502/2330 train_time:87239ms step_avg:58.08ms
step:1503/2330 train_time:87301ms step_avg:58.08ms
step:1504/2330 train_time:87364ms step_avg:58.09ms
step:1505/2330 train_time:87421ms step_avg:58.09ms
step:1506/2330 train_time:87483ms step_avg:58.09ms
step:1507/2330 train_time:87539ms step_avg:58.09ms
step:1508/2330 train_time:87599ms step_avg:58.09ms
step:1509/2330 train_time:87655ms step_avg:58.09ms
step:1510/2330 train_time:87715ms step_avg:58.09ms
step:1511/2330 train_time:87771ms step_avg:58.09ms
step:1512/2330 train_time:87830ms step_avg:58.09ms
step:1513/2330 train_time:87886ms step_avg:58.09ms
step:1514/2330 train_time:87945ms step_avg:58.09ms
step:1515/2330 train_time:88001ms step_avg:58.09ms
step:1516/2330 train_time:88060ms step_avg:58.09ms
step:1517/2330 train_time:88117ms step_avg:58.09ms
step:1518/2330 train_time:88177ms step_avg:58.09ms
step:1519/2330 train_time:88237ms step_avg:58.09ms
step:1520/2330 train_time:88298ms step_avg:58.09ms
step:1521/2330 train_time:88356ms step_avg:58.09ms
step:1522/2330 train_time:88417ms step_avg:58.09ms
step:1523/2330 train_time:88474ms step_avg:58.09ms
step:1524/2330 train_time:88536ms step_avg:58.09ms
step:1525/2330 train_time:88593ms step_avg:58.09ms
step:1526/2330 train_time:88653ms step_avg:58.10ms
step:1527/2330 train_time:88709ms step_avg:58.09ms
step:1528/2330 train_time:88768ms step_avg:58.09ms
step:1529/2330 train_time:88826ms step_avg:58.09ms
step:1530/2330 train_time:88884ms step_avg:58.09ms
step:1531/2330 train_time:88941ms step_avg:58.09ms
step:1532/2330 train_time:89002ms step_avg:58.10ms
step:1533/2330 train_time:89059ms step_avg:58.09ms
step:1534/2330 train_time:89120ms step_avg:58.10ms
step:1535/2330 train_time:89177ms step_avg:58.10ms
step:1536/2330 train_time:89239ms step_avg:58.10ms
step:1537/2330 train_time:89297ms step_avg:58.10ms
step:1538/2330 train_time:89358ms step_avg:58.10ms
step:1539/2330 train_time:89415ms step_avg:58.10ms
step:1540/2330 train_time:89477ms step_avg:58.10ms
step:1541/2330 train_time:89535ms step_avg:58.10ms
step:1542/2330 train_time:89596ms step_avg:58.10ms
step:1543/2330 train_time:89655ms step_avg:58.10ms
step:1544/2330 train_time:89715ms step_avg:58.11ms
step:1545/2330 train_time:89772ms step_avg:58.10ms
step:1546/2330 train_time:89832ms step_avg:58.11ms
step:1547/2330 train_time:89889ms step_avg:58.11ms
step:1548/2330 train_time:89949ms step_avg:58.11ms
step:1549/2330 train_time:90006ms step_avg:58.11ms
step:1550/2330 train_time:90066ms step_avg:58.11ms
step:1551/2330 train_time:90122ms step_avg:58.11ms
step:1552/2330 train_time:90185ms step_avg:58.11ms
step:1553/2330 train_time:90242ms step_avg:58.11ms
step:1554/2330 train_time:90305ms step_avg:58.11ms
step:1555/2330 train_time:90362ms step_avg:58.11ms
step:1556/2330 train_time:90425ms step_avg:58.11ms
step:1557/2330 train_time:90482ms step_avg:58.11ms
step:1558/2330 train_time:90544ms step_avg:58.12ms
step:1559/2330 train_time:90601ms step_avg:58.11ms
step:1560/2330 train_time:90664ms step_avg:58.12ms
step:1561/2330 train_time:90721ms step_avg:58.12ms
step:1562/2330 train_time:90782ms step_avg:58.12ms
step:1563/2330 train_time:90840ms step_avg:58.12ms
step:1564/2330 train_time:90900ms step_avg:58.12ms
step:1565/2330 train_time:90957ms step_avg:58.12ms
step:1566/2330 train_time:91017ms step_avg:58.12ms
step:1567/2330 train_time:91075ms step_avg:58.12ms
step:1568/2330 train_time:91135ms step_avg:58.12ms
step:1569/2330 train_time:91193ms step_avg:58.12ms
step:1570/2330 train_time:91253ms step_avg:58.12ms
step:1571/2330 train_time:91310ms step_avg:58.12ms
step:1572/2330 train_time:91371ms step_avg:58.12ms
step:1573/2330 train_time:91429ms step_avg:58.12ms
step:1574/2330 train_time:91490ms step_avg:58.13ms
step:1575/2330 train_time:91547ms step_avg:58.12ms
step:1576/2330 train_time:91609ms step_avg:58.13ms
step:1577/2330 train_time:91666ms step_avg:58.13ms
step:1578/2330 train_time:91727ms step_avg:58.13ms
step:1579/2330 train_time:91784ms step_avg:58.13ms
step:1580/2330 train_time:91846ms step_avg:58.13ms
step:1581/2330 train_time:91902ms step_avg:58.13ms
step:1582/2330 train_time:91964ms step_avg:58.13ms
step:1583/2330 train_time:92020ms step_avg:58.13ms
step:1584/2330 train_time:92082ms step_avg:58.13ms
step:1585/2330 train_time:92139ms step_avg:58.13ms
step:1586/2330 train_time:92199ms step_avg:58.13ms
step:1587/2330 train_time:92257ms step_avg:58.13ms
step:1588/2330 train_time:92317ms step_avg:58.13ms
step:1589/2330 train_time:92374ms step_avg:58.13ms
step:1590/2330 train_time:92437ms step_avg:58.14ms
step:1591/2330 train_time:92495ms step_avg:58.14ms
step:1592/2330 train_time:92556ms step_avg:58.14ms
step:1593/2330 train_time:92615ms step_avg:58.14ms
step:1594/2330 train_time:92676ms step_avg:58.14ms
step:1595/2330 train_time:92733ms step_avg:58.14ms
step:1596/2330 train_time:92794ms step_avg:58.14ms
step:1597/2330 train_time:92851ms step_avg:58.14ms
step:1598/2330 train_time:92910ms step_avg:58.14ms
step:1599/2330 train_time:92966ms step_avg:58.14ms
step:1600/2330 train_time:93028ms step_avg:58.14ms
step:1601/2330 train_time:93085ms step_avg:58.14ms
step:1602/2330 train_time:93147ms step_avg:58.14ms
step:1603/2330 train_time:93204ms step_avg:58.14ms
step:1604/2330 train_time:93266ms step_avg:58.15ms
step:1605/2330 train_time:93323ms step_avg:58.15ms
step:1606/2330 train_time:93384ms step_avg:58.15ms
step:1607/2330 train_time:93441ms step_avg:58.15ms
step:1608/2330 train_time:93503ms step_avg:58.15ms
step:1609/2330 train_time:93560ms step_avg:58.15ms
step:1610/2330 train_time:93622ms step_avg:58.15ms
step:1611/2330 train_time:93678ms step_avg:58.15ms
step:1612/2330 train_time:93740ms step_avg:58.15ms
step:1613/2330 train_time:93797ms step_avg:58.15ms
step:1614/2330 train_time:93858ms step_avg:58.15ms
step:1615/2330 train_time:93916ms step_avg:58.15ms
step:1616/2330 train_time:93978ms step_avg:58.15ms
step:1617/2330 train_time:94035ms step_avg:58.15ms
step:1618/2330 train_time:94097ms step_avg:58.16ms
step:1619/2330 train_time:94154ms step_avg:58.16ms
step:1620/2330 train_time:94213ms step_avg:58.16ms
step:1621/2330 train_time:94270ms step_avg:58.16ms
step:1622/2330 train_time:94331ms step_avg:58.16ms
step:1623/2330 train_time:94387ms step_avg:58.16ms
step:1624/2330 train_time:94450ms step_avg:58.16ms
step:1625/2330 train_time:94507ms step_avg:58.16ms
step:1626/2330 train_time:94568ms step_avg:58.16ms
step:1627/2330 train_time:94625ms step_avg:58.16ms
step:1628/2330 train_time:94686ms step_avg:58.16ms
step:1629/2330 train_time:94743ms step_avg:58.16ms
step:1630/2330 train_time:94805ms step_avg:58.16ms
step:1631/2330 train_time:94861ms step_avg:58.16ms
step:1632/2330 train_time:94923ms step_avg:58.16ms
step:1633/2330 train_time:94979ms step_avg:58.16ms
step:1634/2330 train_time:95041ms step_avg:58.16ms
step:1635/2330 train_time:95098ms step_avg:58.16ms
step:1636/2330 train_time:95159ms step_avg:58.17ms
step:1637/2330 train_time:95218ms step_avg:58.17ms
step:1638/2330 train_time:95278ms step_avg:58.17ms
step:1639/2330 train_time:95335ms step_avg:58.17ms
step:1640/2330 train_time:95397ms step_avg:58.17ms
step:1641/2330 train_time:95454ms step_avg:58.17ms
step:1642/2330 train_time:95516ms step_avg:58.17ms
step:1643/2330 train_time:95574ms step_avg:58.17ms
step:1644/2330 train_time:95635ms step_avg:58.17ms
step:1645/2330 train_time:95693ms step_avg:58.17ms
step:1646/2330 train_time:95753ms step_avg:58.17ms
step:1647/2330 train_time:95811ms step_avg:58.17ms
step:1648/2330 train_time:95871ms step_avg:58.17ms
step:1649/2330 train_time:95928ms step_avg:58.17ms
step:1650/2330 train_time:95989ms step_avg:58.18ms
step:1651/2330 train_time:96046ms step_avg:58.17ms
step:1652/2330 train_time:96107ms step_avg:58.18ms
step:1653/2330 train_time:96163ms step_avg:58.17ms
step:1654/2330 train_time:96226ms step_avg:58.18ms
step:1655/2330 train_time:96282ms step_avg:58.18ms
step:1656/2330 train_time:96345ms step_avg:58.18ms
step:1657/2330 train_time:96402ms step_avg:58.18ms
step:1658/2330 train_time:96463ms step_avg:58.18ms
step:1659/2330 train_time:96520ms step_avg:58.18ms
step:1660/2330 train_time:96581ms step_avg:58.18ms
step:1661/2330 train_time:96639ms step_avg:58.18ms
step:1662/2330 train_time:96699ms step_avg:58.18ms
step:1663/2330 train_time:96757ms step_avg:58.18ms
step:1664/2330 train_time:96818ms step_avg:58.18ms
step:1665/2330 train_time:96875ms step_avg:58.18ms
step:1666/2330 train_time:96936ms step_avg:58.18ms
step:1667/2330 train_time:96994ms step_avg:58.19ms
step:1668/2330 train_time:97055ms step_avg:58.19ms
step:1669/2330 train_time:97112ms step_avg:58.19ms
step:1670/2330 train_time:97173ms step_avg:58.19ms
step:1671/2330 train_time:97230ms step_avg:58.19ms
step:1672/2330 train_time:97291ms step_avg:58.19ms
step:1673/2330 train_time:97348ms step_avg:58.19ms
step:1674/2330 train_time:97410ms step_avg:58.19ms
step:1675/2330 train_time:97467ms step_avg:58.19ms
step:1676/2330 train_time:97529ms step_avg:58.19ms
step:1677/2330 train_time:97585ms step_avg:58.19ms
step:1678/2330 train_time:97650ms step_avg:58.19ms
step:1679/2330 train_time:97706ms step_avg:58.19ms
step:1680/2330 train_time:97767ms step_avg:58.19ms
step:1681/2330 train_time:97825ms step_avg:58.19ms
step:1682/2330 train_time:97886ms step_avg:58.20ms
step:1683/2330 train_time:97942ms step_avg:58.20ms
step:1684/2330 train_time:98004ms step_avg:58.20ms
step:1685/2330 train_time:98062ms step_avg:58.20ms
step:1686/2330 train_time:98122ms step_avg:58.20ms
step:1687/2330 train_time:98179ms step_avg:58.20ms
step:1688/2330 train_time:98240ms step_avg:58.20ms
step:1689/2330 train_time:98298ms step_avg:58.20ms
step:1690/2330 train_time:98359ms step_avg:58.20ms
step:1691/2330 train_time:98417ms step_avg:58.20ms
step:1692/2330 train_time:98477ms step_avg:58.20ms
step:1693/2330 train_time:98535ms step_avg:58.20ms
step:1694/2330 train_time:98596ms step_avg:58.20ms
step:1695/2330 train_time:98654ms step_avg:58.20ms
step:1696/2330 train_time:98714ms step_avg:58.20ms
step:1697/2330 train_time:98772ms step_avg:58.20ms
step:1698/2330 train_time:98832ms step_avg:58.20ms
step:1699/2330 train_time:98889ms step_avg:58.20ms
step:1700/2330 train_time:98950ms step_avg:58.21ms
step:1701/2330 train_time:99006ms step_avg:58.20ms
step:1702/2330 train_time:99067ms step_avg:58.21ms
step:1703/2330 train_time:99124ms step_avg:58.21ms
step:1704/2330 train_time:99186ms step_avg:58.21ms
step:1705/2330 train_time:99242ms step_avg:58.21ms
step:1706/2330 train_time:99305ms step_avg:58.21ms
step:1707/2330 train_time:99361ms step_avg:58.21ms
step:1708/2330 train_time:99425ms step_avg:58.21ms
step:1709/2330 train_time:99481ms step_avg:58.21ms
step:1710/2330 train_time:99542ms step_avg:58.21ms
step:1711/2330 train_time:99600ms step_avg:58.21ms
step:1712/2330 train_time:99660ms step_avg:58.21ms
step:1713/2330 train_time:99718ms step_avg:58.21ms
step:1714/2330 train_time:99779ms step_avg:58.21ms
step:1715/2330 train_time:99836ms step_avg:58.21ms
step:1716/2330 train_time:99897ms step_avg:58.22ms
step:1717/2330 train_time:99954ms step_avg:58.21ms
step:1718/2330 train_time:100015ms step_avg:58.22ms
step:1719/2330 train_time:100073ms step_avg:58.22ms
step:1720/2330 train_time:100133ms step_avg:58.22ms
step:1721/2330 train_time:100190ms step_avg:58.22ms
step:1722/2330 train_time:100252ms step_avg:58.22ms
step:1723/2330 train_time:100308ms step_avg:58.22ms
step:1724/2330 train_time:100370ms step_avg:58.22ms
step:1725/2330 train_time:100427ms step_avg:58.22ms
step:1726/2330 train_time:100488ms step_avg:58.22ms
step:1727/2330 train_time:100545ms step_avg:58.22ms
step:1728/2330 train_time:100607ms step_avg:58.22ms
step:1729/2330 train_time:100663ms step_avg:58.22ms
step:1730/2330 train_time:100725ms step_avg:58.22ms
step:1731/2330 train_time:100782ms step_avg:58.22ms
step:1732/2330 train_time:100844ms step_avg:58.22ms
step:1733/2330 train_time:100900ms step_avg:58.22ms
step:1734/2330 train_time:100961ms step_avg:58.22ms
step:1735/2330 train_time:101018ms step_avg:58.22ms
step:1736/2330 train_time:101079ms step_avg:58.23ms
step:1737/2330 train_time:101137ms step_avg:58.23ms
step:1738/2330 train_time:101198ms step_avg:58.23ms
step:1739/2330 train_time:101255ms step_avg:58.23ms
step:1740/2330 train_time:101316ms step_avg:58.23ms
step:1741/2330 train_time:101374ms step_avg:58.23ms
step:1742/2330 train_time:101435ms step_avg:58.23ms
step:1743/2330 train_time:101493ms step_avg:58.23ms
step:1744/2330 train_time:101553ms step_avg:58.23ms
step:1745/2330 train_time:101610ms step_avg:58.23ms
step:1746/2330 train_time:101671ms step_avg:58.23ms
step:1747/2330 train_time:101728ms step_avg:58.23ms
step:1748/2330 train_time:101788ms step_avg:58.23ms
step:1749/2330 train_time:101845ms step_avg:58.23ms
step:1750/2330 train_time:101907ms step_avg:58.23ms
step:1750/2330 val_loss:3.8205 train_time:101991ms step_avg:58.28ms
step:1751/2330 train_time:102010ms step_avg:58.26ms
step:1752/2330 train_time:102030ms step_avg:58.24ms
step:1753/2330 train_time:102084ms step_avg:58.23ms
step:1754/2330 train_time:102152ms step_avg:58.24ms
step:1755/2330 train_time:102208ms step_avg:58.24ms
step:1756/2330 train_time:102270ms step_avg:58.24ms
step:1757/2330 train_time:102327ms step_avg:58.24ms
step:1758/2330 train_time:102386ms step_avg:58.24ms
step:1759/2330 train_time:102443ms step_avg:58.24ms
step:1760/2330 train_time:102502ms step_avg:58.24ms
step:1761/2330 train_time:102559ms step_avg:58.24ms
step:1762/2330 train_time:102619ms step_avg:58.24ms
step:1763/2330 train_time:102675ms step_avg:58.24ms
step:1764/2330 train_time:102735ms step_avg:58.24ms
step:1765/2330 train_time:102791ms step_avg:58.24ms
step:1766/2330 train_time:102851ms step_avg:58.24ms
step:1767/2330 train_time:102913ms step_avg:58.24ms
step:1768/2330 train_time:102979ms step_avg:58.25ms
step:1769/2330 train_time:103038ms step_avg:58.25ms
step:1770/2330 train_time:103098ms step_avg:58.25ms
step:1771/2330 train_time:103156ms step_avg:58.25ms
step:1772/2330 train_time:103217ms step_avg:58.25ms
step:1773/2330 train_time:103274ms step_avg:58.25ms
step:1774/2330 train_time:103336ms step_avg:58.25ms
step:1775/2330 train_time:103392ms step_avg:58.25ms
step:1776/2330 train_time:103454ms step_avg:58.25ms
step:1777/2330 train_time:103511ms step_avg:58.25ms
step:1778/2330 train_time:103571ms step_avg:58.25ms
step:1779/2330 train_time:103629ms step_avg:58.25ms
step:1780/2330 train_time:103689ms step_avg:58.25ms
step:1781/2330 train_time:103746ms step_avg:58.25ms
step:1782/2330 train_time:103806ms step_avg:58.25ms
step:1783/2330 train_time:103865ms step_avg:58.25ms
step:1784/2330 train_time:103926ms step_avg:58.25ms
step:1785/2330 train_time:103984ms step_avg:58.25ms
step:1786/2330 train_time:104048ms step_avg:58.26ms
step:1787/2330 train_time:104105ms step_avg:58.26ms
step:1788/2330 train_time:104168ms step_avg:58.26ms
step:1789/2330 train_time:104224ms step_avg:58.26ms
step:1790/2330 train_time:104286ms step_avg:58.26ms
step:1791/2330 train_time:104342ms step_avg:58.26ms
step:1792/2330 train_time:104404ms step_avg:58.26ms
step:1793/2330 train_time:104461ms step_avg:58.26ms
step:1794/2330 train_time:104522ms step_avg:58.26ms
step:1795/2330 train_time:104578ms step_avg:58.26ms
step:1796/2330 train_time:104639ms step_avg:58.26ms
step:1797/2330 train_time:104695ms step_avg:58.26ms
step:1798/2330 train_time:104756ms step_avg:58.26ms
step:1799/2330 train_time:104813ms step_avg:58.26ms
step:1800/2330 train_time:104875ms step_avg:58.26ms
step:1801/2330 train_time:104933ms step_avg:58.26ms
step:1802/2330 train_time:104993ms step_avg:58.26ms
step:1803/2330 train_time:105051ms step_avg:58.26ms
step:1804/2330 train_time:105114ms step_avg:58.27ms
step:1805/2330 train_time:105171ms step_avg:58.27ms
step:1806/2330 train_time:105234ms step_avg:58.27ms
step:1807/2330 train_time:105292ms step_avg:58.27ms
step:1808/2330 train_time:105352ms step_avg:58.27ms
step:1809/2330 train_time:105409ms step_avg:58.27ms
step:1810/2330 train_time:105471ms step_avg:58.27ms
step:1811/2330 train_time:105528ms step_avg:58.27ms
step:1812/2330 train_time:105588ms step_avg:58.27ms
step:1813/2330 train_time:105645ms step_avg:58.27ms
step:1814/2330 train_time:105705ms step_avg:58.27ms
step:1815/2330 train_time:105762ms step_avg:58.27ms
step:1816/2330 train_time:105822ms step_avg:58.27ms
step:1817/2330 train_time:105880ms step_avg:58.27ms
step:1818/2330 train_time:105941ms step_avg:58.27ms
step:1819/2330 train_time:105998ms step_avg:58.27ms
step:1820/2330 train_time:106061ms step_avg:58.28ms
step:1821/2330 train_time:106118ms step_avg:58.27ms
step:1822/2330 train_time:106181ms step_avg:58.28ms
step:1823/2330 train_time:106237ms step_avg:58.28ms
step:1824/2330 train_time:106299ms step_avg:58.28ms
step:1825/2330 train_time:106356ms step_avg:58.28ms
step:1826/2330 train_time:106417ms step_avg:58.28ms
step:1827/2330 train_time:106474ms step_avg:58.28ms
step:1828/2330 train_time:106536ms step_avg:58.28ms
step:1829/2330 train_time:106593ms step_avg:58.28ms
step:1830/2330 train_time:106653ms step_avg:58.28ms
step:1831/2330 train_time:106711ms step_avg:58.28ms
step:1832/2330 train_time:106772ms step_avg:58.28ms
step:1833/2330 train_time:106829ms step_avg:58.28ms
step:1834/2330 train_time:106891ms step_avg:58.28ms
step:1835/2330 train_time:106948ms step_avg:58.28ms
step:1836/2330 train_time:107010ms step_avg:58.28ms
step:1837/2330 train_time:107069ms step_avg:58.28ms
step:1838/2330 train_time:107130ms step_avg:58.29ms
step:1839/2330 train_time:107187ms step_avg:58.29ms
step:1840/2330 train_time:107248ms step_avg:58.29ms
step:1841/2330 train_time:107305ms step_avg:58.29ms
step:1842/2330 train_time:107366ms step_avg:58.29ms
step:1843/2330 train_time:107422ms step_avg:58.29ms
step:1844/2330 train_time:107484ms step_avg:58.29ms
step:1845/2330 train_time:107540ms step_avg:58.29ms
step:1846/2330 train_time:107601ms step_avg:58.29ms
step:1847/2330 train_time:107658ms step_avg:58.29ms
step:1848/2330 train_time:107718ms step_avg:58.29ms
step:1849/2330 train_time:107775ms step_avg:58.29ms
step:1850/2330 train_time:107837ms step_avg:58.29ms
step:1851/2330 train_time:107894ms step_avg:58.29ms
step:1852/2330 train_time:107956ms step_avg:58.29ms
step:1853/2330 train_time:108014ms step_avg:58.29ms
step:1854/2330 train_time:108075ms step_avg:58.29ms
step:1855/2330 train_time:108132ms step_avg:58.29ms
step:1856/2330 train_time:108194ms step_avg:58.29ms
step:1857/2330 train_time:108252ms step_avg:58.29ms
step:1858/2330 train_time:108312ms step_avg:58.30ms
step:1859/2330 train_time:108370ms step_avg:58.29ms
step:1860/2330 train_time:108431ms step_avg:58.30ms
step:1861/2330 train_time:108489ms step_avg:58.30ms
step:1862/2330 train_time:108550ms step_avg:58.30ms
step:1863/2330 train_time:108607ms step_avg:58.30ms
step:1864/2330 train_time:108667ms step_avg:58.30ms
step:1865/2330 train_time:108724ms step_avg:58.30ms
step:1866/2330 train_time:108786ms step_avg:58.30ms
step:1867/2330 train_time:108843ms step_avg:58.30ms
step:1868/2330 train_time:108904ms step_avg:58.30ms
step:1869/2330 train_time:108961ms step_avg:58.30ms
step:1870/2330 train_time:109023ms step_avg:58.30ms
step:1871/2330 train_time:109079ms step_avg:58.30ms
step:1872/2330 train_time:109140ms step_avg:58.30ms
step:1873/2330 train_time:109197ms step_avg:58.30ms
step:1874/2330 train_time:109260ms step_avg:58.30ms
step:1875/2330 train_time:109316ms step_avg:58.30ms
step:1876/2330 train_time:109378ms step_avg:58.30ms
step:1877/2330 train_time:109435ms step_avg:58.30ms
step:1878/2330 train_time:109496ms step_avg:58.30ms
step:1879/2330 train_time:109553ms step_avg:58.30ms
step:1880/2330 train_time:109614ms step_avg:58.31ms
step:1881/2330 train_time:109672ms step_avg:58.31ms
step:1882/2330 train_time:109733ms step_avg:58.31ms
step:1883/2330 train_time:109790ms step_avg:58.31ms
step:1884/2330 train_time:109852ms step_avg:58.31ms
step:1885/2330 train_time:109910ms step_avg:58.31ms
step:1886/2330 train_time:109970ms step_avg:58.31ms
step:1887/2330 train_time:110029ms step_avg:58.31ms
step:1888/2330 train_time:110089ms step_avg:58.31ms
step:1889/2330 train_time:110147ms step_avg:58.31ms
step:1890/2330 train_time:110208ms step_avg:58.31ms
step:1891/2330 train_time:110264ms step_avg:58.31ms
step:1892/2330 train_time:110326ms step_avg:58.31ms
step:1893/2330 train_time:110384ms step_avg:58.31ms
step:1894/2330 train_time:110445ms step_avg:58.31ms
step:1895/2330 train_time:110501ms step_avg:58.31ms
step:1896/2330 train_time:110563ms step_avg:58.31ms
step:1897/2330 train_time:110619ms step_avg:58.31ms
step:1898/2330 train_time:110682ms step_avg:58.32ms
step:1899/2330 train_time:110739ms step_avg:58.31ms
step:1900/2330 train_time:110800ms step_avg:58.32ms
step:1901/2330 train_time:110857ms step_avg:58.31ms
step:1902/2330 train_time:110918ms step_avg:58.32ms
step:1903/2330 train_time:110975ms step_avg:58.32ms
step:1904/2330 train_time:111036ms step_avg:58.32ms
step:1905/2330 train_time:111093ms step_avg:58.32ms
step:1906/2330 train_time:111155ms step_avg:58.32ms
step:1907/2330 train_time:111213ms step_avg:58.32ms
step:1908/2330 train_time:111274ms step_avg:58.32ms
step:1909/2330 train_time:111332ms step_avg:58.32ms
step:1910/2330 train_time:111393ms step_avg:58.32ms
step:1911/2330 train_time:111451ms step_avg:58.32ms
step:1912/2330 train_time:111511ms step_avg:58.32ms
step:1913/2330 train_time:111569ms step_avg:58.32ms
step:1914/2330 train_time:111630ms step_avg:58.32ms
step:1915/2330 train_time:111688ms step_avg:58.32ms
step:1916/2330 train_time:111748ms step_avg:58.32ms
step:1917/2330 train_time:111805ms step_avg:58.32ms
step:1918/2330 train_time:111866ms step_avg:58.32ms
step:1919/2330 train_time:111923ms step_avg:58.32ms
step:1920/2330 train_time:111984ms step_avg:58.32ms
step:1921/2330 train_time:112041ms step_avg:58.32ms
step:1922/2330 train_time:112101ms step_avg:58.33ms
step:1923/2330 train_time:112158ms step_avg:58.32ms
step:1924/2330 train_time:112220ms step_avg:58.33ms
step:1925/2330 train_time:112277ms step_avg:58.33ms
step:1926/2330 train_time:112339ms step_avg:58.33ms
step:1927/2330 train_time:112396ms step_avg:58.33ms
step:1928/2330 train_time:112459ms step_avg:58.33ms
step:1929/2330 train_time:112515ms step_avg:58.33ms
step:1930/2330 train_time:112577ms step_avg:58.33ms
step:1931/2330 train_time:112634ms step_avg:58.33ms
step:1932/2330 train_time:112695ms step_avg:58.33ms
step:1933/2330 train_time:112752ms step_avg:58.33ms
step:1934/2330 train_time:112812ms step_avg:58.33ms
step:1935/2330 train_time:112870ms step_avg:58.33ms
step:1936/2330 train_time:112931ms step_avg:58.33ms
step:1937/2330 train_time:112989ms step_avg:58.33ms
step:1938/2330 train_time:113050ms step_avg:58.33ms
step:1939/2330 train_time:113107ms step_avg:58.33ms
step:1940/2330 train_time:113169ms step_avg:58.33ms
step:1941/2330 train_time:113227ms step_avg:58.33ms
step:1942/2330 train_time:113288ms step_avg:58.34ms
step:1943/2330 train_time:113345ms step_avg:58.34ms
step:1944/2330 train_time:113405ms step_avg:58.34ms
step:1945/2330 train_time:113462ms step_avg:58.34ms
step:1946/2330 train_time:113524ms step_avg:58.34ms
step:1947/2330 train_time:113580ms step_avg:58.34ms
step:1948/2330 train_time:113642ms step_avg:58.34ms
step:1949/2330 train_time:113699ms step_avg:58.34ms
step:1950/2330 train_time:113761ms step_avg:58.34ms
step:1951/2330 train_time:113817ms step_avg:58.34ms
step:1952/2330 train_time:113881ms step_avg:58.34ms
step:1953/2330 train_time:113937ms step_avg:58.34ms
step:1954/2330 train_time:113998ms step_avg:58.34ms
step:1955/2330 train_time:114054ms step_avg:58.34ms
step:1956/2330 train_time:114116ms step_avg:58.34ms
step:1957/2330 train_time:114174ms step_avg:58.34ms
step:1958/2330 train_time:114235ms step_avg:58.34ms
step:1959/2330 train_time:114293ms step_avg:58.34ms
step:1960/2330 train_time:114355ms step_avg:58.34ms
step:1961/2330 train_time:114412ms step_avg:58.34ms
step:1962/2330 train_time:114473ms step_avg:58.35ms
step:1963/2330 train_time:114532ms step_avg:58.35ms
step:1964/2330 train_time:114592ms step_avg:58.35ms
step:1965/2330 train_time:114650ms step_avg:58.35ms
step:1966/2330 train_time:114710ms step_avg:58.35ms
step:1967/2330 train_time:114768ms step_avg:58.35ms
step:1968/2330 train_time:114830ms step_avg:58.35ms
step:1969/2330 train_time:114887ms step_avg:58.35ms
step:1970/2330 train_time:114948ms step_avg:58.35ms
step:1971/2330 train_time:115005ms step_avg:58.35ms
step:1972/2330 train_time:115067ms step_avg:58.35ms
step:1973/2330 train_time:115123ms step_avg:58.35ms
step:1974/2330 train_time:115185ms step_avg:58.35ms
step:1975/2330 train_time:115242ms step_avg:58.35ms
step:1976/2330 train_time:115304ms step_avg:58.35ms
step:1977/2330 train_time:115360ms step_avg:58.35ms
step:1978/2330 train_time:115422ms step_avg:58.35ms
step:1979/2330 train_time:115479ms step_avg:58.35ms
step:1980/2330 train_time:115540ms step_avg:58.35ms
step:1981/2330 train_time:115597ms step_avg:58.35ms
step:1982/2330 train_time:115658ms step_avg:58.35ms
step:1983/2330 train_time:115715ms step_avg:58.35ms
step:1984/2330 train_time:115777ms step_avg:58.36ms
step:1985/2330 train_time:115833ms step_avg:58.35ms
step:1986/2330 train_time:115895ms step_avg:58.36ms
step:1987/2330 train_time:115952ms step_avg:58.36ms
step:1988/2330 train_time:116013ms step_avg:58.36ms
step:1989/2330 train_time:116071ms step_avg:58.36ms
step:1990/2330 train_time:116132ms step_avg:58.36ms
step:1991/2330 train_time:116189ms step_avg:58.36ms
step:1992/2330 train_time:116251ms step_avg:58.36ms
step:1993/2330 train_time:116309ms step_avg:58.36ms
step:1994/2330 train_time:116371ms step_avg:58.36ms
step:1995/2330 train_time:116428ms step_avg:58.36ms
step:1996/2330 train_time:116489ms step_avg:58.36ms
step:1997/2330 train_time:116546ms step_avg:58.36ms
step:1998/2330 train_time:116608ms step_avg:58.36ms
step:1999/2330 train_time:116665ms step_avg:58.36ms
step:2000/2330 train_time:116728ms step_avg:58.36ms
step:2000/2330 val_loss:3.7582 train_time:116810ms step_avg:58.41ms
step:2001/2330 train_time:116829ms step_avg:58.39ms
step:2002/2330 train_time:116849ms step_avg:58.37ms
step:2003/2330 train_time:116908ms step_avg:58.37ms
step:2004/2330 train_time:116972ms step_avg:58.37ms
step:2005/2330 train_time:117029ms step_avg:58.37ms
step:2006/2330 train_time:117094ms step_avg:58.37ms
step:2007/2330 train_time:117151ms step_avg:58.37ms
step:2008/2330 train_time:117212ms step_avg:58.37ms
step:2009/2330 train_time:117268ms step_avg:58.37ms
step:2010/2330 train_time:117329ms step_avg:58.37ms
step:2011/2330 train_time:117385ms step_avg:58.37ms
step:2012/2330 train_time:117445ms step_avg:58.37ms
step:2013/2330 train_time:117501ms step_avg:58.37ms
step:2014/2330 train_time:117561ms step_avg:58.37ms
step:2015/2330 train_time:117618ms step_avg:58.37ms
step:2016/2330 train_time:117678ms step_avg:58.37ms
step:2017/2330 train_time:117734ms step_avg:58.37ms
step:2018/2330 train_time:117797ms step_avg:58.37ms
step:2019/2330 train_time:117855ms step_avg:58.37ms
step:2020/2330 train_time:117919ms step_avg:58.38ms
step:2021/2330 train_time:117978ms step_avg:58.38ms
step:2022/2330 train_time:118042ms step_avg:58.38ms
step:2023/2330 train_time:118099ms step_avg:58.38ms
step:2024/2330 train_time:118160ms step_avg:58.38ms
step:2025/2330 train_time:118217ms step_avg:58.38ms
step:2026/2330 train_time:118278ms step_avg:58.38ms
step:2027/2330 train_time:118335ms step_avg:58.38ms
step:2028/2330 train_time:118395ms step_avg:58.38ms
step:2029/2330 train_time:118451ms step_avg:58.38ms
step:2030/2330 train_time:118512ms step_avg:58.38ms
step:2031/2330 train_time:118569ms step_avg:58.38ms
step:2032/2330 train_time:118629ms step_avg:58.38ms
step:2033/2330 train_time:118686ms step_avg:58.38ms
step:2034/2330 train_time:118747ms step_avg:58.38ms
step:2035/2330 train_time:118804ms step_avg:58.38ms
step:2036/2330 train_time:118866ms step_avg:58.38ms
step:2037/2330 train_time:118923ms step_avg:58.38ms
step:2038/2330 train_time:118987ms step_avg:58.38ms
step:2039/2330 train_time:119044ms step_avg:58.38ms
step:2040/2330 train_time:119106ms step_avg:58.39ms
step:2041/2330 train_time:119165ms step_avg:58.39ms
step:2042/2330 train_time:119226ms step_avg:58.39ms
step:2043/2330 train_time:119284ms step_avg:58.39ms
step:2044/2330 train_time:119344ms step_avg:58.39ms
step:2045/2330 train_time:119401ms step_avg:58.39ms
step:2046/2330 train_time:119463ms step_avg:58.39ms
step:2047/2330 train_time:119520ms step_avg:58.39ms
step:2048/2330 train_time:119580ms step_avg:58.39ms
step:2049/2330 train_time:119637ms step_avg:58.39ms
step:2050/2330 train_time:119697ms step_avg:58.39ms
step:2051/2330 train_time:119754ms step_avg:58.39ms
step:2052/2330 train_time:119815ms step_avg:58.39ms
step:2053/2330 train_time:119872ms step_avg:58.39ms
step:2054/2330 train_time:119935ms step_avg:58.39ms
step:2055/2330 train_time:119991ms step_avg:58.39ms
step:2056/2330 train_time:120055ms step_avg:58.39ms
step:2057/2330 train_time:120111ms step_avg:58.39ms
step:2058/2330 train_time:120173ms step_avg:58.39ms
step:2059/2330 train_time:120230ms step_avg:58.39ms
step:2060/2330 train_time:120291ms step_avg:58.39ms
step:2061/2330 train_time:120348ms step_avg:58.39ms
step:2062/2330 train_time:120409ms step_avg:58.39ms
step:2063/2330 train_time:120466ms step_avg:58.39ms
step:2064/2330 train_time:120527ms step_avg:58.39ms
step:2065/2330 train_time:120584ms step_avg:58.39ms
step:2066/2330 train_time:120645ms step_avg:58.40ms
step:2067/2330 train_time:120702ms step_avg:58.40ms
step:2068/2330 train_time:120764ms step_avg:58.40ms
step:2069/2330 train_time:120822ms step_avg:58.40ms
step:2070/2330 train_time:120883ms step_avg:58.40ms
step:2071/2330 train_time:120941ms step_avg:58.40ms
step:2072/2330 train_time:121003ms step_avg:58.40ms
step:2073/2330 train_time:121061ms step_avg:58.40ms
step:2074/2330 train_time:121122ms step_avg:58.40ms
step:2075/2330 train_time:121180ms step_avg:58.40ms
step:2076/2330 train_time:121241ms step_avg:58.40ms
step:2077/2330 train_time:121299ms step_avg:58.40ms
step:2078/2330 train_time:121358ms step_avg:58.40ms
step:2079/2330 train_time:121415ms step_avg:58.40ms
step:2080/2330 train_time:121476ms step_avg:58.40ms
step:2081/2330 train_time:121533ms step_avg:58.40ms
step:2082/2330 train_time:121594ms step_avg:58.40ms
step:2083/2330 train_time:121651ms step_avg:58.40ms
step:2084/2330 train_time:121712ms step_avg:58.40ms
step:2085/2330 train_time:121769ms step_avg:58.40ms
step:2086/2330 train_time:121830ms step_avg:58.40ms
step:2087/2330 train_time:121887ms step_avg:58.40ms
step:2088/2330 train_time:121950ms step_avg:58.41ms
step:2089/2330 train_time:122006ms step_avg:58.40ms
step:2090/2330 train_time:122069ms step_avg:58.41ms
step:2091/2330 train_time:122127ms step_avg:58.41ms
step:2092/2330 train_time:122187ms step_avg:58.41ms
step:2093/2330 train_time:122245ms step_avg:58.41ms
step:2094/2330 train_time:122306ms step_avg:58.41ms
step:2095/2330 train_time:122363ms step_avg:58.41ms
step:2096/2330 train_time:122424ms step_avg:58.41ms
step:2097/2330 train_time:122481ms step_avg:58.41ms
step:2098/2330 train_time:122541ms step_avg:58.41ms
step:2099/2330 train_time:122600ms step_avg:58.41ms
step:2100/2330 train_time:122661ms step_avg:58.41ms
step:2101/2330 train_time:122718ms step_avg:58.41ms
step:2102/2330 train_time:122780ms step_avg:58.41ms
step:2103/2330 train_time:122838ms step_avg:58.41ms
step:2104/2330 train_time:122898ms step_avg:58.41ms
step:2105/2330 train_time:122955ms step_avg:58.41ms
step:2106/2330 train_time:123017ms step_avg:58.41ms
step:2107/2330 train_time:123073ms step_avg:58.41ms
step:2108/2330 train_time:123136ms step_avg:58.41ms
step:2109/2330 train_time:123193ms step_avg:58.41ms
step:2110/2330 train_time:123253ms step_avg:58.41ms
step:2111/2330 train_time:123310ms step_avg:58.41ms
step:2112/2330 train_time:123372ms step_avg:58.41ms
step:2113/2330 train_time:123428ms step_avg:58.41ms
step:2114/2330 train_time:123491ms step_avg:58.42ms
step:2115/2330 train_time:123548ms step_avg:58.42ms
step:2116/2330 train_time:123609ms step_avg:58.42ms
step:2117/2330 train_time:123666ms step_avg:58.42ms
step:2118/2330 train_time:123728ms step_avg:58.42ms
step:2119/2330 train_time:123785ms step_avg:58.42ms
step:2120/2330 train_time:123845ms step_avg:58.42ms
step:2121/2330 train_time:123903ms step_avg:58.42ms
step:2122/2330 train_time:123964ms step_avg:58.42ms
step:2123/2330 train_time:124022ms step_avg:58.42ms
step:2124/2330 train_time:124085ms step_avg:58.42ms
step:2125/2330 train_time:124143ms step_avg:58.42ms
step:2126/2330 train_time:124203ms step_avg:58.42ms
step:2127/2330 train_time:124261ms step_avg:58.42ms
step:2128/2330 train_time:124322ms step_avg:58.42ms
step:2129/2330 train_time:124380ms step_avg:58.42ms
step:2130/2330 train_time:124442ms step_avg:58.42ms
step:2131/2330 train_time:124500ms step_avg:58.42ms
step:2132/2330 train_time:124561ms step_avg:58.42ms
step:2133/2330 train_time:124618ms step_avg:58.42ms
step:2134/2330 train_time:124680ms step_avg:58.43ms
step:2135/2330 train_time:124738ms step_avg:58.43ms
step:2136/2330 train_time:124798ms step_avg:58.43ms
step:2137/2330 train_time:124855ms step_avg:58.43ms
step:2138/2330 train_time:124916ms step_avg:58.43ms
step:2139/2330 train_time:124973ms step_avg:58.43ms
step:2140/2330 train_time:125035ms step_avg:58.43ms
step:2141/2330 train_time:125091ms step_avg:58.43ms
step:2142/2330 train_time:125154ms step_avg:58.43ms
step:2143/2330 train_time:125210ms step_avg:58.43ms
step:2144/2330 train_time:125271ms step_avg:58.43ms
step:2145/2330 train_time:125328ms step_avg:58.43ms
step:2146/2330 train_time:125391ms step_avg:58.43ms
step:2147/2330 train_time:125447ms step_avg:58.43ms
step:2148/2330 train_time:125509ms step_avg:58.43ms
step:2149/2330 train_time:125566ms step_avg:58.43ms
step:2150/2330 train_time:125627ms step_avg:58.43ms
step:2151/2330 train_time:125684ms step_avg:58.43ms
step:2152/2330 train_time:125745ms step_avg:58.43ms
step:2153/2330 train_time:125803ms step_avg:58.43ms
step:2154/2330 train_time:125864ms step_avg:58.43ms
step:2155/2330 train_time:125922ms step_avg:58.43ms
step:2156/2330 train_time:125982ms step_avg:58.43ms
step:2157/2330 train_time:126042ms step_avg:58.43ms
step:2158/2330 train_time:126102ms step_avg:58.43ms
step:2159/2330 train_time:126160ms step_avg:58.43ms
step:2160/2330 train_time:126222ms step_avg:58.44ms
step:2161/2330 train_time:126279ms step_avg:58.44ms
step:2162/2330 train_time:126341ms step_avg:58.44ms
step:2163/2330 train_time:126399ms step_avg:58.44ms
step:2164/2330 train_time:126460ms step_avg:58.44ms
step:2165/2330 train_time:126517ms step_avg:58.44ms
step:2166/2330 train_time:126578ms step_avg:58.44ms
step:2167/2330 train_time:126636ms step_avg:58.44ms
step:2168/2330 train_time:126697ms step_avg:58.44ms
step:2169/2330 train_time:126753ms step_avg:58.44ms
step:2170/2330 train_time:126815ms step_avg:58.44ms
step:2171/2330 train_time:126872ms step_avg:58.44ms
step:2172/2330 train_time:126934ms step_avg:58.44ms
step:2173/2330 train_time:126991ms step_avg:58.44ms
step:2174/2330 train_time:127052ms step_avg:58.44ms
step:2175/2330 train_time:127108ms step_avg:58.44ms
step:2176/2330 train_time:127170ms step_avg:58.44ms
step:2177/2330 train_time:127227ms step_avg:58.44ms
step:2178/2330 train_time:127288ms step_avg:58.44ms
step:2179/2330 train_time:127345ms step_avg:58.44ms
step:2180/2330 train_time:127407ms step_avg:58.44ms
step:2181/2330 train_time:127464ms step_avg:58.44ms
step:2182/2330 train_time:127526ms step_avg:58.44ms
step:2183/2330 train_time:127583ms step_avg:58.44ms
step:2184/2330 train_time:127644ms step_avg:58.44ms
step:2185/2330 train_time:127702ms step_avg:58.44ms
step:2186/2330 train_time:127763ms step_avg:58.45ms
step:2187/2330 train_time:127821ms step_avg:58.45ms
step:2188/2330 train_time:127882ms step_avg:58.45ms
step:2189/2330 train_time:127941ms step_avg:58.45ms
step:2190/2330 train_time:128002ms step_avg:58.45ms
step:2191/2330 train_time:128060ms step_avg:58.45ms
step:2192/2330 train_time:128121ms step_avg:58.45ms
step:2193/2330 train_time:128177ms step_avg:58.45ms
step:2194/2330 train_time:128239ms step_avg:58.45ms
step:2195/2330 train_time:128295ms step_avg:58.45ms
step:2196/2330 train_time:128357ms step_avg:58.45ms
step:2197/2330 train_time:128413ms step_avg:58.45ms
step:2198/2330 train_time:128475ms step_avg:58.45ms
step:2199/2330 train_time:128531ms step_avg:58.45ms
step:2200/2330 train_time:128594ms step_avg:58.45ms
step:2201/2330 train_time:128651ms step_avg:58.45ms
step:2202/2330 train_time:128712ms step_avg:58.45ms
step:2203/2330 train_time:128769ms step_avg:58.45ms
step:2204/2330 train_time:128831ms step_avg:58.45ms
step:2205/2330 train_time:128888ms step_avg:58.45ms
step:2206/2330 train_time:128949ms step_avg:58.45ms
step:2207/2330 train_time:129006ms step_avg:58.45ms
step:2208/2330 train_time:129068ms step_avg:58.45ms
step:2209/2330 train_time:129125ms step_avg:58.45ms
step:2210/2330 train_time:129186ms step_avg:58.46ms
step:2211/2330 train_time:129244ms step_avg:58.46ms
step:2212/2330 train_time:129304ms step_avg:58.46ms
step:2213/2330 train_time:129362ms step_avg:58.46ms
step:2214/2330 train_time:129423ms step_avg:58.46ms
step:2215/2330 train_time:129481ms step_avg:58.46ms
step:2216/2330 train_time:129541ms step_avg:58.46ms
step:2217/2330 train_time:129605ms step_avg:58.46ms
step:2218/2330 train_time:129660ms step_avg:58.46ms
step:2219/2330 train_time:129717ms step_avg:58.46ms
step:2220/2330 train_time:129778ms step_avg:58.46ms
step:2221/2330 train_time:129835ms step_avg:58.46ms
step:2222/2330 train_time:129896ms step_avg:58.46ms
step:2223/2330 train_time:129952ms step_avg:58.46ms
step:2224/2330 train_time:130014ms step_avg:58.46ms
step:2225/2330 train_time:130070ms step_avg:58.46ms
step:2226/2330 train_time:130133ms step_avg:58.46ms
step:2227/2330 train_time:130189ms step_avg:58.46ms
step:2228/2330 train_time:130252ms step_avg:58.46ms
step:2229/2330 train_time:130308ms step_avg:58.46ms
step:2230/2330 train_time:130371ms step_avg:58.46ms
step:2231/2330 train_time:130429ms step_avg:58.46ms
step:2232/2330 train_time:130492ms step_avg:58.46ms
step:2233/2330 train_time:130549ms step_avg:58.46ms
step:2234/2330 train_time:130611ms step_avg:58.47ms
step:2235/2330 train_time:130668ms step_avg:58.46ms
step:2236/2330 train_time:130729ms step_avg:58.47ms
step:2237/2330 train_time:130787ms step_avg:58.47ms
step:2238/2330 train_time:130847ms step_avg:58.47ms
step:2239/2330 train_time:130905ms step_avg:58.47ms
step:2240/2330 train_time:130966ms step_avg:58.47ms
step:2241/2330 train_time:131023ms step_avg:58.47ms
step:2242/2330 train_time:131084ms step_avg:58.47ms
step:2243/2330 train_time:131141ms step_avg:58.47ms
step:2244/2330 train_time:131202ms step_avg:58.47ms
step:2245/2330 train_time:131260ms step_avg:58.47ms
step:2246/2330 train_time:131320ms step_avg:58.47ms
step:2247/2330 train_time:131378ms step_avg:58.47ms
step:2248/2330 train_time:131438ms step_avg:58.47ms
step:2249/2330 train_time:131495ms step_avg:58.47ms
step:2250/2330 train_time:131558ms step_avg:58.47ms
step:2250/2330 val_loss:3.7106 train_time:131641ms step_avg:58.51ms
step:2251/2330 train_time:131660ms step_avg:58.49ms
step:2252/2330 train_time:131680ms step_avg:58.47ms
step:2253/2330 train_time:131739ms step_avg:58.47ms
step:2254/2330 train_time:131804ms step_avg:58.48ms
step:2255/2330 train_time:131862ms step_avg:58.48ms
step:2256/2330 train_time:131925ms step_avg:58.48ms
step:2257/2330 train_time:131982ms step_avg:58.48ms
step:2258/2330 train_time:132043ms step_avg:58.48ms
step:2259/2330 train_time:132099ms step_avg:58.48ms
step:2260/2330 train_time:132160ms step_avg:58.48ms
step:2261/2330 train_time:132217ms step_avg:58.48ms
step:2262/2330 train_time:132277ms step_avg:58.48ms
step:2263/2330 train_time:132333ms step_avg:58.48ms
step:2264/2330 train_time:132394ms step_avg:58.48ms
step:2265/2330 train_time:132451ms step_avg:58.48ms
step:2266/2330 train_time:132511ms step_avg:58.48ms
step:2267/2330 train_time:132568ms step_avg:58.48ms
step:2268/2330 train_time:132629ms step_avg:58.48ms
step:2269/2330 train_time:132688ms step_avg:58.48ms
step:2270/2330 train_time:132751ms step_avg:58.48ms
step:2271/2330 train_time:132809ms step_avg:58.48ms
step:2272/2330 train_time:132872ms step_avg:58.48ms
step:2273/2330 train_time:132930ms step_avg:58.48ms
step:2274/2330 train_time:132990ms step_avg:58.48ms
step:2275/2330 train_time:133048ms step_avg:58.48ms
step:2276/2330 train_time:133108ms step_avg:58.48ms
step:2277/2330 train_time:133167ms step_avg:58.48ms
step:2278/2330 train_time:133226ms step_avg:58.48ms
step:2279/2330 train_time:133283ms step_avg:58.48ms
step:2280/2330 train_time:133344ms step_avg:58.48ms
step:2281/2330 train_time:133400ms step_avg:58.48ms
step:2282/2330 train_time:133460ms step_avg:58.48ms
step:2283/2330 train_time:133517ms step_avg:58.48ms
step:2284/2330 train_time:133577ms step_avg:58.48ms
step:2285/2330 train_time:133634ms step_avg:58.48ms
step:2286/2330 train_time:133696ms step_avg:58.48ms
step:2287/2330 train_time:133754ms step_avg:58.48ms
step:2288/2330 train_time:133818ms step_avg:58.49ms
step:2289/2330 train_time:133874ms step_avg:58.49ms
step:2290/2330 train_time:133937ms step_avg:58.49ms
step:2291/2330 train_time:133993ms step_avg:58.49ms
step:2292/2330 train_time:134056ms step_avg:58.49ms
step:2293/2330 train_time:134113ms step_avg:58.49ms
step:2294/2330 train_time:134175ms step_avg:58.49ms
step:2295/2330 train_time:134232ms step_avg:58.49ms
step:2296/2330 train_time:134293ms step_avg:58.49ms
step:2297/2330 train_time:134350ms step_avg:58.49ms
step:2298/2330 train_time:134410ms step_avg:58.49ms
step:2299/2330 train_time:134469ms step_avg:58.49ms
step:2300/2330 train_time:134529ms step_avg:58.49ms
step:2301/2330 train_time:134587ms step_avg:58.49ms
step:2302/2330 train_time:134648ms step_avg:58.49ms
step:2303/2330 train_time:134706ms step_avg:58.49ms
step:2304/2330 train_time:134767ms step_avg:58.49ms
step:2305/2330 train_time:134825ms step_avg:58.49ms
step:2306/2330 train_time:134887ms step_avg:58.49ms
step:2307/2330 train_time:134944ms step_avg:58.49ms
step:2308/2330 train_time:135005ms step_avg:58.49ms
step:2309/2330 train_time:135063ms step_avg:58.49ms
step:2310/2330 train_time:135123ms step_avg:58.50ms
step:2311/2330 train_time:135181ms step_avg:58.49ms
step:2312/2330 train_time:135242ms step_avg:58.50ms
step:2313/2330 train_time:135298ms step_avg:58.49ms
step:2314/2330 train_time:135361ms step_avg:58.50ms
step:2315/2330 train_time:135418ms step_avg:58.50ms
step:2316/2330 train_time:135478ms step_avg:58.50ms
step:2317/2330 train_time:135534ms step_avg:58.50ms
step:2318/2330 train_time:135596ms step_avg:58.50ms
step:2319/2330 train_time:135653ms step_avg:58.50ms
step:2320/2330 train_time:135715ms step_avg:58.50ms
step:2321/2330 train_time:135772ms step_avg:58.50ms
step:2322/2330 train_time:135833ms step_avg:58.50ms
step:2323/2330 train_time:135890ms step_avg:58.50ms
step:2324/2330 train_time:135952ms step_avg:58.50ms
step:2325/2330 train_time:136010ms step_avg:58.50ms
step:2326/2330 train_time:136071ms step_avg:58.50ms
step:2327/2330 train_time:136128ms step_avg:58.50ms
step:2328/2330 train_time:136190ms step_avg:58.50ms
step:2329/2330 train_time:136248ms step_avg:58.50ms
step:2330/2330 train_time:136308ms step_avg:58.50ms
step:2330/2330 val_loss:3.6957 train_time:136390ms step_avg:58.54ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
