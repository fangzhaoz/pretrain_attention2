import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--wd', type=str, required=True)
args2 = parser.parse_args()

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    Code adapted from https://github.com/NoahAmsel/PolarExpress/tree/main by @varunneal.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU. 
    Note: A later PR replaced Newton-Shulz with Polar Express for the orthogonalization step

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as 
    needed on the forward pass. This enables attn and mlp weights to be contained within the same 
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable 
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn. 
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.label == 'attn']
        non_attn_subset = [p for p in params if p.label != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups
    
    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn 
        and mlp params when a param group is split across GPUs.
        """
        label_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: label_ranks.get(x.label))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                param = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter param.
                state = self.state[param]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(param)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply polar_express independently to Q,K,V,O
            param_idx = start_idx if start_idx < len(params) else 0
            if getattr(params[param_idx], 'label', None) == 'attn':
                for p in params[param_idx:param_idx+chunk_size]:
                    assert getattr(params[param_idx], 'label', None)=='attn', "GPU cannot mix attn and mlp params"
                batch = 4 * original_shape[0]
                d1 = original_shape[1] 
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = polar_express(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = polar_express(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                grad = param.grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for param in params:
                reduce_scatter_futures[idx].wait()
                rank_size = param.shape[0] // world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=param.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label='mlp'
        self.c_proj.label='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        n = len(self.blocks) // 2

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            # since layer 0 is skipped, layer 11 does not have skip_connection
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2290  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: int = 0.45  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs_adamw_small_wd_tuning", exist_ok=True)
    logfile = f"logs_adamw_small_wd_tuning/{args2.wd}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
# optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.06, momentum=0.95, weight_decay=0.0)
optimizer2 = torch.optim.AdamW(hidden_matrix_params + gate_params, lr=6e-4,  betas=(0.8, 0.8), eps=1e-10, weight_decay=float(args2.wd), fused=True)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then linear decay
def get_lr(step: int):
    x = min(0.9999, step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # on final step return specific ws for validation
    if step == args.num_iterations:
        return args.ws_validate // 2, args.ws_validate
    x = min(step / (1 + args.num_scheduled_iterations), 0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(args.ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = args.ws_schedule[0]
    else:
        new_ws_long = args.ws_schedule[ws_idx]  
        if new_ws_long > ws_long:
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs_adamw_small_wd_tuning/{args2.wd}.txt", exist_ok=True)
            torch.save(log, f"logs_adamw_small_wd_tuning/{args2.wd}.txt/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    step_optimizers(step, optimizers, model)
     
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

dist.destroy_process_group()
====================================================================================================
Running Python 3.12.12 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 20:16:04) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251105+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Tue Nov 11 06:23:14 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000001:00:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   6301MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000002:00:00.0 Off |                    0 |
| N/A   27C    P0             112W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000003:00:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000008:00:00.0 Off |                    0 |
| N/A   26C    P0             115W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000009:00:00.0 Off |                    0 |
| N/A   24C    P0             113W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 0000000A:00:00.0 Off |                    0 |
| N/A   27C    P0             113W / 700W |   1992MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 0000000B:00:00.0 Off |                    0 |
| N/A   25C    P0             110W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 0000000C:00:00.0 Off |                    0 |
| N/A   26C    P0             109W / 700W |   1994MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/2330 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2330 train_time:90ms step_avg:89.83ms
step:2/2330 train_time:196ms step_avg:97.92ms
step:3/2330 train_time:213ms step_avg:71.09ms
step:4/2330 train_time:233ms step_avg:58.16ms
step:5/2330 train_time:286ms step_avg:57.28ms
step:6/2330 train_time:344ms step_avg:57.40ms
step:7/2330 train_time:399ms step_avg:57.04ms
step:8/2330 train_time:458ms step_avg:57.29ms
step:9/2330 train_time:514ms step_avg:57.11ms
step:10/2330 train_time:572ms step_avg:57.22ms
step:11/2330 train_time:627ms step_avg:57.03ms
step:12/2330 train_time:686ms step_avg:57.14ms
step:13/2330 train_time:741ms step_avg:57.00ms
step:14/2330 train_time:799ms step_avg:57.07ms
step:15/2330 train_time:854ms step_avg:56.96ms
step:16/2330 train_time:913ms step_avg:57.04ms
step:17/2330 train_time:968ms step_avg:56.97ms
step:18/2330 train_time:1026ms step_avg:57.02ms
step:19/2330 train_time:1084ms step_avg:57.05ms
step:20/2330 train_time:1145ms step_avg:57.27ms
step:21/2330 train_time:1203ms step_avg:57.30ms
step:22/2330 train_time:1265ms step_avg:57.48ms
step:23/2330 train_time:1320ms step_avg:57.41ms
step:24/2330 train_time:1380ms step_avg:57.51ms
step:25/2330 train_time:1436ms step_avg:57.44ms
step:26/2330 train_time:1495ms step_avg:57.49ms
step:27/2330 train_time:1550ms step_avg:57.41ms
step:28/2330 train_time:1609ms step_avg:57.47ms
step:29/2330 train_time:1665ms step_avg:57.40ms
step:30/2330 train_time:1723ms step_avg:57.43ms
step:31/2330 train_time:1779ms step_avg:57.38ms
step:32/2330 train_time:1837ms step_avg:57.41ms
step:33/2330 train_time:1892ms step_avg:57.34ms
step:34/2330 train_time:1951ms step_avg:57.40ms
step:35/2330 train_time:2007ms step_avg:57.35ms
step:36/2330 train_time:2066ms step_avg:57.40ms
step:37/2330 train_time:2123ms step_avg:57.37ms
step:38/2330 train_time:2184ms step_avg:57.47ms
step:39/2330 train_time:2241ms step_avg:57.46ms
step:40/2330 train_time:2301ms step_avg:57.51ms
step:41/2330 train_time:2357ms step_avg:57.49ms
step:42/2330 train_time:2416ms step_avg:57.52ms
step:43/2330 train_time:2472ms step_avg:57.49ms
step:44/2330 train_time:2531ms step_avg:57.52ms
step:45/2330 train_time:2586ms step_avg:57.47ms
step:46/2330 train_time:2645ms step_avg:57.50ms
step:47/2330 train_time:2700ms step_avg:57.45ms
step:48/2330 train_time:2759ms step_avg:57.47ms
step:49/2330 train_time:2814ms step_avg:57.43ms
step:50/2330 train_time:2873ms step_avg:57.47ms
step:51/2330 train_time:2928ms step_avg:57.42ms
step:52/2330 train_time:2988ms step_avg:57.47ms
step:53/2330 train_time:3044ms step_avg:57.43ms
step:54/2330 train_time:3104ms step_avg:57.48ms
step:55/2330 train_time:3160ms step_avg:57.46ms
step:56/2330 train_time:3220ms step_avg:57.49ms
step:57/2330 train_time:3276ms step_avg:57.47ms
step:58/2330 train_time:3335ms step_avg:57.50ms
step:59/2330 train_time:3391ms step_avg:57.47ms
step:60/2330 train_time:3452ms step_avg:57.53ms
step:61/2330 train_time:3507ms step_avg:57.50ms
step:62/2330 train_time:3567ms step_avg:57.54ms
step:63/2330 train_time:3623ms step_avg:57.51ms
step:64/2330 train_time:3682ms step_avg:57.53ms
step:65/2330 train_time:3738ms step_avg:57.50ms
step:66/2330 train_time:3796ms step_avg:57.51ms
step:67/2330 train_time:3851ms step_avg:57.48ms
step:68/2330 train_time:3910ms step_avg:57.51ms
step:69/2330 train_time:3966ms step_avg:57.48ms
step:70/2330 train_time:4025ms step_avg:57.50ms
step:71/2330 train_time:4081ms step_avg:57.48ms
step:72/2330 train_time:4140ms step_avg:57.50ms
step:73/2330 train_time:4196ms step_avg:57.48ms
step:74/2330 train_time:4256ms step_avg:57.51ms
step:75/2330 train_time:4311ms step_avg:57.48ms
step:76/2330 train_time:4371ms step_avg:57.51ms
step:77/2330 train_time:4427ms step_avg:57.50ms
step:78/2330 train_time:4486ms step_avg:57.51ms
step:79/2330 train_time:4542ms step_avg:57.49ms
step:80/2330 train_time:4601ms step_avg:57.51ms
step:81/2330 train_time:4657ms step_avg:57.49ms
step:82/2330 train_time:4715ms step_avg:57.51ms
step:83/2330 train_time:4771ms step_avg:57.49ms
step:84/2330 train_time:4831ms step_avg:57.51ms
step:85/2330 train_time:4886ms step_avg:57.49ms
step:86/2330 train_time:4945ms step_avg:57.50ms
step:87/2330 train_time:5001ms step_avg:57.48ms
step:88/2330 train_time:5060ms step_avg:57.50ms
step:89/2330 train_time:5116ms step_avg:57.49ms
step:90/2330 train_time:5176ms step_avg:57.51ms
step:91/2330 train_time:5232ms step_avg:57.49ms
step:92/2330 train_time:5291ms step_avg:57.51ms
step:93/2330 train_time:5347ms step_avg:57.50ms
step:94/2330 train_time:5407ms step_avg:57.52ms
step:95/2330 train_time:5463ms step_avg:57.50ms
step:96/2330 train_time:5522ms step_avg:57.52ms
step:97/2330 train_time:5578ms step_avg:57.50ms
step:98/2330 train_time:5637ms step_avg:57.52ms
step:99/2330 train_time:5692ms step_avg:57.49ms
step:100/2330 train_time:5751ms step_avg:57.51ms
step:101/2330 train_time:5807ms step_avg:57.49ms
step:102/2330 train_time:5866ms step_avg:57.51ms
step:103/2330 train_time:5922ms step_avg:57.49ms
step:104/2330 train_time:5981ms step_avg:57.51ms
step:105/2330 train_time:6037ms step_avg:57.49ms
step:106/2330 train_time:6097ms step_avg:57.52ms
step:107/2330 train_time:6153ms step_avg:57.50ms
step:108/2330 train_time:6212ms step_avg:57.52ms
step:109/2330 train_time:6268ms step_avg:57.51ms
step:110/2330 train_time:6327ms step_avg:57.52ms
step:111/2330 train_time:6383ms step_avg:57.50ms
step:112/2330 train_time:6441ms step_avg:57.51ms
step:113/2330 train_time:6497ms step_avg:57.49ms
step:114/2330 train_time:6556ms step_avg:57.50ms
step:115/2330 train_time:6611ms step_avg:57.49ms
step:116/2330 train_time:6671ms step_avg:57.51ms
step:117/2330 train_time:6726ms step_avg:57.49ms
step:118/2330 train_time:6785ms step_avg:57.50ms
step:119/2330 train_time:6841ms step_avg:57.49ms
step:120/2330 train_time:6900ms step_avg:57.50ms
step:121/2330 train_time:6956ms step_avg:57.49ms
step:122/2330 train_time:7015ms step_avg:57.50ms
step:123/2330 train_time:7071ms step_avg:57.49ms
step:124/2330 train_time:7130ms step_avg:57.50ms
step:125/2330 train_time:7186ms step_avg:57.48ms
step:126/2330 train_time:7245ms step_avg:57.50ms
step:127/2330 train_time:7301ms step_avg:57.49ms
step:128/2330 train_time:7360ms step_avg:57.50ms
step:129/2330 train_time:7416ms step_avg:57.49ms
step:130/2330 train_time:7476ms step_avg:57.51ms
step:131/2330 train_time:7531ms step_avg:57.49ms
step:132/2330 train_time:7590ms step_avg:57.50ms
step:133/2330 train_time:7647ms step_avg:57.49ms
step:134/2330 train_time:7705ms step_avg:57.50ms
step:135/2330 train_time:7760ms step_avg:57.48ms
step:136/2330 train_time:7819ms step_avg:57.49ms
step:137/2330 train_time:7876ms step_avg:57.49ms
step:138/2330 train_time:7934ms step_avg:57.49ms
step:139/2330 train_time:7989ms step_avg:57.48ms
step:140/2330 train_time:8049ms step_avg:57.49ms
step:141/2330 train_time:8105ms step_avg:57.48ms
step:142/2330 train_time:8163ms step_avg:57.49ms
step:143/2330 train_time:8219ms step_avg:57.48ms
step:144/2330 train_time:8279ms step_avg:57.49ms
step:145/2330 train_time:8335ms step_avg:57.48ms
step:146/2330 train_time:8394ms step_avg:57.49ms
step:147/2330 train_time:8450ms step_avg:57.48ms
step:148/2330 train_time:8509ms step_avg:57.49ms
step:149/2330 train_time:8565ms step_avg:57.48ms
step:150/2330 train_time:8624ms step_avg:57.49ms
step:151/2330 train_time:8679ms step_avg:57.48ms
step:152/2330 train_time:8739ms step_avg:57.49ms
step:153/2330 train_time:8795ms step_avg:57.48ms
step:154/2330 train_time:8855ms step_avg:57.50ms
step:155/2330 train_time:8910ms step_avg:57.49ms
step:156/2330 train_time:8970ms step_avg:57.50ms
step:157/2330 train_time:9025ms step_avg:57.49ms
step:158/2330 train_time:9084ms step_avg:57.50ms
step:159/2330 train_time:9140ms step_avg:57.48ms
step:160/2330 train_time:9199ms step_avg:57.50ms
step:161/2330 train_time:9256ms step_avg:57.49ms
step:162/2330 train_time:9314ms step_avg:57.49ms
step:163/2330 train_time:9370ms step_avg:57.48ms
step:164/2330 train_time:9430ms step_avg:57.50ms
step:165/2330 train_time:9487ms step_avg:57.49ms
step:166/2330 train_time:9545ms step_avg:57.50ms
step:167/2330 train_time:9601ms step_avg:57.49ms
step:168/2330 train_time:9659ms step_avg:57.50ms
step:169/2330 train_time:9716ms step_avg:57.49ms
step:170/2330 train_time:9776ms step_avg:57.50ms
step:171/2330 train_time:9831ms step_avg:57.49ms
step:172/2330 train_time:9890ms step_avg:57.50ms
step:173/2330 train_time:9946ms step_avg:57.49ms
step:174/2330 train_time:10004ms step_avg:57.50ms
step:175/2330 train_time:10061ms step_avg:57.49ms
step:176/2330 train_time:10119ms step_avg:57.50ms
step:177/2330 train_time:10176ms step_avg:57.49ms
step:178/2330 train_time:10234ms step_avg:57.50ms
step:179/2330 train_time:10290ms step_avg:57.49ms
step:180/2330 train_time:10351ms step_avg:57.51ms
step:181/2330 train_time:10407ms step_avg:57.49ms
step:182/2330 train_time:10467ms step_avg:57.51ms
step:183/2330 train_time:10523ms step_avg:57.50ms
step:184/2330 train_time:10582ms step_avg:57.51ms
step:185/2330 train_time:10638ms step_avg:57.50ms
step:186/2330 train_time:10696ms step_avg:57.51ms
step:187/2330 train_time:10752ms step_avg:57.50ms
step:188/2330 train_time:10813ms step_avg:57.51ms
step:189/2330 train_time:10868ms step_avg:57.50ms
step:190/2330 train_time:10928ms step_avg:57.51ms
step:191/2330 train_time:10983ms step_avg:57.50ms
step:192/2330 train_time:11042ms step_avg:57.51ms
step:193/2330 train_time:11099ms step_avg:57.51ms
step:194/2330 train_time:11157ms step_avg:57.51ms
step:195/2330 train_time:11213ms step_avg:57.50ms
step:196/2330 train_time:11272ms step_avg:57.51ms
step:197/2330 train_time:11328ms step_avg:57.50ms
step:198/2330 train_time:11387ms step_avg:57.51ms
step:199/2330 train_time:11443ms step_avg:57.50ms
step:200/2330 train_time:11503ms step_avg:57.51ms
step:201/2330 train_time:11559ms step_avg:57.51ms
step:202/2330 train_time:11617ms step_avg:57.51ms
step:203/2330 train_time:11673ms step_avg:57.50ms
step:204/2330 train_time:11732ms step_avg:57.51ms
step:205/2330 train_time:11788ms step_avg:57.50ms
step:206/2330 train_time:11847ms step_avg:57.51ms
step:207/2330 train_time:11903ms step_avg:57.50ms
step:208/2330 train_time:11962ms step_avg:57.51ms
step:209/2330 train_time:12018ms step_avg:57.50ms
step:210/2330 train_time:12077ms step_avg:57.51ms
step:211/2330 train_time:12133ms step_avg:57.50ms
step:212/2330 train_time:12192ms step_avg:57.51ms
step:213/2330 train_time:12248ms step_avg:57.50ms
step:214/2330 train_time:12307ms step_avg:57.51ms
step:215/2330 train_time:12364ms step_avg:57.51ms
step:216/2330 train_time:12422ms step_avg:57.51ms
step:217/2330 train_time:12478ms step_avg:57.50ms
step:218/2330 train_time:12537ms step_avg:57.51ms
step:219/2330 train_time:12593ms step_avg:57.50ms
step:220/2330 train_time:12651ms step_avg:57.51ms
step:221/2330 train_time:12707ms step_avg:57.50ms
step:222/2330 train_time:12766ms step_avg:57.51ms
step:223/2330 train_time:12822ms step_avg:57.50ms
step:224/2330 train_time:12881ms step_avg:57.50ms
step:225/2330 train_time:12937ms step_avg:57.50ms
step:226/2330 train_time:12996ms step_avg:57.51ms
step:227/2330 train_time:13052ms step_avg:57.50ms
step:228/2330 train_time:13111ms step_avg:57.51ms
step:229/2330 train_time:13168ms step_avg:57.50ms
step:230/2330 train_time:13226ms step_avg:57.51ms
step:231/2330 train_time:13282ms step_avg:57.50ms
step:232/2330 train_time:13341ms step_avg:57.50ms
step:233/2330 train_time:13397ms step_avg:57.50ms
step:234/2330 train_time:13457ms step_avg:57.51ms
step:235/2330 train_time:13513ms step_avg:57.50ms
step:236/2330 train_time:13572ms step_avg:57.51ms
step:237/2330 train_time:13627ms step_avg:57.50ms
step:238/2330 train_time:13687ms step_avg:57.51ms
step:239/2330 train_time:13743ms step_avg:57.50ms
step:240/2330 train_time:13802ms step_avg:57.51ms
step:241/2330 train_time:13858ms step_avg:57.50ms
step:242/2330 train_time:13916ms step_avg:57.51ms
step:243/2330 train_time:13973ms step_avg:57.50ms
step:244/2330 train_time:14031ms step_avg:57.51ms
step:245/2330 train_time:14088ms step_avg:57.50ms
step:246/2330 train_time:14146ms step_avg:57.51ms
step:247/2330 train_time:14202ms step_avg:57.50ms
step:248/2330 train_time:14261ms step_avg:57.50ms
step:249/2330 train_time:14317ms step_avg:57.50ms
step:250/2330 train_time:14376ms step_avg:57.51ms
step:250/2330 val_loss:4.8892 train_time:14455ms step_avg:57.82ms
step:251/2330 train_time:14473ms step_avg:57.66ms
step:252/2330 train_time:14493ms step_avg:57.51ms
step:253/2330 train_time:14549ms step_avg:57.51ms
step:254/2330 train_time:14613ms step_avg:57.53ms
step:255/2330 train_time:14668ms step_avg:57.52ms
step:256/2330 train_time:14732ms step_avg:57.55ms
step:257/2330 train_time:14788ms step_avg:57.54ms
step:258/2330 train_time:14847ms step_avg:57.55ms
step:259/2330 train_time:14903ms step_avg:57.54ms
step:260/2330 train_time:14962ms step_avg:57.55ms
step:261/2330 train_time:15017ms step_avg:57.54ms
step:262/2330 train_time:15076ms step_avg:57.54ms
step:263/2330 train_time:15132ms step_avg:57.54ms
step:264/2330 train_time:15190ms step_avg:57.54ms
step:265/2330 train_time:15245ms step_avg:57.53ms
step:266/2330 train_time:15304ms step_avg:57.53ms
step:267/2330 train_time:15359ms step_avg:57.53ms
step:268/2330 train_time:15418ms step_avg:57.53ms
step:269/2330 train_time:15475ms step_avg:57.53ms
step:270/2330 train_time:15535ms step_avg:57.54ms
step:271/2330 train_time:15592ms step_avg:57.53ms
step:272/2330 train_time:15651ms step_avg:57.54ms
step:273/2330 train_time:15708ms step_avg:57.54ms
step:274/2330 train_time:15768ms step_avg:57.55ms
step:275/2330 train_time:15824ms step_avg:57.54ms
step:276/2330 train_time:15883ms step_avg:57.55ms
step:277/2330 train_time:15939ms step_avg:57.54ms
step:278/2330 train_time:15998ms step_avg:57.55ms
step:279/2330 train_time:16055ms step_avg:57.54ms
step:280/2330 train_time:16113ms step_avg:57.55ms
step:281/2330 train_time:16168ms step_avg:57.54ms
step:282/2330 train_time:16227ms step_avg:57.54ms
step:283/2330 train_time:16283ms step_avg:57.54ms
step:284/2330 train_time:16341ms step_avg:57.54ms
step:285/2330 train_time:16397ms step_avg:57.53ms
step:286/2330 train_time:16457ms step_avg:57.54ms
step:287/2330 train_time:16514ms step_avg:57.54ms
step:288/2330 train_time:16573ms step_avg:57.54ms
step:289/2330 train_time:16629ms step_avg:57.54ms
step:290/2330 train_time:16689ms step_avg:57.55ms
step:291/2330 train_time:16745ms step_avg:57.54ms
step:292/2330 train_time:16804ms step_avg:57.55ms
step:293/2330 train_time:16859ms step_avg:57.54ms
step:294/2330 train_time:16919ms step_avg:57.55ms
step:295/2330 train_time:16975ms step_avg:57.54ms
step:296/2330 train_time:17035ms step_avg:57.55ms
step:297/2330 train_time:17091ms step_avg:57.54ms
step:298/2330 train_time:17149ms step_avg:57.55ms
step:299/2330 train_time:17205ms step_avg:57.54ms
step:300/2330 train_time:17264ms step_avg:57.55ms
step:301/2330 train_time:17319ms step_avg:57.54ms
step:302/2330 train_time:17378ms step_avg:57.54ms
step:303/2330 train_time:17434ms step_avg:57.54ms
step:304/2330 train_time:17493ms step_avg:57.54ms
step:305/2330 train_time:17550ms step_avg:57.54ms
step:306/2330 train_time:17609ms step_avg:57.55ms
step:307/2330 train_time:17666ms step_avg:57.54ms
step:308/2330 train_time:17724ms step_avg:57.55ms
step:309/2330 train_time:17781ms step_avg:57.54ms
step:310/2330 train_time:17840ms step_avg:57.55ms
step:311/2330 train_time:17895ms step_avg:57.54ms
step:312/2330 train_time:17955ms step_avg:57.55ms
step:313/2330 train_time:18010ms step_avg:57.54ms
step:314/2330 train_time:18070ms step_avg:57.55ms
step:315/2330 train_time:18126ms step_avg:57.54ms
step:316/2330 train_time:18184ms step_avg:57.55ms
step:317/2330 train_time:18241ms step_avg:57.54ms
step:318/2330 train_time:18299ms step_avg:57.54ms
step:319/2330 train_time:18355ms step_avg:57.54ms
step:320/2330 train_time:18413ms step_avg:57.54ms
step:321/2330 train_time:18469ms step_avg:57.54ms
step:322/2330 train_time:18529ms step_avg:57.54ms
step:323/2330 train_time:18585ms step_avg:57.54ms
step:324/2330 train_time:18644ms step_avg:57.54ms
step:325/2330 train_time:18701ms step_avg:57.54ms
step:326/2330 train_time:18759ms step_avg:57.54ms
step:327/2330 train_time:18815ms step_avg:57.54ms
step:328/2330 train_time:18874ms step_avg:57.54ms
step:329/2330 train_time:18930ms step_avg:57.54ms
step:330/2330 train_time:18989ms step_avg:57.54ms
step:331/2330 train_time:19045ms step_avg:57.54ms
step:332/2330 train_time:19104ms step_avg:57.54ms
step:333/2330 train_time:19161ms step_avg:57.54ms
step:334/2330 train_time:19219ms step_avg:57.54ms
step:335/2330 train_time:19275ms step_avg:57.54ms
step:336/2330 train_time:19334ms step_avg:57.54ms
step:337/2330 train_time:19390ms step_avg:57.54ms
step:338/2330 train_time:19449ms step_avg:57.54ms
step:339/2330 train_time:19506ms step_avg:57.54ms
step:340/2330 train_time:19564ms step_avg:57.54ms
step:341/2330 train_time:19620ms step_avg:57.54ms
step:342/2330 train_time:19679ms step_avg:57.54ms
step:343/2330 train_time:19735ms step_avg:57.54ms
step:344/2330 train_time:19795ms step_avg:57.54ms
step:345/2330 train_time:19851ms step_avg:57.54ms
step:346/2330 train_time:19910ms step_avg:57.54ms
step:347/2330 train_time:19966ms step_avg:57.54ms
step:348/2330 train_time:20026ms step_avg:57.54ms
step:349/2330 train_time:20081ms step_avg:57.54ms
step:350/2330 train_time:20141ms step_avg:57.55ms
step:351/2330 train_time:20196ms step_avg:57.54ms
step:352/2330 train_time:20256ms step_avg:57.55ms
step:353/2330 train_time:20313ms step_avg:57.54ms
step:354/2330 train_time:20372ms step_avg:57.55ms
step:355/2330 train_time:20428ms step_avg:57.54ms
step:356/2330 train_time:20487ms step_avg:57.55ms
step:357/2330 train_time:20543ms step_avg:57.54ms
step:358/2330 train_time:20602ms step_avg:57.55ms
step:359/2330 train_time:20658ms step_avg:57.54ms
step:360/2330 train_time:20717ms step_avg:57.55ms
step:361/2330 train_time:20773ms step_avg:57.54ms
step:362/2330 train_time:20833ms step_avg:57.55ms
step:363/2330 train_time:20889ms step_avg:57.54ms
step:364/2330 train_time:20948ms step_avg:57.55ms
step:365/2330 train_time:21004ms step_avg:57.55ms
step:366/2330 train_time:21063ms step_avg:57.55ms
step:367/2330 train_time:21119ms step_avg:57.54ms
step:368/2330 train_time:21178ms step_avg:57.55ms
step:369/2330 train_time:21234ms step_avg:57.55ms
step:370/2330 train_time:21294ms step_avg:57.55ms
step:371/2330 train_time:21350ms step_avg:57.55ms
step:372/2330 train_time:21409ms step_avg:57.55ms
step:373/2330 train_time:21466ms step_avg:57.55ms
step:374/2330 train_time:21524ms step_avg:57.55ms
step:375/2330 train_time:21580ms step_avg:57.55ms
step:376/2330 train_time:21640ms step_avg:57.55ms
step:377/2330 train_time:21696ms step_avg:57.55ms
step:378/2330 train_time:21755ms step_avg:57.55ms
step:379/2330 train_time:21812ms step_avg:57.55ms
step:380/2330 train_time:21872ms step_avg:57.56ms
step:381/2330 train_time:21928ms step_avg:57.55ms
step:382/2330 train_time:21986ms step_avg:57.56ms
step:383/2330 train_time:22042ms step_avg:57.55ms
step:384/2330 train_time:22102ms step_avg:57.56ms
step:385/2330 train_time:22157ms step_avg:57.55ms
step:386/2330 train_time:22216ms step_avg:57.56ms
step:387/2330 train_time:22272ms step_avg:57.55ms
step:388/2330 train_time:22331ms step_avg:57.55ms
step:389/2330 train_time:22387ms step_avg:57.55ms
step:390/2330 train_time:22446ms step_avg:57.55ms
step:391/2330 train_time:22502ms step_avg:57.55ms
step:392/2330 train_time:22562ms step_avg:57.56ms
step:393/2330 train_time:22617ms step_avg:57.55ms
step:394/2330 train_time:22678ms step_avg:57.56ms
step:395/2330 train_time:22733ms step_avg:57.55ms
step:396/2330 train_time:22793ms step_avg:57.56ms
step:397/2330 train_time:22848ms step_avg:57.55ms
step:398/2330 train_time:22908ms step_avg:57.56ms
step:399/2330 train_time:22964ms step_avg:57.55ms
step:400/2330 train_time:23023ms step_avg:57.56ms
step:401/2330 train_time:23078ms step_avg:57.55ms
step:402/2330 train_time:23139ms step_avg:57.56ms
step:403/2330 train_time:23194ms step_avg:57.55ms
step:404/2330 train_time:23254ms step_avg:57.56ms
step:405/2330 train_time:23310ms step_avg:57.56ms
step:406/2330 train_time:23369ms step_avg:57.56ms
step:407/2330 train_time:23426ms step_avg:57.56ms
step:408/2330 train_time:23484ms step_avg:57.56ms
step:409/2330 train_time:23540ms step_avg:57.56ms
step:410/2330 train_time:23599ms step_avg:57.56ms
step:411/2330 train_time:23655ms step_avg:57.55ms
step:412/2330 train_time:23715ms step_avg:57.56ms
step:413/2330 train_time:23771ms step_avg:57.56ms
step:414/2330 train_time:23830ms step_avg:57.56ms
step:415/2330 train_time:23887ms step_avg:57.56ms
step:416/2330 train_time:23946ms step_avg:57.56ms
step:417/2330 train_time:24002ms step_avg:57.56ms
step:418/2330 train_time:24061ms step_avg:57.56ms
step:419/2330 train_time:24116ms step_avg:57.56ms
step:420/2330 train_time:24176ms step_avg:57.56ms
step:421/2330 train_time:24232ms step_avg:57.56ms
step:422/2330 train_time:24291ms step_avg:57.56ms
step:423/2330 train_time:24348ms step_avg:57.56ms
step:424/2330 train_time:24406ms step_avg:57.56ms
step:425/2330 train_time:24462ms step_avg:57.56ms
step:426/2330 train_time:24521ms step_avg:57.56ms
step:427/2330 train_time:24577ms step_avg:57.56ms
step:428/2330 train_time:24638ms step_avg:57.56ms
step:429/2330 train_time:24693ms step_avg:57.56ms
step:430/2330 train_time:24753ms step_avg:57.56ms
step:431/2330 train_time:24809ms step_avg:57.56ms
step:432/2330 train_time:24869ms step_avg:57.57ms
step:433/2330 train_time:24925ms step_avg:57.56ms
step:434/2330 train_time:24984ms step_avg:57.57ms
step:435/2330 train_time:25041ms step_avg:57.56ms
step:436/2330 train_time:25100ms step_avg:57.57ms
step:437/2330 train_time:25156ms step_avg:57.56ms
step:438/2330 train_time:25217ms step_avg:57.57ms
step:439/2330 train_time:25273ms step_avg:57.57ms
step:440/2330 train_time:25331ms step_avg:57.57ms
step:441/2330 train_time:25388ms step_avg:57.57ms
step:442/2330 train_time:25448ms step_avg:57.57ms
step:443/2330 train_time:25504ms step_avg:57.57ms
step:444/2330 train_time:25565ms step_avg:57.58ms
step:445/2330 train_time:25620ms step_avg:57.57ms
step:446/2330 train_time:25681ms step_avg:57.58ms
step:447/2330 train_time:25736ms step_avg:57.57ms
step:448/2330 train_time:25796ms step_avg:57.58ms
step:449/2330 train_time:25852ms step_avg:57.58ms
step:450/2330 train_time:25911ms step_avg:57.58ms
step:451/2330 train_time:25967ms step_avg:57.58ms
step:452/2330 train_time:26026ms step_avg:57.58ms
step:453/2330 train_time:26082ms step_avg:57.58ms
step:454/2330 train_time:26143ms step_avg:57.58ms
step:455/2330 train_time:26198ms step_avg:57.58ms
step:456/2330 train_time:26258ms step_avg:57.58ms
step:457/2330 train_time:26314ms step_avg:57.58ms
step:458/2330 train_time:26372ms step_avg:57.58ms
step:459/2330 train_time:26429ms step_avg:57.58ms
step:460/2330 train_time:26488ms step_avg:57.58ms
step:461/2330 train_time:26545ms step_avg:57.58ms
step:462/2330 train_time:26603ms step_avg:57.58ms
step:463/2330 train_time:26659ms step_avg:57.58ms
step:464/2330 train_time:26720ms step_avg:57.59ms
step:465/2330 train_time:26776ms step_avg:57.58ms
step:466/2330 train_time:26834ms step_avg:57.58ms
step:467/2330 train_time:26890ms step_avg:57.58ms
step:468/2330 train_time:26950ms step_avg:57.59ms
step:469/2330 train_time:27006ms step_avg:57.58ms
step:470/2330 train_time:27065ms step_avg:57.58ms
step:471/2330 train_time:27121ms step_avg:57.58ms
step:472/2330 train_time:27180ms step_avg:57.59ms
step:473/2330 train_time:27236ms step_avg:57.58ms
step:474/2330 train_time:27296ms step_avg:57.59ms
step:475/2330 train_time:27352ms step_avg:57.58ms
step:476/2330 train_time:27411ms step_avg:57.59ms
step:477/2330 train_time:27467ms step_avg:57.58ms
step:478/2330 train_time:27526ms step_avg:57.59ms
step:479/2330 train_time:27582ms step_avg:57.58ms
step:480/2330 train_time:27642ms step_avg:57.59ms
step:481/2330 train_time:27697ms step_avg:57.58ms
step:482/2330 train_time:27758ms step_avg:57.59ms
step:483/2330 train_time:27813ms step_avg:57.58ms
step:484/2330 train_time:27873ms step_avg:57.59ms
step:485/2330 train_time:27930ms step_avg:57.59ms
step:486/2330 train_time:27989ms step_avg:57.59ms
step:487/2330 train_time:28046ms step_avg:57.59ms
step:488/2330 train_time:28104ms step_avg:57.59ms
step:489/2330 train_time:28161ms step_avg:57.59ms
step:490/2330 train_time:28220ms step_avg:57.59ms
step:491/2330 train_time:28276ms step_avg:57.59ms
step:492/2330 train_time:28335ms step_avg:57.59ms
step:493/2330 train_time:28391ms step_avg:57.59ms
step:494/2330 train_time:28450ms step_avg:57.59ms
step:495/2330 train_time:28506ms step_avg:57.59ms
step:496/2330 train_time:28565ms step_avg:57.59ms
step:497/2330 train_time:28622ms step_avg:57.59ms
step:498/2330 train_time:28681ms step_avg:57.59ms
step:499/2330 train_time:28736ms step_avg:57.59ms
step:500/2330 train_time:28797ms step_avg:57.59ms
step:500/2330 val_loss:4.4083 train_time:28876ms step_avg:57.75ms
step:501/2330 train_time:28893ms step_avg:57.67ms
step:502/2330 train_time:28914ms step_avg:57.60ms
step:503/2330 train_time:28972ms step_avg:57.60ms
step:504/2330 train_time:29033ms step_avg:57.61ms
step:505/2330 train_time:29089ms step_avg:57.60ms
step:506/2330 train_time:29152ms step_avg:57.61ms
step:507/2330 train_time:29207ms step_avg:57.61ms
step:508/2330 train_time:29267ms step_avg:57.61ms
step:509/2330 train_time:29322ms step_avg:57.61ms
step:510/2330 train_time:29382ms step_avg:57.61ms
step:511/2330 train_time:29438ms step_avg:57.61ms
step:512/2330 train_time:29495ms step_avg:57.61ms
step:513/2330 train_time:29551ms step_avg:57.60ms
step:514/2330 train_time:29610ms step_avg:57.61ms
step:515/2330 train_time:29665ms step_avg:57.60ms
step:516/2330 train_time:29724ms step_avg:57.61ms
step:517/2330 train_time:29780ms step_avg:57.60ms
step:518/2330 train_time:29839ms step_avg:57.60ms
step:519/2330 train_time:29896ms step_avg:57.60ms
step:520/2330 train_time:29958ms step_avg:57.61ms
step:521/2330 train_time:30015ms step_avg:57.61ms
step:522/2330 train_time:30075ms step_avg:57.61ms
step:523/2330 train_time:30131ms step_avg:57.61ms
step:524/2330 train_time:30192ms step_avg:57.62ms
step:525/2330 train_time:30248ms step_avg:57.61ms
step:526/2330 train_time:30307ms step_avg:57.62ms
step:527/2330 train_time:30363ms step_avg:57.61ms
step:528/2330 train_time:30424ms step_avg:57.62ms
step:529/2330 train_time:30480ms step_avg:57.62ms
step:530/2330 train_time:30538ms step_avg:57.62ms
step:531/2330 train_time:30593ms step_avg:57.61ms
step:532/2330 train_time:30652ms step_avg:57.62ms
step:533/2330 train_time:30707ms step_avg:57.61ms
step:534/2330 train_time:30766ms step_avg:57.61ms
step:535/2330 train_time:30822ms step_avg:57.61ms
step:536/2330 train_time:30882ms step_avg:57.62ms
step:537/2330 train_time:30938ms step_avg:57.61ms
step:538/2330 train_time:31000ms step_avg:57.62ms
step:539/2330 train_time:31056ms step_avg:57.62ms
step:540/2330 train_time:31117ms step_avg:57.62ms
step:541/2330 train_time:31174ms step_avg:57.62ms
step:542/2330 train_time:31233ms step_avg:57.63ms
step:543/2330 train_time:31290ms step_avg:57.62ms
step:544/2330 train_time:31349ms step_avg:57.63ms
step:545/2330 train_time:31404ms step_avg:57.62ms
step:546/2330 train_time:31464ms step_avg:57.63ms
step:547/2330 train_time:31519ms step_avg:57.62ms
step:548/2330 train_time:31578ms step_avg:57.62ms
step:549/2330 train_time:31634ms step_avg:57.62ms
step:550/2330 train_time:31693ms step_avg:57.62ms
step:551/2330 train_time:31749ms step_avg:57.62ms
step:552/2330 train_time:31809ms step_avg:57.62ms
step:553/2330 train_time:31864ms step_avg:57.62ms
step:554/2330 train_time:31925ms step_avg:57.63ms
step:555/2330 train_time:31980ms step_avg:57.62ms
step:556/2330 train_time:32040ms step_avg:57.63ms
step:557/2330 train_time:32097ms step_avg:57.62ms
step:558/2330 train_time:32157ms step_avg:57.63ms
step:559/2330 train_time:32213ms step_avg:57.63ms
step:560/2330 train_time:32273ms step_avg:57.63ms
step:561/2330 train_time:32329ms step_avg:57.63ms
step:562/2330 train_time:32389ms step_avg:57.63ms
step:563/2330 train_time:32445ms step_avg:57.63ms
step:564/2330 train_time:32505ms step_avg:57.63ms
step:565/2330 train_time:32560ms step_avg:57.63ms
step:566/2330 train_time:32619ms step_avg:57.63ms
step:567/2330 train_time:32675ms step_avg:57.63ms
step:568/2330 train_time:32734ms step_avg:57.63ms
step:569/2330 train_time:32790ms step_avg:57.63ms
step:570/2330 train_time:32850ms step_avg:57.63ms
step:571/2330 train_time:32905ms step_avg:57.63ms
step:572/2330 train_time:32966ms step_avg:57.63ms
step:573/2330 train_time:33022ms step_avg:57.63ms
step:574/2330 train_time:33081ms step_avg:57.63ms
step:575/2330 train_time:33138ms step_avg:57.63ms
step:576/2330 train_time:33197ms step_avg:57.63ms
step:577/2330 train_time:33254ms step_avg:57.63ms
step:578/2330 train_time:33313ms step_avg:57.63ms
step:579/2330 train_time:33369ms step_avg:57.63ms
step:580/2330 train_time:33429ms step_avg:57.64ms
step:581/2330 train_time:33485ms step_avg:57.63ms
step:582/2330 train_time:33544ms step_avg:57.64ms
step:583/2330 train_time:33600ms step_avg:57.63ms
step:584/2330 train_time:33659ms step_avg:57.64ms
step:585/2330 train_time:33715ms step_avg:57.63ms
step:586/2330 train_time:33774ms step_avg:57.64ms
step:587/2330 train_time:33830ms step_avg:57.63ms
step:588/2330 train_time:33890ms step_avg:57.64ms
step:589/2330 train_time:33946ms step_avg:57.63ms
step:590/2330 train_time:34006ms step_avg:57.64ms
step:591/2330 train_time:34062ms step_avg:57.64ms
step:592/2330 train_time:34122ms step_avg:57.64ms
step:593/2330 train_time:34178ms step_avg:57.64ms
step:594/2330 train_time:34238ms step_avg:57.64ms
step:595/2330 train_time:34294ms step_avg:57.64ms
step:596/2330 train_time:34354ms step_avg:57.64ms
step:597/2330 train_time:34410ms step_avg:57.64ms
step:598/2330 train_time:34470ms step_avg:57.64ms
step:599/2330 train_time:34525ms step_avg:57.64ms
step:600/2330 train_time:34585ms step_avg:57.64ms
step:601/2330 train_time:34641ms step_avg:57.64ms
step:602/2330 train_time:34699ms step_avg:57.64ms
step:603/2330 train_time:34756ms step_avg:57.64ms
step:604/2330 train_time:34816ms step_avg:57.64ms
step:605/2330 train_time:34872ms step_avg:57.64ms
step:606/2330 train_time:34931ms step_avg:57.64ms
step:607/2330 train_time:34987ms step_avg:57.64ms
step:608/2330 train_time:35047ms step_avg:57.64ms
step:609/2330 train_time:35103ms step_avg:57.64ms
step:610/2330 train_time:35163ms step_avg:57.65ms
step:611/2330 train_time:35220ms step_avg:57.64ms
step:612/2330 train_time:35279ms step_avg:57.65ms
step:613/2330 train_time:35335ms step_avg:57.64ms
step:614/2330 train_time:35394ms step_avg:57.65ms
step:615/2330 train_time:35451ms step_avg:57.64ms
step:616/2330 train_time:35511ms step_avg:57.65ms
step:617/2330 train_time:35567ms step_avg:57.64ms
step:618/2330 train_time:35626ms step_avg:57.65ms
step:619/2330 train_time:35681ms step_avg:57.64ms
step:620/2330 train_time:35741ms step_avg:57.65ms
step:621/2330 train_time:35797ms step_avg:57.64ms
step:622/2330 train_time:35856ms step_avg:57.65ms
step:623/2330 train_time:35913ms step_avg:57.64ms
step:624/2330 train_time:35972ms step_avg:57.65ms
step:625/2330 train_time:36028ms step_avg:57.64ms
step:626/2330 train_time:36088ms step_avg:57.65ms
step:627/2330 train_time:36143ms step_avg:57.64ms
step:628/2330 train_time:36203ms step_avg:57.65ms
step:629/2330 train_time:36259ms step_avg:57.65ms
step:630/2330 train_time:36319ms step_avg:57.65ms
step:631/2330 train_time:36375ms step_avg:57.65ms
step:632/2330 train_time:36434ms step_avg:57.65ms
step:633/2330 train_time:36491ms step_avg:57.65ms
step:634/2330 train_time:36550ms step_avg:57.65ms
step:635/2330 train_time:36605ms step_avg:57.65ms
step:636/2330 train_time:36665ms step_avg:57.65ms
step:637/2330 train_time:36720ms step_avg:57.65ms
step:638/2330 train_time:36780ms step_avg:57.65ms
step:639/2330 train_time:36836ms step_avg:57.65ms
step:640/2330 train_time:36895ms step_avg:57.65ms
step:641/2330 train_time:36952ms step_avg:57.65ms
step:642/2330 train_time:37011ms step_avg:57.65ms
step:643/2330 train_time:37068ms step_avg:57.65ms
step:644/2330 train_time:37126ms step_avg:57.65ms
step:645/2330 train_time:37182ms step_avg:57.65ms
step:646/2330 train_time:37241ms step_avg:57.65ms
step:647/2330 train_time:37298ms step_avg:57.65ms
step:648/2330 train_time:37357ms step_avg:57.65ms
step:649/2330 train_time:37413ms step_avg:57.65ms
step:650/2330 train_time:37472ms step_avg:57.65ms
step:651/2330 train_time:37528ms step_avg:57.65ms
step:652/2330 train_time:37588ms step_avg:57.65ms
step:653/2330 train_time:37644ms step_avg:57.65ms
step:654/2330 train_time:37704ms step_avg:57.65ms
step:655/2330 train_time:37761ms step_avg:57.65ms
step:656/2330 train_time:37819ms step_avg:57.65ms
step:657/2330 train_time:37876ms step_avg:57.65ms
step:658/2330 train_time:37935ms step_avg:57.65ms
step:659/2330 train_time:37990ms step_avg:57.65ms
step:660/2330 train_time:38050ms step_avg:57.65ms
step:661/2330 train_time:38105ms step_avg:57.65ms
step:662/2330 train_time:38166ms step_avg:57.65ms
step:663/2330 train_time:38222ms step_avg:57.65ms
step:664/2330 train_time:38281ms step_avg:57.65ms
step:665/2330 train_time:38337ms step_avg:57.65ms
step:666/2330 train_time:38398ms step_avg:57.65ms
step:667/2330 train_time:38454ms step_avg:57.65ms
step:668/2330 train_time:38514ms step_avg:57.66ms
step:669/2330 train_time:38569ms step_avg:57.65ms
step:670/2330 train_time:38628ms step_avg:57.65ms
step:671/2330 train_time:38684ms step_avg:57.65ms
step:672/2330 train_time:38744ms step_avg:57.65ms
step:673/2330 train_time:38800ms step_avg:57.65ms
step:674/2330 train_time:38859ms step_avg:57.65ms
step:675/2330 train_time:38915ms step_avg:57.65ms
step:676/2330 train_time:38974ms step_avg:57.65ms
step:677/2330 train_time:39030ms step_avg:57.65ms
step:678/2330 train_time:39090ms step_avg:57.65ms
step:679/2330 train_time:39146ms step_avg:57.65ms
step:680/2330 train_time:39205ms step_avg:57.66ms
step:681/2330 train_time:39262ms step_avg:57.65ms
step:682/2330 train_time:39321ms step_avg:57.66ms
step:683/2330 train_time:39378ms step_avg:57.65ms
step:684/2330 train_time:39437ms step_avg:57.66ms
step:685/2330 train_time:39494ms step_avg:57.65ms
step:686/2330 train_time:39553ms step_avg:57.66ms
step:687/2330 train_time:39608ms step_avg:57.65ms
step:688/2330 train_time:39668ms step_avg:57.66ms
step:689/2330 train_time:39724ms step_avg:57.65ms
step:690/2330 train_time:39783ms step_avg:57.66ms
step:691/2330 train_time:39839ms step_avg:57.65ms
step:692/2330 train_time:39898ms step_avg:57.66ms
step:693/2330 train_time:39955ms step_avg:57.66ms
step:694/2330 train_time:40014ms step_avg:57.66ms
step:695/2330 train_time:40070ms step_avg:57.65ms
step:696/2330 train_time:40130ms step_avg:57.66ms
step:697/2330 train_time:40186ms step_avg:57.66ms
step:698/2330 train_time:40246ms step_avg:57.66ms
step:699/2330 train_time:40301ms step_avg:57.66ms
step:700/2330 train_time:40362ms step_avg:57.66ms
step:701/2330 train_time:40418ms step_avg:57.66ms
step:702/2330 train_time:40477ms step_avg:57.66ms
step:703/2330 train_time:40533ms step_avg:57.66ms
step:704/2330 train_time:40592ms step_avg:57.66ms
step:705/2330 train_time:40648ms step_avg:57.66ms
step:706/2330 train_time:40707ms step_avg:57.66ms
step:707/2330 train_time:40762ms step_avg:57.66ms
step:708/2330 train_time:40821ms step_avg:57.66ms
step:709/2330 train_time:40877ms step_avg:57.65ms
step:710/2330 train_time:40937ms step_avg:57.66ms
step:711/2330 train_time:40994ms step_avg:57.66ms
step:712/2330 train_time:41053ms step_avg:57.66ms
step:713/2330 train_time:41109ms step_avg:57.66ms
step:714/2330 train_time:41168ms step_avg:57.66ms
step:715/2330 train_time:41224ms step_avg:57.66ms
step:716/2330 train_time:41284ms step_avg:57.66ms
step:717/2330 train_time:41341ms step_avg:57.66ms
step:718/2330 train_time:41400ms step_avg:57.66ms
step:719/2330 train_time:41457ms step_avg:57.66ms
step:720/2330 train_time:41517ms step_avg:57.66ms
step:721/2330 train_time:41573ms step_avg:57.66ms
step:722/2330 train_time:41633ms step_avg:57.66ms
step:723/2330 train_time:41689ms step_avg:57.66ms
step:724/2330 train_time:41748ms step_avg:57.66ms
step:725/2330 train_time:41804ms step_avg:57.66ms
step:726/2330 train_time:41864ms step_avg:57.66ms
step:727/2330 train_time:41919ms step_avg:57.66ms
step:728/2330 train_time:41979ms step_avg:57.66ms
step:729/2330 train_time:42036ms step_avg:57.66ms
step:730/2330 train_time:42095ms step_avg:57.66ms
step:731/2330 train_time:42151ms step_avg:57.66ms
step:732/2330 train_time:42211ms step_avg:57.66ms
step:733/2330 train_time:42266ms step_avg:57.66ms
step:734/2330 train_time:42327ms step_avg:57.67ms
step:735/2330 train_time:42382ms step_avg:57.66ms
step:736/2330 train_time:42442ms step_avg:57.67ms
step:737/2330 train_time:42499ms step_avg:57.66ms
step:738/2330 train_time:42559ms step_avg:57.67ms
step:739/2330 train_time:42616ms step_avg:57.67ms
step:740/2330 train_time:42674ms step_avg:57.67ms
step:741/2330 train_time:42730ms step_avg:57.67ms
step:742/2330 train_time:42790ms step_avg:57.67ms
step:743/2330 train_time:42845ms step_avg:57.67ms
step:744/2330 train_time:42905ms step_avg:57.67ms
step:745/2330 train_time:42960ms step_avg:57.66ms
step:746/2330 train_time:43020ms step_avg:57.67ms
step:747/2330 train_time:43076ms step_avg:57.67ms
step:748/2330 train_time:43136ms step_avg:57.67ms
step:749/2330 train_time:43192ms step_avg:57.67ms
step:750/2330 train_time:43251ms step_avg:57.67ms
step:750/2330 val_loss:4.2101 train_time:43332ms step_avg:57.78ms
step:751/2330 train_time:43350ms step_avg:57.72ms
step:752/2330 train_time:43369ms step_avg:57.67ms
step:753/2330 train_time:43425ms step_avg:57.67ms
step:754/2330 train_time:43494ms step_avg:57.68ms
step:755/2330 train_time:43549ms step_avg:57.68ms
step:756/2330 train_time:43613ms step_avg:57.69ms
step:757/2330 train_time:43669ms step_avg:57.69ms
step:758/2330 train_time:43729ms step_avg:57.69ms
step:759/2330 train_time:43785ms step_avg:57.69ms
step:760/2330 train_time:43844ms step_avg:57.69ms
step:761/2330 train_time:43899ms step_avg:57.69ms
step:762/2330 train_time:43958ms step_avg:57.69ms
step:763/2330 train_time:44013ms step_avg:57.68ms
step:764/2330 train_time:44072ms step_avg:57.69ms
step:765/2330 train_time:44128ms step_avg:57.68ms
step:766/2330 train_time:44186ms step_avg:57.68ms
step:767/2330 train_time:44242ms step_avg:57.68ms
step:768/2330 train_time:44302ms step_avg:57.68ms
step:769/2330 train_time:44359ms step_avg:57.68ms
step:770/2330 train_time:44422ms step_avg:57.69ms
step:771/2330 train_time:44480ms step_avg:57.69ms
step:772/2330 train_time:44542ms step_avg:57.70ms
step:773/2330 train_time:44600ms step_avg:57.70ms
step:774/2330 train_time:44661ms step_avg:57.70ms
step:775/2330 train_time:44718ms step_avg:57.70ms
step:776/2330 train_time:44780ms step_avg:57.71ms
step:777/2330 train_time:44836ms step_avg:57.70ms
step:778/2330 train_time:44896ms step_avg:57.71ms
step:779/2330 train_time:44953ms step_avg:57.71ms
step:780/2330 train_time:45012ms step_avg:57.71ms
step:781/2330 train_time:45069ms step_avg:57.71ms
step:782/2330 train_time:45129ms step_avg:57.71ms
step:783/2330 train_time:45185ms step_avg:57.71ms
step:784/2330 train_time:45243ms step_avg:57.71ms
step:785/2330 train_time:45300ms step_avg:57.71ms
step:786/2330 train_time:45360ms step_avg:57.71ms
step:787/2330 train_time:45418ms step_avg:57.71ms
step:788/2330 train_time:45479ms step_avg:57.71ms
step:789/2330 train_time:45535ms step_avg:57.71ms
step:790/2330 train_time:45597ms step_avg:57.72ms
step:791/2330 train_time:45654ms step_avg:57.72ms
step:792/2330 train_time:45716ms step_avg:57.72ms
step:793/2330 train_time:45773ms step_avg:57.72ms
step:794/2330 train_time:45834ms step_avg:57.73ms
step:795/2330 train_time:45890ms step_avg:57.72ms
step:796/2330 train_time:45951ms step_avg:57.73ms
step:797/2330 train_time:46008ms step_avg:57.73ms
step:798/2330 train_time:46067ms step_avg:57.73ms
step:799/2330 train_time:46123ms step_avg:57.73ms
step:800/2330 train_time:46184ms step_avg:57.73ms
step:801/2330 train_time:46240ms step_avg:57.73ms
step:802/2330 train_time:46300ms step_avg:57.73ms
step:803/2330 train_time:46357ms step_avg:57.73ms
step:804/2330 train_time:46416ms step_avg:57.73ms
step:805/2330 train_time:46474ms step_avg:57.73ms
step:806/2330 train_time:46534ms step_avg:57.73ms
step:807/2330 train_time:46592ms step_avg:57.74ms
step:808/2330 train_time:46652ms step_avg:57.74ms
step:809/2330 train_time:46710ms step_avg:57.74ms
step:810/2330 train_time:46771ms step_avg:57.74ms
step:811/2330 train_time:46828ms step_avg:57.74ms
step:812/2330 train_time:46888ms step_avg:57.74ms
step:813/2330 train_time:46945ms step_avg:57.74ms
step:814/2330 train_time:47004ms step_avg:57.74ms
step:815/2330 train_time:47061ms step_avg:57.74ms
step:816/2330 train_time:47120ms step_avg:57.75ms
step:817/2330 train_time:47176ms step_avg:57.74ms
step:818/2330 train_time:47236ms step_avg:57.75ms
step:819/2330 train_time:47293ms step_avg:57.74ms
step:820/2330 train_time:47353ms step_avg:57.75ms
step:821/2330 train_time:47410ms step_avg:57.75ms
step:822/2330 train_time:47471ms step_avg:57.75ms
step:823/2330 train_time:47527ms step_avg:57.75ms
step:824/2330 train_time:47588ms step_avg:57.75ms
step:825/2330 train_time:47644ms step_avg:57.75ms
step:826/2330 train_time:47706ms step_avg:57.76ms
step:827/2330 train_time:47763ms step_avg:57.75ms
step:828/2330 train_time:47824ms step_avg:57.76ms
step:829/2330 train_time:47881ms step_avg:57.76ms
step:830/2330 train_time:47941ms step_avg:57.76ms
step:831/2330 train_time:47998ms step_avg:57.76ms
step:832/2330 train_time:48058ms step_avg:57.76ms
step:833/2330 train_time:48115ms step_avg:57.76ms
step:834/2330 train_time:48174ms step_avg:57.76ms
step:835/2330 train_time:48231ms step_avg:57.76ms
step:836/2330 train_time:48291ms step_avg:57.76ms
step:837/2330 train_time:48347ms step_avg:57.76ms
step:838/2330 train_time:48407ms step_avg:57.77ms
step:839/2330 train_time:48464ms step_avg:57.76ms
step:840/2330 train_time:48525ms step_avg:57.77ms
step:841/2330 train_time:48582ms step_avg:57.77ms
step:842/2330 train_time:48642ms step_avg:57.77ms
step:843/2330 train_time:48699ms step_avg:57.77ms
step:844/2330 train_time:48759ms step_avg:57.77ms
step:845/2330 train_time:48816ms step_avg:57.77ms
step:846/2330 train_time:48878ms step_avg:57.78ms
step:847/2330 train_time:48935ms step_avg:57.77ms
step:848/2330 train_time:48995ms step_avg:57.78ms
step:849/2330 train_time:49052ms step_avg:57.78ms
step:850/2330 train_time:49112ms step_avg:57.78ms
step:851/2330 train_time:49168ms step_avg:57.78ms
step:852/2330 train_time:49228ms step_avg:57.78ms
step:853/2330 train_time:49284ms step_avg:57.78ms
step:854/2330 train_time:49345ms step_avg:57.78ms
step:855/2330 train_time:49402ms step_avg:57.78ms
step:856/2330 train_time:49463ms step_avg:57.78ms
step:857/2330 train_time:49520ms step_avg:57.78ms
step:858/2330 train_time:49580ms step_avg:57.79ms
step:859/2330 train_time:49638ms step_avg:57.79ms
step:860/2330 train_time:49698ms step_avg:57.79ms
step:861/2330 train_time:49755ms step_avg:57.79ms
step:862/2330 train_time:49815ms step_avg:57.79ms
step:863/2330 train_time:49872ms step_avg:57.79ms
step:864/2330 train_time:49934ms step_avg:57.79ms
step:865/2330 train_time:49990ms step_avg:57.79ms
step:866/2330 train_time:50050ms step_avg:57.79ms
step:867/2330 train_time:50107ms step_avg:57.79ms
step:868/2330 train_time:50166ms step_avg:57.80ms
step:869/2330 train_time:50223ms step_avg:57.79ms
step:870/2330 train_time:50283ms step_avg:57.80ms
step:871/2330 train_time:50339ms step_avg:57.79ms
step:872/2330 train_time:50400ms step_avg:57.80ms
step:873/2330 train_time:50457ms step_avg:57.80ms
step:874/2330 train_time:50517ms step_avg:57.80ms
step:875/2330 train_time:50575ms step_avg:57.80ms
step:876/2330 train_time:50634ms step_avg:57.80ms
step:877/2330 train_time:50691ms step_avg:57.80ms
step:878/2330 train_time:50752ms step_avg:57.80ms
step:879/2330 train_time:50809ms step_avg:57.80ms
step:880/2330 train_time:50870ms step_avg:57.81ms
step:881/2330 train_time:50927ms step_avg:57.81ms
step:882/2330 train_time:50987ms step_avg:57.81ms
step:883/2330 train_time:51044ms step_avg:57.81ms
step:884/2330 train_time:51104ms step_avg:57.81ms
step:885/2330 train_time:51160ms step_avg:57.81ms
step:886/2330 train_time:51221ms step_avg:57.81ms
step:887/2330 train_time:51278ms step_avg:57.81ms
step:888/2330 train_time:51337ms step_avg:57.81ms
step:889/2330 train_time:51394ms step_avg:57.81ms
step:890/2330 train_time:51455ms step_avg:57.81ms
step:891/2330 train_time:51512ms step_avg:57.81ms
step:892/2330 train_time:51573ms step_avg:57.82ms
step:893/2330 train_time:51631ms step_avg:57.82ms
step:894/2330 train_time:51690ms step_avg:57.82ms
step:895/2330 train_time:51746ms step_avg:57.82ms
step:896/2330 train_time:51807ms step_avg:57.82ms
step:897/2330 train_time:51864ms step_avg:57.82ms
step:898/2330 train_time:51924ms step_avg:57.82ms
step:899/2330 train_time:51980ms step_avg:57.82ms
step:900/2330 train_time:52041ms step_avg:57.82ms
step:901/2330 train_time:52098ms step_avg:57.82ms
step:902/2330 train_time:52158ms step_avg:57.83ms
step:903/2330 train_time:52216ms step_avg:57.82ms
step:904/2330 train_time:52275ms step_avg:57.83ms
step:905/2330 train_time:52332ms step_avg:57.83ms
step:906/2330 train_time:52392ms step_avg:57.83ms
step:907/2330 train_time:52449ms step_avg:57.83ms
step:908/2330 train_time:52510ms step_avg:57.83ms
step:909/2330 train_time:52567ms step_avg:57.83ms
step:910/2330 train_time:52627ms step_avg:57.83ms
step:911/2330 train_time:52683ms step_avg:57.83ms
step:912/2330 train_time:52744ms step_avg:57.83ms
step:913/2330 train_time:52800ms step_avg:57.83ms
step:914/2330 train_time:52862ms step_avg:57.84ms
step:915/2330 train_time:52919ms step_avg:57.83ms
step:916/2330 train_time:52979ms step_avg:57.84ms
step:917/2330 train_time:53035ms step_avg:57.84ms
step:918/2330 train_time:53095ms step_avg:57.84ms
step:919/2330 train_time:53152ms step_avg:57.84ms
step:920/2330 train_time:53212ms step_avg:57.84ms
step:921/2330 train_time:53269ms step_avg:57.84ms
step:922/2330 train_time:53329ms step_avg:57.84ms
step:923/2330 train_time:53386ms step_avg:57.84ms
step:924/2330 train_time:53447ms step_avg:57.84ms
step:925/2330 train_time:53503ms step_avg:57.84ms
step:926/2330 train_time:53563ms step_avg:57.84ms
step:927/2330 train_time:53620ms step_avg:57.84ms
step:928/2330 train_time:53681ms step_avg:57.85ms
step:929/2330 train_time:53737ms step_avg:57.84ms
step:930/2330 train_time:53797ms step_avg:57.85ms
step:931/2330 train_time:53855ms step_avg:57.85ms
step:932/2330 train_time:53915ms step_avg:57.85ms
step:933/2330 train_time:53972ms step_avg:57.85ms
step:934/2330 train_time:54032ms step_avg:57.85ms
step:935/2330 train_time:54089ms step_avg:57.85ms
step:936/2330 train_time:54149ms step_avg:57.85ms
step:937/2330 train_time:54206ms step_avg:57.85ms
step:938/2330 train_time:54265ms step_avg:57.85ms
step:939/2330 train_time:54323ms step_avg:57.85ms
step:940/2330 train_time:54383ms step_avg:57.85ms
step:941/2330 train_time:54440ms step_avg:57.85ms
step:942/2330 train_time:54500ms step_avg:57.86ms
step:943/2330 train_time:54557ms step_avg:57.85ms
step:944/2330 train_time:54618ms step_avg:57.86ms
step:945/2330 train_time:54675ms step_avg:57.86ms
step:946/2330 train_time:54735ms step_avg:57.86ms
step:947/2330 train_time:54792ms step_avg:57.86ms
step:948/2330 train_time:54851ms step_avg:57.86ms
step:949/2330 train_time:54907ms step_avg:57.86ms
step:950/2330 train_time:54968ms step_avg:57.86ms
step:951/2330 train_time:55025ms step_avg:57.86ms
step:952/2330 train_time:55085ms step_avg:57.86ms
step:953/2330 train_time:55142ms step_avg:57.86ms
step:954/2330 train_time:55202ms step_avg:57.86ms
step:955/2330 train_time:55258ms step_avg:57.86ms
step:956/2330 train_time:55319ms step_avg:57.87ms
step:957/2330 train_time:55376ms step_avg:57.86ms
step:958/2330 train_time:55436ms step_avg:57.87ms
step:959/2330 train_time:55494ms step_avg:57.87ms
step:960/2330 train_time:55555ms step_avg:57.87ms
step:961/2330 train_time:55611ms step_avg:57.87ms
step:962/2330 train_time:55673ms step_avg:57.87ms
step:963/2330 train_time:55729ms step_avg:57.87ms
step:964/2330 train_time:55790ms step_avg:57.87ms
step:965/2330 train_time:55846ms step_avg:57.87ms
step:966/2330 train_time:55908ms step_avg:57.88ms
step:967/2330 train_time:55964ms step_avg:57.87ms
step:968/2330 train_time:56025ms step_avg:57.88ms
step:969/2330 train_time:56081ms step_avg:57.88ms
step:970/2330 train_time:56142ms step_avg:57.88ms
step:971/2330 train_time:56199ms step_avg:57.88ms
step:972/2330 train_time:56259ms step_avg:57.88ms
step:973/2330 train_time:56316ms step_avg:57.88ms
step:974/2330 train_time:56376ms step_avg:57.88ms
step:975/2330 train_time:56433ms step_avg:57.88ms
step:976/2330 train_time:56493ms step_avg:57.88ms
step:977/2330 train_time:56550ms step_avg:57.88ms
step:978/2330 train_time:56611ms step_avg:57.88ms
step:979/2330 train_time:56669ms step_avg:57.88ms
step:980/2330 train_time:56728ms step_avg:57.89ms
step:981/2330 train_time:56785ms step_avg:57.88ms
step:982/2330 train_time:56846ms step_avg:57.89ms
step:983/2330 train_time:56902ms step_avg:57.89ms
step:984/2330 train_time:56963ms step_avg:57.89ms
step:985/2330 train_time:57019ms step_avg:57.89ms
step:986/2330 train_time:57080ms step_avg:57.89ms
step:987/2330 train_time:57136ms step_avg:57.89ms
step:988/2330 train_time:57197ms step_avg:57.89ms
step:989/2330 train_time:57254ms step_avg:57.89ms
step:990/2330 train_time:57315ms step_avg:57.89ms
step:991/2330 train_time:57372ms step_avg:57.89ms
step:992/2330 train_time:57432ms step_avg:57.90ms
step:993/2330 train_time:57488ms step_avg:57.89ms
step:994/2330 train_time:57549ms step_avg:57.90ms
step:995/2330 train_time:57606ms step_avg:57.90ms
step:996/2330 train_time:57666ms step_avg:57.90ms
step:997/2330 train_time:57723ms step_avg:57.90ms
step:998/2330 train_time:57783ms step_avg:57.90ms
step:999/2330 train_time:57839ms step_avg:57.90ms
step:1000/2330 train_time:57901ms step_avg:57.90ms
step:1000/2330 val_loss:4.0680 train_time:57982ms step_avg:57.98ms
step:1001/2330 train_time:58001ms step_avg:57.94ms
step:1002/2330 train_time:58020ms step_avg:57.90ms
step:1003/2330 train_time:58075ms step_avg:57.90ms
step:1004/2330 train_time:58142ms step_avg:57.91ms
step:1005/2330 train_time:58198ms step_avg:57.91ms
step:1006/2330 train_time:58261ms step_avg:57.91ms
step:1007/2330 train_time:58317ms step_avg:57.91ms
step:1008/2330 train_time:58379ms step_avg:57.92ms
step:1009/2330 train_time:58435ms step_avg:57.91ms
step:1010/2330 train_time:58494ms step_avg:57.92ms
step:1011/2330 train_time:58551ms step_avg:57.91ms
step:1012/2330 train_time:58610ms step_avg:57.91ms
step:1013/2330 train_time:58666ms step_avg:57.91ms
step:1014/2330 train_time:58726ms step_avg:57.91ms
step:1015/2330 train_time:58782ms step_avg:57.91ms
step:1016/2330 train_time:58841ms step_avg:57.91ms
step:1017/2330 train_time:58900ms step_avg:57.92ms
step:1018/2330 train_time:58963ms step_avg:57.92ms
step:1019/2330 train_time:59021ms step_avg:57.92ms
step:1020/2330 train_time:59083ms step_avg:57.92ms
step:1021/2330 train_time:59141ms step_avg:57.92ms
step:1022/2330 train_time:59201ms step_avg:57.93ms
step:1023/2330 train_time:59257ms step_avg:57.92ms
step:1024/2330 train_time:59319ms step_avg:57.93ms
step:1025/2330 train_time:59376ms step_avg:57.93ms
step:1026/2330 train_time:59437ms step_avg:57.93ms
step:1027/2330 train_time:59493ms step_avg:57.93ms
step:1028/2330 train_time:59552ms step_avg:57.93ms
step:1029/2330 train_time:59608ms step_avg:57.93ms
step:1030/2330 train_time:59668ms step_avg:57.93ms
step:1031/2330 train_time:59725ms step_avg:57.93ms
step:1032/2330 train_time:59784ms step_avg:57.93ms
step:1033/2330 train_time:59842ms step_avg:57.93ms
step:1034/2330 train_time:59901ms step_avg:57.93ms
step:1035/2330 train_time:59959ms step_avg:57.93ms
step:1036/2330 train_time:60022ms step_avg:57.94ms
step:1037/2330 train_time:60079ms step_avg:57.94ms
step:1038/2330 train_time:60140ms step_avg:57.94ms
step:1039/2330 train_time:60197ms step_avg:57.94ms
step:1040/2330 train_time:60257ms step_avg:57.94ms
step:1041/2330 train_time:60315ms step_avg:57.94ms
step:1042/2330 train_time:60375ms step_avg:57.94ms
step:1043/2330 train_time:60432ms step_avg:57.94ms
step:1044/2330 train_time:60491ms step_avg:57.94ms
step:1045/2330 train_time:60548ms step_avg:57.94ms
step:1046/2330 train_time:60607ms step_avg:57.94ms
step:1047/2330 train_time:60663ms step_avg:57.94ms
step:1048/2330 train_time:60723ms step_avg:57.94ms
step:1049/2330 train_time:60780ms step_avg:57.94ms
step:1050/2330 train_time:60840ms step_avg:57.94ms
step:1051/2330 train_time:60896ms step_avg:57.94ms
step:1052/2330 train_time:60957ms step_avg:57.94ms
step:1053/2330 train_time:61015ms step_avg:57.94ms
step:1054/2330 train_time:61077ms step_avg:57.95ms
step:1055/2330 train_time:61134ms step_avg:57.95ms
step:1056/2330 train_time:61195ms step_avg:57.95ms
step:1057/2330 train_time:61252ms step_avg:57.95ms
step:1058/2330 train_time:61313ms step_avg:57.95ms
step:1059/2330 train_time:61370ms step_avg:57.95ms
step:1060/2330 train_time:61429ms step_avg:57.95ms
step:1061/2330 train_time:61486ms step_avg:57.95ms
step:1062/2330 train_time:61547ms step_avg:57.95ms
step:1063/2330 train_time:61603ms step_avg:57.95ms
step:1064/2330 train_time:61663ms step_avg:57.95ms
step:1065/2330 train_time:61719ms step_avg:57.95ms
step:1066/2330 train_time:61779ms step_avg:57.95ms
step:1067/2330 train_time:61836ms step_avg:57.95ms
step:1068/2330 train_time:61896ms step_avg:57.95ms
step:1069/2330 train_time:61954ms step_avg:57.95ms
step:1070/2330 train_time:62014ms step_avg:57.96ms
step:1071/2330 train_time:62070ms step_avg:57.96ms
step:1072/2330 train_time:62131ms step_avg:57.96ms
step:1073/2330 train_time:62188ms step_avg:57.96ms
step:1074/2330 train_time:62250ms step_avg:57.96ms
step:1075/2330 train_time:62307ms step_avg:57.96ms
step:1076/2330 train_time:62367ms step_avg:57.96ms
step:1077/2330 train_time:62424ms step_avg:57.96ms
step:1078/2330 train_time:62484ms step_avg:57.96ms
step:1079/2330 train_time:62540ms step_avg:57.96ms
step:1080/2330 train_time:62602ms step_avg:57.96ms
step:1081/2330 train_time:62658ms step_avg:57.96ms
step:1082/2330 train_time:62719ms step_avg:57.97ms
step:1083/2330 train_time:62775ms step_avg:57.96ms
step:1084/2330 train_time:62835ms step_avg:57.97ms
step:1085/2330 train_time:62893ms step_avg:57.97ms
step:1086/2330 train_time:62952ms step_avg:57.97ms
step:1087/2330 train_time:63010ms step_avg:57.97ms
step:1088/2330 train_time:63070ms step_avg:57.97ms
step:1089/2330 train_time:63127ms step_avg:57.97ms
step:1090/2330 train_time:63188ms step_avg:57.97ms
step:1091/2330 train_time:63245ms step_avg:57.97ms
step:1092/2330 train_time:63305ms step_avg:57.97ms
step:1093/2330 train_time:63362ms step_avg:57.97ms
step:1094/2330 train_time:63423ms step_avg:57.97ms
step:1095/2330 train_time:63480ms step_avg:57.97ms
step:1096/2330 train_time:63540ms step_avg:57.97ms
step:1097/2330 train_time:63597ms step_avg:57.97ms
step:1098/2330 train_time:63657ms step_avg:57.98ms
step:1099/2330 train_time:63713ms step_avg:57.97ms
step:1100/2330 train_time:63774ms step_avg:57.98ms
step:1101/2330 train_time:63831ms step_avg:57.98ms
step:1102/2330 train_time:63891ms step_avg:57.98ms
step:1103/2330 train_time:63949ms step_avg:57.98ms
step:1104/2330 train_time:64008ms step_avg:57.98ms
step:1105/2330 train_time:64065ms step_avg:57.98ms
step:1106/2330 train_time:64125ms step_avg:57.98ms
step:1107/2330 train_time:64182ms step_avg:57.98ms
step:1108/2330 train_time:64243ms step_avg:57.98ms
step:1109/2330 train_time:64300ms step_avg:57.98ms
step:1110/2330 train_time:64360ms step_avg:57.98ms
step:1111/2330 train_time:64417ms step_avg:57.98ms
step:1112/2330 train_time:64477ms step_avg:57.98ms
step:1113/2330 train_time:64535ms step_avg:57.98ms
step:1114/2330 train_time:64594ms step_avg:57.98ms
step:1115/2330 train_time:64651ms step_avg:57.98ms
step:1116/2330 train_time:64711ms step_avg:57.98ms
step:1117/2330 train_time:64767ms step_avg:57.98ms
step:1118/2330 train_time:64827ms step_avg:57.99ms
step:1119/2330 train_time:64884ms step_avg:57.98ms
step:1120/2330 train_time:64945ms step_avg:57.99ms
step:1121/2330 train_time:65002ms step_avg:57.99ms
step:1122/2330 train_time:65061ms step_avg:57.99ms
step:1123/2330 train_time:65118ms step_avg:57.99ms
step:1124/2330 train_time:65179ms step_avg:57.99ms
step:1125/2330 train_time:65236ms step_avg:57.99ms
step:1126/2330 train_time:65296ms step_avg:57.99ms
step:1127/2330 train_time:65353ms step_avg:57.99ms
step:1128/2330 train_time:65413ms step_avg:57.99ms
step:1129/2330 train_time:65470ms step_avg:57.99ms
step:1130/2330 train_time:65530ms step_avg:57.99ms
step:1131/2330 train_time:65587ms step_avg:57.99ms
step:1132/2330 train_time:65647ms step_avg:57.99ms
step:1133/2330 train_time:65704ms step_avg:57.99ms
step:1134/2330 train_time:65764ms step_avg:57.99ms
step:1135/2330 train_time:65821ms step_avg:57.99ms
step:1136/2330 train_time:65881ms step_avg:57.99ms
step:1137/2330 train_time:65938ms step_avg:57.99ms
step:1138/2330 train_time:65999ms step_avg:58.00ms
step:1139/2330 train_time:66055ms step_avg:57.99ms
step:1140/2330 train_time:66116ms step_avg:58.00ms
step:1141/2330 train_time:66174ms step_avg:58.00ms
step:1142/2330 train_time:66234ms step_avg:58.00ms
step:1143/2330 train_time:66290ms step_avg:58.00ms
step:1144/2330 train_time:66351ms step_avg:58.00ms
step:1145/2330 train_time:66408ms step_avg:58.00ms
step:1146/2330 train_time:66468ms step_avg:58.00ms
step:1147/2330 train_time:66525ms step_avg:58.00ms
step:1148/2330 train_time:66586ms step_avg:58.00ms
step:1149/2330 train_time:66643ms step_avg:58.00ms
step:1150/2330 train_time:66703ms step_avg:58.00ms
step:1151/2330 train_time:66759ms step_avg:58.00ms
step:1152/2330 train_time:66821ms step_avg:58.00ms
step:1153/2330 train_time:66877ms step_avg:58.00ms
step:1154/2330 train_time:66937ms step_avg:58.00ms
step:1155/2330 train_time:66993ms step_avg:58.00ms
step:1156/2330 train_time:67055ms step_avg:58.01ms
step:1157/2330 train_time:67112ms step_avg:58.01ms
step:1158/2330 train_time:67172ms step_avg:58.01ms
step:1159/2330 train_time:67229ms step_avg:58.01ms
step:1160/2330 train_time:67290ms step_avg:58.01ms
step:1161/2330 train_time:67347ms step_avg:58.01ms
step:1162/2330 train_time:67406ms step_avg:58.01ms
step:1163/2330 train_time:67462ms step_avg:58.01ms
step:1164/2330 train_time:67523ms step_avg:58.01ms
step:1165/2330 train_time:67580ms step_avg:58.01ms
step:1166/2330 train_time:67640ms step_avg:58.01ms
step:1167/2330 train_time:67697ms step_avg:58.01ms
step:1168/2330 train_time:67758ms step_avg:58.01ms
step:1169/2330 train_time:67815ms step_avg:58.01ms
step:1170/2330 train_time:67875ms step_avg:58.01ms
step:1171/2330 train_time:67932ms step_avg:58.01ms
step:1172/2330 train_time:67992ms step_avg:58.01ms
step:1173/2330 train_time:68049ms step_avg:58.01ms
step:1174/2330 train_time:68109ms step_avg:58.01ms
step:1175/2330 train_time:68166ms step_avg:58.01ms
step:1176/2330 train_time:68226ms step_avg:58.02ms
step:1177/2330 train_time:68283ms step_avg:58.01ms
step:1178/2330 train_time:68344ms step_avg:58.02ms
step:1179/2330 train_time:68401ms step_avg:58.02ms
step:1180/2330 train_time:68462ms step_avg:58.02ms
step:1181/2330 train_time:68519ms step_avg:58.02ms
step:1182/2330 train_time:68579ms step_avg:58.02ms
step:1183/2330 train_time:68636ms step_avg:58.02ms
step:1184/2330 train_time:68697ms step_avg:58.02ms
step:1185/2330 train_time:68753ms step_avg:58.02ms
step:1186/2330 train_time:68813ms step_avg:58.02ms
step:1187/2330 train_time:68870ms step_avg:58.02ms
step:1188/2330 train_time:68930ms step_avg:58.02ms
step:1189/2330 train_time:68987ms step_avg:58.02ms
step:1190/2330 train_time:69046ms step_avg:58.02ms
step:1191/2330 train_time:69103ms step_avg:58.02ms
step:1192/2330 train_time:69163ms step_avg:58.02ms
step:1193/2330 train_time:69220ms step_avg:58.02ms
step:1194/2330 train_time:69280ms step_avg:58.02ms
step:1195/2330 train_time:69337ms step_avg:58.02ms
step:1196/2330 train_time:69397ms step_avg:58.02ms
step:1197/2330 train_time:69454ms step_avg:58.02ms
step:1198/2330 train_time:69515ms step_avg:58.03ms
step:1199/2330 train_time:69572ms step_avg:58.03ms
step:1200/2330 train_time:69632ms step_avg:58.03ms
step:1201/2330 train_time:69689ms step_avg:58.03ms
step:1202/2330 train_time:69749ms step_avg:58.03ms
step:1203/2330 train_time:69806ms step_avg:58.03ms
step:1204/2330 train_time:69866ms step_avg:58.03ms
step:1205/2330 train_time:69923ms step_avg:58.03ms
step:1206/2330 train_time:69984ms step_avg:58.03ms
step:1207/2330 train_time:70041ms step_avg:58.03ms
step:1208/2330 train_time:70101ms step_avg:58.03ms
step:1209/2330 train_time:70158ms step_avg:58.03ms
step:1210/2330 train_time:70218ms step_avg:58.03ms
step:1211/2330 train_time:70276ms step_avg:58.03ms
step:1212/2330 train_time:70336ms step_avg:58.03ms
step:1213/2330 train_time:70393ms step_avg:58.03ms
step:1214/2330 train_time:70453ms step_avg:58.03ms
step:1215/2330 train_time:70510ms step_avg:58.03ms
step:1216/2330 train_time:70570ms step_avg:58.03ms
step:1217/2330 train_time:70628ms step_avg:58.03ms
step:1218/2330 train_time:70688ms step_avg:58.04ms
step:1219/2330 train_time:70745ms step_avg:58.03ms
step:1220/2330 train_time:70804ms step_avg:58.04ms
step:1221/2330 train_time:70860ms step_avg:58.03ms
step:1222/2330 train_time:70921ms step_avg:58.04ms
step:1223/2330 train_time:70978ms step_avg:58.04ms
step:1224/2330 train_time:71038ms step_avg:58.04ms
step:1225/2330 train_time:71095ms step_avg:58.04ms
step:1226/2330 train_time:71156ms step_avg:58.04ms
step:1227/2330 train_time:71213ms step_avg:58.04ms
step:1228/2330 train_time:71273ms step_avg:58.04ms
step:1229/2330 train_time:71330ms step_avg:58.04ms
step:1230/2330 train_time:71390ms step_avg:58.04ms
step:1231/2330 train_time:71447ms step_avg:58.04ms
step:1232/2330 train_time:71508ms step_avg:58.04ms
step:1233/2330 train_time:71564ms step_avg:58.04ms
step:1234/2330 train_time:71624ms step_avg:58.04ms
step:1235/2330 train_time:71681ms step_avg:58.04ms
step:1236/2330 train_time:71742ms step_avg:58.04ms
step:1237/2330 train_time:71798ms step_avg:58.04ms
step:1238/2330 train_time:71860ms step_avg:58.04ms
step:1239/2330 train_time:71917ms step_avg:58.04ms
step:1240/2330 train_time:71976ms step_avg:58.05ms
step:1241/2330 train_time:72033ms step_avg:58.04ms
step:1242/2330 train_time:72093ms step_avg:58.05ms
step:1243/2330 train_time:72150ms step_avg:58.05ms
step:1244/2330 train_time:72210ms step_avg:58.05ms
step:1245/2330 train_time:72267ms step_avg:58.05ms
step:1246/2330 train_time:72328ms step_avg:58.05ms
step:1247/2330 train_time:72385ms step_avg:58.05ms
step:1248/2330 train_time:72446ms step_avg:58.05ms
step:1249/2330 train_time:72502ms step_avg:58.05ms
step:1250/2330 train_time:72564ms step_avg:58.05ms
step:1250/2330 val_loss:3.9880 train_time:72645ms step_avg:58.12ms
step:1251/2330 train_time:72663ms step_avg:58.08ms
step:1252/2330 train_time:72686ms step_avg:58.06ms
step:1253/2330 train_time:72744ms step_avg:58.06ms
step:1254/2330 train_time:72810ms step_avg:58.06ms
step:1255/2330 train_time:72867ms step_avg:58.06ms
step:1256/2330 train_time:72931ms step_avg:58.07ms
step:1257/2330 train_time:72987ms step_avg:58.06ms
step:1258/2330 train_time:73048ms step_avg:58.07ms
step:1259/2330 train_time:73104ms step_avg:58.07ms
step:1260/2330 train_time:73165ms step_avg:58.07ms
step:1261/2330 train_time:73221ms step_avg:58.07ms
step:1262/2330 train_time:73280ms step_avg:58.07ms
step:1263/2330 train_time:73337ms step_avg:58.07ms
step:1264/2330 train_time:73396ms step_avg:58.07ms
step:1265/2330 train_time:73452ms step_avg:58.07ms
step:1266/2330 train_time:73512ms step_avg:58.07ms
step:1267/2330 train_time:73568ms step_avg:58.06ms
step:1268/2330 train_time:73629ms step_avg:58.07ms
step:1269/2330 train_time:73686ms step_avg:58.07ms
step:1270/2330 train_time:73749ms step_avg:58.07ms
step:1271/2330 train_time:73807ms step_avg:58.07ms
step:1272/2330 train_time:73869ms step_avg:58.07ms
step:1273/2330 train_time:73926ms step_avg:58.07ms
step:1274/2330 train_time:73987ms step_avg:58.07ms
step:1275/2330 train_time:74043ms step_avg:58.07ms
step:1276/2330 train_time:74104ms step_avg:58.08ms
step:1277/2330 train_time:74160ms step_avg:58.07ms
step:1278/2330 train_time:74220ms step_avg:58.08ms
step:1279/2330 train_time:74277ms step_avg:58.07ms
step:1280/2330 train_time:74336ms step_avg:58.08ms
step:1281/2330 train_time:74392ms step_avg:58.07ms
step:1282/2330 train_time:74452ms step_avg:58.07ms
step:1283/2330 train_time:74509ms step_avg:58.07ms
step:1284/2330 train_time:74568ms step_avg:58.07ms
step:1285/2330 train_time:74625ms step_avg:58.07ms
step:1286/2330 train_time:74686ms step_avg:58.08ms
step:1287/2330 train_time:74744ms step_avg:58.08ms
step:1288/2330 train_time:74805ms step_avg:58.08ms
step:1289/2330 train_time:74863ms step_avg:58.08ms
step:1290/2330 train_time:74923ms step_avg:58.08ms
step:1291/2330 train_time:74980ms step_avg:58.08ms
step:1292/2330 train_time:75040ms step_avg:58.08ms
step:1293/2330 train_time:75097ms step_avg:58.08ms
step:1294/2330 train_time:75541ms step_avg:58.38ms
step:1295/2330 train_time:75596ms step_avg:58.38ms
step:1296/2330 train_time:75655ms step_avg:58.38ms
step:1297/2330 train_time:75711ms step_avg:58.37ms
step:1298/2330 train_time:75771ms step_avg:58.38ms
step:1299/2330 train_time:75827ms step_avg:58.37ms
step:1300/2330 train_time:75887ms step_avg:58.37ms
step:1301/2330 train_time:75943ms step_avg:58.37ms
step:1302/2330 train_time:76002ms step_avg:58.37ms
step:1303/2330 train_time:76059ms step_avg:58.37ms
step:1304/2330 train_time:76118ms step_avg:58.37ms
step:1305/2330 train_time:76174ms step_avg:58.37ms
step:1306/2330 train_time:76233ms step_avg:58.37ms
step:1307/2330 train_time:76289ms step_avg:58.37ms
step:1308/2330 train_time:76349ms step_avg:58.37ms
step:1309/2330 train_time:76410ms step_avg:58.37ms
step:1310/2330 train_time:76474ms step_avg:58.38ms
step:1311/2330 train_time:76531ms step_avg:58.38ms
step:1312/2330 train_time:76594ms step_avg:58.38ms
step:1313/2330 train_time:76650ms step_avg:58.38ms
step:1314/2330 train_time:76712ms step_avg:58.38ms
step:1315/2330 train_time:76768ms step_avg:58.38ms
step:1316/2330 train_time:76828ms step_avg:58.38ms
step:1317/2330 train_time:76884ms step_avg:58.38ms
step:1318/2330 train_time:76944ms step_avg:58.38ms
step:1319/2330 train_time:76999ms step_avg:58.38ms
step:1320/2330 train_time:77059ms step_avg:58.38ms
step:1321/2330 train_time:77115ms step_avg:58.38ms
step:1322/2330 train_time:77175ms step_avg:58.38ms
step:1323/2330 train_time:77230ms step_avg:58.38ms
step:1324/2330 train_time:77290ms step_avg:58.38ms
step:1325/2330 train_time:77348ms step_avg:58.38ms
step:1326/2330 train_time:77409ms step_avg:58.38ms
step:1327/2330 train_time:77469ms step_avg:58.38ms
step:1328/2330 train_time:77529ms step_avg:58.38ms
step:1329/2330 train_time:77587ms step_avg:58.38ms
step:1330/2330 train_time:77648ms step_avg:58.38ms
step:1331/2330 train_time:77705ms step_avg:58.38ms
step:1332/2330 train_time:77767ms step_avg:58.38ms
step:1333/2330 train_time:77823ms step_avg:58.38ms
step:1334/2330 train_time:77884ms step_avg:58.38ms
step:1335/2330 train_time:77941ms step_avg:58.38ms
step:1336/2330 train_time:78001ms step_avg:58.38ms
step:1337/2330 train_time:78057ms step_avg:58.38ms
step:1338/2330 train_time:78116ms step_avg:58.38ms
step:1339/2330 train_time:78173ms step_avg:58.38ms
step:1340/2330 train_time:78232ms step_avg:58.38ms
step:1341/2330 train_time:78289ms step_avg:58.38ms
step:1342/2330 train_time:78349ms step_avg:58.38ms
step:1343/2330 train_time:78406ms step_avg:58.38ms
step:1344/2330 train_time:78466ms step_avg:58.38ms
step:1345/2330 train_time:78524ms step_avg:58.38ms
step:1346/2330 train_time:78585ms step_avg:58.38ms
step:1347/2330 train_time:78643ms step_avg:58.38ms
step:1348/2330 train_time:78703ms step_avg:58.39ms
step:1349/2330 train_time:78761ms step_avg:58.38ms
step:1350/2330 train_time:78821ms step_avg:58.39ms
step:1351/2330 train_time:78878ms step_avg:58.38ms
step:1352/2330 train_time:78937ms step_avg:58.39ms
step:1353/2330 train_time:78994ms step_avg:58.38ms
step:1354/2330 train_time:79053ms step_avg:58.38ms
step:1355/2330 train_time:79110ms step_avg:58.38ms
step:1356/2330 train_time:79170ms step_avg:58.38ms
step:1357/2330 train_time:79226ms step_avg:58.38ms
step:1358/2330 train_time:79286ms step_avg:58.38ms
step:1359/2330 train_time:79343ms step_avg:58.38ms
step:1360/2330 train_time:79404ms step_avg:58.39ms
step:1361/2330 train_time:79462ms step_avg:58.39ms
step:1362/2330 train_time:79523ms step_avg:58.39ms
step:1363/2330 train_time:79580ms step_avg:58.39ms
step:1364/2330 train_time:79642ms step_avg:58.39ms
step:1365/2330 train_time:79699ms step_avg:58.39ms
step:1366/2330 train_time:79761ms step_avg:58.39ms
step:1367/2330 train_time:79818ms step_avg:58.39ms
step:1368/2330 train_time:79878ms step_avg:58.39ms
step:1369/2330 train_time:79934ms step_avg:58.39ms
step:1370/2330 train_time:79993ms step_avg:58.39ms
step:1371/2330 train_time:80050ms step_avg:58.39ms
step:1372/2330 train_time:80109ms step_avg:58.39ms
step:1373/2330 train_time:80167ms step_avg:58.39ms
step:1374/2330 train_time:80225ms step_avg:58.39ms
step:1375/2330 train_time:80281ms step_avg:58.39ms
step:1376/2330 train_time:80343ms step_avg:58.39ms
step:1377/2330 train_time:80400ms step_avg:58.39ms
step:1378/2330 train_time:80461ms step_avg:58.39ms
step:1379/2330 train_time:80518ms step_avg:58.39ms
step:1380/2330 train_time:80579ms step_avg:58.39ms
step:1381/2330 train_time:80636ms step_avg:58.39ms
step:1382/2330 train_time:80697ms step_avg:58.39ms
step:1383/2330 train_time:80754ms step_avg:58.39ms
step:1384/2330 train_time:80815ms step_avg:58.39ms
step:1385/2330 train_time:80873ms step_avg:58.39ms
step:1386/2330 train_time:80933ms step_avg:58.39ms
step:1387/2330 train_time:80989ms step_avg:58.39ms
step:1388/2330 train_time:81049ms step_avg:58.39ms
step:1389/2330 train_time:81106ms step_avg:58.39ms
step:1390/2330 train_time:81166ms step_avg:58.39ms
step:1391/2330 train_time:81222ms step_avg:58.39ms
step:1392/2330 train_time:81282ms step_avg:58.39ms
step:1393/2330 train_time:81340ms step_avg:58.39ms
step:1394/2330 train_time:81400ms step_avg:58.39ms
step:1395/2330 train_time:81458ms step_avg:58.39ms
step:1396/2330 train_time:81518ms step_avg:58.39ms
step:1397/2330 train_time:81576ms step_avg:58.39ms
step:1398/2330 train_time:81636ms step_avg:58.39ms
step:1399/2330 train_time:81693ms step_avg:58.39ms
step:1400/2330 train_time:81754ms step_avg:58.40ms
step:1401/2330 train_time:81811ms step_avg:58.39ms
step:1402/2330 train_time:81871ms step_avg:58.40ms
step:1403/2330 train_time:81928ms step_avg:58.39ms
step:1404/2330 train_time:81988ms step_avg:58.40ms
step:1405/2330 train_time:82045ms step_avg:58.39ms
step:1406/2330 train_time:82105ms step_avg:58.40ms
step:1407/2330 train_time:82161ms step_avg:58.39ms
step:1408/2330 train_time:82222ms step_avg:58.40ms
step:1409/2330 train_time:82279ms step_avg:58.39ms
step:1410/2330 train_time:82340ms step_avg:58.40ms
step:1411/2330 train_time:82397ms step_avg:58.40ms
step:1412/2330 train_time:82456ms step_avg:58.40ms
step:1413/2330 train_time:82514ms step_avg:58.40ms
step:1414/2330 train_time:82574ms step_avg:58.40ms
step:1415/2330 train_time:82631ms step_avg:58.40ms
step:1416/2330 train_time:82692ms step_avg:58.40ms
step:1417/2330 train_time:82748ms step_avg:58.40ms
step:1418/2330 train_time:82809ms step_avg:58.40ms
step:1419/2330 train_time:82866ms step_avg:58.40ms
step:1420/2330 train_time:82927ms step_avg:58.40ms
step:1421/2330 train_time:82983ms step_avg:58.40ms
step:1422/2330 train_time:83044ms step_avg:58.40ms
step:1423/2330 train_time:83101ms step_avg:58.40ms
step:1424/2330 train_time:83160ms step_avg:58.40ms
step:1425/2330 train_time:83217ms step_avg:58.40ms
step:1426/2330 train_time:83277ms step_avg:58.40ms
step:1427/2330 train_time:83334ms step_avg:58.40ms
step:1428/2330 train_time:83395ms step_avg:58.40ms
step:1429/2330 train_time:83451ms step_avg:58.40ms
step:1430/2330 train_time:83513ms step_avg:58.40ms
step:1431/2330 train_time:83569ms step_avg:58.40ms
step:1432/2330 train_time:83630ms step_avg:58.40ms
step:1433/2330 train_time:83686ms step_avg:58.40ms
step:1434/2330 train_time:83748ms step_avg:58.40ms
step:1435/2330 train_time:83805ms step_avg:58.40ms
step:1436/2330 train_time:83866ms step_avg:58.40ms
step:1437/2330 train_time:83923ms step_avg:58.40ms
step:1438/2330 train_time:83983ms step_avg:58.40ms
step:1439/2330 train_time:84039ms step_avg:58.40ms
step:1440/2330 train_time:84100ms step_avg:58.40ms
step:1441/2330 train_time:84157ms step_avg:58.40ms
step:1442/2330 train_time:84217ms step_avg:58.40ms
step:1443/2330 train_time:84275ms step_avg:58.40ms
step:1444/2330 train_time:84335ms step_avg:58.40ms
step:1445/2330 train_time:84392ms step_avg:58.40ms
step:1446/2330 train_time:84451ms step_avg:58.40ms
step:1447/2330 train_time:84507ms step_avg:58.40ms
step:1448/2330 train_time:84569ms step_avg:58.40ms
step:1449/2330 train_time:84625ms step_avg:58.40ms
step:1450/2330 train_time:84686ms step_avg:58.40ms
step:1451/2330 train_time:84743ms step_avg:58.40ms
step:1452/2330 train_time:84804ms step_avg:58.40ms
step:1453/2330 train_time:84860ms step_avg:58.40ms
step:1454/2330 train_time:84921ms step_avg:58.41ms
step:1455/2330 train_time:84978ms step_avg:58.40ms
step:1456/2330 train_time:85038ms step_avg:58.41ms
step:1457/2330 train_time:85095ms step_avg:58.40ms
step:1458/2330 train_time:85154ms step_avg:58.40ms
step:1459/2330 train_time:85211ms step_avg:58.40ms
step:1460/2330 train_time:85272ms step_avg:58.41ms
step:1461/2330 train_time:85328ms step_avg:58.40ms
step:1462/2330 train_time:85390ms step_avg:58.41ms
step:1463/2330 train_time:85446ms step_avg:58.40ms
step:1464/2330 train_time:85508ms step_avg:58.41ms
step:1465/2330 train_time:85564ms step_avg:58.41ms
step:1466/2330 train_time:85624ms step_avg:58.41ms
step:1467/2330 train_time:85680ms step_avg:58.41ms
step:1468/2330 train_time:85741ms step_avg:58.41ms
step:1469/2330 train_time:85798ms step_avg:58.41ms
step:1470/2330 train_time:85859ms step_avg:58.41ms
step:1471/2330 train_time:85916ms step_avg:58.41ms
step:1472/2330 train_time:85976ms step_avg:58.41ms
step:1473/2330 train_time:86033ms step_avg:58.41ms
step:1474/2330 train_time:86092ms step_avg:58.41ms
step:1475/2330 train_time:86150ms step_avg:58.41ms
step:1476/2330 train_time:86210ms step_avg:58.41ms
step:1477/2330 train_time:86267ms step_avg:58.41ms
step:1478/2330 train_time:86328ms step_avg:58.41ms
step:1479/2330 train_time:86383ms step_avg:58.41ms
step:1480/2330 train_time:86445ms step_avg:58.41ms
step:1481/2330 train_time:86502ms step_avg:58.41ms
step:1482/2330 train_time:86562ms step_avg:58.41ms
step:1483/2330 train_time:86619ms step_avg:58.41ms
step:1484/2330 train_time:86679ms step_avg:58.41ms
step:1485/2330 train_time:86735ms step_avg:58.41ms
step:1486/2330 train_time:86796ms step_avg:58.41ms
step:1487/2330 train_time:86852ms step_avg:58.41ms
step:1488/2330 train_time:86912ms step_avg:58.41ms
step:1489/2330 train_time:86969ms step_avg:58.41ms
step:1490/2330 train_time:87030ms step_avg:58.41ms
step:1491/2330 train_time:87087ms step_avg:58.41ms
step:1492/2330 train_time:87147ms step_avg:58.41ms
step:1493/2330 train_time:87204ms step_avg:58.41ms
step:1494/2330 train_time:87265ms step_avg:58.41ms
step:1495/2330 train_time:87322ms step_avg:58.41ms
step:1496/2330 train_time:87382ms step_avg:58.41ms
step:1497/2330 train_time:87440ms step_avg:58.41ms
step:1498/2330 train_time:87500ms step_avg:58.41ms
step:1499/2330 train_time:87557ms step_avg:58.41ms
step:1500/2330 train_time:87617ms step_avg:58.41ms
step:1500/2330 val_loss:3.9071 train_time:87697ms step_avg:58.46ms
step:1501/2330 train_time:87715ms step_avg:58.44ms
step:1502/2330 train_time:87736ms step_avg:58.41ms
step:1503/2330 train_time:87793ms step_avg:58.41ms
step:1504/2330 train_time:87860ms step_avg:58.42ms
step:1505/2330 train_time:87915ms step_avg:58.42ms
step:1506/2330 train_time:87980ms step_avg:58.42ms
step:1507/2330 train_time:88036ms step_avg:58.42ms
step:1508/2330 train_time:88098ms step_avg:58.42ms
step:1509/2330 train_time:88153ms step_avg:58.42ms
step:1510/2330 train_time:88214ms step_avg:58.42ms
step:1511/2330 train_time:88270ms step_avg:58.42ms
step:1512/2330 train_time:88330ms step_avg:58.42ms
step:1513/2330 train_time:88387ms step_avg:58.42ms
step:1514/2330 train_time:88447ms step_avg:58.42ms
step:1515/2330 train_time:88503ms step_avg:58.42ms
step:1516/2330 train_time:88563ms step_avg:58.42ms
step:1517/2330 train_time:88619ms step_avg:58.42ms
step:1518/2330 train_time:88681ms step_avg:58.42ms
step:1519/2330 train_time:88739ms step_avg:58.42ms
step:1520/2330 train_time:88800ms step_avg:58.42ms
step:1521/2330 train_time:88858ms step_avg:58.42ms
step:1522/2330 train_time:88918ms step_avg:58.42ms
step:1523/2330 train_time:88975ms step_avg:58.42ms
step:1524/2330 train_time:89037ms step_avg:58.42ms
step:1525/2330 train_time:89093ms step_avg:58.42ms
step:1526/2330 train_time:89154ms step_avg:58.42ms
step:1527/2330 train_time:89210ms step_avg:58.42ms
step:1528/2330 train_time:89271ms step_avg:58.42ms
step:1529/2330 train_time:89328ms step_avg:58.42ms
step:1530/2330 train_time:89388ms step_avg:58.42ms
step:1531/2330 train_time:89444ms step_avg:58.42ms
step:1532/2330 train_time:89505ms step_avg:58.42ms
step:1533/2330 train_time:89562ms step_avg:58.42ms
step:1534/2330 train_time:89624ms step_avg:58.42ms
step:1535/2330 train_time:89681ms step_avg:58.42ms
step:1536/2330 train_time:89744ms step_avg:58.43ms
step:1537/2330 train_time:89801ms step_avg:58.43ms
step:1538/2330 train_time:89863ms step_avg:58.43ms
step:1539/2330 train_time:89920ms step_avg:58.43ms
step:1540/2330 train_time:89983ms step_avg:58.43ms
step:1541/2330 train_time:90040ms step_avg:58.43ms
step:1542/2330 train_time:90103ms step_avg:58.43ms
step:1543/2330 train_time:90159ms step_avg:58.43ms
step:1544/2330 train_time:90221ms step_avg:58.43ms
step:1545/2330 train_time:90278ms step_avg:58.43ms
step:1546/2330 train_time:90340ms step_avg:58.43ms
step:1547/2330 train_time:90397ms step_avg:58.43ms
step:1548/2330 train_time:90457ms step_avg:58.43ms
step:1549/2330 train_time:90513ms step_avg:58.43ms
step:1550/2330 train_time:90574ms step_avg:58.44ms
step:1551/2330 train_time:90631ms step_avg:58.43ms
step:1552/2330 train_time:90693ms step_avg:58.44ms
step:1553/2330 train_time:90751ms step_avg:58.44ms
step:1554/2330 train_time:90812ms step_avg:58.44ms
step:1555/2330 train_time:90870ms step_avg:58.44ms
step:1556/2330 train_time:90933ms step_avg:58.44ms
step:1557/2330 train_time:90990ms step_avg:58.44ms
step:1558/2330 train_time:91052ms step_avg:58.44ms
step:1559/2330 train_time:91110ms step_avg:58.44ms
step:1560/2330 train_time:91171ms step_avg:58.44ms
step:1561/2330 train_time:91229ms step_avg:58.44ms
step:1562/2330 train_time:91291ms step_avg:58.44ms
step:1563/2330 train_time:91348ms step_avg:58.44ms
step:1564/2330 train_time:91409ms step_avg:58.45ms
step:1565/2330 train_time:91466ms step_avg:58.44ms
step:1566/2330 train_time:91526ms step_avg:58.45ms
step:1567/2330 train_time:91583ms step_avg:58.45ms
step:1568/2330 train_time:91644ms step_avg:58.45ms
step:1569/2330 train_time:91701ms step_avg:58.45ms
step:1570/2330 train_time:91762ms step_avg:58.45ms
step:1571/2330 train_time:91819ms step_avg:58.45ms
step:1572/2330 train_time:91882ms step_avg:58.45ms
step:1573/2330 train_time:91940ms step_avg:58.45ms
step:1574/2330 train_time:92000ms step_avg:58.45ms
step:1575/2330 train_time:92057ms step_avg:58.45ms
step:1576/2330 train_time:92119ms step_avg:58.45ms
step:1577/2330 train_time:92176ms step_avg:58.45ms
step:1578/2330 train_time:92238ms step_avg:58.45ms
step:1579/2330 train_time:92295ms step_avg:58.45ms
step:1580/2330 train_time:92356ms step_avg:58.45ms
step:1581/2330 train_time:92412ms step_avg:58.45ms
step:1582/2330 train_time:92473ms step_avg:58.45ms
step:1583/2330 train_time:92530ms step_avg:58.45ms
step:1584/2330 train_time:92592ms step_avg:58.45ms
step:1585/2330 train_time:92649ms step_avg:58.45ms
step:1586/2330 train_time:92710ms step_avg:58.46ms
step:1587/2330 train_time:92768ms step_avg:58.45ms
step:1588/2330 train_time:92830ms step_avg:58.46ms
step:1589/2330 train_time:92888ms step_avg:58.46ms
step:1590/2330 train_time:92950ms step_avg:58.46ms
step:1591/2330 train_time:93007ms step_avg:58.46ms
step:1592/2330 train_time:93070ms step_avg:58.46ms
step:1593/2330 train_time:93128ms step_avg:58.46ms
step:1594/2330 train_time:93190ms step_avg:58.46ms
step:1595/2330 train_time:93248ms step_avg:58.46ms
step:1596/2330 train_time:93308ms step_avg:58.46ms
step:1597/2330 train_time:93366ms step_avg:58.46ms
step:1598/2330 train_time:93426ms step_avg:58.46ms
step:1599/2330 train_time:93482ms step_avg:58.46ms
step:1600/2330 train_time:93545ms step_avg:58.47ms
step:1601/2330 train_time:93601ms step_avg:58.46ms
step:1602/2330 train_time:93663ms step_avg:58.47ms
step:1603/2330 train_time:93719ms step_avg:58.46ms
step:1604/2330 train_time:93781ms step_avg:58.47ms
step:1605/2330 train_time:93838ms step_avg:58.47ms
step:1606/2330 train_time:93900ms step_avg:58.47ms
step:1607/2330 train_time:93957ms step_avg:58.47ms
step:1608/2330 train_time:94019ms step_avg:58.47ms
step:1609/2330 train_time:94077ms step_avg:58.47ms
step:1610/2330 train_time:94138ms step_avg:58.47ms
step:1611/2330 train_time:94194ms step_avg:58.47ms
step:1612/2330 train_time:94256ms step_avg:58.47ms
step:1613/2330 train_time:94312ms step_avg:58.47ms
step:1614/2330 train_time:94374ms step_avg:58.47ms
step:1615/2330 train_time:94432ms step_avg:58.47ms
step:1616/2330 train_time:94492ms step_avg:58.47ms
step:1617/2330 train_time:94549ms step_avg:58.47ms
step:1618/2330 train_time:94612ms step_avg:58.47ms
step:1619/2330 train_time:94670ms step_avg:58.47ms
step:1620/2330 train_time:94731ms step_avg:58.48ms
step:1621/2330 train_time:94789ms step_avg:58.48ms
step:1622/2330 train_time:94851ms step_avg:58.48ms
step:1623/2330 train_time:94908ms step_avg:58.48ms
step:1624/2330 train_time:94969ms step_avg:58.48ms
step:1625/2330 train_time:95027ms step_avg:58.48ms
step:1626/2330 train_time:95089ms step_avg:58.48ms
step:1627/2330 train_time:95146ms step_avg:58.48ms
step:1628/2330 train_time:95208ms step_avg:58.48ms
step:1629/2330 train_time:95266ms step_avg:58.48ms
step:1630/2330 train_time:95327ms step_avg:58.48ms
step:1631/2330 train_time:95384ms step_avg:58.48ms
step:1632/2330 train_time:95445ms step_avg:58.48ms
step:1633/2330 train_time:95502ms step_avg:58.48ms
step:1634/2330 train_time:95564ms step_avg:58.48ms
step:1635/2330 train_time:95620ms step_avg:58.48ms
step:1636/2330 train_time:95681ms step_avg:58.48ms
step:1637/2330 train_time:95738ms step_avg:58.48ms
step:1638/2330 train_time:95800ms step_avg:58.49ms
step:1639/2330 train_time:95857ms step_avg:58.48ms
step:1640/2330 train_time:95918ms step_avg:58.49ms
step:1641/2330 train_time:95975ms step_avg:58.49ms
step:1642/2330 train_time:96037ms step_avg:58.49ms
step:1643/2330 train_time:96095ms step_avg:58.49ms
step:1644/2330 train_time:96155ms step_avg:58.49ms
step:1645/2330 train_time:96212ms step_avg:58.49ms
step:1646/2330 train_time:96274ms step_avg:58.49ms
step:1647/2330 train_time:96331ms step_avg:58.49ms
step:1648/2330 train_time:96392ms step_avg:58.49ms
step:1649/2330 train_time:96449ms step_avg:58.49ms
step:1650/2330 train_time:96510ms step_avg:58.49ms
step:1651/2330 train_time:96568ms step_avg:58.49ms
step:1652/2330 train_time:96629ms step_avg:58.49ms
step:1653/2330 train_time:96687ms step_avg:58.49ms
step:1654/2330 train_time:96748ms step_avg:58.49ms
step:1655/2330 train_time:96805ms step_avg:58.49ms
step:1656/2330 train_time:96867ms step_avg:58.49ms
step:1657/2330 train_time:96924ms step_avg:58.49ms
step:1658/2330 train_time:96986ms step_avg:58.50ms
step:1659/2330 train_time:97043ms step_avg:58.49ms
step:1660/2330 train_time:97106ms step_avg:58.50ms
step:1661/2330 train_time:97162ms step_avg:58.50ms
step:1662/2330 train_time:97224ms step_avg:58.50ms
step:1663/2330 train_time:97281ms step_avg:58.50ms
step:1664/2330 train_time:97342ms step_avg:58.50ms
step:1665/2330 train_time:97399ms step_avg:58.50ms
step:1666/2330 train_time:97459ms step_avg:58.50ms
step:1667/2330 train_time:97516ms step_avg:58.50ms
step:1668/2330 train_time:97578ms step_avg:58.50ms
step:1669/2330 train_time:97636ms step_avg:58.50ms
step:1670/2330 train_time:97697ms step_avg:58.50ms
step:1671/2330 train_time:97755ms step_avg:58.50ms
step:1672/2330 train_time:97815ms step_avg:58.50ms
step:1673/2330 train_time:97873ms step_avg:58.50ms
step:1674/2330 train_time:97934ms step_avg:58.50ms
step:1675/2330 train_time:97992ms step_avg:58.50ms
step:1676/2330 train_time:98052ms step_avg:58.50ms
step:1677/2330 train_time:98109ms step_avg:58.50ms
step:1678/2330 train_time:98172ms step_avg:58.51ms
step:1679/2330 train_time:98229ms step_avg:58.50ms
step:1680/2330 train_time:98291ms step_avg:58.51ms
step:1681/2330 train_time:98349ms step_avg:58.51ms
step:1682/2330 train_time:98409ms step_avg:58.51ms
step:1683/2330 train_time:98466ms step_avg:58.51ms
step:1684/2330 train_time:98528ms step_avg:58.51ms
step:1685/2330 train_time:98586ms step_avg:58.51ms
step:1686/2330 train_time:98647ms step_avg:58.51ms
step:1687/2330 train_time:98704ms step_avg:58.51ms
step:1688/2330 train_time:98765ms step_avg:58.51ms
step:1689/2330 train_time:98822ms step_avg:58.51ms
step:1690/2330 train_time:98884ms step_avg:58.51ms
step:1691/2330 train_time:98941ms step_avg:58.51ms
step:1692/2330 train_time:99003ms step_avg:58.51ms
step:1693/2330 train_time:99060ms step_avg:58.51ms
step:1694/2330 train_time:99122ms step_avg:58.51ms
step:1695/2330 train_time:99178ms step_avg:58.51ms
step:1696/2330 train_time:99240ms step_avg:58.51ms
step:1697/2330 train_time:99297ms step_avg:58.51ms
step:1698/2330 train_time:99359ms step_avg:58.52ms
step:1699/2330 train_time:99415ms step_avg:58.51ms
step:1700/2330 train_time:99478ms step_avg:58.52ms
step:1701/2330 train_time:99535ms step_avg:58.52ms
step:1702/2330 train_time:99597ms step_avg:58.52ms
step:1703/2330 train_time:99654ms step_avg:58.52ms
step:1704/2330 train_time:99715ms step_avg:58.52ms
step:1705/2330 train_time:99772ms step_avg:58.52ms
step:1706/2330 train_time:99834ms step_avg:58.52ms
step:1707/2330 train_time:99893ms step_avg:58.52ms
step:1708/2330 train_time:99954ms step_avg:58.52ms
step:1709/2330 train_time:100012ms step_avg:58.52ms
step:1710/2330 train_time:100073ms step_avg:58.52ms
step:1711/2330 train_time:100131ms step_avg:58.52ms
step:1712/2330 train_time:100192ms step_avg:58.52ms
step:1713/2330 train_time:100249ms step_avg:58.52ms
step:1714/2330 train_time:100309ms step_avg:58.52ms
step:1715/2330 train_time:100366ms step_avg:58.52ms
step:1716/2330 train_time:100429ms step_avg:58.53ms
step:1717/2330 train_time:100487ms step_avg:58.52ms
step:1718/2330 train_time:100548ms step_avg:58.53ms
step:1719/2330 train_time:100605ms step_avg:58.53ms
step:1720/2330 train_time:100667ms step_avg:58.53ms
step:1721/2330 train_time:100724ms step_avg:58.53ms
step:1722/2330 train_time:100786ms step_avg:58.53ms
step:1723/2330 train_time:100843ms step_avg:58.53ms
step:1724/2330 train_time:100905ms step_avg:58.53ms
step:1725/2330 train_time:100962ms step_avg:58.53ms
step:1726/2330 train_time:101024ms step_avg:58.53ms
step:1727/2330 train_time:101081ms step_avg:58.53ms
step:1728/2330 train_time:101142ms step_avg:58.53ms
step:1729/2330 train_time:101199ms step_avg:58.53ms
step:1730/2330 train_time:101260ms step_avg:58.53ms
step:1731/2330 train_time:101317ms step_avg:58.53ms
step:1732/2330 train_time:101379ms step_avg:58.53ms
step:1733/2330 train_time:101436ms step_avg:58.53ms
step:1734/2330 train_time:101497ms step_avg:58.53ms
step:1735/2330 train_time:101554ms step_avg:58.53ms
step:1736/2330 train_time:101616ms step_avg:58.53ms
step:1737/2330 train_time:101673ms step_avg:58.53ms
step:1738/2330 train_time:101734ms step_avg:58.54ms
step:1739/2330 train_time:101792ms step_avg:58.53ms
step:1740/2330 train_time:101853ms step_avg:58.54ms
step:1741/2330 train_time:101910ms step_avg:58.54ms
step:1742/2330 train_time:101972ms step_avg:58.54ms
step:1743/2330 train_time:102030ms step_avg:58.54ms
step:1744/2330 train_time:102092ms step_avg:58.54ms
step:1745/2330 train_time:102150ms step_avg:58.54ms
step:1746/2330 train_time:102210ms step_avg:58.54ms
step:1747/2330 train_time:102268ms step_avg:58.54ms
step:1748/2330 train_time:102330ms step_avg:58.54ms
step:1749/2330 train_time:102388ms step_avg:58.54ms
step:1750/2330 train_time:102449ms step_avg:58.54ms
step:1750/2330 val_loss:3.8216 train_time:102530ms step_avg:58.59ms
step:1751/2330 train_time:102548ms step_avg:58.57ms
step:1752/2330 train_time:102568ms step_avg:58.54ms
step:1753/2330 train_time:102623ms step_avg:58.54ms
step:1754/2330 train_time:102695ms step_avg:58.55ms
step:1755/2330 train_time:102751ms step_avg:58.55ms
step:1756/2330 train_time:102814ms step_avg:58.55ms
step:1757/2330 train_time:102871ms step_avg:58.55ms
step:1758/2330 train_time:102931ms step_avg:58.55ms
step:1759/2330 train_time:102988ms step_avg:58.55ms
step:1760/2330 train_time:103049ms step_avg:58.55ms
step:1761/2330 train_time:103105ms step_avg:58.55ms
step:1762/2330 train_time:103165ms step_avg:58.55ms
step:1763/2330 train_time:103222ms step_avg:58.55ms
step:1764/2330 train_time:103281ms step_avg:58.55ms
step:1765/2330 train_time:103338ms step_avg:58.55ms
step:1766/2330 train_time:103397ms step_avg:58.55ms
step:1767/2330 train_time:103454ms step_avg:58.55ms
step:1768/2330 train_time:103519ms step_avg:58.55ms
step:1769/2330 train_time:103578ms step_avg:58.55ms
step:1770/2330 train_time:103641ms step_avg:58.55ms
step:1771/2330 train_time:103699ms step_avg:58.55ms
step:1772/2330 train_time:103760ms step_avg:58.56ms
step:1773/2330 train_time:103818ms step_avg:58.56ms
step:1774/2330 train_time:103880ms step_avg:58.56ms
step:1775/2330 train_time:103937ms step_avg:58.56ms
step:1776/2330 train_time:103998ms step_avg:58.56ms
step:1777/2330 train_time:104055ms step_avg:58.56ms
step:1778/2330 train_time:104115ms step_avg:58.56ms
step:1779/2330 train_time:104172ms step_avg:58.56ms
step:1780/2330 train_time:104232ms step_avg:58.56ms
step:1781/2330 train_time:104289ms step_avg:58.56ms
step:1782/2330 train_time:104349ms step_avg:58.56ms
step:1783/2330 train_time:104406ms step_avg:58.56ms
step:1784/2330 train_time:104467ms step_avg:58.56ms
step:1785/2330 train_time:104524ms step_avg:58.56ms
step:1786/2330 train_time:104587ms step_avg:58.56ms
step:1787/2330 train_time:104645ms step_avg:58.56ms
step:1788/2330 train_time:104708ms step_avg:58.56ms
step:1789/2330 train_time:104765ms step_avg:58.56ms
step:1790/2330 train_time:104827ms step_avg:58.56ms
step:1791/2330 train_time:104884ms step_avg:58.56ms
step:1792/2330 train_time:104945ms step_avg:58.56ms
step:1793/2330 train_time:105002ms step_avg:58.56ms
step:1794/2330 train_time:105062ms step_avg:58.56ms
step:1795/2330 train_time:105120ms step_avg:58.56ms
step:1796/2330 train_time:105180ms step_avg:58.56ms
step:1797/2330 train_time:105237ms step_avg:58.56ms
step:1798/2330 train_time:105298ms step_avg:58.56ms
step:1799/2330 train_time:105356ms step_avg:58.56ms
step:1800/2330 train_time:105415ms step_avg:58.56ms
step:1801/2330 train_time:105473ms step_avg:58.56ms
step:1802/2330 train_time:105533ms step_avg:58.56ms
step:1803/2330 train_time:105592ms step_avg:58.56ms
step:1804/2330 train_time:105654ms step_avg:58.57ms
step:1805/2330 train_time:105711ms step_avg:58.57ms
step:1806/2330 train_time:105772ms step_avg:58.57ms
step:1807/2330 train_time:105830ms step_avg:58.57ms
step:1808/2330 train_time:105890ms step_avg:58.57ms
step:1809/2330 train_time:105947ms step_avg:58.57ms
step:1810/2330 train_time:106008ms step_avg:58.57ms
step:1811/2330 train_time:106065ms step_avg:58.57ms
step:1812/2330 train_time:106126ms step_avg:58.57ms
step:1813/2330 train_time:106183ms step_avg:58.57ms
step:1814/2330 train_time:106245ms step_avg:58.57ms
step:1815/2330 train_time:106303ms step_avg:58.57ms
step:1816/2330 train_time:106363ms step_avg:58.57ms
step:1817/2330 train_time:106421ms step_avg:58.57ms
step:1818/2330 train_time:106482ms step_avg:58.57ms
step:1819/2330 train_time:106541ms step_avg:58.57ms
step:1820/2330 train_time:106601ms step_avg:58.57ms
step:1821/2330 train_time:106659ms step_avg:58.57ms
step:1822/2330 train_time:106722ms step_avg:58.57ms
step:1823/2330 train_time:106780ms step_avg:58.57ms
step:1824/2330 train_time:106841ms step_avg:58.57ms
step:1825/2330 train_time:106899ms step_avg:58.58ms
step:1826/2330 train_time:106960ms step_avg:58.58ms
step:1827/2330 train_time:107017ms step_avg:58.58ms
step:1828/2330 train_time:107079ms step_avg:58.58ms
step:1829/2330 train_time:107135ms step_avg:58.58ms
step:1830/2330 train_time:107197ms step_avg:58.58ms
step:1831/2330 train_time:107254ms step_avg:58.58ms
step:1832/2330 train_time:107314ms step_avg:58.58ms
step:1833/2330 train_time:107371ms step_avg:58.58ms
step:1834/2330 train_time:107432ms step_avg:58.58ms
step:1835/2330 train_time:107489ms step_avg:58.58ms
step:1836/2330 train_time:107551ms step_avg:58.58ms
step:1837/2330 train_time:107608ms step_avg:58.58ms
step:1838/2330 train_time:107670ms step_avg:58.58ms
step:1839/2330 train_time:107727ms step_avg:58.58ms
step:1840/2330 train_time:107789ms step_avg:58.58ms
step:1841/2330 train_time:107846ms step_avg:58.58ms
step:1842/2330 train_time:107907ms step_avg:58.58ms
step:1843/2330 train_time:107964ms step_avg:58.58ms
step:1844/2330 train_time:108026ms step_avg:58.58ms
step:1845/2330 train_time:108083ms step_avg:58.58ms
step:1846/2330 train_time:108144ms step_avg:58.58ms
step:1847/2330 train_time:108202ms step_avg:58.58ms
step:1848/2330 train_time:108262ms step_avg:58.58ms
step:1849/2330 train_time:108320ms step_avg:58.58ms
step:1850/2330 train_time:108380ms step_avg:58.58ms
step:1851/2330 train_time:108438ms step_avg:58.58ms
step:1852/2330 train_time:108500ms step_avg:58.59ms
step:1853/2330 train_time:108557ms step_avg:58.58ms
step:1854/2330 train_time:108620ms step_avg:58.59ms
step:1855/2330 train_time:108678ms step_avg:58.59ms
step:1856/2330 train_time:108739ms step_avg:58.59ms
step:1857/2330 train_time:108798ms step_avg:58.59ms
step:1858/2330 train_time:108858ms step_avg:58.59ms
step:1859/2330 train_time:108916ms step_avg:58.59ms
step:1860/2330 train_time:108977ms step_avg:58.59ms
step:1861/2330 train_time:109034ms step_avg:58.59ms
step:1862/2330 train_time:109097ms step_avg:58.59ms
step:1863/2330 train_time:109153ms step_avg:58.59ms
step:1864/2330 train_time:109214ms step_avg:58.59ms
step:1865/2330 train_time:109271ms step_avg:58.59ms
step:1866/2330 train_time:109331ms step_avg:58.59ms
step:1867/2330 train_time:109388ms step_avg:58.59ms
step:1868/2330 train_time:109449ms step_avg:58.59ms
step:1869/2330 train_time:109507ms step_avg:58.59ms
step:1870/2330 train_time:109568ms step_avg:58.59ms
step:1871/2330 train_time:109625ms step_avg:58.59ms
step:1872/2330 train_time:109687ms step_avg:58.59ms
step:1873/2330 train_time:109744ms step_avg:58.59ms
step:1874/2330 train_time:109805ms step_avg:58.59ms
step:1875/2330 train_time:109863ms step_avg:58.59ms
step:1876/2330 train_time:109924ms step_avg:58.59ms
step:1877/2330 train_time:109982ms step_avg:58.59ms
step:1878/2330 train_time:110042ms step_avg:58.60ms
step:1879/2330 train_time:110100ms step_avg:58.59ms
step:1880/2330 train_time:110161ms step_avg:58.60ms
step:1881/2330 train_time:110219ms step_avg:58.60ms
step:1882/2330 train_time:110280ms step_avg:58.60ms
step:1883/2330 train_time:110338ms step_avg:58.60ms
step:1884/2330 train_time:110398ms step_avg:58.60ms
step:1885/2330 train_time:110457ms step_avg:58.60ms
step:1886/2330 train_time:110518ms step_avg:58.60ms
step:1887/2330 train_time:110575ms step_avg:58.60ms
step:1888/2330 train_time:110636ms step_avg:58.60ms
step:1889/2330 train_time:110694ms step_avg:58.60ms
step:1890/2330 train_time:110756ms step_avg:58.60ms
step:1891/2330 train_time:110813ms step_avg:58.60ms
step:1892/2330 train_time:110873ms step_avg:58.60ms
step:1893/2330 train_time:110931ms step_avg:58.60ms
step:1894/2330 train_time:110992ms step_avg:58.60ms
step:1895/2330 train_time:111049ms step_avg:58.60ms
step:1896/2330 train_time:111111ms step_avg:58.60ms
step:1897/2330 train_time:111167ms step_avg:58.60ms
step:1898/2330 train_time:111230ms step_avg:58.60ms
step:1899/2330 train_time:111287ms step_avg:58.60ms
step:1900/2330 train_time:111348ms step_avg:58.60ms
step:1901/2330 train_time:111405ms step_avg:58.60ms
step:1902/2330 train_time:111466ms step_avg:58.60ms
step:1903/2330 train_time:111523ms step_avg:58.60ms
step:1904/2330 train_time:111585ms step_avg:58.61ms
step:1905/2330 train_time:111642ms step_avg:58.60ms
step:1906/2330 train_time:111703ms step_avg:58.61ms
step:1907/2330 train_time:111761ms step_avg:58.61ms
step:1908/2330 train_time:111822ms step_avg:58.61ms
step:1909/2330 train_time:111879ms step_avg:58.61ms
step:1910/2330 train_time:111940ms step_avg:58.61ms
step:1911/2330 train_time:111999ms step_avg:58.61ms
step:1912/2330 train_time:112059ms step_avg:58.61ms
step:1913/2330 train_time:112117ms step_avg:58.61ms
step:1914/2330 train_time:112180ms step_avg:58.61ms
step:1915/2330 train_time:112238ms step_avg:58.61ms
step:1916/2330 train_time:112299ms step_avg:58.61ms
step:1917/2330 train_time:112358ms step_avg:58.61ms
step:1918/2330 train_time:112418ms step_avg:58.61ms
step:1919/2330 train_time:112476ms step_avg:58.61ms
step:1920/2330 train_time:112536ms step_avg:58.61ms
step:1921/2330 train_time:112594ms step_avg:58.61ms
step:1922/2330 train_time:112656ms step_avg:58.61ms
step:1923/2330 train_time:112713ms step_avg:58.61ms
step:1924/2330 train_time:112773ms step_avg:58.61ms
step:1925/2330 train_time:112830ms step_avg:58.61ms
step:1926/2330 train_time:112892ms step_avg:58.61ms
step:1927/2330 train_time:112949ms step_avg:58.61ms
step:1928/2330 train_time:113010ms step_avg:58.62ms
step:1929/2330 train_time:113067ms step_avg:58.61ms
step:1930/2330 train_time:113129ms step_avg:58.62ms
step:1931/2330 train_time:113186ms step_avg:58.62ms
step:1932/2330 train_time:113247ms step_avg:58.62ms
step:1933/2330 train_time:113304ms step_avg:58.62ms
step:1934/2330 train_time:113366ms step_avg:58.62ms
step:1935/2330 train_time:113423ms step_avg:58.62ms
step:1936/2330 train_time:113484ms step_avg:58.62ms
step:1937/2330 train_time:113542ms step_avg:58.62ms
step:1938/2330 train_time:113603ms step_avg:58.62ms
step:1939/2330 train_time:113660ms step_avg:58.62ms
step:1940/2330 train_time:113722ms step_avg:58.62ms
step:1941/2330 train_time:113781ms step_avg:58.62ms
step:1942/2330 train_time:113841ms step_avg:58.62ms
step:1943/2330 train_time:113900ms step_avg:58.62ms
step:1944/2330 train_time:113960ms step_avg:58.62ms
step:1945/2330 train_time:114017ms step_avg:58.62ms
step:1946/2330 train_time:114080ms step_avg:58.62ms
step:1947/2330 train_time:114137ms step_avg:58.62ms
step:1948/2330 train_time:114199ms step_avg:58.62ms
step:1949/2330 train_time:114256ms step_avg:58.62ms
step:1950/2330 train_time:114316ms step_avg:58.62ms
step:1951/2330 train_time:114373ms step_avg:58.62ms
step:1952/2330 train_time:114434ms step_avg:58.62ms
step:1953/2330 train_time:114491ms step_avg:58.62ms
step:1954/2330 train_time:114553ms step_avg:58.62ms
step:1955/2330 train_time:114610ms step_avg:58.62ms
step:1956/2330 train_time:114671ms step_avg:58.63ms
step:1957/2330 train_time:114728ms step_avg:58.62ms
step:1958/2330 train_time:114789ms step_avg:58.63ms
step:1959/2330 train_time:114846ms step_avg:58.62ms
step:1960/2330 train_time:114908ms step_avg:58.63ms
step:1961/2330 train_time:114964ms step_avg:58.63ms
step:1962/2330 train_time:115027ms step_avg:58.63ms
step:1963/2330 train_time:115083ms step_avg:58.63ms
step:1964/2330 train_time:115146ms step_avg:58.63ms
step:1965/2330 train_time:115203ms step_avg:58.63ms
step:1966/2330 train_time:115264ms step_avg:58.63ms
step:1967/2330 train_time:115322ms step_avg:58.63ms
step:1968/2330 train_time:115383ms step_avg:58.63ms
step:1969/2330 train_time:115441ms step_avg:58.63ms
step:1970/2330 train_time:115503ms step_avg:58.63ms
step:1971/2330 train_time:115561ms step_avg:58.63ms
step:1972/2330 train_time:115622ms step_avg:58.63ms
step:1973/2330 train_time:115680ms step_avg:58.63ms
step:1974/2330 train_time:115741ms step_avg:58.63ms
step:1975/2330 train_time:115799ms step_avg:58.63ms
step:1976/2330 train_time:115859ms step_avg:58.63ms
step:1977/2330 train_time:115917ms step_avg:58.63ms
step:1978/2330 train_time:115976ms step_avg:58.63ms
step:1979/2330 train_time:116034ms step_avg:58.63ms
step:1980/2330 train_time:116095ms step_avg:58.63ms
step:1981/2330 train_time:116152ms step_avg:58.63ms
step:1982/2330 train_time:116213ms step_avg:58.63ms
step:1983/2330 train_time:116270ms step_avg:58.63ms
step:1984/2330 train_time:116331ms step_avg:58.63ms
step:1985/2330 train_time:116388ms step_avg:58.63ms
step:1986/2330 train_time:116451ms step_avg:58.64ms
step:1987/2330 train_time:116508ms step_avg:58.64ms
step:1988/2330 train_time:116569ms step_avg:58.64ms
step:1989/2330 train_time:116626ms step_avg:58.64ms
step:1990/2330 train_time:116687ms step_avg:58.64ms
step:1991/2330 train_time:116744ms step_avg:58.64ms
step:1992/2330 train_time:116806ms step_avg:58.64ms
step:1993/2330 train_time:116863ms step_avg:58.64ms
step:1994/2330 train_time:116924ms step_avg:58.64ms
step:1995/2330 train_time:116982ms step_avg:58.64ms
step:1996/2330 train_time:117044ms step_avg:58.64ms
step:1997/2330 train_time:117101ms step_avg:58.64ms
step:1998/2330 train_time:117162ms step_avg:58.64ms
step:1999/2330 train_time:117220ms step_avg:58.64ms
step:2000/2330 train_time:117281ms step_avg:58.64ms
step:2000/2330 val_loss:3.7608 train_time:117363ms step_avg:58.68ms
step:2001/2330 train_time:117381ms step_avg:58.66ms
step:2002/2330 train_time:117402ms step_avg:58.64ms
step:2003/2330 train_time:117466ms step_avg:58.64ms
step:2004/2330 train_time:117530ms step_avg:58.65ms
step:2005/2330 train_time:117588ms step_avg:58.65ms
step:2006/2330 train_time:117648ms step_avg:58.65ms
step:2007/2330 train_time:117705ms step_avg:58.65ms
step:2008/2330 train_time:117765ms step_avg:58.65ms
step:2009/2330 train_time:117822ms step_avg:58.65ms
step:2010/2330 train_time:117882ms step_avg:58.65ms
step:2011/2330 train_time:117939ms step_avg:58.65ms
step:2012/2330 train_time:117999ms step_avg:58.65ms
step:2013/2330 train_time:118055ms step_avg:58.65ms
step:2014/2330 train_time:118116ms step_avg:58.65ms
step:2015/2330 train_time:118173ms step_avg:58.65ms
step:2016/2330 train_time:118234ms step_avg:58.65ms
step:2017/2330 train_time:118290ms step_avg:58.65ms
step:2018/2330 train_time:118352ms step_avg:58.65ms
step:2019/2330 train_time:118410ms step_avg:58.65ms
step:2020/2330 train_time:118474ms step_avg:58.65ms
step:2021/2330 train_time:118532ms step_avg:58.65ms
step:2022/2330 train_time:118594ms step_avg:58.65ms
step:2023/2330 train_time:118651ms step_avg:58.65ms
step:2024/2330 train_time:118713ms step_avg:58.65ms
step:2025/2330 train_time:118770ms step_avg:58.65ms
step:2026/2330 train_time:118832ms step_avg:58.65ms
step:2027/2330 train_time:118888ms step_avg:58.65ms
step:2028/2330 train_time:118950ms step_avg:58.65ms
step:2029/2330 train_time:119007ms step_avg:58.65ms
step:2030/2330 train_time:119067ms step_avg:58.65ms
step:2031/2330 train_time:119124ms step_avg:58.65ms
step:2032/2330 train_time:119184ms step_avg:58.65ms
step:2033/2330 train_time:119241ms step_avg:58.65ms
step:2034/2330 train_time:119302ms step_avg:58.65ms
step:2035/2330 train_time:119362ms step_avg:58.65ms
step:2036/2330 train_time:119423ms step_avg:58.66ms
step:2037/2330 train_time:119483ms step_avg:58.66ms
step:2038/2330 train_time:119545ms step_avg:58.66ms
step:2039/2330 train_time:119603ms step_avg:58.66ms
step:2040/2330 train_time:119665ms step_avg:58.66ms
step:2041/2330 train_time:119724ms step_avg:58.66ms
step:2042/2330 train_time:119784ms step_avg:58.66ms
step:2043/2330 train_time:119842ms step_avg:58.66ms
step:2044/2330 train_time:119902ms step_avg:58.66ms
step:2045/2330 train_time:119960ms step_avg:58.66ms
step:2046/2330 train_time:120021ms step_avg:58.66ms
step:2047/2330 train_time:120078ms step_avg:58.66ms
step:2048/2330 train_time:120139ms step_avg:58.66ms
step:2049/2330 train_time:120195ms step_avg:58.66ms
step:2050/2330 train_time:120256ms step_avg:58.66ms
step:2051/2330 train_time:120313ms step_avg:58.66ms
step:2052/2330 train_time:120375ms step_avg:58.66ms
step:2053/2330 train_time:120432ms step_avg:58.66ms
step:2054/2330 train_time:120494ms step_avg:58.66ms
step:2055/2330 train_time:120551ms step_avg:58.66ms
step:2056/2330 train_time:120613ms step_avg:58.66ms
step:2057/2330 train_time:120671ms step_avg:58.66ms
step:2058/2330 train_time:120732ms step_avg:58.66ms
step:2059/2330 train_time:120789ms step_avg:58.66ms
step:2060/2330 train_time:120851ms step_avg:58.67ms
step:2061/2330 train_time:120908ms step_avg:58.66ms
step:2062/2330 train_time:120970ms step_avg:58.67ms
step:2063/2330 train_time:121027ms step_avg:58.67ms
step:2064/2330 train_time:121088ms step_avg:58.67ms
step:2065/2330 train_time:121145ms step_avg:58.67ms
step:2066/2330 train_time:121206ms step_avg:58.67ms
step:2067/2330 train_time:121264ms step_avg:58.67ms
step:2068/2330 train_time:121324ms step_avg:58.67ms
step:2069/2330 train_time:121383ms step_avg:58.67ms
step:2070/2330 train_time:121444ms step_avg:58.67ms
step:2071/2330 train_time:121501ms step_avg:58.67ms
step:2072/2330 train_time:121563ms step_avg:58.67ms
step:2073/2330 train_time:121622ms step_avg:58.67ms
step:2074/2330 train_time:121683ms step_avg:58.67ms
step:2075/2330 train_time:121741ms step_avg:58.67ms
step:2076/2330 train_time:121802ms step_avg:58.67ms
step:2077/2330 train_time:121860ms step_avg:58.67ms
step:2078/2330 train_time:121920ms step_avg:58.67ms
step:2079/2330 train_time:121977ms step_avg:58.67ms
step:2080/2330 train_time:122037ms step_avg:58.67ms
step:2081/2330 train_time:122094ms step_avg:58.67ms
step:2082/2330 train_time:122155ms step_avg:58.67ms
step:2083/2330 train_time:122212ms step_avg:58.67ms
step:2084/2330 train_time:122273ms step_avg:58.67ms
step:2085/2330 train_time:122330ms step_avg:58.67ms
step:2086/2330 train_time:122392ms step_avg:58.67ms
step:2087/2330 train_time:122449ms step_avg:58.67ms
step:2088/2330 train_time:122511ms step_avg:58.67ms
step:2089/2330 train_time:122569ms step_avg:58.67ms
step:2090/2330 train_time:122630ms step_avg:58.67ms
step:2091/2330 train_time:122686ms step_avg:58.67ms
step:2092/2330 train_time:122749ms step_avg:58.68ms
step:2093/2330 train_time:122806ms step_avg:58.67ms
step:2094/2330 train_time:122869ms step_avg:58.68ms
step:2095/2330 train_time:122927ms step_avg:58.68ms
step:2096/2330 train_time:122987ms step_avg:58.68ms
step:2097/2330 train_time:123045ms step_avg:58.68ms
step:2098/2330 train_time:123105ms step_avg:58.68ms
step:2099/2330 train_time:123164ms step_avg:58.68ms
step:2100/2330 train_time:123224ms step_avg:58.68ms
step:2101/2330 train_time:123282ms step_avg:58.68ms
step:2102/2330 train_time:123343ms step_avg:58.68ms
step:2103/2330 train_time:123401ms step_avg:58.68ms
step:2104/2330 train_time:123462ms step_avg:58.68ms
step:2105/2330 train_time:123520ms step_avg:58.68ms
step:2106/2330 train_time:123581ms step_avg:58.68ms
step:2107/2330 train_time:123640ms step_avg:58.68ms
step:2108/2330 train_time:123700ms step_avg:58.68ms
step:2109/2330 train_time:123757ms step_avg:58.68ms
step:2110/2330 train_time:123818ms step_avg:58.68ms
step:2111/2330 train_time:123876ms step_avg:58.68ms
step:2112/2330 train_time:123938ms step_avg:58.68ms
step:2113/2330 train_time:123995ms step_avg:58.68ms
step:2114/2330 train_time:124055ms step_avg:58.68ms
step:2115/2330 train_time:124112ms step_avg:58.68ms
step:2116/2330 train_time:124173ms step_avg:58.68ms
step:2117/2330 train_time:124230ms step_avg:58.68ms
step:2118/2330 train_time:124292ms step_avg:58.68ms
step:2119/2330 train_time:124349ms step_avg:58.68ms
step:2120/2330 train_time:124410ms step_avg:58.68ms
step:2121/2330 train_time:124467ms step_avg:58.68ms
step:2122/2330 train_time:124529ms step_avg:58.68ms
step:2123/2330 train_time:124586ms step_avg:58.68ms
step:2124/2330 train_time:124648ms step_avg:58.69ms
step:2125/2330 train_time:124706ms step_avg:58.69ms
step:2126/2330 train_time:124767ms step_avg:58.69ms
step:2127/2330 train_time:124825ms step_avg:58.69ms
step:2128/2330 train_time:124886ms step_avg:58.69ms
step:2129/2330 train_time:124944ms step_avg:58.69ms
step:2130/2330 train_time:125004ms step_avg:58.69ms
step:2131/2330 train_time:125062ms step_avg:58.69ms
step:2132/2330 train_time:125122ms step_avg:58.69ms
step:2133/2330 train_time:125181ms step_avg:58.69ms
step:2134/2330 train_time:125241ms step_avg:58.69ms
step:2135/2330 train_time:125298ms step_avg:58.69ms
step:2136/2330 train_time:125357ms step_avg:58.69ms
step:2137/2330 train_time:125415ms step_avg:58.69ms
step:2138/2330 train_time:125476ms step_avg:58.69ms
step:2139/2330 train_time:125533ms step_avg:58.69ms
step:2140/2330 train_time:125595ms step_avg:58.69ms
step:2141/2330 train_time:125651ms step_avg:58.69ms
step:2142/2330 train_time:125714ms step_avg:58.69ms
step:2143/2330 train_time:125771ms step_avg:58.69ms
step:2144/2330 train_time:125833ms step_avg:58.69ms
step:2145/2330 train_time:125889ms step_avg:58.69ms
step:2146/2330 train_time:125952ms step_avg:58.69ms
step:2147/2330 train_time:126009ms step_avg:58.69ms
step:2148/2330 train_time:126070ms step_avg:58.69ms
step:2149/2330 train_time:126128ms step_avg:58.69ms
step:2150/2330 train_time:126190ms step_avg:58.69ms
step:2151/2330 train_time:126247ms step_avg:58.69ms
step:2152/2330 train_time:126308ms step_avg:58.69ms
step:2153/2330 train_time:126366ms step_avg:58.69ms
step:2154/2330 train_time:126427ms step_avg:58.69ms
step:2155/2330 train_time:126484ms step_avg:58.69ms
step:2156/2330 train_time:126545ms step_avg:58.69ms
step:2157/2330 train_time:126602ms step_avg:58.69ms
step:2158/2330 train_time:126663ms step_avg:58.69ms
step:2159/2330 train_time:126722ms step_avg:58.69ms
step:2160/2330 train_time:126782ms step_avg:58.70ms
step:2161/2330 train_time:126840ms step_avg:58.70ms
step:2162/2330 train_time:126901ms step_avg:58.70ms
step:2163/2330 train_time:126959ms step_avg:58.70ms
step:2164/2330 train_time:127019ms step_avg:58.70ms
step:2165/2330 train_time:127077ms step_avg:58.70ms
step:2166/2330 train_time:127137ms step_avg:58.70ms
step:2167/2330 train_time:127194ms step_avg:58.70ms
step:2168/2330 train_time:127256ms step_avg:58.70ms
step:2169/2330 train_time:127313ms step_avg:58.70ms
step:2170/2330 train_time:127375ms step_avg:58.70ms
step:2171/2330 train_time:127432ms step_avg:58.70ms
step:2172/2330 train_time:127494ms step_avg:58.70ms
step:2173/2330 train_time:127551ms step_avg:58.70ms
step:2174/2330 train_time:127612ms step_avg:58.70ms
step:2175/2330 train_time:127669ms step_avg:58.70ms
step:2176/2330 train_time:127731ms step_avg:58.70ms
step:2177/2330 train_time:127787ms step_avg:58.70ms
step:2178/2330 train_time:127849ms step_avg:58.70ms
step:2179/2330 train_time:127906ms step_avg:58.70ms
step:2180/2330 train_time:127968ms step_avg:58.70ms
step:2181/2330 train_time:128026ms step_avg:58.70ms
step:2182/2330 train_time:128086ms step_avg:58.70ms
step:2183/2330 train_time:128144ms step_avg:58.70ms
step:2184/2330 train_time:128205ms step_avg:58.70ms
step:2185/2330 train_time:128262ms step_avg:58.70ms
step:2186/2330 train_time:128323ms step_avg:58.70ms
step:2187/2330 train_time:128382ms step_avg:58.70ms
step:2188/2330 train_time:128443ms step_avg:58.70ms
step:2189/2330 train_time:128502ms step_avg:58.70ms
step:2190/2330 train_time:128562ms step_avg:58.70ms
step:2191/2330 train_time:128620ms step_avg:58.70ms
step:2192/2330 train_time:128680ms step_avg:58.70ms
step:2193/2330 train_time:128737ms step_avg:58.70ms
step:2194/2330 train_time:128797ms step_avg:58.70ms
step:2195/2330 train_time:128854ms step_avg:58.70ms
step:2196/2330 train_time:128916ms step_avg:58.70ms
step:2197/2330 train_time:128973ms step_avg:58.70ms
step:2198/2330 train_time:129034ms step_avg:58.71ms
step:2199/2330 train_time:129091ms step_avg:58.70ms
step:2200/2330 train_time:129153ms step_avg:58.71ms
step:2201/2330 train_time:129211ms step_avg:58.71ms
step:2202/2330 train_time:129272ms step_avg:58.71ms
step:2203/2330 train_time:129329ms step_avg:58.71ms
step:2204/2330 train_time:129390ms step_avg:58.71ms
step:2205/2330 train_time:129448ms step_avg:58.71ms
step:2206/2330 train_time:129509ms step_avg:58.71ms
step:2207/2330 train_time:129566ms step_avg:58.71ms
step:2208/2330 train_time:129627ms step_avg:58.71ms
step:2209/2330 train_time:129685ms step_avg:58.71ms
step:2210/2330 train_time:129746ms step_avg:58.71ms
step:2211/2330 train_time:129803ms step_avg:58.71ms
step:2212/2330 train_time:129865ms step_avg:58.71ms
step:2213/2330 train_time:129923ms step_avg:58.71ms
step:2214/2330 train_time:129983ms step_avg:58.71ms
step:2215/2330 train_time:130041ms step_avg:58.71ms
step:2216/2330 train_time:130102ms step_avg:58.71ms
step:2217/2330 train_time:130159ms step_avg:58.71ms
step:2218/2330 train_time:130220ms step_avg:58.71ms
step:2219/2330 train_time:130278ms step_avg:58.71ms
step:2220/2330 train_time:130338ms step_avg:58.71ms
step:2221/2330 train_time:130395ms step_avg:58.71ms
step:2222/2330 train_time:130456ms step_avg:58.71ms
step:2223/2330 train_time:130514ms step_avg:58.71ms
step:2224/2330 train_time:130575ms step_avg:58.71ms
step:2225/2330 train_time:130632ms step_avg:58.71ms
step:2226/2330 train_time:130694ms step_avg:58.71ms
step:2227/2330 train_time:130751ms step_avg:58.71ms
step:2228/2330 train_time:130813ms step_avg:58.71ms
step:2229/2330 train_time:130869ms step_avg:58.71ms
step:2230/2330 train_time:130932ms step_avg:58.71ms
step:2231/2330 train_time:130988ms step_avg:58.71ms
step:2232/2330 train_time:131051ms step_avg:58.71ms
step:2233/2330 train_time:131108ms step_avg:58.71ms
step:2234/2330 train_time:131170ms step_avg:58.72ms
step:2235/2330 train_time:131227ms step_avg:58.71ms
step:2236/2330 train_time:131288ms step_avg:58.72ms
step:2237/2330 train_time:131346ms step_avg:58.72ms
step:2238/2330 train_time:131407ms step_avg:58.72ms
step:2239/2330 train_time:131466ms step_avg:58.72ms
step:2240/2330 train_time:131526ms step_avg:58.72ms
step:2241/2330 train_time:131585ms step_avg:58.72ms
step:2242/2330 train_time:131646ms step_avg:58.72ms
step:2243/2330 train_time:131703ms step_avg:58.72ms
step:2244/2330 train_time:131763ms step_avg:58.72ms
step:2245/2330 train_time:131821ms step_avg:58.72ms
step:2246/2330 train_time:131881ms step_avg:58.72ms
step:2247/2330 train_time:131939ms step_avg:58.72ms
step:2248/2330 train_time:131999ms step_avg:58.72ms
step:2249/2330 train_time:132056ms step_avg:58.72ms
step:2250/2330 train_time:132118ms step_avg:58.72ms
step:2250/2330 val_loss:3.7133 train_time:132200ms step_avg:58.76ms
step:2251/2330 train_time:132217ms step_avg:58.74ms
step:2252/2330 train_time:132239ms step_avg:58.72ms
step:2253/2330 train_time:132296ms step_avg:58.72ms
step:2254/2330 train_time:132360ms step_avg:58.72ms
step:2255/2330 train_time:132417ms step_avg:58.72ms
step:2256/2330 train_time:132479ms step_avg:58.72ms
step:2257/2330 train_time:132535ms step_avg:58.72ms
step:2258/2330 train_time:132597ms step_avg:58.72ms
step:2259/2330 train_time:132653ms step_avg:58.72ms
step:2260/2330 train_time:132713ms step_avg:58.72ms
step:2261/2330 train_time:132770ms step_avg:58.72ms
step:2262/2330 train_time:132831ms step_avg:58.72ms
step:2263/2330 train_time:132888ms step_avg:58.72ms
step:2264/2330 train_time:132949ms step_avg:58.72ms
step:2265/2330 train_time:133005ms step_avg:58.72ms
step:2266/2330 train_time:133065ms step_avg:58.72ms
step:2267/2330 train_time:133124ms step_avg:58.72ms
step:2268/2330 train_time:133186ms step_avg:58.72ms
step:2269/2330 train_time:133246ms step_avg:58.72ms
step:2270/2330 train_time:133307ms step_avg:58.73ms
step:2271/2330 train_time:133365ms step_avg:58.73ms
step:2272/2330 train_time:133427ms step_avg:58.73ms
step:2273/2330 train_time:133485ms step_avg:58.73ms
step:2274/2330 train_time:133546ms step_avg:58.73ms
step:2275/2330 train_time:133603ms step_avg:58.73ms
step:2276/2330 train_time:133663ms step_avg:58.73ms
step:2277/2330 train_time:133720ms step_avg:58.73ms
step:2278/2330 train_time:133780ms step_avg:58.73ms
step:2279/2330 train_time:133837ms step_avg:58.73ms
step:2280/2330 train_time:133898ms step_avg:58.73ms
step:2281/2330 train_time:133954ms step_avg:58.73ms
step:2282/2330 train_time:134016ms step_avg:58.73ms
step:2283/2330 train_time:134073ms step_avg:58.73ms
step:2284/2330 train_time:134135ms step_avg:58.73ms
step:2285/2330 train_time:134192ms step_avg:58.73ms
step:2286/2330 train_time:134255ms step_avg:58.73ms
step:2287/2330 train_time:134312ms step_avg:58.73ms
step:2288/2330 train_time:134376ms step_avg:58.73ms
step:2289/2330 train_time:134433ms step_avg:58.73ms
step:2290/2330 train_time:134495ms step_avg:58.73ms
step:2291/2330 train_time:134552ms step_avg:58.73ms
step:2292/2330 train_time:134613ms step_avg:58.73ms
step:2293/2330 train_time:134670ms step_avg:58.73ms
step:2294/2330 train_time:134732ms step_avg:58.73ms
step:2295/2330 train_time:134789ms step_avg:58.73ms
step:2296/2330 train_time:134850ms step_avg:58.73ms
step:2297/2330 train_time:134908ms step_avg:58.73ms
step:2298/2330 train_time:134968ms step_avg:58.73ms
step:2299/2330 train_time:135025ms step_avg:58.73ms
step:2300/2330 train_time:135085ms step_avg:58.73ms
step:2301/2330 train_time:135143ms step_avg:58.73ms
step:2302/2330 train_time:135205ms step_avg:58.73ms
step:2303/2330 train_time:135262ms step_avg:58.73ms
step:2304/2330 train_time:135324ms step_avg:58.73ms
step:2305/2330 train_time:135383ms step_avg:58.73ms
step:2306/2330 train_time:135444ms step_avg:58.74ms
step:2307/2330 train_time:135503ms step_avg:58.74ms
step:2308/2330 train_time:135563ms step_avg:58.74ms
step:2309/2330 train_time:135621ms step_avg:58.74ms
step:2310/2330 train_time:135682ms step_avg:58.74ms
step:2311/2330 train_time:135739ms step_avg:58.74ms
step:2312/2330 train_time:135801ms step_avg:58.74ms
step:2313/2330 train_time:135857ms step_avg:58.74ms
step:2314/2330 train_time:135919ms step_avg:58.74ms
step:2315/2330 train_time:135975ms step_avg:58.74ms
step:2316/2330 train_time:136036ms step_avg:58.74ms
step:2317/2330 train_time:136092ms step_avg:58.74ms
step:2318/2330 train_time:136154ms step_avg:58.74ms
step:2319/2330 train_time:136211ms step_avg:58.74ms
step:2320/2330 train_time:136273ms step_avg:58.74ms
step:2321/2330 train_time:136330ms step_avg:58.74ms
step:2322/2330 train_time:136392ms step_avg:58.74ms
step:2323/2330 train_time:136449ms step_avg:58.74ms
step:2324/2330 train_time:136511ms step_avg:58.74ms
step:2325/2330 train_time:136569ms step_avg:58.74ms
step:2326/2330 train_time:136630ms step_avg:58.74ms
step:2327/2330 train_time:136689ms step_avg:58.74ms
step:2328/2330 train_time:136750ms step_avg:58.74ms
step:2329/2330 train_time:136809ms step_avg:58.74ms
step:2330/2330 train_time:136869ms step_avg:58.74ms
step:2330/2330 val_loss:3.6979 train_time:136951ms step_avg:58.78ms
peak memory allocated: 27822 MiB reserved: 44076 MiB
